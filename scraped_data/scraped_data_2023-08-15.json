[
  {
    "title": "OctoPack: Instruction Tuning Code Large Language Models",
    "link": "https://arxiv.org/pdf/2308.07124.pdf",
    "upvote": "26",
    "text": "OCTOPACK: INSTRUCTION TUNING CODE LARGE\nLANGUAGE MODELS\nNiklas Muennighoff\nQian Liu\nArmel Zebaze\nQinkai Zheng\nBinyuan Hui\nTerry Yue Zhuo\nSwayam Singh\nXiangru Tang\nLeandro von Werra\nShayne Longpre\nn.muennighoff@gmail.com\nABSTRACT\nFinetuning large language models (LLMs) on instructions leads to vast performance\nimprovements on natural language tasks. We apply instruction tuning using code,\nleveraging the natural structure of Git commits, which pair code changes with\nhuman instructions. We compile COMMITPACK: 4 terabytes of Git commits across\n350 programming languages. We benchmark COMMITPACK against other natural\nand synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter\nStarCoder model, and achieve state-of-the-art performance among models not\ntrained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1).\nWe further introduce HUMANEVALPACK, expanding the HumanEval benchmark to\na total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across\n6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OCTOCODER\nand OCTOGEEX, achieve the best performance across HUMANEVALPACK among\nall permissive models, demonstrating COMMITPACK\u2019s benefits in generalizing to a\nwider set of languages and natural coding tasks. Code, models and data are freely\navailable at https://github.com/bigcode-project/octopack.\nimport numpy as np\nimport matplotlib.pyplot as plt\n# generate sample data\nx_data = np.linspace(-5, 5, 20)\ny_data = np.random.normal(0.0, 1.0, x_data.size)\nplt.plot(x_data, y_data, 'o')\nplt.show()\nCode Before\nCommit\nMessage\nCode After\nChange to sin() function with noise\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n# generate sample data\nx_data = np.linspace(-math.pi, math.pi, 30)\ny_data = np.sin(x_data) + np.random.normal(0.0, 0.1, x_data.size)\nplt.plot(x_data, y_data, 'o')\nplt.show()\n1) CommitPack\n2) HumanEvalPack\nFigure 1: OCTOPACK Overview. 1) Sample from our 4TB dataset, COMMITPACK. 2) Performance\nof OCTOCODER, OCTOGEEX and other code models including non-permissive ones (WizardCoder,\nGPT-4) on HUMANEVALPACK spanning 3 coding tasks and 6 programming languages.\narXiv:2308.07124v2  [cs.CL]  18 Feb 2024\nOctoPack: Instruction Tuning Code Large Language Models\n1\nINTRODUCTION\nFinetuning large language models (LLMs) on a variety of language tasks explained via instructions\n(instruction tuning) has been shown to improve model usability and general performance (Wei et al.,\n2022; Sanh et al., 2022; Min et al., 2022; Ouyang et al., 2022). The instruction tuning paradigm has\nalso proven successful for models trained on visual (Liu et al., 2023a; Li et al., 2023a), audio (Zhang\net al., 2023b) and multilingual (Muennighoff et al., 2022b; Wang et al., 2022b) data.\nIn this work, we instruction tune LLMs on the coding modality. While Code LLMs can already\nbe indirectly instructed to generate desired code using code comments, this procedure is brittle\nand does not work when the desired output is natural language, such as explaining code. Explicit\ninstructing tuning of Code LLMs may improve their steerability and enable their application to\nmore tasks. Concurrently to our work, three instruction tuned Code LLMs have been proposed:\nPanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) and InstructCodeT5+ (Wang et al.,\n2023c). These models rely on more capable and closed models from the OpenAI API1 to create\ntheir instruction training data. This approach is problematic as (1) closed-source APIs keep changing\nand have unpredictable availability (Pozzobon et al., 2023; Chen et al., 2023a), (2) it relies on the\nassumption that a more capable model exists (3) it can reinforce model hallucination (Gudibande\net al., 2023) and (4), depending on legal interpretation, OpenAI\u2019s terms of use2 forbid such models:\n\u201c...You may not...use output from the Services to develop models that compete with OpenAI...\u201d. Thus,\nwe consider models trained on OpenAI outputs not usable for commercial purposes in practice and\nclassify them as non-permissive in this work.\nWe focus on more permissively licensed data and avoid using a closed-source model to generate\nsynthetic data. We benchmark four popular sources of code instruction data: (1) xP3x (Muennighoff\net al., 2022b), which contains data from common code benchmarks, (2) Self-Instruct (Wang et al.,\n2023a) data we create using a permissive Code LLM, (3) OASST (K\u00f6pf et al., 2023), which contains\nmostly natural language data and few code examples and (4) COMMITPACK, our new 4TB dataset of\nGit commits. Instruction tuning\u2019s primary purpose is to expand models\u2019 generalization abilities to a\nwide variety of tasks and settings. Thus, we extend the code synthesis benchmark, HumanEval (Chen\net al., 2021; Zheng et al., 2023), to create HUMANEVALPACK: A code benchmark covering code\nsynthesis, code repair, and code explanation across six programming languages.\nInstruction tuning StarCoder (Li et al., 2023b) on a filtered variant of COMMITPACK and OASST\nleads to our best model, OCTOCODER, which surpasses all other openly licensed models (Figure 1),\nbut falls short of the much larger GPT-4 (OpenAI, 2023). GPT-4 is close to maximum performance on\nthe code synthesis variant, notably with a pass@1 score of 86.6% on Python HumanEval. However,\nit performs significantly worse on the code fixing and explanation variants of HUMANEVALPACK,\nwhich we introduce. This suggests that the original HumanEval benchmark may soon cease to be\nuseful due to models reaching close to the maximum performance. Our more challenging evaluation\nvariants provide room for future LLMs to improve on the performance of the current state-of-the-art.\nIn summary, we contribute:\n\u2022 COMMITPACK and COMMITPACKFT: 4TB of permissively licensed code commits across\n350 programming languages for pretraining and a filtered 2GB variant containing high-\nquality code instructions used for finetuning\n\u2022 HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios\n(Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python,\nJavaScript, Java, Go, C++, Rust)\n\u2022 OCTOCODER and OCTOGEEX: The best permissive Code LLMs\n2\nCOMMITPACK: CODE INSTRUCTION DATA\nPrior work has shown that models can generalize to languages included in pretraining, but absent\nduring instruction tuning (Muennighoff et al., 2022b). However, they also show that including such\n1https://openai.com/blog/openai-api\n2https://openai.com/policies/terms-of-use\n2\nOctoPack: Instruction Tuning Code Large Language Models\nMarkdown\nPython\nJavaScript\nJava\nJSON\nRuby\nC\nYAML\nPHP\nC++\nHTML\nXML\nText\nGo\nShell\nC#\nTypeScript\nCSS\nreStructuredText\nPerl\nMakefile\nSwift\nScala\nINI\nRust\nCoffeeScript\nSCSS\nDockerfile\nHTML ERB\nNix\nHaskell\ngettextCatalog\nClojure\nJSX\nLua\nSQL\nKotlin\nGroovy\nAsciiDoc\nJupyter Notebook\nErlang\nTeX\nLess\nEmacs Lisp\nOCaml\nOther (305 lang.)\n1K\n10K\n100K\n1M\n10M\n# Samples CommitPack\n1K\n10K\n100K\n1M\n10M\n# Samples CommitPackFT\nConfiguration (4.61%)\nDependencies (5.38%)\nDocumentation (3.93%)\nBuild System/Tooling (1.30%)\nDeprecation (0.28%)\n(13.32%) Testing\n(0.88%) User Interface\n(25.57%) New Features\nRelease Management (4.14%)\nFormatting/Linting (0.40%)\nRefactoring/Code Cleanup (19.78%)\n(0.62%) Logging/Instrumentation\n(19.02%) Bug Fixes\n(0.64%) Performance Improvements\nDevelopment\nTesting & QA\nBug\nImprovement\nMisc\n1\nFigure 2: Overview of COMMITPACK and COMMITPACKFT. Top: Language distribution\nof the full commit data (COMMITPACK) and the variant filtered for high-quality instructions\n(COMMITPACKFT). See Appendix C for the full distribution. Bottom: Task distribution of commits\non the Python subset of COMMITPACKFT (59K samples) according to GPT-4.\nBase dataset\nSubset\nDataset (\u2193)\nLang.\nSamples\nCode fraction\nLang.\nSamples\nCode fraction\nxP3x\n8\n532,107,156\n0.67%\n8\n5,000\n100%\nStarCoder Self-Instruct\n12\n5,003\n100%\n12\n5,003\n100%\nOASST\n49\n161,443\n0.9%\n28\n8,587\n2.5%\nCOMMITPACKFT\n277\n742,273\n100%\n6\n5,000\n100%\nTable 1: Statistics of code instruction data we consider. We display the number of programming\nlanguages, total samples, and fraction of samples that contain code for permissive instruction datasets.\nFor finetuning on these datasets, we use small subsets with around 5,000 samples each.\nlanguages during instruction tuning boosts their performance further. We hypothesize that code data\nexhibits the same behavior. To improve performance on code-related tasks, we thus construct a code\ninstruction dataset leveraging the natural structure of Git commits.\nCOMMITPACK\nTo create the dataset, we use commit metadata from the GitHub action dump on\nGoogle BigQuery.3 We apply quality filters, filter for commercially friendly licenses, and discard\ncommits that affect more than a single file to ensure commit messages are very specific and to avoid\nadditional complexity from dealing with multiple files. We use the filtered metadata to scrape the\naffected code files prior to and after the commit from GitHub. This leads to almost 4 terabytes\nof data covering 350 programming languages (COMMITPACK). As instruction tuning does not\nrequire so much data (Zhou et al., 2023a; Touvron et al., 2023), we apply several strict filters to\n3https://www.gharchive.org/\n3\nOctoPack: Instruction Tuning Code Large Language Models\nreduce the dataset to 2 gigabytes and 277 languages (COMMITPACKFT). These include filtering\nfor samples where the commit message has specific words in uppercase imperative form at the start\n(e.g. \"Verify ...\"), consists of multiple words, and does not contain external references. All filters are\ndetailed in Appendix D. Figure 2 depicts the distribution of both datasets and the tasks contained\nin COMMITPACKFT. For instruction tuning our models, we select 5,000 random samples from\nCOMMITPACKFT across the 6 programming languages that we evaluate on. In Appendix G, we also\nexperiment with pretraining on the entirety of COMMITPACK.\nAlternatives\nWe consider three additional datasets for instruction tuning presented in Table 1.\nxP3x: xP3x is a large-scale collection of multilingual instruction data with around 532 million\nsamples (Muennighoff et al., 2022b). We focus only on the code subset of xP3x, excluding Neural-\nCodeSearch (Li et al., 2019) which is not licensed permissively, and select 5,000 samples.\nSelf-Instruct: Using the Self-Instruct method (Wang et al., 2022a) and the StarCoder model (Li et al.,\n2023b), we create 5,003 synthetic instructions and corresponding answers.\nOASST: OASST is a diverse dataset of multi-turn chat dialogues (K\u00f6pf et al., 2023). Only a few of\nthe dialogues contain code. We reuse a filtered variant from prior work (Dettmers et al., 2023) and\nadditionally filter out moralizing assistant answers (Appendix D) leading to 8,587 samples.\n3\nHUMANEVALPACK: EVALUATING INSTRUCTION TUNED CODE MODELS\nLanguages: Python, JavaScript, Java, Go, C++, Rust\nSubtasks: HumanEvalFix, HumanEvalExplain, HumanEvalSynthesize\nWrite a Python function `has_close_elements(numbers: List[float],\nthreshold: float) -> bool` to solve the following problem:\nCheck if in given list of numbers, are any two numbers closer to\neach other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\nHumanEvalPack\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) ->\nbool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer\nto each other than given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) ->\nbool:for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\nProvide a concise natural language description of the function using\nat most 213 characters.\nCheck if in given list of numbers, are any two numbers closer to\neach other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\nCheck if in given list of numbers, are any...\n...\nWrite functional code in Python according to the description.\nFix Code\nExplain Code\nSynthesize Code\nModel Input\nMetric: Pass@k\nCreation: Humans\nTarget Output\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) ->\nbool:\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = elem - elem2\n                if distance < threshold:\n                    return True\n    return False\ndef check(has_close_elements):\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) ==\nTrue\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) ==\nFalse\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) ==\nTrue\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\ncheck(has_close_elements)\nFix bugs in has_close_elements.\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) ->\nbool:\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) ->\nbool:\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\nFigure 3: HUMANEVALPACK overview. The first HumanEval problem is depicted across the three\nscenarios for Python. The bug for HUMANEVALFIX consists of a missing \"abs\" statement.\nWhen instruction tuning LLMs using natural language (NL) data, the input is an NL instruction with\noptional NL context and the target output is the NL answer to the task (Wei et al., 2022). When\ninstruction tuning with code (C) data, code may either appear only in the input alongside the NL\ninstruction (NL+C\u2192NL, e.g. code explanation), only in the output (NL\u2192C, e.g. code synthesis), or\nin both input and output (NL+C\u2192C, e.g. code modifications like bug fixing). While prior benchmarks\ncommonly only cover variants of code synthesis, users may want to use models in all three scenarios.\nThus, we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to\ncover all three input-output combinations for six languages (Figure 3).\n4\nOctoPack: Instruction Tuning Code Large Language Models\nHUMANEVALFIX (NL+C\u2192C)\nGiven an incorrect code function with a subtle bug and accom-\npanying unit tests, the model is tasked to fix the function. We manually add a bug to each of the\n164 HumanEval solutions across all 6 languages (984 total bugs). For a given sample, the bugs\nare as similar as possible across the 6 languages enabling meaningful comparison of scores across\nlanguages. Bugs are written such that the code still runs but produces an incorrect result leading to\nat least one unit test failing. Bug statistics and examples are in Appendix L. We also evaluate an\neasier variant of this task where instead of unit tests, models are provided with the correct function\ndocstring as the source of truth to fix bugs, see Appendix K.\nHUMANEVALEXPLAIN (NL+C\u2192NL)\nGiven a correct code function, the model is tasked to\ngenerate an explanation of the code. Subsequently, the same model is tasked to regenerate the code\ngiven only its own explanation. The second step allows us to score this task via code execution and\nmeasure pass@k (Chen et al., 2021) instead of evaluating the explanation itself using heuristic-based\nmetrics like BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) which have major limitations (Reiter,\n2018; Schluter, 2017; Eghbali & Pradel, 2022; Zhou et al., 2023b). To prevent models from copying\nthe solution into the description, we remove any solution overlap of at least 20 characters from\nthe description. We further enforce a character length limit on the model-generated explanation\nequivalent to the length of the docstring describing the function. This limit is specified in the prompt\nfor the model. Note that the function docstring itself is never provided to the model for this task.\nHUMANEVALSYNTHESIZE (NL\u2192C)\nGiven a natural language docstring or comment describing\nthe desired code, the model is tasked to synthesize the correct code. This task corresponds to the\noriginal HumanEval benchmark (Chen et al., 2021). For instruction tuned models, we add an explicit\ninstruction to the input explaining what the model should do. For models that have only gone through\nlanguage model pretraining, we follow Chen et al. (2021) and provide the model with the function\nheader and docstring to evaluate its completion of the function.\nFor all tasks we execute the code generations to compute performance using the pass@k metric (Chen\net al., 2021): a problem is considered solved if any of k code generations passes every test case. We\nfocus on the simplest version of pass@k, which is pass@1: the likelihood that the model solves\na problem in a single attempt. Like Chen et al. (2021), we use a sampling temperature of 0.2 and\ntopp = 0.95 to estimate pass@1. We generate n = 20 samples, which is enough to get reliable\npass@1 estimates (Li et al., 2023b). For GPT-4, we generate n = 1 samples. Using n = 1 instead of\nn = 20 for GPT-4 only changed scores from 75.0% to 75.2% pass@1 on HUMANEVALSYNTHESIZE\nPython while providing 20x cost savings.\nPython HumanEval is the most widely used code benchmark and many training datasets have already\nbeen decontaminated for it (Kocetkov et al., 2022). By manually extending HumanEval, we ensure\nexisting decontamination remains valid to enable fair evaluation. However, this may not hold for all\nmodels (e.g. GPT-4), thus results should be interpreted carefully.\n4\nOCTOCODER: BEST COMMERCIALLY LICENSED CODE LLM\n4.1\nABLATING INSTRUCTION DATA CHOICES\nWe instruction tune the pretrained StarCoder model (Li et al., 2023b) on different combinations of\nour instruction datasets (\u00a72). We evaluate all models on the Python subset of HUMANEVALPACK\nas depicted in Figure 4. Similar to prior work (Taori et al., 2023), we format all instructions into a\nconsistent schema to distinguish question and answer (see Figure 18).\nCOMMITPACKFT enables CodeLLMs to fix bugs\nCOMMITPACKFT is critical for the perfor-\nmance boost on code repair (HUMANEVALFIX), where instruction tuning on only OASST or other\nvariants results in a significantly lower score. This is likely due to COMMITPACKFT including around\n20% of bug fixes among other code-related tasks (Figure 2).\nImportance of samples with natural language targets\nThe pretrained StarCoder model, as well\nas the Self-Instruct variant, perform poorly on code explanation (HUMANEVALEXPLAIN). This is\nbecause both models are only conditioned to write code instead of natural language. We find that to\n5\nOctoPack: Instruction Tuning Code Large Language Models\nCode Fixing\n0\n10\n20\n30\n40\n50\nPass@1 (%)\n+22\nCode Explanation\n+35\nCode Synthesis\n+13\nAverage\n+23\nNo instruction tuning\nSelf-Instruct\nOASST\nSelf-Instruct + OASST\nxP3x-Code + OASST\nCommitPackFT + OASST\nFigure 4: Comparing permissively licensed instruction datasets by instruction tuning StarCoder.\nModels are evaluated on the Python subset of HUMANEVALPACK.\nperform well at explaining code, it is necessary to include samples with natural language as the target\noutput during instruction tuning. Only relying on data with code as the target, such as the Self-Instruct\ndata, will lead to models always outputting code even if the question requires a natural language\noutput. Thus, we mix all other ablations with OASST, which contains many natural language targets.\nWhile the xP3x subset also contains samples with natural language output, many of its target outputs\nare short, which leads to models with a bias for short answers. This is impractical for the explanation\ntask leading to the comparatively low score of mixing xP3x with OASST.\nCOMMITPACKFT+OASST yields best performance\nAll instruction datasets provide similar\nboosts for code synthesis (HUMANEVALSYNTHESIZE), which has been the focus of all prior work on\ncode instruction models (Wang et al., 2023c; Luo et al., 2023; Muennighoff et al., 2022b). We achieve\nthe best average score by instruction tuning on COMMITPACKFT mixed with our filtered OASST data\nyielding an absolute 23% improvement over StarCoder. Thus, we select COMMITPACKFT+OASST\nfor our final model dubbed OCTOCODER. Using the same data, we also instruction tune the 6 billion\nparameter CodeGeeX2 (Zheng et al., 2023) to create OCTOGEEX. Training hyperparameters for both\nmodels are in Appendix P.\n4.2\nCOMPARING WITH OTHER MODELS\nWe benchmark OCTOCODER and OCTOGEEX with state-of-the-art Code LLMs on HUMANEVAL-\nPACK in Table 2. For all models, we use the prompt put forward by the model creators if applicable\nor else a simple intuitive prompt, see Appendix Q.\nOCTOCODER performs best among permissive models\nOCTOCODER has the highest average\nscore across all three evaluation scenarios among all permissive models. With just 6 billion parameters,\nOCTOGEEX is the smallest model benchmarked, but still outperforms all prior permissive Code\nLLMs. GPT-4 (OpenAI, 2023) performs best among all models benchmarked with a significant\nmargin. However, GPT-4 is closed-source and likely much larger than all other models evaluated.\nInstruction tuning generalizes to unseen programming languages\nTrained primarily on natu-\nral language, not code, BLOOMZ (Muennighoff et al., 2022b) performs worse than other models\ndespite having 176 billion parameters. Go and Rust are not contained in BLOOMZ\u2019s instruction\ndata, yet it performs much better than the random baseline of 0.0 for these two languages across\nmost tasks. This confirms our hypothesis that models are capable of generalizing instructions to\nprogramming languages only seen at pretraining, similar to crosslingual generalization for natural\nlanguages (Muennighoff et al., 2022b). To improve programming language generalization fur-\nther, we tune OCTOCODER and OCTOGEEX on many languages from COMMITPACKFT, and this\ngeneralization improvement is reflected in the performance on HUMANEVALPACK\u2019s new languages.\nPretraining weight correlates with programming language performance after instruction tuning\nPrior work has shown that the performance on natural languages after instruction tuning is correlated\nwith the weight of these languages during pretraining (Muennighoff et al., 2022b). The more weight\nduring pretraining, the better the performance after instruction tuning. We find the same to be\n6\nOctoPack: Instruction Tuning Code Large Language Models\nModel (\u2193)\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\nHUMANEVALFIX\nNon-permissive models\nInstructCodeT5+\u2020\n2.7\n1.2\n4.3\n2.1\n0.2\n0.5\n1.8\nWizardCoder\u2020\n31.8\n29.5\n30.7\n30.4\n18.7\n13.0\n25.7\nGPT-4\n47.0\n48.2\n50.0\n50.6\n47.6\n43.3\n47.8\nPermissive models\nBLOOMZ\n16.6\n15.5\n15.2\n16.4\n6.7\n5.7\n12.5\nStarChat-\u03b2\n18.1\n18.1\n24.1\n18.1\n8.2\n3.6\n11.2\nCodeGeeX2\u2217\n15.9\n14.7\n18.0\n13.6\n4.3\n6.1\n12.1\nStarCoder\n8.7\n15.7\n13.3\n20.1\n15.6\n6.7\n13.4\nOCTOGEEX\u2217\n28.1\n27.7\n30.4\n27.6\n22.9\n9.6\n24.4\nOCTOCODER\n30.4\n28.4\n30.6\n30.2\n26.1\n16.5\n27.0\nHUMANEVALEXPLAIN\nNon-permissive models\nInstructCodeT5+\u2020\n20.8\n0.0\n0.0\n0.0\n0.1\n0.0\n3.5\nWizardCoder\u2020\n32.5\n33.0\n27.4\n26.7\n28.2\n16.9\n27.5\nGPT-4\n64.6\n57.3\n51.2\n58.5\n38.4\n42.7\n52.1\nPermissive models\nBLOOMZ\n14.7\n8.8\n12.1\n8.5\n0.6\n0.0\n7.5\nStarChat-\u03b2\n25.4\n21.5\n24.5\n18.4\n17.6\n13.2\n20.1\nCodeGeeX2\u2217\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nStarCoder\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nOCTOGEEX\u2217\n30.4\n24.0\n24.7\n21.7\n21.0\n15.9\n22.9\nOCTOCODER\n35.1\n24.5\n27.3\n21.1\n24.1\n14.8\n24.5\nHUMANEVALSYNTHESIZE\nNon-permissive models\nInstructCodeT5+\u2020\n37.0\n18.9\n17.4\n9.5\n19.8\n0.3\n17.1\nWizardCoder\u2020\n57.3\n49.5\n36.1\n36.4\n40.9\n20.2\n40.1\nGPT-4\n86.6\n82.9\n81.7\n72.6\n78.7\n67.1\n78.3\nPermissive models\nBLOOMZ\n15.6\n14.8\n18.4\n8.4\n6.5\n5.5\n11.5\nStarChat-\u03b2\n33.5\n31.4\n26.7\n25.5\n26.6\n14.0\n26.3\nCodeGeeX2\u2217\n35.9\n32.2\n30.8\n22.5\n29.3\n18.1\n28.1\nStarCoder\n33.6\n30.8\n30.2\n17.6\n31.6\n21.8\n27.6\nOCTOGEEX\u2217\n44.7\n33.8\n36.9\n21.9\n32.3\n15.7\n30.9\nOCTOCODER\n46.2\n39.2\n38.2\n30.4\n35.6\n23.4\n35.5\nTable 2: Zero-shot pass@1 (%) performance across HUMANEVALPACK. InstructCodeT5+,\nWizardCoder, StarChat-\u03b2, StarCoder and OCTOCODER have 16B parameters. CodeGeeX2 and\nOCTOGEEX have 6B parameters. BLOOMZ has 176B parameters. In this work, we call models\n\"permissive\" if weights are freely accessible and usable for commercial purposes. \u2217: Commercial\nlicense available after submitting a form. \u2020: Trained on data that may not be used \u201cto develop models\nthat compete with OpenAI\u201d thus we classify them as non-permissive in this work (see \u00a71).\n7\nOctoPack: Instruction Tuning Code Large Language Models\nthe case for programming languages. Python, Java, and JavaScript collectively make up around\n30% of the pretraining data of StarCoder (Li et al., 2023b). After instruction tuning StarCoder to\nproduce OCTOCODER, we see the best performance among these three languages, especially for\nHUMANEVALSYNTHESIZE. OCTOCODER performs weakest on Rust, which is the lowest resource\nlanguage of StarCoder among the languages we benchmark (1.2% of pretraining data).\nModels struggle with small targeted changes\nHUMANEVALFIX is the most challenging task\nfor most models. They commonly regenerate the buggy function without making any change (e.g.\nWizardCoder in Figure 34) or they introduce new bugs (e.g. GPT-4 in Figure 33). We analyze model\nperformance by bug type in Appendix M and find bugs that require removing excess code are the\nmost challenging. OCTOCODER performs comparatively well across all languages. Instruction tuning\non COMMITPACKFT has likely taught OCTOCODER to make small, targeted changes to fix bugs.\nModels struggle switching between code and text\nSome models fail at HUMANEVALEXPLAIN,\nas they do not generate natural language explanations. We manually inspect explanations for the first\nten samples of the Python split and disqualify a model if none of them are explanations. This is the\ncase for StarCoder and CodeGeeX2, which generate code instead of natural language explanations.\nBLOOMZ and InstructCodeT5+ also occasionally generate code. Other models exclusively generate\nnatural language explanations, not containing any code for inspected samples.\nModels struggle adhering to a specified output length\nHUMANEVALEXPLAIN instructs models\nto fit their explanation within a given character limit (\u00a73). Current models appear to have no\nunderstanding of how many characters they are generating. They commonly write very short and\nthus underspecified explanations (e.g. BLOOMZ in Figure 35) or excessively long explanations that\nend up being cut off (e.g. StarChat-\u03b2 in Figure 38). Future work could investigate how to enable\nmodels to be aware of their generated output length to improve HUMANEVALEXPLAIN performance.\nHumanEval code synthesis is close to saturation\nPure code synthesis on HUMANEVALSYN-\nTHESIZE is the easiest task for all models. With a pass rate of 86.6% for a single solution, GPT-4\nis close to fully saturating the Python subset. GPT-4 was originally found to score 67% on Python\nHumanEval (OpenAI, 2023) and 81% in later work (Bubeck et al., 2023). Our score for GPT-4\nis significantly higher, possibly due to improvements made to the API by OpenAI, contamination\nof HumanEval in GPT-4 training, or slightly different prompting and evaluation. An example of\nour prompt is depicted in Figure 3 (right). We perform very careful evaluation to ensure every\ngeneration is correctly processed. We reproduce the HumanEval score of WizardCoder (Luo et al.,\n2023; Xu et al., 2023a) and find it to also perform well across other languages. For BLOOMZ and\nInstructCodeT5+ our evaluation leads to a higher Python score than they reported, likely because\nof our more careful processing of generations. OCTOCODER has the highest performance for every\nlanguage among permissively licensed models. With a pass@1 of 46.2% on the original Python split,\nOCTOCODER improves by a relative 38% over its base model, StarCoder.\n5\nRELATED WORK\n5.1\nCODE MODELS\nThere has been extensive work on code models tailored to a specific coding task, such as code\nsummarization (Iyer et al., 2016; Ahmad et al., 2020; Zhang et al., 2022a; Shi et al., 2022) or code\nediting (Drain et al., 2021; Zhang et al., 2022c; He et al., 2022; Zhang et al., 2022b; Wei et al., 2023;\nPrenner & Robbes, 2023; Fakhoury et al., 2023; Skreta et al., 2023) (also see work on edit models\nmore generally (Reid & Neubig, 2022; Schick et al., 2022; Dwivedi-Yu et al., 2022; Raheja et al.,\n2023)). These works use task-specific heuristics that limit the applicability of their methods to other\ntasks. In contrast, we aim to build models applicable to all kinds of tasks related to code and beyond.\nThrough large-scale pretraining more generally applicable code models have been developed (Nijkamp\net al., 2022; 2023; Xu et al., 2022a; Christopoulou et al., 2022; Gunasekar et al., 2023; Li et al.,\n2023b; Bui et al., 2023; Scao et al., 2022a;b). However, these models only continue code making\nthem hard to use for tasks such as explaining code with natural language (HUMANEVALEXPLAIN).\nTeaching them to follow human instructions is critical to make them applicable to diverse tasks.\n8\nOctoPack: Instruction Tuning Code Large Language Models\n5.2\nINSTRUCTION MODELS\nTraining models to follow instructions has led to new capabilities in text (Ouyang et al., 2022; Wang\net al., 2022b; Chung et al., 2022) and visual modalities (Xu et al., 2023b; OpenAI, 2023). Prior work\nhas shown its benefits for traditional language tasks (Wei et al., 2022; Longpre et al., 2023a; Iyer\net al., 2022), multilingual tasks (Muennighoff et al., 2022b; 2024; Yong et al., 2022; \u00dcst\u00fcn et al.,\n2024), and dialog (K\u00f6pf et al., 2023; Bai et al., 2022; Ganguli et al., 2022). For coding applications,\nPanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) and InstructCodeT5+ (Wang et al.,\n2023c) are recent models trained with coding instructions. However, they all use the CodeAlpaca\ndataset (Chaudhary, 2023), which is synthetically generated from OpenAI models. Using data from\npowerful closed-source models provides a strong advantage, but limits the model use and has other\nlimitations highlighted in \u00a71. CoEditor (Wei et al., 2023) proposes an \u201cauto-editing\u201d task, trained on\n1650 python commit history repositories. Our work expands this to more general coding tasks via\ninstructions, more languages, and orders of magnitude more commit data.\n5.3\nCODE BENCHMARKS\nMany code synthesis benchmarks have been proposed (Wang et al., 2022d;c; Yu et al., 2023; Lai et al.,\n2023; Du et al., 2023). HumanEval (Chen et al., 2021; Liu et al., 2023b) has emerged as the standard\nfor this task. Prior work has extended HumanEval to new programming languages via automatic\ntranslation mechanisms (Athiwaratkun et al., 2022; Cassano et al., 2023; Orlanski et al., 2023). These\napproaches are error-prone and only translate tests, not the actual solutions, which are needed for\ntasks like code explanation. Thus, we rely only on humans to create all parts of HUMANEVALPACK\nincluding test cases, correct solutions, buggy solutions, and other metadata across 6 languages.\nCode repair is commonly evaluated on Quixbugs (Lin et al., 2017; Prenner & Robbes, 2021; Ye\net al., 2021; Xia & Zhang, 2023; Jiang et al., 2023; Sobania et al., 2023) or Python bugs (He et al.,\n2022; Bradley et al., 2023). The latter does not support code execution, which limits its utility. While\nQuixbugs supports execution with unit tests, it only contains 40 samples in Python and Java. Further,\nthe problems in Quixbugs are generic functions, such as bucket sort. This makes them easy to solve\nand hard to decontaminate training data for. Our benchmark, HUMANEVALFIX, contains 164 buggy\nfunctions for six languages with solutions and unit tests. Further, our coding problems, derived from\nHumanEval, are very specific, such as keeping track of a bank account balance (see Figure 14).\nPrior work on evaluating code explanations (Lu et al., 2021; Cui et al., 2022) has relied on metrics\nsuch as METEOR (Banerjee & Lavie, 2005) or BLEU (Papineni et al., 2002). By chaining code\nexplanation with code synthesis, we can evaluate this task using the execution-based pass@k metric\novercoming the major limitations of BLEU and other heuristics-based metrics (Reiter, 2018).\nLarge-scale benchmarking has proven useful in many areas of natural language processing (Wang\net al., 2019; Kiela et al., 2021; Srivastava et al., 2022; Muennighoff et al., 2022a). By producing 18\nscores (6 languages across 3 tasks) for 9 models, we take a step towards large-scale benchmarking of\ncode models. However, we lack many models capable of generating code (Black et al., 2021; Fried\net al., 2022; Black et al., 2022; Wang & Komatsuzaki, 2021; Biderman et al., 2023b). Future work\nmay consider more models or extending HUMANEVALPACK to new languages or tasks, such as code\nefficiency (Madaan et al., 2023a; Yetistiren et al., 2022) or code classification (Khan et al., 2023).\n6\nCONCLUSION\nThis work studies training and evaluation of Code LLMs that follow instructions. We introduce\nCOMMITPACK, a 4TB dataset of Git commits covering 350 programming languages. We filter this\nlarge-scale dataset to create COMMITPACKFT, 2GB of high-quality code with commit messages\nthat assimilate instructions. To enable a comprehensive evaluation of instruction code models,\nwe construct HUMANEVALPACK, a human-written benchmark covering 3 different tasks for 6\nprogramming languages. We ablate several instruction datasets and find that COMMITPACKFT\ncombined with natural language data leads to the best performance. While our models, OCTOCODER\nand OCTOGEEX, are the best permissively licensed Code LLMs available, they are outperformed\nby closed-source models such as GPT-4. In addition to improving the instruction tuning paradigm,\nfuture work should consider training more capable base models.\n9\nOctoPack: Instruction Tuning Code Large Language Models\nACKNOWLEDGEMENTS\nWe thank Hugging Face for providing compute instances. We are extremely grateful to Rodrigo\nGarcia for the Rust translations, Dimitry Ageev and Calum Bird for help with GPT-4 evaluation,\nLoubna Ben Allal for help on evaluation, Arjun Guha for insightful discussions on chaining evaluation\ntasks to avoid evaluating with BLEU, Lewis Tunstall for help on the OASST data, Victor Sanh and\nNadav Timor for discussions, Jiaxi Yang for logo editing and domain classification prompting design,\nGhosal et al. (2023); Zeng et al. (2023) for design inspiration, Harm de Vries for feedback and all\nmembers of BigCode for general support. Finally, we thank every programmer who takes the time to\nwrite informative commit messages.\nREFERENCES\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based\napproach for source code summarization. arXiv preprint arXiv:2005.00653, 2020.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don\u2019t\nreach for the stars! arXiv preprint arXiv:2301.03988, 2023.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan,\nWasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. Multi-lingual evaluation of\ncode generation models. arXiv preprint arXiv:2210.14868, 2022.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nHannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Car-\nolyn Jane Anderson. Studenteval: A benchmark of student-written prompts for large language\nmodels of code. arXiv preprint arXiv:2306.04556, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant\nwith reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. URL\nhttps://arxiv.org/abs/2204.05862.\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\nevaluation measures for machine translation and/or summarization, pp. 65\u201372, 2005.\nAntonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and\ndocumentation strings for automated code documentation and code generation. arXiv preprint\narXiv:1707.02275, 2017.\nMohammad Bavarian, Heewoo Jun, Nikolas A. Tezak, John Schulman, Christine McLeavey, Jerry\nTworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von\nWerra. A framework for the evaluation of code generation models. https://github.com/b\nigcode-project/bigcode-evaluation-harness, 2022.\nStella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language\nmodels. arXiv preprint arXiv:2304.11158, 2023a.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In International\nConference on Machine Learning, pp. 2397\u20132430. PMLR, 2023b.\n10\nOctoPack: Instruction Tuning Code Large Language Models\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow. If you use this software, please cite it using\nthese metadata, 58, 2021.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022.\nHerbie Bradley, Honglu Fan, Harry Saini, Reshinth Adithyan, Shivanshu Purohit, and Joel Lehman.\nDiff models - a new way to edit code. CarperAI Blog, Jan 2023. URL https://carper.ai/\ndiff-model/.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Conference on Neural Information Processing Systems (NeurIPS), 2020. URL\nhttps://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac\n142f64a-Abstract.html.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nNghi DQ Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven CH Hoi. Codetf:\nOne-stop transformer library for state-of-the-art code llm. arXiv preprint arXiv:2306.00029, 2023.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e:\na scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on\nSoftware Engineering, 2023.\nSahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https:\n//github.com/sahil280114/codealpaca, 2023.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.\nCodet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time?,\n2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023b.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023c.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li,\nQi Zhang, Meng Xiao, Bo Shen, Lin Li, et al. Pangu-coder: Program synthesis with function-level\nlanguage modeling. arXiv preprint arXiv:2207.11280, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.\nHaotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang,\nJianfeng Gao, and Nan Duan. Codeexp: Explanatory code document generation. arXiv preprint\narXiv:2211.15395, 2022.\n11\nOctoPack: Instruction Tuning Code Large Language Models\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344\u201316359, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nKaustubh D Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood,\nAbinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, et al. Nl-augmenter: A\nframework for task-sensitive natural language augmentation. arXiv preprint arXiv:2112.02721,\n2021.\nYangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati,\nParminder Bhatia, Dan Roth, and Bing Xiang. Cocomic: Code completion by jointly modeling\nin-file and cross-file context. arXiv preprint arXiv:2212.10007, 2022.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\npreprint arXiv:2304.07590, 2023.\nDawn Drain, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. Deepdebug: Fixing python\nbugs using stack traces, backtranslation, and code skeletons. arXiv preprint arXiv:2105.09352,\n2021.\nXueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng,\nChaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for evaluating\nllms on class-level code generation. arXiv preprint arXiv:2308.01861, 2023.\nJane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria Lomeli, Patrick Lewis, Gautier Izacard,\nEdouard Grave, Sebastian Riedel, and Fabio Petroni. Editeval: An instruction-based benchmark\nfor text improvements. arXiv preprint arXiv:2209.13331, 2022.\nAryaz Eghbali and Michael Pradel. Crystalbleu: precisely and efficiently measuring the similarity of\ncode. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software\nEngineering, pp. 1\u201312, 2022.\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\nalignment as prospect theoretic optimization, 2024.\nSarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K Lahiri. Towards gener-\nating functionally correct code edits from natural language issue descriptions. arXiv preprint\narXiv:2304.03816, 2023.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. arXiv preprint arXiv:2204.05999, 2022.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166, 2023.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben\nMann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to\nreduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858,\n2022.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language\nmodel evaluation, 2021. URL https://doi.org/10.5281/zenodo.5371628.\n12\nOctoPack: Instruction Tuning Code Large Language Models\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023.\nDeepanway Ghosal, Yew Ken Chia, Navonil Majumder, and Soujanya Poria. Flacuna: Unleashing\nthe problem solving power of vicuna using flan fine-tuning. arXiv preprint arXiv:2307.02053,\n2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\nCritic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint\narXiv:2305.11738, 2023.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha,\nHamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur,\nKhyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,\nTushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik,\nCrystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk,\nSaurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep\nDasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\nSoldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language\nmodels. 2024. URL https://api.semanticscholar.org/CorpusID:267365485.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine,\nand Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717,\n2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644, 2023.\nJingxuan He, Luca Beurer-Kellner, and Martin Vechev. On distribution shift in learning-based bug\ndetectors. In International Conference on Machine Learning, pp. 8559\u20138580. PMLR, 2022.\nVincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global\nrelational models of source code. In International conference on learning representations, 2019.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence\nwith apps. arXiv preprint arXiv:2105.09938, 2021.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nYi Hu, Haotong Yang, Zhouchen Lin, and Muhan Zhang. Code prompting: a neural symbolic method\nfor complex reasoning in large language models. arXiv preprint arXiv:2305.18507, 2023.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code\nusing a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 2073\u20132083, 2016.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O\u2019Horo, Gabriel Pereyra,\nJeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-iml:\nScaling language model instruction meta learning through the lens of generalization. arXiv preprint\narXiv:2212.12017, 2022. URL https://arxiv.org/abs/2212.12017.\nMingi Jeon, Seung-Yeop Baik, Joonghyuk Hahn, Yo-Sub Han, and Sang-Ki Ko. Deep Learning-based\nCode Complexity Prediction. openreview, 2022.\nNan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated\nprogram repair. arXiv preprint arXiv:2302.05020, 2023.\n13\nOctoPack: Instruction Tuning Code Large Language Models\nTae-Hwan Jung. Commitbert: Commit message generation using pre-trained programming language\nmodel. arXiv preprint arXiv:2105.14242, 2021.\nMohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. xcodeeval: A large scale multilingual multitask benchmark for code understanding,\ngeneration, translation and retrieval. arXiv preprint arXiv:2303.03004, 2023.\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Casey A\nFitzpatrick, Peter Bull, Greg Lipstein, Tony Nelli, Ron Zhu, et al. The hateful memes challenge:\nCompetition report. In NeurIPS 2020 Competition and Demonstration Track, pp. 344\u2013360. PMLR,\n2021.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively\nlicensed source code. arXiv preprint arXiv:2211.15533, 2022.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. Openassistant\nconversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327,\n2023.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih,\nDaniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science\ncode generation. In International Conference on Machine Learning, pp. 18319\u201318345. PMLR,\n2023.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen,\net al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural\nInformation Processing Systems, 35:31809\u201331826, 2022.\nJoel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley.\nEvolution through large models. arXiv preprint arXiv:2206.08896, 2022.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\nHongyu Li, Seohyun Kim, and Satish Chandra. Neural code search evaluation dataset. arXiv preprint\narXiv:1908.09804, 2019.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with\nyou! arXiv preprint arXiv:2305.06161, 2023b.\nXueyang Li, Shangqing Liu, Ruitao Feng, Guozhu Meng, Xiaofei Xie, Kai Chen, and Yang Liu.\nTransrepair: Context-aware program repair for compilation errors. In Proceedings of the 37th\nIEEE/ACM International Conference on Automated Software Engineering, pp. 1\u201313, 2022a.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science, 378(6624):1092\u20131097, 2022b.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nDerrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multi-lingual\nprogram repair benchmark set based on the quixey challenge. In Proceedings Companion of\nthe 2017 ACM SIGPLAN international conference on systems, programming, languages, and\napplications: software for humanity, pp. 55\u201356, 2017.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a.\n14\nOctoPack: Instruction Tuning Code Large Language Models\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nchatgpt really correct? rigorous evaluation of large language models for code generation. arXiv\npreprint arXiv:2305.01210, 2023b.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023c.\nTianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code\nauto-completion systems. arXiv preprint arXiv:2306.03091, 2023d.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023e.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023a.\nShayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny\nZhou, Jason Wei, Kevin Robinson, David Mimno, et al.\nA pretrainer\u2019s guide to training\ndata: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint\narXiv:2305.13169, 2023b.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark\ndataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct. arXiv preprint arXiv:2306.08568, 2023.\nAman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming\nYang, Graham Neubig, and Amir Yazdanbakhsh. Learning performance-improving code edits.\narXiv preprint arXiv:2302.07867, 2023a.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023b.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in\ncontext. Annual Conference of the North American Chapter of the Association for Computational\nLinguistics (NAACL), 2022. URL https://arxiv.org/abs/2110.15943.\nMartin Monperrus, Matias Martinez, He Ye, Fernanda Madeiral, Thomas Durieux, and Zhongxing\nYu. Megadiff: A dataset of 600k java source code changes categorized by diff size. arXiv preprint\narXiv:2108.04631, 2021.\nNiklas Muennighoff.\nSgpt: Gpt sentence embeddings for semantic search.\narXiv preprint\narXiv:2202.08904, 2022.\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. Mteb: Massive text embedding\nbenchmark. arXiv preprint arXiv:2210.07316, 2022a. doi: 10.48550/ARXIV.2210.07316. URL\nhttps://arxiv.org/abs/2210.07316.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual\ngeneralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022b.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane\nTazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models.\narXiv preprint arXiv:2305.16264, 2023.\nNiklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and\nDouwe Kiela. Generative representational instruction tuning, 2024.\n15\nOctoPack: Instruction Tuning Code Large Language Models\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\nLin. Lever: Learning to verify language-to-code generation with execution. In International\nConference on Machine Learning, pp. 26106\u201326128. PMLR, 2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\nsynthesis. arXiv preprint arXiv:2203.13474, 2022.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2:\nLessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309,\n2023.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:\nScratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,\n2021. URL https://openreview.net/forum?id=iedYJm92o0a.\nOpenAI. Gpt-4 technical report, 2023.\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud,\nJacob Austin, Rishah Singh, and Michele Catasta. Measuring the impact of programming language\ndistribution. arXiv preprint arXiv:2302.01973, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. In Conference on Neural Information Processing Systems\n(NeurIPS), 2022. URL https://arxiv.org/abs/2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311\u2013318, 2002.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin\nCheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the\ntransformer era. arXiv preprint arXiv:2305.13048, 2023.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models.\nAdvances in Neural Information Processing Systems, 34:11054\u201311070, 2021.\nLuiza Amador Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using\nblack-box apis for toxicity evaluation in research. In ICLR 2023 Workshop on Trustworthy and\nReliable Large-Scale Machine Learning Models, 2023.\nJulian Aron Prenner and Romain Robbes. Automatic program repair with openai\u2019s codex: Evaluating\nquixbugs. arXiv preprint arXiv:2111.03922, 2021.\nJulian Aron Prenner and Romain Robbes. Runbugrun\u2013an executable dataset for automated program\nrepair. arXiv preprint arXiv:2304.01102, 2023.\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin Neural Information Processing Systems, 36, 2024.\nVipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. Coedit: Text editing by task-specific\ninstruction tuning. arXiv preprint arXiv:2305.09857, 2023.\nMachel Reid and Graham Neubig.\nLearning to model editing processes.\narXiv preprint\narXiv:2205.12374, 2022.\nEhud Reiter. A structured review of the validity of bleu. Computational Linguistics, 44(3):393\u2013401,\n2018.\n16\nOctoPack: Instruction Tuning Code Large Language Models\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables\nzero-shot task generalization. International Conference on Learning Representations (ICLR), 2022.\nURL https://openreview.net/forum?id=9Vrb9D0WI4.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022a.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBideman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if\nyou have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022b.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard,\nQingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collaborative\nlanguage model. arXiv preprint arXiv:2208.11663, 2022.\nNatalie Schluter. The limits of automatic summarisation according to rouge. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics, pp.\n41\u201345. Association for Computational Linguistics, 2017.\nNoam M. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\narXiv:1911.02150, 2019.\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu,\nJichuan Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. Pangu-coder2: Boosting large\nlanguage models for code with ranking feedback, 2023.\nEnsheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, and\nHongbin Sun. On the evaluation of neural code summarization. In Proceedings of the 44th\nInternational Conference on Software Engineering, pp. 1597\u20131608, 2022.\nDisha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Repo-\nfusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998,\n2023a.\nDisha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large\nlanguage models of code. In International Conference on Machine Learning, pp. 31693\u201331715.\nPMLR, 2023b.\nShivalika Singh, Freddie Vargus, Daniel Dsouza, B\u00f6rje F Karlsson, Abinaya Mahendiran, Wei-Yin\nKo, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An\nopen-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024.\nMarta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bj\u00f8rn Kristensen, Kourosh\nDarvish, Al\u00e1n Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful prompts:\nInstruction guided task programming with verifier-assisted iterative prompting. arXiv preprint\narXiv:2303.14100, 2023.\nDominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic bug\nfixing performance of chatgpt. arXiv preprint arXiv:2301.08653, 2023.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,\nBen Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha,\nSachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander,\nKyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,\nLuke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,\nand Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining\nresearch. 2024. URL https://api.semanticscholar.org/CorpusID:267364861.\n17\nOctoPack: Instruction Tuning Code Large Language Models\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models, 2022. URL\nhttps://arxiv.org/abs/2206.04615.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\nmodels actually use long-range context? ArXiv, abs/2109.09115, 2021. URL https://api.\nsemanticscholar.org/CorpusID:237572264.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science.\narXiv preprint arXiv:2211.09085, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLewis Tunstall, Nathan Lambert, Nazneen Rajani, Edward Beeching, Teven Le Scao, Leandro von\nWerra, Sheon Han, Philipp Schmid, and Alexander Rush. Creating a coding assistant with starcoder.\nHugging Face Blog, 2023. https://huggingface.co/blog/starchat.\nAhmet \u00dcst\u00fcn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D\u2019souza, Gbemileke Onilude,\nNeel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction\nfinetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827, 2024.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language\nunderstanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019.\nURL https://arxiv.org/abs/1905.00537.\nBen Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model,\n2021.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In International Conference on Learning Representations (ICLR), 2023a. URL https:\n//openreview.net/forum?id=1PL1NIMMrw.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv\npreprint arXiv:2204.07705, 2022b.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751,\n2023b.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.\nCodet5+: Open code large language models for code understanding and generation. arXiv preprint\narXiv:2305.07922, 2023c.\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu, and Graham Neubig. Mconala: a benchmark\nfor code generation from multiple natural languages. arXiv preprint arXiv:2203.08388, 2022c.\n18\nOctoPack: Instruction Tuning Code Large Language Models\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for\nopen-domain code generation. arXiv preprint arXiv:2212.10481, 2022d.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. International\nConference on Learning Representations (ICLR), 2022. URL https://openreview.net/f\norum?id=gEZrGCozdqR.\nJiayi Wei, Greg Durrett, and Isil Dillig. Coeditor: Leveraging contextual changes for multi-round\ncode auto-editing. arXiv preprint arXiv:2305.18584, 2023.\nMinghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models.\narXiv preprint arXiv:2307.03025, 2023.\nChunqiu Steven Xia and Lingming Zhang. Conversational automated program repair. arXiv preprint\narXiv:2301.13246, 2023.\nMengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen,\nLuke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. arXiv\npreprint arXiv:2212.09803, 2022.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv\npreprint arXiv:2304.12244, 2023a.\nFrank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of\nlarge language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium\non Machine Programming, pp. 1\u201310, 2022a.\nShengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, and Hanghang Tong. Combining code context\nand fine-grained code difference for commit message generation. In Proceedings of the 13th\nAsia-Pacific Symposium on Internetware, pp. 242\u2013251, 2022b.\nZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning\nvia instruction tuning, 2023b.\nMichihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In\nInternational Conference on Machine Learning, pp. 11941\u201311952. PMLR, 2021.\nHe Ye, Matias Martinez, Thomas Durieux, and Martin Monperrus. A comprehensive study of\nautomatic program repair on the quixbugs benchmark. Journal of Systems and Software, 171:\n110825, 2021.\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun. Assessing the quality of github copilot\u2019s code\ngeneration. In Proceedings of the 18th International Conference on Predictive Models and Data\nAnalytics in Software Engineering, pp. 62\u201371, 2022.\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to\nmine aligned code and natural language pairs from stack overflow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM, 2018. doi: https://doi.org/10.1145/3196\n398.3196408.\nZheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, et al. Bloom+\n1: Adding language support to bloom for zero-shot prompting. arXiv preprint arXiv:2212.09535,\n2022.\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao\nXie, and Qianxiang Wang. Codereval: A benchmark of pragmatic code generation with generative\npre-trained models. arXiv preprint arXiv:2302.00288, 2023.\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and\nTao Kong. What matters in training a gpt4-style language model with multimodal inputs? arXiv\npreprint arXiv:2307.02469, 2023.\n19\nOctoPack: Instruction Tuning Code Large Language Models\nChunyan Zhang, Junchao Wang, Qinglei Zhou, Ting Xu, Ke Tang, Hairen Gui, and Fudong Liu. A\nsurvey of automatic source code summarization. Symmetry, 14(3):471, 2022a.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\nChen. Repocoder: Repository-level code completion through iterative retrieval and generation.\narXiv preprint arXiv:2303.12570, 2023a.\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint arXiv:2306.02858, 2023b.\nJialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust\nVerbruggen. Repairing bugs in python assignments using large language models. arXiv preprint\narXiv:2209.14876, 2022b.\nJiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. Coditt5:\nPretraining for source code and natural language editing. In 37th IEEE/ACM International\nConference on Automated Software Engineering, pp. 1\u201312, 2022c.\nTianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang.\nCoder reviewer reranking for code generation. In International Conference on Machine Learning,\npp. 41832\u201341846. PMLR, 2023c.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual\nevaluations on humaneval-x. arXiv preprint arXiv:2303.17568, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023a.\nShuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. Codebertscore: Evaluating code\ngeneration with pretrained models of code. arXiv preprint arXiv:2302.05527, 2023b.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba.\nLarge language models are human-level prompt engineers.\narXiv preprint\narXiv:2211.01910, 2022.\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, and Chandan K Reddy.\nXlcost: A benchmark dataset for cross-lingual code intelligence. arXiv preprint arXiv:2206.08474,\n2022.\nTerry Yue Zhuo. Large language models are state-of-the-art evaluators of code generation. arXiv\npreprint arXiv:2304.14317, 2023.\nTerry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian\nLiu, and Niklas Muennighoff. Astraios: Parameter-efficient instruction tuning code large language\nmodels. arXiv preprint arXiv:2401.00788, 2024.\n20\nOctoPack: Instruction Tuning Code Large Language Models\nAPPENDIX\nContents\nA Contributions\n22\nB Artifacts\n22\nC\nCOMMITPACK and COMMITPACKFT Languages\n23\nD Dataset Creation\n29\nE\nComparing Data Before and After Filtering\n32\nF\nComparing COMMITPACK and The Stack\n32\nG Pretraining on COMMITPACK\n33\nH HUMANEVALPACK Statistics\n33\nI\nFull Instruction Data Ablations\n34\nJ\nLine Diff Format for Fixing Code\n34\nK Results on HUMANEVALFIXDOCS\n37\nL\nHUMANEVALFIX Bug Types\n37\nM Performance Breakdown by HUMANEVALFIX Bug Type\n40\nN\nHUMANEVALEXPLAIN with Fill-In-The-Middle\n40\nO HUMANEVALEXPLAIN BLEU and METEOR comparison\n41\nP\nHyperparameters\n41\nQ Prompts\n41\nR Examples\n45\nR.1\nOCTOCODER\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nR.2\nGPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nR.3\nWizardCoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\nR.4\nBLOOMZ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nR.5\nStarCoder\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nR.6\nInstructCodeT5+ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nR.7\nStarChat-\u03b2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nR.8\nDiff Codegen\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nS\nLimitations and Future Work\n58\nT Version Control\n59\nU\nOCTOBADPACK\n60\n21\nOctoPack: Instruction Tuning Code Large Language Models\nA\nCONTRIBUTIONS\nNiklas Muennighoff created COMMITPACK and HUMANEVALPACK, wrote most of the paper and\nled the project. Qian Liu devised many quality filters, ran SantaCoder ablations, investigated early\ntraining decisions and helped edit the paper. Armel Zebaze created the Self-Instruct data and ran\nnumerous ablations. Niklas Muennighoff, Armel Zebaze and Qinkai Zheng created and evaluated\nOCTOCODER and OCTOGEEX. Binyuan Hui pretrained SantaCoder, made major contributions to\nthe presentation and helped edit the paper. Terry Yue Zhuo ran GPT-4 evaluations and helped edit\nthe paper. Xiangru Tang provided help on several experiments for evaluation and helped edit the\npaper. Leandro von Werra provided early guidance, suggested many quality filters and added the\ncommit data to StarCoder pretraining. Niklas Muennighoff, Qian Liu, Binyuan Hui, Swayam Singh\nand Shayne Longpre conducted the data analysis. Shayne Longpre advised the project and made\nlarge contributions to the paper.\nB\nARTIFACTS\nModel\nPublic Link\nOther models\nDiff Codegen 2B (Bradley et al., 2023)\nhttps://hf.co/CarperAI/diff-codegen-2b-v2\nInstructCodeT5+ (Wang et al., 2023c)\nhttps://hf.co/Salesforce/instructcodet5p-16b\nBLOOMZ (Muennighoff et al., 2022b)\nhttps://hf.co/bigscience/bloomz\nStarChat-\u03b2 (Tunstall et al., 2023)\nhttps://hf.co/HuggingFaceH4/starchat-beta\nCodeGeeX2 (Zheng et al., 2023)\nhttps://github.com/THUDM/CodeGeeX2\nSantaCoder (Allal et al., 2023)\nhttps://hf.co/bigcode/santacoder\nStarCoder (Li et al., 2023b)\nhttps://hf.co/bigcode/starcoder\nWizardCoder (Luo et al., 2023)\nhttps://hf.co/WizardLM/WizardCoder-15B-V1.0\nGPT-4 (OpenAI, 2023)\nhttps://openai.com/gpt-4\nData Ablations (Appendix I) - Data\nFiltered xP3x code\nhttps://hf.co/datasets/bigcode/xp3x-octopack\nStarCoder Self-Instruct\nhttps://hf.co/datasets/codeparrot/self-instruct-starcoder\nFiltered OASST\nhttps://hf.co/datasets/bigcode/oasst-octopack\nManual selection (Appendix I)\nhttps://hf.co/datasets/bigcode/co-manual\nData Ablations (Appendix I) - Models\nSelf-Instruct (SI)\nhttps://hf.co/bigcode/starcoder-s\nOASST (O)\nhttps://hf.co/bigcode/starcoder-o\nSI + O\nhttps://hf.co/bigcode/starcoder-so\nxP3x + O\nhttps://hf.co/bigcode/starcoder-xo\nCOMMITPACKFT + O (Formatting)\nhttps://hf.co/bigcode/starcoder-co-format\nCOMMITPACKFT + O (Target loss)\nhttps://hf.co/bigcode/starcoder-co-target\nCOMMITPACKFT + O (Manual)\nhttps://hf.co/bigcode/starcoder-co-manual\nCOMMITPACKFT + xP3x + O\nhttps://hf.co/bigcode/starcoder-cxo\nCOMMITPACKFT + xP3x + SI + O\nhttps://hf.co/bigcode/starcoder-cxso\nSantaCoder ablations (Appendix G, Appendix J)\nCommit format Pretraining\nhttps://hf.co/bigcode/santacoderpack\nCommit format Finetuning\nhttps://hf.co/bigcode/santacoder-cf\nLine diff format Finetuning\nhttps://hf.co/bigcode/santacoder-ldf\nOther datasets\nCOMMITPACK Metadata\nhttps://hf.co/datasets/bigcode/commitpackmeta\nMain artifacts\nCOMMITPACK\nhttps://hf.co/datasets/bigcode/commitpack\nCOMMITPACKFT\nhttps://hf.co/datasets/bigcode/commitpackft\nHUMANEVALPACK\nhttps://hf.co/datasets/bigcode/humanevalpack\nOCTOGEEX\nhttps://hf.co/bigcode/octogeex\nOCTOCODER\nhttps://hf.co/bigcode/octocoder\nTable 3: Used and produced artifacts.\n22\nOctoPack: Instruction Tuning Code Large Language Models\nC\nCOMMITPACK AND COMMITPACKFT LANGUAGES\nCOMMITPACK\nCOMMITPACKFT\nLanguage (\u2193)\nMB\nSamples\n% (MB)\nMB\nSamples\n% (MB)\nTotal\n3709175.78\n57700105\n100.0\n1545.02\n702062\n100.0\njson\n583293.82\n3495038\n15.73\n86.74\n39777\n5.61\nxml\n279208.68\n1923159\n7.53\n23.68\n9337\n1.53\ntext\n270662.6\n1389525\n7.3\n66.66\n46588\n4.31\njavascript\n262824.84\n5401937\n7.09\n125.01\n52989\n8.09\nobjective-c++\n239009.3\n32227\n6.44\n0.38\n86\n0.02\npython\n234311.56\n6189601\n6.32\n132.68\n56025\n8.59\nc\n200876.8\n2779478\n5.42\n21.08\n8506\n1.36\nc++\n186585.26\n2402294\n5.03\n14.14\n4992\n0.92\nmarkdown\n171849.95\n7645354\n4.63\n131.15\n62518\n8.49\njava\n127103.45\n3744377\n3.43\n56.28\n20635\n3.64\nhtml\n105305.28\n2366841\n2.84\n48.42\n20214\n3.13\nyaml\n100466.64\n2592787\n2.71\n190.88\n114320\n12.35\ngo\n86444.62\n1183612\n2.33\n12.13\n5004\n0.79\ncsv\n82946.19\n79268\n2.24\n0.53\n375\n0.03\nphp\n74961.64\n2555419\n2.02\n60.22\n24791\n3.9\njupyter-notebook\n66854.08\n94000\n1.8\n0.1\n48\n0.01\ngettext-catalog\n62296.88\n168327\n1.68\n0.13\n72\n0.01\nsql\n56802.76\n132772\n1.53\n3.74\n2069\n0.24\nunity3d-asset\n39535.01\n17867\n1.07\n0.16\n101\n0.01\ntypescript\n39254.8\n572136\n1.06\n14.28\n5868\n0.92\nowl\n36435.46\n7458\n0.98\n0\n0\n0.0\nruby\n35830.74\n2928702\n0.97\n195.29\n69413\n12.64\nc#\n33669.65\n923157\n0.91\n26.84\n9346\n1.74\nnix\n33547.92\n221281\n0.9\n3.84\n1593\n0.25\nshell\n25109.95\n1017977\n0.68\n66.86\n31217\n4.33\nperl\n21148.93\n374266\n0.57\n4.99\n2288\n0.32\ntex\n17471.11\n89283\n0.47\n0.56\n307\n0.04\ncss\n16306.63\n548818\n0.44\n9.36\n5049\n0.61\nrestructuredtext\n15613.89\n494037\n0.42\n15.73\n6560\n1.02\nrust\n15011.3\n296214\n0.4\n7.24\n2996\n0.47\ngroff\n12020.19\n32923\n0.32\n0.4\n192\n0.03\nini\n8375.16\n297100\n0.23\n21.04\n11360\n1.36\nscala\n8325.96\n316064\n0.22\n11.18\n5040\n0.72\ncoffeescript\n6795.14\n292446\n0.18\n16.96\n5513\n1.1\nhaskell\n6306.12\n217325\n0.17\n3.31\n1389\n0.21\nswift\n5902.72\n319289\n0.16\n16.27\n4849\n1.05\nlua\n5763.12\n139091\n0.16\n1.85\n920\n0.12\nsvg\n5645.44\n27095\n0.15\n0.25\n169\n0.02\ngas\n5585.38\n15121\n0.15\n0.34\n193\n0.02\nocaml\n5355.4\n81360\n0.14\n0.7\n333\n0.05\nerlang\n5043.32\n93685\n0.14\n1.19\n480\n0.08\nmakefile\n4238.51\n343379\n0.11\n2.53\n960\n0.16\nasciidoc\n4138.59\n96671\n0.11\n1.86\n523\n0.12\nemacs-lisp\n3988.65\n83228\n0.11\n1.97\n1015\n0.13\nscss\n3944.94\n288190\n0.11\n13.21\n6829\n0.86\nclojure\n3523.41\n158674\n0.09\n5.07\n2403\n0.33\norg\n3126.22\n30198\n0.08\n0.27\n136\n0.02\ncommon-lisp\n2954.9\n74628\n0.08\n1.45\n778\n0.09\ndiff\n2586.05\n21021\n0.07\n1.48\n680\n0.1\ngroovy\n2569.14\n110057\n0.07\n4.17\n1486\n0.27\nhtml+erb\n2450.68\n225379\n0.07\n23.1\n10910\n1.5\nnesc\n2439.56\n473\n0.07\n0.02\n7\n0.0\n23\nOctoPack: Instruction Tuning Code Large Language Models\ndart\n2395.8\n56873\n0.06\n1.96\n765\n0.13\npowershell\n2289.28\n55381\n0.06\n2.06\n991\n0.13\nf#\n2289.24\n66840\n0.06\n0.66\n254\n0.04\ndm\n2223.14\n55584\n0.06\n0.15\n16\n0.01\nkotlin\n2219.25\n124266\n0.06\n5.37\n2214\n0.35\npascal\n2194.68\n42511\n0.06\n0.05\n25\n0.0\njsx\n2124.74\n139148\n0.06\n5.5\n2199\n0.36\nviml\n1948.21\n74062\n0.05\n1.96\n1063\n0.13\nactionscript\n1844.15\n28819\n0.05\n0.12\n49\n0.01\ncython\n1736.59\n25927\n0.05\n0.31\n123\n0.02\nturtle\n1698.95\n3882\n0.05\n0.05\n21\n0.0\nless\n1616.56\n88634\n0.04\n3.72\n1360\n0.24\nmathematica\n1475.04\n925\n0.04\n0.01\n1\n0.0\nxslt\n1441.46\n27956\n0.04\n0.26\n99\n0.02\nscheme\n1249.24\n30546\n0.03\n0.42\n213\n0.03\nperl6\n1223.16\n12167\n0.03\n0.27\n122\n0.02\nedn\n1186.94\n2289\n0.03\n0.09\n48\n0.01\nfortran\n1178.55\n13463\n0.03\n0.14\n70\n0.01\njava-server-pages\n1173.07\n53574\n0.03\n0.45\n173\n0.03\nstandard-ml\n1133.48\n20097\n0.03\n0.15\n72\n0.01\ncmake\n1132.07\n58446\n0.03\n2.27\n981\n0.15\njson5\n1108.2\n1827\n0.03\n0.08\n33\n0.01\nvala\n1104.51\n14822\n0.03\n0.12\n50\n0.01\nvue\n1093.8\n68967\n0.03\n1.38\n587\n0.09\nfreemarker\n1032.33\n36216\n0.03\n1.03\n510\n0.07\ngraphql\n1004.84\n2009\n0.03\n0.03\n17\n0.0\ntwig\n958.96\n39588\n0.03\n3.96\n1610\n0.26\ntcl\n869.83\n16407\n0.02\n0.29\n103\n0.02\npod\n859.02\n14922\n0.02\n0.15\n54\n0.01\ndockerfile\n849.73\n259379\n0.02\n0.1\n39\n0.01\nyacc\n845.7\n8230\n0.02\n0.01\n3\n0.0\npostscript\n800.73\n903\n0.02\n0.02\n9\n0.0\nracket\n796.64\n16615\n0.02\n0.2\n117\n0.01\neagle\n785.68\n2237\n0.02\n0.01\n4\n0.0\nhaxe\n772.9\n28447\n0.02\n0.34\n174\n0.02\njulia\n752.07\n22695\n0.02\n0.31\n180\n0.02\nhandlebars\n740.82\n49842\n0.02\n3.29\n1429\n0.21\nsmarty\n720.94\n41065\n0.02\n1.59\n737\n0.1\nvisual-basic\n681.52\n10511\n0.02\n0.15\n48\n0.01\nliterate-haskell\n673.74\n10729\n0.02\n0.02\n7\n0.0\nsmalltalk\n665.89\n11741\n0.02\n0.46\n284\n0.03\nisabelle\n655.82\n8359\n0.02\n0.01\n2\n0.0\nnimrod\n652.86\n12023\n0.02\n0.24\n67\n0.02\nzig\n621.38\n4290\n0.02\n0.01\n4\n0.0\nm4\n603.58\n12465\n0.02\n0.26\n101\n0.02\nmax\n603.56\n2259\n0.02\n0\n0\n0.0\nelixir\n558.12\n35473\n0.02\n2.35\n1150\n0.15\nmako\n543.01\n8943\n0.01\n0.76\n170\n0.05\narduino\n534.18\n32350\n0.01\n0.46\n225\n0.03\njade\n531.4\n46993\n0.01\n2.35\n1119\n0.15\nhaml\n502.01\n74792\n0.01\n10.74\n4415\n0.7\nelm\n481.97\n18542\n0.01\n0.62\n265\n0.04\npurebasic\n474.28\n36\n0.01\n0.02\n5\n0.0\ncoldfusion\n470.78\n9263\n0.01\n0.02\n9\n0.0\nlean\n470.03\n7507\n0.01\n0.02\n3\n0.0\nr\n454.32\n12858\n0.01\n0.23\n121\n0.01\ncuda\n437.67\n11450\n0.01\n0.07\n25\n0.0\ntextile\n425.12\n18491\n0.01\n0.18\n61\n0.01\nrobotframework\n421.61\n9211\n0.01\n0.21\n85\n0.01\n24\nOctoPack: Instruction Tuning Code Large Language Models\nabap\n409.62\n1955\n0.01\n0.01\n1\n0.0\nrdoc\n397.03\n38760\n0.01\n0.55\n270\n0.04\nllvm\n382.2\n10727\n0.01\n1.6\n780\n0.1\nada\n380.7\n13258\n0.01\n0.73\n265\n0.05\nbatchfile\n372.16\n43674\n0.01\n2.98\n1466\n0.19\nqml\n361.45\n19360\n0.01\n0.94\n368\n0.06\njasmin\n359.82\n4782\n0.01\n0.05\n9\n0.0\nassembly\n343.62\n8126\n0.01\n0.17\n105\n0.01\ng-code\n334.96\n3690\n0.01\n0.04\n7\n0.0\ncucumber\n331.38\n26677\n0.01\n2.59\n976\n0.17\nhtml+php\n323.35\n18381\n0.01\n0.33\n150\n0.02\nkicad\n321.94\n759\n0.01\n0\n0\n0.0\napi-blueprint\n317.85\n4765\n0.01\n0.06\n23\n0.0\neiffel\n311.48\n373\n0.01\n0.01\n2\n0.0\ntoml\n292.68\n63517\n0.01\n5.58\n3424\n0.36\nmodelica\n284.62\n2611\n0.01\n0.04\n15\n0.0\nbitbake\n277.58\n43239\n0.01\n4.46\n1308\n0.29\nlex\n275.96\n705\n0.01\n0\n0\n0.0\nstylus\n273.06\n21967\n0.01\n0.95\n480\n0.06\nprotocol-buffer\n254.12\n9202\n0.01\n0.52\n181\n0.03\nunknown\n252.23\n30570\n0.01\n3.05\n1597\n0.2\nnit\n244.54\n4951\n0.01\n0.02\n3\n0.0\nfactor\n241.19\n15378\n0.01\n0.36\n113\n0.02\nxs\n239.04\n3215\n0.01\n0.02\n7\n0.0\nsass\n230.65\n23144\n0.01\n1.36\n705\n0.09\npir\n230.2\n6231\n0.01\n0.08\n23\n0.01\nhtml+django\n217.04\n10535\n0.01\n0.85\n399\n0.06\nmediawiki\n214.32\n10188\n0.01\n0.08\n33\n0.01\nlogos\n212.3\n1733\n0.01\n0.04\n19\n0.0\ngenshi\n209.3\n956\n0.01\n0.02\n3\n0.0\ncoldfusion-cfc\n208.16\n4410\n0.01\n0.05\n20\n0.0\nxtend\n179.54\n7775\n0.0\n0.13\n55\n0.01\nsqf\n168.66\n7778\n0.0\n0.09\n45\n0.01\nvhdl\n155.95\n2185\n0.0\n0.02\n5\n0.0\nantlr\n143.55\n3651\n0.0\n0.03\n15\n0.0\nsystemverilog\n140.19\n3944\n0.0\n0.08\n35\n0.01\nhcl\n136.75\n13379\n0.0\n0.91\n421\n0.06\nasp\n136.1\n4286\n0.0\n0.09\n22\n0.01\nnsis\n129.12\n4048\n0.0\n0.06\n15\n0.0\ninform-7\n120.19\n184\n0.0\n0.01\n2\n0.0\nslim\n119.04\n18726\n0.0\n2.06\n1052\n0.13\ngroovy-server-pages\n117.37\n6695\n0.0\n0.07\n25\n0.0\nceylon\n116.14\n7256\n0.0\n0.1\n49\n0.01\nfish\n111.28\n15351\n0.0\n1.33\n813\n0.09\nprocessing\n108.58\n5912\n0.0\n0.07\n35\n0.0\ncomponent-pascal\n105.5\n43\n0.0\n0\n0\n0.0\nlasso\n104.17\n67\n0.0\n0\n0\n0.0\nglsl\n99.49\n9478\n0.0\n0.34\n164\n0.02\nsaltstack\n98.2\n12314\n0.0\n1.41\n617\n0.09\nxbase\n94.42\n1670\n0.0\n0.01\n3\n0.0\nautohotkey\n94.22\n1452\n0.0\n0.02\n15\n0.0\nliquid\n93.79\n2651\n0.0\n0.09\n30\n0.01\npurescript\n92.41\n5024\n0.0\n0.17\n80\n0.01\nagda\n92.06\n4956\n0.0\n0.02\n10\n0.0\ninno-setup\n91.36\n3014\n0.0\n0.06\n16\n0.0\noz\n90.48\n1551\n0.0\n0.03\n8\n0.0\nchapel\n89.62\n26447\n0.0\n0.04\n20\n0.0\narc\n87.21\n758\n0.0\n0.01\n2\n0.0\nopencl\n86.43\n2489\n0.0\n0.05\n23\n0.0\n25\nOctoPack: Instruction Tuning Code Large Language Models\ngraphviz-dot\n85.8\n1525\n0.0\n0.07\n35\n0.0\npawn\n85.42\n580\n0.0\n0.01\n3\n0.0\njsoniq\n75.15\n1343\n0.0\n0.01\n6\n0.0\nbluespec\n72.38\n2500\n0.0\n0.01\n2\n0.0\nsmali\n71.38\n174\n0.0\n0\n0\n0.0\nkrl\n69.87\n1879\n0.0\n0.02\n4\n0.0\nmaple\n68.28\n1311\n0.0\n0.01\n2\n0.0\nunrealscript\n67.67\n585\n0.0\n0.01\n1\n0.0\nooc\n63.19\n3416\n0.0\n0.04\n15\n0.0\npure-data\n62.62\n603\n0.0\n0.01\n1\n0.0\nxquery\n61.96\n2237\n0.0\n0.08\n39\n0.01\ndcl\n59.64\n833\n0.0\n0.04\n19\n0.0\nmoonscript\n59.21\n1951\n0.0\n0.02\n10\n0.0\nawk\n57.18\n2206\n0.0\n0.1\n52\n0.01\npike\n52.87\n1262\n0.0\n0.02\n6\n0.0\nlivescript\n51.23\n5194\n0.0\n0.13\n63\n0.01\nsolidity\n50.86\n3689\n0.0\n0.08\n37\n0.01\nmonkey\n48.26\n1367\n0.0\n0.02\n4\n0.0\njsonld\n48.01\n462\n0.0\n0.02\n6\n0.0\nzephir\n42.68\n1265\n0.0\n0.02\n4\n0.0\ncrystal\n41.92\n4217\n0.0\n0.35\n182\n0.02\nrhtml\n41.02\n4551\n0.0\n0.35\n135\n0.02\nstata\n40.68\n1344\n0.0\n0.02\n10\n0.0\nidris\n39.9\n3025\n0.0\n0.13\n38\n0.01\nraml\n39.39\n948\n0.0\n0.03\n9\n0.0\nopenscad\n37.73\n2178\n0.0\n0.05\n21\n0.0\nred\n35.26\n1108\n0.0\n0.01\n1\n0.0\nc2hs-haskell\n34.47\n1021\n0.0\n0.01\n2\n0.0\ncycript\n33.96\n197\n0.0\n0\n0\n0.0\napplescript\n33.51\n1304\n0.0\n0.04\n19\n0.0\nmupad\n32.49\n178\n0.0\n0.02\n4\n0.0\nliterate-agda\n31.38\n567\n0.0\n0.01\n1\n0.0\nboo\n31.17\n26289\n0.0\n0.01\n2\n0.0\nsourcepawn\n29.53\n717\n0.0\n0.01\n3\n0.0\nqmake\n29.51\n3632\n0.0\n0.32\n140\n0.02\nragel-in-ruby-host\n28.3\n888\n0.0\n0.01\n4\n0.0\nio\n27.95\n1247\n0.0\n0.01\n4\n0.0\ndesktop\n27.65\n5021\n0.0\n0.36\n186\n0.02\npropeller-spin\n26.77\n625\n0.0\n0.01\n1\n0.0\nthrift\n26.75\n1007\n0.0\n0.08\n28\n0.01\nvolt\n25.05\n1660\n0.0\n0.02\n9\n0.0\nxproc\n24.21\n914\n0.0\n0.02\n3\n0.0\nigor-pro\n23.75\n388\n0.0\n0.01\n1\n0.0\nlolcode\n23.74\n24861\n0.0\n0\n0\n0.0\nhtml+eex\n21.41\n2100\n0.0\n0.29\n135\n0.02\nlogtalk\n20.43\n1035\n0.0\n0.06\n21\n0.0\nmirah\n20.1\n706\n0.0\n0.04\n16\n0.0\ngnuplot\n19.68\n889\n0.0\n0.03\n17\n0.0\nliterate-coffeescript\n19.02\n1041\n0.0\n0.05\n19\n0.0\njflex\n18.61\n555\n0.0\n0.01\n1\n0.0\nemberscript\n18.39\n1024\n0.0\n0.02\n7\n0.0\ncobol\n17.0\n24953\n0.0\n0\n0\n0.0\nyang\n16.94\n597\n0.0\n0.02\n6\n0.0\nrebol\n16.47\n239\n0.0\n0.01\n3\n0.0\nlinker-script\n16.08\n1604\n0.0\n0.08\n37\n0.01\ncartocss\n15.92\n555\n0.0\n0.01\n3\n0.0\nurweb\n13.07\n304\n0.0\n0.02\n6\n0.0\nrmarkdown\n13.03\n750\n0.0\n0\n0\n0.0\ndarcs-patch\n13.01\n80\n0.0\n0\n0\n0.0\n26\nOctoPack: Instruction Tuning Code Large Language Models\ncsound\n12.85\n229\n0.0\n0.01\n4\n0.0\nsquirrel\n12.84\n531\n0.0\n0.01\n4\n0.0\napl\n12.56\n586\n0.0\n0.02\n7\n0.0\nhlsl\n12.17\n1529\n0.0\n0.03\n11\n0.0\nlatte\n11.89\n1380\n0.0\n0.02\n7\n0.0\npony\n11.84\n624\n0.0\n0.05\n16\n0.0\nioke\n10.86\n373\n0.0\n0.04\n25\n0.0\nhy\n10.51\n879\n0.0\n0.04\n12\n0.0\nuno\n10.36\n628\n0.0\n0.01\n2\n0.0\npan\n10.34\n637\n0.0\n0.05\n23\n0.0\nxojo\n10.31\n642\n0.0\n0\n0\n0.0\npapyrus\n10.26\n130\n0.0\n0\n0\n0.0\nstan\n10.25\n540\n0.0\n0\n0\n0.0\nslash\n9.9\n640\n0.0\n0.01\n4\n0.0\nsupercollider\n9.8\n318\n0.0\n0.01\n2\n0.0\nvcl\n9.46\n747\n0.0\n0.04\n18\n0.0\nsmt\n9.03\n117\n0.0\n0.01\n3\n0.0\nglyph\n8.95\n7\n0.0\n0\n0\n0.0\nwisp\n8.74\n262\n0.0\n0.01\n3\n0.0\nrenpy\n8.3\n421\n0.0\n0.02\n3\n0.0\nclips\n7.73\n450\n0.0\n0\n0\n0.0\ndns-zone\n7.56\n54\n0.0\n0.01\n2\n0.0\nsas\n7.54\n269\n0.0\n0.01\n1\n0.0\nrouge\n7.2\n396\n0.0\n0.1\n41\n0.01\nec\n7.03\n94\n0.0\n0\n0\n0.0\ndylan\n6.82\n280\n0.0\n0.01\n2\n0.0\ntcsh\n6.52\n748\n0.0\n0.02\n10\n0.0\naspectj\n6.33\n451\n0.0\n0.02\n8\n0.0\nnetlogo\n6.3\n140\n0.0\n0\n0\n0.0\ngap\n6.1\n46\n0.0\n0\n0\n0.0\nfancy\n5.95\n675\n0.0\n0.02\n8\n0.0\ncoq\n5.74\n80\n0.0\n0\n0\n0.0\nclick\n5.74\n9\n0.0\n0\n0\n0.0\ncapn-proto\n5.64\n330\n0.0\n0.04\n12\n0.0\nflux\n5.57\n47\n0.0\n0.01\n3\n0.0\nforth\n5.51\n265\n0.0\n0.01\n2\n0.0\nats\n5.42\n383\n0.0\n0.01\n3\n0.0\nnetlinx\n5.17\n144\n0.0\n0.01\n1\n0.0\nclean\n5.07\n171\n0.0\n0.01\n1\n0.0\nparrot-assembly\n4.66\n227\n0.0\n0.01\n2\n0.0\nalloy\n4.64\n203\n0.0\n0\n0\n0.0\nlfe\n4.58\n287\n0.0\n0.02\n6\n0.0\ngdscript\n4.49\n460\n0.0\n0.03\n9\n0.0\naugeas\n4.44\n395\n0.0\n0.04\n13\n0.0\nsparql\n4.4\n1036\n0.0\n0.04\n23\n0.0\nlilypond\n4.31\n265\n0.0\n0.01\n6\n0.0\nscilab\n4.09\n375\n0.0\n0.02\n10\n0.0\nautoit\n4.06\n279\n0.0\n0\n0\n0.0\nmyghty\n3.86\n105\n0.0\n0\n0\n0.0\nblitzmax\n3.74\n220\n0.0\n0.01\n1\n0.0\ncreole\n3.42\n337\n0.0\n0.01\n2\n0.0\nharbour\n3.34\n107\n0.0\n0.01\n1\n0.0\npiglatin\n3.17\n513\n0.0\n0.02\n11\n0.0\nopa\n3.16\n211\n0.0\n0\n0\n0.0\nsage\n3.03\n414\n0.0\n0.01\n1\n0.0\nston\n2.85\n414\n0.0\n0.01\n6\n0.0\nmaxscript\n2.8\n47\n0.0\n0\n0\n0.0\nlsl\n2.68\n74\n0.0\n0.01\n3\n0.0\ngentoo-ebuild\n2.58\n601\n0.0\n0.06\n16\n0.0\n27\nOctoPack: Instruction Tuning Code Large Language Models\nnu\n2.38\n170\n0.0\n0.01\n2\n0.0\nbro\n2.34\n333\n0.0\n0.01\n3\n0.0\nxc\n2.02\n88\n0.0\n0\n0\n0.0\nj\n1.81\n142\n0.0\n0\n0\n0.0\nmetal\n1.72\n151\n0.0\n0.02\n4\n0.0\nmms\n1.54\n91\n0.0\n0.01\n1\n0.0\nwebidl\n1.51\n96\n0.0\n0.05\n6\n0.0\ntea\n1.47\n29\n0.0\n0\n0\n0.0\nredcode\n1.27\n149\n0.0\n0\n0\n0.0\nshen\n1.2\n71\n0.0\n0\n0\n0.0\npov-ray-sdl\n1.14\n104\n0.0\n0.01\n5\n0.0\nx10\n1.01\n33\n0.0\n0\n0\n0.0\nbrainfuck\n0.96\n167\n0.0\n0.01\n2\n0.0\nninja\n0.95\n187\n0.0\n0.03\n14\n0.0\ngolo\n0.9\n115\n0.0\n0\n0\n0.0\nwebassembly\n0.86\n83\n0.0\n0\n0\n0.0\nself\n0.82\n15\n0.0\n0\n0\n0.0\nlabview\n0.81\n61\n0.0\n0\n0\n0.0\noctave\n0.8\n12\n0.0\n0\n0\n0.0\npogoscript\n0.8\n74\n0.0\n0\n0\n0.0\nd\n0.8\n20\n0.0\n0\n0\n0.0\nhttp\n0.74\n140\n0.0\n0.03\n19\n0.0\necl\n0.66\n48\n0.0\n0.01\n4\n0.0\nchuck\n0.58\n99\n0.0\n0\n0\n0.0\ngosu\n0.52\n60\n0.0\n0\n0\n0.0\nparrot\n0.52\n17\n0.0\n0\n0\n0.0\nopal\n0.47\n69\n0.0\n0\n0\n0.0\nobjective-j\n0.46\n37\n0.0\n0\n0\n0.0\nkit\n0.41\n48\n0.0\n0\n0\n0.0\ngams\n0.38\n18\n0.0\n0\n0\n0.0\nprolog\n0.28\n35\n0.0\n0\n0\n0.0\nclarion\n0.27\n13\n0.0\n0\n0\n0.0\nmask\n0.25\n37\n0.0\n0.01\n4\n0.0\nbrightscript\n0.24\n28\n0.0\n0\n0\n0.0\nscaml\n0.18\n31\n0.0\n0.01\n1\n0.0\nmatlab\n0.16\n29\n0.0\n0\n0\n0.0\nidl\n0.15\n1\n0.0\n0\n0\n0.0\nags-script\n0.12\n31\n0.0\n0\n0\n0.0\nlookml\n0.12\n10\n0.0\n0\n0\n0.0\napacheconf\n0.11\n59\n0.0\n0.01\n2\n0.0\noxygene\n0.1\n9\n0.0\n0\n0\n0.0\ntxl\n0.1\n3\n0.0\n0\n0\n0.0\ngf\n0.09\n39\n0.0\n0\n0\n0.0\nrenderscript\n0.06\n54\n0.0\n0\n0\n0.0\nmtml\n0.05\n13\n0.0\n0.01\n2\n0.0\nunified-parallel-c\n0.05\n6\n0.0\n0\n0\n0.0\ndogescript\n0.04\n10\n0.0\n0\n0\n0.0\ngentoo-eclass\n0.04\n6\n0.0\n0\n0\n0.0\nzimpl\n0.04\n7\n0.0\n0\n0\n0.0\nirc-log\n0.04\n9\n0.0\n0\n0\n0.0\nfantom\n0.03\n11\n0.0\n0\n0\n0.0\nnumpy\n0.03\n1\n0.0\n0\n0\n0.0\ncirru\n0.02\n4\n0.0\n0\n0\n0.0\nxpages\n0.02\n7\n0.0\n0.01\n1\n0.0\nnginx\n0.02\n6\n0.0\n0.01\n2\n0.0\nobjdump\n0.02\n1\n0.0\n0\n0\n0.0\npython-traceback\n0.02\n10\n0.0\n0\n0\n0.0\nrealbasic\n0.01\n1\n0.0\n0\n0\n0.0\nbefunge\n0.01\n2\n0.0\n0\n0\n0.0\n28\nOctoPack: Instruction Tuning Code Large Language Models\nbison\n0.01\n1\n0.0\n0\n0\n0.0\nm\n0.01\n1\n0.0\n0\n0\n0.0\nomgrofl\n0.01\n1\n0.0\n0\n0\n0.0\nTable 4: Programming language distribution of COMMITPACK and COMMITPACKFT. Short-\ncuts: MB=Megabytes, owl=web-ontology-language, pir=parrot-internal-representation, dcl=digital-\ncommand-language, mms=module-management-system, gf=grammatical-framework\nD\nDATASET CREATION\nCOMMITPACK\nWe use the GitHub archive available on GCP which contains metadata from GitHub\ncommits up to 2016.4 It contains around 3TB of GitHub activity data for more than 2.8 million GitHub\nrepositories including more than 145 million unique commits, over 2 billion different file paths and\nthe contents of the latest revision for 163 million files.5 We apply the filters in Table 5 to this dataset.\nThe resulting dataset containing only metadata is uploaded at https://hf.co/datasets/big\ncode/commitpackmeta. As the activity dataset only contains commit ids without the actual code\nchanges, we scrape the code from GitHub. We use the metadata and the GitHub API to scrape the\nchanged file prior and after the respective commit. Some repositories referenced in the activity data\nare no longer accessible, thus we discard them. This results in COMMITPACK with approximately 4\nterabytes uploaded at https://hf.co/datasets/bigcode/commitpack.\nDescription\nDetails\nLicense\nOnly keep samples licensed as MIT, Artistic-2.0, ISC, CC0-1.0, EPL-1.0, MPL-\n2.0, Apache-2.0, BSD-3-Clause, AGPL-3.0, LGPL-2.1, BSD-2-Clause or with-\nout license.\nLength\nOnly keep code where the commit message has at least 5 and at most 10,000\ncharacters\nNoise\nRemove code where the lowercased commit message is any of \u2019add files via\nupload\u2019, \"can\u2019t you see i\u2019m updating the time?\", \u2019commit\u2019, \u2019create readme.md\u2019,\n\u2019dummy\u2019, \u2019first commit\u2019, \u2019heartbeat update\u2019, \u2019initial commit\u2019, \u2019mirroring from\nmicro.blog.\u2019, \u2019no message\u2019, \u2019pi push\u2019, \u2019readme\u2019, \u2019update\u2019, \u2019updates\u2019, \u2019update\n_config.yaml\u2019, \u2019update index.html\u2019, \u2019update readme.md\u2019, \u2019update readme\u2019, \u2019up-\ndated readme\u2019, \u2019update log\u2019, \u2019update data.js\u2019, \u2019update data.json\u2019, \u2019update data.js\u2019,\n\u2019pi push\u2019 or starts with \u2019merge\u2019\nSingle file\nRemove samples that contain changes across multiple files\nOpt-out\nRemove samples from repositories owned by users that opted out of The\nStack (Kocetkov et al., 2022)\nTable 5: COMMITPACK filters.\nCOMMITPACKFT\nPrior work has shown the importance of careful data filtering to maintain\nquality (Yin et al., 2018; Dhole et al., 2021; Lauren\u00e7on et al., 2022; Longpre et al., 2023b; Singh\net al., 2024). To create a smaller version focused on commits that resemble high-quality instructions,\nwe further filter COMMITPACK to create COMMITPACKFT using the steps outlined in Table 6.\nWe also checked for any contamination with HumanEval (Chen et al., 2021) but did not find any\nsolution or docstring present in COMMITPACKFT. This is likely because our commit data only\ngoes up to 2016, which is several years prior to the release of HumanEval. Our filters reduce the\ndataset by a factor of around 1000 resulting in close to 2 gigabytes across 277 languages. To gain\na deeper understanding of the rich content within COMMITPACKFT, we analyze commits on its\nPython subset (56K samples). We first collect the most prevalent commit domain by prompting\nGPT-4 with: \"I\u2019d like to know the main types of commits on Github and\naim to cover as comprehensively as possible.\". Subsequently, we use GPT-4 to\nclassify each sample using the prompt in Figure 5. The task distribution is visualized in Figure 2.\n4https://www.gharchive.org/\n5https://github.blog/2016-06-29-making-open-source-data-more-available/\n29\nOctoPack: Instruction Tuning Code Large Language Models\nDescription\nDetails\nLength\nRemove samples where the before code has more than 50,000 characters\nLength\nRemove samples where the after code has 0 characters\nDifference\nRemove samples where the before and after code are the same (e.g. file name\nchanges)\nDifference\nRemove samples that contain a hashtag (to avoid references to issues)\nExtension\nRemove samples where the filename of the code after has an atypical extension\nfor the programming language (e.g. only keep \u2019.py\u2019 for Python)\nFilename\nRemove samples where the filename is contained in the commit message (as we\ndo not use the filename in finetuning)\nLength\nOnly keep samples where the commit message has more than 10 and less than\n1000 characters\nWords\nOnly keep samples where the commit message can be split into more than 4 and\nless than 1000 space-separated words\nClean\nRemove any appearances of \u2019[skip ci]\u2019, \u2019[ci skip]\u2019, sequences at the beginning\nor end that are in brackets, sequences at the beginning that end with \u2019:\u2019 and strip\nwhitespace at the beginning or end\nCapitalized\nOnly keep samples where the message starts with an uppercase letter\nTokens\nOnly keep samples where the concatenation of the code before, a special token\nand the code after has at least 50 tokens and at most 768 tokens according to\nthe StarCoder tokenizer\nInstructions\nOnly keep samples where the lowercased commit message starts with any of\nthe words in Table 7\nNoise\nRemove samples where the lowercased commit message contains any of \u2019auto\ncommit\u2019, \u2019update contributing\u2019, \u2019<?xml\u2019, \u2019merge branch\u2019, \u2019merge pull request\u2019,\n\u2019signed-off-by\u2019, \"fix that bug where things didn\u2019t work but now they should\",\n\"put the thingie in the thingie\", \"add a beter commit message\", \"code review\",\n\"//codereview\", \"work in progress\", \"wip\", \"https://\", \"http://\", \"| leetcode\",\n\"cdpcp\", \" i \", \"i\u2019ve\" , \"i\u2019m\" or both \"thanks to\" and \"for\"\nRegex\nRemove samples where the lowercased commit message has a match for\nany of the regular expressions (?:v)?\\d+\\.\\d+\\.\\d+(?=$|\\S),\n^[a-f0-9]+(?:-[a-f0-9]+)*$,\n([a-f0-9]{40}),\nissue\\s*\\d+, bug\\s*\\d+ or feature\\s*\\d+\nDownsample\nWith 90% probability remove samples where the commit message starts with\n\"Bump\", \"Set version\" or \"Update version\"\nTable 6: COMMITPACKFT filters applied to COMMITPACK. With the commit message we refer\nto the commit message subject only, not the body.\n30\nOctoPack: Instruction Tuning Code Large Language Models\n\"abort\u2019, \u2019accelerate\u2019, \u2019access\u2019, \u2019accumulate\u2019, \u2019add\u2019, \u2019address\u2019, \u2019adjust\u2019, \u2019advance\u2019, \u2019align\u2019, \u2019al-\nlot\u2019, \u2019allow\u2019, \u2019amplify\u2019, \u2019annotate\u2019, \u2019append\u2019, \u2019apply\u2019, \u2019archive\u2019, \u2019arrange\u2019, \u2019attach\u2019, \u2019augment\u2019,\n\u2019automate\u2019, \u2019backup\u2019, \u2019boost\u2019, \u2019break\u2019, \u2019bring\u2019, \u2019brush up\u2019, \u2019build\u2019, \u2019bump\u2019, \u2019call\u2019, \u2019change\u2019,\n\u2019check\u2019, \u2019choose\u2019, \u2019clarify\u2019, \u2019clean\u2019, \u2019clear\u2019, \u2019clone\u2019, \u2019comment\u2019, \u2019complete\u2019, \u2019compress\u2019, \u2019con-\ncatenate\u2019, \u2019configure\u2019, \u2019connect\u2019, \u2019consolidate\u2019, \u2019convert\u2019, \u2019copy\u2019, \u2019correct\u2019, \u2019cover\u2019, \u2019create\u2019,\n\u2019customize\u2019, \u2019cut\u2019, \u2019deal with\u2019, \u2019debug\u2019, \u2019decipher\u2019, \u2019declare\u2019, \u2019decommission\u2019, \u2019decomplexify\u2019,\n\u2019decompress\u2019, \u2019decrease\u2019, \u2019decrypt\u2019, \u2019define\u2019, \u2019delete\u2019, \u2019deploy\u2019, \u2019designate\u2019, \u2019destroy\u2019, \u2019detach\u2019,\n\u2019determine\u2019, \u2019develop\u2019, \u2019diminish\u2019, \u2019disable\u2019, \u2019discard\u2019, \u2019disentangle\u2019, \u2019dismantle\u2019, \u2019divide\u2019,\n\u2019document\u2019, \u2019downgrade\u2019, \u2019drop\u2019, \u2019duplicate\u2019, \u2019edit\u2019, \u2019embed\u2019, \u2019emphasize\u2019, \u2019enable\u2019, \u2019encrypt\u2019,\n\u2019enforce\u2019, \u2019enhance\u2019, \u2019enlarge\u2019, \u2019enumerate\u2019, \u2019eradicate\u2019, \u2019escalate\u2019, \u2019establish\u2019, \u2019exclude\u2019,\n\u2019exit\u2019, \u2019expand\u2019, \u2019expedite\u2019, \u2019expire\u2019, \u2019extend\u2019, \u2019facilitate\u2019, \u2019fix\u2019, \u2019format\u2019, \u2019gather\u2019, \u2019generalize\u2019,\n\u2019halt\u2019, \u2019handle\u2019, \u2019hasten\u2019, \u2019hide\u2019, \u2019implement\u2019, \u2019improve\u2019, \u2019include\u2019, \u2019increase\u2019, \u2019increment\u2019,\n\u2019indent\u2019, \u2019index\u2019, \u2019inflate\u2019, \u2019initialize\u2019, \u2019insert\u2019, \u2019install\u2019, \u2019integrate\u2019, \u2019interpolate\u2019, \u2019interrupt\u2019,\n\u2019introduce\u2019, \u2019isolate\u2019, \u2019join\u2019, \u2019kill\u2019, \u2019leverage\u2019, \u2019load\u2019, \u2019magnify\u2019, \u2019maintain\u2019, \u2019make\u2019, \u2019man-\nage\u2019, \u2019mark\u2019, \u2019mask\u2019, \u2019mend\u2019, \u2019merge\u2019, \u2019migrate\u2019, \u2019modify\u2019, \u2019monitor\u2019, \u2019move\u2019, \u2019multiply\u2019,\n\u2019normalize\u2019, \u2019optimize\u2019, \u2019orchestrate\u2019, \u2019order\u2019, \u2019package\u2019, \u2019paraphrase\u2019, \u2019paste\u2019, \u2019patch\u2019, \u2019plug\n\u2019, \u2019prepare\u2019, \u2019prepend\u2019, \u2019print\u2019, \u2019provision\u2019, \u2019purge\u2019, \u2019put\u2019, \u2019quit\u2019, \u2019raise\u2019, \u2019read\u2019, \u2019reannotate\u2019,\n\u2019rearrange\u2019, \u2019rebase\u2019, \u2019reboot\u2019, \u2019rebuild\u2019, \u2019recomment\u2019, \u2019recompile\u2019, \u2019reconfigure\u2019, \u2019reconnect\u2019,\n\u2019rectify\u2019, \u2019redact\u2019, \u2019redefine\u2019, \u2019reduce\u2019, \u2019refactor\u2019, \u2019reformat\u2019, \u2019refresh\u2019, \u2019reimplement\u2019, \u2019rein-\nforce\u2019, \u2019relocate\u2019, \u2019remove\u2019, \u2019rename\u2019, \u2019reorder\u2019, \u2019reorganize\u2019, \u2019repackage\u2019, \u2019repair\u2019, \u2019rephrase\u2019,\n\u2019replace\u2019, \u2019reposition\u2019, \u2019reschedule\u2019, \u2019reset\u2019, \u2019reshape\u2019, \u2019resolve\u2019, \u2019restructure\u2019, \u2019return\u2019, \u2019revert\u2019,\n\u2019revise\u2019, \u2019revoke\u2019, \u2019reword\u2019, \u2019rework\u2019, \u2019rewrite\u2019, \u2019rollback\u2019, \u2019save\u2019, \u2019scale\u2019, \u2019scrub\u2019, \u2019secure\u2019,\n\u2019select\u2019, \u2019send\u2019, \u2019set\u2019, \u2019settle\u2019, \u2019simplify\u2019, \u2019solve\u2019, \u2019sort\u2019, \u2019speed up\u2019, \u2019split\u2019, \u2019stabilize\u2019, \u2019standard-\nize\u2019, \u2019stipulate\u2019, \u2019stop\u2019, \u2019store\u2019, \u2019streamline\u2019, \u2019strengthen\u2019, \u2019structure\u2019, \u2019substitute\u2019, \u2019subtract\u2019,\n\u2019support\u2019, \u2019swap\u2019, \u2019switch\u2019, \u2019synchronize\u2019, \u2019tackle\u2019, \u2019tag\u2019, \u2019terminate\u2019, \u2019test\u2019, \u2019throw\u2019, \u2019tidy\u2019,\n\u2019transform\u2019, \u2019transpose\u2019, \u2019trim\u2019, \u2019troubleshoot\u2019, \u2019truncate\u2019, \u2019tweak\u2019, \u2019unblock\u2019, \u2019uncover\u2019,\n\u2019undo\u2019, \u2019unify\u2019, \u2019uninstall\u2019, \u2019unplug\u2019, \u2019unpublish\u2019, \u2019unravel\u2019, \u2019unstage\u2019, \u2019unsync\u2019, \u2019untangle\u2019,\n\u2019unwind\u2019, \u2019update\u2019, \u2019upgrade\u2019, \u2019use\u2019, \u2019validate\u2019, \u2019verify\u2019, \u2019watch\u2019, \u2019watermark\u2019, \u2019whitelist\u2019,\n\u2019withdraw\u2019, \u2019work\u2019, \u2019write\"\nTable 7: Commit message starting words allowed in COMMITPACKFT.\nPlease categorize the following commit message, which may fall into more than one category.\n### Category\nBug fixes, New features, Refactoring/code cleanup, Documentation, Testing, User interface,\nDependencies, Configuration, Build system/tooling, Performance improvements, Formatting/Linting,\nSecurity, Technical debt repayment, Release management, Accessibility, Deprecation, Logging/In-\nstrumentation, Internationalization\n### Commit Message\nAdd the blacklist checking to the bulk\n### Classification\nBug fixes, New features\n### Commit Message\n{COMMIT_MESSAGE}\n### Classification\nFigure 5: GPT-4 1-shot prompt for classifying commits in COMMITPACKFT.\n31\nOctoPack: Instruction Tuning Code Large Language Models\nxP3x\nWe use a subset of xP3x (Muennighoff et al., 2022b) focusing on code datasets consisting of\nAPPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022b), Jupyter Code Pairs,6 MBPP (Austin\net al., 2021), XLCoST (Zhu et al., 2022), Code Complex (Jeon et al., 2022), Docstring Corpus\n(Barone & Sennrich, 2017), Great Code (Hellendoorn et al., 2019) and State Changes.7\nOASST\nWe reuse a filtered variant of OASST (K\u00f6pf et al., 2023) from prior work (Dettmers et al.,\n2023) and apply additional filters to remove responses that refuse to comply with the user request. To\ncompute the programming languages and code fraction for OASST depicted in Table 1, we count all\nresponses containing e.g. \u2018\u2018\u2018python or \u2018\u2018\u2018py for the Python programming language. There are\ncode samples that are not enclosed in backticks or do not specify the language, thus we are likely\nunderestimating the actual fraction of code data for OASST in Table 1.\nE\nCOMPARING DATA BEFORE AND AFTER FILTERING\nIn Table 8 we compare word statistics prior to and after filtering COMMITPACK to create COMMIT-\nPACKFT. The mean commit subject and message length increases suggesting that messages are more\ninformative in COMMITPACKFT. The code lengths decrease significantly as we limit the number of\nallowed tokens in the filters in Table 6. This is intended, as we would like to maximize the amount\nof training signal per token. The code before and after the commit are usually largely the same.\nBy filtering for short samples, we ensure that there are more differences between the code before\nand after, thus making the model learn faster. The percentage of code changed between pre- and\npost-commit is 77.6/59.1 = 1.31 (a 31% increase) as opposed to 3269.8/3269.9 = 1.007 (a 0.7%\nincrease). Thus, the filtered data carries significantly more signal per token with fewer repetitions of\nthe code prior to the commit.\nMetric\nBefore Filter\nAfter Filter\nDifference\nSubject Length (words)\n5.7\u00b10.02\n6.9\u00b10.01\n+1.28\nMessage Length (words)\n8.7\u00b10.06\n9.9\u00b10.05\n+1.34\nPre-Commit Code Length (words)\n3269.9\u00b1298.8\n59.1\u00b10.19\n-3210.9\nPost-Commit Code Length (words)\n3269.8\u00b1299.5\n77.6\u00b10.23\n-3214.2\nTable 8: The effect of data filters on subject, message, and code lengths. We compare differences\nin word statistics of COMMITPACK and COMMITPACKFT.\nF\nCOMPARING COMMITPACK AND THE STACK\nIn Table 9 we provide statistics on repositories and usernames of COMMITPACK and The Stack (Ko-\ncetkov et al., 2022). COMMITPACK contains a total of 1,934,255 repositories. Around half (49.3%) of\nthem are also in The Stack. However, The Stack only provides the raw code files of these repositories\nfrom some fixed point in time. COMMITPACK contains the changes made to the code files in the form\nof commits. Thus, the same code file may appear multiple times in COMMITPACK for each change\nthat was made to it. Therefore, The Stack only contains 3 terabytes of data, while COMMITPACK\ncontains close to 4.\nStatistic (\u2193)\nCOMMITPACK\nThe Stack 1.2\nShared\nShared (%)\nRepositories\n1,934,255\n18,712,378\n954,135\n49.3%\nUsernames\n825,885\n6,434,196\n663,050\n80.3%\nTable 9: Overlap in repositories and usernames of COMMITPACK and The Stack.\n6https://hf.co/datasets/codeparrot/github-jupyter-text-code-pairs\n7https://hf.co/datasets/Fraser/python-state-changes\n32\nOctoPack: Instruction Tuning Code Large Language Models\nG\nPRETRAINING ON COMMITPACK\nDue to the scale of COMMITPACK, it is also adequate as a large-scale pretraining dataset. We\nhave included parts of COMMITPACK during the pretraining of StarCoder (Li et al., 2023b) in the\nformat of <commit_before>code_before<commit_msg>message<commit_after>\ncode_after. We also pretrain a new model, named SANTACODERPACK, with the same architec-\nture as SantaCoder (Allal et al., 2023) on COMMITPACK using this format. We filter COMMITPACK\nfor our six evaluation languages and samples that fit within 8192 tokens leaving us a total of 35B\ntokens. Following prior work (Muennighoff et al., 2023), we train on this data repeated close to 4\ntimes for a total of 131B tokens taking 14 days. Detailed hyperparameters are in Appendix P.\nIn Table 10, we benchmark StarCoder and SANTACODERPACK on HUMANEVALFIX using the\nabove-detailed commit format. We find that the commit format leads to very strong performance\nfor StarCoder often surpassing the instruction tuned OCTOCODER from Table 2. However, this\npretraining format is not suitable for HUMANEVALEXPLAIN limiting its universality. For SAN-\nTACODERPACK, we find performance comparable to SantaCoder, including checkpoints at 131B\nand 236B tokens. SANTACODERPACK performs slightly worse on Python than SantaCoder. We\nhypothesize that this discrepancy is due to a multilingual tax, as SANTACODERPACK needs to\naccommodate three additional coding languages (Go, C++ and Rust). SantaCoder has thus more\ncapacity allocated to Python, JavaScript, and Java.\nSANTACODERPACK may also be bottlenecked by its small model size of 1.1B parameters. More\nresearch into what exactly happens during pretraining (Xia et al., 2022; Biderman et al., 2023a) and\nhow to unify pretraining and instruction tuning are needed. Prior work has also found that including\nraw code data during pretraining benefits some natural language tasks (Muennighoff et al., 2023;\nSoldaini et al., 2024; Groeneveld et al., 2024). Future work may consider the effects of including\ncode commit data on natural language tasks.\nModel (\u2193)\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\nSantaCoder (131B tokens) Instruct Format\n6.5\n4.2\n2.9\n-\n-\n-\n-\nSantaCoder (236B tokens) Instruct Format\n7.1\n4.2\n1.8\n-\n-\n-\n-\nSANTACODERPACK (131B tokens) Commit Format\n3.2\n4.9\n1.8\n3.6\n4.2\n1.7\n3.3\nStarCoder Commit Format\n32.7\n33.6\n33.0\n31.9\n31.6\n20.2\n30.5\nTable 10: Zero-shot pass@1 (%) performance on HUMANEVALFIX of pretraining experiments.\nH\nHUMANEVALPACK STATISTICS\nTable 11 displays statistics of HUMANEVALPACK. Docstrings are largely the same across languages\nleading to similar statistics except for Rust. As Rust is already a very verbose language as seen by\nits maximum solution length in Table 11, we do not include examples of how to call the function at\nthe end of its docstrings (see Python docstrings with examples in e.g. Figure 11). Rust also has type\nannotations for every function so providing these examples is not as needed as it is for e.g. JavaScript\nwhich lacks type annotations.\nStatistic (\u2193)\nPython\nJavaScript\nJava\nGo\nC++\nRust\nDocstring Avg. Length (chars)\n354\n352\n357\n365\n352\n231\nDocstring Min. Length (chars)\n56\n56\n56\n56\n56\n23\nDocstring Max. Length (chars)\n1207\n1207\n1211\n1207\n1207\n1067\nSolution Avg. Length (chars)\n181\n234\n259\n354\n295\n339\nSolution Min. Length (chars)\n16\n19\n18\n29\n17\n17\nSolution Max. Length (chars)\n864\n1325\n1399\n1333\n1144\n2157\nTable 11: Statistics of HUMANEVALPACK computed across the 164 samples for each language.\n33\nOctoPack: Instruction Tuning Code Large Language Models\nI\nFULL INSTRUCTION DATA ABLATIONS\nWe\nprovide\ntabular\nresults\nof\nthe\nablations\nfrom\nFigure\n4\nin\nTable\n12.\nWe\ntry\nsome\nadditional\nmixtures,\nhowever,\nnone\nof\nthem\nperform\nbetter\nthan\nCOM-\nMITPACKFT\n+\nOASST.\nWe\nexperiment\nwith\nchanging\nthe\nformatting\nto\nbe\n<commit_before>old code<commit_msg>message<commit_after>new code for\nCOMMITPACKFT and <commit_before><commit_msg>input<commit_after>output\nfor OASST referred to as the \"Formatting\" ablation. We hypothesized that aligning the formatting\nduring instruction tuning with the commit format that we used during pretraining (Appendix G)\nwould improve performance.\nWhile it seems to improve performance for HUMANEVALFIX\ncompared to our default formatting (see Figure 18), it reduces performance on the other tasks leading\nto a worse average score of 35.3 in Table 12. \"Target Loss\" refers to an ablation where we mask\nloss for inputs as is commonly done during instruction tuning (Muennighoff et al., 2022b). While\nthis leads to the best performance on HUMANEVALSYNTHESIZE, its average performance is worse\ncompared to COMMITPACKFT + OASST, where the loss is computed over the full sequence. We\nalso perform an ablation where we manually select 1178 high-quality samples (725 from OASST and\n89, 61, 86, 72, 70 and 75 from COMMITPACKFT for Python, JavaScript, Java, Go, C++ and Rust,\nrespectively). However, this manual selection did not outperform random selection for OCTOCODER.\nIt performed better for OCTOGEEX, however, hence we used it for OCTOGEEX. We hypothesize that\nour models could achieve significantly better performance by further improving the quality of the\ninstruction data beyond. This may necessitate very careful human selection of samples and manual\nediting of the data to ensure a uniform style in the outputs. We leave such explorations to future work.\nHUMANEVALPACK Python\nInstruction Tuning Dataset (\u2193)\nFix\nExplain\nSynthesize\nAverage\nWithout instruction tuning\n8.7\n0.0\n33.6\n14.1\nSelf-Instruct (SI)\n23.6\n0.6\n43.0\n22.2\nOASST\n23.1\n34.5\n46.4\n34.7\nSI + OASST\n24.9\n28.7\n46.2\n33.3\nxP3x + OASST\n28.4\n28.4\n45.0\n33.9\nCOMMITPACKFT + OASST\n30.4\n35.1\n46.2\n37.2\nCOMMITPACKFT + OASST (Formatting)\n31.1\n28.9\n45.8\n35.3\nCOMMITPACKFT + OASST (Target loss)\n29.8\n31.2\n47.8\n36.3\nCOMMITPACKFT + OASST (Manual)\n27.2\n29.6\n45.8\n34.2\nCOMMITPACKFT + xP3x + OASST\n30.9\n29.5\n45.9\n35.4\nCOMMITPACKFT + SI + xP3x + OASST\n31.4\n33.8\n46.0\n37.1\nTable 12: Zero-shot pass@1 (%) performance across the Python split of HUMANEVALPACK for\nStarCoder instruction tuning data ablations.\nJ\nLINE DIFF FORMAT FOR FIXING CODE\nWe finetune SantaCoder to experiment with different formatting strategies for fixing bugs comparing\nfull code generation and code diff generation. When fixing a code bug, usually only a small part of\nthe code needs to change. Only generating the code diff corresponding to the necessary change can\nmake inference significantly more efficient by avoiding repeated characters in the output generation.\nWe finetune SantaCoder on the Python, Java and JavaScript subset of COMMITPACKFT. We exclude\nother languages as SantaCoder has only been pretrained on these three languages (Allal et al., 2023).\nCommit Format\nFor full code generation, we reuse the format that we employed for commits in\nStarCoder pretraining from Appendix G: <commit_before>code_before<commit_msg>\nmessage<commit_after>code_after. However, SantaCoder has not seen this format during\npretraining and does not have special tokens like StarCoder for the delimiters. Thus, for SantaCoder\ne.g. <commit_before> is tokenized as [\u2019<\u2019, \u2019commit\u2019, \u2019_\u2019, \u2019before\u2019, \u2019>\u2019].\n34\nOctoPack: Instruction Tuning Code Large Language Models\nUnified diff format\nFor code diff generation, a simple solution is using the unified diff format,8\nwhich is a standard way to display changes between code files in a compact and readable format\n(Lehman et al., 2022; Jung, 2021; Xu et al., 2022b; Monperrus et al., 2021). We depict an example of\nthis format in Figure 6. However, the unified diff format still requires the model to output several\nunchanged lines below and after the actual modification. Thus, its efficiency gains are limited and\nthere is still unnecessary duplication of the input.\nfrom typing import List\ndef has_close_elements(numbers: List[float],\nthreshold: float) -> bool:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers)\n:\nif idx != idx2:\ndistance = elem - elem2\nif distance < threshold:\nreturn True\nreturn False\nfrom typing import List\ndef has_close_elements(numbers: List[float],\nthreshold: float) -> bool:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers)\n:\nif idx != idx2:\ndistance = abs(elem - elem2)\nif distance < threshold:\nreturn True\nreturn False\n@@ -4,7 +4,7 @@\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\n-\ndistance = elem - elem2\n+\ndistance = abs(elem - elem2)\nif distance < threshold:\nreturn True\nFigure 6: The first problem from the HUMANEVALFIX Python split and the necessary change\nto fix the bug in unified diff format. Top: Code with and without the bug from Figure 11. Bottom:\nNecessary change to fix the bug in unified diff format.\n-\n7\ndistance = elem - elem2\n+\n7\ndistance = abs(elem - elem2)\nFigure 7: The line diff format for the problem from Figure 6.\nLine diff format\nTo address the inefficiencies of the unified diff format, we propose the line diff\nformat for representing code differences. There are two requirements for our format: (1) The diff\ncan be unambiguously applied to the code before the commit to generate the code after the commit,\nand (2) the code diff should be as short as possible to maximize efficiency by avoiding the inclusion\nof unchanged code. In Figure 7, we show how our format addresses these. The line diff format\nkeeps track of each change sequentially line-by-line to ensure the code can be correctly modified.\nBy focusing only on the lines that change, we reduce the number of characters in the diff by 70%\ncompared to the unified diff representation in Figure 6.\nBoth the unified diff format and our line diff format require the model to predict line numbers.\nThis is very challenging when training on raw code as models need to count and keep track of line\nnumbers. To simplify line number prediction, we automatically add line numbers to the raw code in\nthe finetuning dataset for the line diff format. This allows the model to simply copy the line number\ninto the output simplifying the diff generation. However, it diminishes efficiency slightly by adding\nadditional input tokens that the model needs to process.\nAs summarized in Table 13, finetuning SantaCoder using the line diff format significantly improves\nperformance compared to prior finetuning on HUMANEVALFIX across all languages. It also out-\nperforms finetuning using the commit format, which only provides gains on JavaScript and Java\ncompared to no finetuning. However, finetuning on the diff format may converge slower than the\ncommit format as the diff format significantly differs from the raw code seen during pretraining.\n8https://en.wikipedia.org/wiki/Diff#Unified_format\n35\nOctoPack: Instruction Tuning Code Large Language Models\nFigures 8, 9, 10 show line diff generations of our model. A limitation of our current line diff im-\nplementation is that it does not handle code insertion well. The inserted lines may change the line\nnumbers of all following lines, which can result in problems when applying the diff. Further, the\ndiff format is not useful for HUMANEVALEXPLAIN and HUMANEVALSYNTHESIZE. Future work\ncould consider training models that can both be instructed to use the line diff format, such as for\nHUMANEVALFIX, but also explain or synthesize code without producing a diff.\nModel\nPython\nJavaScript\nJava\nSantaCoder\n7.1\n4.2\n1.8\nSantaCoder + Commit format finetuning\n3.8\n5.3\n9.2\nSantaCoder + Line diff format finetuning\n9.9\n9.7\n10.0\nTable 13: Zero-shot pass@1 (%) performance on HUMANEVALFIX of SantaCoder formatting\nexperiments.\n-\n3\nlet depth = 0, max_depth = 0;\n+\n3\nlet depth = 0, max_depth = 1;\n- 12\nreturn max_depth;\n+ 12\nreturn max_depth - 1;\n- 14\nreturn paren_string.split(\u2019 \u2019)\n- 15\n.filter(x => x != \u2019\u2019)\n- 16\n.map(x => parseParenGroup(x));\n- 17 }\n+ 14\nlet paren_list = paren_string.split(\u2019 \u2019);\n+ 15\nlet nested_parens = paren_list.map(x => parseParenGroup(x));\n+ 16\nreturn nested_parens.reduce((prev, curr) => {\n+ 17\nif (prev == 0) {\n+ 18\nreturn curr;\n+ 19\n} else {\n+ 20\nreturn curr - 1;\n+ 21\n}\n+ 22\n});\n+ 23 }\nFigure 8: A line diff generation of our model on a JavaScript HUMANEVALFIX problem.\n- 18\nif (current_depth < 0) {\n+ 18\nif (current_depth < 0 && current_string.length() > 0) {\nFigure 9: A line diff generation of our model on a Java HUMANEVALFIX problem.\n36\nOctoPack: Instruction Tuning Code Large Language Models\n-\n2\nfor i, l1 in enumerate(l):\n-\n3\nfor j in range(i, len(l)):\n+\n2\nfor i in range(0, len(l)):\n+\n3\nfor j in range(i+1, len(l)):\nFigure 10: A line diff generation of our model on a Python HUMANEVALFIX problem.\nK\nRESULTS ON HUMANEVALFIXDOCS\nThe default version of HUMANEVALFIX does not include docstrings, but only provides the unit tests\nto the model alongside the buggy function. An alternative is providing docstrings as the source of\nground truth for the model to fix the buggy function. Solving from docstrings is generally easier for\nmodels than from tests, as models can also solve it via pure code synthesis without looking at the\nbuggy function at all. We provide results of some models on this variant in Table 14. For StarCoder,\nwe distinguish two prompting formats: An instruction to fix bugs like in Figure 3 or the commit\nformat it has seen during pretraining (Appendix G). OCTOCODER performs strongly on this variant.\nHowever, directly using StarCoder with the commit format from pretraining (Appendix G) is slightly\nbetter. This is in line with the commit format from pretraining also performing slightly better on\nHUMANEVALFIX in Table 10 compared to OCTOCODER in Table 2. Diff Codegen 2B (Bradley et al.,\n2023) performs poorly as its predicted code diffs are often irrelevant to the actual bug, see Figure 39.\nModel\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\nNon-permissive models\nGPT-4\n88.4\n80.5\n82.9\n81.1\n82.3\n68.9\n80.7\nPermissive Models\nDiff Codegen 2B\n0.0\n0.1\n0.0\n0.3\n0.0\n0.2\n0.1\nStarCoder Commit Format\n58.8\n49.2\n43.9\n55.2\n51.5\n41.8\n50.1\nStarCoder Instruct Format\n41.7\n30.7\n44.3\n34.5\n28.7\n14.0\n26.5\nOCTOCODER\n53.8\n48.1\n54.3\n54.9\n49.2\n32.1\n48.7\nTable 14: Zero-shot pass@1 (%) performance on HUMANEVALFIXDOCS.\nL\nHUMANEVALFIX BUG TYPES\nTable 15 contains an overview of bugs that were manually added by one of the authors to HumanEval\nsolutions for the construction of HUMANEVALFIX. Figures 11-16 contain an example of each type\nfrom the Python split. The bug type for each problem is the same across all programming languages\nin HUMANEVALFIX, but for a few samples it affects a different part of the solution due to the code\nsolutions not being perfectly parallel across languages.\nBug type\nSubtype\nExplanation\nExample\nCount\nMissing logic\nMisses code needed to solve the problem\nFigure 11\n33\nExcess logic\nContains excess code leading to mistakes\nFigure 12\n31\nWrong logic\nValue misuse\nAn incorrect value is used\nFigure 13\n44\nOperator misuse\nAn incorrect operator is used\nFigure 14\n25\nVariable misuse\nAn incorrect variable is used\nFigure 15\n23\nFunction misuse\nAn incorrect function is used\nFigure 16\n8\nTotal\n164\nTable 15: HUMANEVALFIX bug types.\n37\nOctoPack: Instruction Tuning Code Large Language Models\nfrom typing import List\ndef has_close_elements(numbers: List[float\n], threshold: float) -> bool:\n\"\"\" Check if in given list of numbers,\nare any two numbers closer to\neach other than\ngiven threshold.\n>>> has_close_elements([1.0, 2.0,\n3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0,\n4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(\nnumbers):\nif idx != idx2:\ndistance = abs(elem -\nelem2)\nif distance < threshold:\nreturn True\nreturn False\nfrom typing import List\ndef has_close_elements(numbers: List[float\n], threshold: float) -> bool:\n\"\"\" Check if in given list of numbers,\nare any two numbers closer to\neach other than\ngiven threshold.\n>>> has_close_elements([1.0, 2.0,\n3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0,\n4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(\nnumbers):\nif idx != idx2:\ndistance = elem - elem2\nif distance < threshold:\nreturn True\nreturn False\nFigure 11: Missing logic bug example. The buggy code (right) misses the \u2019abs\u2019 statement.\ndef truncate_number(number: float) ->\nfloat:\n\"\"\" Given a positive floating point\nnumber, it can be decomposed into\nand integer part (largest integer\nsmaller than given number) and\ndecimals\n(leftover part always smaller than 1).\nReturn the decimal part of the number.\n>>> truncate_number(3.5)\n0.5\n\"\"\"\nreturn number % 1.0\ndef truncate_number(number: float) ->\nfloat:\n\"\"\" Given a positive floating point\nnumber, it can be decomposed into\nand integer part (largest integer\nsmaller than given number) and\ndecimals\n(leftover part always smaller than 1).\nReturn the decimal part of the number.\n>>> truncate_number(3.5)\n0.5\n\"\"\"\nreturn number % 1.0 + 1.0\nFigure 12: Excess logic bug example. The buggy code (right) incorrectly adds 1 to the result.\nfrom typing import List, Tuple\ndef sum_product(numbers: List[int]) ->\nTuple[int, int]:\n\"\"\" For a given list of integers,\nreturn a tuple consisting of a\nsum and a product of all the\nintegers in a list.\nEmpty sum should be equal to 0 and\nempty product should be equal to\n1.\n>>> sum_product([])\n(0, 1)\n>>> sum_product([1, 2, 3, 4])\n(10, 24)\n\"\"\"\nsum_value = 0\nprod_value = 1\nfor n in numbers:\nsum_value += n\nprod_value *= n\nreturn sum_value, prod_value\nfrom typing import List, Tuple\ndef sum_product(numbers: List[int]) ->\nTuple[int, int]:\n\"\"\" For a given list of integers,\nreturn a tuple consisting of a\nsum and a product of all the\nintegers in a list.\nEmpty sum should be equal to 0 and\nempty product should be equal to\n1.\n>>> sum_product([])\n(0, 1)\n>>> sum_product([1, 2, 3, 4])\n(10, 24)\n\"\"\"\nsum_value = 0\nprod_value = 0\nfor n in numbers:\nsum_value += n\nprod_value *= n\nreturn sum_value, prod_value\nFigure 13: Value misuse bug example. The buggy code (right) incorrectly initializes the product to\n0.\n38\nOctoPack: Instruction Tuning Code Large Language Models\nfrom typing import List\ndef below_zero(operations: List[int]) ->\nbool:\n\"\"\" You\u2019re given a list of deposit and\nwithdrawal operations on a bank\naccount that starts with\nzero balance. Your task is to detect\nif at any point the balance of\naccount fallls below zero, and\nat that point function should return\nTrue. Otherwise it should return\nFalse.\n>>> below_zero([1, 2, 3])\nFalse\n>>> below_zero([1, 2, -4, 5])\nTrue\n\"\"\"\nbalance = 0\nfor op in operations:\nbalance += op\nif balance < 0:\nreturn True\nreturn False\nfrom typing import List\ndef below_zero(operations: List[int]) ->\nbool:\n\"\"\" You\u2019re given a list of deposit and\nwithdrawal operations on a bank\naccount that starts with\nzero balance. Your task is to detect\nif at any point the balance of\naccount fallls below zero, and\nat that point function should return\nTrue. Otherwise it should return\nFalse.\n>>> below_zero([1, 2, 3])\nFalse\n>>> below_zero([1, 2, -4, 5])\nTrue\n\"\"\"\nbalance = 0\nfor op in operations:\nbalance += op\nif balance == 0:\nreturn True\nreturn False\nFigure 14: Operator misuse bug example. The buggy code (right) incorrectly checks for equality\nwith 0.\nfrom typing import List\ndef mean_absolute_deviation(numbers: List[\nfloat]) -> float:\n\"\"\" For a given list of input numbers,\ncalculate Mean Absolute\nDeviation\naround the mean of this dataset.\nMean Absolute Deviation is the average\nabsolute difference between each\nelement and a centerpoint (mean in\nthis case):\nMAD = average | x - x_mean |\n>>> mean_absolute_deviation([1.0, 2.0,\n3.0, 4.0])\n1.0\n\"\"\"\nmean = sum(numbers) / len(numbers)\nreturn sum(abs(x - mean) for x in\nnumbers) / len(numbers)\nfrom typing import List\ndef mean_absolute_deviation(numbers: List[\nfloat]) -> float:\n\"\"\" For a given list of input numbers,\ncalculate Mean Absolute\nDeviation\naround the mean of this dataset.\nMean Absolute Deviation is the average\nabsolute difference between each\nelement and a centerpoint (mean in\nthis case):\nMAD = average | x - x_mean |\n>>> mean_absolute_deviation([1.0, 2.0,\n3.0, 4.0])\n1.0\n\"\"\"\nmean = sum(numbers) / len(numbers)\nreturn sum(abs(x - mean) for x in\nnumbers) / mean\nFigure 15: Variable misuse bug example. The buggy code (right) incorrectly divides by the mean.\ndef flip_case(string: str) -> str:\n\"\"\" For a given string, flip lowercase\ncharacters to uppercase and\nuppercase to lowercase.\n>>> flip_case(\u2019Hello\u2019)\n\u2019hELLO\u2019\n\"\"\"\nreturn string.swapcase()\ndef flip_case(string: str) -> str:\n\"\"\" For a given string, flip lowercase\ncharacters to uppercase and\nuppercase to lowercase.\n>>> flip_case(\u2019Hello\u2019)\n\u2019hELLO\u2019\n\"\"\"\nreturn string.lower()\nFigure 16: Function misuse bug example. The buggy code (right) incorrectly uses the \u2019lower()\u2019\nfunction.\n39\nOctoPack: Instruction Tuning Code Large Language Models\nM\nPERFORMANCE BREAKDOWN BY HUMANEVALFIX BUG TYPE\nAll bugs in HUMANEVALFIX are categorized into bug types as described in Appendix L. In Table 16,\nwe break down the HUMANEVALFIX performance of select models from Table 2 by bug type.\nWe find that models struggle most with bugs that require removing excess logic (e.g. Figure 12).\nWizardCoder is only able to solve 11% of excess logic bugs while solving about four times more\nbugs that relate to value misuse. The performance of OCTOGEEX and OCTOCODER is more stable\nthan WizardCoder across the different bug types, possibly due to the diversity of COMMITPACKFT\nas displayed in Figure 2. GPT-4 performs best across all bug types.\nBug type\nSubtype\nOCTOGEEX\nOCTOCODER\nWizardCoder\nGPT-4\nMissing logic\n24.2\n24.4\n31.2\n45.5\nExcess logic\n16.3\n16.9\n11.0\n38.7\nWrong logic\nValue misuse\n33.2\n34.7\n45.1\n50.0\nOperator misuse\n32.8\n42.0\n34.4\n56.0\nVariable misuse\n35.7\n33.7\n30.4\n43.5\nFunction misuse\n25.0\n37.5\n37.5\n50.0\nOverall\n28.1\n30.4\n31.8\n47.0\nTable 16: Breakdown of HUMANEVALFIX Python pass@1 (%) performance by bug type for\nselect models. Statistics for each bug type are in Table 15.\nN\nHUMANEVALEXPLAIN WITH FILL-IN-THE-MIDDLE\nIn Table 2, all models are prompted with the same instruction to provide an explanation (but using\nslightly different formats, see Appendix Q). For StarCoder, we can alternatively prompt it with the Fill-\nin-the-Middle (FIM) technique (Bavarian et al., 2022), which it already knows from pretraining (Li\net al., 2023b). To do so, we provide it with a prompt as shown in Figure 17 to generate the docstring.\nWhile a docstring is not necessarily an explanation, it can be similar. The results in Table 17 show\nthat it performs significantly better with this prompting strategy than in Table 2. However, it still falls\nshort of OCTOCODER and other models likely due to the imperfect approximation of an explanation.\n<fim_prefix>from typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n\"\"\" <fim_suffix>\n\"\"\"\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = abs(elem - elem2)\nif distance < threshold:\nreturn True\nreturn False<fim_middle>\nFigure 17: FIM prompt example for StarCoder.\nModel\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\nStarCoder FIM\n19.4\n17.6\n16.3\n11.8\n17.9\n16.7\n16.6\nTable 17: Performance of StarCoder on HUMANEVALEXPLAIN with FIM.\n40\nOctoPack: Instruction Tuning Code Large Language Models\nO\nHUMANEVALEXPLAIN BLEU AND METEOR COMPARISON\nBy default, we use pass@k to evaluate HUMANEVALEXPLAIN (\u00a73). In Table 18 we compare it\nwith BLEU (Papineni et al., 2002) and METEOR (Banerjee & Lavie, 2005). While our pass@k\nformulation does not require a ground truth explanation, BLEU and METEOR do. This can be a\nlimiting factor. For this evaluation, we use the function docstrings as the ground-truth explanation to\ncompute the BLEU and METEOR scores. We compute BLEU and METEOR for each of the n = 20\ngenerations (3) and select the highest score. The scores are then averaged across the 164 samples\nfor each language. Rust scores are the highest which is likely due to Rust docstrings containing no\nexample function calls (Appendix H).\nMetric (\u2193)\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\npass@1\n35.1\n24.5\n27.3\n21.1\n24.1\n14.8\n24.5\nBLEU-1\n7.1\n7.0\n6.3\n6.1\n6.4\n8.1\n6.8\nBLEU-2/3/4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nMETEOR\n7.8\n7.6\n7.2\n7.4\n7.0\n9.4\n7.7\nTable 18: Comparison of different metrics for HUMANEVALEXPLAIN on explanations by\nOCTOCODER. Pass@1 is computed with respect to a generated solution given the explanation (\u00a73)\nwhile BLEU and METEOR are computed by comparing the explanation with the docstring.\nP\nHYPERPARAMETERS\nStarCoder finetuning (OCTOCODER)\nFor all experiments finetuning StarCoder, we use a learning\nrate of 5e-4 with a cosine schedule and linear warmup. We use a batch size of 32 and train for up to\none epoch, as we did not observe benefits from more steps. OCTOCODER was trained for 35 steps\nwith a sequence length of 2048 and packing corresponding to 2.2 million total finetuning tokens.\nWe use LoRA (Hu et al., 2021) as we did not observe a significant difference from full finetuning.\nFollow-up work has further investigated this choice (Zhuo et al., 2024).\nCodeGeeX finetuning (OCTOGEEX)\nTo create OCTOGEEX, we finetune CodeGeeX2 for 35\nsteps with a batch size of 48 and a learning rate of 5e-5 largely following the OCTOCODER setup.\nSantaCoder finetuning\nFor all experiments finetuning SantaCoder, we use a learning rate of 5e-5\nwith a cosine schedule and linear warmup. We finetune SantaCoder using a batch size of 64 for up to\n200,000 steps.\nSantaCoder pretraining (SANTACODERPACK)\nWe follow the setup from Allal et al. (2023) to\npretrain on COMMITPACK except for using a sequence length of 8192 and the StarCoder tokenizer,\nwhich has special tokens for the commit format delimiters (see Appendix G). SANTACODERPACK\nutilizes Multi Query Attention (MQA) (Shazeer, 2019) but removes Fill-in-the-Middle (FIM) (Bavar-\nian et al., 2022). We conducted pretraining on 32 A100 GPUs, totaling 250k training steps, with a\nglobal batch size of 64. Other hyperparameter settings follow SantaCoder, including using Adam\nwith \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128, and a weight decay of 0.1. The learning rate is set to 2 \u00d7 10\u22124\nand follows a cosine decay after warming up for 2% of the training steps.\nQ\nPROMPTS\nThe prompting format can significantly impact performance. In the spirit of true few-shot learn-\ning (Perez et al., 2021) we do not optimize prompts and go with the format provided by the respective\nmodel authors or the most intuitive format if none is provided. For each task, we define an instruction,\nan optional context and an optional function start (Table 19). The function start is provided to make\nsure the model directly completes the function without having to search for the function in the model\noutput. These three parts are then combined in slightly different ways for each model (Figures 18-24).\nWe implement our evaluation using open-source frameworks (Ben Allal et al., 2022; Gao et al., 2021).\n41\nOctoPack: Instruction Tuning Code Large Language Models\nHUMANEVALFIX\nInstruction\nFix bugs in has_close_elements.\nContext\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = elem - elem2\nif distance < threshold:\nreturn True\nreturn False\nFunction start\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nHUMANEVALEXPLAIN\nInstruction\n(Describe)\nProvide a concise natural language description of the code using at most\n213 characters.\nContext\n(Describe)\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = abs(elem - elem2)\nif distance < threshold:\nreturn True\nreturn False\nInstruction\n(Synthesize)\nWrite functional code in Python according to the description.\nContext\n(Synthesize)\n{Description generated by the model}\nFunction start\n(Synthesize)\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nHUMANEVALSYNTHESIZE\nInstruction\nWrite a Python function \u2018has_close_elements(numbers: List[float], thresh-\nold: float) -> bool\u2018 to solve the following problem:\nCheck if in given list of numbers, are any two numbers closer to each other\nthan given threshold.\n\u00bb> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n\u00bb> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\nFunction start\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n\"\"\" Check if in given list of numbers, are any two numbers closer\nto each other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nTable 19: Instructions and function examples used. If no function start or no context is present,\nthat part is not added to the prompt (and the preceding newline is also removed).\n42\nOctoPack: Instruction Tuning Code Large Language Models\nQuestion: {instruction}\n{context}\nAnswer:\n{function_start}\nFigure 18: OCTOCODER and OCTOGEEX prompting format\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{instruction}\n{context}\n### Response:\n{function_start}\nFigure 19: WizardCoder prompting format from their codebase.9\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{instruction}\n{context}\n### Response:{function_start}\nFigure 20: InstructCodeT5+ decoder prompting format from their codebase.10 The encoder\nreceives the same prompt without the function start.\n9https://github.com/nlpxucan/WizardLM/blob/9c6fb34d012d60dc4f31348ee0a8\ne35335c04564/WizardCoder/src/humaneval_gen.py#L38\n10https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/humaneval/gen\nerate_codet5p.py#L89\n11https://huggingface.co/HuggingFaceH4/starchat-beta\n43\nOctoPack: Instruction Tuning Code Large Language Models\n<|system|>\n<|end|>\n<|user|>\n{instruction}\n{context}<|end|>\n<|assistant|>\n{function_start}\nFigure 21: StarChat-\u03b2 prompting format from their documentation.11\n{context}\n{instruction}\n{function_start}\nFigure 22: Default prompting format (StarCoder, BLOOMZ). Used for Figure 3.\n{context}\n{instruction}\nFigure 23: GPT-4 prompting format. Same as Figure 22 but excluding the function start, as we\nfound the model would not continue from the provided function start, likely due to the chat mode.\n{context}\n{instruction}\nStart your code with:\n{func_start}\nFigure 24: GPT-4 prompting format for HUMANEVALEXPLAIN (Synthesize). We include the\nfunction start for the synthesize part of HUMANEVALEXPLAIN in case the function name is not\nincluded in the code description generated by GPT-4.\n<NME> {filename}\n<BEF> {context}\n<MSG> {instruction}\n<DFF>\nFigure 25: Prompting format for Diff Codegen 2B evaluated in Appendix K.\n44\nOctoPack: Instruction Tuning Code Large Language Models\nR\nEXAMPLES\nR.1\nOCTOCODER\nInput:\nQuestion: Fix bugs in sum_product.\n#include<stdio.h>\n#include<vector>\nusing namespace std;\n#include<algorithm>\n#include<math.h>\n#include<stdlib.h>\nvector<int> sum_product(vector<int> numbers){\nint sum=0,product=0;\nfor (int i=0;i<numbers.size();i++)\n{\nsum+=numbers[i];\nproduct*=numbers[i];\n}\nreturn {sum,product};\n}\nAnswer:\n#include<stdio.h>\n#include<vector>\nusing namespace std;\n#include<algorithm>\n#include<math.h>\n#include<stdlib.h>\nvector<int> sum_product(vector<int> numbers){\nOCTOCODER:\nint sum=0,product=1;\nfor (int i=0;i<numbers.size();i++)\n{\nsum+=numbers[i];\nproduct*=numbers[i];\n}\nreturn {sum,product};\n}\nFigure 26: Correct solution from OCTOCODER to a HUMANEVALFIX C++ problem. The\ncorresponding Python problem is depicted in Figure 13.\n45\nOctoPack: Instruction Tuning Code Large Language Models\nInput:\nQuestion: Write a Java function \u2018public String decodeShift(String s)\u2018 to solve the following problem:\ntakes as input string encoded with encodeShift function. Returns decoded string.\nAnswer:\nimport java.util.*;\nimport java.lang.*;\nclass Solution {\n/**\nreturns encoded string by shifting every character by 5 in the alphabet.\n*/\npublic String encodeShift(String s) {\nStringBuilder sb = new StringBuilder();\nfor (char ch : s.toCharArray()) {\nsb.append((char) (\u2019a\u2019 + ((ch + 5 - \u2019a\u2019) % 26)));\n}\nreturn sb.toString();\n}\n/**\ntakes as input string encoded with encodeShift function. Returns decoded string.\n*/\npublic String decodeShift(String s) {\nOCTOCODER:\nStringBuilder sb = new StringBuilder();\nfor (char ch : s.toCharArray()) {\nsb.append((char) (\u2019a\u2019 + ((ch - \u2019a\u2019 - 5) % 26)));\n}\nreturn sb.toString();\n}\n}\nFigure 27: Incorrect solution from OCTOCODER to a HUMANEVALSYNTHESIZE Java problem.\nThe correct solution would be to add 21 to ch instead of subtracting 5 from it.\n46\nOctoPack: Instruction Tuning Code Large Language Models\nInput:\nQuestion: Provide a concise natural language description of the code using at most 671 characters.\nfunc FruitDistribution(s string,n int) int {\nsplit := strings.Split(s, \" \")\nfor _, i := range split {\natoi, err := strconv.Atoi(i)\nif err != nil {\ncontinue\n}\nn = n - atoi\n}\nreturn n\n}\nAnswer:\nOCTOCODER:\nThe function FruitDistribution takes two arguments, a string s and an integer n. The function splits\nthe string s into a slice of strings using the space character as the delimiter. The function iterates over\nthe slice and converts each string to an integer using the strconv.Atoi function. If the conversion is\nsuccessful, the integer is subtracted from n. The function returns the value of n after all conversions\nhave been performed.\nInput (without previous input):\nQuestion: Write functional code in Go according to the description.\nThe function FruitDistribution takes two arguments, a string s and an integer n. The function splits\nthe string s into a slice of strings using the space character as the delimiter. The function iterates over\nthe slice and converts each string to an integer using the strconv.Atoi function. If the conversion is\nsuccessful, the integer is subtracted from n. The function returns the value of n after all conversions\nhave been performed.\nAnswer:\nfunc FruitDistribution(s string,n int) int {\nOCTOCODER:\nfruits := strings.Split(s, \" \")\nfor _, fruit := range fruits {\nif num, err := strconv.Atoi(fruit); err == nil {\nn -= num\n}\n}\nreturn n\n}\nFigure 28: Correct solution from OCTOCODER to a HUMANEVALEXPLAIN Go problem. Given\nits own description OCTOCODER writes a correct solution to the problem. Its solution is more\ncompact and efficient than the original solution.\n47\nOctoPack: Instruction Tuning Code Large Language Models\nR.2\nGPT-4\nUser:\nWrite a Go function \u2018func RescaleToUnit(numbers []float64) []float64\u2018 to solve the following problem:\nGiven a list of numbers (of at least two elements), apply a linear transform to that list,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> RescaleToUnit([1.0, 2.0, 3.0, 4.0, 5.0])\n[0.0, 0.25, 0.5, 0.75, 1.0]\nGPT-4:\nHere is a Go function that solves the problem:\npackage main\nimport (\n\"fmt\"\n\"math\"\n)\nfunc RescaleToUnit(numbers []float64) []float64 {\nmin := numbers[0]\nmax := numbers[0]\n// Find the min and max values in the list\nfor _, number := range numbers {\nif number < min {\nmin = number\n}\nif number > max {\nmax = number\n}\n}\n// Apply the linear transform to each number in the list\nfor i, number := range numbers {\nnumbers[i] = (number - min) / (max - min)\n}\nreturn numbers\n}\nfunc main() {\nfmt.Println(RescaleToUnit([]float64{1.0, 2.0, 3.0, 4.0, 5.0}))\n}\nThis function first finds the minimum and maximum values in the list of numbers. Then it applies\nthe linear transformation to each number in the list. The transformation is \u2018(number - min) / (max -\nmin)\u2018, which scales the number such that \u2018min\u2018 becomes 0 and \u2018max\u2018 becomes 1. The transformed\nnumbers are returned in a new list.\nFigure 29: Incorrect solution from GPT-4 to a HUMANEVALSYNTHESIZE Go problem. GPT-4\nincludes an import that it does not use (math) which leads to a compilation error in Golang and thus\nthe function does not pass the test cases.\n48\nOctoPack: Instruction Tuning Code Large Language Models\nUser:\nProvide a concise natural language description of the code using at most 246 characters.\ndef truncate_number(number: float) -> float:\nreturn number % 1.0\nGPT-4:\nThe code defines a function that takes a float as input and returns its fractional part by using the\nmodulus operator with 1.0.\nUser (without previous conversation history):\nThe code defines a function that takes a float as input and returns its fractional part by using the\nmodulus operator with 1.0.\nWrite functional code in Python according to the description.\nStart your code with:\ndef truncate_number(number: float) -> float:\nGPT-4:\ndef truncate_number(number: float) -> float:\nreturn number % 1.0\nFigure 30: Correct solution from GPT-4 to a HUMANEVALEXPLAIN Python problem. GPT-4\nwrites a description that is significantly shorter than the docstring and thus within the character limit.\nIt then generates the perfect solution from its own description. The docstring for this problem can be\nseen in Figure 12.\n49\nOctoPack: Instruction Tuning Code Large Language Models\nUser:\nfn has_close_elements(numbers:Vec<f32>, threshold: f32) -> bool{\nfor i in 0..numbers.len(){\nfor j in 1..numbers.len(){\nif i != j {\nlet distance:f32 = numbers[i] - numbers[j];\nif distance < threshold{\nreturn true;\n}\n}\n}\n}\nreturn false;\n}\ndef check(has_close_elements):\nassert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\nassert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\nassert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\nassert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\nassert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\nassert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\nassert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\ncheck(has_close_elements)\nFix bugs in has_close_elements.\nGPT-4:\nThe function has_close_elements is supposed to check if there are any two elements in the vector that\nare closer than a given threshold. However, there are a few issues with the current implementation:\n1. The distance calculation is not correct. It should be the absolute difference between the two\nnumbers, not just the difference.\n2. The second loop should start from i+1, not 1. This is because we only need to compare each\nnumber with the numbers after it, not the numbers before it.\nHere is the corrected function:\nfn has_close_elements(numbers:Vec<f32>, threshold: f32) -> bool{\nfor i in 0..numbers.len(){\nfor j in i+1..numbers.len(){\nlet distance:f32 = (numbers[i] - numbers[j]).abs();\nif distance < threshold{\nreturn true;\n}\n}\n}\nreturn false;\n}\nThis function will now correctly check if there are any two elements in the vector that are closer than\nthe given threshold.\nFigure 31: Correct solution from GPT-4 to a HUMANEVALFIX Rust problem. GPT-4 correctly\nfixes the bug by adding the necessary \u2019abs\u2019 statement to compute the absolute value. It additionally\nimproves the efficiency of the function by starting the inner loop from \u2019i+1\u2019 instead of 1 like in the\noriginal solution. As some problems in HUMANEVALPACK are not perfectly efficient, evaluating\nthe ability of models to improve the efficiency of the solutions is another interesting benchmarking\ndirection (Yetistiren et al., 2022).\n50\nOctoPack: Instruction Tuning Code Large Language Models\nUser:\ndef car_race_collision(n: int):\nreturn n**3\ndef check(car_race_collision):\nassert car_race_collision(2) == 4\nassert car_race_collision(3) == 9\nassert car_race_collision(4) == 16\nassert car_race_collision(8) == 64\nassert car_race_collision(10) == 100\ncheck(car_race_collision)\nFix bugs in car_race_collision.\nGPT-4:\nThe function car_race_collision seems to be working fine as it is. It takes an integer \u2019n\u2019 as input and\nreturns the cube of \u2019n\u2019. There don\u2019t seem to be any bugs in this function.\nHowever, if the function is supposed to calculate something specific related to a car race\ncollision and it\u2019s not doing that correctly, we would need more information about the expected\nbehavior to fix it.\nFigure 32: Incorrect solution from GPT-4 to a HUMANEVALFIX Python problem. GPT-4 does\nnot catch that the test cases expect the function to square, not cube n. OCTOCODER correctly modifies\nthe 3 to 2 in 6 out of n = 20 generations.\n51\nOctoPack: Instruction Tuning Code Large Language Models\nUser:\ndef rounded_avg(n, m):\nif m < n:\nreturn -1\nsummation = 0\nfor i in range(n, m+1):\nsummation += i\nreturn bin(round(summation/(m - n)))\ndef check(rounded_avg):\n# Check some simple cases\nassert rounded_avg(1, 5) == \"0b11\"\nassert rounded_avg(7, 13) == \"0b1010\"\nassert rounded_avg(964,977) == \"0b1111001010\"\nassert rounded_avg(996,997) == \"0b1111100100\"\nassert rounded_avg(560,851) == \"0b1011000010\"\nassert rounded_avg(185,546) == \"0b101101110\"\nassert rounded_avg(362,496) == \"0b110101101\"\nassert rounded_avg(350,902) == \"0b1001110010\"\nassert rounded_avg(197,233) == \"0b11010111\"\n# Check some edge cases that are easy to work out by hand.\nassert rounded_avg(7, 5) == -1\nassert rounded_avg(5, 1) == -1\nassert rounded_avg(5, 5) == \"0b101\"\ncheck(rounded_avg)\nFix bugs in rounded_avg.\nGPT-4:\nThe function rounded_avg is supposed to calculate the average of all numbers between n and m\n(inclusive), round it, and then return the binary representation of the result.\nThere are several bugs in the function:\n1. The function should return -1 if m < n, but it should also return -1 if m == n because in this case,\nthe denominator of the average calculation would be 0, which would cause a division by zero error.\n2. The function calculates the average by summing all numbers between n and m and then dividing\nby (m - n). However, this is incorrect because the number of integers between n and m (inclusive) is\n(m - n + 1), not (m - n).\n3. The function uses the bin function to convert the result to binary. However, the bin function returns\na string that starts with \u20190b\u2019 to indicate that it is a binary number. If the function is supposed to return\nthe binary representation as an integer, it should remove this prefix.\nHere is the corrected function:\ndef rounded_avg(n, m):\nif m <= n:\nreturn -1\nsummation = 0\nfor i in range(n, m+1):\nsummation += i\naverage = round(summation / (m - n + 1))\nreturn int(bin(average)[2:])\nThis function now correctly calculates the average, rounds it, and returns the binary representation as\nan integer. If n is greater than or equal to m, it returns -1.\nFigure 33: Incorrect solution from GPT-4 to a HUMANEVALFIX Python problem. The bug\nconsists of a \"+1\" missing after \"m-n\". GPT-4 correctly fixes that, however, it introduces two new\nbugs: Retuning \"-1\" when m=n and removing the \"0b\", both of which evidently lead to the provided\nunit tests failing.\n52\nOctoPack: Instruction Tuning Code Large Language Models\nR.3\nWIZARDCODER\nInput:\nBelow is an instruction that describes a task. Write a response that appropriately completes the\nrequest.\n### Instruction: Fix bugs in bf.\ndef bf(planet1, planet2):\nplanet_names = (\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupyter\", \"Saturn\", \"Uranus\", \"\nNeptune\")\nif planet1 not in planet_names or planet2 not in planet_names or planet1 == planet2:\nreturn ()\nplanet1_index = planet_names.index(planet1)\nplanet2_index = planet_names.index(planet2)\nif planet1_index < planet2_index:\nreturn (planet_names[planet1_index + 1: planet2_index])\nelse:\nreturn (planet_names[planet2_index + 1 : planet1_index])\ndef check(bf):\n# Check some simple cases\nassert bf(\"Jupiter\", \"Neptune\") == (\"Saturn\", \"Uranus\"), \"First test error: \" + str(len(bf\n(\"Jupiter\", \"Neptune\")))\nassert bf(\"Earth\", \"Mercury\") == (\"Venus\",), \"Second test error: \" + str(bf(\"Earth\", \"\nMercury\"))\nassert bf(\"Mercury\", \"Uranus\") == (\"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\"), \"Third\ntest error: \" + str(bf(\"Mercury\", \"Uranus\"))\nassert bf(\"Neptune\", \"Venus\") == (\"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\"), \"Fourth\ntest error: \" + str(bf(\"Neptune\", \"Venus\"))\n# Check some edge cases that are easy to work out by hand.\nassert bf(\"Earth\", \"Earth\") == ()\nassert bf(\"Mars\", \"Earth\") == ()\nassert bf(\"Jupiter\", \"Makemake\") == ()\ncheck(bf)\n### Response:\ndef bf(planet1, planet2):\nWizardCoder:\nplanet_names = (\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupyter\", \"Saturn\", \"Uranus\", \"\nNeptune\")\nif planet1 not in planet_names or planet2 not in planet_names or planet1 == planet2:\nreturn ()\nplanet1_index = planet_names.index(planet1)\nplanet2_index = planet_names.index(planet2)\nif planet1_index < planet2_index:\nreturn (planet_names[planet1_index + 1: planet2_index])\nelse:\nreturn (planet_names[planet2_index + 1 : planet1_index])\nFigure 34: Incorrect solution from WizardCoder to a HUMANEVALFIX Python problem. The\nbug consists of \"Jupiter\" being misspelled as \"Jupyter\" in the function. WizardCoder returns the same\nfunction without making a single change leaving the bug. This is a particularly difficult bug that only\nGPT-4 and OCTOCODER (only for 1 out of the n=20 generations) are able to solve.\n53\nOctoPack: Instruction Tuning Code Large Language Models\nR.4\nBLOOMZ\nInput:\nconst generateIntegers = (a, b) => {\nif (a > b) {\nlet tmp = a;\na = b;\nb = tmp;\n}\nlet y = []\nfor (let i = a; i <= b; i++) {\nif (i == 2 || i == 4 || i == 6 || i == 8) { y.push(i) }\n}\nreturn y\n}\nProvide a concise natural language description of the code using at most 218 characters.\nBLOOMZ:\nGenerate a list of integers from a to b.\nInput (without previous input):\nGenerate a list of integers from a to b.\nWrite functional code in JavaScript according to the description.\nconst generateIntegers = (a, b) => {\nBLOOMZ:\nlet integers = [];\nfor (let i = a; i <= b; i++) {\nintegers.push(i);\n}\nreturn integers;\n}\nFigure 35: Incorrect solution from BLOOMZ to a HUMANEVALEXPLAIN JavaScript problem.\nBLOOMZ misses in its description that the function generates only the even digits between a and\nb, in ascending order. Thus, its synthesized function based on its own description simply collects\nall integers between a and b. BLOOMZ likely left out this part due to its bias for short and concise\nanswers. However, this bias backfires for this task.\nR.5\nSTARCODER\n54\nOctoPack: Instruction Tuning Code Large Language Models\nInput:\nimport java.util.*;\nimport java.lang.*;\nclass Solution {\npublic Map<String, Integer> histogram(String test) {\nMap<String, Integer> dict1 = new HashMap<>();\nList<String> list1 = Arrays.asList(test.split(\" \" ));\nint t = 0;\nfor (String i : list1) {\nif (Collections.frequency(list1, i) > t && !i.isEmpty()) {\nt = Collections.frequency(list1, i);\n}\n}\nif (t > 0) {\nfor (String i : list1) {\nif (Collections.frequency(list1, i) == t) {\ndict1.put(i, t);\n}\n}\n}\nreturn dict1;\n}\n}\nProvide a concise natural language description of the code using at most 423 characters.\nStarCoder:\npublic class Solution {\npublic Map<String, Integer> histogram(String test) {\nint t = 0;\n}\n}\nif (t > 0) {\nWrite functional code in Java according to the description.\nimport java.util.*;\nimport java.lang.*;\nclass Solution {\npublic Map<String, Integer> histogram(String test) {\nStarCoder:\nMap<String, Integer> map = new HashMap<String, Integer>();\nint t = 0;\nfor (int i = 0; i < test.length(); i++) {\nif (test.charAt(i) == \u2019t\u2019) {\nt++;\n}\n}\nmap.put(\"t\", t);\nreturn map;\n}\n}\nFigure 36: Incorrect solution from StarCoder to a HUMANEVALEXPLAIN Java problem. Star-\nCoder is not capable of generating natural language descriptions and instead copies the solution.\nHowever, our post-processing removes substrings from the solution (\u00a73) leading to a useless descrip-\ntion. Given this description it generates an irrelevant function. As StarCoder cannot generate natural\nlanguage explanations, we disqualify it from HUMANEVALEXPLAIN setting its score to 0 in Table 2.\n55\nOctoPack: Instruction Tuning Code Large Language Models\nR.6\nINSTRUCTCODET5+\nInput:\nBelow is an instruction that describes a task. Write a response that appropriately completes the\nrequest.\n### Instruction: Fix bugs in has_close_elements.\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = elem - elem2\nif distance < threshold:\nreturn True\nreturn False\ndef check(has_close_elements):\nassert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\nassert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\nassert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\nassert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\nassert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\nassert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\nassert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\ncheck(has_close_elements)\n### Response:from typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\nInstructCodeT5+:\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = elem - elem2\nif distance < threshold:\nreturn True\nreturn False\nFigure 37: Incorrect solution from InstructCodeT5+ to a HUMANEVALFIX Python problem.\nInstructCodeT5+ simply repeated the function without adding the missing \"abs\" statement to turn the\ndistance into an absolute value. This is one of the simplest bugs that most other models are able to fix.\nFor InstructCodeT5+ the prompt starts directly after \"Response:\" without a newline (Figure 20).\nR.7\nSTARCHAT-\u03b2\n56\nOctoPack: Instruction Tuning Code Large Language Models\nInput:\n#include<stdio.h>\n#include<math.h>\n#include<vector>\n#include<string>\n#include<algorithm>\nusing namespace std;\n#include<stdlib.h>\nint specialFilter(vector<int> nums){\nint num=0;\nfor (int i=0;i<nums.size();i++)\nif (nums[i]>10)\n{\nstring w=to_string(nums[i]);\nif (w[0]%2==1 and w[w.length()-1]%2==1) num+=1;\n}\nreturn num;\n}\nProvide a concise natural language description of the code using at most 296 characters.\nStarChat-\u03b2:\nThis code is a solution to the \"Special Array\" problem on LeetCode. It uses C++ and includes several\nstandard libraries, including vector, string, and algorithm.\nThe code starts by declaring some variables and importing the necessary libraries. It then defines a\nspecialFilter function that takes\nInput (without previous input):\nThis code is a solution to the \"Special Array\" problem on LeetCode. It uses C++ and includes several\nstandard libraries, including vector, string, and algorithm.\nThe code starts by declaring some variables and importing the necessary libraries. It then defines a\nspecialFilter function that takes\nWrite functional code in C++ according to the description.\n#include<stdio.h>\n#include<math.h>\n#include<vector>\n#include<string>\n#include<algorithm>\nusing namespace std;\n#include<stdlib.h>\nint specialFilter(vector<int> nums){\nStarChat-\u03b2:\nint n = nums.size();\nint sum = 0;\nfor(int i = 0; i < n; i++){\nsum += pow(nums[i], 2);\n}\nint target = (int)sqrt(sum);\nvector<int> res;\nfor(int i = 0; i < n; i++){\nif(nums[i] > target){\nres.push_back(nums[i]);\n}\n}\nreturn res;\n}\nFigure 38: Incorrect solution from StarChat-\u03b2 to a HUMANEVALEXPLAIN C++ problem.\nStarChat-\u03b2 has a bias for excessively long answers. StarChat-\u03b2\u2019s description includes irrelevant parts\nlike \"..problem on LeetCode..includes several standard libraries..\" (despite the problem not existing\non LeetCode). As the description exceeds the 296-character limit for the code, it is cut off at the end.\n57\nOctoPack: Instruction Tuning Code Large Language Models\nR.8\nDIFF CODEGEN\nInput:\n<NME> truncate_number.py\n<BEF> def truncate_number(number: float) -> float:\n\"\"\" Given a positive floating point number, it can be decomposed into\nand integer part (largest integer smaller than given number) and decimals\n(leftover part always smaller than 1).\nReturn the decimal part of the number.\n>>> truncate_number(3.5)\n0.5\n\"\"\"\nreturn number % 1.0 + 1.0\n<MSG> Fix bugs in truncate_number.\n<DFF>\nDiff Codegen 2B\n@@ -4,7 +4,7 @@ def truncate_number(number: float) -> float:\nand integer part (largest integer smaller than given number) and decimals\n(leftover part always smaller than 1).\n-\nReturn the decimal part of the number.\n+\nReturn\nFigure 39: Incorrect solution from Diff Codegen 2B to a HUMANEVALFIXDOCS Python problem.\nDiff Codegen 2B suggests an irrelevant diff modifying parts of the docstring. The model commonly\noutputs diffs that modify the docstring or an import statement and rarely addresses the actual bug.\nS\nLIMITATIONS AND FUTURE WORK\nModel Execution\nA promising avenue for improving performance on HUMANEVALFIX is letting\nthe model execute the given code or its own generated code and inspect its output (Chen et al., 2022;\n2023c; Yasunaga & Liang, 2021; Li et al., 2022a; Gao et al., 2023; Dong et al., 2023; Zhang et al.,\n2023c; Madaan et al., 2023b; Ni et al., 2023; Gou et al., 2023; Hu et al., 2023; Taylor et al., 2022;\nNye et al., 2021). This could allow the model to discover which unit tests are failing and for what\nreason. The model could then simply iterate on the function until all unit tests are passing. We leave\nexplorations of this strategy to improve performance on HUMANEVALPACK to future work.\nMulti-file changes\nFor the creation of COMMITPACK, we have filtered out any commits that affect\nmultiple files to ensure commits are very specific and account for the fact that most current models\nare only capable of operating on a single file. Allowing models to take multiple files as input and\nmodify multiple files given a single instruction is a promising direction for future work. There is\nactive research on using repository-level context (Ding et al., 2022; Shrivastava et al., 2023a;b; Zhang\net al., 2023a; Liu et al., 2023d) and the necessary long context windows (Dai et al., 2019; Press et al.,\n2021; Sun et al., 2021; Dao et al., 2022; Peng et al., 2023; Liu et al., 2023c; Chen et al., 2023b).\nLength-awareness\nCurrent Code LLMs including OCTOCODER struggle with awareness about the\nlength of their generated output. For HUMANEVALEXPLAIN, we instruct the models to limit their\noutput to a given number of characters. While it is trivial for humans to count characters and adhere\nto the limit, all models tested frequently generate far too many characters. Prior work has shown that\nhuman raters are biased towards preferring longer texts (Wu & Aji, 2023) regardless of content. All\nmodels evaluated are instruction tuned on text that was at least indirectly assessed by human raters,\nhence they may be biased towards generating longer texts even if it means including literary bloat.\nBetter evaluation\nEvaluating code instruction models is challenging for several reasons:\n(1) Prompting: The prompt can significantly impact the performance of large language mod-\n58\nOctoPack: Instruction Tuning Code Large Language Models\nels (Brown et al., 2020; Zhou et al., 2022; Muennighoff, 2022; Babe et al., 2023). To ensure fair\nevaluation we use the prompting format put forth by the respective authors of the models and a simple\nintuitive prompt for models without a canonical prompt (see Appendix Q). However, this may put\nmodels without a canonical prompt recommendation (e.g. BLOOMZ, GPT-4) at a slight disadvantage.\nOCTOCODER and OCTOGEEX perform best when prompted using the same format we use during\ntraining (Figure 18) and we recommend always using this format at inference.\n(2) Processing: Models may accidentally impair otherwise correct code by e.g. including a natural\nlanguage explanation in their output. We largely circumvent this issue through the use of strict\nstopping criteria and careful postprocessing (e.g. for GPT-4 we check if it has enclosed the code in\nbackticks, and if so, extract only the inner part of the backticks discarding its explanations).\n(3) Execution: When executing code to compute pass@k, it is important that the generated code\nmatches the installed programming language version. Models may inadvertently use expressions\nfrom a different version (e.g. they may use the Python 2 syntax of print \"hi\", which would fail\nin a Python 3 environment). In our evaluation, we did not find this to be a problem, however, as\nmodels become more capable, it may make sense to specify the version. Future prompts may include\nthe version (e.g. \u201cuse JDK 1.18.0\u201d) or provide models with an execution environment that has the\nexact version installed that will be used for evaluation.\n(4) Comprehensiveness: Executing code can only reflect functional correctness lacking a comprehen-\nsive understanding of quality. Compared to execution-based evaluation, the human judgment of code\nquality can be considered more comprehensive as humans can consider factors beyond correctness.\nDirectly hiring human annotators can be inefficient and expensive, and therefore researchers have\nexplored approaches to automate human-aligned evaluation via LLMs (Fu et al., 2023; Liu et al.,\n2023e; Zhuo, 2023). However, recent work (Wang et al., 2023b) suggests LLM-based evaluation\ncan be biased towards certain contexts. Future work on automating the human-aligned evaluation of\ninstruction tuned Code LLMs while avoiding such bias is needed.\nReward Models\nOur commit datasets, COMMITPACK and COMMITPACKFT, also lend themselves\nwell for learning human preferences (Ethayarajh et al., 2024; Rafailov et al., 2024). The changed\ncode after a commit generally represents a human-preferred version of the code (else the code would\nnot have been modified). Thus, one could train a reward model that given the code before and after\na commit, learns that the code afterward is better. Similar to prior work (Christiano et al., 2017;\nOuyang et al., 2022), this reward model could then be used to guide a language model to generate\ncode that is preferred by humans.\nT\nVERSION CONTROL\nV1 \u2192 V2:\n\u2022 Added Appendix O on HUMANEVALEXPLAIN metrics\n\u2022 Added Appendix N on Fill-in-the-Middle\n\u2022 Expanded the motivation for limiting the number of tokens in COMMITPACKFT in Ap-\npendix E\n\u2022 Fixed the StarCoder HUMANEVALFIXDOCS in Appendix K thanks to Abhijeet Awasthi\n\u2022 Specified the programming languages of CommitPackFT (CommitPack has 350 languages\nwhile CommitPackFT has 277 after filtering)\n\u2022 Made small writing improvements throughout\n59\nOctoPack: Instruction Tuning Code Large Language Models\nU\nOCTOBADPACK\nFigure 40: OCTOPACK (left) and her evil brother OCTOBADPACK (right).\n60\n"
  },
  {
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "link": "https://arxiv.org/pdf/2308.06873.pdf",
    "upvote": "24",
    "text": "1\nSpeechX: Neural Codec Language Model as a\nVersatile Speech Transformer\nXiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen,\nMin Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka\nAbstract\u2014Recent advancements in generative speech models\nbased on audio-text prompts have enabled remarkable innova-\ntions like high-quality zero-shot text-to-speech. However, existing\nmodels still face limitations in handling diverse audio-text speech\ngeneration tasks involving transforming input speech and pro-\ncessing audio captured in adverse acoustic conditions. This paper\nintroduces SpeechX, a versatile speech generation model capable\nof zero-shot TTS and various speech transformation tasks, deal-\ning with both clean and noisy signals. SpeechX combines neural\ncodec language modeling with multi-task learning using task-\ndependent prompting, enabling unified and extensible modeling\nand providing a consistent way for leveraging textual input\nin speech enhancement and transformation tasks. Experimental\nresults show SpeechX\u2019s efficacy in various tasks, including zero-\nshot TTS, noise suppression, target speaker extraction, speech\nremoval, and speech editing with or without background noise,\nachieving comparable or superior performance to specialized\nmodels across tasks. See https://aka.ms/speechx for demo sam-\nples.\nIndex Terms\u2014Speech generation, audio-text input, multi-\ntask learning, zero-shot text-to-speech, noise suppression, target\nspeaker extraction, speech editing, speech removal\nI. INTRODUCTION\nThe technology of generative models has undergone rapid\nand transformative advancements in various machine learning\napplications, encompassing text [1], [2], vision [3], and au-\ndio [4]. These advancements have had significant implications\nfor both the industry and society at large. Notably, generative\nmodels using multi-modal input have emerged as a remarkable\ninnovation [5], [6], [7], [8], [9], [10].\nIn the speech domain, one prominent speech generation\ntask that leverages audio-text input is zero-shot text-to-speech\n(TTS). Zero-shot TTS involves converting a given text into\nspeech with the voice characteristics and speaking style of\na desired talker by using only a brief audio sample of\nthat person. Early studies in zero-shot TTS employed fixed-\ndimensional speaker embeddings [11], [12], [13], [14]. This\napproach limited their usage to TTS alone and did not ade-\nquately support speaker cloning capabilities.\nIn contrast, recent approaches have embraced more generic\nformulations, such as masked speech prediction [15] or neural\ncodec language modeling [16], [17], [18], [19]. These novel\napproaches directly utilize the target speaker\u2019s audio without\ncompressing it into a fixed-dimensional representation. Conse-\nquently, these models have not only achieved remarkable zero-\nshot TTS performance but also demonstrated additional capa-\nbilities, including voice conversion [15], [18] and speech edit-\nThe authors are with Microsoft Corporation, Redmond, WA 98052, USA.\ning [15]. This enhanced flexibility holds tremendous promise\nfor unlocking new possibilities in speech generation models.\nHowever, despite their impressive achievements, these re-\ncent generative models still have certain limitations, partic-\nularly when it comes to addressing various audio-text-based\nspeech generation tasks involving transforming input speech.\nFor instance, existing speech editing models [20], [21] are\nrestricted to handling clean signals only, lacking the ability to\nmodify spoken content while preserving background sounds.\nAdditionally, to perform denoising, the model discusssed\nin [15] necessitates the noisy signal to be surrounded by\nclean speech segments, imposing significant constraints on\nits practical applications. In the context of transforming non-\nclean speech, another particularly useful task is target speaker\nextraction [22], [23], [24]. Target speaker extraction involves\nextracting the voice of a desired speaker from a speech\nmixture containing multiple talkers. The desired speaker can\nbe specified using a short voice recording of that individual.\nDespite its potential significance as discussed in [25], this task\nremains unaddressed by existing generative speech models.\nIt is noteworthy that traditional approaches to speech en-\nhancement tasks, such as denoising and target speaker ex-\ntraction, have relied on regression models for faithful signal\nrecovery. However, these prior methods typically required\ndistinct expert models for each task, which is not ideal,\ngiven the potential diversity of acoustic disturbances [26].\nFurthermore, there has been a lack of comprehensive audio-\ntext-based speech enhancement models that leverage reference\ntranscriptions to generate intelligible speech, except for lim-\nited studies focusing only on particular speech enhancement\ntasks [27], [28].\nGiven the aforementioned considerations and the successful\nprecedents in other domains, the creation of audio-text-based\ngenerative speech models unifying generation and transforma-\ntion capabilities assumes crucial research importance. These\nmodels should possess an overarching capability to tackle a\ndiverse array of speech generation tasks. We propose that such\nmodels should be equipped with the following key properties:\n\u2022 Versatility: Similar to unified or foundation models de-\nveloped in other machine learning domains, the unified\naudio-text-based generative speech models must han-\ndle a wide range of tasks involving speech generation\nfrom audio and text inputs. These tasks should encom-\npass not only zero-shot TTS but also various forms\nof speech transformation, including speech enhancement\nand speech editing, to name a few.\narXiv:2308.06873v1  [eess.AS]  14 Aug 2023\n2\nFig. 1. Overview of SpeechX. SpeechX handles multiple audio-text-based speech generation tasks, including noise suppression, speech removal, target speaker\nextraction, zero-shot TTS, clean speech editing, and noisy speech editing, by using neural codec language model conditioned on the text and acoustic token\nstream. Text input is optional for some tasks.\n\u2022 Robustness: It is essential for the unified models to\nexhibit robustness to various acoustic distortions since\nthey are likely to be applied in acoustically challenging\nenvironments. By ensuring reliable performance, these\nmodels can be deemed highly usable in real-world sce-\nnarios where background sounds are prevalent.\n\u2022 Extensibility: The unified models must employ flexible\narchitectures, allowing for seamless extensions of task\nsupport. One approach to achieving this involves accom-\nmodating additional elements, such as input tokens or\nextra modules. Such flexibility will empower the models\nto adapt to future speech generation tasks efficiently.\nIn pursuit of this objective, this paper introduces a versatile\nspeech generation model capable of performing multiple tasks,\nincluding zero-shot TTS, noise suppression using an optional\ntranscript input, speech removal, target speaker extraction\nusing an optional transcript input, and speech editing for both\nquiet and noisy acoustic environments (Fig. 1). We refer to\nour proposed model as SpeechX1. As with VALL-E, SpeechX\nadopts a language modeling approach that generates codes of\na neural codec model, or acoustic tokens, based on textual and\nacoustic inputs. To enable the handling of diverse tasks, we\nincorporate additional tokens in a multi-task learning setup,\nwhere the tokens collectively specify the task to be executed.\nExperimental results, using 60K hours of speech data from\nLibriLight [29] as a training set, demonstrate the efficacy\nof SpeechX, showcasing comparable or superior performance\ncompared to expert models in all the aforementioned tasks.\nNotably, SpeechX also exhibits novel or expanded capabilities,\nsuch as preserving background sounds during speech editing\nand leveraging reference transcriptions for noise suppression\nand target speaker extraction. Audio samples showcasing the\ncapabilities of our proposed SpeechX model are available at\nhttps://aka.ms/speechx.\n1X stands for transformation to highlight that our model performs various\nspeech transformation tasks in addition to zero-shot TTS.\nII. RELATED WORK\nA. Autoregressive generative models\nGenerative models based on a language modeling approach\nusing autoregressive Transformers, also known as decoder-\nonly Transformers, have garnered significant success in various\napplication domains. Notable examples of such models include\nthe GPT series [1], [2] and DALL-E [30]. The autoregressive\napproach has also been extended to the audio and speech\ndomains. AudioLM [4] and MusicLM [10] are pioneering\nefforts that exploit multiple types of tokens, each with a\ndistinct time scale and degree of semantic granularity, allowing\nfor hierarchical token generation. This hierarchical structure,\ncomprising both coarse and fine-grained tokens, enables the\nsynthesis of sounds with both nuanced details and long-term\nregularities.\nFor zero-shot TTS, VALL-E [16] and SPEAR-TTS [17] em-\nploy the autoregressive Transformers by representing textual\n(semantic) and acoustic tokens as a single data stream. This\napproach enables the models to perform zero-shot speaker\nadaptation, facilitating the generation of TTS voices that\nmimic a specific person\u2019s voice. It was demonstrated that\nthese models could perform zero-shot TTS from speech clips\nas short as three seconds. A notable advantage of these\nautoregressive speech generation models is their ability to\nperform TTS without requiring a separate duration model. This\nstreamlined architecture simplifies the training process and po-\ntentially offers increased flexibility needed to subsume various\nspeech generation tasks. For this reason, we opt to build our\nSpeechX models by using autoregressive Transformers.\nB. Multi-task generative speech models\nSeveral papers have recently reported efforts in developing\naudio-text-based speech generation models that support zero-\nshot TTS and several related tasks. These tasks include voice\nor style conversion (Make-A-Voice [18], NaturalSpeech2 [31],\n3\nand Voicebox [15]), speech editing (Mega-TTS [21] and Voice-\nbox), and denoising (NaturalSpeech2 and Voicebox). Voice-\nbox has showcased noteworthy advancements by facilitating\na multitude of tasks through its masked speech prediction\nprinciple. Nevertheless, its capabilities are still limited to clean\nspeech generation alone, falling short of effectively dealing\nwith noisy speech or encompassing conventional audio en-\nhancement tasks such as noise suppression and target speaker\nextraction.\nIn this study, we deal with both clean and noisy speech and\nunify the generation and transformation tasks. To accomplish\nthis, we extend VALL-E by performing multi-task learning\nwith task-dependent prompts. The resulting model, which we\ncall SpeechX, exhibits versatility in various speech processing\ntasks. The model excels not only in speech generation tasks\nlike zero-shot TTS and speech editing but also performs\neffectively in enhancement tasks such as noise suppression\nand target speaker extraction. It also realizes novel capabilities,\nsuch as editing spoken content while retaining the background\nnoise or effectively leveraging transcriptions for enhancement\ntasks.\nIII. METHOD\nA. Overview\nFig. 1 illustrates an overview of the SpeechX architecture.\nBuilding upon the principles introduced in VALL-E, SpeechX\nemploys a neural codec language model based on Transform-\ners. The model learns to perform conditional generation of\na neural code sequence, denoted as O, based on two input\nprompts: textual prompt T and acoustic prompt A. The neural\ncodes may also be referred to as acoustic tokens.\nThe textual prompt T is a sequence of phonemes obtained\nby applying grapheme-to-phoneme conversion2 to an input\ntext. The textual prompt conveys the semantic information,\nand thus it is called semantic tokens. Conversely, the acoustic\nprompt A encapsulates the acoustic information of an input\nspeech signal. It is obtained by converting the input audio into\na sequence of acoustic tokens with an encoder of the neural\ncodec model. Furthermore, to specify the task to be executed,\nor equivalently the desired output, we incorporate additional\ntokens in the acoustic prompt. The details will be explained\nin Section III-C. The output O is a sequence of neural codes\nof the desired signal, which is then translated into a waveform\nsignal with the codec decoder.\nWe use EnCodec [32] as the neural codec model, following\nthe prior work. Encodec is based on an encoder-decoder\narchitecture with L quantization layers. In our experiments,\nwe use L = 8 to be consistent with the configuration of [16].\nEach layer of the EnCodec model produces discrete codes\nconsisting of 1024 entries at a sampling rate of 75 Hz.\nWe emphasize that the proposed simple architecture cap-\nitalizes on the end-to-end modeling capability of the neural\nlanguage modeling approach. In contrast to other zero-shot\nTTS or speech generation methods, this approach eliminates\nthe need for a separate model, such as a speaker embedding\n2https://github.com/Kyubyong/g2p\nmodel or a duration model, apart from the neural codec model.\nThis key property allows SpeechX to acquire knowledge of\ndiverse tasks with varying requirements and input-output rela-\ntionships, thereby facilitating a versatile and highly extensible\nspeech generation process.\nB. Neural codec language model\nAs with VALL-E [16], SpeechX makes use of auto-\nregressive (AR) and non-auto-regressive (NAR) Transformer\nmodels. Specifically, the AR model is used to output the neural\ncodes corresponding to the first quantization layer of EnCodec.\nOn the other hand, the NAR model generates the neural codes\nof all the layers above the first layer, namely the second\nthrough eighth layers. Combining the AR and NAR models\nprovides a reasonable trade-off between generation flexibility\nand inference speed, as discussed in [16].\nLet output O be specifically represented as matrix O =\n[ot,l] \u2208 NT \u00d7L, where ot,l represents the code for the l-th\ncodec layer at time frame t and it can take one of the 1024\nvalues. The output sequence length is denoted by T. The AR\nmodel comprises a stack of Transformer decoder layers [33]\nand is optimized by minimizing the negative log-likelihood of\nthe first layer code of the desired output, which is defined as\nfollows:\nLAR = \u2212\nT\nX\nt=1\nlog P(ot,1|T , A, o<t,1; \u03b8AR),\n(1)\nwhere o<t,1 = [o1,1, \u00b7 \u00b7 \u00b7 , ot\u22121,1], while \u03b8AR represents the\nAR Transformer model parameters. Different embedding pro-\njections are applied to the textual and acoustic tokens, and\nthey are superimposed by sinusoidal positional embeddings.\nNote that the AR model in SpeechX is conditioned on the\nconcatenated embeddings of both the acoustic and textual\nprompts. This formulation differs from that of VALL-E, where\nthe AR model is only conditioned on the textual prompt and\nthe past acoustic history.\nAfter obtaining the first layer codes with the AR model, the\nNAR model is used to generate the lth layer codes based on the\ntext and acoustic prompts as well as the output codes for the\nfirst l\u22121 layers, which have already been produced. The model\nis used repeatedly for l = 2, \u00b7 \u00b7 \u00b7 , 8. Since we use the same\nNAR model for the remaining seven layers, the NAR model\nis trained to minimize the following negative log-likelihood\nfunction:\nLNAR = \u2212\n8\nX\nl=2\nlog P(o:,l|T , A, o:,<l; \u03b8NAR),\n(2)\nwhere \u03b8NAR represents the NAR model parameters, while o:,l\ndenotes the entire sequence of ot,l for the lth layer, and o:,<l =\n[o:,1, \u00b7 \u00b7 \u00b7 , o:,l\u22121]. In this formulation, in order for the single\nNAR model to process each of the seven layers, the acoustic\ntokens from the first to (l \u2212 1)th layers, o:,<l, are embedded\nand summed up.\nC. Task-based prompting\nSpeechX aims to handle multiple tasks with one model. To\nthis end, we adopt task-based prompting, as illustrated in Table\nI and explained in detail below.\n4\nTABLE I\nTASK-BASED PROMPTING: PROMPTS AND DESIRED OUTPUT FOR INDIVIDUAL TASKS. G2P(\u00b7) DENOTES GRAPHEME-TO-PHONEME CONVERSION.\nTask\nTextual prompt T\nAcoustic prompt A\nDesired output O\nNoise suppression\nG2P(text) / null\n<ns>, C(s + n)\nC(s)\nSpeech removal\nG2P(text) / null\n<sr>, C(s + n)\nC(n)\nTarget speaker extraction\nG2P(text) / null\nC(s\u2032\n1), <tse>, C(s1 + s2)\nC(s1)\nZero-shot TTS\nG2P(text)\nC(s)\nC(s\u2032)\nClean speech editing\nG2P(text)\nC(spre), <soe>, <mask>, <eoe>, C(spost)\nC(spre), C(sedit), C(spost)\nNoisy speech editing\nG2P(text)\nC(spre + npre), <soe>, C(smid + nmid), <eoe>, C(spost + npost)\nC(spre + npre), C(sedit + nmid), C(spost + npost)\nNoise suppression is a task of extracting clean speech\nsignal s from its noise-corrupted observation s + n, where\nn denotes the noise. For the noise suppression task, we\nincorporate a special token, denoted as <ns>, to form the\nacoustic prompt, resulting in A = [<ns>, C(s + n)]. Here,\nC(\u00b7) denotes the function used to convert an audio signal\ninto a neural codec token sequence. While the textual prompt\nT\nis supposed to be provided by a user as a reference\ntranscription, we let the use of the textual prompt be optional\nto accommodate the scenario where the human transcription is\nunavailable. The desired output is the acoustic token sequence\nof the clean audio, C(s).\nSpeech removal involves removing speech from a noisy\nspeech signal while preserving the background noise. It is\nuseful for removing only unwanted speech from recordings.\nTo address this task, we employ a special token, <sr>, to\nconstruct the acoustic prompt as A = [<sr>, C(s + n)]. The\ndesired output is the acoustic token sequence of the noise\nsignal, C(n). As in the case of noise suppression, the textual\nprompt can be omitted.\nTarget speaker extraction aims at isolating clean speech\ns1 of a target speaker from a mixture of s1 and interfering\nspeech s2 from a secondary speaker. The target speaker is\nidentified through a short enrollment audio s\u2032\n1 of that indi-\nvidual, where we assumed three seconds for the enrollment.\nFor this task, we form the acoustic prompt by concatenating\nthe acoustic tokens extracted from the enrollment audio,\nC(s\u2032\n1), and those of the mixed speech, C(s1 + s2), with a\ntask-specifying token, denoted as <tse>. That is, we have\nA = [C(s\u2032\n1), <tse>, C(s1+s2)]. The desired output is C(s1).\nAs with the previous tasks, the inclusion of the textual prompt\nis optional.\nZero-shot TTS aims to generate a speech signal s\u2032 by lever-\naging both the provided input text and an enrollment speech\ns. The goal is to ensure that the speech characteristics of s\u2032\nclosely resemble those of s, while also accurately reflecting\nthe input text. For this task, we employ the acoustic tokens\nextracted from the enrollment audio, denoted as C(s), as the\nacoustic prompt. The model generates acoustic tokens for the\nsynthesized speech, C(s\u2032), based on the input text. These\nacoustic tokens are then converted into the corresponding\nwaveform.\nClean speech editing is defined as modifying a segment\nof input speech to align with an input text. Let s denote\nthe input speech signal to be edited. We divide s into three\ndistinct portions, spre, smid, and spost, with smid being\nthe target segment for editing, without loss of generality\n(spre and spost can be empty). We construct the acous-\ntic prompt as [C(spre), <soe>, <mask>, <eoe>, C(spost)],\nwhere new tokens <soe>, <mask>, <eoe> are introduced\nto specify the task and the speech segment designated for\nediting. The desired output is a sequence of neural codes,\n[C(spre), C(sedit), C(spost)], where the spoken content of\n[spre, sedit, spost] matches the input text. The speaker char-\nacteristics of sedit must be consistent with those of spre and\nspost.\nNoisy speech editing, in contrast, operates on noisy speech\nas input, aiming to modify the speech content within a\nsegment while keeping the underlying background noise in-\ntact. Therefore, this task would be more challenging than\nthe clean speech editing task because the model needs to\ndistinguish between speech and noise during the editing pro-\ncess. To accomplish this objective, it is crucial to provide\nthe model with the complete input speech signal instead of\nmasking out the segment for editing with <mask> token.\nTherefore, we construct the acoustic prompt as [C(spre +\nnpre), <soe>, C(smid+nmid), <eoe>, C(spost+npost)], with\nthe subscripts corresponding to pre, mid, or post as previously\ndefined. The desired output comprises a sequence of neural\ncodes, [C(spre+npre), C(sedit+nmid), C(spost+npost)]. This\nformulation makes it clear that the model must transform smid\ninto sedit based on the text input while retaining nmid.\nIn practical speech editing scenarios, the input text is\noften obtained by first applying automatic speech recognition\n(ASR) to the input speech and then having a user edit the\ntranscription. In such situations, it is simple to identify the\npositions at which <soe> and <eoe> must be inserted. Also,\nit is noteworthy that, in clean speech editing, the use of\n<mask> allows the model to adaptively change the output\nspeech length in such a way that the output speech sounds\nnatural in terms of speaking speed.\nThe outlined task-based prompting strategy equips the\nSpeechX model with the ability to uniquely decide the desired\noutput during inference. This approach enables flexibility\nfor incorporating additional tasks. Adding new tasks entails\nintegrating corresponding prompting schemes and continu-\ning model training from an existing checkpoint, where only\nembeddings for newly introduced task-specific tokens are\nrandomly initialized. This can be performed without changing\nthe underlying model architecture.\nD. Model training\nDuring training, we randomly sample the task for each\nmodel update at an equal probability. This is intended to ensure\nthe model does not unduly favor any particular tasks. For noise\nsuppression, speech removal, and target speaker extraction\n5\ntasks, we include the textual prompt at a 50% probability\nso that the model equally experiences both text and text-less\nscenarios.\nTo help model to acquire basic generation capabilities, we\nfirst train the model only for zero-shot TTS and then continue\nthe training process using all the tasks to perform multi-task\nlearning. In other words, we initialize the model with an\nexisting VALL-E model checkpoint. Precisely speaking, the\nSpeechX model trained solely for zero-shot TTS exhibits slight\ndivergence from VALL-E. This difference arises from the fact\nthat the former explicitly incorporates a distinct enrollment\naudio, originating from the same speaker, for each training\nsample, while the latter does not. Nevertheless, for the sake of\nsimplicity, we refer to this initialization approach as VALL-\nE initialization. When starting the multi-task training stage,\nrandomly initialized embeddings are appended for the special\ntokens related to the task-dependent prompts. This two-stage\ntraining strategy substantially enhances performance across all\ntasks, as evidenced by our experimental results.\nIV. EVALUATION SETUPS\nEvaluating versatile speech generation models like SpeechX\nrequires performing an array of tests, each focusing on individ-\nual tasks. To keep the experiments manageable as well as en-\nsure consistency across the tasks, we used evaluation datasets\nthat were derived from the test-clean split of LibriSpeech\nfor all evaluations. In this section, we provide the details\nof our evaluation setups. Following previously established\npractices [15], [16], we selected the test samples with durations\nbetween 4 and 10 seconds.\nA. Evaluation data\nZero-shot TTS: For each test sample, we used the reference\ntranscription to create the textual prompt. The acoustic prompt\nwas generated by randomly choosing another utterance of the\nsame speaker and extracting a 3-second-long clip.\nNoise suppression: We mixed each test sample with a noise\nsample randomly picked from the MUSAN dataset [34] at a\nsignal-to-noise ratio (SNR) which was randomly determined\nfrom the range between 0 dB and 20 dB. The task was to\nrecover the uncorrupted speech from the noisy speech. The\nacoustic prompt was obtained by applying EnCodec to the\nnoisy signal. As regards the textual prompt, we considered\nboth text-less (i.e., using no semantic prompt) and text-guided\nnoise suppression, where we used the reference transcription\nfor the text-guided setting.\nTarget speaker extraction: We mixed each test sam-\nple with an utterance of a different speaker at a signal-to-\ninterference ratio (SIR) which was randomly determined from\nthe range between 0 dB and 20 dB. Also, we randomly chose\none or more other utterances of the same speaker to create\na 3-second-long enrollment clip to help models identify who\nthe desired speaker is. Both the mixed and enrollment signals\nwere used to derive the acoustic prompt as described in Section\nIII-C. The task was to recover the original uncorrupted speech\nof the target speaker. As with the noise suppression task, we\nconsidered both text-less and text-guided settings.\nClean speech editing: For each test sample, we randomly\nselected a period of length between 10% and 50% of the\nwhole utterance. We replaced the speech of the selected period\nwith another randomly chosen speech sample of the same\nspeaker. Given the partially replaced, speaker homogeneous\nspeech and the reference transcription, the task was to generate\na speech signal that follows the transcription without changing\nthe speaker characteristics and the unreplaced portion of the\ninput signal. In our experiments, we used the correct <soe>\nand <eoe> locations based on the knowledge of the replaced\nsegment.\nNoisy speech editing: We added a randomly picked MU-\nSAN noise sample to each test sample of the clean speech\nediting task. The SNR was chosen from the range of 0 dB to\n20 dB. Given the noise-corrupted partially replaced speech and\nthe reference transcription, the task was to generate a noisy\nspeech signal that follows the transcription without changing\nthe background noise, the speaker characteristics, and the\nunreplaced portion of the input speech.\nSpeech removal: The same dataset was used as the one\nused for noise suppression. Given a noisy speech signal, the\ntask was to extract the noise signal by removing the speech.\nWe considered only the textless case. Consequently, the input\nexclusively comprised the acoustic prompt corresponding to\nthe noisy speech.\nB. Metrics\nFor consistency and reproducibility, we opted to use objec-\ntive metrics for individual tasks as described below.\nWord error rate (WER): We employed the WER as a met-\nric to evaluate the fidelity of the generated audio in adhering to\nthe provided transcription. The ASR system utilized for our\nexperiments was NeMo\u2019s stt en conformer transducer large\nmodel3, which is based on the Conformer Transducer archi-\ntecture [35]. We selected this particular ASR model based on\nits superior stability and robustness against noise and process-\ning artifacts in comparison to other publicly available ASR\nmodels, as was observed during our preliminary experiments.\nRobustness in ASR is particularly crucial for tasks such as\nnoise suppression and noisy speech editing. The WER metric\nwas employed across all tasks, with the exception of speech\nremoval.\nSpeaker similarity score (SIM): The speaker similarity\nscore served as a metric to assess the coherence of the\ngenerated speech in relation to the speaker\u2019s characteristics.\nThis score was calculated as the cosine similarity between the\nspeaker embeddings of the generated speech and the desired\nspeech signals. The computation of speaker embeddings was\nperformed using NeMo\u2019s TitaNet-Large4. We employed the\noriginal audio data instead of utilizing an EnCodec-processed\nsignal for the speaker similarity measurement to capture and\n3https://huggingface.co/nvidia/stt en conformer transducer xlarge\n4https://huggingface.co/nvidia/speakerverification en titanet large\n6\nreflect any potential speech deformation effects that may arise\ndue to the use of the EnCodec model. SIM was used in zero-\nshot TTS, clean speech editing, and noisy speech editing.\nDNSMOS: For evaluation in the noise suppression and\ntarget speaker extraction tasks, we utilized DNSMOS [36],\na well-established model-based metric for predicting the per-\nceived quality of acoustically corrupted speech5. Specifically,\nwe employed the OVRL score from the DNSMOS P.835\nmodel. To evaluate the performance of target speaker extrac-\ntion, we employed a personalized DNSMOS model, which was\ntailored for this particular task and is available on the same\nwebpage.\nPerceptual Evaluation of Speech Quality (PESQ): For\nthe noise suppression and target speaker extraction tasks,\nwe also utilized PESQ [37]. Unlike DNSMOS, PESQ is an\nintrusive metric that necessitates the clean reference signals.\nConsequently, PESQ is expected to assess the fidelity of the\ngenerated audio with respect to the original clean data.\nMel-cepstral distortion (MCD): MCD6 is a metric used\nto quantify the dissimilarity between two sequences of mel\ncepstra. We employed this metric to objectively measure the\nspeech removal accuracy by comparing the estimated noise\nwith the ground truth noise audio.\nV. EXPERIMENTS\nA. Training data\nWe sourced clean speech data from LibriLight, comprising\n60 thousand hours of untranscribed English reading speech\nfrom over 7,000 speakers [29], as was performed in the zero-\nshot TTS experiment using VALL-E [16]. To meet the specific\ntraining requirements for each task, data simulation was per-\nformed by following the methods employed for creating the\nevaluation data, as elaborated below. Note that, as discussed\nin Section III-D, we formed individual training mini-batches\nbased on randomly selected tasks for each iteration.\nFor the noise suppression and speech removal tasks, we\nmixed the clean speech with noise samples from the DNS\nchallenge corpus [38] at SNRs between -5 dB and 20 dB.\nOur models were trained to recover the acoustic tokens of\nthe clean speech and noise for noise suppression and speech\nremoval, respectively. For the target speaker extraction task,\nwe mixed the individual clean speech samples with those of\nother randomly chosen speakers with SIRs ranging from -\n5 dB to 20 dB. As regards clean speech editing, for each\nclean utterance, we randomly selected a subsegment of length\nranging from 10% to 70%, and then substituted it with\nanother audio segment from the same speaker with different\ncontent. We saved the start and end times of the replaced\nsegment, which were used to insert the <soe> and <eoe>\ntokens to the correct positions in the acoustic prompt during\ntraining. Furthermore, to create training samples for noisy\nspeech editing, we added noise samples used in the noise\nsuppression task to the partially replaced clean audio. As a\n5https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS\n6https://pypi.org/project/pymcd\nresult, we obtained pairs of noisy partially replaced speech\nand the corresponding original noisy speech, which served as\nthe training data for the noisy speech editing task. The SNR\nrange used for noisy speech editing training was also [\u22125, 20]\ndB.\nSince LibriLight does not provide reference transcriptions,\nwe adopted a pseudo-labeling approach to derive the semantic\nprompts, i.e., the phoneme sequences of the individual training\nsamples, by following [15], [16]. Specifically, we transcribed\nthe LibriLight training data with an off-the-shelf Kaldi model\nthat was trained on the 960-hour Librispeech data with 3x\nspeed perturbation7.\nB. Model and training configurations\nBoth the SpeechX AR and NAR models share the same\nTransformer architecture, featuring 12 layers, 16 attention\nheads, an embedding dimension of 1024, a feed-forward layer\ndimension of 4096, and a dropout rate of 0.1.\nWe conducted experiments employing two initialization\nmethods: random initialization and VALL-E initialization (re-\nfer to Section III-D for details). In the random initialization\nscenario, we trained the SpeechX model for 800K iterations.\nThe model optimization utilized the AdamW optimizer, with\nthe learning rate undergoing a warm-up phase for the initial\n32K updates, peaking at 5 \u00d7 10\u22124, before transitioning into a\nlinear decay phase. Conversely, with VALL-E initialization,\nwe opted for 400K iterations, as the initial model already\nunderwent zero-shot TTS training over 400K iterations. In\nthis instance, the learning rate scheduler was retained, but the\nwarm-up period was shortened to the first 20K updates.\nC. Baseline expert models\nWe employed expert models for different tasks to establish\ncomparison baselines. For zero-shot TTS, we utilized VALL-E\nby following the model configuration outlined in the original\npaper [16]. For the noise suppression task, we employed a\nnon-causal Deep Complex Convolutional Recurrent Network\n(DCCRN) [39], which is a widely recognized model for noise\nsuppression. Our training data for DCCRN came from Mi-\ncrosoft\u2019s internal dataset, and we further fine-tuned the model\nusing the ASR objective based on the training recipe of [40].\nFor target speaker extraction, we leveraged VoiceFilter [22],\nemploying a bidirectional LSTM configuration. We relied on\na publicly available implementation of VoiceFilter8. Finally,\nfor speech editing, we employed A3T [20] as the baseline.\nThe implementation of A3T that we used is also publicly\naccessible9.\nD. Results\n1) Result overview: Table. II shows the performance anal-\nysis of SpeechX in various tasks compared to the individual\nexpert models. We can see that initializing the model parame-\nters using an exiting VALL-E model checkpoint was beneficial\nacross all tasks, especially in terms of WER.\n7https://kaldi-asr.org/models/m13\n8https://github.com/Edresson/VoiceSplit\n9https://github.com/richardbaihe/a3t\n7\nTABLE II\nRESULTS FOR VARIOUS SPEECH GENERATION/TRANSFORMATION TASKS BY SPEECHX COMPARED TO EXPERT MODELS FOR INDIVIDUAL TASKS.\nTEXTUAL PROMPTS WERE USED FOR NOISE SUPPRESSION AND TARGET SPEAKER EXTRACTION. IN ZERO-SHOT TTS, \u201cNO PROCESSING\u201d ROW SHOWS\nTHE RESULTS OF DESIRED SPEECH SIGNALS.\nModel\nNoise suppression\nTarget speaker extraction\nZero-shot TTS\nClean speech editing\nNoisy speech editing\nSpeech removal\nWER\u2193 DNSMOS\u2191 PESQ\u2191\nWER\u2193 DNSMOS\u2191 PESQ\u2191\nWER\u2193\nSIM\u2191\nWER\u2193\nSIM\u2191\nWER\u2193\nSIM\u2191\nMCD\u2193\nNo processing\n3.29\n2.42\n1.93\n12.55\n3.04\n2.27\n1.71\n1.00\n38.29\n0.96\n42.48\n0.87\n12.57\nExpert model\nDCCRN [39], [40]\nVoiceFilter [22]\nVALL-E [16]\nA3T [20]\nA3T [20]\nN/A\n6.39\n3.25\n3.52\n5.09\n3.39\n2.90\n5.90\n0.57\n17.17\n0.29\n32.17\n0.18\nSpeechX (random init.)\n2.56\n3.05\n2.24\n3.12\n3.46\n2.27\n5.40\n0.57\n8.10\n0.75\n15.33\n0.64\n3.04\nSpeechX (VALL-E init.)\n2.48\n3.05\n2.24\n2.53\n3.46\n2.28\n4.66\n0.58\n5.63\n0.76\n13.95\n0.65\n3.05\nTABLE III\nRESULTS OF NOISE SUPPRESSION AND TARGET SPEAKER EXTRACTION\nWITH OR WITHOUT TEXTUAL PROMPT.\nPrompt\nNoise suppression\nTarget speaker extraction\nWER\u2193\nDNSMOS\u2191\nPESQ\u2191\nWER\u2193\nDNSMOS\u2191\nPESQ\u2191\nw/ text\n2.48\n3.05\n2.24\n2.53\n3.46\n2.28\nw/o text\n6.76\n3.05\n2.20\n5.00\n3.01\n2.23\nIn noise suppression and target speaker extraction, SpeechX\nexhibited superior performance in terms of WER compared to\nthe respective expert models. Conventional regression-based\nnoise suppression and target speaker extraction models are\nknown to suffer from processing artifacts, which our WER\nresults confirmed. SpeechX was able to avoid this detrimental\neffect thanks to the audio-text-based generation capability. On\nthe other hand, in terms of DNSMOS and PESQ scores, it\nlagged behind the expert models. This can largely be attributed\nto the impact of the codec model used, as discussed in detail in\nSection V-D5. The investigation into the speech removal task\nrevealed that SpeechX demonstrated substantial improvement\nin MCD, showcasing its efficacy in removing speech. These\nresults underscore the versatility of the SpeechX model in han-\ndling enhancement-related tasks, while also highlighting the\nusefulness of the audio-text-based speech generation capability\nthat SpeechX provides.\nIn the zero-shot TTS task, SpeechX demonstrated a slight\nadvantage over the baseline VALL-E model in terms of WER\nwhile concurrently achieving a comparable speaker similar-\nity score10. Furthermore, for the clean speech editing task,\nSpeechX exhibited significant improvement over the baseline\nA3T model. The WER observed in the speech editing task was\nslightly higher than the WER obtained in the zero-shot TTS\ntask, even though one might anticipate that they should fall\nwithin the same range. This discrepancy could be attributed\nto certain test samples where the length of non-edited speech\nwas shorter than three seconds. These results highlight that\nSpeechX is equally effective in tasks primarily focusing on\nspeech generation capability, rather than transformation ability.\n2) Speech editing for clean and noisy speech: Table II also\ncompares the speech editing results between clean and noisy\nspeech in terms of WER and SIM. Editing noisy speech poses\n10To avoid potential confusion, it should be noted that our experimental\nsetup corresponds to the non-continual evaluation configuration utilized in\nthe original VALL-E work.\ngreater challenges than clean speech, as it requires modifying\nthe spoken content while preserving background noise. This\ndifficulty is evident from a WER gap of 38.29% vs. 42.48%\nobserved between the clean and noisy audio signals to be\nedited as well as A3T\u2019s limited WER improvement from\n42.48% to 32.17%.\nNonetheless, the SpeechX model successfully edited the\nnoisy speech, reducing the WER to 13.95% after processing.\nThis demonstrates the model\u2019s robustness to acoustic noise\nin the input signal. The high SIM score of 0.65 shows the\nmodel largely preserved speaker characteristics, even with\nnoise present. Our observation revealed the model retained\nbackground noise, as confirmed by our provided demo sam-\nples. Fig. 2 compares mel spectrograms for two exemplary\npairs of input and generated speech signals. In the first\nexample, the input speech contained periodic noise in the\nmiddle frequency range. SpeechX preserved this background\nnoise over the full input duration while selectively modifying\nonly the foreground speech during the period beginning at two\nseconds. A similar observation can be made for the second\nexample, wherein the alteration was applied to the first half\nof the speech content. In summary, the results demonstrate\nSpeechX model\u2019s effectiveness at noisy speech editing while\nmaintaining speaker identity and background noise. Future\nwork should develop a metric to quantitatively evaluate noise\ncloning capability.\n3) Effectiveness of text input in noise suppression and\ntarget speaker extraction: With SpeechX, it is feasible to\nperform noise suppression and target speaker extraction using\nsolely the acoustic prompt as input. To assess the efficacy\nof incorporating additional text input in the SpeechX model,\nwe conducted noise suppression and target speaker extraction\nexperiments where we employed only the acoustic prompt as\nthe model input. Specifically, the input for noise suppression\ncomprised the noisy speech, while for target speaker extrac-\ntion, it consisted of the mixed speech and the target speaker\u2019s\nenrollment audio.\nThe experimental results are presented in Table III. For both\ntasks, omitting the text input resulted in a noticeable increase\nin WER, whereas the degradation in DNSMOS and PESQ\nscores was modest. These findings suggest that leveraging\nthe text input was particularly beneficial for enhancing the\nintelligibility of the output speech. In target speaker extraction,\na significant impact on the DNSMOS score was observed,\nindicating that the text input aids in disentangling the target\n8\nTABLE IV\nEFFECTS OF ADDING TASKS DURING TRAINING. ZS: ZERO-SHOT, SE: SPEECH EDITING, NS: NOISE SUPPRESSION, SR: SPEECH REMOVAL, TSE: TARGET\nSPEAKER EXTRACTION.\nTraining tasks\nZero-shot TTS\nSpeech editing (clean/noisy)\nNoise suppression\nSpeech removal\nTarget speaker extraction\nWER\u2193\nSIM\u2191\nWER\u2193\nSIM\u2191\nWER\u2193\nDNSMOS\u2191\nMCD\u2193\nWER\u2193\nDNSMOS\u2191\nZS-TTS\n5.90\n0.57\n-\n-\n-\n-\n-\n-\n-\nZS-TTS + SE\n4.55\n0.58\n5.79\n/\n13.80\n0.76\n/\n0.65\n-\n-\n-\n-\n-\nZS-TTS + SE + NS/SR\n5.11\n0.57\n6.91\n/\n13.23\n0.77\n/\n0.66\n2.59\n3.03\n3.04\n-\n-\nZS-TTS + SE + NS/SR + TSE\n4.66\n0.58\n5.63\n/\n13.95\n0.76\n/\n0.65\n2.48\n3.05\n3.05\n2.53\n3.46\nFig. 2. Mel spectrograms of pre-edit and post-edit noisy signals. The pre-edit\nsignal was obtained by applying EnCodec compression and decompression\nwithout intermediate processing to highlight the change made by SpeechX\u2019s\nneural codec language model. See Section V-D5 for discussion on codec\u2019s\nimpact.\nspeaker\u2019s voice from the interfering talker. Notably, while\nrelying solely on the acoustic prompt led to WER degradation,\nthe achieved WERs were still comparable to those of the\nbaseline expert models.\n4) Effect of multi-task training: We also conducted ex-\nperiments where we used subsets of the tasks during train-\ning to explore potential interactions between different tasks.\nSpecifically, in addition to the VALL-E model and the fully-\ntrained SpeechX models that used the complete set of the\ntasks, we trained two additional SpeechX models: one trained\nexclusively for zero-shot TTS and speech editing tasks, and\nTABLE V\nIMPACT OF NEURAL CODEC ON PERFORMANCE METRICS FOR CLEAN AND\nNOISY SPEECH.\nAudio type\nWER\u2193\nDNSMOS\u2191\nPESQ\u2191\nSIM\u2191\nRaw clean speech\n1.71\n3.22\n4.64\n1.00\n,\u2192 EnCodec\n1.81\n2.97\n2.69\n0.81\nRaw noisy speech\n3.29\n2.42\n1.93\n0.95\n,\u2192 EnCodec\n5.08\n2.19\n1.63\n0.75\nthe other trained on the zero-shot TTS, speech editing, noise\nsuppression, and speech removal data.\nTable IV shows the experimental results. The inclusion of\nspeech editing during training led to an enhancement in WER\nfor zero-shot TTS while allowing the model to learn about the\nspeech editing task. Considering the strong parallels between\nzero-shot TTS and speech editing, this improvement can be\nattributed to the speech editing training task introducing addi-\ntional variations to the distribution of the training data. Further\ninclusion of the noise suppression and speech removal tasks\nduring training resulted in degradation in clean speech editing\nperformance, while concurrently enhancing the performance\nfor noisy speech editing. This suggests that exposing the model\nto noisy speech samples from these additional tasks improved\nthe model\u2019s robustness to acoustic noise at the expense of clean\nspeech generation. Also, it is noteworthy that introduction of\nthe target speaker extraction tasks to the training data did not\ncompromise the model\u2019s proficiency in noise suppression and\nspeech removal.\n5) Limitation of current neural codec model: The perfor-\nmance of SpeechX is inherently constrained by the accuracy of\nthe neural codec model employed for acoustic tokenization. It\nshould be noted that, in all previous experiments, we compared\nSpeechX\u2019s results with the reference (i.e., no-processing and\nexpert model) results obtained without any neural codec pro-\ncessing. To gain a more precise interpretation of SpeechX\u2019s\nresults, we conducted an additional experiment where we\napplied compression and decompression to the LibriSpeech\ntest-clean data without any intermediate processing, measuring\nEnCodec\u2019s impact on performance metrics.\nTable V shows the experimental results. It is evident that\nprocessing the signals with the codec model resulted in varying\ndegrees of performance regression across all metrics. Notably,\nthe PESQ score dropped from 4.64 to 2.69 for the clean speech\ninput. Our assessment indicates that while EnCodec produced\nslightly noticeable speech quality degradation, the significant\nPESQ degradation may be partly attributed to the mismatch be-\ntween the PESQ algorithm and EnCodec\u2019s training objective.\n9\nWhile we utilized EnCodec due to its accessibility and prior\nusage, future work should address this issue by developing an\nacoustic tokenization model more suitable for handling speech\nunder various acoustic conditions.\nVI. CONCLUSION\nIn this paper, we described SpeechX, a novel versatile\nspeech generation model capable of handling diverse audio-\ntext-based speech generation tasks, including zero-shot TTS,\nnoise suppression, speech removal, target speaker extraction,\nand speech editing. For noise suppression and target speaker\nextraction, the proposed model provides a unified way for\nincorporating the knowledge of transcriptions. Also, regarding\nspeech editing, SpeechX enables modifying the spoken content\nof a speech signal that contains a fair amount of background\nnoise. SpeechX adopts a language modeling approach to\ngenerate acoustic tokens conditioned on textual and acoustic\nprompts, where additional task-dependent tokens are incorpo-\nrated in a multi-task learning framework to support various\nspeech transformation capabilities beyond zero-shot TTS. We\ndemonstrated SpeechX\u2019s efficacy through comprehensive ex-\nperiments. The proposed model represents an important step\ntoward unified generative speech models. Further research can\nbuild on this work by expanding the tasks supported, enhanc-\ning robustness, and developing more advanced conditioning\nmechanisms.\nREFERENCES\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei, \u201cLanguage models are few-shot learners,\u201d in Advances in\nNeural Information Processing Systems, vol. 33, 2020, pp. 1877\u20131901.\n[2] OpenAI, \u201cGPT-4 technical report,\u201d arXiv preprint arXiv:2204.06125,\n2023.\n[3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d in 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022, pp. 10 674\u201310 685.\n[4] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Shar-\nifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi et al., \u201cAudi-\noLM: a language modeling approach to audio generation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 31, pp.\n2523\u20132533, 2023.\n[5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson,\nK. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Ruther-\nford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L.\nMenick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a.\nBi\u00b4nkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan,\n\u201cFlamingo: a visual language model for few-shot learning,\u201d in Advances\nin Neural Information Processing Systems, vol. 35, 2022, pp. 23 716\u2013\n23 736.\n[6] J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and\nL. Wang, \u201cGIT: A generative image-to-text transformer for vision and\nlanguage,\u201d Transactions on Machine Learning Research, 2022.\n[7] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman,\n\u201cDreamBooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation,\u201d arXiv preprint arXiv:2208.12242, 2023.\n[8] Z. Yang, M. Khademi, Y. Xu, R. Pryzant, Y. Fang, C. Zhu, D. Chen,\nY. Qian, M. Gao, Y.-L. Chen, R. Gmyr, N. Kanda, N. Codella, B. Xiao,\nY. Shi, L. Yuan, T. Yoshioka, M. Zeng, and X. Huang, \u201ci-Code V2: An\nautoregressive generation framework over vision, language, and speech\ndata,\u201d arXiv preprint arXiv:2305.12311, 2023.\n[9] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor-\nsos, F. d. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov,\nH. Muckenhirn, D. Padfield, J. Qin, D. Rozenberg, T. Sainath, J. Schalk-\nwyk, M. Sharifi, M. T. Ramanovich, M. Tagliasacchi, A. Tudor, M. Ve-\nlimirovi\u00b4c, D. Vincent, J. Yu, Y. Wang, V. Zayats, N. Zeghidour, Y. Zhang,\nZ. Zhang, L. Zilka, and C. Frank, \u201cAudioPaLM: A large language model\nthat can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023.\n[10] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon,\nQ. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghi-\ndour, and C. Frank, \u201cMusicLM: Generating music from text,\u201d arXiv\npreprint arXiv:2301.11325, 2023.\n[11] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00a8olge, and M. A.\nPonti, \u201cYourTTS: Towards zero-shot multi-speaker TTS and zero-shot\nvoice conversion for everyone,\u201d in Proceedings of the 39th International\nConference on Machine Learning, 2022, pp. 2709\u20132720.\n[12] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen,\nR. Pang, I. Lopez Moreno, and Y. Wu, \u201cTransfer learning from speaker\nverification to multispeaker text-to-speech synthesis,\u201d in Advances in\nNeural Information Processing Systems, 2018.\n[13] E. Cooper, C.-I. Lai, Y. Yasuda, F. Fang, X. Wang, N. Chen, and\nJ. Yamagishi, \u201cZero-shot multi-speaker text-to-speech with state-of-the-\nart neural speaker embeddings,\u201d in 2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6184\u2013\n6188.\n[14] E. Casanova, C. Shulby, E. G\u00a8olge, N. M. M\u00a8uller, F. S. de Oliveira,\nA. Candido Jr., A. da Silva Soares, S. M. Aluisio, and M. A. Ponti, \u201cSC-\nGlowTTS: An efficient zero-shot multi-speaker text-to-speech model,\u201d\nin Proc. Interspeech 2021, 2021, pp. 3645\u20133649.\n[15] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson,\nV. Manohar, Y. Adi, J. Mahadeokar, and W.-N. Hsu, \u201cVoicebox: Text-\nguided multilingual universal speech generation at scale,\u201d arXiv preprint\narXiv:2306.15687, 2023.\n[16] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen,\nY. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cNeural codec\nlanguage models are zero-shot text to speech synthesizers,\u201d arXiv\npreprint arXiv:2301.02111, 2023.\n[17] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin,\nM. Sharifi, M. Tagliasacchi, and N. Zeghidour, \u201cSpeak, read and prompt:\nHigh-fidelity text-to-speech with minimal supervision,\u201d arXiv preprint\narXiv:2302.03540, 2023.\n[18] R. Huang, C. Zhang, Y. Wang, D. Yang, L. Liu, Z. Ye, Z. Jiang,\nC. Weng, Z. Zhao, and D. Yu, \u201cMake-A-Voice: Unified voice synthesis\nwith discrete representation,\u201d arXiv preprint arXiv:2305.19269, 2023.\n[19] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu,\nH. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cSpeak foreign languages\nwith your own voice: Cross-lingual neural codec language modeling,\u201d\narXiv preprint arXiv:2303.03926, 2023.\n[20] H. Bai, R. Zheng, J. Chen, M. Ma, X. Li, and L. Huang, \u201cA3T:\nAlignment-aware acoustic and text pretraining for speech synthesis\nand editing,\u201d in Proceedings of the 39th International Conference on\nMachine Learning, 2022, pp. 1399\u20131411.\n[21] Z. Jiang, Y. Ren, Z. Ye, J. Liu, C. Zhang, Q. Yang, S. Ji, R. Huang,\nC. Wang, X. Yin et al., \u201cMega-TTS: Zero-shot text-to-speech at scale\nwith intrinsic inductive bias,\u201d arXiv preprint arXiv:2306.03509, 2023.\n[22] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. Hershey, R. A.\nSaurous, R. J. Weiss, Y. Jia, and I. L. Moreno, \u201cVoiceFilter: Targeted\nvoice separation by speaker-conditioned spectrogram masking,\u201d arXiv\npreprint arXiv:1810.04826, 2018.\n[23] K. \u02c7Zmol\u00b4\u0131kov\u00b4a, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani,\nL. Burget, and J. \u02c7Cernock\u00b4y, \u201cSpeakerBeam: Speaker aware neural\nnetwork for target speaker extraction in speech mixtures,\u201d IEEE Journal\nof Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800\u2013814,\n2019.\n[24] S. E. Eskimez, T. Yoshioka, H. Wang, X. Wang, Z. Chen, and X. Huang,\n\u201cPersonalized speech enhancement: new models and comprehensive\nevaluation,\u201d in 2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2022, pp. 356\u2013360.\n[25] K. Zmolikova, M. Delcroix, T. Ochiai, K. Kinoshita, J. \u02c7Cernock`y, and\nD. Yu, \u201cNeural target speech extraction: An overview,\u201d IEEE Signal\nProcessing Magazine, vol. 40, no. 3, pp. 8\u201329, 2023.\n[26] J. Serr`a, S. Pascual, J. Pons, R. O. Araz, and D. Scaini, \u201cUniver-\nsal speech enhancement with score-based diffusion,\u201d arXiv preprint\narXiv:2206.03065, 2022.\n[27] K. Kinoshita, M. Delcroix, A. Ogawa, and T. Nakatani, \u201cText-informed\nspeech enhancement with deep neural networks,\u201d in Proc. Interspeech\n2015, 2015, pp. 1760\u20131764.\n10\n[28] K. Schulze-Forster, C. S. J. Doire, G. Richard, and R. Badeau, \u201cJoint\nphoneme alignment and text-informed speech separation on highly\ncorrupted speech,\u201d in 2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2020, pp. 7274\u20137278.\n[29] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar\u00b4e,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nbenchmark for ASR with limited or no supervision,\u201d in ICASSP 2020,\n2020, pp. 7669\u20137673.\n[30] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,\nand I. Sutskever, \u201cZero-shot text-to-image generation,\u201d in Proceedings\nof the 38th International Conference on Machine Learning, 2021, pp.\n8821\u20138831.\n[31] K. Shen, Z. Ju, X. Tan, Y. Liu, Y. Leng, L. He, T. Qin, S. Zhao, and\nJ. Bian, \u201cNaturalSpeech 2: Latent diffusion models are natural and zero-\nshot speech and singing synthesizers,\u201d arXiv preprint arXiv:2304.09116,\n2023.\n[32] A. D\u00b4efossez, J. Copet, G. Synnaeve, and Y. Adi, \u201cHigh fidelity neural\naudio compression,\u201d arXiv preprint arXiv:2210.13438, 2022.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances\nin Neural Information Processing Systems, vol. 30, 2017.\n[34] D. Snyder, G. Chen, and D. Povey, \u201cMUSAN: A music, speech, and\nnoise corpus,\u201d arXiv preprint arXiv:1510.08484, 2015.\n[35] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-\naugmented transformer for speech recognition,\u201d in Proc. Interspeech\n2020, 2020, pp. 5036\u20135040.\n[36] C. K. Reddy, V. Gopal, and R. Cutler, \u201cDnsmos: A non-intrusive\nperceptual objective speech quality metric to evaluate noise suppressors,\u201d\nin 2021 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2021, pp. 6493\u20136497.\n[37] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, \u201cPerceptual\nevaluation of speech quality (PESQ) \u2013 a new method for speech\nquality assessment of telephone networks and codecs,\u201d in 2001 IEEE\ninternational conference on acoustics, speech, and signal processing.\nProceedings, vol. 2, 2001, pp. 749\u2013752.\n[38] H. Dubey, A. Aazami, V. Gopal, B. Naderi, S. Braun, R. Cutler, A. Ju,\nM. Zohourian, M. Tang, H. Gamper, M. Golestaneh, and R. Aich-\nner, \u201cICASSP 2023 deep noise suppression challenge,\u201d arXiv preprint\narXiv:2303.11510, 2023.\n[39] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang,\nand L. Xie, \u201cDCCRN: Deep complex convolution recurrent network for\nphase-aware speech enhancement,\u201d in Proc. Interspeech 2020, 2020, pp.\n2472\u20132476.\n[40] S. E. Eskimez, X. Wang, M. Tang, H. Yang, Z. Zhu, Z. Chen, H. Wang,\nand T. Yoshioka, \u201cHuman listening and live captioning: Multi-task\ntraining for speech enhancement,\u201d in Proc. Interspeech 2021, 2021, pp.\n2686\u20132690.\n"
  },
  {
    "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
    "link": "https://arxiv.org/pdf/2308.06721.pdf",
    "upvote": "23",
    "text": "IP-Adapter: Text Compatible Image Prompt Adapter for\nText-to-Image Diffusion Models\nHu Ye, Jun Zhang\u2217, Sibo Liu, Xiao Han, Wei Yang\nTencent AI Lab\n{huye, junejzhang, siboliu, haroldhan, willyang}@tencent.com\nABSTRACT\nRecent years have witnessed the strong power of large text-to-image diffusion models for the\nimpressive generative capability to create high-fidelity images. However, it is very tricky to generate\ndesired images using only text prompt as it often involves complex prompt engineering. An alternative\nto text prompt is image prompt, as the saying goes: \"an image is worth a thousand words\". Although\nexisting methods of direct fine-tuning from pretrained models are effective, they require large\ncomputing resources and are not compatible with other base models, text prompt, and structural\ncontrols. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image\nprompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter\nis decoupled cross-attention mechanism that separates cross-attention layers for text features and\nimage features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters\ncan achieve comparable or even better performance to a fully fine-tuned image prompt model. As\nwe freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to\nother custom models fine-tuned from the same base model, but also to controllable generation using\nexisting controllable tools. With the benefit of the decoupled cross-attention strategy, the image\nprompt can also work well with the text prompt to achieve multimodal image generation. The project\npage is available at https://ip-adapter.github.io.\n1\nIntroduction\nImage generation has made remarkable strides with the success of recent large text-to-image diffusion models like\nGLIDE [1], DALL-E 2 [2], Imagen [3], Stable Diffusion (SD) [4], eDiff-I [5] and RAPHAEL [6]. Users can write text\nprompt to generate images with the powerful text-to-image diffusion models. But writing good text prompt to generate\ndesired content is not easy, as complex prompt engineering [7] is often required. Moreover, text is not informative to\nexpress complex scenes or concepts, which can be a hindrance to content creation. Considering the above limitations of\nthe text prompt, we may ask if there are other prompt types to generate images. A natural choice is to use the image\nprompt, since an image can express more content and details compared to text, just as often said: \"an image is worth a\nthousand words\". DALL-E 2[2] makes the first attempt to support image prompt, the diffusion model is conditioned\non image embedding rather than text embedding, and a prior model is required to achieve the text-to-image ability.\nHowever, most existing text-to-image diffusion models are conditioned on text to generate images, for example, the\npopular SD model is conditioned on the text features extracted from a frozen CLIP [8] text encoder. Could image prompt\nbe also supported on these text-to-image diffusion models? Our work attempts to enable the generative capability with\nimage prompt for these text-to-image diffusion models in a simple manner.\nPrior works, such as SD Image Variations2 and Stable unCLIP3, have demonstrated the effectiveness of fine-tuning\nthe text-conditioned diffusion models directly on image embedding to achieve image prompt capabilities. However,\nthe disadvantages of this approach are obvious. First, it eliminates the original ability to generate images using text,\nand large computing resources are often required for such fine-tuning. Second, the fine-tuned models are typically not\nreusable, as the image prompt ability cannot be directly transferred to the other custom models derived from the same\ntext-to-image base models. Moreover, the new models are often incompatible with existing structural control tools such\nas ControlNet [9], which poses significant challenges for downstream applications. Due to the drawbacks of fine-tuning,\n\u2217Corresponding author\n2https://huggingface.co/lambdalabs/sd-image-variations-diffusers\n3https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip\narXiv:2308.06721v1  [cs.CV]  13 Aug 2023\nwearing sunglasses\n+\n+\n+\n+\n+\nImage Prompt\nStructural Controls\nRealistic Model \nInpainting\nText Prompt\nVariation\nAnime Model \nRealistic Model Anime Model \nFigure 1: Various image synthesis with our proposed IP-Adapter applied on the pretrained text-to-image diffusion\nmodels with different styles. The examples on the right show the results of image variations, multimodal generation,\nand inpainting with image prompt, while the left examples show the results of controllable generation with image\nprompt and additional structural conditions.\nsome studies [10] opt to replace the text encoder with an image encoder while avoiding fine-tuning the diffusion model.\nAlthough this method is effective and simple, it still has several drawbacks. At first, only the image prompt is supported,\npreventing users from simultaneously using text and image prompt to create images. Furthermore, merely fine-tuning\nthe image encoder is often not sufficient to guarantee image quality, and could lead to generalization issues.\nIn this study, we are curious about whether it is possible to achieve image prompt capability without modifying the\noriginal text-to-image models. Fortunately, previous works are encouraging. Recent advances in controllable image\ngeneration, such as ControlNet [9] and T2I-adapter [11], have demonstrated that an additional network can be effectively\nplugged in the existing text-to-image diffusion models to guide the image generation. Most of the studies focus on image\ngeneration with additional structure control such as user-drawn sketch, depth map, semantic segmentation map, etc.\nBesides, image generation with style or content provided by reference image has also been achieved by simple adapters,\nsuch as the style adapter of T2I-adapter [11] and global controller of Uni-ControlNet [12]. To achieve this, image\nfeatures extracted from CLIP image encoder are mapped to new features by a trainable network and then concatenated\nwith text features. By replacing the original text features, the merged features are fed into the UNet of the diffusion\nmodel to guide image generation. These adapters can be seen as a way to have the ability to use image prompt, but the\ngenerated image is only partially faithful to the prompted image. The results are often worse than the fine-tuned image\nprompt models, let alone the model trained from scratch.\nWe argue that the main problem of the aforementioned methods lies in the cross-attention modules of text-to-image\ndiffusion models. The key and value projection weights of the cross-attention layer in the pretrained diffusion model are\ntrained to adapt the text features. Consequently, merging image features and text features into the cross-attention layer\nonly accomplishes the alignment of image features to text features, but this potentially misses some image-specific\ninformation and eventually leads to only coarse-grained controllable generation (e.g., image style) with the reference\nimage.\nTo this end, we propose a more effective image prompt adapter named IP-Adapter to avoid the shortcomings of\nthe previous methods. Specifically, IP-Adapter adopts a decoupled cross-attention mechanism for text features and\nimage features. For every cross-attention layer in the UNet of diffusion model, we add an additional cross-attention\nlayer only for image features. In the training stage, only the parameters of the new cross-attention layers are trained,\nwhile the original UNet model remains frozen. Our proposed adapter is lightweight but very efficient: the generative\nperformance of an IP-Adapter with only 22M parameters is comparable to a fully fine-tuned image prompt model from\nthe text-to-image diffusion model. More importantly, our IP-Adapter exhibits excellent generalization capabilities and\nis compatible with text prompt. With our proposed IP-Adapter, various image generation tasks can be easily achieved,\nas illustrated in Figure 1.\nTo sum up, our contributions are as follows:\n\u2022 We present IP-Adapter, a lightweight image prompt adaptation method with the decoupled cross-attention\nstrategy for existing text-to-image diffusion models. Quantitative and qualitative experimental results show\nthat a small IP-Adapter with about 22M parameters is comparable or even better than the fully fine-tuned\nmodels for image prompt based generation.\n2\n\u2022 Our IP-Adapter is reusable and flexible. IP-Adapter trained on the base diffusion model can be generalized to\nother custom models fine-tuned from the same base diffusion model. Moreover, IP-Adapter is compatible\nwith other controllable adapters such as ControlNet, allowing for an easy combination of image prompt with\nstructure controls.\n\u2022 Due to the decoupled cross-attention strategy, image prompt is compatible with text prompt to achieve\nmultimodal image generation.\n2\nRelated Work\nWe focus on designing an image prompt adapter for the existing text-to-image diffusion models. In this section, we\nreview recent works on text-to-image diffusion models, as well as relevant studies on adapters for large models.\n2.1\nText-to-Image Diffusion Models\nLarge text-to-image models are mainly divided into two categories: autoregressive models and diffusion models.\nEarly works, such as DALLE [13], CogView [14, 15] and Make-A-Scene [16], are autoregressive models. For the\nautoregressive model, an image tokenizer like VQ-VAE [17] is used to convert an image to tokens, then an autoregressive\ntransformer [18] conditioned on text tokens is trained to predict image tokens. However, autoregressive models often\nrequire large parameters and computing resources to generate high-quality images, as seen in Parti [19].\nRecently, diffusion models (DMs) [20, 21, 22, 23] has emerged as the new state-of-the-art model for text-to-image\ngeneration. As a pioneer, GLIDE uses a cascaded diffusion architecture with a 3.5B text-conditional diffusion model at\n64\u00d764 resolution and a 1.5B text-conditional upsampling diffusion model at 256\u00d7256 resolution. DALL-E 2 employs\na diffusion model conditioned image embedding, and a prior model was trained to generate image embedding by giving\na text prompt. DALL-E 2 not only supports text prompt for image generation but also image prompt. To enhance the\ntext understanding, Imagen adopts T5 [24], a large transformer language model pretrained on text-only data, as the text\nencoder of diffusion model. Re-Imagen [25] uses retrieved information to improve the fidelity of generated images for\nrare or unseen entities. SD is built on the latent diffusion model [4], which operates on the latent space instead of pixel\nspace, enabling SD to generate high-resolution images with only a diffusion model. To improve text alignment, eDiff-I\nwas designed with an ensemble of text-to-image diffusion models, utilizing multiple conditions, including T5 text,\nCLIP text, and CLIP image embeddings. Versatile Diffusion [26] presents a unified multi-flow diffusion framework to\nsupport text-to-image, image-to-text, and variations within a single model. To achieve controllable image synthesis,\nComposer [27] presents a joint fine-tuning strategy with various conditions on a pretrained diffusion model conditioned\non image embedding. RAPHAEL introduces a mixture-of-experts (MoEs) strategy [28, 29] into the text-conditional\nimage diffusion model to enhance image quality and aesthetic appeal.\nAn attractive feature of DALL-E 2 is that it can also use image prompt to generate image variations. Hence, there are\nalso some works to explore to support image prompt for the text-to-image diffusion models conditioned only on text.\nSD Image Variations model is fine-tuned from a modified SD model where the text features are replaced with the image\nembedding from CLIP image encoder. Stable unCLIP is also a fine-tuned model on SD, in which the image embedding\nis added to the time embedding. Although the fine-tuning model can successfully use image prompt to generate images,\nit often requires a relatively large training cost, and it fails to be compatible with existing tools, e.g., ControlNet [9].\n2.2\nAdapters for Large Models\nAs fine-tuning large pre-trained models is inefficient, an alternative approach is using adapters, which add a few trainable\nparameters but freeze the original model. Adapters have been used in the field of NLP for a long time [30]. Recently,\nadapters have been utilized to achieve vision-language understanding for large language models [31, 32, 33, 34, 35].\nWith the popularity of recent text-to-image models, adapters have also been used to provide additional control for the\ngeneration of text-to-image models. ControlNet [9] first proves that an adapter could be trained with a pretrained text-to-\nimage diffusion model to learn task-specific input conditions, e.g., canny edge. Almost concurrently, T2I-adapter [11]\nemploys a simple and lightweight adapter to achieve fine-grained control in the color and structure of the generated\nimages. To reduce the fine-tuning cost, Uni-ControlNet [12] presents a multi-scale condition injection strategy to learn\nan adapter for various local controls.\nApart from the adapters for structural control, there are also works for the controllable generation conditioned on the\ncontent and style of the provided image. ControlNet Shuffle 1 trained to recompose images, can be used to guide the\n1https://github.com/lllyasviel/ControlNet-v1-1-nightly\n3\nA girl with sunglasses\nText \nEncoder\nImage\nEncoder\nLinear\nLN\nImage Features\nText Features\nDecoupled Cross-Attention\n\ud835\udc99\ud835\udc95\n\ud835\udc99\ud835\udc95\"\ud835\udfcf\nCross \nAttention\nCross \nAttention\nDenoising U-Net\nFrozen \nmodules\nTrainable \nmodules\nFigure 2: The overall architecture of our proposed IP-Adapter with decoupled cross-attention strategy. Only the newly\nadded modules (in red color) are trained while the pretrained text-to-image model is frozen.\ngeneration by a user-provided image. Moreover, ControlNet Reference-only1 was presented to achieve image variants\non SD model through simple feature injection without training. In the updated version of T2I-adapter, a style adapter\nis designed to control the style of generated images using a reference image by appending image features extracted\nfrom the CLIP image encoder to text features. The global control adapter of Uni-ControlNet also projects the image\nembedding from CLIP image encoder into condition embeddings by a small network and concatenates them with\nthe original text embeddings, and it is used to guide the generation with the style and content of reference image.\nSeeCoder [10] presents a semantic context encoder to replace the original text encoder to generate image variants.\nAlthough the aforementioned adapters are lightweight, their performance is hardly comparable to that of the fine-tuned\nimage prompt models, let alone one trained from scratch. In this study, we introduce a decoupled cross-attention\nmechanism to achieve a more effective image prompt adapter. The proposed adapter remains simple and small but\noutperforms previous adapter methods, and is even comparable to fine-tuned models.\n3\nMethod\nIn this section, we first introduce some preliminaries about text-to-image diffusion models. Then, we depict in detail\nthe motivation and the design of the proposed IP-Adapter.\n3.1\nPrelimiaries\nDiffusion models are a class of generative models that comprise two processes: a diffusion process (also known as\nthe forward process), which gradually adds Gaussian noise to the data using a fixed Markov chain of T steps, and a\ndenoising process that generates samples from Gaussian noise with a learnable model. Diffusion models can also be\nconditioned on other inputs, such as text in the case of text-to-image diffusion models. Typically, the training objective\nof a diffusion model, denoted as \u03f5\u03b8, which predicts noise, is defined as a simplified variant of the variational bound:\nLsimple = Ex0,\u03f5\u223cN (0,I),c,t\u2225\u03f5 \u2212 \u03f5\u03b8\n\u0000xt, c, t\n\u0001\n\u22252,\n(1)\nwhere x0 represents the real data with an additional condition c, t \u2208 [0, T] denotes the time step of diffusion process,\nxt = \u03b1tx0 + \u03c3t\u03f5 is the noisy data at t step, and \u03b1t, \u03c3t are predefined functions of t that determine the diffusion process.\nOnce the model \u03f5\u03b8 is trained, images can be generated from random noise in an iterative manner. Generally, fast\nsamplers such as DDIM [21], PNDM [36] and DPM-Solver [37, 38], are adopted in the inference stage to accelerate\nthe generation process.\nFor the conditional diffusion models, classifier guidance [23] is a straightforward technique used to balance image\nfidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training\n1https://github.com/Mikubill/sd-webui-controlnet\n4\na classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach,\nthe conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the\nsampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5\u03b8(xt, c, t) and\nunconditional model \u03f5\u03b8(xt, t):\n\u02c6\u03f5\u03b8(xt, c, t) = w\u03f5\u03b8(xt, c, t) + (1 \u2212 w)\u03f5\u03b8(xt, t),\n(2)\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c.\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment\nof generated samples.\nIn our study, we utilize the open-source SD model as our example base model to implement the IP-Adapter. SD is a\nlatent diffusion model conditioned on text features extracted from a frozen CLIP text encoder. The architecture of the\ndiffusion model is based on a UNet [40] with attention layers. Compared to pixel-based diffusion models like Imagen,\nSD is more efficient since it is constructed on the latent space from a pretrained auto-encoder model.\n3.2\nImage Prompt Adapter\nIn this paper, the image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate\nimages with image prompt. As mentioned in previous sections, current adapters struggle to match the performance of\nfine-tuned image prompt models or the models trained from scratch. The major reason is that the image features cannot\nbe effectively embedded in the pretrained model. Most methods simply feed concatenated features into the frozen\ncross-attention layers, preventing the diffusion model from capturing fine-grained features from the image prompt. To\naddress this issue, we present a decoupled cross-attention strategy, in which the image features are embedded by newly\nadded cross-attention layers. The overall architecture of our proposed IP-Adapter is demonstrated in Figure 2. The\nproposed IP-Adapter consists of two parts: an image encoder to extract image features from image prompt, and adapted\nmodules with decoupled cross-attention to embed image features into the pretrained text-to-image diffusion model.\n3.2.1\nImage Encoder\nFollowing most of the methods, we use a pretrained CLIP image encoder model to extract image features from the\nimage prompt. The CLIP model is a multimodal model trained by contrastive learning on a large dataset containing\nimage-text pairs. We utilize the global image embedding from the CLIP image encoder, which is well-aligned with\nimage captions and can represent the rich content and style of the image. In the training stage, the CLIP image encoder\nis frozen.\nTo effectively decompose the global image embedding, we use a small trainable projection network to project the image\nembedding into a sequence of features with length N (we use N = 4 in this study), the dimension of the image features\nis the same as the dimension of the text features in the pretrained diffusion model. The projection network we used in\nthis study consists of a linear layer and a Layer Normalization [41].\n3.2.2\nDecoupled Cross-Attention\nThe image features are integrated into the pretrained UNet model by the adapted modules with decoupled cross-attention.\nIn the original SD model, the text features from the CLIP text encoder are plugged into the UNet model by feeding into\nthe cross-attention layers. Given the query features Z and the text features ct, the output of cross-attention Z\u2032 can be\ndefined by the following equation:\nZ\u2032 = Attention(Q, K, V) = Softmax(QK\u22a4\n\u221a\nd\n)V,\n(3)\nwhere Q = ZWq, K = ctWk, V = ctWv are the query, key, and values matrices of the attention operation\nrespectively, and Wq, Wk, Wv are the weight matrices of the trainable linear projection layers.\nA straightforward method to insert image features is to concatenate image features and text features and then feed them\ninto the cross-attention layers. However, we found this approach to be insufficiently effective. Instead, we propose a\ndecoupled cross-attention mechanism where the cross-attention layers for text features and image features are separate.\nTo be specific, we add a new cross-attention layer for each cross-attention layer in the original UNet model to insert\nimage features. Given the image features ci, the output of new cross-attention Z\u2032\u2032 is computed as follows:\nZ\u2032\u2032 = Attention(Q, K\u2032, V\u2032) = Softmax(Q(K\u2032)\u22a4\n\u221a\nd\n)V\u2032,\n(4)\n5\nwhere, Q = ZWq, K\u2032 = ciW\u2032\nk and V\u2032 = ciW\u2032\nv are the query, key, and values matrices from the image features. W\u2032\nk\nand W\u2032\nv are the corresponding weight matrices. It should be noted that we use the same query for image cross-attention\nas for text cross-attention. Consequently, we only need add two paramemters W\u2032\nk, W\u2032\nv for each cross-attention layer. In\norder to speed up the convergence, W\u2032\nk and W\u2032\nv are initialized from Wk and Wv. Then, we simply add the output of\nimage cross-attention to the output of text cross-attention. Hence, the final formulation of the decoupled cross-attention\nis defined as follows:\nZnew = Softmax(QK\u22a4\n\u221a\nd\n)V + Softmax(Q(K\u2032)\u22a4\n\u221a\nd\n)V\u2032\nwhere Q = ZWq, K = ctWk, V = ctWv, K\u2032 = ciW\u2032\nk, V\u2032 = ciW\u2032\nv\n(5)\nSine we freeze the original UNet model, only the W\u2032\nk and W\u2032\nv are trainable in the above decoupled cross-attention.\n3.2.3\nTraining and Inference\nDuring training, we only optimize the IP-Adapter while keeping the parameters of the pretrained diffusion model fixed.\nThe IP-Adapter is also trained on the dataset with image-text pairs1, using the same training objective as original SD:\nLsimple = Ex0,\u03f5,ct,ci,t\u2225\u03f5 \u2212 \u03f5\u03b8\n\u0000xt, ct, ci, t\n\u0001\n\u22252.\n(6)\nWe also randomly drop image conditions in the training stage to enable classifier-free guidance in the inference stage:\n\u02c6\u03f5\u03b8(xt, ct, ci, t) = w\u03f5\u03b8(xt, ct, ci, t) + (1 \u2212 w)\u03f5\u03b8(xt, t)\n(7)\nHere, we simply zero out the CLIP image embedding if the image condition is dropped.\nAs the text cross-attention and image cross-attention are detached, we can also adjust the weight of the image condition\nin the inference stage:\nZnew = Attention(Q, K, V) + \u03bb \u00b7 Attention(Q, K\u2032, V\u2032)\n(8)\nwhere \u03bb is weight factor, and the model becomes the original text-to-image diffusion model if \u03bb = 0.\n4\nExperiments\n4.1\nExperimental Setup\n4.1.1\nTraining Data\nTo train the IP-Adapter, we build a multimodal dataset including about 10 million text-image pairs from two open\nsource datasets - LAION-2B [42] and COYO-700M [43].\n4.1.2\nImplementation Details\nOur experiments are based on SD v1.52, and we use OpenCLIP ViT-H/14 [44] as the image encoder. There are 16\ncross-attention layers in SD model, and we add a new image cross-attention layer for each of these layers. The total\ntrainable parameters of our IP-Adapter including a projection network and adapted modules, amount to about 22M,\nmaking the IP-Adapter quite lightweight. We implement our IP-Adapter with HuggingFace diffusers library [45] and\nemploy DeepSpeed ZeRO-2 [13] for fast training. IP-Adapter is trained on a single machine with 8 V100 GPUs for\n1M steps with a batch size of 8 per GPU. We use the AdamW optimizer [46] with a fixed learning rate of 0.0001 and\nweight decay of 0.01. During training, we resize the shortest side of the image to 512 and then center crop the image\nwith 512 \u00d7 512 resolution. To enable classifier-free guidance, we use a probability of 0.05 to drop text and image\nindividually, and a probability of 0.05 to drop text and image simultaneously. In the inference stage, we adopt DDIM\nsampler with 50 steps, and set the guidance scale to 7.5. When only using image prompt, we set the text prompt to\nempty and \u03bb = 1.0.\n1Note that it is also possible to train the model without text prompt since using image prompt only is informative to guide the\nfinal generation.\n2https://huggingface.co/runwayml/stable-diffusion-v1-5\n6\nFigure 3: The visual comparison of our proposed IP-Adapter with other methods conditioned on different kinds and\nstyles of images.\n7\nTable 1: Quantitative comparison of the proposed IP-Adapter with other methods on COCO validation set. The best\nresults are in bold.\nMethod\nReusable to\ncustom models\nCompatible with\ncontrollable tools\nMultimodal\nprompts\nTrainable\nparameters\nCLIP-T \u2191\nCLIP-I \u2191\nTraining from scratch\nOpen unCLIP\n%\n%\n%\n893M\n0.608\n0.858\nKandinsky-2-1\n%\n%\n%\n1229M\n0.599\n0.855\nVersatile Diffusion\n%\n%\n!\n860M\n0.587\n0.830\nFine-tunining from text-to-image model\nSD Image Variations\n%\n%\n%\n860M\n0.548\n0.760\nSD unCLIP\n%\n%\n%\n870M\n0.584\n0.810\nAdapters\nUni-ControlNet (Global Control)\n!\n!\n!\n47M\n0.506\n0.736\nT2I-Adapter (Style)\n!\n!\n!\n39M\n0.485\n0.648\nControlNet Shuffle\n!\n!\n!\n361M\n0.421\n0.616\nIP-Adapter\n!\n!\n!\n22M\n0.588\n0.828\nFigure 4: The generated images of different diffusion models with our proposed IP-Adapter. The IP-Adapter is only\ntrained once.\n4.2\nComparison with Existing Methods\nTo demonstrate the effectiveness of our method, we compare our IP-Adapter with other existing methods on generation\nwith image prompt. We select three types of methods: training from scratch, fine-tuning from text-to-image model, and\n8\nFigure 5: Visualization of generated samples with image prompt and additional structural conditions. Note that we\ndon\u2019t need fine-tune the IP-Adapter.\nFigure 6: Comparison of our IP-Adapter with other methods on different structural conditions.\nadapters. For the method trained from scratch, we select 3 open source models: open unCLIP1 which is a reproduction\nof DALL-E 2, Kandinsky-2-1 2 which is a mixture of DALL-E 2 and latent diffusion, and Versatile Diffusion [26].\nFor the fine-tuned models, we choose SD Image Variations and SD unCLIP. For the adapters, we compare our IP-\nAdapter with the style-adapter of T2I-Adapter, the global controller of Uni-ControlNet, ControlNet Shuffle, ControlNet\nReference-only and SeeCoder.\n1https://github.com/kakaobrain/karlo\n2https://github.com/ai-forever/Kandinsky-2\n9\nFigure 7: Examples of image-to-image and inpainting with image prompt by our IP-Adapter.\n4.2.1\nQuantitative Comparison\nWe use the validation set of COCO2017 [47] containing 5,000 images with captions for quantitative evaluation. For a\nfair comparison, we generate 4 images conditioned on the image prompt for each sample in the dataset, resulting in\ntotal 20,000 generated images for each method. We use two metrics to evaluate the alignment with the image condition:\n\u2022 CLIP-I: the similarity in CLIP image embedding of generated images with the image prompt.\n\u2022 CLIP-T: the CLIPScore [48] of the generated images with captions of the image prompts.\nWe calculate the average value of the two metrics on all generated images with CLIP ViT-L/141 model. As the open\nsource SeeCoder is used with additional structural controls and ControlNet Reference-only is released under the web\nframework, we only conduct qualitative evaluations. The comparison results are shown in Table 1. As we observe, our\nmethod is much better than other adapters, and is also comparable or even better than the fine-tuned model with only\n22M parameters.\n4.2.2\nQualitative Comparison\nWe also select various kinds and styles of images to qualitatively evaluate our method. For privacy reasons, the images\nwith real face are synthetic. For SeeCoder, we also use the scribble control with ControlNet to generate images. For\nControlNet Reference-only, we also input the captions generated with BLIP caption model [49]. For each image prompt,\nwe random generate 4 samples and select the best one for each method to ensure fairness. As we can see in Figure 3,\nthe proposed IP-Adapter is mostly better than other adapters both in image quality and alignment with the reference\nimage. Moreover, our method is slightly better than the fine-tuned models, and also comparable to the models trained\nfrom scratch in most cases.\nIn conclusion, the proposed IP-Adapter is lightweight and effective method to achieve the generative capability with\nimage prompt for the pretrained text-to-image diffusion models.\n4.3\nMore Results\nAlthough the proposed IP-Adapter is designed to achieve the generation with image prompt, its robust generalization\ncapabilities allow for a broader range of applications. As shown in Table 1, our IP-Adapter is not only reusable to\ncustom models, but also compatible with existing controllable tools and text prompt. In this part, we show more results\nthat our adapter can generate.\n4.3.1\nGeneralizable to Custom Models\nAs we freeze the original diffusion model in the training stage, the IP-Adapter can also be generalizable to the custom\nmodels fine-tuned from SD v1.5 like other adapters (e.g., ControlNet). In other words, once IP-Adapter is trained,\nit can be directly reusable on custom models fine-tuned from the same base model. To validate this, we select three\n1https://huggingface.co/openai/clip-vit-large-patch14\n10\nFigure 8: Generated examples of our IP-Adapter with multimodal prompts.\ncommunity models from HuggingFace model library1: Realistic Vision V4.0, Anything v4, and ReV Animated. These\nmodels are all fine-tuned from SD v1.5. As shown in Figure 4, our IP-Adapter works well on these community\nmodels. Furthermore, the generated images can mix the style of the community models, for example, we can generate\nanime-style images when using the anime-style model Anything v4. Interestingly, our adapter can be directly applied to\nSD v1.4, as SD v1.5 is trained with more steps based on SD v1.4.\n4.3.2\nStructure Control\nFor text-to-image diffusion models, a popular application is that we can create images with additional structure control.\nAs our adapter does not change the original network structure, we found that the IP-Adapter is fully compatible with\nexisting controllable tools. As a result, we can also generate controllable images with image prompt and additional\nconditions. Here, we combine our IP-Adapter with two existing controllable tools, ControlNet and T2I-Adapter. Figure\n5 shows various samples that are generated with image prompt and different structure controls: the samples of the first\ntwo rows are generated with ControlNet models, while the samples in the last row are generated with T2I-Adapters.\nOur adapter effectively works with these tools to produce more controllable images without fine-tuning.\nWe also compare our adapter with other adapters on the structural control generation, the results are shown in Figure\n6. For T2I-Adapter and Uni-ControlNet, we use the default composable multi-conditions. For SeeCoder and our\nIP-Adapter, we use ControlNet to achieve structural control. For ControlNet Shuffle and ControlNet Reference-only,\nwe use multi-ControlNet. As we can see, our method not only outperforms other methods in terms of image quality, but\nalso produces images that better align with the reference image.\n4.3.3\nImage-to-Image and Inpainting\nApart from text-to-image generation, text-to-image diffusion models also can achieve text-guided image-to-image\nand inpainting with SDEdit [50]. As demonstrated in Figure 7, we can also obtain image-guided image-to-image and\ninpainting by simply replacing text prompt with image prompt.\n4.3.4\nMultimodal Prompts\nFor the fully fine-tuned image prompt models, the original text-to-image ability is almost lost. However, with the\nproposed IP-Adapter, we can generate images with multimodal prompts including image prompt and text prompt. We\nfound that this capability performs particularly well on community models. In the inference stage with multimodal\nprompts, we adjust \u03bb to make a balance between image prompt and text prompt. Figure 8 displays various results with\nmultimodal prompts using Realistic Vision V4.0 model. As we can see, we can use additional text prompt to generate\nmore diverse images. For instance, we can edit attributes and change the scene of the subject conditioned on the image\nprompt using simple text descriptions.\n1https://huggingface.co/models\n11\nFigure 9: Comparison with multimodal prompts between our IP-Adapter with other methods.\nWe also compare our IP-Adapter with other methods including Versatile Diffusion, BLIP Diffusion [31], Uni-ControlNet,\nT2I-Adapter, ControlNet Shuffle, and ControlNet Reference-only. The comparison results are shown in Figure 9.\nCompared with other existing methods, our method can generate superior results in both image quality and alignment\nwith multimodal prompts.\n4.4\nAblation Study\n4.4.1\nImportance of Decoupled Cross-Attention\nIn order to verify the effectiveness of the decoupled cross-attention strategy, we also compare a simple adapter without\ndecoupled cross-attention: image features are concatenated with text features, and then embedded into the pretrained\n12\nFigure 10: Comparison results of our IP-Adapter with simple adapter. The decoupled cross-attention strategy is not\nused in the simple adapter.\nFigure 11: The visual difference of generated samples between the IP-Adapter with global features and the IP-Adapter\nwith fine-grained features.\ncross-attention layers. For a fair comparison, we trained both adapters for 200,000 steps with the same configuration.\nFigure 10 provides comparative examples with the IP-Adapter with decoupled cross-attention and the simple adapter.\nAs we can observe, the IP-Adapter not only can generate higher quality images than the simple adapter, but also can\ngenerate more consistent images with image prompts.\n4.4.2\nComparison of Fine-grained Features and Global Features\nSince our IP-Adapter utilizes the global image embedding from the CLIP image encoder, it may lose some information\nfrom the reference image. Therefore, we design an IP-Adapter conditioned on fine-grained features. First, we extract\nthe grid features of the penultimate layer from the CLIP image encoder. Then, a small query network is used to learn\nfeatures. Specifically, 16 learnable tokens are defined to extract information from the grid features using a lightweight\ntransformer model. The token features from the query network serve as input to the cross-attention layers.\nThe results of the two adapters are shown in Figure 11. Although the IP-Adapter with finer-grained features can generate\nmore consistent images with image prompt, it can also learn the spatial structure information, which may reduce the\ndiversity of generated images. However, additional conditions, such as text prompt and structure map, can be combined\nwith image prompt to generate more diverse images. For instance, we can synthesize novel images with the guidance of\nadditional human poses.\n13\n5\nConclusions and Future Work\nIn this work, we propose IP-Adapter to achieve image prompt capability for the pretrained text-to-image diffusion\nmodels. The core design of our IP-Adapter is based on a decoupled cross-attention strategy, which incorporates separate\ncross-attention layers for image features. Both quantitative and qualitative experimental results demonstrate that our\nIP-Adapter with only 22M parameters performs comparably or even better than some fully fine-tuned image prompt\nmodels and existing adapters. Furthermore, our IP-Adapter, after being trained only once, can be directly integrated\nwith custom models derived from the same base model and existing structural controllable tools, thereby expanding\nits applicability. More importantly, image prompt can be combined with text prompt to achieve multimodal image\ngeneration.\nDespite the effectiveness of our IP-Adapter, it can only generate images that resemble the reference images in content\nand style. In other words, it cannot synthesize images that are highly consistent with the subject of a given image\nlike some existing methods, e.g., Textual Inversion [51] and DreamBooth [52]. In the future, we aim to develop more\npowerful image prompt adapters to enhance consistency.\nReferences\n[1] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\narXiv preprint arXiv:2112.10741, 2021.\n[2] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10684\u201310695, 2022.\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers.\narXiv preprint arXiv:2211.01324, 2022.\n[6] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-\nimage generation via large mixture of diffusion paths. arXiv preprint arXiv:2305.18295, 2023.\n[7] Sam Witteveen and Martin Andrews. Investigating prompt engineering in diffusion models. arXiv preprint\narXiv:2211.15462, 2022.\n[8] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[9] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\n[10] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free diffusion:\nTaking\" text\" out of text-to-image diffusion models. arXiv preprint arXiv:2305.16223, 2023.\n[11] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:\nLearning adapters to dig out more controllable ability for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023.\n[12] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong.\nUni-controlnet: All-in-one control to text-to-image diffusion models. arXiv preprint arXiv:2305.16322, 2023.\n[13] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u2013\n8831. PMLR, 2021.\n[14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural\nInformation Processing Systems, 34:19822\u201319835, 2021.\n14\n[15] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902, 2022.\n[16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages\n89\u2013106. Springer, 2022.\n[17] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information\nprocessing systems, 30, 2017.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[19] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\nYinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation.\narXiv preprint arXiv:2206.10789, 2022.\n[20] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR,\n2015.\n[21] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[22] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,\n2020.\n[23] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural\ninformation processing systems, 34:8780\u20138794, 2021.\n[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research, 21(1):5485\u20135551, 2020.\n[25] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-\nimage generator. arXiv preprint arXiv:2209.14491, 2022.\n[26] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and\nvariations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022.\n[27] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable\nimage synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023.\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Out-\nrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538,\n2017.\n[29] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\n[30] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\nMona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on\nMachine Learning, pages 2790\u20132799. PMLR, 2019.\n[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[32] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n[33] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199,\n2023.\n[34] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010,\n2023.\n[35] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\nWhat matters in training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469,\n2023.\n15\n[36] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds.\narXiv preprint arXiv:2202.09778, 2022.\n[37] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for\ndiffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems,\n35:5775\u20135787, 2022.\n[38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for\nguided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.\n[39] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image seg-\nmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\n[41] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294,\n2022.\n[43] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:\nImage-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.\n[44] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\nOpenclip. https://github.com/mlfoundations/open_clip, 2021.\n[45] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers: State-of-the-art diffusion models.\nhttps://github.com/huggingface/\ndiffusers, 2022.\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755.\nSpringer, 2014.\n[48] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n[49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022.\n[50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\nGuided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\n[51] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint\narXiv:2208.01618, 2022.\n[52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.\n16\n"
  },
  {
    "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
    "link": "https://arxiv.org/pdf/2308.07317.pdf",
    "upvote": "21",
    "text": "Platypus: Quick, Cheap, and Powerful\nRefinement of LLMs\nAriel N. Lee\u2217\nBoston University\nariellee@bu.edu\nCole J. Hunter\u2217\nBoston University\ncolejh@bu.edu\nNataniel Ruiz\u2020\nBoston University\nnruiz9@bu.edu\nAbstract\nWe present Platypus, a family of fine-tuned and merged Large Language Models\n(LLMs) that achieved the strongest performance and stood at first place in Hugging-\nFace\u2019s Open LLM Leaderboard * at the time of writing. In this work we describe\n(1) our curated dataset Open-Platypus, that is a subset of other open datasets and\nwhich we release to the public (2) our process of fine-tuning and merging LoRA\nmodules in order to conserve the strong prior of pretrained LLMs, while bringing\nspecific domain knowledge to the surface (3) our efforts in checking for test data\nleaks and contamination in the training data, which can inform future research.\nSpecifically, the Platypus family achieves strong performance in quantitative LLM\nmetrics across model sizes, topping the global Open LLM leaderboard while using\njust a fraction of the fine-tuning data and overall compute that are required for\nother state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be\ntrained on a single A100 GPU using 25k questions in 5 hours. This is a testament\nof the quality of our Open-Platypus dataset, and opens opportunities for more\nimprovements in the field. Project page: https://platypus-llm.github.io\n*Equal Contribution.\n\u2020NR is currently at Google and his contributions were done as work at BU prior to his tenure at the company.\n*https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\nWorkshop on Instruction Tuning and Instruction Following at NeurIPS 2023.\narXiv:2308.07317v2  [cs.CL]  14 Mar 2024\n1\nIntroduction\nOur research centers around improving the performance of base Large Language Models (LLMs) by\nfine-tuning models using parameter efficient tuning (PEFT) on a small, yet powerful, curated dataset\nOpen-Platypus. This work lives in the context of recent advancements in the domain of LLMs. The\nrapid growth of these models was kick-started by the emergence of scaling laws [19]. Soon after,\n100B+ parameter models like PaLM [6] and GPT-3 [3] were proposed. Task specific models came\nnext, such as Galactica for scientific tasks [38]. Chinchillia [16] was introduced along with a novel\nscaling law approach that shifts the emphasis from model size to the number of processed tokens.\nTo challenge the dominance of closed source models like OpenAI\u2019s GPT-3.5 and GPT-4, Meta\nreleased the original LLaMa models [39], now known for their computational efficiency during\ninference. Open-source initiatives such as BLOOM [33] and Falcon [2] have also been released to\nchallenge the hegemony of their closed-source counterparts. Recently, Meta AI released LLaMa-2\nmodels [40]. Shortly after the initial release the 70B parameter model was fine-tuned by StabilityAI\nto create StableBeluga2 [26] using an Orca-style dataset [28]. As the the scale of both network\narchitectures and training datasets have grown, the push towards employing LLMs as generalist tools\nable to handle a wide array of tasks has intensified. For the largest models, their abilities as generalists\nmake them well-suited for many NLP tasks [29], with smaller models struggling to maintain the\nsame level of versatility.\nA number of strategies have been employed to try and bridge this divide. A prominent method known\nas knowledge distillation [17, 15, 46] aims to transfer knowledge from a large, more performant\nteacher model to a smaller student model, preserving performance while reducing computational\noverhead. Recently, the most popular method involves distilling the knowledge from a large train-\ning dataset into a small one, again making it less computationally expensive than traditional ap-\nproaches [48]. These methods also tend to take advantage of instruction tuning [43], which has proven\nan effective method for improving the general performance of LLMs. Projects like Stanford\u2019s Al-\npaca [37] and WizardLM [47] provide frameworks for generating high-quality, instruction formatted\ndata. Fine-tuning base models on these types of datasets and applying self-instruct methodology [42]\nhas led to marked improvements in both their quantitative and qualitative performance [7].\nThe Mixture of Experts approach [35, 34] employs conditional computation, activating network\nsections based on individual examples. This technique boosts model capacity without a linear rise in\ncomputation. Sparse variants, like the Switch Transformer [11], activate select experts per token or\nexample, introducing network sparsity. Such models excel in scalability across domains and retention\nin continual learning, as seen with Expert Gate [1]. Yet, ineffective expert routing can result in\nunder-training and uneven specialization of experts.\nFollowing the recent arrival of LoRA is QuantizedLoRA (QLoRA) [8], which has been recognized\nas an efficient and cost-effective methodology. The authors of [8] concurrently released Guanaco,\na new model family. The best Guanaco models currently rank 7th and 12th on the Hugging Face\nleaderboard as of the time of writing. Notwithstanding, our initial decision to employ LoRA occurred\nbefore the release of QLoRA, and we stuck with it since it proved effective within our existing\nworkflow\u2014namely being compatible and successful at model merging. Since our future goals include\nreducing training time and cost, we would be excited to use quantized LoRA in our pipeline and\ncompare results.\nOther approaches have centered on training LLMs in specific tasks such as coding [25], quantitative\nreasoning [22], and biomedical knowledge [36]. This specialized training has its own merits. By\nfocusing on narrower domains, these models can achieve higher accuracy rates and more relevant\noutput in their respective fields.\nOne large limitation of this approach, especially for domain-specific models derived from large,\npre-trained ones, is that the fine-tuning process can be time-consuming and costly. Our work seeks\nto address these issues by focusing on refining a training recipe aimed to maintain the benefits of\ninstruction tuning, namely generalized improvement, while also imparting specific domain knowledge.\nWe find that domain specific datasets increase performance on a selected category of tasks, which\nwhen combined with merging significantly reduces training time. Our core contributions are as\nfollows:\n2\n\u2022 Open-Platypus, a small-scale dataset that consists of a curated sub-selection of public text\ndatasets. The dataset is focused on improving LLMs\u2019 STEM and logic knowledge, and is\nmade up of 11 open-source datasets. It is comprised mainly of human-designed questions,\nwith only \u223c10% of questions generated by an LLM. The main advantage of Open-Platypus\nis that, given its size and quality, it allows for very strong performance with short and cheap\nfine-tuning time and cost. Specifically, one can train their own 13B model on a single A100\nGPU using 25k questions in 5 hours.\n\u2022 A description of our process of similarity exclusion in order to reduce the size of our dataset,\nas well as reduce data redundancy.\n\u2022 A detailed look into the ever-present phenomenon of contamination of open LLM training\nsets with data contained in important LLM test sets, and a description of our training data\nfiltering process in order to avoid this pitfall.\n\u2022 A description of our selection and merging process for our specialized fine-tuned LoRA\nmodules.\n2\nMethods\n2.1\nCurating Open-Platypus\nOur decisions regarding data selection for fine-tuning the LLaMa-2 models were influenced by (1)\nthe Superficial Alignment Hypothesis presented by [50], which states that model knowledge is almost\nentirely learned during pre-training, and that with minimal training data it is possible to achieve\nexcellent results aligning model outputs; (2) the LLaMa2 introductory paper in which [40] state that\nthe base models had not yet reached saturation; and (3) the work of [12], highlighting the importance\nof high-quality input data for training effective models. Put into practice, and keeping in mind our\ngoal of optimizing training time and model performance, our approach to fine-tuning the LLaMa-2\nmodels was a balanced blend of the three points above. By focusing on depth in specific areas,\ndiversity of input prompts, and keeping the size of the training set small, we aimed to maximize\nthe precision and relevance of our models\u2019 outputs. To achieve this, we curated a content filtered,\ninstruction tuned dataset which draws from a variety of open-source datasets. In this context, \u2019content\nfiltered\u2019 refers to our choice for the train set to almost exclusively include data which is related to our\ndomain of interest, namely STEM.\nOpen-Platypus is made up of 11 open-source datasets, detailed in Table 1. It is comprised mainly of\nhuman-designed questions, with \u223c10% of questions generated by an LLM. Given our focus on STEM\nand logic, we primarily pulled from datasets geared towards those subjects, supplementing them\nwith keyword-filtered content from datasets with a broader subject coverage, namely Openassistant-\nGuanaco [8] and airoboros [9]. The backbone of Open-Platypus is a modified version of MATH [14]\nthat has been supplemented with expanded step-by-step solutions from PRM800K [23].\nWe employed the Alpaca instruction-tuning format, wherein each question is structured with an\ninstruction, input, and output. In many cases the input is empty. However, for some datasets consisting\nof multiple choice questions, specifically ARB [32] and ReClor [49], we integrated the formatting\ncontext {Choose A, B, C, or D} as input for each question. For ScienceQA [24], we opted to\ninclude long-form answers to the multiple choice questions, omitting an explicit statement of the\ncorrect choice entirely.\n2.2\nRemoving similar & duplicate questions\nHaving collected data from a number of sources, we then ran it through a de-duplication process\nto minimize the chances of memorization [21]. First, we removed all instructions which were\nword-for-word duplicates, followed by removal of instructions which had 80% cosine similarity with\nthe SentenceTransformers [30] embeddings of other instructions in our train set. In both cases, we\ndefaulted to keeping the question-answer pair which had the more verbose answer. Our motivation\nbehind this was that longer answers likely translate to more detailed explanations and/or step-by-step\nsolutions.\n3\nTable 1: Datasets, Licenses, and Number of Leaked Questions. With respect to Open-Platypus,\nafter using keyword searches to filter for STEM and logic, we removed any training questions with\nsimilarity > 80% to any test set question. *The datasets marked with asterisks were not added to\nOpen-Platypus but we include them because we ran contamination checks when considering which\nmodels to merge.\nDataset Name\nLicense Type\n# Leaked Questions\nPRM800K: A Process Supervision\nDataset [23]\nMIT\n77\nMeasuring Mathematical Problem\nSolving With the MATH Dataset [14]\nMIT\n77\nScienceQA: Science Question\nAnswering [24]\nCreative Commons Attribution-\nNonCommercial-ShareAlike 4.0\n0\nSciBench: Evaluating College-Level\nScientific Problem-Solving Abilities\nof Large Language Models [41]\nMIT\n0\nReClor: A Reading Comprehension\nDataset Requiring Logical\nReasoning[49]\nNon-commercial\n0\n*SciQ: Crowdsourcing Multiple\nChoice Science Questions [44]\nCreative Commons\nAttribution-NonCommercial 3.0\n71\nTheoremQA: A Theorem-driven\nQuestion Answering Dataset [5]\nMIT\n0\nleetcode-solutions-python\n-testgen-gpt4 [20]\nNone listed\n0\nairoboros-gpt4-1.4.1 [9]\nother\n13\ntigerbot-kaggle\n-leetcodesolutions-en-2k[31]\napache-2.0\n0\nARB: Advanced Reasoning\nBenchmark for Large Language\nModels [32]\nMIT\n0\nOpenassistant-guanaco [8]\napache-2.0\n13\n*ehartford/dolphin (first 25k\nrows) [10]\napache-2.0\n0\n2.3\nContamination Check\nA core component of our methodology revolves around ensuring that none of the benchmark test\nquestions inadvertently leak into the training set, which is a fairly common occurrence. We seek to try\nand prevent memorization of test data skewing the benchmark results. With that in mind, we did allow\nfor some leniency in determining whether questions should be marked as duplicates and removed\nfrom the training set. Allowing some flexibility in identifying suspect questions acknowledges that\nthere are multiple ways to phrase a query, and general domain knowledge might prevent a question\nfrom being considered duplicate.\nTo that end, we developed the following heuristics to guide manual filtering of questions from\nOpen-Platypus that scored > 80% similarity to any benchmark questions. We categorize potential\nleaks into three groups: duplicate, gray-area, and similar but different. For our purposes, we err on\nthe side of caution and remove all of them from our train set.\nDuplicate\nQuestions marked as duplicate contamination are essentially exact copies of questions\nfound in the test sets. This includes training questions with an extra word or minor rearrangement in\nrelation to a benchmark question. Duplicate contamination is the only category we count as \"true\"\ncontamination and corresponds to the number of leaked questions listed in Table 1. Specific examples\nof this can be seen in Figure 1.\nGray-area\nThe next group, termed gray-area, encompasses questions that are not exact duplicates\nand fall within the realm of general knowledge. While we leave the final judgement of these questions\nto the open-source community, we believe they often necessitate expert knowledge. Notably, this\ncategory includes questions with identical instructions but answers that are synonymous (for instance,\n4\n\"skin\" versus \"epidermis\" as illustrated in 2). It also covers questions that, though phrased differently,\nhave their precise or detailed answers present in the training output.\nSimilar but different\nThe final category includes questions that, despite having high cosine\nsimilarity scores, yield very different answers. This can often be attributed to subtle changes in the\nquestion\u2019s structure, resulting in completely distinct responses. A great example of this is the question\nin Figure 3, where modifications to the rotation axis and quadrant definition significantly alter the\nfinal answer.\nTrain\nTest\nHow many edges does a complete\ngraph with 10 vertices have?\nA complete graph with n vertices has n*(n-\n1)/2 edges. So, a complete graph with 10\nvertices has 10*(10-1)/2 = 10*9/2 = 45\nedges.\nA discrete graph is complete if there\nis an edge connecting any pair of ver-\ntices. How many edges does a com-\nplete graph with 10 vertices have?\nD: 45\nFigure 1: Comparison of train and test questions in the duplicate contamination group.\nTrain\nTest\nWhat is the largest organ of the hu-\nman body?:\nA: kidneys\nB: the heart\nC: epidermis\nD: liver\nC: epidermis\nWhat is the largest organ in the hu-\nman body?\nA: stomach\nB: brain\nC: skin\nD: liver\nC: skin\nFigure 2: Comparison of train and test questions in the gray-area.\n2.4\nFine-tuning & merging\nAfter refining the dataset and triple checking for contamination, our methodology centers on two main\npoints: the effectiveness of Low Rank Approximation (LoRA) [18] training and the built-in model\nmerging capabilities of the State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) library [27].\nDifferent from full fine-tuning methods, LoRA freezes pre-trained model weights and adds rank\ndecomposition matrices into each layer of the transformer. This reduces the number of trainable\nparameters for downstream tasks and by extension, the time and cost of training. For example, our\n13B model was fine-tuned using 1 A100 80GB for 5 hours and our 70B model using 4 A100s 80GB\nfor 22 hours. As a benchmark for comparison, Stanford notes that their full fine-tune of Alpaca-7B\ntook 3 hours on 8 A100s 80GB. In addition to PEFT and LoRA, we fine-tuned our models using the\nHugging Face transformers library [45]. As previously mentioned, we utilized Stanford Alpaca\u2019s\nprompt formatting template [37], which can found in the Appendix.\nOur initial attempts at fine-tuning the models focused on the attention modules v_proj, q_proj,\nk_proj, and o_proj. We later moved onto the gate_proj, down_proj, and up_proj modules as\nrecommended by [13], due to their analysis showing superior performance compared to the attention\nmodules, with the exception being situations where the trainable parameters are a tiny fraction\n(< 0.1%) of total parameters. For consistency, we adopted this strategy for both the 13 and 70 billion\nparameter fine-tunes, which translated to 0.27% and 0.2% trainable parameters, respectively. Please\nsee the full list of hyperparameters in Table 8. The only difference between our 13B and 70B models\nis the initial learning rate\u2014we had to lower the initial learning rate for the 70B model from 4e-4 to\n3e-4 because the loss went to zero after 15 steps. LoRA rank defines the dimensions of the low-rank\n5\nTrain\nTest\nThe region $\\mathscr{R}$ enclosed\nby the curves $y=x$ and $y=x^2$ is\nrotated about the $x$-axis. Find the\nvolume of the resulting solid.\nThe curves $y=x$ and $y=x^2$ intersect\nat the points $(0,0)$ and $(1,1)$.\nThe region between them, the solid of\nrotation, and a cross-section perpendicular\nto the $x$-axis are shown in Figure. A\ncross-section in the plane $P_x$ has\nthe shape of a washer (an annular ring)\nwith inner radius $x^2$ and outer radius\n$x$, so we find the cross-sectional area\nby subtracting the area of the inner\ncircle from the area of the outer circle:\n\\r\\n$$\\r\\nA(x)=\\\\pi x^2-\\\\pi\n\\\\left(x^2\\\\right)^2= \\\\pi\n\\\\left(x^2-x^4\\\\right)\\r\\n$$\\r\n\\n\nTherefore\nwe\nhave\n\\r\\n$$\n\\r\\n\\\\begin{aligned} \\r\\nV &\n=\\\\int_0^1 A(x) d x=\\\\int_0^1\n\\\\pi\\\\left(x^2-x^4\\\\right)\nd x \\\\\\\\\\r\\n& =\\\\pi\n\\\\left[\\\\frac{x^3}{3}\n- \\\\frac{x^5}{5}\n\\\\right]_0^1 = \\\\frac{2\n\\\\pi}{15}\\r\\n\\\\end{aligned}\\r\\n$$\nThe region bounded by the curves y\n= x and y = x^2 in the first quadrant\nof the xy-plane is rotated about the\ny-axis. The volume of the resulting\nsolid of revolution is\nB: pi / 6\nFigure 3: Comparison of train and test questions with high cosine similarity scores which are actually\nquite different.\nmatrices, and LoRA alpha is the scaling factor for the weight matrices. The weight matrix is scaled\nby lora_alpha\nlora_rank , and a higher alpha value assigns more weight to the LoRA activations. We chose 16\nsince this was common practice in training scripts we reviewed and chose a 1:1 ratio so as not to\noverpower the base model.\nAfter reviewing the datasets in Table 1, we deliberately chose not to merge with any models trained\nusing contaminated datasets. For example, we merged with the new Dolphin-70B LLM after\nconfirming no test questions had leaked into the training set. We performed contamination checks on\ndatasets used to train models we merged with to the best of our abilities, but some datasets have not\nbeen publicly released. While we cannot offer absolute assurances for any merged models with closed-\nsource datasets, we proceed giving the benefit of the doubt. Additional details regarding merging\nconsiderations are included in the next section, as this is dependent on the fine-tune benchmark\nresults.\n3\nResults\nIn this section, we present a detailed analysis of our models\u2019 performance, bench-marking them\nagainst other state-of-the-art models. Our primary objective was to discern the effects of merging\nboth broad and niche models and to assess the advantages of fine-tuning on our dataset. Moving\nforward, base model refers to the model on which the LoRA adapters are merged.\nPer the Hugging Face Open LLM Leaderboard data from 8/10/23 (Table 2), our Platypus2-70B-\ninstruct variant has outperformed its competitors, securing the top position with an average score of\n73.13. Notably, our Stable-Platypus2-13B model, as shown in Table 3, stands out as the premier 13\nbillion parameter model with an average score of 63.96.\n6\nTable 2: Top 15 open-source models on 8/10/23, including GPT-4 and GPT-3.5, according to the\nHugging Face Open LLM leaderboard. Please note that GPT-4 and GPT-3.5 are not part of the official\nleaderboard but we have added their benchmark results for a closed-source model comparison. Our\nmodels are in 1st, 5th, 11th, and 15th. ARC-challenge is 25-shot, HellaSwag is 10-shot, MMLU is\n5-shot, and TruthfulQA is 0-shot. *Note: Camel-Platypus2-70B is currently pending evaluation on\nthe leaderboard, so we have included our local benchmark results instead.\nModel\nAvg.\nScores (%)\nARC HellaSwag MMLU TruthfulQA\ngpt-4\n84.3\n96.3\n95.3\n86.4\n59\n1. Platypus2-70B-instruct\n73.13 71.84\n87.94\n70.48\n62.26\n2. upstage/Llama-2-70b-instruct-v2\n72.95 71.08\n87.89\n70.58\n62.25\n3. psmathur/model_007\n72.72 71.08\n87.65\n69.04\n63.12\n4. upstage/Llama-2-70b-instruct\n72.29\n70.9\n87.48\n69.8\n60.97\ngpt-3.5\n71.9\n85.2\n85.5\n70\n47\n5. *Camel-Platypus2-70B\n71.60 71.16\n87.66\n69.80\n57.77\n6. stabilityai/StableBeluga2\n71.42 71.08\n86.37\n68.79\n59.44\n7. quantumaikr/llama-2-70b-fb16\n-guanaco-1k\n71.41 70.48\n87.33\n70.25\n57.56\n8. augtoma/qCammel-70-x\n70.97 68.34\n87.87\n70.18\n57.47\n9. jondurbin/airoboros-l2-70b-gpt4-1.4.1\n70.93 70.39\n87.82\n70.31\n55.2\n10. dfurman/llama-2-70b-dolphin-peft\n70.76 69.62\n86.82\n69.18\n57.43\n11. Dolphin-Platypus2-70B\n70.69 70.39\n86.7\n69.04\n56.65\n12. TheBloke/llama-2-70b-Guanaco-\nQLoRA-fp16\n70.63 68.26\n88.32\n70.23\n55.69\n13. psmathur/model_420\n70.55 70.14\n87.73\n70.35\n54\n14. psmathur/model_51\n70.41 68.43\n86.71\n69.31\n57.18\n15. Platypus2-70B\n70.06 70.65\n87.15\n70.08\n52.37\nThe objective of our model merging strategy is to assess the synergistic effects of integrating\nwith broad models like Instruct and Beluga, or specialized models such as Camel. An interesting\nobservation was with the Dolphin merge, where instead of using the conventional Platypus adapters,\nwe opted for the exported Platypus merged with the base LLaMa-2. This decision was influenced\nby our contamination check experiments of the Dolphin dataset. Dolphin-Platypus2-70-B is the\nonly merge that did not do better than both the base and adapter models. Additionally, there was\na smaller score discrepancy between the base Platypus and Dolphin models than the other models\nbeing discussed. This led us back to Camel, which had previously shown promising results in our\ninitial tests using 13B.\nPost fine-tuning, both the 13B and 70B models demonstrated marked improvements over the base\nLLaMa-2 models, particularly in the ARC and TruthfulQA benchmarks. This prompted us to explore\nthe potential of merging with other fine-tuned variants. While the 70B merges showed marginal\nvariations from the baseline scores, the 13B merges, especially with Stable Beluga, displayed\nsignificant enhancements. For instance, the merge with Stable Beluga outperformed its constituent\nmodels by at least 0.5% across most benchmarks, with a notable 2.91% increase in TruthfulQA.\nAdditionally, Stable-Platypus2-13B also showed an overall increase of +1.05% jump over base model.\nGiven that TruthfulQA questions are primarily \"knowledge\" questions (as opposed to \"reasoning\"\nquestions), the consistent improvement in TruthfulQA scores across merges suggests that merging\nmodels effectively broadens the knowledge base rather than enhancing reasoning capabilities. This\nobservation aligns with the nature of TruthfulQA questions, which are primarily knowledge-based.\nThe LLaMa-2 paper\u2019s assertion that model saturation hasn\u2019t been reached further supports the idea\nthat merging can introduce \"new\" information to the model [40].\n7\nTable 3: Top 13b open-source models according to the Hugging Face leaderboard on 8/10/23. These\nrankings are for 13b parameter models only. Our models are 1st, 7th, and 20th. ARC-challenge is\n25-shot, HellaSwag is 10-shot, MMLU is 5-shot, and TruthfulQA is 0-shot.\nModel\nAvg.\nScores (%)\nARC HellaSwag MMLU TruthfulQA\n1. Stable-Platypus2-13B\n63.96 62.71\n82.29\n58.3\n52.52\n2. Open-Orca/OpenOrcaxOpenChat-\nPreview2-13B\n63.83 62.54\n82.96\n58.65\n51.17\n3. psmathur/orca_mini_v3_13b\n63.45 63.14\n82.35\n56.52\n51.81\n4. Gryphe/MythoMix-L2-13b\n63.11 61.09\n83.86\n55.42\n52.08\n5. stabilityai/StableBeluga-13B\n62.91 62.03\n82.27\n57.71\n49.61\n6. The-Face-Of-Goonery/Huginn-13b\n-FP16\n62.82 60.58\n82.53\n53.71\n54.46\n7. Camel-Platypus2-13B\n62.62 60.75\n83.61\n56.51\n49.6\n...\n...\n...\n...\n...\n...\n13. augtoma/qCammel-13B\n62.19 60.84\n83.66\n56.73\n47.54\n...\n...\n...\n...\n...\n...\n20. Platypus2-13B\n61.35 61.26\n82.56\n56.7\n44.86\nThe results underscore the potential of model merging as a strategy to enhance performance. The\nchoice of models for merging, whether broad or focused, plays a pivotal role in determining the\noutcome. Our experiments with Dolphin, for instance, underscore the importance of iterative testing\nand model selection. The consistent performance of models like Camel-Platypus2-70B across\ndifferent benchmarks further emphasizes this point.\nIn the ARC-Challenge, Hellaswag, and TruthfulQA tests, the Camel-Platypus2-70B model exhibited\nthe most significant positive change with a +4.12% improvement in ARC-challenge. This suggests\nthat the Camel-Platypus2-70B model, when merged with the Platypus adapter, is potentially the most\neffective combination for tasks related to the ARC-Challenge.\nFor the MMLU tests, the results were more varied. The Platypus2-70B-instruct model displayed\na remarkable +18.18% improvement in abstract_algebra, while the Camel-Platypus2-13B model\nshowed a decline of -15.62%. This indicates that the effectiveness of the merge varies depending\non the specific domain of the test. Notably, in machine_learning, the Camel-Platypus2-70B model\ndemonstrated a significant increase of +26.32%, reinforcing the potential of this model in specific\ndomains.\nDrawing from the broader content of our paper, these results underscore the importance of selecting\nthe appropriate model for merging with the Platypus adapter. The performance enhancements or\ndeclines are not uniform across all domains, emphasizing the need for domain-specific evaluations\nbefore finalizing a merge.\n3.1\nDeep dive into the benchmark metric tasks\nThe Appendix contains a breakdown of each MMLU task by change in percent and percent change.\nThe rest of this discussion will be referencing percent change, but we include both for transparency.\nA deeper dive into the performance metrics of the base models revealed that two models with very\nsimilar scores do not necessarily merge into a superior model.\nARC-Challenge, Hellaswag, TruthfulQA-MC: Table 4\n8\n\u2022 Most Notable Improvement: The Camel-Platypus2-70B model in the ARC-challenge test\nexhibited the highest positive change with a +4.12% improvement. This indicates that for\ntasks related to the ARC-Challenge, the Camel-Platypus2-70B model, when merged with\nthe Platypus adapter, is potentially the most effective.\n\u2022 Consistent Performer: The Stable-Platypus2-13B model showed consistent positive changes\nacross all three tests compared to the base model, indicating its reliable performance when\nmerged with the Platypus adapter.\n\u2022 Variability in Results: The results for TruthfulQA were particularly varied, with the Stable-\nPlatypus2-13B model showing a significant +5.87% improvement, while the Dolphin-\nPlatypus2-70B model showed a decline of -1.37%.\nMMLU: Table 6)\n\u2022 Standout Performance: In the machine_learning test, the Camel-Platypus2-70B model\ndisplayed a remarkable +26.32% improvement, indicating its potential effectiveness in\nmachine learning domains when merged with the Platypus adapter.\n\u2022 Diverse Results: The results for the formal_logic test were diverse, with the Stable-Platypus2-\n13B model showing a significant +27.27% improvement, while the Camel-Platypus2-13B\nmodel showed a decline of -2.13%.\n\u2022 Consistent Domains: In domains like marketing, the changes across all models were minimal,\nsuggesting that the impact of merging with the Platypus adapter might be limited in certain\ndomains.\n\u2022 Significant Declines: The college_physics test showed significant declines for the Platypus2-\n70B-instruct, Dolphin-Platypus2-70B, and Camel-Platypus2-70B models, with changes of\n-20.93%, -13.16%, and -18.42% respectively. This indicates potential compatibility issues\nor inefficiencies when these models are merged with the Platypus adapter for tasks related\nto college physics.\nThe tables provide a comprehensive view of how different models perform when merged with\nthe Platypus adapter across various domains. It\u2019s evident that the effectiveness of the merge is\ndomain-specific, and there\u2019s no one-size-fits-all solution. Researchers and practitioners should\ncarefully evaluate the performance enhancements or declines in their specific domain of interest\nbefore finalizing a merge.\n4\nBroader Impacts & Future Work\nModern LLMs often require considerable computational resources, making their training and infer-\nence costs restrictive for those with limited budgets. While techniques like quantization and LoRA\nprovide some relief, a notable observation from the Hugging Face leaderboard is the success of\nsmaller models in specific tasks, such as role-playing and question answering. It may be strategic\nto harness the efficiency of these compact models and merge them with the precision of individual\nadapters. In that ecosystem, the similarity between inputs and training data is used as an a posteriori\nfactor, biasing the outputs to be informed by similar data. This method essentially exploits the corre-\nlation between inputs and their similar training data to influence outputs. Mixture of Experts (MoEs)\npresents a promising avenue for further enhancing accuracy, given the success of domain-specific\ntraining. Future exploration could also involve integrating alpaca and orca-style datasets, as well as\nexamining the potential of QLoRA within our pipeline.\nBuilding on this perspective, LIMA [50] suggests a future characterized by an array of small,\nmeticulously curated datasets for niche domains. The advantages of this approach are evident:\nstreamlined fine-tuning processes and rapid cosine similarity searches across average training inputs\nof adapters.\nAn intriguing inquiry is the applicability of the LIMA strategy within the LoRA and PEFT landscapes.\nThis question warrants further investigation in subsequent studies. Future work might delve deeper\ninto understanding the nuances of model merging, especially in the context of models with similar\nbaseline scores. The potential of leveraging models like Lazarus, a successful LoRA merge of 6\nmodels [4], could also be explored.\n9\n5\nLimitations\nPlatypus, being a fine-tuned variant of LLaMa-2, inherits many of the base model\u2019s limitations while\nintroducing some unique challenges due to its specialized training. Like LLaMa-2, Platypus does\nnot receive continuous knowledge updates after its pretraining and fine-tuning phases. This static\nknowledge base can lead to outdated or incomplete information over time. Furthermore, there remains\na risk of Platypus generating non-factual content or unqualified advice, especially when faced with\nambiguous or misleading prompts.\nWhile Platypus has been fine-tuned to improve its proficiency in STEM and logic, its primary focus,\nlike LLaMa-2, has been on English-language data. Although it might exhibit some capability in\nother languages, this proficiency is not guaranteed and can be inconsistent due to limited non-\nEnglish pretraining data. Additionally, like its predecessor, Platypus can generate potentially harmful,\noffensive, or biased content, especially when trained on publicly available datasets. While efforts\nhave been made to address these issues through data cleaning, challenges persist, especially for\nnon-English languages where comprehensive datasets might be lacking.\nThe capabilities of Platypus, like other AI models, can be misused for malicious purposes, such as\nspreading misinformation or probing sensitive topics. While our model is for non-commercial use\nonly due to the license of the training set, we have followed Meta\u2019s Responsible Use Guide with\nrespect to fine-tuning. We have not done any adversarial attack testing or read teaming, so before\ndeploying any applications of Platypus, developers should perform safety testing and tuning tailored\nto their specific applications of the model.\nDue to its specialized training, particularly in STEM and logic questions, Platypus might exhibit\nlimitations when faced with topics outside its primary domain of expertise. Please exercise cau-\ntion\u2014it\u2019s essential to adhere to guidelines for responsible use and consider additional fine-tuning and\ndeployment measures to ensure optimal and safe performance.\nAny users of the Platypus family should ensure that there is no contamination between the Platypus\ntraining data and any benchmark test sets not explicitly used in this paper. For example, the creators\nof PRM800K combined the MATH train and test sets to increase training quality. We used both the\ntrain and test sets of PRM800K during training, barring any questions that were too similar to the\nbenchmark datasets.\nAll aforementioned limitations pertain to our merged model variants. Again, we deliberately chose\nnot to merge with any models that used contaminated datasets during training. While we cannot offer\nabsolute assurances, we proceed giving the benefit of the doubt. We\u2019d like to stress the importance of\ndue diligence when choosing to deploy any LLM or dataset.\nLastly, we note that keyword search and cosine similarity of sentence embeddings may not be\nexhaustive filtering methods. While we are confident there is no contamination in our cleaned training\ndata, it is unlikely but not impossible that some questions slipped through the cracks.\nAcknowledgments\nA very special thank you to both Hugging Face, for creating a space where anyone can evaluate and\nrelease LLMs, and Meta AI for sharing LLaMa-2, the backbone of our fine-tuned models. We would\nalso like to thank the creators of LoRA, without whom we could not have afforded to fine-tune a 70B\nvariant of LLaMa-2.\nReferences\n[1] R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of\nexperts. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages\n7120\u20137129, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.\n[2] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet,\nD. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo. Falcon-40b: an open\nlarge language model with state-of-the-art performance, 2023.\n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\n10\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners, 2020.\n[4] CalderaAI. 30b-lazarus. https://huggingface.co/CalderaAI/30B-Lazarus, 2023.\n[5] W. Chen, M. Yin, M. Ku, E. Wan, X. Ma, J. Xu, T. Xia, X. Wang, and P. Lu. Theoremqa: A\ntheorem-driven question answering dataset. arXiv preprint arXiv: 2305.12524, 2023.\n[6] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,\nX. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,\nS. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.\n[7] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani,\nS. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros,\nM. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai,\nH. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei.\nScaling instruction-finetuned language models. arXiv: 2210.11416, 2022.\n[8] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of\nquantized llms. arXiv preprint arXiv: 2305.14314, 2023.\n[9] J.\nDurbin.\nairoboros-gpt4-1.4.1.\nhttps://huggingface.co/jondurbin/\nairoboros-gpt4-1.4.1, 2023.\n[10] ehartford. dolphin. https://huggingface.co/datasets/ehartford/dolphin, 2023.\n[11] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity, 2022.\n[12] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi,\nP. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck,\nR. Eldan, A. T. Kalai, Y. T. Lee, and Y. Li. Textbooks are all you need. arXiv: 2306.11644,\n2023.\n[13] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of\nparameter-efficient transfer learning. In International Conference on Learning Representations,\n2022.\n[14] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the math dataset. NeurIPS, 2021.\n[15] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network, 2015.\n[16] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.\nTraining compute-optimal large language models, 2022.\n[17] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and\nT. Pfister. Distilling step-by-step! outperforming larger language models with less training data\nand smaller model sizes, 2023.\n[18] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA:\nLow-rank adaptation of large language models. In International Conference on Learning\nRepresentations, 2022.\n[19] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei. Scaling laws for neural language models, 2020.\n[20] N. U. P. R. Lab.\nleetcode-solutions-python-testgen-gpt4.\nhttps://huggingface.co/\ndatasets/nuprl/leetcode-solutions-python-testgen-gpt4, 2023.\n[21] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Dedupli-\ncating training data makes language models better, 2022.\n11\n[22] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,\nC. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving\nquantitative reasoning problems with language models, 2022.\n[23] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\nI. Sutskever, and K. Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv: 2305.20050, 2023.\n[24] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.\nLearn to explain: Multimodal reasoning via thought chains for science question answering,\n2022.\n[25] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang.\nWizardcoder: Empowering code large language models with evol-instruct, 2023.\n[26] D. Mahan, R. Carlow, L. Castricato, N. Cooper, and C. Laforte. Stable beluga models, 2023.\n[27] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, and S. Paul. Peft: State-of-the-art parameter-\nefficient fine-tuning methods. https://github.com/huggingface/peft, 2022.\n[28] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progres-\nsive learning from complex explanation traces of gpt-4. arXiv: 2306.02707v1, 2023.\n[29] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang. Is chatgpt a general-purpose\nnatural language processing task solver?, 2023.\n[30] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 11 2019.\n[31] T. Research.\ntigerbot-kaggle-leetcodesolutions-en-2k.\nhttps://huggingface.co/\ndatasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k, 2023.\n[32] T. Sawada, D. Paleka, A. Havrilla, P. Tadepalli, P. Vidas, A. P. Kranias, J. J. Nay, K. Gupta, and\nA. Komatsuzaki. Arb: Advanced reasoning benchmark for large language models, 2023.\n[33] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni,\nF. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model,\n2022.\n[34] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n[35] S. Shen, L. Hou, Y. Zhou, N. Du, S. Longpre, J. Wei, H. W. Chung, B. Zoph, W. Fedus, X. Chen,\nT. Vu, Y. Wu, W. Chen, A. Webson, Y. Li, V. Zhao, H. Yu, K. Keutzer, T. Darrell, and D. Zhou.\nMixture-of-experts meets instruction tuning:a winning combination for large language models,\n2023.\n[36] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis,\nD. Neal, M. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. Mansfield, S. Prakash, B. Green,\nE. Dominowska, B. A. y Arcas, N. Tomasev, Y. Liu, R. Wong, C. Semturs, S. S. Mahdavi,\nJ. Barral, D. Webster, G. S. Corrado, Y. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan.\nTowards expert-level medical question answering with large language models, 2023.\n[37] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\nStanford alpaca: An instruction-following llama model. GitHub repository, 2023.\n[38] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez,\nand R. Stojnic. Galactica: A large language model for science, 2022.\n[39] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\nefficient foundation language models, 2023.\n[40] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n12\n[41] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun,\nand W. Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large\nlanguage models. arXiv: 2307.10635, 2023.\n[42] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560v2,\n2023.\n[43] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.\nFinetuned language models are zero-shot learners, 2022.\n[44] J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. arXiv:\n1707.06209, 2017.\n[45] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages 38\u201345, Online, Oct. 2020. Association\nfor Computational Linguistics.\n[46] M. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F. Aji. Lamini-lm: A diverse herd of\ndistilled models from large-scale instructions. CoRR, abs/2304.14402, 2023.\n[47] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm:\nEmpowering large language models to follow complex instructions, 2023.\n[48] R. Yu, S. Liu, and X. Wang. Dataset distillation: A comprehensive review, 2023.\n[49] W. Yu, Z. Jiang, Y. Dong, and J. Feng. Reclor: A reading comprehension dataset requiring\nlogical reasoning. International Conference on Learning Representations (ICLR), April 2020.\n[50] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, S. Zhang, G. Ghosh,\nM. Lewis, L. Zettlemoyer, and O. Levy. Lima: Less is more for alignment. arXiv: 2305.11206,\n2023.\n13\nAppendix\nAlpaca Formatting Example with Input\nBelow is an instruction that describes a task, paired with an input\nthat provides further context. Write a response that appropriately\ncompletes the request.\n### Instruction:\n{instruction}\n### Input:\n{input}\n### Response:\nAlpaca Formatting Example without Input\nBelow is an instruction that describes a task. Write a response that\nappropriately completes the request.\n### Instruction:\n{instruction}\n### Response:\nTable 4: Percent change over \"Base\" Model - ARC-Challenge, Hellaswag, TruthfulQA-MC. In this\ncontext, base model refers to the model on which the adapters are merged.\nTest Name\nCamel-P2-13B\nStable-P2-13B\nP2-70B-ins\nDolphin-P2-70B\nCamel-P2-70B\narc_challenge\n-0.14\n+1.10\n+1.08\n+1.10\n+4.12\nhellaswag\n-0.06\n+0.02\n+0.06\n-0.14\n-0.24\ntruthfulqa_mc\n+4.33\n+5.87\n+0.02\n-1.37\n+0.53\nTable 5: Change in Percent over \"Base\" Model - ARC-Challenge, Hellaswag, TruthfulQA-MC. In\nthis context, base model refers to the model on which the adapters are merged.\nTest Name\nCamel-P2-13B\nStable-P2-13B\nP2-70B-ins\nDolphin-P2-70B\nCamel-P2-70B\narc_challenge\n-0.09\n+0.68\n+0.77\n+0.77\n+2.82\nhellaswag\n-0.05\n+0.02\n+0.05\n-0.12\n-0.21\ntruthfulqa_mc\n+2.06\n+2.91\n+0.01\n-0.78\n+0.31\n14\nTable 6: Percent Change over \"Base\" Model - MMLU. In this context, base model refers to the model\non which the adapters are merged\nTest Name\nC-P2-13\nS-P2-13\nP2-70-ins\nD-P2-70\nC-P2-70\nabstract_algebra\n-15.62\n-6.06\n+18.18\n-11.11\n+11.76\nanatomy\n-6.67\n+12.90\n-9.09\n+1.16\n0.00\nastronomy\n-3.23\n+8.75\n-7.81\n-7.20\n-6.25\nbusiness_ethics\n-3.51\n+1.69\n-4.05\n+2.86\n-2.67\nclinical_knowledge\n-2.52\n0.00\n+2.06\n+0.53\n+1.05\ncollege_biology\n+8.43\n+8.99\n+0.83\n+2.59\n-4.92\ncollege_chemistry\n+2.56\n-2.70\n-6.12\n0.00\n0.00\ncollege_computer_science\n0.00\n-2.17\n-3.33\n-7.02\n-10.00\ncollege_mathematics\n+6.67\n+8.82\n+4.76\n+2.56\n+5.13\ncollege_medicine\n-5.38\n+2.15\n+4.39\n+2.70\n+0.86\ncollege_physics\n+3.33\n-2.94\n-20.93\n-13.16\n-18.42\ncomputer_security\n-1.43\n-12.16\n-1.30\n-3.80\n+1.32\nconceptual_physics\n+3.13\n+4.55\n-4.82\n-3.85\n0.00\neconometrics\n+10.26\n+14.71\n+3.77\n+4.08\n+5.77\nelectrical_engineering\n-15.79\n-8.86\n-7.45\n-10.00\n-9.28\nelementary_mathematics\n+6.02\n-3.10\n-3.39\n+4.22\n+0.59\nformal_logic\n-2.13\n+27.27\n+13.56\n+12.07\n+22.41\nglobal_facts\n+21.21\n+2.63\n+4.26\n-6.52\n-5.66\nhs_biology\n-4.19\n-5.29\n+2.39\n+1.64\n-0.40\nhs_chemistry\n-3.41\n-1.14\n-3.51\n+3.85\n+5.66\nhs_computer_science\n-8.20\n0.00\n-1.27\n0.00\n-3.75\nhs_european_history\n+1.80\n0.00\n+4.32\n+2.17\n+0.72\nhs_geography\n-2.70\n-0.68\n+0.58\n-5.06\n-1.74\nhs_government_and_politics\n+8.33\n+4.40\n+1.66\n-1.67\n-1.10\nhs_macroeconomics\n-4.37\n+1.34\n+1.81\n+2.61\n-1.42\nhs_mathematics\n-7.69\n+15.19\n-5.81\n-10.87\n-21.51\nhs_microeconomics\n-2.26\n-2.11\n+2.20\n+1.12\n+1.12\nhs_physics\n-3.51\n-4.00\n+1.41\n-2.67\n-4.17\nhs_psychology\n+1.42\n+4.59\n+0.41\n-0.82\n+0.61\nhs_statistics\n+3.19\n+7.37\n+2.31\n+4.96\n+2.34\nhs_us_history\n+5.23\n+8.50\n-2.12\n+0.54\n-3.21\nhs_world_history\n+5.75\n+3.37\n+0.94\n+1.44\n+2.36\nhuman_aging\n+1.40\n-4.00\n+2.26\n-1.14\n+1.15\nhuman_sexuality\n-1.32\n-3.37\n-5.31\n-1.83\n-7.14\ninternational_law\n+2.33\n-2.15\n+0.96\n-2.80\n+1.94\njurisprudence\n-5.19\n-2.47\n+1.12\n-2.20\n0.00\nlogical_fallacies\n-4.63\n-1.74\n+2.29\n0.00\n-5.11\nmachine_learning\n-15.38\n-14.00\n+22.81\n+16.07\n+26.32\nmanagement\n-2.63\n-1.27\n+2.35\n0.00\n+3.53\nmarketing\n+1.08\n-2.58\n+0.95\n+0.94\n+0.94\nmedical_genetics\n+13.21\n-5.97\n0.00\n-1.39\n-1.45\nmiscellaneous\n+1.86\n+0.66\n+0.15\n-0.29\n-0.59\nmoral_disputes\n+1.81\n-0.45\n-2.96\n-1.15\n-5.04\nmoral_scenarios\n+3.54\n+19.74\n+7.95\n+17.71\n+6.37\nnutrition\n-5.43\n0.00\n-2.98\n+2.23\n-2.54\nphilosophy\n+1.00\n+2.45\n0.00\n+1.25\n+1.25\nprehistory\n+1.46\n+6.83\n0.00\n+3.01\n-1.47\nprofessional_accounting\n+10.00\n+4.10\n-1.23\n+3.29\n-1.90\nprofessional_law\n+8.01\n+10.05\n+6.61\n+5.31\n+5.13\nprofessional_medicine\n+4.29\n+9.59\n-1.49\n-2.50\n-3.40\nprofessional_psychology\n+4.69\n+3.64\n-1.07\n+0.22\n+0.22\npublic_relations\n-5.33\n+5.71\n-4.88\n-1.25\n0.00\nsecurity_studies\n-2.03\n-3.16\n-5.47\n-3.08\n-0.52\nsociology\n-5.92\n-6.16\n+1.14\n+1.14\n+0.58\nus_foreign_policy\n-8.54\n-4.82\n-4.44\n-4.40\n-3.33\nvirology\n-5.41\n-1.28\n+1.14\n-2.20\n+4.60\nworld_religions\n+0.75\n+0.75\n-2.00\n-2.03\n-3.29\nNote: C-P2-13 refers to Camel-Platypus2-13B, S-P2-13 refers to Stable-Platypus2-13B, D-P2-70 refers to\nDolphin-Platypus2-70B, and C-P2-70 refers to Camel-Platypus2-70B.\n15\nTable 7: Change in Percent over \"Base\" Model - MMLU. In this context, base model refers to the\nmodel on which the adapters are merged.\nTest Name\nC-P2-13\nS-P2-13\nP2-70-ins\nD-P2-70\nC-P2-70\nabstract_algebra\n-5.00\n-2.00\n+6.00\n-4.00\n+4.00\nanatomy\n-3.70\n+5.93\n-5.93\n+0.74\n0.00\nastronomy\n-1.97\n+4.61\n-6.58\n-5.92\n-5.26\nbusiness_ethics\n-2.00\n+1.00\n-3.00\n+2.00\n-2.00\nclinical_knowledge\n-1.51\n0.00\n+1.51\n+0.38\n+0.75\ncollege_biology\n+4.86\n+5.56\n+0.69\n+2.08\n-4.17\ncollege_chemistry\n+1.00\n-1.00\n-3.00\n0.00\n0.00\ncollege_computer_science\n0.00\n-1.00\n-2.00\n-4.00\n-6.00\ncollege_mathematics\n+2.00\n+3.00\n+2.00\n+1.00\n+2.00\ncollege_medicine\n-2.89\n+1.16\n+2.89\n+1.73\n+0.58\ncollege_physics\n+0.98\n-0.98\n-8.82\n-4.90\n-6.86\ncomputer_security\n-1.00\n-9.00\n-1.00\n-3.00\n+1.00\nconceptual_physics\n+1.28\n+2.13\n-3.40\n-2.55\n0.00\neconometrics\n+3.51\n+4.39\n+1.75\n+1.75\n+2.63\nelectrical_engineering\n-8.28\n-4.83\n-4.83\n-6.21\n-6.21\nelementary_mathematics\n+2.12\n-1.06\n-1.59\n+1.85\n+0.26\nformal_logic\n-0.79\n+9.52\n+6.35\n+5.56\n+10.32\nglobal_facts\n+7.00\n+1.00\n+2.00\n-3.00\n-3.00\nhs_biology\n-2.90\n-3.55\n+1.94\n+1.29\n-0.32\nhs_chemistry\n-1.48\n-0.49\n-1.97\n+1.97\n+2.96\nhs_computer_science\n-5.00\n0.00\n-1.00\n0.00\n-3.00\nhs_european_history\n+1.21\n0.00\n+3.64\n+1.82\n+0.61\nhs_geography\n-2.02\n-0.51\n+0.51\n-4.55\n-1.52\nhs_government_and_politics\n+6.74\n+3.63\n+1.55\n-1.55\n-1.04\nhs_macroeconomics\n-2.56\n+0.77\n+1.28\n+1.79\n-1.03\nhs_mathematics\n-2.59\n+4.44\n-1.85\n-3.70\n-7.41\nhs_microeconomics\n-1.26\n-1.26\n+1.68\n+0.84\n+0.84\nhs_physics\n-1.32\n-1.32\n+0.66\n-1.32\n-1.99\nhs_psychology\n+1.10\n+3.49\n+0.37\n-0.73\n+0.55\nhs_statistics\n+1.39\n+3.24\n+1.39\n+2.78\n+1.39\nhs_us_history\n+3.92\n+6.37\n-1.96\n+0.49\n-2.94\nhs_world_history\n+4.22\n+2.53\n+0.84\n+1.27\n+2.11\nhuman_aging\n+0.90\n-2.69\n+1.79\n-0.90\n+0.90\nhuman_sexuality\n-0.76\n-2.29\n-4.58\n-1.53\n-6.11\ninternational_law\n+1.65\n-1.65\n+0.83\n-2.48\n+1.65\njurisprudence\n-3.70\n-1.85\n+0.93\n-1.85\n0.00\nlogical_fallacies\n-3.07\n-1.23\n+1.84\n0.00\n-4.29\nmachine_learning\n-5.36\n-6.25\n+11.61\n+8.04\n+13.39\nmanagement\n-1.94\n-0.97\n+1.94\n0.00\n+2.91\nmarketing\n+0.85\n-2.14\n+0.85\n+0.85\n+0.85\nmedical_genetics\n+7.00\n-4.00\n0.00\n-1.00\n-1.00\nmiscellaneous\n+1.40\n+0.51\n+0.13\n-0.26\n-0.51\nmoral_disputes\n+1.16\n-0.29\n-2.31\n-0.87\n-4.05\nmoral_scenarios\n+1.56\n+8.60\n+4.80\n+9.50\n+3.58\nnutrition\n-3.27\n0.00\n-2.29\n+1.63\n-1.96\nphilosophy\n+0.64\n+1.61\n0.00\n+0.96\n+0.96\nprehistory\n+0.93\n+4.32\n0.00\n+2.47\n-1.23\nprofessional_accounting\n+4.26\n+1.77\n-0.71\n+1.77\n-1.06\nprofessional_law\n+3.46\n+4.17\n+3.65\n+2.87\n+2.87\nprofessional_medicine\n+2.57\n+5.15\n-1.10\n-1.84\n-2.57\nprofessional_psychology\n+2.61\n+2.12\n-0.82\n+0.16\n+0.16\npublic_relations\n-3.64\n+3.64\n-3.64\n-0.91\n0.00\nsecurity_studies\n-1.22\n-2.04\n-4.49\n-2.45\n-0.41\nsociology\n-4.48\n-4.48\n+1.00\n+1.00\n+0.50\nus_foreign_policy\n-7.00\n-4.00\n-4.00\n-4.00\n-3.00\nvirology\n-2.41\n-0.60\n+0.60\n-1.20\n+2.41\nworld_religions\n+0.58\n+0.58\n-1.75\n-1.75\n-2.92\nNote: C-P2-13 refers to Camel-Platypus2-13B, S-P2-13 refers to Stable-Platypus2-13B, D-P2-70 refers to\nDolphin-Platypus2-70B, and C-P2-70 refers to Camel-Platypus2-70B.\n16\nTable 8: Hyperparameters for 13B and 70B Models\nHyperparameter\nPlatypus2-13B / 70B\nbatch size\n16\nmicro batch size\n1\nnum epochs\n1\nlearning rate\n4e-4 / 3e-4\ncutoff len\n4096\nlora rank\n16\nlora alpha\n16\nlora dropout\n0.05\nlora target modules\ngate_proj, down_proj, up_proj\ntrain on inputs\nFalse\nadd eos token\nFalse\ngroup by length\nFalse\nprompt template\nalpaca\nlr scheduler\ncosine\nwarmup steps\n100\n17\n"
  },
  {
    "title": "CausalLM is not optimal for in-context learning",
    "link": "https://arxiv.org/pdf/2308.06912.pdf",
    "upvote": "17",
    "text": "Published as a conference paper at ICLR 2024\nCAUSALLM IS NOT OPTIMAL FOR IN-CONTEXT LEARN-\nING\nNan Ding\nTomer Levinboim\nJialin Wu\nSebastian Goodman\nRadu Soricut\nGoogle Research\n{dingnan,tomerl,jialinwu,seabass,rsoricut}@google.com\nABSTRACT\nRecent empirical evidence indicates that transformer based in-context learning\nperforms better when using a prefix language model (prefixLM), in which in-\ncontext samples can all attend to each other, compared to causal language models\n(causalLM), which use auto-regressive attention that prohibits in-context samples\nto attend to future samples. While this result is intuitive, it is not understood from\na theoretical perspective. In this paper we take a theoretical approach and analyze\nthe convergence behavior of prefixLM and causalLM under a certain parameter\nconstruction. Our analysis shows that both LM types converge to their stationary\npoints at a linear rate, but that while prefixLM converges to the optimal solution\nof linear regression, causalLM convergence dynamics follows that of an online\ngradient descent algorithm, which is not guaranteed to be optimal even as the\nnumber of samples grows infinitely. We supplement our theoretical claims with\nempirical experiments over synthetic and real tasks and using various types of\ntransformers. Our experiments verify that causalLM consistently underperforms\nprefixLM in all settings.\n1\nINTRODUCTION\nTransformer-based models (Vaswani et al., 2017) have become the default foundational model for\nvarious machine learning applications such as natural language processing (Devlin et al., 2018; Brown\net al., 2020; Chowdhery et al., 2022) and computer vision (Dosovitskiy et al., 2020). Beyond their\ntraditional usage in machine learning applications, it has recently been discovered that pretraining\nlarge transformers on a vast amounts of data leads them to develop a striking ability referred to as\nin-context learning (ICL) (Brown et al., 2020). Specifically, once such pretraining is complete, these\nmodels are able to solve new tasks at inference time (without changing their parameters) by simply\ningesting a short sequence (prefix) of labeled examples from a task and then computing a prediction\nfor a query example.\nThe ICL capability was first demonstrated by GPT-3 (Brown et al., 2020), where a causalLM (a\nTransformer decoder with auto-regressive attention masks) was used as the main model architecture.\nHowever, follow up work empirically found that restricting the auto-regressive masks on the entire\nsequence is too prohibitive and therefore proposed the so-called prefixLM (Raffel et al., 2020b; Tay\net al., 2022) which allows full attention within the prefix tokens. Moreover, the latest models (such\nas PaLM2 (Google et al., 2023)) adopt a mixture of different LM objectives during pretraining to\nachieved state-of-art performance across a diverse set of tasks and capabilities.\nHowever, beyond the few empirical results in those and related papers, there is yet no theoretical ex-\nplanation that accounts for the different ICL behavior of prefixLM and causalLM. Indeed, theoretical\nstudies of ICL are difficult due to the complicated non-linearity of the (ordinary) transformer archi-\ntecture. However, recent work (Von Oswald et al., 2023) focusing on ICL of linear regression was\nable to show that a specifically designed parameter construction of a one-layer Linear Self-Attention\n(LSA) transformer can simulate a single step of gradient descent by using the in-context examples as\ntraining data. Moreover, a different recent study (Zhang et al., 2023) used gradient flow to prove that\na randomly initialized LSA-transformer indeed converges to such a construction during training.\n1\narXiv:2308.06912v3  [cs.LG]  20 Feb 2024\nPublished as a conference paper at ICLR 2024\nIn this paper, we continue the theoretical line of work above by investigating the convergence\nproperties of ICL for both prefixLM and causalLM multi-layer LSA-transformers in a linear regression\nsetting. We summarizes our contributions as follows:\n\u2022 We first present a clear, formal proof that establishes the relationship between a multi-layer\nLSA and multi-step gradient descents in linear regression.\n\u2022 We then show that both causalLM and prefixLM based multi-layer LSA-transformers\nconverge to their respective stationary points with linear rates of convergence. We prove\nthat the stationary point of prefixLM corresponds to the optimal least square solution of\nthe linear regression problem, while the stationary points of causalLM correspond to the\nweights obtained along the iterations of online gradient descent with non-decaying step\nsizes. Importantly, the stationary points obtained by causalLM may not become optimal\neven as the number of in-context examples increases, which indicates that causalLM is not\noptimal for in-context learning.\n\u2022 Finally, we verify the above theoretical insights by conducting experiments with LSA-\ntransformers as well as ordinary softmax attention based transformers on various synthetic\ntasks including linear and non-linear regression, and multiclass classifications. We also\ncompare causalLM and prefixLM ICL based on LLMs including T5 (Roberts et al., 2022)\nand PaLM2 (Google et al., 2023), as well as the multimodal model PaLI-X (Chen et al.,\n2023). Our experimental results support our theoretical findings and consistently show the\nsuperiority of prefixLM over causalLM on ICL for such settings.\n2\nBACKGROUND\nWe begin by reviewing a few types of transformer attention and in-context learning (ICL), as well as\na specific transformer construction for linear regression ICL by (Von Oswald et al., 2023) which our\ntheories will be based on. The discussions of other related work are deferred to Appendix A.\n2.1\nTRANSFORMERS: SSA, LSA, CAUSALLM, AND PREFIXLM\nGiven a sequence of input vectors Z = (z1, . . . , zn), the output of standard Softmax Self-Attention\n(SSA) layer is\nzj \u2190 zj + P V Z softmax(Z\u22a4 K\u22a4 Q zj),\nwhere P, V, K, Q respectively corresponds to the output projection, value transformation, key\ntransformation and query transformation.\nSince the softmax attention of standard transformers is non-linear, its theoretical analysis becomes\ncomplicated even for a single layer. For this reason, theoretical approaches to analyze transformers\nhave often resorted to the Linear Self-Attention (LSA) layer (Von Oswald et al., 2023; Zhang et al.,\n2023), which simply drops the softmax function from the attention,\nzj \u2190 zj + P V Z(Z\u22a4 K\u22a4 Q zj) = zj + P V\nn\nX\ni=1\nzi\n\u0010\nz\u22a4\ni K\u22a4 Q zj\n\u0011\n.\n(1)\nFurthermore, since each input zj can attend to all positions j \u2208 {1 . . . n}, this form of attention is\ncategorized as full (or bidirectional) attention, and is typically used in the transformer encoder.\nOn the other hand, a (linear) transformer decoder uses the auto-regressive attention\nzj \u2190 zj + P V\nj\nX\ni=1\nzi\n\u0010\nz\u22a4\ni K\u22a4 Q zj\n\u0011\n.\n(2)\nwhich restricts each token zj to attend only to previous positions (and itself) from {1 . . . j}. This\nrestriction is due to the role of the decoder as a causal language model (causalLM) which predicts the\nnext token in the context of the previously generated ones.\nThe original transformer involves both a full attention based encoder and an auto-regressive atten-\ntion based decoder. However, prominent NLP research has often chosen either encoder-only (e.g.\n2\nPublished as a conference paper at ICLR 2024\nBERT (Devlin et al., 2018)) or decoder-only (e.g. GPT (Brown et al., 2020), PaLM (Chowdhery\net al., 2022)) models according to the task at hand. This is partially for the purpose of halving the\nparameter sizes.\nAnother version of attention, between full and auto-regressive, followed from the observation that\nsome tasks can benefit from a prefix sequence such as context or prompt. That is, the input sequence Z\nis composed of n\u2032 prefix tokens (z1, . . . , zn\u2032) configured for the task, while the tokens (zn\u2032+1, . . . , zn)\nrepresent the sample. Specifically, prefixLM (Raffel et al., 2020b) suggests the following attention\n(in its LSA version):\nzj \u2190 zj + P V\nmax(j,n\u2032)\nX\ni=1\nzi\n\u0010\nz\u22a4\ni K\u22a4 Q zj\n\u0011\n,\nwhere max(j, n\u2032) ensures each prefix token zj with j < n\u2032 can attend to all prefix tokens.\n2.2\nIN-CONTEXT LEARNING\nA formal framework of in-context learning has been described in various existing literature such as\n(Garg et al., 2022; Zhang et al., 2023). Here, we briefly review the problem setting and introduce\nnotation that will be used across the paper.\nIn-context learning refers to the ability of models to produce context-driven predictions at inference\ntime. That is, at inference time, a model is fed with a sequence consisting of input-label pairs and\na query input (x1, y1, . . . , xn, yn, xquery) and its goal is to predict the label yquery of xquery using\nthe context examples (x1, y1, . . . , xn, yn) (specifically, without changing the model parameters).\n2.3\nLINEAR REGRESSION IN-CONTEXT LEARNERS\nLinear regression is a classical machine learning problem. Given a set of input-label pairs (xi, yi),\nthe goal is to find an optimal weight vector w that minimizes the l2-loss:\nL(w) = 1\n2n\nn\nX\ni=1\n\u2225 w xi \u2212yi\u22252\n2.\nThe gradient of the loss is \u2207wL = 1\nn\nPn\ni=1(w xi \u2212yi) x\u22a4\ni , and a gradient descent algorithm with\nstep size \u03b7 follows the update rule:\nw(l) = w(l\u22121) + \u03b7\nn\nn\nX\ni=1\n(yi \u2212 w(l\u22121) xi) x\u22a4\ni .\n(3)\nUsing linear regression as a lens to study in-context learning was first proposed in (Garg et al.,\n2022), where the authors laid out an approach for training transformers to in-context learn a class of\nsimple predictors, including linear regression. However, no theoretical study was provided. More\nrecently, and most relevant to our work, (Von Oswald et al., 2023) proposed a succinct construction\nthat demonstrates how a single LSA layer can effectively implement a single gradient descent step.\nAccording to their setup the input is formulated as\nZ = (z(0)\n1 , . . . , z(0)\nn ), where z(0)\nj\n=\n\u0012\nxj\nyj\n\u0013\n(4)\nand the parameter matrices of (1) are set as:\nK = Q =\n\u0012\nId\u00d7d\n0\n0\n0\n\u0013\n, V =\n\u00120d\u00d7d\n0\nw(0)\n\u22121\n\u0013\n, P = \u03b7\nn I,\n(5)\nwhere w(0) is an initial weight vector. (Von Oswald et al., 2023) then showed that this configuration\nresults in an update of their so-called transformed target yj \u2190 yj + \u03b7 (\u2207w(0)L) xj, and that this\ntarget update is equivalent to the one performed by a single-step gradient descent of linear regression.\nAlthough the construction of (Von Oswald et al., 2023) connected LSA-based ICL to the gradient\ndescent of linear regression, the \"transformed target\" view seems unnatural* to work with. Moreover,\ntheir extension from single-layer to multi-layer LSA is unfortunately unclear.\n*The traditional ML formulation updates the weight vector or the model prediction, while the groundtruth\ntarget remains fixed.\n3\nPublished as a conference paper at ICLR 2024\n3\nMULTI-LAYER IN-CONTEXT LEARNER\nIn this section, we provide a formal proof that a multi-layer LSA under the construction of (Von Os-\nwald et al., 2023) progresses identically to multi-step gradient descent.\nInstead of the \"transformed target\" view, the following proposition explicitly connects the GD weights\nof (3) to the outputs of the multi-layer LSA under the constructions of K, Q, P and V in (5). Note\nthat we keep w(0) = 0 in the proposition because it simplifies the equations and makes the outputs\nmore meaningful. However, such specification is not mandatory, and we provide general propositions,\nfor arbitrary w(0), in Appendix C.\nProposition 1 For a multi-layer LSA satisfying the construction (5) and with w(0) = 0, if its input Z\nis formatted as (4), then its l-th layer output is z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4, where \u03b4(l)\nj\n= yj \u2212 w(l) xj and\nw(l) is the l-th updated weight from the gradient descents update rule in (3).\nProof Sketch: Plugging in K, Q, P and V of (5) with w(0) = 0 and z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4 into (1), we\nobtain that for all l > 0,\n\u0012 xj\n\u03b4(l)\nj\n\u0013\n=\n\u0012 xj\n\u03b4(l\u22121)\nj\n\u0013\n\u2212 \u03b7\nn\nn\nX\ni=1\n\u0012\n0\n\u03b4(l\u22121)\ni\n\u0013\nx\u22a4\ni xj .\nSince zj never changes its first d-dimension corresponding to xj, we can simplify it and focus only\non \u03b4(l)\nj , which is the last output coordinate of the j-th LSA-layer,\n\u03b4(l)\nj\n= \u03b4(l\u22121)\nj\n\u2212 \u03b7\nn\nn\nX\ni=1\n\u03b4(l\u22121)\ni\nx\u22a4\ni xj,\n(6)\nwith \u03b4(0)\nj\n= yj. Defining \u02dcy(l)\nj\n= yj \u2212 \u03b4(l)\nj\nand rearranging (6), we obtain \u02dcy(0)\nj\n= 0 and \u2200l > 0:\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nn\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj .\n(7)\nFinally, using (7) and the fact that \u02dcy(0)\nj\n= 0 = w(0) xj, it can be proved by induction that \u2200l : \u02dcy(l)\nj\n=\nw(l) xj. A complete proof is provided in Appendix B.\nTo summarize, the newly introduced variable \u02dcy(l)\nj\nis exactly the prediction of the l-th gradient descent\nweights w(l) for xj , and \u03b4(l)\nj\nis the difference between the true label yj and the predicted \u02dcy(l)\nj .\nTherefore, \u02dcy(l)\nj\nserves as a bridge to connect the LSA output \u03b4(l)\nj\nand the GD weight w(l).\nSo far, we have dealt with the behavior of LSA layers with full attention. In what follows, we move\non to the practical setting of in-context learning, where the input contains not only n in-context\n(training) examples in the format of (4), but also an additional (test) query z(0)\nquery = (x\u22a4\nquery, 0)\u22a4.\nIn particular, we will focus on the two common ICL variants: prefixLM and causalLM, each with a\ndifferent type of attention.\n3.1\nPREFIXLM ICL\nA prefixLM ICL treats the in-context examples Z as the prefix and uses full attention on the first n\npositions, so that they can each freely attend to each other. The last query vector zquery can also\nattend to any example in Z, but cannot attend to itself\u2020. As a result, the updates of the prefixLM-ICL\nunder the same construction follow (6), with the outputs of the l-th layer being,\n\u03b4(l)\nj\n= yj \u2212 \u02dcy(l)\nj\n= yj \u2212 w(l) xj,\nand \u03b4(l)\nquery = \u2212\u02dcy(l)\nquery = \u2212 w(l) xquery,\n\u2020This is because the query does not contain a meaningful label. Attending to itself would cause it to include\nits last-dim input as a label, which would contaminate the resulting multi-layer prediction. This observation was\nnot considered in (Von Oswald et al., 2023).\n4\nPublished as a conference paper at ICLR 2024\n\u2192 0\n\u2192 \nFigure 1: The inputs/outputs of a multi-layer in-context learner. We omitted xj and xquery since they\nare unchanged.\nwhere the initial \u02dcy(0)\nj\n= \u02dcy(0)\nquery = 0.\nIntuitively, the dynamics of the prefixLM ICL is as follows: all \u02dcy(l)\nj\nstarts as 0 at l = 0, and gradually\napproach to the true label yj as l increases, so that the difference (also as the output) \u03b4(l)\nj\ngradually\napproaches to 0. At the same time, \u03b4(l)\nquery starts at 0, and gradually approaches to \u2212yquery, the\nnegation of the query label. Figure 1 provides an illustration of these dynamics.\n3.2\nCAUSALLM ICL\nA causalLM applies auto-regressive attention throughout the entire sequence. Therefore, plugging\nthe same K, Q, P, V into (2), the update rules of (6) and (7) become:\n\u03b4(l)\nj\n= \u03b4(l\u22121)\nj\n\u2212 \u03b7\nn\nj\nX\ni=1\n\u03b4(l\u22121)\ni\nx\u22a4\ni xj,\n(8)\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nj\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj\n(9)\n\u2021with \u03b4(l)\nj\n= yj \u2212 \u02dcy(l)\nj . Moreover, since different \u03b4j, \u02dcyj are exposed to different ranges of inputs, there\nis no uniform w as in (3) that is associated with all \u02dcyj. Instead, if we define wj for each different\nposition j with w(0)\nj\n= 0 and\nw(l)\nj\n= w(l\u22121)\nj\n+ \u03b7\nn\nj\nX\ni=1\n(yi \u2212 w(l\u22121)\ni\nxi) x\u22a4\ni\n(10)\nthen we have the following proposition:\nProposition 2 For a multi-layer causalLM-LSA satisfying (5) with w(0) = 0, if its input Z is\nformatted as (4), then its l-th layer output is z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4, where \u03b4(l)\nj\n= yj \u2212 w(l)\nj xj and w(l)\nj\nfollow (10).\nThe proof of Proposition 2 is provided in Appendix B. Similar to prefixLM-ICL, causalLM-ICL also\nhas \u02dcy(l)\nj\n= w(l)\nj xj, and\n\u03b4(l)\nquery = \u2212\u02dcy(l)\nquery = \u2212 w(l)\nn xquery .\nIn summary, causalLM-ICL and prefixLM-ICL are associated with different update rules: w(l)\nj\nfollows (10) while w(l) follows (3). Specifically, in causalLM, it can be seen that the w(l\u22121)\ni\n\u2021There is another way of update which changes \u03b7/n to \u03b7/j for the j-th example. We provide more details in\nAppendix D and show it performs worse than the main version in (8).\n5\nPublished as a conference paper at ICLR 2024\ncorresponding to the first positions are biased due to restricted access to only a few data points and\nfurthermore, that these biases are propagated to later positions by (10). In prefixLM on the other hand,\neach position has access to all the data and a single w(l) can be used across the entire sequence as in\n(3). Although Eq. (3) and Eq. (10) only hold for the structured LSA case, the profound difference\nbetween causalLM and prefixLM stems from their architectural difference and therefore we believe\nextends to general transformers, as indicated by our experimental results in Section 5.\n4\nCONVERGENCE OF THE MULTI-LAYER IN-CONTEXT LEARNERS\nIn this section, we prove that both multi-layer prefixLM and causalLM converge to their respective\nstationary points with increasing layers (and with linear rates). In addition, we show that the stationary\npoint of prefixLM corresponds to the optimal least-square solution of the linear regression problem,\nwhile the ones corresponding to causalLM are equivalent to the iterative weights of online gradient\ndescent of linear regression, which are known to be sub-optimal for a limited number of examples.\n4.1\nCONVERGENCE OF THE PREFIXLM ICL\nThe fact that a multi-layer prefixLM computation exactly follows the update rule of w(l) as in (3),\nimplies that the layer outputs of prefixLM have the same dynamics of multi-step gradient descent on\na linear regression problem. The convergence properties of such dynamics are well-known, and are\nstated in the following proposition:\nProposition 3 If w(l) follows the iterative updates of (3), then there exists a stationary point w\u2217\nwith coefficients satisfying:\ny X\u22a4 = w\u2217 X X\u22a4,\nwhere y = (y1, . . . , yn) and X = (x1, . . . , xn). Furthermore, the iterative weights w(l) converge to\nw\u2217 with a linear rate of convergence:\nw(l) \u2212 w\u2217 = (w(l\u22121) \u2212 w\u2217)(I \u2212 \u03b7\nn X X\u22a4).\nThat is, Proposition 3 holds for the multi-layer prefixLM, so that the same exact w\u2217 is also the\nstationary point of prefixLM, to which it converges in a linear rate. Furthermore this stationary point\nis exactly the (optimal) least square solution of the linear regression problem.\n4.2\nCONVERGENCE OF THE CAUSALLM ICL\nFollowing the update rule of (10), we can view a multi-layer causalLM as implicitly maintaining\ndifferent weight vectors wj for each position j. In what follows, we show that: (a) Each such position\nj has its own stationary point w\u2217\nj, which appears to be different from the global optimal point w\u2217 of\nlinear regression; (b) even when the number of in-context samples n grows to infinity, convergence to\nw\u2217 is not guaranteed.\nSpecifically, in Appendix B we provide a proof for the following proposition:\nProposition 4 If w(l)\nj\n= Pj\ni=1 a(l)\ni,j x\u22a4\ni follows the iterative updates of (10), then\na(l)\ni,j = a(l)\ni,i \u2261 a(l)\ni\n\u2200j \u2265 i,\nand there exist stationary points w\u2217\nj = Pj\ni=1 a\u2217\ni x\u22a4\ni (for j \u2208 1, . . . , n) with coefficients from\na\u2217 = (a\u2217\n1, . . . , a\u2217\nn) that satisfy y = a\u2217 T, where\nT =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nx\u22a4\n1 x1\nx\u22a4\n1 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n1 xn\n0\nx\u22a4\n2 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n2 xn\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\nx\u22a4\nn xn\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8 .\nFurthermore, the coefficients a(l) converges to the stationary point a\u2217 with linear rate of convergence:\na(l) \u2212 a\u2217 = (a(l\u22121) \u2212 a\u2217)(I \u2212 \u03b7\nn T).\n6\nPublished as a conference paper at ICLR 2024\nThis proposition implies that the stationary points w\u2217\nj of causalLM-ICL are different from w\u2217, the\nleast square solution of linear regression. However, a natural question is: if j increases, would w\u2217\nj\nultimately converge to the optimal solution?\nTo answer this question, the next proposition shows that the stationary points w\u2217\nj follow an online\ngradient descent algorithm, whose loss and gradient at the j-th step is,\nLj(wj) = 1\n2(wj xj+1 \u2212yj+1)2,\n\u2207wjLj(wj) = (wj xj+1 \u2212yj+1) x\u22a4\nj+1 .\nProposition 5 Assuming that w\u2217\nj is the stationary points obtained in Proposition 4, then\nw\u2217\nj+1 = w\u2217\nj \u2212\n1\n\u2225 xj+1 \u22252\n2\n\u2207w\u2217\nj Lj(w\u2217\nj).\n(11)\nThe proof of Proposition 5 is provided in Appendix B. Note that online gradient descent is known\nto converge to an optimal solution only with a decaying step size j\u2212\u03bd for \u03bd > 0 (Jentzen &\nVon Wurstemberger, 2020). Since the step size of (11) does not decay, we conclude that causalLM\nmay not converge to w\u2217 even with increasing layers and increasing number of in-context examples.\nMore concretely, as for the case of in-context learning, where the number of in-context examples n is\nlimited, convergence to the optimal solution w\u2217 cannot be achieved by causalLM-ICL.\n5\nNUMERICAL EXPERIMENTS\nOur experiments contain three parts.\n\u2022 We first use LSA-transformers on linear regression problems to directly verify our theorems. In\nSection 5.1, we show that despite that the in-context example (training) error of causalLM and\nprefixLM both decays in linear rates, the query (test) error of causalLM is significantly larger,\nwhich indicates its stationary solution is not optimal.\n\u2022 Secondly, we use ordinary softmax transformers on a few synthetic tasks including linear re-\ngression, nonlinear regression and multiclass classification. In Section 5.2, we show that our\ntheoretical insights generalize to other tasks types (i.e., that ICL prefixLM still outperforms\ncausalLM in all these cases).\n\u2022 Lastly, in Section 5.3, we conduct LLM based ICL experiments using T5 (Roberts et al., 2022).\nWe also provide additional experimental results on PaLM2 (Google et al., 2023) as well as large\nmultimodal models (PaLI-X, Chen et al. (2023)) in Appendix E.6 and E.7.\n5.1\nLSA-TRANSFORMERS ON LINEAR REGRESSION\nIn order to directly verify our theorems from Section 4, we first study in-context learning on linear\nregression problem with the LSA transformer of (5). Each of the input sequence contains 40 in-\ncontext examples and 200 queries, and each query attends to all the in-context examples but does not\nattend to each other. See Appendix E for an illustration. The data input xi of the sequence is sampled\nfrom U(\u22121, 1)16. Each sequence is associated with a single weight vector w that is sampled from\nN(0, I), and the labels are computed as yi = w xi. Assuming the prediction of each layer is \u02dcy(l)\ni , we\nevaluate the MSE \u2225yi \u2212 \u02dcy(l)\ni \u22252\n2 on both in-context and query examples across different layers l.\nThe results are plotted in Figure 2 left (for prefixLM) and middle (for causalLM). Our results are\naveraged over 64 randomly generated sequences. As we can see, although both prefixLM and\ncausalLM has a linear rate of convergence (with respect to the number of layers) on the in-context\nexamples, the query errors of causalLM are stuck above the 10\u22121 level, while the query error of\nprefixLM decays in the same linear rate as its training error.\nFurthermore, in Figure 2 right, we plot the query errors of the stationary points (following Proposi-\ntion 4, corresponding to the outputs of infinite layers) of causalLM-ICL with increasing number of\nin-context examples up to 300. Although causalLM-ICL is able to eventually converge to optimal\nsolution when \u00b5x = 0, it takes more than 100 examples to reach below 10\u22122. The convergence is\n7\nPublished as a conference paper at ICLR 2024\n0\n10\n20\n30\n40\n50\nNumber of layers\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nMSE Error\nConvergence of multi-layer prefixLM-ICL\nIn-context examples\nQuery examples\n0\n10\n20\n30\n40\n50\nNumber of layers\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nMSE Error\nConvergence of multi-layer causalLM-ICL\nIn-context examples\nQuery examples\n50\n100\n150\n200\n250\n300\nNumber of training exemplars\n10\n7\n10\n5\n10\n3\n10\n1\n101\nQuery MSE error\nStationary points of causalLM-ICL\nx = 0\nx = 1\nx = 2\nx = 3\nFigure 2: Left/Middle: the MSE on in-context examples and query examples of multi-layer LSA-\nbased prefixLM/causalLM-ICLs with 40 in-context training examples. Right: the query MSE of\ncausalLM-ICL\u2019s stationary points (per Proposition 4) using up to 300 in-context examples.\nLR\nN-LR\nMC\nPrefixLM-SL\n8.6e-3\n1.5e-4\n24.1\nCausalLM-SL\n1.9e-1\n2.7e-3\n27.0\nPrefixLM-UL\n2.5e-3\n9.0e-5\n27.6\nCausalLM-UL\n1.6e-2\n2.9e-3\n32.1\nTable 1: The test query errors of the unshared-layer (UL) and sharing-layer (SL) transformer-ICLs on\nlinear regression (LR), non-linear regression (NLR), and multiclass classification (MC) tasks. Both\nregression tasks report mean squared errors; and the MC task reports the classification error.\neven worse as we vary the input distribution x \u223c U(\u22121, 1)d + \u00b5x with increasing \u00b5x \u2208 {0, 1, 2, 3},\nwhich demonstrates that causalLM-ICL is not optimal for few-shot ICL.\n5.2\nSTANDARD TRANSFORMERS ON SYNTHETIC TASKS\nPrevious experiments provided a proof of concept verification of the propositions from Section 4.\nNext we verify if a standard softmax transformer-based prefixLM and causalLM ICL exhibit similar\ndifferences on various types of synthetic tasks including linear regression, non-linear regression and\nmulticlass classification.\nAll three tasks used 16-dim inputs with x \u223c U(\u22121, 1)16 and w \u223c N(0, I). For non-linear regression,\nwe apply a sigmoid activation on the logit such that y = sigmoid(w x); and for multiclass classifica-\ntion, we randomly generate three wc \u223c N(0, I), and assign labels based on y = argmaxc {wc x}.\nWe trained a few 24-layer transformers containing 128 hidden units and 2 heads. Besides of the\ncomparisons of prefixLM and causalLM, we also compare the transformers with or without sharing\nlayers (SL vs UL). In particular, the sharing-layer transformer can be considered a recurrent sys-\ntem (Dehghani et al., 2018) where the dynamic is continual along the layers and a stationary point\nmay exist given infinite number of layers, which makes it closer to our constructed LSA.\nThe ICL training dataset contains 64,000 training sequences. Each sequence contains 40 in-context\nexamples and 20 queries, where queries are independent of each other similar to Section 5.1. The\ntransformers are trained with batch size 64 for 100 epochs. More details of the hyper-parameters of\nthe experiments are provided in Appendix E.\nWe evaluate the ICL performance using 64 holdout test sequences and report the test errors on\nthe query examples. The results are summarized in Table 1. We find that both prefixLM-SL and\nprefixLM-UL significantly outperform their counterparts of causalLM in all cases. As a side note,\ntransformer-SL appears to outperform transformer-UL in the classification tasks, which indicates the\noverfitting problem of the latter due to over-parameterization. In addition, we also add probes at the\noutput of each SL-transformer layer to visualize the test errors of intermediate layers in Figure 3.\nComparing Figure 3 and Figure 2 (left/middle) reveals some similarities. Although the test query\nerrors of causalLM decay in roughly the same rate as the ones of prefixLM in earlier layers, the\ndecays become much slower in later layers possibly due to the nature of its non-optimal stationary\npoints. These results suggest that the title argument of the paper also holds beyond LSA-based\ntransformers and linear regression.\n8\nPublished as a conference paper at ICLR 2024\n0\n5\n10\n15\n20\n25\nLayer\n10\n2\n10\n1\n100\nTest error\nLinear Regression\nPrefixLM\nCausalLM\n0\n5\n10\n15\n20\n25\nLayer\n10\n3\n10\n2\n10\n1\nTest error\nNon-linear Regression\nPrefixLM\nCausalLM\n0\n5\n10\n15\n20\n25\nLayer\n0.24\n0.26\n0.28\n0.30\n0.32\n0.34\nTest error\nMulticlass Classification\nPrefixLM\nCausalLM\nFigure 3: The test query errors of the 24-layer SL-transformers based prefixLM/causalLM-ICLs on\nlinear regression (left), non-linear regression (middle), and multiclass classification (right).\nMMLU\nBBH\nBase\nLarge\nXL\nBase\nLarge\nXL\nPrefixLM\n28.8\n32.0\n39.5\n27.4\n32.2\n35.8\nCausalLM\n28.0\n26.9\n30.5\n24.8\n29.8\n32.0\nTable 2: The averaged test query accuracies on 5-shot MMLU (57 tasks) and 3-shot BBH (23 tasks)\nwith FLAN-finetuned T5 DecoderOnly prefixLM/causalLM checkpoints.\n5.3\nICL ON LARGE LANGUAGE MODELS\nIn order to compare the ICL performance of causalLM and prefixLM in a large language model\nsetting, we conduct experiments using the publicly available T5 family of models (Roberts et al.,\n2022). Note that the existing public T5X \u00a7 checkpoints are all based on EncDec models, which are\nsimilar to prefixLM. Thus, it would be unfair and unnatural to compare with causalLM by simply\nreplacing the bidirectional attention of the encoder to the causal attention during the finetuning\nstage. To make a more reasonable comparison, we reran the pretraining stages of T5 on the C4\ncorpus (Raffel et al., 2020a) from a random initialization point using a span corruption objective,\nbut in the DecoderOnly setting. Moreover, for each size (from Base, Large and XL) of the models,\nwe pretrained two checkpoints, one with prefixLM and the other with causalLM, each for 1M\nsteps using the same T5 pretraining recipe. After pretraining, we use the FLAN recipe (Chung\net al., 2022) to finetune each checkpoint (40k steps for Base, 20k steps for Large and XL) with its\npretrained attention mask and evaluate the ICL capability of the finetuned models on two benchmarks:\nMMLU (Hendrycks et al., 2020) and BBH (Suzgun et al., 2022).\nTable 2 shows that for all three sizes of T5X DecoderOnly models, the MMLU and BBH accuracies\nof prefixLM surpasses that of causalLM consistently and such gap widens as the size of the model\nbecomes larger. This result empirically verifies that our conjecture generalizes to the practical case.\nWe supply additional empirical evidence on state-of-the-art models in Appendix E.6 and E.7.\n6\nCONCLUSION\nIn this paper, we analyzed the convergence properties of two types of widely-used transformer-based\nlanguage models (causalLM and prefixLM), during in-context learning. Using a simplified LSA\nattention in a linear regression setting, we proved that both LM types converge to their stationary\npoints in linear rates, but that their stationary points have significantly different properties. In\nparticular, the stationary points of prefixLM coincides with the optimal least square solution; while\nthe ones of causalLM is equivalent to the weights of an online learning system, that is not guaranteed\nto converge to the optimal solution. Our experiments verified the above theoretical results, and also\nempirically extend the findings to general transformer on non-linear regression as well as classification\ntasks. Finally, we compare causalLM and prefixLM on a few large language models and find that\nprefixLM also consistently wins over causalLM in practical few-shot tasks.\n\u00a7https://github.com/google-research/t5x\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe want to specially thank Xinhua Zhang for helpful discussions and references about online learning.\nWe also thank Yi Tay for comments regarding the PaLM2 checkpoints.\nREFERENCES\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algo-\nrithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661,\n2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani,\nDaniel M. Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Min-\nderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim M. Alabdulmohsin, Lucas Beyer, Julien\nAmelot, Kenton Lee, Andreas Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai,\nNeil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model.\nArXiv, abs/2305.18565, 2023. URL https://api.semanticscholar.org/CorpusID:\n258967670.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn\nin-context? a case study of simple function classes. Advances in Neural Information Processing\nSystems, 35:30583\u201330598, 2022.\nGoogle, Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,\nSiddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad\nFienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang\n10\nPublished as a conference paper at ICLR 2024\nLi, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nArnulf Jentzen and Philippe Von Wurstemberger. Lower error bounds for the stochastic gradient\ndescent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning\nrates. Journal of Complexity, 57:101438, 2020.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128\u20133137,\n2015.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837, 2022.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020a. URL\nhttp://jmlr.org/papers/v21/20-074.html.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020b.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis\nBulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma,\nAlexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan\nSepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and\ndata with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.\norg/abs/2203.17189.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nMelbourne, Australia, July 2018.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven\nZheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, volume 30, 2017.\n11\nPublished as a conference paper at ICLR 2024\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev,\nAndrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In\nInternational Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv\npreprint arXiv:2303.03846, 2023.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context\nlearning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.\narXiv preprint arXiv:2306.09927, 2023.\n12\nPublished as a conference paper at ICLR 2024\nCausalLM is not optimal for in-context learning\nAppendices\nA\nRELATED WORK\nEver since GPT-3 (Brown et al., 2020) exhibited its in-context learning abilities in various language\ninference and translation tasks, there has been tremendous interest in understanding the mechanics\nbehind In-Context Learning (ICL). Currently, there are two main camps of thought that try to explain\nICL: (1) the representation camp, which views ICL behavior as a topic model that extracts relevant\nmemories based on the topic of the context (Xie et al., 2021; Min et al., 2022) - these works support\nthis view with the findings that in-context learner sometimes behaved similarly even when the label\nof the training examples were permuted (Min et al., 2022). (2) the algorithmic camp, which holds that\nLLMs learns to implement a learning algorithm (Garg et al., 2022; Aky\u00fcrek et al., 2022; Von Oswald\net al., 2023) and then run it during ICL - these works usually propose a construction of the transformer\nparameters and show that it can solve certain simple tasks (e.g. linear regression), then empirically\nverify that transformers track the behavior of the algorithm of interest.\nMoreover, recent studies of large-scale data and model (Wei et al., 2023) discovered that large\nlanguage models seem to exhibit certain emergent behavior, where, ICL is memory-based on small-\nto-medium sized models or data, but becomes more algorithm-based on larger model and data. For\nexample, (Wei et al., 2023) showed that a large language model is able to respond accordingly to the\nflipped label in in-context examples, opposing the findings of (Min et al., 2022).\nSince most ICL applications only involve few shots of context examples, it seems reasonable to\nconjecture that the memory of a deep representation and a shallow predictor algorithm may co-exist in\ncontributing the in-context learning capabilities. Since the representation learning of large language\nmodels have been universally acknowledged, it is more interesting to investigate how transformer\nlearns to in-context learn shallow predictors with few-shot examples.\nFocusing on work from the algorithmic camp, we note that (Garg et al., 2022) were the first to\nsuggest using linear regression to study in-context learning. The authors empirically found that a\n12-layer transformer is able to achieve similar results as a least-square solver on a 20-dim linear\nregression problem with around 20 in-context examples. Beyond linear regression, they also found\nthat transformers can in-context learn a few other classes of shallow predictors, including two-layer\nRelu networks.\nProbably the first formal theoretical investigation of the linear regression in-context learners is\n(Aky\u00fcrek et al., 2022). They first showed that a transformer layer can approximately conduct four\nbasic operations: mov, mul, div, aff. They then cleverly combined these four operations and showed\nthat a gradient descent step of linear regression can be implemented with a 4-head 8-layer transformer\nwith O(d) hidden units, where d is the dimension of the inputs x. Despite their novel construction,\nthe result itself provides only a loose upper bound on the model size (or depth) that is required for\nsimulating linear regression within a transformer - for example, (Von Oswald et al., 2023) reported\nthat a 2 or 5-layer transformer already achieves significantly better results than a single-step gradient\ndescent for linear regression.\nBecause of the significant discrepancy between the construction of (Aky\u00fcrek et al., 2022) and the\nempirical results, the one-layer LSA construction of (Von Oswald et al., 2023) appears to be more\nappealing and matches the experimental results better. Moreover, a most recent work by (Zhang et al.,\n2023) used gradient flow to prove that by initializing w(0) = 0, such matrix constructions can indeed\nbe learned by an LSA transformer. This is why our paper follows this construction and studies its\nmulti-layer convergence properties with different types of attention (prefixLM vs causalLM).\nIn terms of the comparison between prefixLM and causalLM, such research work can be traced\nback as early as (Raffel et al., 2020b), where they showed prefixLM outperforms causalLM in\nvarieties of NL tasks. Later, UL-2 (Tay et al., 2022) proposed to mix prefixLM and span corruption\nobjectives, and found it to be more efficient than the causalLM objective alone. It was also shown\n13\nPublished as a conference paper at ICLR 2024\nin (Chung et al., 2022), that U-PaLM (a UL2-finetuning PaLM) outperforms PaLM (causalLM only)\nin various ICL tasks. Indeed, for the reasons above, some of the latest models have included prefixLM\nobjectives in the pretraining mix (for example PaLM-2 by Google et al. (2023)). On the other hand,\nprominent models such as Flamingo as well as the ones in the GPT-family are still based on the\ncausalLM structure, so the comparison between prefixLM and causalLM remains important and\nrelevant. Furthermore, all previous studies were done in an empirical manner, whereas we set out to\nexplain their differences from a theoretical perspective and back the theory with empirical evidence.\nWhile we are not the first to follow this path, our work is the first to provide a theoretical justification\nfor the advantage of prefixLM over causalLM in a multi-layer transformer ICL setting by analyzing\ntheir theoretical convergence properties.\nB\nPROOFS\nIn this section, we provide proofs of the propositions introduced in Section 3 and Section 4.\nProposition 1\nFor a multi-layer LSA satisfying (5) with w(0) = 0, if its input Z is formatted as (4),\nthen its l-th layer output is z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4, where \u03b4(l)\nj\n= yj \u2212 w(l) xj and w(l) is the weight from\nthe l-th step gradient descents as in (3).\nProof:\nPlugging in K, Q, P and V of (5) with w(0) = 0 into (1), we have\n\u0012 xj\n\u03b4(l)\nj\n\u0013\n=\n\u0012 xj\n\u03b4(l\u22121)\nj\n\u0013\n+ \u03b7\nn\n\u0012\n0d\u00d7d\n0\n0\n\u22121\n\u0013\n\u00b7\n n\nX\ni=1\n\u0012 xi\n\u03b4(l\u22121)\ni\n\u0013\n(x\u22a4\ni , \u03b4(l\u22121)\ni\n)\n\u0012\nId\u00d7d\n0\n0\n0\n\u0013 \u0012 xj\n\u03b4(l\u22121)\nj\n\u0013!\n=\n\u0012 xj\n\u03b4(l\u22121)\nj\n\u0013\n\u2212 \u03b7\nn\nn\nX\ni=1\n\u0012\n0\n\u03b4(l\u22121)\ni\n\u0013\nx\u22a4\ni xj .\nIt is easy to see that zj never changes its first d-dimension corresponding to xj. Therefore, we can\nsimplify the above equation and focus only on the last coordinate \u03b4(l)\nj , where we have\n\u03b4(l)\nj\n= \u03b4(l\u22121)\nj\n\u2212 \u03b7\nn\nn\nX\ni=1\n\u03b4(l\u22121)\ni\nx\u22a4\ni xj,\n(12)\nwith \u03b4(0)\nj\n= yj. Defining \u02dcy(l)\nj\n= yj \u2212 \u03b4(l)\nj\nand rearranging (12), we obtain \u02dcy(0)\nj\n= 0 and\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nn\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj .\n(13)\nNext we prove \u02dcy(l)\nj\n= w(l) xj by induction. Since w(0) = 0, it is clear that \u02dcy(0)\nj\n= w(0) xj = 0 for\nall j.\nIf \u02dcy(l\u22121)\nj\n= w(l\u22121) xj for all j, then\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nn\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj\n= w(l\u22121) xj + \u03b7\nn\nn\nX\ni=1\n(yi \u2212 w(l\u22121) xi) x\u22a4\ni xj\n=\n \nw(l\u22121) +\nn\nX\ni=1\n(yi \u2212 w(l\u22121) xi) x\u22a4\ni\n!\nxj\n= w(l) xj .\n\u25a1\n14\nPublished as a conference paper at ICLR 2024\nProposition 2\nFor a multi-layer causalLM-LSA satisfying (5) with w(0) = 0, if its input Z is\nformatted as (4), then its l-th layer output is z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4, where \u03b4(l)\nj\n= yj \u2212 w(l)\nj xj and w(l)\nj\nfollow (10).\nProof:\nPlugging in K, Q, P and V of (5) with w(0) = 0 into (2), we have\n\u03b4(l)\nj\n= \u03b4(l\u22121)\nj\n\u2212 \u03b7\nn\nj\nX\ni=1\n\u03b4(l\u22121)\ni\nx\u22a4\ni xj\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nj\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj\nwith \u02dcy(l)\nj\n= yj \u2212 \u03b4(l)\nj . Next we prove \u02dcy(l)\nj\n= w(l)\nj xj by induction. Since w(0)\nj\n= 0, it is clear that\n\u02dcy(0)\nj\n= w(0)\nj\nxj = 0 for all j.\nIf \u02dcy(l\u22121)\nj\n= w(l\u22121)\nj\nxj for all j, then\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nj\nn\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj\n= w(l\u22121)\nj\nxj + \u03b7\nn\nj\nX\ni=1\n(yi \u2212 w(l\u22121)\ni\nxi) x\u22a4\ni xj\n=\n \nw(l\u22121)\nj\n+\nn\nX\ni=1\n(yi \u2212 w(l\u22121)\ni\nxi) x\u22a4\ni\n!\nxj\n= w(l)\nj xj .\n\u25a1\nProposition 3\nIf w(l) follows the iterative updates of (3), then there exists a stationary point w\u2217\nwith coefficients satisfying:\ny x\u22a4 = w\u2217 X X\u22a4,\nwhere y = (y1, . . . , yn) and X = (x1, . . . , xn). Furthermore, the iterative weights w(l) converges\nto the stationary point w\u2217 with linear rate of convergence:\nw(l) \u2212 w\u2217 = (w(l\u22121) \u2212 w\u2217)(I \u2212 \u03b7\nn X X\u22a4).\nProof:\nFrom (3), we have\nw(l) = w(l\u22121) + \u03b7\nn\nn\nX\ni=1\n(yi \u2212 w(l\u22121) xi) x\u22a4\ni\n|\n{z\n}\n(\u2217)\n.\nThe stationary point must satisfy (\u2217) = 0. Written in vectorized form, we have\ny X\u22a4 = w\u2217 X X\u22a4 .\n(14)\nNow plugging (14) back to (3), we have\nw(l) = w(l\u22121) + \u03b7\nn\n\u0010\nw\u2217 X X\u22a4 \u2212 a w(l\u22121) X X\u22a4\u0011\n,\nwhich can be reorganized to\nw(l) \u2212 w\u2217 = (w(l\u22121) \u2212 w\u2217)(I \u2212 \u03b7\nn X X\u22a4).\n\u25a1\n15\nPublished as a conference paper at ICLR 2024\nProposition 4\nIf w(l)\nj\n= Pj\ni=1 a(l)\ni,j x\u22a4\ni follows the iterative updates of (10), then\na(l)\ni,j = a(l)\ni,i \u2261 a(l)\ni\n\u2200j \u2265 i,\nand there exists stationary points w\u2217\nj = Pj\ni=1 a\u2217\ni x\u22a4\ni (for j \u2208 1, . . . , n) with coefficients from\na\u2217 = (a\u2217\n1, . . . , a\u2217\nn) that satisfy y = a\u2217 T, where\nT =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nx\u22a4\n1 x1\nx\u22a4\n1 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n1 xn\n0\nx\u22a4\n2 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n2 xn\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\nx\u22a4\nn xn\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8 .\nFurthermore, the coefficients a(l) converges to the stationary point a\u2217 with linear rate of convergence:\na(l) \u2212 a\u2217 = (a(l\u22121) \u2212 a\u2217)(I \u2212 \u03b7\nn T).\nProof:\nFirst notice that according to (10), we have\nw(l)\nj\n= w(l\u22121)\nj\n+ \u03b7\nn\nj\nX\ni=1\n(yi \u2212 w(l\u22121)\ni\nxi) x\u22a4\ni\n=\nj\nX\ni=1\n\u0010\na(l\u22121)\ni,j\n+ \u03b7\nn(yi \u2212 w(l\u22121)\ni\nxi)\n\u0011\nx\u22a4\ni\nor\na(l)\ni,j = a(l\u22121)\ni,j\n+ \u03b7\nn(yi \u2212 w(l\u22121)\ni\nxi)\n\u2200j \u2265 i.\nSince a(0)\ni,j = 0, and the above update is the same for any j given any i, then it is obvious by induction\nthat\na(l)\ni,j = a(l)\ni,i \u2261 a(l)\ni\n\u2200j \u2265 i.\nTherefore, we can simplify w(l)\nj\n= Pj\ni=1 a(l)\ni\nx\u22a4\ni .\nNow plugging into (10), we have\nj\nX\ni=1\na(l)\ni\nx\u22a4\ni\n=\nj\nX\ni=1\na(l\u22121)\ni\nx\u22a4\ni + \u03b7\nn\nj\nX\ni=1\n(yi \u2212\ni\nX\nk=1\na(l\u22121)\nk\nx\u22a4\nk xi) x\u22a4\ni\n=\nj\nX\ni=1\n \na(l\u22121)\ni\n+ yi \u2212 \u03b7\nn\n \ni\nX\nk=1\na(l\u22121)\nk\nx\u22a4\nk xi\n!!\nx\u22a4\ni ,\nwhich is equivalent to\na(l)\ni\n= a(l\u22121)\ni\n+ \u03b7\nn\n \nyi \u2212\ni\nX\nk=1\na(l\u22121)\nk\nx\u22a4\nk xi\n!\n|\n{z\n}\n(\u2217)\n.\n(15)\nThe stationary points satisfy (\u2217) = 0, which gives\ny1 =a\u2217\n1 x\u22a4\n1 x1\ny2 =a\u2217\n1 x\u22a4\n1 x2 +a\u2217\n2 x\u22a4\n2 x2\n. . .\nyn =a\u2217\n1 x\u22a4\n1 xn + . . . + a\u2217\nn x\u22a4\nn xn,\n16\nPublished as a conference paper at ICLR 2024\nor in the vectorized form y = a\u2217 T, where\nT =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nx\u22a4\n1 x1\nx\u22a4\n1 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n1 xn\n0\nx\u22a4\n2 x2\n\u00b7 \u00b7 \u00b7\nx\u22a4\n2 xn\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\nx\u22a4\nn xn\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8 .\nNow plugging in y = a\u2217 T back to (15) and vectorize it, yields\na(l) = a(l\u22121) + \u03b7\nn (a\u2217 T \u2212 a T) ,\nwhich can be reorganized to\na(l) \u2212 a\u2217 = (a(l\u22121) \u2212 a\u2217)(I \u2212 \u03b7\nn T).\n\u25a1\nProposition 5\nAssuming that w\u2217\nj is the stationary points obtained in Proposition 4, then\nw\u2217\nj+1 = w\u2217\nj \u2212\n1\n\u2225 xj+1 \u22252\n2\n\u2207w\u2217\nj Lj(w\u2217\nj).\nProof:\nRecall the online learning system with a sequence of data-label pairs (xj, yj) has the\nfollowing online loss and its gradient at the j-th step,\nLj(wj) = 1\n2(wj xj+1 \u2212yj+1)2,\n\u2207wjLj(wj) = (wj xj+1 \u2212yj+1) x\u22a4\nj+1 .\nAccording to Proposition 4, we have y = a\u2217 T, which gives\nyj+1 =a\u2217\n1 x\u22a4\n1 xj+1 + . . . + a\u2217\nj x\u22a4\nj xj+1\n+ a\u2217\nj+1 x\u22a4\nj+1 xj+1\n= w\u2217\nj xj+1 +a\u2217\nj+1 x\u22a4\nj+1 xj+1\n(16)\nwhere the last equation is due to w\u2217\nj = Pj\ni=1 a\u2217\ni x\u22a4\ni .\nSince w\u2217\nj = Pj\ni=1 a\u2217\ni x\u22a4\ni , we have\nw\u2217\nj+1 = w\u2217\nj +a\u2217\nj+1 x\u22a4\nj+1\n= w\u2217\nj +\n1\n\u2225 xj+1 \u22252\n2\n\u0000a\u2217\nj+1 x\u22a4\nj+1 xj+1\n\u0001\nx\u22a4\nj+1\n= w\u2217\nj \u2212\n1\n\u2225 xj+1 \u22252\n2\n(w\u2217\nj xj+1 \u2212yj+1) x\u22a4\nj+1\n= w\u2217\nj \u2212\n1\n\u2225 xj+1 \u22252\n2\n\u2207w\u2217\nj Lj(w\u2217\nj)\nwhere the third equation is because of (16).\n\u25a1\nC\nMULTI-LAYER LSA CONSTRUCTION WITH NON-ZERO W(0)\nIn this section, we introduce the proposition that connects a multi-layer LSA following the construc-\ntion of (5) but with non-zero w(0) and the multi-step gradient descents of linear regression.\nProposition 6 For a multi-layer LSA satisfying the construction (5), if its input Z is formatted as (4),\nthen its l-th layer output is z(l)\nj\n= (x\u22a4\nj , \u03b4(l)\nj )\u22a4, where \u03b4(l)\nj\n= yj \u2212 (w(l) \u2212 w(0)) xj and w(l) is the\nl-th updated weight from the gradient descents update rule in (3).\n17\nPublished as a conference paper at ICLR 2024\nProof:\nPlugging in K, Q, P and V of (5) into (1), we have\n\u03b4(l)\nj\n= \u03b4(l\u22121)\nj\n\u2212 \u03b7\nn\nn\nX\ni=1\n\u0010\n\u03b4(l\u22121)\ni\n\u2212 w(0) xi\n\u0011\nx\u22a4\ni xj,\n(17)\nwith \u03b4(0)\nj\n= yj. Defining \u02dcy(l)\nj\n= yj \u2212 \u03b4(l)\nj\n+ w(0) xj and rearranging the (17), we obtain \u02dcy(0)\nj\n= 0 and\n\u02dcy(l)\nj\n= \u02dcy(l\u22121)\nj\n+ \u03b7\nn\nn\nX\ni=1\n(yi \u2212 \u02dcy(l\u22121)\ni\n) x\u22a4\ni xj .\nThen it is easy to prove \u02dcy(l)\nj\n= w(l) xj by induction, similar to the proof of Proposition 1.\n\u25a1\nD\nCAUSALLM WITH ATTENTION-LENGTH-BASED COEFFICIENTS\nSince there are j terms in the summation of (10), another reasonable update for causalLM would be\nw(l)\nj\n= w(l\u22121)\nj\n+\u03b7\nj\nj\nX\ni=1\n(yi \u2212 w(l\u22121)\ni\nxi) x\u22a4\ni ,\n(18)\nwhich we call causalLM2. For causalLM2, we have the following proposition.\nProposition 7 If w(l)\nj\n= Pj\ni=1 a(l)\ni,j x\u22a4\ni follows the iterative updates of (18), then\na(l)\ni,j \u2261 1\nj a(l)\ni\n\u2200j \u2265 i,\nand there exists stationary points w\u2217\nj = 1\nj\nPj\ni=1 a\u2217\ni x\u22a4\ni (for j \u2208 1, . . . , n) with coefficients from\na\u2217 = (a\u2217\n1, . . . , a\u2217\nn) that satisfy y = a\u2217 S, where\nS =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nx\u22a4\n1 x1\n1\n2 x\u22a4\n1 x2\n\u00b7 \u00b7 \u00b7\n1\nn x\u22a4\n1 xn\n0\n1\n2 x\u22a4\n2 x2\n\u00b7 \u00b7 \u00b7\n1\nn x\u22a4\n2 xn\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\n1\nn x\u22a4\nn xn\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8 .\nFurthermore, the coefficients a(l) converges to the stationary point a\u2217 with the following rate of\nconvergence:\na(l) \u2212 a\u2217 = (a(l\u22121) \u2212 a\u2217)(I \u2212\u03b7 S).\nThe condition number \u03ba(S) is about n/2 greater than the one of \u03ba(T), which makes causalLM2\nconverge much slower than causalLM.\nOne can also prove that the stationary point of causalLM2 corresponds to the following online system\nwith online loss and gradient at the j-th step,\nLj( \u02dcwj) = 1\n2( \u02dcwj xj+1 \u2212yj+1)2,\n\u2207 \u02dcwjLj( \u02dcwj) = ( \u02dcwj xj+1 \u2212yj+1) x\u22a4\nj+1,\nwhere \u02dcw =\nj\nj+1 w.\nProposition 8 Assuming that w\u2217\nj is the stationary points obtained in Proposition 4, then\nw\u2217\nj+1 = \u02dcw\u2217\nj \u2212\n1\n\u2225 xj+1 \u22252\n2\n\u2207 \u02dcw\u2217\nj Lj( \u02dcw\u2217\nj).\nSince the step does not have j\u2212\u03bd (\u03bd > 0) decay, such online system is not guaranteed to converge,\ntherefore suffers the same problem as the original causalLM in Section 3.2.\nIn Figure 4, we plot the query MSE error of the stationary points of causalLM2-ICL with increasing\nnumber of in-context examples. We can see that the online system corresponding to causalLM2-ICL\nconverges even slower than the ones of causalLM-ICL in Figure 2 right.\n18\nPublished as a conference paper at ICLR 2024\n50\n100\n150\n200\n250\n300\nNumber of training exemplars\n10\n1\n100\n101\nQuery MSE error\nStationary points of causalLM2-ICL\nx = 0\nx = 1\nx = 2\nx = 3\nFigure 4: The test error on the stationary point of the causalLM2-ICL with up to 300 in-context\nexamples.\nFigure 5: The illustration of the attention mask. Green arrows represent the attentions between in-\ncontext examples. The dashed arrows only applies for prefixLM. Red arrows represent the attentions\nfrom queries to in-context examples. The query examples should not attend to themselves because\nthe inputs do not contain labels.\nE\nADDITIONAL EXPERIMENTAL DETAILS AND RESULTS\nE.1\nEXPERIMENT SETTINGS FOR SECTION 5.1\nIn order to directly verify the theorem, we used the constructed LSA-based transformer, with\nK = Q =\n\u0012\nId\u00d7d\n0\n0\n0\n\u0013\n, V =\n\u0012\n0d\u00d7d\n0\n0\n\u22121\n\u0013\nand P = \u03b7\nn I. Although not a trained transformer, it\nwas recently proved in (Zhang et al., 2023) that a randomly initialized LSA-transformer does converge\nto such a construction. In addition, we did an ablation test of \u03b7 = {0.1, 0.2, 0.4, 0.8, 1.6, 3.2} and\nchose \u03b7 = 1.6 as it converges the fastest without any divergence problems.\nWe randomly generated 64 sequences for ICL evaluation. For each sequence, we put the first 40\nexamples as the in-context examples and the last 200 examples as the query examples. The queries\nare independent of each other without attention. See Figure 5 for an illustration of the transformer\nattention mask. Such multi-query design is for training efficiency purpose only and is equivalent to\n200 sequences with the same w and input examples xi, but different one query per sequence.\nE.2\nEXPERIMENT SETTINGS FOR SECTION 5.2\nIn order to verify that our theorems can be qualitatively applied beyond LSA and linear regression, we\nconducted several experiments over various synthetic tasks using regular transformers. We based our\n19\nPublished as a conference paper at ICLR 2024\nLR\nN-LR\nMC\nPrefixLM-SL-L2\n8.6e-3\n1.5e-4\n24.1\nCausalLM-SL-L2\n1.9e-1\n2.7e-3\n27.0\nPrefixLM-SL-no-L2\n6.7e-3\n1.5e-4\n24.5\nCausalLM-SL-no-L2\n5.0e-2\n1.9e-3\n30.5\nPrefixLM-UL-L2\n7.6e-3\n1.7e-4\n26.7\nCausalLM-UL-L2\n4.4e-2\n2.5e-3\n30.4\nPrefixLM-UL-no-L2\n2.5e-3\n9.0e-5\n27.6\nCausalLM-UL-no-L2\n1.6e-2\n2.9e-3\n32.1\nTable 3: The test query errors of the unshared-layer (UL) and sharing-layer (SL) transformer-ICLs\nwith or without L2 regularizer on linear regression (LR), non-linear regression (NLR), and multiclass\nclassification (MC) tasks.\n100\n101\n102\n103\n# batches of training examplars\n10\n2\n10\n1\n100\n101\nQuery MSE\nLinear Regression\nPrefixLM\nCausalLM\n100\n101\n102\n103\n# batches of training examplars\n10\n3\n10\n2\n10\n1\nQuery MSE\nNon-linear Regression\nPrefixLM\nCausalLM\n100\n101\n102\n103\n# batches of training examplars\n0.3\n0.4\n0.5\n0.6\nTest Error\nMulticlass Classification\nPrefixLM\nCausalLM\nFigure 6: The test query errors of the SL-transformers based prefixLM/causalLM-ICLs with various\nnumbers of training sequences on linear regression (left), non-linear regression (middle), and multi-\nclass classification (right).\ncode from the repository of (Aky\u00fcrek et al., 2022)\u00b6 and applied their default training hyperparameters\nof the code. We used a transformer of 24 layers with 128 hidden units and 2 heads. The FFN\nintermediate size is 4 \u00d7 128 = 512. The learning schedule is based on cosine decay with base\nlearning rate 1e-4, for 100 epochs. In addition, since the target of the outputs of the in-context\nexamples are 0 (see Fig. 1), we optionally add an additional L2 regularizer on the outputs of the\nin-context examples. See the comparison between the transformers with or without the L2-regularizer\nin Table 3. In Table 1 of the main paper, the reported numbers correspond to the SL-transformer\nwith the L2 regularizer and the UL-transformer without the L2 regularizer. Across all these settings\nprefixLM consistently beats causalLM as our theorem predicts.\nE.3\nTHE IMPACT OF THE SIZE OF THE TRAINING DATA\nHere we investigate the performance of prefixLM and causalLM as a function of the number of\ntraining samples. In Fig. 6, we plot their after having trained on 10 batches all the way up to 1000\nbatches (as in Section 5.2). We observe that when the amount of training data is low, ICL falls into the\nmemorization regime, in which models perform perfectly on the training data, but do not generalize\nwell to unseen test sequences. However, prefixLM transitions to the generalization regime earlier\nthan causalLM, which is reflected by the positions of the largest gap between the two. (30 batches in\nLR, 100 batches in N-LR, and 10 batches in MC).\nE.4\nTESTING WITH FEWER IN-CONTEXT EXAMPLES\nIn causalLM, different positions in the sequence are trained with different numbers of in-context\nexamples (ICEs). This may bring advantage to pretrained causalLM models when tested with fewer\nnumber of in-context examples than it was trained on. To compare causalLM and prefixLM in such\nsetting, we use the same models as before that were trained with 40 in-context examples, but test\nthem on fewer (16, 24, 32) in-context examples. Note that 16 is the minimum number of examples to\n\u00b6https://github.com/google-research/google-research/tree/master/\nincontext\n20\nPublished as a conference paper at ICLR 2024\n16 Test ICEs\nLR\nN-LR\nMC\nPrefixLM-SL\n1.01\n2.1e-2\n42.8\nCausalLM-SL\n1.76\n2.7e-2\n43.3\nPrefixLM-UL\n0.97\n1.9e-2\n42.9\nCausalLM-UL\n1.12\n3.2e-2\n46.6\nTable 4: The test query errors with 16 ICEs on linear regression (LR), non-linear regression (NLR),\nand multiclass classification (MC) tasks.\n24 Test ICEs\nLR\nN-LR\nMC\nPrefixLM-SL\n1.4e-1\n2.0e-3\n33.4\nCausalLM-SL\n7.0e-1\n1.0e-2\n35.9\nPrefixLM-UL\n1.0e-1\n1.7e-3\n37.1\nCausalLM-UL\n1.3e-1\n1.0e-2\n41.2\nTable 5: The test query errors with 24 ICEs on linear regression (LR), non-linear regression (NLR),\nand multiclass classification (MC) tasks.\nsolve our 16-dim synthetic regression problems. The errors of prefixLM and causalLM are provided\nin the following Tables 4, 5, 6, where regression tasks (LR, N-LR) report mean squared errors and\nthe MC task reports the classification error. From the tables we see that prefixLM still consistently\noutperforms causalLM, even when testing with fewer in-context examples than used during training\ntime.\nE.5\nPERMUTATION ON IN-CONTEXT EXAMPLES\nWe further consider a simple approach for mitigating the problems of causalLM by randomly\npermuting the in-context examples during training time. This is motivated by the observation that\nfor causalLM, every permutation representations a different view of the context in the example. The\nresults of this experiment (Table 7) show that this style of causalLM training indeed improves over\nthe fixed order training setting compared to the unpermuted ICEs (Table 1). However, prefixLM still\noutperforms causalLM in general.\nE.6\nIN-CONTEXT LEARNING USING PALM2\nGoing beyond the publicly available T5 models, we further verify our findings by conducting FLAN-\nbased finetuning experiments using the state-of-the-art PaLM2 family of models (Google et al.,\n2023). PaLM2 models were pretrained with a mixture of objectives that includes different LM\ntypes, which make them a relatively fair starting point to compare causalLM and prefixLM after\nfinetuning. In practice we finetune three sizes of PaLM2 language models: Gecko, Otter and Unicorn||.\nWe use the same default recipe for FLAN-PaLM2 finetuning (Google et al., 2023; Chung et al.,\n2022) and finetune the PaLM2 checkpoints for either causalLM or prefixLM. We then evaluate the\nICL capability of the finetuned models on the Massive Multi-task Language Understanding (5-shot\nMMLU) tasks (Hendrycks et al., 2020).\n||https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\n32 Test ICEs\nLR\nN-LR\nMC\nPrefixLM-SL\n2.4e-2\n4.7e-4\n32.4\nCausalLM-SL\n3.1e-1\n5.0e-3\n34.6\nPrefixLM-UL\n9.5e-3\n3.4e-4\n36.2\nCausalLM-UL\n4.0e-2\n5.7e-3\n37.3\nTable 6: The test query errors with 32 ICEs on linear regression (LR), non-linear regression (NLR),\nand multiclass classification (MC) tasks.\n21\nPublished as a conference paper at ICLR 2024\nPermuted ICEs\nLR\nN-LR\nMC\nPrefixLM-SL\n9.0e-3\n1.5e-4\n24.1\nCausalLM-SL\n1.9e-1\n2.5e-3\n26.9\nPrefixLM-UL\n2.6e-3\n9.5e-5\n26.1\nCausalLM-UL\n1.1e-2\n1.8e-3\n26.2\nTable 7: The test query errors with randomly permuted ICEs on linear regression (LR), non-linear\nregression (NLR), and multiclass classification (MC) tasks.\nGecko\nOtter\nUnicorn\nPrefixLM\n46.6\n64.8\n81.4\nCausalLM\n43.3\n61.0\n78.0\nTable 8: The average test query accuracies on 5-shot MMLU tasks with FLAN-finetuned PaLM2-\nGecko/Otter/Unicorn prefixLM/causalLM checkpoints. (Google et al., 2023) reported a similar\naveraged accuracy of 81.2 on Unicorn-PrefixLM.\nTable 8 shows that for all three sizes of PaLM2, the MMLU accuracy (average over the 57 tasks) of\nprefixLM surpasses that of causalLM by more than 3%. This result again empirically verifies that our\nconjecture generalizes to the practical case, using a state of the art LLM**.\nE.7\nIN-CONTEXT LEARNING WITH MULTIMODAL MODELS\nLastly, we also demonstrate that prefix attention masks benefit ICL in multimodal models across\nvarious settings. We conducted experiments using both 4-shot and 8-shot COCO image captioning\ntasks on the Karpathy split (Karpathy & Fei-Fei, 2015) using the PaLI-X model (Chen et al., 2023), a\n55B multimodal pretrained model.\nThe PaLI-X model employs an encoder-decoder architecture where ViT encoded image tokens and\ntext tokens are fed to the multimodal encoder and decoder to generate outputs. During pretraining,\nthe text prompts were split into two parts. The first part is the input to the multimodal prefix-encoder\nthat self-attends to all the image and text tokens on the encoder side, following the style of prefixLM.\nThe second part is the input to the causal-decoder that self-attends to only the previous text tokens on\nthe decoder side, following the style of causalLM, and cross-attends to encoder tokens.\nThe prefix-encoder and causal-decoder nature allows us to consider different variants of the attention\nmasks and placements of the in-context texts to showcase the benefits of prefix attention masks. We\ndesign two main categories of few-shot experiments with five self-attention mask settings, detailed\nbelow. We finetune the PaLI-X pretrained model using each setting\u2019s attention mask with 4-shot\nEpisodic WebLI dataset (Chen et al., 2023) for 20k steps.\nIn the first category, we place the few-shot text tokens on the encoder side and study the effect of\nmanipulating the encoder self-attention masks, leaving the causal-decoder unchanged. Specifically,\n**Besides of PaLM2, we also find that any checkpoint that is pretrained with a mixture of prefixLM and\ncausalLM tends to do better with prefixLM for in-context learning. However, we do not claim that prefixLM\nwould necessarily outperform causalLM when using solely causalLM pretrained checkpoints.\n4-shot\n8-shot\nPrefix encoder\n106.7\n107.5\nBlock-causal encoder\n104.8\n106.0\nCausal encoder\n102.3\n104.9\nPrefix decoder\n103.9\n104.2\nCausal decoder\n102.4\n92.9\nTable 9: Cider scores of COCO captioning using various attention masks. The Prefix variant\noutperforms the Causal ones. Note that the official PaLI-X (Chen et al., 2023) reported a 4-shot Cider\nof 107.6, which was also based on the prefix encoder mask, but was finetuned with additional image\ncaptioning data from the Conceptual Captions 3M dataset (Sharma et al., 2018).\n22\nPublished as a conference paper at ICLR 2024\nconsidering a 2-shot ICL case for simplicity, we adapt the prefix encoder attention mask Aenc\nprefix in\n(19) into two causal variants, block-causal and causal encoder attention masks as Aenc\nb\u2212causal in (20)\nand Aenc\ncausal in (21). In this case, the block-causal version is more inline with exposing the encoder\nto the examples one at a time, while the causal one strictly follows auto-regressive attention.\nAenc\nprefix =\nI1\nT1\nI2\nT2\nIt\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n1\n1\n1\n1\n1\nI1\n1\n1\n1\n1\n1\nT1\n1\n1\n1\n1\n1\nI2\n1\n1\n1\n1\n1\nT2\n1\n1\n1\n1\n1\nIt\n(19)\nAenc\nb\u2212causal =\nI1\nT1\nI2\nT2\nIt\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n1\n1\n1\n1\n1\nI1\n1\n1\n1\n1\n1\nT1\n1\n1\n1\nI2\n1\n1\n1\nT2\n1\nIt\n(20)\nAenc\ncausal =\nI1\nT1\nI2\nT2\nIt\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n1\n1\n1\n1\n1\nI1\n\u29f9\n1\n1\n1\nT1\n1\n1\n1\nI2\n\u29f9\n1\nT2\n1\nIt\n(21)\nI1, I2, It denotes the image tokens for the two shots and the target and T1, T2 denotes the text tokens\nfor the two shots. 1 denotes a matrix of all 1s and \u201c\u29f9\u201d denote an upper triangular matrix with 1s. A\n1 at row i and column j indicates that token j is allowed to attend to token i. We report the results on\nfew-shot COCO captioning in the top half of Table 9. We observe consistent improvement over both\n4- and 8-shot when changing the encoder attention mask from causal mask, to block causal mask,\nand then to prefix mask.\nAdec\ncausal =\nT1\nT2\nTt\n \n!\n\u29f9\n1\n1\nT1\n\u29f9\n1\nT2\n\u29f9\nTt\n(22)\nAdec\nprefix =\nT1\nT2\nTt\n \n!\n1\n1\n1\nT1\n1\n1\n1\nT2\n\u29f9\nTt\n(23)\nSimilarly, in the second category, we place the few-shot text on the decoder side and study the effect\nof manipulating the decoder attention masks, leaving the prefix encoder unchanged. We adapt the\ncausal decoder attention mask Adec\ncausal in (22) to prefix attention mask Adec\nprefix in (23). Note that\nall the image tokens from the prefix-encoder side are visible to all text tokens (on the decoder) via\ncross attention. However, the image tokens cannot attend to the text because of the encoder-decoder\narchitecture. The second half of Table 9 reports the results of using prefix and causal decoder attention.\nEven though the decoder is pretrained in the causal manner, with additional finetuning using prefix\nmasks, the new prefix decoder achieves a Cider score of 103.9 in 4-shot ICL, outperforming the\nfinetuned causal decoder by 1.5. Furthermore, the prefix decoder also appears to be more robust when\nextrapolating to 8-shot evaluation (Cider 104.2), compared to the causal decoder (Cider 92.9).\nIn summary, the LLM experiments in Section 5.3 as well as the multimodal experiments in this\nsection show that our conjectures hold up in practice with various types of large-scale models and a\nwide range of settings.\n23\n"
  },
  {
    "title": "RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs",
    "link": "https://arxiv.org/pdf/2308.07228.pdf",
    "upvote": "8",
    "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nRestoreFormer++: Towards Real-World\nBlind Face Restoration from Undegraded\nKey-Value Pairs\nZhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, Fellow, IEEE,\nand Ping Luo, Member, IEEE\nAbstract\u2014Blind face restoration aims at recovering high-quality face images from those with unknown degradations. Current algorithms\nmainly introduce priors to complement high-quality details and achieve impressive progress. However, most of these algorithms ignore\nabundant contextual information in the face and its interplay with the priors, leading to sub-optimal performance. Moreover, they pay less\nattention to the gap between the synthetic and real-world scenarios, limiting the robustness and generalization to real-world applications.\nIn this work, we propose RestoreFormer++, which on the one hand introduces fully-spatial attention mechanisms to model the contextual\ninformation and the interplay with the priors, and on the other hand, explores an extending degrading model to help generate more\nrealistic degraded face images to alleviate the synthetic-to-real-world gap. Compared with current algorithms, RestoreFormer++ has\nseveral crucial benefits. First, instead of using a multi-head self-attention mechanism like the traditional visual transformer, we introduce\nmulti-head cross-attention over multi-scale features to fully explore spatial interactions between corrupted information and high-quality\npriors. In this way, it can facilitate RestoreFormer++ to restore face images with higher realness and fidelity. Second, in contrast to the\nrecognition-oriented dictionary, we learn a reconstruction-oriented dictionary as priors, which contains more diverse high-quality facial\ndetails and better accords with the restoration target. Third, we introduce an extending degrading model that contains more realistic\ndegraded scenarios for training data synthesizing, and thus helps to enhance the robustness and generalization of our RestoreFormer++\nmodel. Extensive experiments show that RestoreFormer++ outperforms state-of-the-art algorithms on both synthetic and real-world\ndatasets. Code will be available at https://github.com/wzhouxiff/RestoreFormerPlusPlus.\nIndex Terms\u2014Blind Face Restoration, Transformer, Cross-Attention Mechanism, Dictionary, Computer Vision, Real-World.\n\u2726\n1\nINTRODUCTION\nB\nLIND face restoration aims at recovering high-quality\nfaces from a series of unknown degradations, such as\nblur, noise, downsampling, compression artifacts, etc. These\ndegradations are complex and diverse in real-world scenar-\nios, leaving limited information in the degraded face image.\nTherefore, the face restored directly from its degraded one is\nnot good enough, even with powerful DNN structures [8], [9],\n[10], [11], [12]. Introducing priors to complement additional\nhigh-quality details can effectively solve this issue [1], [2],\n[4], [5], [6], [13], [14], [15], [16], [17], [18], [19].\nDespite the acknowledgment of progress, current prior-\nbased algorithms mainly depend on geometric priors [4], [13],\n[14], [15], [16], [17], [18] or recognition-oriented references [1],\nwhich are not accordant to the restoration task and thus\nlead to sub-optimal performance. The geometric priors are\nlandmarks [13], [14], facial parsing maps [4], [15], or facial\ncomponent heatmaps [16] that mainly provide shape informa-\n\u2022\nZhouxia Wang and Wenping Wang are with The University of Hong Kong,\nHong Kong SAR, China.\nE-mail: {wzhoux@.connect, wenping@cs}.hku.hk\n\u2022\nJiawei Zhang is with SenseTime Research, China.\nE-mail: zhjw1988@gmail.com\n\u2022\nTianshui Chen is with The Guangdong University of Technology,\nGuangzhou, China.\nE-mail: tianshuichen@gmail.com\n\u2022\nPing Luo is with The University of Hong Kong and Shanghai AI Laboratory,\nChina.\nE-mail: pluo@cs.hku.hk\ntion to aid face restoration. Recognition-oriented references\nlike the facial component dictionaries in DFDnet [1] are\nextracted with a recognition model and only cover limited\nfacial components, such as eyes, mouth, and nose. Therefore,\nthe restored faces of these algorithms tend to lack details. For\nexample, in Fig. 1, the results of PSFRGAN [4], whose priors\nare facial parsing maps, and DFDnet [1] fail to recover facial\ndetails, especially in hair areas. Although the generative\npriors encapsulated in a face generation network aim at face\nreconstruction and achieve superior performance compared\nto the previous two kinds of priors, their restored results\nstill fail to yield fine-grained facial details or exist obvious\nartifacts. Examples are the restored results of Wan et al. [2]\nand GPEN [5] in Fig. 1.\nOn the other hand, effectively integrating the identity\ninformation in the degraded face and high-quality details\nin the priors is a critical step to attaining face images in\nboth realness and fidelity. However, current methods either\ntake the degraded faces as supervision, e.g., PULSE [3], or\nlocally combine these two kinds of information by pixel-\nwise concatenation [20], [21], [22], spatial feature transform\n(SFT) [1], [4], [19], [23], or deformable operation [6]. They\nignore the useful contextual information in the face image\nand its interplay with priors, and thus most of them cannot\ntrade off the fidelity and realness of their restored results well.\nA typical example is PULSE [3]. As shown in Fig. 1, their\nrestored results perform well in realness, but their identities\ncannot be preserved.\narXiv:2308.07228v1  [cs.CV]  14 Aug 2023\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nInput\nDFDNet [1]\nWan et al. [2]\nPULSE [3]\nPSFRGAN [4]\nGPEN [5]\nVQFR [6]\nRestoreFormer [7] RestoreFormer++\nreal-world\nECCV 20\nCVPR 20\nCVPR 20\nCVPR 21\nCVPR 21\nECCV 22\nCVPR 22\nOurs\nFig. 1. Comparisons with state-of-the-art face restoration methods on some degraded real-world images. Our conference version, RestoreFormer [7],\nproduces restored results with rich details and complete structures, making them more natural and authentic than the results of other methods. Our\ncurrent version, RestoreFormer++, extends the multi-scale mechanism and EDM to remove haze from the degraded face images and process uneven\ndegradation (highlighted with a red box in the third sample), resulting in a clearer and more pleasant look.\nIn this work, we propose RestoreFormer++, which in-\ntroduces fully-spatial attention mechanisms to model the\ncontextual information in the face image and its interplay\nwith priors matched from a reconstruction-oriented dic-\ntionary. Unlike the existing ViT methods [24], [25], [26],\n[27] that achieve fully-spatial attentions with multi-head\nself-attention mechanism (MHSA) (Fig. 2 (a)), our Restore-\nFormer++ is equipped with multi-head cross-attention mech-\nanism (MHCA) (Fig. 2 (b)) whose queries are the features\nof degraded face image while key-value pairs are high-\nquality facial priors. In addition, MHCAs are applied to\nmulti-scale features, enabling RestoreFormer++ to model\nthe contextual information based on both semantic and\nstructural information and effectively improve the restored\nperformance in both realness and fidelity. It is also worth\nmentioning that the priors adopted in our work have\nbetter quality since they are from a reconstruction-oriented\nhigh-quality dictionary (ROHQD). Its elements are learned\nfrom plenty of uncorrupted faces by a high-quality face\ngeneration neural network implemented with the idea of\nvector quantization [28]. They are rich in high-quality facial\ndetails specifically aimed at face reconstruction (see Fig. 3 for\na more intuitive comparison with the recognition-oriented\ndictionary).\nIn addition, RestoreFomer++ contains an extending de-\ngrading model (EDM) to generate more realistic degraded\nface images for alleviating the synthetic-to-real-world gap\nand further improving its robustness and generalization\ntoward real-world scenarios. Observations show that in\nthe real world, besides blur, noise, downsampling, and\ncompression artifacts, haze and uneven degradation are also\ncommon. Relevant examples are shown in Fig. 1. However,\nexisting methods cannot handle these degradations well.\nTherefore, we introduce haze and uneven degradation into\nour EDM, which enables RestoreFormer++ to effectively\nremove the haze covered in the degraded face images and\navoid the artifacts raised by uneven degradation. Besides,\nEDM applies a spatial shift operation on the high-quality\nface before synthesizing the degraded face to reduce the\neffect introduced by inaccurate face alignment. Due to the\nspecificity of face structure, aligning the degraded face to\na reference face (in this work, the reference face is from\nFFHQ [29], and its landmarks are shown as green points\nin Fig. 6 and Fig. 8) is helpful for the restoration of face\nimages [1], [4], [19]. However, misalignment caused by\nsevere degradation will lead to errors while restoring with\nexisting methods. For example, as shown in the second\nsample in Fig. 8, its left eyebrow is aligned with the left\neye of the reference image, and the existing methods, such as\nPSFGAN [4], GFP-GAN [19], and our conference version [7],\ntend to restore the left eye near the eyebrow area instead\nof its original area in the degraded face image. The small\nspatial shift adopted in EDM can improve the tolerance of\nRestoreFormer++ for face alignment error, thus improving\nits restoration performance as in Fig. 8 (g).\nThis work is an extension of our conference version [7]. In\nthis version, we strengthen the work from three aspects. First,\nwe extend our multi-head attention mechanisms used for\nfusing the degraded facial features and their corresponding\nhigh-quality facial priors from single-scale to multi-scale.\nThis enables RestoreFormer++ to model contextual informa-\ntion based on both semantic and structural information, effec-\ntively improving the restored performance in both realness\nand fidelity. Second, we proposed an extending degrading\nmodel (EDM) to alleviate the synthetic-to-real-world gap\nand further improve the robustness and generalization of\nour RestoreFormer++ toward real-world scenarios. Finally,\nwe conduct more experiments and analyses to verify the\nsuperiority of RestoreFormer++ against existing methods and\nthe contributions of each component in RestoreFormer++.\nIn conclusion, our main contributions are as follows:\n\u2022\nWe propose RestoreFormer++, which on the one hand\nintroduces multi-head cross-attention mechanisms\nto model the fully-spatial interaction between the\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\ndegraded face and its corresponding high-quality\npriors and on the other hand, explores an extending\ndegrading model to synthesize more realistic de-\ngraded face images for model training. It can restore\nface images with higher realness and fidelity for both\nsynthetic and real-world scenarios.\n\u2022\nWe introduce a reconstruction-oriented high-quality\ndictionary learning algorithm to generate priors that\nare more accordant to the face restoration task and\nthus provide suitable priors to RestoreFormer++ to\nrestore faces with better realness.\n\u2022\nThe extending degrading model contains more kinds\nof realistic degradations and simulates the face mis-\naligned situation to further alleviate the synthetic-\nto-real-world gap. It improves the robustness and\ngeneralization of RestoreFormer++.\n\u2022\nExtensive experiments show that RestoreFormer++\noutperforms current leading competitors on both\nsynthetic and real-world datasets. We also conduct\ndetailed ablation studies to analyze the contribution\nof each component to give a better understanding of\nRestoreFormer++.\nThe remaining of this work is organized as follows. We\nreview the most related works in Sec. 2 and detailedly\nintroduce the RestoreFormer++ in Sec. 3. We then present\nexperiments with comparison and analysis in Sec. 4. Finally,\nconclusions are in Sec. 5.\n2\nRELATED WORKS\n2.1\nBlind Face Restoration\nBlind face restoration aims to recover high-quality faces from\nface images that have undergone unknown and complex\ndegradations. Owing to the effectiveness of Deep Neural\nNetworks (DNN) [8], [30], [31], researchers [9], [10], [11],\n[12] have attempted to restore high-quality faces directly\nfrom degraded ones using DNN-based approaches. However,\nsince the information contained in degraded faces is limited,\nresearchers have sought assistance from additional priors,\nsuch as geometric priors [4], [13], [14], [15], [16], [17], [18],\n[32], [33], reference priors [1], [20], [21], [22], and generative\npriors [2], [3], [19], [34]. Most geometric priors are predicted\nfrom the degraded faces, and the quality of these priors is\nsignificantly constrained by the degree of degradation in\nthe face images, which further impacts the final restoration\nresults. Reference priors, which are high-quality faces distinct\nfrom degraded ones, alleviate the limitations of geometric\npriors. However, exemplars [20], [21], [22] with the same\nidentity as the degraded face are not always available, and\nfacial component dictionaries extracted from high-quality\nface images are partial and recognition-oriented, restricting\nthe performance of reference-based methods. Recent stud-\nies [2], [3], [5], [19] have suggested that generative priors\nencapsulated in well-trained high-quality face generators\npossess considerable potential for blind face restoration, and\nworks [6], [35], [36], published concurrently or after our\nconference version, propose obtaining high-quality priors\nfrom a codebook similar to our ROHQD. However, most of\nthese previous studies employ pixel-wise concatenation [20],\n[21], [22], spatial feature transform (SFT) [1], [4], [19], [23],\nor deformable operation [6] to fuse the degraded feature\nand priors. Both SFT [23] and deformable networks [37] are\nimplemented with convolutional layers, and their receptive\nfields limit the attentive areas, leading to the neglect of useful\ncontextual information when fusing degraded information\nand its corresponding priors.\nIn contrast, our RestoreFormer++ is a unified framework\nfor globally modeling the contextual information in the face\nwith fully-spatial attention while fusing the features of the\ndegraded face and their corresponding priors matched from a\nreconstruction-oriented dictionary. Due to the rich contextual\ninformation and high-quality priors, RestoreFormer++ per-\nforms better than previous related methods in both realness\nand fidelity.\n2.2\nVision Transformer\nThese years, transformer [38] designed with attention mecha-\nnism performs pretty well on natural language processing\nareas [39], [40] and researchers turn to explore the potential\npossibility of transformer on computer vision. The first\nattempt is ViT [26], a pure transformer that takes sequences\nof image patches as input. It achieves high performance\non image classification tasks. Then more works extend the\ntransformer to object detection [24], [27], segmentation [41],\nand even low-level vision [25], [42], [43], [44], [45], [46],\nwhich may suffer from more difficulties on efficiency. In\nthe low-level vision, Chen et al. [25] take the advantages\nof transformer on a large scale pre-training to build a\nmodel that covers many image processing tasks. Esser\net al. [42] apply the transformer on codebook-indices directly\nto make the generation of a very high-resolution image\npossible. Zhu et al. [46] exploit the global structure of the\nface extracted by the transformer to help the synthesis of\nphoto-sketch. Most of these works tend to search the global\ninformation in the patches of an image with a self-attention\nmechanism. To model the interplay between the degraded\nface and its corresponding priors cooperating with contextual\ninformation, RestoreFormer++ adopts multi-scale multi-head\ncross-attention mechanisms whose queries are the features\nof the corrupted face and key-value pairs are the priors.\n2.3\nFace Degrading Model\nSince there is no real training pair in blind face restoration,\nmost previous works synthesize the training pairs with a\ndegrading model. The degrading model proposed in [12]\nmainly consists of blur kernels, downsampling, and Gaussian\nnoise. In this version, Gaussian noise is added before down-\nsampling. Li et al. [22] find that adding Gaussian noise after\ndownsampling can better simulate the long-distance image\nacquisition. They further upgrade the degrading model with\nJPEG compression. Most of the later methods follow this\ndegrading model for degraded face synthesis except the\nwork proposed by Wan et al. [2] that mainly focuses on\nthe old photos that suffer from scratch texture. To further\ndiminish the gap between the synthetic and real-world\ndatasets, our EDM extends the degrading model proposed\nin [22] with additional commonly existing degradations: haze\nand uneven degradation. It also applies a spatial shift to high-\nquality face images while synthesizing the degraded face to\nalleviate the inherent bias introduced by face alignment.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nV\nK\nQ\nMulti-Headed Attention\nAdd &  Normalize\nFeed Forward\n\ud835\udc81\ud835\udc4e\n\ud835\udc81\ud835\udc51\n0\nV\nK\nQ\nMulti-Headed Attention\nAdd &  Normalize\nFeed Forward\n\ud835\udc81\ud835\udc51\n0\n\ud835\udc81\ud835\udc53\n\ud835\udc81\ud835\udc5d0\nMHCAs\n\ud835\udc70\ud835\udc70\ud835\udc51\ud835\udc51\n\u0de0\ud835\udc70\ud835\udc70\ud835\udc51\ud835\udc51\nROHQD \n\u2026\n\ud835\udd3b\ud835\udd3b\n\ud835\udc81\ud835\udc81\ud835\udc51\ud835\udc51\n0\n\ud835\udc81\ud835\udc81\ud835\udc5d\ud835\udc5d0\n\ud835\udc81\ud835\udc81\ud835\udc51\ud835\udc51\n1\n\ud835\udc81\ud835\udc81\ud835\udc51\ud835\udc51\n\ud835\udc46\ud835\udc46\u22121\n\u2026\n\ud835\udc81\ud835\udc81\ud835\udc5d\ud835\udc5d1\n\ud835\udc81\ud835\udc81\ud835\udc5d\ud835\udc5d2\n\ud835\udc81\ud835\udc81\ud835\udc5d\ud835\udc5dS\n\u2026\nMHCAs\nMHCAs\n\u2026\nFusion Block \n\ud835\udc04\ud835\udc04\ud835\udc51\ud835\udc51\n\ud835\udc03\ud835\udc03\ud835\udc51\ud835\udc51\n(a) MHSA\n(b) MHCA\n(c) RestoreFormer++\nFig. 2. Framework of RestoreFormer++. (a) MHSA is a transformer with multi-head self-attention used in most of the previous ViTs [24], [25],\n[26], [27]. Its queries, keys, and values are from the degraded information Z0\nd. (b) MHCA is a transformer with a multi-head cross-attention used\nin the proposed RestoreFormer++. It globally fuses the degraded information Z0\nd and the corresponding high-quality priors Z0\np by taking Z0\nd as\nqueries while Z0\nextraction (Zs p as key-value pairs. (c) The whole pipeline of RestoreFormer++. First, a degraded face image Id is sent to Ed for multi-scale feature\nd, s \u2208 {0, 1, . . . , S \u2212 1}, S is the number of scales used for fusion). Then, the degraded feature Zs\nd interacts with its corresponding\npriors Z0\np matched from ROHQD D or previous fused output Zs\np with MHCAs. Finally, a high-quality face \u02c6Id is restored from the final fused result ZS\np\nby the decoder Dd.\n3\nRESTOREFORMER++\nIn this section, we will introduce the proposed Restore-\nFormer++ with the whole restored pipeline shown in\nFig. 2 (c). The pipeline consists of four components: an\nencoder Ed, a reconstruction-oriented high-quality dictionary\nD (ROHQD), a fusion block consisting of several Multi-\nHead Cross-Attention blocks (MHCAs), and a decoder Dd.\nFirst, a degraded face image Id is sent to Ed for feature\nextraction (Zs\nd, s \u2208 {0, 1, . . . , S \u2212 1}, S is the number of\nscales used for fusing). Then, the degraded feature Zs\nd fuses\nwith its corresponding priors Z0\np matched from ROHQD D\nor previous fused output Zs\np with MHCAs. Finally, a high-\nquality face \u02c6Id is restored from the final fused result ZS\np by\nthe decoder Dd.\nWe will introduce the details of the restoration process\nin Sec. 3.1 and describe the learning of the reconstruction-\noriented high-quality dictionary (ROHQD) in Sec. 3.2. Be-\nsides, we will explain our extending degraded model (EDM)\nused for synthesizing degraded face images in Sec. 3.3.\n3.1\nRestoration\nRestoreFormer++ aims at globally modeling the contextual\ninformation in a face and the interplay with priors for restor-\ning a high-quality face image with both realness and fidelity.\nViT (Vision Transformer) [38] is such an effective method for\nmodeling contextual information in computer vision. How-\never, most of the previous ViT-based methods [24], [25], [26],\n[27] model the contextual information with multi-head self-\nattention (MHSA) whose queries, keys and values are from\ndifferent patches in the same image. In this work, we propose\nto simultaneously model the contextual information and the\ninterplay between the degraded face and its corresponding\npriors. Therefore, our RestoreFormer++ adopts multi-head\ncross-attention (MHCA) mechanisms whose queries are from\nthe features of degraded faces, while key-value pairs are\nfrom the corresponding priors. To clarify the delicate design\nof our MHCA for blind face restoration, we will first describe\nMHCA by comparing it with MHSA before going deep into\nthe restoration process.\nMHSA. As Fig. 2 (a) shown, MHSA aims at searching the\ncontextual information in one source (for convenience, we set\nit as our degraded feature Z0\nd \u2208 RH\u2032\u00d7W \u2032\u00d7C, where H\u2032, W \u2032\nand C are the height, width and the number of channels of\nthe feature map, respectively). Its queries Q, keys K, and\nvalues V can be formulated as:\nQ = Z0\ndWq + bq , K = Z0\ndWk + bk , V = Z0\ndWv + bv,\n(1)\nwhere Wq/k/v \u2208 RC\u00d7C and bq/k/v \u2208 RC are learnable\nweights and bias.\nMulti-head attention is a mechanism for attaining pow-\nerful representations. It is implemented by separating\nthe Q, K, and V into Nh blocks along the channel di-\nmension and gets {Q1, Q2, . . . , QNh}, {K1, K2, . . . , KNh},\nand {V1, V2, . . . , VNh}, where Qi/Ki/Vi \u2208 RH\u2032\u00d7W \u2032\u00d7Ch,\nCh =\nC\nNh , and i \u2208 [0, Nh \u2212 1]. Then the attention map\nis represented as:\nZi = softmax(QiK\u22ba\ni\n\u221aCh\n)Vi, i = 0, 1, . . . , Nh \u2212 1.\n(2)\nBy concatenating all Zi, we get the final output of multi-head\nattention:\nZmh =\nconcat\ni=0,...,Nh\u22121 Zi.\n(3)\nIn the conventional transformer, the attention output is\nadded back to the input before sequentially processed by a\nnormalization layer and a feed-forward network, which can\nbe formulated as:\nZa = FFN(LN(Zmh + Z0\nd)),\n(4)\nwhere LN is a layer normalization, FFN is a feed-forward\nnetwork implemented with two convolution layers, and Za\nis the final output of MHSA.\nMHCA. As shown in Fig. 2 (b), since the MHCA adopted\nin our Restoreformer++ aims at modeling the contextual\ninformation in the face images and simultaneously attaining\nidentity information in the degraded face and high-quality\nfacial details in the priors, it takes both the degraded feature\nZ0\nd and the corresponding priors Z0\np as inputs. In MHCA,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nthe queries Q are from the degraded feature Z0\nd while the\nkeys K and values V are from the priors Z0\np:\nQ = Z0\ndWq + bq , K = Z0\npWk + bk , V = Z0\npWv + bv,\n(5)\nIts following operations for attaining the multi-head atten-\ntion output Zmh are the same as Eq. 2 and Eq. 3. Since\nhigh-quality priors play a more important role in blind\nface restoration, Zmh is added with Z0\np instead of Z0\nd in\nRestoreFormer++. The rest operations are:\nZf = MHCA(Z0\nd, Z0\np) = FFN(LN(Zmh + Z0\np)).\n(6)\nRestoration. As described before, the restored pipeline\nconsists of four components. The first component Ed is used\nfor extracting multi-scale features Zs\nd (s = {0, 1, . . . , S \u2212 1},\nS means the number of scales) from the degraded face image\nId. Then, we can get the priors Z0\np of Z0\nd from ROHQD\nD = {dm}M\u22121\nm=0 (dm \u2208 RC, M is the number of elements in\nD) with minimum Euclidean distance:\nZ0\np(i, j) = arg min\ndm\u2208D\n\u2225Z0\nd(i, j) \u2212 dm\u22252\n2,\n(7)\nwhere (i, j) is the spatial position of map Z0\np and Z0\nd and\n|| \u00b7 ||2 means the L2-norm. After attaining the degraded\nfeatures Zs\nd (s = {0, 1, . . . , S \u2212 1}) and Z0\np, these two kinds\nof information are fused in the Fusion Block. In this block,\nfor each scale, the degraded features and priors or previous\nfused results (for convenience, we denote the fused results\nof each scale as Zs\np (s = {1, . . . , S})) are fused with MHCAs,\nwhich consists of K MHCA. We formula this procedure as\nfollows:\nZs+1\np\n= MHCAs(Zs\nd, Zs\np)\n= MHCA(Zs\nd, . . . , MHCA(Zs\nd, MHCA(Zs\nd, Zs\np))),\ns = {0, 1, . . . , S \u2212 1}.\n(8)\nFinally, ZS\np is fed into the rest layers of the decoder Dd for\nrecovering the high-quality face image \u02c6Id.\nLearning. For attaining high-quality faces with both realness\nand fidelity, we design the objective functions from three\naspects: content learning, realness learning, and identity\nlearning.\nContent learning. We adopt L1 loss and perceptual loss [47],\n[48] for ensuring the content consistence between the restored\nface image \u02c6Id and its ground truth Ih:\nLl1 = |Ih \u2212 \u02c6Id|1 ; Lper = \u2225\u03d5(Ih) \u2212 \u03d5(\u02c6Id)\u22252\n2,\n(9)\nwhere Ih is the ground truth high-quality image, \u03d5 is the\npretrained VGG-19 [49], and the feature maps are extracted\nfrom {conv1, . . . , conv5}. Besides, for improving the accu-\nracy of the matched priors, we tend to guide the extracted\nfeatures Z0\nd to approach their selected priors Z0\np and the\ncorresponding objective function is:\nLp = \u2225Z0\np \u2212 Z0\nd\u22252\n2.\n(10)\nRealness learning. We adopt adversarial losses for the\nlearning of realness in this work. Since some crucial facial\ncomponents, such as the eyes and mouth, play an important\nrole in face presenting [19], our adversarial losses are not\nonly applied to the whole face image but also applied to\nVGG\nK-means\n\u2026\n\u2026\nComponent  Dictionaries\nROHQD\n\u2026\nVector \nQuantization\nDh\nEh\n\ud835\udc81\u210e\n\ud835\udc81\ud835\udc5d\n\ud835\udc70\u210e\n\ud835\udd3b\n(a) Component Dictionaries\n(b) ROHQD\nFig. 3. Recognition-Oriented Dictionary v.s. Reconstruction-Oriented\nDictionary. (a) Component Dictionaries, proposed in DFDNet [1], are\nrecognition-oriented dictionaries since they are extracted with an off-\nline image recognition model (VGG [49]). (b) ROHQD, proposed in\nthis paper, is a reconstruction-oriented dictionary since it is learned\nwith a high-quality face generation network incorporating the idea of\nvector quantization [28]. Priors from ROHQD contain more facial details\nspecifically aimed at face restoration.\nthese key facial components independently. These losses are\nexpressed as:\nLadv = [log D(Ih) + log(1 \u2212 D(\u02c6Id))],\nLcomp =\nX\nr\n[log Dr(Rr(Ih)) + log(1 \u2212 Dr(Rr(\u02c6Id)))], (11)\nwhere D and Dr are the discriminators for the whole face\nimage and a certain region r (r \u2208{left eye, right eye, mouth}),\nrespectively. The region r is attained with Rr implemented\nwith ROI align [50].\nIdentity learning. In this work, except extracting the identity\ninformation from the degraded face by fusing it with the\nhigh-quality priors, we also adopt an identity loss [19] to\nattain the identity supervision from the ground truth:\nLid = \u2225\u03b7(Ih) \u2212 \u03b7(\u02c6Id)\u22252\n2,\n(12)\nwhere \u03b7 denotes the identity feature extracted from Arc-\nFace [51] which is a well-trained face recognition model.\nTherefore, the overall objective function is:\nLRF = Ll1 + \u03bbperLper + \u03bbpLp + \u03bbadvLadv\n+ \u03bbcompLcomp + \u03bbidLid,\n(13)\nwhere \u03bb... are the weighting factors for different losses.\n3.2\nReconstruction-Oriented High-Quality Dictionary\nIn this subsection, we introduce the learning of the\nReconstruction-Oriented High-Quality Dictionary (ROHQD)\nD = {dm}M\u22121\nm=0 (dm \u2208 RC, M is the number of elements)\nused in RestoreFormer++.\nDifferent from the facial component dictionaries [1]\n(Fig. 3 (a)) whose elements are high-quality facial details\nof specific facial components extracted with an off-line\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\n\ud835\udc70\ud835\udc89(\ud835\udc70\ud835\udc82)\n\ud835\udc7a\ud835\udc89\ud835\udc8a\ud835\udc87\ud835\udc95\n\ud835\udc8c\ud835\udf48\ud835\udfd0\n\ud835\udc8c\ud835\udf48\ud835\udfcf\n\ud835\udf36\ud835\udfcf\n\ud835\udf36\ud835\udfd0\n\u2193\ud835\udc93\ud835\udfd0\n\u2193\ud835\udc93\ud835\udfcf\n\ud835\udc8f\ud835\udf39\ud835\udfd0\n\ud835\udc3d\ud835\udc43\ud835\udc38\ud835\udc3a\ud835\udc92\ud835\udfd0\n\ud835\udc3d\ud835\udc43\ud835\udc38\ud835\udc3a\ud835\udc92\ud835\udfcf\n\ud835\udc8f\ud835\udf39\ud835\udfcf\n\u2191\ud835\udc93\ud835\udfd0\n\u2191\ud835\udc93\ud835\udfcf\n\ud835\udc74\n\ud835\udfcf \u2212 \ud835\udc74\n\ud835\udc70\ud835\udc85\nFig. 4. The whole pipeline of the extending degrading model (EDM). The degradations represented in GREEN are the operations contained in\nthe traditional degrading model (Eq. 19) while the degradations in RED are the additional operations extended by EDM (Eq. 20 to 22). Specifically, a\nhigh-quality face image Ia is first shifted with an operator Shift. Then, it is sequentially degraded with blur, haze, downsampling, noise, and JPEG\ncompression. The degraded face images will be upsampled back to the size of the original image. The degraded faces attained after \u2191r1 and \u2191r2 are\ndegraded from the same high-quality face image but with two different degraded parameters: \u03b11 and \u03b12, \u03c31 and \u03c32, r1 and r2, \u03b41 and \u03b42, and q1 and\nq2. They are independently and randomly sampled from their own uniform distributions. Combining these two degraded faces with a mask M, the\nfinal unevenly degraded face image Id is attained.\nrecognition-oriented model (VGG [49]), our ROHQD pro-\nvides richer high-quality facial details specifically aimed at\nface reconstruction. We achieve this goal by deploying a\nhigh-quality face encoder-decoder network with the idea\nof vector quantization [28]. As shown in Fig. 3 (b), this\nencoder-decoder network takes a high-quality face image\nIh \u2208 RH\u00d7W \u00d73 as input and encodes it to feature Zh \u2208\nRH\u2032\u00d7W \u2032\u00d7C with encoder Eh. Then, instead of decoding Zh\ndirectly back to the high-quality face with decoder Dh, it\nquantizes feature vectors in Zh with the index of the nearest\nvectors in D and attains Zp \u2208 RH\u2032\u00d7W \u2032\u00d7C:\nZp(i, j) = arg min\ndm\u2208D\n\u2225Zh(i, j) \u2212 dm\u22252\n2,\n(14)\nwhere (i, j) is the spatial position of map Zp and Zh. Finally,\na high-quality face image \u02c6Ih is restored from Zp by the\ndecoder Dh.\nLearning. The whole pipeline shown in Fig. 3 (b) is essentially\na high-quality face generation network. Therefore, we apply\nan L1 loss, a perceptual loss, and an adversarial loss to the\nfinal result \u02c6Ih with the supervision from its high-quality\ninput Ih:\nL\u2032\nl1 = \u2225Ih \u2212 \u02c6Ih\u22251,\nL\u2032\nper = \u2225\u03d5(Ih) \u2212 \u03d5(\u02c6Ih)\u22252\n2,\nL\u2032\nadv = [log D(Ih) + log(1 \u2212 D(\u02c6Ih))].\n(15)\nThe definitions of \u03d5 and D are same as Eq. 9 and Eq. 11. It\nis worth noting that since Eq 14 is non-differentiable, the\ngradients back-propagated from \u02c6Ih reach Zh by copying the\ngradients of Zp to Zh directly [28].\nThe ultimate goal of ROHQD in this work is to optimize\nD to attain high-quality facial details used for face restoration.\nTherefore, we update the elements dm constructed Zp\n(Eq. 14) by forcing them to be close to their corresponding\nhigh-quality features Zh with L2 loss:\nL\u2032\nd = \u2225 sg[Zh] \u2212 Zp\u22252\n2,\n(16)\nwhere sg[\u00b7] denotes the stop-gradient operation. Besides, as\ndescribed in [28], to avoid collapse, a commitment loss is\nneeded to adjust the learning pace of the encoder Eh and\ndictionary D. The commitment loss is represented as:\nL\u2032\nc = \u2225Zh \u2212 sg[Zp]\u22252\n2.\n(17)\nFinally, the objective function for learning ROHQD is:\nLROHQD = L\u2032\nl1 + \u03bbperL\u2032\nper + \u03bbadvL\u2032\nadv + \u03bbdL\u2032\nd + \u03bbcL\u2032\nc,\n(18)\nwhere \u03bb... are the weighting factors.\n3.3\nExtending Degrading Model\nTo diminish the distance between the synthetic training data\nand the real-world data and further improve the robustness\nand generalization of RestoreFormer++, EDM extends the\ndegrading model [19], [21], [22] whose original expression is:\nId = {[(Ih \u2297 k\u03c3) \u2193r +n\u03b4]JP EGq} \u2191r,\n(19)\nwhere Ih is a high-quality face image and Id is the final syn-\nthetic degraded face image. Ih is first blurred by a Gaussian\nblur kernel k\u03c3 with sigma \u03c3. Then, it is downsampled by r\nwith bilinear interpolation and added with a white Gaussian\nnoise n\u03b4 whose sigma is \u03b4. Next, the intermediate degraded\nresult is further compressed with JPEG compression, whose\nquality is q. After that, it is upsampled back to the size of\nIh with scale r. Then we get the final synthetic degraded\nface image Id. These operations are sequentially described\nin Fig. 4 with GREEN color.\nExcepting the common degradations described in Eq. 19,\nEDM adds haze and uneven degradation with a certain\nprobability since they also obviously exist in the real-world\ndegraded faces (examples are in Fig. 8). In addition, EDM\nalso attempts to ease the error introduced by face alignment\nin real-world data (the third sample in Fig. 6 and the second\nsample in Fig. 8) by disturbing the perfect alignment in the\nsynthetic training set with a spatial shift operation. The EDM\nis expressed as:\nIh = Shift(Ia, sh, sw),\n(20)\nIe\nd = {[(\u03b1(Ih\u2297k\u03c3)+(1\u2212\u03b1)Ihaze) \u2193r +n\u03b4]JP EGq} \u2191r, (21)\nId =M \u2299 Ie\nd(\u03b11, \u03c31, r1, \u03b41, q1)+\n(1 \u2212 M) \u2299 Ie\nd(\u03b12, \u03c32, r2, \u03b42, q2).\n(22)\nIa is the well aligned high-quality face image (Ia is equal to\nIh in Eq. 19) and Shift(\u00b7) means spatially shifting Ia with sh\nand sw pixels in height and width dimensions, respectively.\nThen the degraded face image is synthesized from the shifted\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\nhigh-quality face image Ih. We synthesize haze in Eq. 21.\nBefore downsampled, the blurry face image will be combined\nwith Ihaze with ratio \u03b1 : (1\u2212\u03b1), \u03b1 \u2208 [0, 1]. Ihaze is a globally\nwhite image. In Eq. 21, the degraded result Ie\nd is a globally\nevenly degraded face image. To attain an unevenly degraded\nface image Id, we first synthesize two evenly degraded\nfaces, Ie\nd(\u03b11, \u03c31, r1, \u03b41, q1) and Ie\nd(\u03b12, \u03c32, r2, \u03b42, q2), whose\nparameters: \u03b11 and \u03b12, \u03c31 and \u03c32, r1 and r2, \u03b41 and \u03b42,\nand q1 and q2, are independently and randomly sampled\nfrom uniform distributions (the experimental setting of\nthe uniform distribution of each parameter in this paper\nis described in Subsec. 4.2). Then we combine these two\nunevenly degraded face images with a mask map M whose\nsize is the same as Ie\nd. The whole map of M is set to 0 except\nthat a random L \u00d7 L patch of it is set to 1 (L is smaller\nthan both the height and width of Ie\nd). \u2299 is an element-\nwise multiplication operation. The whole pipeline of EDM\nis described in Fig. 4, and the operations in RED are the\nadditional degradations extended by EDM.\n4\nEXPERIMENTS AND ANALYSIS\n4.1\nDatasets\nTraining Datasets. ROHQD is trained on FFHQ [29], which\ncontains 70000 high-quality face images resized to 512 \u00d7 512.\nRestoreFormer++ is also trained on synthesized data attained\nby applying EDM to the high-quality face images in FFHQ.\nTesting Datasets. We evaluate RestoreFormer++ on one\nsynthetic dataset and three real-world datasets. The synthetic\ndataset, CelebA-Test [52], contains 3000 samples and is\nattained by applying EDM on the testing set of CelebA-\nHQ [52]. The three real-world datasets include LFW-Test [53],\nCelebChild-Test [19], and WebPhoto-Test [19]. Specifically,\nLFW-Test contains 1711 images and is built with the first\nimage of each identity in the validation set of LFW [53]. Both\nCelebChild-Test and WebPhoto-Test are collected from the\nInternet by Wang et al. [19]. They respectively own 180 and\n407 degraded face images.\n4.2\nExperimental Settings and Metrics\nSettings. The encoder and decoder in the RestoreFormer++\nand ROHQD are constructed with 12 residual blocks and 5\nnearest downsampling/upsampling operations. Each MH-\nCAs contains K = 3 MHCA. The input size of the model is\n512\u00d7512\u00d73. After encoding, the size of Zd is 16\u00d716\u00d7256.\nROHQD contains M = 1024 elements whose length is 256.\nAs for EDM, sh, sw, \u03b1, \u03c3, r, \u03b4, q, and L are randomly sampled\nfrom{0 : 32}, {0 : 32}, {0.7 : 1.0}, {0.2 : 10}, {1 : 8},\n{0 : 20}, {60 : 100}, and {128 : 256}, respectively. While\ntraining, the batch size is set to 16 and the weighting factors\nof the loss function are \u03bbper = 1.0, \u03bbp = 0.25, \u03bbadv = 0.8,\n\u03bbcomp = 1.0, \u03bbid = 1.0, \u03bbd = 1.0, and \u03bbc = 0.25. Both\nRestoreFormer++ and ROHQD are optimized by Adam [54]\nwith learning rate 0.0001. Noted that we do not update the\nelements of the ROHQD while training RestoreFormer++.\nMetrics. In this paper, we evaluate the state-of-the-art meth-\nods and our RestoreFormer++ objectively and subjectively.\nFrom the objective aspect, we adopt the widely-used non-\nreference metric FID [55] to evaluate the realness of the\nrestored face images and introduce an identity distance\n(denoted as IDD) to judge the fidelity of the restored face\nimages. IDD is the angular distance between the features of\nthe restored face image and its corresponding ground truth.\nFeatures are extracted with a well-trained face recognition\nmodel ArcFace [51]. Besides, we adopt PSNR, SSIM, and\nLPIPS [56] to build a more comprehensive comparison. From\nthe subjective aspect, we deploy a user study to evaluate\nthe quality of the restored results from the perspective of\nhumans.\n4.3\nComparison with State-of-the-art Methods\nIn this subsection, we compare our RestoreFormer++ with\nstate-of-the-art prior-based methods, including DFDNet [1]\nbased on component dictionaries, PSFRGAN [4] imple-\nmented with facial parsing maps, Wan et al. [2], PULSE [3],\nGPEN [5], and GFP-GAN [19] restored with generative priors,\nand VQFR [6] utilized codebook. We also compare Restore-\nFormer++ with our conference version, RestoreFormer. Com-\npared to RestoreFormer++, RestoreFormer is trained with\nsynthetic data attained with the traditional degrading model\nrather than EDM and its fusion between the degraded face\nand priors only involves one scale. Comparisons between\nthese methods and our proposed method are conducted on\nsynthetic and real-world datasets.\n4.3.1\nPerformance on Synthetic Dataset\nThe quantitative results of the aforementioned state-of-the-\nart methods and our RestoreFormer++ on the synthetic\ndataset CelebA-Test [52] are in TABLE 1. We can see that\nRestoreFormer++ performs better than other methods on\nFID and IDD, which means that the restored faces of\nRestoreFormer++ are more real and their identities are closer\nto the degraded faces. Our RestoreFormer++ also achieves\ncomparable performance in terms of PSNR, SSIM, and LIPIS,\nwhich are pixel-wise and perceptual metrics. These metrics\nhave been proved not that consistent with the subjective\njudgment of human beings [48], [58]. We also find that the\nvisualized results of GPEN [5] which performs better on\nPSNR, SSIM, and LIPIS are over-smooth and lack details.\nVisualized results are shown in Fig. 5. Compared to other\nmethods, the restored results of our RestoreFormer++ have\na more natural look and contain more details, especially\nin the eyes, mouth, and glasses. Besides, our method can\nrestore a more complete face, such as the left eye in the\nfirst sample and the glasses in the second sample. Due to\nsevere degradations, most existing methods fail to restore\nthe left eye and glasses, although they can properly restore\nthe right eye and part of the glasses. On the contrary, since\nour RestoreFormer++ can model the contextual information\nin the face, its restored left eye and glasses are more natural\nand complete by utilizing the related information in the right\neye area and the clear part of the glasses. The quantitative\nresults in TABLE 1 show that RestoreFormer++ attains an\nobvious improvement compared to the conference version,\nRestoreFormer, due to the participation of EDM and multi-\nscale mechanism. More detailed analyses of the contribu-\ntions of these components are discussed in Subsec. 4.4.3\nand Subsec. 4.4.5, and more visualized results are in the\nsupplementary materials.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nInput\nDFDNet [1]\nPSFRGAN [4] GFP-GAN [19]\nGPEN [5]\nVQFR [6]\nOurs\nGT\nFig. 5. Qualitative comparison on the CelebA-Test [52]. The results of our RestoreFormer++ have a more natural and complete overview and\ncontain more details in the areas of eyes, mouth, and glasses. Note that DFDNet [1] relies on dlib [57] for facial detection while matching priors from\nits facial component dictionaries, and failure in detection results in no restoration, as seen in the second result. Zoom in for a better view.\nInput\nDFDNet [1]\nWan et al. [2]\nPSFRGAN [4] GFP-GAN [19]\nGPEN [5]\nVQFR [6]\nOurs\nFig. 6. Qualitative comparison on three real-world datasets: LFW-Test [53], CelebChild-Test [19], and WebPhoto-Test [19] (from top to down,\nrespectively). The results of our RestoreFormer++ have a more natural and complete overview and contain more details in the areas of eyes, glasses,\nhair, and mouth. The green points on the third degraded face image are the reference landmarks used for face alignment. In this sample, its mouth is\naligned to the landmark of the nose, and the existing methods, e.g., [2], [4], restore the mouth with a nose-like shape. Benefiting from the spatial shift\nadopted in EDM, our restored result looks more natural. Zoom in for a better view.\n4.3.2\nPerformance on Real-world Datasets\nThe quantitative and qualitative results of our Restore-\nformer++ and the compared methods on three real-world\ndatasets are in TABLE 2 and Fig. 6, respectively. Accord-\ning to TABLE 2, RestoreFormer++ performs superiorly on\nFID compared to other methods. The qualitative results\nin Fig. 6 also reveal that although most of the current\nmethods can attain clear faces from the corrupted face\nimages with slight degradations (the first two samples),\nRestoreFormer++ attains more details on the crucial areas,\nsuch as the eyes with glasses, hair, and ear. That mainly\nbenefits from the contextual information in the face and\nour learned reconstruction-oriented high-quality dictionary.\nBesides, since our RestoreFormer++ is further enhanced with\nEDM, it can remove the haze covered on the face image\nand avoid restoration artifacts caused by misalignment, thus\nattaining more natural and pleasant results. For example,\nafter face alignment, the mouth of the last sample in Fig. 6 is\naligned to the reference landmark of the nose, which leads\nto the restored mouth of Wan et al. [2] and PSFRGAN [4]\nis nose-like. Although the restored results of other existing\nmethods look better, they still look weird. With EDM, the\nrestored result of RestoreFormer++ looks more natural.\nIn addition, as shown in TABLE 2, in the real-world\ndatasets, the performance of RestoreFormer++ is better\nor comparable to our conference version, RestoreFormer.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nTABLE 1\nQuantitative comparisons on CelebA-Test [52]. Our RestoreFormer++\nperforms better in terms of FID and IDD, which indicates the realness\nand fidelity of the restored results of our method. It also gets comparable\nresults on PSNR, SSIM, and LPIPS.\nMethods\nFID\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nIDD\u2193\nInput\n132.05\n24.91\n0.6637\n0.4986\n0.9306\nDFDNet [1]\n50.88\n24.09\n0.6107\n0.4516\n0.7700\nWan et al. [2]\n67.13\n23.01\n0.6174\n0.4789\n0.8058\nPSFRGAN [4]\n40.69\n24.30\n0.6273\n0.4220\n0.7284\nPULSE [3]\n84.03\n20.73\n0.6151\n0.4745\n1.2267\nGPEN [5]\n48.97\n25.44\n0.6965\n0.3562\n0.6434\nGFP-GAN [19]\n40.87\n24.39\n0.6671\n0.3575\n0.6127\nVQFR [6]\n38.51\n23.82\n0.6379\n0.3544\n0.6354\nRestoreFormer\n39.90\n24.19\n0.6232\n0.3716\n0.5677\nRestoreFormer++\n38.41\n24.40\n0.6339\n0.3619\n0.5375\nGT\n41.66\n\u221e\n1\n0\n0\nTABLE 2\nQuantitative comparisons on three real-world dataset in terms of FID.\nRestoreFormer++ performs better.\nMethods\nLFW-Test\nCelebChild-Test\nWebPhoto-Test\nInput\n126.12\n144.36\n170.46\nDFDNet [1]\n72.87\n110.85\n100.45\nPSFRGAN [4]\n53.17\n105.65\n83.50\nWan et al. [2]\n71.24\n115.15\n99.91\nPULSE [3]\n66.08\n104.06\n86.39\nGPEN [5]\n55.52\n107.57\n86.07\nGFP-GAN [19]\n50.30\n111.78\n87.82\nVQFR [6]\n50.22\n103.96\n74.22\nRestoreFormer\n48.11\n104.01\n75.49\nRestoreFormer++\n48.48\n102.66\n74.21\nRestoreFormer is slightly superior to RestoreFormer++ on\nLFW-Test [53] since the degree of the degradation in this\ndataset is generally slight, and the delicate design in Restore-\nFormer is enough for attaining high-quality restored results.\nHowever, since the degradation in CelebChild-Test [19] and\nWebPhoto-Test [19] are more severe, RestoreFormer++, with\nadditional EDM and multi-scale mechanism, can handle\nthese two datasets better compared to RestoreFormer. More\nvisualizations are in the supplementary materials.\nBesides, a user study is adopted to collect the subjec-\ntive judgment of human beings on the real-world dataset\nWebPhto-Test [19]. Specifically, we randomly select 100\nsamples from the real-world dataset and conduct pair\ncomparisons between our conference version RestoreFormer\nand three other methods: DFDNet [1], PSFRGAN [4], and\nGFP-GAN [19]. Subjective comparisons between Restore-\nFormer++, RestoreFormer, and VQFR [6] are also conducted.\nWe invite 100 volunteers to make their subjective selection\non these pair comparisons. The statistic results are in Tab 3.\nIt shows that a high percentage of volunteers vote for the\nresults of our RestoreFormer and RestoreFormer++ as the\nmore natural and pleasant restored results compared to other\nmethods, and the restored results of RestoreFormer++ are\nbetter than those of RestoreFormer.\nTABLE 3\nUser study results on WebPhoto-Test [19]. For \u201ca/b\u201d, a is the percentage\nwhere our RestoreFormer or RestoreFomer++ is better than the\ncompared method, and b is the percentage where the compared method\nis considered better than our RestoreFormer or RestoreFomer++.\nMethods\nDFDNet [1]\nPSFRGAN [4]\nGFP-GAN [19]\nRestoreFormer\n89.60%/10.40%\n68.81%/31.19%\n79.21%/20.79%\nMethods\nVQFR [6]\nRestoreFormer\nRestoreFormer++ 67.82%/32.18%\n66.91%/33.19%\n4.4\nAblation Study\nOur proposed RestoreFormer++ consists of several com-\nponents, including MHCA, EDM, ROHQD, multi-scale\nmechanism, and several losses. It also contains two kinds\nof inputs: the degraded face and high-quality priors. Each\ncomponent plays an important role in the whole restoration\npipeline. The followings are the detailed analyses of the\neffectiveness of these components. A discussion about the\nefficiency of our proposed method is also included.\n4.4.1\nAnalysis of Spatial Attention Mechanism\nIn RestoreFormer++, global spatial attention mechanism\nis used to model the rich facial contextual information\nin the face image and its interplay with priors for aiding\nthe face restoration. To validate the effectiveness of the\nspatial attention mechanism, we compare our single-scale Re-\nstoreFormer++ with and without attention mechanisms. As\nshown in TABLE 4, both exp1 and exp2 only get information\nfrom the degraded face image. By adopting self-attention\n(MHSA) to model contextual information, exp2 performs\nbetter than exp1 which is without MHSA in terms of FID and\nIDD. This conclusion is also valid when comparing exp4 to\nexp6, whose inputs include both degraded information and\nadditional high-quality priors. In exp4, we replace MHCA in\nRestoreFormer++ with SFT [23] for locally fusing these two\nkinds of information. Since it ignores the facial contextual\ninformation in the face image, its result in Fig. 7 (d) fails to\nrestore natural eyes. Exp6 is a version of RestoreFormer++\nimplemented with a single-scale fusion mechanism. It uses\nMHCA for globally fusing degraded information and priors.\n(1)-(4) in Fig. 7 are its multi-head (4 heads) attention maps\nof the left eye region in scale 16 \u00d7 16. It shows that the\nhighlighted areas not only occur in the left eye area but also\nin other regions of the face image, especially the more related\nright eye region. It means that apart from the information\nin the left areas, our RestoreFormer++ with MHCA can also\nutilize the related information in other areas to restore the\nleft eye with more natural appearance (Fig. 7 (e)).\n4.4.2\nAnalysis of Degraded Information and Priors.\nIn this subsection, we analyze the roles of the degraded\ninformation extracted from the degraded face image and its\ncorresponding high-quality priors matched from ROHQD.\nIn exp2 and exp3 (TABLE 4), we replace the MHCA in our\nsingle-scale RestoreFormer++ with MHSA, whose queries,\nkeys, and values are all from either the degraded information\nor the high-quality priors. We can see that exp2 attains a\nbetter average IDD score which means it performs better in\nfidelity. In contrast, exp3 has a better FID score, meaning\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\n(a) Input\n(b) exp2\n(c) exp3\n(d) exp4\n(e) exp6\n(f) exp7\n(g) RestoreFormer++\n(h) GT\nPSNR: 26.21\nPSNR: 25.29\nPSNR: 23.83\nPSNR: 23.98\nPSNR: 25.40\nPSNR: 25.97\nPSNR: 26.56\nPSNR: \u221e\nIDD: 1.0689\nIDD: 0.5322\nIDD: 0.7981\nIDD: 0.6239\nIDD: 0.5220\nIDD: 0.4560\nIDD: 0.4689\nIDD: 0\n(1) s16h0\n(2) s16h1\n(3) s32h0\n(4) s32h1\n(5) s64h0\n(6) s64h1\nFig. 7. The qualitative visualizations from (b) to (g) are results of the experiments whose settings correspond to those in TABLE 4. The result of (e)\nexp6, which takes the degraded face and priors as inputs, is better than the results of (b) exp2 and (c) exp3 in both realness and fidelity since exp2\nand exp3 only take one of these two kinds of resources as input. By globally fusing the features of degraded faces and priors with MHCAs, (e) exp6\nalso performs better than (d) exp4 implemented with the local fusion algorithm SFT [19]. In (g), RestoreFormer++ with a two-scale setting can avoid\nthe weird eye shape restored in (e) exp6 implemented with a single-scale setting, but while extending to a three-scale setting, the result of (f) exp7\ncannot see an obvious improvement compared to (g). Images from (1) to (6) are the heatmaps of the left eye area attained on different scales. \u2018sxhy\u2019\nmeans the y-th (y \u2208 {0, 1}) head attention map in x \u00d7 x (x \u2208 {16, 32, 64}) resolution. In the low resolution, heatmaps (1)-(2) mainly focus on the\nmost related eye areas while in the middle resolution, heatmaps (3)-(4) expand to salient edges that help the restoration of the shape of the left\neye. In high resolution, heatmaps (5)-(6) focus on more detailed edges. They yield less further improvement on the final restoration, and thus our\nRestoreFormer++ adopts a two-scale setting.\nTABLE 4\nQuantitative results of ablation studies on CelebA-Test [52]. \u2018degraded\u2019 and \u2018prior\u2019 mean fusion information from degraded input and ROHQD,\nrespectively. \u2018none\u2019 and \u2018MHSA\u2019 respectively mean the network uses either \u2018degraded\u2019 or \u2018prior\u2019 information without or with a self-attention mechanism.\n\u2018SFT\u2019, \u2018MHCA-D\u2019 and \u2018MHCA-P\u2019 use both \u2018degraded\u2019 and \u2018prior\u2019 information. \u2018SFT\u2019 uses SFT [19] to fuse the information, while \u2018MHCA-D\u2019 and\n\u2018MHCA-P\u2019 use multi-head cross attention. The difference between \u2018MHCA-D\u2019 and \u2018MHCA-P\u2019 is that \u2018MHCA-D\u2019 fuses Zmh with Zs\nd while \u2018MHCA-P\u2019\nfuses Zmh with Zs\np. \u2018S\u2019 is the number of feature scales used for fusion. S = 1 means the fusion only exists in 16 \u00d7 16 resolution while S = 2 means\nthe fusion are involved in both 16 \u00d7 16 and 32 \u00d7 32 resolutions. S = 3 means it is further extended to 64 \u00d7 64 resolution. The proposed\nRestoreFormer++ integrated with \u2018MHCA-P\u2019 and set with more than one scale performs the best relative to other variants.\nsources\nmethods\nmetrics\nNo. of exp.\ndegraded\nprior\nnone\nMHSA\nSFT\nMHCA-D\nMHCA-P\nS\nFID\u2193\nIDD\u2193\nexp1\n\u2713\n\u2713\n1\n48.33\n0.6520\nexp2\n\u2713\n\u2713\n1\n47.96\n0.6461\nexp3\n\u2713\n\u2713\n1\n42.53\n0.7467\nexp4\n\u2713\n\u2713\n\u2713\n1\n44.67\n0.6373\nexp5\n\u2713\n\u2713\n\u2713\n1\n42.25\n0.6038\nexp6\n\u2713\n\u2713\n\u2713\n1\n39.31\n0.5677\nexp7\n\u2713\n\u2713\n\u2713\n3\n39.11\n0.5355\nRestoreFormer++\n\u2713\n\u2713\n\u2713\n2\n38.41\n0.5375\nits results contain more realness. By globally fusing the\ndegraded information and priors with MHCA in our single-\nscale RestoreFormer++ (exp6 in TABLE 4), it performs better\nthan exp2 and exp3 in both IDD and FID, which means that\nour RestoreFormer++ can restore faces with both realness\nand fidelity. The visualized results in Fig. 7 show that the\nresult of exp2 (Fig. 7 (b)) is more similar to GT but contains\nfewer details compared to (c) and (e), which are the results\nof exp3 and exp6, respectively. Although the details in (c) are\nricher, it looks less similar to the GT, especially in the eyes.\nOn the contrary, Our result shown in (e) is similar to GT and\nmeanwhile contains rich details, and thus presents pleasantly.\nBesides, according to Fig. 2 (b) and Eq. 6, we tend to add the\nattended feature Zmh to Z0\np rather than Z0\nd (corresponding\nto exp5 in TABLE 4), since we experimentally find that it can\nattain better performance.\n4.4.3\nAnalysis of Multi-scale Mechanism\nOur multi-scale mechanism aims to facilitate Restore-\nFormer++ by modeling contextual information based on both\nsemantic and structural information, thereby improving the\nrestoration performance in both realness and fidelity. First,\nwe apply MHCAs to fuse the degraded features and priors\nat a resolution of 16 \u00d7 16, which is the smallest resolution\nin our model (this setting corresponds to exp6 in TABLE 4).\nThe features of a face at this scale are semantic information\nof facial components, such as eyes, mouth, nose, etc. The\nhighlighted areas in the attention maps of the left eye in\nFig. 7 (1)-(2) are eyes areas, which reveal that the restoration\nof the left eye in Fig. 7 (e) is achieved by leveraging contextual\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\nTABLE 5\nQuantitative results of methods with or without EDM measured on FID\u2193.\nMethods with EDM perform better than those without EDM on\nCelebChild-Test [19] and WebPhoto-Test [19] datasets whose\ndegradations are more diverse and severe and perform comparably on\nLFW-Test [53] dataset with more common degradations.\nRestoreFormer++ is better than the other methods in both settings.\nMethods\nLFW-Test [53]\nCelebChild-Test [19]\nWebPhoto-Test [19]\nPSFRGAN [60]\n53.17\n105.65\n83.50\nPSFRGAN w/ EDM\n53.20\n104.22\n82.28\nGFP-GAN [19]\n50.30\n111.78\n87.82\nGFP-GAN w/ EDM\n50.72\n109.08\n86.17\nOurs w/o EDM\n48.10\n103.86\n75.42\nOurs\n48.48\n102.66\n74.21\ninformation from its semantic-related areas. Compared with\nthe results in (d) attained with SFT [19], a spatial-based fusion\napproach, the restored left eye of (e) is more complete and\nreal. However, its edge shape is not smooth enough, leading\nto a weird look. Therefore, we extend MHCAs to features\nwith a larger scale, 32\u00d732 (corresponding to Restoreformer++\nin TABLE 4), and attain a restored result with a more natural\nlook as shown in Fig. 7 (g). Its corresponding attention maps\nin Fig. 7 (3)-(4) show that apart from related eye areas, its\nhighlighted areas diffuse to some salient edges that help\nreconstruct the smooth and natural shape of the left eye . FID\nand IDD scores on CelebA-Test [52] in TABLE 4 indicate that\nincreasing the number of scales from one to two can improve\nrestoration performance in both realness and fidelity. To\nmake further exploration, we extend MHCAs to the features\nat a resolution of 64\u00d764 (corresponding to exp7 in TABLE 4).\nIts attention maps (Fig. 7 (5)-(6)) focus on more detailed\nstructures such as hairs. However, its restored result in\nFig. 7 (f) does not show an obvious improvement compared\nto (g) attained with a two-scale setting. Its quantitative results\nin TABLE 4 show that it attains a better IDD score but worse\nFID score than RestoreFormer++ implemented with a two-\nscale setting. Comprehensively considering efficiency, where\nthe running time of the three-scale setting increases by about\n17% compared to the two-scale setting (TABLE 7), we adopt\na two-scale setting in RestoreFormer++.\n4.4.4\nAnalysis of ROHQD.\nComparisons between our RestoreFormer++ and DFDNet [1],\nwhose priors are recognition-oriented, have validated the\neffectiveness of ROHQD. To further evaluate the contribution\nof ROHQD in RestoreFormer++, we replace ROHQD with a\nrecognition-oriented dictionary with the same learning pro-\ncess as ROHQD. We implement it by replacing the encoders\nEd and Eh with a VGG [49]. Similar to [1], we initialize\nthese encoders with weights attained with ImageNet [59]\nand freeze them while training. We conduct experiments\non CelebA-Test [52]. Its scores in terms of FID and IDD\nare 50.39 and 0.7572, which is worse than RestoreFormer++\nimplemented with ROHQD. It indicates that the facial details\nin ROHQD that are accordant to reconstruction tasks are\nhelpful for face restoration.\n4.4.5\nAnalysis of EDM.\nIn this subsection, we analyze the effect of EDM on blind\nface restoration. To make a more comprehensive comparison,\nexcept conducting experiments on our RestoreFormer++\nwith and without EDM, we also retrained PSFRGAN [60]\nand GFP-GAN [19] with EDM. The quantitative results in\nTABLE 5 show that methods with EDM perform better or\ncomparable compared to their counterpart without EDM\non three real-world datasets, especially on CelebChild-\nTest [19] and WebPhoto-Test [19], whose degradations are\nmore diverse and severe. Since the degradations of LFW-\nTest [53] are relatively slight and regular, EDM has little\neffect on this dataset. Besides, qualitative results show\nthe notable effectiveness of EDM in real-world face image\nrestoration, particularly towards face images with uneven\ndegradation, haze, and aligned bias. In the first sample in\nFig. 8, the degradation in the red box is uneven (degradation\nin the center is more severe than the neighbouring regions),\nleading to a weird look of the restored right eye (blue area).\nCompared to methods without EDM, their counterpart with\nEDM attains a clearer and more natural right eye. The second\nsample in Fig. 8 is covered with haze, and its left eyebrow\nis aligned to the left eye of the reference face. The restored\nresults of methods without EDM are unclear, and their left\neyes are not in the right position. After fine-tuning with EDM,\ntheir results become clear, and their left eyes are restored to\nthe right position with a more natural look. More restored\nresults of our proposed method with or without EDM are\nshown in Fig. 9.\nDiscussion about color changes. Blind face restoration\naims to remove the degradations in a face image and recover\nits high-quality facial structures. Its colors will be restored if\nthe degraded face image contains colors. For example, pink\ncolor can be observed on the cheek and forehead of the first\nsample in Fig. 1. Methods including Wan et al. [2], PULSE [3],\nGPEN [5], VQFR [6], and our RestoreFormer [7] detect and\nrecover this color. Since our priors matched from ROHQD\nare more accordant to blind face restoration and contextual\ninformation is considered while fusing the degraded face\ninformation and high-quality priors, the color restored with\nour RestoreFormer looks more harmonized and natural.\nBesides, since haze is common in real-world degraded face\nimages, our EDM is proposed to endow RestoreFormer++\nwith the capability of haze removal. As shown in Fig. 10,\nthe capacity of haze removal of RestoreFormer++ aims to\nrecover the original colors of the degraded face image. The\ncolors of the restored results are close to their Ground Truth.\nThe restored result of a gray degraded image is almost gray.\nWith the help of EDM, the restored results of RestorFormer++\ncontain more facial details and look clearer.\nTABLE 6\nQuantitative results on CelebA-Test [53] of different loss settings.\nLl1\nLper\nLp\nLadv\nLcomp\nLid\nFID\u2193\nIDD\u2193\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n45.14\n0.6052\n\u2713\n\u2713\n\u2713\n\u2713\n54.14\n0.5378\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n48.19\n0.5384\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n38.69\n0.5873\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n48.66\n0.5619\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n38.41\n0.5375\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\n(a) Input\n(b) PSFGAN [60]\n(c) PSFGAN w/ EDM\n(d) GFP-GAN [19]\n(e) GFP-GAN w/ EDM\n(f) Ours w/o EDM\n(g) Restoreformer++\nFig. 8. Qualitative results of methods with or without EDM. Methods with EDM can rectify the distortion (area in the blue box) introduced by uneven\ndegradation (area in the red box in the first sample) or misalignment (areas in the red box in the second sample) and remove the haze covered on the\nface image (the second sample). In the input of the second example, the green points are the reference landmarks defined in the training dataset.\nThe reference landmark of the left eye is aligned to the left eyebrow of this sample, leading to the unnatural restored results of the methods without\nEDM. However, the methods with EDM can alleviate this issue.\nInput\nOurs w/o EDM\nRestoreFormer++\nFig. 9. More qualitative results of our methods with or without EDM. With\nthe design of EDM, RestoreFormer++ can remove the haze covered on\nthe faces, mitigate the artifacts raised by uneven degradation, alleviate\nthe influence of bias introduced by face misalignment, and restore faces\nwith a more natural appearance.\n4.4.6\nAnalysis of Losses.\nDuring the training process, RestoreFormer++ involves\nseveral loss functions, including L1 loss (Ll1) and perceptual\nloss ( Lper) for content consistence learning, degradation\nInput\nRestoreFormer++\nGT\nFig. 10. Restored results of RestoreFormer++ on two synthetic degraded\nface images from CelebA-Test [53]. The first sample shows that Restore-\nFormer++ aims to recover the original colors of the degraded face image.\nIt does not change the colors of the restored results compared to the GT.\nThe second sample indicates that our RestoreFormer++ tends to keep\nthe gray degraded face image gray after restoration.\nremoval loss (Lp) for reducing the prior searching error,\nadversarial loss on the whole image (Ladv) and key com-\nponents (Lcomp) for realness learning, and identity loss\n(Lid) for identity learning. To analyze the effect of each\nloss function, we remove one or two of them from the\noriginal RestoreFormer++. Since the training process without\nLl1 tends to collapse, we keep it in all the experiments.\nAs shown in TABLE 6, no matter removing which loss\nfunction, both scores in terms of FID and IDD increase,\nwhich means the realness and fidelity of their results become\nworse. Specifically, Ladv and Lcomp affect the realness of the\nfinal restored results more compared to their fidelity. This\nphenomenon conforms to the characteristics of adversarial\nloss which aims at generating real images. On the contrary,\nLid and Lp affect the fidelity of the restored faces more\nthan their realness. Lid specifically aim at identity reserving.\nLp can reduce the bias introduced by noisy information by\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\nremoving degradations in the encoding time. Lper has similar\neffects on the realness and fidelity of the restored results for\nthe reason that it can keep the semantic information while\ngenerating high-quality images [47].\nTABLE 7\nAnalysis of efficiency in terms of running time, flops, and model size.\nMethods\nTime/s\nFlops/G\nSize/M\nS\nDFDNet [1]\n2.218\n602.732\n240.117\n-\nPSFRGAN [4]\n0.2076\n337.870\n67.026\n-\nGPEN [5]\n0.1193\n168.279\n71.005\n-\nGFP-GAN [19]\n0.0083\n54.734\n76.564\n-\nVQFR [6]\n0.7309\n1071.492\n71.829\n-\nRestoreFormer++\n0.2229\n343.071\n72.680\n1\nRestoreFormer++\n0.2260\n345.490\n73.473\n2\nRestoreFormer++\n0.2643\n374.493\n74.265\n3\n4.5\nAnalysis of Efficiency\nTo analyze the efficiency, we test the running time, Flops,\nand model size of several current methods and our Restore-\nFormer++ with different scale settings on a GeForce GTX\n1060. Comprehensively considering the efficiency in TABLE 7\nand the effectiveness in TABLE 1 and TABLE 2, we can\nsee that RestoreFormer++ attains high performance with\nmore modest resource consumption. Although GPEN [5]\nand GFP-GAN [19] can run faster, their performance is\nslightly inferior to the performance of RestoreFormer++.\nBesides, although the performance of VQFR [6] approaches\nto our RestoreFormer++ and their codebook shares the\nsame idea as our ROHQD, it is time-consuming and needs\nmore computations because its codebook is built at 32 \u00d7 32\nresolution and the fusions between the degraded feature\nand priors involve resolutions from 32 \u00d7 32 to 512 \u00d7 512,\nOn the contrary, our RestoreFormer++ can attain better\nperformance with fusions only in several smaller scales\n(16 \u00d7 16 and 32 \u00d7 32). Since the performance of the three-\nscale setting is comparable to two scales setting but has\na large increase in resource consumption, we adopt the\ntwo-scale setting in our RestoreFormer++. It also validates\nthat by modeling the contextual information in the face\nwith MHCAs, feature interactions in smaller resolutions\nare enough for RestoreFormer++ to attain high-quality face\nimages with both realness and fidelity.\n4.6\nLimitations\nRestoreFormer++ cannot handle faces with obstacles or large\nposes well, which are also two issues for other methods. As\nshown in Fig. 11, the face in the first sample is covered with\na tennis racket. PSFRGAN [4] and GFP-GAN [19] tend to\nremove the obstacle. Although our method can keep most of\nthe tennis racket, it leads to artifacts. In the second sample,\nall the approaches cannot restore complete glasses due to\nthe large facial pose. These limitations mainly result from\nthe bias in the training data \u2013 most of the high-quality face\nimages in FFHQ [29] are near-frontal and without obstacles.\nIn the future, we will try to make an effort to mitigate these\nInput\nPSFRGAN [4]\nGFP-GAN [19]\nRestoreFormer++\nFig. 11. RestoreFormer++ cannot handle faces with obstacles or large\nposes well. Other methods have similar issues.\nlimitations from two aspects: 1) extending the diversity of\nFFHQ [29] with more facial poses and obstacles. 2) explicitly\nmodeling the information of facial poses and obstacles, and\nthen merging it into the face restoration model.\n5\nCONCLUSION\nIn this work, we propose a RestoreFormer++, which on the\none hand introduces multi-head cross-attention mechanisms\nto model the fully-spatial interaction between the degraded\nface and its corresponding high-quality priors, and on the\nother hand, explores an extending degrading model to\nsynthetic more realistic degraded face images for training.\nThe goal of RestoreFormer++ is to restore face images with\nhigher realness and fidelity for both synthetic and real-world\nscenarios. Specifically, the multi-head cross-attention mecha-\nnisms take the features of degraded faces as queries and the\ncorresponding high-quality priors as the key-value pairs. It\nmodels the contextual information based on both semantic\nand structural information in features at different resolutions.\nBesides, the priors provided by our reconstruction-oriented\nhigh-quality dictionary are learned from plenty of high-\nquality face images with a face generation network coop-\nerating with vector quantization. They are more accordant\nto the face restoration task, enabling RestoreFormer++ to\nattain faces with better realness. What is more, the extending\ndegrading model contains more realistic degradations and\nconsiders face misalignment situations, allowing our Restore-\nFormer++ to further alleviate the synthetic-to-real-world gap\nand improve its robustness and generalization towards real-\nworld scenarios. Finally, we conduct extensive experiments\non both synthetic and real-world datasets to demonstrate\nthe superiority of the proposed method. We also discuss the\nlimitations of RestoreFormer++ and our future works for\nfurther improvement.\nACKNOWLEDGMENTS\nThis paper is partially supported by the National Key\nR&D Program of China No.2022ZD0161000 and the General\nResearch Fund of Hong Kong No.17200622.\nREFERENCES\n[1]\nX. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang, \u201cBlind face\nrestoration via deep multi-scale component dictionaries,\u201d in ECCV,\n2020.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\n[2]\nZ. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen,\n\u201cBringing old photos back to life,\u201d in CVPR, 2020.\n[3]\nS. Menon, A. Damian, S. Hu, N. Ravi, and C. Rudin, \u201cPulse:\nSelf-supervised photo upsampling via latent space exploration\nof generative models,\u201d in CVPR, 2020.\n[4]\nC. Chen, X. Li, L. Yang, X. Lin, L. Zhang, and K.-Y. K. Wong,\n\u201cProgressive semantic-aware style transformation for blind face\nrestoration,\u201d in CVPR, 2021.\n[5]\nT. Yang, P. Ren, X. Xie, and L. Zhang, \u201cGan prior embedded network\nfor blind face restoration in the wild,\u201d in CVPR, 2021.\n[6]\nY. Gu, X. Wang, L. Xie, C. Dong, G. Li, Y. Shan, and M.-M. Cheng,\n\u201cVqfr: Blind face restoration with vector-quantized dictionary and\nparallel decoder,\u201d arXiv preprint arXiv:2205.06803, 2022.\n[7]\nZ. Wang, J. Zhang, R. Chen, W. Wang, and P. Luo, \u201cRestoreformer:\nHigh-quality blind face restoration from undegraded key-value\npairs,\u201d in CVPR, 2022.\n[8]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classifica-\ntion with deep convolutional neural networks,\u201d NIPS, 2012.\n[9]\nQ. Cao, L. Lin, Y. Shi, X. Liang, and G. Li, \u201cAttention-aware face\nhallucination via deep reinforcement learning,\u201d in CVPR, 2017.\n[10] H. Huang, R. He, Z. Sun, and T. Tan, \u201cWavelet-srnet: A wavelet-\nbased cnn for multi-scale face super resolution,\u201d in ICCV, 2017.\n[11] Y. Shi, G. Li, Q. Cao, K. Wang, and L. Lin, \u201cFace hallucination\nby attentive sequence optimization with reinforcement learning,\u201d\nIEEE transactions on pattern analysis and machine intelligence, vol. 42,\nno. 11, pp. 2809\u20132824, 2019.\n[12] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister, and M.-H. Yang,\n\u201cLearning to super-resolve blurry face and text images,\u201d in ICCV,\n2017.\n[13] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang, \u201cFsrnet: End-to-end\nlearning face super-resolution with facial priors,\u201d in CVPR, 2018.\n[14] D. Kim, M. Kim, G. Kwon, and D.-S. Kim, \u201cProgressive face\nsuper-resolution via attention to facial landmark,\u201d arXiv preprint\narXiv:1908.08239, 2019.\n[15] Z. Shen, W.-S. Lai, T. Xu, J. Kautz, and M.-H. Yang, \u201cDeep semantic\nface deblurring,\u201d in CVPR, 2018.\n[16] X. Yu, B. Fernando, B. Ghanem, F. Porikli, and R. Hartley, \u201cFace\nsuper-resolution guided by facial component heatmaps,\u201d in ECCV,\n2018.\n[17] X. Yu, B. Fernando, R. Hartley, and F. Porikli, \u201cSuper-resolving\nvery low-resolution face images with supplementary attributes,\u201d in\nCVPR, 2018.\n[18] S. Zhu, S. Liu, C. C. Loy, and X. Tang, \u201cDeep cascaded bi-network\nfor face hallucination,\u201d in ECCV, 2016.\n[19] X. Wang, Y. Li, H. Zhang, and Y. Shan, \u201cTowards real-world blind\nface restoration with generative facial prior,\u201d in CVPR, 2021.\n[20] B. Dogan, S. Gu, and R. Timofte, \u201cExemplar guided face image\nsuper-resolution without facial landmarks,\u201d in CVPRW, 2019.\n[21] X. Li, W. Li, D. Ren, H. Zhang, M. Wang, and W. Zuo, \u201cEnhanced\nblind face restoration with multi-exemplar images and adaptive\nspatial feature fusion,\u201d in CVPR, 2020.\n[22] X. Li, M. Liu, Y. Ye, W. Zuo, L. Lin, and R. Yang, \u201cLearning warped\nguidance for blind face restoration,\u201d in ECCV, 2018.\n[23] X. Wang, K. Yu, C. Dong, and C. C. Loy, \u201cRecovering realistic\ntexture in image super-resolution by deep spatial feature transform,\u201d\nin CVPR, 2018.\n[24] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in\nECCV, 2020.\n[25] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, \u201cPre-trained image processing transformer,\u201d in\nCVPR, 2021.\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., \u201cAn image is worth 16x16 words: Transformers for image\nrecognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[27] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr:\nDeformable transformers for end-to-end object detection,\u201d arXiv\npreprint arXiv:2010.04159, 2020.\n[28] A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu, \u201cNeural discrete\nrepresentation learning,\u201d arXiv preprint arXiv:1711.00937, 2017.\n[29] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architec-\nture for generative adversarial networks,\u201d in CVPR, 2019.\n[30] T. Chen, L. Lin, R. Chen, X. Hui, and H. Wu, \u201cKnowledge-guided\nmulti-label few-shot learning for general image recognition,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 44,\nno. 3, pp. 1371\u20131384, 2022.\n[31] T. Chen, T. Pu, Y. Xie, H. Wu, L. Liu, and L. Lin, \u201cCross-domain\nfacial expression recognition: A unified evaluation benchmark and\nadversarial graph learning,\u201d IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2021.\n[32] X. Li, G. Duan, Z. Wang, J. Ren, Y. Zhang, J. Zhang, and K. Song,\n\u201cRecovering extremely degraded faces by joint super-resolution and\nfacial composite,\u201d in ICTAI, 2019.\n[33] X. Hu, W. Ren, J. Yang, X. Cao, D. P. Wipf, B. Menze, X. Tong, and\nH. Zha, \u201cFace restoration via plug-and-play 3d facial priors,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2021.\n[34] J. Gu, Y. Shen, and B. Zhou, \u201cImage processing using multi-code\ngan prior,\u201d in CVPR, 2020.\n[35] S. Zhou, K. C. Chan, C. Li, and C. C. Loy, \u201cTowards robust blind\nface restoration with codebook lookup transformer,\u201d arXiv preprint\narXiv:2206.11253, 2022.\n[36] Y. Zhao, Y.-C. Su, C.-T. Chu, Y. Li, M. Renn, Y. Zhu, C. Chen, and\nX. Jia, \u201cRethinking deep face restoration,\u201d in CVPR, 2022.\n[37] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n\u201cDeformable convolutional networks,\u201d in ICCV, 2017.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nin NIPS, 2017.\n[39] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage\nmodels are few-shot learners,\u201d arXiv preprint arXiv:2005.14165, 2020.\n[40] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[41] H. Wang, Y. Zhu, H. Adam, A. Yuille, and L.-C. Chen, \u201cMax-\ndeeplab: End-to-end panoptic segmentation with mask transform-\ners,\u201d in CVPR, 2021.\n[42] P. Esser, R. Rombach, and B. Ommer, \u201cTaming transformers for\nhigh-resolution image synthesis,\u201d in CVPR, 2021.\n[43] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku,\nand D. Tran, \u201cImage transformer,\u201d in ICML, 2018.\n[44] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, \u201cLearning texture\ntransformer network for image super-resolution,\u201d in CVPR, 2020.\n[45] L. Zhao, Z. Zhang, T. Chen, D. N. Metaxas, and H. Zhang,\n\u201cImproved transformer for high-resolution gans,\u201d arXiv preprint\narXiv:2106.07631, 2021.\n[46] M. Zhu, C. Liang, N. Wang, X. Wang, Z. Li, and X. Gao, \u201cA sketch-\ntransformer network for face photo-sketch synthesis,\u201d IJCAI, 2021.\n[47] J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual losses for real-time\nstyle transfer and super-resolution,\u201d in ECCV, 2016.\n[48] C. Ledig, L. Theis, F. Husz\u00b4ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang et al., \u201cPhoto-\nrealistic single image super-resolution using a generative adversar-\nial network,\u201d in CVPR, 2017.\n[49] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\nfor large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556,\n2014.\n[50] K. He, G. Gkioxari, P. Doll\u00b4ar, and R. Girshick, \u201cMask r-cnn,\u201d in\nICCV, 2017.\n[51] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular\nmargin loss for deep face recognition,\u201d in CVPR, 2019.\n[52] Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes\nin the wild,\u201d in ICCV, 2015.\n[53] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, \u201cLabeled\nfaces in the wild: A database forstudying face recognition in un-\nconstrained environments,\u201d in Workshop on faces in\u2019Real-Life\u2019Images:\ndetection, alignment, and recognition, 2008.\n[54] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza-\ntion,\u201d arXiv, 2014.\n[55] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochre-\niter, \u201cGans trained by a two time-scale update rule converge to a\nlocal nash equilibrium,\u201d NIPS, 2017.\n[56] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe\nunreasonable effectiveness of deep features as a perceptual metric,\u201d\nin CVPR, 2018.\n[57] D. E. King, \u201cDlib-ml: A machine learning toolkit,\u201d The Journal of\nMachine Learning Research, 2009.\n[58] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, and L. Zelnik-Manor,\n\u201cThe 2018 pirm challenge on perceptual image super-resolution,\u201d in\nECCVW, 2018.\n[59] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet:\nA large-scale hierarchical image database,\u201d in CVPR, 2009.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\n[60] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, \u201cGenerative pretraining from pixels,\u201d in ICML, 2020.\nZhouxia Wang received the B.Eng. and M.Eng.\ndegrees from the School of Data and Computer\nScience at Sun Yat-sen University, Guangzhou,\nChina, in 2015 and 2018. She is currently a Ph.D.\nstudent at the University of Hong Kong (HKU),\nHong Kong SAR, China. Her research interests\ninclude deep learning and low-level vision.\nJiawei Zhang is a research scientist at Sense-\nTime Research. He received a PhD degree from\nCity University of Hong Kong in 2018, a mas-\nter degree from Institute of Acoustics, Chinese\nAcademy of Sciences in 2014 and a bachelor\ndegree from University of Science and Technol-\nogy of China in 2011 receptively. His research\ninterests include image deblurring, image super-\nresolution and related computer vision problems.\nTianshui Chen received the Ph.D. degree in\ncomputer science from the School of Data and\nComputer Science at Sun Yat-sen University,\nGuangzhou, China, in 2018. From 2016 to 2017,\nhe was a research assistant at Hong Kong Poly-\ntechnic University. He is currently an associated\nprofessor at Guangdong University of Technology.\nHis current research interests include computer\nvision and machine learning. He has authored\nand coauthored more than 30 papers published\nin top-tier academic journals and conferences,\nincluding T-PAMI, T-NNLS, T-IP, T-MM, CVPR, ICCV, AAAI, IJCAI, ACM\nMM, etc. He has served as a reviewer for numerous academic journals\nand conferences. He was the recipient of the Best Paper Diamond Award\nat IEEE ICME 2017.\nWenping Wang is now Professor of the Depart-\nment of Computer Science & Engineering at\nTexas A&M University. He has been Chair Profes-\nsor and Head (2012-2017) of the Department of\nComputer Science at the University of Hong Kong.\nHis research interests are computer graphics and\ncomputer vision. He has over 300 technical publi-\ncations in related fields. He serves or has served\nas journal associate editor of CAGD, CAG, TVCG,\nCGF, IEEE Computer Graphics and Applications,\nand IEEE Transactions on Computers. He has\nchaired a number of international conferences, including SPM 2006,\nSMI 2009, Pacific Graphics 2012, GD/SPM\u201913, SIGGRAPH Asia 2013,\nGeometry Summit 2019 and Geometry Summit 2023. He has been\nFounding Chairman of Asian Graphics Association (2016-2020).\nPing Luo is an Assistant Professor in the de-\npartment of computer science, The University of\nHong Kong (HKU). He received his PhD degree in\n2014 from Information Engineering, the Chinese\nUniversity of Hong Kong (CUHK), supervised by\nProf. Xiaoou Tang and Prof. Xiaogang Wang. He\nwas a Postdoctoral Fellow in CUHK from 2014\nto 2016. He joined SenseTime Research as a\nPrincipal Research Scientist from 2017 to 2018.\nHis research interests are machine learning and\ncomputer vision. He has published 100+ peer-\nreviewed articles in top-tier conferences and journals such as TPAMI,\nIJCV, ICML, ICLR, CVPR, and NIPS. His work has high impact with\n18000+ citations according to Google Scholar. He has won a number of\ncompetitions and awards such as the first runner up in 2014 ImageNet\nILSVRC Challenge, the first place in 2017 DAVIS Challenge on Video Ob-\nject Segmentation, Gold medal in 2017 Youtube 8M Video Classification\nChallenge, the first place in 2018 Drivable Area Segmentation Challenge\nfor Autonomous Driving, 2011 HK PhD Fellow Award, and 2013 Microsoft\nResearch Fellow Award (ten PhDs in Asia).\n"
  },
  {
    "title": "Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation",
    "link": "https://arxiv.org/pdf/2308.07316.pdf",
    "upvote": "6",
    "text": "Jurassic World Remake: Bringing Ancient Fossils Back to Life via\nZero-Shot Long Image-to-Image Translation\nAlexander Martin\namart50@u.rochester.edu\nUniversity of Rochester\nRochester, NY, USA\nHaitian Zheng\nhzheng15@ur.rochester.edu\nUniversity of Rochester\nRochester, NY, USA\nJie An\njan6@cs.rochester.edu\nUniversity of Rochester\nRochester, NY, USA\nJiebo Luo\njluo@cs.rochester.edu\nUniversity of Rochester\nRochester, NY, USA\nFigure 1: We present Revive-2I, a zero-shot image-to-image translation method that transforms given fossils images (columns 1\nand 3) respectively into images of living animals of specified species based on input prompts, such as Dinosaurs (columns 2 and\n4).\nABSTRACT\nWith a strong understanding of the target domain from natural\nlanguage, we produce promising results in translating across large\ndomain gaps and bringing skeletons back to life. In this work, we use\ntext-guided latent diffusion models for zero-shot image-to-image\ntranslation (I2I) across large domain gaps (longI2I), where large\namounts of new visual features and new geometry need to be gener-\nated to enter the target domain. Being able to perform translations\nacross large domain gaps has a wide variety of real-world applica-\ntions in criminology, astrology, environmental conservation, and\npaleontology. In this work, we introduce a new task Skull2Animal\nfor translating between skulls and living animals. On this task, we\nfind that unguided Generative Adversarial Networks (GANs) are\nThis work is licensed under a Creative Commons Attribution-\nNoDerivs International 4.0 License.\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0108-5/23/10.\nhttps://doi.org/10.1145/3581783.3612708\nnot capable of translating across large domain gaps. Instead of these\ntraditional I2I methods, we explore the use of guided diffusion and\nimage editing models and provide a new benchmark model, Revive-\n2I, capable of performing zero-shot I2I via text-prompting latent\ndiffusion models. We find that guidance is necessary for longI2I\nbecause, to bridge the large domain gap, prior knowledge about\nthe target domain is needed. In addition, we find that prompting\nprovides the best and most scalable information about the target\ndomain as classifier-guided diffusion models require retraining for\nspecific use cases and lack stronger constraints on the target domain\nbecause of the wide variety of images they are trained on.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Computer vision; Natural lan-\nguage processing.\nKEYWORDS\nimage-to-image translation, large domain gap, stable diffusion\narXiv:2308.07316v1  [cs.CV]  14 Aug 2023\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nAlexander Martin, Haitian Zheng, Jie An, & Jiebo Luo\nACM Reference Format:\nAlexander Martin, Haitian Zheng, Jie An, and Jiebo Luo. 2023. Jurassic World\nRemake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-\nImage Translation. In Proceedings of the 31st ACM International Conference\non Multimedia (MM \u201923), October 29-November 3, 2023, Ottawa, ON, Canada.\nACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3581783.3612708\n1\nINTRODUCTION\nImage-to-image translation (I2I) refers to the task of translating an\ninput image from a given source domain into a given target domain.\nTranslating between domains is an important problem with a wide\nrange of applications in computer vision or computer graphics like\nimage inpainting [33, 44], medical imaging and disease diagnosis\n[3, 21, 43], image editing [12, 26\u201328], and style transfer [8, 15, 24].\nIn literature, I2I has been explored under a variety of settings,\nincluding paired [16, 39, 47], unpaired [4, 9, 34, 38, 45, 48], and\nfew-shot [23, 36] image translations, with generative models such\nas Generative Adversarial Networks (GANs) [10], diffusion mod-\nels [37], and text-guided diffusion models [31]. Despite the promis-\ning results in generating high-quality samples, the existing works\nmainly focus on translating between domains of images with small\ndomain gaps, i.e., translating from photos to paintings or translat-\ning different types of animals (zebras to horses, cats to dogs). While\nthose tasks do not require generating very different new visual\nfeatures or inferences about shape during the translation process,\nthis oversimplified setting may not reflect many practical use cases,\nsuch as translating cats to humans [45], or pumpkins to volcanoes\n[1], that require translating images across domain gaps with large\ngeometry or semantic shift, i.e., long I2I.\nSome longI2I tasks, like translating between animals and humans,\nlack verifiable constraints on the translation process. Because these\ntranslation processes are fictitious, with no potential ground truth,\nmodels can randomly learn any potential mapping as long as the\nresult is similar to the target domain. To offer better constraints\nto the generation process, image editing tasks [12, 26, 27] have\nbeen proposed to provide local edits to an image, like changing the\nsubject of the image without changing the background. These tasks\nstill require the generation of new geometry or a semantic shift,\nbut provide a stricter constraint for acceptable translations.\nBeing able to perform constrained longI2I, longI2I with verifiable\nresults that limit what is considered a valid translation, is an impor-\ntant feature for any I2I model. A model that is capable of longI2I\ncould be used by law enforcement, taking a sketch of a perpetra-\ntor and providing a realistic photo of the person to help identify\nthem [19, 48]. It could be used by wildlife conservationists to show\nthe effects of climate change on cities, ecosystems, and habitats\n[22, 40, 46] or to show the hazards and impacts of wildfires [18]\non towns and in nature. Or it could be used by paleontologists to\ntranslate the ancient fossils of dinosaurs and other extinct animals\ninto their living counterparts.\nTo ground these claims, we introduce and explore the task of\ntranslating skulls into living animals (Skull2Animal). This task\nrequires generating a large amount of new visual features, gener-\nating new textures and colors. It also requires the models to make\ninferences about the geometry in the target domain, having to\nunderstand how to fill in parts of the animal with fat and muscle\nwhile some parts stay tighter to the skull. Unlike previous tasks\nfrom [1, 45], the translation process is not fictitious, providing\nverifiable results which lead to a more constrained generation\nprocess and building off the tasks of [26, 27] the translation process\nrequires extensive inference about geometry.\nUnguided GAN-based methods like CycleGAN [48] and ACL-\nGAN [45] are limited in longI2I tasks because they lack an under-\nstanding of the target domain. To add guidance, Dual Diffusion\nImplicit Bridges (DDIB) [38] uses two classifier-guided diffusion\nmodels [7] to translate images between ImageNet [6] classes. The\nclassifiers provide the diffusion process with knowledge about the\ntarget domain, but the classifiers are limited to the ImageNet classes\nthey are trained on, requiring retraining for new classes and uses.\nInstead of using classifier guidance, we propose replacing the\nclassifier with prompts. This has been previously done by null-text\ninversion [26], where they edit an image by modifying the prompt\nused to create the image. By generating the new image with a\nnatural language understanding of what should change between\neach image, they successfully provide local and global edits to an\nimage with text.\nTo demonstrate the power of natural language for longI2I, we\npropose Revive-2I for zero-shot I2I by prompting pre-trained latent\ndiffusion models. This method is based on the translation process of\nDDIB but makes three changes to the methodology by 1) performing\nthe diffusion process in the latent space, 2) replacing the need\nfor a trained classifier for guidance with a text prompt, and 3)\nperforming partial steps of the full forward diffusion process. These\nchanges make the diffusion process faster, able to incorporate a\nbroader range of classes, and robust to different I2I tasks. With a\nstrong understanding of the target domain from natural language,\nwe produce promising results in translating across large domain\ngaps and bringing skeletons back to life. The dataset and code are\navailable at https://tinyurl.com/skull2animal.\nTo summarize, our contributions are three-fold:\n\u2022 We propose a novel longI2I task Skull2Animal, translating\nskulls into living animals.\n\u2022 We baseline existing image-to-image translation methods\n(CycleGAN, ACL-GAN, DDIB, Null-text Inversion) for this\ntask.\n\u2022 We propose Revive-2I to perform zero-shot I2I via prompting\nlatent diffusion models.\n2\nRELATED WORK\nGANs for UI2I. Unpaired image-to-image translation (UI2I) has\nseen many GAN models introduced centered around a cycle consis-\ntency loss [20, 25, 41, 48]. Cycle consistency constrains the genera-\ntion process so that a translated image is able to be translated back\ninto the original image. These models have been found to do well\nin I2I tasks, like style transfer, that require that the core content\nand structure be retained during translation. However, when tasked\nwith removing objects or performing changes to the structure of an\nimage, models like CycleGAN [48] or UNIT [25] have been shown\nto retain artifacts from the original image because of the strict pixel-\nlevel constraint [14, 45]. To circumvent the strict constraint of cycle\nconsistency and its drawbacks, methods have been introduced to\nperform UI2I on tasks that require more complex mappings, beyond\nstyle and texture. ACL-GAN [45] introduces an adversarial cycle\nJurassic World Remake\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nconsistency constraint. This loss removes the pixel-level constraint\nof CycleGAN, removing the need to be translated back into the\nexact image, but still retaining important source features.\nDiffusion Methods for UI2I. Recently, pixel-based diffusion mod-\nels have been shown to be used for cycle-consistent image-to-\nimage translation. Translation with Dual Diffusion Implicit Bridges\n(DDIBs) [38] requires two different probability flow ordinary dif-\nferential equations (PF, ODE). The source ODE converts the input\nimages into the latent space and the target ODE synthesizes the\nimage in the target domain. They denote this conversion ODESolve\nas the mapping from \ud835\udc65(\ud835\udc610) to \ud835\udc65(\ud835\udc611):\n\ud835\udc42\ud835\udc37\ud835\udc38\ud835\udc46\ud835\udc5c\ud835\udc59\ud835\udc63\ud835\udc52(\ud835\udc65(\ud835\udc610);\ud835\udc63\ud835\udf03,\ud835\udc610,\ud835\udc611) = \ud835\udc65(\ud835\udc610) +\n\u222b \ud835\udc611\n\ud835\udc610\n\ud835\udc63\ud835\udf03 (\ud835\udc61,\ud835\udc65(\ud835\udc61)),\ud835\udc51\ud835\udc61\n(1)\nTo perform image-to-image translation, DDIB uses two separate dif-\nfusion models. Starting with an image \ud835\udc65\ud835\udc60 in the source domain, the\nlatent representation,\ud835\udc65\ud835\udc59, is produced using\ud835\udc65\ud835\udc59 = \ud835\udc42\ud835\udc37\ud835\udc38\ud835\udc46\ud835\udc5c\ud835\udc59\ud835\udc63\ud835\udc52(\ud835\udc65\ud835\udc60;\ud835\udc63\ud835\udc60\n\ud835\udf03, 0, 1).\nThen the latent representation can be used again in a different\nODESolve to obtain the target image \ud835\udc65\ud835\udc61 = \ud835\udc42\ud835\udc37\ud835\udc38(\ud835\udc65\ud835\udc59, \ud835\udc63\ud835\udc61\n\ud835\udf03, 1, 0). In\ntheir work, they prove that DDIBs offer exact cycle consistency, an\nimportant feature in unpaired image-to-image translation.\nText-Based Diffusion Methods for UI2I. While diffusion models\nare able to perform UI2I, adding new classes to the methods re-\nquires retraining the diffusion models or their classifier guidance.\nThis is computationally expensive, reducing the scalability of dif-\nfusion methods. Instead of training new models, encoding natural\nlanguage with models like CLIP [29] or ALIGN [17] can be used\nto guide the diffusion process in tasks like text-to-image synthe-\nsis [30\u201332, 42]. These models have shown the ability to generate\nhigh-quality visual features by conditioning on text prompts.\nIn this work, we build off of stable diffusion [31], a text-guided la-\ntent diffusion model. Stable diffusion performs the diffusion process\nin a latent space, which is perceptually equivalent to pixel-based dif-\nfusion, but more computationally suitable for the diffusion process.\nIn the latent space, they additionally model the conditional distri-\nbution between a latent and a text prompt to perform text-to-image\nsynthesis.\nImage Editing Methods for UI2I. Image editing with text-guided\ndiffusion models was first proposed by Prompt-to-Prompt [12].\nPrompt-to-Prompt performs an intuitive image editing (translation)\nby editing prompts that correspond to generated images to create\nnew images with the change in the prompt reflected in the new\nimage. Building off of Prompt-to-Prompt, null-text inversion [26]\nuses DDIM inversion to be able to edit real images with the Prompt-\nto-Prompt technique. Taking a real image and an associated caption\nas input, the image is inverted with a DDIM model to give a diffusion\ntrajectory. The trajectory can then be used for null-text optimization\nthat reconstructs the input image. With this optimization done, the\nimage can then be edited using the technique from Prompt-to-\nPrompt.\n3\nDATASET\nAs depicted in Figure 2, our collected Skull2Animal dataset com-\nprises unpaired images of skulls and living animals. These images\nwere curated through a process wherein skull photographs, taken\nFigure 2: Example images from the collected Skull2Dog\ndataset.\nby a private photographer1, were selected for inclusion. Each skull\nimage in the dataset was captured through a comprehensive 360-\ndegree rotation around the skull, from which a subset was manually\nselected by one of the authors. The selection criteria stipulated that\nthe skull must be oriented in a manner compatible with the per-\nspectives captured in the Animal Faces-HQ (AFHQ) dataset [5].\nThis led to the aggregation of skull images aligned 90\u00b0left and\n90\u00b0right from a forward-facing perspective. With the skull images\ncollected, corresponding living animals needed to be collected. The\nSkull2Animal dataset consists of 4 different types of mammals: dogs\n(Skull2Dog), cats (Skull2Cat), leopards (Skull2Leopard), and foxes\n(Skull2Fox). The living animals of the dataset are sampled from the\nAFHQ. The dataset is partitioned by ImageNet class using a ResNet\n[11] classifier trained on ImageNet1k [6].\nIn this paper, we will focus on the Skull2Dog subset as it provides\nthe most skull images. In the Skull2Dog dataset, there are 6 different\nskulls: Boston Terrier, Boxer, Chihuahua, Great Dane, Pekingese,\nand Rottweiler. After an initial pass through AFHQ, there are 422\ndog images that fall into those classes. To add additional images to\nthe split, 799 dog images were randomly selected from the remain-\nder of the AFHQ dataset, giving 1201 skull and dog images. This\ncollection of skull and dog images was then randomly split into a\ntraining set of 1080 images and a test set of 121 test images. A more\nin-depth breakdown of the data partitions and breed counts can be\nfound in Appendix A.\n4\nMETHODOLOGY\nRecently, DDIB [38] proposes leveraging a pre-trained diffusion\nmodel to define and solve ordinary differential equations (ODE)\nfor image-to-image translation. DDIB shows promising results in\ntransferring between predefined classes. However, as the diffusion\nprocess is conditioned on limited classes from ImageNet, DDIB\ncannot transfer between fine-grain and arbitrary domains, such\nas images of an endangered specie and its fossil. Moreover, DDIB\nsometimes introduces misalignment due to the lack of cross-domain\nconstraints. Finally, diffusion on pixel space is computationally ex-\nhaustive. To address those limitations, we propose Revive-2I, a\nflexible and efficient framework for long I2I. Specifically, 1) we per-\nform the diffusion process in the latent space with stable diffusion\n[31], 2) we replace the need for a trained classifier for guidance\nwith a text prompt, and 3) we do not perform the full forward pro-\ncess. These changes make the Revive-2I translation process much\n1Skull Photos courtesy of Nick Mann, used with permission\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nAlexander Martin, Haitian Zheng, Jie An, & Jiebo Luo\nquicker, able to incorporate a broader range of classes (anything\nthat can be describe in natural language), and robust to different\nI2I tasks. The Revive-2I process can be broken down into two steps:\nencoding and text-guided decoding.\nFigure 3: Our Revive-2I for zero-shot long image-to-image\ntranslation. Our method takes a source domain (top left)\nimage and a text prompt as input and outputs a target domain\nimage (bottom left).\nEncoding: Starting with a source image \ud835\udc65\ud835\udc60 \u2208 R\ud835\udc3b \u00d7\ud835\udc4a \u00d73, we en-\ncode the \ud835\udc65\ud835\udc60 into the latent representation \ud835\udc67\ud835\udc60 = E(\ud835\udc65\ud835\udc60), where\n\ud835\udc67\ud835\udc60 \u2208 R\u210e\u00d7\ud835\udc64\u00d73 (\u210e = \ud835\udc3b\n\ud835\udc53 ,\ud835\udc64 = \ud835\udc4a\n\ud835\udc53 ) downsampled by factor \ud835\udc53 = 8.\nBy doing this we operate in a perceptually equivalent, but computa-\ntionally more suitable space for the diffusion process [31]. With the\nlatent representation of the source image, we then apply the for-\nward process to obtain the noised encoding \ud835\udc67\ud835\udc60\n\ud835\udc47 . This forward step\nis equivalent to \ud835\udc42\ud835\udc37\ud835\udc38\ud835\udc46\ud835\udc5c\ud835\udc59\ud835\udc63\ud835\udc52(\ud835\udc65\ud835\udc60;\ud835\udc63\ud835\udc60\n\ud835\udf03, 0, 1) of DDIB, but in the latent\nspace.\nVaried Encoding. One might argue that any I2I task is possible\nwith text-guided diffusion models as long as the right prompt is\nused. Instead of trying to find the best prompt for the task, we\ntry taking different numbers of steps in the forward process. To\nconvert the latent source representation, \ud835\udc67\ud835\udc60, into the fully noised\nGaussian distribution, we take\ud835\udc47 = 100 steps of the forward process.\nTo create partiala encodings, we take 50\n100, 60\n100, 70\n100, 80\n100, 90\n100, and\n95\n100 forward steps. Taking partial steps in the forward diffusion\nprocess allows the translation process to better preserve the source\ncontent. This makes the generation more faithful to the source\ncontent but allows for the text prompt to inject the features of the\ntarget domain. Please refer to Appendix C for an applied proof of\nthe varied encoding.\nPrompting: To incorporate the target domain into the diffusion\nprocess, we replace the classifier guidance from DDIB with a text\nprompt. If classifier guidance were used for the Skull2Animal trans-\nlation process, the classifier would need to be trained for new target\ndomains constrained to dog heads. In contrast, using text guidance,\nthere is no training needed to perform the Skull2Animal task mak-\ning the Revive-2I method able to be used across a large variety\nof skull images, even ones not included in ImageNet1K. The text\nprompt also allows for the method to constrain the generation pro-\ncess better. A classifier trained on ImageNet will have seen a wide\nvariety of images under a single label. This might lead to unwanted\nartifacts or additional content generated in the images, like people\nor the full dog body. However, with a text prompt, the diffusion\nprocess can be more constrained to the ideal target domain.\nThe prompt used for Skull2Dog translation is \u201ca photo of the head\nof\u201d combined with the ImageNet label. For example, translating\nthe Boston terrier skull would use the prompt \u201ca photo of the head\nof a Boston terrier dog.\u201d This prompt provides two guides to the\ntranslation process. First, we provide the same class-level guidance\nin DDIB by using the ImageNet class name. Second, we better\nconstrain the generation process to only produce dog heads.\nGuided Decoding: We leverage the pretrained Stable Diffusion\n[31] for text-guided decoding. Specifically, to combine the text\nprompt \ud835\udc66 with the latent representation \ud835\udc67\ud835\udc60\n\ud835\udc47 , we first project the text\nprompt using the same text encoder as stable diffusion \ud835\udf0f\ud835\udf03. This\nprojects \ud835\udc66 to an intermediate representation \ud835\udf0f\ud835\udf03 (\ud835\udc66) \u2208 R\ud835\udc40\u00d7\ud835\udc51\ud835\udf0f . Then\nthis intermediate representation is mapped to intermediate layers\nof the UNet used for denoising via a cross-attention layer. So by\ntaking a prompt \ud835\udc66 and latent representation \ud835\udc67\ud835\udc60\n\ud835\udc47 , the first step of the\ndenoising process through UNet guides \ud835\udc67\ud835\udc60\n\ud835\udc47 to the target domain as\n\ud835\udc67\ud835\udc61\n\ud835\udc47 \u22121. With the latent representation in the target domain, it can be\nfurther denoised by the reverse diffusion process giving the final\nlatent representation \ud835\udc67\ud835\udc61. The combination of the UNet denoising\nand reverse process is equivelent to \ud835\udc42\ud835\udc37\ud835\udc38(\ud835\udc65\ud835\udc59, \ud835\udc63\ud835\udc61\n\ud835\udf03, 1, 0) of DDIB in the\nlatent space. From the latent \ud835\udc67\ud835\udc61, we can decode the representation\ninto the pixel space \u02dc\ud835\udc65\ud835\udc61 = D(\ud835\udc67\ud835\udc61) resulting in an image \u02dc\ud835\udc65\ud835\udc61 in the\ntarget domain.\nExperimental Setup: To evaluate the Skull2Dog translation pro-\ncess, we split the data input 1080 samples for training and 121\nsamples for testing. We train CycleGAN [48], and ACL-GAN [45]\non these splits to test the GAN methods. For the guided diffusion\nmethod, we use the 256x256 classifier and diffusion models from [7]\nand follow the same method from DDIB [38] taking 1000 steps in\nthe diffusion process to translate the images. For image editing we\nuse the initial prompt \u201ca photo of the skull of\" and the same target\nprompt used for Revive-2I. To evaluate the generation results, we\nuse Fr\u00e9chet inception distance (FID) [13, 35], Kernel inception dis-\ntance (KID) [2] and two different top-1 classification, top-1 all and\ntop-1 class, accuracies using ResNet-50 [11] trained for ImageNet1K\n[6]. Top-1 all (All @1) is scored with the top-1 classification being\nany of the 100 dog breeds used in ImageNet1K and top-1 class (Class\n@1) is scored with the top-1 classification being the correct dog\nbreed for the specific skull. When these are scored at 100%, top-1\nall has every image classified as a dog and top-1 class has every\nimage classified as the correct breed.\n5\nRESULTS\nThe unguided GAN methods (CycleGAN and ACL-GAN) are unable\nto successfully perform the Skull2Animal task. As seen in Figure 4\nand Table 1, CycleGAN is capable of learning the gist of a dog. The\nmodel can produce features that are characteristic of dogs like the\nnose, eyes, and fur, but it cannot properly structure the features\nor understand the direction the dog is facing. In Table 1, the top-1\nall score for CycleGAN is high because it learns this gist of the\ndog and dog-like features, but its top-1 class score is incredibly\nlow because it cannot structure the features it learns to represent\na specific breed or similar looking breed depending on the skull.\nACL-GAN is unable to perform the translation. The adversarial\ncycle consistency loss helps retain the important features from the\nJurassic World Remake\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFigure 4: Skull2Dog Translations Rows:(top to bottom) Boston terrier, Boxer, Chihuahua, Great Dane, Pekingese, Rottweiler\nTable 1: Quantitative evaluation on Skull2Dog. Lower scores\nare better for metrics with down arrows (\u2193), and vice versa.\nFID \u2193\nKID\u2193\nAll @1\u2191\nClass @1\u2191\nCycleGAN [48]\n212.66\n0.1325\n77.00\n5.70\nACL-GAN [45]\n342.16\n0.3134\n38.84\n1.65\nDDIB [38]\n204.42\n0.0698\n73.68\n56.14\nNull-text [26]\n308.19\n0.1559\n1.79\n1.79\nRevive-2I50%\n236.59\n0.118\n43.80\n20.66\nRevive-2I60%\n177.30\n0.0584\n70.25\n34.70\nRevive-2I70%\n155.066\n0.0437\n93.89\n59.50\nRevive-2I80%\n147.85\n0.0368\n100\n67.77\nRevive-2I90%\n149.42\n0.0365\n100\n89.26\nRevive-2I95%\n143.29\n0.0345\n100\n92.56\nRevive-2I100%\n155.9\n0.0391\n100\n92.56\nsource image in other I2I tasks, but when translating across large\ndomain gaps, the loss retains the entire skull. This leads to the\nmodel learning to place dog like features in the background of the\nimage and retain the skull in the center. This is able to confuse a\nclassification model to classify some images as dogs, but because\nof the lack of structure, it cannot fall under the correct classes.\nThe image editing method, null-text inversion, is unable to create\nenough new visual features to translate from skulls to living animals.\nIn some cases the editing process is able to create dog like fur\ntextures or place a nose on the skull, but it is not able to step too far\nfrom the source image to generate new geometry. While this might\nbe a useful feature in translation tasks between similar images, it\nrestricts the longI2I process leading to low top-1 all and class scores.\nThe guided diffusion methods (DDIB and Revive-2I) are able to\ntranslate between skulls and animals because of the understanding\nthey have of the target domain. DDIB\u2019s classifier guidance gives\nthe model a good understanding of the target domain allowing the\nmodel to successfully translate into the domain of living animals.\nHowever, the classifier has seen a wide variety of images under\nthe same class label. Because of this, the translation process can\nresult in the full body of the dog or other subjects like people in\nthe scenes. Fixing this would require retraining the classifier for\nnew class labels specific to the dog head use case.\nRevive-2I\u2019s text prompt allows for a more constrained trans-\nlation process without the need to retrain the diffusion model or\nclassifier. However, this constraint is not perfect as every diffusion\nstep from 50% to 100% can be shown to produce full-body dogs. We\nfind that the model best capable of translating from skulls to living\nanimals is Revive-2I with 95% (95/100) of the forward steps taken\nin the encoding. At 95% of the forward process, the forward step\nhas not converted the image into a full isotropic Gaussian distri-\nbution. By retaining those last few steps in the forward process,\nthe content of the image retains its influence when combined with\nthe text prompt. This can be seen as for most cases in Figure 4,\nby not completing the full forward process, the dogs face in the\ndirection of the skull. It is only when the full forward process is\ntaken, that the source image loses its influence on the target image.\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nAlexander Martin, Haitian Zheng, Jie An, & Jiebo Luo\nTable 2: Results of different prompting techniques on visual\nquality and classification.\nFID \u2193\nKID\u2193\nAll @1\u2191\nClass @1\u2191\nDDIB [38]\n204.42\n0.0698\n73.68\n56.14\nRevive-2I95%\n143.29\n0.0345\n100\n92.56\nGeneric\n183.16\n0.0965\n88.43\n1.65\nClass\n155.83\n0.0391\n89.69\n80.41\n+head\n160.50\n0.0334\n80.16\n64.46\nThis can be seen in the dogs\u2019 faces changing directions to face for-\nward. While reducing the steps taken in the forward process helps\nmaintain faithfulness to the source image in most cases, the Revive-\n2I method is not always perfect. At each fraction of the forward\nprocess, the diffusion process will sometimes result in the full body\nof the dog and as the amount of steps taken in the forward process\ndecrease, the faithfulness to the target domain also decreases. This\ncan be seen in some images resulting in dog images that look like\nstickers, puppets, or masks. Additional failure cases can be found\nin Appendix D.\n6\nPROMPT ANALYSIS\nThe initial prompt, \u201ca photo of the head of <class_name>,\" was used\nto mimic the class level constraint used in DDIB [38], but slightly\nconstrain the generation process to animal heads only. We explore\nthe following additional prompts with the same set up on the Revive-\n2I95% model: 1) \"A photo of the head of a dog\", a generalized version\nof the class name prompt (Generic), 2) \"<Class_name>\", to provide\nonly the class-level guidance provided by DDIB (Class), and 3)\n\"<Class_name> head\", to remove the context of \"a photo\" (+head).\nAs seen in Figure 5, the prompts constrain the generation process\nin different ways. The generic prompt generates images of the most\npopular dogs with the most classifications being Labrador retriever\n(39 images). While this prompt is a good generalization and shows\nthat Revive-2I can be used in abstract cases, it also demonstrates\nthat the more popular a domain is the more it might dominate a\nresult and leave out underrepresented, and possibly correct, target\nimages. The class name prompt provides more freedom in the gener-\nation process producing better image quality scores than the other\nablation prompts. However, the freedom given in the prompt does\nnot always constrian the generation process to dogs (the underwear\nboxers generated for a boxer) or allows for the generation of addi-\ntional artifacts (the wooden board visible in chihuahua). While the\n+head prompt does not constrain the generation process to living\ndogs, producing images commonly classified as masks or stickers.\n7\nLIMITATIONS\nOne limitation is faithfulness to the source image. Deviations from\nthe source image can be seen when the animal faces a different\ndirection than the skull or when additional artifacts like the animals\nbody are generated. This unfaithfulness comes from the strong un-\nderstanding of the target domain provided by guidance. While this\nhelps the model reach the target domain, it sometimes provides too\nmuch information. It is challenging to provide the correct amount\nof information to not generate additional features or not enough\ninformation to fully translate the image.\nFigure 5: Results of different prompts. Rows:(top to bottom)\nBoston terrier, Boxer, Chihuahua, Great Dane, Pekingese,\nRottweiler\nAnother limitation is in producing unseen classes. For example,\nwhen translating the bones of dinosaurs it would be helpful to\nguide the translation process with \"a photo of a mammal\". This\nwas attempted with \"a photo of a dog head\", however, this only\nproduced the most popular breeds like Labrador retrievers. This\nresult is undesirable in cases where the translation class is unknown\nand needs generalization.\n8\nCONCLUSION\nIn this paper, we propose the task of translating skulls into living\nanimals (Skull2Animal). The task requires the generation of a large\nnumber of new visual features, inference about geometry in the\ntarget image, and provides a verifiable constraint on the translation\nprocess. We show that traditional I2I methods using GANs are not\nable to successfully bridge the domain gap and instead propose the\nuse of guided diffusion models. By providing a classifier or text\nprompt, we can encode more information about the target domain\ninto the diffusion process. However, our method still lacks full\nfaithfulness to the source image. Thus, being able to encode enough\ninformation about the target domain but retaining the intrinsic\nsource context, like head direction, is an interesting direction for\nfuture research.\n9\nACKNOWLEDGEMENTS\nWe greatly appreciate Nick Mann for allowing for the use of his skull\nphotos. Thanks Kurtis for his discussion. This work was supported\nin part by the Georgen Institute for Data Science at the University\nof Rochester.\nJurassic World Remake\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nREFERENCES\n[1] Matthew Amodio and Smita Krishnaswamy. 2019. TraVeLGAN: Image-to-image\nTranslation by Transformation Vector Learning. arXiv:1902.09631 [cs.CV]\n[2] Miko\u0142aj Bi\u0144kowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton.\n2021. Demystifying MMD GANs. arXiv:1801.01401 [stat.ML]\n[3] Zekang Chen, Jia Wei, and Rui Li. 2022.\nUnsupervised Multi-Modal Med-\nical Image Registration via Discriminator-Free Image-to-Image Translation.\narXiv:2204.13656 [cs.CV]\n[4] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and\nJaegul Choo. 2018. StarGAN: Unified Generative Adversarial Networks for Multi-\nDomain Image-to-Image Translation. arXiv:1711.09020 [cs.CV]\n[5] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. StarGAN v2:\nDiverse Image Synthesis for Multiple Domains. arXiv:1912.01865 [cs.CV]\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-\nageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition. 248\u2013255. https://doi.org/10.1109/CVPR.\n2009.5206848\n[7] Prafulla Dhariwal and Alex Nichol. 2021. Diffusion Models Beat GANs on Image\nSynthesis. arXiv:2105.05233 [cs.LG]\n[8] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2015. A Neural Algo-\nrithm of Artistic Style. arXiv:1508.06576 [cs.CV]\n[9] Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie, Kwang In Kim, and James\nTompkin. 2019. Improving Shape Deformation in Unsupervised Image-to-Image\nTranslation. arXiv:1808.04325 [cs.CV]\n[10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\nAdversarial Networks. arXiv:1406.2661 [stat.ML]\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual\nLearning for Image Recognition. arXiv:1512.03385 [cs.CV]\n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-Prompt Image Editing with Cross Attention Control.\narXiv:2208.01626 [cs.CV]\n[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. 2018. GANs Trained by a Two Time-Scale Update Rule Converge\nto a Local Nash Equilibrium. arXiv:1706.08500 [cs.LG]\n[14] Bingwen Hu, Zhedong Zheng, Ping Liu, Wankou Yang, and Mingwu Ren. 2020.\nUnsupervised Eyeglasses Removal in the Wild. arXiv:1909.06989 [cs.CV]\n[15] Eleftherios Ioannou and Steve Maddock. 2022. Depth-aware Neural Style Transfer\nusing Instance Normalization. (2022). https://doi.org/10.2312/CGVC.20221165\n[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2018.\nImage-to-Image\nTranslation\nwith\nConditional\nAdversarial\nNetworks.\narXiv:1611.07004 [cs.CV]\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.\nLe, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021.\nScaling Up Visual\nand Vision-Language Representation Learning With Noisy Text Supervision.\narXiv:2102.05918 [cs.CV]\n[18] Tanqiu Jiang, Sidhant K. Bendre, Hanjia Lyu, and Jiebo Luo. 2021. From Static to\nDynamic Prediction: Wildfire Risk Assessment Based on Multiple Environmental\nFactors. arXiv:2103.10901 [cs.LG]\n[19] Min Woo Kim and Nam Ik Cho. 2023. WHFL: Wavelet-Domain High Frequency\nLoss for Sketch-to-Image Translation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV). 744\u2013754.\n[20] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim.\n2017. Learning to Discover Cross-Domain Relations with Generative Adversarial\nNetworks. arXiv:1703.05192 [cs.CV]\n[21] Lingke Kong, Chenyu Lian, Detian Huang, Zhenjiang Li, Yanle Hu, and Qichao\nZhou. 2021. Breaking the Dilemma of Medical Image-to-image Translation.\narXiv:2110.06465 [eess.IV]\n[22] Yuncheng Li, Jifei Huang, and Jiebo Luo. 2015. Using user generated online\nphotos to estimate and monitor air pollution in major cities. In Proceedings of the\n7th International Conference on Internet Multimedia Computing and Service. ACM.\nhttps://doi.org/10.1145/2808492.2808564\n[23] Jianxin Lin, Yingxue Pang, Yingce Xia, Zhibo Chen, and Jiebo Luo. 2020. Tu-\niGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images.\narXiv:2004.04634 [cs.CV]\n[24] Zheng Lin, Zhao Zhang, Kang-Rui Zhang, Bo Ren, and Ming-Ming Cheng. 2022.\nInteractive Style Transfer: All is Your Palette. arXiv:2203.13470 [cs.CV]\n[25] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. 2018. Unsupervised Image-to-Image\nTranslation Networks. arXiv:1703.00848 [cs.CV]\n[26] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\n2022. Null-text Inversion for Editing Real Images using Guided Diffusion Models.\narXiv:2211.09794 [cs.CV]\n[27] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jing-\nwan Lu, and Jun-Yan Zhu. 2023.\nZero-shot Image-to-Image Translation.\narXiv:2302.03027 [cs.CV]\n[28] Ori Press, Tomer Galanti, Sagie Benaim, and Lior Wolf. 2020. Emerging Dis-\nentanglement in Auto-Encoder Based Unsupervised Image Content Transfer.\narXiv:2001.05017 [cs.CV]\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models\nFrom Natural Language Supervision. arXiv:2103.00020 [cs.CV]\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022.\nHierarchical Text-Conditional Image Generation with CLIP Latents.\narXiv:2204.06125 [cs.CV]\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models.\narXiv:2112.10752 [cs.CV]\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Den-\nton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Moham-\nmad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding. arXiv:2205.11487 [cs.CV]\n[33] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and\nMohammad Norouzi. 2021. Image Super-Resolution via Iterative Refinement.\narXiv:2104.07636 [eess.IV]\n[34] Hiroshi Sasaki, Chris G. Willcocks, and Toby P. Breckon. 2021. UNIT-DDPM:\nUNpaired Image Translation with Denoising Diffusion Probabilistic Models.\narXiv:2104.05358 [cs.CV]\n[35] Maximilian Seitzer. 2020. pytorch-fid: FID Score for PyTorch. https://github.com/\nmseitzer/pytorch-fid. Version 0.3.0.\n[36] Jake Snell, Kevin Swersky, and Richard S. Zemel. 2017. Prototypical Networks\nfor Few-shot Learning. arXiv:1703.05175 [cs.LG]\n[37] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli.\n2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics.\narXiv:1503.03585 [cs.LG]\n[38] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. 2023. Dual Diffusion\nImplicit Bridges for Image-to-Image Translation. arXiv:2203.08382 [cs.CV]\n[39] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. 2018. High-Resolution Image Synthesis and Semantic Manipulation\nwith Conditional GANs. arXiv:1711.11585 [cs.CV]\n[40] Xitong Yang, Zheng Xu, and Jiebo Luo. 2018. Towards Perceptual Image Dehazing\nby Physics-Based Disentanglement and Adversarial Training. Proceedings of the\nAAAI Conference on Artificial Intelligence 32, 1 (Apr. 2018).\n[41] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. 2018. DualGAN: Unsupervised\nDual Learning for Image-to-Image Translation. arXiv:1704.02510 [cs.CV]\n[42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vi-\njay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson,\nWei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.\n2022. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.\narXiv:2206.10789 [cs.CV]\n[43] Rui Zhang, Tomas Pfister, and Jia Li. 2019. Harmonic Unpaired Image-to-image\nTranslation. arXiv:1902.09727 [cs.CV]\n[44] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen Zuo, Haibo Chen,\nWei Xing, and Dongming Lu. 2020. UCTGAN: Diverse Image Inpainting Based\non Unsupervised Cross-Space Translation. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR). 5740\u20135749. https://doi.org/10.1109/\nCVPR42600.2020.00578\n[45] Yihao Zhao, Ruihai Wu, and Hao Dong. 2021. Unpaired Image-to-Image Transla-\ntion using Adversarial Consistency Loss. arXiv:2003.04858 [cs.CV]\n[46] Ruobing Zheng, Ze Luo, and Baoping Yan. 2019. Exploiting Time-Series Image-\nto-Image Translation to Expand the Range of Wildlife Habitat Analysis. In\nProceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and\nThirty-First Innovative Applications of Artificial Intelligence Conference and Ninth\nAAAI Symposium on Educational Advances in Artificial Intelligence (Honolulu,\nHawaii, USA) (AAAI\u201919/IAAI\u201919/EAAI\u201919). AAAI Press, Article 102, 8 pages.\nhttps://doi.org/10.1609/aaai.v33i01.3301825\n[47] Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen,\nZhongfei Zhang, and Fang Wen. 2021. CoCosNet v2: Full-Resolution Correspon-\ndence Learning for Image Translation. arXiv:2012.02047 [cs.CV]\n[48] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2020.\nUn-\npaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.\narXiv:1703.10593 [cs.CV]\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nAlexander Martin, Haitian Zheng, Jie An, & Jiebo Luo\nTable 3: Breakdown of animal distributions in Skull2Animal.\nWild Animal\nLiving Images\nSkull Images\nKit fox\n84\n152\nFennec fox\n0\n201\nRed fox\n281\n201\nArtic fox\n55\n0\nGrey fox\n44\n0\nFox\n464\n554\nClouded leopard\n0\n291\nLeopard\n477\n192\nSnow leopard\n282\n0\nJaguar\n253\n0\nCheetah\n665\n242\nLeopard\n1678\n726\nBoston terrier\n22\n222\nBoxer\n71\n198\nChihuahua\n215\n168\nGreat dane\n27\n204\nPekinese\n26\n199\nRottweiler\n61\n210\nOther (random)\n779\n0\nDogs\n1201\n1201\nWild Animals\n3343\n2481\nA\nDATASET STATISTICS\nThe Skull2Animal dataset can be broken down into 4 species: cats,\ndogs, foxes, and leopards (Table 3). Because this dataset was built\noff of AFHQ [5], it only has mammals. The skull portion of the\ndataset contains many more types of animal species. To request\nadditional skull images or for inquiries about the data, please reach\nout to the first author or visit the project page https://tinyurl.com/\nskull2animal.\nB\nPARAMETER SETTINGS\nTo understand more about the performance of the method, we ex-\nplore different levels of classifier free guidance (cfg). We find similar\nresults to Rombach et al [31], that increasing the cfg increases the\nalignment between image and text, but increasing the guidance\nbeyond a certain threshold yields a decrease in image quality. This\ncan be seen in Table 4, as the original Revive-2I model (cfg scale of\n7.5), yields the best alignment with the class labels for each skull\nwhile also having the lowest FID.\nFID \u2193\nKID\u2193\nAll @1\u2191\nClass @1\u2191\nRevive-2I95%\n143.29\n0.0345\n100\n92.56\nScale 5.0\n146.78\n0.0345\n100\n84.30\nScale 6.0\n143.93\n0.0340\n100\n88.43\nScale 7.0\n148.05\n0.0341\n100\n90.91\nScale 8.0\n148.40\n0.0352\n100\n90.91\nScale 9.0\n152.14\n0.0340\n100\n90.91\nScale 10.0\n151.31\n0.0370\n100\n90.91\nTable 4: Different classifier free guidance scales.\nC\nSMALL DOMAIN GAP TRANSLATIONS\n(SHORTI2I)\nTo demonstrate the ability of the Revive-2I method, we extend\nthe study to the shortI2I, image-to-image translation across small\ndomain gaps, and the task of translating pictures into paintings by\nClaude Monet using the Monet2Photo dataset2 from CycleGAN\n[48]. As previously done in section 4, we use a variety of forward\nprocess lengths. For this ablation, we use \ud835\udc47 = 100 forward steps\nand use \ud835\udc47\n4 (25), \ud835\udc47\n2 (50), 3\ud835\udc47\n4 (75), and \ud835\udc47 (100) forward steps. Each\nmethod is guided by the same prompt \u2018a painting by Claude Monet\u2019\non the same images.\nAs can be seen in Figure 6, Revive-2I is capable of doing shortI2I,\nhowever some steps perform better than others. It would appear\nthat \ud835\udc47\n4 is too few steps, preserving the content of the image, but\nnot transferring the style of the Monet photos. Steps 3\ud835\udc47\n4 and \ud835\udc47\nare too many in the forward process, losing the intrinsic source\nfeatures and instead generating new photos. With \ud835\udc47\n2 forward steps,\nthe image content is well preserved and style is transferred into\nthe Monet painting domain, demonstrating the methods ability to\nprovide global edits to images like null-text inversion [26].\nD\nFAILURE CASES\nIn this section we evaluate the failure cases of Revive-2I\u2019s diffusion\nstep combinations for 50% and 95%. We only evaluate these two\ncases because we find that 50% to 70% are very similar in results\nand 95% was the best version of the model found is section 5.\n50%: In terms of faithfulness to the source image, Revive-2I50%\nretains the most of the source content. In Figure 7, it can be seen\nthat the living dogs face the same way as the skull for each breed.\nHowever, the quality of the generated dogs is poor. These results can\nbe accounted to the source content playing to great of an influence\nin the translation process. This result is good in cases of shortI2I\n(Appendix C), but when a large amount of visual features need to be\ngenerated, more influence from the text prompt is needed. It may be\nimportant to note that the errors of Revive-2I50% are very similar to\nthe results of the image editing technique null-text inversion [26].\n95%: While Revive-2I95% is able to create the visual features\nneeded, it sometimes encounters the same problems as DDIB\u2019s [38]\nclassifier guidance where the full body image of the dog is created\n(Figure 8). This is error is most likely caused in the cross-attention\nbetween the prompt and latent in the UNet layers. We leave this\nexploration for future work.\nE\nLEOPARD AND FOX DATASETS\nLastly, we show the Revive-2I95% model on the leopard and fox\ndatasets in Figure 9 and Figure 10. The prompt used for the fox and\nleopard generations is \u201ca photo of the head of\" and the ImageNet\nclass (the same prompt as section 4). For example, the clouded\nleopard prompt is \u201ca photo of the head of a clouded leopard\" and\nthe fennec fox is \u201ca photo of the head of a fennec fox\".\nReceived June 07 2023; accepted July 16 2023\n2The\ndataset\nwas\nobtained\nfrom\nhttps://www.kaggle.com/datasets/balraj98/\nmonet2photo\nJurassic World Remake\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFigure 6: Revive-2I on Monet2Photo (photo\u2192Monet): Columns: Input, 25%, 50%, 75%, 100%.\nFigure 7: Failures of Revive-2I50% on Skull2Dog. In order: Boston Terrier, Boxer, Rottweiler.\nFigure 8: Failures of Revive-2I95% on Skull2Dog. In order: Boston Terrier, Rottweiler, Chihuahua.\nFigure 9: Revive-2I95% on Skull2Leopard. In order: Cheetah, Clouded Leopard, Leopard.\nFigure 10: Revive-2I95% on Skull2Fox. In order: Fennec Fox, Kit Fox, Red Fox.\n"
  },
  {
    "title": "The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
    "link": "https://arxiv.org/pdf/2308.07286.pdf",
    "upvote": "5",
    "text": "The Devil is in the Errors: Leveraging Large Language Models for\nFine-grained Machine Translation Evaluation\nPatrick Fernandes\u2217,2,3,4\nDaniel Deutsch1\nMara Finkelstein1\nParker Riley1\nAndr\u00e9 F. T. Martins3,4,5\nGraham Neubig2,6\nAnkush Garg1\nJonathan H. Clark1\nMarkus Freitag1\nOrhan Firat1\n1Google\n2Carnegie Mellon University\n3Instituto Superior T\u00e9cnico\n4Instituto de Telecomunica\u00e7\u00f5es\n5Unbabel\n6Inspired Cognition\npfernand@cs.cmu.edu\nAbstract\nAutomatic evaluation of machine translation\n(MT) is a critical tool driving the rapid iter-\native development of MT systems. While\nconsiderable progress has been made on esti-\nmating a single scalar quality score, current\nmetrics lack the informativeness of more de-\ntailed schemes that annotate individual er-\nrors, such as Multidimensional Quality Met-\nrics (MQM). In this paper, we help fill this\ngap by proposing AUTOMQM, a prompt-\ning technique which leverages the reasoning\nand in-context learning capabilities of large\nlanguage models (LLMs) and asks them\nto identify and categorize errors in transla-\ntions. We start by evaluating recent LLMs,\nsuch as PaLM and PaLM-2, through simple\nscore prediction prompting, and we study\nthe impact of labeled data through in-context\nlearning and finetuning. We then evaluate\nAUTOMQM with PaLM-2 models, and we\nfind that it improves performance compared\nto just prompting for scores (with particu-\nlarly large gains for larger models) while\nproviding interpretability through error spans\nthat align with human annotations.\n1\nIntroduction\nEvaluating natural language generation systems has\nalways been challenging, and as the output qual-\nity of these systems has improved, evaluation has\nbecome even more challenging and critical. For ex-\nample, in Machine Translation (MT), a field where\nevaluation has garnered considerable attention, pre-\nvious standard automatic surface-level metrics such\nas BLEU (Papineni et al., 2002) are becoming less\nreliable as the quality of generation systems im-\nproves, with little remaining correlation with hu-\nman judgments (Freitag et al., 2022).\nTo keep pace with the constantly improving qual-\nity of MT output, the next generation of automatic\n\u2217 Work done while working part-time at Google.\nSource: \u201cAvaliar tradu\u00e7\u00e3o \nautom\u00e1tica \u00e9 dif\u00edcil.\u201d\nCandidate: \u201cEvaluating \nautomatic translation are easy.\u201d\nScore the following translation from 0 to 100:\nPortuguese: {source}; English:{candidate}\nIdentify the errors in the translation\nPortuguese: {source}; English:{candidate}\nScore: 25\nErrors: \u2018easy\u2019 - major/accuracy; \u2018are\u2019 - minor/\ufb02uency\nMQM\nScore: -5x1(major) - 1x1(minor) = -6\nA\u1d1c\u1d1b\u1d0fMQM\nScore Prediction\nFigure 1: Illustration of how AUTOMQM uses\nLLMs to assess the quality of a translation. Rather\nthan asking for a single quality score, AUTOMQM\nprompts models to identify and classify errors, and\nuses the MQM framework to produce a score.\nmetrics is rapidly evolving. Learned automatic\nmetrics that leverage human-judgments to finetune\nlanguage models (Sellam et al., 2020; Rei et al.,\n2022a) currently represent the state-of-the-art in au-\ntomatic evaluation benchmarks like the WMT Met-\nrics task (Freitag et al., 2022), and show high corre-\nlation with human judgments. However, these met-\nrics typically output a single, uninterpretable qual-\nity score, making it difficult to understand the type\nand extent of errors identified by them. The lack of\ninsights makes it difficult for model developers to\nleverage these metrics to improve their systems.\nUnlike automatic metrics that only provide a\nsingle scalar value as quality score, state-of-the-art\nhuman evaluation methodologies like Multidi-\nmensional Quality Metrics (MQM; Lommel et al.,\n2014; Freitag et al., 2021a) ask professional\nannotators to identify and label error spans with\na category and severity. This much richer feedback\ncan be used to gain a better understanding of the\ncurrent limitations of the model under evaluation\narXiv:2308.07286v1  [cs.CL]  14 Aug 2023\nand improve it.\nIn this paper, we ask whether large language\nmodels (LLMs) in combination with a few human\nannotations can be used to design an automatic\nmetric that generates rich feedback similar to that\ngenerated by human experts in MQM. This work\nis motivated by recent papers that demonstrated\nthat LLMs can be used as automatic metrics (Liu\net al., 2023b) to generate a single quality score. In\nparticular, Kocmi and Federmann (2023) showed\nthat LLMs can be prompted to assess the quality\nof machine-generated translations, even achieving\nstate-of-the-art performance on assessing system-\nlevel quality. However, previous work only pro-\nvides a limited view of the capabilities of LLMs for\nmachine translation evaluation: the focus has pre-\ndominantly been on score prediction (i.e. predict-\ning a numerical value for quality), without consid-\nering the use of any annotated data (either through\nin-context learning or finetuning), and only in high-\nresource language pairs.\nWe provide a large-scale study of the capabilities\nof LLMs (from the PaLM and PaLM-2 families;\nChowdhery et al., 2022; Anil et al., 2023) for ma-\nchine translation evaluation (both with and without\na reference translation), provide a novel compari-\nson between prompting and finetuning, and investi-\ngate the performance in the low-resource scenario.\nInspired by findings that the performance of LLMs\ncan be improved by prompting them for rationales\nof their predictions (Wei et al., 2022; Lu et al.,\n2023), we also propose AUTOMQM, a prompt-\ning technique for MT evaluation that asks LLMs\nto identify error spans in a translation and to clas-\nsify these errors according to the MQM framework,\nwith a quality score derived automatically from the\nidentified errors. A key advantage of AUTOMQM\nis its interpretability, as users can inspect the errors\nresponsible for a score (Figure 1).\nOur contributions can be summarized as follows:\n\u2022 We confirm the finding of Kocmi and Feder-\nmann (2023) that LLMs are zero-shot state-of-\nthe-art system-level evaluators, but show low\ncorrelation with human judgment compared\nto learned metrics at the segment-level.\n\u2022 We show that finetuning an LLM with hu-\nman judgment mitigates its low segment-level\nperformance (particularly for smaller LLMs),\nshowing similar correlations with human judg-\nment at both the system-level and segment-\nlevel to state-of-the-art learned metrics.\n\u2022 We are the first to evaluate LLM-based evalu-\nation methods on low-resource language pairs.\nWe find that their performance is promising,\nbut lags behind state-of-the-art learned met-\nrics.\n\u2022 We find that, with AUTOMQM, PaLM-2 mod-\nels can be prompted to generate rich MQM-\nlike annotations, outperforming their score\nprediction counterparts at the segment-level.\n\u2022 Furthermore, annotations predicted by PaLM-\n2 models correctly identify over 50% of words\nthat are part of major errors, and are compa-\nrable to the ones produced by state-of-the-art\nsupervised word-level evaluators.\nOur findings might have significant implica-\ntions for not only MT evaluation, but evaluation\nof machine-generated text in general, and further\nhighlight the potential of using LLMs to provide\nAI Feedback (Fernandes et al., 2023).\n2\nBackground: MT Evaluation\nMachine translation evaluation is one of the most\nwell-studied evaluation problems in NLP (Callison-\nBurch et al., 2008; Freitag et al., 2022). In this task,\ngiven\n1. a source sentence in a (source) language\n2. a candidate translation in a (target) language\nan evaluation metric assesses the quality of the\ncandidate translation by how well it conveys the\nmeaning of the source sentence while considering\nother factors like fluency. Like many other natu-\nral language generation evaluation problems, this\ntask is difficult because the set of correct transla-\ntions for a given source sentence is often very large\nand not entirely known in advance. To simplify\nthe problem of machine translation evaluation, of-\nten (3) a reference translation (typically created\nby a professional human translator) is included as\nadditional information when assessing the candi-\ndate translation. This sub-problem is known as\nreference-based evaluation (as opposed reference-\nless evaluation or quality estimation).\nUp until recently, human evaluation of machine\ntranslation was carried out predominantly with the\naim of assigning a single quality score to a can-\ndidate translation. Consequently, learned metrics,\nwhich leverage collected human judgment data, are\ntrained for and evaluated on the same task of score\nprediction (i.e., assigning a single quality score to\na candidate translation), and can achieve high cor-\nrelation with human-provided scores (Freitag et al.,\n2022).\nHowever, framing machine translation evalu-\nation as a score prediction task is problematic:\nany scoring or ranking of translations is implicitly\nbased on an identification of errors in the candidate\ntranslations, and asking raters to solely provide a\nsingle score can lead to rushed and noisy judgments\n(Freitag et al., 2021a).\nThis insight has led to the adoption of the\nMultidimensional Quality Metrics (MQM) frame-\nwork (Lommel et al., 2014; Freitag et al., 2021a)\nas the gold standard for evaluating machine transla-\ntion. The MQM framework asks human evaluators\nto identify error spans in candidate translations and\nclassify those errors according to various dimen-\nsions, e.g., fluency, accuracy, ... (see Appendix A\nfor a more detailed description of MQM). Impor-\ntantly, the MQM framework does not ask anno-\ntators to provide a quality score for each transla-\ntion, and instead derives one automatically from\nthe identified error spans and their classifications.\nHowever, despite its richness, most automatic met-\nrics that leverage MQM data only use the final qual-\nity score produced by the framework and discard\nthe error span information and classification.\n3\nRelated Work\nThe success of learned machine translation met-\nrics (Sellam et al., 2020; Rei et al., 2022a; Freitag\net al., 2022; Qin et al., 2022), which finetune neu-\nral network models pretrained on large amounts of\n(unsupervised) data, highlighted the importance of\nleveraging transfer learning to achieve metrics with\nbetter correlation with human judgments. More re-\ncently, generative LLMs (OpenAI, 2023; Anil et al.,\n2023) have consistently demonstrated impressive\nresults in natural language understanding and zero-\nand few-shot transfer and, naturally, interest in em-\nploying these models for (translation) evaluation\nhas increased. Kocmi and Federmann (2023) first\nexplored the use of GPT models for evaluating\nmachine translation tasks, showing their potential\nas zero-shot evaluators, and others have since ex-\ntended GPT-based evaluation to other generation\nproblems (Jain et al., 2023; Liu et al., 2023b).\nPerrella et al. (2022) first highlighted that MQM\nannotations could be leveraged to allow pretrained\nmodels to predict major and minor errors and, sim-\nilarly to AUTOMQM, used the identified errors\nto automatically score translations. However, their\napproach relied on weaker encoder-only or encoder-\ndecoder language models, required supervised data\nto work, and overall underperformed other top met-\nrics. We compare against their MaTASe metric in\nour experiments. Lu et al. (2023) showed that do-\ning error analysis, a prompting technique similar to\nAUTOMQM, could lead to better ChatGPT-based\nevaluators. However, they still relied on the LLM\nto provide a score once it identified errors (rather\nthan do it automatically using something like the\nMQM framework). Furthermore, they provided\na very limited meta-evaluation using only 40 ex-\namples per language pair. Concurrently with our\nwork, Xu et al. (2023) proposed INSTRUCTSCORE,\na LLaMA-based evaluator that asks models to iden-\ntify and categorize errors in translation (as well as\nproviding a natural language explanation for each\nerror). However, the authors only explore a 7B\nparameter model and don\u2019t leverage zero- and few-\nshot capabilities of models as in this work. Instead,\nthey rely on a more complex approach of distilling\nthe knowledge of a more capable GPT-4 LLM.\nAdditionally, WMT Word-Level Quality Esti-\nmation shared tasks (Fonseca et al., 2019; Zerva\net al., 2022) leverage MQM data by converting\nspan-level annotations of errors (normally of ma-\njor severity) to word-level tags and Task 2 in the\nWMT19 Quality Estimation shared task evaluation\nexplicitly evaluated submissions of span-level anno-\ntations (although most submissions still consisted\nof models that predicted word-level tags which\nwere converted to spans). We also compare against\nstate-of-the-art word-level quality estimation mod-\nels.\n4\nUsing LLMs to Predict Quality Scores\nRecent works have shown that large language mod-\nels are versatile, general-purpose models that can\nbe used to tackle many problems in NLP, includ-\ning evaluation (Kocmi and Federmann, 2023; Jain\net al., 2023; Liu et al., 2023b). We begin by explor-\ning how LLMs can be used for machine translation\nevaluation through score prediction.\n4.1\nPrompting\nWe start by measuring how far we can push the\nperformance of LLMs with just prompting (Liu\net al., 2023a): by defining the task of MT evaluation\nand quality estimation as textual templates (with\na general description of the problem and \u201cslots\u201d\nfor the inputs and outputs), we can use general-\npurpose LLMs to perform these tasks at inference-\ntime, without any parameter updates.\nThroughout the paper, we choose to use Kocmi\nand Federmann (2023)\u2019s GEMBA-SQM prompt\n(Figure 2), which asks models to generate (a string\nrepresentation of) a score from 0-100. We choose\nthis prompt for two reasons: firstly, early explo-\nrations with theirs and other prompts showed that\nthis generally performed well. Secondly, using a\nsingle prompt ensures a fairer comparison between\nthe capabilities of different models.1\nScore the following translation from\n{src_lang} to {tgt_lang} with respect\nto the human reference on a continuous\nscale from 0 to 100 that starts with\n\"No meaning preserved\", goes through\n\"Some meaning preserved\", then \"Most\nmeaning preserved and few grammar mistakes\",\nup to \"Perfect meaning and grammar\".\n{src_lang} source: \"{source}\"\n{tgt_lang} human reference: \"{reference}\"\n{tgt_lang} translation: \"{candidate}\"\nScore (0-100): {score}\nFigure 2: The score prediction prompt used in this paper.\nEquivalent to the GEMBA-SQM prompt in Kocmi and\nFedermann (2023). Parts in purple are only included\nfor reference-based evaluation, while parts in orange\nrepresent slots for outputs and are only included for in-\ncontext examples.\nIn-Context Learning\nA surprising emergent ca-\npability of LLMs is their ability to improve on\nprompting-based tasks by including a very small\namount of labeled data as part of the prompt/con-\ntext (Brown et al., 2020) and without parameter up-\ndates, a technique called in-context learning (ICL).\nWe thus investigate the impact that ICL has on\nLLMs\u2019 ability to assess translation quality. Recent\nworks have shown that the impact of ICL is tightly\ntied with the exact examples included in the prompt,\nwith a poor selection procedure leading to no im-\nprovements or even worse performance than the\nzero-shot case (Jain et al., 2023). We therefore ex-\nplore two sampling approaches to select in-context\nexamples from a pre-defined \u201cpool\u201d of translation\nquality assessments: uniform sampling and strati-\nfied sampling, where the example pool is bucketed\nby score ranges and examples are sampled from\neach bucket.\n1While this prompt wasn\u2019t the best for system-level, it led\nto the best segment-level performance in GEMBA.\n4.2\nFinetuning\nIt has previously been shown that LLMs are capa-\nble of zero-shot evaluation (Kocmi and Federmann,\n2023), but the extent to which finetuning on human\njudgment data can further boost the performance of\nLLMs has not been studied. In the WMT\u201922 Met-\nrics Shared Task (Freitag et al., 2022), all top sub-\nmissions were learned metrics; that is, pretrained\nmodels finetuned on human judgment data2.\nThus,\nwe investigate whether LLMs are\namenable to finetuning on human judgment data.\nLLMs used in top-performing metrics are gener-\nally much larger than the pretrained language mod-\nels leveraged by previous learned metrics (which\ngenerally have fewer than 1 billion parameters).\nMoreover, most learned metrics leverage pretrained\nencoder-only rather than (decoder-only) prefix lan-\nguage models.\nWe experiment with finetuning\nLLMs using two objectives:\n\u2022 Regression (R): Commonly used for training\nlearned metrics (Rei et al., 2022a), the ob-\njective here is a regression loss (e.g., mean\nsquared error) between continuous scores ob-\ntained from the model (for example, with a\nregression head) and the human scores.\n\u2022 Generative Classification (GC): We bucket\nscores into discrete classes (see \u00a76.1) and treat\nthe MT evaluation task as a text-to-text classi-\nfication problem (Raffel et al., 2020).\n5\nUsing LLMs to Predict Error Spans\nWhile producing quality scores that correlate with\nhuman judgments is an important part of transla-\ntion quality assessment, metrics that solely do score\nprediction suffer from problems of interpretabil-\nity: if a metric assigns a low score, the downstream\nusers are left in the dark about which parts of the\ntranslation were responsible for the score and thus\nneed to be corrected. This is especially problematic\nin cases where the metric assigns a wrong score to\na translation, as it is much harder to diagnose why\nthe evaluation model made a mistake, and iden-\ntify and prevent similar mistakes in the future. In\nfact, reducing translation quality to a single score\nhas proven problematic even for human annotators:\nasking raters to solely provide a single score can\nlead to rushed and noisy judgments (Freitag et al.,\n2While these metrics all leverage powerful pretrained (lan-\nguage) models, these generally aren\u2019t considered LLMs\nBased on the given source and reference, identify the major and minor errors in this\ntranslation. Note that Major errors refer to actual translation or grammatical errors,\nand Minor errors refer to smaller imperfections, and purely subjective opinions about\nthe translation.\n{src_lang} source: \"{source}\"\n{tgt_lang} human reference: \"{reference}\"\n{tgt_lang} translation: \"{candidate}\"\nErrors: {error1:span} - {error1:severity}/{error1:category}; {error2:span} - ...\nFigure 3: The AUTOMQM prompt used in this paper. Parts in purple are only included for reference-based\nevaluation, while parts in orange represent slots for outputs, and are only included for in-context examples.\n2021a) and the current gold standard for transla-\ntion quality evaluation involving human annotators\nis instead based on methodologies like the MQM\nframework (see \u00a72) , which provide richer feedback\nby identifying error spans, categorizing them, and\nevaluating their severity.\nInterestingly, another emergent phenomenon in\nLLMs is the success of chain-of-thought prompt-\ning (Wei et al., 2022): when defining a prompt\nfor a particular task, if we instruct the model to\nproduce a series of intermediate reasoning steps\n(\u201clet\u2019s think step-by-step\u201d), it tends to generate\na free-text rationale before generating an output,\nand this often improves the performance on the\ntask at hand (Liu et al., 2023b). Furthermore, this\nchain-of-thought prompting can be used to obtain\nstructured rationales from LLMs, and this can lead\nto better performance than with free-text rationales\n(Lu et al., 2023).\nMotivated by these findings,\nwe propose\nAUTOMQM, a prompting technique for transla-\ntion quality assessment that instructs LLMs to iden-\ntify errors in a translation, and categorize the type of\nerror according to the MQM framework (Lommel\net al., 2014). Furthermore, we don\u2019t ask the model\nto produce a score, as the MQM framework pro-\nvides an algorithmic procedure to obtain one from\nidentified errors: the total score is the sum of penal-\nties for all errors identified, where (roughly) major\nerrors get penalized with \u22125 and minors with \u22121\n(see Appendix A for a more detailed description of\nthe scoring algorithm).3 Figure 3 shows the main\nAUTOMQM prompt used in this paper.\nImportantly, obtaining meaningful AUTOMQM\nresults in a zero-shot setting is a substantially more\nchallenging task compared to score prediction: we\nfound that, without any in-context examples, LLMs\ntend to produce outputs that are either uninforma-\n3This is similar to methods that leverage external executors\nto improve the performance of LLMs (Gao et al., 2022)\nLP\n#Sys\n#Seg\nen\u2192de\n13\n1315\nzh\u2192en\n14\n1875\nen\u2192ru\n15\n1315\nLP\n#Sys\n#Seg\nen\u2192kk\n11\n998\nkk\u2192en\n11\n1000\nen\u2192gu\n11\n998\ngu\u2192en\n11\n1016\nTable 1: The number of systems and segments that\nhave MQM scores (left) and DA scores (right) used as\nground-truth in this work.\ntive or difficult to parse. Thus we only consider the\nAUTOMQM task in the few-shot scenario. Based\non the findings from \u00a76.2, we explore the impact\nof in-context learning by sampling from the exam-\nple pool using stratified sampling extended with a\nset of rejection criteria (Appendix B), which en-\nsures that the example set has a balance between\nmajor and minor errors as well as diversity in the\ncategories of errors.\n6\nExperiments\n6.1\nExperimental Setup\nData\nThe metrics in this work are evaluated\non both high-resource and low-resource language\npairs. The three high-resource language pairs come\nfrom the WMT\u201922 Metrics Shared Task (Freitag\net al., 2022): en\u2192de, zh\u2192en, and en\u2192ru. The\nground-truth translation quality scores are derived\nfrom MQM ratings in which expert annotators\nmarked error spans in the translations with different\nseverity levels which are automatically converted\nto a numeric score (see \u00a72). The four low-resource\nlanguage pairs come from the WMT\u201919 Metrics\nShared Task (Ma et al., 2019): en\u2194gu and en\u2194kk.\nSince MQM ratings are not available for the low-\nresource pairs, the ground truth quality scores are\ndirect assessment (DA) scores. DA scores are qual-\nity assessments assigned by non-expert raters on a\nscale from 0-100, then normalized per rater. See Ta-\nble 1 for statistics about the number of MT systems\nand segments for every language pair.\nAdditionally, in our experiments, AUTOMQM\nrequired in-context examples with MQM anno-\ntations to work, so we restrict our evaluation of\nAUTOMQM to en\u2192de and zh\u2192en because there\nare available MQM ratings from the WMT\u201921 Met-\nrics Shared Task (Freitag et al., 2021b) that we can\nuse as in-context learning example pools.\nModels\nWe base most of our experiments on the\nfollowing LLMs:\n\u2022 PaLM: A 540 billion parameter autoregres-\nsive Transformer model trained on 780 billion\ntokens of high-quality text (Chowdhery et al.,\n2022). It showed remarkable performance on\na wide-range of NLP tasks, including Machine\nTranslation (Vilar et al., 2022).\n\u2022 PaLM-2:\nThe successor to PaLM, the\nPaLM-2 family of LLMs (Anil et al., 2023)\nbuilds upon recent research insights, such as\ncompute-optimal scaling, a more multilingual\nand diverse pre-training mixture, and architec-\ntural/optimization improvements. We mainly\nuse two model sizes in the family: PaLM-2 BI-\nSON and (the larger) PaLM-2-UNICORN.4 In\naddition we explore the impact of instruction-\ntuning by using a UNICORN model finetuned\non the FLAN dataset (Wei et al., 2021).\nFor score prediction, we compare PaLM and\nPaLM-2 against the GPT family of LLMs (Brown\net al., 2020; OpenAI, 2023) by leveraging the\nresults and outputs from the GEMBA evaluator\n(Kocmi and Federmann, 2023). We then evaluate\nthe performance of AUTOMQM with only PaLM-2\nmodels (which performed best in score prediction).\nAdditionally, for the high-resource languages,\nwe compare to a set of strong baseline eval-\nuation metrics, MetricX-XXL and COMET-22,\nwhich were the two top-performing metrics in the\nWMT\u201922 Metrics Shared Task. MetricX-XXL and\nCOMET-22 are both finetuned regression models\ntrained on DA data from WMT that are initialized\nwith mT5 (Xue et al., 2021) and XLM-R (Conneau\net al., 2020), respectively.\nFor the AUTOMQM experiments, we also com-\npare against MATESE, a comparable submission\nto the WMT\u201922 Metrics Shared task that finetuned\na XLM-R model to identify major and minor errors,\nand computed a score automatically. Since we were\n4Information about exact number of parameters of PaLM-2\nmodels is not publicly available.\nunable to obtain the span-level predictions for the\nMATESE submission, we also compare against the\ntop submission to the WMT\u201922 Word-Level Qual-\nity Estimation Shared Task (Zerva et al., 2021):\nword-level COMETKIWI (COMET-WL) (Rei et al.,\n2022b), also based on an XLM-R model trained on\na combination of sentence- and word-level data. To\ndo so, we re-run this model on the WMT\u201922 Met-\nrics Shared Task data, and convert the predicted\nword-level OK/BAD tags into spans.5\nFinetuning\nFor regression finetuning, we use a\nreal-valued logit, extracted from a fixed index in the\nfirst target token\u2019s logit vector, as the quality signal.\n(In particular, we leverage a special, unused, vocab-\nulary token.) This was the technique used to train\nMetricX-XXL in the WMT 2022 Shared Task sub-\nmission (Freitag et al., 2022). The regression-based\nmodel was trained on WMT direct assessment (DA)\ndata from the years 2015 through 2020.\nFor generative classification, we bucket the\nscores in the training data into five classes, where\nclass boundaries are assigned so that each class\ncontains an equal number of training examples. We\nthen map labels to verbal ratings from the follow-\ning set, based on their bucket: [\"very bad\", \"bad\",\n\"ok\", \"good\", \"very good\"]. To evaluate the model,\npredictions are mapped back to integer labels from\n1 to 5. Any predictions not containing a substring in\nthe label set are considered invalid and are mapped\nto 0. We experimented with finetuning on both DA\nand MQM 2020 (Freitag et al., 2021a) data, and\nfound that the latter performed slightly better.\nTo assess the impact of model size, we also\nfinetune two additional (smaller) PaLM-2 models,\nwhich we call S and M, comparing their finetuned\nand zero-shot performance.6\nMetric Meta-Evaluation\nThe quality of an au-\ntomatic evaluation metric is estimated by compar-\ning the agreement between the metric scores and\nground-truth quality scores on a large number of\ntranslations from different MT systems, a process\nknown as metric meta-evaluation. This work re-\nports three different agreement scores, as follows.\nThe first is system-level accuracy, which calcu-\nlates the percent of system pairs that are ranked the\nsame by the metric and ground-truth scores, micro-\n5We consider a span as any maximal consecutive sequence\nof words marked as BAD, assigning every span the major\nseverity.\n6We use a small variation of the zero-shot prompt, asking\nmodels for scores from the same 5 buckets used in finetuning.\naveraged over a set of language pairs (Kocmi et al.,\n2021). System-level scores are defined as the aver-\nage score across all segments.\nAt the segment-level, the standard correlation\nthat is reported by WMT is Kendall\u2019s \u03c4. However,\nrecent work pointed out problems with Kendall\u2019s \u03c4\nwith respect to ties (Deutsch et al., 2023). In short,\ndifferent variants of \u03c4 are inconsistent with respect\nto ties and even biased against metrics that predict\nties, as our metrics do in this work. Deutsch et al.\n(2023) recommend reporting a pairwise accuracy\nscore, which rewards metrics for correctly ranking\ntranslations as well as correctly predicting ties, in\ncombination with a tie calibration procedure that\nautomatically introduces ties into metric scores so\nthat the meta-evaluation is fairer. This accuracy\nscore, denoted acc\u2217, ranges between 0 and 1, and\na random metric would achieve 33% accuracy. We\nreport the \u201cgroup-by-item\u201d variant of the pairwise\naccuracy score from Deutsch et al. (2023) in ad-\ndition to Pearson\u2019s \u03c1, a complementary signal to\nrank-based correlations that measure the strength of\nthe linear relationship between two variables (and\none of the standard correlations reported in WMT).\nSpan Meta-Evaluation\nSince AUTOMQM pro-\nvides not only scores but also the identified error\nspans, we can compare the predicted spans with the\nerrors marked by annotators in the MQM annota-\ntions. We evaluate quality of predicted spans using:\n(1) Span Precision (SP), which measures the over-\nlap of predicted spans and gold (annotated) spans;\nand (2) Major recall (MR), which captures the per-\ncentage of gold major errors that were predicted as\nerrors (either minor or major).\nMore formally, consider the set of ground truth\nspans S\u22c6, where each span consists of a sequence of\nwords, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6\nmaj \u2286\nS\u22c6 be the subset containing only the major errors.\nGiven a span set S, we define its positional set\nP(S) as the set containing the positions of all the\nwords in every span in S. For example, assuming a\nspan si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth\nposition in the text, its corresponding positional set\nwill include the positions {n, n+1, ..., n+len(si)\u2212\n1}. Then for a set of predicted spans \u02c6S, SP and\nMR are defined as:\nSP( \u02c6S) = |P( \u02c6S) \u2229 P(S\u22c6)|\n|P( \u02c6S)|\n(1)\nMR( \u02c6S) =\n|P( \u02c6S) \u2229 P(S\u22c6\nmaj)|\n|P(S\u22c6\nmaj)|\n(2)\nIntuitively, we care for overall precision (regard-\nless of severity) since we want to make sure pre-\ndicted errors tend to be marked by annotators as\nwell, but for recall we care mostly for major errors,\nas these have a larger impact on translation qual-\nity and are more critical to identify. Additionally,\nwe also report the (3) Matthews Correlation Coeffi-\ncient (MCC), one of the official metrics in the word-\nlevel quality estimation tasks (Zerva et al., 2022).\n6.2\nResults\n6.2.1\nScore Prediction\nTable 2 summarizes the meta-evaluation results, at\nthe system and segment level, for both the zero-shot\nprompting and finetuning settings.\nPrompting\nA first observation is almost all zero-\nshot LLM evaluators have higher system-level per-\nformance than learned metrics (with and without\nreferences), with PaLM 540B and PaLM-2 UNI-\nCORN achieving the best performance. At the seg-\nment level, the story is more complicated: similarly\nto Kocmi et al. (2022), we find that none of the\nLLMs we explored was able to consistently out-\nperform the baseline learned metrics. We see that\nPaLM-540B is a particularly poor reference-based\nevaluator, which is surprising given its system-level\nperformance.\nUnexpectedly, instruction-tuning\nwith FLAN seems to degrade performance, with\nFLAN-PaLM-2 UNICORN achieving poor perfor-\nmance at both the system and segment levels.7\nNevertheless, PaLM-2 models achieve high cor-\nrelations with human judgments, and the reference-\nless PaLM-2 BISON is competitive with the learned\nbaselines, particularly at assessing alternative trans-\nlations of the same sentence (acc\u2217). When com-\nparing PaLM-2 models with Kocmi et al. (2022)\u2019s\nGPT-based GEMBA evaluator (Table 3), we see\nthat both families of LLMs perform similarly,\nwith PaLM-2 models exhibiting higher system-\nlevel performance than GPT-based GEMBA, while\nGEMBA achieves better segment-level accuracy,\nparticularly in the reference-less setting.\nFigure 4 shows the distribution of scores pro-\nduced by PaLM- and PaLM-2-based evaluators.\nWe find that, despite being prompted to give a score\nin the 0-100 range, these models almost always out-\nput one of a very limited set of scores (e.g. 0, 50,\n90, 95). Given Kocmi and Federmann (2023)\u2019s\n7Note that this might be a problem with the FLAN dataset\nand not instruction-tuning in general, as the GPT models are\nalso instruction-tuned and perform well.\nSystem-Level\nSegment-Level\nAll (3 LPs)\nEN-DE\nZH-EN\nEN-RU\nModel\nRef?\nAccuracy\n\u03c1\nacc\u22c6\n\u03c1\nacc\u22c6\n\u03c1\nacc\u22c6\nBaselines\nMetricX-XXL\n\u2713\n85.0%\n0.549\n61.1%\n0.581\n54.6%\n0.495\n60.6%\nCOMET-22\n\u2713\n83.9%\n0.512\n60.2%\n0.585\n54.1%\n0.469\n57.7%\nCOMET-QE\n\u2717\n78.1%\n0.419\n56.3%\n0.505\n48.8%\n0.439\n53.4%\nPrompting\nPaLM 540B\n\u2713\n90.1%\n0.247\n55.4%\n0.255\n48.5%\n0.180\n48.6%\nPaLM-2 BISON\n\u2713\n88.7%\n0.394\n56.8%\n0.322\n49.3%\n0.322\n52.8%\nPaLM-2 UNICORN\n\u2713\n90.1%\n0.401\n56.3%\n0.349\n51.1%\n0.352\n55.3%\nFLAN-PaLM-2 UNICORN\n\u2713\n75.9%\n0.197\n55.6%\n0.139\n46.1%\n0.198\n52.0%\nPaLM 540B\n\u2717\n84.3%\n0.239\n56.1%\n0.270\n43.1%\n0.300\n51.8%\nPaLM-2 BISON\n\u2717\n85.0%\n0.355\n57.0%\n0.299\n48.6%\n0.303\n53.1%\nPaLM-2 UNICORN\n\u2717\n84.3%\n0.275\n56.1%\n0.252\n48.3%\n0.209\n49.8%\nFLAN-PaLM-2 UNICORN\n\u2717\n69.7%\n0.116\n54.6%\n0.112\n43.8%\n0.156\n47.8%\nFinetune\nPaLM-2 BISON (R)\n\u2713\n88.0%\n0.511\n61.0%\n0.459\n51.5%\n0.458\n59.5%\nPaLM-2 BISON (GC)\n\u2713\n86.1%\n0.400\n59.2%\n0.444\n49.3%\n0.365\n56.0%\nPaLM-2 UNICORN (R)\n\u2713\n87.6%\n0.508\n61.1%\n0.412\n52.6%\n0.460\n60.4%\nPaLM 2 BISON (R)\n\u2717\n87.6%\n0.490\n59.9%\n0.439\n53.4%\n0.437\n59.2%\nPaLM 2 BISON (GC)\n\u2717\n86.1%\n0.368\n57.5%\n0.420\n47.3%\n0.390\n54.9%\nPaLM 2 UNICORN (GC)\n\u2717\n86.1%\n0.407\n57.9%\n0.402\n45.6%\n0.411\n55.3%\nTable 2: Meta-evaluation results at system and segment-level for the high-resource language pairs.\nFinetuned (R) and (GC) represent the regression and generative classification objectives (\u00a74.2). \u2713and \u2717\nrepresent reference-based and reference-less metrics, respectively.\nSystem\nSegment acc\u22c6\nModel\nRef?\nAll\nEN-DE ZH-EN EN-RU\nGEMBA\nGPT-3.5\n\u2713\n85.4% 54.9% 49.5% 47.5%\nGPT-4\n\u2713\n88.7% 57.8% 52.6% 55.0%\nGPT-3.5\n\u2717\n82.5% 56.1% 49.7% 49.3%\nGPT-4\n\u2717\n89.1% 56.4% 53.4% 54.8%\nBISON\n\u2713\n88.7% 56.8% 49.3% 52.8%\nUNICORN\n\u2713\n90.1% 56.3% 51.1% 55.3%\nBISON\n\u2717\n85.0% 57.0% 48.6% 53.1%\nUNICORN\n\u2717\n84.3% 56.1% 48.3% 49.8%\nTable 3: Comparison between PaLM-2 and GPT-based\nGEMBA (Kocmi et al., 2022) at the system and segment\nlevels for the high-resource language pairs.\nsimilar findings with GPT models, it seems that\nthis is a consequence of the pretraining objective.\nFinetuning\nDespite their already-great perfor-\nmance in the zero-shot setting, we find that fine-\ntuning LLMs can further improve LLM evaluators\u2019\nsegment-level scores. This is particularly obvious\nfor the reference-less evaluators, where a finetuned\nPaLM-2 BISON achieves state-of-the-art perfor-\nmance in segment-level correlations and compa-\nrable system-level accuracy across all language\npairs.\nMoreover, when we look at how perfor-\nmance scales with parameter count (Figure 5), we\nobserve an interesting trend: while smaller models\nare not capable of being effective zero-shot evalu-\nators, finetuning them leads to competitive perfor-\nmance, and only a slight decrease when compared\nError\n1\n10\n100\n1000\n10000\n0\n5 10\n20\n50\nGold MQM (+100)\nPaLM-2 (Unicorn)\nPaLM (540B)\nFLAN-PaLM-2 (Unicorn)\n60\n70\n80 85 90 95 100\nScore\n# occurrences (en-de)\nFigure 4:\nDistribution of scores for various LLM\nreference-based evaluators, on the EN-DE test set. Note\nthat the y axis is in log-scale.\nto their larger finetuned counterparts.\nIn-context Learning\nFigure 6 shows the mean\nand interquartile range (IQR) of the performance\nas we increase the number of in-context examples\nk (with 100 example sets per k) sampled with strat-\nified sampling (see Appendix C for uniform). Sur-\nprisingly, despite evidence of the benefits of in-\ncontext learning for many tasks, we found that\nincluding in-context examples during evaluation\n(almost) never led to better performance, either\nwith uniform or stratified sampling.\nTo investigate the cause of this disappointing per-\nformance, we looked at how particular in-context\nexample sets affect the distribution of scores pro-\nduced by LLM-based evaluators. Figure 7 shows\nthe distribution of scores over the whole test set\nfor the 1-shot and 2-shot settings, with different\nS\nM\nBison\nModel\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPearson (EN-DE)\nZero-Shot\nFinetune\nFigure 5: Behavior of Pearson as we scale the LLM\u2019s\nparameter count. Note that the x axis is not to-scale\nwith regard to parameter count.\n0\n1\n2\n3\n4\n# of in-context examples\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nPearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 6: Mean Pearson and its interquartile range\n(IQR) in the WMT22 EN-DE test set, as we increase the\nnumber of in-context examples with stratified sampling\nin-context examples sets. We can see that output\ndistribution is heavily biased by the scores in the\nin-context examples: despite never predicting 79\nin the zero-shot setting, when a single example\nwith that score is included, it starts to dominate\nthe model predictions. This seems to hint that\nLLMs \u201coverfit\u201d to the specific scores provided as\nexamples, rather than generalizing to the broader\nevaluation task, which could explain the lackluster\nperformance of in-context learning.\n6.3\nLow Resource Languages\nTable 4 shows the performance of PaLM-2 mod-\nels at score prediction for low-resource transla-\ntion. Overall, we find that similar to high-resource\nLPs, these models are good zero-shot evaluators,\nwith system-level accuracies around 90%. How-\never, zero-shot LLMs underperform learned met-\nrics, even when these metrics also weren\u2019t exposed\nto data in these low-resource languages.\n6.3.1\nAUTOMQM\nFigure 8 shows the mean and interquartile range\n(IQR) of the performance of PaLM-2 BISON with\nAUTOMQM, as we increase the number of in-\ncontext examples (again, with 100 example sets per\nError\n1\n10\n100\n1000\n10000\n0\n5 10\n20\nScore: 79.0\nScore: 94.0\nScore: 99.0\n50\n60\n70\n80 85 90 95 100\nScore\n# occurrences (en-de)\nError\n10\n100\n1000\n10000\n0\n5 10\n20\n50\nScore: 85.0+95.0\nScore: 99.0+90.0\n60\n70\n80 85 90 95 100\nScore\n# occurrences (en-de)\nFigure 7: Distribution of scores for PaLM-2 (BISON)\nmodels for 1-shot (top) and 2-shot (bottom) setups,\nwith various in-context learning sets for each (and their\nscores in the legend)\nSystem\nSegment \u03c1\nModel\nRef?\nAll\nEN-KK EN-GU KK-EN GU-EN\nBaseline\nMetricX-XXL\u22c6 \u2713\n94.0% 0.666 0.701 0.539 0.409\nPrompting\nBISON\n\u2713\n92.2% 0.605 0.540 0.462 0.339\nUNICORN\n\u2713\n87.4% 0.609 0.621 0.495 0.384\nBISON\n\u2717\n89.8% 0.567 0.478 0.381 0.313\nUNICORN\n\u2717\n84.4% 0.536 0.523 0.433 0.334\nTable 4: Meta-evaluation results for system-level accu-\nracy and segment-level Pearson on the low-resource lan-\nguages, using PaLM-2 for score prediction. \u22c6Note that\nthe baseline is slightly different from the high-resource\ncase, being trained on the same data but without these\nlow-resource language pairs.\nk). Contrary to the performance with score predic-\ntion, we find that performance with AUTOMQM\nseems to (mostly) scale with the number of in-\ncontext examples: performance increases monoton-\nically with up to 4 in-context examples and plateaus\nthereafter. Additionally, the variance across the in-\ncontext learning sets seems to be lower, with most\nexample sets exhibiting less than 0.05 Pearson dif-\nference from the best-performing sets. All this sug-\ngests that LLM evaluators are much more robust to\nthe choice of in-context examples when prompted\nfor AUTOMQM rather than for score prediction.\nWe also find that the behavior of in-context learn-\ning is quite similar for both reference-based and\nreference-less evaluation tasks. Finally, we observe\n0\n1\n2\n3\n4\n5\n6\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nPearson (EN-DE)\nPaLM-2 (Bison)\nPaLM-2 (Bison) ref-free\n0\n1\n2\n3\n4\n5\n6\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPearson (ZH-EN)\nPaLM-2 (Bison)\nPaLM-2 (Bison) ref-free\nFigure 8: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples\nin the AUTOMQM prompt, for EN-DE (left) and ZH-EN (right).\nSystem-Level\nSegment-Level\nAll (2 LPs)\nEN-DE\nZH-EN\nModel\nRef?\nAccuracy\n\u03c1\nacc\u22c6\n\u03c1\nacc\u22c6\nBaselines\nMetricX-XXL\n\u2713\n81.1%\n0.549\n61.1%\n0.581\n54.6%\nMATESE\n\u2713\n79.9%\n0.391\n58.8%\n0.528\n51.5%\nCOMET-QE\n\u2717\n76.9%\n0.419\n56.3%\n0.505\n48.8%\nMATESE-QE\n\u2717\n73.4%\n0.298\n57.9%\n0.468\n50.1%\nCOMET-WL\n\u2717\n71.6%\n0.418\n57.1%\n0.406\n51.5%\nScore Prediction\nPaLM-2 BISON\n\u2713\n86.4%\n0.394\n56.8%\n0.322\n49.3%\nPaLM-2 UNICORN\n\u2713\n86.4%\n0.401\n56.3%\n0.349\n51.1%\nPaLM-2 BISON\n\u2717\n84.0%\n0.355\n57.0%\n0.299\n48.6%\nPaLM-2 UNICORN\n\u2717\n80.5%\n0.275\n56.1%\n0.252\n48.3%\nAutoMQM\nPaLM-2 BISON\n\u2713\n84.0%\n0.369\n59.2%\n0.355\n48.4%\nPaLM-2 UNICORN\n\u2713\n87.6%\n0.432\n59.1%\n0.442\n51.8%\nPaLM 2 BISON\n\u2717\n87.6%\n0.297\n55.2%\n0.331\n48.0%\nPaLM 2 UNICORN\n\u2717\n83.4%\n0.368\n56.4%\n0.429\n50.2%\nTable 5: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system\nand segment levels for multiple language pairs.\nthat the example sets that perform well for one task\ngenerally work well for the other, with performance\non both settings given a fixed in-context set being\nhighly correlated, as shown in Figure 9.\n0.0\n0.1\n0.2\n0.3\nPearson w/o Reference\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPearson with Reference\nCorr = 0.941\n1\n2\n3\n4\n5\n6\nNumber of Examples\nFigure 9:\nScatter plot of the Pearson of PaLM-2\n(BISON) models, with/without including the reference\nin the prompt, for each in-context learning setting tried.\nTable 5 shows the meta-evaluation results for\nPaLM-2 BISON and UNICORN prompted with\nAUTOMQM (using the best-performing in-context\nlearning sets in Figure 8). For ease of comparison,\nwe also report their performance when prompted\nfor score prediction, as well as the performance\nof the baselines. Overall, prompting LLMs with\nAUTOMQM seems to lead to significant improve-\nments in evaluating machine translation quality,\nparticularly for larger models: UNICORN achieves\nbetter performance (across all meta evaluations)\nwith it than when prompted for score prediction,\nand its reference-less version is competitive with\nthe best learned metric even at the segment level.\nHowever, for the smaller BISON, the benefits of\nAUTOMQM are less clear, with both techniques\nperforming comparably. This hints that scale is\nnecessary for zero- and few- shot fine-grained evalu-\nation (like with AUTOMQM). We also find that the\ndistribution of scores produced by LLMs prompted\nwith AUTOMQM is much closer to the gold MQM\ndistribution, with models outputting a much larger\nset of scores, and in the same ranges as annotators\ndo (see Figure 10).\n40\n35\n30\n25\n20\n15\n10\n5\n0\n1\n10\n100\n1000\n10000\nGold MQM\nPaLM-2 (Bison)\nPaLM-2 (Unicorn)\nScore\n# occurrences (en-de)\nFigure 10: Distribution of scores for PaLM-2 models\nusing AUTOMQM, on WMT22 EN-DE\nEN-DE\nZH-EN\nModel\nR? SP\nMR MCC SP\nMR MCC\nBaselines\nCOMET-WL \u2717 0.267 0.250 0.161 0.364 0.178 0.152\nAutoMQM\nBISON\n\u2713 0.095 0.749 0.060 0.252 0.255 0.109\nUNICORN\n\u2713 0.175 0.628 0.193 0.238 0.476 0.143\nBISON\n\u2717 0.119 0.520 0.092 0.224 0.311 0.091\nUNICORN\n\u2717 0.150 0.580 0.150 0.229 0.488 0.133\nTable 6: Span-level meta-evaluation on WMT22 for\nPaLM-2 models using AutoMQM. SR and MR represent\nspan precision and major recall, respectively.\nFinally, when evaluating the error spans pro-\nduced by LLMs prompted with AUTOMQM (Ta-\nble 6), we find that PaLM-2 models are able to\nidentify most of the major errors. However, it does\nseem to over-predict errors (with errors predicted\nby UNICORN having on average \u223c5 words per span\nvs \u223c2 words in the ground truth) and have overall\nlow span precision. Similarly to overall score cor-\nrelations, scale also seems to be important for the\nquality of spans produced by AUTOMQM, with\nUNICORN outperforming BISON at most metrics.\nAdditionally, UNICORN prompted with AutoMQM\npredicts spans of comparable quality to the ones\nproduced by current state-of-the-art learned word-\nlevel evaluators (trained on a considerable number\nof fine-grained annotations derived from MQM):\nwhile word-level models are more precise, their\noverall span correlation (MCC) is comparable, and\nthey miss considerably more major errors than\nLLMs (despite only leveraging a handful of an-\nnotations).\n7\nConclusion\nIn this study, we have systematically investi-\ngated the capabilities of large language models\nfor machine translation evaluation through score\nprediction, and proposed AUTOMQM, a novel\nprompting technique that leverages the Multidi-\nmensional Quality Metrics (MQM) framework for\ninterpretable MT evaluation using LLMs.\nWe demonstrated that just prompting LLMs for\nscore prediction leads to state-of-the-art system-\nlevel evaluators, but still falls short of the best\nlearned metrics at the segment-level (with fine-\ntuning being necessary to close this gap). Then\nwe showed that AUTOMQM can further improve\nthe performance of LLMs without finetuning while\nproviding interpretability through error spans that\nalign with human annotations.\nOur findings surrounding finetuning LLMs for\nscore prediction hint that LLMs\u2019 performance in\nmachine translation evaluation could be further im-\nproved by finetuning these models on fine-grained\nhuman judgment data (like MQM) and is a direc-\ntion we are actively pursuing. Additionally, the\ngeneral-purpose nature of LLMs may enable the\napplication of similar prompting techniques (lever-\naging some fine-grained evaluation schemes) to\nother evaluation problems (Wu et al., 2023).\nAcknowledgements\nWe would like to thank Ricardo Rei, Marcos\nTreviso and Chryssa Zerva for helping run the\nword-level QE baselines, and George Foster who\nprovided feedback on an earlier version of this\nwork. This work was partially supported by EU\u2019s\nHorizon Europe Research and Innovation Actions\n(UTTER, contract 101070631), the P2020 pro-\ngram MAIA (LISBOA-01-0247-FEDER-045909),\nthe Portuguese Recovery and Resilience Plan,\nand the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnolo-\ngia through contracts SFRH/BD/150706/2020 and\nUIDB/50008/2020.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin\nJohnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bai-\nley, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy\nMeier-Hellstern, Gaurav Mishra, Erica Moreira,\nMark Omernick, Kevin Robinson, Sebastian\nRuder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yu-\njing Zhang, Gustavo Hernandez Abrego, Jun-\nwhan Ahn, Jacob Austin, Paul Barham, Jan\nBotha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng,\nColin Cherry, Christopher A. Choquette-Choo,\nAakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi\nDave, Mostafa Dehghani, Sunipa Dev, Jacob De-\nvlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Fein-\nberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lu-\ncas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi\nHashemi, Le Hou, Joshua Howland, Andrea\nHu, Jeffrey Hui, Jeremy Hurwitz, Michael Is-\nard, Abe Ittycheriah, Matthew Jagielski, Wen-\nhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee,\nBenjamin Lee, Eric Li, Music Li, Wei Li,\nYaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao\nLin, Zhongtao Liu, Frederick Liu, Marcello\nMaggioni, Aroma Mahendru, Joshua Maynez,\nVedant Misra, Maysam Moussalem, Zachary\nNado, John Nham, Eric Ni, Andrew Nystrom,\nAlicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily\nReif, Bryan Richter, Parker Riley, Alex Cas-\ntro Ros, Aurko Roy, Brennan Saeta, Rajku-\nmar Samuel, Renee Shelby, Ambrose Slone,\nDaniel Smilkov, David R. So, Daniel Sohn, Si-\nmon Tokumine, Dasha Valter, Vijay Vasude-\nvan, Kiran Vodrahalli, Xuezhi Wang, Pidong\nWang, Zirui Wang, Tao Wang, John Wieting,\nYuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue,\nPengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou,\nSlav Petrov, and Yonghui Wu. 2023. Palm 2\ntechnical report.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sas-\ntry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark\nChen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are\nfew-shot learners. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages\n1877\u20131901. Curran Associates, Inc.\nChris Callison-Burch, Philipp Koehn, Christof\nMonz, Josh Schroeder, and Cameron Shaw\nFordyce, editors. 2008.\nProceedings of the\nThird Workshop on Statistical Machine Transla-\ntion. Association for Computational Linguistics,\nColumbus, Ohio.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko,\nJoshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchin-\nson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robin-\nson, Liam Fedus, Denny Zhou, Daphne Ip-\npolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omer-\nnick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Dou-\nglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel. 2022. Palm: Scaling language model-\ning with pathways.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm\u00e1n, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nUnsupervised cross-lingual representation learn-\ning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 8440\u20138451, Online. Associa-\ntion for Computational Linguistics.\nDaniel Deutsch, George Foster, and Markus Freitag.\n2023. Ties Matter: Modifying Kendall\u2019s Tau for\nModern Metric Meta-Evaluation.\nPatrick Fernandes, Aman Madaan, Emmy Liu,\nAnt\u00f3nio Farinhas, Pedro Henrique Martins,\nAmanda Bertsch, Jos\u00e9 G. C. de Souza, Shuyan\nZhou, Tongshuang Wu, Graham Neubig, and\nAndr\u00e9 F. T. Martins. 2023. Bridging the gap:\nA survey on integrating (human) feedback for\nnatural language generation.\nErick Fonseca, Lisa Yankovskaya, Andr\u00c3\u00a9 F. T.\nMartins, Mark Fishel, and Christian Federmann.\n2019. Findings of the wmt 2019 shared tasks\non quality estimation. In Proceedings of the\nFourth Conference on Machine Translation (Vol-\nume 3: Shared Task Papers, Day 2), pages 1\u201312,\nFlorence, Italy. Association for Computational\nLinguistics.\nMarkus Freitag, George Foster, David Grang-\nier, Viresh Ratnakar, Qijun Tan, and Wolfgang\nMacherey. 2021a. Experts, errors, and context:\nA large-scale study of human evaluation for ma-\nchine translation. Transactions of the Associ-\nation for Computational Linguistics, 9:1460\u2013\n1474.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-\nkiu Lo, Craig Stewart, Eleftherios Avramidis,\nTom Kocmi, George Foster, Alon Lavie, and\nAndr\u00e9 F. T. Martins. 2022. Results of WMT22\nmetrics shared task: Stop using BLEU \u2013 neu-\nral metrics are better and more robust. In Pro-\nceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 46\u201368, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for\nComputational Linguistics.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-\nkiu Lo, Craig Stewart, George Foster, Alon\nLavie, and Ond\u02c7rej Bojar. 2021b. Results of the\nWMT21 metrics shared task: Evaluating metrics\nwith expert-based human evaluations on TED\nand news domain. In Proceedings of the Sixth\nConference on Machine Translation, pages 733\u2013\n774, Online. Association for Computational Lin-\nguistics.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri\nAlon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. 2022.\nPal: Program-\naided language models.\narXiv preprint\narXiv:2211.10435.\nSameer\nJain,\nVaishakh\nKeshava,\nSwar-\nnashree Mysore Sathyendra, Patrick Fernandes,\nPengfei Liu, Graham Neubig, and Chunting\nZhou. 2023. Multi-dimensional evaluation of\ntext summarization with in-context learning.\nTom Kocmi, Rachel Bawden, Ond\u02c7rej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles,\nPhilipp Koehn, Christof Monz, Makoto Mor-\nishita, Masaaki Nagata, Toshiaki Nakazawa,\nMichal Nov\u00e1k, Martin Popel, and Maja Popovi\u00b4c.\n2022. Findings of the 2022 conference on ma-\nchine translation (WMT22).\nIn Proceedings\nof the Seventh Conference on Machine Trans-\nlation (WMT), pages 1\u201345, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators\nof translation quality.\nTom Kocmi, Christian Federmann, Roman Grund-\nkiewicz, Marcin Junczys-Dowmunt, Hitokazu\nMatsushita, and Arul Menezes. 2021. To ship or\nnot to ship: An extensive evaluation of automatic\nmetrics for machine translation. In Proceedings\nof the Sixth Conference on Machine Translation,\npages 478\u2013494, Online. Association for Compu-\ntational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\nJiang, Hiroaki Hayashi, and Graham Neubig.\n2023a. Pre-train, prompt, and predict: A sys-\ntematic survey of prompting methods in natu-\nral language processing. ACM Comput. Surv.,\n55(9).\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-\neval: Nlg evaluation using gpt-4 with better hu-\nman alignment.\nArle Lommel, Hans Uszkoreit, and Aljoscha Bur-\nchardt. 2014. Multidimensional quality metrics\n(MQM): A framework for declaring and describ-\ning translation quality metrics. Revista Trad-\num\u00e0tica: tecnologies de la traducci\u00f3.\nQingyu Lu, Baopu Qiu, Liang Ding, Liping Xie,\nand Dacheng Tao. 2023. Error analysis prompt-\ning enables human-like translation evaluation in\nlarge language models: A case study on chatgpt.\narXiv preprint.\nQingsong Ma, Johnny Wei, Ond\u02c7rej Bojar, and\nYvette Graham. 2019. Results of the WMT19\nmetrics shared task: Segment-level and strong\nMT systems pose big challenges. In Proceedings\nof the Fourth Conference on Machine Transla-\ntion (Volume 2: Shared Task Papers, Day 1),\npages 62\u201390, Florence, Italy. Association for\nComputational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. Bleu: a method for auto-\nmatic evaluation of machine translation. In Pro-\nceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, pages\n311\u2013318, Philadelphia, Pennsylvania, USA. As-\nsociation for Computational Linguistics.\nStefano Perrella, Lorenzo Proietti, Alessandro\nScir\u00e8, Niccol\u00f2 Campolungo, and Roberto Nav-\nigli. 2022. MaTESe: Machine translation eval-\nuation as a sequence tagging problem. In Pro-\nceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 569\u2013577, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for\nComputational Linguistics.\nYiwei Qin, Weizhe Yuan, Graham Neubig, and\nPengfei Liu. 2022. T5score: Discriminative fine-\ntuning of generative evaluation metrics. ArXiv,\nabs/2212.05726.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex-\nploring the limits of transfer learning with a uni-\nfied text-to-text transformer. J. Mach. Learn.\nRes., 21(1).\nRicardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya\nGlushkova, Alon Lavie, Luisa Coheur, and An-\ndr\u00e9 F. T. Martins. 2022a. COMET-22: Unbabel-\nIST 2022 submission for the metrics shared task.\nIn Proceedings of the Seventh Conference on Ma-\nchine Translation (WMT), pages 578\u2013585, Abu\nDhabi, United Arab Emirates (Hybrid). Associa-\ntion for Computational Linguistics.\nRicardo Rei, Marcos Treviso, Nuno M. Guer-\nreiro, Chrysoula Zerva, Ana C Farinha, Christine\nMaroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova,\nDuarte Alves, Luisa Coheur, Alon Lavie, and\nAndr\u00e9 F. T. Martins. 2022b. CometKiwi: IST-\nunbabel 2022 submission for the quality esti-\nmation shared task. In Proceedings of the Sev-\nenth Conference on Machine Translation (WMT),\npages 634\u2013645, Abu Dhabi, United Arab Emi-\nrates (Hybrid). Association for Computational\nLinguistics.\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 7881\u20137892, Online. Associa-\ntion for Computational Linguistics.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming\nLuo, Viresh Ratnakar, and George Foster. 2022.\nPrompting palm for translation: Assessing strate-\ngies and performance.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned\nlanguage models are zero-shot learners. In In-\nternational Conference on Learning Representa-\ntions.\nJason Wei, Xuezhi Wang, Dale Schuurmans,\nMaarten Bosma, brian ichter, Fei Xia, Ed H.\nChi, Quoc V Le, and Denny Zhou. 2022. Chain\nof thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Infor-\nmation Processing Systems.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri,\nAlane Suhr, Prithviraj Ammanabrolu, Noah A.\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi.\n2023. Fine-grained human feedback gives better\nrewards for language model training.\nWenda Xu, Danqing Wang, Liangming Pan, Zhen-\nqiao Song, Markus Freitag, William Yang Wang,\nand Lei Li. 2023. Instructscore: Towards ex-\nplainable text generation evaluation with auto-\nmatic feedback.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. In Proceedings of the 2021 Conference\nof the North American Chapter of the Associ-\nation for Computational Linguistics: Human\nLanguage Technologies, pages 483\u2013498, Online.\nAssociation for Computational Linguistics.\nChrysoula Zerva, Fr\u00e9d\u00e9ric Blain, Ricardo Rei,\nPiyawat Lertvittayakumjorn, Jos\u00e9 G. C. de\nSouza, Steffen Eger, Diptesh Kanojia, Duarte\nAlves, Constantin Or\u02d8asan, Marina Fomicheva,\nAndr\u00e9 F. T. Martins, and Lucia Specia. 2022.\nFindings of the WMT 2022 shared task on\nquality estimation. In Proceedings of the Sev-\nenth Conference on Machine Translation (WMT),\npages 69\u201399, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Lin-\nguistics.\nChrysoula Zerva, Daan van Stigt, Ricardo Rei,\nAna C Farinha, Pedro Ramos, Jos\u00e9 G. C. de\nSouza, Taisiya Glushkova, Miguel Vera, Fabio\nKepler, and Andr\u00e9 F. T. Martins. 2021. IST-\nunbabel 2021 submission for the quality estima-\ntion shared task. In Proceedings of the Sixth\nConference on Machine Translation, pages 961\u2013\n972, Online. Association for Computational Lin-\nguistics.\nA\nMultidimensional Quality Metric (MQM)\nThe Multidimensional Quality Metrics (MQM) framework is a flexible human-evaluation framework\ndeveloped to evaluate and categorize errors in translations. Annotators are instructed to identify all errors\nwithin each segment in a document, paying particular attention to document context. See Table 7 for the\nannotator guidelines provided.\nYou will be assessing translations at the segment level, where a segment may contain one or more\nsentences. Each segment is aligned with a corresponding source segment, and both segments are\ndisplayed within their respective documents. Annotate segments in natural order, as if you were reading\nthe document. You may return to revise previous segments.\nPlease identify all errors within each translated segment, up to a maximum of five. If there are more than\nfive errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because\nthe translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error\nthat spans the entire segment.\nTo identify an error, highlight the relevant span of text, and select a category/sub-category and severity\nlevel from the available options. (The span of text may be in the source segment if the error is a source\nerror or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a\nsentence contains two words that are each mistranslated, two separate mistranslation errors should be\nrecorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most\nsevere. If all have the same severity, choose the first matching category listed in the error typology (eg,\nAccuracy, then Fluency, then Terminology, etc).\nPlease pay particular attention to document context when annotating. If a translation might be questionable\non its own but is fine in the context of the document, it should not be considered erroneous; conversely,\nif a translation might be acceptable in some context, but not within the current document, it should be\nmarked as wrong.\nThere are two special error categories: Source error and Non-translation. Source errors should be\nannotated separately, highlighting the relevant span in the source segment. They do not count against the\nfive-error limit for target errors, which should be handled in the usual way, whether or not they resulted\nfrom a source error. There can be at most one Non-translation error per segment, and it should span the\nentire segment. No other errors should be identified if Non-Translation is selected.\nTable 7: MQM annotator guidelines\nAnnotators are asked to assign both an error severity and category. Error severity (either major or\nminor) is assigned independently of category. Spans with no marked errors have neutral severity and no\ncategory. Possible error categories are displayed in Table 8.\nSince MQM doesn\u2019t ask annotators for quality scores, those scores are derived automatically from the\nidentified error spans and their classifications, based on a weighting of each error severity and category.\nTable 9 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to\n25 (worst). The final segment-level score is an average over scores from all annotators. In some settings\n(e.g. calculating correlation for learned metrics), the scores are negated.\nWe use the same weighting to obtain scores from errors identified by AUTOMQM.\nB\nSampling in-context learning examples for AutoMQM\nFigure 11 shows the rejection criteria used when sampling example sets as discussed in \u00a74.\nC\nAdditional Results\nFigures 12, 13, and 14 present additional experimental results.\nError Category\nDescription\nAccuracy\nAddition\nTranslation includes information not present in the source.\nOmission\nTranslation is missing content from the source.\nMistranslation\nTranslation does not accurately represent the source.\nUntranslated text\nSource text has been left untranslated.\nFluency\nPunctuation\nIncorrect punctuation (for locale or style).\nSpelling\nIncorrect spelling or capitalization.\nGrammar\nProblems with grammar, other than orthography.\nRegister\nWrong grammatical register (eg, inappropriately informal pronouns).\nInconsistency\nInternal inconsistency (not related to terminology).\nCharacter encoding\nCharacters are garbled due to incorrect encoding.\nTerminology\nInappropriate for context\nTerminology is non-standard or does not fit context.\nInconsistent use\nTerminology is used inconsistently.\nStyle\nAwkward\nTranslation has stylistic problems.\nLocale\nAddress format\nWrong format for addresses.\nconvention\nCurrency format\nWrong format for currency.\nDate format\nWrong format for dates.\nName format\nWrong format for names.\nTelephone format\nWrong format for telephone numbers.\nTime format\nWrong format for time expressions.\nOther\nAny other issues.\nSource error\nAn error in the source.\nNon-translation\nImpossible to reliably characterize distinct errors.\nTable 8: MQM hierarchy.\nSeverity\nCategory\nWeight\nMajor\nNon-translation\n25\nall others\n5\nMinor\nFluency/Punctuation\n0.1\nall others\n1\nNeutral\nall\n0\nTable 9: MQM error weighting.\n1 def check_icl_set(\n2\nexamples: pd.DataFrame,\n3\nmin_errors=3,\n4\nmajmin_threshold=2,\n5\ncat_diversity=2,\n6\nmin_clen=20,\n7\nmax_clen=400,\n8 ):\n9\n# Check if they have the same number of spans as severity/category\n10\nif not examples.apply(\n11\nlambda r:\n12\nlen(r[\u2019span\u2019]) == len(r[\u2019severity\u2019]) and len(r[\u2019span\u2019]) == len(r[\u2019category\u2019]),\n13\naxis=1\n14\n).all():\n15\nreturn False\n16\n17\n# Check if there are at least min_errors\n18\nif examples[\u2019severity\u2019].apply(lambda svs: len(svs)).sum() < min_errors:\n19\nreturn False\n20\n21\n# Check that there\u2019s a balance of major and minor errors.\n22\nmajor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019major\u2019 for s in svs])).sum()\n23\nminor_count = examples[\u2019severity\u2019].apply(lambda svs: sum([s==\u2019minor\u2019 for s in svs])).sum()\n24\nif abs(major_count - minor_count) > majmin_threshold:\n25\nreturn False\n26\n27\n# Check that at least cat_diversity error types are represented.\n28\ncategories = examples[\u2019category\u2019].apply(lambda cs: [c.split(\"/\")[0] for c in cs])\n29\nrepresented_error_types = set().union(*categories.tolist())\n30\nif len(represented_error_types) < cat_diversity:\n31\nreturn False\n32\n33\ntop_clen = examples.apply(\n34\nlambda row: max(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)\n35\n), axis=1).max()\n36\nbot_clen = examples.apply(\n37\nlambda row: min(len(row[s]) for s in (\u2019source\u2019, \u2019reference\u2019, \u2019candidate\u2019)),\n38\naxis=1).min()\n39\n40\nif top_clen > max_clen or bot_clen < min_clen:\n41\nreturn False\n42\n43\n# All checks passed.\n44\nreturn True\nFigure 11: Rejection criteria used when sampling in-context learning examples for AUTOMQM.\n0\n1\n2\n3\n4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\n0\n1\n2\n3\n4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context\nexamples in the score prediction prompt, sampled with uniform (left) and stratified (right) sampling, for\nWMT22 EN-DE.\n0\n1\n2\n3\n4\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\n0\n1\n# of in-context examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPearson\nPaLM-2 (Bison) ref-based\nPaLM-2 (Bison) ref-free\nFigure 13: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context\nexamples in the score prediction prompt, sampled with uniform (left) and stratified (right) sampling, for\nWMT22 ZH-EN.\nError\n1\n10\n100\n1000\n10000\n0\n5 10\n20\n50\n60\n70\n80 85 90 95 100\nScore\n# occurrences (zh-en)\nFigure 14: Distribution of scores for various LLM reference-based evaluators, on the ZH-EN test set. Note that the\ny axis is in log-scale.\n"
  },
  {
    "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use",
    "link": "https://arxiv.org/pdf/2308.06595.pdf",
    "upvote": "4",
    "text": "VisIT-Bench: A Benchmark for Vision-Language\nInstruction Following Inspired by Real-World Use\nYonatan Bitton*1,2 Hritik Bansal*3 Jack Hessel*4 Rulin Shao5 Wanrong Zhu6\nAnas Awadalla5 Josh Gardner5 Rohan Taori7 Ludwig Schmidt4,5,8\nAbstract\nWe introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark\nfor evaluation of instruction-following vision-language models for real-\nworld use. Our starting point is curating 70 \u201cinstruction families\u201d that we\nenvision instruction tuned vision-language models should be able to address.\nExtending beyond evaluations like VQAv2 and COCO, tasks range from\nbasic recognition to game playing and creative generation. Following cura-\ntion, our dataset comprises 592 test queries, each with a human-authored\ninstruction-conditioned caption. These descriptions surface instruction-\nspecific factors, e.g., for an instruction asking about the accessibility of\na storefront for wheelchair users, the instruction-conditioned caption de-\nscribes ramps/potential obstacles. These descriptions enable 1) collecting\nhuman-verified reference outputs for each instance; and 2) automatic evalu-\nation of candidate multimodal generations using a text-only LLM, aligning\nwith human judgment. We quantify quality gaps between models and refer-\nences using both human and automatic evaluations; e.g., the top-performing\ninstruction-following model wins against the GPT-4 reference in just 27%\nof the comparison. VisIT-Bench is dynamic to participate, practitioners\nsimply submit their model\u2019s response on the project website; Data, code\nand leaderboard is available at https://visit-bench.github.io/.\nInstruction \nCreate a catchy title \nfor a country song \nbased upon the \nadvice printed on the \nwheel cover.\nInstruction-Conditioned Caption \nThere is a jeep parked on the street with \nthe top down. There is a wheel cover on the \nspare tire in the back with an inscription \nthat says, \"Quit your Job, Buy a Ticket, Get a \nTan, Fall in Love, Never Return.\"\nHuman-Verified GPT4 Response \n\"Tan Lines & Ticket Stubs: A Love Story \nUnbound\" - A country song about \nleaving it all behind, finding love, and \nembracing the freedom of the open \nroad.\nFigure 1: An example from VisIT-Bench, featuring an image, a challenging instruction, an\ninstruction-conditioned caption, and a human-verified GPT4 response. These elements are\nused for evaluating multimodal chatbots and updating a dynamic leaderboard.\n\u2217Equal contribution. Contact yonatanbitton1@gmail.com,hbansal@ucla.edu,jackh@allenai.org,\nschmidt@cs.washington.edu. 1Hebrew University\n2Google Research\n3UCLA\n4Allen Institute for AI\n5University of Washington\n6UCSB\n7Stanford 8LAION ,\nPreprint. Under review.\narXiv:2308.06595v4  [cs.CL]  26 Dec 2023\n1\nIntroduction\nA long-standing challenge for artificial intelligence is to build general-purpose assistants\nthat can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For\ntextual tasks, several recent works [2, 3, 4, 5, 6, 7] have shown that fine-tuning language\nmodels such as GPT-3 and LLaMA with supervised instruction+response examples [8, 9, 10]\nenables them to respond to imperative requests and questions without task-specific training.\nZero-shot generalization is promising not only for standard academic benchmarks, but \u2013\nperhaps more-so \u2013 for creative, useful, and real-world queries that downstream users of\nlanguage technologies are likely to make.\nOn the multimodal side, recent instruction-following vision-language models also provide\na zero-shot interface. Given an image (or multiple images) and a query (e.g., \u201chow many\napples are in this image?\u201d or \u201cWhat is this?\u201d or \u201cWrite a poem in the style of Robert Frost\nabout this scene.\u201d) a textual response is provided. Recent works like OpenFlamingo [11, 12],\nLLaVA [13] and others [14, 15, 16, 17, 18], have implemented this interface with promising\ninitial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20]\nare commonly used to assess performance, less is know about how models perform on\nbroader, open-ended queries that resemble real-world user behavior. Evaluations of such\nqueries typically rely on informal and qualitative approaches.\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual\nInsTruction Benchmark), a dynamic benchmark consisting of 592 challenging vision-\nlanguage instructions. Each instance contains an instruction, input image(s), a instruction-\nconditioned caption (a human-crafted caption for the image(s)/instruction), and a human\nverified reference (Figure 1). Instructions are image-contextual imperative requests or ques-\ntions, e.g., for an image of pancakes, a user asks \u201chow can I cook this in a healthy way?\u201d.\nDifferent from existing zero-shot evaluations, many of the instructions focus on open-ended\ngeneration requests (e.g., \u201cwrite a poem...\u201d or \u201cwhat should I bring if I were to visit here?\u201d).\nWe created VisIT-Bench to cover a wide array of \u201cinstruction families\u201d. Our starting point\nwas a set of 70 \u201cwish-list\u201d tasks such as \u201chome renovation\u201d and \u201cgardening tips\u201d collected by\nthe authors:1 each requiring varied high-level skills from recognition to complex reasoning\n(Figure 2). We derived 25/70 instruction families from benchmark tasks such as Visual\nQuestion Answering (VQA) [21] and robust change captioning [22] into a chatbot-style\nformat (this reformatting differs from prior work [14, 17, 13], as we focus on open-ended\nchatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\nWe started with 10 images for each instruction family. Our annotators, guided by an\nexample, create a new instruction, and provide a (permissively licensed) image. For each\ninstruction, we next collect instruction-conditioned captions \u2013 unlike prior work [23, 24]\nthese descriptions are designed not only to describe the image in general, but also, surface\ninformation targeted to the instruction. Finally, we use instruction-conditioned captions to\ngenerate a reference candidate output from GPT-4; an additional human verification step\ndiscards GPT-4 references deemed to be incorrect.\nWe conduct a large-scale empirical comparison of multimodal instruction-following models\nusing VisIT-Bench (\u00a74). We first gather predictions for each instance from 7 candidate\nmodels. Then, we collect 5K human judgements of output quality by pitting model outputs\nhead-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements.\nThis analysis not only reveals significant differences between models (e.g., that LLaVA-\n13b [13] is generally preferred to Panda [18]), but also, that the human verified references\nin our corpus are preferred significantly more than the ones generated using multimodal\nmodels. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25, 26],\n1We recognize that promising applications may not be covered by our set; and we don\u2019t necessarily advocate\nfor deploying models in all cases we cover \u2013 we hope VisIT-Bench can help to quantify shortcomings and risks.\n2\nReasoning Over \nPlots\nHow much is the \nbrowser usage for \nFirefox and Safari?\nArt Knowledge\nTeach me about this \npainting.\nRecognition\nWhere is this?\nLocation \nUnderstanding\nIf you are going for a \npicnic at this location, \nwhat items should you \ncarry with you?\nContextual \nKnowledge of Events\nTell me what is notable \nor important about the \nevent in this photo.\nFigurative Speech \nExplanation\nSomeone said that this \nman is an angel. Why?\nChemical \nIdentification\nWhich chemical \ncompound does this \nimage represent?\nHazard Identification\nIf you are driving and \ncome across this \nscenario, what should \nyou do?\nGame Playing\nWhat is the poker hand \nshown in the picture? \nIs this a good hand?\nHome Renovation\nHere is a photo of my \nbathroom. How can I \ndesign it nicer?\nFigure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we\nenvision instruction-following vision-language models should be able to follow.\nwhich provide relative \u201cskill\u201d rating estimates encoding the probability that model A will\nbe preferred to model B; and 2) win rate versus our references, which provides an absolute\nmetric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it\nonly wins in a pairwise setting against the reference in 27.4% of cases.\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs\nof model responses based on factors like correctness, relevance, and fluency. Using the\ninstruction-conditioned caption and the instruction, GPT-4 determines the better response\nbetween two options, expediting iteration compared to human preferences. We explore\nreference-free and reference-backed versions of this metric. Compared to various metrics\n(BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our\nevaluation aligns best with human preferences. For example, it achieves a 94% agreement\nrate in the cases where all five annotators agree. See Figure 7 for a schematic of the process.\nWhile it is difficult to a priori envision all possible scenarios under which more performant\nmultimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving\nvision-language models \u201cin the wild.\u201d Table 1 presents a summary of our contributions in\ncomparison to the recent works [32, 14, 17, 33, 34, 35] in the evaluation of multimodal\nchatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate\nfuture model evaluations, available in https://visit-bench.github.io/.\nTable 1: Comparison with related works for evaluating instruction-following vision-language\nmodels. Win-rates* refers to the model win-rates against a reference output/model.\nMultiInstruct [32]\nOwl [17]\nInstructBLIP [14]\nM3IT [33]\nLVLM [34]\nGAVIE [35]\nVisIT-Bench\nNumber of Models\n1\n5\n3\n4\n8\n5\n10\nNumber of Skills Tested\n9\n6\n13\n13\n47\n16\n70\nMultiple-Images\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nVideo\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nMulti-Turn Conversations\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\nMultilingual Conversations\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nInstruction-conditioned Captions\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nChatbot-style Responses\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nDataset-specific Evaluation\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\nHuman Evaluation\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\nAuto/GPT-4 Evaluation\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\nWin-rates*\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\nElo Rating\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n3\n2\nVisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark\n\u201cProvide a highly descriptive caption to allow a \ntext-only recipient to follow the instruction\u201d\nInstruction Generation\n\u201cCreate a new, challenging instruction \ninspired by the seed task for that family\u201d\nModel Output Evaluation\n\u201cEvaluate model predictions by answering the \nfollowing questions\u2026\u201d.\nImage\nSeed\nInstruction\nWhy is he waving? What \nhappened in this event?\nOutput\nMartin Luther King Jr. is waving \nto acknowledge and greet the \ncrowd of protesters who have \ngathered to hear him speak [...]\nNew \nInstruction\nWhy is this guy raising his hand?\nSelected \nImage\nImage\nInstruction There is one color of candy \nin the bowl that is missing \nfrom the spoon. Which color \nis not represented in the \nspoon full of candies?\nAnnotated\nCaption\nBrightly colored round \ncandies, with colors of red, \norange, yellow, green, pink \nand dark brown. A purple \nspoon is raised above the bin \nof candy, bringing them in for \na close-up. The spoon has \nred, orange, yellow, green \nand dark brown candy in it.\nInstruction\nI am the player with the white chess \npieces. Is there a move I can take on \nthis turn to capture one of my \nopponent\u2019s pieces?\nImage\nGPT-4 \nOutput\nYes, you can capture the black pawn \nat e5 with your white pawn at e4.\nOutput \nEval\nDoes GPT-4 correctly follow the instruction? \n[   ] Yes [X] No\nIf answered \u2018No\u2019 for the above, mark one of \nthe following:\n[   ] N/A\n[X] The dense caption is detailed enough, \nthe problem is in GPT-4\u2019s answer\n[  ] The dense caption is underspeci\ufb01ed, not \nGPT-4\u2019s fault\nTask \nFamily\nContextual Knowledge of Events\nInstruction-\nConditioned\nCaption\nThere is a wooden chess board [...] \nThe black rooks are at a8 and h8. The \nblack knights are at c6 and g8. [...]\nInstruction-Conditioned Caption Generation\nFigure 3: Data collection steps: (1) Instruction Generation - Annotators derive instances\nfrom a seed task, see Figure 3 (left). (2) Caption Generation - Rich instruction-conditioned\ncaptions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model\nEvaluation - GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top\nblocks show rater instructions; bottom blocks depict outputs.\nVisIT-Bench was built to emulate real-world applications of multimodal models through\nimage-text tasks, creating an extensive and practical benchmark. These tasks, or \u2018instruction\nfamilies\u2019, are seen as key capabilities of a high-performing vision-and-language model.\nAlthough our selections are not exhaustive, they provide a broad basis for evaluating beyond\nacademic benchmarks. We prioritize family coverage vs. number of instances-per-task. The\nfinal corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench\nSheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45\nnewly assembled instruction families or reformatted from 25 existing datasets (see Table 5).\nNotably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4).\n2.1\nData Collection\nThe authors of this work perform an initial annotation step of curating instruction families.\nFor each instruction family not derived from an existing task (45 out of 70), we designate\na name for the family (e.g., \u201cContextual Knowledge of Events\u201d) and identify an image-\ninstruction pair that exemplifies the category, along with a sample response (\u201cMartin Luther\nKing Jr. is waving to acknowledge and greet the crowd of protesters [...]\u201d). 10 sample\nfamiles are in Figure 2.\nThe following steps are carried out in collaboration with crowdworkers, who receive an\nhourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction\nexample as a guiding seed task crowdworkers formulate a new instruction that examines\nthe same instruction family (\u201cinstruction generation\u201d); (2) crowdworkers create detailed\nimage captions that describe the image and allow an entity, relying solely on this text, to\ninterpret and execute the given instruction successfully (\u201cinstruction-conditioned caption\ngeneration\u201d); (3) crowdworkers assess the correctness of GPT-4\u2019s response to the instruction\n4\nIn this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them \ncarefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):  \nNLVR2\nNone of the dogs in these images are wearing clothing.\nTwo small chihuahuas, one with all white \nfur, and the other with a light brown coat \nof fur on its back, but white fur on its \nhead and neck, are near a small pink \npillow that has large white words \nproclaiming, \"BE MINE\" on it. The all white \nchihuahua is resting its head on the \npillow, while the other one is laying \nbehind it.\n1\nA small all white chihuahua is standing \nand looking straight at the camera. It's \nwearing a pink and white polka-dotted \ndress, over which is a pink shirt with \nsome white text on it. Under the dog is a \ncement walkway, and there is grass to the \ndog's right and to the right of the \nwalkway.\n2\nGPT-4\nNo. The second image has a dog wearing a pink and white polka-dotted dress and a pink \nshirt with white text on it.\nFigure 4: An example of a multi-image instruction task from VisIT-Bench. Originally\nsourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2\nformat includes a sentence to analyze, two images, and a binary response. Our adaptation\nadds a zero-shot instruction prompt, a instruction-conditioned caption for each image and a\nhuman-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction,\nfacilitate automatic evaluation of future chatbot responses to this instance.\n(\u201cmodel output evaluation\u201d). We further elaborate on these steps using human annotators\nbelow.\nRe-formatting existing datasets.\n25/70 instruction families (corresponding to 25*10=250\ninstances) are re-formatted versions of existing vision-language tasks (See Appendix C for\nfull list).2 This process involves re-formatting tasks into chatbot-style instruction/response\nversions. In re-formatting, we re-write instructions to retain the original task\u2019s goal while\nmaintaining the original images, see Figure 4. These repurposed tasks are integrated into our\ndata collection process, ensuring uniformity between the chatbot-style answers in the full\nVisIT-Bench instances and the reinterpreted tasks.\nInstruction Generation.\nHere, annotators create a new instance from the same instruction\nfamily as a given example, along with an instruction and corresponding image. For instance,\nin Figure 3 (left), the instruction family is \u201cContextual Knowledge of Events\u201d, and the\nexample instruction is \u201cWhy is he waving? What happened in this event?\u201d alongside an\nimage of Martin Luther King, Jr. To collect images, annotators were instructed to use\nOpenverse (https://openverse.org/) for Creative Commons licened images.\nInstruction-Conditioned Caption Generation.\nAnnotators are provided with the image\nand instruction, and are tasked to construct a caption that is rich enough to allow an entity,\nsolely receiving the text they author, to follow the instruction. This caption will later facilitate\nGPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call\nthese instructions instruction-conditioned captions. See Figure 3 (middle) for an example:\nan annotator doesn\u2019t just mention the skittles and a spoon, but, given the query regarding\nspecific colors, they indicate the exact colors in detail.\nModel Output Evaluation.\nThe goal of this stage is to gather human-validated reference\nchatbot responses for each multimodal instruction query. We initially obtain response\ncandidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT4\u2019s\nprompt is: \u201cConsider an image depicted by: <caption>\u2019. Now, briefly follow this instruction,\n2Users of VisIT-Bench should also cite the original datasets.\n5\nand you can add a short explanation: <instruction>\u2019. Response: This prompt is employed\nfor both single and multiple image instances, with appropriate modifications for the latter.\nThen we verify each response with human annotators.3 If a response is marked incorrect, the\nannotator identifies whether the issue lies with the detail level of the instruction-conditioned\ncaptions or with GPT-4\u2019s response itself. For VisIT-Bench, we discard any case marked\nas incorrect for either reason.4 An example is given in Figure 3 (right), where GPT-4\u2019s\ncandidate reference response aims to answer a question about a chess position (which it does\nso incorrectly, and thus, the instance is discarded).\n2.2\nData Collection Annotation and Results\nWe conduct the data collection steps in Figure 3 using Amazon\u2019s Mechanical Turk (MTurk)\nplatform. Prior to annotating, each MTurk worker passed a qualification test, which involved\nfive to ten sample tasks designed to assess their ability to generate high-quality annotations.\nMore detailed information about the execution process and full user interface examples can\nbe found in Appendix B.\nTable 2: Human rating metrics for the VisIT-Bench\ndataset: overall, single-, and multi-image tasks.\nMetrics\nOverall\nSingle\nMulti\nGPT-4 Correct (%)\n87.3\n91.5\n63.0\nProblem in Caption (%)\n4.0\n3.6\n6.0\nProblem in GPT-4 (%)\n7.7\n3.8\n30.0\nOur annotation results are sum-\nmarized in Table 2.\nWe mea-\nsure the throughput of the col-\nlection and filtration pipeline.\nFor single-image instances, our\npipeline\u2019s yield was 91.5% from\nthe original candidate set. How-\never, the success rate dropped\nto 63.0% in the more complex\nmulti-image tasks, accompanied\nby an uptick in issues either in the captions (6.0%) or GPT-4\u2019s responses (30.0%). This drop\nsuggests that multi-image queries may pose a more difficult data collection challenge.\n3\nVisIT-Bench Analysis\nWe analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.\n3.1\nAre instruction-conditioned captions necessary?\nTo clarify the role of the instruction-conditioned captions we collect, we conducted an\nexperiment covering 150 single-image instances. Instead of using our instruction-conditioned\ncaptions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning\nmodel. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a\ntext-based chatbot response. This process is depicted in Figure 5.\nWe manually evaluated whether the resulting output accurately followed the instructions. We\nfind that while instruction-conditioned captions led to correct outputs in 91% of the cases,\nthe success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the\nimportance of instruction-conditioned captions in the construction of VisIT-Bench, and show\nthat the instances in our dataset are sophisticated enough such that most are not solvable by\nusing a simple Socratic model [38] baseline of caption \u2192 LLM.\n3An alternate annotation scheme would have been to task annotators to write target responses from scratch.\nThe rationale for using GPT-4 verification instead is derived from prior results that show promising human-\nmachine collaboration of this form [37].\n4The annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the\nresponses. We did not find or discard any instances.\n6\nWould a disabled wheelchair bound \nindividual find it easy to go into this store?\nBLIP2 Caption\na sign on a brick wall\nInstruction-Conditioned Caption\nThis image is of an exterior store front \nand sidewalk, showing two portable \nwedge ramps, one green and one red. \nThe ramps are positioned against the \nstep to two retail doorways, and seem to \nbridge the vertical distance between the \nlevel of the sidewalk and the step up to \nthe doorway. The ramps are printed with \n\"STOPGAP.CA\" on the top surface.\nIt is not possible to determine if a \ndisabled wheelchair-bound individual \nwould find it easy to go into the store \nbased on the image of \"a sign on a brick \nwall.\" Additional information about the \nstore's entrance, accessibility features, \nand layout is needed to make that \nassessment.\nYes, a disabled wheelchair-bound \nindividual would likely find it easy to go \ninto this store, as the portable wedge \nramps provide a smooth transition from \nthe sidewalk to the doorway, making it \nmore accessible for them.\nGPT-4\nGPT-4\nOverall \nSuccess:\n31%\nOverall \nSuccess:\n91%\nFigure 5: This experiment evaluates the value of instruction-conditioned captions in accurate\ninstruction-following tasks. Given an image and instruction, GPT-4 generates responses using\nboth a instruction-conditioned caption and a less detailed BLIP-2 [15] caption. The latter\u2019s\nimprecision leads to an error, emphasizing the need for detailed, task-specific captions.\n3.2\nWhat skills are required for VisIT-Bench?\nThe full list of instruction families we cover are in Appendix Table 6.\nFollowing\n[39], for the VisIT-Bench instructions, we extract the most frequent root verbs and\ntheir direct nouns (a full plot is in Figure 6).\nThe most common include:\n\u2018an-\nswer question\u2019, \u2018write story/poem\u2019, \u2018create title\u2019, etc.\nThere\u2019s also a long-tail of\ndiverse requests that demand comprehension, commonsense, and cross-modal under-\nstanding, e.g., \u2018identifying objects\u2019 to \u2018need ingredient\u2019 to \u2018connect device\u2019.\nAd-\nditional qualitative examination reveals a range of underlying skills required rang-\ning from \u2018emotion identification\u2019 to complex reasoning tasks such as \u2018paper folding\u2019.\nFigure 6: Most frequently occurring verbs (inner cir-\ncle) and their top 4 direct nouns (outer circle) in the\nVisIT-Bench instructions.\n3.3\nWhat is\ncontained in VisIT-Bench images?\nWe detect all the COCO [20] ob-\njects present in the images from our\ndataset using Yolov5-L [40]; The\nmost common detected objects in\nVisIT-Bench are \u201cperson\u201d (\u223c 900 de-\ntections), chair, and car (\u223c 100). But,\na long tail of rarer objects exists as\nwell: full distribution in Appendix\nFigure 10. Overall, to perform well\nat VisIT-Bench, a model must ac-\ncount for a broad range of scenes\nand objects.\n4\nExperiments\nWe evaluate a range of state-of-the-\nart publicly accessible vision-and-\nlanguage chatbots on the 592 in-\nstances in VisIT-Bench. In \u00a74.1, we\nprovide the details of the instruction-\nfollowing models in our benchmark.\nFollowing this, we collect the human preferences for pairwise model generations to achieve a\nhuman-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2. We\n7\nGPT-4\nInstruction-\nConditioned \nCaption\nA cave man \nwith a \nheadband is \nholding a \nmoney bill \nthat has the \nface of a cave \nman on it.\nPandaGPT\nThe unusual aspect of this image is that it appears to be a combination of two different \nscenes or images. The main focus is a cartoon drawing of a man holding a sign that says \n\"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave, \nand a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of \nunrelated subjects in the same image creates a peculiar and unexpected visual effect.\nInstructBLIP\nAn unusual aspect of this image is the man's appearance. He has long hair, a beard, and is \nwearing a loincloth, which is an ancient form of clothing worn by men in some cultures. \nAdditionally, he is holding a stack of money in his hand, which is a modern-day item. This \ncombination of ancient and modern elements creates an interesting contrast and raises \nquestions about the context and purpose of the image.\nWhat is unusual \nabout this image?\nInstructBLIP\nFigure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4\nevaluator to compare two instruction-following models with an instruction and a instruction-\nconditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\nthen develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved\ngiven new and improved models. Finally, we establish the trustworthiness of our automatic\nevaluation method by performing agreement analysis with the human judgments in \u00a74.3\n4.1\nModels\nWe evaluate LLaVA-13B [13], InstructBLIP-13B [14], MiniGPT4-7B [42], mPLUG-Owl-\n7B [17], LlamaAdapter-v2-7B [16], PandaGPT-13B [18], VisualChatGPT [43], Multimodal\nGPT [44], OpenFlamingo v1 [11, 45] , Otter v1 [46], Lynx [47] and idefics [48]. For the\nexecution-based VisualChatGPT [43], we implement a chat window for each sample, hold\ninputs and intermediate chains of thoughts and actions in memory, and feed the images and\nthe instruction sequentially. For OpenFlamingo [11] and Otter [46], we feed the image(s)\nand the instruction in an interleaved format. For the others, we feed the image to the vision\nfeature extractor and feed the instruction as a prompt to the text encoder.5\n4.2\nHuman Evaluation\nWe collect 5K pairwise human preference judgements across an initial set of 6 models and\nthe human-verified references. For 1K uniformly randomly sampled tuples of (query, model\nA, model B), we collect 5 crowdworker judgements each. Preferences are collected in a\n\u201cforced choice\u201d setting, annotators are instructed to decide based on accuracy, helpfulness,\nand detail. We provide the template for the human annotation process in Appendix Figure 15.\nWe summarize the results with two metrics:\nRelative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human\njudgement as a \u201cmatch.\u201d6 The difference between the Elo ratings of two different models\nprovides an estimate for the win probability when pitting model A vs. model B. More details\nare in Appendix D.\nAbsolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified\nreference. We use the 1.4K pairwise human judgments where one of A or B is the reference.\nWe report the percent of cases where the human judge prefers the output from that model\nvs. the human-verified GPT-4 reference output. Because we do not allow for ties in our\nforced-choice setup, if the annotator believes the responses are of equal quaity, they choose\none arbitrarily.\n5Following the authors\u2019 instructions, we run all models using default settings to obtain the best possible\nresponses. We include specific samples for reproducibility. We acknowledge hyperparameter impact and are\nwilling to reassess submissions to VisIT-Bench if conditions were sub-optimal.\n6We use the following code/hyperparameters for Elo ratings: https://github.com/lm-sys/FastChat/\nblob/main/fastchat/serve/monitor/elo_analysis.py\n8\nTable 3: Human scoring results for the models, shown as both an ELO rating and win-rate\nagainst the reference. In total, this summarizes 5.0K pairwise human judgments. matches\ncolumn indicates the number of total matches in which a particular model participates.\nWin-rate vs. reference indicates the win-rate of a model against the reference outputs.\nModel\nElo\nmatches\nWin-rate vs. reference (w/ # ratings)\nSingle Image\nHuman Verified GPT-4 Reference\n1223\n1439\n\u2013\nLLaVA (13B)\n1085\n1462\n26.23% (n=244)\nLlamaAdapter-v2 (7B)\n1061\n1507\n27.41% (n=259)\nmPLUG-Owl (7B)\n995\n1345\n14.95% (n=214)\nInstructBLIP (13B)\n957\n1315\n12.37% (n=194)\nMiniGPT-4 (7B)\n893\n1513\n14.72% (n=299)\nPandaGPT (13B)\n786\n1441\n10.48% (n=229)\nMultiple Images\nHuman Verified GPT-4 Reference\n1193\n210\n\u2013\nmPLUG-Owl\n997\n190\n15.38% (n=78)\nOtter v1\n917\n147\n3.17% (n=63)\nOpenFlamingo v1\n893\n171\n4.35% (n=69)\nResults\nTable 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human\nVerified GPT-4 reference achieves a higher rating than all alternatives, validating the quality\nof our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an\nestimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and\nan estimated win rate of 93% against the lowest performing model in this setup, PandaGPT\n(Elo =786). This result can partly be explained by the training process of the underlying\nmodels: The improved performance of LLaVA (13B) might be attributed to its fine-tuning\nprocess, which utilized 150K instruction-tuning data that is rich in both diversity and quality.\nInterestingly, despite achieving a slightly lower Elo (the computation of which is based on all\nhead-to-head \u201cmatches\u201d, rather than just ones against the human reference), LlamaAdapter-\nv2 (7B) wins with the highest rate against the reference. However, the complexity and variety\nof models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors\ninfluencing performance. While we make a preliminary attempt to unravel these intricacies\nin Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive\nfuture research.\n4.3\nAutomatic Evaluation and Leaderboard\nHistory Knowledge\nTechnical Support\nHouse Understanding\nGestures Understanding\nFigurative Speech\nAstronomy\nHome Renovation\nTraffic Signs\nColor\nCatchy Titles\nPhysical Knowledge\nHuman Emotion\nWho to call?\nRecognition\nChemical Identification\nCounting\nGame Playing\nWrite a Story\nExercise\nGardening Tips\nGraph Reasoning\nEvents Knowledge\nAnimals\nFashion products\nArt knowledge\nDirections\nDressing sense\nLocation Understanding\nMedical\nGeometry Problems\nClimate understanding\nCultural Knowledge\nDifferently abled\nFood Recipe\nArchitectural Styles\nPop Culture\nCapacity Guesstimate\nHazard Identification\nBuilding materials\nTour guide\nOCR Math\nPaper folding\nAnagrams\nBoard Games\nWrite a Poem\ngd_vcr\nvizwiz\ntextcaps\nokvqa\naokvqg\nnewyork\nvqa\ngqa\nkilogram\nwhoops\nharmful_memes\nscienceqa\naokvqa\nvcr\nclevr\ngeneral\n0\n5\n10\n15\n20\n25\nWin rate vs GPT-4 (%)\nFrom-the-wild\nExisting datasets\nInstructBLIP (13B)\nMiniGPT-4 (7B)\nLlamaAdapter-v2 (7B)\nLLaVA (13B)\nmPLUG-Owl (7B)\nOtter\nFigure 8: Reference-free assesment win rate vs. human-verified GPT4 response for each\ninstruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-\nwild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.\n9\nTable 4: Current reference-free Elo rankings as of September 25th, 2023. In total, these\nrankings summarize 31,735 \u201cmatches\u201d between models; each match consists of 2 queries to\nGPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are\nadded to the leaderboard, and more pairs of models are evaluated head-to-head for more\ninstances.\nCategory\nModel\nElo\n# Matches\nWin vs. Reference (w/ # ratings)\nSingle Image\nHuman Verified GPT-4 Reference\n1,382\n5,880\n\u2014\nLLaVA-Plus (13B)\n1,203\n678\n35.07% (n=134)\nLLaVA (13B)\n1,095\n5,420\n18.53% (n=475)\nmPLUG-Owl (7B)\n1,087\n5,440\n15.83% (n=480)\nLlamaAdapter-v2 (7B)\n1,066\n5,469\n14.14% (n=488)\nLynx(8B)\n1,037\n787\n11.43% (n=140)\nidefics (9B)\n1,020\n794\n9.72% (n=144)\nInstructBLIP (13B)\n1,000\n5,469\n14.12% (n=503)\nOtter v1 (9B)\n962\n5,443\n7.01% (n=499)\nVisualGPT (Da Vinci 003)\n941\n5,437\n1.57% (n=510)\nMiniGPT-4 (7B)\n926\n5,448\n3.36% (n=506)\nOctopus V2 (9B)\n925\n790\n8.90% (n=146)\nOpenFlamingo V1 (9B)\n851\n5,479\n2.95% (n=509)\nPandaGPT (13B)\n775\n5,465\n2.70% (n=519)\nMultimodal GPT\n731\n5,471\n0.19% (n=527)\nMultiple Images\nHuman Verified GPT-4 Reference\n1,192\n180\n-\nmPLUG-Owl\n995\n180\n6.67% (n=60)\nOtter v1\n911\n180\n1.69% (n=59)\nOpenFlamingo v1\n902\n180\n1.67% (n=60)\n5/5\n(n=336)\n4/5\n(n=200)\n3/5\n(n=154)\nAgreement Level\n48\n56\n64\n72\n80\n88\n96\n% Agree with Human (Higher=better)\nrandom\n~upper bound\nGPT-4-no-ref\nGPT-4-ref\nBERTScore\nMETEOR\nROUGE\nBLEU\nLength\nCIDEr\nFigure 9: Correlations between evaluation met-\nrics and human preferences are ranked in per-\nformance order, with our reference free evalu-\nation (GPT-4-no-ref) showing the strongest\nalignment.\nBottom line:\nrandom chance\n(50%), top line: upper performance bound.\nBecause it is costly to gather human pair-\nwise preference judgements for new model\nsubmissions, to support faster model devel-\nopment, we seek an automatic evaluation\nprocedure that produces high correlation\nwith our human evaluation setup.\nAutomatic\nevaluation\nmetric\ncandi-\ndates.\nWe\nconsider\nseveral\nexisting\nreference-backed\nevaluation\nmetrics:\nBLEU-4 [27] ,\nROUGE-L [28],\nME-\nTEOR [29], CIDEr [30], and BERTScore\n[31], we use the RoBERTa-Large english\nversion [49], treating the human-verified\nGPT-4 reference as the evaluation reference.\nWe additionally report two baseline metrics:\nrandom, which assigns a random score\nwithout accounting for the candidate, and\nlength, which assigns a score equal to the\nnumber of non-whitespace tokens in the\ncandidate.\nBeyond existing metrics and\nbaselines, following the recent line of work utilizing API-accessed LLMs with a prompt for\nautomatic evaluation [6, 50], we consider two GPT-4[7] backed evaluation metrics.\nSpecifically, we provide the LLM with: 1) a system prompt describing the desired evaluation\nbehavior; 2) the instruction-conditioned caption for the image; 3) the instruction to be\nfollowed; and 4) two candidate generations dubbed \u201cResponse A\u201d and \u201cResponse B\u201d. We\nalso consider a reference-backed version where the human-verified reference is provided as\nwell. We provide our prompts in Appendix E. To mitigate potential biases in \u201cA\u201d and \u201cB\u201d\npositioning, for all pairs of candidates, we run two queries covering both possible orderings.\n10\nOur prompt encourages the model to think step-by-step so that its chain-of-thought process\nis made explicit [51, 52]. Despite strongly encouraging the model to select between the\ntwo references in a forced-choice setup, it sometimes refuses and outputs \u201ctie\u201d which we\naccount for later. We call the reference-free version of this metric \u201cGPT4-no-ref\u201d, and the\nreference-backed version of this metric \u201cGPT4-ref\u201d.\nEvaluating evaluation metrics.\nWe measure the correlation between the candidate metrics\nand human judgements using a pairwise framework. Specifically, we use a subset of the\n5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate\ninstances are model-generated (rather than human-verified references), we have 5 pairwise\njudgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there\nis 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the\npercent of time the metric is able to accurately reconstruct a majority vote judgement from\nthe 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \u201ctie\u201d (this\nhappens in 10-15% of cases overall) \u2013 for fair comparison with the other metrics in forced\nchoice setting, we randomly choose one of the two options when GPT-4 reports a tie.\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The\nbest performing metric is our newly proposed GPT-4 based metric, which accurately recon-\nstructs majority-vote pairwise human judgments better than alternatives (p < .05; binomial\nproportion CI nonoverlapping). For example, for instances where 5/5 annotators agree,\nGPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time,\nwhereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately\n80%/78%/70% of the time; among the metrics we consider, these are reasonable options for\nstatic/offline evaluation without relying on OpenAI API access, especially when compared\nto our length baseline metric, which achieves only 60%. Notably, the reference-backed\nversion of the newly proposed GPT-4 based metric achieves comparable (but slightly worse)\nperformance compared to the reference-free version. Thus, we adopt the reference-free\nversion, which additionally enables us to place the references themselves into the the Elo\nsetup, because they are not used in the prompts.\nSystem-level Correlation. We summarize the LLM\u2019s pairwise judgements using the same\nmetrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a\nhuman judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably,\namong the 7 systems for which we gathered human ratings for, the automatic metric produces\nthe same ordering compared to human evaluation (\u03c1 = 1.0, p < .01).\nShortcomings of proposed metric. While the relative ranking of models produced by the\nautomatic metric correlates strongly with the ranking produced by human judgements, the\nwin rate vs. reference according to human judgement (Table 3) are higher overall compared\nto the win-rate vs. reference according to the automatic metric ??. One plausible explanation\nfor this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely\nmatch its own response distribution.\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across\nall the single-image instruction families. We find that there is no model that performs the\nbest and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting\nthe strengths and weaknesses of the instruction-following models along various real-world\nuse-cases.\n5\nRelated Work\nMultimodal Models for Image-Text Understanding: Recently, the field of machine\nlearning has experienced a rapid proliferation of new models which can perform various\nimage-text tasks [12, 15, 13, 53, 18, 14]. This growth has been driven by several factors, in-\ncluding the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal\n11\nC4 [11]), improved software and hardware frameworks, and advances in modality-specific\nmodels such as language models (e.g., [10]). Our work specifically evaluates models which\ncan generate textual outputs, given one or more images, and text. Recent examples of such\nmodels include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo\n[12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal\ncapabilities but has not yet seen a release of the multimodal variant).\nInstruction Following: \u201cInstruction-following\u201d is an emerging paradigm for training models\nvia language, where instead of being trained to complete only a single, fixed task (such as\nimage classification or captioning), models are trained to follow textual instructions that\ndescribe an arbitrary task, with the aim of generalizing to novel instructions. Examples\nof instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55],\nInstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities\nof these models are influenced by the quality of the training dataset, there has also been\nextensive work on developing instruction-following datasets [39, 56, 57, 13, 58] .\nTo build these models, two broad approaches have been shown to be effective. One approach\nfocuses on leveraging existing pretrained task-specific tools such as image captioners [15],\nobject detectors [59] and text-to-image generators [60] by either creating multimodal prompt\ninterfaces [43, 61] or by executing LLM-generated programs [62, 63, 64]. The other approach\n[13, 16, 65, 46, 66, 17, 11] focuses on building a single pretrained model that can follow\ninstructions by supervised finetuning on multimodal vision-language data.\nDespite the success of both these approaches on the existing vision-language datasets e.g.,\nVQA, GQA, Image Captioning [21, 67, 20], there is a lack of a high-quality benchmarking\ndataset for multimodal instruction-following tasks that reliably replicates the way in which\nhumans would interact with multimodal chatbots in the wild. Similar to the image-text\nmodels discussed above, many instruction-following models have been released directly\nas open-source without undergoing peer review or thorough evaluation. As a result, the\neffectiveness of these models for many tasks is not well-understood.\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both\nto (re)assess, and to accelerate, progress on many machine learning tasks [68]. For exam-\nple, our work draws particularly from the fields of computer vision and natural language\nprocessing, where benchmarking datasets have been critical drivers of progress. On the\nvision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical\nyardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72],\nGLUE/SuperGLUE [73, 74] and more [75, 76] seen wide use. Recent work has indicated\nthat improvements on these high-quality benchmark datasets is not the result of overfitting,\nand is a reliable indicator of genuine progress beyond the benchmark data [77, 78, 79, 80].\nHowever, high-quality benchmarking datasets and evaluation methods do not yet exist\nfor multimodal instruction-following. As a result, it is difficult to assess progress in this\ndirection, which both reduces the field\u2019s ability to identify true breakthroughs and increases\nvulnerability to potential pitfalls of evaluation that have hampered progress in other areas of\nmachine learning [68, 81].\n6\nConclusion\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal\nchatbots\u2019 capabilities. Going beyond prior efforts, VisIT-Bench\u2019s collection process centers\npotential real-world use cases, and 70 diverse instruction families encompassing a range of\ntasks from recognition to complex reasoning. Our benchmark not only offers human-verified\nreference outputs for all examples but also gives an Elo-based ranking system for multimodal\nchatbots that correlates with human judgements. Our experiments reveal a gap between\nmodel and human performance.We release data, code, and automatic metrics, encouraging\n12\ncommunity involvement. We hope VisIT-Bench can provide a new quantification of progress\nand shortcomings of multimodal AI systems.\n7\nLimitations\nAlthough VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate\nevery possible vision-language task. We hope to add more categories of tasks over time. In\nterms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction\nand response. This does not encompass multi-turn interactions between users and chatbots,\nwhich presents a promising direction for future research. Our study focuses on image-text\nmodalities. Future extensions could expand the scope to include other modalities like audio\nand video, enabling a more comprehensive evaluation. Additionally, while the dataset offers\na wide variety of tasks, a larger number of examples per category could provide more depth.\nFinally, while our GPT-4 based metric correlates well with human judgement both at the\ninstance level and at the system level, we see some evidence that the GPT-4 based metric\nhas a stronger preference for GPT-4 based generations compared to humans. Thus, models\nwhich train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our\nevaluation.\nAcknowledgements\nWe thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable\nfeedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT\nis supported by the NSF GRFP under Grant No. DGE 1656518.\nReferences\n[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,\nAndy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language\nassistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n[2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730\u201327744, 2022.\n[3] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are\nzero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n[4] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar,\nDavid Stap, et al. Super-naturalinstructions: Generalization via declarative instructions\non 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 5085\u20135109, 2022.\n[5] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-\nfollowing llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca,\n2023.\n[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n13\n[7] OpenAI. Gpt-4 technical report. arXiv, 2023.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing\nsystems, 33:1877\u20131901, 2020.\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n[11] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\nZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,\nPang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open-\nflamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n[12] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-\nanne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\nSahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\nlearning. ArXiv, abs/2204.14198, 2022.\n[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction\ntuning, 2023.\n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards\ngeneral-purpose vision-language models with instruction tuning.\narXiv preprint\narXiv:2305.06500, 2023.\n[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597, 2023.\n[16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\nPan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010, 2023.\n[17] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\nWang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em-\npowers large language models with multimodality. arXiv preprint arXiv:2304.14178,\n2023.\n[18] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:\nOne model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\nMaking the V in vqa matter: Elevating the role of image understanding in visual\nquestion answering. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 6904\u20136913, 2017.\n14\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects\nin context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer,\n2014.\n[21] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings\nof the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\n[22] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages\n4624\u20134633, 2019.\n[23] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio\nFerrari. Connecting vision and language with localized narratives. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part V 16, pages 647\u2013664. Springer, 2020.\n[24] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo\nLuo.\nPromptcap: Prompt-guided task-aware image captioning.\narXiv preprint\narXiv:2211.09699, 2022.\n[25] Arpad E Elo. The proposed uscf rating system. its development, theory, and applica-\ntions. Chess Life, 22(8):242\u2013247, 1967.\n[26] Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023.\nURL https://lmsys.org/blog/2023-05-03-arena/.\n[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for\nautomatic evaluation of machine translation. In ACL, 2002.\n[28] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text\nSummarization Branches Out, 2004.\n[29] Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation\nwith improved correlation with human judgments. In ACL workshop on Evaluation\nMeasures for MT and Summarization, 2005.\n[30] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-\nbased image description evaluation. In CVPR, 2015.\n[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\nBERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n[32] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal\nzero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n[33] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li,\nYazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards\nmulti-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n[34] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng,\nSiyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation\nbenchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n[35] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\nAligning large multi-modal model with robust instruction tuning. arXiv preprint\narXiv:2306.14565, 2023.\n15\n[36] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A\ncorpus for reasoning about natural language grounded in photographs. arXiv preprint\narXiv:1811.00491, 2018.\n[37] Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Re-\nframing human-ai collaboration for generating free-text explanations. arXiv preprint\narXiv:2112.08674, 2021.\n[38] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong,\nStefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani,\net al. Socratic models: Composing zero-shot multimodal reasoning with language.\narXiv preprint arXiv:2204.00598, 2022.\n[39] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self\ngenerated instructions. arXiv preprint arXiv:2212.10560, 2022.\n[40] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look\nonce: Unified, real-time object detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 779\u2013788, 2016.\n[41] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici,\nGabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-\nand-language benchmark of synthetic and compositional images. arXiv preprint\narXiv:2303.07274, 2023.\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-\n4: Enhancing vision-language understanding with advanced large language models.\narXiv preprint arXiv:2304.10592, 2023.\n[43] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan\nDuan. Visual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint arXiv:2303.04671, 2023.\n[44] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\nlanguage model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\n[45] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu,\nKalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo:\nAn open-source framework for training large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390, 2023.\n[46] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei\nLiu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint\narXiv:2305.03726, 2023.\n[47] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong\nLu. Sparkles: Unlocking chats across multiple images for multimodal instruction-\nfollowing models. arXiv preprint arXiv:2308.16463, 2023.\n[48] Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh,\nAnton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe\nKiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset\nof interleaved image-text documents, 2023.\n[49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n16\n[50] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\nCarlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation\nframework for methods that learn from human feedback, 2023.\n[51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models.\nIn NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n[52] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL\nhttps://arxiv.org/abs/2205.11916.\n[53] Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe\nMorency. Cross-modal attention congruence regularization for vision-language rela-\ntion alignment. arXiv preprint arXiv:2212.10549, 2022.\n[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\net al. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. arXiv preprint arXiv:2210.08402, 2022.\n[55] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey\nLevine, and Dawn Song. Koala: A dialogue model for academic research. Blog post,\nApril 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n[56] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruc-\ntion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n[57] Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei\nChang. Dynosaur: A dynamic growth paradigm for instruction-tuning data curation.\narXiv preprint arXiv:2305.14327, 2023.\n[58] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint\narXiv:2305.11206, 2023.\n[59] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan\nLi, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded language-image pre-training. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\n[60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. High-resolution image synthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n10684\u201310695, 2022.\n[61] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal\nAhmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[62] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python\nexecution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[63] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional\nvisual reasoning without training. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 14953\u201314962, 2023.\n[64] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reason-\ning with large language models. arXiv preprint arXiv:2304.09842, 2023.\n17\n[65] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\nlanguage model for dialogue with humans, 2023.\n[66] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\nMa, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language\nis not all you need: Aligning perception with language models.\narXiv preprint\narXiv:2302.14045, 2023.\n[67] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\n[68] Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are\nwe learning yet? a meta review of evaluation failures across machine learning.\nIn Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n[69] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision,\n115:211\u2013252, 2015.\n[70] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from\ntiny images. 2009.\n[71] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unan-\nswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[72] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic com-\npositionality over a sentiment treebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing, pages 1631\u20131642, 2013.\n[73] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. Glue: A multi-task benchmark and analysis platform for natural language\nunderstanding. arXiv preprint arXiv:1804.07461, 2018.\n[74] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for\ngeneral-purpose language understanding systems. Advances in neural information\nprocessing systems, 32, 2019.\n[75] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.\nA\nframework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n[76] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\nevaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n[77] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of\nnatural distribution shift on question answering models. In International Conference\non Machine Learning, pages 6905\u20136916. PMLR, 2020.\n[78] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\ncifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.\n18\n[79] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\nimagenet classifiers generalize to imagenet? In International conference on machine\nlearning, pages 5389\u20135400. PMLR, 2019.\n[80] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz\nHardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine\nlearning. Advances in Neural Information Processing Systems, 32, 2019.\n[81] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\nA survey on multimodal large language models. arXiv preprint arXiv:2306.13549,\n2023.\n[82] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploita-\ntion. arXiv preprint arXiv:2203.08242, 2022.\n[83] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test\ndata in plain text: Practical strategies for mitigating data contamination by evaluation\nbenchmarks. arXiv preprint arXiv:2305.10160, 2023.\n[84] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cog-\nnition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 6720\u20136731, 2019.\n[85] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the\nvision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860,\n2021.\n[86] Jack Hessel, Ana Marasovi\u00b4c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers,\nRobert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor\"\nunderstanding\" benchmarks from the new yorker caption contest. arXiv preprint\narXiv:2209.06293, 2022.\n[87] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional\nlanguage and elementary visual reasoning. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2901\u20132910, 2017.\n[88] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D\nHawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv\npreprint arXiv:2211.16492, 2022.\n[89] Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar,\nand Tanmoy Charkraborty. What do you meme? generating explanations for visual\nsemantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n[90] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,\nOyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal\nreasoning via thought chains for science question answering. In The 36th Conference\non Neural Information Processing Systems (NeurIPS), 2022.\n[91] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa:\nA visual question answering benchmark requiring external knowledge. In Proceedings\nof the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u2013\n3204, 2019.\n[92] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and\nRoozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world\nknowledge. arXiv, 2022.\n19\n[93] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C\nMiller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz:\nnearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM\nsymposium on User interface software and technology, pages 333\u2013342, 2010.\n[94] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a\ndataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart II 16, pages 742\u2013758. Springer, 2020.\n[95] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti,\nand Siva Reddy.\nImage retrieval from contextual descriptions.\narXiv preprint\narXiv:2203.15867, 2022.\n[96] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences be-\ntween pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n[97] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel\nStanovsky.\nVasr:\nVisual analogies of situation recognition.\narXiv preprint\narXiv:2212.04542, 2022.\n[98] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal,\nGabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to\nchallenge vision-and-language models. Advances in Neural Information Processing\nSystems, 35:26549\u201326564, 2022.\n[99] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative\nlanguage. arXiv preprint arXiv:2303.15445, 2023.\n[100] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu,\nXiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram\nunderstanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n[101] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer\nLevy. Pick-a-pic: An open dataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023.\n20\nAppendix\nA\nLicense and Intended Use\nThe VisIT-Bench dataset, along with its various contributions such as instructions, reference\noutputs, and model ranking annotations, is licensed under the Creative Commons Attribution\n4.0 International License (CC BY 4.0). This license applies to all the images we have\ndirectly contributed, each of which carries a public license specification in the \u201cpublic\nimages metadata\u201d field within the dataset sheets. However, the dataset also incorporates\nimages sourced from pre-existing collections. For these images, the original licensing terms\nare respected and remain applicable.\nVisIT-Bench\u2019s primary purpose is to function as a dynamic benchmark that continuously\nevolves and evaluates instruction-following vision-language models. In the current landscape,\ncommercial chatbots are often trained on non-disclosed and non-public datasets, which\nraises concerns about potential data contamination and inadvertent training on our evaluation\ndata [8]. This risk is further highlighted by recent studies [82, 83]. To mitigate such concerns,\nwe have chosen to withhold the complete VisIT-Bench test set from public disclosure, while\nstill making the images and instructions available for direct download. Researchers, however,\ncan utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their\nmodel predictions for evaluation. We will assess their models using the undisclosed test\nset, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing\nthe test data upon receiving reasonable and justified requests, particularly when additional\nanalysis is necessary, provided that requesters agree to our non-contamination policy which\nprohibits the use of this data for training commercial chatbots. This approach strikes a\nbalance between the need for robust model evaluation and the mitigation of potential data\ncontamination.\nA\nDataset Analysis\nbaseball glove\ntrain\nrefrigerator\ncarrot\nkeyboard\ncouch\ntoilet\ncat\nstop sign\nmotorcycle\nsurfboard\nscissors\ngiraffe\nlaptop\nknife\nbicycle\nhorse\nboat\ntraffic light\nspoon\ntruck\nbaseball bat\ncow\napple\nkite\nsuitcase\nbench\ntv\ndonut\norange\ncell phone\numbrella\ndog\nsports ball\npotted plant\ndining table\nbackpack\nbird\nvase\nremote\nwine glass\nhandbag\nclock\ncake\nbowl\ntie\nbook\ncup\nbottle\ncar\nchair\nperson\nObjects\n101\n102\n103\nCount (log-scale)\nFigure 10: Count of the various COCO objects present in the VisIT-Bench images on a\nlog-scale. The object detection was performed by Yolov5-Large [40]. We observe that the\nobject \u2018person\u2019 occurs most of the time in the dataset images i.e., 875 times.\n21\nB\nInterfaces for Collecting Human Annotations\nIn this section, we provide the templates we used to collect human annotations for the\ninstruction generation (Figure 11), the dense caption generation (Figure 12), the model\nverification (Figure 13 and Figure 14), and the model rating (Figure 15).\nFigure 11: An interface that collects the image and corresponding instruction.\nFigure 12: An interface that collects the dense caption given an image-and-instruction pair.\nFigure 13: A sample of the model verification where the GPT4 follows the instruction\ncorrectly.\n22\nFigure 14: A sample of the model verification where the GPT4 failed to follow the instruction\ndue to its incorrect reasoning.\nFigure 15: An interface that collects the feedback of the model rating.\nC\nExisting Datasets incorporated in VisIT-Bench\nIn Table 5, we listed the existing datasets that are incoprated in our VisIT-Bench. Among\nthese datasets, 15 contain a single image in each sample pair, and 10 require reasoning based\non multiple images.\n23\nTable 5: List of existing datasets in VisIT-Bench, categorized as single and multiple image\ndatasets.\nSingle\nDataset\nTopic\nVQA [21]\nVisual Question Answering\nVCR [84]\nCognition-level Visual Understanding\nGD-VCR [85]\nGeo-Diverse Commonsense Reasoning\nWHOOPS [41]\nWhat Makes this Image Strange\nNewyork Caption [86]\nHumor Understanding\nCLEVR [87]\nVisual Question Answering\nKilogram [88]\nTangrams Identification\nHarmful Memes [89]\nMemes Understanding\nScienceQA [90]\nScience Question Answering\nOK-VQA [91]\nOutside Knowledge Visual Question Answering\nAOK-VQA [92]\nOutside Knowledge Visual Question\nAOK-VQA [92]\nQuestion Generation\nVizWiz [93]\nVisual Question Answering\nGQA [67]\nVisual Question Answering on Scene Graphs\nTextCaps [94]\nVisual Question Answering on Texts\nMultiple\nRobust Change Captioning [22]\nDescribing What has Change in a Scene\nNLVR2 [36]\nTesting Visual Language Bias\nImageCoDE [95]\nImage Retrieval\nSpot-the-Diff [96]\nIdentifying Differences\nVASR [97]\nVisual Analogies\nWinoGavil [98]\nVisual Associations\nIRFL (Metaphor) [99]\nFigurative Speech Understanding\nIRFL (Idioms) [99]\nFigurative Speech Understanding\nIconQA [100]\nAbstract Diagram Understanding\nPick-a-Pic[101]\nText-to-Image User Preferences\nTable 6: List of skills and existing datasets in VisIT-Bench\n\u2018scienceqa\u2019, \u2018ocr math\u2019, \u2018recognition\u2019, \u2018okvqa\u2019, \u2018house plan understand-\ning\u2019, \u2018nlvr2\u2019, \u2018gardening tips\u2019, \u2018textcaps\u2019, \u2018architectural styles\u2019, \u2018dressing\nsense\u2019, \u2018winoground\u2019, \u2018food recipe\u2019, \u2018paper folding\u2019, \u2018whoops\u2019, \u2018spot\nthe diff\u2019, \u2018winogavil\u2019, \u2018imagecode\u2019, \u2018exercise\u2019, \u2018art knowledge\u2019, \u2018gqa\u2019,\n\u2018physical knowledge\u2019, \u2018contextual knowledge of events\u2019, \u2018home renova-\ntion\u2019, \u2018aokvqa\u2019, \u2018animals\u2019, \u2018vasr\u2019, \u2018counting\u2019, \u2018board games\u2019, \u2018solving\ngeometry problems\u2019, \u2018who to call?\u2019, \u2018clevr\u2019, \u2018building materials\u2019, \u2018hazard\nidentification\u2019, \u2018pickapick\u2019, \u2018astronomy\u2019, \u2018figurative speech explanation\u2019,\n\u2018write a story\u2019, \u2018gestures understanding\u2019, \u2018newyork\u2019, \u2018cultural knowledge\u2019,\n\u2018aokvqg\u2019, \u2018traffic sign identification\u2019, \u2018pop culture\u2019, \u2018fashion products\u2019,\n\u2018harmful memes\u2019, \u2018write a poem\u2019, \u2018vizwiz\u2019, \u2018guesstimate of capacity\u2019,\n\u2018location understanding\u2019, \u2018graph reasoning\u2019, \u2018vqa\u2019, \u2018game playing\u2019, \u2018dif-\nferently abled\u2019, \u2018chemical identification\u2019, \u2018history knowledge\u2019, \u2018climate\nand weather understanding\u2019, \u2018irfl metaphor\u2019, \u2018human emotion recog-\nnition\u2019, \u2018medical\u2019, \u2018gd vcr\u2019, \u2018vcr\u2019, \u2018technical support\u2019, \u2018catchy titles\u2019,\n\u2018kilogram\u2019, \u2018anagrams\u2019, \u2018color\u2019, \u2018tour guide\u2019, \u2018directions\u2019, \u2018irfl idiom\u2019,\n\u2018rcc\u2019\n24\nD\nElo Rating\nFor many years, the Elo rating has been popular in ranking players in zero-sum games such\nas chess [25]. Recently, it has been adopted to rate large language models (LLMs) against\neach other on the user instructions. In this work, we adopt the same strategy to rank a set\nof instruction-following vision-language models, that can grow dynamically with further\nadvances in the field.\nGiven two multimodal chatbots Ca and Cb with their absolute Elo rating Ra and Rb, respec-\ntively. Simply put, the probability of Ca winning over Cb in a head-to-head battle is given\nby:\nP(Ca wins over Cb) =\n1\n1 + 10(Ra\u2212Rb)/400\n(1)\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the\nweightage for each win and loss in a head-to-head battle between two models. In our work,\nwe use the open implementation of Elo for LLMs by FastChat at https://github.com/\nlm-sys/FastChat/blob/main/fastchat/serve/monitor/elo_analysis.py.\nE\nGPT-4 Pairwise Evaluation Prompts\nThe specific prompts we use to extract pairwise judgements from our language model are\nprovided in Table 16 (reference-free version) and Table 17 (reference-backed version). When\napplied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the\nmodel. But, in some cases, the model either produces a pairwise judgement in an unexpected\nformat, or, refuses to issue a judgement at all. For cases like these, we issue an additional\nquery to ChatGPT to extract an answer (or decide there is no answer) using an additional\nprompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we\ncall the result a tie.\n25\nsystem prompt (human authored)\nYou are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\nan instruction in the context of an image. More specifically, you will be given the following:\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n2. An instruction: This is a question, an imperative request, or something similar about the image which requires a\nresponse.\n3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of\nthe image.\nYour job is to judge whether response A or response B better. A and B are randomly ordered.\nSome things to remember:\n- Even though you are just a language model, the image description will be sufficiently detailed so that your judgements\ncan be accurate.\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency,\nspecificity, etc.\n- You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\nuser (human authored)\nI will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address\nthe instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to\nfirst reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning\nstep-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\"\nwhere X is either A or B.\nassistant (human authored)\nSure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B.\nThen, I will think step-by-step and provide my ultimate judgement as to which response is better.\nuser (from human-authored template)\nOK. Here is the image description, the instruction, and the two response options, Response A and Response B. Image\ncontext: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are\npeople sitting on the background with their snowboards.\nInstruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\nResponse A: no\nResponse B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white.\nThink step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\nassistant (machine generated; used to derive the prediction)\nStep 1: Identify the key information from the image description.\nThe skier is wearing:\n- Burgundy pants\n- White shoes\nStep 2: Assess the accuracy of each response in addressing the instruction.\nBoth Response A and Response B convey that the trousers and shoes are not of the same color.\nStep 3: Compare specificity and detail of each response.\nResponse A is concise and provides a straightforward answer, but lacks detail.\nResponse B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\nStep 4: Consider the fluency and correctness.\nBoth responses are fluent and grammatically correct.\nOverall, Response B is better.\nFigure 16: The prompt we use for reference-free evaluation, along with a sample in-\nstance/completion from GPT-4 [7].\n26\nsystem prompt (human authored)\nYou are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\nan instruction in the context of an image. More specifically, you will be given the following:\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n2. An instruction: This is a question, an imparative request, or something similar about the image which requires a\nresponse.\n3. A reference output: This is a high-quality example output that humans have judged to be an accurate response for the\ninput instruction.\n4. Two responses, response A and response B: These two responses attempt to address the instruction in the context of\nthe image.\nYour job is to judge whether response A or response B better. A and B are randomly ordered. Some things to remember:\n- Even though you are just a language model, the image description will be sufficiently detailed so that your judgements\ncan be accurate.\n- Take the high-quality reference into account when making your judgements, but remember: some instructions are\nmore open-ended than others, so for those cases, a high quality response can differ from the reference.\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency,\nspecificity, etc.\n- You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\nuser (human authored)\nI will describe an image to you, and provide an instruction. Then, I will provide a reference output which is an example\nof a high quality output for that instruction in the context of the image. Then, I will give you two candidate responses\nthat address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job\nis to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning\nstep-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\"\nwhere X is either A or B.\nassistant (human authored)\nSure, please provide the image context, the instruction, the reference, and the two candidate responses, Response A and\nResponse B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\nuser (from human-authored template)\nOK. Here is the image description, the instruction, the high-quality reference, and the two response options, Response A\nand Response B.\nImage context: There is a white plate that has on it a small pizza with many topics, mostly unrecognizable, due to the\npie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\nInstruction: Someone told the creator of this item that they are very talented. What is the implied meaning?\nHigh quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza\nis overcooked and almost burnt, which is not a sign of talent.\nResponse A: When someone says that they are \"very talented\" in the context of the image, it implies that the person\nresponsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person\npraising the pizza\u2019s creator recognizes their culinary skills, which can be seen through the combination of ingredients\nand the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result\nthey achieved.\nResponse B: It is difficult to determine the implied meaning without further context or information about the image. It\nappears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion,\nand mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a\ndish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.\nThink step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\nassistant (machine generated; used to derive the prediction)\nStep 1: Comparing both responses with the high-quality reference, it\u2019s clear that Response A describes the pizza as\nwell-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it\nwith the reference.\nStep 2: Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts\nthis by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of\nthe pizza.\nStep 3: In terms of fluency, both responses are easily understood and written in coherent language.\nStep 4: Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a\npositive meaning when it should be sarcastic. Response B doesn\u2019t fully capture the sarcasm but is closer to the truth\nabout the pizza being badly cooked.\nStep 5: Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator\u2019s talent, while\nResponse B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza.\nOverall, Response B is better.\nFigure 17: The prompt we use for reference-backed evaluation, along with a sample in-\nstance/completion from GPT-4 [7].\n27\nsystem prompt (human authored)\nYou are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out\nfrom longer explanations. You will be given several sentences describing a thought process which should eventually\ncontain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\nSome things to remember:\n- Keep your answers short: only output \"Final Answer: X\" where X is A or B\n- If the explanation doesn\u2019t mention a final choice explicitly, you can output \"Unknown\"\n. . .\n(+ 2 in-context examples (human authored))\n. . .\nExample One: GPT-4 gives a definitive answer, but in the wrong format\nuser (GPT-4\u2019s raw pairwise judgement)\nStep 1: Compare the two responses to the instruction\n- The instruction asks if people are required to stop and requests an explanation.\nStep 2: Assess Response A\n- Response A indicates that people should stop due to the presence of a stop sign. However, it doesn\u2019t consider the fact\nthat the sign is pressed against the bus, and the LED lights are off.\nStep 3: Assess Response B\n- Response B simply says \"yes,\" acknowledging that people should stop, but it doesn\u2019t provide the explanation required\nby the instruction.\nStep 4: Compare specificity, fluency, and other factors\n- Response A is more specific and fluent but misconstrues the image description.\n- Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\nOverall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that\nResponse B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also\nexplain how the situation indicates that people should stop.\nPlease extract the final answer from the above text.\nassistant (machine generated; used to derive the prediction)\nFinal Answer: Response B is slightly better, but it should also explain how the situation indicates that people should\nstop.\nExample Two: GPT-4 refuses to answer.\nuser (GPT-4\u2019s raw pairwise judgement)\nStep 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire\nspecifically highlighted in a darker green.\nStep 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since\nthe image context indicates New Hampshire being highlighted.\nStep 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This\nresponse is also not accurate, as the image context states New Hampshire is the highlighted colony.\nStep 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\nStep 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\nAfter considering these factors, none of the responses are accurate given the information provided in the image context.\nHowever, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are\nincorrect and neither stands out as being better than the other.\nPlease extract the final answer from the above text.\nassistant (machine generated; used to derive the prediction)\nFinal Answer: Unknown.\nFigure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses,\nalong with two example outputs. In one case, GPT-4\u2019s pairwise judgement is given, but in a\nformat different than requested. In the other case, GPT-4 \u201cdefies\" the prompt, and refuses to\nissue a judgement because both options are bad.\n28\n"
  }
]