[
  {
    "title": "StemGen: A music generation model that listens",
    "link": "https://arxiv.org/pdf/2312.08723.pdf",
    "upvote": "43",
    "text": "STEMGEN: A MUSIC GENERATION MODEL THAT LISTENS\nJulian D. Parker\nJanne Spijkervet\u2217\nKaterina Kosta\u2217\nFurkan Yesiler\nBoris Kuznetsov\nJu-Chiang Wang\nMatt Avent\nJitong Chen\nDuc Le\nSAMI, ByteDance Inc.\nABSTRACT\nEnd-to-end generation of musical audio using deep learning tech-\nniques has seen an explosion of activity recently. However, most\nmodels concentrate on generating fully mixed music in response to\nabstract conditioning information. In this work, we present an al-\nternative paradigm for producing music generation models that can\nlisten and respond to musical context. We describe how such a model\ncan be constructed using a non-autoregressive, transformer-based\nmodel architecture and present a number of novel architectural and\nsampling improvements. We train the described architecture on both\nan open-source and a proprietary dataset. We evaluate the produced\nmodels using standard quality metrics and a new approach based on\nmusic information retrieval descriptors. The resulting model reaches\nthe audio quality of state-of-the-art text-conditioned models, as well\nas exhibiting strong musical coherence with its context.\nIndex Terms\u2014 Music Generation, Deep Learning, LLMs, Gen-\nerative Models\n1. INTRODUCTION\nEnd-to-end generation of musical audio using deep learning tech-\nniques is a fairly new discipline, the formative work having arguably\nstarted with WaveNet [1]. Recent work has shown a large jump\nin the quality and diversity of generated music by borrowing tech-\nniques from the image and language processing fields. Some ap-\nproaches operate on tokenized audio representations [2, 3] using\ntechniques from the large language model (LLM) literature [4, 5, 6].\nOther approaches use score-matching techniques to generate audio\ndirectly [7, 8], or encoded as a continuous latent representation [9].\nMusic can generally be thought of as the sum of a number of\nindependent but strongly related individual parts, conceived at dif-\nferent levels of elaboration and textural layout which are combined\ntogether to produce a full piece of music [10, p. 194]. A part may\ncorrespond to a musician playing a particular instrument, or to some-\nthing more abstract like the output of a sampler or synthesizer. These\nparts are often colloquially referred to as stems. Music production\ncan be thought of as an iterative procedure that seeks to add and re-\nfine these stems to fit the aesthetic preferences of the producer or\nmusician. In order for the resulting mixed music to sound coherent,\neach of these stems must be constructed to be sympathetic with the\ncontext of the existing composition, both musically and texturally. A\ngenerative model that performs this task would be a desirable tool for\nthose making music, and one which could support existing creative\nworkflows instead of supplanting them.\nMost existing generative models for musical audio are condi-\ntioned on abstract information, varying from text descriptions [4, 5,\n7] to style categories [11], and cannot be used easily for this iterative\ncomposition approach. Models that can listen to the context audio\ndirectly and generate a musically appropriate response are rarer but\n*Equal contribution\n Song Stems\nContext\nMix\nTarget\nStem\nModel\nRandom\nsubmix\nRandom\nstem\nFig. 1: Schematic diagram of the StemGen training paradigm.\nhave been proposed in the context of generating musical accompa-\nniments for singing [12] and in the context of generating multi-track\ninstrumental music [13]. However, these models place fixed seman-\ntic restrictions on both the context audio and the generated response.\nIn this work we propose a novel training paradigm for produc-\ning end-to-end generative models that are able to listen to a musical\ncontext and generate an appropriate response. We propose a net-\nwork architecture for implementing such a model, based on a non-\nautoregressive language model with a transformer backbone. This\narchitecture is extended from previous literature [6] with a novel ap-\nproach to combining multiple streams of audio tokens. We introduce\ntwo novel improvements to generation: multi-source classifier-free-\nguidance, and causal-bias during iterative decoding. We train the\narchitecture on two datasets of stem-based musical audio and evalu-\nate the resulting models using standard objective metrics, a novel ap-\nproach using an amalgamation of music information retrieval (MIR)\nmetrics, and listening tests.\n2. MODELLING APPROACH\nGiven a dataset that contains pieces of music separated into their\nstems, we can construct pairs of data consisting of a context-mix,\nwhich represents the musical context, and a target-stem, which rep-\nresents a response to that context. If the piece of music contains M\nstems, the context-mix can be constructed as a mixture of N < M\nstems selected at random from the full set. The target-stem can then\nbe taken to be a single stem, selected at random from those not in-\ncluded in the context-mix. This procedure means that a single song\nwith N stems can result in N\n2 (2N \u2212 2) training pairs. This acts as a\nintrinsic form of data augmentation, and likely somewhat mitigates\nthe scarcity of high quality music data in stem separated format.\nThe training problem can be formulated in one of two ways, ei-\nther as learning the joint distribution p(t, a) of the context-mixes,\na, and target-stems, t, or as learning the conditional distribution of\ntarget-stems given the context-mixes, p(t|a). We refer to modelling\np(t|a) as conditional generation and modelling p(t, a) as joint gen-\neration. In this work, we address the conditional generation prob-\nlem. Fig .1 shows an overview of this paradigm.\narXiv:2312.08723v2  [cs.SD]  16 Jan 2024\nConditioning\nContext-Mix\nTarget-Stem\nTokenizer\nTokenizer\nMasking\n Token\nCombiner\nTransformer\nLogits\nCont.\nEmbeddings\nFig. 2: Schematic showing overall architecture of the StemGen model during training.\n2.1. Model Architecture\nAs with many recent models [4, 5, 6], we reformulate the problem as\nmodelling sequences of abstract tokens rather than modelling sam-\npled waveforms. This allows the application of powerful techniques\nfrom the domain of language modelling. In order to produce such a\nsequence of tokens from audio waveforms, we use a separate audio\nencoding model [2]. This model employs a residual vector quan-\ntizer (RVQ), which means that each temporal frame of the signal is\ndescribed by a set of multiple hierarchical tokens. There exist dif-\nferent strategies for modelling this token sequence. MusicLM [4]\nseparates the tokens into coarse and fine sets, flattens each set into\na single stream with one token per-step, and then models them au-\ntoregressively. MusicGen [5] also takes an autoregressive approach,\nbut presents a number of methods for combining all the tokens for a\ntime-step into a single sequence element. VampNet [6], which is de-\nrived from the speech model SoundStorm [14], also combines some\ntokens into a single sequence element (albeit split into coarse and\nfine groups in a manner similar to MusicLM), but then models the\nresulting two sequences in a non-autoregressive manner, similar to\na masked language model. Our approach is similar to MusicGen in\nthat it employs a single model instead of separate coarse/fine mod-\nels, and combines all token levels into a single sequence element.\nHowever we employ a non-autoregressive approach to training and\nsampling, which is more similar to VampNet. The overall structure\nof the architecture during training is shown in Fig. 2\nIn contrast to VampNet & MusicGen, we must combine mul-\ntiple channels of audio (context-mix and target-stem) into a single\nsequence element. We achieve this by producing two sets of em-\nbeddings, one for each audio channel. Each embedding is produced\nby summing the embeddings for each hierarchical token level for\nthat audio channel. These two embeddings are then concatenated\nalong the embedding dimension, and used as a single sequence el-\nement. Before concatenation, embeddings are also calculated for\nany relevant non-audio conditioning. This could consist of encoded\ntext embeddings, embeddings from a model such as CLAP [15], or\nembeddings produced from available categorical metadata such as\ninstrument type or musical genre. This conditional embedding is\nthen summed into the embeddings for each audio channel, before\nconcatenation. Fig. 3 illustrates this token combination process.\nTraining is conducted using the masking procedure of Sound-\nStorm [14], but with one modification \u2013 we ensure that when train-\ning a particular hierarchical token level, all lower hierarchical lev-\nels (corresponding to finer residuals in the RVQ) are completely\nmasked. Fig. 4 illustrates this. This masking procedure is only ap-\nplied to the tokens for the target-stem. Given that we want to train\na conditional-generation model, context-mix tokens are left com-\npletely unmasked. If we wanted to train a joint-generation model,\nthe context-mix tokens could be included in the masking procedure.\n2.1.1. Causally biased iterative decoding\nTo sample new outputs using a trained masked-LM, an iterative pro-\ncess is used. For a particular token level, we start with the entire\nsequence masked. Candidate samples are then generated for the en-\nContext Mix Tokens\nTarget Stem Tokens\n Token Combiner\nConditioning\nTokens\nShared\ncodebooks\nFig. 3: Schematic showing how the Q RVQ levels of both the\ncontext-mix and the target stem are converted into continuous em-\nbeddings using codebooks E1...Q, and then combined with each\nother along with conditioning information.\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\nFig. 4: Example masking pattern when training to generate the 2nd\nlevel of a Q-level audio tokenizer. q1...Q denote the RVQ levels, and\ns0...n the sequence elements (equivalent to time steps in this case).\ntire sequence by sampling from the logits predicted by the backbone\nof the masked-LM for the current state of the sequence. These candi-\ndate samples are then ranked according to some criteria (see below),\nand the top-k are retained. The process is then repeated using the\nupdated sequence, until all tokens are unmasked. It is then repeated\nfor the next hierarchical token level. The criteria used to rank the\ncandidate samples at each step has a strong effect on the quality of\ngenerated output. SoundStorm ranks samples by the confidence of\nthe model, as given by the value of the logit for the sampled token.\nVampNet extends this by adding Gumbel noise to the confidence\nrankings with a defined weighting. We found that these sampling\nprocedures produced poor output in many cases. Biasing the gen-\neration towards confidence leads to monotonous and uninteresting\noutput, whereas relying too heavily on random selection leads to\npoor transients and unnatural amplitude fluttering in the output. We\npropose an extension to this sampling approach that encourages ear-\nlier sequence elements to be sampled first, enforcing a kind of fuzzy\n1\nws = 0.0\nws = 1.0\n7 6 5 4 3 2\n8\nIteration\nFig. 5: Example of iterative decoding for sequence of a single token\nlevel over 8 iterations, with and without causal-bias. Black denotes\nwhich sequence elements have been sampled.\ncausality. The resulting ranking function is given by:\n\u03c1(xn) = wc.c(xn) + ws.(1 \u2212 n/N) + wrX\n(1)\nwhere xn is the candidate sample at sequence index n, N is the total\nsequence length, c(xn) is the model\u2019s confidence in the candidate\nsample as calculated by applying softmax to the logits, X \u223c U(0, 1)\nis a uniformly distributed random variable, and wp, ws, wr are scalar\nweights. In Fig. 5 we show the evolution of a single sequence during\niterative decoding, with and without causal-bias.\n2.1.2. Multi-source classifier-free-guidance\nWe employ classifier-free-guidance [16] during the sampling of can-\ndidate tokens. In addition to applying this process to the non-audio\nconditioning, we also apply it to the audio context conditioning,\nwith the hope of enforcing stronger alignment to the audio context.\nThe classifier-free-guidance can be applied over both conditioning\nsources simultaneously using the standard formulation:\nlog(p(t)p(c|t)\u03bb) \u2248 \u03bb log(p(t|c)) + (1 \u2212 \u03bb) log(p(t))\n(2)\nwhere c is a combined conditioning source containing both the\ncontext-mix and any other conditioning and \u03bb is the guidance scale.\nWe also introduce a technique for weighting the guidance for the\nmultiple conditioning sources independently:\nlog(p(t)\nN\nY\ni=1\np(ci|t)\u03bbi) \u2248\nN\nX\ni=1\n\u03bbi log(p(t|ci)\n+ (1 \u2212\nN\nX\ni=1\n\u03bbi) log(p(t))\n(3)\nwhere the ci are independent conditioning sources, and \u03bbi are the\nguidance scales for each conditioning source. The derivation follows\ntrivially from that of single-source classifier-free guidance, via the\napplication of Bayes\u2019 rule. Note that it is important to train the model\nwith independent dropout for each conditioning source.\n3. EXPERIMENTS\nAll models are trained on a single NVIDIA A100 GPU with 80GB\nof VRAM. The batch size is 40. Training proceeds until validation\naccuracy has clearly stopped increasing for 20k steps, which was\ngenerally after around 150k total training steps. This translates to\naround 4 days of training time. The AdamW optimizer is used, with\na tri-stage learning rate schedule consisting of 10k warmup steps,\n50k steps at the maximum learning rate of 0.0005, and then a slow\nexponential decay over 300k steps.\nAs a baseline, we train on the Slakh [17] dataset, which con-\nsists of 145 hours of synthetic musical audio separated into stems.\nIn addition, we train on an internal dataset of 500 hours of licensed\nhuman-played music separated into stems. Individual songs were\nchunked into 1 minute segments during data preparation.\nWhen\nconstructing training pairs, a random 20s crop of each segment is\nchosen and the random submixing procedure described above in\nSec. 2 is applied.\nThe resulting context-mix and target-stem are\nchecked to make sure they are not silent, and discarded if so. Both\ndatasets contain metadata specifying the instrument type of each\nstem, with classes corresponding to the 18 General MIDI instru-\nment categories [18]. We use a trainable codebook with 18 entries\nto translate this conditioning information into an embedding. This\nmodel therefore has two conditioning sources: the context-mix and\nthe instrument category, referred to as ca and ci respectively.\nWe use the publicly available checkpoint of Encodec [2] also\nused for MusicGen [5]. This codec operates on 32kHz audio, and\ndatasets are resampled appropriately. The encoded audio has a frame\nrate of 50Hz, with four tokens in the range 0 . . . 2047 per frame.\nAs with any LM-like structure, the above described model ar-\nchitecture is broadly agnostic to the network type used for the main\ndecoder block. In practice, most models of this type employ a trans-\nformer. We follow this convention and employ a LLaMa-type trans-\nformer [19] for the experiments presented here. The transformer has\nan embedding dimension of 1024 and the number of attention heads\nis 16. 20 layers are used, giving a total parameter count of \u223c 250M.\n3.1. Evaluation Metrics\nWe analyze the audio samples generated by our system in two\nobjective evaluation strands. The first is Fr\u00b4echet Audio Distance\n(FAD) [20], which we calculate on VGGish embeddings as per\nthe original paper (referred to in results as FAD). The second is\nthe Music Information Retrieval Descriptor Distance (MIRDD).\nSimilarly to FAD, MIRDD is calculated on two populations. In-\nstead of comparing distributions in an abstract embedding space,\nMIRDD compares the distributions with respect to a number of MIR\n(pitch-,rhythm-, and structural-based) descriptors.\nMost of the descriptors are inspired by the research for melody\nexpectation and prediction. The use of such musical quantities has\nbeen shown to enhance melodic expectation when embodied in a\ncognitively plausible system for music prediction and demonstrated\nas a successful system for evaluating objectively music generation\nmodels that fit subjective input (for more details see [21, 22]). De-\nscriptors have been developed for evaluating monophonic genera-\ntions from composition models in MIDI format [23]. We expand the\nconcept by introducing a set designed to evaluate generated audio in\nboth monophonic and polyphonic manner.\nWe compute the descriptors for every audio in the reference and\ntest populations, utilising the outcome of the models for audio tran-\nscription [24], beat and downbeat detection [25], key and chords es-\ntimation [26], as well as structure extraction [27]. This gives us a set\nof vectors of real values (for continuous quantities) or integer counts\n(for discrete quantities). MIRDD is computed, extracting the KL-\nDivergence of the probability distribution pairs per descriptor and\naveraging the values. A lower MIRDD score is considered better.\nThe list of descriptors used is the following: key signature, length of\nnote pitch range, amount of unique pitch classes, maximum vertical\ndensity of percussive notes (computed in a 0.6s window size), num-\nber of beats per bar, chord labels variety, chord tonality alteration\nratio (moving from minor to major and vice versa), note pitches,\nnote pitch classes, chord triad labels and structure labels.\n3.2. Results\nFor evaluation, we generated sets of 400 example outputs from each\nmodel. The examples were produced following a similar procedure\nto that used for the construction of training examples, but pulling\nfrom a separate set of test data not contained within the training set.\nFor each example we generate a context-mix, and randomly choose a\ntarget-stem category. The generated stem produced by the model for\nthis set of conditioning is placed into one population. The real stem\nfor the matching target category is placed into another population.\nThese two populations can then be compared with a variety of met-\nrics. This procedure ensures there is no systematic error introduced\nby an imbalance in target categories between the two populations.\nWe do not perform pairwise evaluation, but instead compare the dis-\ntributions of the two populations. FAD-type metrics are calculated\nbetween the two populations of isolated stems, whereas MIRDD is\ncalculated on modified populations consisting of stems summed with\ntheir original context-mix. This allows the MIRDD metric to better\npenalize poor musical alignment and coherence.\n3.2.1. Sampling hyper-parameters & ablations\nIn order to validate our novel improvements to the decoding pro-\ncedure, we perform two ablations. For these ablations we use the\nmodel trained on the Slakh dataset. Firstly, we test the impact of\nmulti-source classifier-free-guidance, as described in Sec. 2.1.2, by\ncalculating metrics over a variety of guidance scales, \u03bba and \u03bbi, with\nrespect to our two conditioning sources ca and ci. The results can\nbe seen in Tab. 1. A value of 1.0 for guidance scale is equivalent to\nno guidance with respect to that conditioning source. The exact best\nvalues for guidance scale are very dependent on the conditioning in-\nformation, so for cases where the guidance scale is greater than 1.0\nwe show results averaged over 1000 examples at different values of\n\u03bbi, \u03bba up to a maximum of 4.0. The results confirm that adding guid-\nance over multiple sources is beneficial. We settle on \u03bba = \u03bbi = 3.0\nas a general setting for evaluation.\nFAD\nMIRDD\n\u03bbi = \u03bba = 1.0\n4.30\n0.41\n\u03bbi > 1.0, \u03bba = 1.0\n3.66\n0.26\n\u03bbi = 1.0, \u03bba > 1.0\n3.20\n0.29\n\u03bbi > 1.0, \u03bba > 1.0\n3.17\n0.25\nTable 1: Averaged objective metrics with different ranges of guid-\nance scales.\nSecondly, we test the impact of causal bias during iterative de-\ncoding by comparing various relative strengths of causal-bias. The\nother ranking weights, wc and wr are set to 0.1 and 1.0 respectively.\nA casual-bias weight ws of 0 gives a decoding scheme similar to\nthat used in VampNet [6]. The results can be seen in Tab. 2. We can\nsee that adding a small amount of causal-bias has a positive effect\non FAD and MIRDD metrics, indicating an increase in sound qual-\nity and musical alignment. This lessens as ws increases further. We\ntherefore settle on ws = 0.1 for further experiments.\nws\n0.0\n0.1\n0.2\n0.5\nFAD\n3.18\n3.12\n3.13\n3.17\nMIRDD\n0.32\n0.14\n0.18\n0.20\nTable 2: Objective evaluation metrics for various values of causal-\nbias weight ws.\nDuring iterative decoding, we use 128, 64, 32 and 32 steps re-\nspectively for the four hierarchical levels of the tokenizer. This is\nuniversal across all results presented here.\n3.2.2. Final evaluation\nWe evaluate both the model trained on Slakh, and that trained on\nour internal dataset. Both models are sampled from using a best set\nof sampling parameters derived above in Sec. 3.2.1. Additionally\nwe sampled from each model using a set of naive sampling param-\neters, which are equivalent to removing classifier-free-guidance and\ncausal-bias in decoding. We show objective metrics for these sets\nof outputs in Tab. 3. Whilst a direct comparison is not possible due\nto the different task, the FAD scores for the models trained on both\nSlakh and the internal dataset are consistent with those seen for state-\nof-the-art text-conditioned models [5]. It is also clear that the larger\nsize and human-content of the internal dataset leads to an apprecia-\nble improvement in output quality. Examples from these models can\nbe heard at the website associated with this work *\nFAD\nMIRDD\nSlakh (naive)\n4.3\n0.43\nSlakh (best)\n3.12\n0.14\nInternal (naive)\n3.39\n0.40\nInternal (best)\n1.96\n0.36\nTable 3: Objective evaluation metrics for both models with best and\nnaive sampling parameters.\nWe also conducted a listening test in the Mean Opinion Score\n(MOS) format, by asking 10 participants with music training to ver-\nify the subjective quality of the produced model. We constructed\nthree sets of outputs by mixing generated or real stems with their\ncorresponding context-mix. The generated stems were taken from\nthe naive and best sets of output from the model trained on the in-\nternal dataset as evaluated in Tab. 3. The real stems were taken from\nthe references sets used for previous evaluations. We collated a set\nof 60 mixes using this technique (equally split between naive, best\nand real) and asked listeners to rate the overall quality on a Likert\nscaled from very bad (1) to very good (5). The results are shown\nin Table 4, and confirm that the proposed model with best sampling\nparameters is capable of creating plausible musical outcomes.\nReal\nBest\nNaive\nMOS\n3.64\n3.46\n2.89\nTable 4: MOS test results for real stems, and generated stems for the\nmodel trained on the internal dataset with naive and best sampling\nparameters.\n4. CONCLUSIONS\nIn this work we defined a framework for training contextual mu-\nsic generation models using datasets of music split into stems. We\npresented a non-autoregressive language model-like architecture for\nimplementing this, along with novel changes to support multiple au-\ndio channels, and novel sampling methods. We trained this architec-\nture on two datasets, and presented ablations that verify the impact\nof our novel sampling changes. We evaluate the final models using\nobjective metrics, a novel MIR-based metric, and with a subjective\nlistening test. This evaluation shows that the audio quality of the re-\nsulting model is competitive with state-of-the-art music generation\nmodels, and that the musical alignment with context is good.\n*https://julian-parker.github.io/stemgen/\n5. REFERENCES\n[1] A. vd. Oord et al., \u201cWaveNet: A Generative Model for Raw\nAudio,\u201d arXiv, 2016, 1609.03499.\n[2] A. D\u00b4efossez, J. Copet, G. Synnaeve, and Y. Adi, \u201cHigh Fidelity\nNeural Audio Compression,\u201d arXiv, 2022, 2210.13438.\n[3] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Ku-\nmar, \u201cHigh-Fidelity Audio Compression with Improved RVQ-\nGAN,\u201d arXiv, 2023, 2306.06546.\n[4] A. Agostinelli et al.,\n\u201cMusicLM: Generating Music From\nText,\u201d arXiv, 2023, 2301.11325.\n[5] J. Copet et al., \u201cSimple and Controllable Music Generation,\u201d\narXiv, 2023, 2306.05284.\n[6] H. F. Garcia, P. Seetharaman, R. Kumar, and B. Pardo, \u201cVamp-\nNet: Music Generation via Masked Acoustic Token Model-\ning,\u201d arXiv, 2023, 2307.04686.\n[7] Q. Huang et al., \u201cNoise2Music: Text-conditioned Music Gen-\neration with Diffusion Models,\u201d arXiv, 2023, 2302.03917.\n[8] C. Hawthorne, I. Simon, A. Roberts, N. Zeghidour, J. Gardner,\nE. Manilow, and J. Engel, \u201cMulti-instrument Music Synthesis\nwith Spectrogram Diffusion,\u201d arXiv, 2022, 2206.05408.\n[9] F. Schneider, Z. Jin, and B. Sch\u00a8olkopf,\n\u201cMo\u02c6usai:\nText-\nto-Music Generation with Long-Context Latent Diffusion,\u201d\narXiv, 2023, 2301.11757.\n[10] Nicholas Cook, Music, Imagination, and Culture, ACLS Hu-\nmanities E-Book. Clarendon Press, 1990.\n[11] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, \u201cJukebox: A Generative Model for Music,\u201d arXiv,\n2020, 2005.00341.\n[12] C. Donahue et al., \u201cSingSong: Generating musical accompa-\nniments from singing,\u201d arXiv, 2023, 2301.12662.\n[13] G. Mariani, I. Tallini, E. Postolache, M. Mancusi, L. Cosmo,\nand E. Rodol`a, \u201cMulti-Source Diffusion Models for Simul-\ntaneous Music Generation and Separation,\u201d\narXiv, 2023,\n2302.02257.\n[14] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour,\nand M. Tagliasacchi, \u201cSoundStorm: Efficient Parallel Audio\nGeneration,\u201d arXiv, 2023, 2305.09636.\n[15] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, \u201cCLAP:\nLearning Audio Concepts From Natural Language Supervi-\nsion,\u201d arXiv, 2022, 2206.04769.\n[16] J. Ho and T. Salimans, \u201cClassifier-Free Diffusion Guidance,\u201d\narXiv, 2022, 2207.12598.\n[17] E. Manilow, G. Wichern, P. Seetharaman, and J. Le Roux,\n\u201cCutting music source separation some Slakh: A dataset to\nstudy the impact of training data quality and quantity,\u201d in Proc.\nIEEE Workshop on Applications of Signal Processing to Audio\nand Acoustics (WASPAA). IEEE, 2019.\n[18] MIDI\nManufacturers\nAssociation,\n\u201cComplete\nMIDI\n1.0 Detailed Specification,\u201d\nhttp://www.midi.org/\ntechspecs/gm.php, 1999/2008.\n[19] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Ro-\ndriguez, A. Joulin, E. Grave, and G. Lample, \u201cLLaMA: Open\nand Efficient Foundation Language Models,\u201d\narXiv, 2023,\n2302.13971.\n[20] K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi, \u201cFr\u00b4echet\nAudio Distance: A Reference-Free Metric for Evaluating Mu-\nsic Enhancement Algorithms,\u201d\nin Proc. Interspeech 2019,\n2019, pp. 2350\u20132354.\n[21] M. T. Pearce, The construction and evaluation of statistical\nmodels of melodic structure in music perception and composi-\ntion, Ph.D. thesis, City University London, 2005.\n[22] M. Pearce and G. Wiggins, \u201cExpectation in melody: The influ-\nence of context and learning,\u201d Music Perception, vol. 23, pp.\n377\u2013405, 06 2006.\n[23] L.-C. Yang and A. Lerch,\n\u201cOn the evaluation of generative\nmodels in music,\u201d Neural Computing and Applications, vol.\n32, 05 2020.\n[24] W.-T. Lu, J.-C. Wang, and Y.-N. Hung,\n\u201cMultitrack Music\nTranscription with a Time-Frequency Perceiver,\u201d in IEEE Int.\nConf. on Acoustics, Speech and Sig. Proc. (ICASSP), 2023, pp.\n1\u20135.\n[25] Y.-N. Hung, J.-C. Wang, X. Song, W.-T. Lu, and M. Won,\n\u201cModeling Beats and Downbeats with a Time-Frequency\nTransformer,\u201d\nin IEEE Int. Conf. on Acoustics, Speech and\nSig. Proc. (ICASSP), 2022, pp. 401\u2013405.\n[26] W.-T. Lu and J.-C. Wang and M. Won and K. Choi and X.\nSong, \u201cSpecTNT: a Time-Frequency Transformer for Music\nAudio,\u201d\nin International Society for Music Information Re-\ntrieval Conference, 2021.\n[27] J.-C. Wang, Y.-N. Hung, and J. B. L. Smith,\n\u201cTo Catch A\nChorus, Verse, Intro, or Anything Else: Analyzing a Song with\nStructural Functions,\u201d in IEEE Int. Conf. on Acoustics, Speech\nand Sig. Proc. (ICASSP), 2022, pp. 416\u2013420.\n"
  },
  {
    "title": "TinyGSM: achieving >80% on GSM8k with small language models",
    "link": "https://arxiv.org/pdf/2312.09241.pdf",
    "upvote": "33",
    "text": "TinyGSM: achieving > 80% on GSM8k with small language models\nBingbin Liu1, Sebastien Bubeck2, Ronen Eldan2, Janardhan Kulkarni2,\nYuanzhi Li2, Anh Nguyen2, Rachel Ward2, Yi Zhang2\n1Carnegie Mellon University\n2Microsoft Research\nAbstract\nSmall-scale models offer various computational advantages, and yet to which extent size is critical for problem-\nsolving abilities remains an open question. Specifically for solving grade school math, the smallest model size\nso far required to break the 80% barrier on the GSM8K benchmark remains to be 34B. Our work studies how\nhigh-quality datasets may be the key for small language models to acquire mathematical reasoning. We introduce\nTinyGSM, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully\nby GPT-3.5. After finetuning on TinyGSM, we find that a duo of a 1.3B generation model and a 1.3B verifier\nmodel can achieve 81.5% accuracy, outperforming existing models that are orders of magnitude larger. This\nalso rivals the performance of the GPT-3.5 \u201cteacher\u201d model (77.4%), from which our model\u2019s training data is\ngenerated. Our approach is simple and has two key components: 1) the high-quality dataset TinyGSM, 2) the use\nof a verifier, which selects the final outputs from multiple candidate generations.\n1\nIntroduction\nOne fascinating phenomenon regarding large language models (LLMs) is the emergence of capbilities as both the\nmodel and dataset sizes scale up (Wei et al., 2022a; Chan et al., 2022). Among many capabilities, mathematical\nreasoning is one particular aspect that has received tremendous attention Lewkowycz et al. (2022); Lightman et al.\n(2023). However, it is unclear to what extend scale is a necessity for mathematical reasoning, and the potential of\nsmall language models (SLMs) remains largely under-explored.\nIn this work, we push the boundaries of SLMs\u2019 math reasoning capabilities. As a first step towards general\nmathematics, our primary testing ground is grade school math problems, to solve which require both mathematical\nunderstanding and language comprehension. The gold-standard benchmark in this regime is GSM8K (Cobbe et al.,\n2021), a collection of 8.8K grade-school math word problems (with a 7k-1k train-test split) that involve 2 to 11\nreasoning steps. GSM8K has been widely regarded to be challenging for LLMs. Even though the questions appear\nrather simple for humans, there have been few models that achieve > 80%, and they are commonly of prohibitive\nsizes, e.g. 34B and above (see Table 1).\nOur goal is to break the 80% barrier on GSM8K while keeping the model size friendly. As previous work\nshows (Gunasekar et al., 2023; Li et al., 2023; Eldan & Li, 2023), training data quality is one of the most important\nfactors for enhancing performance of small models. In particular, prompt-engineered synthetic data generation from\ngigantic models such as GPT-3.5/4 enjoys the clear advantage of desirable data hygiene and controllable diversity.\nThis constituents a teacher-student scenario where the student learns from teacher\u2019s generations. On the tasks that\nthe model model already excels at, their guided generations remain one of the highest quality data one can collect\nfor training significantly smaller student models. It is also understood that the student model\u2019s performance likely\nends up inferior than the teacher, and may fall far short especially when the student is considerably smaller than\nthe teacher (Mirzadeh et al., 2019; Gudibande et al., 2023) \u2014after all, the teacher places an information-theoretic\nbottleneck on the student.\nTo our surprise, in the case of GSM8K, we are able to bridge the performance gap between the student and\nteacher, by utilizing a tiny amount of labeled real data (the original GSM8K training set of 7k questions) to train an\nindependent verifier model. At test time, the verifier score and select among multiple candidate answers generated\nfrom the student, and then we output the highest score generation as the final submission. Note the idea of using\n1\narXiv:2312.09241v1  [cs.LG]  14 Dec 2023\n108\n109\n1010\n1011\nModel size (# parameters)\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nPhi-GSM models\nPhi-GSM models+Verifier\nOther open-source models\nGPT-4 using Python\nFigure 1: Our results on GSM8K. Please refer to Table 1 for details.\na verifier is proposed by the seminal GSM8K paper (Cobbe et al., 2021), and here we demonstrate its power of\nbridging the teacher-student gap, and we conduct a more thorough examination of factors affecting its efficacy.\nThe contributions of our work are the following:\n\u2022 We introduce TinyGSM, a synthetic dataset containing GSM8K-style math word problems paired with Python\nsolutions, generated fully by GPT-3.5-turbo. TinyGSM consists of 12.3M questions which amount to 1.8B tokens.\nWe demonstrate TinyGSM\u2019s high-quality by finetuning the Phi-1.5 1.3B model (before the use of verifiers) which\nimproves its accuracy from 44.6% to 68.2% on the GSM8K test set. Notably, our smallest 125M model can also\nachieve 63.1% after finetuning on TinyGSM.\n\u2022 We demonstrate the power of verifiers on small-scale models. When integrated with a verifier for scoring\ngenerations, our models, named Phi-GSM models, achieve performance on par with other open source models\nthat are orders of magnitude larger. In particular, our 1.3B model achieves 81.5% accuracy on GSM8K, as shown\nin Figure 1. This marks a new state-of-the-arts on billion-parameter-scale models, significantly outperforming\nexisting open-source models and even rivaling the 77.4% accuracy of GPT-3.5, from which TinyGSM is generated.\nFor verifier training, we identify data diversity as a crucial element for a verifier\u2019s success, and find that the\nscaling of the verifier may be more effective than scaling of the generator: while scaling up from a 125M generator\nto a 1.3B generator only gives a 5.1% increase in performance (Table 1), scaling up the verifier from 125M to\n1.3B leads to a 7.2% performance boost (Figure 4).\n2\nRelated works\nDistilling from synthetic data: While scaling up has been a useful strategy, it is possible to outpace conventional\nscaling laws by better use of data (Sorscher et al., 2022). In the data-scarce case, quality synthetic data serves as an\neffective workaround (Eldan & Li, 2023; Gunasekar et al., 2023), and the scaling in dataset size can compensate for\na small model size (Edelman et al., 2023). Additionally, our work uses samples in the true distribution (i.e. the\nGSM8K train set) differently: given the small dataset size, we believe that the most sample-efficient way to utilize\nthe true train set is to train a verifier\u2014while the 7.4k samples in the GSM8K training set is too small for language\nmodel finetuning, it is sufficient for training a good quality verifier that provides 10% performance boost. While\nthere have been potential concerns of learning from synthetic data such as loosing diversity or having a drifted\ndistribution mean (Alemohammad et al., 2023; Shumailov et al., 2023), Alemohammad et al. (2023) showed that\nsuch degradation can be avoided by including fresh samples from the true distribution during training.\n2\nModel\nBase model\nModel size\nAnswer format\nEval method\nGSM8K (%)\nLlama-2 (Touvron et al., 2023)\n-\n7B\nnlp\npass@1\n14.6\n13B\n28.7\n34B\n42.2\n70B\n56.8\nMetaMath (Yu et al., 2023b)\nLlama-2\n7B\nnlp\npass@1\n66.5\n13B\n72.3\n70B\n82.3\nWizardMath (Luo et al., 2023)\nLlama-2\n7B\nnlp\npass@1\n54.9\n13B\n63.9\n70B\n81.6\nMAmmoTH (Yue et al., 2023)\nCode-Llama\n7B\ncode\npass@1\n59.4\nCode-Llama\n12B\n64.7\nCode-Llama\n34B\n72.7\nLlama-2\n70B\nnlp\n76.9\nMistral (Jiang et al., 2023)\n-\n7B\nnlp\nmaj1@8\n52.2\n8\u00d77B\n58.4\nOVM (Yu et al., 2023a)\nLlama-2\n7B+7B\nnlp\nverify100@1\n73.7\nMistral\n7B+7B\n84.7\nLlemma (Azerbayev et al., 2023)\nLlama-2\n7B\nnlp\npass@1\n36.4\n34B\n51.5\nToRA-Code (Gou et al., 2023)\nLlama-2\n7B\ncode\nCOT@1\n72.6\n13B\n75.8\n34B\n80.7\n70B\n84.3\nOrca 2 (Mitra et al., 2023)\nLlama-2\n7B\nnlp\npass@1\n55.72\n13B\n65.73\nGemini Pro\n-\n-\nnlp\nmaj1@32\n86.5\nGemini Ultra (Gemini Team)\n94.4\nGPT-3.5-0613\n-\n-\ncode\npass@1\n77.4*\nGPT-4-0613 (OpenAI, 2023)\n97.0*\nPhi-1.5 (Li et al., 2023)\n-\n1.3B\ncode\npass@1\n44.6\nPhi-GSM\nPhi-1.5-tiny\n125M\ncode\npass@1\n63.1\nPhi-1.5-small\n350M\n65.9\nPhi-1.5\n1.3B\n68.2\nPhi-2\n2.7B\n74.3\nPhi-GSM+V\nPhi-1.5-tiny\n125M+125M\ncode\nverify48@1\n68.9\nPhi-1.5-small\n350M+350M\n71.3\nPhi-1.5\n1.3B+1.3B\n81.5\nTable 1: Results on GSM8K. * denotes results measured by ourselves. Accuracies above 80% are labeled in bold.\n\u20188\u00d77B\u2019 stands for mixture of 8 experts, and each expert is of 7B parameters. \u20187B+7B\u2019 means a combination of a 7B\ngeneration model plus a 7B verifier model. \u2018+V\u2019 denotes the use of verifier models.\n3\nMath word problem datasets GSM8K (Cobbe et al., 2021) has been the most common used math word\nproblem dataset for its quality and size. In comparison, earlier datasets such as MAWPS (Koncel-Kedziorski et al.,\n2016), ASDiv (Miao et al., 2020) and SVAMP (Patel et al., 2021) are either much smaller in size or of less difficulty.\nHowever, GSM8K questions are too clean to test for robustness. Motivated by the observation that language models\nare not robust to the presence of irrelevant context, Shi et al. (2023a) proposed GSM-IC (irrelevant context). Another\nproblem is the GSM8K dataset itself is still too small for training language models. (Ni et al., 2023b) addressed this\nwith self-sampled data. In a work concurrent to ours, Yu et al. (2023b) bootstraps an original dataset using various\naugmentation techniques, such as generating multiple answers for the solution, question rephrasing, and backward\nreasoning. The proposed MetaMath dataset consists of 40000 questions from GSM8K and MATH (Hendrycks et al.,\n2021). In comparison, TinyGSM is significantly larger, encompassing 12.3M questions (or equivalently 1.8B tokens).\nLeveraging multiple generations: An important component of our method is to leverage multiple generation.\nThis idea has been proven successful in many prior works. A notable example is \u201cself-consistency\u201d (Wang et al.,\n2022), which selects the most frequent response among candidates and integrates well with other methods such as\nprogressive-hint prompting (Zheng et al., 2023) and model selection (Zhao et al., 2023). However, self-consistency\nwas not particularly helpful in our setup as mentioned in Section 4.2. More related to and a direct inspiration of our\nwork is Cobbe et al. (2021), which uses a verifier to select the best response among 100 candidates, leading to an\n20% accuracy boost. Our work conducts a more thorough study on the design choices of the verifier, including data\ndiversity and the effect of verifier sizes. Another design choice orthogonal to ours is the supervision signals, such as\noutcome-based supervision versus process supervision (Lightman et al., 2023).\nLearning from partial or process supervision: In our experiments, we evaluate on the final accuracy\nonly but train on full programs. Prior work has studied the effect of process versus outcome based supervision.\nProcess-based supervision is shown to be particularly helpful for complex math problems (Lightman et al., 2023),\nthough for general problems one needs to consider a cost-efficacy tradeoff (Uesato et al., 2022). When process\nsupervision is not available, Ni et al. (2023b) proposed to learn from \u201cself-sampled\u201d solutions, which allows the\nmodel to learn from partially correct self-generated solutions selected based on the execution trace.\nSelf-improvement: Several works have explored the idea of \u201cself-improvement\u201d where a model evaluates\nand corrects its own generations, mostly relying on the self-debugging ability of GPT4. Examples include \u201cself-\nrefine\u201d (Madaan et al., 2023) and \u201cself-verify\u201d (Weng et al., 2022; Zhou et al., 2023), both of which ask the\nmodel to iteratively provide feedback or verifications on its own generations and refine if needed. However, such\nself-improvement abilities have not been discovered in small language models. This motivated our use of a separate\nverifier model, which is initialized from the generative model but needs to be fully finetuned for verification.\nPrompt-based methods: Prompt-based methods, which find prompts to improve the later conditional\ngenerations, are particularly effective for large models. Examples include in-context learning (Brown et al., 2020),\nwhere the model learns to perform novel tasks from few-shot examples provided in the prompt, as well as Chain-of-\nThought (Wei et al., 2022b), which shows that explicitly generating intermediate steps can significantly help with\nreasoning abilities. However, similar to self-improvements, prompting is targeted at large language models and do\nnot apply for SLMs.\n3\nThe TinyGSM dataset\nOur objective is to assess the capability of a small language model (SLM) on mathematical reasoning. Ideally,\nenhancing this mathematical reasoning ability should not compromise the model\u2019s competence in language compre-\nhension. This makes math word problems, which necessitate both mathematical and language understanding, a\nsuitable test ground. We focus on the GSM8K dataset (Cobbe et al., 2021), consisting of around 8k grade-school\nmath word problems. The math concepts in the dataset are elementary and within standard grade-school curricula,\nbut the challenges posed by the natural language problem statement introduce an additional layer of complexity to\nthe task.\nTinyGSM: augmenting GSM8K with synthetic generations\nDespite the high quality, the GSM8K training set\nonly contains 7473 problems, which is too small for training a reasonably sized language model (Ni et al., 2023a).\nTo alleviate the size issue, we augment the GSM8K training set using GPT-3.5-turbo generated synthetic problems.\nWe prompt GPT-3.5-turbo to generate problem variants similar to a given question (but not the solution) randomly\n4\ndef simple_math_problem () -> int:\n\"\"\"\nIn preparation for her party , Sarah buys 10 trays\nof food and 8 cases of beverages.\nEach tray costs $50 and each case of beverages\ncosts $20.\nWhat is the total cost of the trays and beverages?\n\"\"\"\ntrays = 10\ntray_cost = 50\ncases = 8\ncase_cost = 20\ntray_total = trays * tray_cost\ncase_total = cases * case_cost\ntotal_cost = tray_total + case_total\nresult = total_cost\nreturn result\ndef simple_math_problem () -> int:\n\"\"\"\nKim has 4 times the number of crayons\nthan 8 less than the number of markers\nshe has.\nIf she has 60 crayons , how many markers\ndoes she have?\n\"\"\"\nnumber_crayons = 60\nnumber_markers = number_crayons // 4 +\n8\nresult = number_markers\nreturn result\nFigure 2: Examples from TinyGSM. The question is given as the docstring of a function, and the solution is the\ncode in the function body.\nsampled from the GSM8K training set. Each problem variant contains both a question and the corresponding solution\nwritten in Python, as shown in Figure 2.1 Using code allows us to leverage a Python interpreter, circumventing\nlanguage models\u2019 known limitation regarding numerical calculations and code execution.\nTo enhance robustness, we also generated synthetic problems whose questions contain irrelevant information. This\nis achieved by augmenting the GSM-IC dataset (Shi et al., 2023a), which is an augmentation of GSM8K specifically\ndesigned to introduce irrelevant context (IC) to the question statement. These GSM-IC variants constitute to\napproximately one third of TinyGSM.\nThe resulting synthetic dataset contains 12.3M problems (i.e. question-solution pairs) 2 with, based on the\noriginal 7.4k training set questions and their IC variants. For each question in the GSM8K train set, the prompt\nbased on this question is shared across API calls, and the source of randomness comes entirely from the generation\nprocess. To encourage diversity, we use temperature sampling and specify in the prompt to encourage the problem\nvariants to be grammatically diverse and contain multiple steps; the exact prompts are provided in Figure 3 and\nin Appendix A.1.\nFiltering\nTo ensure the quality of the synthetic data in TinyGSM, we filter out problems that are too short or\ndo not contain numbers, as well as code solutions which are not executable. Note that we do not check for the\ncorrectness of the question or the generated solutions, since the \u201cground truth\u201d solution is not available. Given the\neffectiveness of self-consistency (Wang et al., 2022), one might want to filter the problems by keeping the ones which\nhave majority vote only. We did not adopt this strategy since we find that GPT-3.5-turbo\u2019s generations are only\nconsistent on easy problems 3, hence such consistency filtering will remove challenging problems, resulting in a\ndataset that is too easy to be useful.\n4\nSolving grade school math with small language models\nThe 1.3B version of our phi-GSM models is able to achieve 81.5% accuracy on GSM8K, a dataset that remains\nchallenging for small-scale models. The performance comes from sufficient good quality synthetic data and the use\n1Note that the generated problems may be mathematically valid yet violating common sense. For example, some quantities may not\nbe integers.\n2This corresponds to 1.8B tokens, which costs around $3600 to generate according to GPT commercial pricing.\n3\u201cEasy\u201d problems refer to the ones for which a 350M model, trained on a part of TinyGSM, already produces same final answer as\nGPT-3.5-turbo. For example, for an early version of our 350M model, the model only achieves around 50% on the GSM8K test set, but\ncan achieve more than 87% on synthetic questions with consistent answers. In other words, adding more easy problems like these will\nnot help our 350M model bridge the performance gap between itself and GPT-3.5-turbo.\n5\nConsider the following grade -school math problem: {{ question }}\nGenerate 10 different math problems similar to this math problem.\n- Make sure each question uses diverse NLP and includes multiple logical steps.\n- After each generated problem , write down a ** detailed and complete Python program ** to solve the question **step\nby step** (do NOT give the result directly , **DO NOT write calculations in the comments **).\n- The program should contain multiple lines of code and end with \u2019result = XXX \u2019 (Make sure to replace XXX with the\nactual result of the python program).\n- Make sure your Python program is complete and solves the problem. Do **NOT** write things like \u2019solution to be\ncompleted \u2019, result = ?, insert your code here etc.\n- Give the complete solution to solve the problem , written in Python. Do not write things like \u2019insert your code\nhere \u2019.\n- In each new question , **first end with <|endofquestion |>**, and then start writing the program. Each program\nshould end with <|endofprogram |>.\n- Example format: Question X: New question (at least 4 sentences long and use diverse NLP) (without the solution)\n<|endofquestion|> Complete python code with entire solutions and the correct indent (<| endofprogram |>])\nFigure 3: The prompt template for generating TinyGSM.\nof a verifier, which we describe in this section.\n4.1\nLearning from synthetic data\nWe finetune the Phi-1.5 125M, 350M and 1.3B models on our TinyGSM from Section 3, and in particular, the 1.3B\nmodel reaches 68.2% accuracy.4 5 We use the Adam optimizer with FP16 during training, with a linear warm-up\nand a maximum learning rate of 1e-4, a weight decay of 0.01, and an effective batch size of 1024. The finetuning\nphase takes up to 20k steps in total. As shown in Figure 1, even without verifiers, our models are already competitive\nto models of size from 7B and larger. As an anecdote, an earlier and worse performing version of our Phi-GSM 1.3B\nmodel gets 94% (or 82.5% from 350M at pass@32, whereas the 750M CodeT5+ model (Wang et al., 2023) gets\n73.8% (or 70.5% from 220M) at pass@100.\n4.2\nImproving small models with a verifier\nWhile sufficient synthetic data can significantly boost model performance, the performance is still below 70%. Does\nfurther improvement necessitate larger model and more data then? There may be two concerns: First, there may be\na diminishing return in adding extra parameters and data; for instance, while there is a 10% increase in performance\nwhen increasing from around one third of the final size of TinyGSM to two thirds, the final one third of the data\nprovided only marginal gain. Moreover, even if the small language model is able to fully match the quality of the\nsynthetic data, GPT-3.5-turbo itself can only achieves 77.4% test accuracy on GSM8K, which seemingly poses a\nlimit on the performance of any models distilling from its generations.\nIn this section, we show that the use of a verifier can be an effective strategy orthogonal to introducing more\nand better data, and can even help SLMs exceed the accuracy of GPT-3.5-turbo generations. The main observation\nthat the best of multiple generations significantly outperforms a single generation. These generations could be\nlow-temperature generations from different checkpoints of a single run, where taking the best out of generations\nfrom 5 checkpoints of (an early version of) our 350M model reaches 75% accuracy, similar to findings in temporal\nensembling (Laine & Aila, 2016) and snapshot ensembles (Huang et al., 2017). 6 The generations could also be\nfrom high-temperature generations based on a single checkpoint; for instance, the pass@32 accuracy of our 1.3B\nmodel is 94%.\nThis suggests a promising direction of leveraging multiple generations: we can obtain a great performance boost\nif we are able to identify the best generation. This idea is effective yet natural: The probabilistic nature of the\n4The Phi-1.5-small 350M and Phi-1.5-125M variants are pretrained on the same pretraining data as the Phi-1.5 1.3B model.\n5Performance of training on TinyGSM from scratch is reported in Table 2.\n6For utilizing multiple checkpoints, an option is to use model soup (Wortsman et al., 2022); however, a uniform soup did not improve\nthe accuracy. Another option is to perform EMA, which has been shown effective in Block et al. (2023). We found that EMA was not\nhelpful when applied to the 1k-step-interval checkpoints; more frequent averaging is likely required.\n6\ngenerative process naturally leads to the fact that multiple generations of a language model are more likely to\ncontain a correct solution than a single one. Empirically, it has been widely observed that pass@k accuracy, namely,\nthe accuracy by taking the best of k generations, is often much higher than pass@1. The main challenge is that\nwithout knowing the labels, the definition of \u201cbest\u201d is usually unclear. A workaround is to apply some form of\nself-selection, such as by choosing the one with the highest logit or the most consistent solution (Wang et al., 2022;\nLi et al., 2022). There is, however, a notable limitation: generations can be consistent and confident yet inaccurate,\nmaking the self-consistency approach through majority voting less effective (Li et al., 2022).\nGiven these observations and inspired by findings in (Cobbe et al., 2021), we propose to use a separate verifier for\nselecting candidate generations. For each base generation SLM, we train a verifier to predict whether a generation\nis a correct solution to the given question. During inference, we generate multiple candidate generations using\ntemperature sampling, and select the one with the highest verifier score.\nTraining data\nThe training data consists of the SLM\u2019s generations on the labele GSM8K training set questions,\npaired with the binary labels indicating whether a generation leads to the correct numerical answer. We sample 48\ngenerations for each training set question. The binary label for each generation is based on the final execution result\nand the ground truth label only, and we do not verify the correctness of intermediate steps. Note that this is the\nonly time where the GSM8K training set is directly utilized in training.\nVerfier model size\nBase generation model size\n125M\n350M\n1.3B\n125M\n68.9\n68.8\n71.7\n350M\n67.3\n71.3\n78.3\n1.3B\n76.1\n79.2\n81.5\n125M\n350M\n1.3B\n50\n60\n70\n80\n90\ntest acc (%)\n63.1\n65.9\n68.2\n68.9\n71.3\n81.5\ngeneration model only\nwith paired verifier\nFigure 4: Pass@1 results on GSM8K test set with verifiers. For each test question, we sample 48 candidate answers\nfrom the base generation model, from which we submit the one with highest verifier score as the final answer. The\nverifier\u2019s score on a candidate answer is determined using its score on the last token.\nTraining setup\nThe verifier is trained with a sequence-to-sequence task, where we use the binary label on the\nentire sequence to supervise each token. We find this approach improves consistently over training with a sequence\nclassification task (i.e. only predicting a binary label on the entire sequence). The verifier model is initialized to\nbe the same as the SLM, with an additional prediction head shared across all positions. All network parameters\nare updated during verifier training, which significantly outperforms alternatives where only the prediction head is\nupdated, or where the network is trained from scratch.\nCheckpoint and data diversity\nThe diversity of the training data proves to be important, even if this may\nresult in slightly sacrificing the data quality7: we find that including data generated using various temperatures\nand different checkpoints improves the performance of the verifier. In particular, the training data for the verifier\nis generated from checkpoints at 1k, 6k and 12k steps, and both training and testing data use a mixture of data\ngenerated with temperature 0.5 and 0.7. Compared to training with generations from a single checkpoint, the use of\nmultiple checkpoints improves the performance from 78.9% to 81.5%.\nNote also that in general, while we train one verifier for each generative model, verifiers transfer reasonably well\nacross models. For example, our 1.3B model was able to reach 78.1% with a 350M verifier trained on generations\nfrom a 350M model.\n7Worse quality data refers to generations from earlier checkpoints, or sampled with a higher temperature: we found that the accuracy\nfor temperature=1 can be more than 10% worse than temperature=0 (i.e. greedy decoding).\n7\nQuestion:\nSophia is thinking of taking a road trip in her car, and would like to know how far she can drive on a single\ntank of gas. She has traveled 100 miles since last filling her tank, and she needed to put in 4 gallons of gas to\nfill it up again. The owner\u2019s manual for her car says that her tank holds 12 gallons of gas. How many miles can\nSophia drive on a single tank of gas?\nGenerations:\n1) correct:\ndistance_traveled = 100\ngas_used = 4\ntank_size = 12\nmiles_per_gallon= distance_traveled /\ngas_used\nresult = miles_per_gallon * tank_size\n2) wrong:\ndistance_traveled = 100\ngas_put_in = 4\ngas_tank_size = 12\nresult = (distance_traveled\n+ gas_put_in) / (gas_tank_size -\ngas_put_in)\n3) correct:\nmiles_traveled = 100\ngallons_added = 4\ntotal_gallons = 12\nmiles_per_gallon = miles_traveled //\ngallons_added\ntotal_miles = (total_gallons -\ngallons_added) * miles_per_gallon +\nmiles_traveled\nresult = total_miles\nFigure 5: Visualization of the verifier\u2019s token-level predictions. Text colors denote the verifier\u2019s prediction scores:\ncorrect, potentially correct, potentially wrong, and wrong. In all the three examples, the verifier\u2019s final prediction\n(on the last token) aligns with the groundtruth labels. In generation 1) and 2) the verifier\u2019s token-level scores appear\nto be interpretable and aligned with human assessment. However, in generation 3), the scores are rather strange.\nThis suggests the verifier relies on special patterns of the model generations that may not be unversaly generalizable,\neven though its final predictions are fairly reliable.\nGeneration model size vs verifier size\nIn Figure 4, we present results from a cross-examination of various\ngeneration model sizes + verifier model sizes. Interestingly, while the best accuracy is achieved with configuration\nwith largest sizes, the verifier size seems to play a bigger role than the generation model size. The effect of model\nsize scaling is surprisingly mild: as shown in Table 1, increasing the base generation model from 125M (Phi-1.5-tiny)\nto 1.3B (Phi-1.5) only gives a 6% boost. On the other hand, the verifier seems to be much more parameter efficient.\nFor example, 125M generation model + 1.3B verifier can achieve 76.1%, while 1.3B generation model + 125M\nverifier gets only 71.7% Figure 4.\n5\nRobustness and decontamination\n5.1\nContamination test\nWhile we never use the GSM8K test set during training, TinyGSM consists entirely of synthetic data generated by\nGPT models, which may be contaminated since GPT-3.5-turbo may have been exposed to the test set during its\nown training, which would have led to some generated synthetic samples being replicating part of the test set. To\nprevent contamination, we decontaminate TinyGSM by checking for n-gram matches. We use n = 13 following\nstandard practices (Brown et al., 2020; Wei et al., 2021; Du et al., 2022), 8 and remove punctuation and numbers\nbefore computing the matching. Out of the 11.0M unique synthetic questions 9 , 22 questions have a nonzero\n13-gram match with the test set, and 38k questions (i.e. around 0.35% of the full set) have non-zero 8-gram matches.\nExamples of 13-gram matches are provided in Appendix A.2.\n8n-gram matching is not sufficient for guarding against some other types of contamination (e.g. with respect to paraphrasing).\nHowever, we are not aware of better checks.\nOne alternative is to check embedding similarity, though our clustering results on\nCodeGen 350M (Nijkamp et al., 2022) embeddings suggest that the embedding mostly reflects the semantic (topics) rather than\nstructural/functional similarity, making it unfit for checking similarity in math questions. To our knowledge, state-of-the-art papers on\ntraining set contamination only test for exact matching (Shi et al., 2023b; Oren et al., 2023), and checking for contamination beyond\nexact match remains an open problem.\n9The number of unique questions is smaller than the number of question-solution pairs since some questions were sampled more than\nonce in the second step of the 2-step generation (Appendix A.1) and hence have multiple solutions.\n8\n5.2\nEvaluation on SVAMP\nVerfier model size\nBase generation model size\n125M\n350M\n1.3B\n125M\n63.2\n70.0\n72.2\n350M\n64.6\n68.7\n72.3\n1.3B\n74.1\n79.0\n75.6\nFigure 6: SVAMP test accuracies.\nFor evaluating robustness of our models, we test on the SVAMP (Simple Variations on Arithmetic Math word\nProblems) dataset (Patel et al., 2021), consisting of 1000 math word problem questions with a focus on arithmetics.\nSVAMP constructed by applying certain types of variations to a set of base questions. Even though the base\nquestions are generally considered easier than GSM8K 10, the variations may often confuse LLMs, thus making it\na challenging benchmark for robustness. Our 1.3B model achieves 75.6% on SVAMP without further finetuning,\nindicating the robustness of the model.\n6\nDiscussions\nIn this work, we showed a simple approach that enabled a 1.3B generation model to achieve 81.5% on the GSM8K\ndataset, setting a new state-of-the-art for small language models and raising the performance curve for scaling. Our\napproach consists of two simple steps: 1) collecting TinyGSM, a GPT-3.5 generated synthetic dataset which we\nwill fully release, and 2) using a verifier that scores how likely a generation is correct, whose quality is boosted by\nutilizing diverse generations. Our results provide positive evidence that small language models have more potentials\nto be unlock and can be used for efficient. For future directions,\n\u2022 Leveraging different formats: TinyGSM uses Python code as solutions, inspired by the observation that language\nmodels tend to struggle at calculations. However, we found that different solution formats, i.e. code versus natural\nlanguage, can be complementary: while code helps circumvent errors related to execution or calculation, it tends\nto perform worse at questions that require equation solving, likely due to the fact that the Python syntax does not\nnaturally support equations. Properly combining both formats has the potential to further boost performance.\n\u2022 The effect of verifier size: Our results show that given a budget on the model size, scaling the verifier may\nbe a more efficient use of the parameters. This counters our intuition that verification is an easier task than\ngeneration (which involves search), though there might be connections to findings in GAN training where the size\nof discriminator (Arora et al., 2018). Exploring the parameter efficiency in a generation model versus a verifier is\nan interesting future direction.\nReferences\nSina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel\nLeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative models go mad. arXiv preprint\narXiv: 2307.01850, 2023.\nSanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics.\nIn International Conference on Learning Representations, 2018.\nURL https://openreview.net/forum?id=\nBJehNfW0-.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia\nDeng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint\narXiv: 2310.10631, 2023.\n10See Table 1 in Xie et al. (2023).\n9\nAdam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang. Butterfly effects of sgd\nnoise: Error amplification in behavior cloning and autoregression. arXiv preprint arXiv: 2310.11428, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\nLarochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nStephanie C. Y. Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X. Wang, Aaditya K Singh, Pierre H.\nRichemond, J. Mcclelland, and Felix Hill. Data distributional properties drive emergent in-context learning in\ntransformers. Neural Information Processing Systems, 2022. doi: 10.48550/arXiv.2205.05055.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\nmath word problems. arXiv preprint arXiv: Arxiv-2110.14168, 2021.\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas\nDixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022,\nBaltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 5547\u20135569. PMLR,\n2022. URL https://proceedings.mlr.press/v162/du22c.html.\nBenjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature\nlearning: Data, compute, width, and luck. arXiv preprint arXiv: 2309.03800, 2023.\nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?\narXiv preprint arXiv: Arxiv-2305.07759, 2023.\nGoogle Gemini Team. Gemini: A family of highly capable multimodal models.\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen.\nTora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv: 2309.17452,\n2023.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn\nSong. The false promise of imitating proprietary llms. arXiv preprint arXiv: 2305.15717, 2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin\nWang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all\nyou need. arXiv preprint arXiv: 2306.11644, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the math dataset. NeurIPS Datasets and Benchmarks, 2021.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, J. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles:\nTrain 1, get m for free. International Conference on Learning Representations, 2017.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed.\nMistral 7b. arXiv preprint arXiv: 2310.06825, 2023.\n10\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word\nproblem repository. In Proceedings of the 2016 conference of the north american chapter of the association for\ncomputational linguistics: human language technologies, pp. 1152\u20131157, 2016.\nS. Laine and Timo Aila. Temporal ensembling for semi-supervised learning. International Conference on Learning\nRepresentations, 2016.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose\nSlone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant\nMisra. Solving quantitative reasoning problems with language models, 2022.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making large language\nmodels better reasoners with step-aware verifier. arXiv preprint arXiv: 2206.02336, 2022.\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng\nChen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via\nreinforced evol-instruct. arXiv preprint arXiv: 2308.09583, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\nShrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean\nWelleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. arXiv preprint\narXiv: 2303.17651, 2023.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english\nmath word problem solvers.\nAnnual Meeting of the Association for Computational Linguistics, 2020.\ndoi:\n10.18653/v1/2020.acl-main.92.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh.\nImproved knowledge distillation via teacher assistant. arXiv preprint arXiv: 1902.03393, 2019.\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen,\nAnastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed\nKhanpour, and Ahmed Awadallah. Orca 2: Teaching small language models how to reason, 2023.\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and\nJianfeng Gao.\nLearning math reasoning from self-sampled correct and partially-correct solutions.\nIn The\nEleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?\nid=4D4TSJE6-K.\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and\nJianfeng Gao.\nLearning math reasoning from self-sampled correct and partially-correct solutions.\nIn The\nEleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?\nid=4D4TSJE6-K.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:\n2203.13474, 2022.\nOpenAI. Gpt-4 technical report, 2023.\nYonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. Proving test set\ncontamination in black box language models. arXiv preprint arXiv: 2310.17623, 2023.\n11\nArkil Patel, S. Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems?\nNorth American Chapter Of The Association For Computational Linguistics, 2021. doi: 10.18653/V1/2021.\nNAACL-MAIN.168.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, E. Chi, Nathanael Scharli, and Denny\nZhou. Large language models can be easily distracted by irrelevant context. International Conference on Machine\nLearning, 2023a. doi: 10.48550/arXiv.2302.00093.\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke\nZettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv: 2310.16789, 2023b.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of\nrecursion: Training on generated data makes models forget. arXiv preprint arXiv: 2305.17493, 2023.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws:\nbeating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536,\n2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie\nKambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv: 2307.09288, 2023.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell,\nGeoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback. arXiv\npreprint arXiv: 2211.14275, 2022.\nXuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, E. Chi, and Denny Zhou. Self-consistency improves chain\nof thought reasoning in language models. International Conference on Learning Representations, 2022. doi:\n10.48550/arXiv.2203.11171.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+: Open\ncode large language models for code understanding and generation. arXiv preprint arXiv: 2305.07922, 2023.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, A. Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V.\nLe. Finetuned language models are zero-shot learners. International Conference on Learning Representations,\n2021.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, E. Chi, Tatsunori Hashimoto, Oriol Vinyals, P. Liang, J. Dean, and W. Fedus.\nEmergent abilities of large language models. Trans. Mach. Learn. Res., 2022a. doi: 10.48550/arXiv.2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022b.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language models are better\nreasoners with self-verification. arXiv preprint arXiv: 2212.09561, 2022.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos,\nHongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging\nweights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri,\n12\nStefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (eds.), International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine\nLearning Research, pp. 23965\u201323998. PMLR, 2022. URL https://proceedings.mlr.press/v162/wortsman22a.\nhtml.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv: 2305.00633, 2023.\nFei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathematical reasoning.\narXiv preprint arXiv: 2311.09724, 2023a.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models.\narXiv preprint arXiv: 2309.12284, 2023b.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth:\nBuilding math generalist models through hybrid instruction tuning. arXiv preprint arXiv: 2309.05653, 2023.\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language\nmodels for reasoning. arXiv preprint arXiv: 2305.14333, 2023.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning\nin large language models. ARXIV.ORG, 2023. doi: 10.48550/arXiv.2304.09797.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\nZhan, and Hongsheng Li. Solving challenging math word problems using gpt-4 code interpreter with code-based\nself-verification. arXiv preprint arXiv: 2308.07921, 2023.\n13\nA\nAdditional details on TinyGSM\nA.1\nOther prompts\nThe majority of the TinyGSM was generated using the prompt in Figure 3, where GPT-3.5-turbo is asked to\ngenerate both the question and the corresponding solution. The remaining data, including all data based on GSM-IC,\nis generated using a two-step process, where the first step prompts the model to generate question variants, and\nthe second step asks to generate Python solutions given a question variant generated in the first step. The exact\nprompts are provided in Figure 7\u2013Figure 9.\nDesign 10 grade -school math problems similar to a given math problem.\n- Make sure the language is different and sufficiently rephrased.\n- Feel free to change the scenario , story and quantities.\n- Make sure the new problems are no easier than the given problem and require more steps to solve.\n- Make sure the new problems are self -complete , clear , unambiguous , and coherent.\n- Please do not include hints or solutions for the new problems.\n## The original math problem\n{{ question }}\n## New problems\n- Problem 1:\nFigure 7: The prompt template for generating question variants based on GSM8K.\nPlease write 10 questions similar in style to a given question , where the new questions also contain\nirrelevant information. Make sure to change the name and scenarios.\n# Original question\n{{ question }}\n# New questions\n- Question 1:\nFigure 8: The prompt template for generating question variants based on GSM8K-IC.\nA.2\nContamination check: 13-gram collisions\nThere are 22 questions (out of 11.0M) with 13-gram collisions to test set questions. Examples are shown in Figure 10.\nB\nPretrained vs Random Init\nIn this section, we present a comparison of training on TinyGSM from a random initialization versus from a pretrained\nmodel.\nModel size\n125M\n350M\n1.3B\nRandom Init\n53.1\n55.5\n57.3\npretrained\n63.3\n65.9\n68.2\nTable 2: Comparison of performance with and without pretraining.\n14\nPlease generate a detailed and complete Python program to solve a given math question.\n- Use variables to represent the quantities in the question.\n- Solve the question **step by step** (do NOT give the result directly , **DO NOT write calculations in the\ncomments **).\n- The program should contain multiple lines of code and end with \u2019result = XXX \u2019 (Make sure to replace XXX with\nthe actual result of the python program !!!).\n- Then , the result should be printed out in the format of f\u2019<<<{result}>>>\u2019.\n- Make sure your Python program is complete and solves the problem. Do **NOT** write things like \u2019solution to\nbe completed \u2019, result = ?, insert your code here etc.\n- Give the complete solution to solve the problem in Python. Do not write things like \u2019insert your code here \u2019.\n- You should solely rely on the Python program to solve the question. Do not do calculations in the comments.\nThe comment should not include any numbers.\n- Try to use fewer comments since they are expensive.\n- If you really want to solve equations like x = ..., try to use \u2018\u2018import sympy \u2018\u2018 and ** sympy.solve ()**. sympy\n.solve(expression) returns the roots of the expression. Do not write down calculations in the comments!\n- If you need to calculate the ceiling of a number , use \u2018import math \u2018 then \u2018math.ceil().\u2018\n- End the Python program with <|endofprogram|> in a new line.\n### Question\n{{ question }}\n### Program\nFigure 9: The prompt template for generating code solution for a given question.\n# Q: Daniel has\nbrothers His older brother is\nyears older than\ntimes Daniel \u2019s age when Daniel was\nyears\nold His younger brother is\nyears old which is\nthe age of the older brother What is their combined age\nmatch:\nIn a family there are\nbrothers and\nsisters All sisters are the same age which is\nOne of the brothers is\n** years old which is\nthe age of the older brother What is** the total age of all these siblings\n# Q: Two cars are driving on a highway The first car is traveling at an average speed of\nmiles per hour\nwhile the second car takes a minute break after driving for\nminutes how long can they remain stopped before the\nfirst car catches up with them\nmatch:\n**Two cars are driving on a highway The first car is traveling at an average speed of\nmiles per hour** when\nthe second car passes it at an average speed of\nmiles per hour\nIf both cars continue on the highway at the same\nspeed how many miles will separate them after\nhours\n# Q: Leo and Nora sold lemonade at a stand Leo sold\ncups at\ncents each and Nora sold\ncups at\ncents each\nThey decided to split the money equally How much money did each of them get\nmatch:\nWhile walking down the street with his\nyoung siblings Greg found\nTo be fair to his siblings he ** decided to\nsplit the money equally How much money did each of them get**\n# Q: A bookstore is selling a book for\nwhile is a\ndiscount from the original price What was the original\nprice of the book\nmatch:\nKyle bought last year \u2019s bestselling book for\nThis is with a ** discount from the original price What was the\noriginal price of the book**\nFigure 10: The prompt template for generating code solution for a given question.\n15\n"
  },
  {
    "title": "CogAgent: A Visual Language Model for GUI Agents",
    "link": "https://arxiv.org/pdf/2312.08914.pdf",
    "upvote": "28",
    "text": "CogAgent: A Visual Language Model for GUI Agents\nWenyi Hong1* Weihan Wang1* Qingsong Lv2 Jiazheng Xu1* Wenmeng Yu2\nJunhui Ji2 Yan Wang2 Zihan Wang1* Yuxuan Zhang2* Juanzi Li1\nBin Xu1 Yuxiao Dong1 Ming Ding2\u2020 Jie Tang1\u2020\n1Tsinghua University 2Zhipu AI\n{hwy22@mails, jietang@}.tsinghua.edu.cn, ming.ding@zhipuai.cn\nAbstract\nPeople are spending an enormous amount of time on dig-\nital devices through graphical user interfaces (GUIs), e.g.,\ncomputer or smartphone screens.\nLarge language mod-\nels (LLMs) such as ChatGPT can assist people in tasks\nlike writing emails, but struggle to understand and interact\nwith GUIs, thus limiting their potential to increase automa-\ntion levels. In this paper, we introduce CogAgent, an 18-\nbillion-parameter visual language model (VLM) specializ-\ning in GUI understanding and navigation. By utilizing both\nlow-resolution and high-resolution image encoders, CogA-\ngent supports input at a resolution of 1120\u00d71120, enabling\nit to recognize tiny page elements and text. As a general-\nist visual language model, CogAgent achieves the state of\nthe art on five text-rich and four general VQA benchmarks,\nincluding VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA,\ninfoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based meth-\nods that consume extracted HTML text on both PC and\nAndroid GUI navigation tasks\u2014Mind2Web and AITW, ad-\nvancing the state of the art. The model and codes are avail-\nable at https://github.com/THUDM/CogVLM.\n1. Introduction\nAutonomous agents in the digital world are ideal assistants\nthat many modern people dream of. Picture this scenario:\nYou type in a task description, then relax and enjoy a cup\nof coffee while watching tasks like booking tickets online,\nconducting web searches, managing files, and creating Pow-\nerPoint presentations get completed automatically.\nRecently, the emergence of agents based on large lan-\nguage models (LLMs) is bringing us closer to this dream.\nFor example, AutoGPT [33], a 150,000-star open-source\nproject, leverages ChatGPT [29] to integrate language un-\n*Work was done when interned at Zhipu AI.\n\u2020Corresponding authors\nderstanding with pre-defined actions like Google searches\nand local file operations. Researchers are also starting to\ndevelop agent-oriented LLMs [7, 42]. However, the poten-\ntial of purely language-based agents is quite limited in real-\nworld scenarios, as most applications interact with humans\nthrough Graphical User Interfaces (GUIs), which are char-\nacterized by the following perspectives:\n\u2022 Standard APIs for interaction are often lacking.\n\u2022 Important information including icons, images, dia-\ngrams, and spatial relations are difficult to directly con-\nvey in words.\n\u2022 Even in text-rendered GUIs like web pages, elements\nlike canvas and iframe cannot be parsed to grasp their\nfunctionality via HTML.\nAgents based on visual language models (VLMs) have\nthe potential to overcome these limitations. Instead of re-\nlying exclusively on textual inputs such as HTML [28] or\nOCR results [31], VLM-based agents directly perceive vi-\nsual GUI signals. Since GUIs are designed for human users,\nVLM-based agents can perform as effectively as humans,\nas long as the VLMs match human-level vision understand-\ning. In addition, VLMs are also capable of skills such as\nextremely fast reading and programming that are usually\nbeyond the reach of most human users, extending the po-\ntential of VLM-based agents. A few prior studies utilized\nvisual features merely as auxiliaries in specific scenarios.\ne.g. WebShop [39] which employs visual features primar-\nily for object recognition purposes. With the rapid devel-\nopment of VLM, can we naturally achieve universality on\nGUIs by relying solely on visual inputs?\nIn this work, we present CogAgent, a visual language\nfoundation model specializing in GUI understanding and\nplanning while maintaining a strong ability for general\ncross-modality tasks. By building upon CogVLM [38]\u2014a\nrecent open-source VLM, CogAgent tackles the following\nchallenges for building GUI agents:\n1\narXiv:2312.08914v2  [cs.CV]  21 Dec 2023\nFigure 1. Samples of visual agents generated by CogAgent. More samples are demonstrated in the Appendix.\n2\n\u2022 Training Data. Most current VLMs are pre-trained\non datasets like LAION [32], consisting of natural im-\nages on the Web. However, we notice that the GUI\nimages share a different distribution from natural im-\nages. We thus construct a large-scale annotated dataset\nabout GUIs and OCR for continual pre-training.\n\u2022 High-Resolution vs. Compute. In GUIs, tiny icons\nand text are ubiquitous, and it is hard to recognize\nthem in commonly-used 224 \u00d7 224 resolution. How-\never, increasing the resolution of input images results\nin significantly long sequence length in language mod-\nels. For example, a 1120 \u00d7 1120 image corresponds\nto a sequence of 6400 tokens if the patch size is 14,\ndemanding excessive training and inference compute.\nTo address this, we design a cross-attention branch\nthat allows for a trade-off between the resolution and\nthe hidden size within a proper computation budget.\nSpecifically, we propose to combine the original large\nViT [12] (4.4B parameters) used in CogVLM [38] and\na new small high-resolution cross-module (with image\nencoder of 0.30B parameters) to jointly model visual\nfeatures.\nOur experiments show that:\n\u2022 CogAgent tops popular GUI understanding and\ndecision-making benchmarks, including AITW [31]\nand Mind2Web [10]. To the best of our knowledge,\nthis is the first time that a generalist VLM can out-\nperform LLM-based methods with extracted structured\ntext.\n\u2022 Though CogAgent focuses on GUIs, it achieves\nstate-of-the-art generalist performance on nine visual\nquestion-answering benchmarks including VQAv2 [1],\nOK-VQA\n[23],\nTextVQA\n[34],\nST-VQA\n[4],\nChartQA\n[24],\ninfoVQA\n[26],\nDocVQA\n[25],\nMM-Vet [41], and POPE [19].\n\u2022 The separated design of high- and low-resolution\nbranches in CogAgent significantly lows the compute\ncost for consuming high-resolution images, e.g., the\nnumber of the floating-point operations (FLOPs) for\nCogAgent-18B with 1120 \u00d7 1120 inputs is less than\nhalf that of CogVLM-17B with its default 490 \u00d7 490\ninputs.\nCogAgent is open-sourced at https://github.\ncom/THUDM/CogVLM. It represents an effort to promote\nthe future research and application of AI agents, facilitated\nby advanced VLMs.\n2. Method\nIn this section, we will first introduce the architecture of Co-\ngAgent, especially the novel high-resolution cross-module,\nand then illustrate the process of pre-training and alignment\nin detail.\n2.1. Architecture\nThe architecture of CogAgent is depicted in Fig. 2. We\nbuild our model based on a pre-trained VLM (on the right\nside of the image), and propose to add a cross-attention\nmodule to process high-resolution input (on the left side\nof the image).\nAs our base VLM, We select CogVLM-\n17B [38], an open-sourced and state-of-the-art large vison-\nlanguage model.\nSpecifically, We employ EVA2-CLIP-\nE [35] as the encoder for low-resolution images (224\u00d7224\npixels), complemented by an MLP adapter that maps its\noutput into the feature space of the visual-language de-\ncoder. The decoder, a pre-trained language model, is en-\nhanced with a visual expert module introduced by Wang\net al. [38] to facilitate a deep fusion of visual and language\nfeatures. The decoder processes a combined input of the\nlow-resolution image feature sequence and text feature se-\nquence, and autoregressively outputs the target text.\nSimilar to most VLMs, the original CogVLM can only\naccommodate images of relatively low resolution (224 or\n490), which hardly meets the demands of GUI where the\nscreen resolution of computers or smartphones is typically\n720p (1280 \u00d7 720 pixels) or higher. It is a common prob-\nlem among VLMs, e.g. LLaVA [21] and PALI-X [8] are\npre-trained at a low resolution of 224 \u00d7 224 on the general\ndomain. The primary reason is that high-resolution image\nbrings prohibitive time and memory overhead: VLMs usu-\nally concatenate text and image feature sequence as input\nto the decoder, thus the overhead of self-attention module\nis quadratic to the number of visual tokens (patches), which\nis quadratic to the image\u2019s side length. There are some ini-\ntial attempts to reduce costs for high-resolution images. For\ninstance, Qwen-VL [2] proposes a position-aware vision-\nlanguage adapter to compress image features, but only re-\nduces sequence length by four and has a maximum reso-\nlution of 448 \u00d7 448. Kosmos-2.5 [22] adopts a Perceiver\nResampler module to reduce the length of the image se-\nquence. However, the resampled sequence is still long for\nself-attention in the large visual-language decoder (2,048\ntokens), and can only be applied to restricted text recogni-\ntion tasks.\nTherefore, we propose a novel high-resolution cross-\nmodule as a potent complement to the existing structure\nfor enhancing understanding at high resolutions, which not\nonly maintains efficiency confronting high-resolution im-\nages, but also offers flexible adaptability to a variety of\nvisual-language model architectures.\n2.2. High-Resolution Cross-Module\nThe structural design of high-resolution cross-module is\nmainly based on the following observations:\n3\nInput Image (                  ) \nInput Image (               ) \nInput Text\nWord Embedding\ni-th layer\nL-th layer\n\u2026\nconcat\nHigh-Resolution Cross-Module\nOriginal VLM\n[Target Text]\nMLP Adapter\nTask: How can I find an \napartment that offers free \nWi-Fi?\nPlan: 1.Locate and select \nthe price filter option. \n2.Select the 'Free Wi-Fi' \noption. 3. Apply the filters \nto update the search \nresults, and choose one \nsatisfying apartment. \nAction: Move the cursor \nto the 'Price filter' on the \nleft sidebar where it says \n'Your previous filters\u2019, and \nclick on the 'Free Wi-Fi' \ncheckbox.\n[ High-resolution image feature ]\n[Low-resolution image feature] [Text feature]\ndownsample\nHigh-resolution \nImage Encoder\n(light-weight)\nLow-resolution \nImage Encoder\nVisual Language Decoder\n(hidden size = 4096)\n1st layer\nCross-Attention \n(hidden size = 1024)\ncross-attn\ncross-attn\n\u2026\nFigure 2. Model architecture of CogAgent. We adopt CogVLM as\nthe original VLM.\n1. At a modest resolution such as 224 \u00d7 224, images\ncan depict most objects and layouts effectively, yet\nthe resolution falls short in rendering text with clar-\nity.\nHence, our new high-resolution module should\nemphasize text-related features, which are vital for un-\nderstanding GUIs.\n2. While pre-trained VLMs in general domain often\nneed large hidden sizes (e.g.\n4,096 in PALI-X and\nCogVLM, 5,120 in LLaVA), VLMs tailored for text-\ncentered tasks like document OCR require smaller hid-\nden sizes to achieve satisfying performance (e.g. 1,536\nin Kosmos-2.5 and Pix2Struct [16]). This suggests that\ntext-related features can be effectively captured using\nsmaller hidden sizes.\nAs shown in Fig. 2, the high-resolution cross-module\nacts as a new branch for higher-resolution input, which\naccepts images of size 1120 \u00d7 1120 pixels in our im-\nplementation.\nDifferent from the original low-resolution\ninput branch, the high-resolution cross-module adopts a\nmuch smaller pre-trained vision encoder (visual encoder of\nEVA2-CLIP-L [35] in our implementation, 0.30B parame-\nters), and uses cross-attention of a small hidden size to fuse\nhigh-resolution image features with every layer of VLLM\ndecoder, thus reducing the computational cost. To be con-\ncrete, for an input image, it is resized to 1120 \u00d7 1120 and\n224\u00d7224 and fed into the high-resolution cross-module and\nthe low-resolution branch respectively, then encoded into\nimage feature sequences Xhi and Xlo with two distinct-sized\nimage encoders in parallel. The visual language decoder re-\ntains its original computations, while the only change is to\nintegrate a cross-attention between Xhi and hidden states in\nevery decoder layer.\nFormally, suppose that the input hidden states of\nthe i-th attention layer in the decoder are Xini\n\u2208\nRB\u00d7(LIlo+LT )\u00d7Ddec, and the output hidden states of cross-\nmodule\u2019s image encoder are Xhi \u2208 RB\u00d7(LIhi)\u00d7Dhi, where B\nis the batch size, LIlo, LIhi and LT are the lengths of the\nlow-resolution image, high-resolution image and text se-\nquences, Ddec and Dhi is the hidden size of the decoder and\nhigh-resolution encoder\u2019s output respectively. Each layer\u2019s\nattention procedure can be formulated as\nX\u2032\ni = MSA(layernorm(Xini)) + Xini,\n(1)\nXouti = MCA(layernorm(X\u2032\ni), Xhi) + X\u2032\ni,\n(2)\nwhere MSA and MCA represent multi-head self-attention\nwith visual expert and multi-head cross-attention, while\nX\u2032\ni and Xouti represent their respective output features\nwith the residual connection. To implement cross-attention\nbetween them, we add learnable transformation matrices\nW i\nKcross, W i\nVcross \u2208 RDhi\u00d7Dcross to get Ki\ncross = XhiW i\nKcross,\nV i\ncross = XhiW i\nVcross \u2208 RLIhi\u00d7Dcross, and W i\nQcross \u2208 RDdec\u00d7Dcross\nto get Qi\ncross = X\u2032\niW i\nQcross \u2208 R(LIlo+LT )\u00d7Dcross in every de-\ncoder layer. With the residual connection in Eq. 2, the cross-\nattention with high-resolution images can be perceived as\na complement to the features of low-resolution images,\nthereby effectively utilizing the previous pre-trained model\nin low resolution.\nComputational complexity. Let the number of attention\nhead be Hcross and Hdec in cross-attention and self-attention,\nand the dimension of each head be dcross = Dcross/Hcross\nand ddec = Ddec/Hdec. If using our high-resolution cross-\nmodule, the computational complexity of attention is\nTimproved = O\n\u0000(LIlo + LT )LIhiHcrossdcross\n+ (LIlo + LT )2Hdecddec\n\u0001\n.\n(3)\nNote that dcross and Hcross can be flexibly adjusted accord-\ning to computational budget and model performance. If not\nutilizing the high-resolution cross-module and directly sub-\nstituting low-resolution images with high-resolution ones,\nthe computational complexity would be\nToriginal = O\n\u0000(LIhi + LT )2Hdecddec\n\u0001\n.\n(4)\nIn our implementation, dcross = 32, Hcross = 32, and\nwe inherits ddec = 128, Hdec = 32 from CogVLM-17B.\nBoth high- and low-resolution encoders patchify images\nwith 14 \u00d7 14-pixel patches, thus LIhi = 6400, LIlo = 256.\nOur method leads to at least\nLIhi+LT\nLIlo+LT = 6400+LT\n256+LT \u00d7 accel-\neration which is a stringent lower bound (refer to Appendix\nfor detailed derivation), and reduces memory overhead at\nthe same time.\n4\n2.3. Pre-training\nTo enhance the model\u2019s ability to comprehend high-\nresolution images and adapt it for GUI application scenar-\nios, we focus our pre-training efforts on the following as-\npects: the capability to recognize texts of various sizes, ori-\nentations, and fonts in high-resolution images, the ground-\ning ability of text and objects in the image, and a special-\nized understanding capability for GUI imagery such as web\npage. We divide our pre-train data into three parts based on\nthe aforementioned aspects, with samples in the Appendix.\nAll the pre-training data are derived from publicly available\ndatasets. The construction methods are detailed below.\nText recognition. Our data includes (1) Synthetic render-\nings with text from language pre-training dataset (80M).\nThis is similar to the Synthetic Document Generator in Kim\net al. [15], with text of varying font, size, color and orienta-\ntion, and diverse image background from LAION-2B [32].\n(2) Optical Character Recognition (OCR) of natural im-\nages (18M). We collect natural images from COYO [6] and\nLAION-2B [32] and employ Paddle-OCR [13] to extract the\ntexts and their bounding boxes, and filter out images with\nno text boxes.\nPaddle-OCR may introduce some errors,\nwhich can be ameliorated through integration with other\npre-training datasets and subsequent fine-tuning processes.\n(3) Academic documents (9M). We follow Nougat [5] to\nconstruct image-text pairs including text, formula and tables\nfrom the source code (LaTeX) release on arXiv. For (1)(3),\nwe apply the same data augmentation as Nougat which in-\ncludes erosion, gaussian noise, gaussian blur, image com-\npression, and elastic transform, etc. For (2), we additionally\nemployed more aggressive rotation and flipping data aug-\nmentation techniques, thereby enhancing the model\u2019s ro-\nbustness in recognizing text.\nVisual grounding. It is imperative for GUI agents to pos-\nsess the capability to accurately comprehend and locate\ndiverse elements within images.\nTherefore, we incorpo-\nrated a range of grounding data into pre-training. We fol-\nlow CogVLM [38] to use a constructed visual grounding\ndataset of 40M images with image-caption pairs sampled\nfrom LAION-115M [18], which associate entities in the\ncaption with bounding boxes to indicate their positions.\nThe format of the bounding box is [[x0, y0, x1, y1]], where\n(x0, y0) and (x1, y1) represent the coordinates of upper-left\nand lower-right corners which are normalized to [000, 999].\nIf multiple objects are indicated by a single noun phrase,\ntheir boxes are separated by semicolons in double square\nbrackets. We have also collected grounding data on web\npage elements, which will be introduced in the next part.\nGUI imagery.\nOur approach innovatively addresses the\nscarcity and limited relevance of GUI images in datasets\nlike LAION and COYO, which predominantly feature nat-\nural images. GUI images, with their distinct elements such\nas input fields, hyperlinks, icons, and unique layout charac-\nteristics, require specialized handling. To boost the model\u2019s\ncapability in interpreting GUI imagery, we have conceptu-\nalized two pioneering GUI grounding tasks: (1) GUI Re-\nferring Expression Generation (REG) \u2013 where the model\nis tasked with generating HTML code for DOM (Docu-\nment Object Model) elements based on a specified area in a\nscreenshot, and (2) GUI Referring Expression Comprehen-\nsion (REC) \u2013 which involves creating bounding boxes for\ngiven DOM elements. To facilitate robust training in GUI\ngrounding, we have constructed the CCS400K (Common\nCrawl Screenshot 400K) dataset. This extensive dataset is\nformed by extracting URLs from the latest Common Crawl\ndata, followed by capturing 400,000 web page screenshots.\nAlongside these screenshots, we compile all visible DOM\nelements and their corresponding rendered boxes using\nPlaywright1, supplementing the dataset with 140 million\nREC and REG question-answer pairs. This rich dataset en-\nsures comprehensive training and understanding of GUI el-\nements. To mitigate the risk of overfitting, we employ a\ndiverse range of screen resolutions for rendering, selected\nrandomly from a list of commonly used resolutions across\nvarious devices. Additionally, to prevent the HTML code\nfrom becoming overly extensive and unwieldy, we perform\nnecessary data cleaning by omitting redundant attributes in\nthe DOM elements, following the method outlined in [16].\nWe also incorporate publicly available text-image\ndatasets including LAION-2B and COYO-700M (after re-\nmoving the broken URLs, NSFW images, and images with\nnoisy captions and political bias) during pre-training.\nWe pre-train our CogAgent model for a total of 60,000\niterations with a batch size of 4,608 and a learning rate of\n2e-5. We freeze all parameters except the newly added high-\nresolution cross-module for the first 20,000 steps, resulting\nin a total number of 646M (3.5%) trainable parameters, then\nadditionally unfreeze the visual expert in CogVLM for the\nnext 40,000 steps. We warm up with curriculum learning by\nfirst training on easier text recognition (synthetic renderings\nand OCR on natural images) and image captioning, then se-\nquentially incorporating harder text recognition (academic\ndocument), grounding data and web page data, as we ob-\nserved that it leads to faster convergence and more stable\ntraining in our preliminary experiments.\n2.4. Multi-task Fine-tuning and Alignment\nTo enhance our model\u2019s performance for diverse tasks and\nensure it aligns with free-form human instructions in the\nGUI setting, we further fine-tune our model on a broad\nrange of tasks. We manually collected over two thousand\nscreenshots from mobile phones and computers, each anno-\ntated with screen elements, potential tasks, and methods of\noperation in the question-answering format by human anno-\ntators (details illustrated in the Appendix). We also utilize\n1https://playwright.dev\n5\nMind2Web [10] and AITW [31], datasets focusing on web\nand Android behaviors which comprise tasks, sequences of\nactions and corresponding screenshots, and convert them\ninto a natural language question-and-answer format using\nGPT-4. Besides, we incorporate multiple publicly available\nvisual question-answering (VQA) datasets encompassing a\nvariety of tasks into our alignment dataset. We unfreeze all\nmodel parameters during this stage and train for 10k itera-\ntions with a batch size of 1024 and a learning rate of 2e-5.\n3. Experiments\nTo evaluate the foundational capabilities and GUI-related\nperformance of our model, we conduct extensive experi-\nments on a broad range of datasets. First, we conduct eval-\nuations on eight VQA benchmarks, as well as MM-Vet [41]\nand POPE [19], which validate our model\u2019s enhanced abil-\nity in visual understanding, especially on those that are re-\nliant on text recognition. Then we evaluate our model on\nMind2Web and AITW datasets, as the representative of two\nmajor GUI scenarios \u2014 computers and smartphones.\n3.1. Foundational Visual Understanding\nWe first extensively evaluate CogAgent\u2019s foundational vi-\nsual understanding capability across eight VQA bench-\nmarks, covering a wide range of visual scenes. The bench-\nmarks can be divided into two categories: general VQA, in-\ncluding VQAv2 [1] and OK-VQA [23], and text-rich VQA,\nincluding TextVQA [34], OCR-VQA [27], ST-VQA [4],\nDocVQA [25], InfoVQA [26] and ChartQA [24]. The latter\ncategory emphasizes the understanding of visually-situated\ntext, including documents, charts, photographs containing\ntext, etc. Contrary to models individually fine-tuned for op-\ntimal performance on each downstream task, our model is\nfine-tuned collectively on all datasets simultaneously, yield-\ning a single generalist model which is then evaluated across\nall datasets. The goal of generalist evaluation is to better\nmirror real-world situations of visual agents where typically\na single model is used, and to demonstrate the model\u2019s ver-\nsatility and robustness across tasks.\nThe results are presented in Tab. 1. For general VQA,\nCogAgent achieves state-of-the-art generalist results on\nboth datasets.\nFor text-rich VQA, CogAgent achieves\nstate-of-the-art results on 5 out of 6 benchmarks, signif-\nicantly surpassing generalist competitors (TextVQA+8.0,\nChartQA+2.1, InfoVQA+2.3, DocVQA+16.2), even out-\nperforming the task-specific state-of-the-art models on\nTextVQA(+4.7), STVQA(+0.6) and DocVQA(+1.6). No-\ntably, compared to the generalist results of CogVLM which\nCogAgent is initially based on, CogAgent demonstrates\ncertain improvements on both general and Text-rich VQA\ntasks, suggesting the efficacy of our proposed model archi-\ntecture and training methods.\nFurthermore, we conducted zero-shot tests of our model\non the challenging MM-Vet [41] and POPE [19] datasets,\nboth of which are instrumental in gauging the multi-modal\ncapabilities and the generalization performance in com-\nplex tasks including conversation question-answering, de-\ntailed descriptions, complex reasoning tasks. MM-Vet is\ndesigned with six core tasks to assess multi-modal models\u2019\nproficiency in handling intricate assignments, and POPE-\nadversarial models on their susceptibility to hallucinations.\nOur experimental results, as detailed in Table 2, show-\ncase that our model significantly outperforms other existing\nmodels in both datasets. Notably, on the MM-Vet dataset,\nour model achieved a remarkable score of 52.8, surpassing\nthe closest competitor, LLaVA-1.5, by a substantial margin\n(+16.5). On the POPE-adversarial evaluation, our model at-\ntained a score of 85.9, demonstrating superior handling of\nhallucinations compared to other models.\nThese results indicate CogAgent\u2019s robust performance in\nfoundational visual understanding, especially in the inter-\npretation of images with embedded text. With these core\ncompetencies, the model can be feasibly applied to various\nvisual agent tasks across different GUI environments.\n3.2. GUI Agent: Computer Interface\nWe evaluate CogAgent on Mind2Web, a dataset for web\nagents that includes over 2,000 open-ended tasks collected\nfrom 137 real-world websites across 31 domains. Each en-\ntry in the dataset comprises a high-level task description, a\nsequence of actions, and webpage snapshots in a variety of\nformats, including HTML and screenshots. Given task de-\nscription, current webpage snapshot and previous actions as\ninputs, agents are expected to predict the subsequent action.\nWe follow the setting of Deng et al. [10] in our experiments,\nand report step success rate (step SR) metric. Further details\nare attached in the Appendix.\nSeveral language models were evaluated on this bench-\nmark. For instance, AgentTuning [42] and MindAct [10]\nevaluated Llama2-70B and Flan-T5-XL in a fine-tuned set-\nting, and GPT-3.5 and GPT-4 in a in-context learning set-\nting.\nHowever, limited by the input modality of lan-\nguage models, these models could only use heavily cleansed\nHTML as the representation of screen inputs. To the best of\nour knowledge, no visually-based web agents have been ex-\nperimented with on this benchmark.\nWe fine-tune our model on the train set and evalu-\nate on three out-of-domain subsets, i.e.\ncross-website,\ncross-domain, and cross-task.\nWe additionally fine-tune\nLLaMA2-7B and LLaMA2-70B as the baseline of fine-\ntuned LLMs, and adopt the same HTML cleansing process\nas Deng et al. [10] to construct HTML input. The results\nare presented in Sec. 3.2. Compared to other methods, our\napproach achieved significant performance improvements\nacross all three subsets, surpassing LLaMA2-70B, which\n6\nMethod\nGeneral VQA\nText-rich VQA\nVQAv2\nOKVQA\nOCRVQA\nTextVQA\nSTVQA\nChartQA\nInfoVQA\nDocVQA\ntask-specific fine-tuning models\nPix2Struct [16]\n-\n-\n-\n-\n-\n58.6\n40.0\n76.6\nBLIP-2 [18]\n82.2\n59.3\n72.7\n-\n-\n-\n-\n-\nPALI-X-55B [8]\n86.0\n66.1\n75.0\n71.4\n79.9\n70.9\n49.2\n80.0\nCogVLMtask-specific [38]\n84.7\n64.7\n74.5\n69.7\n-\n-\n-\n-\ngeneralist models\nUReader [40]\n-\n57.6\n-\n-\n-\n59.3\n42.2\n65.4\nQwen-VL [2]\n79.5\n58.6\n75.7\n63.8\n-\n65.7\n-\n65.1\nQwen-VL-chat [2]\n78.2\n56.6\n70.5\n61.5\n-\n66.3\n-\n62.6\nLlava-1.5 [20]\n80.0\n-\n-\n61.5\n-\n-\n-\n-\nFuyu-8B [3]\n74.2\n60.6\n-\n-\n-\n-\n-\n-\nCogVLMgeneralist [38]\n83.4\n58.9\n74.1\n68.1\n-\n-\n-\n-\nCogAgent (Ours)\n83.7\n61.2\n75.0\n76.1\n80.5\n68.4\n44.5\n81.6\nTable 1. Performance on Visual Question Answering benchmarks. Bold text indicates the best score among the generalist category, and\nunderlined text represents the best score across both generalist and task-specific categories.\nis nearly 4\u00d7 the scale of CogAgent, by 11.6%, 4.7%, and\n6.6%, respectively. This reflects not only the capability of\nour model but also the advantages of employing a visual\nagent in computer GUI scenarios.\n3.3. GUI Agent: Smartphone Interface\nTo evaluate our model on diverse smartphone interfaces and\ntasks, we utilize Android in the Wild (AITW) dataset [31] ,\na large-scale dataset for Android device agents. It comprises\n715k operation episodes, covering 30k distinct task instruc-\ntions, four Android versions, and eight device types featur-\ning varying screen resolutions. Each episode in the dataset\nconsists of a goal described in natural language, followed\nby a sequence of actions and corresponding screenshots.\nThe training target is to predict the next action based on the\ngiven goal, historical actions, and the screenshot. AITW\nconsiders a wide range of action types, including tapping,\nswiping, typing, going home, going back, entering, etc. For\neach action, models are required to predict the exact action\ntype; for tap, swipe and type, models are further required\nMethod\nLLM\nMM-Vet\nPOPEadv\nBLIP-2 [18]\nVicuna-13B\n22.4\n-\nOtter [17]\nMPT-7B\n24.7\n-\nMiniGPT4 [44]\nVicuna-13B\n24.4\n70.4\nInstructBLIP [9]\nVicuna-13B\n25.6\n77.3\nLLaVA [21]\nLLaMA2-7B\n28.1\n66.3\nLLaMA-Adapter v2 [14]\nLLaMA-7B\n31.4\n-\nDreamLLM [11]\nVicuna-7B\n35.9\n76.5\nLLaVA-1.5 [20]\nVicuna-13B\n36.3\n84.5\nEmu [36]\nLLaMA-13B\n36.3\n-\nCogAgent (Ours)\nVicuna-7B\n52.8\n85.9\nTable 2. Evaluation of CogAgent on conversational style QA\nand hallucination assessment. Regarding the POPE dataset, we\nuse its adversarial subset for this evaluation.\nMethod\ncross-task cross-website cross-domain overall\nRepresentations of screen inputs: HTML\nGPT-3.5[29](few-shot)\n18.6\n17.4\n16.2\n17.4\nGPT-4[30]\u2020(few-shot)\n36.2\n30.1\n26.4\n30.9\nFlan-T5XL [10]\n52.0\n38.9\n39.6\n43.5\nLLaMA2-7B[37]\n52.7\n47.1\n50.3\n50.1\nLLaMA2-70B[37]\n55.8\n51.6\n55.7\n54.4\nRepresentations of screen inputs: Image\nCogAgent (Ours)\n62.3\n54.0\n59.4\n58.2\nTable 3. Performance on Mind2Web. \u2020 denotes element selec-\ntion from top-10 element candidates, others from top-50, follow-\ning Deng et al. [10]. Results for GPT-3.5 and GPT-4 are from\nDeng et al. [10].\nMethod\nGoogleApp Install WebShop General Single Overall\nRepresentations of screen inputs: textual description (OCR+icon)\nGPT-3.5[29](few-shot)\n10.47\n4.38\n8.42\n5.93\n9.39\n7.72\nLLaMA2-7B[37]\u2020\n30.99\n35.18\n19.92\n28.56\n27.35\n28.40\nRepresentations of screen inputs: image\nAuto-UIunified[43]\n71.37\n76.89\n70.26\n68.24\n84.58\n74.27\nCogAgent (Ours)\n74.95\n78.86\n71.73\n65.38\n93.49\n76.88\nTable 4. Performance on Android in the Wild (AITW) dataset.\n\u2020 represents models individually fine-tuned on each subset, while\nothers are unified models across all subsets.\nThe results of\nLLaMA2 and GPT-3.5 are from Zhan and Zhang [43].\nto predict the position, direction, and content to be typed,\nrespectively.\nWe conduct comparisons with two kinds of baselines:\nlanguage models using the textual description of UI ele-\nments provided by the original dataset (text OCR and icon)\nas the representations of screen inputs2, and visual-language\n2Some Android applications may have View Hierarchy which is more\nfriendly to language-based agents, but most of them tend to be poor quality\nor missing altogether. Therefore, as a large-scale, general-purpose dataset,\nAITW retained the results of OCR detection and icon detection as textual\n7\nmodels using images as the screen inputs. We simultane-\nously fine-tuned on all the subsets, yielding a unified model\nwhich is then evaluated on all test sets. As the GoogleApps\nsubset is 10-100 times larger than other subsets, we down-\nsample it to 10% to avoid data imbalance.\nResults are shown in Tab. 4. CogAgent achieves state-\nof-the-art performance compared to all previous methods.\nIn comparison to language-based methods, our model sur-\npasses both baselines by a large margin. In comparison to\nthe visual-language baseline, Auto-UI, our model achieves\n+2.61 improvements in the overall performance.\nIn in-\nstances of inaccuracies, we randomly sample hundreds of\ncases, and upon reassessment, more than 40% are deter-\nmined to be correct (refer to the appendix for details). This\ndiversity arises from the multiple valid pathways inherent in\nmobile interactions, resulting in a range of responses.\n4. Ablation Study\nTo thoroughly comprehend the impact of various compo-\nnents in the methodology, we conduct ablation studies on\ntwo aspects, model architecture and training data. The eval-\nuation is conducted on diverse datasets, including multiple\nVQA datasets (STVQA, OCRVQA, DocVQA) and a web\nagent dataset (Mind2Web).\nFor VQA datasets, we fine-\ntune the model on four datasets together for 3,000 iters with\na batch size of 1,280, and report the generalist score; for\nMind2Web, models are fine-tuned for 2,400 iters with a\nbatch size of 128 and use top-10 setting. Training iterations\nare fewer than those in the main experiment, aiming to con-\ntrol variables within the constraints of a limited budget.\n4.1. Model Architecture\nTo ascertain the efficacy of the high-resolution cross-\nmodule, we compare it with directly increasing the resolu-\ntion using the original model architecture of CogVLM, and\nablate on two perspectives: computational efficiency and\nmodel performance.\nTo measure computational overhead, we use floating\npoint operations (FLOPs) as the metric, and conduct exper-\niments on multiple resolutions including 224, 490, 756, and\n1120. From Fig. 3 we can see that, as the image resolution\nincreases, models that use a high-resolution cross-module\nexperience only a modest rise in computational overhead,\ndemonstrating an almost linear relationship with the num-\nber of image patches. In contrast, using the original model\nstructure, i.e. CogVLM, leads to a significant increase in\nthe number of FLOPs at higher resolutions. Its FLOPs can\neven be more than 10 times higher compared to employing\na cross-module at a resolution of 1120, which is the resolu-\ntion utilized by CogAgent.\nrepresentations of screenshots.\n256\n490\n756\n1120\nResolution\n20\n40\n60\n80\n100\n120\n140\nTFLOPs\n8.8\n10.1\n12.6\n29.1\n66.3\n143.2\n7.8\nwith cross-module\noriginal architecture\nFigure 3. Comparison of FLOPs during forward propagation for\ndifferent model architectures and resolutions.\nWe further compare the model performance in Tab. 5,\nwhich indicates that models with high-resolution cross-\nmodule at the resolution of 756 require only 1/2 of the com-\nputational resources used by the original structure at the\nresolution of 490, while delivering significantly better per-\nformance. Additionally, the high-resolution cross-module\nallows for further increasing models\u2019 acceptable resolution\nwithin a limited computational budget, thereby yielding ad-\nditional performance improvements.\nhigh-res base cross STVQA OCRVQA DocVQA Mind2Web training TFLOPs\nmodule res\nres\ntime/it (s)\n%\n224\n\u2014\n48.0\n70.2\n28.6\n34.6\n2.36\n7.77\n%\n490\n\u2014\n68.1\n74.5\n57.6\n40.7\n6.43\n29.14\n!\n224 756\n73.6\n74.2\n62.3\n40.7\n3.57\n10.08\n!\n224 1120\n78.2\n75.9\n74.1\n41.4\n5.17\n12.56\nTable 5. Ablation study on model architecture. Training time is\nevaluated on A800 with the batch size of 8. Models are pre-trained\nwith Caption+OCR data.\n4.2. Pre-train Data\npre-train data base res cross res STVQA OCRVQA DocVQA Mind2Web\nCap\n490\n\u2014\n68.1\n74.5\n57.6\n38.6\nCap+OCR\n490\n\u2014\n72.5\n75.0\n59.8\n40.7\nCap+OCR\n224\n1120\n78.2\n75.9\n74.1\n41.4\nAll\n224\n1120\n79.4\n75.6\n76.4\n54.2\nTable 6. Ablation study on pre-train data with sequentially added\nimage captioning, OCR and other pre-train data.\nWe further conduct an ablation study on pre-training\ndata, which is an integral part of training visual agents.\nBuilding upon the image-caption data commonly used in\nvisual-language training, we sequentially add OCR data\n(denoted as Cap+OCR), as well as GUI and grounding data\n(denoted as All). The results in Tab. 6 indicate that each\npart of data broadly contributes to enhanced performance.\nNotably, web and grounding data have a significant impact\non the Mind2Web dataset, underscoring the importance of\n8\nconstructing domain-specific pre-train data in the training\nof GUI agents.\n5. Conclusion\nWe introduce CogAgent, a VLM-based GUI agent with en-\nhanced pre-train data construction and efficient architecture\nfor high-resolution input. CogAgent achieves state-of-the-\nart performance on a wide range of VQA and GUI bench-\nmarks, and will be open-sourced.\nCogAgent is an initial exploration of VLM-based GUI\nagent, and still has some shortcomings, e.g. imprecise out-\nput coordinates and incapability of processing multiple im-\nages, necessitating further research.\nAcknowledgments\nWe thank Xiaohan Zhang from Zhipu AI for managing the\ndata annotation team, and Zhao Xue, Aohan Zeng, Yifan\nAn, Chenxu Guo from Zhipu AI and Tsinghua for data man-\nagement.\nReferences\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425\u2013\n2433, 2015. 3, 6, 11\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3,\n7\n[3] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa\u02d8gnak Tas\u00b8\u0131rlar.\nIntroducing our multimodal models, 2023. 7\n[4] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc\u00b8al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 4291\u20134301, 2019. 3, 6, 12\n[5] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and\nRobert Stojnic. Nougat: Neural optical understanding for\nacademic documents.\narXiv preprint arXiv:2308.13418,\n2023. 5\n[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun\nLee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-text pair dataset.\nhttps://github.com/\nkakaobrain/coyo-dataset, 2022. 5\n[7] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,\nKarthik Narasimhan, and Shunyu Yao. Fireact: Toward lan-\nguage agent fine-tuning. arXiv preprint arXiv:2310.05915,\n2023. 1\n[8] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 3, 7\n[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 7\n[10] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel\nStevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web:\nTowards a generalist agent for the web.\narXiv preprint\narXiv:2306.06070, 2023. 3, 6, 7, 12\n[11] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 7\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[13] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei\nLiu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing\nDang, et al. Pp-ocr: A practical ultra lightweight ocr system.\narXiv preprint arXiv:2009.09941, 2020. 5\n[14] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\nsual instruction model.\narXiv preprint arXiv:2304.15010,\n2023. 7\n[15] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon\nNam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang-\ndoo Yun, Dongyoon Han, and Seunghyun Park.\nOcr-free\ndocument understanding transformer. In European Confer-\nence on Computer Vision, pages 498\u2013517. Springer, 2022.\n5\n[16] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu,\nFangyu Liu, Julian Martin Eisenschlos, Urvashi Khandel-\nwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.\nPix2struct: Screenshot parsing as pretraining for visual lan-\nguage understanding. In International Conference on Ma-\nchine Learning, pages 18893\u201318912. PMLR, 2023. 4, 5, 7\n[17] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi\nPu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-\nit: Multi-modal in-context instruction tuning. arXiv preprint\narXiv:2306.05425, 2023. 7\n[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 5, 7\n[19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 3, 6, 11\n[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 7\n9\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 3, 7\n[22] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li\nDong, Weiyao Luo, et al. Kosmos-2.5: A multimodal literate\nmodel. arXiv preprint arXiv:2309.11419, 2023. 3\n[23] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195\u20133204, 2019. 3, 6, 11\n[24] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question an-\nswering about charts with visual and logical reasoning. arXiv\npreprint arXiv:2203.10244, 2022. 3, 6, 12\n[25] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocvqa: A dataset for vqa on document images. In Proceed-\nings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200\u20132209, 2021. 3, 6, 12\n[26] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 1697\u20131706, 2022. 3, 6,\n12\n[27] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In 2019 international conference\non document analysis and recognition (ICDAR), pages 947\u2013\n952. IEEE, 2019. 6, 11\n[28] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al.\nWebgpt:\nBrowser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332, 2021. 1\n[29] OpenAI. Introducing chatgpt. 2022. 1, 7\n[30] OpenAI. Gpt-4 technical report, 2023. 7\n[31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana\nRiva, and Timothy Lillicrap. Android in the wild: A large-\nscale dataset for android device control.\narXiv preprint\narXiv:2307.10088, 2023. 1, 3, 6, 7, 13\n[32] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 3,\n5\n[33] Significant-Gravitas. Autogpt. https://github.com/\nSignificant-Gravitas/AutoGPT, 2023. 1\n[34] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 3, 6, 11\n[35] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 3, 4\n[36] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 7\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 7\n[38] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 1, 3, 5, 7\n[39] Shunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. Webshop: Towards scalable real-world web in-\nteraction with grounded language agents. Advances in Neu-\nral Information Processing Systems, 35:20744\u201320757, 2022.\n1\n[40] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nGuohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,\net al. Ureader: Universal ocr-free visually-situated language\nunderstanding with multimodal large language model. arXiv\npreprint arXiv:2310.05126, 2023. 7\n[41] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 3, 6,\n11\n[42] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,\nYuxiao Dong, and Jie Tang. Agenttuning: Enabling gener-\nalized agent abilities for llms. abs/2310.12823, 2023. 1, 6,\n12\n[43] Zhuosheng Zhan and Aston Zhang. You only look at screens:\nMultimodal chain-of-action agents. abs/2309.11436, 2023.\n7, 13\n[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 7\n10\nCogAgent: A Visual Language Model for GUI Agents\nAppendix\n1. Details of Training Configurations\nWe report the detailed training settings of CogAgent in Ta-\nble 7, and model configurations of CogAgent in Table 8.\nConfigurations\nPre-train\nMulti-task\nTotal steps\n60, 000\n10, 000\nWarmup steps\n500\n500\nBatch size\n4, 608\n1, 024\nLearning rate\n2 \u00d7 10\u22125\nLearning rate decay\nCosine\nWeight decay\n0.05\nDropout ratio\n0.1\nAdam \u03f5\n1 \u00d7 10\u22125\nAdam \u03b2\n(0.9, 0.95)\nTable 7.\nTraining settings of pre-training and multi-task fine-\ntuning.\nVLM decoder\nArchitecture\nVicuna-1.5-7B + visual expert\nLayers\n32\nHidden size\n4, 096\nAttention heads\n32\nLow-resolution visual encoder\nArchitecture\nEVA2-CLIP-E\nInput resolution\n224 \u00d7 224\nPatch size\n14 \u00d7 14\nHigh-resolution visual encoder\nVisual encoder\nEVA2-CLIP-L\nInput resolution\n1120 \u00d7 1120\nPatch size\n14 \u00d7 14\nCross Attention\nHidden size\n1, 024\nAttention heads\n32\nTable 8. Model configurations of CogAgent.\n2. Details of Evaluation Datasets\nIn this section, we will provide a detailed overview of the\ndatasets used in our evaluations .\n2.1. General VQA\n\u2022 VQAv2 [1]. VQAv2 is designed for visual question an-\nswering with natural images, covering a wide range of\nquestion types including yes/no, numerical counting,\nand more open-ended inquiries. The dataset comprised\nof a collection exceeding 200,000 images, paired with\nmore than 1.1 million questions.\nEach question is\npaired with 10 answers annotated by different anno-\ntators.\n\u2022 OK-VQA [23]. OK-VQA (Outside Knowledge Visual\nQuestion Answering) dataset is constructed to evaluate\nvisual question-answering skills that require external\nknowledge. Models need to combine image content\nand common sense to answer questions. The dataset\nincludes 14,055 open-ended questions, each accompa-\nnied by 5 ground truth answers.\n\u2022 MM-Vet [41]. MM-Vet is designed to evaluate the\noverall capability of generalist visual language models\nin a zero-shot manner. It integrates 6 core VL capa-\nbilities to solve complex tasks (including recognition,\nOCR, knowledge, language generation, spatial aware-\nness, and math), and explores 16 distinct integrations\nresulting from the combination of these capabilities.\nAs for evaluation metrics, it utilizes a language model-\nbased evaluator tailored for open-ended responses.\n\u2022 POPE [19].\nPOPE (Polling-based Object Probing\nEvaluation) is a dataset constructed to assess the object\nhallucination problem in large visual language mod-\nels. It employs a polling-based object probing method,\ntransforming hallucination assessment into a binary\nclassification challenge. This is achieved by prompting\nlarge vision-language models (LVLMs) with straight-\nforward Yes-or-No queries regarding the objects in\nquestion (for example, \u201dIs there a car in the image?\u201d).\nOur evaluation is conducted under the dataset\u2019s most\nchallenging setting: the adversarial setting.\n2.2. Text-rich VQA\n\u2022 OCR-VQA [27].\nOCR-VQA dataset comprises\n207,572 images of book covers, paired with more than\n1 million question-answer pairs.\nThe questions in-\nquire about book information including title, edition,\nyear, author, and genre of the book, which requires text\nrecognition and comprehension abilities.\n\u2022 TextVQA [34]. TextVQA is a benchmark of visual\nreasoning based on text in images. Models need to\n11\nTask\nDataset\nDescription\nSplit\nMetrics\nGeneral VQA\nVQAv2\nVQA on natural images.\ntest-dev\nVQA Score(\u2191)\nOK-VQA\nVQA on natural images requiring outside knowledge.\nval\nVQA Score (\u2191)\nMM-Vet\nConversational style VQA on integrated capabilities.\ntest\nGPT-4 score(\u2191)\nPOPE\nVQA for hallucination assessment. The adversarial setting is used.\ntest\nF1 score(\u2191)\nText-rich VQA\nOCR-VQA\nVQA on images of book covers.\ntest\nEM (\u2191)\nTextVQA\nVQA on natural images containing text.\ntest\nVQA Score (\u2191)\nST-VQA\nVQA on natural images requiring textual understanding.\ntest\nANLS (\u2191)\nChartQA\nVQA about charts with visual and logical reasoning.\ntest\nVQA Score (\u2191)\nInfoVQA\nVQA on infographics.\ntest\nANLS (\u2191)\nDocVQA\nVQA on document images.\ntest\nANLS (\u2191)\nGUI Agent\nMind2Web\nWeb behavior prediction given snapshots and historical actions.\ntest\nstep SR (\u2191)\nAITW\nAndroid behavior prediction given snapshots and historical actions.\ntest\nMatching Score (\u2191)\nTable 9. Summary of the evaluation benchmarks.\nincorporate the textual information in the images and\nreason over it to answer TextVQA questions. It com-\nprises a total of 28,408 images and 45,336 questions.\n\u2022 ST-VQA [4]. ST-VQA is designed to emphasize the\nsignificance of exploiting the semantic information\npresent within images in textual form during the VQA\nprocess. It comprises tasks of diverse difficulties, for\nwhich recognizing the scene text and performing nec-\nessary reasoning is required to generate the answer.\nThe dataset comprises 23,038 images sourced from\nmultiple public datasets and 31,791 question-answer\npairs.\n\u2022 ChartQA [24]. ChartQA is a benchmark of question-\nanswering about logical and visual reasoning on\ncharts. It consists of 20,882 charts curated from four\ndifferent online sources, 9,608 manual written ques-\ntions, as well as 23,111 questions automatically gen-\nerated with T5 according to human-written chart sum-\nmaries.\n\u2022 InfographicVQA(InfoVQA) [26].\nThe task of In-\nfoVQA is to answer questions centering on a given in-\nfographic image. The answers to most questions can\nbe extracted from the given documents, while the an-\nswers to a small percentage of questions are not extrac-\ntive. There are 5K Images collected from the Internet\nand 30K manually annotated questions in the dataset.\n\u2022 DocVQA [25].\nDocVQA focuses on question-\nanswering given a document image. The answer for\nquestions is often a span of text from the given docu-\nments. There are 12K images and 50K manually an-\nnotated questions in the datasets.\n2.3. GUI Agent\nCogAgent is evaluated on two GUI agent datasets,\nMind2Web and Android in the Wild (AITW), correspond-\ning to computer agent and smartphone agent respectively.\n\u2022 Mind2Web [10]. Mind2Web is designed to develop\nand evaluate web agents capable of executing intri-\ncate tasks on various websites based on language di-\nrections. While existing datasets for web agents com-\nmonly rely on simulated or overly simplified web data,\nMind2Web utilizes real-world websites and is anno-\ntated by human annotators. It gathers data from 137\nwebsites covering 31 domains, and collects over 2,000\nopen-ended tasks, each accompanied by a crowd-\nsourced action sequence.\nIn mind2web, evaluated agents are asked to accom-\nplish a designated task on a chosen website by per-\nforming a sequence of actions. Each instance (i.e. a\nspecific task) in Mind2Web contains a task description,\naction sequence, and webpage snapshots. Each action\nin the sequence is a (Target element, Operation) pair,\nand Operation includes Click, Type (with additional\nvalue), and Select (with additional value). Each action\nis paired with the concurrent webpage snapshots in a\nvariety of formats including raw HTML code, DOM\ntree, screenshot, etc.\nAs for CogAgent, we choose\nscreenshot images as the input representation of web-\nsites; as for other language-based agents, HTML is\nchosen as the input representation.\nFollowing Deng et al. [10] and Zeng et al. [42], we for-\nmalize the problem as: first choose the target webpage\nelement among top-k (k=10 or 50) candidates, then\npredict specific operations. The top-k candidates are\nprovided by the candidate generation model in Deng\net al. [10]. Step success rate (step SR) is reported on 3\nout-of-domain test sets (cross-website, cross-domain,\n12\ncross-task) as metric. Only predictions with the same\ntarget element and operation as the ground truth are\nregarded as correct.\n\u2022 Android in the Wild (AITW) [31]. AITW is con-\nstructed to develop and evaluate Android device-\ncontrol systems that are capable of understanding and\nacting upon human natural language instructions by di-\nrectly manipulating the device\u2019s user interface. This\ndataset significantly surpasses others in its category in\nterms of size, encompassing 715k episodes across 30k\ndistinct instructions, and covering four Android ver-\nsions (v10\u201313). It also includes eight types of devices,\nranging from Pixel 2 XL to Pixel 6, each with differ-\nent screen resolutions. AITW consists of five subsets:\nGoogleApps, Install, WebShopping, General, and Sin-\ngle. The subsets have distinct tasks, while are in the\nsame data format. Each episode (i.e. action sequence)\ncomprises three components: a goal instruction pro-\nvided in natural language, a user action sequence, and\na corresponding screenshots sequence.\nAs for screenshots, AITW only provides screenshot\nimages and does not provide tree-based representa-\ntions of UI. This is because a large portion of them\nin Smartphone applications are of low quality or even\ndo not exist, and adopting the tree-based representa-\ntion would strongly limit agents\u2019 applications. For vi-\nsual agents, screenshots are provided to the agents in\nimage format; for language-model-based agents eval-\nuated by Zhan and Zhang [43], the textual representa-\ntions of OCR and icons formatted in HTML syntax are\nprovided. As for actions, AITW considers a variety of\naction types including tapping, swiping, typing, going\nhome, going back, entering, etc. For each action, mod-\nels are required to predict the exact action type; for tap,\nswipe and type, models are further required to predict\nthe position, direction, and content to be typed, respec-\ntively. The detailed standard for computing matching\nscores is provided in Rawles et al. [31].\n3.\nDerivation\nof\nAcceleration\nfor\nHigh-\nResolution Cross-Module\nSuppose that LIlo, LIhi and LT are the lengths of the low-\nresolution image, high-resolution image and text sequences.\nLet Hcross, Hdec be the number of attention heads in cross-\nattention and self-attention, and dcross, ddec be the dimension\nof each attention head.\nIf using our high-resolution cross-module, the computa-\ntional complexity of attention is\nTimproved = O\n\u0000(LIlo + LT )LIhiHcrossdcross\n+ (LIlo + LT )2Hdecddec\n\u0001\n.\n(5)\nIf not utilizing the high-resolution cross-module and\ndirectly substituting low-resolution images with high-\nresolution ones, the computational complexity would be\nToriginal = O\n\u0000(LIhi + LT )2Hdecddec\n\u0001\n.\n(6)\nThe reduction factor of the computational complexity in\nattention, Toriginal/Timproved, equals to\n(LIhi + LT )2Hdecddec\n(LIlo + LT )LIhiHcrossdcross + (LIlo + LT )2Hdecddec\n(7)\n= LIhi + LT\nLIlo + LT\n(LIhi + LT )Hdecddec\nLIhiHcrossdcross + (LIlo + LT )Hdecddec\n(8)\n= LIhi + LT\nLIlo + LT\n(LIhi + LT ) Hdecddec\nHcrossdcross\nLIhi + (LIlo + LT ) Hdecddec\nHcrossdcross\n(9)\n\u2022 Case 1: LIlo, LT \u226a LIhi.\nGiven that LIlo is much smaller than LIhi, when LT\nalso satisfies being much smaller than LIhi, both\nLIlo\nLIhi\nand\nLT\nLIhi become first-order small quantities. If con-\nducting a 0-th order approximation for the complexity\nreduction factor, we obtain:\nToriginal\nTimproved\n=\nLIhi(1 + LT\nLIhi )\nLIlo + LT\nLIhi(1 + LT\nLIhi ) Hdecddec\nHcrossdcross\nLIhi(1 +\nLIlo+LT\nLIhi\nHdecddec\nHcrossdcross )\n(10)\n=\nLIhi(1 + LT\nLIhi )\nLIlo + LT\n(1 + LT\nLIhi ) Hdecddec\nHcrossdcross\n1 +\nLIlo+LT\nLIhi\nHdecddec\nHcrossdcross\n(11)\n\u2248\nLIhi\nLIlo + LT\n(12)\n\u2022 Case 2: Our implementation.\nIn our implementation, dcross = 32, Hcross = 32, and\nwe inherits ddec = 128, Hdec = 32 from CogVLM-\n17B. Both high- and low-resolution encoders patchify\nimages with 14 \u00d7 14-pixel patches, thus LIhi = 6400,\nLIlo = 256. Subsitute the values to Eq. (9), we obtain\nToriginal\nTimproved\n=6400 + LT\n256 + LT\n4(6400 + LT )\n6400 + 4(256 + LT )\n(13)\n>6400 + LT\n256 + LT\n(14)\nThat is to say, our method leads to at least\nLIhi+LT\nLIlo+LT =\n6400+LT\n256+LT \u00d7 acceleration which is a stringent lower\nbound.\nTaking pre-training as an example, with\nLT typically below 512 in earlier stages, our high-\nresolution cross-module can get an over 25\u00d7 reduction\nin computational cost.\n13\n4. Performance Analysis on AITW\nAndroid in the Wild (AITW) dataset is a large-scale dataset\nfor Android device agents.\nOur analysis centers on in-\nstances where the CogAgent\u2019s responses diverge from the\nannotated ground truth, focusing on these discrepancies.\nIn our analysis, we conducted a sampling and manual\nverification of cases where there was a discrepancy between\nthe model\u2019s responses and the labeled answers. We sampled\ninstances and discovered that in 42% of these, the model\u2019s\nresponse actually represented an alternative correct method,\nrather than an error. This phenomenon is common due to the\nmultiplicity of ways to accomplish a function on a smart-\nphone. For example, many phones are equipped with both\nGoogle\u2019s app and a Google search bar, and searches can be\nconducted using either. During the use of smartphones, hu-\nman users may exhibit simultaneous needs for both in-depth\nexploration and extensive browsing. For example, when\nbrowsing news, CogAgent might identify relevant results on\nthe current page and select them immediately, while some\nusers may prefer to scroll through the entire page first. Fur-\nthermore, the data collection for annotations might involve\nredundant or overly detailed steps, whereas CogAgent can\ndirectly find the most efficient path. Fig. 4 illustrates these\ntypical examples of multiple correct answers.\nGiven the nature of smartphone usage, wherein accom-\nplishing a task often involves multiple viable paths, it is ad-\nvisable for future studies to explore a virtual environment\nwith an evaluation of the accuracy of task completion fol-\nlowing a sequence of actions, aligning more closely with\nreal-world human usage scenarios.\nCogAgent:\ntap [[0.512, 0.403]].\nUser:\nSet an alarm for 3pm.\nGround Truth:\ntap [[0.285, 0.455]].\nCogAgent:\ntap [[0.672, 0.784]].\nUser:\nOpen a new Chrome incognito tab.\nGround Truth:\ntap [[0.528, 0.870]].\nCogAgent:\ntap [[0.318, 0.246]].\nUser:\nWhat's the news in Brazil?\nGround Truth:\nscroll down.\nCogAgent:\ntap [[0.434, 0.190]].\nUser:\nWhat's the price of the Galaxy \nphone on eBay?\nGround Truth:\nscroll right.\nFigure 4. Instances in the AITW dataset where CogAgent provides\naccurate responses that differ from the annotated ground truth.\n5. Samples of Pre-train Data\nSamples of pre-train data are listed in this section, including\ndata for text recognition (Fig. 5), visual grounding (Fig. 6),\nand webpage-html pairs (Fig. 7).\nFigure 5. Samples of pre-train data for text recognition.\n14\nFigure 6. Samples of pre-train data for visual grounding.\nFigure 7. Samples of webpage-html pairs.\n15\n6. Details of Fine-Tuning Data\n6.1. Human annotation\nTo enable CogAgent to function as an agent on various apps\nand websites, we assembled a team of over ten annotators\nto collect and label screenshots. To ensure the quality of\nthe annotations and prevent overly simplistic, homogenized\ncontent, the annotation process was divided into two phases.\nIn the first phase, annotators were required to indepen-\ndently select from a pool of apps and websites, and capture\nscreenshots of pages strongly relevant to the main function\nof that app/website, with no less than 20 screenshots for\neach app/website. Subsequently, for each screenshot, the\nannotators would label them according to the following as-\npects:\n1. Five buttons. List the names of five clickable buttons\nfrom the screenshots. The types of buttons should be\ndiverse.\n2. Three clickable areas. List three clickable areas from\nthe screenshots, e.g. text boxes, search boxes, click-\nable images, hyperlinks, etc.\n3. Two questions extracting information from the im-\nage. Ask questions involving the textual information\nin the images. For instance, \u201cIn what year did the user\nin the image register?\u201d .\n4. One operation requirement. What function of the\nwebpage/app would you use in this interface? For ex-\nample, adding the comment: \u201ccool!\u201d. The requirement\nshouldn\u2019t be vague such as \u201cadding comment\u201d.\nIn the second phase, annotators are required to pro-\nvide grounding annotation for the questions and operational\nmethods proposed in the first phase (the aforementioned\nparts 3 and 4). For example, for Figure 8, a possible an-\nnotation would be:\n1. Buttons: Back; Search; Subscribed; Home; Library.\n2. Clickable areas: Avatar in the middle top; Video pre-\nview in the middle bottom; Personal profile.\n3. Question 1: Based on the page, how many followers\ndoes this author have?\nAnswer: According to the personal profile at the top\n[[013,568,802,188]], this author has 4.97M followers.\nQuestion 2: Based on the page, how many videos has\nthis author posted?\nAnswer:\nAccording to the personal profile at the\ntop [[013,568,802,188]], this author has posted 502\nvideos.\nFigure 8. Samples of human-collected screenshot.\n4. Operation requirement: Based on the page, write out\nthe steps to complete the following function: Follow\nthis author.\nAnswer: Click on Subscribed [[049,826,728,078]] to\nfollow this author.\n6.2. Conversion of Agent Datasets\nTo convert Mind2Web to natural language with GPT4, we\nuse the following prompt:\nImagine that you are a robot operating a computer.\nLike how humans operate the computer, you can move\nthe mouse, click with the mouse, or type some texts\nwith the keyboard.\n**Your ultimate task is: \u201cFind the lowest-priced\nround trip flight with hotel on May 2 from Kathmandu,\nNepal KTM to Shanghai, China PVG and return on\nMay 5. Book a double room and check out with the\ndefault flights.\u201d.**\n16\nYou are given previous actions: (format: element \u2192\noperation)\n1. [link] Flight + Hotel \u2192 CLICK,\n2.\n[textbox] Where from?\n\u2192 TYPE: KATH-\nMANDU,\n3. [div] Tribhuvan Intl Airport (KTM), Nepal \u2192\nCLICK,\n4. [textbox] Where to? \u2192 TYPE: SHANGHAI,\n5.\n[div] Pudong Intl Airport (PVG), China \u2192\nCLICK,\n6. [span] Sat 8 Apr - Sat 15 Apr \u2192 CLICK,\n7. [checkbox] 2 May 2023 \u2192 CLICK,\n8. [checkbox] 5 May 2023 \u2192 CLICK,\nFurthermore, you **secretly** know the following\nsteps of action. **(But you have to pretend this is the\nresult of your derivation and you don\u2019t know it be-\nfore)**\n1. [button] Search \u2192 CLICK,\n2. [button] Recommended \u2192 CLICK,\n3. [div] Lowest Price \u2192 CLICK,\n4. [svg] \u2192 CLICK,\n5. [button] Choose your room \u2192 CLICK,\n6.\n[button] Book Business Double Room A \u2192\nCLICK,\n7. [svg] \u2192 CLICK,\n8. [svg] \u2192 CLICK,\n9. [button] Choose \u2192 CLICK\nPlease formulate an operational guide for future op-\nerations for solving the task. The guide includes:\n1. Plan: A **multi-step future** plan that does\nnot involve low-level operations **(start from cur-\nrent screen and action, DON\u2019T include previous ac-\ntions)**; steps indexed by numbers. **Be sure to pre-\ntend that you don\u2019t know the future interface and ac-\ntions, and don\u2019t know the elements not existing on the\ncurrent screen.**\n2.\nAction:\nThe specific **immediate** action\nthat needs to be taken (Corresponding to the** very\nnext action** [button] Search \u2192 CLICK at the box\n{\u201cx left\u201d: 0.876, \u201cy left\u201d: 0.308, \u201cwidth\u201d: 0.063,\n\u201cheight\u201d: 0.034}), which should be detailed enough to\ndirectly control the operation, such as specifying the\nexact location (or box) to click, the exact content of\nthe text to type, the item to select.\n**Output Format:** A JSON dictionary strictly\nfollowing the format:\n{ \u201cplan\u201d:\n\u201c...<Your Plan Here>\u201d,\n\u201caction\u201d:\n\u201c...<Your Action Here>\u201d, \u201coperation\u201d:\n\u201c[button]\nSearch \u2192 CLICK at the box \u201cx left\u201d: 0.876, \u201cy left\u201d:\n0.308, \u201cwidth\u201d: 0.063, \u201cheight\u201d: 0.034\u201d} **Please do\nnot output any content other than the JSON format.\nDon\u2019t mention elements that only appear in HTML\nsuch as svg, div.**\nThe returned answer from GPT-4 would be:\n{ \u201cplan\u201d: \u201c1. After searching, you\u2019ll see a list of flight\nand hotel packages. 2. Review the recommended op-\ntions, usually highlighted for convenience. 3. Focus\non finding the lowest priced option. You might need to\nnavigate through various sorting or filtering features\nto do this. 4. Once you\u2019ve found the lowest priced op-\ntion, select it to view more details. 5. In the detailed\nview, you\u2019ll have to choose your room type. Look for\noptions that fit your requirements, like a business dou-\nble room. 6. After selecting the room, proceed to fi-\nnalize your booking. This might involve confirming\nyour choice and entering any required information. 7.\nReview all the details carefully before finalizing the\nbooking to ensure everything is correct.\u201d,\n\u201caction\u201d: \u201cClick the \u2019Search\u2019 button to proceed\nwith the search based on the entered criteria. This but-\nton is located towards the right side of the screen.\u201d,\n\u201coperation\u201d: \u201c[button] Search \u2192 CLICK at the\nbox {\u201cx left\u201d: 0.876, \u201cy left\u201d: 0.308, \u201cwidth\u201d: 0.063,\n\u201cheight\u201d: 0.03}\u201d }\n7. More Generated Samples of CogAgent\nTo further demonstrate the capabilities of CogAgent, we\nprovide additional generated samples in this section.\nFigure 9. More generated samples of CogAgent.\n17\nFigure 10. More generated samples of CogAgent.\n18\nFigure 11. More generated samples of CogAgent.\n19\nFigure 12. More generated samples of CogAgent.\n20\nFigure 13. More generated samples of CogAgent.\n21\nFigure 14. More generated samples of CogAgent.\n22\nFigure 15. More generated samples of CogAgent.\n23\nFigure 16. More generated samples of CogAgent.\n24\nFigure 17. More generated samples of CogAgent.\n25\nFigure 18. More generated samples of CogAgent.\n26\nFigure 19. More generated samples of CogAgent.\n27\n"
  },
  {
    "title": "VideoLCM: Video Latent Consistency Model",
    "link": "https://arxiv.org/pdf/2312.09109.pdf",
    "upvote": "21",
    "text": "VideoLCM: Video Latent Consistency Model\nXiang Wang1\u2217 Shiwei Zhang2\u2020 Han Zhang3 Yu Liu2 Yingya Zhang2 Changxin Gao1 Nong Sang1\u2020\n1Key Laboratory of Image Processing and Intelligent Control,\nSchool of Artificial Intelligence and Automation, Huazhong University of Science and Technology\n2Alibaba Group\n3Shanghai Jiao Tong University\n{wxiang,cgao,nsang}@hust.edu.cn, {zhangjin.zsw,ly103369,yingya.zyy}@alibaba-inc.com, hzhang9617@gmail.com\n\u0003Young man with dwarfism \nrecording a vlog at home\u0004\nText-to-video generation\n\u0003Beef burger in close up\u0004\n\u0003Beautiful woman playing \nwith her hair\u0004\n\u0003The Close up of Smiling \nWoman Winking Eyes, \nWhite Background\u0004\n\u0003Monarch butterfly \ndrinking nectar\u0004\n\u0003Fruitfully young red \ngrapes hanging on vineyard, \nhanging on a bush in a \nbeautiful sunny day \u0004\nCompositional video synthesis\n\u0003woman lying and \npeacefully sleeping on sofa\u0004\n\u0001First frame\u0002\nFigure 1. Video examples synthesized by the proposed VideoLCM with 4 inference steps. VideoLCM is a plug-and-play technique and\ncan be integrated into text-to-video generation and compositional video synthesis paradigms.\nAbstract\nConsistency models have demonstrated powerful capa-\nbility in efficient image generation and allowed synthesis\nwithin a few sampling steps, alleviating the high computa-\ntional cost in diffusion models. However, the consistency\n\u2217 Intern at Alibaba Group.\n\u2020 Corresponding authors.\nmodel in the more challenging and resource-consuming\nvideo generation is still less explored.\nIn this report,\nwe present the VideoLCM framework to fill this gap,\nwhich leverages the concept of consistency models from\nimage generation to efficiently synthesize videos with min-\nimal steps while maintaining high quality.\nVideoLCM\nbuilds upon existing latent video diffusion models and in-\n1\narXiv:2312.09109v1  [cs.CV]  14 Dec 2023\ncorporates consistency distillation techniques for training\nthe latent consistency model. Experimental results reveal\nthe effectiveness of our VideoLCM in terms of computa-\ntional efficiency, fidelity and temporal consistency. Notably,\nVideoLCM achieves high-fidelity and smooth video synthe-\nsis with only four sampling steps, showcasing the potential\nfor real-time synthesis. We hope that VideoLCM can serve\nas a simple yet effective baseline for subsequent research.\nThe source code and models will be publicly available.\n1. Introduction\nRecently, the field of video generation has witnessed\ntremendous advancements in synthesizing photo-realistic\nand temporally coherent video content, especially with the\ndevelopment of diffusion models [4, 6, 14, 17, 38, 44,\n46, 47, 54, 58, 60].\nTraditional diffusion-based meth-\nods such as videoLDM [5], Make-A-Video [38] and Mod-\nelScopeT2V [38], have achieved significant performance by\nincorporating additional temporal layers into existing image\ndiffusion models [30, 31] to handle the temporal dynamics\nin videos. Nevertheless, these diffusion-based approaches\ninevitably require substantial sampling steps to synthesize\nvideos during inference, e.g., 50-step DDIM sampling [40].\nThis limitation hinders the efficient and rapid synthesis of\nhigh-quality videos.\nTo address the challenge of high sampling cost in dif-\nfusion models, the concept of consistency models has been\nintroduced in image generation [23, 24, 41, 53], achieving\nremarkable progress by enabling efficient image synthesis\nwith a minimal number of steps (e.g., 4 steps vs. 50 steps).\nDespite its success, the application of the consistency model\nin the domain of video synthesis still remains largely unex-\nplored.\nTo fill this research gap, we propose the VideoLCM\nframework. Our method builds upon existing latent diffu-\nsion models in video generation and leverages the idea of\nconsistency distillation to train a video latent consistency\nmodel. By incorporating the VideoLCM framework, we\naim to alleviate the need for extensive sampling steps while\nmaintaining high-quality video synthesis. The quantitative\nand qualitative results demonstrate the effectiveness of our\napproach. Remarkably, our method achieves high-fidelity\nvideo synthesis with only 4\u223c6 sampling steps, showcasing\nits potential for fast and real-time synthesis. In comparison,\nprevious methods such as ModelScopeT2V [44] and Vide-\noLDM [5] typically require approximately 50 steps based\non the DDIM solver to achieve similarly satisfactory re-\nsults. In addition to text-to-video generation, we further\nextend the consistency model to compositional video syn-\nthesis. Experimental results indicate that in compositional\nvideo synthesis tasks, such as compositional depth-to-video\nsynthesis, VideoLCM can produce visually satisfactory and\ntemporally continuous videos with even fewer steps, such as\njust 1 step.\nIn summary, the proposed VideoLCM bridges the gap\nbetween diffusion models and consistency models in video\ngeneration, enabling efficient synthesis of high-quality\nvideos. By exploring the potential of consistency models\nin video generation, we aim to contribute to the field of fast\nvideo synthesis and provide a simplified and effective base-\nline for future research.\n2. Related Work\nThe relevant fields related to this work include text-to-\nimage generation, consistency model, and video generation.\nNext, we provide a brief review of these fields.\nText-to-image generation.\nIn recent years, significant\nprogress has been made in image generation with the de-\nvelopment of generative models [7, 10, 13, 15, 19, 22,\n34, 37], especially with the emergence of diffusion mod-\nels [20, 27, 29, 31\u201333]. Typical methods for text-to-image\ngeneration, such as DALLE-2 [30], propose a two-stage ap-\nproach where the input text is first converted into image em-\nbeddings using a prior model, followed by the generation of\nimages based on these embeddings. Stable Diffusion [31]\nintroduces a VAE-based approach in the latent space to de-\ncrease computational demand and optimizes the model with\nlarge-scale datasets [36]. Subsequent methods like Control-\nNet [57] and Composer [18] have incorporated additional\nconditional inputs, such as depth maps or sketches, for spa-\ntially controllable image synthesis.\nConsistency model. To alleviate the limitation of requiring\na large number of inference steps in diffusion models, the\nconsistency model [41] has been developed. Built upon the\nprobability flow ordinary differential equation (PF-ODE) ,\nconsistency models learn to map any point at any time step\nto the starting of the trajectory, i.e., the original clean image.\nThe consistency model facilitates efficient one-step image\ngeneration without sacrificing the advantages of multi-step\niterative sampling, thereby enabling more high-quality re-\nsults through multi-step inference. Building upon the foun-\ndation of the consistency model, LCM [23] explores con-\nsistency models in the latent space to save memory con-\nsumption and improve inference efficiency. Subsequently,\nseveral methods [24, 35, 53] have also investigated efficient\ngeneration techniques and achieved impressive results. In-\nspired by the efficiency of the consistency model and its\nsignificant success in image generation, we extend the ap-\nplication of the consistency model to the domain of video\ngeneration.\nVideo generation. The pursuit of visually appealing and\ntemporally coherent synthesis is crucial in video genera-\ntion.\nEarly methods [3, 39, 42, 48\u201350] primarily relied\non generative adversarial networks (GANs), resulting in\n2\nEMA\n\ud83d\udd25\nStudent\n(consistency model)\nTeacher\n(diffusion model)\nStudent\n(consistency model)\n\ufe61\n\ud835\udf3d\n\ud835\udf3d\ufe61\n\ud835\udc65!\nForward\ndiffusion\n\ud835\udc65\"#$\n\"\ud835\udc65\"\n\ud835\udc65\"#$\n\ud835\udc65!\n%&'(\n\ud835\udc65!\n\u2217 %&'(\nLoss\n\ud835\udc65!\n*'+,-'&\nDDIM \nstep\n\ud83d\udd25 Tunable\nFrozen\n(Source video)\nFigure 2. The overall pipeline of the proposed VideoLCM. Given a source video x0, a forward diffusion operation is first performed\nto add noise to the video. Then, the noised xn+k is entered into the student and teacher model to predict videos. \u02c6xn is estimated by the\nteacher model and fed into the EMA student model. To learn self-consistency, a loss is imposed to constrain the output of the two student\nmodels to be consistent. The whole consistency distillation is conducted in the latent space, and conditional guidance is omitted for ease of\npresentation. The teacher model is a video diffusion model, and the student model shares the same network structure as the teacher model\nand is initialized with the parameters of the teacher model.\npoor video quality and limited generalization to unseen do-\nmains [5]. With the rise of diffusion models [31], which\nprovide stable training and impressive results, recent ap-\nproaches [1, 4, 6, 12, 14, 16, 21, 25, 26, 38, 44, 45, 51, 52,\n55, 56, 58, 61] have started exploring diffusion-based video\ngeneration. To generate temporally consistent videos, main-\nstream text-to-video methods such as ModelScopeT2V [44]\nand VideoLDM [5] achieve long-term temporal modeling\nby inserting temporal modules into a 2D UNet and train-\ning on large-scale datasets [2]. There are also some meth-\nods [26, 38, 43, 58, 61] focusing on using super-resolution\nand frame interpolation models to generate more realistic\nvideos. To achieve spatiotemporal controllability, control-\nlable video synthesis methods [8, 9, 11, 28, 46, 59, 60] have\nbeen proposed. In particular, VideoComposer [46] presents\na compositional video synthesis paradigm that enables flex-\nible control over the generated videos through textual, spa-\ntial, and temporal conditions. Despite significant advance-\nments, these methods rely heavily on extensive iterative de-\nnoising processes to obtain satisfactory results, posing chal-\nlenges for fast video generation. To address this issue, in\nthis work, we propose the VideoLCM framework based on\nconsistency models for fast and efficient video synthesis.\n3. Method\nThe proposed VideoLCM builds upon the foundations\nof latent consistency models. We first briefly describe the\npreliminaries about latent consistency models. Then, we\nwill present the details of the proposed VideoLCM. The\noverall structure of VideoLCM is displayed in Fig. 2.\n3.1. Preliminaries\nTo achieve fast image generation, song et al. [41] brings\ninto the conception of the consistency model, which aims to\noptimize a model that learns to map any point at any time\nstep to the starting of the PF-ODE trajectory. Formally, the\nself-consistency property can be formulated as:\nf\u03b8(xt, t) = f\u03b8(xt\u2032, t\u2032), \u2200t, t\u2032 \u2208 [\u03f5, T]\n(1)\nwhere \u03f5 is a time step, T is the overall denoising step, and\nxt denotes the noised input.\nTo accelerate the training and extract the strong prior\nknowledge of the established diffusion model [31], consis-\ntency distillation is usually adopted:\nL(\u03b8, \u03b8\u2217; \u03a6) = E[d(f\u03b8(xtn+1, tn+1)), f\u03b8\u2217(\u02c6xtn, tn))]\n(2)\nwhere \u03a6 means the applied ODE solver and the model pa-\nrameter \u03b8\u2217 is obtained from the exponential moving average\n(EMA) of \u03b8. \u02c6xtn is the estimation of xtn:\n\u02c6xtn \u2190 xtn+1 + (tn \u2212 tn\u22121)\u03a6(xtn+1, tn+1)\n(3)\nLCM [23] conducts the above consistency optimization in\nthe latent space and applies classifier-free guidance [15] in\nEq. (3) to inject control signals, such as textual prompts.\nFor more details, please refer to the original works [23, 41].\n3.2. VideoLCM\nFollowing LCM, the proposed VideoLCM is also estab-\nlished in the latent space to reduce the computational bur-\nden. To leverage the powerful knowledge within large-scale\n3\npretrained video diffusion models and speed up the train-\ning process, we apply the consistency distillation strategy\nto optimize our model. Note that the pretrained video diffu-\nsion model can be the text-to-video generation model (e.g.,\nModelScopeT2V [44]) or the compositional video synthesis\nmodel (e.g., VideoComposer [46]).\nIn VideoLCM, we apply DDIM [40] as the basic ODE\nsolver \u03a8 to estimate \u02c6xtn:\n\u02c6xtn \u2248 xtn+1 + \u03a8(xtn+1, tn+1, tn, c)\n(4)\nwhere c means the conditional inputs, which can be textual\nprompts in text-to-video generation or multiple combined\nsignals in compositional video synthesis task.\nSince classifier-free guidance is pivotal in synthesizing\nhigh-quality content [15], we also leverage classifier-free\nguidance in the consistency distillation stage and use a fac-\ntor w to control the guidance scale:\n\u02c6xtn \u2248xtn+1 + (1 + w)\u03a8(xtn+1, tn+1, tn, c)\n\u2212 w\u03a8(xtn+1, tn+1, tn, \u03d5)\n(5)\nIn LCM [23], w is variable and can be fed into the network\nfor modulation, but this changes the structure of the initial\nnetwork because a module encoding w needs to be added.\nIn order to keep the initial parameters and design of the con-\nsistency model consistent with the teacher diffusion model,\nwe train the consistency model with a fixed value of w, such\nas 9.0. Note that classifier-free guidance is only applied to\nthe teacher diffusion model in training and is not required\nduring the inference process of the consistency model.\nVideoLCM is a plug-and-play technique compatible\nwith text-to-video generation and compositional video syn-\nthesis. During the inference phrase, we can sample 4\u223c6\nLCM steps to produce plausible results on text-to-video\ngeneration.\nFor compositional video synthesis, take the\ncompositional depth-to-video task as an example, 2\u223c4 steps\nare sufficient, and sometimes even 1 step.\n4. Experiments\nIn this section, we first introduce the details of the exper-\nimental setup. Then, quantitative and qualitative compar-\nisons will be represented to evaluate the effectiveness of the\nproposed VideoLCM framework.\n4.1. Experimental setup\nDatasets. We train our video consistency model on two\nwidely used datasets, i.e., WebVid10M [2] and LAION-\n5B [36].\nWebVid10M is a video-text dataset containing\napproximately 10.7M videos. To further enhance tempo-\nral diversity and improve visual quality, we additionally uti-\nlize about 1M internal video-text data to train VideoLCM.\nLAION-5B is an image-text dataset that is used to provide\nhigh-quality visual-text correspondence.\nTable 1. Inference latency on text-to-video generation task. All\nexperiments are performed on an NVIDIA A100 GPU. The infer-\nence overhead of generating eight videos at a time is reported.\nMethod\nStep\nResolution\nLatency\nBaseline\nDDIM 50-step\n16 \u00d7 256 \u00d7 256\n60s\nVideoLCM\nLCM 4-step\n16 \u00d7 256 \u00d7 256\n10s\nBaseline\nDDIM 50-step\n16 \u00d7 448 \u00d7 256\n104s\nVideoLCM\nLCM 4-step\n16 \u00d7 448 \u00d7 256\n16s\nImplementation details.\nFor convenience, we directly\nleverage the existing pretrained video diffusion models in\nTF-T2V [47] as the teacher model and fix the network\nparameters in the consistency distillation process.\nTF-\nT2V [47] is a technique that exploits text-free videos to\nscale up video diffusion models and can be applied to main-\nstream text-to-video generation and compositional video\nsynthesis framework. The model structure of the consis-\ntency model is the same as the teacher diffusion model and\nis initialized with the teacher\u2019s model parameters. AdamW\noptimizer with a learning rate of 1e-5 is adopted to train\nVideoLCM. The EMA rate used in our experiments is 0.95.\nWe sample 16 frames and crop a 448 \u00d7 256 center region\nfrom each source video as model input. The training loss\nused in VideoLCM is a Huber loss by default. The entire\nnetwork is trained on 4 NVIDIA A100 GPUs (one for im-\nage and three for video), requiring approximately 4k train-\ning iterations to produce relatively reliable video generation\nresults.\n4.2. Time efficiency\nWe measure the inference time required for text-to-video\nsynthesis using our proposed VideoLCM and compare it\nwith the baseline method [47]. The comparative results are\nexhibited in Tab. 1. From the results, we can observe that\nsince our method merely requires 4 LCM steps for infer-\nence, it is much faster than the baseline method with 50\nDDIM steps. It should be noted that in addition to itera-\ntive denoising, the inference cost also includes text feature\nencoding, latent code decoding, etc. In addition, we notice\nthat VideoLCM saves more time on high-resolution genera-\ntion compared to baseline. For example, generating a video\nof 16 \u00d7 256 \u00d7 256 size saves 50 seconds (i.e., 60s \u2212 10s),\nwhile 16x448x256 saves 88 seconds. The above compari-\nson demonstrates the high efficiency of our approach.\n4.3. Ablation study on inference steps\nIn Fig. 3, we present the experimental visualization of\nvarying inference steps in the text-to-video task. The results\nindicate that when the sampling step is too small, such as\nstep = 1, the generated videos suffer from blurriness, with\nmany details being inaccurately represented, and the tem-\nporal structure of objects cannot be preserved. As the num-\nber of iteration steps increases, the visual quality gradually\n4\n\u0001Red apple on a paper bag\u0002\nLCM steps = 1\n\u0001Smiling successful bearded arabian businessman in suit enjoying phone chat\u0002\nLCM steps = 2\nDDIM steps = 50\n(Baseline)\nLCM steps = 4\nLCM steps = 6\nLCM steps = 8\nFigure 3. Ablation study on text-to-video task under varying steps. Larger steps generally yield better visual quality and time continuity.\n\u0001instant noodle spicy salad with pork on  plate - Asian food style\u0002\nLCM steps = 1\n\u0001The Close up of Smiling Woman Winking Eyes, White Background\u0002\nLCM steps = 2\nDDIM steps = 50\n(Baseline)\nLCM steps = 4\nLCM steps = 6\nLCM steps = 8\nDepth sequence\nFigure 4. Ablation study on compositional depth-to-video synthesis task under different inference steps. Since the additional depth\nsequence can provide prior guidance about structure and temporal, our method can produce plausible results with fewer steps or even only\none step.\n5\n\u0004Aerial view of Brown castle \nin Portofino\u0005\n\u0004Happy european woman \ndoctor with glasses in medical \nmask wearing white medical \ncoat and stethoscope\u0005\n\u0004Colorful Macaroons are \nrotating \u0001top view\u0002\u0005\n\u0004Positive black guy crossing \nfingers, hopes for fortune\u0005\n\u0004Rocks Splash Ocean Close \nUp\u0003 \u0001HD\u0002\u0005\n\u0004For tangerines in colander \nfalling water drops.\u0005\n\u0004Happy woman gives green \nseedling to little daughter at \nhome\u0005\n\u0004girl paints eyes with \nmascara\u0005\n\u0004Pretty Girl White Bikini \nWalking Beside Ocean\u0005\n\u0004Winter Carpathian \nlandscape, Christmas trees in \nthe snow\u0005\nFigure 5. Qualitative visualization results on text-to-video generation task. Videos are synthesized by performing 4 denoising steps.\nimproves, and the temporal structure can be better main-\ntained. For example, when using 4\u223c6 steps, comparable\nresults to DDIM\u2019s 50 steps can be achieved, while signifi-\ncantly reducing the sampling steps and improving the gen-\neration speed. Moreover, we additionally perform an abla-\ntion study on compositional depth-to-video synthesis. As\nillustrated in Fig. 4, we observe that with only a few steps,\nsuch as 1 step, the generated results can display good vi-\nsual quality. With an increase in the step size, many details\nbecame even more apparent. We attribute this phenomenon\nto the fact that compared to text-to-video generation, com-\npositional depth-to-video synthesis can leverage additional\nstructured control information, which reduces the difficulty\nof predicting videos from pure noise and enables achieving\nhigh-quality results with fewer steps. Empirically, we find\nthat 2\u223c4 steps can produce relatively stable compositional\nsynthesis contents. The results on both text-to-video and\ncompositional video generation demonstrate the effective-\nness of the proposed VideoLCM, achieving a good balance\nbetween quality and speed.\n4.4. More qualitative results\nTo comprehensively showcase the capabilities of our\nmethod in handling various video generation tasks, we\npresent additional visualization examples in Fig. 5. These\nresults are generated with 4 LCM steps. From the visual-\nization results, it is evident that our method achieves im-\npressive generation outcomes in terms of visual quality and\ntemporal continuity.\nIn Figs. 6 to 8, we present more results of compositional\n6\n\u0001Portrait of infant boy \nlooking to camera\u0002\n\u0001A big tusker elephant \nwalking across camera\u0002\n\u0001Timelapse sunset floating \nmosque on the sea at \nBagan Luar\u0002\n\u0001Young handsome bearded \nPersian businessman \nagainst green background\u0002\n\u0001Blue h2 hydrogen fuel \ntank renewable Ecological \nfuture Ecology concept grid \nenergy storage 4k\u0002\n\u0001Tall black athlete \ncarrying basket ball in \nhands, thinking about \ngame strategy\u0002\n\u0001Man pushing trolley full \nof goods in diy warehouse, \nhome improvement project\u0002\n\u0001Rice sprinkled with \nbonito\u0002\nFigure 6. Qualitative results on compositional depth-to-video synthesis task. Videos are synthesized by performing 4 denoising steps.\nvideo generation using 4 sampling steps, including com-\npositional depth-to-video synthesis (Fig. 6), compositional\nsketch-to-video synthesis (Fig. 7), and compositional video\ninpainting (Fig. 8). These qualitative results demonstrate\nstable generation quality and highlight the controllability of\nour method in accordance with the input conditioning. Our\nmethod can be applied to multiple mainstream video gen-\neration tasks mentioned above, revealing the generality of\nthe VideoLCM framework and the vast potential for other\nfuture applications.\n4.5. Limitations\nIn our VideoLCM, we explore the application of the\nconsistency model in video generation tasks, including\ntext-to-video generation and compositional video synthesis.\nHowever, there are certain limitations: 1) Our method re-\nlies on a strong teacher model as the distillation target. 2)\nThe consistency distillation process requires finetuning the\nmodel. While consistency distillation only requires a small\nnumber of training steps, it may lead to unsatisfactory re-\nsults when the training data for the teacher model is unavail-\n7\n\u0003Asian man stands \nconfidently and inspired \nwith his arms outstretched \nin rain background cloudy \nsky and lightning flashes\u0004\n\u0003A girl is standing in \nsmoke holding a mask in \nher hand. White smoke.\u0004\n\u0003A big tusker elephant \nwalking across camera\u0004\n\u0003MS, Gentoo Penguin \n\u0001Pygoscelis papua\u0002 walking \non snow, Antarctica\u0004\n\u0003Man wearing protective \nsuit showing palms with \ninscription\u0004\n\u0003Pleased brunette woman \ntalking by smartphone and \nlooking around\u0004\n\u0003beauty mixed race bride \nin studio\u0004\n\u0003View of ancient City wall \nat night, xian, shaanxi, \nChina\u0004\nFigure 7. Qualitative results on compositional sketch-to-video synthesis task. Videos are synthesized by performing 4 denoising steps.\nable or from different domains. 3) Even though our method\nreduces the inference steps to 4\u223c6, real-time video genera-\ntion, like image generation, is still not achieved. Exploring\nmore stable and efficient video generation algorithms while\nensuring the high quality of the generated video content is\na promising future direction.\n5. Conclusion\nIn this work, we propose the VideoLCM framework\nthat extents latent consistency models to the video gener-\nation field. Our approach leverages existing latent video\ndiffusion models and employs the consistency distillation\ntechnique to enable efficient and fast video synthesis. Ex-\nperimental results demonstrate the effectiveness of our ap-\nproach, with high-fidelity video synthesis achieved in just\nfour steps, showcasing its real-time synthesis potential com-\npared to prior methods requiring approximately 50 DDIM\nsteps. We hope that VideoLCM can serve as a simple yet\neffective baseline for subsequent research work.\nAcknowledgements. This work is supported by the Na-\n8\n\u0001Aucanada Lighthouse \nand beach in Majorca, \nBalearic Islands - Spain\u0002\n\u0001Portrait of three \nmillennial frenemies \nstanding on the midway at \na carnival\u0002\n\u0001Delicate white and pink \nflowers of the apple tree in \nthe wind in the rays of the \nwarm sun\u0002\n\u0001Excited older best \nemployee get appreciated \nby business team\u0002\n\u0001Langkawi, Malaysia\u0002\n\u0001Man wearing protective \nsuit showing palms with \ninscription\u0002\n\u0001Old church in England\u0002\nFigure 8. Qualitative visualizations on compositional video inpainting task. Videos are synthesized by performing 4 denoising steps.\ntional Natural Science Foundation of China under grant\nU22B2053 and Alibaba Group through Alibaba Research\nIntern Program.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 3\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 3, 4\n[3] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chel-\nlappa, and Hans Peter Graf. Conditional GAN with discrim-\ninative filter generation for text-to-video synthesis. In IJCAI,\npage 2, 2019. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575, 2023.\n2, 3\n[6] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. In ICCV, pages 23040\u201323050, 2023. 2, 3\n9\n[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T Freeman, Michael Rubinstein, et al. Muse: Text-\nto-image generation via masked generative Transformers.\narXiv preprint arXiv:2301.00704, 2023. 2\n[8] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffu-\nsion model for controllable video synthesis. arXiv preprint\narXiv:2304.14404, 2023. 3\n[9] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 3\n[10] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n2\n[11] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, pages 7346\u20137356, 2023. 3\n[12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, pages\n22930\u201322941, 2023. 3\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 27,\n2014. 2\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4\n[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3\n[17] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and\nLiefeng Bo. Animate anyone: Consistent and controllable\nimage-to-video synthesis for character animation.\narXiv\npreprint arXiv:2311.17117, 2023. 2\n[18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. ICML, 2023. 2\n[19] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling up\nGANs for text-to-image synthesis. In CVPR, pages 10124\u2013\n10134, 2023. 2\n[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007\u20136017, 2023. 2\n[21] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3\n[22] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng\nZheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao.\nCones: Concept neurons in diffusion models for customized\ngeneration. In ICML, 2023. 2\n[23] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 2, 3, 4\n[24] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von\nPlaten, Apolin\u00b4ario Passos, Longbo Huang, Jian Li, and Hang\nZhao. Lcm-lora: A universal stable-diffusion acceleration\nmodule. arXiv preprint arXiv:2311.05556, 2023. 2\n[25] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion: Decomposed diffusion models\nfor high-quality video generation. In CVPR, pages 10209\u2013\n10218, 2023. 3\n[26] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[28] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models.\nIn CVPR, pages\n18444\u201318455, 2023. 3\n[29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In ICML, pages 16784\u201316804. PMLR, 2022. 2\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 3\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023.\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 2\n[34] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 2\n10\n[35] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin\nRombach. Adversarial diffusion distillation. arXiv preprint\narXiv:2311.17042, 2023. 2\n[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeurIPS, 35:25278\u2013\n25294, 2022. 2, 4\n[37] Yujun Shen and Bolei Zhou. Closed-form factorization of la-\ntent semantics in GANs. In CVPR, pages 1532\u20131540, 2021.\n2\n[38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. ICLR, 2023. 2, 3\n[39] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. StyleGAN-v: A continuous video generator with the\nprice, image quality and perks of StyleGAN2.\nIn CVPR,\npages 3626\u20133636, 2022. 2\n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 2, 4\n[41] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. In ICML, 2023. 2, 3\n[42] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocoGAN: Decomposing motion and content for\nvideo generation. In CVPR, pages 1526\u20131535, 2018. 2\n[43] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n3\n[44] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2,\n3, 4\n[45] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv preprint arXiv:2305.10874, 2023. 3\n[46] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Videocomposer: Compositional video synthesis\nwith motion controllability. NeurIPS, 2023. 2, 3, 4\n[47] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing,\nBiao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and\nNong Sang. A recipe for scaling up text-to-video generation\nwith text-free videos. arXiv, 2023. 2, 4\n[48] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. G3an: Disentangling appearance and motion for\nvideo generation. In CVPR, pages 5264\u20135273, 2020. 2\n[49] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal GAN\nfor video generation. In WACV, pages 1160\u20131169, 2020.\n[50] Yuhan Wang, Liming Jiang, and Chen Change Loy. Styleinv:\nA temporal style modulated inversion network for uncondi-\ntional video generation. In ICCV, pages 22851\u201322861, 2023.\n2\n[51] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan,\nZhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and\nHongming Shan.\nDreamvideo:\nComposing your dream\nvideos with customized subject and motion. arXiv preprint\narXiv:2312.04433, 2023. 3\n[52] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, pages 7623\u20137633, 2023. 3\n[53] Jie Xiao, Kai Zhu, Han Zhang, Zhiheng Liu, Yujun Shen, Yu\nLiu, Xueyang Fu, and Zheng-Jun Zha. Ccm: Adding con-\nditional controls to text-to-image consistency models. 2023.\n2\n[54] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 2\n[55] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In CVPR, pages 18456\u201318466, 2023. 3\n[56] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 3\n[57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, pages 3836\u20133847, 2023. 2\n[58] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2, 3\n[59] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 3\n[60] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and\nJun Zhu. Controlvideo: Adding conditional control for one\nshot text-to-video editing. arXiv preprint arXiv:2305.17098,\n2023. 2, 3\n[61] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3\n11\n"
  },
  {
    "title": "Mosaic-SDF for 3D Generative Models",
    "link": "https://arxiv.org/pdf/2312.09222.pdf",
    "upvote": "14",
    "text": "Mosaic-SDF for 3D Generative Models\nLior Yariv1,3 *\nOmri Puny3\nNatalia Neverova1\nOran Gafni1\nYaron Lipman2,3\n1GenAI, Meta\n2FAIR, Meta\n3Weizmann Institute of Science\nAbstract\nCurrent diffusion or flow-based generative models for\n3D shapes divide to two: distilling pre-trained 2D image\ndiffusion models, and training directly on 3D shapes. When\ntraining a diffusion or flow models on 3D shapes a cru-\ncial design choice is the shape representation. An effective\nshape representation needs to adhere three design princi-\nples: it should allow an efficient conversion of large 3D\ndatasets to the representation form; it should provide a good\ntradeoff of approximation power versus number of parame-\nters; and it should have a simple tensorial form that is com-\npatible with existing powerful neural architectures. While\nstandard 3D shape representations such as volumetric grids\nand point clouds do not adhere to all these principles simul-\ntaneously, we advocate in this paper a new representation\nthat does. We introduce Mosaic-SDF (M-SDF): a simple\n3D shape representation that approximates the Signed Dis-\ntance Function (SDF) of a given shape by using a set of\nlocal grids spread near the shape\u2019s boundary. The M-SDF\nrepresentation is fast to compute for each shape individually\nmaking it readily parallelizable; it is parameter efficient as\nit only covers the space around the shape\u2019s boundary; and\nit has a simple matrix form, compatible with Transformer-\nbased architectures. We demonstrate the efficacy of the M-\nSDF representation by using it to train a 3D generative\nflow model including class-conditioned generation with the\n3D Warehouse dataset, and text-to-3D generation using a\ndataset of about 600k caption-shape pairs.\n1. Introduction\nImage-based generative models have rapidly advanced in\nrecent years due to improvements in generation methodolo-\ngies (e.g., Diffusion Models), model architecture and condi-\ntioning (e.g., Text-to-Image Models, Attention/Transformer\nlayers), and the consolidation of large, high quality im-\nage datasets. Although generation of 3D shapes have pro-\ngressed as well, it has not seen the same level of progress\ndemonstrated in image generation.\n*Work done while interning at Meta.\nCurrent works for 3D generation divide mostly into two\ngroups: Optimization based: 2D image generative priors\nare used for training/optimizing 3D shape [20, 35, 46]. The\nmain benefit is leveraging existing powerful image models,\nbut the generation process is expensive/slow as it requires\ntraining a new model for each sample. Furthermore, us-\ning only image priors is often insufficient to build consis-\ntent 3D shape (e.g., the Janus effect) [7]. Forward based:\nthe 3D shapes are generated by a forward process, such as\nsolving an Ordinary Differnetial Euqation (ODE), making\ngeneration process more efficient than optimization based\nmethods. However, forward based model are first trained\non a dataset of 3D shapes using, e.g., Diffusion or Flow\nModels, where to that end shapes are first transformed into\nsome canonical 3D representation, e.g., volumetric grid or\na point cloud. Forward based models work directly with 3D\nshapes and suffer from two limitations: first, they require\n3D shape datasets for training and these still lag behind im-\nage datasets in terms of quantity and quality. Second, in\ncontrast to images, 3D shapes do not occupy the full 3D\nspace and therefore an effective 3D shape representation is\nmore challenging to find/work with.\nIn this work we focus on forward based 3D generative\nmodels. To enable high quality, large scale 3D generation\nthe 3D shape representation of choice should adhere the fol-\nlowing design principles:\n(i) Preprocess efficiency: can be efficiently computed\nfor a large collection of shapes.\n(ii) Parameter efficiency: provides a good approxima-\ntion vs. parameter count trade-off.\n(iii) Simple structure: has a simple tensorial structure,\ncompatible with expressive neural architectures.\nExamining existing 3D representations used for training\ngenerative 3D models including volumetric grids [19, 31],\ntri-planes [10, 41, 45], point clouds [26, 33, 50, 54], meshes\n[23] and neural fields [15] we find that all are lacking in one\nor more of the above design principles. For example, vol-\numetric grids scale cubicaly and maintain redundant infor-\nmation in the entire space, while tri-planes tend to be more\nefficient but still encode full 3D tensors and require a neural\nnetwork trained jointly on the training set. Point clouds do\nnot provide a full surface information; meshes do not enjoy\n1\narXiv:2312.09222v1  [cs.CV]  14 Dec 2023\nModels to M-SDF \n(Algorithm 1)\nFlow Matching \n(Algorithm 2)\nTransformer\nLoss\n+\n-\nTransformer\nTransformer\nTransformer\n\u2026\nt\nSampling \n(Algorithm 3)\n1 \u2212 \u03c1t\nFigure 1. Method overview. Train (top): First we convert the\ndataset of shapes to M-SDF representations (Algorithm 1), next\nwe train a Flow Matching model with the M-SDF representations\n(Algorithm 2). Sampling (bottom): We random a noisy M-SDF\nand numerically solve the ode in equation 13 (Algorithm 3).\na simple tensorial structure in general; and neural fields are\nencoded by weight vector with non-trivial symmetries [32].\nThe goal of this work is to introduce Mosaic-SDF (M-\nSDF), a simple novel 3D shape representation that achieves\nall desired properties listed above. With M-SDF we are able\nto train a forward based flow generative model on a datasets\nof 3D shapes achieving high quality and diversity. In a nut-\nshell, M-SDF approximates an arbitrary Signed Distance\nFunction (SDF) with a set of small (7 \u00d7 7 \u00d7 7) volumetric\ngrids with different centers and scales. Namely, M-SDF of a\nsingle shape is a matrix X of dimension n \u00d7 d, where each\nrow represent a single grid, and it can be fitted to a given\nshape\u2019s SDF in less then 2 minutes with a single Nvidia\nA100 GPU. Furthermore, as X is a matrix representing a\nset it is compatible with existing expressive architectures,\nsimilar to the ones trained on points clouds [33]. Lastly,\nsince the grids are centered near the shape\u2019s surface M-SDF\nprovides a parameter efficient representation.\nWe have used the M-SDF representation to train a Flow\nMatching model [21] on two main datasets: subset of 3D\nWarehouse [14] consisting of \u223c50K 3D models divided to\n55 classes and a dataset of \u223c600K shapes and a matching\ntext description [27]. We find that M-SDF to be the method\nof choice for 3D geometry representation and facilitate sim-\nple, generalizable, high quality, forward based 3D genera-\ntive models.\n2. Related work\nOur work focuses on designing a representation that is bene-\nficial for 3D generation. Below we review relevant previous\nworks categorized by the type of representation they used\nfor 3D shapes.\nGrids and Triplanes. Several works suggested to train\na diffusion model to predict a grid-based representation,\nwhich can be either 3D voxel grids [13, 19, 31], or using\nthe Triplanes representation [10, 41, 45]. DiffRF [31] rep-\nresents each shape as a volumetric radiance field on a voxel\ngrid. [19] represents voxalized truncated signed distance\nfunctions (TSDF) encoded into a lower resolution features\ngrid. Neural Wavelet [13] also advocates voxel grid struc-\nture containing wavelet coefficients of a truncated SDF sig-\nnal. The main drawback of voxel grids is that they are re-\nstricted to a fixed and usually low resolution grids, mainly\ndue to their cubic scaling.\nThe cubic scaling of volumetric grids motivated the Tri-\nplane representation [3, 39] using lower dimensional grids\n(1D and 2D) and encode a 3D function as a small MLP ap-\nplied to sums of outer products. [10, 41, 45] utilize 2D dif-\nfusion model backbones on 3D training scenes represented\nby 2D feature planes. Despite the elegant correspondence\nto 2D images, prepossessing a large data set of 3D shapes\ninto Triplane representation is compute intensive, requires\nto learn a shared MLP and in some cases a shared auto-\nencoder [10, 45]. Compared to our representation, grid-\nbased representations contains many redundant empty vox-\nels, since the shape\u2019s surface usually occupied only a small\nfraction of the grid.\nNeural Fields. Following the considerable success of\nImplicit Neural Representations (INRs) for modeling 3D\nshapes [29, 30, 34], several works suggest a generative\nmodel that produces a parametrization of an implicit func-\ntion. [6, 53] employed a GAN to generate latent vector\nor volume representing an implicit function with a shared\nMLP. Shape-E [15] suggests a diffusion model that directly\npredicts the weights of a Multi-Layer Perceptron (MLP).\nHowever, training an MLP for each shape in a large dataset\nis compute intensive, and should consider the symmetries\nof MLP weights [32].\nPoint clouds. [26, 33, 50, 54] suggest to generate 3D\nPoint clouds, taking advantage of existing permutation-\nequivariant architectures [36, 43]. Although point-clouds\nare easy to compute during preprocess and hence suit-\nable for training on large datasets, post-processing it into\na smooth surface results in loss of details or requires ex-\ntensive number of points, which are hard to generate with\npermutation-equivariant models [44]. Point-E [33] suggests\nto train an additional upsampler diffusion model to scale\nthe size of the generated point cloud, however it still fails to\ndescribe thin structures.\n2\nOther representations. There are papers that suggested\nmethods that are related to our work and not fall into one\nof the categories above. [2] offers to generate a set of Volu-\nmetric Primitives [24] composed on top a coarse mesh, su-\npervised with another 3D generative model\u2019s outputs. [51]\nand [52] presented different representation for encoding oc-\ncupancy field using a set of either structured or unstructured\nlatent vectors. Similarly to our representation, they are com-\npatible with the transformer architecture [43], trained as the\ngenerative model. However, all these methods require a sig-\nnificant compute intensive preprocssing stage for fitting a\ngeneralized representation for a large scale dataset. In con-\ntrast, our representation is both compact, has a simple struc-\nture, interpretable, and can be formed independently and\nquickly for each shape in the dataset.\n3. Method\nIn this section we present our 3D shape representation based\non an efficient approximation of the Signed Distance Func-\ntion (SDF), we detail how it is computed for a dataset of\nshapes in a preprocess stage, and how it is used for training\na flow-based generative model.\n3.1. Mosaic-SDF Shape Representation\nWe advocate a simple and effective representation for the\nSigned Distance Functions (SDFs) of 3D shapes, suitable\nfor generative models, that we call Mosaic-SDF (M-SDF).\nOur main focus is on building a representation that satisfies\nproperties (i)-(iii) from Section 1.\nSigned Distance Function (SDF).\nOur goal is to build a\nsimple low parameter count approximation to the SDF of a\nshape S \u2282 R3, where x \u2208 S are points inside the shape,\nx /\u2208 S are outside and x \u2208 \u2202S are on the shape\u2019s boundary.\nThe SDF of the shape is defined by\nFS(x) =\n(\n\u2212d(x, \u2202S)\nx \u2208 S\nd(x, \u2202S)\nx /\u2208 S ,\n(1)\nwhere the unsigned distance function is defined by\nd(x, \u2202S) = miny\u2208\u2202S \u2225x \u2212 y\u22252.\nFigure 2.\nMosaic-SDF\nrepresentation.\nM-SDF representation.\nWe\napproximate FS with a volu-\nmetric function FX : R3 \u2192\nR defined by the parameters\nX. The core idea is to define\nFX as a weighted combination\nof a set of local grids.\nFol-\nlowing the three principles out-\nlined above, (i)-(iii) we define the representation X to be\nthe set of tuples\nX = {(pi, si, Vi)}i\u2208[n] ,\n(2)\nwhere [n] = {1, . . . , n}, pi \u2208 R3 are 3D point locations,\nsi \u2208 R are local scales, and Vi \u2208 Rk\u00d7k\u00d7k are local vol-\numetric grids. See Figure 2 for an illustration of the local\ngrids. We denote by IVi : [\u22121, 1]3 \u2192 R the trilinear inter-\npolants of the values Vi over the origin-centered volumetric\ngrid G of the cube [\u22121, 1]3 defined by\nG =\n\u001a2 (i1, i2, i3) \u2212 n \u2212 1\nn \u2212 1\n\f\f\fi1, i2, i3 \u2208 [k]\n\u001b\n.\n(3)\nBy convention the interpolants IVi vanish outside the cube\n[\u22121, 1]3, i.e., IVi(x) = 0 for x /\u2208 [\u22121, 1]3. The parametric\nSDF approximation is then defined by\nFX(x) =\nX\ni\u2208[n]\nwi (x) IVi\n\u0012x \u2212 pi\nsi\n\u0013\n(4)\nwhere wi(\u00b7) are scalar weight functions that define the con-\ntribution of the i\u2019th local grid; the wi(\u00b7) are supported in the\ncube [\u22121, 1]3 and satisfying partition of unity, i.e.,\nX\ni\u2208[n]\nwi(x) = 1,\n\u2200x \u2208 R3.\n(5)\nWe opt for\nwi(x) =\n\u00afwi(x)\nP\nj\u2208[n] \u00afwj(x), \u00afwi(x) = ReLU\n\u0014\n1\u2212\n\r\r\r\r\nx \u2212 pi\nsi\n\r\r\r\r\n\u221e\n\u0015\n.\nThe domain of definition of FX is the union of local grids,\nD(X) = \u222ai\u2208[n]B\u221e(pi, si),\n(6)\nwhere B\u221e(p, s) =\n\b\nx \u2208 R3| \u2225x \u2212 p\u2225\u221e < s\n\t\nis the infin-\nity ball of radius s centered at p.\n3.2. Computing M-SDF for Shape Dataset\nGiven an input shape S (as defined above) from some\ndataset of shapes, we would like to compute its Mosaic-SDF\nrepresentation X such that FX \u2248 FS in a neighborhood of\nthe shape\u2019s boundary surface, \u2202S. The computation of X\nconsists of two stages: (i) Initialization: where we find X\nsuch that the domain of definition of FX covers the surface\nof the shape, i.e., \u2202S \u2282 D(X); and (ii) Fine-tuning: where\nwe optimize X to improve approximation FX \u2248 FS. This\nalgorithm can be applied to each shape individually making\nit simple to parallelize and is computationally efficient com-\npared to alternatives. The algorithm for converting a shape\nS to its M-SDF representation is summarized in Algorithm\n1. Figure 3 compares M-SDF representation and some of\nthe popular existing representations for a fixed budget of\nparameters. Note that M-SDF provides the highest qual-\nity approximation while is only \u00d72 slower than the fastest\nmethod, i.e., the 3D volumetric grid. Later, in Section 4.2\nwe provide a more detailed evaluations and comparisons.\n3\nGT\nM-SDF (1 min)\nINR (30 min)\n3D Grid (0.5 min)\nTriplane (6 min)\nFigure 3. M-SDF representation: we compare the ground truth shape (left), with zero levelsets of (from left to right) M-SDF, Implicit\nNeural Representation (INR), Volumetric 3D Grid, and Triplane. All representations adhere the same budget of 355K parameters. Note\nthat M-SDF is provides the highest fidelity with an efficient computation time.\nInitialization.\nWe assume all shapes\u2019 boundaries \u2202S are\nprovided in a way that allows basic operations like sampling\nand SDF computation, e.g., as triangular meshes. We nor-\nmalize all shapes so the \u2202S fit in the cube [\u22121, 1]3. We\ninitialize the volumes centers {pi}i\u2208[n] using farthest point\nsampling [8] over the shape\u2019s boundary \u2202S. Second, we set\nsi = s for all i \u2208 [n], where s is the minimal value that\nachieves a full coverage of the shape\u2019s boundary, i.e.,\ns = min\n\b\ns > 0\n\f\f \u2202S \u2282 D(X)\n\t\n,\n(7)\nwhere D(X) is defined in equation 6. To initialize the local\nvolumes Vi we simply store the corresponding values of the\nSDF FS at the local grid coordinates, i.e., Vi = FS(pi +\nsiG), where G defined in equation 3.\nFine-Tuning.\nAlthough the initialization already provides\na valid approximation to the shape\u2019s SDF it can be further\nimproved with quick fine-tuning. We optimize the initial-\nized Mosaic-SDF representation X to reduce the following\nloss [9, 34] striving to regress the SDF\u2019s values and first\nderivatives:\nL(X) = L1(X) + \u03bbL2(X),\n(8)\nwhere\nL1(X) = 1\nm\nX\nj\u2208[m]\n\u2225FX(xj) \u2212 FS(xj))\u22251 ,\n(9)\nL2(X) = 1\nm\nX\nj\u2208[m]\n\u2225\u2207xFX(yj) \u2212 \u2207xFS(yj)\u22252 ,\n(10)\nwhere \u2225\u00b7\u22251, \u2225\u00b7\u22252 represent the 1 and 2-norms (resp.), \u03bb >\n0 is a hyper-parameter.\nThe sampling points {xi}j\u2208[m],\n{yj}j\u2208[m] are spread uniformly over the shapes\u2019 boundaries\n\u2202S and their neighborhood; more details are provided in\nSection 4.1.\nAlgorithm 1: Mosaic-SDF preprocess.\nInput : Shape S, set size n, grid resolution k,\n\u03bb \u2265 0\n\u25b7 Initialization\n{pi}i\u2208[n] \u2190 farthest point sampling of \u2202S\n{si}i\u2208[n] \u2190 s minimal covering scalar\n\u25b7 eq. 7\nfor i \u2190 1 to n do\nVi \u2190 FS(pi + G \u00b7 si)\n\u25b7 G in eq. 3\n\u25b7 Fine-tuning\nX \u2190 {(pi, si, Vi)}i\u2208[n]\nwhile not converged do\nTake gradient step with \u2207XL(X)\n\u25b7 eq. 8\nOutput: X\n3.3. Mosaic-SDF Generation with Flow Matching\nAt this point we assume to be a given a dataset of N\nshapes paired with condition vectors (e.g., classes or text\nembeddings),\n\b\n(Si, ci)\n\t\ni\u2208[N], and in a preprocess step we\nconverted, using Algorithm 1, all shapes to M-SDF form,\n\b\n(Xi, ci)\n\t\ni\u2208[N]. Our goal is to train a flow based genera-\ntive model taking random noise to M-SDF samples. Given\nan M-SDF sample X the shape\u2019s boundary can be extracted\nvia zero level set contouring of FX, e.g., with Marching\n4\nCubes [25].\nBelow we recap the fundamentals of flow-\nbased models adapted to our case and present the full train-\ning (Algorithm 2) and sampling (Algorithm 3).\nFlow-based generative model.\nFlow-based generative\nmodels are designed to find a transformation taking sam-\nples from some simple noise distributions X0 \u223c p(X0) to\nconditioned data samples X1 \u223c q(X1|c). Our noise distri-\nbution is, as customary, a standard Gaussian p = N(0, I),\nand our empirical target distribution is\nq(\u00b7, c) = 1\nN\nX\ni\u2208[N]|ci=c\n\u03b4(\u00b7 \u2212 Xi),\n(11)\nwhere \u03b4(X) is a near-delta function (e.g., a Gaussian with\nsmall standard deviation \u03c3) centered at 0. In our case, the\ndata points are matrices,\nX \u2208 Rn\u00d7d, where d = 3 + 1 + k3,\n(12)\nwith their row order considered only up to a permutation. A\nflow model is modeled with a learnable velocity field [4] de-\nnoted U \u03b8 : [0, 1]\u00d7Rn\u00d7d \u00d7Rc \u2192 Rn\u00d7d, where \u03b8 represents\nits learnable parameters, and c the dimension of an optional\nconditioning variable. Sampling from a flow model repre-\nsented by U \u03b8 is done by first randomizing X0 \u223c p(X0),\nand second, solving the ODE\n\u02d9Xt = U \u03b8\nt (Xt, c),\n(13)\nwith initial condition X0, from time t = 0 until time t =\n1, and c \u2208 Rc is the condition vector. Lastly, the desired\nsample of the model is defined to be X1.\nSymmetric Data.\nAs mentioned above X \u2208 Rn\u00d7d rep-\nresents a set of n elements in Rd (i.e., the elements are the\nrows of X). Differently put, permutation of the rows of X,\ni.e., P X with P being a permutation matrix, is a symmetry\nof this representation, namely represents the same object\nX \u223c= P X. Consequently, we would like our generative\nmodel to generate X and P X with the same probability.\nOne way to achieve this is to consider noise density p that\nis permutation invariant, i.e.,\np(P X) = p(X),\nfor all X, P ,\n(14)\nand a permutation equivariant flow field U \u03b8, i.e.,\nU \u03b8\nt (P X, c) = P U \u03b8\nt (X, c),\nfor all t, X, c, P .\n(15)\nIndeed as proved in [18] (Theorem 1 and 2) equations 14\nand 15 imply that the generations X(1) using an equivari-\nant model U \u03b8 and invariant noise p are also permutation\nequivariant and X(1) and P X(1) are generated with the\nsame probability, as required. One benefit of this set sym-\nmetry is the existence of a powerful permutation equivariant\nneural architecture, namely a Transformer [33, 42] without\npositional encoding.\nAlgorithm 2: Flow Matching training.\nInput : M-SDF dataset\n\b\nXi\t\ni\u2208[N], puncond, \u03c3\nInitialize U \u03b8\nt\nwhile not converged do\nt \u223c U([0, 1])\n\u25b7 sample time\n(x1, c) \u223c q(x1, c) \u25b7 sample data and condition\nc \u2190 \u2205 with probability puncond \u25b7 null condition\nX0 \u223c p(X0)\n\u25b7 sample noise\nXt \u2190 tX1 + (1 \u2212 \u03c1t)X0\n\u25b7 eq. 17\n\u02d9Xt \u2190 X1 \u2212 \u03c1X0\nTake gradient step on \u2207\u03b8\u2225U \u03b8\nt (Xt, c) \u2212 \u02d9Xt\u22252\nOutput: U \u03b8\nFlow Matching.\nWe use the recent formulation of Flow\nMatching (FM) [21] with its permutation equivariant variant\n[17]. Flow Matching models are similar to diffusion models\nin taking noise to data but directly regress the velocity field\nof a noise-to-data target flow and consequently have several\nbenefits such as flexibility of noise-to-data paths, they are\nwell defined for the entire time interval from noise to data\n(i.e., work with 0 SNR noise), easier to sample due to lower\nkinetic energy [40], and provide a competitive generation\nquality. We train Flow Matching with Classifier Free Guid-\nance (CFG) [11] by minimizing the loss\nL(\u03b8) = Et,b,p(X0),q(X1,c)\n\r\r\rU \u03b8\nt (Xt, \u00afc(b)) \u2212 \u02d9Xt\n\r\r\r\n2\n(16)\nwhere t \u223c U([0, 1]) is the uniform distribution, b \u2208\nB(puncond) is a Bernoulli random variable taking values 0, 1\nwith probability (1 \u2212 puncond), puncond (resp.), \u00afc = (1 \u2212 b) \u00b7\nc + b \u00b7 \u2205 where \u2205 is a symbol of null conditioning, and\nXt is a path interpolating noise X0 and data X1. We opt\nfor paths Xt that form Optimal Transport displacement map\n[28] conditioned on a training sample X1 \u223c q(X1), i.e.,\nXt = (1 \u2212 \u03c1t)X0 + tX1,\n\u03c1 = 1 \u2212 \u03c3\n(17)\nwhere \u03c3 > 0 is a hyper-parameter chosen to be \u03c3 = 10\u22125 in\nour case. This path choice is referred to as Conditional Opti-\nmal Transport (Cond-OT) and it takes samples from p(X0)\nto samples from N(X1, \u03c32I). Equivalent formulations of\nFlow Matching are also introduced in [1, 22].\n4. Experiments\n4.1. Implementation details\nDatasets and preprocess.\nWe train our generative model\non two main datasets: a subset of 3D Warehouse data [14] as\nin [51, 52] consisting of 50K 3D polygonal meshes classi-\nfied to 55 different categories, and a dataset of 600K polyg-\nonal meshes with matching text descriptions [27]. We pre-\nprocess each polygonal mesh as follows: First, scale it to be\n5\nFigure 4. Conditional samples from a Flow Matching model trained on M-SDF representations of 3D Warehouse.\nAlgorithm 3: ODE sampling.\nInput : trained model U \u03b8, condition c,\nguidance parameter \u03c9, number of steps m\nX0 \u223c p(X0)\n\u25b7 sample noise\nh \u2190 1\nm\n\u25b7 step size\nVt(\u00b7) \u2190 (1 + \u03c9)U \u03b8\nt (\u00b7|c) \u2212 \u03c9U \u03b8\nt (\u00b7|\u2205)\u25b7 CFG velocity\nfor t = 0, h, . . . , 1 \u2212 h do\nXt+h \u2190 ODEStep(Vt, Xt) \u25b7 ODE solver step\nOutput: X1\nbounded inside the unit cube [\u22121, 1]3. Second, make it wa-\ntertight using [12, 47]. Third, we run Algorithm 1 for 1K it-\nerations, with n = 1024, k = 7, and \u03bb = 0.1. Consequently\nthe M-SDF tensor representation is X \u2208 R1024\u00d7(3+1+73)\n(see equation 12) and has 355K parameters in total. This\nlast step takes less than 2 minutes on a single Nvidia A100\nGPU (it takes a bit longer than the experiment in Figure 3\nsince here we used \u03bb > 0). Fourth, noting that the M-SDF\nrepresentation has three channels, (pi, si, Vi), we normal-\nize pi, si to have zero mean and unit max norm (using 50K\nrandom samples of each channel).\nFlow Matching model architecture and training.\nWe\ntrain a Flow Matching generative model [21] where for\nU \u03b8 we use the transformer-based architecture [42] without\npositional encoding to achieve permutation equivariance,\ncompatible with our M-SDF tensorial representation. Each\nelement in the set (i.e., row) of the noisy sample Xt \u2208 Rn\u00d7d\nis fed in as token, as well as the time t and the condition-\ning c. Our transformer is built with 24 layers with 16 heads\nand 1024 hidden dimension, which result in a 328M param-\neter model. We train U \u03b8 for 500K iterations with batch size\nof 1024 using the ADAM optimizer [16] and learning rate of\n1e\u22124 with initial warm-up of 5K iterations. We additionally\nperform EMA (Exponential Moving Average) to the trans-\nfomer\u2019s weights. Both training were done on 8 nodes of 8\nNVIDIA A100 GPUs, which takes around a week.\n4.2. Representation evaluation\nWe start with comparing M-SDF to existing popular SDF\nrepresentations used in 3D generative models focusing on\ntwo key aspects: preprocess efficiency, and parameter effi-\nciency. We only consider SDF representations computed\nindependently for each individual shape, i.e., does not\nuse latent space representations defined by a global en-\ncoder/decoder. The main reason for this choice is that all\nmethods, including M-SDF, can be adapted to work on la-\ntent space, which is an orthogonal design choice. We com-\npare to: 3D Volumetric Grid (3D-Grid), Triplane and Im-\nplicit Neural Representation (INR).\nWe consider 100 random (GT) models from 3D Ware-\nhouse [14] and for each SDF representation (M-SDF, 3D-\nGrid, Triplane, INR) we log its average preprocess time and\nsurface approximation quality for varying parameter bud-\nget. The preprocess time is measured as the wall-clock time\nit takes a single Nvidia A100 GPU to compute the repre-\nsentation. For Triplane, INR and M-SDF we use the loss\nin equation 8 with \u03bb = 0; 3D-Grid is computed by evalu-\nating the GT SDF at the grid nodes. The surface approxi-\nmation quality is measured by the Chamfer Distance (CD)\nbetween the extracted surface mesh from the SDF represen-\n6\n70K\n355K\n750K\n1.0M\n1.4M\n1.8M\n0.001\n2\n4\n6\n8\n0.01\n2\n3\n4\n5\nINR\n3D Grid\nINR (PE)\nM-SDF (init.)\nTriplane (lin.)\nM-SDF\nTriplane\nParameter count\nQuality (CD)\n1\n3\n6\n10\n13\n16\n20\n0.001\n2\n5\n0.01\n2\n5\n0.1\n2\n5\nINR\nINR (PE)\nTriplane (lin.)\nTriplane\n3D Grid\nM-SDF\nTime (minutes)\nQuality (CD)\n(a)\n(b)\nFigure 5. (a) 3D approximation quality vs. representation parame-\nter count; (b) pre-process training time vs. 3D approximation qual-\nity for a fixed representation budget of 355K parameters.\ntation and the GT mesh. Figure 5 summarizes the results\nin two graphs: (a) shows the surface approximation quality\nof each representation for different parameter budgets; and\n(b) surface approximation quality versus preprocess time of\neach representation. For INR we additionally examine the\noption of using Positional Encoding (PE) [30], which incor-\nporates high frequencies as input to the MLP. For Triplane\nwe evaluated the two alternatives of aggregating the pro-\njected features, as suggested in [39], using learned linear\ndecoder (denoted as lin.) or using a small MLP decoder.\nWe note that while previous works that use the Triplane\nrepresentation [3, 41] suggested additional regularizations,\nwe tested Triplane with the same supervision and loss as\nthe other methods that require optimization, namely INR\nand M-SDF. This can potentially explain the degradation in\nthe approximation quality as the parameter count increases.\nAdditionally, for M-SDF we report the surface approxima-\ntion quality both at initialization and after fine-tuning (see\nSection 3.2). As can be seen in the graphs, M-SDF is su-\nperior to INR in terms of surface approximation per pa-\nrameter budget while is computable in only a fraction of\nthe time. 3D Grids are the only faster baseline to M-SDF\nbut their approximation-parameter trade-off is considerably\nworse (see also Figure 3).\n4.3. Class conditional generation\nIn this section we evaluate our class-conditioned generative\nFM model trained on the 3D Warehouse [14] where the 55\nclasses of objects in this dataset are used as conditions. We\nfollow the split to train/val/test suggested by [51]. Follow-\ning and extending protocols of past works, Table 1 reports\nquantitative evaluation for the 5 largest classes (containing\nover 3K shapes each) comparing M-SDF and relevant re-\ncent baselines.\nEvaluation metrics.\nWe make use of several standard\nmetrics for evaluating the performance of our M-SDF gen-\nFPD (\u2193)\nKPD (\u2193)\nCOV (\u2191,%)\nMMD (\u2193)\n1-NNA (\u2193,%)\nCD\nEMD\nCD\nEMD\nCD\nEMD\nairplane\n3DILG\n1.83\n3.22\n41.09\n32.67\n4.69\n4.73\n82.67\n84.41\nNW\n0.81\n1.26\n51.98\n45.05\n3.36\n4.19\n68.32\n73.76\nS2VS\n0.94\n1.65\n51.98\n40.59\n3.80\n4.45\n69.06\n76.73\nOurs\n0.44\n0.50\n52.48\n51.49\n3.54\n3.78\n62.62\n69.55\ncar\n3DILG\n2.84\n6.24\n18.86\n20.57\n4.67\n3.83\n93.43\n90.57\nNW\n-\n-\n-\n-\n-\n-\n-\n-\nS2VS\n1.32\n2.17\n37.71\n40.00\n4.13\n3.52\n84.57\n86.57\nOurs\n0.46\n0.48\n45.71\n51.43\n2.87\n2.75\n70.00\n66.00\nchair\n3DILG\n1.64\n2.00\n37.87\n39.94\n20.37\n10.54\n74.11\n69.38\nNW\n1.41\n1.29\n43.79\n47.04\n16.53\n9.47\n59.47\n64.20\nS2VS\n0.77\n0.63\n51.78\n52.37\n16.97\n9.44\n58.43\n60.80\nOurs\n0.52\n0.19\n48.22\n55.03\n15.47\n9.13\n51.04\n55.62\nsofa\n3DILG\n3.19\n5.83\n25.95\n29.11\n26.41\n10.71\n84.81\n77.85\nNW\n-\n-\n-\n-\n-\n-\n-\n-\nS2VS\n1.17\n1.70\n48.73\n51.90\n10.83\n7.25\n62.66\n57.91\nOurs\n0.63\n0.62\n46.20\n48.10\n12.43\n7.60\n61.71\n55.70\ntable\n3DILG\n2.86\n4.13\n29.45\n30.88\n22.96\n10.18\n78.27\n78.74\nNW\n1.49\n2.20\n51.07\n47.98\n13.27\n7.72\n56.41\n58.67\nS2VS\n0.83\n0.92\n53.44\n49.41\n14.06\n8.01\n59.74\n61.05\nOurs\n0.47\n0.21\n52.97\n53.21\n13.49\n7.74\n51.31\n50.59\nTable 1. Evaluation of our class conditioning generation model\ntrained on 3D Warehouse [14] compared to baselines. We report\nresults on the 5 largest classes in the dataset. KPD is multiplied by\n103, MMD-CD is multiplied by 103 and MMD-EMD is multiplied\nby 102.\n3DILG\nNW\nS2VS\nOurs\nFigure 6. Class conditioning generation of 3D shapes compared to\nrelevant baselines. Note the high fidelity demonstrated in M-SDF\ngenerated shapes compared to the (overly smooth) baselines.\nerative models. All these metrics quantify differences be-\ntween a set of reference shapes Sr and generated shapes\nSg. To measure distances between shapes we follow pre-\nvious works (e.g., [26, 50, 52]) and use the Chamfer Dis-\ntance (CD) and Earth Moving Distance (EMD). Using\nthese distances we compute: Maximum Mean Discrepancy\n(MMD), Coverage (COV), and 1-nearest neighbor accuracy\n(1-NNA) to quantify fidelity, diversity and distributional\nsimilarity, respectively. Furthermore, following [33, 52] we\n7\nA large salad bowl\nAn electronic toy\nA small private jet\nA modern  \nleather sofa\nA scary t-rex\nRunning \n shoes\nA beautiful  \nearring\nA helicopter with \n large rotor\nA tiger\nA diamond ring\nA beautiful  \nnecklace\nA great  \nwhite shark\nA beautiful  \nring\nA beautiful ring with \n gem embedded\nA teapot\nA round teapot\nA teapot next to a mug\nA police car\nA teapot with \n two handles\nA spider\nA wine bottle\nTwo large salad bowls\nA heavy hammer\nFigure 7. Text-to-3D samples from a Flow Matching model trained on M-SDF representations.\nuse the 3D analogs of the Fr\u00b4echet Inception Distance (FID)\nand Kernel Inception Distance (KID), commonly used to\nevaluate image generative models. We employ a pre-trained\nPointNet++ [37] to extract features. As in [52] we will re-\nfer to these metrics as Fr\u00b4echet PointNet++ Distance (FPD)\nand Kernel PointNet++ Distance (KPD). Additional infor-\nmation and implementation details are provided in the sup-\nplementary material.\nBaselines.\nWe compare to Neural Wavelet (NW) [13] that\ngenerate a grib-based representation. Note that this works\ntrains an unconditional model for each class separately,\nmaking it arguably simpler than a single conditioned model\non the entire 55 classes. We additionally compare to 3DILG\n[51] and 3DShape2VecSet (S2VS) [52], that suggest to gen-\nerate structured or unstructured latent vectors, encoding an\noccupancy field using a transformer.\nAs can be read in Table 1 our M-SDF based generative\nmodel compares favorably to the baselines, achieving best,\nor second best performance in all metrics. Figure 6 shows\nqualitative comparison for two classes common for all base-\nlines, i.e., chairs and airplanes. Note that generation with\nM-SDF allows higher fidelity and sharper surfaces com-\npared to baselines which tend to be overly smooth. Figure\n4 shows additional class conditional generations with our\nM-SDF model.\n643\n1283\n2563\n5123\n3DILG\n0.3\n2.33\n18.44\n159.56\nNW\n-\n-\n0.61\n-\nS2VS\n0.06\n0.36\nOOM\nOOM\nOurs\n0.05\n0.34\n2.74\n21.48\nTable 2. Surface extraction time\n(in seconds).\nSDF evaluation time.\nAn\nadditional\nadvan-\ntage of M-SDF com-\npared to other perfor-\nmant baselines is the\nrelative efficiency and\nflexibility in extracting\nthe zero level set of the SDF FX (equation 4).\n3DILG\nand S2VS require forward pass in a transformer for func-\ntion evaluation. NW is restricted to generate a 3D grid in\na single resolution. M-SDF can be evaluated efficiently by\nquerying only the relevant local grids. In Table 2 we log,\nfor each method, the total time of extracting the zero lev-\nelset of the SDF (with Marching Cubes) using cubic grids\nof different resolutions (643, 1283, 2563, and 5123); OOM\nstands for Out of Memory.\n4.4. Text-to-3D generation\nLastly we provide a qualitative evaluation of our M-SDF\nbased generative FM model trained on a dataset of 600K\nshapes with matching text captions [27]. We utilize a pre-\ntrained text model [38] as our textual embedding, passing\nthis embedding to the model as conditioning. Figure 7 de-\npicts pairs of generated shapes and the text conditions used\nto generate it.\n5. Summary and Future Work\nWe presented a novel 3D shape representation, Mosaic-\nSDF, that is geared towards 3D generative models and\noffers a simple and efficient preprocessing, favorable\napproximation-parameter trade-off, and a simple tensorial\nstructure compatible with powerful modern architectures\n(i.e., transformers). We have used M-SDF to train Flow\nMatching generative models and demonstrated state of the\nart results for forward-based models. We believe that M-\nSDF is the method of choice for 3D generation however\nstill posses some limitations and can be extended in sev-\neral ways: First, currently we only encode the SDF, missing\ntexture/color/light information. An interesting extension is\nto incorporate texture and/or light field data. Second, in\nour architecture we use a simple linear layer passing the lo-\ncal grids into the transformer. A possible extension here\nis to incorporate convolution layers and/or autoencoders to\nfurther increase resolution/data reuse of the representation.\nLastly, making M-SDF equivariant to orientations, e.g., by\nadding local coordinate frames, can improve the generaliza-\ntion abilities of the trained model, which is currently only\npermutation equivariant.\n8\nAcknowledgements\nWe would like to thank to Itai Gat, Matt Le and Ricky Chen\nfor their valuable advice and help on training Flow Based\ngenerative models. We further thank Biao Zhang for shar-\ning with us the evaluation code for the class conditioning\nexperiment. OP is supported by a grant from Israel CHE\nProgram for Data Science Research Centers and the Min-\nerva Stiftung.\nReferences\n[1] Michael S Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants. arXiv preprint\narXiv:2209.15571, 2022. 5\n[2] Mallikarjun B R, Xingang Pan, Mohamed Elgharib, and\nChristian Theobalt. Gvp: Generative volumetric primitives,\n2023. 3\n[3] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In arXiv, 2021. 2, 7\n[4] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid K Duvenaud.\nNeural ordinary differential equa-\ntions. Advances in neural information processing systems,\n31, 2018. 5\n[5] Ricky T. Q. Chen. torchdiffeq, 2018. 12\n[6] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019.\n2\n[7] Wikipedia contributors. Janus \u2014 wikipedia, the free ency-\nclopedia, 2010. [2023]. 1\n[8] Yuval Eldar, Michael Lindenbaum, Moshe Porat, and\nYehoshua Y Zeevi. The farthest point strategy for progres-\nsive image sampling. IEEE Transactions on Image Process-\ning, 6(9):1305\u20131315, 1997. 4, 11\n[9] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. In Proceedings of Machine Learning and Systems\n2020, pages 3569\u20133579. 2020. 4\n[10] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration, 2023. 1, 2\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[12] Jingwei Huang, Hao Su, and Leonidas Guibas. Robust water-\ntight manifold surface generation method for shapenet mod-\nels. arXiv preprint arXiv:1802.01698, 2018. 6\n[13] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. 2022. 2,\n8\n[14] Trimble Inc. 3d warehouse. 2, 5, 6, 7, 12\n[15] Heewoo Jun and Alex Nichol. Shap-e: Generating condi-\ntional 3d implicit functions, 2023. 1, 2\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6, 12\n[17] Leon Klein, Andreas Kr\u00a8amer, and Frank No\u00b4e. Equivariant\nflow matching. arXiv preprint arXiv:2306.15030, 2023. 5\n[18] Jonas K\u00a8ohler, Leon Klein, and Frank No\u00b4e. Equivariant flows:\nexact likelihood generative learning for symmetric densi-\nties. In International conference on machine learning, pages\n5361\u20135370. PMLR, 2020. 5\n[19] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-\nsdf: Text-to-shape via voxelized diffusion. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 1, 2\n[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 1\n[21] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matthew Le. Flow matching for genera-\ntive modeling. In The Eleventh International Conference on\nLearning Representations, 2023. 2, 5, 6\n[22] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022. 5\n[23] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ.\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu.\nMeshd-\niffusion: Score-based generative 3d mesh modeling.\nIn\nInternational Conference on Learning Representations,\n2023. 1\n[24] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Trans. Graph., 40(4), 2021. 3\n[25] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 5, 11\n[26] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 1, 2, 7, 11\n[27] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 2, 5, 8\n[28] Robert J McCann.\nA convexity principle for interacting\ngases. Advances in mathematics, 128(1):153\u2013179, 1997. 5\n[29] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019. 2\n[30] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 7\n9\n[31] Norman\nM\u00a8uller,\nYawar\nSiddiqui,\nLorenzo\nPorzi,\nSamuel Rota Bulo,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner.\nDiffrf:\nRendering-guided 3d radiance field\ndiffusion.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition,\npages\n4328\u20134338, 2023. 1, 2\n[32] Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya,\nGal Chechik, and Haggai Maron.\nEquivariant architec-\ntures for learning in deep weight spaces.\narXiv preprint\narXiv:2301.12780, 2023. 2\n[33] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts, 2022. 1, 2, 5, 7,\n11\n[34] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 2, 4\n[35] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 1\n[36] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. arXiv preprint arXiv:1612.00593, 2016.\n2\n[37] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413, 2017. 8, 11\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learn-\ning Research, 21(140):1\u201367, 2020. 8, 12\n[39] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR, 2023. 2, 7\n[40] Neta Shaul, Ricky TQ Chen, Maximilian Nickel, Matthew\nLe, and Yaron Lipman. On kinetic optimal probability paths\nfor generative models. In International Conference on Ma-\nchine Learning, pages 30883\u201330907. PMLR, 2023. 5\n[41] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion, 2022. 1, 2, 7\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998\u20136008, 2017. 5,\n6\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems. Curran Associates, Inc.,\n2017. 2, 3\n[44] Edward Wagstaff,\nFabian B Fuchs,\nMartin Engelcke,\nMichael A Osborne, and Ingmar Posner. Universal approxi-\nmation of functions on sets. The Journal of Machine Learn-\ning Research, 23(1):6762\u20136817, 2022. 2\n[45] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo.\nRodin: A genera-\ntive model for sculpting 3d digital avatars using diffusion.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4563\u20134573,\n2023. 1, 2\n[46] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 1\n[47] Francis\nWilliams.\nPoint\ncloud\nutils,\n2022.\nhttps://www.github.com/fwilliams/point-cloud-utils. 6\n[48] Xu\nYan.\nPointnet,pointnet++\npytorch.\nhttps://github.com/yanx27/Pointnet Pointnet2 pytorch,\n2019. 11\n[49] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. arXiv, 2019.\n11\n[50] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent\npoint diffusion models for 3d shape generation. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022.\n1, 2, 7, 11\n[51] Biao Zhang, Matthias Nie\u00dfner, and Peter Wonka. 3DILG: Ir-\nregular latent grids for 3d generative modeling. In Advances\nin Neural Information Processing Systems, 2022. 3, 5, 7, 8,\n11, 12\n[52] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models, 2023. 3, 5, 7, 8,\n11, 12\n[53] Xin-Yang Zheng, Yang Liu, Peng-Shuai Wang, and Xin\nTong. Sdf-stylegan: Implicit sdf-based stylegan for 3d shape\ngeneration. In Comput. Graph. Forum (SGP), 2022. 2\n[54] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 5826\u20135835, 2021. 1, 2\n10\nMosaic-SDF for 3D Generative Models\nSupplementary Material\nA. Generative model evaluation\nIn this section we provide additional information on the ex-\nperiments described in Section 4.3.\nA.1. Metrics\nWe measure distances between shape distributions follow-\ning previous works [26, 49, 50, 52]. We quantify differences\nbetween a set of reference shapes Sr and a set of generated\nshapes Sg. We describe a shape Y \u2208 Sr as a point cloud\nof size N sampled from a reference mesh using the farthest\npoint sampling [8]. Similarly, X \u2208 Sg is a point cloud sam-\npled from a generated surface mesh, extracted as the 0-level\nset of the SDF (or 0.5 level set for occupancy function) us-\ning the Marching Cubes algorithm [25].\nGeometric shape similarity.\nThe Chamfer Distance\n(CD) and the Earth Mover Distance (EMD) measure sim-\nilarity between two point clouds:\nCD(X, Y) =\nX\nx\u2208X\nmin\ny\u2208Y\u2225x \u2212 y\u22252\n2 +\nX\ny\u2208Y\nmin\nx\u2208X\u2225x \u2212 y\u22252\n2 (18)\nEMD(X, Y) =\nmin\n\u03b3:X\u2192Y\nX\nx\u2208X\n\u2225x \u2212 \u03b3(y)\u22252\n(19)\nwhere \u03b3 is the bijection between the point clouds, and\nN = 5K. In the following, we denote by D(X, Y) dis-\ntance measure between two point clouds, referring to either\nCD or EMD.\nGeometrical distances between sets of shapes.\nThe CD\nand EMD distances between point clouds are used to define\nthe following distances between sets of shapes Sr and Sg:\nCoverage (COV) quantifying the diversity of Sg by count-\ning the number of reference shapes that are matched to at\nleast one generated shape; Minimum Matching Distance\n(MMD) measuring the fidelity of the generated shapes to the\nreference set; and 1-Nearest Neighbor Accuracy (1-NNA)\ndescribing the distributional similarity between the gener-\nated shapes and the reference set, quantifying both quality\nand diversity. Next we provide the mathematical definitions\nof these distance measures:\nCOV(Sg, Sr)= |\n\b\narg minY\u2208Sr D(X, Y)|X \u2208 Sg\n\t\n|\n|Sr|\n(20)\nMMD(Sg, Sr) =\n1\n|Sr|\nX\nY\u2208Sr\nmin\nX\u2208Sg D(X, Y)\n(21)\n1-NNA(Sg, Sr)=\nP\nX\u2208Sg\nI [NX \u2208 Sg]+ P\nY\u2208Sr\nI [NY \u2208 Sr]\n|Sr| + |Sg|\n(22)\nwhere I(\u00b7) is the indicator function and NX is the nearest\nneighbor of X in the set Sr \u222a Sg \u2212 {X}.\nPerceptual distances between sets of shapes.\nAlong-\nside the geometric distance-based metrics, we adopt the 3D\nanalogs of the Fr\u00b4echet Inception Distance (FID) and Ker-\nnel Inception Distance (KID) suggested in previous works\n[33, 52]. In the 3D case, FID/KID are computed on the fea-\nture sets computed by pushing N = 2046 point samples\ninto a pre-trained PointNet++ network [37]. We denote by\nR and G the sets of the extracted features from the refer-\nence shapes Sr and the generated shaped Sg, respectively.\nWe can further define (\u00b5r, \u03a3r) as the the mean and covari-\nance statistics computed from the feature set R, and sim-\nilarly (\u00b5g, \u03a3g) for G. As in [52], the Fr\u00b4echet PointNet++\nDistance (FPD) and Kernel PointNet++ Distance (KPD) are\ndefined by\nFPD(Sg, Sr)=\u2225\u00b5g\u2212\u00b5r\u2225+Tr\n\u0010\n\u03a3g + \u03a3r\u22122(\u03a3g\u03a3r)\n1\n2\n\u0011\n(23)\nFPD(Sg, Sr) =\n \n1\n|R|\nX\nx\u2208R\nmax\ny\u2208G K(x, y)\n!2\n(24)\nwhere K(\u00b7, \u00b7) is a polynomial kernel function distance.\nA.2. Computation of distance metrics\nFollowing previous works we compute the geometric dis-\ntances, i.e., COV, MMD and 1-NNA, with the reference set\nof shapes, Sr, chosen to be the test split; and we generate an\nequal number of shape from our generated set Sg. The per-\nclass test split given by [51] consists of 5% of the shapes\nfrom each class, with varying numbers of shapes in each\nclass. We used the following released codes to compute\nCD1 and EMD2, and computed the distance metrics from\nthe official code of [50]3.\nFor computing the perceptual distances, FPD and KPD,\nwe follow [52] and use 1K generated shapes Sg, and as the\nreference set Sr we take 1K randomly sampled shapes from\nthe train split. We utilize a pre-trained PointNet++ model\nfrom [48] to extract the features R and G.\nTo run the baselines, we use the official implementation\nof each method together with the pre-trained model they\nsupply: The per-class unconditional models from Neural\nWavelet4, and the class conditioned model from 3DILG5\nand 3DShape2VecSet6.\n1https://github.com/ThibaultGROUEIX/ChamferDistancePytorch\n2https://github.com/daerduoCarey/PyTorchEMD\n3https://github.com/nv-tlabs/LION/tree/main\n4https://github.com/edward1997104/Wavelet-Generation\n5https://github.com/1zb/3DILG\n6https://github.com/1zb/3DShape2VecSet\n11\nB. Additional implementation details\nIn this section we provide additional implementation details\nmissing from the main paper.\nComputing M-SDF representation.\nAs described in\nSection 3.2 and algorithm 1, the computation of the M-SDF\nrepresentation for a given shape S consists of two stages:\ninitialization and fine-tuning. For both stages we require a\ngood estimation of the ground truth SDF FS, and for that we\nuse the open-source library of Kaolin-Wisp7. For the fine-\ntuning stage we sample for supervision a set of 300K points\non the surface, and 200K points near the surface perturbed\nwith Gaussian noise with variance 0.01. In each fine-tuning\nstep we sample a random batch of 16K points, used to com-\npute the loss in equation 8. We run the fine-tuning for 1K\nsteps with the ADAM optimizer [16] and learning rate of\n1e\u22124.\nConditioning tokens.\nTo complete the architecture de-\nscription in Section 4.1, we add details regarding the condi-\ntioning mechanism c. For the class conditioning generation\n4.3 we use a learned per-class embedding where each class\nis described using a latent vector of size 128. For a selected\nclass latent we first apply a linear layer projecting it to the\ntransformer dimension, i.e., 1024 before feeding it to the\ntransformer. For the text conditioning generation 4.4, we\nutilize a pre-trained text model [38] as our textual embed-\nding, result in a token embedding with feature size of 768\nand maximum sequence length of 32. We feed these addi-\ntional 32 tokens to the transformer, after applying a linear\nlayer projecting to 1024.\nNFE\nTime\n1-NNA (\u2193,%)\n(sec.)\nCD\nEMD\nMidpoint-25\n50.00\n6.13\n59.16\n67.57\nMidpoint-50\n100.00\n12.19\n61.88\n69.06\nDOPRI\n138.46\n16.97\n57.67\n64.85\nTable 3. Generation complexity and\nquality for different ODE solvers.\nGeneration timing.\nIn Table 3 we re-\nport the time (in sec-\nonds) and Number\nof Function Evalua-\ntions (NFE) for gen-\nerating one sample\naccording to algorithm 3, using different ODE solvers:\nMidpoint rule, and Dormand\u2013Prince method (DOPRI) [5].\nWe further report the effect of the different solvers on the\nquality of the generated shapes using the 1-NNA metric.\nFor this experiment we evaluated our class-conditioning\nmodel, on 300 generated samples from the \u2019airplane\u2019 class.\nPlease note that in all of the paper\u2019s experiments and eval-\nuations we used the DOPRI as our ODE solver, however\nas Table 3 indicates using the Midpoint method, with ei-\nther 25 or 50 steps, results in faster generation and only a\nmild degradation in quality. Using recent advances in fast\nsampling of flow models is expected to reduce these times\nfurther.\n7https://github.com/NVIDIAGameWorks/kaolin-wisp\n3DILG\nNW\nS2VS\nOurs\n3DILG\nS2VS\nOurs\nFigure 8. Class conditioned generation of 3D shapes compared to\nrelevant baselines.\nC. Additional results\nClass conditioning generation.\nIn Figure 8 we show\nadditional qualitative comparison of the class-conditioned\ngeneration compared to the relevant baselines. On top we\nshow the common classes across all baselines. Below the\ndashed line we further present other classes in a comparison\nto 3DILG[51] and S2VS[52], which trained a class condi-\ntioning model similarly to us. Note that M-SDF generation\nare overall sharper with more details, while baselines tend\nto over smooth.\nGuidance scale ablation.\nWe perform an ablation study\nregarding the guidance scale \u03c9 we use in the sampling al-\ngorithm 3. Figure 9 depicts the generation samples using\ndifferent guidance scales, with both our class-conditioning\nmodel (top) and the text-conditioning model (bottom). We\nfurther provide quantitative comparison in Table 4, when\nsampling our class conditioning model using various guid-\ning scales. Here, we perform similar evaluation to the class\nconditioning evaluation in Table 1, and report metrics for\nthe 5 largest classes in the 3D Warehouse dataset [14]. As\nsomewhat expected, the \u03c9 = 0 performs best when com-\nparing shape distributions, however qualitatively, taking a\nhigher \u03c9 tends to result in a more \u201dcommon\u201d or \u201daverage\u201d\nshape. In the main paper we therefore opted \u03c9 = 0 for\nthe class-conditional shape generation, and \u03c9 = 5 for the\ntext-conditioned shape generation.\n12\nFPD (\u2193)\nKPD (\u2193)\nCOV (\u2191,%)\nMMD (\u2193)\n1-NNA (\u2193,%)\nCD\nEMD\nCD\nEMD\nCD\nEMD\nairplane\nOurs \u03c9 = 0\n0.37\n0.37\n50.99\n48.02\n3.46\n3.71\n57.67\n64.85\nOurs \u03c9 = 1\n0.71\n0.75\n44.06\n40.59\n4.03\n3.92\n65.35\n74.75\nOurs \u03c9 = 2\n0.80\n0.79\n37.13\n41.09\n4.65\n3.84\n70.54\n73.27\nOurs \u03c9 = 5\n1.10\n1.19\n36.63\n35.64\n4.90\n4.14\n74.50\n78.22\nOurs \u03c9 = 10\n1.97\n2.90\n27.72\n27.72\n6.25\n4.54\n86.14\n81.93\ncar\nOurs \u03c9 = 0\n0.45\n0.47\n42.86\n45.14\n2.75\n2.78\n65.71\n70.00\nOurs \u03c9 = 1\n0.85\n1.00\n32.00\n37.71\n3.14\n2.87\n77.14\n72.29\nOurs \u03c9 = 2\n0.96\n1.15\n29.71\n35.43\n3.24\n2.85\n75.14\n72.29\nOurs \u03c9 = 5\n1.08\n1.28\n28.57\n34.86\n3.41\n3.02\n80.86\n74.86\nOurs \u03c9 = 10\n1.39\n2.11\n24.00\n28.57\n3.53\n3.18\n85.71\n83.71\nchair\nOurs \u03c9 = 0\n0.51\n0.20\n45.86\n51.48\n16.08\n9.17\n55.92\n55.47\nOurs \u03c9 = 1\n0.78\n0.64\n44.08\n42.60\n18.70\n10.39\n56.07\n64.79\nOurs \u03c9 = 2\n0.94\n0.85\n38.46\n42.60\n20.16\n10.59\n66.27\n70.56\nOurs \u03c9 = 5\n1.34\n1.42\n30.77\n33.43\n22.43\n11.13\n74.26\n74.26\nOurs \u03c9 = 10\n1.67\n1.92\n31.07\n30.47\n22.51\n11.62\n76.18\n77.96\nsofa\nOurs \u03c9 = 0\n0.64\n0.65\n44.94\n50.63\n11.21\n7.21\n59.49\n58.23\nOurs \u03c9 = 1\n1.31\n1.64\n36.08\n36.71\n15.31\n8.08\n69.30\n63.92\nOurs \u03c9 = 2\n1.68\n2.25\n27.85\n34.81\n17.50\n8.44\n80.70\n72.78\nOurs \u03c9 = 5\n2.51\n4.02\n22.15\n31.65\n20.03\n8.89\n87.66\n79.43\nOurs \u03c9 = 10\n3.48\n6.57\n17.72\n25.32\n21.07\n9.86\n89.56\n80.70\ntable\nOurs \u03c9 = 0\n0.49\n0.18\n52.26\n55.58\n13.10\n7.60\n52.14\n51.54\nOurs \u03c9 = 1\n1.26\n1.43\n39.90\n43.47\n15.16\n8.47\n65.20\n63.06\nOurs \u03c9 = 2\n1.97\n2.55\n32.30\n31.12\n18.63\n9.81\n73.40\n73.99\nOurs \u03c9 = 5\n3.08\n4.55\n17.81\n18.05\n36.52\n14.99\n89.31\n88.95\nOurs \u03c9 = 10\n4.34\n8.34\n13.54\n14.25\n54.02\n19.28\n95.72\n94.42\nTable 4. Ablation study on the Classifier Free Guidance (CFG) scale used\nfor sampling \u03c9. KPD and MMD-CD multiplied by 103, MMD-EMD by\n102.\n\u201dFerris wheel\u201d\n\u201dA teddy bear\u201d\n\u201dA beautiful earring with diamonds\u201d\n\u03c9 = 0\n\u03c9 = 1\n\u03c9 = 2\n\u03c9 = 5\n\u03c9 = 10\nFigure 9.\nAblation of guidance scale \u03c9 use in sampling our class-\nconditioned model (top) and our text-conditioned model (bottom).\nText-to-3D generation.\nIn Figure 10 we show additional\ngenerated shapes from our text-conditioned model.\nA tra\ufb03c cone\nA baby alien\nEarphones\nA \ufb02ying butter\ufb02y\nAn engagement ring\nA gun\nA rectangle \nsofa\nA cat toy\nA snowy mountain\nA gold\ufb01sh\nA big pumpkin \nA stool\nA tasty doughnut\nAn apple\nTwo cupcakes\nThree cupcakes\nFour cupcakes\nA sandwich\nA bucket hat\nA magical crown\nA banana\nA cheeseburger\nA deer\nA wooden barrel\nAn alien wearing \nA santa hat\nA magical \nsword\nA humanoid robot\nA big lizard\nA long lizard\nA cat\ufb01sh\nA bird\nA dinosaur\nA cat wearing \n    boots\nA screwdriver\nA cactus  \nin a pot\nA baby \npenguin\nAn old glove\nAn old ri\ufb02e\nA power drill\nFigure 10. Additional text-to-3D samples from a Flow Matching\nmodel trained on M-SDF representations of 600K pairs of shapes\nand text.\n13\n"
  },
  {
    "title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
    "link": "https://arxiv.org/pdf/2312.08578.pdf",
    "upvote": "14",
    "text": "A Picture is Worth More Than 77 Text Tokens:\nEvaluating CLIP-Style Models on Dense Captions\nJack Urbanek\u2217,\u2020, Florian Bordes\u2217,1,2,3, Pietro Astolfi1, Mary Williamson1,\nVasu Sharma1, Adriana Romero-Soriano1,2,4,5\n1FAIR at Meta, 2Mila, 3Universite de Montreal, 4McGill University, 5Canada CIFAR AI chair\n\u2217Joint first author, \u2020Work done at Meta\nCuration methods for massive vision-language datasets trade off between dataset size and quality.\nHowever, even the highest quality of available curated captions are far too short to capture the rich\nvisual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the\nDensely Captioned Images (DCI) dataset, containing 8012 natural images human-annotated with mask-\naligned descriptions averaging above 1000 words each. With precise and reliable captions associated\nwith specific parts of an image, we can evaluate vision-language models\u2019 (VLMs) understanding of\nimage content with a novel task that matches each caption with its corresponding subcrop. As current\nmodels are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which\neach caption length is limited. We show that modern techniques that make progress on standard\nbenchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly,\nwe finetune CLIP using sDCI and show significant improvements over the baseline despite a small\ntraining set. By releasing the first human annotated dense image captioning dataset, we hope to enable\nthe development of new benchmarks or fine-tuning recipes for the next generation of VLMs to come.\nDate: December 15, 2023\nCorrespondence: fbordes@meta.com\nCode: https://github.com/facebookresearch/DCI\n1\nIntroduction\nState-of-the-art\nvision-language\nmodels\n(VLMs)\nare often trained on large scale datasets such\nas\nLAION-400M\n(Schuhmann\net\nal.,\n2021),\nYFCC100M (Thomee et al., 2016), or other undis-\nclosed datasets crawled from the web.\nThese\ndatasets are formed by collecting images from the\nweb and using alt-text (or other local text on the\nwebpage) to create loose image-text pairs. These\ncan then be filtered down trading off on quantity for\nquality (Sharma et al., 2018; Radenovic et al., 2023).\nStill, recent work has demonstrated that throwing\nthese loose captions out entirely in favor of generated\ncaptions, with enhanced quality and density, can\nproduce improved results (Doveh et al., 2023a).\nOther works (Li et al., 2023b,c; Xu et al., 2023a;\nAbbas et al., 2023) have demonstrated that it is\npossible to get CLIP-level performance using a vastly\nreduced compute, often by throwing away portions\nof the data resulting in more balance between image\nand text modalities. However, those approaches rely\non automatic pipelines which do not generate reliable\nand long captions that can capture rich visual\ndetails in an image. From this it appears no existing\ndataset has high-quality image descriptions that are\ntightly-coupled enough with the image to train for or\nevaluate a deep alignment between the two domains.\nIn the absence of high quality captions to evaluate\nVLMs, benchmarks such as ARO (Yuksekgonul et al.,\n2023) and VL-Checklist (Zhao et al., 2023) often\ncomplement image-caption pairs with hard negatives\nthat are generated by slightly altering the initial\n(positive) description. Progress on these benchmarks\nhas been rooted in training VLMs with negatives of\nsimilar construction to the tests (Yuksekgonul et al.,\n2023) rendering the methodologies ineffective on\ndatasets such as Winoground (Thrush et al., 2022).\nRecent works (Lin et al., 2023) have called the\nevaluation capacity of many of these benchmarks into\nquestion, given how effective language-prior-based\nmethods perform.\nMore specifically, given the\nunlikeliness of the hard negative captions in these\nbenchmarks, a good text encoder can achieve close\nto 100% accuracy without looking at the images.\nMoreover, Bordes et al. (2023) have shown that most\nimprovements observed on ARO or VL-Checklist\n1\narXiv:2312.08578v1  [cs.CV]  14 Dec 2023\ndo not translate on simple synthetic benchmarks for\nwhich the negative caption is as likely as the positive\none. Since the use of VLMs is significantly increasing,\nit is crucial to make sure that we have a diverse suite\nof reliable benchmarks to asses their abilities.\nIn this paper, we introduce the Densely Captioned\nImages dataset, a collection of 8012 images with dense\nand mask-aligned descriptions averaging above 1000\nwords each. One such example is provided in Fig-\nure 1, displaying just a subset of the collected text\npaired with their aligned masks. We demonstrate\nhow to leverage this dataset to evaluate VLMs in two\nways after summarizing captions to fit into CLIP\u2019s\n77 token limit, both with a negatives-based test as\nwell as a novel matching task, referred to as subcrop-\ncaption matching, that requires selecting appropriate\ncaptions for different regions of the same image. We\nevaluate existing baselines, and observe that no mod-\nels perform well at both concurrently, and improved\nperformance via negatives-based training comes at\nthe cost of decreased performance on subcrop-caption\nmatching. We also run some experiments using the\nsummarized DCI as fine-tuning dataset to evaluate\nthe effectiveness of these captions for improving a\nmodel\u2019s performance on other benchmarks, and com-\npare the efficiency per-example to that from the auto-\nmated annotation setup in DAC (Doveh et al., 2023a).\nTo summarize, our contributions are:\n\u2022 We release the Densely Captioned Images (DCI)\ndataset, which contains dense and mask-aligned\ncaptions, alongside an LLM-summarized version\n(sDCI) containing captions under 77 tokens for\nuse with current VLMs.\n\u2022 We provide a new benchmark for VLMs based\non sDCI to evaluate fine-grained vision-language\nunderstanding, and show that no existing model\ncan perform well at matching captions from\nwithin one image to corresponding sub-sections\nof that image.\n\u2022 We show that fine-tuning with high quality\nimage-caption pairs is as good on ARO and\nVL-Checklist as fine-tuning on at least 10\u00d7 the\nautomatically annotated data, and that even\nwithout utilizing explicit negatives these pairs\ncan improve performance on VL-C-Object from\n81.17% to 88.37% .\n2\nRelated Works\nThe massive, loosely-labeled dataset approach that\nhas enabled VLMs like CLIP (Radford et al., 2021)\nand powerful successors like BLIP2 (Li et al., 2023a),\nFlamingo (Alayrac et al., 2022), CM3leon (Yu et al.,\n2023), and many others, has been a clear forward step\nin vision-language modeling. Still recent benchmarks\nshow that models trained in this manner display clear\ndrawbacks in reasoning skills. Additional techniques\nhave been proposed and adopted recently to close\nthis gap, discussed below.\nVision-Language Datasets.\nOver the last decade,\nthere have been significant dataset collection efforts\nconnecting images and text. Earlier works focused\non curating datasets by leveraging human annota-\ntions, see e.g., COCO (Chen et al., 2015), Visual\nGenome (Krishna et al., 2016), and Flickr30k (Young\net al., 2014). The process resulted in high quality an-\nnotations, which were however oftentimes limited by\nthe caption content \u2013 i.e., relatively short phrases (5.1\nto 10.3 words on average) grounded at image level or\nregion level \u2013 and the data annotation scale (30k to\n130k images). To increase scale, researchers gathered\nweb-crawled data and introduced large scale datasets\nsuch as YFCC100M (Thomee et al., 2016), which\ncontains 100M media objects. Yet, crawling the web\noftentimes results in little correspondence between\nimage and text pairs. To reduce noise between image\nand text pairs, efforts such as SBU (Ordonez et al.,\n2011) queried Flickr and filtered the noisy results,\nobtaining a \u223c1M images. Moreover, Conceptual Cap-\ntions (CC) (Sharma et al., 2018) crawled a dataset\nof \u223c12M images and alt-text pairs, and included a\nprotocol to filter noisy text-image pairs, resulting in\n3M data points. Relaxing the filtering protocol allows\nto trade data quality for scale. Crawling alt-text also\nresulted in relatively short text descriptions with 10.3\nwords on average, which are most often grounded at\nimage level. Localized Narratives (Pont-Tuset et al.,\n2020) was introduced as a dense visual grounding\ndataset leveraging a multi-modal annotation pro-\ncedure, collecting \u223c850k text-image pairs with 36.5\nwords/caption on average.\nRedCaps (Desai et al.,\n2021) constituted another effort yielding large scale\n(\u223c12M) web-curated data by exploring alternate\ndata sources of high quality data instead of devising\ncomplex filtering strategies. Wikipedia-based image-\ntext dataset (WIT) (Srinivasan et al., 2021) extended\ndataset creation efforts by gathering a multilingual\ndataset of text-image-pairs consisting of \u223c11.5M\nimages. LAION-5B (Schuhmann et al., 2022) further\nincreased the web-crawling efforts by gathering a\nmultilingual dataset of text-image pairs, and filtered\nthe collected data with a pre-trained CLIP (Radford\net al., 2021) model. Following, LAION-CAT (Rade-\nnovic et al., 2023) reduced noisy examples from\nLAION-5B by filtering for caption complexity, i.e.,\ncaptions that do not contain any action, and for\n2\nFigure 1 One example from the Densely Captioned Images dataset. Only part of the submask hierarchy is shown.\n3\ntext spotting, i.e., images that contain rendered text.\nMetaCLIP (Xu et al., 2023b) has also been released\nas an open dataset for reproducing CLIP. These very\nlarge scale datasets have been successfully used to\nadvance the state-of-the-art of VLMs.\nVision-Language Evaluation Benchmarks.\nSeveral\nrecent advances in visual-language learning have fo-\ncused on creating comprehensive benchmarks to eval-\nuate model performance in more holistic ways. These\nbenchmarks are instrumental in pushing the envelope\nof what VLM can understand and process, ensur-\ning they move beyond superficial image-text match-\ning towards genuine understanding of intricate re-\nlationships between visual and linguistic elements.\nIn particular, VL-CheckList (Zhao et al., 2023) and\nARO (Yuksekgonul et al., 2023) assess the VLM ca-\npabilities beyond average downstream task accuracy,\nby focusing on a model\u2019s ability to understand ob-\njects, attributes, order or relations. ARO\u2019s extensive\nscope, uncovers limitations in VLMs such as poor\nrelational understanding and lack of order sensitiv-\nity. Winoground (Thrush et al., 2022) tests models\nfor visio-linguistic compositional reasoning by asking\nVLM to match two images with two captions con-\ntaining the same set of words but in different orders.\nThis task requires models to discern the meaning\nconveyed by the order of words, reflecting different vi-\nsual scenes. Current VLMs perform only marginally\nbetter than chance, highlighting a significant gap\nin compositional reasoning. CREPE (Compositional\nREPresentation Evaluation) (Ma et al., 2023) evalu-\nates two aspects of compositionality: systematicity\nand productivity. Systematicity is measured by the\nmodel\u2019s ability to represent seen versus unseen atoms\nand their compositions, while productivity gauges\nthe model\u2019s capacity to understand an unbounded\nset of increasingly complex expressions. Finally, PUG\n(Photorealistic Unreal Graphics) (Bordes et al., 2023)\nuses synthetic data to asses the compositional reason-\ning abilities of VLMs by progressively increasing the\ncomplexity of a given generated scene. One issue with\nthese evaluation datasets is their frequent reliance\non COCO, either directly as in ARO, or through\nVisual Genome as in VL-Checklist or CREPE. It is\ndifficult to find an evaluation dataset of sufficient\nscale without COCO.\nVision-Language models.\nRecent VLM advance-\nments have built upon the foundational work of\nCLIP (Radford et al., 2021), which leveraged large-\nscale image-text pairs to jointly pre-train an image\nencoder and a text encoder to predict which images\nare paired with which texts in a contrastive learning\nparadigm. NegCLIP build upon CLIP by leveraging\nnegative captions when training. BLIP (Bootstrap-\nping Language-Image Pre-training) (Li et al., 2022b)\nuses a new framework that bootstraps the captions\nfrom noisy web data for both understanding and\ngeneration tasks.\nIts successor BLIP-2 (Li et al.,\n2023a) further streamlines the process by utilizing\noff-the-shelf frozen pre-trained image encoders and\nlanguage models, bridging the modality gap with a\nlightweight querying mechanism. Clip-rocket (Fini\net al., 2023) improves VLM baselines by showing\nthat applying image and text augmentations makes\nup for most of the improvement attained by prior\nVLMs. Flava (Singh et al., 2022) proposes a founda-\ntion VLM model by combining existing VLMs objec-\ntives together with auxiliary in-modality losses for the\ntext and vision encoders. X-VLM (Zeng et al., 2021)\nachieves success with a pretraining method matching\nsub-portions of the text to regions of the image at mul-\ntiple granularities. These models introduces improve-\nments over CLIP, focusing on efficiency, adaptability,\nand reducing the need for extensive labeled datasets,\nthereby pushing the boundaries of vision-language\npre-training. The closest work to our approach is DAC\n(Densely Aligned Captions) (Doveh et al., 2023a),\nwhich improves with an automated LLM based\npipeline the caption quality and density. By showing\nthat DAC-enhanced CLIP models exhibit substantial\ngains on some benchmarks, this work underscores the\ncritical role that caption quality and density play in\nthe efficacy of VLMs. We build on this insight and ex-\nplore how to further increase the caption quality and\ndensity by relying on human annotators, and analyze\nhow that impacts downstream model performance.\n3\nDataset Construction\nThe Densely Captioned Images dataset, or DCI, con-\nsists of 8012 images from SA-1B (Kirillov et al., 2023),\neach with a complete description aiming to capture\nthe full visual detail of what is present in the im-\nage. Much of the description is directly aligned to\nsubmasks of the image. An example is shown in Fig-\nure 1. In the top left we see the full image of a water\npump, with an associated description. The italicized\nsection is collected as a standard caption, aiming to\nsummarize the full image in about a sentence, sim-\nilar to existing caption datasets. The remainder of\nthat first description contains details about the re-\nlationship between visible entities in the image, as\nwell as in-depth descriptions of regions that are not\ndescribed as part of the submasks. All other text\ndescribing the image is associated with submasks of\nthe image. Each submask has its own free-text label\n4\nFigure 2 Annotation view for writing description for masks of the image. The masked region appears highlighted for\nclarity.\n(not pictured) and description, and may also contain\nfurther submasks. Here for instance we see submasks\nfor windows and balconies as being contained in the\nsubmask capturing three buildings in the background.\n3.1\nPreparation\nIn order to collect the data, we first select images from\na random privacy-mitigated subset of SA-1B. We then\nprocedurally extract subregions of each image to an-\nnotate, as we found in initial trials that crowdsourcing\nboth regions and descriptions concurrently overcom-\nplicated the task and successful annotation rate. For\nthis process, we turn to the Segment Anything Model\n(SAM) (Kirillov et al., 2023) and adapt their standard\nmethod to extract all masks from an image.\nFor the extraction process, SAM usually relies on\na grid of points across the entire image. In order\nto increase the possibility of selecting interesting\nregions worth annotating, we additionally apply a\ncanny filter (Canny, 1986) and select random points\nwithin a radius from discovered edges. We then run\nSAM to detect all masks using both the grid and the\nnear-edge points. Once the masks are returned, we\nestablish a hierarchy of submasks by thresholding the\nnumber of overlapping pixels between two masks to\ndetermine if one is a submask of the other, or if the\ntwo masks should be joined. This helps reduce some\nof the noise introduced by the automatic masking\nprocess, and leaves us with a tree-like structure for\nthe masks. Lastly, we remove any masks that are too\nsmall.We note that undergoing this process does not\nresult in every detail of each image being selected as\na candidate for annotation, and as such instances in\nthe DCI dataset are not expected to have complete\nsubmask-aligned coverage of all elements one could\nrecognize in or discuss about an image.\n3.2\nCollection Process\nWe use Mephisto (Urbanek and Ringshia, 2023) to\nhost our task, pay crowdworkers to provide annota-\ntions on the dataset, and additionally run qualifica-\ntion steps. Workers that pass our qualifications are\neligible to work on the main task which contains 3\nstages:\n1. Workers are provided with the whole image, and\nasked to provide a short description of it. This\nis considered the standard caption.\n2. Workers are provided with submasks of the im-\nage, one at a time starting with the leaves of the\nmask tree, displaying a SAM-selected region of\nthe image as well as an indicator for where that\nregion comes from. They are generally asked\nto provide a label and complete description for\nthe pictured region, though are allowed to mark\nthe region as \u2018uninteresting\u2019 and only provide\na label, or \u2018bad\u2019 and provide nothing. These\noptions allow us to focus worker time on use-\nful annotations and help capture some of the\nnoise of the automatic selection process. This\nis shown in Figure 2. For masks that contain\nsubmasks, workers are also provided with over-\nlays that show the regions already annotated,\nand are asked to annotate in terms of what has\nalready been written.\n3. After completing all the submasks, the worker\nis then shown the complete image again and\nasked to provide an overall description, paying\nattention to the relationship between previously\nannotated regions and attempting to include\n5\nFigure 3 Example of a Llama2-generated summary and negative that comprise sDCI. Each image and submask have\nmultiple summarizations and negatives. We also compare the caption quality between DAC (Doveh et al., 2023a)\nand DCI. In contrast to DCI that relies on human annotations, DAC used an automatic pipeline based on LLM for\ncaptioning. As we observe in this example, the DAC captions can suffer from hallucinations and miss important\nelements of the photo. In this work we argue that while improving automatic pipeline is an important research\ndirection, for now the captions proposed are not reliable enough to be used to evaluate models and assess their abilities.\nin-depth detail about regions that were not pre-\nviously annotated.\nAn in-depth description of the filtering and quality\nassurance process can be found in Appendix B while\nthe Datasheet (Gebru et al., 2021) is available in Ap-\npendix F. Complete annotation instructions, dataset\ndownload links as well as reproducible code are avali-\nable on our GitHub. The DCI dataset is released\nunder the CC-BY-NC license.\n3.3\nFitting DCI into 77 CLIP tokens\nUltimately, we collected an average of 1111 words\n(1279 CLIP tokens) per image, with a median of 941\nwords.\nThis proves problematic for evaluating or\nfine-tuning CLIP-based VLMs given their maximum\ntext token length of 77.\nEmbedding pooling\nmethods (Chen et al., 2018) to extend the effective\ninput size for text modeling is an active research\narea (Chalkidis et al., 2022; Zhang et al., 2022), and\ncurrent work suggests average-pooling embeddings\nover these longer descriptions would be ineffective.\nOne possible approach would be to utilize the subsec-\ntions of the image while providing the corresponding\nsubcaption, in a manner akin to a multi-modal multi-\ncrop approach (Caron et al., 2021). Still, even when\nconsidering just the 91,424 submasks, the average to-\nken length is nearly 200 per caption. We instead use\nthe longer context capabilities of Llama2 (Touvron\net al., 2023) to summarize down the overall infor-\nmation in the image into CLIP-consumable portions.\nWe generate multiple captions for each image and\nsubmask, using prompts that attempt to summarize\ndown recursively until the result is in bounds. As\nthis modification to the dataset is generated auto-\nmatically, the summarizations may have introduced\nnoise, and may not capture all of the detail in the full\noriginal captions. Summarizations also occasionally\nmix references or include context in a submask that\nisn\u2019t the main focus. Still, the summaries are fairly\nhigh quality and more dense than those found in\nother datasets, especially when using more than one\ndistinct summarization per image. We also prompt\nthe LLM to generate negatives from these summaries,\nachieving a set of particularly hard negatives for CLIP\nto evaluate. We call this version of the dataset sum-\nmarized DCI (sDCI). Examples of full caption, LLM\nsummary and LLM negative are included in Figure 3\nand contrasted with DAC (Doveh et al., 2023a) data.\nMore detail including the prompts used can be found\nin Appendix A.\nUltimately, this fitting step produces a lower bound\n6\non the level of vision-language understanding \u2018res-\nolution\u2019 that the overall DCI dataset is capable of\nevaluating a model for. As newer models arise that\nare able to handle embedding much larger quantities\nof text content, it will be possible to make full use of\nDCI\u2019s original annotated captions.\n3.4\nStatistics\nDataset\nImgs\nCaps\nToks/Cap\nToks/Img\nDCI\n8,012\n8,012\n1,282.09\n1,282.09\nDCIsub\n99,445\n99,445\n199.33\n199.33\nsDCI\n8,012\n87,268\n49.21\n536.00\nsDCIsub\n99,445\n714,630\n36.60\n263.01\nLNCOCO\n142,845\n142,845\n49.11\n49.11\nLNCOCO<77\n127,456\n127,456\n43.70\n43.70\nCOCO\n123,287\n616,767\n13.54\n67.74\nTable 1 Comparison of DCI dataset statistics to other\ndatasets, focusing on average CLIP tokens per image\nor caption. Note the 26x difference between DCI and\nthe previous longest annotated dataset, Localized Nar-\nratives (LN). sub denotes including submasks and their\ndescriptions as examples, and sDCI refers to the LLM-\nsummarized version of DCI that fits captions to 77 tokens\n(Sec.\n3.3), while LNCOCO<77 simply drops examples\nlonger than 77 tokens (\u223c 10.8%).\nAll-in-all the Densely Captioned Images dataset is\nfar more dense than Localized Narratives on COCO\nimages (Pont-Tuset et al., 2020) (later referred to as\nLNCOCO) and nearly 100\u00d7 more dense than standard\nCOCO captions (Chen et al., 2015). After reducing\nto CLIP-bounded summaries, it still contains more\ntext density than both.\nComplete details can be\nfound in Table 1.\nHere we see that the multiple-summarization method\nof sDCI produces fairly similar token per image val-\nues to the original dataset while keeping individual\ncaptions\u2019 token lengths in bounds for CLIP. To get\nLocalized Narratives into the 77 token bound, we\nsimply drop longer examples.\n4\nEvaluating VLMs with summarized\nDCI\n4.1\nMethodology\nUsing the 8012 images in the summarized Densely\nCaptioned Images (sDCI) dataset, we construct a few\ndifferent evaluations. As noted above, the ability to\nselect multiple submasks from the same image and\ninclude them in the same batch allows us to create\na CLIP-style test, wherein the model can evaluate\na full batch of images and captions and score cor-\nrectly which caption belongs to which image. As\nwe provide models with a crop around the selected\nmasks, we call this Subcrop-Caption Matching (SCM),\nand we use a batch size of 8. We can run against\nour LLM-generated negatives as well. Given that\nLLM-summarization has provided us with multiple\ncaptions and negatives per image and submask, we\nsupply the first unless noted otherwise. With this in\nmind, we construct 6 evaluations as follows:\n[All SCM]: Group each image with their subcrops,\nalongside one summarized caption per subcrop. Then\nuse the model to find the most likely caption associ-\nated to each subcrop. This test measures the ability\nof the VLM to distinguish between the different parts\nthat compose an image.1\n[All Neg]: Select one LLM summarized caption and\nthe corresponding LLM-generated negative for each\nimage and subcrop. Score a model on its ability to\ndistinguish between the positive and negative.\n[All Pick5-SCM]: Use the same setup as All SCM, but\nrather than using only one caption per subcrop, we\nuse 5 LLM generated captions per subcrop. We score\na model as succeeding only when the worst-scoring\npositive caption scores higher than the best-scoring\ncaption of any other image in the batch. This test\nevaluates if the representation space is structured\nsuch that captions belonging to a specific image are\nclosest to the target image in the space.\n[All Pick5-Neg]: Use the same setup as All Neg, but\nrather than using one caption, we use 5 LLM sum-\nmarized captions for each image and subcrop. If any\nof these captions score worse than the negative, the\nmodel fails the example.\n[Base Neg]: Using only the 8012 base images without\nsubcrops, evaluate the model\u2019s ability to distinguish\nbetween an LLM generated caption and its corre-\nsponding LLM-generated negative. Note, this is a\nstrict subset of All Neg, though these captions are\non the longer side on average and cover a different\ndistribution.\n[All Hard-Negs]: Using the same setup as All Neg, but\nrather than using a single negative, use the negative\nacross all LLM-generated negatives that CLIP scores\nhighest.\n4.2\nResults\nWe compare in Table 2 the sDCI performances given\nby different state-of-the-art models: CLIP (Radford\n1Since we used sDCI to fit current models token length, it\nis possible that some of the summaries remove the information\nthat make possible to distinguish between the captions. Ideally\nthis test should be performed on the non-summarized version\nonce VLMs can handle 1000+ tokens.\n7\nAll\nAll Pick5\nBase\nAll\nModel\nSCM\nNeg\nSCM\nNeg\nNeg\nHard Negs\nCLIP Baseline (Radford et al., 2021)\n40.12%\n60.63%\n11.28%\n24.03%\n67.66%\n41.29%\nNegCLIP (Yuksekgonul et al., 2023)\n43.35%\n56.00%\n13.22%\n4.82%\n76.69%\n50.84%\nBLIP (Li et al., 2022b)\n39.13%\n54.02%\n10.73%\n5.51%\n63.41%\n53.23%\nFlava (Singh et al., 2022)\n38.08%\n47.99%\n8.01%\n9.82%\n11.6%\n45.59%\nX-VLM (Zeng et al., 2021)\n38.45%\n53.46%\n10.96%\n5.10%\n44.29%\n52.42%\nDACLLM(Doveh et al., 2023a)\n37.48%\n81.75%\n8.27%\n37.96%\n90.54%\n71.29%\nDACSAM (Doveh et al., 2023a)\n37.90%\n84.22%\n6.78%\n39.91%\n89.68%\n73.68%\nTable 2 sDCI test result: We compare existing baselines on our Subcrop-Caption Matching (SCM) and negatives tests.\nAdditional results are available in Table 10 in the Appendix. We note our best model fine-tuned on sDCI from section\n5 achieved 64.02% and 31.60% on a held-out test of All SCM and All SCM Pick5 respectively, setting an upper bound for\nmodel performance.\net al., 2021), NegCLIP (Yuksekgonul et al., 2023),\nBLIP2 (Li et al., 2023a), Flava (Singh et al., 2022)\nand X-VLM (Zeng et al., 2021). Additional exper-\niments on different architectures and pretraining\ndatasets are available in Table 10 (see Appendix).\nThe CLIP baseline starts at 40.12% on All SCM and\n60.63% on All Neg. The only model to improve over\nCLIP on SCM tasks is NegCLIP, which follows the\nfact that the hard image negatives that NegCLIP\nis trained on provide the most similar task to what\nwe test of any of these models. None of the models\ntrained without an explicit CLIP-loss component out-\nperform CLIP on SCM tasks, but DAC ultimately\nperforms the worst.\nPerformance on the Pick5 variations of each task\nfollow the trends of the standard performance. Per-\nformance on BaseNeg for Flava point to a weakness in\ncomparing longer text examples, given the significant\ndrop from 47.99% to 11.6% that is not demonstrated\nin other models.\nInterestingly, models trained absent of CLIP (BLIP,\nFlava, X-VLM) experience a far less noticeable drop\nin performance between All Neg and All Hard Negs.\nThis validates that sDCI\u2019s CLIP-hard negatives are\nnot simply a higher proportion of \u2018impossible\u2019 nega-\ntives, but rather capture some underlying trait about\nthe negatives that CLIP models and their descen-\ndants all struggle with.\nNone of the models presented perform well across all\nof the sDCI test set. Given each of the CLIP-style\nmodels have some kind of advantage on this test set\ndue to being trained on some objective that sDCI\ndirectly evaluates, we expect that the BLIP, Flava,\nand X-VLM scores are somewhat representative for\nexisting state-of-the-art models\u2019 true performance on\nthis test set.\n5\nUsing summarized DCI as fine-\ntuning dataset\nTo evaluate the use and difficulty of the sDCI dataset\nfor training, we fine-tune state-of-the-art models with\nit. In particular, we use a ViT/32B CLIP model in\nall of our experiments, requiring use of the CLIP-\nbounded version of our dataset. We split sDCI into\n7800 train, 100 validation, 112 test samples for this\npurpose. We use a training batch size of 32 and a\nlearning rate of 5e-5 for all experiments, and run\nfor up to 10 epochs. We train using both standard\nCLIP loss as well as an additional Negatives loss\ncomponent, which follows the \u2018text negative\u2019 of Neg-\nCLIP (Yuksekgonul et al., 2023). Given the tiny size\nof our finetuning sets relative to the 400M pretraining\nimages, we use LoRA (Hu et al., 2021) to reduce the\ntrainable parameters. We train a model with and\nwithout negatives loss.\nIn order to make good use of the multiple summa-\nrized captions we have per image and submask, we\nrandomly select one to be used in each individual\nepoch. We call this method Pick1. We describe this\nmethod and other ablations we attempted in more\ndetail in Appendix C.\nWe follow the experimental setup of DAC (Doveh\net al., 2023a) by evaluating our sDCI fine-tuned\nCLIP on the ARO and VL-Checklist benchmarks.\nWe compare to DAC directly as it is the most similar\nwork to ours in attempting to increase caption\ndensity. As noted in Figure 3, these automatically\ngenerated captions are generally noisy. As DAC is\nusing 3M images for fine-tuning, we performed a\nsmall ablation on the number of DAC images to use\nfor fine-tuning to be similar to our base image count\n(10,000 compared to our 8,012), or to our full mask\ncount (100,000 compared to our 99,445).\n8\nARO\nVL-Checklist\nModel\nVG-R\nVG-A\nCOCO\nFLICKR\nObject\nAttribute\nRelation\nsDCIP 1\n76.23%\n67.56%\n88.58%\n91.30%\n80.71%\n68.69%\n70.12%\nsDCIP 1NL0\n57.34%\n61.98%\n39.36%\n44.62%\n88.37%\n70.42%\n61.28%\nDACLLM10,000\n61.53%\n63.89%\n46.28\u00b11.5%\n59.41\u00b11.9%\n66.90%\n57.4%\n56.96%\nDACLLM100,000\n61.0%\n63.6%\n48.2%\n61.42%\n66.87%\n57.22%\n57.18%\nDACLLM500,000\n60.1%\n63.8%\n50.2%\n61.6%\n66.54%\n57.39%\n56.77%\nDACLLM3,000,000\n81.28%\n73.91%\n94.47%\n95.68%\n87.30%\n77.27%\n86.41%\nDACSAM3,000,000\n77.16%\n70.5%\n91.22%\n93.88%\n88.50%\n75.83%\n89.75%\nCLIP Baseline Radford et al. (2021)\n59.98%\n63.18%\n47.9%\n60.2%\n81.17%\n67.67%\n61.95%\nBLIP2 Li et al. (2023a)\n41.16%\n71.25%\n13.57%\n13.72%\n84.14%\n80.12%\n70.72%\nNegCLIP Yuksekgonul et al. (2023)\n81%\n71%\n86%\n91%\n81.35%\n72.24%\n63.53%\nSVLC Doveh et al. (2023b)\n80.61%\n73.03%\n84.73%\n91.7%\n85%\n71.97%\n68.95%\nTable 3 sDCI fine-tuned CLIP performance against the ARO and VL-Checklist benchmark. We compare CLIP\nfine-tuned with sDCI against models fine-tuned using DAC captions. Since the DAC dataset contains 3M images\nwhereas sDCI contains only 8012 images, we performed an ablation of the number of training images used in the DAC\ndataset. In this instance, DACLLM10000 refer to fine-tuning CLIP using only 10,000 images from DAC. We plot the\nmean across 5 different seeds and display the standard deviation when it is above 1% accuracy. We observe that\ntraining on sDCI lead to significant improvement in comparison to DAC for a comparable number of examples.\n5.1\nResults\nIn Table 3, we show that, while the DCI Pick1 model\ntrained with negatives loss (sDCIP 1) does not reach\nthe performance of DAC models trained on 3M im-\nages, it does improve over the CLIP baseline on most\nmetrics2, and outperforms some baselines trained on\nmore data. sDCIP 1 does however outperform both\nsample-limited ablations of DAC, suggesting that\na small number of highly aligned image to dense\ntext pairs are more effective for training models than\nlarger quantities of more loosely aligned or sparse\ndata. Unsurprisingly, the version trained without\nnegatives loss, sDCIP 1NL0, does not improve across\nmost benchmarks, and even somewhat degrades when\ncompared to the CLIP baseline.3 Of note however is\nthe significant bump in VL-Object, alongside some\nimprovement to VL-Attribute. Improvements here\nsuggest that the sDCI dataset successfully includes\nmore object, and to a lesser degree attribute, infor-\nmation than the captions in the source dataset for\nCLIP. It does, however, point to a limitation of using\nthe LLM summarizations and not incorporating mask\ninformation, as relational information is sometimes\nlost.\n2The decreased performance on VL-Object may be ex-\nplained by our LLM-generated negatives not closely covering\nthe test set negatives.\n3The degradation is likely due to the distribution shift and\nsmall sample size, given the training objective is the same as\nCLIP.\n6\nConclusion and Future Work\nIn closing, we introduce the Densely Captioned Im-\nages dataset, and display clear use for it as a eval-\nuation benchmark. We also show initial potential\nfor using the dataset for fine-tuning. Given that in\norder to evaluate today\u2019s models on DCI we had to\nreduce the size of the text to only 77 tokens, DCI\nshould prove to be useful for a longer period of time\nas models that are able to consume and utilize larger\namounts of text context become the norm. We en-\nvision that in those cases the full human annotated\ncaptions without length reduction would be provided.\nToday\u2019s context size limitation also prevented us from\nfine-tuning existing models on the highly aligned text-\nimage data within DCI, as existing models don\u2019t have\nenough context size to handle the full text, but the\ndataset isn\u2019t nearly large enough to pre-train a new\nset of models that could use the full text. It could be\nrelevant to treat developing highly aligned text-image\ndatasets in a similar manner to that used in machine\ntranslation for low-resource languages, which run into\na similar issue with cost and difficulty to collect. This\narea of work has relied on automated methods such\nas bitext mining (Team et al., 2022) to bootstrap\nup from an initial set of expertly collected examples,\nwhich DCI may already provide the foundation for.\nFurther, we haven\u2019t attempted to incorporate the\npixel-level masks that the dataset has in any of our\nexperiments, instead opting to use crops around the\nmasks to retain parity with our test set. This dataset\nis unique for both the extreme density and high de-\ngree of alignment present, and in this introductory\nwork we\u2019ve only scratched the surface of using this\ninformation to its fullest extent.\n9\nReferences\nAmro Kamal Mohamed Abbas, Kushal Tirumala, Daniel\nSimig, Surya Ganguli, and Ari S. Morcos.\nSemd-\nedup: Data-efficient learning at web-scale through se-\nmantic deduplication.\nIn ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foun-\ndation Models, 2023. https://openreview.net/forum?\nid=4vlGm9gv6c.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Ro-\nman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,\nZhitao Gong, Sina Samangooei, Marianne Monteiro, Ja-\ncob Menick, Sebastian Borgeaud, Andrew Brock, Aida\nNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,\nRicardo Barreira, Oriol Vinyals, Andrew Zisserman,\nand Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning, 2022.\nFlorian Bordes, Shashank Shekhar, Mark Ibrahim, Diane\nBouchacourt, Pascal Vincent, and Ari S. Morcos. Pug:\nPhotorealistic and semantically controllable synthetic\ndata for representation learning. In Advances in Neural\nInformation Processing Systems, 2023.\nJohn Canny. A computational approach to edge detection.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, PAMI-8(6):679\u2013698, 1986. doi: 10.1109/\nTPAMI.1986.4767851.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised\nlearning of visual features by contrasting cluster assign-\nments, 2021.\nIlias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos\nMalakasiotis, and Desmond Elliott. An exploration of\nhierarchical attention transformers for efficient long\ndocument classification, 2022.\nQian Chen, Zhen-Hua Ling, and Xiaodan Zhu. Enhancing\nsentence embedding with generalized pooling, 2018.\nXinlei\nChen,\nHao\nFang,\nTsung-Yi\nLin,\nRamakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollar, and\nC. Lawrence Zitnick. Microsoft coco captions: Data\ncollection and evaluation server, 2015.\nKaran Desai, Gaurav Kaul, Zubin Aysola, and Justin\nJohnson. Redcaps: web-curated image-text data cre-\nated by the people, for the people, 2021.\nSivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig,\nDonghyun Kim, Paola Cascante-bonilla, Amit Alfassy,\nRameswar Panda, Raja Giryes, Rogerio Feris, Shimon\nUllman, and Leonid Karlinsky.\nDense and aligned\ncaptions (dac) promote compositional reasoning in vl\nmodels, 2023a.\nSivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar\nPanda, Roei Herzig, Eli Schwartz, Donghyun Kim,\nRaja Giryes, Rogerio Feris, Shimon Ullman, and Leonid\nKarlinsky. Teaching structured vision&language con-\ncepts to vision&language models, 2023b.\nEnrico Fini, Pietro Astolfi, Adriana Romero-Soriano,\nJakob Verbeek, and Michal Drozdzal. Improved base-\nlines for vision-language pre-training. Transactions on\nMachine Learning Research (TMLR), 2023.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione,\nJennifer Wortman Vaughan, Hanna Wallach, Hal\nDaum\u00e9 III au2, and Kate Crawford. Datasheets for\ndatasets, 2021.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language\nmodels, 2021.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDoll\u00e1r, and Ross Girshick. Segment anything, 2023.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\nKalantidis, Li-Jia Li, David A. Shamma, Michael S.\nBernstein, and Fei-Fei Li. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image\nannotations, 2016.\nChunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan\nZhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong\nHu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao.\nElevater:\nA benchmark and toolkit for evaluating\nlanguage-augmented visual models, 2022a.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. BLIP: bootstrapping language-image pre-training\nfor unified vision-language understanding and genera-\ntion. CoRR, abs/2201.12086, 2022b.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training\nwith frozen image encoders and large language models,\n2023a.\nXianhang Li, Zeyu Wang, and Cihang Xie. An inverse\nscaling law for clip training, 2023b.\nYanghao Li, Haoqi Fan, Ronghang Hu, Christoph Fe-\nichtenhofer, and Kaiming He. Scaling language-image\npre-training via masking, 2023c.\nZhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan\nZhang, and Deva Ramanan. Visualgptscore: Visio-\nlinguistic reasoning with multimodal generative pre-\ntraining scores, 2023.\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-\nlanguage foundation models reason compositionally?,\n2023.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\nIm2text:\nDescribing images using 1 million cap-\n10\ntioned photographs. In J. Shawe-Taylor, R. Zemel,\nP. Bartlett, F. Pereira, and K.Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems,\nvolume 24. Curran Associates, Inc., 2011.\nhttps:\n//proceedings.neurips.cc/paper_files/paper/2011/\nfile/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo,\nRadu Soricut, and Vittorio Ferrari. Connecting vision\nand language with localized narratives, 2020.\nFilip Radenovic, Abhimanyu Dubey, Abhishek Kadian,\nTodor Mihaylov, Simon Vandenhende, Yash Patel,\nYi Wen, Vignesh Ramanathan, and Dhruv Mahajan.\nFiltering, distillation, and hard negatives for vision-\nlanguage pre-training. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pages 6967\u20136977, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable vi-\nsual models from natural language supervision. CoRR,\nabs/2103.00020, 2021.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs, 2021.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, Patrick Schramowski, Srivatsa Kundurthy,\nKatherine Crowson, Ludwig Schmidt, Robert Kaczmar-\nczyk, and Jenia Jitsev. Laion-5b: An open large-scale\ndataset for training next generation image-text models,\n2022.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. Conceptual captions: A cleaned, hy-\npernymed, image alt-text dataset for automatic im-\nage captioning.\nIn Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 2556\u20132565, Mel-\nbourne, Australia, July 2018. Association for Com-\nputational Linguistics.\ndoi: 10.18653/v1/P18-1238.\nhttps://aclanthology.org/P18-1238.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-\nlaume Couairon, Wojciech Galuba, Marcus Rohrbach,\nand Douwe Kiela. FLAVA: A foundational language\nand vision alignment model. In CVPR, 2022.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen, Mike\nBendersky, and Marc Najork. Wit: Wikipedia-based\nimage text dataset for multimodal multilingual ma-\nchine learning.\nIn Proceedings of the 44th Interna-\ntional ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (SIGIR \u201921), 2021.\nhttps://arxiv.org/abs/2103.01913.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur\n\u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman,\nSemarley Jarrett,\nKaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau Tran,\nPierre Andrews, Necip Fazil Ayan, Shruti Bhosale,\nSergey Edunov, Angela Fan, Cynthia Gao, Vedanuj\nGoswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexan-\ndre Mourachko, Christophe Ropers, Safiyyah Saleem,\nHolger Schwenk, and Jeff Wang.\nNo language left\nbehind: Scaling human-centered machine translation,\n2022.\nBart Thomee, David A. Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. YFCC100m. Communications of\nthe ACM, 59(2):64\u201373, jan 2016. doi: 10.1145/2812802.\nhttps://doi.org/10.1145%2F2812802.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. Winoground: Probing vision and language mod-\nels for visio-linguistic compositionality, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nMoya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-\nthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,\nMarcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\nabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,\nRashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xi-\naoqing Ellen Tan, Binh Tang, Ross Taylor, Adina\nWilliams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,\nIliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kam-\nbadur, Sharan Narang, Aurelien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. Llama\n2: Open foundation and fine-tuned chat models, 2023.\nJack Urbanek and Pratik Ringshia. Mephisto: A frame-\nwork for portable, reproducible, and iterative crowd-\nsourcing, 2023.\nHu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell\nHowes, Gargi Ghosh, Luke Zettlemoyer, and Christoph\nFeichtenhofer. Cit: Curation in training for effective\nvision-language data. arXiv preprint arXiv:2301.02241,\n2023a.\nHu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,\nRussell Howes, Vasu Sharma, Shang-Wen Li, Gargi\n11\nGhosh, Luke Zettlemoyer, and Christoph Feichtenhofer.\nDemystifying clip data, 2023b.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. Transactions of the Association for Com-\nputational Linguistics, 2:67\u201378, 2014. doi: 10.1162/\ntacl_a_00166. https://aclanthology.org/Q14-1006.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu,\nBinh Tang, Brian Karrer, Shelly Sheynin, Candace\nRoss, Adam Polyak, Russell Howes, Vasu Sharma,\nPuxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel\nSinger, Shang-Wen Li, Susan Zhang, Richard James,\nGargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi,\nAsli Celikyilmaz, Luke Zettlemoyer, and Armen Agha-\njanyan. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning, 2023.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou. When and why vision-\nlanguage models behave like bags-of-words, and what\nto do about it? In International Conference on Learn-\ning Representations, 2023.\nhttps://openreview.net/\nforum?id=KRLUvxh8uaX.\nYan Zeng, Xinsong Zhang, and Hang Li. Multi-grained\nvision language pre-training: Aligning texts with visual\nconcepts. arXiv preprint arXiv:2111.08276, 2021.\nHang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li,\nJiancheng Lv, Nan Duan, and Weizhu Chen. Pooling-\nformer: Long document modeling with pooling atten-\ntion, 2022.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-checklist: Evaluating pre-trained vision-language\nmodels with objects, attributes and relations, 2023.\n12\nAppendix\nA\nLLM Summarization and Negatives\nIn this section we discuss the prompting strategies\nwe used on the LLaMA 70B-chat model to produce\nour summarized captions and negatives.\nA.1\nSummarization\nIn order to generate multiple summaries for each\nimage and subsection of the image, we used the\nfollowing method.\nFirst, we attempted to create\na first-pass summary of either the full image or of\nmasks with the following prompts:\nFull Image: You are given a full-text description of\nan image.\nYou should summarize it into about\n65 words, being sure to include as much salient\nvisual information as possible given the 65 word\nconstraint, especially information from the start\nof the original desc- ription. The new description\nshould apply for the original image.\nRespond\nwith only the summary, in one line.\nSubmask: You are given a description of part of an\nimage. You should summarize it into a single line\nno longer than 65 words, being sure to include as\nmuch salient visual information as possible given\nthe 65. Don\u2019t include any details not present in\nthe provided description. Respond with only the\nsummary, in one line.\nThis provided us with once caption per image and\nsubmask, which is used universally as the first cap-\ntion for each. We, however, wanted to have more\noptions to capture potential details that may have\nbeen missed by the first summarization. To that end\nwe repeated the following prompt until we had at\nleast 6 captions per.\nMulti-caption Prompt: You are providing descrip-\ntions of an image.\nThe goal is to create a list\nof summarized descriptions that all accurately de-\nscribe the same image, but may pay attention to\nslightly different details. A complete description\nwill be provided. Complete 5 entries in this list,\neach a few sentences long.\nProvide in the for-\nmat:\\n1. <description>\\n2. ...\nA.2\nNegatives\nTo generate negatives, we came up with three\ndistinct prompts to produce negatives when given a\nsummary. All of the negatives are generated from\nthe first summarized caption. Each attempted to\nraise different kinds of reasoning, either for simple\nbasic edits, changes to structural information, or\nreconstructing a new sentence from the same bag of\nwords.\nbasic: You are given the description for an image.\nYou should provide a mostly similar description,\nchanging the original one slightly, but introducing\nenough significant differences such that the two\ndescriptions could not possibly be for the same\nimage.\nKeep the description length the same.\nProvide just the updated description.\nlayout: You are given the description for an image.\nYou should provide a mostly similar description,\nwith one part of it changed in a way that visibly\nalters the structure, layout, or content of the\nimage.\nThe change should introduce enough\ndifference such that the two descriptions could\nnot be for the same image. Keep the description\nlength the same.\nProvide just the updated\ndescription.\nswaps:\nYou are given a description.\nSelecting\nONLY from the same words, construct a random\nresponse with the same words in a completely new\norder.\nThe new description should not remain\naccurate to the first. Try to use all of the words\nfrom the original description, and keep the length\nthe same, but use the words nearly randomly such\nthat the scene makes less sense. Try to pair nouns\nand adjectives differently than in the original.\nUltimately, in initial ablations we found that the\nswaps prompt produced the best model performance,\nso we use these negatives as the default.\nA.3\nCommon Issues\nOne main issue with LLM-based summarization is\nthat LLaMA2 was not particularly effective at pro-\nducing answers within the specified word boundary.\nThus, we additionally use prompts that attempt to\nget the model to shorten the output response so long\nas it is somewhat close in length. If after enough tries\nthis didn\u2019t work, we\u2019d return to the original prompt.\nReduce: You are given the description for a scene.\nSummarize and rephrase the description such that\nthe new description is a few words shorter, but\nretains key information.\nAnother issue is that the chat fine-tuning on the\nLLaMA2 model made it frequently preface the re-\nsponse with additional affirmative text, like \u201csure\nI can do that...\u201d. Often this could be removed by\nsimply removing the everything before the last line\nbreak.\n13\nA.4\nOutput Text Examples\nBelow we provide an example of a summarization for\na base image (which includes details from all of the\nsubimages during the prompting stage) as well as a\nsummarization for one submask. We also share one\nexample of each type of type of negative for both.\nA.4.1\nBase Image\nDescription:\nThe upper floor of an ornate design\nbuilding with the words \u201cDem Wahren, Schonen\u201d vis-\nible on its top. There\u2019s a light source with five light\nbulbs decorating each pillar, at least two marble stat-\nues and multiple arches between each pillar.\nAn\nornate building with at least two floors. There\u2019s four\nsmall pillars with three black guard rails and four\nlight stands on the pillars. There\u2019s two marble stat-\nues on either end of the upper floor. The upper floor\nalso has five arches with decorative symbols above\nthem. There\u2019s six visible tall supporting pillars on\neither side of the arches. On the top is the visible\nwords: Dem Wahren Schoenon. On the bottom floor\nis three arches with a black light hanging from each\nof them. The inner arches and interiors are somewhat\nvisible. In front of the building is a black streetlight.\n...(submask text omitted)...\nSelected Summaries (3/8):\n(1) An ornate building\nwith four pillars, three arches, and a decorative facade.\nThe building has a upper floor with five light bulbs,\ntwo marble statues, and multiple arches. The lower\nfloor has three black guard rails and a streetlight.\nThe image includes various decorative elements such\nas swirling branches, leaves, and faces.\n(2) The image shows the upper floor of an ornate\nbuilding with five pillars, each with a light source\nand decorative elements. The building has a grand\nentrance with multiple arches and marble statues.\nThe words \u201cDem Wahren, Schonen\u201d are visible on the\ntop of the building.\n(3) The image highlights the right side of the upper\nfloor, focusing on the decorative elements on the\npillars and the arches.\nThe right most pillar has\na thinner middle and wider bottom, and the top\npillar decoration has swirly branches and a spread\nout decoration like a opened flower. The image also\nshows the inner pillars and the black guard rails.\nbasic negative:\nA grand building with four pillars,\nthree arches, and a detailed facade showcasing myth-\nical creatures and a clock.\nThe building\u2019s upper\nfloor boasts five light bulbs, two marble statues, and\nseveral arches, while the lower floor features three\nblack guard rails and a streetlight that illuminates\nthe vibrant flowers and trees outside.\nlayout negative:\nA grand building with intricate\ndetails, including four pillars, three arches, and a dec-\norative facade now covered in vines and moss, giving\nit a weathered and natural appearance. The upper\nfloor features five light bulbs, two marble statues,\nand multiple arches, while the lower floor has three\nblack guard rails and a streetlight.\nswaps negative:\nA facade with five arches, three\npillars, and four light bulbs. The building has a dec-\norative upper floor with two marble statues, multiple\nblack guard rails, and a streetlight. The lower floor\nhas three arches and various decorative elements such\nas swirling branches, leaves, and faces.\nA.4.2\nSubmask\nDescription:\nFront black street light - The black\nstreet light in front of the ornate building. It has\na thin support beam holding it up with little lines\ngoing down it. There\u2019s curved branches on the beams\nsupporting the lights. Each light has the same de-\nsign, with little pointy spires or points on its top\ntip. There\u2019s about 6 visible lights on this street light.\n...(submask text omitted)...\nSelected Summaries (3/8):\n(1) A black street light\nstands in front of an ornate building, supported by\na thin beam with curved branches. Six lights with\npointy spires are visible, with the closest one featuring\nclear glass panes and a lantern-like shape.\n(2) The front black street light is decorated with\ncurved branches and has a lantern-like shape. The\nlight closest to the image has a clear glass pane with\nridges on the edges and a small decorative circle above\nit.\n(3) This image highlights the unique design of a black\nstreet light, with a lantern-like shape and pointed\nspires on top. The light has a clear glass panel with\nridges and a decorative circle, and is supported by\na thin beam and curved branches. The image also\nshows the ornate building in the background.\nbasic negative:\nA white street light stands in front\nof a modern skyscraper, supported by a thick concrete\npillar. Five lights with sleek, angular designs are\nvisible, with the closest one featuring frosted glass\npanes and a cylindrical shape.\nlayout negative:\nA black street light stands in front\nof a modern skyscraper, supported by a thick concrete\npillar. Six LED lights with sleek, angular designs are\n14\nvisible, with the closest one featuring a frosted glass\ncover and a cylindrical shape.\nswaps negative:\nA lantern-like shape stands in front\nof an ornate building, supported by a clear glass panes\nwith curved branches. Six pointy spires are visible,\nwith the closest one featuring a thin beam and black\nstreet light.\nB\nCrowdsourcing Methodology\nExtending from the overall description for the main\ntask in Section 3.2, we here provide an overview of\nthe precursor qualification tasks, as well as some\nadditional details of the preparation and packaging\nprocess for the dataset. The complete set of instruc-\ntions, as well as code to be able to directly reproduce\nthe collection for this dataset, is available on the\nproject GitHub4.\nNote, we do not include the code for our initial version\nof the collection task, wherein workers were asked\nto both select the regions of the image that were\nworth annotating and annotate them in the same\npass, as these proved both incredibly time-consuming\nand often lower quality than using model-based mask\ngeneration and allowing workers to filter low-quality\nmasks.\nB.1\nQuality Assurance\nHigh-quality\ndatasets\nrely\non\ngetting\nsolidly-\nperforming crowdworkers, which nowadays can prove\nto be an adversarial task initially. To remedy this\nsituation, we set up a multi-stage process wherein\nworkers could complete precursor tasks (for which\nthey were compensated) that we could use to deter-\nmine eligibility in the main task pool.\nStage one of the task asked a few questions about a\npreset image, wherein we asked workers to provide a\nfew sentences describing the image, then note a few\nthings in the image they might include descriptions of\nif they were to need to write 1000 meaningful words\nabout the image. After filtering out answers from\nbots, we manually reviewed answers for quality on\nthe provided description, as well as having included\nany of a few details in the image that we felt were\nhard to notice on first pass in either the description\nor list of additional things they might describe.\nWorkers with solid fluency in English and solid at-\ntention to detail were moved into stage two, wherein\n4https://github.com/facebookresearch/DCI/tree/main/\nreproduction/crowdsourcing\nthey had access to the full task and were eligible to\ncomplete it three times. In this stage we evaluated\nresponses for a general understanding of the more\ncomplex task interface, and allowed workers who com-\npleted this stage or only made minor mistakes to the\nfull tasks. Workers who did make minor mistakes\nwere given feedback to help understanding.\nIn the last stage, work was audited regularly from\neach worker to provide feedback about description\nquality, proper use of disqualifying bad masks, and\noverall tradeoff between time spent and work com-\npleted.\nWe used this information to limit over-\ncontribution from single individual, filter out a few\nworkers that provided decreasing quality over time,\nand bonus workers who were slower than our target\npace but providing exceptional quality.\nB.2\nInstruction Details\nBefore entering the task, the worker was walked\nthrough an example task to familiarize them with the\ninterface and the goal of the task. We additionally\nprovided a few specific scenarios to anticipate com-\nmon questions, like what to do when two masks are\nthe same, or how to deal with writing for a mask that\ncontains a mask that was already done, or what to\ndo on images that were much simpler than the norm.\nA complete list of these instructions is available with\nour released code.\nB.3\nWorker Metrics\nOver the course of a month, we made the first quali-\nfication round eligible to a large cohort of workers on\nMechanical Turk, and fielded over 800 submissions.\nOf these, roughly 250 workers made it through to the\nsecond task.\nIn the second stage, we hand-reviewed around 600\ntasks (up to 3 from each worker from the first stage),\nand ultimately moved a group of 120 workers to the\nfull task. Of these, around 80 were regular contribu-\ntors over the course of our collection.\nDuring the main task, we enacted controls to ensure\nthat no worker provided more than 10% of the cur-\nrently collected data at any time. We used automated\nmetrics around words per image, words per minute,\nand unique words per image to act as an overview\nof worker quality, and hand-reviewed examples that\nwere far outside of our expected bounds for these\nmetrics.\n15\nC\nExtended Ablations\nIn order to evaluate ideal use techniques for the\nDensely Captioned Images dataset for the purpose of\nfine tuning, we run some ablation experiments and\ncompare to performance on fine tuning on COCO\nand Localized Narratives.\nC.1\nDCI Fine-tuning ablation methods\nC.1.1\nPickN Caption Training\nIn order to make use of all of the available captions\nfor each image, we pick some number of captions\nfrom those available. For N = 1, this just results\nin selecting one of the captions for each image ran-\ndomly in each epoch. For N > 1, we instead provide\nmultiple captions during both CLIP loss and nega-\ntives loss calculations. In these circumstances, loss\nis calculated between the worst positive and the best\nnegative. As each image is supposed to have unique\nand high-quality captions across the whole set, the\nexpectation here is that any caption for one image\nshould score better than any caption for another.\nC.1.2\nImage-based Batching\nWhen training with sDCI submasks, we have the\nopportunity to provide the model with exceptionally\ndifficult negatives during the CLIP loss calculation,\nnamely other submasks from the same image. We call\nthis ablation ImgGroup and run it for each sDCI\nmodel to determine the impact that hard negatives\nlike these have on CLIP training.\nC.1.3\nNegatives Loss\nIn order to evaluate the impact that over-weighing\nnegatives during train time has on model perfor-\nmance, we launch one job with 9 times the nega-\ntives loss used in our standard experiments. This\nattempts to evaluate how easily the negative con-\nstruction techniques in these standard benchmarks\ncould be \u2018gamed\u2019.\nC.1.4\nNegatives Selection\nWhile we have LLM-generated negatives readily avail-\nable for the sDCI dataset (as described in Section A,\nwe also wanted to compare to negatives for the LN\nand COCO datasets. For this we used a spacy-based\nswapping technique similar to NegCLIP (Yuksekgonul\net al., 2023) or DAC (Doveh et al., 2023a), wherein\nnoun phrases, verbs, and adjectives were randomly\nswapped inside of a given caption to create a neg-\native.\nWe used these spacy-swaps for all COCO\nand LN runs, but we also include spacy-swap abla-\ntions for DCI-trained models to compare with the\nLLM-generated negatives.\nC.2\nDatasets and Training methods\nOverall we use the same training parameters outlined\nin Section 5. We use the following five datasets in\nour ablations:\n1. sDCIsub: All of the complete images and subim-\nages with LLM generated captions (referred to\nabove as DCIsub<77).\n2. LN: All images and captions from the COCO\nsubset of Localized Narrations with a CLIP\ntoken count under 77 (referred to above as\nLNCOCO<77).\n3. LN8012: The first 8012 images of LN, used to be\na same-size comparison to DCI-trained models.\n4. COCO:\nAll images and captions from the\nCOCO 2017 set.\n5. COCO8012 :The first 8012 images of COCO,\nused to be a same-size comparison to DCI-\ntrained models.\nFor each sweep we select the best model as determined\nby highest score on validation metrics from the same\ndataset used for training (for instance using COCO\nvalid for a model trained on COCO train).\nFor sDCI, we split the test set into a training set of\n7800, a valid set of 100, and a test set of 112. All\nmetrics below are reported on the 112 image test set.\nC.3\nResults\nC.3.1\nAggregate DCI Ablations\nA complete table of our sDCI ablations can be seen\nin Table 4, however it is more dense information than\nis likely useful in seeing overall trends. Instead, we\naggregate over the different ablation methods.\nIn Table 5, for ARO and VL-C we observe that gen-\nerally constructing batches from the entire dataset\nrather than from masks in the same image is better\nfor performance.\nWe also see that using Pick1\nis the most ef-\nfective method for making use out of the LLM-\nsummarizations of captions we\u2019ve created, as it allows\nthe model to see more of the related text (better than\nfirst) without potentially penalizing it for situations\nwhere two related overlapping captions are provided\nat the same time (which may be an issue with Pick5).\n16\nConditions\nARO\nVL-Checklist\nsDCI All\nAll Pick5\nBase\nAll\nCaptsNLBatch\nVG-R\nVG-A\nCOCO FLICKR Object AttributeRelation\nSCM\nNeg\nSCM\nNeg\nNeg\nH-Neg\nFirst\n9 Rand 75.34% 62.72% 84.33%\n87.86% 76.99%\n67.77% 68.27%\n51.71%\n97.20%\n8.69% 79.41%\n98.21% 88.30%\nFirst\n9 Group 74.82% 58.26% 85.05%\n88.70% 69.44%\n68.65% 68.93%\n54.38% 96.79%\n9.44% 79.69% 94.64%\n88.65%\nPick1 9 Rand 76.04% 65.51% 86.69%\n89.72% 73.35%\n65.04% 69.48%\n44.12% 95.55% 12.59% 93.57% 93.75%\n82.97%\nPick1 9 Group 76.41% 63.71% 82.89%\n88.04% 72.68%\n65.44% 63.33%\n50.14% 95.62% 19.36% 94.05% 92.86%\n84.06%\nPick5 9 Rand\n78.61% 65.65% 86.83%\n91.40% 68.61%\n66.10%\n71.82%\n39.33% 96.10%\n7.46%\n94.94%\n98.21%\n85.64%\nPick5 9 Group 75.65%\n68.92% 85.45%\n90.26% 70.17%\n65.09% 63.80%\n43.23% 96.17% 11.56% 94.32% 96.43%\n83.79%\nFirst\n1 Rand 64.00% 60.79% 83.93%\n87.46% 76.73%\n68.75% 63.53%\n54.17% 96.17% 14.64% 75.92% 96.43%\n85.91%\nFirst\n1 Group 73.09% 62.60% 79.54%\n84.84% 72.64%\n69.37% 61.68%\n61.76% 95.76% 19.63% 76.81% 97.32%\n85.43%\nPick1 1 Rand 76.23% 67.56%\n88.58% 91.30% 80.71%\n68.69% 70.12%\n50.48% 94.39% 19.02% 88.85% 95.54%\n81.40%\nPick1 1 Group 64.97% 65.05% 80.32%\n87.62% 75.01%\n66.70% 60.75%\n58.14% 94.60% 26.81% 89.88% 96.43%\n83.58%\nPick5 1 Rand 72.56% 61.32% 78.00%\n83.68% 77.64%\n66.74% 66.65%\n49.93% 94.19% 18.60% 86.87% 95.54%\n80.78%\nPick5 1 Group 57.96% 61.49% 78.27%\n84.06% 64.80%\n63.05% 61.73%\n51.44% 94.66% 19.97% 90.63% 93.75%\n81.81%\nFirst\n0 Rand 55.61% 55.83% 29.60%\n39.22% 80.70%\n67.71% 62.10%\n57.25% 74.21% 22.23% 30.30% 80.36% 67.58%\nFirst\n0 Group 49.91% 59.04% 41.68%\n50.90% 73.11%\n67.35% 60.03%\n63.13% 75.44% 27.84% 33.11% 81.25%\n69.49%\nPick1 0 Rand 57.34% 61.98% 39.36%\n44.62%\n88.37%\n70.42% 61.28%\n56.77% 74.08% 23.73% 34.54% 83.93% 64.43%\nPick1 0 Group 50.88% 56.67% 46.62%\n51.82% 76.83%\n67.41% 58.98%\n64.02% 71.55%\n31.60% 35.16% 78.57%\n66.28%\nPick5 0 Rand 53.64% 62.39% 36.70%\n43.78% 81.33%\n69.79% 61.40%\n56.77% 75.99% 22.16% 30.78% 82.14% 68.60%\nPick5 0 Group 46.89% 55.67% 32.05%\n37.06% 75.24%\n68.98% 63.35%\n61.97% 77.84% 26.88% 32.35% 79.46% 73.05%\nCLIP Baseline 59.98% 63.18%\n47.9%\n60.2% 81.17%\n67.67% 61.95%\n37.82% 60.12% 10.94% 23.19% 67.86%\n39.95%\nTable 4 Full sDCI ablation analysis against all benchmarks. Cells are colored in comparison to the CLIP Baseline.\nARO\nVL-Checklist\nAblation\nVG-R\nVG-A\nCOCO\nFLICKR\nObject\nAttribute\nRelation\nRand\n67.71%\n62.64%\n68.22%\n73.23%\n77.71%\n67.89%\n66.07%\nImgGroup\n63.40%\n61.27%\n67.99%\n73.70%\n72.21%\n66.89%\n62.51%\nNeg Loss 9\n76.15%\n64.13%\n85.21%\n89.33%\n71.87%\n66.35%\n67.61%\nNeg Loss 1\n68.14%\n63.14%\n81.44%\n86.49%\n74.59%\n67.22%\n64.08%\nNeg Loss 0\n52.38%\n58.60%\n37.67%\n44.57%\n78.43%\n68.61%\n61.19%\nFirst\n65.46%\n59.87%\n67.36%\n73.16%\n74.94%\n68.27%\n64.09%\nPick1\n66.98%\n63.41%\n70.74%\n75.52%\n76.99%\n67.28%\n63.99%\nPick5\n64.22%\n62.57%\n66.22%\n71.71%\n72.97%\n66.63%\n64.79%\nTable 5 sDCI Grouped ablation against standard benchmarks. Results from Table 4 are averaged across different\nablations.\n17\nWe do observe that using high negatives loss is very\neffective at gaining additional performance on most\nof these metrics, however this does not include VL-\nObject and VL-Attribute, likely due to our method of\nconstructing negatives not correlating very well with\nthe types of negatives created in these tests. While\nthis provides great scores, it mostly just provides\nevidence towards the aforementioned issues with these\ntypes of evaluations Lin et al. (2023).\nIn Table 6, we observe a slightly different story. Here\nImgGroup appears to be the best method for select-\ning images, only performing worse on the Base Neg\ntest. This is expected though, as the test set is con-\nstructed with sequential examples in a manner similar\nto the ImgGroup ablation, which doesn\u2019t have any\neffect on Base Neg given there are no submasks to\ndeal with in that test.\nNo individual setting for negatives loss performs best\non all metrics, however it is unsurprising that neg-\natives loss 0 results in the highest performance on\nSubcrop-Caption Matching tasks (given their similar-\nity to the CLIP-style learning objective), and not as\nwell as negatives-trained models on negatives tasks.\nThe PickN ablations are somewhat surprising, as first\ncaptions generally performed the best overall, and\nPick1 outperformed training on Pick5 when testing\non Pick5.\nC.3.2\nDCI fine-tuning performance\nIn Table 7 we report our different ablations perfor-\nmance compared to the LN and COCO baselines, as\nwell as CLIP and DAC as comparison points. We\nexpect sDCI models to outperform all other baselines,\ngiven the test set is out-of-distribution for the other\nmodels. Still, there are some interesting observations\navailable in this table.\nFirst, for sDCI, Localized Narrations, and COCO,\nonly training on the smallest subset of images (7800)\nand without using masks, DCI ends up performing\nonly as well as COCO for Subcrop-Caption matching,\nbut both outperform Localized Narratives by a no-\nticeable margin. This may point to data in Localized\nNarratives generally being less sample-efficient than\nbaseline COCO captions.\nSecond, moving to LLM-based captions increases\nperformance on all negatives at the expense of per-\nformance on Subcrop-Caption masking. This would\nimply that the captions generated by LLMs may ac-\ntually be overfitting to the test task to the detriment\nof performance on other metrics.\nThird, training directly on negatives as a method\nof improving models\u2019 vision-language understand-\ning universally decreases performance on Subcrop-\nCaption masking, a task that also definitely takes\nstrong vision-language understanding. This is seen\nregardless of the training dataset used, or of any other\nsDCI ablation involved.\nC.3.3\nLinear Transferability\nWe evaluate a subset of all of our models, trained on\neach of our candidate datasets, on the Elevater Li\net al. (2022a) benchmark to determine linear transfer-\nability for our models. We expect some degradation\ngiven the relatively small size of our training datasets.\nWe also evaluate them on zero-shot ImageNet specif-\nically.\nOverall, in table 9 we observe a slight degradation in\nlinear probe performance on their included datasets\nacross all shots. In our ImageNet zero-shot evalua-\ntion reported in Table 8, we note that sDCI trained\nwithout negatives suffers much less degradation com-\npared to with negatives, ending up with comparable\nperformance to the DAC models.\nD\nAdditional Selected Examples\nWe include a few additional examples from the DCI\ndataset, selected from a random subset of 20 instances\nto highlight certain elements of the dataset. In each\nwe share a subset of the masks available per image.\nFigure 4 shows the level in-depth that the descriptions\ngo to. Despite only being an image of shoes on some\ngrass, DCI contains descriptions down to the details\nof the crossing pattern on the toecap or the specks\nof light colored materials in a clump of dirt on the\nground.\nFigure 5 contains a fairly complex scene of various\ntiled stone buildings and light fixtures, however the\ndescription is able to identify a tree that is mostly\nobscured by a foreground building, as well as be in-\ndepth enough to describe the shape at the top of a\nlamppost in the image as a \u201ctiny urn\u201d.\nFigure 6 stands as another example of retaining useful\nand aligned information even when there\u2019s a high\namount of potential complexity in the image. While\n\"An antique blue car in front of a row of trees\" may\nbe a standard caption for this image, we instead have\ndetails of the orientation of the car, details of what is\nvisible in frame, and the resolution of the text goes all\nthe way down to the small circular sticker on the rear\npassenger side door, or the screws on the reflector on\nthe front bumper.\n18\nAll\nAll Pick5\nBase\nAll\nAblation\nSCM\nNeg\nSCM\nNeg\nNeg\nHard Negs\nRand\n51.17%\n88.65%\n16.57%\n68.35%\n91.57%\n78.40%\nImgGroup\n56.47%\n88.71%\n21.45%\n69.56%\n90.08%\n79.57%\nNL 9\n47.15%\n96.24%\n11.52%\n89.33%\n95.68%\n85.57%\nNL 1\n54.32%\n94.96%\n19.78%\n84.83%\n95.84%\n83.15%\nNL 0\n59.99%\n74.85%\n25.74%\n32.71%\n80.95%\n68.24%\nFirst\n57.07%\n89.26%\n17.08%\n62.54%\n91.37%\n80.89%\nPick1\n53.95%\n87.63%\n22.19%\n72.68%\n90.18%\n77.12%\nPick5\n50.45%\n89.16%\n17.77%\n71.65%\n90.92%\n78.95%\nTable 6 sDCI Grouped ablation against the 112 heldout sDCI test images. Results from Table 4 are averaged across\ndifferent ablations.\nTraining Parameters\nAll\nAll Pick5\nBase\nAll\nDataset\nCaptions\nNegatives\nBatching\nSCM\nNeg\nSCM\nNeg\nNeg\nHard Negs\nsDCI8012\nFirst\nLLM\nRand\n38.30%\n84.61%\n9.10%\n69.22%\n92.86%\n76.54%\nsDCI\nPick1\nLLM\nImgGroup\n58.14%\n94.60%\n26.81%\n89.88%\n96.43%\n83.58%\nsDCI\nPick1\nLLM\nRand\n50.48%\n94.39%\n19.02%\n88.85%\n95.54%\n81.40%\nsDCI\nFirst\nLLM\nImgGroup\n61.76%\n95.76%\n19.63%\n76.81%\n97.32%\n85.43%\nsDCI\nFirst\nLLM\nRand\n54.17%\n96.17%\n14.64%\n75.92%\n96.43%\n85.91%\nsDCI\nFirst\nSpacy\nRand\n55.13%\n87.35%\n19.08%\n59.30%\n89.29%\n75.85%\nsDCI\nPick1\nNone\nImgGroup\n64.02%\n71.55%\n31.60%\n35.15%\n78.57%\n66.28%\nsDCI\nPick1\nNone\nRand\n56.77%\n74.08%\n23.73%\n34.54%\n83.93%\n64.43%\nsDCI\nFirst\nNone\nImgGroup\n63.13%\n75.44%\n27.84%\n33.11%\n81.25%\n69.49%\nsDCI\nFirst\nNone\nRand\n57.25%\n74.21%\n22.23%\n30.30%\n80.36%\n67.58%\nLN\nFirst\nSpacy\nRand\n37.82%\n76.95%\n9.37%\n46.31%\n86.61%\n63.20%\nLN8012\nFirst\nSpacy\nRand\n34.27%\n75.58%\n7.73%\n37.82%\n83.04%\n61.63%\nLN\nFirst\nNone\nRand\n41.45%\n58.82%\n12.72%\n21.75%\n80.36%\n53.42%\nCOCO\nFirst\nSpacy\nRand\n40.97%\n79.21%\n12.65%\n52.74%\n91.07%\n64.71%\nCOCO8012\nFirst\nSpacy\nRand\n38.51%\n79.75%\n11.70%\n55.27%\n86.61%\n64.16%\nCOCO\nFirst\nNone\nRand\n42.00%\n61.35%\n13.95%\n21.41%\n82.14%\n52.60%\nCLIP Baseline\n37.82%\n60.12%\n10.94%\n23.19%\n67.86%\n39.95%\nDACLLM\n36.87%\n81.12%\n8.00%\n35.91%\n86.61%\n70.66%\nDACSAM\n36.46%\n84.40%\n6.91%\n40.83%\n89.29%\n73.94%\nTable 7 Dense Captions test results. We compare DCI-trained models to models trained on Localized Narratives and\nCOCO datasets, as well as to baselines.\n19\nImageNet 0-Shot\nModel\nValid. Accuracy\nCLIP (baseline)\n60.96%\nsDCIP 1\n42.51%\nsDCIP 1NL0\n51.44%\nDAC-LLM\n52.65%\nDAC-SAM\n53.43%\nTable 8 ImageNet zero-shot.\nElevater N-Shot\nModel\n0\n5\n20\n50\nFull\nsDCIP 1\n45.10% 60.78% 69.54% 72.91% 77.49%\nsDCIP 1NL0 51.29% 61.59% 70.80% 73.39% 77.92%\nCLIP\n55.59% 64.85% 71.90% 74.38% 78.96%\nTable 9 Elevater scores for linear probe across 20 bench-\nmark datasets\nFigure 7 displays a more active scene of two women\ncooking bread, however it still captures in-depth de-\nscriptions of everything contained in the image in-\ncluding flour spread on a table, ornamental details\non a tablecloth, and additionally the placement of a\nbowl and a knife that were not captured in their own\nmasks, but still were successfully annotated.\nE\nEvaluating additional baselines on\nsDCI\nWe evaluate a large quantity of available VLMs from\nRN50, ViT, roberta, convnext, and coca architectures\non the DCI test set. Results can be seen in Table 10.\nGenerally we observe that larger models perform bet-\nter, but no model excels at all tasks. In contrast to\nwhat is observed for models trained with negatives,\nperformance from these pretraining objectives is posi-\ntively correlated between Subcrop-Caption Matching\nand detecting negatives.\nThe most performant model on SCM averaged be-\ntween the two tasks is coca_ViT-L-14 mscoco_fine-\ntuned_laion2b_s13b_b90k. The most performant\nmodel in average across negatives tasks is ViT-g-14\nlaion2b_s34b_b88k\n20\nFigure 4 One example from the Densely Captioned Images dataset, highlighting how in-depth descriptions are provided\neven for relatively simple scenes.\n21\nFigure 5 One example from the Densely Captioned Images dataset, highlighting how text is still highly aligned even\nwith complex masks.\n22\nFigure 6 One example from the Densely Captioned Images dataset, highlighting the high resolution of the text in\nalignment with the image, down to details of the stickers on car windows or screws on the reflectors.\n23\nFigure 7 One example from the Densely Captioned Images dataset, displaying a scene with a complex interaction. The\naligned description captures the action itself, alongside in-depth details like the clasp on the box on the wall, or the\nhand of one of the women pictured.\n24\nTraining Parameters\nAll\nAll Pick5\nBase\nAll\nArch\nDataset\nSCM\nNeg\nSCM\nNeg\nNeg\nHard Negs\nRN50\nyfcc15m\n39.79% 55.54%\n6.69% 19.23% 72.28%\n55.09%\nRN50\ncc12m\n41.32% 48.62%\n8.65% 19.70% 66.21%\n49.53%\nRN50-quickgelu\nopenai\n41.60% 62.10% 11.64% 23.36% 72.38%\n53.98%\nRN50-quickgelu\ncc12m\n42.02% 47.50%\n9.31% 17.92% 65.84%\n48.63%\nViT-B-32\nopenai\n40.12% 60.63% 11.28% 24.03% 67.66%\n41.29%\nViT-B-32\nlaion400m_e32\n45.35% 62.83% 14.68% 22.07% 79.09%\n57.23%\nViT-B-32\nlaion2b_s34b_b79k\n45.48% 59.95% 15.41% 20.55% 78.02%\n56.19%\nViT-B-32\ndatacomp_m_s128m_b4k\n40.37% 58.36%\n8.78% 19.89% 73.29%\n55.62%\nViT-B-32\ncommonpool_m_clip_s128m_b4k\n41.63% 58.19% 10.79% 19.63% 71.84%\n54.67%\nViT-B-32\ncommonpool_m_laion_s128m_b4k\n40.34% 58.90%\n9.02% 20.45% 71.85%\n55.75%\nViT-B-32\ncommonpool_m_image_s128m_b4k\n41.18% 58.16% 10.06% 20.25% 73.51%\n55.46%\nViT-B-32\ncommonpool_m_text_s128m_b4k\n41.29% 58.27% 10.27% 20.99% 73.33%\n55.04%\nViT-B-32\ncommonpool_m_basic_s128m_b4k\n40.93% 58.18%\n9.82% 19.61% 73.38%\n55.18%\nViT-B-32\ncommonpool_m_s128m_b4k\n40.08% 58.77% 10.02% 20.33% 72.83%\n55.07%\nViT-B-32\ndatacomp_s_s13m_b4k\n29.00% 55.49%\n1.38% 18.28% 61.65%\n54.17%\nViT-B-32\ncommonpool_s_clip_s13m_b4k\n34.01% 59.17%\n4.56% 21.49% 68.10%\n56.35%\nViT-B-32\ncommonpool_s_laion_s13m_b4k\n29.00% 55.53%\n1.34% 18.47% 61.52%\n54.28%\nViT-B-32\ncommonpool_s_image_s13m_b4k\n29.00% 55.49%\n1.32% 18.34% 61.65%\n54.17%\nViT-B-32\ncommonpool_s_text_s13m_b4k\n33.70% 57.49%\n4.08% 19.70% 67.29%\n55.21%\nViT-B-32\ncommonpool_s_basic_s13m_b4k\n32.26% 56.97%\n3.48% 19.02% 64.94%\n54.89%\nViT-B-32\ncommonpool_s_s13m_b4k\n32.35% 58.95%\n4.84% 22.65% 67.45%\n55.67%\nViT-B-32-quickgelu\nopenai\n40.12% 60.63% 11.32% 24.15% 67.66%\n41.29%\nViT-B-32-quickgelu\nlaion400m_e31\n45.14% 61.08% 14.76% 22.02% 78.31%\n55.64%\nViT-B-32-quickgelu\nlaion400m_e32\n45.05% 61.02% 14.67% 21.84% 78.46%\n55.56%\nViT-B-16\nopenai\n39.42% 61.28% 11.26% 25.88% 70.93%\n51.97%\nViT-B-16\nlaion400m_e31\n44.67% 61.33% 14.64% 22.64% 78.01%\n56.04%\nViT-B-16\nlaion400m_e32\n44.64% 61.26% 14.72% 22.71% 77.87%\n55.98%\nViT-B-16\nlaion2b_s34b_b88k\n45.28% 60.71% 15.36% 20.60% 77.73%\n57.43%\nViT-B-16\ndatacomp_l_s1b_b8k\n44.34% 59.31% 14.23% 20.36% 77.03%\n55.83%\nViT-B-16\ncommonpool_l_clip_s1b_b8k\n44.21% 60.55% 14.04% 20.61% 76.80%\n55.81%\nViT-B-16\ncommonpool_l_laion_s1b_b8k\n44.51% 60.32% 14.23% 20.31% 76.65%\n56.15%\nViT-B-16\ncommonpool_l_image_s1b_b8k\n44.07% 58.63% 13.85% 19.61% 76.51%\n55.22%\nViT-B-16\ncommonpool_l_text_s1b_b8k\n44.52% 60.02% 14.14% 20.60% 78.49%\n56.80%\nViT-B-16\ncommonpool_l_basic_s1b_b8k\n44.36% 59.88% 13.83% 19.79% 77.92%\n56.43%\nViT-B-16\ncommonpool_l_s1b_b8k\n43.24% 58.16% 13.39% 18.98% 75.69%\n54.15%\nViT-L-14\nopenai\n37.91% 59.63% 10.96% 24.79% 69.21%\n52.40%\nViT-L-14\nlaion400m_e31\n44.08% 63.49% 14.95% 21.81%\n80.12%\n57.84%\nViT-L-14\nlaion400m_e32\n44.13% 63.31% 14.92% 21.65% 79.96%\n57.72%\nViT-L-14\nlaion2b_s32b_b82k\n45.21% 58.67% 15.28% 19.80% 77.05%\n56.63%\nViT-L-14\ndatacomp_xl_s13b_b90k\n44.32% 63.07% 15.50% 22.29% 79.87%\n59.44%\nViT-L-14\ncommonpool_xl_clip_s13b_b90k\n44.69% 61.10% 15.48% 21.25% 77.40%\n57.98%\nViT-L-14\ncommonpool_xl_laion_s13b_b90k\n45.30% 62.63% 15.72% 21.36% 79.62%\n59.24%\nViT-L-14\ncommonpool_xl_s13b_b90k\n43.42% 61.94% 14.61% 21.49% 76.31%\n58.64%\nViT-H-14\nlaion2b_s32b_b79k\n45.26% 63.27% 15.62% 21.32% 79.01%\n60.16%\nViT-g-14\nlaion2b_s12b_b42k\n44.97% 61.28% 15.73% 21.02% 78.98%\n58.38%\nViT-g-14\nlaion2b_s34b_b88k\n44.89%\n63.58% 15.60% 22.12% 78.03%\n60.34%\nroberta-ViT-B-32\nlaion2b_s12b_b32k\n45.87% 58.47% 15.75% 19.78% 76.52%\n55.27%\nxlm-roberta-base-ViT-B-32\nlaion5b_s13b_b90k\n45.67% 58.72% 15.81% 19.35% 77.86%\n56.25%\nxlm-roberta-large-ViT-H-14 frozen_laion5b_s13b_b90k\n45.58% 61.49%\n16.65% 20.52% 79.82%\n59.75%\nconvnext_base\nlaion400m_s13b_b51k\n43.41% 55.72% 12.72% 28.90% 65.41%\n54.00%\nconvnext_base_w\nlaion2b_s13b_b82k\n43.75% 54.92% 14.14% 28.97% 66.72%\n53.28%\nconvnext_base_w\nlaion2b_s13b_b82k_augreg\n39.74% 59.96% 13.22% 22.24% 76.14%\n55.76%\nconvnext_base_w\nlaion_aesthetic_s13b_b82k\n43.92% 54.26% 13.99%\n29.70% 66.39%\n53.22%\nconvnext_base_w_320\nlaion_aesthetic_s13b_b82k\n42.42% 54.32% 13.25% 29.46% 65.74%\n52.86%\nconvnext_base_w_320\nlaion_aesthetic_s13b_b82k_augreg\n40.20% 60.64% 13.20% 23.37% 77.51%\n57.05%\nconvnext_large_d\nlaion2b_s26b_b102k_augreg\n39.41% 61.43% 12.88% 22.19% 76.25%\n57.40%\nconvnext_large_d_320\nlaion2b_s29b_b131k_ft\n39.42% 60.70% 12.61% 22.39% 76.66%\n57.07%\nconvnext_large_d_320\nlaion2b_s29b_b131k_ft_soup\n40.72% 60.37% 13.49% 22.19% 77.06%\n57.10%\ncoca_ViT-B-32\nlaion2b_s13b_b90k\n45.27% 59.30% 15.16% 20.27% 76.75%\n55.49%\ncoca_ViT-B-32\nmscoco_finetuned_laion2b_s13b_b90k\n16.00% 50.39%\n0.08% 19.74% 46.74%\n50.79%\ncoca_ViT-L-14\nlaion2b_s13b_b90k\n45.03% 59.40% 15.88% 20.68% 78.16%\n56.96%\ncoca_ViT-L-14\nmscoco_finetuned_laion2b_s13b_b90k\n46.34% 60.02% 16.46% 21.54% 79.77%\n57.73%\nTable 10 Summarized Dense Captions test results on OpenCLIP models. We compare various baseline models on our\nSubcrop-Caption Matching (SCM) and negatives tests.\n25\nF\nDCI Dataset Datasheet\nThe following are our answers to the Datasheets for\nDatasets (Gebru et al., 2021) question list.\nF.1\nMotivation\nFor what purpose was the dataset created? To create\nan initial dataset of highly aligned text and image\npairs that were not yet available, primarily for evalu-\nating how well existing models can make use of all of\nthe data.\nWho created the dataset and on behalf of which entity?\nResearchers on Meta\u2019s FAIR research team created\nit on their own behalf.\nWho funded the creation of the dataset? Meta\nF.2\nComposition\nWhat do the instances that comprise the dataset repre-\nsent? Images with text annotations\nHow many instances are there in total? 8012 images\nwith complete mask-aligned annotations.\nDoes the dataset contain all possible instances or is it\na sample of instances from a larger set? Images were\nsampled from a random subset of SA-1B\u2019s (Kirillov\net al., 2023) underlying image dataset.\nWhat data does each instance consist of? One image,\na top-level caption and description, and then a list of\nsubmask-subcaption pairings covering a significant\nportion of the image\u2019s content\nIs there a label or target associated with each instance?\nJust the text descriptions, no categorization is done.\nIs any information missing from individual instances?\nNot all of the image is covered in the submask-aligned\ncaptions, so the descriptions may still be considered\nincomplete.\nAre relationships between individual instances made\nexplicit? There is no clear relationships between in-\nstances in the dataset.\nAre there recommended data splits?\nThe dataset\nis intended primarily as a test set, however we also\nprovide a finetuneing train/valid/test split for those\nwanting to use it for experiments.\nAre there any errors, sources of noise, or redundancies in\nthe dataset? Though attempts were made to keep the\ndataset high-quality, annotator error can be present\nthrough the dataset, modeling errors may cause some\nmasks to have been omitted, and the LLM-based\naugmentation for scaling captions to CLIP length\nmay introduce noise as well.\nIs the dataset self-contained, or does it link to or other-\nwise rely on external resources? Self-contained\nDoes the dataset contain data that might be considered\nconfidential? Not to the authors\u2019 knowledge\nDoes the dataset contain data that, if viewed directly,\nmight be offensive, insulting, threatening, or might oth-\nerwise cause anxiety? Not to the author\u2019s knowledge\nF.3\nCollection Process\nHow was the data associated with each instance ac-\nquired? The images were selected from the SA-1B\nimage dataset, and underwent a combination of au-\ntomated and manual annotation\nWhatmechanismsorprocedureswereusedtocollectthe\ndata? We use the Mephisto framework, as well as cus-\ntom annotation interfaces, to collect the data. Com-\nplete details are available on the project\u2019s GitHub.\nIf the dataset is a sample from a larger set, what was the\nsampling strategy? Random selection from a single\nsubset\nWho was involved in the data collection process and how\nwere they compensated? Crowdworkers were paid\nwell above minimum wage for their time spent.\nOver what timeframe was the data collected? Spring\nthrough Fall of 2023.\nWere any ethical review processes conducted? This\ncollection process underwent internal review.\nF.4\nPreprocessing/cleaning/labeling\nWas any preprocessing/cleaning/labeling of the data\ndone? The dataset was preprocessed using the Seg-\nment Anything Model in order to identify the regions\nof the image to be annotated.\nWas the \u201craw\u201d data saved in addition to the prepro-\ncessed/cleaned/labeled data? The raw data is unmod-\nified by the extraction process.\nIs the software that was used to preprocess/clean/label\nthe data available? All of the software used to con-\nstruct the dataset will be made available alongside\nthe dataset release on the project\u2019s GitHub.\nF.5\nUses\nHas the dataset been used for any tasks already? The\ndataset is used for the Densely Captioned Images\ntest set.\nAre there tasks for which the dataset should not be\nused? The dataset is intended as a test set. Any use\noutside of this is unplanned by the authors.\nF.6\nDistribution\nWill the dataset be distributed to third parties outside of\nthe entity on behalf of which the dataset was created?\nThe dataset will be made broadly available\nHow will the dataset will be distributed? A download\n26\nscript will be made available on the project GitHub,\nalongside a copy of the code used to collect and\nprepare the original dataset.\nWhen will the dataset be distributed? Upon release of\nthe associated publication.\nWill the dataset be distributed under a copyright or\nother intellectual property (IP) license, and/or under\napplicable terms of use (ToU)? The dataset will be\nreleased under CC-By-NC.\nHave any third parties imposed IP-based or other\nrestrictions on the data associated with the instances?\nNot to the authors\u2019 knowledge.\nDo any export controls or other regulatory restrictions\napply to the dataset or to individual instances? Not to\nthe authors\u2019 knowledge.\nF.7\nMaintenance\nWho\nwill\nbe\nsupporting/hosting/maintaining\nthe\ndataset? Meta\u2019s FAIR team will host this dataset.\nHow can the owner/curator/manager of the dataset be\ncontacted? On the project\u2019s GitHub page.\nIs there an erratum? Changes will be noted on the\nproject\u2019s github.\nWill the dataset be updated? The authors have no\nclear schedule to update or alter the dataset.\nIf the dataset relates to people, are there applicable\nlimits on the retention of the data associated with the\ninstances?\nAll known instances of people in the\ndataset have been face-blurred as per the SA-1B\nrelease, and no retention policy is known.\nIf others want to extend/augment/build on/contribute\nto the dataset, is there a mechanism for them to do so?\nThey may do so from the project GitHub, following\nthe terms included therein.\n27\n"
  },
  {
    "title": "Pixel Aligned Language Models",
    "link": "https://arxiv.org/pdf/2312.09237.pdf",
    "upvote": "12",
    "text": "Pixel Aligned Language Models\nJiarui Xu1,2\u2217\nXingyi Zhou1\nShen Yan1\nXiuye Gu1\nAnurag Arnab1\nChen Sun1\nXiaolong Wang2\nCordelia Schmid1\n1Google Research\n2UC San Diego\nPixelLLM\nThe black dog on the screen\nimage + text\nimage only\n[281, 100, 624, 488]\nimage + location\nPixel-aligned Image Captioning\na\u00a0cat\u00a0reaches\u00a0up\u00a0\na\u00a0paw\u00a0to\u00a0touch\u00a0a\n\u00a0dog\u00a0on\u00a0a\u00a0\ntelevision\u00a0screen\nReferring \nLocalization\na cat holding \nits paw up to \na tv screen.\nLocation-conditioned \nCaptioning\nFigure 1. We propose Pixel-Aligned Language Model (PixelLLM) to equip large language models with localization capability. The model\nis pre-trained on localized image captioning data [38], where each word is labeled with a pixel location, to learn the alignment between\nwords and image pixels. PixelLLM can be applied to various localization tasks, for example, location-conditioned captioning when taking\nlocation as input, and referring localization when generating locations as outputs.\nhttps://jerryxu.net/PixelLLM/\nAbstract\nLarge language models have achieved great success in\nrecent years, so as their variants in vision. Existing vision-\nlanguage models can describe images in natural languages,\nanswer visual-related questions, or perform complex rea-\nsoning about the image. However, it is yet unclear how lo-\ncalization tasks, such as word grounding or referring local-\nization, can be performed using large language models. In\nthis work, we aim to develop a vision-language model that\ncan take locations, for example, a set of points or boxes,\nas either inputs or outputs. When taking locations as in-\nputs, the model performs location-conditioned captioning,\nwhich generates captions for the indicated object or region.\nWhen generating locations as outputs, our model regresses\npixel coordinates for each output word generated by the\nlanguage model, and thus performs dense word ground-\ning. Our model is pre-trained on the Localized Narrative\ndataset, which contains pixel-word-aligned captioning from\nhuman attention. We show our model can be applied to\nvarious location-aware vision-language tasks, including re-\nferring localization, location-conditioned captioning, and\ndense object captioning, archiving state-of-the-art perfor-\nmance on RefCOCO and Visual Genome.\n*Work done during a Google internship. \u0000{jiaruixu, zhouxy}@google.com\n1. Introduction\nImagine a baby waving their hands, pointing to the color-\nful toys, and yelling the toy names.\nPointing and nam-\ning is a natural and convenient way to describe the vi-\nsual world, and provide dense visual-language alignment\nthat is synchronized over time.\nCan we design an in-\ntelligent model [15, 35, 40, 42] that leverages such in-\nformation to obtain vision-language alignment? Prior re-\nsearch [2, 10, 14, 25, 26, 32, 44, 57] has tried to align vi-\nsual information to the pre-trained LLMs for various vision-\nlanguage tasks such as producing long and detailed descrip-\ntions [32] or conversation [26]. However, most of these\nworks take the entire image as input, and produce all out-\nputs in text. Detailed understanding of specific regions and\nobjects, and their exact locations, is not well-studied in the\ncontext of LLMs. We aim to propose a new architecture and\na training procedure that can achieve the goal, which helps\nus answer the important research question: can large lan-\nguage models achieve spatial understanding and reasoning\nfrom the visual world? If so, how?\nTo this end, we introduce PixelLLM, a vision-language\nmodel with fine-grained localization ability by densely\naligning each output word to a pixel location. We realize\nthis by adding a small MLP on top of the word features of a\narXiv:2312.09237v1  [cs.CV]  14 Dec 2023\nlanguage model, to regress to a pixel location of each word.\nThe weights of the language model can be kept frozen, or\nupdated by Low-rank finetuning (LoRA) [18]. Furthermore,\nour model can take the location prompt or text prompt, to\ngenerate outputs specific to the prompt. Fig. 1 shows the\ntasks that our model can do or can adapt to.\nWe show our concrete architecture in Fig. 2. It includes\nan image encoder, a prompt encoder, and a prompt feature\nextractor which maps prompt-conditioned image features to\nthe text embedding space. These prompt-conditioned im-\nage features and an optional text prompt, are then directly\nfed as the prefix of a large-language model, which produces\nthe captioning and per-word localization outputs. We show\nthis architecture is general and can adapt to various vision-\nlanguage tasks, with any combination of language or loca-\ntion as input or output.\nWhile our model requires dedicated word-pixel aligned\ndata to train, we note that such annotations already exist\nat large scale [38]. The localized narratives dataset [38]\ncontains annotations of human annotators narrating a given\nimage, together with a mouse trajectory of the annotators\u2019\nattention during the narration. This gives synchronized lo-\ncations for all words in the narration sentence, which can be\nused to train our model. While not all word-location pairs\nare visually meaningful or accurate, we argue that they are\nvaluable as they are from actual human attention.\nWe evaluate our model on popular vision tasks by\nadapting our architecture and fine-tuning on downstream\ndatasets, including referring localization on RefCOCO [53],\nlocation conditioned captioning on RefCOCO and Vi-\nsual Genome [23], and dense object captioning on Visual\nGenome [23]. Our model achieves the state-of-the-art per-\nformance on all these tasks, with 89.8 P@0.5 on RefCOCO\nreferring localization, 19.9 CIDEr on Visual Genome condi-\ntioned captioning, and 17.0 mAP on dense object caption-\ning. Ablations on RefCOCO [53](Tab. 4) show our dense\nper-pixel localization formulation is the key to the high per-\nformance, with a 3.7 point gain compared to alternative lo-\ncalization formulations.\nOur contributions are summarized below:\n\u2022 We introduce PixelLLM, a vision-language model that\noutputs a caption for an input image together with the\nlocalization of each word. PixelLLM takes as input an\nimage, and optionally a location prompt or a text prompt.\n\u2022 Our model can utilize the dedicated localized narrative\ndataset [38], which comes with image captions and tra-\njectories localizing each word, for per-word localization.\n\u2022 Our model is flexible and can be adapted to various\nvision-language tasks, including referring localization\nand segmentation, location-conditioned captioning, and\ndense captioning, with state-of-the-art performance.\nCode and models will be released.\n2. Related work\nLarge-scale vision and language models. Building large\nvision and language models that are generally capable of vi-\nsual captioning or question answering is a trending research\ntopic [7, 10, 19, 39, 44, 46, 54, 57]. Flamingo [1] inserts\nvisual features into language models through a gating func-\ntion. BLIP2 [26] connects vision features [13] to frozen\nlarge-language models [40] using learnable queries [3].\nLLaVa [32] distills large language models to visual inputs\nby creating instructional image-text pairs. The majority of\nthese works focus on language-only outputs. While lan-\nguage is general enough for image-level tasks, region-level\nlocalization tasks are more challenging. Our work builds\non existing vision-language models [26, 44], and adds lo-\ncalization ability using a per-token regression head.\nModeling locations in language models. We are not the\nfirst to study localization in the context of large language\nmodels [4, 24, 47, 52, 55]. Pix2Seq [5] first proposes to\nmodel localization as spatial bin tokens in the vocabulary,\nand tackle object detection as an auto-regressive problem.\nFollowing this idea, UniTab [50] embeds bounding boxes\nbin tokens in captions, which enables predicting object lo-\ncations when seeing nouns in the output. Kosmos [36] uses\na similar representation, and creates interleaved location-\ncaption datasets by distilling grounding models [27]. A key\ndistinction for our work is the localization representation.\nWhile existing works try to model locations as a word in\nthe language model vocabulary, we use a regression repre-\nsentation. Also, most existing works [24, 32, 36] rely on\ndistilled data from existing models [32, 48], and our repre-\nsentation enables us to utilize the existing human annotation\nfrom Localized Narratives dataset [38].\nReferring expression localization is a popular task in lan-\nguage and vision, aiming to localize a sentence query [33,\n53]. MDETR [21] formulates this with the cross-attention\nmechanism in DETR [3] style, where the language features\nare the queries and the visual features are keys. UniTab [50]\nintegrates box output as the follow-up spatial tokens in a\nlanguage decoder.\nShikra [4] directly extracts formatted\nbounding boxes from the raw string generated by the large\nlanguage model. We tackle this task by regression given the\nlanguage and vision features using a large-language model.\nAttention trajectory modeling is a new task enabled by\nthe localized narratives dataset [38]. It aims to align a long\nand detailed image caption to a trajectory of human atten-\ntion, collected from the mouse trace while annotating the\ncaption. The only existing work that evaluates on this task\nis MITR [34], which trains linear layers on frozen region\nfeatures. While this is effective, the knowledge learned in\nMITR couldn\u2019t transfer to other tasks given the task-specific\ndesign. To the best of our knowledge, we are the first to\npropose an end-to-end training framework for this task and\ntrain it at scale.\nDescribe the \nimage in detail:\nA\ndog standing by\ntable with wine and cake on\nit\nVocabulary Mapping Layer\nMLP\noptional input\nImage \nEncoder\nPrompt \nFeature \nExtractor\nPrompt \nEncoder\nLarge \nLanguage \nModel\nglobal box prompt\nFigure 2.\nOverview of PixelLLM architecture for pixel-aligned captioning. We first encode the input location prompt (global box\nprompt in this case) and the input image with the prompt encoder P and the image encoder V respectively. Then we input the prompt\nfeature l and the image feature f into the prompt feature extractor to extract location-specific visual feature fl. The large language model\nL then auto-regressively predicts the next text tokens conditioned on previous text tokens and the visual feature. We apply a simple MLP\nlayer on the token features before the vocabulary mapping layer of LLM, which predicts the coordinates of each text token. The alignment\nbetween the caption and the trace is represented by color gradient\n3. Preliminary\nGiven an input image I \u2208 RH\u00d7W \u00d73, our first goal is to\ngenerate an image caption s, a sequence of word tokens1 :\ns = [w1, w2, \u00b7 \u00b7 \u00b7 , wn]. Each token wi \u2208 [0, |V |] is an inte-\nger indexing a vocabulary V . This task is known as image\ncaptioning [6], and is widely studied in vision and language\nresearch [7, 26, 44].\nOptionally, we hope to take additional prompt inputs, to\nspecify the region or concepts that the model should focus\non. The prompts can be locations, indicated by a sequence\nof points x \u2208 Rm\u00d72 or a box b \u2208 R4, or texts (in the form of\na sequence of word tokens t = [w1, \u00b7 \u00b7 \u00b7 ]). Before introduc-\ning our proposed approach, we review existing techniques\nfor standard image captioning and input prompt encoding.\nImage captioning produces a sentence s given an input\nimage I.\nPopular architectures [7, 26, 44] first encode\nthe image as a feature f \u2208 RN\u00d7C using an image en-\ncoder V (e.g., a ViT [12]), where N is the number of to-\nkens and C is the dimension size. They then feed f to an\nauto-regressive [16, 43] language model L. The language\nmodel produces the word sequence one-by-one, condition-\ning on the vision feature and previously predicted words:\nwi = L(f, w1:i\u22121). The architecture of the language model\ncan be a decoder-only architecture with a stack of self-\nattention layers [44], or a pair of encoder-decoder with\ncross-attention [40].\nPrompt encoder. In addition to an image input, prompt\nencoders adapt other input modalities to the same feature\nspace as the image feature f. We follow SAM [22] to en-\ncode points or box coordinates using a location prompt en-\n1Technically, a word can be multiple tokens. For presentation clarity,\nwe assume a word is a token. We use \u201cword\u201d and \u201ctoken\u201d interchangeably.\ncoder P consisting of a sine-cosine position embedding fol-\nlowed by linear layers. For text prompt, we follow GIT [44]\nand BLIP2 [26] to concatenate query word embedding with\nvisual features as the prefix features for the language model.\nWith these components, one can already compose a\nprompt-conditioned image captioning model. However, it\nis not yet feasible to generate location outputs, e.g., localiz-\ning the queried concept or grounding the answer to a region.\nNext, we introduce our Pixel-Aligned Language Model\nthat seamlessly integrates per-token localization ability into\nvision-language models.\n4. Pixel-Aligned Language Model\nLocalization has been studied in multiple forms in computer\nvision.\nFor instance, localizing all instances given a set\nof vocabularies (object detection [30]), localizing a natural\nlanguage query (referring localization [53]), or associating\neach entity in a sentence to a bounding box (entity ground-\ning [37]).\nUnder the framework of vision-language models, we\npropose to formulate localization as aligning each word in\nthe output sentence to a pixel location. Specifically, besides\nthe sentence output s, we also output a sequence of points p\nwith the same length as the sentence, p = [p1, p2, \u00b7 \u00b7 \u00b7 , pn],\npi \u2208 R2, each corresponds to the respective word token\nin the sentence. In contrast to existing works [27, 50] that\nonly nouns may be grounded to a region, we don\u2019t enforce\nthe model to ignore non-visual tokens, so that the model can\nalso learn relation terms, e.g. \u201cholding\u201d.\nNext, we first introduce novel components of our archi-\ntecture (Sec. 4.1).\nWe then describe how we train it on\nlarge-scale densely annotated datasets (Sec. 4.2). Finally,\nwe show our architecture can be applied to various localiza-\ntion tasks with no or minor modifications (Sec. 4.3).\n4.1. Architecture\nFig. 2 gives an overview of our architecture. The input is\nan image I and an optional location prompt b. If no loca-\ntion prompt is provided, we use a global box prompt of the\nentire image, i.e. b = (0, 0, H, W), where H and W are\nthe image size. Otherwise, the model is expected to focus\non the locations indicated by the prompt. The output is the\nsentence s and its aligned point trajectory p.\nUsing the image encoder V and location prompt encoder\nP introduced above, we obtain the image feature f = V(I)\nand location prompt feature P(b). Here f is the feature of\nthe entire image, we use an prompt feature extractor E to\nextract the feature specified by the location prompt:\nfl = E(f, P(b))\n(1)\nwhere fl \u2208 RN\u00d7C is the location-specific visual feature.\nWe instantiate the prompt feature extractor E using a\ntwo-way transformer with a set of learnable tokens q, in-\nspired by QFormer [26]. Specifically, the two-way trans-\nformer takes [P(b), q] and f as the query or key/value al-\nternatively at each layer, and finally takes the learnable to-\nken features at the last layer. The output feature fl con-\nveys the feature specific to the location prompt b. Con-\nceptually, prompt feature extractor has a similar function as\nROIAlign [17], but it is learnable and doesn\u2019t require fea-\nture interpolation and sampling. We compare and discuss\nthe differences in Sec. 5.3.2.\nDense location outputs from language models. Given the\nlocation-specific feature fl, we can already feed it to a lan-\nguage model to do captioning, using auto-regressive decod-\ning: wi = L(fl, w1:i\u22121). We note the last linear layer of\na language model is a vocabulary mapping layer, mapping\nfrom the language feature space to the one-hot vocabulary\nindex. Let L\u2212 denote the language model without the last\nvocabulary mapping layer, the decoding process (for sim-\nplicity, we show greedy decoding here) can be rewritten as\nwi = argmax(v \u00b7 L\u2212(fl, w1:i\u22121))\n(2)\nwhere v \u2208 R|V |\u00d7C is the weight of the linear vocabulary\nmapping layer.\nTo use the same language features for localization, we\nsimply add a small MLP in parallel with the vocabulary\nmapping layer, which maps the language feature to a 2-\ndimension location output2:\npi = MLP(L\u2212(fl, w1:i\u22121))\n(3)\n2There are multiple trace points per word token, we show the single\npoint prediction for simplicity. In practice, we output the two points that\ntightly bound the trace of each token.\nNote we do not feed back the localization output for auto-\nregressive decoding, to avoid affecting the original text de-\ncoding process. The location prediction runs on the fly to-\ngether with language decoding, only adding minor compu-\ntation overhead. This design is language model agnostic,\nand can be applied to any language model without interfer-\ning with the original language generation ability. To take\ntext prompt as input, we directly concatenate text prompt\nword embeddings with the visual feature fl.\n4.2. Training\nWe train our model using a human-annotated caption-\nlocation aligned dataset Localized Narrative [38] (LN). Lo-\ncalized Narrative asks annotators to narrate a given image\nwhile simultaneously moving the mouse over the region\nthey are describing. The narration and the mouse trace are\nsynchronized, which gives the location of each single word\nof the narration. Although the mouse trace could be noisy,\nit is still a cheap and effective way to obtain dense location\nsupervision. Thus, this dataset contains all the needed triple\nannotations (I, s, p): the image I, the captioning sentence\ns, and the location trajectory p.\nWe use the standard label-smoothed cross-entropy loss\nto train the captioning output, and use an L1 regression loss\nto train the localization output:\nL = 1\nn\nn\nX\ni=1\n(CE(L(fl, w1:i\u22121), wi) + \u03bb|\u02c6pi \u2212 pi|)\n(4)\nwhere \u02c6pi is the predicted location for the i-th word, \u03bb is the\nlocalization loss weight, and n is the caption length.\n4.3. Adapting to downstream vision tasks.\nOur architecture can take any combination of text/ loca-\ntion as inputs or outputs, and thus can be applied to various\nlocation-related vision tasks. In this section, we show exam-\nples of adapting our architecture for three popular tasks: re-\nferring localization and segmentation, location-conditioned\ncaptioning, and dense object captioning.\nReferring localization and segmentation takes an image\nI and a sentence query t as input, and aims to produce a\nbounding box \u02c6b \u2208 R4 that corresponds to the query. To\napply our framework to this task, we set the location prompt\nas a global box (i.e., \u00afb = (0, 0, W, H)), and use the query\nsentence as the conditioned sentence in Eq. 3. By default,\nour model outputs a trajectory, rather than a single bounding\nbox. While one can form a bounding box by taking the\nboundaries of the trajectory, we observe this is suboptimal,\nas the trajectory boundaries are not tight object boundaries\nrequired in evaluation. We thus train the model to output an\naccurate object bounding box at the <EOS> token, using the\nsame regression MLP layers. We take the single box output\nPrompt \nEncoder\nImage \nEncoder\nPrompt \nFeature \nExtractor\na bottle in the middle\na glass on the left\na cake on the right\nMLP\nLLM\nFigure 3.\nOur model for referring expression localization\npipeline. To apply PixelLLM, we don\u2019t need to generate the text\ntokens. Instead, we directly input the query t into the LLM L\u2212\nto extract the token features before the vocabulary mapping layer.\nWe then apply MLP to the last token predict the bounding boxes.\nat the <EOS> token as the output:\n\u02c6b = MLP\n\u0010\nL\u2212(E(V(I), P(\u00afb)), [t, <EOS>])\n\u0011\n(5)\nAs our model already include the image backbone and\nprompt encoder from SAM [22], we can further obtain the\nsegmentation mask by simply plugging in the mask decoder\nof SAM. Our model can thus also used for referring seg-\nmentation, by producing a mask on top of the predicted box\n\u02c6b.\nLocation-conditioned captioning takes an image I and a\nbounding box b as a location prompt, and produces a cap-\ntion sentence sb that corresponds to the indicated object\nfrom the box query. Our model can be directly applied to\nthis task, using our prompt encoder and auto-regressive lan-\nguage model, while ignoring the per-word location output.\nsb\ni = L(E(V(I), P(b)), sb\n1:i\u22121)\n(6)\nDense object captioning aims to first detect all objects in\na given image, and then caption them. Our framework does\nnot by default detect objects. To get bounding box can-\ndidates, we add a proposal head after our image encoder.\nWe then feed the resulting bounding boxes separately to the\nlocation prompt, to perform location-conditioned caption-\ning for each. Specifically, we use a Simple Feature Pyra-\nmid [29] to upsample the visual features f to a pyramid of\nfeatures, and use a CenterNet [56] head for detection. We\nfine-tune the model with the detection losses and the caption\nlosses together end-to-end.\nan object of\nProposal Head\na bottle in the middle\na glass on the left\na cake on the right\nPrompt \nEncoder\nLLM\nImage \nEncoder\nPrompt \nFeature \nExtractor\nFigure 4. Our model for location-conditioned captioning and\ndense object captioning. For location-conditioned captioning, the\ninput bounding boxes are provided. For dense object captioning,\nwe first apply a proposal head on the image feature to generate\nthe bounding boxes. We input bounding boxes and image features\ninto the prompt encoder and prompt feature extractor to extract\nthe location-specific feature for each bounding box. The language\nmodel auto-regressively predicts the caption of each object.\n5. Experiments\nWe first introduce our implementation details.\nThen we\ncompare our results against the state of the art on various\ndownstream vision tasks. Lastly, we analyze the effective-\nness of our proposed components.\n5.1. Implementation details\nArchitecture. Our visual encoder V consists of two paral-\nlel backbones: ViT-H initialized with SAM [22] and ViT-L\ninitialized with EVA02 [13]. The reason for using two back-\nbones is twofold. First, using the SAM backbone and keep-\ning it frozen enables us to inherit the strong segmentation\nability from SAM, and provide strong localization features\nfor our model. Second, while keeping the SAM backbone\nfrozen, we use another tunable backbone to learn seman-\ntic features. We concatenate the two features on channel\ndimension when feeding to the prompt feature extractor.\nFor the language model L, we use the instruction fine-\ntuned T5-XL [9, 40] following BLIP2 [26]. We apply low-\nrank adaption (LoRA) [18] of rank 32 on query and value\nprojection layers of self-attention blocks to adapt the T5-\nXL to the vision tasks. The other parameters of T5-XL are\nfrozen during training. The prompt feature extractor is a\n2-layer transformer with 32 learnable tokens. Following\nPrefixLM paradigm [40], we concatenate the prompt fea-\nture extractor outputs with the text embedding of the prefix\ntext, and feed it to the language model L. To generate the\non right side of the image, we can see a human is holding a stick \nand wear cap on his head. in the middle we can see water and \nsome yellow color. at the bottom, there is a grass and ground we \ncan see.\nin this image we can see a bird \non the stem. and we can see \nthe stems with leaves. finally, \nwe can see the sky.\nin this picture there is a bottle and glass \ntowards the right. funnel, whiskers, cups \nare hanged to the rod towards the left. in \nthe bottom left there is a stove and \nspoons. in the background there are tiles.\nin this picture through glass window we can see a dog \nsitting . we can see a belt around its neck . we can see a \nman standing near to the vehicle on the road . on the \nbackground we can see trees , grass and it ' s very blur .\nin this picture we can see three cows standing on the \ngrass . there is a tree and few mountains are visible in the \nbackground .\nPixel-aligned \nCaptioning\nReferring \nSegmentation\nDense Object \nCaptioning\nFigure 5.\nQualitative results on pixel-aligned captioning (row 1), referring segmentation (row 2), and dense object captioning (row 3).\nThe generated trace semantically corresponds to the caption, represented by color gradient\n. In referring segmentation, our model\ncorrectly understands the descriptive referring expressions, e.g. \u201csugar powdered\u201d, \u201cwith white feathers\u201d. For dense object captioning, our\nmodel could generate the region-level caption that captures the spatial relationship, e.g. \u201cshelves on the wall\u201d. Zoom in for the best view.\nsegmentation mask, we directly employ the mask decoder\nof SAM [22] on the prompt embedding of the predicted\nbounding box. We also include a learnable embedding to\nenable finetuning the mask on downstream datasets.\nTraining details.\nWe first pretrain our model on We-\nbLI dataset[7] with the image captioning objective only,\nto initialize the prompt feature extractor weights following\nBLIP2 [26]. During this pretraining, we follow BLIP2(T5-\nXL) [26] to randomly split the image caption into the prefix\nand the suffix We concatenate the prefix text with visual fea-\ntures and input them into the encoder of T5-XL. The suffix\ntext is used as the generation target for the T5-XL decoder.\nWe run the pretraining for 10 epochs, using a standard [26]\ncaptioning input size 224\u00d7224.\nWe then train on the Localized Narrative [38] dataset\nwith the joint captioning and localization objectives in\nEq. 4. The default caption in LN contains multiple sen-\ntences covering multiple objects in the image. We consider\neach sentence (split by period) as one training target, rather\nthan concatenating all sentences together. We train for 5\nepochs using an input size of 384\u00d7384. The parameters of\nthe prompt encoder, SAM, mask decoder, and T5-XL are\nfrozen during all training stages. In all settings, we set the\nlocalization weight \u03bb to 0.1. We use a label-smooth factor\nof 0.1 for our captioning loss. We provide further details on\ndatasets in the supplement.\n5.2. Joint Captioning and trace generation\nWe first evaluate our model trained on the Localized Nar-\nrative [38] dataset, on its validation split. We first show\nqualitative results on joint captioning and trace generation\non the top row of Fig. 5. We can see that the generated trace\nsemantically corresponds well to the generated caption. We\nnext quantitatively evaluate our model under the controlled\ntrace generation setting on the COCO subset of Localized\nNarrative and compare to MITR [34], using their Local\nBipartite Matching score (LBM) as the evaluation metric\n(lower better). We refer to the MITR paper for details about\nthe metric.\nPixelLLM achieves 0.153 LBM (k=0), out-\nperforms 0.163 of MITR [34]. This confirms our model\u2019s\nability of trace generation task compared to existing works.\nWe also highlight that our trace outputs are produced from\nlanguage model features, rather than visual features as in\nMITR [34].\n5.3. SOTA Comparison on Downstream Tasks\nWe next fine-tune our model pretrained on Localized Nar-\nratives [38] above on downstream tasks and compare with\nthe state-of-the-art models.\n5.3.1\nReferring Localization and Segmentation\nAs introduced in Sec. 4.3, referring localization aims to\nproduce a bounding box or a segmentation mask given\nRefCOCO\nRefCOCO+\nRefCOCOg\nModels\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nbounding box P@0.5\nTransVG[11]\n81.0\n82.7\n78.4\n64.8\n70.7\n56.9\n68.7\n67.7\nUNITER[8]\n81.4\n87.0\n74.2\n75.9\n81.5\n66.7\n74.0\n68.7\nOFA[45]\n80.0\n83.7\n76.4\n68.3\n76.0\n61.8\n67.6\n67.6\nUniTAB[50]\n86.3\n88.8\n80.6\n78.7\n83.2\n69.5\n80.0\n80.0\nMDETR[21]\n86.8\n89.6\n81.4\n79.5\n84.1\n70.6\n81.6\n80.9\nVisionLLM[47]\n86.7\n-\n-\n-\n-\n-\n-\nShikra-7B[4]\n87.0\n90.6\n80.2\n81.6\n87.4\n71.1\n82.3\n82.2\nFerret-7B[52]\n87.5\n91.4\n82.5\n80.8\n87.4\n73.1\n83.9\n84.8\nPixelLLM (Ours)\n89.8\n92.2\n86.4\n83.2\n87.0\n78.9\n84.6\n86.0\nsegmentation mask cIoU\nReLA[31]\n73.8\n76.5\n70.2\n66.0\n71.0\n57.7\n65.0\n66.0\nX-Decoder[58]\n-\n-\n-\n-\n-\n-\n64.6\n-\nSEEM[59]\n-\n-\n-\n-\n-\n-\n65.7\n-\nLISA[24]\n74.9\n79.1\n72.3\n65.1\n70.8\n58.1\n67.9\n70.6\nPixelLLM (Ours)\n76.9\n78.5\n74.4\n69.2\n72.1\n64.5\n70.7\n72.4\nTable 1.\nState-of-the-art comparison on referring localization and segmentation on RefCOCO. We report the official evaluation\nmetrics: precision at IoU threshold 0.5 for referring (box) localization, and mask cIoU for referring segmentation. Numbers of other\nmethods are taken from the original publications. Our model achieves state-of-the-art on all settings for both box and mask outputs.\na query text.\nFollowing common practice in RefCOCO\ndatasets [21], we use the combination of RefCOCOs [30,\n33, 53] and GoldG [21] as the training set for referring ex-\npression localization and segmentation. Note each bound-\ning box (segmentation mask) is annotated with several re-\nferring expressions, and we train on all of them using a re-\ngression loss on our bounding box prediction (Eq. 2).\nWe report the official evaluation metrics, bounding box\nP@0.5 (precision at 0.5 IoU threshold), and segmentation\ncIoU (cumulative IoU). Tab. 1 shows the results on valida-\ntion and test split of RefCOCO[30], RefCOCO+[53] and\nRefCOCOg[33]. Our PixelLLM outperforms previous and\nconcurrent works [4, 21, 24, 47, 50, 52, 58, 59] on al-\nmost all splits of three datasets: 2.3 P@0.5 and 1.4 cIoU\non RefCOCO val, 1.7 P@0.5 and 1.5 cIoU on RefCOCO+\nval, versus the second best.\nWe compare to 7B models\nof [4, 52], which are in comparable size with ours (4B).\nWhile this is a system-level comparison, as models use dif-\nferent backbones and pretraining, it justifies our model\u2019s\ncapability generalizes to this tasks, as our model is not de-\nsigned specifically for referring.\n5.3.2\nDense Object Captioning\nDense object captioning aims to detect and caption objects\nin an image. As introduced in Sec. 4.3, our model adds\na proposal head on top of our visual features. The pro-\nposal head is randomly initialized and finetuned for this\ndata.\nWe train and evaluate dense object captioning on\nVisual Genome [23] dataset. We report the official eval-\nuation metric, mAP in Tab. 2. In this setting, we use the\nsame proposal network [56] and language model [44] with\nModel\nmAP\nFCLN [20]\n5.39\nJIVC [49]\n9.31\nImgG [28]\n9.25\nCOCD [28]\n9.36\nCOCG [28]\n9.82\nCAG-Net [51]\n10.51\nTDC+ROCSU [41]\n11.49\nGRiT [48]\n15.48\nPixelLLM (Ours)\n17.02\nTable 2. State-of-the-art comparison on dense object caption-\ning on Visual Genome [23]. We report the official evaluation\nmetric mAP. Our model performs the best among all methods.\nGRiT [48]. A key difference between our architecture and\nGRiT [48] is the region feature extractor, where GRiT uses\nROIAlign [17], to explicitly extract image features within\nthe bounding boxes. PixelLLM employs a prompt encoder\nand prompt feature extractor which don\u2019t require the image\nfeature sampling and interpolation like ROIAlign. Our Pix-\nelLLM outperforms GRiT by 1.5 mAP, this again shows the\ngeneralization ability of our architecture and pertaining as\nwell as the effectiveness of our prompt feature extractor.\n5.3.3\nLocation Conditioned Captioning\nFinally, we evaluate location-conditioned captioning on\nRefCOCOg[33] and Visual Genome[23].\nThe input is a\nground truth bounding box, and the output is the caption\ncorresponding to the indicated region. The evaluation met-\nrics are per-box METEOR and CIDEr, similar to standard\nRefCOCOg\nVisual Genome\nModel\nMETEOR\nCIDEr\nMETEOR\nCIDEr\nGRiT [48]\n13.8\n75.8\n17.1\n142.0\nKosmos-2 [36]\n14.1\n62.3\n-\n-\nGPT4RoI [55]\n-\n-\n17.4\n145.2\nPixelLLM\n14.3\n82.3\n19.9\n148.9\nTable 3. State-of-the-art comparison on Location-conditioned\nCaptioning on RefCOCOg [33] and Visual Genome [23]. We\nreport averaged per-object METEOR and CIDEr on each dataset.\nOur model outperforms both our GRiT [48] baseline and recent\nstate-of-the-art models.\nLocalization Method\nPretrain data RefCOCO P@0.5\nRaw string ([4, 52])\nGoldG\n84.4\nExtra token ([36, 50])\nGoldG\n86.1\nRegress. on token features\nGoldG\n87.6\nRegress. on token features (ours)\nLN+GoldG\n89.8\nTable 4. Ablation on Localization Supervision We compare to\nalternative localization methods including using raw string [4, 52]\nand using extra tokens [36, 50]. Both alternatives could only use\nthe sparsely labeled data GoldG [21], and not the densely labeled\ndata Localized Narratives (LN) [38]. Our regression formulation\noutperforms the alternatives, and can benefit more by using the\ndense supervision [38].\nimage captioning [6]. We reuse our dense object captioning\nmodel in Sec. 5.3.2 and drop the proposal head.\nWe compare to Kosmos-2[36] and GPT4RoI [55], who\nreported results on this task.\nWe also report results of\nGRiT [48] evaluated by ourselves. Kosmos-2[36] encodes\nthe bounding box coordinates in the text prefix and gen-\nerates the caption conditioned on it. while our PixelLLM\nexplicitly encodes the bounding box coordinates using our\nprompt encoder. Tab. 3 shows the results. Our PixelLLM\noutperforms previous works, where the improvements can\ncome from our location formulation as well as the localized\nnarrative pretraining.\n5.4. Ablation study\nImportance of per-token localization.\nNext, we ab-\nlate the importance of our per-token regression formula-\ntion for localization in LLMs.\nWe compare two exist-\ning alternatives: (1) encode the bounding box coordinates\nas raw strings, and directly use the original LLM output\nhead [4, 52]. (2) discretize the bounding box coordinates\ninto bins following Pix2Seq [5], and encode the bin num-\nber as extra tokens in LLM\u2019s vocabulary [36, 50]. We note\nboth these two ideas are used to sparsely encode locations,\ni.e., encode locations for nouns. Otherwise, if we apply\nthese ideas densely for all words, the output sequence length\nwould be 5\u00d7 longer, which is not easily feasible to train\nwell. Our regression formulation allows us to train both\nsparse localization data [21] (supervise selected words) or\ndense data [38] (supervise location outputs for all words).\nLanguage Model\nParams\nw/ LoRA\nRefCOCO P@0.5\nT5 Small\n80M\n\u2717\n67.0\nT5 Base\n250M\n\u2717\n70.3\nT5 Large\n780M\n\u2717\n73.6\nT5 XL\n3B\n\u2717\n81.9\nT5 Small\n80M\n\u2713\n75.6\nT5 Base\n250M\n\u2713\n80.8\nT5 Large\n780M\n\u2713\n84.8\nT5 XL\n3B\n\u2713\n89.8\nTable 5. Ablation on language model size and LoRA. We report\nRefCOCO official metrics under different language model sizes.\nWe observe the localization performance increases consistently\nwith language model size, and using LoRA finetuning is better\nthan freezing the language models.\nTab. 4 shows the results. First, with all sparse supervi-\nsion, our formulation (3rd row) slightly improves the two\nalternatives (+1.5 points). The improvement could be from\na better localization decoding process: the alternatives mix\ndecoding regular words and localizations, which the two\ntasks can interfere with each other. In our model localiza-\ntion is decoded in parallel. Second, our formulation enables\nus to utilize the densely annotated Localized Narratives [38]\nas additional training data (4th row). This further improves\nour performance by 2.3 points. This convincingly shows\nthat our dense word-pixel alignment training objective is\nbeneficial for referring localization tasks.\nModel size and LoRA fine-tuning. We ablate the language\nmodel size and LoRA fine-tuning. We report the P@0.5 on\nRefCOCO validation set in Tab. 5. As we increase the size\nof the language model, the accuracy on RefCOCO improves\nconsistently both with and without LoRA fine-tuning. This\nis expected, as larger models encapsulate more knowledge,\nwhich yields stronger understanding and adapting ability.\nWe note that the T5 models with LoRA [18] outperform\nthe without LoRA counterparts by 8-10 points. This in-\ndicates that LoRA could help the frozen language model\nadapt to localization tasks better. It is also worth noting\nthat even without LoRA, the frozen T5-XL performs on par\nwith models that fine-tune the text encoder jointly [8, 45].\nIt is evidence that the frozen large language model like T5\nencompasses strong localization ability, which could be re-\nvealed by our PixelLLM.\n6. Conclusion\nWe proposed a vision-language model, PixelLLM, which\ncan take an image and any combination of location or text\nas input or output. PixelLLM generates captions, and aligns\neach output word to a pixel location. Our model achieved\nstate-of-the-art performance on referring localization, dense\ncaptioning, and conditioned captioning. We hope our work\ncan inspire follow-up works on connecting vision and lan-\nguage, and building general intelligent vision systems.\nAcknowledgements. We thank David Ross and Bo Hu for their\nvaluable feedback, and Xinyu Zhang for the help on figures.\nThe project was supported, in part, by NSF CAREER Award\nIIS-2240014, Amazon Research Award, Intel Rising Star Faculty\nAward, and Qualcomm Innovation Fellowship.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. NeurIPS, 35,\n2022. 2\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 2\n[4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic. arXiv:2306.15195, 2023. 2, 7, 8\n[5] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework for\nobject detection. ICLR, 2022. 2, 8\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv:1504.00325, 2015. 3, 8\n[7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-\nscaled multilingual language-image model. In ICLR, 2023.\n2, 3, 6\n[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning.\nIn ECCV,\n2020. 7, 8\n[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,\nYi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\nDehghani, Siddhartha Brahma, et al.\nScaling instruction-\nfinetuned language models. arXiv:2210.11416, 2022. 5\n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 1, 2\n[11] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang\nZhou, and Houqiang Li. Transvg: End-to-end visual ground-\ning with transformers. In ICCV, 2021. 7\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv:2010.11929,\n2020. 3\n[13] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-\nlong Wang, and Yue Cao. Eva-02: A visual representation\nfor neon genesis. arXiv:2303.11331, 2023. 2, 5\n[14] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\nsual instruction model.\narXiv preprint arXiv:2304.15010,\n2023. 1\n[15] Google. Bard. https://bard.google.com/, 2023. 1\n[16] Alex Graves.\nGenerating sequences with recurrent neural\nnetworks. arXiv:1308.0850, 2013. 3\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 4, 7\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language mod-\nels. arXiv:2106.09685, 2021. 2, 5, 8\n[19] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv:2302.14045, 2023. 2\n[20] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap:\nFully convolutional localization networks for dense caption-\ning. In CVPR, 2016. 7\n[21] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In ICCV, 2021. 2, 7, 8, 11\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. ICCV, 2023. 3, 5, 6\n[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 2017. 2, 7, 8, 11\n[24] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-\ntion via large language model. arXiv:2308.00692, 2023. 2,\n7\n[25] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 1\n[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 1, 2, 3, 4, 5, 6\n[27] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training. In CVPR, 2022. 2, 3\n[28] Xiangyang Li, Shuqiang Jiang, and Jungong Han. Learning\nobject context for dense captioning. In AAAI, 2019. 7\n[29] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. ECCV, 2022. 5\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 3, 7, 11\n[31] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gen-\neralized referring expression segmentation. In CVPR, 2023.\n7\n[32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. NeurIPS, 2023. 1, 2\n[33] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nCVPR, 2016. 2, 7, 8\n[34] Zihang Meng, Licheng Yu, Ning Zhang, Tamara L Berg,\nBabak Damavandi, Vikas Singh, and Amy Bearman. Con-\nnecting what to say with where to look by modeling human\nattention traces. In CVPR, 2021. 2, 6, 11\n[35] OpenAI.\nChatgpt.\nhttps://openai.com/blog/\nchatgpt, 2022. 1\n[36] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shao-\nhan Huang, Shuming Ma, and Furu Wei.\nKosmos-2:\nGrounding multimodal large language models to the world.\narXiv:2306.14824, 2023. 2, 8\n[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 3, 11\n[38] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu\nSoricut, and Vittorio Ferrari.\nConnecting vision and lan-\nguage with localized narratives. In ECCV, 2020. 1, 2, 4,\n6, 8, 11\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 1, 2, 3, 5\n[41] Zhuang Shao, Jungong Han, Demetris Marnerides, and Kurt\nDebattista. Region-object relation-aware dense captioning\nvia transformer. IEEE Transactions on Neural Networks and\nLearning Systems, 2022. 7\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv:2302.13971, 2023. 1\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017. 3\n[44] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\nGit: A generative image-to-text transformer for vision and\nlanguage. arXiv:2205.14100, 2022. 1, 2, 3, 7\n[45] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In ICML, 2022. 7, 8\n[46] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. CVPR, 2023. 2\n[47] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al. Visionllm: Large language model is also an open-\nended decoder for vision-centric tasks. arXiv:2305.11175,\n2023. 2, 7\n[48] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv:2212.00280, 2022. 2, 7, 8, 11\n[49] Linjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li.\nDense captioning with joint inference and visual context. In\nCVPR, 2017. 7\n[50] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling. In ECCV, 2022. 2, 3, 7, 8\n[51] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang\nWang, and Jing Shao. Context and attribute grounded dense\ncaptioning. In CVPR, 2019. 7\n[52] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity. arXiv:2310.07704, 2023. 2, 7, 8\n[53] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions. In ECCV, 2016. 2, 3, 7\n[54] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter:\nEfficient fine-tuning of language models\nwith zero-init attention. arXiv:2303.16199, 2023. 2\n[55] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv:2307.03601, 2023. 2, 8\n[56] Xingyi Zhou, Dequan Wang, and Philipp Kr\u00a8ahenb\u00a8uhl. Ob-\njects as points. arXiv:1904.07850, 2019. 5, 7\n[57] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language mod-\nels. arXiv:2304.10592, 2023. 1, 2\n[58] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,\nLu Yuan, et al. Generalized decoding for pixel, image, and\nlanguage. In CVPR, 2023. 7\n[59] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. arXiv:2304.06718, 2023. 7\nA. Dataset details\nLocalized Narratives [38]. We use the COCO subset of the\nLocalized Narratives [38] for both training and evaluation.\nIt consists of 134, 272 training images and 8, 573 validation\nimages. 5, 000 images are annotated with 5 captions and\n5 traces per image, and the rest are annotated with 1 cap-\ntion and 1 trace per image. We use all the traces for eval-\nuation of controlled trace generation when comparing with\nMITR [34]\nGoldG [21].\nWe use the GoldG dataset prepared in\nMDETR [21] for referring localization. It which consists\nof images from COCO [30], Visual Genome [23], and\nFlickr30k [37]. We filtered out all the validation and testing\nimages of RefCOCO, RefCOCO+, and RefCOCOg from\nour combined training set, yielding 160, 280 training im-\nages in total.\nVisual Genome [23].\nWe use the Visual Genome split\nprepared in GRiT [48] for dense object captioning, with\n77, 396 training images and 5, 000 test images.\nB. Qualitative results\nWe provide qualitative more results on pixel-aligned cap-\ntioning, referring localization and segmentation, and dense\nobject captioning in Figure 6, 7, and 8, respectively.\nin this image i can see a train on the railway track. i can see few \npoles, wires, a building, few trees, few vehicles on the road. i can \nalso see few poles, few wires, few wires and few wires. i can also \nsee few buildings.\nin this image i can see a sign \nboard which is in red and black \ncolor attached to the pole. in \nthe background i can see the \ntrees, mountains and the sky.\nin this image i can see a cat which is in black and white \ncolor standing on the car. i can see a black color car and a \nlamp. in the background i can see few objects on the floor, \nfew objects, few objects, few objects on the wall and the \nceiling.\nin this image i can see two zebras. i can see grass. in the \nbackground, i can see fence and trees.\nin this image i can see a person sitting on the horse and jumping \nthe horse. the horse is in brown color. the person is wearing \nwhite dress and white helmet. at the back side i can see few \npeople standing. the person is wearing red color t - shirt and \nwhite pant. at the back side i can see a building. \nin this image i can see an elephant \nwhich is in brown color, at right i can \nsee a person standing and wearing \nblue dress, at the background i can \nsee few trees in green color, sky in \nwhite color.\nin this image i can see a cat sitting on the chair. i can see \nfew books, a table , few books , a bag and few other \nobjects on the floor. i can see a bed and few clothes on \nthe bed. i can see a bag and few other objects on the \nfloor.\nin this image i can see a boy is standing on the \nskateboard. i can see he is wearing a helmet. i can \nalso see he is wearing a blue t shirt and blue pant. \nin the background i can see the wall, few trees, \nfew buildings, few clouds and the sky.\nin this image i can see the building which is in the middle \nand i can see the traffic signal lights attached to the pole. \nto the right i can see the trees and the buildings. in the \nbackground i can see the sky.\nin this image i can see a person wearing brown color t - shirt, \nbrown color hat and holding a disc in his hand. i can see a car on \nthe road. in the background i can see few vehicles on the road, \nfew trees, mountains, a mountain and the sky.\nin this image i can see a suitcase which \nis in grey color, at right i can see a \ncouch in brown color, at left i can see a \ntable in brown color, a lamp, a table in \ncream color, at the background i can \nsee a wall in cream color.\nin this image i can see a person holding a carrot and a \nanimal. in the background i can see a fence, trees, a \nhorse and the sky.\nin this image i can see a woman \nstanding and holding a racket. she is \nwearing a white top and black skirt. \nat the back side i can see a net and \ntrees. the sky is in blue color.\nin this image there is a cat sitting in the box. there is a hat \non the cat. there are few objects on the table. there are \nfew objects on the table.\nin this image i can see a bed and on it i can see a blanket and a \nblanket. i can also see a curtain and a table on which i can see a \ncandle and a candle. i can also see few frames on the wall. i can \nalso see few curtains and i can also see this image is little bit in a \nroom.\nFigure 6.\nQualitative results on pixel-aligned captioning. Zoom\nin for the best view.\nFigure 7. Qualitative results on referring segmentation. Zoom in for the best view.\nFigure 8. Qualitative results on dense object captioning. Zoom in for the best view.\n"
  },
  {
    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
    "link": "https://arxiv.org/pdf/2312.08618.pdf",
    "upvote": "11",
    "text": "Zebra: Extending Context Window with Layerwise Grouped\nLocal-Global Attention\nKaiqiang Song\u2217, Xiaoyang Wang\u2217, Sangwoo Cho\u2217, Xiaoman Pan, Dong Yu\nTencent AI Lab, Seattle\n{riversong, shawnxywang, swcho, xiaomanpan, dyu}@global.tencent.com\nAbstract\nThis paper introduces a novel approach to en-\nhance the capabilities of Large Language Mod-\nels (LLMs) in processing and understanding\nextensive text sequences, a critical aspect in\napplications requiring deep comprehension and\nsynthesis of large volumes of information. Rec-\nognizing the inherent challenges in extending\nthe context window for LLMs, primarily built\non Transformer architecture, we propose a new\nmodel architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic\ntime and memory complexity issues associated\nwith full attention in the Transformer by em-\nploying grouped local-global attention layers.\nOur model, akin to a zebra\u2019s alternating stripes,\nbalances local and global attention layers, sig-\nnificantly reducing computational requirements\nand memory consumption. Comprehensive ex-\nperiments, including pretraining from scratch,\ncontinuation of long context adaptation train-\ning, and long instruction tuning, are conducted\nto evaluate the Zebra\u2019s performance. The re-\nsults show that Zebra achieves comparable or\nsuperior performance on both short and long\nsequence benchmarks, while also enhancing\ntraining and inference efficiency.\n1\nIntroduction\nTo effectively leverage the power of Long Context\nin Large Language Models (LLMs), it is essen-\ntial to develop and refine techniques that enable\nthese models to process and interpret extensive text\nsequences accurately. This capability is particu-\nlarly significant in applications that demand deep\nunderstanding and synthesis of large volumes of\ninformation, such as summarization (Huang et al.,\n2021; Hu et al., 2023; Song et al., 2022; Kry\u00b4sci\u00b4nski\net al., 2021), reading comprehension (Nguyen et al.,\n2016; Fan et al., 2019; Zhong et al., 2021; Yang\net al., 2023), long-form generation (Guan et al.,\n2021; Deng et al., 2022; Roziere et al., 2023), and\n*Equal Contribution\ncomplex reasoning (Wei et al., 2022; Yao et al.,\n2023; Chen et al., 2023a).\nHowever, it is challenging to extend the con-\ntext window from different viewpoints: First, the\npredominant LLM model uses Transformer archi-\ntecture (Vaswani et al., 2017). Such models like\nBERT (Devlin et al., 2018), GPT (OpenAI, 2023),\nand T5 (Raffel et al., 2020) employ full attention\nin each layer which inherently incurs quadratic\ntime and memory complexity. This may potentially\ndiminish the efficiency of both the training and in-\nference processes. Second, attention computation\nover an extremely long sequence might lead to an\nalmost even distribution, potentially causing the\nomission of vital information (Han et al., 2023).\nThis may further lead to the issue of being \u201clost in\nthe middle\u201d (Liu et al., 2023). Finally, the distribu-\ntion of training signals for long and short sequences\nis imbalanced. It is evident that longer sequences\nare infrequent in both plain text and instruction-\ntuning data. Consequently, this rarity poses a chal-\nlenge in effectively capturing long-term dependen-\ncies during the training process.\nTo tackle the above issues, we propose to group\nlocal-global attention layers into blocks during the\ntraining and inference phases. This strategy en-\nhances efficiency while yielding results comparable\nto those of a global attention Transformer. Notably,\nit attains equivalent performance levels with merely\nhalf the computational effort required for training.\nAdditionally, this approach significantly reduces\nmemory consumption during inference by main-\ntaining a local Key-Value (K-V) cache specifically\nfor the local attention layers.\nIn Section 2.1, we list the two critical com-\nponents essential for a long-context model as\nwell as the potential alternatives for consideration.\nThese encompass diverse attention mechanisms\nand methodologies for positional embedding. Sub-\nsequently, in Section 2.2, we conduct a comparative\nanalysis of these alternatives, presenting their em-\narXiv:2312.08618v1  [cs.CL]  14 Dec 2023\npirical outcomes for a comprehensive evaluation.\nIntegrating these insights, we name our model Ze-\nbra, drawing an analogy to the alternating black\nand white stripes of a zebra, which resemble the\ngrouped local and global layers in our model\u2019s ar-\nchitecture.\nTo validate the proposed model at large scales,\nSection 3 details the continuation of training the\nLlama-2-7B model (Touvron et al., 2023) using\nlong-context adaptation training through Zebra.\nThis approach not only exhibits comparable per-\nformance on short-sequence benchmarks but also\nachieves superior perplexity results for longer se-\nquences.\nAdditionally, in Section 4, we con-\nduct fine-tuning of Zebra using a combination\nof both short and long instruction-tuning datasets.\nThis is followed by a systematic evaluation of the\nmodel\u2019s performance across a range of benchmark\ndatasets. It demonstrates generally better perfor-\nmance on both long and short benchmarks com-\npared to Llama-2-7b-chat. To Conclude, our con-\ntribution is 3-fold:\n\u2022 We develop a novel architecture, referred to\nas Zebra, which incorporates grouped local-\nglobal attention layers and rotary positional\nembedding.\n\u2022 We conduct comprehensive experiments and\ndetailed analyses of the Zebra framework\nacross various settings, including pretraining\nfrom scratch, continuation of training, and ex-\ntensive instruction tuning. The findings from\nthese results demonstrate the advantage of Ze-\nbra model architecture.\n\u2022 Additionally, we analyze the training and in-\nference efficiency for Zebra and provide the\npseudocode for implementation.\n2\nZebra\n2.1\nModel Architecture Design\nTo extend the context window for Transformer\nmodels, two critical elements must be addressed:\nFirst, the Attention mechanism that allows the\nmodel to efficiently focus on and process relevant\nparts of long sequences. However, it is important\nto note that the computational of attention escalates\nquadratically, leading to a decrease in efficiency\nas the length of the sequence increases. Conse-\nquently, addressing this computational challenge is\nessential for maintaining effectiveness over longer\nsequences. Second, the Positional Embedding that\nimparts a structured signal indicative of the sequen-\ntial order of tokens. It is vital to employ a posi-\ntional embedding that is not only robust but also\nexhibits strong generalization capabilities, particu-\nlarly for processing long sequences. This ensures\nthe model\u2019s effectiveness in maintaining sequence\nintegrity over longer spans of data.\n2.1.1\nAttention\nIn Figure (1a, 1b, 1c), we showcase three rep-\nresentative variants of single attention layers in-\ncluding global attention, local attention, and lo-\ncal attention with global approximations. Addi-\ntional sparse attention models like blockwise at-\ntention(Qiu et al., 2019), dilated window atten-\ntion (Beltagy et al., 2020), stride attention (Child\net al., 2019), Sinkhorn Attention (Tay et al., 2020a),\ntransient global attention (Guo et al., 2021) are\nconsidered potential alternatives for basic local at-\ntention. For the sake of clarity and focus in our\nresearch, we confine our analysis to two primary\nvariants: local attention and local attention with\nglobal approximations. This decision allows for a\nmore targeted exploration of these specific atten-\ntion mechanisms within our work. Moreover, we\nalso consider using different strategies among dif-\nferent layers. In Figure 1d, we combine several\nlocal layers with one global attention layer as a\ngroup and stack such groups for the model.\nConsidering one head of the self-attention layer\nin a decoder-only transformer, the query, key, and\nvalue of i-th position and l-th layer are defined as\nprojections of the last layer hidden states h(l\u22121)\ni\n:\nq(l)\ni\n=W T\nq h(l\u22121)i\n(1)\nk(l)\ni\n=W T\nk h(l\u22121)i\n(2)\nv(l)\ni\n=W T\nv h(l\u22121)i\n(3)\nWe denote the similarity between i-th query and\nj-th key as:\nSim(i, j) = exp(qT\ni kj/\n\u221a\nD)\n(4)\nwhere D is a normalized factor usually equal to the\nmodel dimension.\nGlobal Attention: It is the most common attention,\nwhere each token has attention to all the positions\nbefore it and itself:\n\u03b1i,j =\nSim(i, j)\nPi\nt=0 Sim(i, t)\n(5)\nKeys: \nQueries:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n(a) Global Attention\nKeys: \nQueries:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nW=3\n(b) Local Attention\nKeys: \nQueries:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nW=3\nC=2\n(c) Global-Approximation\n(d) Group Attention\nFigure 1: Four different attention strategies to be compared in this work. (a) Global Attention, where each token has its attention\nto all previous tokens and itself; (b) Local Attention, where each token only has the attention within its local window; (c) Local\nAttention with Global Approximation is newly introduced in this work, where each token not only has attention to its local\nwindow but also has an approximated attention from the remaining non-local chunks; (d) Group Attention is our introduced\nlayerwise grouped local-global attention strategy, where we group L layers and apply the global attention at the first layer of\neach group (the remaining layers use local attention).\nwhere \u03b1i,j is the attention value of i-th query over\nj-th key. The context vector is then defined as a\nweighted sum of value vectors:\ncontexti =\ni\nX\nj=0\n\u03b1i,jvj\n(6)\nLocal Attention: Each query only considers the\nkey-value pairs within its local window.\n\u03b1i,j =\nSim(i, j)\nPi\nt=min(0,i\u2212w) Sim(i, t)\n(7)\nwhere w is the window size of local attention.\nLocal Attention w/ Global Approximation: In-\nspired by transient global attention (Guo et al.,\n2021), we approximate the global attention output\nby combining multiple non-local tokens as a chunk,\nand take the attention over local tokens and non-\nlocal chunks. Each non-local chunk\u2019s key-value\npairs are estimated using the following equations:\n\u02c6kj =\nj\u2217c\u22121\nX\nt=(j\u22121)\u2217c\nkt + ln(c)\n(8)\n\u02c6vj =\nj\u2217c\u22121\nX\nt=(j\u22121)\u2217c\nvt + ln(c)\n(9)\nwhere c is the chunk size, and ln(c) is a compensa-\ntion term for each chunk.\nLayerwise Grouped Local-Global Attention: In-\nstead of using identical layers for the entire net-\nwork, we propose to use grouped local-global at-\ntention layers. In figure 1d, we group every L layer\nand use only one global attention layer at the first\nlayer of each group. We apply local attention de-\nscribed in Equation (7) for the remaining layers.\nh(l) =\n\u001a\nG-Block(h(l\u22121))\nl\nmod L == 0\nL-Block(h(l\u22121))\notherwise\n(10)\nTo simplify, we use Group Attention to denote the\nlayerwise grouped local-global attention.\n2.1.2\nPositional Embedding\nIn the Transformer architecture, positional embed-\ndings are commonly used to encode the sequence\norder information. In this study, we incorporate\nthree widely recognized types of positional embed-\ndings to facilitate a comprehensive analysis.\nAbsolute Positional Embedding: The vanilla\nTransformer (Vaswani et al., 2017) advocates to\nuse an absolute sinusoidal positional embedding:\nPE(pos, 2i) =sin(pos/100002i/d))\nPE(pos, 2i + 1) =cos(pos/100002i/d)\nwhere pos is the position index, d is the model di-\nmension, and i is the iterative variable for different\ndimensions. After the work of the vanilla Trans-\nformer, a trainable absolute positional embedding\nhas been introduced (Devlin et al., 2018; Radford\net al., 2018), serving as a replacement for the fixed\nsinusoidal pattern. Such positional embedding is\ndirectly added to the semantic embedding:\nEMB(word, pos) = WE(word) + PE(pos)\n(11)\nwhere word is the input token index, and pos is the\nabsolute position index.\nMost recently, the relative positional embed-\ndings (Shaw et al., 2018; Yang et al., 2019) are\nintroduced to eliminate the positional bias while\nimproving the performance. These approaches also\nfacilitate the model\u2019s ability to extend its contex-\ntual window, a process known as position extrapo-\nlation. Within this framework, two principal types\nof relative positional embeddings are taken into\nconsideration.\nAlibi Positional Embedding (Press et al., 2022),\nwhich applies the relative positional embedding by\ndirectly adding a bias term to the attention matrix.\n\u03b1i,j = Softmaxi\nj(Sim(i, j) \u2212 (i \u2212 j) \u2217 m)\n(12)\nwhere m is a head-specific scalar and (i \u2212 j) is the\nrelative distance between query and key positions.\nBy canceling out the \u2212i \u2217 m term, we have\n\u03b1i,j = Softmaxi\nj(Sim(i, j) + j \u2217 m)\n(13)\nRotary Positional Embedding (Su et al., 2023)\nrotates the conjugate dimensions of query and key\nvectors, such that the relative distance is encoded\nduring the attention calculation.\neq =(W T\nq hi)ei(i\u03b8)\n(14)\nek =(W T\nk hi)ei(i\u03b8)\n(15)\nwhere i denotes the imaginary unit, and i is the\npositional index. For each pair of conjugate dimen-\nsions, the similarity between query and key can be\nwritten as:\nSim(i, j) = RE[(W T\nq hi)T (W T\nk hj)ei(i\u2212j)\u03b8]\n(16)\nwhere RE takes the real value of the complex num-\nber. The overall similarity is consequently defined\nas the cumulative measure of similarities across all\ncorresponding dimensions.\n2.2\nExperiments for Model Design\nModel Size\n117M\n345M\nNum Layers\n12\n24\nHidden Size\n768\n1024\nNum Heads\n12\n16\nK-V Channel\n64\n64\nFF Layer Hidden Size\n3072\n4096\nTable 1: Parameters of two models with different sizes.\nWe conduct experiments with various attention\nstrategies and positional embedding methods as de-\nscribed earlier. Two GPT models with 117M and\n345M parameters as detailed in Table 1, are trained\nfrom scratch to assess different model architectures.\nThe training sequence length used for these exper-\niments ranges from 1024, 4096, to 16384. A 10%\nof the training data from the Pile dataset (Gao et al.,\n2020) is utilized for model training. Its testing\nand validation data is used in experiments of this\nsection for evaluation. We employ an Adam Opti-\nmizer (Kingma and Ba, 2014) for training with the\nbeta values of 0.9 and 0.99. The training process\nspans 20, 000 steps with a batch size of 2M tokens.\nThe initial learning rate is set to 1e\u22123 with a warm-\nup step of 2, 000, followed by linear decay to 1e\u22125.\nWeight decay is set to 0.01, and the gradient clip-\nping is set to 1.0. For the local attention, a window\nsize of w = 1, 024 is applied. For local attention\nwith global approximation, we employ a chunk size\nof c = 16. We group every three layers for local\nand global layers. For rotary embedding (Su et al.,\n2023), the RoPE theta is configured as 131, 072 to\nenhance its generalization performance on longer\nsequences. All experiments are implemented using\nMegatron-LM1 (Shoeybi et al., 2019; Narayanan\net al., 2021; Korthikanti et al., 2023).\n2.2.1\nAttention\nFigure 2 shows the testing perplexity (PPL) differ-\nence between each attention strategy and the global\nattention method on the 117M model. From the\nfigures, we have a few observations: First, global\nattention has the best overall performance; Sec-\nond, the performance gap between group attention\nand global attention is small but stable when the\ntraining sequence length is getting longer; Third,\nas the training sequence length grows, the perfor-\nmance of local attention and global approximation\nattention drops a lot for longer sequences, though\nit may benefit the shorter ones. As group attention\nhas less computation but achieves a similar perfor-\nmance compared to global attention, it has a high\nscalability potential.\nTo better compare the global and group attention\nstrategies, we take both performance and compu-\ntation into consideration. In Figure 3, we draw\nthe curve of the estimated TFLOPS and the vali-\ndation PPL on three different training lengths with\nDeepSpeed FLOPS profiler2 for the 117M model.\nWe observe that group attention achieves a simi-\nlar performance with less computation than global\nattention. When the local window is equal to the\ntraining sequence length (i.e., 1k training length\nin Figure 3), the gain is negligible. However, as\nthe training sequence gets longer (e.g., 4k or 16k\ntraining length in Figure 3), the gain becomes mag-\nnified. This verifies that group attention has better\nscalability compared to global attention.\n2.2.2\nPositional Embedding\nTable 2 shows the perplexity results comparing dif-\nferent positional embeddings with the 117M and\n345M models. We find that no significant perfor-\nmance differences are observed among the three\npositional embeddings for sequence lengths within\nthe training sequence 16,384. This result is in line\nwith (Taylor et al., 2022; Kazemnejad et al., 2023)\n1https://github.com/NVIDIA/Megatron-LM\n2https://www.deepspeed.ai/tutorials/\nflops-profiler/\nFigure 2: The testing PPL gap between each method and the baseline system (global attention) on 1024, 4096, and 16384\ntraining sequence length. The smaller the better. In this experiment, we split the entire testing set into different splits according\nto their length. Each split contains the instances within the length range of x\n2 + 1 to x, except the first one (length \u2264 128).\nFigure 3: The validation PPL vs TFLOPs for global attention(red) and group attention(blue) on 1024, 4096, and 16384 training\nsequence lengths.\nobservation. While the absolute positional em-\nbedding encounters challenges in extrapolating to\nlonger sequences, both Alibi and Rotary positional\nembeddings demonstrate similar capabilities for\nsequences exceeding the training sequence length\nof 16,384. It is important to note that, in our ex-\nperiments, the Alibi positional embedding requires\nfull precision (fp32) computation to prevent posi-\ntion collision. Consequently, we opt for the Rotary\npositional embedding in the Zebra model.\n2.2.3\nTraining Sequence length\nFigure 4: Perplexity on test sequences with 1k, 4k, and 16k\ntraining sequence lengths. In this experiment, we split the\nentire testing set into different splits according to their length.\nEach split contains the instances within the length range of\nx\n2 + 1 to x, except the first one (length \u2264 128).\nWe experiment with training sequence lengths\nof 1024, 4096, and 16384 with a 117M model.\nThe corresponding validation perplexity with the\nthree training sequence lengths is 14.29, 12.98, and\n12.76, respectively. In Figure 4, we observe train-\ning with longer sequence length generally performs\nbetter than those training with shorter sequence\nlength, especially on longer test splits. Meanwhile,\nthe perplexity of the model with longer training\nlength drops a little on short test splits. Interest-\ningly, as we only train with 16k length, the perplex-\nity is still going down on the 32k test split. The\nresults suggest that training with a longer sequence\nlength helps with performance.\n2.3\nConclusion on Model Architecture\nBased on the experiments and analysis above, we\ndecide to apply Rotary positional embedding and\ngroup attention strategy for extending LLM\u2019s con-\ntext window. The model is denoted as Zebra due\nto the analogous arrangement of grouped local and\nglobal attention layers, resembling the black and\nwhite color stripes on a zebra.\n3\nLong Context Adaptation Training\nIn this section, we expand our examination of the\nZebra architecture by applying it to a larger-scale\nmodel, utilizing Long Context Adaptation Train-\ning (LCAT). LCAT is essential for handling large\nPos. Emb.\nPPL on Pile Testset with Different Lengths\nMin Len\n1\n129\n257\n513\n1025\n2049\n4097\n8193\n16385\n32789\n65537\nMax Len\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\n131072\n117M\nAbsolute\n24.62\n20.34\n17.00\n17.06\n16.42\n11.84\n10.02\n8.84\n-\n-\n-\nAlibi\n24.54\n20.29\n17.01\n17.05\n16.41\n11.87\n10.08\n8.93\n7.60\n8.83\n18.47\nRotary\n24.70\n20.37\n17.03\n17.05\n16.41\n11.84\n10.06\n8.92\n7.62\n8.86\n18.51\n\u2206PPL (Max - Min)\n0.16\n0.08\n0.03\n0.01\n0.01\n0.03\n0.06\n0.09\n0.02\n0.03\n0.04\n345M\nAbsolute\n19.41\n16.12\n13.57\n13.61\n13.19\n9.86\n8.37\n7.46\n-\n-\n-\nAlibi\n19.27\n16.02\n13.51\n13.55\n13.11\n9.78\n8.36\n7.44\n6.44\n7.38\n14.84\nRotary\n19.25\n16.01\n13.48\n13.51\n13.08\n9.74\n8.32\n7.42\n6.43\n7.37\n14.77\n\u2206PPL (Max - Min)\n0.16\n0.11\n0.09\n0.10\n0.11\n0.12\n0.05\n0.04\n0.01\n0.01\n0.07\nTable 2: Perplexity with different positional embeddings and model sizes on the Pile test set. The minimum value of each model\nis indicated in bold. All the systems are trained with 16, 384 sequence length. In this experiment, we split the entire testing set\ninto different splits according to their length. Each split contains the instances within the length range of its minimum length to\nits maximum length. \u2206PPL is the gap between the best-performing system and the worst-performing system of each test split.\ncontexts. This modification enables the model to\nadapt to an expanded context through Zebra ar-\nchitecture, while also providing extended signals\nduring training. This approach allows the model to\neffectively learn and utilize long-range dependen-\ncies. Training such a model from scratch requires\na lot of computing power, so we start with an al-\nready developed model called LLAMA-2 (Touvron\net al., 2023) as our base. From there, we train the\nZebra model with different volumes of data. All\nour experiments are conducted with the 7B-sized\nmodel.\n3.1\nTraining Details\nZebra layers are organized into groups, each con-\nsisting of four layers (L = 4). The first layer within\neach group functions as the global layer, while the\nremaining three layers serve as local layers with an\nattention local window size of 512. Each batch con-\ntains 1, 572, 864 tokens. As shown in Table 5, Ze-\nbra models are trained with 24, 576 or 32, 768 se-\nquence lengths with different data sizes where mul-\ntiple documents are packed with a BOS token and\nan EOS token. The Adam Optimizer (Kingma and\nBa, 2014) is utilized, with beta values of (0.9, 0.95)\nand an Adam epsilon of 1e\u22125. Training incorpo-\nrates a cosine learning schedule with a linear warm-\nup of 100 steps. The maximum learning rate is\n1e\u22124, and the minimum learning rate is 1e\u22125. Gra-\ndient clipping is applied with a 1.0 threshold and\nweight decay is set to 0.1.\n3.2\nData Recipes\nFigure 5 presents an analysis of the distribution\nimbalance in the Pile dataset (Gao et al., 2020), par-\nticularly highlighting the variations due to different\nsequence lengths. A notable observation is that\ndoubling the sequence length, when measured in\nbytes, results in a reduction of the dataset\u2019s volume\nFigure 5: Sequence Length vs. Number of Instances on\nThe Pile Dataset.\nto merely one-fourth of its initial size. In order\nto sufficiently capture longer text segments in our\nmodel, we have strategically integrated the LCAT\ndata with sources that typically contain longer data,\nsuch as books. All data is extracted from diverse\nsources within the Pile dataset, followed by recom-\nbination with various domains. Table 3 illustrates\nthe detailed mapping of the original data source\nto the domain split. The data within each domain\nsplit is shuffled, and the specified data quantities,\nas outlined in Table 3, are selected.\n3.3\nEvaluation Results\nThe long context adaptation training is essential\nfor Zebra to refine the initial parameters from the\nLLAMA-2 model. Due to the architectural dis-\nparities between the LLAMA model and the Ze-\nbra model, our objective is to achieve compara-\nble performance with LLAMA-2 on both the short-\ncontext tasks and the long-context tasks after train-\ning with long sequences. We first evaluate the\npre-trained models in Table 4 on Gutenberg (PG-\n19) (Rae et al., 2019) by computing the perplex-\nDomain\nData Source\nData Version\nv0\nv1\nv2\nBook\nBookCorpus2, Books3\n100GB\n50GB\n100GB\nWebPage\nPile-CC, OpenWebText2, HackerNews\n0\n50GB\n100GB\nKnowledge\nUSPTO Backgrounds, PubMed Abstracts\n0\n50GB\n100GB\nWikipedia(en), NIH ExPorter\nQ&A\nStack Exchange\n0\n20GB\n30GB\nCode\nGithub\n0\n20GB\n20GB\nTranslation\nEuroParl\n0\n10GB\n10GB\nTotal\n-\n100GB\n200GB\n360GB\n# Token Used\n-\n15.7B\n50.3B\n100.7B\nTable 3: Data source for each domain and various combinations of data size employed for LCAT training.\nModel Architecture\nTraining\nRoPE Parameters\nPPL with Different Test Length\nLen.\nData\nScale\ntheta\n4096\n8192\n16384\n24576\n32768\nAverage\nLLAMA2\n-\n-\n1\n10,000\n7.57\n126.53\n1008.50\n2,037.35\n2814.72\n1198.93\nLlama2-PI\n-\n-\n4\n10,000\n15.27\n15.31\n18.35\n64.12\n178.97\n58.40\n4\n40,000\n48.71\n56.96\n71.75\n82.38\n101.17\n72.19\n16\n160,000\n195.06\n202.61\n212.50\n220.79\n240.52\n214.30\n16\n160,000\n239.38\n255.36\n339.49\n427.84\n532.97\n359.01\nLLAMA2-LCAT\n32k\nv1\n16\n160,000\n7.13\n6.85\n6.75\n6.63\n6.55\n6.78\nZebra-LCAT\n24k\nv0\n16\n160,000\n9.02\n8.80\n8.50\n8.52\n8.41\n8.65\n24k\nv1\n16\n160,000\n7.32\n7.06\n6.92\n6.85\n6.91\n7.02\n32k\nv1\n16\n160,000\n7.36\n6.99\n6.84\n6.73\n6.75\n6.93\n32k\nv2\n16\n160,000\n7.14\n6.90\n6.71\n6.62\n6.57\n6.79\nTable 4: PPL on Gutenberg (PG-19). The data for this evaluation is from the Pile training split but excluded in LCAT training.\nity across various lengths. Notably, Zebra-LCAT\ntrained with an extended context length of 32k and\nlarger data (v2) exhibits the most favorable perplex-\nity results, comparable to the performance of the\nLLAMA-2-LCAT model.\nAdditionally, we assess the performance of our\nmodels on a set of common pretraining bench-\nmarks to ensure robust performance across standard\nshort-context tasks (Touvron et al., 2023; Xiong\net al., 2023). As shown in Table 5, the LLAMA-\n2-LCAT 32k model continually trained from the\noriginal LLAMA-2 model using the 50B-token v1\ndata in Table 3 results in a slight degradation in\nperformance compared to LLAMA-2. This can\nbe attributed to the impact of long context adapta-\ntion training, as observed in previous work (Chen\net al., 2023b), which results in degradation on short-\ncontext benchmarks. Given that global attention\ntypically yields superior performance compared\nto local attention (Rae and Razavi, 2020; Beltagy\net al., 2020; Sun et al., 2023), the LLAMA-2-LCAT\nperformance indicates potential performance upper\nlimit of models with local attention. Comparing\nLLAMA-2-LCAT to Zebra-LCAT trained with the\nsame amount of data (50B), the results demonstrate\nsimilarities except for MMLU. We speculate the\nperformance decrease in MMLU is originated from\nthe architectural change of the model, potentially\nresulting in the forgetting of partial knowledge in\nModel\nTks\nMMLU\nCS\nOQA Avg\nLLAMA2\n2T\n45.3\n63.9\n48.9\n52.7\nLLAMA2 LONG 2T+400B\n47.8\n64.9\n51.0\n54.6\nLLAMA2-LCAT\n2T+50B\n44.4\n61.4\n45.6\n50.5\nZebra-LCAT\n2T+15B\n32.6\n59.4\n41.0\n44.3\nZebra-LCAT\n2T+50B\n39.1\n61.2\n46.3\n48.9\nZebra-LCAT\n2T+100B\n41.8\n61.3\n46.0\n49.7\nTable 5: Perfomance of 7B models on short-context bench-\nmarks. The scores in the first two rows are from (Xiong et al.,\n2023). Commonsense (CS) score as the average of PIQA (Bisk\net al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al.,\n2019), WinoGrande (Sakaguchi et al., 2019), ARC easy and\nchallenge (Clark et al., 2018), OpenBookQA (Mihaylov et al.,\n2018) and CommonsenseQA (Talmor et al., 2019); OpenQA\n(OQA) score as the average of 5-shot performance on Natu-\nralQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi\net al., 2017).\nLLAMA-2. Finally, as illustrated in Table 5, train-\ning Zebra models with varying token quantities\nindicates that more tokens contribute to narrowing\nthe performance gaps relative to the LLAMA-2-\nLCAT model.\n3.4\nConclusion on LCAT\nOur model exhibits comparable performance to the\nfull attention model (LLAMA-2-LCAT) in both per-\nplexity and downstream tasks. This equivalence is\nachieved while ensuring faster training and higher\nGPU throughput. Based on the experimental find-\nings, we choose Zebra-LCAT trained with a 32k\ncontext window and 100B tokens for subsequent\ninstruction tuning (a.k.a. supervised fine-tuning) in\nlong context window, as outlined in Section 4.\n4\nLong Instruction Tuning\nThe Long Context Adaptation Training (LCAT), as\ndiscussed in Section 3, facilitates the adaptation of\nthe Zebra model architecture from its LLAMA-2\nfoundation model architecture. This adaptation in-\nvolves the incorporation of grouped local-global\nlayers and position interpolation by training the\nmodel with the pre-training learning objectives\nin long context.\nFurthermore, we hypothesize\nthat LCAT contributes to enhancing the model by\nincorporating additional knowledge and capabili-\nties that can only be acquired through training in\nlonger context windows, as suggested by prior re-\nsearch (Xiong et al., 2023).\nTo align the Zebra-LCAT model for compre-\nhensive open-domain language understanding and\ngeneration tasks based on its acquired knowledge,\nwe conduct supervised fine-tuning on the Zebra-\nLCAT model with Long Instruction Tuning (LIT).\nThe objective is to empower the model to profi-\nciently handle both short and long-context tasks,\nadhering to the specifications outlined in human\ninstructions.\n4.1\nInstruction Tuning Data Recipe\nTo align the model for both short and long-context\ntasks, our instruction tuning (IT) data comprises\ninstances of both types. Table 6 shows the statistics\non the average token number per instance. The\nLLAMA tokenizer is utilized for tokenizing the in-\nstances to calculate the token numbers.\n# of Instances Avg. Tokens # / Instance\nShort IT Data\n344,818\n208.3\nLong IT Data\n108,963\n6113.5\nOverall\n453,781\n1626.3\nTable 6: Statistics of our instruction tuning (IT) data for\nZebra LIT. Our short instruction tuning data (Short IT Data)\ncontains more instances than our long instruction tuning data\n(Long IT Data) but Long IT Data has a significantly larger\naverage token number per instance.\n4.1.1\nShort Instruction Tuning Data\nOur short instruction tuning data primarily incorpo-\nrates publicly available English instruction tuning\ndatasets, including LIMA (Zhou et al., 2023), Al-\npaca (Taori et al., 2023), and ShareGPT (Tey). Ad-\nditionally, we introduce samples from hh-rlhf (Gan-\nguli et al., 2022) with \u201cselected\u201d responses for\nmulti-turn dialogue. To customize the profile of our\nZebra assistant, our dataset further encompasses\ncarefully curated short instances.\n4.1.2\nLong Instruction Tuning Data\nFor long instruction tuning, we address two cases:\nFirst case, where the user provides a lengthy doc-\nument and instructs the system to process the sub-\nstantial information and complete the specified task\nsuccinctly; Second case, where the input is rel-\natively short, but the desired output needs to be\nmore extensive. The former case encompass tasks\nof summarization, question-answering (QA), and\nmachine reading comprehension (MRC), while the\nlatter involves writing tasks. In writing tasks, the\nuser provides key information, major characters, or\nan outline, tasking the AI assistant with composing\na natural language document, such as a letter or a\nchapter in a novel.\nTo systematically empower LLMs for long tasks\nin both scenarios, we carefully curate high-quality\ninstruction tuning data for three specific tasks: sum-\nmarization, long-MRC, and writing.\nSummarization: We select 3,000 news reports\nfrom CNN / Daily Mail (See et al., 2017) and 2,000\nmulti-turn long dialogue transcripts from Media-\nSum (Zhu et al., 2021) as documents to summarize.\nGiven an document, we randomly choose one out\nof ten predefined prompts to instruct GPT-4 (Ope-\nnAI, 2023) to generate a summary. Our long in-\nstruction tuning data for summarization hence con-\nsists of 5, 000 instances with a document and the\nrandomly selected summarization prompt as input,\nand the GPT-4 generated summary as output.\nLong-MRC: We create synthetic long-MRC data\nutilizing the long documents from Guttenberg PG-\n19 corpus (Rae et al., 2019). Given a long docu-\nment (e.g., a chapter or a chapter chunk) in the cor-\npus, we first divide the long document into text seg-\nments in approximately even length. For each text\nsegment, we prompt GPT-4 to write one question-\nanswer pair grounding on the information from the\ntext segment. Each long-MRC instruction tuning\ninstance then consists of the long document and the\ngenerated question as input x, and the generated\nanswer as output y. We collect in total of 1,245\nsuch instances.\nWriting: We further utilize texts from Gutten-\nberg PG-19 corpus to generate the data for writ-\ning. Given a document (e.g., a chapter or a chapter\nchunk) in the corpus, we prompt ChatGPT (Ope-\nnAI, 2022) to extract its key elements includ-\ning \u201ccentral idea\u201d, \u201coutline\u201d, \u201ckeywords\u201d, \u201ctime\u201d,\n\u201cplace\u201d, \u201ckey figures\u201d, \u201ccause\u201d, \u201cprocess\u201d, and \u201cre-\nsult\u201d. We use predefined rules to randomly select a\nsubset of these elements, dynamically fill them into\nthe instruction template, and hence use the com-\npleted instruction containing selected key elements\nas our input. The original document is then the cor-\nresponding output. We collect 328 such instances.\nBesides our curated data for tasks of summa-\nrization, long-MRC, and writing, we further incor-\nporate 102k training instances randomly selected\nfrom public datasets including BigPatent (Sharma\net al., 2019), GovReport (Huang et al., 2021),\nGSM8k (Cobbe et al., 2021), CUAD (Hendrycks\net al., 2021), MultiDoc2Dial (Feng et al., 2021),\nMulti-News (Fabbri et al., 2019), Natural Ques-\ntion (Kwiatkowski et al., 2019), Musique (Trivedi\net al., 2022), NarrativeQA (Ko\u02c7cisk\u00fd et al., 2018),\nQasper (Dasigi et al., 2021), QMSum (Zhong et al.,\n2021), QuALITY (Pang et al., 2022), SPACE (An-\ngelidis et al., 2021), SQuALITY (Wang et al.,\n2022), SummScreen (Chen et al., 2022), and\nTOEFL-QA (Tseng et al., 2016; Chung et al., 2018).\nThese datasets cover traditional long tasks like sum-\nmarization, QA, and multi-choice QA.\n4.2\nLIT Training Details\nThe Zebra-LIT training uses the Zebra-LCAT\nmodel as backbone and thus inherits Zebra-\nLCAT\u2019s model structure including the grouped at-\ntention setting. Zebra-LIT is trained with ls =\n16, 384 sequence length.\nWe pack training in-\nstances with less than ls tokens into the ls-token\nsequences. The padding tokens are added to the\nright. For long instruction instances with more\nthan ls tokens, we truncate only its input docu-\nment but keep all the instruction text. The batch\nsize is 2, 097, 152 tokens per batch. We use Adam\nOptimizer with beta values of (0.9, 0.95) and an\nAdam epsilon of 1e\u22128. Training incorporates a co-\nsine learning schedule with a linear warm-up of\n32 steps. The maximum learning rate is 2e\u22125, and\nthe minimum learning rate is 6e\u22126. We conduct\n4-epoch training with our instruction tuning data.\nIn LIT, we calculate the loss on the output tokens\ny only. The rest hyper-parameters are the same as\nLCAT training.\n4.3\nEvaluation Results\nWe assess the performance of our instruction-tuned\nmodel on short and long-context benchmarks, as\npresented in Table 7 and 8, respectively. Fine-\ntuning models on long instruction data reveals\nsome degradation in performance on short-context\nbenchmarks, such as MMLU, as depicted in Ta-\nble 7. However, the model consistently outper-\nforms the LLAMA-2-chat model overall. Addi-\ntionally, we evaluate the model on long-context\nbenchmarks (Shaham et al., 2023). The results,\nas shown in Table 8, indicate that our model per-\nforms comparably or surpasses LLAMA2-chat in\nQA datasets but falls short in datasets employing\nRouge (Lin, 2004) evaluation metrics. It is notewor-\nthy that the long-context evaluation of LLMs poses\na non-trivial challenge, as highlighted in (Xiong\net al., 2023). Automatic metrics, such as Rouge,\nemployed in benchmarks, only consider n-gram\noverlaps with a reference, which may not consis-\ntently align with human preferences. We anticipate\nthat incorporating more substantial and diverse fine-\ntuning data can significantly contribute to improved\nmodel performance. Overall, our instruction-tuned\nmodel demonstrates better performance on both\nshort and long-context benchmarks, affirming the\neffectiveness of the architecture.\n5\nDiscussion\n5.1\nScalability\nFigure 6: Perplexity (PPL) on Pile validation set vs number of\ntraining tokens for different model sizes and training sequence\nlengths.\nDue to resource constraints, our scalability ex-\nperiments are limited to models with 117M and\n345M parameters. As illustrated in Figure 6, em-\nploying a larger context window enhances model\nperformance for the 117M model. However, a no-\ntable observation is the difficulty in differentiating\nthe performance curves when the sequence length\nis increased from 4, 096 to 16, 384 tokens. We at-\ntribute this marginal difference to two primary fac-\ntors. Firstly, the amount of training data, capped at\nModel (7B)\ncot/gsm8k\ncot/math\ncot/bbh\ncot/mmlu\nhuman-eval-plus\nAvg.\nLLAMA2-chat\n25.0\n3.8\n30.7\n40.7\n9.8\n22.0\nZebra-LIT\n30.4\n2.2\n33.6\n38.8\n15.2\n24.0\nTable 7: Performance of instruction tuned 7B models on short-context benchmarks. In this evaluation, we follow the setting of\nFastEval (tju01), which focuses mostly on the zero-shot setting.\nModel (7B)\nSummarization\nQA\nGR\nSS\nQM\nSQAL\nQspr\nNrtv\nQALT\nMuSQ\nAvg.\nLLAMA2-chat\n11.1\n12.7\n14.2\n19.0\n14.8\n11.0\n38.1\n6.7\n15.9\nZebra-LIT\n13.6\n5.3\n13.6\n17.3\n21.4\n22.6\n33.3\n12.0\n17.4\nTable 8: Performance of instruction tuned 7B models on long-context benchmarks, validation set on ZeroScrolls.\n20 billion tokens, is likely insufficient to manifest\na significant difference. Secondly, both the train-\ning and validation datasets predominantly comprise\nshorter sequences, thereby inherently constraining\nthe potential improvement in PPL. Despite these\nlimitations, we anticipate that the benefits of the\nincreased model size will become more apparent\nwith larger and more varied datasets. Evidently,\nas the model size increases from 117M to 345M,\nthe performance gains significantly, notably from\n12.87 to 10.34.\n5.2\nEfficiency\nMethod\nO Complexity\nAttention\nOther\nTotal\nGlobal\nDN2\nD2N\nDN2 + D2N\nLocal\nDW N\nD(D + W )N\nGA\nD\nC2 N2 + DW N\nD\nC2 N2 + D(D + W )N\nGroup\nD\nL N2 + DW N\nD\nL N2 + D(D + W )N\nTable 9: Complexities of attention and other operations.\nTraining Efficiency: In Table 9, we present a de-\ntailed comparison of computational complexities\nfor different attention operations, as well as other\nrelevant operations, across the four distinct atten-\ntion strategies outlined in Section 2.1.1. This com-\nparison indicates that with an increase in sequence\nlength, the computational demand of global atten-\ntion exhibits a quadratic growth pattern. On the\nother hand, both the Global Approximation and\nGroup Attention strategies, though also following\na quadratic growth trajectory, do so with signif-\nicantly lower coefficients. Notably, Local Atten-\ntion demonstrates the best computational efficiency,\nrequiring the least amount of resources as the se-\nquence length extends.\nInference Efficiency: Because of the implemen-\ntation of local attention in the majority of Trans-\nformer layers, Zebra does not require retaining all\nKey-Value (K-V) pairs in the cache. Consequently,\nthis approach significantly reduces the GPU mem-\nory requirements during inference, thereby poten-\ntially increasing the inference throughput.\nThe detailed pseudo-code for Zebra training and\ninference can be found in Appendix A.1.\n6\nRelated Work\nAttention. The Transformer architecture has a\nself-attention component with O(N2) computation\ncomplexity. Numerous studies have been proposed\nto enhance the time and memory efficiency of\nTransformer models. One approach involves lever-\naging sparse attention patterns, enabling the trans-\nformation of full quadratic attention computations\nto O(N log N) or linear complexity. Our work\nfalls within this method by grouping sparse and full\nattention patterns. Methods such as Sinkhorn (Tay\net al., 2020b), Longformer (Beltagy et al., 2020),\nETC (Ainslie et al., 2020), and BigBird (Zaheer\net al., 2020) have been introduced to incorporate\nboth sparse and full attention mechanisms. Another\nset of approaches involves utilizing the low-rank\napproximation of the attention matrix. This in-\ncludes methods such as Linformer (Wang et al.,\n2020), Performer (Choromanski et al., 2022), and\nRandom Feature Attention (Peng et al., 2021).\nPositional Embedding. In Transformer models,\npositional embeddings can be primarily catego-\nrized into two types: absolute and relative. Ear-\nlier versions of Transformers utilize absolute po-\nsitional encoding. For instance, the vanilla Trans-\nformer (Vaswani et al., 2017) model adds sinu-\nsoidal positional embeddings to the word embed-\ndings, whereas GPT (Radford et al., 2018) and\nBERT (Devlin et al., 2018) introduce learnable po-\nsitional embeddings. Currently, it has become more\ncommon to use relative positional embedding. For\ninstance, Transformer-XL (Dai et al., 2019) and\nT5 (Raffel et al., 2020) adds learnable attention\nlogit bias into attention layers. Alibi (Press et al.,\n2022) biases attention scores based on the distance\nbetween key and query elements. RoPE (Su et al.,\n2023) multiplies the keys and queries of every at-\ntention layer by sinusoidal embeddings. The Alibi\nand RoPE methods are further improved through\nthe incorporation of an additional bias term (Sun\net al., 2022; Chi et al., 2023).\nLLM. In the early stages, open-source large lan-\nguage models such as OPT (Zhang et al., 2022),\nBLOOM (Workshop et al., 2023), and Llama-\n1 (Touvron et al., 2023) have a context window\nlength limited to 2048 tokens. Later, a smaller\nopen-source model Starcoder (Li et al., 2023), with\n15B parameters, manage to extend its context win-\ndow length to 8000 by optimizing computational\nefficiency with Multi-Query Attention (Shazeer,\n2019) and FlashAttention (Dao et al., 2022). Fol-\nlowing this, LLAMA-2 (Touvron et al., 2023), an-\nother open-source model, expands its default length\nto 4096 tokens. The open-source community then\ndiscover that by interpolating Rotary Positional Em-\nbeddings3, the context window length could be fur-\nther extended to 8192 tokens. Subsequently, (Chen\net al., 2023b) expand and validate this approach,\nknown as Position Interpolation, to further enhance\nthe window length capability. LLAMA-2 under-\ngoes extended training with long-context contin-\nual pertaining, extending up to 32,768 tokens, the\npositional interpolation method, and FlashAtten-\ntion (Dao et al., 2022) showing its enhanced effec-\ntiveness (Xiong et al., 2023). Similar approaches\nare employed to extend the context length (Peng\net al., 2023; Du et al., 2022; Dacheng Li* and\nZhang, 2023) by fine-tuning pretrained models\nwith long documents. LongLoRA (Chen et al.,\n2023c) adopts a fine-tuning approach with shifted\nlocal attention for more efficient training to fur-\nther extend context length. As of December 2023,\nclosed-source large language models have signifi-\ncantly expanded their context window capabilities,\nreaching scales of up to 100,000 tokens. For in-\nstance, GPT-4-Turbo4 supports a context window\nof 128,000 tokens, while Claude-2.15 supports up\nto 200,000 tokens. The commercialization of these\nclosed-source models is heavily reliant on long con-\ntext understanding. For example, it allows users to\nupload long text files for queries or engage in ex-\ntended dialogues with extensive historical records.\nLong Evaluation. The majority of benchmarks\nfor evaluating large language models are primarily\n3https://www.reddit.com/r/LocalLLaMA/comments/\n14fgjqj/a_simple_way_to_extending_context_to_8k\n4https://platform.openai.com/docs/models/\ngpt-4-and-gpt-4-turbo\n5https://docs.anthropic.com/claude/reference/\nselecting-a-model\nfocused on tasks involving short context. How-\never, the evaluation dimensions for long context\nand short context may differ significantly. For ex-\nample, (Liu et al., 2023) develop a multi-document\nquestion answering task, which demonstrates the\nphenomenon of being lost in the middle, a chal-\nlenge not present in short context. The foundation\nfor comprehensive evaluation of long context un-\nderstanding is currently still underdeveloped. Re-\ncently, there have been efforts to develop bench-\nmarks specifically for long context analysis, such\nas (Shaham et al., 2023; Kwan et al., 2023; Dong\net al., 2023; Bai et al., 2023; An et al., 2023).\n7\nConclusion\nIn this work, we introduce Zebra, a novel architec-\nture designed to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and inter-\npreting long text sequences. Through the innova-\ntive use of grouped local-global attention layers\nand rotary positional embedding, Zebra addresses\ncritical challenges associated with extending the\ncontext window in LLMs. Our extensive exper-\niments and analyses demonstrate that Zebra not\nonly maintains comparable performance on short-\nsequence benchmarks but also excels in handling\nlonger sequences, as evidenced by its superior long\nbenchmark performances and perplexity results on\nGutenberg (PG-19). This indicates that our ap-\nproach effectively balances the need for efficiency\nwith the demand for high performance in long-\ncontext scenarios. The grouped local-global at-\ntention mechanism, in particular, proves to be a\ncrucial component in achieving this balance, offer-\ning a significant reduction in computational and\nmemory requirements while maintaining, and in\nsome cases enhancing model performance. More-\nover, the application of Zebra among diverse short\nand long downstream tasks showcases its versatility\nand robustness across various NLP tasks.\nIn conclusion, Zebra represents a significant\nstep forward in the realm of long-context language\nprocessing. Its ability to efficiently handle exten-\nsive text sequences without compromising on per-\nformance opens up new possibilities for the ap-\nplication of LLMs in a variety of complex and\ninformation-rich environments. We believe that Ze-\nbra sets a new standard for long-context modeling\nand will inspire further innovation in the field.\nLimitations\nWhile our work introduces a novel model architec-\nture and exhibits promising accuracy and efficiency,\nit is not without limitations.\nDue to computation resource constraints, we\nhave not yet evaluated the model architecture with\na parameter size larger than 7B. Though a larger\nmodel typically brings stronger performance, it is\nstill valuable to further verify with Zebra.\nMoreover, our current evaluation, especially for\nlong-context alignment tasks, largely relies on au-\ntomatic metrics like Rouge and F-1 employed by\npublic benchmarks. Such metrics evaluating n-\ngram overlapping with a reference have been under\ndebate before the era of LLMs. We anticipate a\ncomprehensive evaluation strategy for long-context\nalignment to be proposed in the future.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 268\u2013284, Online. Association\nfor Computational Linguistics.\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li,\nJun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\nL-eval: Instituting standardized evaluation for long\ncontext language models.\nStefanos Angelidis, Reinald Kim Amplayo, Yoshihiko\nSuhara, Xiaolan Wang, and Mirella Lapata. 2021.\nExtractive opinion summarization in quantized trans-\nformer spaces. Transactions of the Association for\nComputational Linguistics, 9:277\u2013293.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:\nA bilingual, multitask benchmark for long context\nunderstanding. arXiv preprint arXiv:2308.14508.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nJiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song,\nXiaoyang Wang, Dong Yu, and Jianshu Chen. 2023a.\nSkills-in-context prompting: Unlocking composi-\ntionality in large language models. arXiv preprint\narXiv:2308.00304.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2022. SummScreen: A dataset for abstrac-\ntive screenplay summarization. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8602\u20138615, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023b. Extending context window\nof large language models via positional interpolation.\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai,\nZhijian Liu, Song Han, and Jiaya Jia. 2023c. Lon-\nglora: Efficient fine-tuning of long-context large lan-\nguage models.\nTa-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and\nPeter Ramadge. 2023. Dissecting transformer length\nextrapolation via the lens of receptive field analysis.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 13522\u201313537, Toronto, Canada.\nAssociation for Computational Linguistics.\nRewon Child,\nScott Gray,\nAlec Radford,\nand\nIlya\nSutskever.\n2019.\nGenerating\nlong\nse-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2022. Rethinking attention with per-\nformers.\nYu-An Chung, Hung-Yi Lee, and James Glass. 2018.\nSupervised and unsupervised transfer learning for\nquestion answering. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n1585\u20131594, New Orleans, Louisiana. Association for\nComputational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018.\nThink you have solved question\nanswering?\ntry arc, the ai2 reasoning challenge.\narXiv:1803.05457v1.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nAnze Xie Ying Sheng Lianmin Zheng Joseph E. Gonza-\nlez Ion Stoica Xuezhe Ma Dacheng Li*, Rulin Shao*\nand Hao Zhang. 2023. How long can open-source\nllms truly promise on context length?\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978\u20132988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599\u20134610, On-\nline. Association for Computational Linguistics.\nYuntian Deng, Volodymyr Kuleshov, and Alexander M\nRush. 2022. Model criticism for long-form text gen-\neration. arXiv preprint arXiv:2210.08444.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nZican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,\nand Ji-Rong Wen. 2023. Bamboo: A comprehen-\nsive benchmark for evaluating long text modeling\ncapacities of large language models. arXiv preprint\narXiv:2309.13345.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320\u2013335.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1074\u20131084, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering.\narXiv preprint\narXiv:1907.09190.\nSong Feng, Siva Sankalp Patel, Hui Wan, and Sachin-\ndra Joshi. 2021. Multidoc2dial: Modeling dialogues\ngrounded in multiple documents. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nJian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-\nbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie\nHuang. 2021. Openmeva: A benchmark for eval-\nuating open-ended story generation metrics. arXiv\npreprint arXiv:2105.08920.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2021. Longt5: Efficient text-to-text transformer for\nlong sequences. arXiv preprint arXiv:2112.07916.\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng\nJi, and Sinong Wang. 2023. Lm-infinite: Simple\non-the-fly length generalization for large language\nmodels. arXiv preprint arXiv:2308.16137.\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer\nBall. 2021. Cuad: An expert-annotated nlp dataset\nfor legal contract review.\nYebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck\nDernoncourt, Hassan Foroosh, and Fei Liu. 2023.\nMeetingbank: A benchmark dataset for meeting sum-\nmarization. arXiv preprint arXiv:2305.17529.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1419\u20131436, Online.\nAssociation for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. triviaqa: A Large Scale Distantly\nSupervised Challenge Dataset for Reading Compre-\nhension. arXiv e-prints, page arXiv:1705.03551.\nAmirhossein\nKazemnejad,\nInkit\nPadhi,\nKarthikeyan Natesan Ramamurthy,\nPayel Das,\nand Siva Reddy. 2023.\nThe impact of positional\nencoding on length generalization in transformers.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, G\u00e1bor Melis, and Ed-\nward Grefenstette. 2018. The NarrativeQA reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. 2023. Reducing ac-\ntivation recomputation in large transformer models.\nProceedings of Machine Learning and Systems, 5.\nWojciech Kry\u00b4sci\u00b4nski, Nazneen Rajani, Divyansh Agar-\nwal, Caiming Xiong, and Dragomir Radev. 2021.\nBooksum:\nA collection of datasets for long-\nform narrative summarization.\narXiv preprint\narXiv:2105.08209.\nWai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen\nSun, Liangyou Li, Lifeng Shang, Qun Liu, and\nKam-Fai Wong. 2023. M4le: A multi-ability multi-\nrange multi-task multi-domain long-context evalua-\ntion benchmark for large language models. arXiv\npreprint arXiv:2310.19240.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,\nThomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko,\nNicolas Gontier, Nicholas Meade, Armel Zebaze,\nMing-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,\nBenjamin Lipkin, Muhtasham Oblokulov, Zhiruo\nWang, Rudra Murthy, Jason Stillerman, Siva Sankalp\nPatel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,\nZhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,\nWenhao Yu, Swayam Singh, Sasha Luccioni, Paulo\nVillegas, Maxim Kunakov, Fedor Zhdanov, Manuel\nRomero, Tony Lee, Nadav Timor, Jennifer Ding,\nClaire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri\nDao, Mayank Mishra, Alex Gu, Jennifer Robinson,\nCarolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-\nish Contractor, Siva Reddy, Daniel Fried, Dzmitry\nBahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro\nvon Werra, and Harm de Vries. 2023. Starcoder: may\nthe source be with you!\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023.\nLost in the middle:\nHow lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1\u2013\n15.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human-generated machine read-\ning comprehension dataset.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He,\nand Samuel Bowman. 2022. QuALITY: Question\nanswering with long input texts, yes! In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5336\u20135358,\nSeattle, United States. Association for Computational\nLinguistics.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context window\nextension of large language models.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A. Smith, and Lingpeng Kong. 2021.\nRandom feature attention.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,\nSinong Wang, and Jie Tang. 2019. Blockwise self-\nattention for long document understanding. arXiv\npreprint arXiv:1911.02972.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7524\u20137529, Online. Association\nfor Computational Linguistics.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap. 2019. Com-\npressive transformers for long-range sequence mod-\nelling. arXiv preprint.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. Winogrande: An adver-\nsarial winograd schema challenge at scale.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463\u2013\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073\u2013\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-\nrant, and Omer Levy. 2023. Zeroscrolls: A zero-\nshot benchmark for long text understanding. arXiv\npreprint arXiv:2305.14196.\nEva Sharma, Chen Li, and Lu Wang. 2019.\nBIG-\nPATENT: A large-scale dataset for abstractive and\ncoherent summarization. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2204\u20132213, Florence, Italy. Asso-\nciation for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\narXiv preprint arXiv:1803.02155.\nNoam Shazeer. 2019. Fast transformer decoding: One\nwrite-head is all you need.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nKaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu,\nand Fei Liu. 2022. Towards abstractive grounded\nsummarization of podcast transcripts. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4407\u20134418, Dublin, Ireland. Association for\nComputational Linguistics.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2023. Roformer: En-\nhanced transformer with rotary position embedding.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2022. A length-extrapolatable\ntransformer.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2023. A length-extrapolatable\ntransformer. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 14590\u201314604,\nToronto, Canada. Association for Computational Lin-\nguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020a. Sparse sinkhorn attention.\nIn International Conference on Machine Learning,\npages 9438\u20139447. PMLR.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. 2020b. Sparse sinkhorn attention. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML\u201920. JMLR.org.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science.\nSteven Tey. Sharegpt.\ntju01. Fasteval.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022.\nMuSiQue: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nBo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and\nLin-Shan Lee. 2016. Towards machine comprehen-\nsion of spoken content: Initial toefl listening compre-\nhension test by machine.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Richard Yuanzhe Pang, Angelica Chen, Ja-\nson Phang, and Samuel R. Bowman. 2022. SQuAL-\nITY: Building a long-document summarization\ndataset the hard way. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1139\u20131156, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luc-\ncioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz,\nMaraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta\u00b8sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois\nLavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina Voloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zden\u02c7ek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ana Santos, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdol-\nlahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Con-\ntractor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n\nPeri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc P\u00e0mies, Maria A Castillo, Mari-\nanna Nezhurina, Mario S\u00e4nger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nTh\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023.\nBloom: A 176b-parameter\nopen-access multilingual language model.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz,\nMadian Khabsa, Han Fang, Yashar Mehdad, Sharan\nNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,\nSergey Edunov, Mike Lewis, Sinong Wang, and Hao\nMa. 2023. Effective long-context scaling of founda-\ntion models.\nXianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang\nWang, Xiaoman Pan, Linda Petzold, and Dong Yu.\n2023. OASum: Large-scale open domain aspect-\nbased summarization. In Findings of the Association\nfor Computational Linguistics: ACL 2023, pages\n4381\u20134401, Toronto, Canada. Association for Com-\nputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791\u20134800, Florence,\nItaly. Association for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5905\u20135921, Online. Association for Computational\nLinguistics.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nL. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\nZettlemoyer, and Omer Levy. 2023. Lima: Less is\nmore for alignment. ArXiv, abs/2305.11206.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. Mediasum: A large-scale media interview\ndataset for dialogue summarization. arXiv preprint\narXiv:2103.06410.\nA\nAppendix\nA.1\nGroup Attention Implementation\nAs detailed in Section 2.2, our architecture employs a layerwise grouped local-global attention approach,\nsegregating the application of local and global attention across different layers. The methodology for\nlocal attention is outlined in Algorithm 1. In contrast, the global attention mechanism adheres to the\nconventional multi-head attention paradigm found in transformers. It is important to note that during\ninference when leveraging the Key-Value (K-V) cache, the process for local attention layers deviates\nfrom Algorithm 1. Instead, we exclusively implement the standard global attention framework while\nmaintaining the latest w K-V states. This approach streamlines the attention process while ensuring\nefficiency and effectiveness in inference.\nA.2\nCase Study\nAs discussed in Section 4.3, evaluating long-context tasks presents challenges. Firstly, common evaluation\nmetrics may misrepresent response quality, particularly in summarization datasets where the reference\nsummary may not align with diverse reader interests or knowledge levels. Furthermore, the limited\ndiversity in long-context tasks may result in inadequate training for instruction tuning, often stemming\nfrom summarization, QA, or information extraction datasets with long documents. A more varied set of\ntasks could potentially enhance the capability of instruction-tuned models. Lastly, as demonstrated in\nthe subsequent section, instances are frequently constructed with inaccurate gold labels or information\nnot covered in given documents, intensifying the difficulty of model evaluation and potentially yielding\nerroneous results. We present exemplars and results employing Zebra, LLAMA, and ChatGPT-3.5 on\nZeroScrolls (Shaham et al., 2023). Due to the limited space and long documents, summarization examples\nare not presented.\nAlgorithm 1: Local Attention\nData: Q, K, V \u2208 Rbsz\u00d7len\u00d7n_heads\u00d7dim_per_head\nData: M \u2208 [0, 1]bsz\u00d7n_heads\u00d7len\u00d7len is attention mask\nData: w is the window size\nData: \u03bb is the normalizing factor\n// padding the sequence length to multiple of window size\n// after padding shape:\n// bsz \u00d7 padded_len \u00d7 n_heads \u00d7 dim_per_head\n1 Q \u2190 pad_to_multiple(Q, w);\n2 K \u2190 pad_to_multiple(K, w);\n3 V \u2190 pad_to_multiple(V, w);\n// split Q, K, V into blocks\n// after spliting shape:\n// bsz \u00d7 n_blocks \u00d7 w \u00d7 n_heads \u00d7 dim_per_head\n4 Qlocal \u2190 split_into_blocks(Q);\n5 Klocal \u2190 split_into_blocks(K);\n6 Vlocal \u2190 split_into_blocks(V );\n// for K and V merge each block and the blocks before it\n// after merging shape:\n// bsz \u00d7 n_blocks \u00d7 2 \u2217 w \u00d7 n_heads \u00d7 dim_per_head\n7 Klocal \u2190 concatenate_2_blocks(K);\n8 Vlocal \u2190 concatenate_2_blocks(V );\n// calculate attention score\n// the attention score shape:\n// bsz \u00d7 n_heads \u00d7 n_blocks \u00d7 w \u00d7 2 \u2217 w\n9 attn_score \u2190 ;\n10\ntorch.einsum(\u2032...qhd, ...khd\u2212 > ...hqk\u2032, Qlocal, Klocal) ;\n11\n.transpose(1, 2) ;\n// multiply with the normalizing factor\n12 attn_score \u2190 \u03bb \u2217 attn_score ;\n// Extract the attention from the original attention mask\n13 Moriginal \u2190 extract_original_blocks(M, w) ;\n// Prepare the attention for a single block (w \u00d7 2 \u2217 w\n14 Mblock \u2190 create_blockwise_mask(w);\n// Taking both attention mask into consideration\n15 Moverall \u2190 Moriginal \u2229 Mblock\n// merge the sequence length dim\n// after merging shape:\n// bsz \u00d7 n_heads \u00d7 padded_len \u00d7 2 \u2217 w\n16 new_shape \u2190 (bsz, n_heads, padded_len, 2 \u2217 w) ;\n17 attn_score \u2190 attn_score.reshape(new_shape) ;\n18 Moverall \u2190 Moverall.reshape(new_shape) ;\n// softmax\n19 attn_prob \u2190 softmax(attn_score, Moverall) ;\n// reshape back in block format\n// shape: bsz \u00d7 n_blocks \u00d7 w \u00d7 n_heads \u00d7 2 \u2217 w\n20 new_shape \u2190 (bsz, n_heads, n_blocks, w, 2 \u2217 w) ;\n21 attn_prob \u2190 attn_prob.reshape(new_shape).transpose(1, 2) ;\n// get context vector\n// shape: bsz \u00d7 n_blocks \u00d7 w \u00d7 n_heads \u00d7 dim_per_head\n22 attn_outputs \u2190 ;\n23\ntorch.einsum(\u2032...hqd, ...khd\u2212 > ...qhd\u2032, attn_prob, Vlocal) ;\n// reshape to output format\n24 new_shape \u2190 (bsz, padded_len, n_heads, dim_per_head) ;\n// Don\u2019t forget to remove the padding ones\n25 attn_outputs \u2190 attn_outputs.reshape(new_shape)[:, seq_len, :, :] ;\nResult: attn_outputs\nNarrativeQA\nInstruction You are given a story, which can be either a novel or a movie script, and a question.\nAnswer the question as concisely as you can, using a single phrase if possible. Do not provide any\nexplanation.\nDocument [...] INTRODUCTION. The Crito seems intended to exhibit the character of Socrates\nin one light only, not as the philosopher, fulfilling a divine mission and trusting in the will of\nheaven, but simply as the good citizen, who having been unjustly condemned is willing to give up\nhis life in obedience to the laws of the state... The days of Socrates are drawing to a close; the fatal\nship has been seen off Sunium, as he is informed by his aged friend and contemporary Crito, who\nvisits him before the dawn has broken; he himself has been warned in a dream that on the third day\nhe must depart. Time is precious, and Crito has come early in order to gain his consent to a plan of\nescape. This can be easily accomplished by his friends, who will incur no danger in making the\nattempt to save him, but will be disgraced for ever if they allow him to perish. He should think of\nhis duty to his children, and not play into the hands of his enemies. Money is already provided\nby Crito as well as by Simmias and others, and he will have no difficulty in finding friends in\nThessaly and other places. Socrates is afraid that Crito is but pressing upon him the opinions of\nthe many: whereas, all his life long he has followed the dictates of reason only and the opinion\nof the one wise or skilled man. There was a time when Crito himself had allowed the propriety\nof this. And although some one will say \u2019the many can kill us,\u2019 that makes no difference; but a\ngood life, in other words, a just and honourable life, is alone to be valued. All considerations of\nloss of reputation or injury to his children should be dismissed: the only question is whether he\nwould be right in attempting to escape. Crito, who is a disinterested person not having the fear of\ndeath before his eyes, shall answer this for him. Before he was condemned they had often held\ndiscussions, in which they agreed that no man should either do evil, or return evil for evil, or betray\nthe right. Are these principles to be altered because the circumstances of Socrates are altered?\nCrito admits that they remain the same. Then is his escape consistent with the maintenance of\nthem? To this Crito is unable or unwilling to reply. Socrates proceeds:\u2013Suppose the Laws of\nAthens to come and remonstrate with him: they will ask \u2019Why does he seek to overturn them?\u2019 and\nif he replies, \u2019they have injured him,\u2019 will not the Laws answer, \u2019Yes, but was that the agreement?\nHas he any objection to make to them which would justify him in overturning them? Was he not\nbrought into the world and educated by their help, and are they not his parents? He might have\nleft Athens and gone where he pleased, but he has lived there for seventy years more constantly\nthan any other citizen.\u2019 Thus he has clearly shown that he acknowledged the agreement, which he\ncannot now break without dishonour to himself and danger to his friends. Even in the course of the\ntrial he might have proposed exile as the penalty, but then he declared that he preferred death to\nexile. And whither will he direct his footsteps? In any well-ordered state the Laws will consider\nhim as an enemy. Possibly in a land of misrule like Thessaly he may be welcomed at first, and\nthe unseemly narrative of his escape will be regarded by the inhabitants as an amusing tale. But if\nhe offends them he will have to learn another sort of lesson. Will he continue to give lectures in\nvirtue? That would hardly be decent. [...]\nQuestion Who persuades Socrates to escape?\nGround Truth Crito, Crito convinces Socrates to leave his cell.\nZebra Output Crito \u2713\nLLAMA Output According to the text, it is Crito who persuades Socrates to escape. \u2713\nChatGPT-3.5 Output Crito \u2713\nIn this example, while all models provide correct answers, LLAMA responds in a more verbose manner,\ndeviating from the given instruction.\nQasper\nInstruction You are given a scientific article and a question. Answer the question as concisely as\nyou can, using a single phrase or sentence if possible. If the question cannot be answered based on\nthe information in the article, write \u201cunanswerable\u201d. If the question is a yes/no question, answer\n\u201cyes\u201d, \u201cno\u201d, or \u201cunanswerable\u201d. Do not provide any explanation.\nDocument [...] For all the features, we separated the words in the tweets based on the language\nannotation (Section 2.3) and prepared the feature vector for each tweet by combining the vectors\nfor both the languages . Previous researches shows that letter n-grams are very efficient for clas-\nsifying text. They are language independent and does not require expensive text pre-processing\ntechniques like tokenization, stemming and stop words removal, hence in the case of code-mix\ntexts, this could yield good results BIBREF16 , BIBREF17 . Since the number of n-grams can be\nvery large we took trigrams which occur more than ten times in the corpus. For classifying humor\nin texts, it is important to understand the semantics of the sentence. Thus, we took a three word\nwindow as a feature to train our classification models to incorporate the contextual information.\nMany jokes and idioms sometimes have common words. We identified those words and took\nthem as as a feature for classification. In the preprocessing step, we decomposed hashtags using\ncamel cases and added them along with the words. Hence, common words in the hashtags were\nalso included in the feature vector. Classification Approach and\nResultsWe experimented with four different classifiers, namely, support vector machine BIBREF18\n, random forest, extra tree and naive bayes classifier BIBREF19 . Chi square feature selection\nalgorithm is applied to reduces the size of our feature vector. For training our system classifier, we\nused Scikit-learn BIBREF19 . 10-fold cross validation on 3543 code-mixed tweets was carried out\nby dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing.\nMean accuracy is calculated by taking the average of the accuracy obtained in each iteration of\nthe testing process. Table 2 shows the accuracy for each feature when trained using mentioned\nclassifiers along with the accuracy when all the features are used along with the overall accuracy.\nSupport vector machine with radial basis function kernel and extra tree classifier performs better\nthan other classifiers and yields 69.3% and 67.8% accuracy respectively. The reason kernel SVM\nyields the best result is because the number of observations is greator than the number of features\nBIBREF20 . N-grams proved to be the most efficient in all classification models followed by\ncommon words and hastags. Bag-of-words feature performed the worst in SVM, random forest\nand extra tree classifier but yielded better result in naive bayes classifiers. Accuracies mentioned in\ntable 2 were calculated using fine tuning of model parameters using grid search. Conclusion and\nFuture Work\nIn this paper, we describe a freely available corpus of 3453 English-Hindi code-mixed tweets. The\ntweets are annotated with humorous(H) and non-humorous(N) tags along with the language tags at\nthe word level. The task of humor identification in social media texts is analyzed as a classification\nproblem and several machine learning classification models are used. The features used in our\nclassification system are n-grams, bag-of-words, common words and hashtags. N-grams when\ntrained with support vector machines with radial basis function kernel performed better than other\nfeatures and yielded an accuracy of 68.5%. The best accuracy (69.3%) was given by support vector\nmachines with radial basis function kernel. This paper describes the initial efforts in automatic\nhumor detection in code-mixed social media texts. Corpus can be annotated with part-of-speech\ntags at the word level which may yield better results in language detection. [...]\nQuestion What type of system does the baseline classification use?\nGround Truth support vector machine BIBREF18 , random forest, extra tree and naive bayes\nclassifier BIBREF19, Classification system use n-grams, bag-of-words, common words and\nhashtags as features and SVM, random forest, extra tree and NB classifiers.\nZebra Output The baseline classification uses n-grams, bag-of-words, common words and\nhashtags. \u2713\nLLAMA Output to the article, the baseline classification system uses a support vector machine\n(SVM) classifier. \u2717\nChatGPT-3.5 Output Supervised classification system \u2717\nMusique\nInstruction You are given several paragraphs from Wikipedia and a question. Answer the question\nas concisely as you can, using a single phrase if possible. If the question cannot be answered based\non the information in the paragraphs, write \u201cunanswerable\u201d. Do not provide any explanation.\nDocument [...] Title: Israel\nIsrael, officially the State of Israel, is a country in the Middle East, on the southeastern shore of the\nMediterranean Sea and the northern shore of the Red Sea. It has land borders with Lebanon to the\nnorth, Syria to the northeast, Jordan on the east, the Palestinian territories of the West Bank and\nGaza Strip to the east and west, respectively, and Egypt to the southwest. The country contains\ngeographically diverse features within its relatively small area. Israel\u2019s economy and technology\ncenter is Tel Aviv, while its seat of government and proclaimed capital is Jerusalem, although the\nstate\u2019s sovereignty over East Jerusalem is not recognised internationally. The population of Israel\nwas estimated in 2017 to be 8,777,580 people, of whom 74.7% were Jewish, 20.8% Arab and 4.5%\nothers.\nTitle: The World (archipelago)\nThe World or The World Islands, (\u201cJuzur al- \u00afAlam\u201d) is an artificial archipelago of various small\nislands constructed in the rough shape of a world map, located in the waters of the Persian Gulf,\noff the coast of Dubai, United Arab Emirates. The World islands are composed mainly of sand\ndredged from Dubai\u2019s shallow coastal waters, and are one of several artificial island developments\nin Dubai. The World\u2019s developer is Nakheel Properties, and the project was originally conceived by\nSheikh Mohammed bin Rashid Al Maktoum, the ruler of Dubai. The actual construction was done\nby two Dutch (joint venture) specialist companies, Van Oord and Boskalis. The same companies\nalso created the Palm Jumeirah.\nTitle: Israel\nIsrael is considered the most advanced country in Southwest Asia and the Middle East in economic\nand industrial development. Israel\u2019s quality university education and the establishment of a highly\nmotivated and educated populace is largely responsible for spurring the country\u2019s high technology\nboom and rapid economic development. In 2010, it joined the OECD. The country is ranked 3rd in\nthe region and 38th worldwide on the World Bank\u2019s Ease of Doing Business Index as well as in\nthe World Economic Forum\u2019s Global Competitiveness Report. It has the second-largest number of\nstartup companies in the world (after the United States) and the largest number of NASDAQ-listed\ncompanies outside North America.\nTitle: Israel\nIsrael is one of the world\u2019s technological leaders in water technology. In 2011, its water technology\nindustry was worth around $2 billion a year with annual exports of products and services in the\ntens of millions of dollars. The ongoing shortage of water in the country has spurred innovation in\nwater conservation techniques, and a substantial agricultural modernization, drip irrigation, was\ninvented in Israel. [...]\nQuestion When was the region that lies to the north of the region Israel is located and where The\nWorld is located created?\nGround Truth 1930\nZebra Output 1946 \u2717\nLLAMA Output The region that lies to the north of Israel was created in the mid-18th century. \u2717\nChatGPT-3.5 Output unanswerable \u2717\nThis example underscores that the pertinent information for the given question cannot be discerned\nwithin the context, indicating that certain examples may not have been accurately constructed. The\nparagraphs do not provide information to answer when the region north of Israel and The World archipelago\nwas created. There is information about Israel and The World islands located in Dubai, but no mention of\nthe region to the north of Israel. The Zebra and LLAMA models manifest hallucinations, whereas GPT\nanswers accurately following the given instruction.\nMusique\nInstruction You are given several paragraphs from Wikipedia and a question. Answer the question\nas concisely as you can, using a single phrase if possible. If the question cannot be answered based\non the information in the paragraphs, write \u201cunanswerable\u201d. Do not provide any explanation.\nDocument Title: Jerome Quinn\nBorn in Green Bay, Wisconsin, Quinn was a realtor and served on the Green Bay Common\nCouncil, the Brown County, Wisconsin Board of Supervisors, the local Board of Education, and\nthe Wisconsin State Assembly from 1955 until 1973. He was a Republican.\nTitle: Max Rosenthal Tenement in Bydgoszcz\nMax Rosenthal House is a historical tenement located at Gda\u00b4nska Street N\u00b042 in downtown\nBydgoszcz, Poland, built when the city was part of the Kingdom of Prussia. It is registered on the\nKuyavian-Pomeranian Voivodeship Heritage List.\nTitle: Borders of China\nChina shares international borders with 14 sovereign states. In addition, there is a 30 - km border\nwith the special administrative region of Hong Kong, which was a British dependency before 1997,\nand a 3 km border with Macau, a Portuguese territory until 1999. With a land border of 22,117\nkilometres (13,743 mi) in total it also has the longest land border of any country.\nTitle: Notogawa, Shiga\nNotogawa Station (Location: N35.179899,E136.165913) is the only Japan Railway station in\nHigashiomi. The station is a rapid stop on the JR Biwako Line, located between stations in\nOmi-Hachiman to the east and Hikone to the west. The town shares a small border with Lake Biwa\nto the northwest.\nTitle: Tenement at Gdanska street 75, Bydgoszcz\nTenement at Gdanska street 75 is a historical habitation building located at Gda\u00b4nska Street N\u00b075,\nin Bydgoszcz. It is registered on the Kuyavian-Pomeranian Voivodeship Heritage List.\nTitle: Tenement (band)\nTenement is a three piece rock band from Appleton, Wisconsin formed in 2006. They are often\nassociated with the American hardcore punk scene. Their recorded output has been described\nas everything from \u201cnoise pop\u201d to \u201cfuzz punk\u201d, while in a live setting they are often known for\nexperimentation, improvisation, and high-energy performance. The visual art of singer/guitarist\nAmos Pitsch is associated with most of their records, as well as several records by other notable\npunk and hardcore bands. In 2016, they were included in \u201cRolling Stone\u2019s\u201d \u201c10 Great Modern\nPunk Bands\u201d.\nTitle: Pulaski High School\nPulaski High School is a public high school in Pulaski, Wisconsin, in Brown County, Wisconsin\n(school district also serves parts of Shawano, Outagamie and Oconto counties), that serves students\nin grades 9 through 12. Its mascot is the Red Raider.[...]\nQuestion What is the capital of the county that shares a border with the county where the band\nTenement was formed?\nGround Truth Green Bay\nZebra Output Appleton \u2717\nLLAMA Output Appleton. \u2717\nChatGPT-3.5 Output Appleton, Wisconsin \u2717\nThis example shows that the ground truth is not correct. The paragraph states that the band Tenement\nis \u201ca three piece rock band from Appleton, Wisconsin formed in 200\u201d. There is no information about\nthe capital of the county that shares a border with Appleton in the document but Appleton is the closest\nanswer given the context.\n"
  },
  {
    "title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments",
    "link": "https://arxiv.org/pdf/2312.09067.pdf",
    "upvote": "11",
    "text": "HOLODECK: Language Guided Generation of 3D Embodied AI Environments\nYue Yang*1, Fan-Yun Sun*2, Luca Weihs*4, Eli Vanderbilt4, Alvaro Herrasti4,\nWinson Han4, Jiajun Wu2, Nick Haber2, Ranjay Krishna3,4, Lingjie Liu1,\nChris Callison-Burch1, Mark Yatskar1, Aniruddha Kembhavi3,4, Christopher Clark4\n1University of Pennsylvania, 2Stanford University,\n3University of Washington, 4Allen Institute for Artificial Intelligence\nyueyang1996.github.io/holodeck/\na spa with large hot tubs\nthree professors\u2019 office connected to a long hallway, the professor in office 1 is a fan of Star Wars\na 1b1b apartment of a researcher who has a cat\nVictorian-style living room\nModern-style living room\nan arcade room with a pool table\na sculpture museum with diverse statues\nFigure 1. Example outputs of HOLODECK\u2014a large language model powered system, which can generate diverse types of environments\n(arcade, spa, museum), customize for styles (Victorian-style), and understand fine-grained requirements (\u201chas a cat\u201d, \u201cfan of Star Wars\u201d).\nAbstract\n3D simulated environments play a critical role in Embod-\nied AI, but their creation requires expertise and extensive\nmanual effort, restricting their diversity and scope. To miti-\ngate this limitation, we present HOLODECK, a system that\ngenerates 3D environments to match a user-supplied prompt\nfully automatedly. HOLODECK can generate diverse scenes,\ne.g., arcades, spas, and museums, adjust the designs for\n*Equal technical contribution. Work done while at PRIOR@AI2.\nstyles, and can capture the semantics of complex queries\nsuch as \u201capartment for a researcher with a cat\u201d and \u201coffice\nof a professor who is a fan of Star Wars\u201d. HOLODECK lever-\nages a large language model (GPT-4) for common sense\nknowledge about what the scene might look like and uses\na large collection of 3D assets from Objaverse to populate\nthe scene with diverse objects. To address the challenge of\npositioning objects correctly, we prompt GPT-4 to generate\nspatial relational constraints between objects and then opti-\nmize the layout to satisfy those constraints. Our large-scale\n1\narXiv:2312.09067v1  [cs.CV]  14 Dec 2023\nhuman evaluation shows that annotators prefer HOLODECK\nover manually designed procedural baselines in residential\nscenes and that HOLODECK can produce high-quality out-\nputs for diverse scene types. We also demonstrate an exciting\napplication of HOLODECK in Embodied AI, training agents\nto navigate in novel scenes like music rooms and daycares\nwithout human-constructed data, which is a significant step\nforward in developing general-purpose embodied agents.\n1. Introduction\nThe predominant approach in training embodied agents in-\nvolves learning in simulators [8, 23, 25, 38, 43, 54]. Gen-\nerating realistic, diverse, and interactive 3D environments\nplays a crucial role in the success of this process.\nExisting Embodied AI environments are typically crafted\nthrough manual design [6, 13, 25, 26], 3D scanning [8, 41,\n43], or procedurally generated with hard-coded rules [7].\nHowever, these methods require considerable human effort\nthat involves designing a complex layout, using assets sup-\nported by an interactive simulator, and placing them into\nscenes while ensuring semantic consistency between the\ndifferent scene elements. Therefore, prior work on produc-\ning 3D environments mainly focuses on limited environ-\nment types. To move beyond these limitations, recent works\nadapt 2D foundational models to generate 3D scenes from\ntext [11, 18, 56]. However, these models often produce\nscenes with significant artifacts, such as mesh distortions,\nand lack the interactivity necessary for Embodied AI. More-\nover, there are models tailored for specific tasks like floor\nplan generation [19, 45] or object arrangement [36, 52]. Al-\nthough effective in their respective domains, they lack overall\nscene consistency and rely heavily on task-specific datasets.\nIn light of these challenges, we present HOLODECK, a\nlanguage-guided system built upon AI2-THOR [25], to auto-\nmatically generate diverse, customized, and interactive 3D\nembodied environments from textual descriptions. Shown\nin Figure 2, given a description (e.g., a 1b1b apartment of\na researcher who has a cat), HOLODECK uses a Large Lan-\nguage Model (GPT-4 [35]) to design the floor plan, assign\nsuitable materials, install the doorways and windows and\narrange 3D assets coherently in the scene using constraint-\nbased optimization. HOLODECK chooses from over 50K\ndiverse and high-quality 3D assets from Objaverse [9] to\nsatisfy a myriad of environment descriptions.\nMotivated by the emergent abilities of Large Language\nModels (LLMs) [51], HOLODECK exploits the common-\nsense priors and spatial knowledge inherently present in\nLLMs. This is exemplified in Figure 1, where HOLODECK\ncreates diverse scene types such as arcade, spa and museum,\ninterprets specific and abstract prompts by placing relevant\nobjects appropriately into the scene, e.g., an \u201cR2-D2\u201d1 on\n1A fictional robot character in the Star Wars.\nthe desk for \u201ca fan of Star Wars\u201d. Beyond object selection\nand layout design, HOLODECK showcases its versatility in\nstyle customization, such as creating a scene in a \u201cVictorian-\nstyle\u201d by applying appropriate textures and designs to the\nscene and its objects. Moreover, HOLODECK demonstrates\nits proficiency in spatial reasoning, like devising floor plans\nfor \u201cthree professors\u2019 offices connected by a long hallway\u201d\nand having regular arrangements of objects in the scenes.\nOverall, HOLODECK offers a broad coverage approach to\n3D environment generation, where textual prompts unlock\nnew levels of control and flexibility in scene creation.\nThe effectiveness of HOLODECK is assessed through its\nscene generation quality and applicability to Embodied AI.\nThrough large-scale user studies involving 680 participants,\nwe demonstrate that HOLODECK significantly surpasses ex-\nisting procedural baseline PROCTHOR [7] in generating\nresidential scenes and achieves high-quality outputs for var-\nious scene types. For the Embodied AI experiments, we\nfocus on HOLODECK\u2019s application in aiding zero-shot ob-\nject navigation in previously unseen scene types. We show\nthat agents trained on scenes generated by HOLODECK can\nnavigate better in novel environments (e.g., Daycare and\nGym) designed by experts.\nTo summarize, our contributions are three-fold: (1) We\npropose HOLODECK, a language-guided system capable\nof generating diverse, customized, and interactive 3D en-\nvironments based on textual descriptions; (2) The human\nevaluation validates HOLODECK\u2019s capability of generating\nresidential and diverse scenes with accurate asset selection\nand realistic layout design; (3) Our experiments demonstrate\nthat HOLODECK can aid Embodied AI agents in adapting to\nnew scene types and objects during object navigation tasks.\n2. Related Work\nEmbodied AI Environments. Previous work mainly relies\non 3D artists to design the environments [6, 13, 25, 26, 38,\n54], which is hard to scale up or construct scenes from 3D\nscans [41, 43, 46] to reduce human labor, but the scenes\nare less interactive. The procedural generation framework\nPROCTHOR [7] showcases its potential to generate large-\nscale interactive environments for training embodied agents.\nPhone2Proc [8] uses a phone scan to create training scenes\nthat are semantically similar to the desired real-world scene.\nA concurrent work, RoboGen [50], proposes to train robots\nby generating diversified tasks and scenes. These works\nparallel our concept, HOLODECK, which aims to train gener-\nalizable embodied agents and presents an avenue for further\nexploration in text-driven 3D interactive scene generation.\nLarge Language Model for Scene Design. Many works on\nscene design either learn spatial knowledge priors from ex-\nisting 3D scene databases [4, 29, 47\u201349, 52, 57] or leverage\nuser input and refine the 3D scene iteratively [3, 5]. However,\nhaving to learn from datasets of limited categories such as\n2\nPlease help select the objects\nwith the following format\ncategory | description | size | \nquantity | children objects\nUser input: a 1b1b apartment of a researcher who has a cat\nFloor Module & Wall Modules\nDoorway & Window Modules\nObject Selection Module\nLayout Design Module\nPlease tell me how to arrange\nthese objects using constraints.\nobject-name | constraint-1 |\nconstraint-2 | \u2026\u2026\nPlease design the doorway\nwith the following format:\nroom 1 | room 2 | connection \ntype | size | style\nPlease design the floor plan\nwith the following format:\nroom type | floor material | \nwall material | vertices\n1. living room | oak hardwood | \ncream-colored stucco | \n[(0, 0), (0, 6), (7, 6), (7, 0)]\n2. kitchen | blue marble tile\u2026\n1. living room | bathroom |\ndoorway | single | wooden door \nwith black handle\n2. living room | kitchen | open\n1. coffee table | large round\nwood | (100, 100, 45) | 1 | mug\n2. cat tower | multi-level | (60,\n60, 180) | 1 | cat toy\n1. sofa | edge\n2. coffee table | middle | near\nsofa | face to, sofa\n3. tv stand | edge | face to sofa\nPlease design the window with\nthe following format:\nroom | direction | type | size |\nquantity | height (in cm)\n1. living room | west | hung | \n(130, 130) | 4 | 80\n2. kitchen | south | slider | \n(150, 92) | 2 | 100 \u2026\nRoom Vertices\nRetrieve Assets: multi-level cat tower\nSelect Materials\noak hardwood\nSpatial Relational Constraints\nabove\nface to, far\non top of\nnear\nabove\ntv stand\npainting-1\npainting-2\ncoffee table\nmug\nsofa\nPrompt\nLLM Response\nFigure 2. Given a text input, HOLODECK generates the 3D environment through multiple rounds of conversation with an LLM.\n3D-FRONT [12] restricts their applicability. Recently, Large\nLanguage Models (LLMs) were shown to be useful in gen-\nerating 3D scene layouts [10, 28]. However, their methods\nof having LLMs directly output numerical values can yield\nlayouts that defy physical plausibility (e.g., overlapping as-\nsets). In contrast, HOLODECK uses LLMs to sample spatial\nrelational constraints and a solver to optimize the layout, en-\nsuring physically plausible scene arrangements. Our human\nstudy shows a preference for HOLODECK-generated layouts\nover those generated end-to-end by LLMs. (see Sec 4.3).\nText-driven 3D Generation. Early endeavors in 3D genera-\ntion focus on learning the distribution of 3D shapes and/or\ntextures from category-specific datasets [16, 33, 53, 55, 58].\nSubsequently, the advent of large vision-language models\nlike CLIP [40] enables zero-shot generation of 3D textures\nand objects [14, 20, 27, 30, 31, 37]. These works excel at\ngenerating 3D objects but struggle to generate complex 3D\nscenes. More recently, emerging works generate 3D scenes\nby combining pre-trained text-to-image models with depth\nprediction algorithms to produce either textured meshes or\nNeRFs [11, 18, 56]. However, these approaches yield 3D rep-\nresentations that lack modular composability and interactive\naffordances, limiting their use in embodied AI. In contrast,\nHOLODECK utilizes a comprehensive 3D asset database to\ngenerate semantically precise, spatially efficient, and interac-\ntive 3D environments suitable for training embodied agents.\n3. HOLODECK\nHOLODECK is a promptable system based on AI2-THOR [7,\n25], enriched with massive assets from Objaverse [9], which\ncan produce diverse, customized, and interactive Embodied\nAI environments with the guidance of large language models.\nAs shown in Figure 2, HOLODECK employs a system-\natic approach to scene construction, utilizing a series of\nspecialized modules: (1) the Floor & Wall Module develop\nfloor plans, constructs wall structures and selects appropri-\nate materials for the floors and walls; (2) the Doorway &\nWindow Module integrates doorways and windows into the\ncreated environment; (3) the Object Selection Module re-\ntrieves appropriate 3D assets from Objaverse, and (4) the\nConstraint-based Layout Design Module arranges the assets\nwithin the scene by utilizing spatial relational constraints to\nensure that the layout of objects is realistic.\nIn the following sections, we introduce our prompting\napproach that converts high-level user natural language spec-\nifications into a series of language model queries for con-\nstructing layouts. We then provide a detailed overview of\neach module shown in Figure 2 and how they contribute to\nthe final scene. Finally, we illustrate how HOLODECK lever-\nages Objaverse assets to ensure diversity in scene creation\nand efficiency for Embodied AI applications. Comprehen-\nsive details of HOLODECK can be found in the supplements.\n3\na high school building with six classrooms connected\nto the two sides of a long hallway\na house with a living room in the center and four bedrooms connected to each side of \nthe living room, shaped like a cross, each bedroom is connected to a bathroom\nclassrooms\nhallway\nbathrooms\nliving room\nbedrooms\nFigure 3. Floorplan Customizability. HOLODECK can interpret complicated input and craft reasonable floor plans correspondingly.\na prison cell\na bedroom of a girl who loves the pink color\na wine cellar with red wall bricks\na 80s bar with checkered flooring\nFigure 4. Material Customizability. HOLODECK can adapt the floor and wall materials to make the scenes more realistic.\nan apartment for a disabled person who needs to use wheelchair\na sunroom with floor-to-ceiling windows covering all walls\nFigure 5. Door & window Customizability. HOLODECK can adjust the size, quantity, position, etc., of doors & windows based on the input.\nOverall Prompt Design. Each module in Figure 2 takes in-\nformation from a language model and converts it to elements\nincluded in the final layout. An LLM prompt is designed\nfor each module with three elements: (1) Task Description:\noutlines the context and goals of the task; (2) Output For-\nmat: specifies the expected structure and type of outputs\nand (3) One-shot Example: a concrete example to assist the\nLLM\u2019s comprehension of the task. The text within the blue\ndialog boxes of Figure 2 represents examples of simplified\nprompts2. LLM\u2019s high-level responses to these prompts are\npost-processed and then used as input arguments for the\nmodules to yield low-level specifications of the scene.\nThe Floor & Wall Module, illustrated in the first panel\nof Figure 2, is responsible for creating floor plans, con-\nstructing wall structures, and selecting materials for floors\nand walls. Each room is represented as a rectangle, de-\nfined by four tuples that specify the coordinates of its cor-\nners. GPT-4 directly yields the coordinates for placing the\nrooms and suggests realistic dimensions and connectivity for\n2The complete prompts (available in the supplementary materials) in-\nclude additional guidance for LLMs to avoid common errors we observe.\nFor example, by adding a sentence, \u201cthe minimal area per room is 9 m2\u201d,\nHOLODECK can avoid generating overly small rooms.\nthese rooms. Figure 3 illustrates several examples of diverse\nlayouts this module proposes where HOLODECK generates\nprompt-appropriate, intricate, multi-room floor plans.\nThis module also chooses materials for the floors and\nwalls, which is crucial for enhancing the realism of environ-\nments. HOLODECK can match LLM proposals to one of 236\nmaterials, each available in 148 colors, enabling semantic\ncustomization of scenes. As shown in Figure 4, HOLODECK\ncan generate scenes with suitable materials based on the type\nof scene, such as opting for concrete walls and floors in a\nprison cell scenario. Inputs with specific texture require-\nments are often reflected in the final design, for example,\n\u201cpink color\u201d, \u201cred wall bricks,\u201d and \u201ccheckered floor\u201d.\nThe Doorway & Window Module, illustrated in the second\npanel of Figure 2, is responsible for proposing room connec-\ntions and windows. Each of these two properties is queried\nseparately from the LLM. The LLM can propose doorways\nand windows that match 40 door styles and 21 window types,\neach of which can be modified by several properties, includ-\ning size, height, quantity, etc. For instance, Figure 5 shows\nHOLODECK\u2019s tailored designs on doors and windows, such\nas wider doors for \u201cwheelchair accessibility\u201d and multiple\nfloor-to-ceiling windows in a \u201csunroom\u201d setting.\n4\nan aerospace exhibition hall\na hunter cabinet with wall-mounted animals\na study room of a boy who likes Pok\u00e9mon\na compact home gym with a ceiling fan\nFloor Objects\nWall Objects\nCeiling Objects\nSmall Objects\nFigure 6. Objects Customizability. HOLODECK can select and place appropriate floor/wall/small/ceiling objects conditioned on the input.\nbed\ncabinet\nbathtub\ntoilet\nbasket\ntowel rail\ntoilet paper\nholder\nside of\nnear\nnear\nabove\nabove\nnear\nside of\nnightstand-1\nnightstand-2\nottoman\ndresser\nwall clock\nabove\nin front of\nnear\nside of\nnear\nside of\nnear\nBedroom\nprojector screen\ntable\nchair-0\nchair-1\nchair-2\nchair-3\nchair-4\nchair-5\nnear, face to\nnear, face to\nabove\nalign\nalign\nalign\nnear\nface to\nface to\nnear\nConference room\nBathroom\nFigure 7. Examples of Spatial Relational Constraints generated by LLM and their solutions found by our constraint satisfaction algorithm.\na classic dining room with a long wooden table\na garage with a red sedan and a black bicycle\nFigure 8. Output Diversity. HOLODECK can generate multiple variants for the same input with different assets and layouts.\nThe Object Selection Module, illustrated in the third panel\nof Figure 2, allows the LLM to propose objects that should be\nincluded in the layout. Leveraging the extensive Objaverse\nasset collection, HOLODECK can fetch and place diverse\nobjects in the scene. Queries are constructed with LLM-\nproposed descriptions and dimensions, like \u201cmulti-level cat\ntower, 60 \u00d7 60 \u00d7 180 (cm)\u201d to retrieve the optimal asset\nfrom Objaverse. The retrieval function3 considers visual and\ntextual similarity and dimensions to ensure the assets match\nthe design. Figure 6 shows the capability of HOLODECK to\ncustomize diverse objects on the floor, walls, on top of other\nitems, and even on the ceiling.\nThe Constraint-based Layout Design Module, illustrated\nin the fourth panel of Figure 2, generates the positioning and\norientation of objects. Previous work [10] shows LLM can\ndirectly provide the absolute value of the object\u2019s bounding\nbox. However, when attempting to place a diverse lot of\nassets within environments, this method frequently leads to\nout-of-boundary errors and object collisions. To address this,\ninstead of letting LLM directly operate on numerical values,\nwe propose a novel constraint-based approach that employs\nLLM to generate spatial relations between the objects, e.g.,\n\u201ccoffee table, in front of, sofa\u201d, and optimize the layout based\n3We use CLIP [40] to measure the visual similarity, Sentence-BERT\n[42] for the textual similarity, and 3D bounding box sizes for the dimension.\non the constraints. Given the probabilistic nature of LLMs,\nHOLODECK can yield multiple valid layouts given the same\nprompt as shown in Figure 8.\nSpatial Relational Constraints. We predefined ten types of\nconstraints, organized into five categories: (1) Global: edge,\nmiddle; (2) Distance: near, far; (3) Position: in front of, side\nof, above, on top of; (4) Alignment: center aligned and (5)\nRotation: face to. LLM selects a subset of constraints for\neach object, forming a scene graph for the room (examples\nshown in Figure 7). Those constraints are treated softly, al-\nlowing for certain violations when finding a layout to satisfy\nall constraints is not feasible. Besides those soft constraints,\nwe enforce hard constraints to prevent object collisions and\nensure that all objects are within the room\u2019s boundaries.\nConstraint Satisfaction. We first reformulate the spatial rela-\ntional constraints defined above into mathematical conditions\n(e.g., two objects are center-aligned if they share the same\nx or y coordinate). To find layouts that satisfy constraints\nsampled by LLMs, we adopt an optimization algorithm to\nplace objects autoregressively. The algorithm first uses LLM\nto identify an anchor object and then explores placements\nfor the anchor object. Subsequently, it employs Depth-First-\nSearch (DFS)4 to find valid placements for the remaining\n4Given the linear nature of constraints, a Mixed Integer Linear Program-\nming (MILP) solver can also be employed. While we assume the DFS\n5\nHolodeck\n59.8%\n(1347)\nProcTHOR\n17.0%\n(382)\nEqual\n23.2%\n(523)\nAsset Selection\nHolodeck\n56.9%\n(1283)\nProcTHOR\n28.0%\n(631)\nEqual\n15.0%\n(338)\nLayout Coherence\nHolodeck\n64.4%\n(1450)\nProcTHOR\n23.3%\n(524)\nEqual\n12.3%\n(277)\nOverall Preference\nFigure 9. Comparative human evaluation of HOLODECK and PROC-\nTHOR across three criteria. The pie charts show the distribution of\nannotator preferences, showing both the percentage and the actual\nnumber of annotations favoring each system.\nobjects. A placement is only valid if all the hard constraints\nare satisfied. For example, in Figure 7, bed is selected as the\nanchor object in the bedroom, and the nightstands are placed\nsubsequently. The algorithm is executed for a fixed time\n(30 seconds) to get multiple candidate layouts and return the\none that satisfies the most total constraints. We verify the\neffectiveness of our constraint-based layout in Sec 4.3.\nLeveraging Objaverse Assets, HOLODECK is able to sup-\nport the creation of diverse and customized scenes. We\ncurate a subset of assets suitable for indoor design from\nObjaverse 1.0. These assets are further annotated by GPT-4-\nVison [34] automatically with additional details, including\ntextual descriptions, scale, canonical views, etc.5 Together\nwith the assets from PROCTHOR, our library encompasses\n51,464 annotated assets. To import Objaverse assets into\nAI2-THOR for embodied AI applications, we optimize the\nassets by reducing mesh counts to minimize the loading\ntime in AI2-THOR, generating visibility points and colliders.\nMore details on importing Objaverse assets into AI2-THOR\nare available in the supplementary materials.\nIn the following sections, we will evaluate the quality and\nutility of the scenes generated by HOLODECK.\n4. Human Evaluation\nWe conduct comprehensive human evaluations to assess the\nquality of HOLODECK scenes, with a total of 680 graduate\nstudents participating in three user studies: (1) a compar-\native analysis on residential scenes with PROCTHOR as\nthe baseline; (2) an examination of HOLODECK\u2019s ability in\ngenerating diverse scenes, and (3) an ablation study to vali-\ndate the effectiveness of our layout design method. Through\nthese user studies, we demonstrate that HOLODECK can cre-\nate residential scenes of better quality than previous work\nwhile being able to extend to a wider diversity of scene types.\n4.1. Comparative Analysis on Residential Scenes\nThis study collects human preference scores to compare\nHOLODECK with PROCTHOR [7], the sole prior work capa-\nsolver in our experiments, we analyze the MILP solver in the supplements.\n5GPT-4-Vision can take in multiple images, we prompt it with multi-\nview screenshots of 3D assets to get the annotations.\nBathroom\nBedroom\nKitchen\nLiving Room\n20\n25\n30\n35\n40\nCLIP Score\niTHOR*\nHolodeck\nProcTHOR\nFigure 10. CLIP Score comparison over four residential scene\ntypes. * denotes iTHOR scenes are designed by human experts.\nble of generating complete, interactable scenes. Our compar-\nison focuses on residential scenes, as PROCTHOR is limited\nto four types: bathroom, bedroom, kitchen, and living room.\nSetup. We prepared 120 scenes for human evaluation, com-\nprising 30 scenes per scene type, for both HOLODECK and\nthe PROCTHOR baseline. The PROCTHOR baseline has\naccess to the same set of Objaverse assets as HOLODECK.\nFor HOLODECK, we take the scene type, e.g., \u201cbedroom\u201d,\nas the prompt to generate the scenes. We pair scenes of\nthe same scene type from the two systems, resulting in 120\npaired scenes for human evaluation. For each paired scene,\nwe display two shuffled top-down view images of the scenes\nfrom the two systems. We ask the annotator to choose which\nscene is better or equally good based on three questions:\n(1) Asset Selection: which selection of 3D assets is more\naccurate/faithful to the scene type? (2) Layout Coherence:\nwhich arrangement of 3D assets adheres better to realism and\ncommon sense (considering the position and orientation)?\nand (3) Overall Preference: which of the two scenes would\nyou prefer given the scene type?\nHumans prefer HOLODECK over PROCTHOR. Figure 9\npresents a clear preference for HOLODECK in the compara-\ntive human evaluation against PROCTHOR, with a major-\nity of annotators favoring HOLODECK for Asset Selection\n(59.8%), Layout Coherence (56.9%), and showing a signifi-\ncant preference in Overall Preference (64.4%).\nIn addition to human judgments, we employ CLIP Score6\n[17] to quantify the visual coherence between the top-down\nview of the scene and its corresponding scene type embedded\nin a prompt template \u201ca top-down view of [scene type]\u201d. Be-\nsides, we add human-designed scenes from iTHOR [25]\nas the upper bound for reference. Figure 10 shows the\nCLIP scores of HOLODECK exceed PROCTHOR with great\nmargins and closely approach the performance of iTHOR,\ndemonstrating HOLODECK\u2019s ability to generate visually co-\nherent scenes faithful to the designated scene types. The\nCLIP Score experiment agrees with our human evaluation.\n6Here, we use OpenCLIP [22] with ViT-L/14 trained on LAION-2B\n[44]. We use cosine similarity times 100 as the CLIP Score.\n6\nlaundromat\nnursery\nwarehouse\nbookstore\nflorist shop\nlaboratory\nbakery\nbuffet\nFigure 11. Human evaluation on 52 scene types from MIT Scenes [39] with qualitative examples. The three horizontal lines represent the\naverage score of iTHOR, HOLODECK, and PROCTHOR on four types of residential scenes (bedroom, living room, bathroom and kitchen.)\n4.2. HOLODECK on Diverse Scenes\nTo evaluate HOLODECK\u2019s capability beyond residential\nscenes, we have humans rate its performance on 52 scene\ntypes7 from MIT Scenes Dataset [39], covering five cate-\ngories: Stores (deli, bakery), Home (bedroom, dining room),\nPublic Spaces (museum, locker room), Leisure (gym, casino)\nand Working Space (office, meeting room).\nSetup. We prompt HOLODECK to produce five outputs for\neach type using only the scene name as the input, accumu-\nlating 260 examples across the 52 scene types. Annotators\nare presented with a top-down view image and a 360-degree\nvideo for each scene and asked to rate them from 1 to 5\n(with higher scores indicating better quality), considering\nasset selection, layout coherence, and overall match with\nthe scene type. To provide context for these scores, we in-\nclude residential scenes from PROCTHOR and iTHOR in\nthis study, with 20 scenes from each system.\nHOLODECK can generate satisfactory outputs for most\nscene types. Figure 11 demonstrates the human preference\nscores for diverse scenes with qualitative examples. Com-\npared to PROCTHOR\u2019s performance in residential scenes,\nHOLODECK achieves higher human preference scores over\nhalf of (28 out of 52) the diverse scenes. Given that PROC-\nTHOR relies on human-defined rules and residential scenes\nare relatively easy to build with common objects and sim-\nple layout, HOLODECK\u2019s breadth of competence highlights\nits robustness and flexibility in generating various indoor\nenvironments. However, we notice that HOLODECK strug-\ngles with scenes requiring more complex layouts such as\n7Limited by the PROCTHOR framework, we filter those scenes types\nthat require special structures such as swimming pool, subway, etc.\nMethod\nBathroom\nBedroom\nKitchen\nLiving Room\nAverage\nABSOLUTE\n0.369\n0.343\n0.407\n0.336\n0.364\nRANDOM\n0.422\n0.339\n0.367\n0.348\n0.369\nEDGE\n0.596\n0.657\n0.655\n0.672\n0.645\nCONSTRAINT\n0.696\n0.745\n0.654\n0.728\n0.706\nTable 1. Mean Reciprocal Rank (\u2191) of different layouts ranked\nby human. CONSTRAINT: using spatial relational constraints;\nABSOLUTE: LLM-defined absolute positions; RANDOM: randomly\nplace the objects and EDGE: put objects at the edge of the room.\nrestaurant or unique assets unavailable in Objaverse, e.g.,\n\u201ca dental x-ray machine\u201d for the scene dental office. Future\nwork can improve the system by incorporating more assets\nand introducing more sophisticated layout algorithms.\n4.3. Ablation Study on Layout Design\nThis user study aims to validate the effectiveness of\nHOLODECK\u2019s constraint-based layout design method.\nBaselines. We consider four layout design methods: (1)\nCONSTRAINT: the layout design method of HOLODECK; (2)\nABSOLUTE: directly obtaining the absolute coordinates and\norientation of each object from LLM akin to LayoutGPT\n[10]; (3) RANDOM: randomly place all objects in the room\nwithout collision; (4) EDGE: placed objects along the walls.\nSetup. We modify the residential scenes of HOLODECK\nused in 4.1 by altering the layouts using the previously men-\ntioned methods while keeping the objects in the scene identi-\ncal. We present humans with four shuffled top-down images\nfrom each layout strategy and ask them to rank the four lay-\nouts considering out-of-boundary, object collision, reachable\nspace, and layout realism.\nConstraint-based layout is more reliable. Table 1 reports\nthe Mean Reciprocal Rank of different layout design meth-\n7\nOffice\nDaycare\nMusic Room\nGym\nArcade\nAverage\nMethod\nSuccess\nSPL\nSuccess\nSPL\nSuccess\nSPL\nSuccess\nSPL\nSuccess\nSPL\nSuccess\nSPL\nRandom\n3.90\n0.039\n4.05\n0.041\n5.20\n0.052\n2.84\n0.029\n2.54\n0.025\n3.71\n0.037\nPROCTHOR [7]\n8.77\n0.031\n2.87\n0.011\n6.17\n0.027\n0.68\n0.002\n2.06\n0.005\n4.11\n0.015\n+OBJAVERSE (ours)\n18.42\n0.068\n8.99\n0.061\n25.69\n0.157\n18.79\n0.101\n13.21\n0.076\n17.02\n0.093\n+HOLODECK (ours)\n25.05\n0.127\n15.61\n0.127\n31.08\n0.202\n18.40\n0.110\n11.84\n0.069\n20.40\n0.127\nTable 2. Zero-shot ObjectNav on NOVELTYTHOR. PROCTHOR is the model pretrained on PROCTHOR-10K [7]. +OBJAVERSE and\n+HOLODECK stand for models finetuned on the corresponding scenes. We report Success (%) and Success weighted by Path Length (SPL).\nNovel Environment: music room\nNovel object: piano\nZero-shot Object Navigation on NOVELTYTHOR\nPretraining on PROCTHOR\nFine-tuning on HOLODECK\nFigure 12. Zero-shot object navigation in novel scenes. Given a\nnovel scene type, e.g., Music Room, HOLODECK can synthesize\nnew scenes for fine-tuning to improve the performance of pretrained\nagents in expert-designed environments.\nods. HOLODECK\u2019s constraint-based approach outperforms\nthe other methods significantly on bathroom, bedroom and\nliving room. CONSTRAINT and EDGE perform similarly on\nkitchen, where it is common to align most objects against\nwalls. The ABSOLUTE method performs no better than RAN-\nDOM due to its tendency to create scenes with collision and\nboundary errors (see examples in the supplement), typically\nrated poorly by humans. These results endorse spatial rela-\ntional constraints as a viable strategy for generating scenes\nthat adhere to commonsense logic.\n5. Object Navigation in Novel Environments\nAs illustrated in Figure 12, one application of HOLODECK is\nsynthesizing training environments to better match a novel\ntesting distribution. To study this application, we consider\nObjectNav [1], a common task in which a robot must nav-\nigate toward a specific object category. As existing bench-\nmarks [6, 7, 41] for ObjectNav consider only household\nenvironments and support a very limited collection of object\ntypes (16 object types in total combining the above bench-\nmarks), we introduce NOVELTYTHOR, an artist-designed\nbenchmark to evaluate embodied agents in diverse environ-\nments. Subsequently, we use the ObjectNav model pre-\ntrained on PROCTHOR-10K [25] and finetune it on 100\nscenes generated by HOLODECK. These scenes are created\nby prompting HOLODECK with the novel scene type as input.\nThe model is then evaluated on NOVELTYTHOR.\nNOVELTYTHOR. We have two professional digital artists\nmanually create 10 novel testing environments with two\nexamples for each of the five categories: Office, Daycare,\nMusic Room, Gym, and Arcade. Each scene contains novel\nobject types not included in the existing ObjectNav tasks,\ne.g., \u201cpiano\u201d in Music Room, \u201ctreadmill\u201d in Gym, etc. Across\nNOVELTYTHOR, there are 92 unique object types.\nBaselines. For all methods except the one of random ac-\ntion, we use the same pre-trained ObjectNav model from\nPROCTHOR-10K [25], which has been trained for \u2248400M\nsteps to navigate to 16 object categories.\nTo adapt the\nagent to novel scenes without human-construct training data,\nwe consider two methods: (1) +HOLODECK: we prompt8\nHOLODECK to automatically create 100 scenes for each\nscene type; (2) +OBJAVERSE: a strong baseline by enhanc-\ning PROCTHOR with HOLODECK\u2019s scene-type-specific ob-\nject selection, specifically, those scenes are populated with\nsimilar Objaverse assets chosen by HOLODECK.\nModel. Our ObjectNav models use the CLIP-based architec-\ntures of [24], which contains a CNN visual encoder and a\nGRU to capture temporal information. We train each model\nwith 100 scenes for 50M steps, which takes approximately\none day on 8 Quadro RTX 8000 GPUs. We select the check-\npoint of each model based on the best validation performance\non its own validation scenes.\nResults. Table 2 shows zero-shot performance on NOV-\nELTYTHOR. HOLODECK achieves the best performance\non average and surpasses baselines with considerable mar-\ngins on Office, Daycare, and Music Room. On Gym and\nArcade, +HOLODECK and +OBJAVERSE perform similarly.\nGiven that the main difference between +HOLODECK and\n+OBJAVERSE scenes is in the object placements, the ob-\nserved difference suggests that HOLODECK is more adept\nat creating layouts that resemble those designed by humans.\nFor example, We can observe in Figure 12 that the music\nroom in NOVELTYTHOR contains a piano, violin cases,\nand cellos that are in close proximity to each other. The\nmusic room generated by HOLODECK also shows a similar\narrangement of these objects, highlighting the \u201ccommon-\nsense\u201d understanding of our method. PROCTHOR struggles\nin NOVELTYTHOR, often indistinguishably from random,\nbecause of poor object coverage during training.\n8Here, we prompt with the scene name and its paraphrases to get more\ndiverse outputs, e.g., we use \u201cgame room\u201d, \u201camusement center\u201d for Arcade.\n8\n6. Conclusion and Limitation\nWe propose HOLODECK, a system guided by large language\nmodels to generate diverse and interactive Embodied AI\nenvironments with text descriptions. We assess the quality of\nHOLODECK with large-scale human evaluation and validate\nits utility in Embodied AI through object navigation in novel\nscenes. We plan to add more 3D assets to HOLODECK and\nexplore its broader applications in Embodied AI in the future.\nReferences\n[1] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Olek-\nsandr Maksymets, Roozbeh Mottaghi, Manolis Savva,\nAlexander Toshev, and Erik Wijmans. ObjectNav Revisited:\nOn Evaluation of Embodied Agents Navigating to Objects.\nIn arXiv:2006.13171, 2020. 8\n[2] Michel B\u00b4enichou, Jean-Michel Gauthier, Paul Girodet, Ger-\nard Hentges, Gerard Ribi`ere, and Olivier Vincent. Experi-\nments in mixed-integer linear programming. Mathematical\nProgramming, 1:76\u201394, 1971. 13\n[3] Angel Chang, Manolis Savva, and Christopher D Manning.\nInteractive learning of spatial knowledge for text to 3d scene\ngeneration. In Proceedings of the Workshop on Interactive\nLanguage Learning, Visualization, and Interfaces, pages 14\u2013\n21, 2014. 2\n[4] Angel X Chang, Mihail Eric, Manolis Savva, and Christo-\npher D Manning. Sceneseer: 3d scene design with natural\nlanguage. arXiv preprint arXiv:1703.00050, 2017. 2\n[5] Yu Cheng, Yan Shi, Zhiyong Sun, Dezhi Feng, and Lixin\nDong. An interactive scene generation using natural language.\nIn 2019 International Conference on Robotics and Automa-\ntion (ICRA), pages 6957\u20136963. IEEE, 2019. 2\n[6] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha\nKembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador,\nDustin Schwenk, Eli VanderBilt, Matthew Wallingford, et al.\nRobothor: An open simulation-to-real embodied ai platform.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 3164\u20133174, 2020. 2, 8\n[7] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nJordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali\nFarhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Proc-\nTHOR: Large-Scale Embodied AI Using Procedural Genera-\ntion. In NeurIPS, 2022. Outstanding Paper Award. 2, 3, 6, 8,\n12\n[8] Matt Deitke, Rose Hendrix, Ali Farhadi, Kiana Ehsani, and\nAniruddha Kembhavi. Phone2proc: Bringing robust robots\ninto our chaotic world. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 9665\u20139675, 2023. 2\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,\nAniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13142\u201313153, 2023. 2, 3\n[10] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar-\njun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and\nWilliam Yang Wang. Layoutgpt: Compositional visual plan-\nning and generation with large language models.\narXiv\npreprint arXiv:2305.15393, 2023. 3, 5, 7\n[11] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation. arXiv\npreprint arXiv:2302.01133, 2023. 2, 3\n[12] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming\nWang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia,\nBinqiang Zhao, et al. 3d-front: 3d furnished rooms with\nlayouts and semantics. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 10933\u2013\n10942, 2021. 3\n[13] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca,\nMartin Schrimpf, James Traer, Julian De Freitas, Jonas Kubil-\nius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld:\nA platform for interactive multi-modal physical simulation.\narXiv preprint arXiv:2007.04954, 2020. 2\n[14] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided distil-\nlation from 3d-aware diffusion. In International Conference\non Machine Learning, pages 11808\u201311826. PMLR, 2023. 3\n[15] Gurobi Optimization, LLC. Gurobi Optimizer Reference\nManual, 2023. 13\n[16] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping\nplato\u2019s cave: 3d shape from adversarial rendering. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 9984\u20139993, 2019. 3\n[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation metric\nfor image captioning. arXiv preprint arXiv:2104.08718, 2021.\n6\n[18] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner.\nText2room: Extracting textured\n3d meshes from 2d text-to-image models. arXiv preprint\narXiv:2303.11989, 2023. 2, 3\n[19] Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver Van Kaick,\nHao Zhang, and Hui Huang. Graph2plan: Learning floor-\nplan generation from layout graphs. ACM Transactions on\nGraphics (TOG), 39(4):118\u20131, 2020. 2\n[20] Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas\nGuibas.\nAladdin: Zero-shot hallucination of stylized 3d\nassets from abstract scene descriptions.\narXiv preprint\narXiv:2306.06212, 2023. 3\n[21] J. D. Hunter. Matplotlib: A 2d graphics environment. Com-\nputing in Science & Engineering, 9(3):90\u201395, 2007. 12\n[22] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal\nShankar, Hongseok Namkoong, John Miller, Hannaneh Ha-\njishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021.\nIf you use this software, please cite it as below. 6, 12\n[23] Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexan-\nder Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, S. Cher-\nnova, and Dhruv Batra. Sim2real predictivity: Does evalu-\nation in simulation predict real-world performance? IEEE\nRobotics and Automation Letters, 5:6670\u20136677, 2019. 2\n[24] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and\nAniruddha Kembhavi. Simple but effective: Clip embeddings\n9\nfor embodied ai. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n8\n[25] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,\nLuca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Ab-\nhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D\nEnvironment for Visual AI. arXiv, 2017. 2, 3, 6, 8\n[26] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen,\nSanjana Srivastava, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Chen Wang,\nGabrael Levine, Michael Lingelbach, Jiankai Sun, et al.\nBehavior-1k: A benchmark for embodied ai with 1,000 ev-\neryday activities and realistic simulation. In Conference on\nRobot Learning, pages 80\u201393. PMLR, 2023. 2\n[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-\nto-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 3\n[28] Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong\nLin, Hui Xiong, and Lin Wang. Towards language-guided\ninteractive 3d generation: Llms as layout interpreter with\ngenerative feedback. arXiv preprint arXiv:2305.15808, 2023.\n3\n[29] Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, S\u00a8oren\nPirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas\nGuibas, and Hao Zhang. Language-driven synthesis of 3d\nscenes from scene databases. ACM Transactions on Graphics\n(TOG), 37(6):1\u201316, 2018. 2\n[30] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 3\n[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n3\n[32] George A Miller. Wordnet: a lexical database for english.\nCommunications of the ACM, 38(11):39\u201341, 1995. 14\n[33] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. Hologan: Unsupervised\nlearning of 3d representations from natural images. In Pro-\nceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 7588\u20137597, 2019. 3\n[34] OpenAI. GPT-4V(ision) System Card, 2023. 6\n[35] R OpenAI. Gpt-4 technical report. arXiv, pages 2303\u201308774,\n2023. 2\n[36] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten\nKreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres-\nsive transformers for indoor scene synthesis. Advances in\nNeural Information Processing Systems, 34:12013\u201312026,\n2021. 2\n[37] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 3\n[38] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu\nWang, Sanja Fidler, and Antonio Torralba.\nVirtualhome:\nSimulating household activities via programs. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8494\u20138502, 2018. 2\n[39] Ariadna Quattoni and Antonio Torralba. Recognizing indoor\nscenes. In 2009 IEEE conference on computer vision and\npattern recognition, pages 413\u2013420. IEEE, 2009. 7, 14, 17\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 5, 12\n[41] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-\njmans, Oleksandr Maksymets, Alexander Clegg, John M\nTurner, Eric Undersander, Wojciech Galuba, Andrew West-\nbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv\nBatra. Habitat-matterport 3d dataset (HM3d): 1000 large-\nscale 3d environments for embodied AI. In Thirty-fifth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2021. 2, 8\n[42] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence\nembeddings using siamese bert-networks. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational Linguistics,\n2019. 5, 12\n[43] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu,\nVladlen Koltun, Jitendra Malik, et al. Habitat: A platform\nfor embodied ai research. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 9339\u2013\n9347, 2019. 2\n[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. LAION-5b: An open large-scale dataset for training\nnext generation image-text models. In Thirty-sixth Confer-\nence on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022. 6, 12\n[45] Mohammad Amin Shabani, Sepidehsadat Hosseini, and Yasu-\ntaka Furukawa. Housediffusion: Vector floorplan generation\nvia a diffusion model with discrete and continuous denoising.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5466\u20135475, 2023. 2\n[46] Andrew Szot, Alexander Clegg, Eric Undersander, Erik\nWijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa\nMukadam, Devendra Singh Chaplot, Oleksandr Maksymets,\net al. Habitat 2.0: Training home assistants to rearrange their\nhabitat. Advances in Neural Information Processing Systems,\n34:251\u2013266, 2021. 2\n[47] Fuwen Tan, Song Feng, and Vicente Ordonez. Text2scene:\nGenerating compositional scenes from textual descriptions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6710\u20136719, 2019. 2\n10\n[48] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Jus-\ntus Thies, and Matthias Nie\u00dfner. Diffuscene: Scene graph\ndenoising diffusion probabilistic model for generative indoor\nscene synthesis. arXiv preprint arXiv:2303.14207, 2023.\n[49] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner.\nSceneformer: Indoor scene generation with transformers. In\n2021 International Conference on 3D Vision (3DV), pages\n106\u2013115. IEEE, 2021. 2\n[50] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian\nWang, Katerina Fragkiadaki, Zackory Erickson, David Held,\nand Chuang Gan. Robogen: Towards unleashing infinite data\nfor automated robot learning via generative simulation. arXiv\npreprint arXiv:2311.01455, 2023. 2\n[51] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models. arXiv preprint arXiv:2206.07682,\n2022. 2\n[52] Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul\nSajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas\nGuibas. Lego-net: Learning regular rearrangements of ob-\njects in rooms. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 19037\u2013\n19047, 2023. 2\n[53] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of ob-\nject shapes via 3d generative-adversarial modeling. Advances\nin neural information processing systems, 29, 2016. 3\n[54] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\nMalik, and Silvio Savarese. Gibson env: Real-world per-\nception for embodied agents. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n9068\u20139079, 2018. 2\n[55] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. In Proceedings\nof the IEEE/CVF international conference on computer vision,\npages 4541\u20134550, 2019. 3\n[56] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing\nLiao. Text2nerf: Text-driven 3d scene generation with neural\nradiance fields. arXiv preprint arXiv:2305.11588, 2023. 2, 3\n[57] Yiqun Zhao, Zibo Zhao, Jing Li, Sixun Dong, and Shenghua\nGao.\nRoomdesigner: Encoding anchor-latents for style-\nconsistent and shape-compatible indoor scene generation.\narXiv preprint arXiv:2310.10027, 2023. 2\n[58] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 5826\u20135835, 2021. 3\n11\nA. Details of HOLODECK\nA.1. Efficiency and Cost\nTo create an interactive house of k rooms, HOLODECK uses\n3+3\u00d7k API calls. More specifically, utilizing OpenAI\u2019s gpt-\n4-1106-preview model incurs an approximate cost of $ 0.2\nper room. With our current implementation, HOLODECK can\ngenerate a single room in about 3 minutes. This includes the\ntime for API calls and layout optimization using a MacBook\nequipped with an M1 chip.\nA.2. Floor & Wall Modules\nIn the LLM outputs in the Floor Module, the following\ndetails are provided for each room:\n\u2022 room type: the room\u2019s name, e.g., kitchen, bedroom.\n\u2022 floor material: a description of the floor\u2019s appearance.\n\u2022 wall material: a description of the wall\u2019s appearance.\n\u2022 vertices: four tuples {(xi, yi), i \u2208 [1, 2, 3, 4]}, represent-\ning the coordinates of the room\u2019s corners.\nMaterial Selection. We have an image representation for\neach of 236 materials, consistent with the material setup\nin PROCTHOR [7]9. Using CLIP10 [40], we calculate the\nsimilarity between the material descriptions provided by\nthe Large Language Model (LLM) and these images. The\nmaterial with the highest similarity score is selected. Addi-\ntionally, we utilize the 148 colors from Matplotlib [21] to\nrefine the material selection by choosing the color closest to\nthe description with CLIP.\nWall height. We have the LLM suggest a suitable wall height\nbased on the user\u2019s input. For example, it may recommend a\nhigh ceiling for commercial spaces like museums.\nA.3. Doorway & Window Modules\nIn HOLODECK, we take advantage of the diverse collection\nof doors and windows introduced in PROCTHOR [7], fea-\nturing a diverse collection of 40 doors (refer to examples in\nFigure 13) and 21 windows (see Figure 14). The LLM pro-\nvides essential information to aid in the selection of doors:\n\u2022 room 1 & room 2: the two rooms connected by the door,\nfor example, bedroom and kitchen.\n\u2022 connection type: one of the three connection types: door-\nframe (frame without a door), doorway (frame with a\ndoor), and open (no wall separating the rooms).\n\u2022 size: the size of the door: single (one meter in width) or\ndouble (two meters in width).\n\u2022 door style: a description of the door\u2019s appearance.\nWe have an image for each door, and we utilize CLIP to\nselect the door that most closely matches the description.\nWe have the LLM provide the following data about windows:\n9Procthor splits the set of materials into wall and floor materials. For\nHOLODECK, we merge them in one pool for retrieval.\n10We employ OpenCLIP [22] with ViT-L/14, trained on the LAION-2B\ndataset [44], for all CLIP-related components in this paper.\ndoorway\ndoorframe\nsingle\ndouble\nFigure 13. Examples of different doors in HOLODECK.\nfixed\nslider\nhung\nFigure 14. Examples of different windows in HOLODECK.\n\u2022 room type: the room where the window will be installed.\n\u2022 direction: the wall\u2019s direction (south, north, east, or west\n) where the window will be placed.\n\u2022 type: one of the three window types: fixed, slider or hung.\n\u2022 size: the width and height of the window.\n\u2022 quantity: the number of windows installed on each wall.\n\u2022 height: the distance from the floor to the window\u2019s base.\nA.4. Object Selection Module\nIn Objaverse, each 3D asset o \u2208 O is associated with the\nfollowing metadata - a textual description of the asset t, the\n3D bounding box size of the asset (w, d, h), and a set of 2D\nimages I captured from three different angles (0\u00b0, 45\u00b0, and\n\u221245\u00b0). For each object proposed by LLM o\u2032, we have the\nLLM output a detailed description of the object (t\u2032) and its\n3D bounding box size (w\u2032, d\u2032, h\u2032) for retrieval purposes. To\nevaluate the similarity between a candidate 3D asset in the\nrepository o =\n\u0000t, (w, d, h) , I\n\u0001\nand the object proposed by\nthe LLM o\u2032 \u0010\nt\u2032,\n\u0000w\u2032, d\u2032, h\u2032\u0001\u0011\n, we use three metrics:\n\u2022 Visual Similarity (V) measures the CLIP similarity be-\ntween the 2D renderings of the candidate asset and the tex-\ntual description of the LLM-proposed object: V(o, o\u2032) =\nmaxi\u2208I CLIP(i, t\u2032).\n\u2022 Textual Similarity (T ) measures the similarity between\nthe textual description of the candidate 3D asset and the\ntextural description of the LLM-proposed object. This\nmetric is crucial in improving the accuracy of the retrieval\nprocess since it ensures that we retrieve the asset within\nthe correct category. We use the sentence transformer\n(SBERT) [42] with all-mpnet-base-v2 checkpoint to cal-\nculate the scores: T = SBERT(t, t\u2032).\n\u2022 Size Discrepancy (S) measures the discrepancy in the\nsize of the 3D bounding box size of the candidate as-\n12\nPlace the anchor object sofa, at the edge of the room\nPlace the coffee table, in front of, face to, and near the sofa\nCreate grids in the room to\nreduce the search space.\nGet all valid placements by\nsatisfying the edge constraint.\nRandomly choose one placement\nfor the anchor object.\nChoose the optimal placement\nwhich satisfies most constraints.\nGet all valid placements by\nsatisfying hard constraints.\nsofa\nsofa\nsofa\ndoor\ndoor\ndoor\ndoor\ndoor\ncoffee table\nInitial State\nFigure 15. Example of using the DFS-based Constraint Satisfaction algorithm to place the objects.\nset and the LLM-proposed object.\nThere are similar\nobjects with different sizes in the asset repository, and\nthe size of objects is an important factor in designing\nscenes, e.g., we need a larger sofa for a large living\nroom. The size matching score is computed as: S(o, o\u2032) =\n\u0000|w \u2212 w\u2032| + |h \u2212 h\u2032| + |d \u2212 d\u2032|\n\u0001\n/3. Two objects of simi-\nlar size will have a smaller value of S.\nThe overall matching score M\n\u0000o, o\u2032\u0001\nis a weighted sum\nof the above metrics:\nM\n\u0000o, o\u2032\u0001\n= \u03b1 \u00b7 V\n\u0000o, o\u2032\u0001\n+ \u03b2 \u00b7 T (o, o\u2032) \u2212 \u03b3 \u00b7 S(o, o\u2032) (1)\nwith weights \u03b1 = 100, \u03b2 = 1, and \u03b3 = 10. The asset with\nthe highest matching score is selected.\nA.5. Layout Design Module\nIn this module, we position the set of objects O chosen in\nSec A.4, applying spatial relational constraints provided by\nthe LLM. We define various constraints for floor objects:\n\u2022 Global constraint: edge; middle.\n\u2022 Distance constraint: near (object); far (object).\n\u2022 Position constraint: in front of (object); side of (object).\n\u2022 Alignment constraint: center align with (object).\n\u2022 Direction constraint: face to (object).\nThe LLM can combine these constraints to form a con-\nstraint list Co for each object o \u2208 O. For instance, as shown\nin Figure 15, the constraints for a \u201ccoffee table\u201d are [middle,\nin front of (sofa), face to (sofa), near (sofa)].\nFor floor object placement, we employ two solvers:\nDepth-First-Search (DFS) Solver and Mixed Integer Lin-\near Programming (MILP) [2] Solver.\nDepth-First-Search Solver. In the DFS solver, each object\nis defined by five variables (x, y, w, d, rotation). (x, y) is the\n2D coordinates of the object\u2019s center, w and d are the width\nand depth of the 2D bounding box of the object, and rota-\ntion can be one of 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0. The constraints\nlisted above are treated softly, allowing certain violations\nwhen finding a layout. Beyond these soft constraints, we\nimplement hard constraints essential for object placement:\nthese constraints prevent object collisions and ensure that\nobjects remain within the designated room boundaries. Vio-\nlation of these hard constraints results in the object not being\nplaced. Figure 15 demonstrates that our DFS solver initiates\ngrids to establish a finite search space. It first explores differ-\nent placements for the anchor object selected by the LLM.\nSubsequent steps involve optimizing the placement for the\nremaining objects, adhering to the hard constraints, and sat-\nisfying as many soft constraints as possible.11 The algorithm\ncan yield multiple solutions, with the final selection meeting\nthe most constraints.\nMixed Integer Linear Programming (MILP) Solver is par-\nticularly effective for structured layout design. It optimizes\na linear objective function subject to linear constraints with\nsome non-discrete variables. This approach is well-suited\nfor our layout optimization problem in HOLODECK.\nIn our MILP formulation, each object\u2019s position is deter-\nmined by four variables: (x, y, rotate90, rotate180). The vari-\nables rotate90 and rotate180 are boolean, indicating rotations\nof 90 and 180 degrees, respectively. For example, if rotate90\nand rotate180 are both true, it signifies a 270-degree rotation\nof the object. We translate all previously mentioned con-\nstraints into linear ones for the MILP problem. For instance,\nto align Object A with Object B at the center, a constraint in\nthe form of Ax = Bx or Ay = By is implemented, where\nAx, Ay and Bx, By represent the centers of Objects A and B,\nrespectively. Note that the constraint is non-linear due to the\nOR operator. To model this linearly in MILP, we can intro-\nduce binary auxiliary variables and additional constraints to\ncapture the logic of the OR condition. For solving the MILP,\nwe utilize GUROBI [15], a state-of-the-art solver known for\nits efficiency and robustness.\nIn MILP solver, all constraints specified in the previous\nsection are applied as hard constraints except that the Dis-\ntance constraints (near and far) are uniquely modeled as part\nof the objective. For a visual comparison of these solvers\u2019\noutcomes in HOLODECK, refer to Figure 21.\nWall & Small Objects. The placement of wall objects is\ndetermined by two specific attributes:\n\u2022 Above (Floor Object): This denotes the floor object di-\n11The evaluation of an object\u2019s placement is based on the number of\nconstraints satisfied. Placements that satisfy a greater number of constraints\nreceive higher weights. However, any placement that violates hard con-\nstraints is rejected.\n13\nAsset ID: f1440a39fdab4f5282ad4a37fdcaa8c5\n180\u00b0\n270\u00b0\n90\u00b0\n0\u00b0\nSynset\ngame_console.n.01\nCategory\nvideo game console\nWidth\n10.2 cm\nLength\n23.9 cm\nHeight\n1.4 cm\nVolume\n341.16 cm^3\nMass\n0.4 kg\nFront View\n180\u00b0\nMaterials\nplastic\nmetal\nglass\nONFLOOR\nFalse\nONOBJECT\nTrue\nONWALL\nFalse\nONCEILING\nFalse\nDescription\nThis is a Nintendo Switch, a popular hybrid video game console \nthat can be used both as a stationary and portable device. It has \njoy-con controllers attached on either side of the screen, one in \nblue and the other in red.\nFigure 16. Example of an asset\u2019s attributes annotated by GPT-4-V.\nrectly underneath the wall object.\n\u2022 Height: Specifies the exact distance from the floor to the\nbase of the wall object, measured in centimeters.\nTo place small surface objects on top of larger ob-\njects, we first have LLM propose the placements and uti-\nlizeRandomSpawn12 function in AI2-THOR. This method\nallows for randomized and efficient positioning of small\nobjects on larger surfaces.\nA.6. GPT-4-V for 3D Asset Annotation\nWe annotate the 3D assets used in HOLODECK with Ope-\nnAI\u2019s GPT-4-V API to enhance the accuracy of object re-\ntrieval and placement. As illustrated in Figure 16, GPT-4-V\ntakes a set of four images as inputs, each showing an ob-\nject from orthogonal rotations (0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0) and\noutputs the following attributes for the 3D object:\n\u2022 Category: a specific classification of the object, such as\n\u201cchair\u201d, \u201ctable\u201d, \u201cbuilding\u201d, etc.\n\u2022 Synset: the nearest WordNet [32] synset will be used as\nthe object type in object navigation tasks.\n\u2022 Width, Length, Height: physical dimensions in centime-\nters, defining the object\u2019s bounding box sizes.\n\u2022 Volume: approximate volume in cubic centimeters (cm3).\n\u2022 Mass: estimated object mass in kilograms (kg).\n\u2022 Front View: an integer denoting the view representing the\nfront of the object, often the most symmetrical view.\n\u2022 Description: a detailed textual description of the object.\n\u2022 Materials: a list of materials constituting the object.\n\u2022 Placement Attributes: Boolean values (ONCEILING, ON-\nWALL, ONFLOOR, ONOBJECT) indicating typical place-\nment locations. For example, \u201cTrue\u201d for a ceiling fan\u2019s\nplacement on the ceiling.\n12AI2-THOR RandomSpawn Documentation\nA.7. Importing Objaverse Assets into AI2-THOR\nThe transformation of Objaverse assets into interactive ob-\njects in AI2-THOR involves a complex, multi-step pipeline.\nInitially, the process starts with downloading and con-\nverting various 3D models into a mesh format optimized\nfor runtime loading. We then generate visibility points on\nthe mesh surface, enabling AI2-THOR to determine object\nvisibility. This is followed by 3D decomposition, where the\nmesh is split into simpler convex meshes to facilitate rapid\nand realistic collision detection. The final step involves com-\npressing textures (i.e., albedo, normal, and emission) and the\nmodel format to streamline performance.\nHandling many assets in numerous scenes is challenging,\nmainly due to the large mesh counts of Objaverse assets\nand the traditional compile-time asset packaging approach\nof game engines like Unity. To address this, we implement\ncaching layers for objects, reducing the loading time for\nrepeated use in different scenes. Additionally, we develop\na system to unload objects from memory, allowing efficient\nmanagement of thousands of 3D objects at runtime.\nB. Prompt\nThe complete prompt templates of HOLODECK\u2019s modules\nare provided in Figure 17 and 18. The prompt for annotating\n3D assets using GPT-4-V is shown in Figure 19.\nC. Qualitative Examples\nIn Figure 20, we showcase an additional 20 scenes generated\nby HOLODECK. These 20 scene types are chosen from the\nMIT dataset [39], distinct from examples in the main paper.\nFigure 21 presents a comparative analysis of layouts created\nby five methods. Figure 22 offers a visual comparison of res-\nidential scenes from iTHOR, PROCTHOR, and HOLODECK,\nhighlighting the differences and capabilities of each system.\nD. NOVELTYTHOR\nNOVELTYTHOR comprises human-designed scenes crafted\nto challenge embodied agents in unique and diverse environ-\nments with a wide array of assets from Objaverse.\nTo integrate Objaverse assets into Unity, we developed\ntools that run a conversion pipeline on various operating sys-\ntems, including macOS and Windows. This flexibility also\nenables the inclusion of assets beyond those found in Obja-\nverse. A user-friendly interface was designed for our artists\nand designers, facilitating asynchronous asset integration\nwhile optimizing storage efficiency.\nThe critical step of this process is the generation of Unity\ntemplates (prefabs) for the assets and their associated re-\nsources, leading to the creation of the scenes discussed in\nthis paper. Figures 23 and 24 showcase top-down views of\nthe 10 NOVELTYTHOR scenes, spanning five categories.\n14\nFloor plan Prompt: You are an experienced room designer. Please assist me in crafting a floor plan. Each room is a\nrectangle. You need to define the four coordinates and specify an appropriate design scheme, including each room\u2019s\ncolor, material, and texture.\nAssume the wall thickness is zero.\nPlease ensure that all rooms are connected, not\noverlapped, and do not contain each other. The output should be in the following format: room name | floor material |\nwall material | vertices (coordinates). Note: the units for the coordinates are meters.\nFor example:\nliving room | maple hardwood, matte | light grey drywall, smooth | [(0, 0), (0, 8), (5, 8), (5, 0)]\nkitchen | white hex tile, glossy | light grey drywall, smooth | [(5, 0), (5, 5), (8, 5), (8, 0)]\nHere are some guidelines for you:\n1. A room\u2019s size range (length or width) is 3m to 8m. The maximum area of a room is 48 m2. Please provide a floor\nplan within this range and ensure the room is not too small or too large.\n2. It is okay to have one room in the floor plan if you think it is reasonable.\n3. The room name should be unique.\nNow, I need a design for {input}.\nAdditional requirements: {additional requirements}.\nYour response should be direct and without additional text at the beginning or end.\nWall Height Prompt: I am now designing {input}. Please help me decide the wall height in meters. Answer with a number,\nfor example, 3.0. Do not add additional text at the beginning or in the end.\nDoorway Prompt: I need assistance in designing the connections between rooms. The connections could be of three types:\ndoorframe (no door installed), doorway (with a door), or open (no wall separating rooms). The sizes available for\ndoorframes and doorways are single (1m wide) and double (2m wide).\nEnsure that the door style complements the design of the room.\nThe output format should be:\nroom 1 | room 2 |\nconnection type | size | door style. For example:\nexterior | living room | doorway | double | dark brown metal door\nliving room | kitchen | open | N/A | N/A\nliving room | bedroom | doorway | single | wooden door with white frames\nThe design under consideration is {input}, which includes these rooms: {rooms}.\nThe length, width, and height of each room in meters are: {room sizes}\nCertain pairs of rooms share a wall: {room pairs}. There must be a door to the exterior.\nAdhere to these additional requirements {additional requirements}.\nProvide your response succinctly, without additional text at the beginning or end.\nWindow Prompt: Guide me in designing the windows for each room. The window types are: fixed, hung, and slider.\nThe available sizes (width x height in cm) are:\nfixed: (92, 120), (150, 92), (150, 120), (150, 180), (240, 120), (240, 180)\nhung: (87, 160), (96, 91), (120, 160), (130, 67), (130, 87), (130, 130)\nslider: (91, 92), (120, 61), (120, 91), (120, 120), (150, 92), (150, 120)\nYour task is to determine the appropriate type, size, and quantity of windows for each room, bearing in mind the room\u2019s\ndesign, dimensions, and function.\nPlease format your suggestions as follows: room | wall direction | window type | size | quantity | window base height\n(cm from floor). For example: living room | west | fixed | (130, 130) | 1 | 50\nI am now designing {input}. The wall height is {wall height} cm.\nThe walls available for window installation (direction, width in cm) in each room are: {walls}\nPlease note: It is not mandatory to install windows on every available wall. Within the same room, all windows must\nbe the same type and size. Also, adhere to these additional requirements: {additional requirements}.\nProvide a concise response, omitting any additional text at the beginning or end.\nFigure 17. Prompt templates for Floor Module, Wall Module, Doorway Module, and Window Module.\n15\nObject Selection Prompt: You are an experienced room designer, please assist me in selecting large *floor*/*wall*\nobjects and small objects on top of them to furnish the room.\nYou need to select appropriate objects to satisfy\nthe customer\u2019s requirements. You must provide a description and desired size for each object since I will use it to\nretrieve objects. If multiple identical items are to be placed in the room, please indicate the quantity and variance\ntype (same or varied). Present your recommendations in JSON format:\n{ object name:{\n\"description\": a short sentence describing the object,\n\"location\": \"floor\" or \"wall\",\n\"size\":\nthe desired size of the object, in the format of a list of three numbers, [length, width, height] in\ncentimeters,\n\"quantity\": the number of objects (int),\n\"variance type\": \"same\" or \"varied\",\n\"objects on top\": a list of small children objects (can be empty) which are placed *on top of* this object. For each\nchild object, you only need to provide the object name, quantity and variance type.\nFor example, {\"object name\":\n\"book\", \"quantity\": 2, \"variance type\": \"varied\"} } }\n*ONE-SHOT EXAMPLE*\nCurrently, the design in progress is *INPUT*, and we are working on the *ROOM TYPE* with the size of ROOM SIZE. Please\nalso consider the following additional requirements: REQUIREMENTS.\nHere are some guidelines for you:\n1. Provide reasonable type/style/quantity of objects for each room based on the room size to make the room not too\ncrowded or empty.\n2. Do not provide rug/mat, windows, doors, curtains, and ceiling objects which have been installed for each room.\n3. I want at least 10 types of large objects and more types of small objects on top of the large objects to make the\nroom look more vivid.\nPlease first use natural language to explain your high-level design strategy for *ROOM TYPE*, and then follow the\ndesired JSON format *strictly* (do not add any additional text at the beginning or end).\nLayout Design Prompt: You are an experienced room designer. Please help me arrange objects in the room by assigning\nconstraints to each object. Here are the constraints and their definitions:\n1. global constraint:\n1) edge: at the edge of the room, close to the wall, most of the objects are placed here.\n2) middle: not close to the edge of the room.\n2. distance constraint:\n1) near, object: near to the other object, but with some distanbce, 50cm < distance < 150cm.\n2) far, object: far away from the other object, distance >= 150cm.\n3. position constraint:\n1) in front of, object: in front of another object.\n2) side of, object: on the side (left or right) of another object.\n4. alignment constraint: 1) center aligned, object: align the center of the object with the center of another object.\n5. Rotation constraint: 1) face to, object: face to the center of another object.\nFor each object, you must have one global constraint and you can select various numbers of constraints and any\ncombinations of them and the output format must be: object | global constraint | constraint 1 | constraint 2 | ...\nFor example: sofa-0 | edge\ncoffee table-0 | middle | near, sofa-0 | in front of, sofa-0 | center aligned, sofa-0 | face to, sofa-0\ntv stand-0 | edge | far, coffee table-0 | in front of, coffee table-0 | center aligned, coffee table-0 | face to,\ncoffee table-0\nHere are some guidelines for you:\n1. I will use your guideline to arrange the objects *iteratively*, so please start with an anchor object which doesn\u2019t\ndepend on the other objects (with only one global constraint).\n2. Place the larger objects first.\n3. The latter objects could only depend on the former objects.\n4. The objects of the *same type* are usually *aligned*.\n5. I prefer objects to be placed at the edge (the most important constraint) of the room if possible which makes the\nroom look more spacious.\n6. Chairs must be placed near to the table/desk and face to the table/desk.\nNow I want you to design {room type} and the room size is {room size}.\nHere are the objects that I want to place in the {room type}: {objects}\nPlease first use natural language to explain your high-level design strategy, and then follow the desired format\n*strictly* (do not add any additional text at the beginning or end) to provide the constraints for each object.\nFigure 18. Prompt templates for Object Selection Module and Layout Design Module.\n16\n3D Asset annotation Prompt: Please annotate this 3D asset with the following values (output valid JSON):\n\"annotations\": {\n\"category\": a category such as \"chair\", \"table\", \"building\", \"person\", \"airplane\", \"car\", \"seashell\", \"fish\", etc.\nTry to be more specific than \"furniture\",\n\"synset\": the synset of the object that is most closely related. This could be \"cat.n.01\", \"glass.n.03\", \"bank.n.02\",\n\"width\": approximate width in cm. For a human being, this could be \"45\",\n\"length\": approximate length in cm. For a human being, this could be \"25\",\n\"height\": approximate height in cm. For a human being, this could be \"182\",\n\"volume\": approximate volume in cm3. For a human being, this could be \"62000\",\n\"mass\": approximate mass in kilogram. For a human being, this could be \"72\",\n\"frontView\": which of the views represents the front of the object (value should be an integer with the first image\nbeing 0). Note that the front view of an object, including furniture, tends to be the view that exhibits the highest\ndegree of symmetry,\n\"description\": a description of the object (don\u2019t use the term \"3D asset\" here),\n\"materials\": a Python list of the materials that the object appears to be made of (roughly in order of most used\nmaterial to least used),\n\"onCeiling\": whether this object can appear on the ceiling or not, return true or false with no explanations. This\nwould be true for a ceiling fan but false for a chair,\n\"onWall\": whether this object can appear on the wall or not, return true or false with no explanations. This would be\ntrue for a painting but false for a table,\n\"onFloor\": whether this object can appear on the floor or not, return true or false with no explanations. This would\nbe true for a piano but false for a curtain,\n\"onObject\": whether this object can appear on another object or not, return true or false with no explanations. This\nwould be true for a laptop but not for a sofa }\nPlease output the JSON now.\nFigure 19. Prompt template for annotating 3D assets with GPT-4-V.\nart studio\nlocker room\ncorridor\nhair salon\npantry\nretro kitchen\ncasino\ndental office\nshoe shop\njewelry shop\nrestaurant\ntoy shop\nchildren room\nhospital room\ncomputer room\nclothing store\nwaiting room\ncloset\ngreenhouse\ngrocery store\nFigure 20. Additional examples of different scene types from MIT Scenes [39].\n17\nBATHROOM\nBEDROOM\nKITCHEN\nLIVING ROOM\nCONSTRAINT \n(DFS)\nCONSTRAINT \n(MILP)\nABSOLUTE\nRANDOM\nEDGE\nFigure 21. Qualitative comparison of the different layout methods.\n18\nHOLODECK\nPROCTHOR\nITHOR\nBATHROOM\nBEDROOM\nKITCHEN\nLIVINGROOM\nFigure 22. Qualitative examples on four types of residential scenes from iTHOR, PROCTHOR, and HOLODECK.\nGym 01\nGym 02\nFigure 23. Top-down view of NOVELTYTHOR. Each scene type has two instances.\n19\nMusic Room 02\nDaycare 01\nMusic Room 01\nDaycare 02\nOffice 01\nOffice 02\nArcade 01\nArcade 02\nFigure 24. Top-down view of NOVELTYTHOR (continued).\n20\n"
  },
  {
    "title": "SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance",
    "link": "https://arxiv.org/pdf/2312.08889.pdf",
    "upvote": "10",
    "text": "SEEAvatar: Photorealistic Text-to-3D Avatar Generation\nwith Constrained Geometry and Appearance\nYuanyou Xu\nZongxin Yang\nYi Yang\nReLER, CCAI, Zhejiang University\nyoxu@zju.edu.cn, zongxinyang1996@gmail.com, yangyics@zju.edu.cn\nRealistic shading\nDetailed face\nDetailed hand\nNormal\nNormal\nNormal\nJoker and Batman fighting on a \ncar in the city night.\nNewton and Einstein conducting \nscientific experiments in a church.\nA Chinese woman and a monk\npracticing kung fu on the lake.\nFigure 1. We present three cases to demonstrate the application of the generated avatars in classic graphics workflows. In above scenes,\nthe avatars are generated by our method and represented as meshes and textures, and exported to Blender for posing and rendering. The\nmeshes have decent body shapes and highly detailed structures in hands and face. The textures have correct colors and rich local details.\nWith the high quality geometry and texture assets, photorealistic portraits can be created. Additional 3D assets are used to build the scenes.\nAbstract\nPowered by large-scale text-to-image generation models,\ntext-to-3D avatar generation has made promising progress.\nHowever, most methods fail to produce photorealistic re-\nsults, limited by imprecise geometry and low-quality ap-\npearance. Towards more practical avatar generation, we\npresent SEEAvatar, a method for generating photorealistic\n3D avatars from text with SElf-Evolving constraints for de-\ncoupled geometry and appearance. For geometry, we pro-\npose to constrain the optimized avatar in a decent global\nshape with a template avatar. The template avatar is initial-\nized with human prior and can be updated by the optimized\navatar periodically as an evolving template, which enables\nmore flexible shape generation. Besides, the geometry is\nalso constrained by the static human prior in local parts like\nface and hands to maintain the delicate structures. For ap-\npearance generation, we use diffusion model enhanced by\nprompt engineering to guide a physically based rendering\npipeline to generate realistic textures. The lightness con-\nstraint is applied on the albedo texture to suppress incorrect\nlighting effect. Experiments show that our method outper-\nforms previous methods on both global and local geome-\ntry and appearance quality by a large margin. Since our\nmethod can produce high-quality meshes and textures, such\nassets can be directly applied in classic graphics pipeline\nfor realistic rendering under any lighting condition. Project\npage at: https://yoxu515.github.io/SEEAvatar/.\n1. Introduction\nWith the development of computer vision and computer\ngraphics techniques [20, 26, 33, 40, 45], creating and seeing\na world from fantasy have been made possible. Although\nthe film industry and gaming industry have developed ma-\nture workflows to create ideal visual contents and effects\nfrom 3D assets, producing photorealistic 3D assets often re-\nquires expensive scanning machines and intensive work by\nprofessional artists. Easier solutions for generating photore-\nalistic 3D assets are meaningful in applications like virtual\nreality, game and film production.\nTherefore, we are committed to constructing a frame-\nwork for creating photorealistic 3D avatars from text. How-\never, such a task is challenging because high realism re-\nquires both high resolution and high quality geometry and\n1\narXiv:2312.08889v2  [cs.CV]  26 Dec 2023\nAlbus Dumbledore\nJack Sparrow\nDoctor Strange\nBarack Obama\nKobe Bryant\nElsa in Frozen\nHermione Granger\nWonder Woman\nBruce Lee\nHillary Clinton\nFigure 2. Generated avatars from text prompts. Full body colors and normals are rendered in the front view, and faces are rendered in a\ncloser distance. Please zoom in for better view.\nappearance. For geometry, 1) decent body proportions and\ndiverse clothes styles should be correctly generated. For ex-\nample, an avatar with a large head and a small body will\nlook unnatural [23, 51]. 2) The detailed shape structures\nin hands, face and clothes should be delicately articulated.\nMissing hands or facial features is a severe detriment to re-\nalism [6, 16]. For appearance, 1) correct color style and\nhigh resolution details are necessary for visually pleasing\nresults. Over-saturated colors [6, 14] or low resolution de-\ntails [21] will result in unrealistic results. 2) In order to\nachieve photorealistic results under any light conditions, ex-\nplicit meshes and textures are needed for physically based\nrendering (PBR) in classic graphics pipeline. Avatars rep-\nresented in neural fields with entangled color and geometry\n[28] are hard to be applied into classic tools and workflows\nfor further application like relighting [6, 16, 21]. A recent\nwork Fantasia3D [8] for 3D content generation decouples\ngeometry and appearance. However, it fails to generate high\nquality 3D avatars from text. The normal-based guidance\nfor geometry modeling lacks stability and fails to generate\navatars with fine human shapes and well-structured details.\nIn addition, the PBR pipeline guided by the diffusion model\nin appearance modeling often absorbs lighting into albedo\ntextures. These problems hinder Fantasia3D from produc-\ning favorable results for the avatar generation task.\nIn order to tackle above challenges, we present SEEA-\nvatar for photorealistic avatar generation from text.\nWe\nfollow the decoupled framework from [8], and further im-\nprove the geometry and appearance generation with SElf-\nEvolving constraints. For geometry generation, we set up\nconstraints for both global shape and local structures. More\nspecifically, we set two model avatars, one is the current\navatar and the other is the template avatar. The current\navatar is represented as DMTet [43], which is guided by 2D\ndiffusion model. The template avatar is responsible for con-\nstraining the current avatar in a decent human shape. We\n2\napply signed distance function (SDF) constraint for shape\ncontrolling and normal constraint for surface smoothing be-\ntween the template avatar and current avatar. The simplest\nway is to set the template avatar constantly as the human\nprior SMPL-X [25, 32]. However, this will impede gen-\nerating shapes which are different from the original prior,\nfor example, a woman in a dress (Fig. 7 (c)). To make the\ngeneration more flexible, we loosen the constraint by up-\ndating the template avatar periodically. This can enhance\nthe potential to generate more diverse shapes. Although the\ngeneration becomes flexible, delicate local structures like\nhands will also be easier to be ruined. Therefore, we further\nemploy the static human prior for local constraints. The\ncombination of the evolving template avatar and the static\nhuman prior can produce avatars with decent body propor-\ntion, flexible clothes style and delicate local structures.\nFor appearance generation, we first employ prompt en-\ngineering to enhance the diffusion model, which has been\nwidely used by the open community [1, 2] but was often\nignored by prior works. Proper positive prompt and nega-\ntive prompt can largely improve the appearance quality and\nenrich the details. Although the quality is improved, we\nfind the diffusion model tends to generate fancy lighting ef-\nfects and it would be hard for the model to decompose the\nlighting from the albedo. A key observation is that in early\nsteps, the generated albedo is close to pure colors (Fig. 4).\nSimilar to the geometry constraints, we also set an evolv-\ning template avatar for appearance generation. We constrain\nthe lightness of the current avatar\u2019s albedo by the template\nalbedo, which can suppress the incorrect light effects.\nIn summary, our method is able to generate photorealis-\ntic avatars under geometry and appearance constraints. Our\ncontributions are as following:\n\u2022 We incorporate the decoupled geometry and appearance\nframework for photorealistic avatar generation. The gen-\nerated avatars are represented by high quality meshes and\ntextures, which are friendly to classic graphics pipelines\nfor further applications.\n\u2022 For the geometry generation, we propose to constrain the\ncurrent avatar by an evolving template avatar. Both global\nand local constraints are applied on SDFs and normals,\nwhich enable us to generate avatars with decent global\nshapes and fine local structures.\n\u2022 For appearance generation, we enhance the diffusion\nmodel with prompt engineering. Besides, we propose to\nconstrain the lightness of current avatar\u2019s albedo by the\nevolving template, which produces high quality PBR tex-\ntures with less lighting involved.\n2. Related work\nText-to-3D generation A common way for 3D content\ngeneration methods is to guide 3D representations by 2D\nmodels trained on image and text pairs.\nEarly methods\nlike CLIP-Mesh [29], CLIP-forge [42] and DreamFields\n[18] use CLIP [35] as guidance. Later, diffusion models\n[12, 36, 38, 41, 46] have shown great potential in image gen-\neration. DreamFusion [34] propose to distill 2D diffusion\nmodels into 3D neural fields with score distillation sam-\npling (SDS). Score Jacobian Chaining (SJC) [47] follows\nthe similar idea with a different formulation. Magic3D [24]\nproposes a two stage method combing Nerf with mesh fine-\ntuning. ProlificDreamer [49] improves the generation qual-\nity by Variational Score Distillation (VSD). Fantasia3D [8]\ndecouples geometry and appearance and is able to generate\nexplicit meshes and textures. Generating textures on given\nmeshes has also been studied by some works [7, 37, 44].\nAlthough these methods can generate diverse objects, they\nare not delicately designed for avatar generation. We incor-\nporate the decoupled framework from [8], and specialize in\nphotorealistic geometry and appearance generation.\nText-to-3D avatar generation AvatarCLIP [13] uses CLIP\nloss to guide distance and color fields [48] for avatar gen-\neration. DreamAvatar [6] uses diffusion model as guidance\nand optimize dual space neural fields to realize pose con-\ntrol. AvatarCraft [19] propose implicit neural representa-\ntion with controllable shapes and poses. DreamWaltz [16]\nincorporates the pose conditioned ControlNet [55] to op-\ntimize an animatable avatar representation. DreamHuman\n[21] uses imGHUM [5] as human prior for pose control and\ngenerates human avatars with semantic zoom of multiple\nbody parts. AvatarFusion [14] generates avatars with sepa-\nrate body and clothes neural fields and renders by fusing the\nrepresentations. AvatarVerse [52] trains a dense pose based\nControlNet and generates avatars with progressive training.\nTADA [23] directly optimizes the displacement and color\nfor the human prior meshes, and achieves fully animatable\navatars. Some methods also focus on head [10, 54] or upper\nbody [53] generation. Most of these methods use entangled\ncolor and geometry neural fields, which makes them hard\nto be applied in classic graphics pipelines for realistic ren-\ndering under any lighting condition. In addition, volumtric\nrendering can be heavy in memory consumption, which pre-\nvents them from rendering high resolution images.\n3. Method\n3.1. Preliminary\nScore distillation sampling (SDS) SDS is proposed in\nDreamFusion as a loss for optimizing 3D representation by\na 2D diffusion model. A 2D image can be rendered from 3D\nrepresentation as x = g(\u03b8). The SDS loss minimizes the\ndifference between the predicted noise \u03f5\u03d5 with the added\nrandom noise \u03f5:\n\u2207\u03b8LSDS(\u03b8, x) = Et,\u03f5[w(t)(\u03f5\u03d5(zt; y, t) \u2212 \u03f5)\u2202x\n\u2202\u03b8 ]\n(1)\n3\nEvolving Template \n(\ud835\udc53!\"#, \ud835\udc27!\"#)\nHuman Prior (\ud835\udc53$, \ud835\udc27$)\nLocal \nConstraints\nEq. (4) (7)\n\u201cFull body of Joker\u201d\n\u201cFace of Joker\u201d\u2026\nGlobal \nConstraints\nEq. (5) (8)\nUpdate\nCurrent Avatar (DMTet \ud835\udf13%)\n\ud835\udcdb\ud835\udc12\ud835\udc03\ud835\udc12\n\ud835\udc27\ud835\udc28\ud835\udc2b\ud835\udc26\nEq. (3) (6)\nDiffusion \nModel\nAdd \nNoise\nInitialize\nMarching Tet.\nLightness \nConstraint\nEq. (15)\nCurrent Avatar (iNGP)\nEvolving Template\nFull-body or Part Shading\n\ud835\udcdb\ud835\udc12\ud835\udc03\ud835\udc12\n\ud835\udc1c\ud835\udc28\ud835\udc25\ud835\udc28\ud835\udc2b\nEq. (12)\nDiffusion \nModel\n(b) Appearance generation\nAdd \nNoise\nCurrent Albedo\nSample\n(a) Geometry generation\nRender\nTet. \nSubdiv.\nAppearance \nNetwork \ud835\udf13&\nUpdate Interval \ud835\udeff&\nCurrent Mesh\nInterval \ud835\udeff%\nAppearance \nTemplate \ud835\udf13&\u2032\n\ud835\udc58', \ud835\udc58(, \ud835\udc58)\nFull-body or Part Normal\nFull-body or Part Prompt\n\u201cFull body of Joker\u201d\n\u201cFace of Joker\u201d\u2026\nFull-body or Part Prompt\nTemplate Albedo\nFigure 3. Overview, consisting of geometry (\u00a73.2) and appearance (\u00a73.3) generation. For geometry generation, we use DMTet [43] as the\n3D shape representation, and optimize it by normal-based SDS loss. The global SDF and normal constraints are applied between current\navatar and the evolving template, and local constraints are from the static human prior. For appearance, we use iNGP [30] to represent\nthe PBR texture field. Albedo/roughness/normal are sampled from the appearance representation for shading and optimized by color SDS\nloss. The lightness constraint is applied between the current albedo and the template albedo to suppress lighting effects.\nwhere zt is the noised image, t is the time step, y is the text\ncondition, and w(t) is a weighting function determined by\nthe time step.\nSMPL-X SMPL is proposed in [25] as a parametric human\nmodel. SMPL-X [32] is an extension of SMPL by integrat-\ning head [22] and hand [39] models. SMPL-X uses stan-\ndard vertex-based linear blend skinning with blend shapes,\nparameterized by shape, pose and facial expression param-\neters. We use the semantic label of the vertices to 1) extract\nbody parts like face or hands for our local constraints, 2)\nlocalize camera poses for local part render.\nDMTet Deep Marching Tetrahedra [9, 43] is proposed for\nhigh resolution 3D shape synthesis, as a hybrid represen-\ntation of a tetrahedral grid and an implicit sign distance\nfunction (SDF). The SDF can be efficiently represented by\nmulti-resolution hash encoding [30] with a MLP network.\nExplicit mesh can be extracted from the tetrahedral grid by\nMarching Tetrahedra (MT) in a differentiable manner.\n3.2. Geometry generation\n3.2.1\nHuman prior optimization\nGiven a text description y of the target avatar, we start from\noptimizing the shape of the human prior. The mesh can\nbe derived from SMPL-X with shape parameters \u03b2. The\nnormal image is obtained by differentiable rendering. We\nguide the optimization of \u03b2 by the diffusion model by SDS\nloss. In this stage, the basic body shape of the avatar is\ndetermined, for example, fat or thin. After optimizing shape\nparameters, the human prior is used for the following stages.\n3.2.2\nInitialization\nWe set two avatars during optimization: one is the current\navatar, the other is the template avatar. The current avatar is\nrepresented by DMTet and its SDF fcur is parameterized as\na MLP network \u03c8g, while the template avatar and its SDF\nftmp are initialized by the human prior mesh and its SDF\nf0. Note there is no parameter for the template avatar.\nAt the beginning, we attempt to align the current avatar\nwith the human prior. We sample points P = {pi \u2208 R3}\naround the mesh surface together with some random points\nand optimize the following loss:\nLinit\nSDF = \u2225fcur(P) \u2212 f0(P)\u22252\n2 =\nX\npi\u2208P\n\u2225fcur(pi) \u2212 f0(pi)\u22252\n2.\n(2)\nAfter initialization, the current avatar and template avatar\nare both aligned with the human prior.\n3.2.3\nGeometry deformation stage\nIn the coarse stage, the mesh of the current avatar is ex-\ntracted from DMTet by Marching Tetrahedra.\nThen the\nmesh is passed to the differentiable renderer. The normal\nimage n \u2208 Rh\u00d7w\u00d73 and the mask a \u2208 Rh\u00d7w\u00d71 are ren-\ndered by a sampled camera pose (details are in Sec. 4.1).\nThen they are concatenated and scaled into a smaller size\nby interpolation as na \u2208 Rh\u2032\u00d7w\u2032\u00d74. Noises are directly\nadded to the na for SDS loss:\n\u2207\u03c8gLnorm\nSDS (\u03d5, na) = Et,\u03f5[w(t)(\u03f5\u03d5(zna\nt ; y, t) \u2212 \u03f5)\u2202na\n\u2202\u03c8g\n]\n(3)\nIn this stage, the avatar\u2019s global shape will be determined in\na coarse manner. Since the guidance is directly on the nor-\nmal space but not latent space, this geometry deformation\ncould be intense but unstable and easy to deviate from the\ncorrect shape and lose local structures from human prior.\nGlobal evolving SDF constraint In order to keep the ge-\nometry in a decent human shape during optimization, we\nimpose SDF constraint on the current avatar from the tem-\nplate avatar. Specifically, we constantly optimize the global\n4\ntemplate loss during generation:\nLglb\nSDF = \u2225fcur(P) \u2212 ftmp(P)\u22252\n2 .\n(4)\nThe full formulation is similar to Eq. (2). Although the con-\nstraint can avoid severe deformation from the human prior,\nit will also sacrifice the generation flexibility and it will be-\ncome hard to generate diverse geometry shapes. To alleviate\nthe issue, we update the template avatar periodically as the\nself-evolving constraint fcur \u21d2 fref. More specifically, the\nmesh of the current avatar is extracted in every \u03b4g step, and\nthe mesh is converted to SDF as the new template.\nLocal static SDF constraint An evolving template avatar\nhelps to loosen the constraint, but some delicate local struc-\ntures from human prior may also be ruined due to the in-\nstability of the coarse stage SDS optimization. In order to\nkeep the detailed structures of the face and hands, we fur-\nther employ a local template loss:\nLloc\nSDF =\nX\ni\nwi \u2225fcur(Qi) \u2212 f0(Qi)\u22252\n2\n(5)\nwhere f0 is the SDF of the original human prior mesh, and\nQi is sampled point set for the i-th local part. Since SMPL-\nX has semantic labels, we select face, hands and feet as\nlocal template parts. Feet are mainly for avatars with bare\nfeet. The loss of all parts are summed with weights wi.\n3.2.4\nGeometry refining stage\nIn refining stage, the normal image n is mapped to latent\nspace by the auto-encoder as zn before adding noise for\nSDS loss:\n\u2207\u03c8gLnorm\nSDS (\u03d5, n) = Et,\u03f5[w(t)(\u03f5\u03d5(zn\nt ; y, t) \u2212 \u03f5)\u2202zn\n\u2202n\n\u2202n\n\u2202\u03c8g ]. (6)\nDifferent from the coarse stage, the refining stage will not\ncause large deformation but generate detailed shapes like\nwrinkles in clothes.\nHowever, the initial resolution of\nDMTet in coarse stage is set as 256, which might be in-\nsufficient to express fine details in high quality.\nTet subdivision In order to increase the resolution, we sub-\ndivision the tetrahedral grid during refineing stage. In previ-\nous works [17, 31, 43], the subdivision is performed only in\ntetrahedra intersecting with the surface. However, we find\nsuch a strategy may cause discontinuous faces in the mesh.\nSo we subdivide the tetrahedral grid around the surface in a\nfixed interval. In detail, we select the tetrahedra with mean\nvertex SDF value smaller than 0.2. The local subdivision\nwill only increase a little GPU memory consumption, but\nthe tetrahedra become 8\u00d7 around the surface.\nGlobal and local normal constraints As the resolution be-\ncomes high, the geometry will exhibit noisy and bumpy\ncharacteristics due to the large number of vertices. Inspired\nby [17], we employ normals to smooth the surface. Simi-\nlar to our SDF constraints, normal constraints are imposed\nboth globally and locally on the current avatar by the tem-\nplate and prior:\nLglb\nnorm = \u2225ncur \u2212 ntmp\u22252\n2 ,\n(7)\nLloc\nnorm =\nX\ni\nki\n\r\rncur \u2212 ni\n0\n\r\r2\n2.\n(8)\nwhere ncur/ntmp is the normal image rendered from the\ncurrent/template avatar, and ni\n0 is rendered by only a part\nmesh from the human prior. ki is the weight for the i-th\npart. We mainly use face for this normal constraint since it\nis the most important local part. In summary, during the ge-\nometry generation, the total loss consists of SDS loss, SDF\nloss and normal loss. Both the SDF loss and normal loss\ninclude global and local terms:\nLgeo = \u03bbSDSLnorm\nSDS + LSDF + Lnorm\n(9)\nLSDF = \u03b1rLglb\nSDF + \u03b1lLloc\nSDF\n(10)\nLnorm = \u03b2rLglb\nnorm + \u03b2lLloc\nnorm\n(11)\n3.3. Appearance generation\nGiven the mesh generated in geometry stages, we then gen-\nerate appearance for it. The appearance is represented as\na neural field with multi-resolution hash encoding [30] pa-\nrameterized by a MLP \u03c8c. For the query points from the\ngenerated mesh, the MLP network predicts diffuse term kd\nas albedo color, specular term ks including roughness and\nmetalness, and the normal term kn following PBR work-\nflow. With these textures, the RGB image x is rendered\nwith a differentiable renderer [11, 31], given a HDRI as the\nenvironment light. Then, the RGB image is guided by the\ndiffusion model by SDS loss:\n\u2207\u03c8cLcolor\nSDS (\u03d5, x) = Et,\u03f5[w(t)(\u03f5\u03d5(zx\nt ; y, t) \u2212 \u03f5)\u2202zx\n\u2202x\n\u2202x\n\u2202\u03c8c ]. (12)\n3.3.1\nUniform scaling\nThe standard normalization for the query points is non-\nuniform or anisotropic, i.e. points normalized to [0, 1]3 by\n(p \u2212 pmin)/(pmax \u2212 pmin), given the corners of the 3D\nbounding box of all points as pmax, pmin \u2208 R3.\nFor a\npoint p(x, y, z), the scaled factors for each dimension will\nbe (sx, sy, sz) = pmax \u2212 pmin. We find this operation will\nresult in artifacts in the texture along the depth dimension\n(Fig. 8 (a)). The scale of the avatar along the depth dimen-\nsion is relatively small compared with wideness and height.\nTherefore, we use a uniform scaling which produces no ar-\ntifacts along the depth dimension:\np\u2032 = (p \u2212 pmin)/s,\ns = max(sx, sy, sz).\n(13)\n5\n(a) \ud835\udc58! and lightness at step 200\n(b) \ud835\udc58! and lightness at step 2000\nFigure 4. Albedo and lightness at early step and late step (\u00a73.3.2).\n3.3.2\nSelf-evolving lightness constraint\nModel enhancement In order to generate appearance with\nricher details, we use prompt engineering to enhance the\ndiffusion model. The technique is widely used in practice\nby the community, but we notice prior works often use sim-\nple prompt, which may not fully activate the potential of\nthe model. Proper positive prompt and negative prompt can\nlargely improve the appearance quality. More details can be\nfound in Sec. 4.1.\nLighting effects Although the quality is improved with the\nhelp of prompt engineering, we also observe that the dif-\nfusion model tends to produce fancy lighting effects which\nis inconsistent with the environment lighting. As a result,\nthe differentiable rendering system may absorb the lighting\neffects into albedo colors. An important empirical observa-\ntion is that the albedo colors do not include many lighting\neffects in early steps, as shown in Fig. 4. We find the gener-\nation process follows the coarse to fine rule: pure colors are\nfirst generated for each part, and local details and lighting\neffects are generated in later steps. Therefore, our idea is to\nconstrain late steps by early steps.\nLightness constraint Similar to the geometry generation,\nwe set up the current avatar with a template avatar. The tem-\nplate avatar\u2019s appearance is initialized by the current avatar\nin an early step. We constrain the distance between the\nlightness (or luminance) of the current avatar and template\navatar. However, directly constraining the current avatar\nwith the template avatar may prevent current avatar from\ngenerating fine details. To alleviate the problem, the tem-\nplate avatar\u2019s appearance network \u03c8\u2032\nc is updated by the cur-\nrent avatar\u2019s network \u03c8c every \u03b4c steps. In addition, we\nscale the albedo images into a smaller scale, which will\nmake the constraint only effective on coarse lighting cues\nand loose on local details. Specifically, we optimize the fol-\nlowing loss for lightness constraint:\nY (I) = (Ir + Ig + Ib)/3,\n(14)\nLlgt =\n\r\rS(Y (kcur\nd )) \u2212 S(Y (ktmp\nd\n))\n\r\r2\n2\n(15)\nwhere kcur\nd /ktmp\nd\nis the rendered albedo of current/template\navatar, and S is the down-scaling function.\n4. Experiment\n4.1. Implementation details\nCamera sampling Similar to DreamHuman [21], we sam-\nple camera poses for multiple parts of the body. We obtain\nsemantic labels from SMPL-X [32] and set head, arms and\nfeet as separate parts. In each training step, the full body\nor one part is randomly chosen as the center of the view\nand the camera is also randomly rotated for rendering. The\nprompt patterns are like \u201cA full length DSLR photo of ...\u201d\nfor full body and \u201cA DSLR photo of ...\u2019s ...\u201d for local parts.\nDiffusion models For geometry generation, we use Stable\nDiffusion 2.1 base (SD2.1b) [4]. For appearance genera-\ntion, we use Realistic Vision 5.1 (RV5.1) [3] by default.\nSD2.1b is also tested but we find RV5.1 can generate more\nvisually pleasing appearance. More details and examples\ncan be found in appendices.\nPrompt engineering Using proper prompt is crucial for\ngenerating high quality 2D images from diffusion mod-\nels. We take in the experience from the community [1, 2]\nand add auxiliary positive prompt and negative prompt dur-\ning appearance generation.\nFor positive prompt, we use\n\u201cmasterpiece, Studio Quality, 8k, ultra-HD, next genera-\ntion\u201d to enhance the generation quality. The positive prompt\nis appended behind the prompt for target avatars.\nFor\nnegative prompt, the following categories are concerned:\ncolor (\u201cnoise,pattern, strange color...\u201d), structure (\u201cpoorly\ndrawn face, mutation,ugly...\u201d) and quality (\u201clow quality,\nlowres, error...\u201d). For historical avatars like Lincoln, adding\n\u201csculpture,statue\u201d will prevent the model from generating\ngrayscale appearance.\nGeneration For geometry generation, the normal images\nare rendered in 512 \u00d7 512. The update interval of tem-\nplate avatar is set as 5000 by default. A smaller interval\ncan be set for complex avatars like long hair or long dress.\nFor appearance generation, the RGB images are rendered in\n768 \u00d7 768, and final textures are sampled from the 3D rep-\nresentation and saved in 4096\u00d74096. The initial step of the\ntemplate avatar is 400 and update interval is set as 200. We\nuse four Nvidia RTX 3090 GPUs with 24GB memory for all\nexperiments. The generation process includes 9k steps for\ngeometry and 2k steps for appearance, which takes around\n2 hours. More details can be found in appendices.\n4.2. Qualitative comparison\nThe qualitative comparison results are shown in Fig. 5. For\nwell-known iconic characters, we compare our method with\nDreamAvatar [6], DreamWaltz [16] and AvatarVerse [52].\nThe target avatars are \u201cSpiderMan\u201d and \u201cJoker\u201d. For cus-\ntom characters, we compare our method with DreamHuman\n[21], AvatarVerse [52], TADA [23] and HumanNorm [15].\nThe target avatars are \u201cA karate master\u201d and \u201cA woman in\njean skirt\u201d.\n6\nDreamAvatar [Arxiv 23.04]\nAvatarVerse [Arxiv 23.08]\nDreamWaltz [NeurIPS23]\nOurs\nDreamAvatar [Arxiv 23.04]\nAvatarVerse [Arxiv 23.08]\nDreamWaltz [NeurIPS23]\nOurs\nTADA [3DV24]\nDreamHuman [NeurIPS23]\nHumanNorm [Arxiv 23.10]\nOurs\nAvatarVerse [Arxiv 23.08]\nDreamHuman [NeurIPS23]\nHumanNorm [Arxiv 23.10]\nOurs\nJoker\nSpiderman\nWoman in jean skirt\nkarate master\nFigure 5. Qualitative comparison (\u00a74.2). We compare our method with DreamAvatar [6], DreamWaltz [16], AvatarVerse [52], DreamHu-\nman [21], TADA [23] and HumanNorm [15]. Front and back views are presented, and faces and hands are cropped and enlarged for\ndetailed comparison. Please zoom in for better view.\nMethod\nGeo.G\nGeo.L\nAppr.G\nAppr.L\nAvg.\nDreamWaltz\n[16]\n2.10\n1.72\n1.92\n1.72\n1.86\nTADA\n[23]\n2.88\n2.71\n2.77\n2.69\n2.76\nOurs\n4.39\n4.36\n4.31\n4.38\n4.36\nTable 1.\nQuantitative comparison (\u00a74.2).\nWe compare our\nmethod with recent works by human evaluation. Both geometry\n(Geo.) and appearance (Appr.) are evaluated in global (G) and\nlocal (L) aspects. Higher score is better, ranging from 1 to 5.\nFrom the perspective of geometry quality, our results are\nbetter on both the global shape and local structures. On the\none hand, avatars are more strictly aligned with the human\nprior due to global geometry constraints, so the body ratios\nare more natural. Local structures in face and hands/feet\nare also well preserved thanks to the local geometry con-\nstraints. On the other hand, with the help of the evolving\ntemplate, shapes like loose karate uniform or skirt can also\nbe generated with fine details.\nAs for appearance, our avatars are equipped with high\nresolution textures with rich details including the facial fea-\ntures and clothing, without any over-saturation. Photoreal-\nistic images can be produced with classic graphics pipelines\nwith our high quality textures.\n(a) Cristiano Ronaldo in blue \nsuit, with afro hair\n(b) Barack Obama in blue T-\nshirt, shorts and sneakers\nFigure 6. Geometry and appearance editing by text prompts\n(\u00a74.4), such as different hair, clothes styles, colors, etc.\n4.3. Quantitative evaluation\nWe sampled 25 avatar prompts and invited 15 volunteers\nfor human evaluation. We compare our method with nerf-\nbased DreamWaltz [16] and mesh-based TADA [23], which\nhave available source code. The generated avatars are ren-\ndered in videos and volunteers are asked to give scores\nfrom 1 to 5 in four aspects: global shapes (body propor-\ntion, clothes style), local structures (face, hands), global ap-\npearance (color style) and local appearance (texture details).\nThe results are in Tab. 1. Our method has the highest scores\nin all aspects, especially in local geometry and appearance.\n7\n(a) full constraints\n(c) w/o evolving\n(e) w/o subdivision\n(d)  w/o \u2112!\"#$\n(b) w/o \u2112%&'\nAnne Hathaway,long wavy hair, wearing an ankle length dress\nFigure 7. Ablation study of geometry generation (\u00a75.1). The\neffect of SDF/normal constraint and evolving template are shown.\n4.4. Applications\nSince the geometry and appearance is decoupled in our\nframework, it would be more flexible for geometry and ap-\npearance editing. Examples are shown in Fig. 6. Avatar\u2019s\nbody shape, clothing, hair style with different colors can be\nedited with different text prompts. In addition, our method\ngenerates avatars with meshes and textures, which can be\neasily exported to graphics tools like Blender for further ap-\nplication like relighting and posing. Demos are in Fig. 1.\n5. Ablation Study\n5.1. Geometry generation\nThe ablation results of the geometric constraints are shown\nin Fig. 7. The prompt is \u201cAnne Hathaway, long wavy hair,\nwearing an ankle length dress\u201d. In (a) and (b), it is demon-\nstrated that the SDF constraint from the template avatar is\ncrucial for generating global and local shapes conforming\nto the human prior. If SDF constraints are missing, gener-\nating with only normal constraints will not produce good\ngeometry because the depth is not controlled. In addition,\nan evolving template is also important, shown in (a) and (c).\nIf the template avatar is not evolving, the generated geome-\ntry can only be close to the human prior, noticing the dress\nand hair are not generated correctly as the prompt requires.\nTet subdivision pushes the representation to a higher reso-\nlution, but it will also cause noisy surfaces like (d), if the\nnormal constraints are not used.\n5.2. Appearance generation\nThe ablation results of the appearance generation are shown\nin Fig. 8. (a) shows that non-uniform scaling will result\nin striped artifacts in depth dimension, and uniform scal-\ning can solve the issue. (b) demonstrates that the negative\nprompt can reduce over-saturation and noisy artifacts. In\n(c) and (d), if the constraint is removed, the guidance diffu-\nsion model tends to generate strong light effects, and fake\nspeculars will be \u201cbaked\u201d into the albedo (kd). With the\nhelp of our constraint, the specular parts in the albedo are\nsuppressed and the overall lightness is more uniform.\n(c) \ud835\udc58! with lightness constraint\n(d) \ud835\udc58! without lightness constraint\n(a) uniform/non-uniform scaling\n(b) with/without negative prompt\nFigure 8. Ablation study of appearance generation (\u00a75.2). The\neffect of uniform scaling, negative prompt and the lightness con-\nstraint are shown. Please zoom in for better view.\n6. Limitations and future work\nAlthough high quality avatars can be generated by our\nmethod, there are still some limitations. Examples can be\nfound in appendices. For geometry, some highly detailed\nstructures are hard to be represented, such as hair strands\nand eyelashes.\nIn addition, very loose clothes or com-\nplex accessories can not be well generated in our frame-\nwork. More delicate representations may be designed to\nsolve these issues.\nFor appearance, despite we follow the PBR workflow,\nthe roughness values may not be accurately generated. In\nFig. 1, sharp specular can be observed on Einstein\u2019s sweater,\nwhich are caused by a too low roughness value. Besides,\nthere are still some lighting and shadows baked into albedo\ncolors.\nOur constraint is an empirical strategy by lever-\naging the property of the model itself. Introducing extra\ndata as prior knowledge may help with the problem [50].\nIn addition, the generated appearance may not be exactly\naligned with the geometry. This problem may be alleviated\nby adding more controls in appearance guidance [15, 23].\n7. Conclusion\nIn this work, we present SEEAvatar, a method for pho-\ntorealistic avatar generation with constrainted geometry\nand appearance.\nThe proposed constraints for geometry\nare able to control the avatar in a decent global human\nshape with flexible clothes styles, while maintain detailed\nlocal structures from the human prior.\nThe SDF and\nnormal constraints also help the geometry to deform\nwith smooth surfaces in the subdivided high resolution\nrepresentation.\nThe proposed lightness constraint for\nappearance generation can effectively suppress the lighting\neffect in albedo colors.\nAs a result, the high qual-\nity 3D meshes and textures generated by our method can\nbe applied in classic workflows for photorealistic rendering.\n8\nReferences\n[1] Civitai. https://civitai.com. 3, 6\n[2] Hugging Face. https://huggingface.co. 3, 6\n[3] Realistic Vision.\nSG161222/Realistic_Vision_\nV5.1_noVAE. 6, 1\n[4] Stable Diffusion v2.1 base. https://huggingface.\nco / stabilityai / stable - diffusion - 2 - 1 -\nbase. 6, 1\n[5] Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu.\nimghum: Implicit generative models of 3d human shape and\narticulated pose. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 5461\u20135470,\n2021. 3\n[6] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 2, 3, 6, 7\n[7] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner.\nText2tex:\nText-driven\ntexture synthesis via diffusion models.\narXiv preprint\narXiv:2303.11396, 2023. 3\n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 3, 1\n[9] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Learning deformable\ntetrahedral meshes for 3d reconstruction. Advances In Neu-\nral Information Processing Systems, 33:9936\u20139947, 2020. 4\n[10] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang\nDeng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee K Wong.\nHeadsculpt:\nCrafting 3d head avatars with text.\narXiv\npreprint arXiv:2306.03038, 2023. 3\n[11] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape, light, and material decomposition from images using\nmonte carlo rendering and denoising. Advances in Neural\nInformation Processing Systems, 35:22856\u201322869, 2022. 5\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[13] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022. 3, 1, 2\n[14] Shuo Huang, Zongxin Yang, Liangting Li, Yi Yang, and\nJia Jia.\nAvatarfusion: Zero-shot generation of clothing-\ndecoupled 3d avatars using 2d diffusion.\narXiv preprint\narXiv:2307.06526, 2023. 2, 3, 1\n[15] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying\nFeng, Yebin Liu, and Qing Wang. Humannorm: Learning\nnormal diffusion model for high-quality and realistic 3d hu-\nman generation. arXiv preprint arXiv:2310.01406, 2023. 6,\n7, 8\n[16] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. arXiv\npreprint arXiv:2305.12529, 2023. 2, 3, 6, 7\n[17] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided\nreconstruction of lifelike clothed humans.\narXiv preprint\narXiv:2308.08545, 2023. 5\n[18] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 3\n[19] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\ncraft: Transforming text into neural human avatars with\nparameterized shape and pose control.\narXiv preprint\narXiv:2303.17606, 2023. 3\n[20] James T Kajiya. The rendering equation. In Proceedings of\nthe 13th annual conference on Computer graphics and inter-\nactive techniques, pages 143\u2013150, 1986. 1\n[21] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. arXiv\npreprint arXiv:2306.09329, 2023. 2, 3, 6, 7\n[22] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4d scans. ACM Trans. Graph., 36(6):194\u20131, 2017. 4\n[23] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang,\nYangyi Huang,\nJustus Thies,\nand Michael J Black.\nTada!\ntext to animatable digital avatars.\narXiv preprint\narXiv:2308.10899, 2023. 2, 3, 6, 7, 8, 1\n[24] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 3\n[25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2, pages 851\u2013866. 2023. 3, 4\n[26] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 1\n[27] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 1, 2\n[28] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[29] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 3\n9\n[30] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 4, 5\n[31] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 5\n[32] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black.\nExpressive body capture:\n3d hands,\nface, and body from a single image.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10975\u201310985, 2019. 3, 4, 6\n[33] Bui Tuong Phong. Illumination for computer generated pic-\ntures. In Seminal graphics: pioneering efforts that shaped\nthe field, pages 95\u2013101. 1998. 1\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 3\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3\n[37] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint arXiv:2302.01721, 2023. 3\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 3\n[39] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-\nbodied hands: Modeling and capturing hands and bodies to-\ngether. arXiv preprint arXiv:2201.02610, 2022. 4\n[40] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n\u201d grabcut\u201d interactive foreground extraction using iterated\ngraph cuts.\nACM transactions on graphics (TOG), 23(3):\n309\u2013314, 2004. 1\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 3\n[42] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18603\u201318613,\n2022. 3\n[43] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 2, 4, 5\n[44] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,\nMatthias Nie\u00dfner, and Angela Dai. Texturify: Generating\ntextures on 3d shape surfaces. In European Conference on\nComputer Vision, pages 72\u201388. Springer, 2022. 3\n[45] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo\ntourism: exploring photo collections in 3d. In ACM siggraph\n2006 papers, pages 835\u2013846. 2006. 1\n[46] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3\n[47] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 3\n[48] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 3\n[49] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 3\n[50] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. Mat-\nlaber: Material-aware text-to-3d via latent brdf auto-encoder.\narXiv preprint arXiv:2308.09278, 2023. 8, 1\n[51] Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, and\nXun Cao. Avatarbooth: High-quality and customizable 3d\nhuman avatar generation. arXiv preprint arXiv:2306.09864,\n2023. 2, 1\n[52] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar cre-\nation from text and pose. arXiv preprint arXiv:2308.03610,\n2023. 3, 6, 7\n[53] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Jus-\ntus Thies, and Michael J Black.\nText-guided generation\nand editing of compositional 3d avatars.\narXiv preprint\narXiv:2309.07125, 2023. 3\n[54] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu.\nDreamface:\nProgressive generation of ani-\nmatable 3d faces under text guidance.\narXiv preprint\narXiv:2304.03117, 2023. 3\n[55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3\n10\nSEEAvatar: Photorealistic Text-to-3D Avatar Generation\nwith Constrained Geometry and Appearance\nSupplementary Material\nProject Page\n8. Additional qualitative comparison results\nWe add comparison results with general text-to-3D genera-\ntion methods, including LatentNeRF [27], and Fantasia3D\n[8]. Additional comparison with other text-to-avatar gen-\neration methods including Avatar-CLIP [13], AvatarBooth\n[51] and AvatarFusion [14] are also provided. Comparison\nresults of two avatars, \u201cBarack Obama\u201d and \u201cHillary Clin-\nton\u201d are shown in Fig. 9.\nGeneral text-to-3D generation methods are likely to lose\ndecent body shapes due to the lack of human prior knowl-\nedge. Even initialized with the human body, Fantasia3D\nstill generates poor results because the normal guidance for\ngeometry is unstable. Methods designed for text-to-avatar\ngeneration are better than general methods, but they still\nfail to control body proportions and maintain local struc-\ntures. Compared with them, our method is able to generate\nphotorealistic results with high resolution and high quality\ngeometry and appearance.\n9. Different diffusion models\nFor appearance generation, we use Realistic Vision 5.1\n(RV5.1) [3] as the guidance diffusion model. Here we com-\npare it with Stable Diffusion 2.1 base (SD2.1b) [4]. First,\nwe enhance both of the models with prompt engineering,\nand results are shown in Fig. 10. As for the raw results,\nRV5.1 is better than SD2.1b. If the positive prompt is used,\nboth of the models yield richer details with higher contrast.\nIf the negative prompt is used, the generation quality can be\nlargely enhanced. Using both positive and negative prompt\ncan produce results with rich details and high quality. How-\never, RV5.1 has strong biases on female characters. The\nface of wonder woman in Fig. 10 is not correct in the re-\nsults of RV5.1, while SD2.1b can be used to make up for\nthis shortcoming.\nWe also find that RV5.1 has better color styles compared\nwith SD2.1b, as shown in Fig. 11. RV5.1 shows more visu-\nally pleasing colors while SD2.1b may produce results with\nhigh contrast and saturation. Note the lightness constraint\nis not used in above results.\n10. Additional ablation study\nFor geometry generation, we present the results of different\nupdating intervals of the evolving template in Fig. 12. The\nprompt is \u201cAnne Hathaway,long wavy hair, wearing an an-\nkle length dress\u201d. When the template is static human prior\nin (a), the global shape will be limited and the dress can not\nbe generated. Once with an evolving template, the dress can\nbe generated, while updating the template more frequently\nleads to more flexible shapes.\nFor appearance generation, we present the results of dif-\nferent updating intervals and different image sizes for light-\nness constraint in Fig. 13. A larger interval will help to\nsuppress the lighting effect, while the details may also be\naffected. Similarly, applying lightness constraint directly on\nthe raw size (768 \u00d7 768) without down-sampling will also\nhave stronger positive effect on suppressing lighting effect,\nbut also more negative effect on details.\n11. Limitations and failure cases\nWe present some failure cases in Fig. 14. For geometry, (a)\nshows the case of generating a very loose princess dress.\nSince the dress is much deviated from the global human\nprior, which is beyond the capability of the evolving tem-\nplate, so our method can\u2019t generate it well. (b) and (c) show\ncases of local structures like glasses and hats. Generating\nthese structures may be conflict with our local constraints\nfrom the human prior, so the results are not ideal.\n(d) shows the misalignment problem between the geom-\netry and appearance. Sometimes the appearance model fails\nto recognize correct geometry semantics and generates fake\nor wrong appearance. This problem may be alleviated by\nadding control in appearance guidance, like in [23]. (e) il-\nlustrates the fake lighting problem. Although our lightness\nconstraint is able to suppress incorrect lighting effects, the\nproblem is not perfectly solved. (f) shows the wrong spec-\nular caused by the wrong roughness. Our method is not\nable to generate accurate PBR parameters for materials. In-\ntroducing extra data of decoupled textures for training or\nfinetuning may solve the issues better, like in [50].\n1\nLatentNeRF [CVPR23]\nFantasia3D [ICCV23]\nOurs\nAvatarCLIP [SIGGRAPH22]\nAvatarBooth [Arxiv 23.06]\nAvatarFusion [MM23]\nBarack Obama\nHillary Clinton\nFigure 9. Qualitative comparison with text-to-3D and text-to-avatar methods (\u00a78). Text-to-3D methods for general objects are Latent-\nNeRF [27] and Fantasia3D [8], and methods for avatars are AvatarCLIP [13], AvatarFusion [14] and AvatarBooth [51].\nSD2.1b\nRV5.1\nboth\n+negative\n+positive\nraw\nMessi wearing FC Barcelona jersey\ncropped\nboth\n+negative\n+positive\nraw\ncropped\nWonder Woman\nFigure 10. Different diffusion models for appearance generation (\u00a79). Stable Diffusion 2.1 base (SD2.1b) and Realistic Vision 5.1 (RV5.1)\nare tested under different prompt strategies including raw prompt, adding postive prompt, adding negative prompt and adding both. Faces\nare croppped for better view.\nSD2.1b\nRV5.1\nMessi\nObama\nJoker\nBatman\nFigure 11. Different color styles of diffusion models for appear-\nance generation (\u00a79).\n(c) interval 1000\n(b) interval 5000\n(a) w/o update\n(d) interval 500\nFigure 12. Different settings for geometry evolving template\n(\u00a710). Detailed parts are cropped for better view.\n(b) interval 200\n(c) interval 400\n(d) w/o resize\n(a) raw\nFigure 13. Different settings for appearance evolving template\n(\u00a710). Detailed parts are cropped for better view.\n(a) loose princess dress\n(b) glasses\n(c) low hat\n(d) texture misalignment\n(e) fake lighting (f) wrong roughness\nFigure 14. Failure cases (\u00a711). (a,b,c) are for geometry and (d,e,f)\nare for appearance.\n2\n"
  },
  {
    "title": "Vision-Language Models as a Source of Rewards",
    "link": "https://arxiv.org/pdf/2312.09187.pdf",
    "upvote": "10",
    "text": "Vision-Language Models as a Source of Rewards\n\u2217 Kate Baumli\nSatinder Baveja\nFeryal Behbahani\nHarris Chan\nGheorghe Comanici\nSebastian Flennerhag\nMaxime Gazeau\nKristian Holsheimer\nDan Horgan\nMichael Laskin\nClare Lyle\nHussain Masoom\nKay McKinney\nVolodymyr Mnih\nAlexander Neitz\nFabio Pardo\nJack Parker-Holder\nJohn Quan\nTim Rockt\u00e4schel\nHimanshu Sahni\nTom Schaul\nYannick Schroecker\nStephen Spencer\nRichie Steigerwald\nLuyu Wang\nLei Zhang\nGoogle DeepMind\nAbstract\nBuilding generalist agents that can accomplish many goals in rich open-ended\nenvironments is one of the research frontiers for reinforcement learning. A key\nlimiting factor for building generalist agents with RL has been the need for a\nlarge number of reward functions for achieving different goals. We investigate\nthe feasibility of using off-the-shelf vision-language models, or VLMs, as sources\nof rewards for reinforcement learning agents. We show how rewards for visual\nachievement of a variety of language goals can be derived from the CLIP family of\nmodels, and used to train RL agents that can achieve a variety of language goals.\nWe showcase this approach in two distinct visual domains and present a scaling\ntrend showing how larger VLMs lead to more accurate rewards for visual goal\nachievement, which in turn produces more capable RL agents.\n1\nIntroduction\nMany of the biggest successes of reinforcement learning (RL, [29]) have been in domains where\na clear reward function was readily available. Reward functions that have been successfully used\ninclude game win/loss [25, 32, 35], change in game score [2, 21], change in underlying state like\nforward motion [17] and negative distance to a desired state configuration. With these successful\napplications of RL to challenging tasks there has been growing interest in building generalist agents\ncapable of achieving many challenging goals in rich environments.\nOne of the main limiting factors in applying RL to building generalist agents is the need for many\nreward functions for different goals. Building a reward function which, when maximized, will lead to\nthe achievement of a particular goal can be challenging, time consuming, and hard to scale [16, 22].\nThis is true both in simulated environments, where determining whether an agent successfully\nachieved an abstract goal like building a house from the underlying state variables is difficult to\n\u2217Authors listed in alphabetical order. Contributions in section 6.\nCorresponding emails: {vmnih,feryal,harrischan}@google.com\nPreprint.\narXiv:2312.09187v2  [cs.LG]  22 Feb 2024\nexpress in code, and in the real world, where a reward has to be computed from observations. These\nchallenges have put the spotlight on automatic ways of generating reward functions for training\ngeneralist agents [16, 40].\nOne particularly promising direction that has emerged recently is the use of vision-language models\n(VLMs) for building reward functions in visual environments [4, 7, 10, 20]. VLMs trained on large\ndatasets of paired images and text have been show to perform well on a variety of visual detection,\nclassification and question answering tasks out of the box and can act as good initializations for\nfinetuning on specialized tasks or datasets. Recent work showed that a pretrained CLIP model [23]\nfinetuned on paired Minecraft videos and text can be used as an effective shaping reward for training\nagents to achieve hunting and foraging tasks in Minecraft [10]. Similarly, [7] showed how a pretrained\nFlamingo [1] VLM can be finetuned on visual question-answering data to produce an accurate visual\nsuccess detector for a variety of natural language goals.\nEncouraged by the success of these approaches we explore the feasibility of using off-the-shelf VLMs\nto derive accurate reward functions for language goals in visual environments. We propose a way\nof deriving a sparse binary reward for visual achievement of language goals from pretrained CLIP\nimage and language embeddings and show how it can be used to train agents to achieve a variety of\nlanguage goals in the Playhouse [31] and AndroidEnv [33] visual environments. We believe that our\nwork presents a compelling demonstration of how off-the-shelf VLMs can be used to train grounded\nlanguage agents without the need for finetuning on environment specific data.\n2\nRelated Work\n2.1\nVLMs Rewards\nA number of prior works have investigated using pretrained models such as CLIP as a reward function\nfor RL. Most closely related to our work is MineDojo [10] which first finetunes CLIP with Minecraft\nvideos to form MineCLIP. Then MineCLIP is used as a dense shaping reward function in addition\nto a ground truth binary reward function for programmatic tasks where ground truth is available,\nexcept for creative tasks where no ground truth is available. The main differences between this work\nand MineDojo are that (i) we use an off-the-shelf CLIP model, (ii) we do not utilize any ground\ntruth information during RL training. and (iii) we train a single RL agent that can solve thousands\nof language-based goals. Similar to MineDojo, CLIP4MC [6] finetunes CLIP on a curated dataset\nof Minecraft Youtube videos. CLIP4MC then uses PPO as the RL training backbone and the policy\nis trained on the CLIP4MC reward in the same form as MineCLIP reward together with the sparse\ncompletion where available. CLIP has also been used for reward-shaping simulated robotics domain\n[4, 20, 27]. RoboCLIP [27] computes a sparse trajectory level reward as the cosine similarity score\nbetween the embedding of the trajectory video and the embedding of the text description of the task.\nVLM-RMs [24] proposes goal-baseline regularization by projecting out irrelevant CLIP embedding\nspace dimensions from the goal embedding using a secondary baseline text prompt embedding,\napplied to training a controller for a simulated humanoid robot. CLIP encoder was used for semantic\nintrinsic reward signal for curiosity-driven exploration in [30]. VIPER [9] uses the conditional\nlog-likelihood of a frozen action-free video prediction model, such as VideoGPT [39], summed with\nan entropy bonus as the reward signal for reinforcement learning agents without groundtruth task\nrewards in DeepMind Control Suite [34], Atari [2], and RLBench [14] tasks.\nAn orthogonal approach for scaling up reward modeling is to use Large Language Models (LLMs) to\ntranslate task specification into reward functions executable programs, resulting in sparse rewards\n[41] or dense rewards [19, 37]. Earlier works also use LLMs itself as binary reward in negotiation\ngames [15], collaborative human-AI interaction games [13], or dense reward based on the cosine\nsimilarity of a caption (hard-coded or learned) of transition and goals in a 2D Crafter environment\n[8, 11].\n2.2\nVLMs for Hindsight Relabeling\nSeveral works used VLMs for hindsight relabeling to do behaviour cloning. [28] uses Flamingo for\nhindsight relabeling and then behaviour cloning in the Playhouse environment (similar to our Lift\ntask). DIAL [36] uses CLIP finetuned on robotic demonstration and autonomous data that has been\n2\nGo near a \nhelicopter\nLift the pink \nboat\nImage\nEncoder\nLanguage\nEncoder\nSoftmax\n& \nthreshold\nbinary\nreward\nCosine\nsimilarity\nGoal\nObservation\n(single or multi-frame)\nNegatives\nScores\nFigure 1: Architecture for Vision-Language Models (VLMs) as rewards. The VLM trained con-\ntrastively contains an image encoder f\u03b8 and language encoder g\u03b8. We embed the current environment\nobservation frame(s) using the image encoder, along with the desired goal language descriptions l\nand negative language descriptions using the language encoder. The reward is computed by taking\nthe cosine similarity scores and applying softmax and thresholding.\nhindsight labeled by human annotators to further relabel a larger set of offline robotic trajectories,\nthen also perform behaviour cloning a language-conditioned policy.\n2.3\nVLMs as Success Detectors\nSuccessVQA [7] finetunes smaller 3B Flamingo models to act as success detectors by formulating the\nproblem as a Visual Question Answering (VQA). Most of their experiments focus on how accurate\nthe success detector is on offline datasets across various domains (robotics, Playhouse, Ego4D), with\none experiment using the SuccessVQA model for reward-filtered behaviour-cloning.\n3\nMethod\nIn this work we investigate how to leverage contrastive VLMs like CLIP to produce text-based reward\nmodels. Our aim is to produce a reward function that, given a language-based goal and an image\nobservation, produces a scalar reward corresponding to whether or not that goal has been achieved.\nConcretely, we construct a reward model which consists of an image encoder f\u03b8(o) and text encoder\ng\u03b8(l) where o \u2208 O is an observation and l \u2208 L is a text-based goal. The reward model inputs both\nobservation and goal r(o, l) and outputs a binary reward: 1 if the goal has been achieved and 0\notherwise.\nWe operate in a partial observed Markov Decision Process (POMDP) setting where the agent sees\nobservations ot \u2208 O, acts in an environment with actions at \u2208 A, observes the next observation\naccording to the environment dynamics ot+1 \u223c P(ot+1|at, ot), and receives a reward rt \u223c R with a\ndiscount factor \u03b3 \u2208 (0, 1]. In addition to the standard POMDP, our agent is given a language-based\ngoal l \u223c L for the duration of the episode and the reward is computed via a VLM as a function\nrt = r(ot+1, l). The episode terminates either on timeout T or when rt = 1 meaning that the goal\nhas been achieved according to the intrinsic VLM reward function. Note that we assume no access to\nthe ground truth reward during training, and only use the ground truth reward for evaluation.\n3.1\nTurning CLIP into a reward function\nWe first compute the probability pot,l that the agent achieves the goal given by language description l\nafter acting in the environment, out of a set of potential goals l\u2032 \u2208 L in the task set L, by applying\nsoftmax with temperature \u03c4 over the cosine similarity between the embedding of the state, f\u03b8(ot+1),\nand the embedding of the language description g\u03b8(l) across the set of potential goals:\npot,l =\nexp(f\u03b8(ot) \u00b7 g\u03b8(l)/\u03c4)\nP\nl\u2032 exp(f\u03b8(ot) \u00b7 g\u03b8(l\u2032)/\u03c4)\n(1)\n3\nLift a \nblue mug\nGo near a \ngreen roll\nOpen clock\nOpen Chrome\nOpen Calendar\nPlayhouse\nAndroidEnv\nPut a table \nlamp next \nto a plane\nFigure 2: Environments and example tasks. (Left) Playhouse [31] consists of Find, Lift, and Pick and\nPlace tasks. (Right) AndroidEnv [33] consists of opening app tasks across various apps on Android.\nWe then convert this reward function into a binary reward function by thresholding the probability on\nthe hyperparameter value \u03b2 \u2208 [0, 1]:\nrt \u2261 r(ot+1, l) = I[pot+1,l > \u03b2]\n(2)\nwhere I is an indicator function. To avoid stochastic rewards, we sample negatives uniformly from\nthe task set l \u2208 L and fix them throughout the duration of RL training. In our experiments, the task\nset is a discrete set of language goals that are known to be achievable in the environment. We leave\ninvestigations on generalizing the negatives sampling from generative distribution, such as samples\nfrom an LLM, for future work.\n4\nExperiments\nIn our experiments, we aim to answer the following questions:\n1. Does maximizing the VLM reward maximize the ground truth reward? (Section 4.4.1)\n2. How does scaling the size of the VLM affect the performance of the VLM reward? (Section\n4.4.2)\n3. Can online agent performance be predicted from offline evaluation metrics? (Section 4.4.2)\n4. What are the effects of prompt engineering on the performance of the VLM reward? (Section\n4.4.3)\n4.1\nExperimental Setup\nOur experimental setup is similar to that of standard online RL where an agent maximizes returns\nvia trial-and-error interactions with an environment. The only difference is that rather than training\nagainst a ground truth or hand-engineered reward function, our agent trains against rewards emitted\nby a VLM. For evaluation purposes, we plot both the VLM rewards, which we call intrinsic, and\nground truth rewards, which are provided by access to the simulator. We train a language-conditioned\npolicy that is initialized randomly, while the reward model is a pre-trained CLIP VLM whose weights\nare frozen. We use Muesli [12] as the RL algorithm to optimize the policy against the VLM reward\nfunction. The environment samples a goal from the set of training goals, and the agent is tested on\nheld-out goals during the evaluation.\n4.2\nCLIP architecture\nWe use pre-trained CLIP [23] models for all experiments. We investigate two types of encoder\narchitectures for CLIP. The first utilizes Normalizer-Free Networks (NFNets) [3], which uses a\nResNet image encoder with adaptive gradient clipping in lieu of batch normalization. The second is\nSwin [18, 38], which is a hierarchical vision transformer architecture for image encoding. We use the\nBERT architecture [5] as the language encoder in all CLIP models.\n4.3\nEnvironments and tasks\nWe experimented with two domains for visual environments: (1) Playhouse [31], and (2) AndroidEnv\n[33]. Figure 2 illustrates example observations from the two domains and their tasks. Playhouse\n4\n(a)\nOpen Apps (Common)\nOpen Apps (Diverse)\n(b)\nOpen Apps (Common)\nOpen Apps (Diverse)\n(c)\nFigure 3: Performance of an agent over the course of online reinforcement learning training when\noptimizing against the learned VLM reward. We measure both the (1) learned VLM reward return\nduring training and (2) ground truth reward on held-out evaluation tasks. There is strong correlation\nbetween optimizing the learned VLM reward and the ground truth reward.\nis a Unity-based environment with an egocentric view inside a procedurally-generated home. We\nexperimented with three families of tasks of increasing difficulty: (1) Find, (2) Lift, and (3) Pick and\nPlace. In the Find task, the goal is to locate the object and approach the object as closely as possible.\nIn the Lift task, the agent must also grab the object and lift it. Finally, in the Pick and Place task, the\nagent needs to pick up the desired object and place it near another specified object.\nAndroidEnv [33] is an open-source environment built on the Android operating system, allowing\nthe agent to interact through touchscreen gestures on an emulated Android device in real-time. is an\nAndroid device emulator that can be computer controlled. The observation space consists of realtime\nRGB images of the Android screen. The action space consists of swipe, touch, and type actions and\nmimics the way a human would interact with the device. We run experiments within two task sets\nwithin this environment. The first task, Open Common Apps, involves opening one of ten common\napps such as Gmail, Google Sheets, Chrome, Calculator, Clock, Messages, Phone, Google Calendar,\nand Google Docs. The second task, Open Diverse Apps, expands the common app task set with fifty\nadditional less well-known apps.\n4.4\nExperimental Results\n4.4.1\nMaximizing learned rewards maximizes ground truth\nFor each of the environments and task families, we train a language goal-conditioned agent to\nmaximize purely the VLM reward, without observing the ground truth reward. During the course\nof training, we also log the ground truth reward that the agent would have received. We observe in\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nRecall\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nPrecision\nCLIP-NFNet 0.2B\nCLIP-NFNet 0.8B\nCLIP-Swin 0.8B\nCLIP-Swin 1.4B\n0M\n25M\n50M\n75M\n100M\n125M\n150M\nUnique Learner Frames\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nGround Truth Return (Eval)\nCLIP-NFNet 0.2B\nCLIP-NFNet 0.8B\nCLIP-Swin 0.8B\nCLIP-Swin 1.4B\nFigure 4: Scaling reward model size. (Left) Precision-Recall curves for varying VLM architecture\nand sizes on an offline fixed dataset of Playhouse trajectories. (Right) Ground truth returns on\nheld-out evaluation tasks for Playhouse over the course of training with varying VLM reward sizes.\n5\nPrompt Template\nA\nOpen [TASK]\nB\nOpen the [TASK] app\nC\nScreenshot of [TASK]\nD\nScreenshot of [TASK] on Android\nFigure 5: (Left) Prompt engineering effects on the AndroidEnv Open App task. More descriptive and\nspecific prompts perform better when used as rewards. (Right) Prompt templates for the AndroidEnv\nOpen App tasks.\nFigure 3 that the agent trained to maximize reward also maximizes the ground truth reward. Since the\nVLM reward is not perfectly accurate, there is a systematic gap between the VLM reward versus the\nground truth reward. However, within the training budget, we did not observe reward hacking [26] of\nour VLM reward, where the true reward drops off while the proxy VLM reward continues to increase.\n4.4.2\nScaling reward function model\nWe investigate the scaling behaviour of the reward model as a function of the size of the underlying\nVLM. Firstly, we evaluate the accuracy of the VLM reward against ground truth binary reward on an\noffline, fixed dataset of Playhouse trajectories. Figure 4 (left) shows precision-recall curves obtained\nby a selection of models (CLIP with NFNet [3] and Swin [18, 38] vision encoders) evaluated against\nour Playhouse task set. The sensitivity is adjusted by varying the threshold \u03b2 above which the reward\nis given. We observe that increasing the size of the VLM used for the reward model (from 200M\nto 1.4B parameters) improves the precision-recall curves. Figure 4 (right) shows the ground truth\nreturns for held-out evaluation tasks over the course of training which are not given to the agent,\nwhen trained with VLM reward signals with different base models. We observe that the improved\naccuracy of the VLMs on offline datasets, when used as the only reward signal, does translate to\nbetter agent performance on ground truth evaluation metrics.\n4.4.3\nPrompt Engineering VLM Reward\nManually prompt-engineering the text template has been found to improve the zero-shot classification\nof CLIP [23]. Similarly, we found that prompt-engineering the text template for our VLM reward\ncan have a significant impact on the ground truth performance of the agent policies trained on the\nVLM reward. Figure 5(left) compares the ground truth return evaluation of agent policies trained on\nvarious prompt templates (Figure 5 (right)). We hypothesize that, due to the training distribution of\nCLIP image captions, using action verbs such as \u201copen\u201d in the prompt is not as helpful as providing a\ndescription of the successful states, such as \u201cScreenshot of [TASK] on Android\u201d.\n5\nConclusion\nThis work shows how accurate rewards for visual achievement of language goals can be derived from\noff-the-shelf VLMs like CLIP without finetuning on domain-specific datasets. Our scaling trend\nanalysis showed that the quality of the rewards derived from CLIP-style models increases as the\nnumber of parameters in the visual encoder grows. Taken together, we believe these results suggest\nthat as the quality of VLMs improves it may be possible to train generalist RL agents in rich visual\nenvironments without the need for any task or domain-specific finetuning of the reward model.\n6\n6\nAuthors and Contributions\nWe list authors alphabetically by last name. Please direct all correspondence to Volodymyr Mnih\n(vmnih@google.com), Feryal Behbahani (feryal@google.com), and Harris Chan\n(harrischan@google.com).\n6.1\nFull-time Contributors:\n\u2022 Kate Baumli: agent research, infrastructure engineering\n\u2022 Satinder Baveja: advising\n\u2022 Feryal Behbahani: research vision, team leadership, agent research\n\u2022 Harris Chan: reward function research, paper writing\n\u2022 Gheorghe Comanici: agent research\n\u2022 Sebastian Flennerhag: agent research\n\u2022 Maxime Gazeau: reward function research\n\u2022 Dan Horgan: engineering leadership\n\u2022 Michael Laskin: reward function research, paper writing\n\u2022 Volodymyr Mnih: research vision, team leadership, reward function research, paper writing\n\u2022 Alexander Neitz: agent research, evaluation\n\u2022 Fabio Pardo: reward function research\n\u2022 John Quan: agent research\n\u2022 Himanshu Sahni: reward function research, evaluation\n\u2022 Tom Schaul: agent research\n\u2022 Yannick Schroecker: agent research, evaluation\n\u2022 Stephen Spencer: infrastructure engineering, evaluation\n\u2022 Richie Steigerwald: evaluation, infrastructure engineering\n\u2022 Luyu Wang: reward function research, infrastructure engineering\n\u2022 Lei Zhang: agent research\n6.2\nPart-time Contributors:\n\u2022 Kristian Holsheimer: infrastructure engineering\n\u2022 Clare Lyle: agent research\n\u2022 Kay McKinney: project management\n\u2022 Hussain Masoom: project management\n\u2022 Jack Parker-Holder: agent research\n\u2022 Tim Rockt\u00e4schel: advising\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems, 35:\n23716\u201323736, 2022.\n[2] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artificial Intelligence\nResearch, Vol. 47:253\u2013279, 2012. cite arxiv:1207.4708.\n[3] Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale\nimage recognition without normalization. In International Conference on Machine Learning,\npages 1059\u20131071. PMLR, 2021.\n7\n[4] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran. Can\nfoundation models perform zero-shot task specification for robot manipulation? In Learning\nfor Dynamics and Control Conference, pages 893\u2013905. PMLR, 2022.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.\norg/N19-1423.\n[6] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An\nrl-friendly vision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023.\n[7] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando\nde Freitas, and Serkan Cabi. Vision-language models as success detectors. arXiv preprint\narXiv:2303.07280, 2023.\n[8] Yuqing Du, Olivia Watkins, Zihan Wang, C\u00e9dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. arXiv preprint arXiv:2302.06692, 2023.\n[9] Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg,\nYoungwoon Lee, Danijar Hafner, and Pieter Abbeel. Video prediction models as rewards\nfor reinforcement learning. In Thirty-seventh Conference on Neural Information Processing\nSystems, 2023. URL https://openreview.net/forum?id=HWNl9PAYIP.\n[10] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems,\nvolume 35, pages 18343\u201318362. Curran Associates, Inc., 2022.\n[11] Danijar Hafner.\nBenchmarking the spectrum of agent capabilities.\narXiv preprint\narXiv:2109.06780, 2021.\n[12] Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theo-\nphane Weber, David Silver, and Hado van Hasselt. Muesli: Combining improvements in policy\noptimization, 2021.\n[13] Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai\ncoordination. arXiv preprint arXiv:2304.07297, 2023.\n[14] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot\nlearning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):\n3019\u20133026, 2020.\n[15] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with\nlanguage models. arXiv preprint arXiv:2303.00001, 2023.\n[16] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable\nagent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,\n2018.\n[17] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning,\n2019.\n[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\n8\n[19] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh\nJayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design\nvia coding large language models. arXiv preprint arXiv:2310.12931, 2023.\n[20] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell. Zero-shot reward specification via\ngrounded natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepes-\nvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages\n14743\u201314752. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/\nmahmoudieh22a.html.\n[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n[22] Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej\nVecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. Data-efficient deep\nreinforcement learning for dexterous manipulation. arXiv preprint arXiv:1704.03073, 2017.\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In Marina\nMeila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR,\n18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a.html.\n[24] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Vision-\nlanguage models are zero-shot reward models for reinforcement learning. arXiv preprint\narXiv:2310.12921, 2023.\n[25] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander\nDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the\ngame of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, jan 2016.\nISSN 0028-0836.\n[26] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger.\nDefining and\ncharacterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460\u2013\n9471, 2022.\n[27] Sumedh A Sontakke, Jesse Zhang, S\u00e9bastien MR Arnold, Karl Pertsch, Erdem B\u0131y\u0131k, Dorsa\nSadigh, Chelsea Finn, and Laurent Itti. Roboclip: One demonstration is enough to learn robot\npolicies. arXiv preprint arXiv:2310.07899, 2023.\n[28] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distill-\ning internet-scale vision-language models into embodied agents. In Andreas Krause, Emma\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, edi-\ntors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pages 32797\u201332818. PMLR, 23\u201329 Jul 2023.\n[29] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,\nCambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.\n[30] Allison Tam, Neil Rabinowitz, Andrew Lampinen, Nicholas A Roy, Stephanie Chan, DJ Strouse,\nJane Wang, Andrea Banino, and Felix Hill. Semantic exploration from language abstractions\nand pretrained representations. Advances in Neural Information Processing Systems, 35:25377\u2013\n25389, 2022.\n[31] DeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico\nCarnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, Tim\nHarley, Felix Hill, Peter C Humphreys, Alden Hung, Jessica Landon, Timothy Lillicrap, Hamza\nMerzic, Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, Greg Wayne, Nathaniel\n9\nWong, Chen Yan, and Rui Zhu. Creating multimodal interactive agents with imitation and\nself-supervised learning, 2022.\n[32] Gerald Tesauro. Temporal difference learning and td-gammon. J. Int. Comput. Games Assoc.,\n18(2):88, 1995.\n[33] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali\nAhmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning\nplatform for android. arXiv preprint arXiv:2105.13231, 2021.\n[34] Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel,\nTom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks\nfor continuous control. Software Impacts, 6:100022, 2020.\n[35] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-\njtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds,\nDan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dal-\nibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor\nCai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen,\nDani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lil-\nlicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver.\nAlphaStar:\nMastering the Real-Time Strategy Game StarCraft II.\nhttps://deepmind.com/blog/\nalphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.\n[36] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman,\nSergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation\nwith vision-language models. arXiv preprint arXiv:2211.11736, 2022.\n[37] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang,\nand Tao Yu. Text2reward: Automated dense reward function generation for reinforcement\nlearning. arXiv preprint arXiv:2309.11489, 2023.\n[38] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-\nsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553, 2021.\n[39] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation\nusing vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\n[40] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-\ndation models for decision making: Problems, methods, and opportunities. arXiv preprint\narXiv:2303.04129, 2023.\n[41] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez\nArenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language\nto rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023.\n10\n"
  },
  {
    "title": "FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection",
    "link": "https://arxiv.org/pdf/2312.09252.pdf",
    "upvote": "9",
    "text": "FineControlNet: Fine-level Text Control for Image Generation with\nSpatially Aligned Text Control Injection\nHongsuk Choi*, Isaac Kasahara*, Selim Engin, Moritz Graule, Nikhil Chavan-Dafle, and Volkan Isler\n(* Indicates equal contributions)\nSamsung AI Center, New York\nInput\nFineControlNet (Ours)\nControlNet\nA firefighter on the \nleft and a woman in a \ngreen shirt on the \nright in a park\nAn orange on the left \nand a watermelon on \nthe right on a table\nPose\nSketch\nFigure 1. Our method, FineControlNet, generates images adhering to the user specified identities and setting, while maintaining the\ngeometric constraints. Existing methods, such as ControlNet, merge or ignore the appearance and identity specified in the prompt.\nAbstract\nRecently introduced ControlNet has the ability to steer\nthe text-driven image generation process with geometric\ninput such as human 2D pose, or edge features.\nWhile\nControlNet provides control over the geometric form of\nthe instances in the generated image, it lacks the capabil-\nity to dictate the visual appearance of each instance. We\npresent FineControlNet to provide fine control over each\ninstance\u2019s appearance while maintaining the precise pose\ncontrol capability.\nSpecifically, we develop and demon-\nstrate FineControlNet with geometric control via human\npose images and appearance control via instance-level text\nprompts.\nThe spatial alignment of instance-specific text\nprompts and 2D poses in latent space enables the fine\ncontrol capabilities of FineControlNet.\nWe evaluate the\nperformance of FineControlNet with rigorous comparison\nagainst state-of-the-art pose-conditioned text-to-image dif-\nfusion models. FineControlNet achieves superior perfor-\nmance in generating images that follow the user-provided\ninstance-specific text prompts and poses.\n1. Introduction\nText-to-image diffusion models have become a popular area\nof research.\nWith the release of production-ready mod-\nels such as DALL-E 2 [31] and Stable Diffusion [1, 32],\nusers are able to generate images conditioned on text that\ndescribes characteristics and details of instances and back-\nground. ControlNet [40] enabled finer grained spatial con-\ntrol (i.e., pixel-level specification) of these text-to-image\nmodels without re-training the large diffusion models on\ntask-specific training data. It preserves the quality and capa-\nbilities of the large production-ready models by injecting a\ncondition embedding from a separately trained encoder into\nthe frozen large models.\nWhile these models can incorporate the input text de-\nscription at the scene level, the user cannot control the gen-\nerated image at the object instance level. For example, when\nprompted to generate a cohesive image with a person of spe-\ncific visual appearance/identity on the left and a person of\na different appearance/identity on the right, these models\nshow two typical failures. Either one of the specified de-\nscriptions is assigned to both the persons in the generated\nimage, or the generated persons show visual features which\narXiv:2312.09252v1  [cs.CV]  14 Dec 2023\nappear as interpolation of both the specified descriptions.\nBoth of these peculiar failure modes can be seen in Fig-\nure 1. This lack of text-driven fine instance-level control\nlimits the flexibility a user has while generating an image.\nWe address this limitation by presenting a method, called\nFineControlNet, that enables instance-level text condition-\ning, along with the finer grained spatial control (e.g. human\npose). Specifically, we develop our method in the context\nof text-to-image generation with human poses as control in-\nput. However, note that the approach itself is not limited to\nthis particular control input as we demonstrate in our sup-\nplementary materials.\nGiven a list of paired human poses and appear-\nance/identity prompts for each human instance, FineCon-\ntrolNet generates cohesive scenes with humans with distinct\ntext-specified identities in specific poses. The pairing of ap-\npearance prompts and the human poses is feasible via large\nlanguage models [27, 38] (LLMs) or direct instance-specific\ninput from the user. The paired prompts and poses are fed to\nFineControlNet that spatially aligns the instance-level text\nprompts to the poses in latent space.\nFineControlNet is a training-free method that inherits the\ncapabilities of the production-ready large diffusion models\nand is run in end-to-end fashion. FineControlNet works by\ncareful separation and composition of different conditions\nin the reverse diffusion (denoising) process. In the initial\ndenoising step, the complete noise image is copied by the\nnumber of instances. Then, the noise images are processed\nby conditioning on separate pairs of text and pose controls\nin parallel, using the frozen Stable Diffusion and Control-\nNet. During the series of cross attention operations in Sta-\nble Diffusion\u2019s UNet, the embeddings are composited us-\ning masks generated from the input poses and copied again.\nThis is repeated for every denoising step in the reverse diffu-\nsion process. Through this latent space-level separation and\ncomposition of multiple spatial conditions, we can generate\nimages that are finely conditioned, both in text and poses,\nand harmonize well between instances and environment as\nshown in Figure 2.\nTo evaluate our method, we compare against the state-of-\nthe-art models for text and pose conditioned image gener-\nation. We demonstrate that FineControlNet achieves supe-\nrior performance in generating images that follow the user-\nprovided instance-specific text prompts and poses com-\npared with the baselines.\nOur key contributions are as follows:\n\u2022 We introduce a novel method, FineControlNet, which\ngives a user the ability to finely control the image gen-\neration. It fine controls generation of each instance in a\ncohesive scene context, using instance-specific geometric\nconstraints and text prompts that describe distinct visual\nappearances.\n\u2022 We create a curated dataset and propose new metrics that\nInput\nOutput\nA woman in an orange vest on the left and a man \nin a white baseball cap on the right in a canoe\nA cowboy on the left shaking hands with a knight \nin armor on the right at sunset\nFigure 2. Our FineControlNet generates images that ensure natu-\nral interaction between instances and environments, while preserv-\ning the specified appearance/identity and pose of each instance.\nfocus on the evaluation of fine-level text control on image\ngeneration. The curated dataset contains 1000+ images\ncontaining multiple (2 to 15) humans in different poses\nper scene from the MSCOCO dataset [21]. We label each\nhuman pose with appearance/identity description that will\nbe used for generation, along with a setting description of\nthe scene.\n\u2022 Finally, we demonstrate our method\u2019s ability to provide\nthe fine-grained text control against extensive state-of-\nthe-art baselines.\nOur FineControlNet shows over 1.5\ntimes higher text-image consistency metrics for distin-\nguished multiple human image generation. Furthermore,\nwe provide comprehensive qualitative results that support\nthe robustness of our method.\n2. Related work\nText-to-Image Models: Text-to-image models have be-\ncome a major area of research in the computer vision\ncommunity. AlignDRAW [23] introduced one of the first\nmodels to produce images conditioned on text. The field\ngained significant traction with the release of the visual-\nlanguage model CLIP [29] along with image generation\nmodel DALL-E [30]. Diffusion models [13, 35, 36] became\nthe design of choice for text-to-image generation, starting\nwith CLIP guided diffusion program [8], Imagen [34], and\nGLIDE [26]. Latent Diffusion Model [32], DALL-E 2 [31],\nand VQ-Diffusion [11] performed the diffusion process in\nlatent space which has become popular for computational\nefficiency. Due to the stunning quality of image genera-\ntion, large diffusion models trained on large-scale Internet\ndata [1, 24, 31] are now available as commercial products\nas well as sophisticated open-sourced tools.\nLayout- and Pose-conditioned Models: Generating im-\nages based on input types besides text is a popular con-\ncurrent research topic. Initial results from conditional gen-\nerative adversarial network (GAN) showed great potential\nfor downstream applications. Pix2Pix [14] could take in\nsketches, segmentation maps, or other modalities and trans-\nform them into realistic images. LostGAN [37] proposed\nlayout- and style-based GANs that enable controllable im-\nage synthesis with bounding boxes and category labels. Re-\ncently, as text-to-image diffusion models became more pop-\nular, models that could condition both on text and other ge-\nometric inputs were studied. LMD: LLM-grounded Diffu-\nsion [20], MultiDiffusion [3], eDiff-I [2], and GLIGEN [19]\ncondition the image generation on text as well as other 2D\nlayout modalities such as segmentation masks.\nThis al-\nlowed for text-to-image models to generate instances in spe-\ncific areas of the image, providing more control to the user.\nHowever, these methods are limited to positional con-\ntrol of individual instances, and did not extend to seman-\ntically more complex but spatially sparse modalities such\nas keypoints (e.g.\nhuman pose).\nFor example, human\npose involves more information than positional information,\nsuch as action and interaction with environment. GLIGEN\nshowed keypoints grounded generation, but it does not sup-\nport multiple instances\u2019 pose control along with instance-\nspecific text prompts. We analyze that it is mainly due to\nthe concatenation approach for control injection [19] that\ncannot explicitly spatially align the text and pose embed-\ndings as in our method.\nRecently, ControlNet [40] allowed the user to condition\nStable Diffusion [1] on both text as well as either seg-\nmentation, sketches, edges, depth/normal maps, and hu-\nman pose without corrupting the parameters of Stable Diffu-\nsion. Along with the concurrent work T2I [25], ControlNet\nsparked a large interest in the area of pose-based text-to-\nimage models, due to their high quality of generated human\nimages. HumanSD [16] proposed to fine-tune the Stable\nDiffusion using a heatmap-guided denoising loss. UniCon-\ntrol [28] and DiffBlender [17] unified the separate control\nencoders to a single encoder that can handle different com-\nbinations of text and geometric modalities, including human\npose. While these methods, including ControlNet, produce\nhigh quality images of multiple humans in an image, they\nlack the capability to finely dictate what individual human\nshould look like through a high level text description per hu-\nman. To address this limitation, we introduce FineControl-\nNet, a method for generation conditioned on text and poses\nthat has the capability to create images that are true-to-the-\nprompt at the instance-level as well as harmonized with the\noverall background and context described in the prompt.\n3. FineControlNet\nThe motivation for FineControlNet is to provide users with\ntext and 2D pose control beyond position for individual in-\nstances (i.e. human) during image generation. FineCon-\ntrolNet achieves this by spatially aligning the specific text\nembeddings with the corresponding instances\u2019 2D poses.\n3.1. Preliminary\nTo better highlight our method and contributions, we first\nexplain the key ideas of diffusion-based image generation.\nImage generation using probabilistic diffusion mod-\nels [26, 31] is done by sampling the learned distribution\np\u03b8(x0) that approximates the real data distribution q(x0),\nwhere \u03b8 is learnable parameters of denoising autoencoders\n\u03f5\u03b8(x). During training, the diffusion models gradually add\nnoise to the image x0 and produce a noisy image xt. The\ntime step t is the number of times noise is added and uni-\nformly sampled from {1, . . . , T}.\nThe parameters \u03b8 are\nguided to predict the added noise with the loss function\nLDM = Ex,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\ni\n.\n(1)\nDuring inference, the sampling (i.e. reverse diffusion) is\napproximated by denoising the randomly sampled Gaussian\nnoise xT to the image x0 using the trained network \u03f5\u03b8(x).\nConditional image generation is feasible via modeling\nconditional distributions as a form of p\u03b8(x0|c), where c\nis the conditional embedding that is processed from text\nor a task-specific modality. Recent latent diffusion meth-\nods [32, 40] augment the UNet [33]-based denoising au-\ntoencoders by applying cross attention between noisy image\nembedding zt and conditional embedding c. The network\nparameters \u03b8 are supervised as below:\nLLDM = Ex,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, ct, cf)\u22252\n2\ni\n,\n(2)\nwhere ct is a text embedding and cf is a task-specific em-\nbedding that is spatially aligned with an image in general.\n3.2. Spatial Alignment of Text and 2D Pose\nWhile the conditional image generation works reasonably\nwell at a global level, it becomes challenging when fine con-\ntrol over each of the instances with text prompts is desired.\nSince text is not a spatial modality that can be aligned with\nimage, it is ambiguous to distribute the text embeddings to\ncorresponding desired regions.\nWe formulate this text-driven fine control problem as\nspatially aligning instance-level text prompts to correspond-\ning 2D geometry conditions (i.e. 2D poses). Given a list\nof 2D poses {p2D\ni }N\n1 , we create a list of attention masks\n{mi}N\n1 , where N is the number of humans. We obtain these\nmasks by extracting occupancy from 2D pose skeletons and\nA soldier on the left, an astronaut\nin the middle, and a woman in a\nwhite dress on the right in an \nancient temple\nLLM- or user-based \nassociation of inputs\nFineControlNet\nA soldier in an ancient temple\nAn astronaut in an ancient temple\nA woman in a white dress in an ancient temple\n2D skeleton maps Occupancy maps Copied noise maps\nInput human poses and a text prompt\nFigure 3. Method Overview. Given a set of human poses as well as text prompts describing each instance in the image, we pass triplets\nof skeleton/mask/descriptions to FineControlNet. By separately conditioning different parts of the image, we can accurately represent the\nprompt\u2019s description of the appearance details, relative location and pose of each person.\nh\nhN\nh2\nh1\nLatent embedding in \nthe UNet feature level\ncopy\nPose-conditioned \ncontrol embeddings\nText embeddings describing\ndifferent identities\nPose-based attention masks\nQN\nQ2\nQ1\nV\u2019N\nV\u20192\nV\u20191\nKN\nK2\nK1\nVN\nV2\nV1\nCross attention\n\u2211\nOnly in decoding\nPose \nEncoder\nText \nEncoder\nBatch-wise sum\nWeighted sum\nh\u2019\nxT\nxt\nxt-1\nx0\n\u2026\n\u2026\nrepeat \nL times\ncf\nct\nm\nReverse diffusion process\nFigure 4.\nDuring the reverse diffusion process, FineControlNet\nperforms composition of different instances\u2019 embeddings at each\nof the L layers of UNet. Triplets of pose, text, and copy of the\nlatent embedding h are passed through each block of the UNet\narchitecture in parallel. The embeddings are composited after the\ncross attention using the normalized attention masks.\ndilating them with a kernel size of H/8, where H is the\nheight of the image. The occupancy maps are normalized\nby softmax and become attention masks {mi}N\n1 , where sum\nof mask values add up to 1 at every pixel.\nWe define the latent embedding h at each time step\nt, which collectively refers to the outputs of UNet cross-\nattention blocks, as composition of multiple latent embed-\ndings {hi}N\n1 :\nh = m1 \u00b7 h1 + m2 \u00b7 h2 + \u00b7 \u00b7 \u00b7 + mN \u00b7 hN ,\n(3)\nwhere hi embeds the ith instance\u2019s text condition in the\nencoding step and text and 2D pose conditions in the de-\ncoding step, and mi is a resized attention mask. Now, h\ncontains spatially aligned text embeddings of multiple in-\nstances. The detailed composition process is described in\nFigure 4. It graphically depicts how we implement equation\n(3) in a UNet\u2019s cross attention layer for text and 2D pose\ncontrol embeddings. In both encoding and decoding stages\nof UNet, copied latent embeddings {hi}N\n1 are conditioned\non instance-level text embeddings {ct}N\n1 by cross attention\nin parallel. In the decoding stage of UNet, instance-level\n2D pose control embeddings {cf}N\n1 are added to the copied\nlatent embeddings {hi}N\n1 before the cross attention.\nOur composition of latent embeddings is inspired by the\ninpainting paper Repaint [22]. It is a training-free method\nand performs composition of known and unknown regions\nof noisy image xt similar to ours. However, the composition\nin the latent space level is fundamentally more stable for a\ngeneration purpose. In each DDIM [36] step of the reverse\ndiffusion, xt\u22121 is conditioned on the predicted x0 as below:\nxt\u22121 = \u221a\u03b1t\u22121x0 +\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u00b7 \u03f5\u03b8(xt) + \u03c3t\u03f5t ,\n(4)\nx0 =\n\u0012xt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt)\n\u221a\u03b1t\n\u0013\n,\n(5)\nwhere \u03b1t is the noise variance at the time step t, \u03c3 ad-\njusts the stochastic property of the forward process, and \u03f5t is\nstandard Gaussian noise independent of xt. As shown in the\nabove equation, compositing multiple noisy images xt\u22121 as\nthat in inpainting literature is essentially targeting interpo-\nlation of multiple denoised images for generation. On the\ncontrary, the latent-level composition mathematically sam-\nples a unique solution from a latent embedding that encodes\nspatially separated text and pose conditions. Figure 5 sup-\nports the motivation of the latent-level composition.\nInput:\nA woman on the \nleft and a clown on \nthe right on a cliff\nOutput:\nComposition at latent \nembedding h\nComposition at noisy \nimage x\nFigure 5. Comparison between applying the composition step at\nthe noise image x vs. at the latent embedding h. Different from\nours, visual features of instances are blended in a generated image\ndue to the composition at x.\n3.3. Implementation of FineControlNet\nFineControlNet is a training-free method that is built\nupon pretrained Stable Diffusion v1.5 [1] and ControlNet\nv1.1 [40]. We implemented our method using ControlNet\u2019s\npose-to-image model.\nWe modified its reverse diffusion\nprocess for fine-level text control of multiple people at in-\nference time. If a single pair of text and pose is given, our\nmethod is the same with ControlNet. The whole process\nis run in an end-to-end fashion. We do not require a pre-\ndenoising stage to obtain fixed segmentation of instances as\nLMD [20] nor inpainting as post-processing for harmoniza-\ntion. The overall pipline of FineControlNet is depicted in\nFigure 3.\nPrompt parsing:\nOur method requires instance level\nprompts that describe the expected appearance of each hu-\nman. This differs slightly from competing methods, which\ngenerally only take in one prompt that describes the whole\nscene. While the user can manually prescribe each instance\ndescription to each skeleton (i.e. 2D pose) just as easily as\nwriting a global prompt, large language models (LLM) can\nalso be used as a pre-processing step in our method. If the\nuser provides a global level description of the image con-\ntaining descriptions and relative locations for each skele-\nton, many of the current LLM can take the global prompt\nand parse it into instance level prompts. Then given the\ncenter points of each human skeleton and the positioning\nlocation from the global prompt, an LLM could then assign\neach instance prompt to the corresponding skeleton. This\nautomates the process and allows for a direct comparison\nof methods that take in detailed global prompts and meth-\nods that take in prompts per skeleton. An example of such\nprocessing is included in Figure 3.\nHarmony: We provide users with harmony parameters in\naddition to the default parameters of ControlNet. Our text-\ndriven fine control of instances has a moderate trade-off be-\ntween identity instruction observance of each instance and\nthe overall quality for image generation. For example, if hu-\nman instances are too close and the resolutions are low, it is\nmore likely to suffer from identity blending as ControlNet.\nIn such examples, users can decrease the softmax temper-\nature of attention masks {mi}N\n1 , before normalization. It\nwill lead to better identity observance, but could cause dis-\ncordant with surrounding pixels or hinder the denoising pro-\ncess due to unexpected discretization error in extreme cases.\nAlternatively, users can keep the lower softmax temperature\nfor initial DDIM steps and revert it back to a default value.\nIn practice, we use 0.001 as the default softmax temperature\nand apply argmax on the dilated pose occupancy maps for\nthe first quarter of the entire DDIM steps.\n4. Experiment\nIn this section, we describe the set of experiments we con-\nducted to assess the performance of FineControlNet.\n4.1. Setting\nBaselines: For quantitative comparison, we chose the fol-\nlowing state-of-the-art models in pose and text conditioned\nimage generation: ControlNet [40], UniControl [28], Hu-\nmanSD [16], T2I [25], GLIGEN [19], and DiffBlender [17].\nThese models allow for the user to specify multiple skeleton\nlocations, as well as a global prompt. We convert the hu-\nman poses in our MSCOCO-based dataset to Openpose [5]\nformat for ControlNet, UniControl, and T2I, and convert\nto MMPose [7] for HumanSD. DiffBlender also allows the\nuser to specify bounding boxes with additional prompts per\nbounding box. For a fair comparison with our method, we\ninput instance-level descriptions for bounding boxes around\neach skeleton in a similar fashion to our method.\nDiff-\nBlender and GLIGEN only support square inputs, so scenes\nare padded before being passed to these models, while the\nrest of the models take the images in their original aspect ra-\ntio. All models are used with their default parameters when\nrunning on our benchmark.\nDataset: To evaluate the performance of our method and\nof the baselines at generating the images with multiple peo-\nple using fine-grained text control, we introduce a curated\ndataset.\nThis dataset contains over one thousand scenes\nwith 2+ human poses per scene, extracted from the vali-\ndation set of MSCOCO dataset [21]. The total number of\nimages is 1,126 and the total number of persons is 4,466.\nFor the text annotation of the sampled data, we generated\na pool of 50+ instance labels that describe a single person\u2019s\nappearance, and a pool of 25+ settings that describe the con-\ntext and background of the scene. We randomly assign each\nhuman pose an instance-level description, and each scene\na setting description. We also include a global description\nthat contains all of the instance descriptors with their rel-\native positions described in text along with the setting de-\nTable 1. Comparison with state-of-the-art pose and text-conditioned diffusion models. FineControlNet demonstrates superior scores in\nour CLIP Identity Observance (CIO) metrics and is competitive in the rest of the metrics with the baselines. The CIO metrics measure the\naccuracy of each instance\u2019s appearance with relation to the prompt.\nMethods\nImage Quality\nCLIP Identity Observance (CIO)\nPose Control Accuracy\nHND\u2193\nFID\u2193\nCIOsim \u2191\nCIO\u03c3 \u2191\nCIOdiff \u2191\nAP\u2191\nAPM\u2191\nAPL\u2191\nControlNet [40]\n5.85\n23.0\n0.34\n1.4\u00b11.5\n56.1\n10.2\n60.6\n5.4\u00b16.0\nDiffBlender [17]\n3.93\n23.0\n0.35\n1.4\u00b11.4\n20.9\n0.0\n21.6\n1.6\u00b11.8\nGLIGEN [19]\n4.02\n22.2\n0.34\n1.0\u00b11.2\n64.9\n5.4\n67.5\n1.9\u00b12.6\nHumanSD [16]\n5.77\n22.8\n0.34\n1.1\u00b11.2\n75.5\n32.0\n77.1\n2.2\u00b12.5\nUniControl [28]\n4.10\n23.4\n0.34\n1.4\u00b11.5\n55.1\n9.6\n58.4\n5.5\u00b15.9\nT2I [25]\n10.30\n23.1\n0.34\n1.4\u00b11.5\n58.3\n14.1\n62.1\n5.8\u00b16.5\nFineControlNet(Ours)\n4.05\n24.2\n0.56\n2.9\u00b12.3\n63.2\n16.7\n65.9\n2.4\u00b13.1\nscriptor. This global description is used for baselines that\ncan only be conditioned given a single prompt.\nMetrics: The goal of our experiments is to evaluate our\nmodel\u2019s ability to generate cohesive and detailed images,\ngenerating pose-accurate humans, and allow for instance-\nlevel text control over the humans in the scene.\nWe report the Fr\u00b4echet Inception Distance (FID) [12] met-\nric to measure the quality of the generated images, using the\nvalidation set of HumanArt [15] as the reference dataset.\nFor measuring the text-image consistency between the\ngenerated image and the input text prompts, we introduce a\nset of new metrics called CLIP Identity Observance (CIO),\nbased on CLIP [29] similarity scores at the instance level.\nThe first variant of this metric, CIOsim, computes the simi-\nlarity of text and image embeddings using the instance de-\nscriptions and the local patches around the input human\nposes. While CIOsim evaluates the similarity of the gen-\nerated image and the text prompt, it does not accurately\nmeasure the performance on synthesizing distinct identities\nfor each instance. To address this limitation, we introduce\nCIO\u03c3 and CIOdiff that measure how distinctly each instance\ndescription is generated in the image.\nGiven a local patch I around an instance from the image\nand a set of text prompts P that describe all the instances,\nCIO\u03c3 computes a softmax-based score,\nCIO\u03c3 =\nexp{CLIP(I, P \u2217)}\nP\nP \u2208P exp{CLIP(I, P)}\n(6)\nwhere P \u2217 is the text prompt corresponding to the in-\nstance in the local patch. For the next metric, we compute\nthe difference between the CLIP similarities of P \u2217 com-\npared to text prompts describing other instances in the im-\nage, and define CIOdiff as,\nCIOdiff = CLIP(I, P \u2217) \u2212\nX\nP \u2208P\u2212{P \u2217}\nCLIP(I, P)\n|P|\u22121\n(7)\nTo evaluate the pose control accuracy of methods, we\ntest HigherHRNet [6] on our benchmark following Hu-\nmanSD [16]. HigherHRNet is the state-of-the-art 2D pose\nestimator, and the weights are trained on MSCOCO and\nHuman-Art [15] by the authors of HumanSD. We report\nthe average precision (AP) of Object Keypoint Similarity\n(OKS) [21] measured in different distance thresholds. The\nsuperscript categorizes the resolution of people in an image\nand measures the average precision only for persons in that\ncategory, where M and L denote medium and large respec-\ntively. Note that these metrics are pseudo metrics, because\nthey are susceptible to inaccuracies of the 2D pose estimator\nindependent from the inaccuracy of image generation.\n4.2. Comparison with State-of-the-Art Methods\nWe conduct quantitative analysis based on our benchmark\nand provide an extensive qualitative comparison.\nQuantitative Analysis:\nWe evaluate methods on our\nbenchmark dataset and assess the overall image quality,\nidentity observance, and pseudo pose control accuracy of\ngenerated images using the metrics described in section 4.1.\nTable 1 shows the results from running our method as well\nas the six baselines.\nWe use FID metric as an overall measure of the qual-\nity that each method produces. We found that while Diff-\nBlender and GLIGEN achieved the best results in this cate-\ngory, our method is within the top half of the baselines.\nFor pose control accuracy evaluation, we report over-\nall AP as well as AP with only medium-sized skeletons\nand only large-sized skeletons. Our method performs ro-\nbustly in these categories, only behind HumanSD and com-\nparable to GLIGEN. We believe the strong performance\nof HumanSD in APs is due to the the training bias of the\ntested HigherHRNet [6]. The HigherHRNet is trained on\nHuman-Art dataset for 2D pose estimation. HumanSD uses\nthe same training dataset for image generation, while other\nmethods are not explicitly trained on this dataset.\nThe Human Number Difference (HND) metric reports\non average the difference between the number of ground\ntruth skeletons in the input vs. the number of detected skele-\ntons on the generated image. We find our method performs\nOurs\nOurs\nOurs\nControlNet\nDiffBlender\nHumanSD\nGLIGEN\nUniControl\nT2I\nOurs\nOurs\nOurs\nControlNet\nDiffBlender\nHumanSD\nGLIGEN\nUniControl\nT2I\nInput\nFineControlNet (Ours)\nBaselines\nA woman in a beige skirt on the left and a man \nwith an olive green sweater on the right in a \npark\nA man with a pink jacket on the left, a man with \na green jacket in the middle, and a scientist on \nthe right inside a high-tech laboratory\nA woman in a beige skirt on the left and robin \nhood on the right on the surface of the moon\nA man wearing a white shirt on the left, a man \nin a teal jacket in the middle, and a man \nwearing a maroon shirt on the right on a beach\nAn astronaut on the left, and a woman in a \nyellow shirt on the right at a festival\nA man in a navy blue shirt on the left, a woman \nin a beige skirt in the middle, and a man in a \nnavy blue shirt on the right in a museum.\nFigure 6. Qualitative results comparing FineControlNet against six state-of-the-art baselines. The input description and pose conditions\nare provided on the left side. FineControlNet consistently follows the user specified appearance prompts better in its image generations.\nadequately here, outperforming ControlNet, T2I, and Uni-\nControl but underperforming against the other baselines.\nThe metric CLIP Identity Observance (CIO) is most rel-\nevant to the problem that we are targeting within this paper.\nThis metric measures the instance description against the\nimage patch that is supposed to match that description us-\ning a pretrained CLIP model [29]. CIOsim does this directly,\nand we find that our method outperforms all the baselines\nin loyalty to the prompt. To further understand how much\nblending of different visual features happens for each meth-\nods outputs, we introduce the metrics CIO\u03c3 and CIOdiff.\nThese metrics not only compare against the ground truth\ninstance description, but also punish generated instances if\nthey match other instances descriptions. For example if the\ninstance label is \u201cAstronaut\u201d and the other instance in the\nimage is \u201cSoldier\u201d, the image will be punished if CLIP de-\ntects similarity between the image crop of an astronaut with\nthe text \u201cSoldier\u201d. We found that our method strongly out-\nperforms the baselines with these metrics, further demon-\nstrating the effectiveness of using our reverse diffusion pro-\ncess conditioning individual instances only with their re-\nspective descriptions. We found that since DiffBlender is\nalso given instance descriptions assigned to the skeleton lo-\ncations in the form of bounding boxes, it came in second\nplace overall in CIO. However it was not immune to blend-\ning features as seen in our qualitative results section.\nQualitative Analysis: We generate a full page of qualita-\ntive results from our benchmark test in Figure 6 to demon-\nstrate visually the difference between our method and the\nbaselines. We also provide the input skeletons and prompt\nfor the reader to compare how closely each method\u2019s images\nadhere to the description of the scene. We find that while all\nthe methods can produce visually pleasing results and can\nincorporate the words from the description into the image,\nonly ours is reliably capable of adhering to the prompt when\nit comes to the human poses/descriptions of each instance.\nFor example, ControlNet tends to blend or ignore fea-\ntures from all given text descriptions for example ignoring\n\u201drobin hood\u201d and only creating \u201dwoman in a beige skirt\u201d.\nGLIGEN also struggles to maintain identities and seems to\naverage the different instance descriptions.\nDiffBlender could sometimes distinguish instances into\nthe correct locations, but would struggle adhering to both\nthe pose condition and the text condition at the same time.\nUnicontrol appeared to focus on certain parts of the prompt,\ne.g. \u201cpink jacket\u201d, and use them to generate every instance.\nFinally, HumanSD and T2I both suffer from the same\nissues of blending visual features between instances. Com-\nparing the baselines to our method, we can see clear im-\nprovement in maintaining identity and characteristics from\nthe input description.\nTable 2.\nAblation on level of composition. While our ablation\nusing embedding x produced higher pose scores and our ablation\nh-v2 produced higher CIO scores, we found that using embedding\nh struck a good balance between pose and appearance control.\nembedding\nFID\u2193\nCIO\u03c3 \u2191\nCIOdiff \u2191\nAP\u2191\nx\n2.86\n0.35\n1.39\n68.0\nh-v2\n3.36\n0.60\n3.20\n45.7\nh (Ours)\n4.05\n0.56\n2.91\n63.2\n4.3. Ablation Study\nWe study alternative ways of composition of different in-\nstances\u2019 conditions. First, we perform the composition in\nthe level of denoised image x. Second, we modify the cur-\nrent composition in the level of latent embedding h, which\nwe name h-v2 in Table 2. We apply the composition of dif-\nferent pose embeddings before the cross attention and add\nthem to {hi}N\n1 . The pre-composition before the cross at-\ntention is repeated for every decoding step of UNet. The\nfinal output of the UNet is then composed using the atten-\ntion masks {mi}N\n1 .\nAs shown in Table 2, the composition of x presents\n37.5% lower CIO\u03c3, while the pose control accuracy in-\ncreases only by 7.9% .\nThe quantitative results support\nour statement that it is essentially targeting interpolation of\nmultiple denoised images for generation. The results are\nalso aligned with observation in Figure 5. Our modified\ncomposition in h level shows the slightly better accuracy\nin CIO, but gives the worst pose control accuracy that is\n27.7% lower than ours. We conjecture that the composition\nof pose embeddings prior to the cross attention weakens the\nindividual instance\u2019s control signal after the attention, due\nto the distribution of attention to other instances\u2019 poses.\n5. Conclusion\nWe introduced FineControlNet, a novel method to finely\ncontrol instance level geometric constraints as well as\nappearance/identity details for text-to-image generation.\nSpecifically, we demonstrated the application for generating\nimages of humans in specific poses and of distinct appear-\nances in a harmonious scene context. FineControlNet de-\nrives its strength from the spatial alignment of the instance-\nlevel text prompts to the poses in latent space. During the\nreverse diffusion process, the repeated composition of em-\nbeddings of spatial-geometric and appearance-text descrip-\ntions leads to a final image that is conditioned on text and\nposes, and is consistent with the overall scene description.\nTo evaluate the performance of FineControlNet and\ncomparable baselines for pose-conditioned text-to-image\ngeneration, we introduced a curated benchmark dataset\nbased off of the MSCOCO dataset.\nWith qualitative\nand quantitative analysis, we observed that FineControl-\nNet demonstrated superior performance on instance-level\ntext-driven control compared with the state-of-the-art base-\nlines. FineControlNet provides the enhanced control over\nthe form and appearance for image generation, pushing the\nfrontiers of text-to-image generation capabilities further.\nReferences\n[1] Stability AI.\nStable diffusion v1.5 model card.\nhttps : / / huggingface . co / runwayml /\nstable-diffusion-v1-5/, 2022. 1, 2, 3, 5\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash\nVahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. ed-\niffi: Text-to-image diffusion models with an ensemble\nof expert denoisers. arXiv preprint arXiv:2211.01324,\n2022. 3\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali\nDekel. Multidiffusion: Fusing diffusion paths for con-\ntrolled image generation. In ICML, 2023. 3\n[4] John Canny. A computational approach to edge detec-\ntion. TPAMI, 1986. 11\n[5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei,\nand Yaser Sheikh. Openpose: Realtime multi-person\n2d pose estimation using part affinity fields, 2019. 5\n[6] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui\nShi, Thomas S Huang, and Lei Zhang. Higherhrnet:\nScale-aware representation learning for bottom-up hu-\nman pose estimation. In CVPR, 2020. 6\n[7] MMPose Contributors. Openmmlab pose estimation\ntoolbox and benchmark. https://github.com/\nopen-mmlab/mmpose, 2020. 5\n[8] K. Crowson. Clip guided diffusion hq 256x256, 2021.\n2\n[9] Hugging Face. Diffusers multicontrolnet. https:\n//github.com/huggingface/diffusers/\ntree/multi_controlnet, 2023. 14, 16\n[10] Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-\nHyun Lee, Jingeun Lee, and Minchul Shin. Towards\nlight-weight and real-time line segment detection. In\nAAAI, 2022. 11\n[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen,\nBo Zhang, Dongdong Chen, Lu Yuan, and Baining\nGuo.\nVector quantized diffusion model for text-to-\nimage synthesis. In CVPR, 2022. 2\n[12] Martin Heusel,\nHubert Ramsauer,\nThomas Un-\nterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge\nto a local nash equilibrium. Advances in neural infor-\nmation processing systems, 30, 2017. 6\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. In NeurIPS, 2020. 2\n[14] Phillip Isola,\nJun-Yan Zhu,\nTinghui Zhou,\nand\nAlexei A Efros. Image-to-image translation with con-\nditional adversarial networks. In CVPR, 2017. 3\n[15] Xuan Ju, Ailing Zeng, Wang Jianan, Xu Qiang, and\nZhang Lei.\nHuman-art: A versatile human-centric\ndataset bridging natural and artificial scenes. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), year=2023.\n6\n[16] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang,\nLei Zhang, and Qiang Xu.\nHumansd:\nA native\nskeleton-guided diffusion model for human image\ngeneration. arXiv preprint arXiv:2304.04269, 2023.\n3, 5, 6\n[17] Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik\nKim, and Namhyuk Ahn. Diffblender: Scalable and\ncomposable multimodal text-to-image diffusion mod-\nels. arXiv preprint arXiv:2305.15194, 2023. 3, 5, 6\n[18] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-\nShu Fang, and Cewu Lu.\nCrowdpose:\nEfficient\ncrowded scenes pose estimation and a new bench-\nmark. In CVPR, 2019. 15\n[19] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou\nMu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. pages 22511\u201322521, 2023. 3, 5, 6\n[20] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell.\nLlm-grounded diffusion: Enhancing prompt under-\nstanding of text-to-image diffusion models with large\nlanguage models. arXiv preprint arXiv:2305.13655,\n2023. 3, 5\n[21] Tsung-Yi Lin,\nMichael Maire,\nSerge Belongie,\nLubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Pi-\notr Doll\u00b4ar. Microsoft coco: Common objects in con-\ntext, 2015. 2, 5, 6\n[22] Andreas Lugmayr, Martin Danelljan, Andres Romero,\nFisher Yu, Radu Timofte, and Luc Van Gool.\nRe-\npaint: Inpainting using denoising diffusion probabilis-\ntic models. In CVPR, 2022. 4\n[23] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba,\nand Ruslan Salakhutdinov. Generating images from\ncaptions with attention. In ICLR, 2016. 2\n[24] Midjourney.\nMidjourney.\nhttps : / / www .\nmidjourney.com/, 2023. 2\n[25] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,\nZhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-\nadapter: Learning adapters to dig out more control-\nlable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023. 3, 5, 6\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photore-\nalistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741,\n2021. 2, 3\n[27] OpenAI.\nChatgpt.\nhttps://openai.com/\nblog/chatgpt/, 2022. 2\n[28] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi\nYang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,\nCaiming Xiong, Silvio Savarese, et al. Unicontrol: A\nunified diffusion model for controllable visual gener-\nation in the wild. arXiv preprint arXiv:2305.11147,\n2023. 3, 5, 6\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from nat-\nural language supervision. In ICML, 2021. 2, 6, 8\n[30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In\nICML, 2021. 2\n[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,\nCasey Chu, and Mark Chen.\nHierarchical text-\nconditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125, 2022. 1, 2, 3\n[32] Robin\nRombach,\nAndreas\nBlattmann,\nDominik\nLorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In CVPR, 2022. 1, 2, 3, 15\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, 2015. 3\n[34] Chitwan Saharia, William Chan, Saurabh Saxena,\nLala Li,\nJay Whang,\nEmily L Denton,\nKam-\nyar Ghasemipour, Raphael Gontijo Lopes, Burcu\nKaragol Ayan, Tim Salimans, et al. Photorealistic text-\nto-image diffusion models with deep language under-\nstanding. In NeurIPS, 2022. 2\n[35] Jascha\nSohl-Dickstein,\nEric\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics.\nIn\nICML, 2015. 2\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 2, 4\n[37] Wei Sun and Tianfu Wu. Image synthesis from recon-\nfigurable layout and style. In ICCV, 2019. 3\n[38] Hugo Touvron,\nThibaut Lavril,\nGautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al.\nLlama:\nOpen and effi-\ncient foundation language models.\narXiv preprint\narXiv:2302.13971, 2023. 2\n[39] Saining Xie and Zhuowen Tu.\nHolistically-nested\nedge detection. In ICCV, 2015. 11\n[40] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image diffusion\nmodels. In ICCV, 2023. 1, 3, 5, 6, 11, 14, 15, 16, 17,\n18\nSupplementary Material of\nFineControlNet: Fine-level Text Control for Image Generation with\nSpatially Aligned Text Control Injection\nHED\nM-LSD\nCanny Edge\nSketch\nA ___ helicopter and a ___ \nsports car ____.\nA ___ desk and a ___ \nbookshelf in a ____.\nA ___ and a ___ in ____.\nA ___ and a ___ in ____.\nblack, grey, in a snowstorm\nempty, cluttered, study\nyellow cat, black dog, a park\nbaseball, soccer ball, the grass\ngreen, blue, on a cliffside\nwhite, darkwood, landfill\nblack cat, white dog, the snow\norange, watermelon, a table\npolice, purple, in city at night\nblack, white, medieval castle\ntiger, wolf, the jungle\nmoon, earth, outer space\nFigure 7. Results of FineControlNet applied to different control modalities of Canny [4] edges, M-LSD [10] lines, HED [39] edges, and\na sketch input. As shown above our method has the ability to not only work on human pose inputs, but other modalities as well using the\nsame approach described in our method section but applied to different ControlNet[40] models. Under each column is the modality name,\nthe sample input image, the prompt template, and three examples images with the corresponding input prompt information. Our method\ndemonstrates the ability to finely control each instance.\nIn this supplementary material, we present more ex-\nperimental results that could not be included in the main\nmanuscript due to the lack of space.\n6. Different Control Modality\nWe present results demonstrating the efficacy of our\nFineControlNet architecture using various geometric con-\ntrol modalities, including Canny [4] edges, M-LSD [10]\nlines, HED [39] edges, and sketch inputs. As illustrated\nin Figure 7, our framework enables fine-grained text-based\ncontrol over individual instances while maintaining coher-\nence across the generated scene. Through spatially aligned\ntext injection, each instance faithfully reflects the corre-\nsponding textual prompt, with harmonized style and light-\ning that is consistent both within and between instances. For\nexample, the bottom left image generated from the prompt\nTable 3. Robustness Study regarding factors of \u201cnumber of people\u201d, \u201cscale of a person\u201d, and \u201cdistance between people\u201d.\nMetrics\nNumber of People\nScale of a Person\nDistance between People\n3\n5\n7\n1\n0.75\n0.5\n0.25\n0.1\n1\n0.75\n0.5\n0.25\nCIOsim \u2191\n28.2\n26.9\n26.5\n28.2\n27.5\n26.4\n23.2\n20.3\n28.2\n27.8\n27.8\n25.3\nCIO\u03c3\n0.74\n0.46\n0.32\n0.74\n0.69\n0.62\n0.55\n0.42\n0.74\n0.7\n0.69\n0.48\nCIOdiff \u2191\n5.3\u00b12.4 3.2\u00b11.9 2.2\u00b11.3 5.3\u00b12.4 4.6\u00b12.5 3.6\u00b11.9 2.0\u00b11.4 0.9\u00b10.7 5.3\u00b12.4 4.8\u00b12.3 4.6\u00b12.6 2.2\u00b11.3\nA woman in a silver dress on the left, a\nknight in armor in the middle, and a judge on the right on a cliff\nA woman with a turquoise dress on the left \nand a racecar driver on the right in the desert\nFrom left to right: A man wearing a maroon shirt, a racecar driver , a detective,\na woman wearing a black dress, and a woman wearing a white dress on a cliff\nFrom left to right: A detective, a woman in an orange shirt, a woman in a light blue shirt, a man wearing a maroon shirt, \na woman wearing a black dress,  a man wearing a yellow sweater, a woman in a pink dress on a rooftop\nFigure 8. Qualitative results depending on the number of people, which is the number of 2D poses given. Every 2D human pose in the\nentire figure has the same resolution. The input skeleton map with 7 poses is resized to match the page.\n\u201cA police helicopter and a purple sports car in city at night\u201d\nsupports these claims; both vehicles exhibit glossy textures\nand lighting congruent with the nocturnal urban setting.\n7. How Robust is FineControlNet?\nWe analyze the robustness of FineControlNet to variations\nin number of people, scale, and inter-personal distance.\nQuantitative experiments recording CLIP Identity Obser-\nvance (CIO) scores (Table 3) and qualitative results (Fig-\nures 8-10) demonstrate performance under differing condi-\ntions.\nVarying the number of input 2D poses while fixing scale\nand spacing reveals strong text-image consistency for 2-3\npeople, with gradual degradation as count increases to 5 and\n7 (Table 3; Figure 8). For instance, the fourth person from\nthe left in Figure 8 fails to wear the prompted dress, illus-\ntrating compromised identity observance. We posit that as\ninstance count rises, pressure to balance identity adherence\nagainst holistic visual harmonization becomes more severe,\nincreasing feature sharing between instances.\nExperiments assessing robustness to variations in human\nscale utilize three input poses while fixing inter-personal\ndistances. As depicted in Figure 9 and Table 3, identity\nobservance degrades gradually with increased downscaling,\ntied to losses in latent feature resolution. Performance re-\nmains reasonable down to 50% scale, with more significant\ndrops emerging under extreme miniaturization. Note input\npose map resolution is constant at 512 pixels in height.\nSimilarly, distance experiments alter spacing around a\ncentral pose with three total people at fixed scale. Results\nin Figure 10 and Table 3 demonstrate consistent identity\nretention given non-overlapping inputs, with overlap intro-\nducing instance dropping or blending.\nTogether, these analyses quantify trade-offs between fi-\nA woman in a pink dress on the left, a knight in armor \nin the middle, and a king with a crown on a ship\nA woman in a silver dress on the left, a knight in armor\nin the middle, and a judge on the righton a cliff\nFrom top to bottom, scales of 2D pose skeletons \nare 1.0, 0.75, 0.5, 0.25, and 0.1\nFigure 9. Qualitative results depending on the scale of a person, which represents the relative resolution of each pose in the input. We used\nthe same seed for image generation for every scale variation.\nA racecar driver on the left, an astronaut in the middle, \nand a woman wearing a black dress on the right at a bar\nA woman in a silver dress on the left, a knight in armor\nin the middle, and a judge on the righton a cliff\nFrom top to bottom, distances between 2D pose \nskeletons are 1.0, 0.75, 0.5, 0.25 in normalized scale\nFigure 10. Qualitative results depending on the distance between people. Closer distance could cause blending between different instances\u2019\ntext embeddings and generate mixed appearance of instances. We used the same seed for image generation for every inter-personal distance\nvariation.\ndelity and spatial configurations. Performance gracefully\nhandles reasonable perturbations but breaks down at data\ndistribution extremes. Addressing such generalization lim-\nits highlights an area for further improvement.\n8. Difference with MultiControlNet\nWe compare FineControlNet to MultiControlNet [40], an\nextension of ControlNet supporting multiple geometric\nmodalities (e.g.\npose, depth) with a single text prompt.\nFor equivalence, we modify MultiControlNet to condition\non instance-specific texts over multiple poses. Experiments\nutilize a third-party HuggingFace Diffusers [9] implementa-\ntion. Results in Figure 12 demonstrate compromised adher-\nence to per-instance textual prompts compared to FineCon-\ntrolNet, stemming from lack of spatial text-latent alignment\nand careful latent composition. Moreover, MultiControl-\nNet fails to process more than two inputs, generating blurry\nNumber of persons per image\nRatio value per person\nFigure 11. Statistics of our curated dataset. The y-axis indicates\nthe counts that fall in bins in the x-axis.\nand abstract imagery. These contrasts highlight the impor-\ntance of FineControlNet\u2019s spatially aware text injection and\ncarefully engineered latent fusion for fine-grained, multi-\ninstance control.\n9. More Qualitative Results\nAdditional qualitative results of FineControlNet\u2019s ability\nto address instance-specific constraints are shown in Fig-\nures 13 and 14. The input poses and prompts are shown\nin the leftmost columns and at the bottom of each row of\nimages, respectively.\nThe results of FineControlNet are\nprovided in the middle two columns, with and without the\nposes overlaid on the generated images. We also show the\noutputs of ControlNet [40] using the same pair of input\nposes and text prompts as a reference for comparison in the\nrightmost columns. For both methods, we use the same seed\nnumbers which are sampled from a uniform distribution.\n10. Limitations\nDespite showing promising results, our method can some-\ntimes suffer from several failure modes, which include: 1)\ninstance-specific controls being affected by the setting de-\nscription, 2) human faces synthesized with poor quality, 3)\nimplausible environments for the specified poses, and 4)\nmisaligned input poses and generated images. The results\nof FineControlNet showing these failures are presented in\nFigure 15.\nWe observe that instance controls may get altered by the\ntext prompt for the setting, especially in environments with\nsmall diversity of instances in the training dataset of im-\nages used for Stable Diffusion [32]. In addition, similar to\nControlNet [40], our method can synthesize human faces\nthat look unrealistic. We also can see unrealistic pairings\nof instances and environments in some of the generated im-\nages by FineControlNet. Even when satisfying the instance\nand setting specifications separately, our method can gen-\nerate physically implausible scenes, such as floating peo-\nple, as it does not have an explicit mechanism that prevents\nfrom doing so. Finally, FineControlNet can generate images\nwhose poses are misaligned or with bad anatomy, particu-\nlarly when the input poses are challenging.\n11. Dataset\nWe provide the histograms of numbers of people per im-\nage, person\u2019s bounding box resolution per image area ra-\ntio, and CrowdIndex [18] in Figure 11, for our curated\ndataset. CrowdIndex computes the ratio of the number of\nother persons\u2019 joints against the number of each person\u2019s\njoints. Higher CrowdIndex indicates higher chance of oc-\nclusion and interaction between people. The low resolu-\ntion ratio and the higher CrowdIndex are related to the dif-\nficulty of identity and pose control due to discretization in\nlatent space and ambiguity of instance assignment in atten-\ntion masks.\nInput human poses\nFineControlNet (Ours)\nMultiControlNet\nA woman in a silver dress on the left and a man with a green jacket on the right in Times Square\nOurs with poses\nA woman in a yellow shirt on the left and a woman in a pink dress on the right in an ancient temple\nA chef on the left, a detective in the middle, and a woman in a silver dress on the right in Times Square\nA man in a grey shirt on the left and a woman wearing a white dress on the right in a dense forest\nFigure 12. Comparison between our FineControlNet and MultiControlNet [9, 40]. MultiControlNet produces blurry images, which also\nhave blended appearance/identity between instances. In addition, more than two geometric control inputs paired with different text prompts\noften cause a complete failure. We provide the images of poses overlaid on FineControlNet\u2019s generated outputs for reference.\nA police officer on the left and a woman in a yellow shirt on the right in a lab\nInput human poses\nFineControlNet (Ours)\nOurs with poses\nA man in a turquoise jacket on the left, a man in a purple shirt in the middle, and \na woman in a black dress on the right in a forest\nFrom left to right: A detective, a racecar driver, a librarian, and an astronaut at sunset\nA man in a white shirt on the left, a woman in a turquoise dress in the middle, \nand a construction worker on the right in a lab\nA woman in a light blue shirt on the left, a woman in a green dress in the middle, \nand a man in a pink jacket on the right in a forest\nA woman in a green dress on the left and a man in a white shirt on the right at a wedding\nControlNet\nFigure 13. Additional supplementary results demonstrating our method\u2019s ability to finely control each instance in the image. We show\nthe input poses (left) and prompt (bottom) along with the results from our method with and without overlaid poses (middle), and Control-\nNet\u2019s [40] output with the same text prompt (right) for comparison.\nInput human poses\nFineControlNet (Ours)\nOurs with poses\nA knight in armor on the left and a ballerina on the right in a park\nA scientist on the left, a man in a maroon shirt in the middle, and a sailor on the right in a park\nA woman in a light blue shirt on the left, a man in a white shirt in the middle, \nand a woman in a silver dress on the right in a park\nControlNet\nA woman in a silver dress on the left and a man in a pink jacket on the right in a parking lot\nFigure 14. Additional supplementary results demonstrating our method\u2019s ability to finely control each instance in the image. We show\nthe input poses (left) and prompt (bottom) along with the results from our method with and without overlaid poses (middle), and Control-\nNets\u2019s [40] output with the same text prompt (right) for comparison.\nA man in a pink jacket on the left, a man in a green sweater in the middle, and \na construction worker on the right on the moon\nInput human poses\nFineControlNet (Ours)\nOurs with poses\nA librarian on the left and a chef on the right in a forest\nA woman in a yellow shirt on the left and a woman in a white dress on the right in a museum\nA man in a purple shirt on the left, a ballerina in the middle, and a ballerina on the right at a birthday party\nFailure Case\n1. Instances \ninfluenced by \nsetting\n2. Poor face \ngeneration \nquality\n3. Unrealistic \nenvironments \nfor pose\n4. Misaligned \nwith pose or \nbad anatomy\nFigure 15. Failure cases. We demonstrate possible failure cases of FineControlNet that will be further studied in future work.\n"
  },
  {
    "title": "ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks",
    "link": "https://arxiv.org/pdf/2312.08583.pdf",
    "upvote": "9",
    "text": "ZeroQuant(4+2): Redefining LLMs Quantization with a New\nFP6-Centric Strategy for Diverse Generative Tasks\nXiaoxia Wu\u2217, Haojun Xia\u2020, Stephen Youn, Zhen Zheng, Shiyang Chen\u2021, Arash Bakhtiari,\nMichael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase,\nLeon Song\u2217, Zhewei Yao\nDeepSpeed of Microsoft\nAbstract\nThis study examines 4-bit quantization methods like GPTQ in large language models (LLMs),\nhighlighting GPTQ\u2019s overfitting and limited enhancement in Zero-Shot tasks. While prior works merely\nfocusing on zero-shot measurement, we extend task scope to more generative categories such as code\ngeneration and abstractive summarization, in which we found that INT4 quantization can significantly\nunderperform. However, simply shifting to higher precision formats like FP6 has been particularly\nchallenging, thus overlooked, due to poor performance caused by the lack of sophisticated integration\nand system acceleration strategies on current AI hardware. Our results show that FP6, even with a\ncoarse-grain quantization scheme, performs robustly across various algorithms and tasks, demonstrating\nits superiority in accuracy and versatility. Notably, with the FP6 quantization, StarCoder-15B model\nperforms comparably to its FP16 counterpart in code generation, and for smaller models like the 406M\nit closely matches their baselines in summarization.\nNeither can be achieved by INT4.\nTo better\naccommodate various AI hardware and achieve the best system performance, we propose a novel 4+2\ndesign for FP6 to achieve similar latency to the state-of-the-art INT4 fine-grain quantization. With our\ndesign, FP6 can become a promising solution to the current 4-bit quantization methods used in LLMs.1\n1\nIntroduction\nLarge Language Models (LLMs) such as GPT-3 [5] have significantly advanced the field of natural language\nprocessing. These models have shown exceptional capabilities in various complex tasks, from text generation\nto language understanding. However, the widespread adoption of LLMs is challenged by their extensive\ncomputational and memory demands. This issue is particularly acute in resource-constrained environments,\nwhere deploying such large models is not feasible. To mitigate these challenges, post-training quantization\nhas been recognized as a crucial technique [6, 20, 46, 41]. It enables the compression of these models for\nefficient utilization in limited-resource settings without the need for extensive retraining. Nevertheless, this\napproach often necessitates a balance between reducing the model size and maintaining accuracy [14].\nRecent developments in the field of quantization, particularly in 4-bit quantization, have demonstrated\npotential in compressing LLMs effectively as their quality drops are greatly minimized due to advance\nalgorithm design such as GPTQ [19] and LoRC [69]. However, these advancements have predominantly\nfocused on zero-shot evaluations and the acceptable quality drops are for larger model size greater 13B,\nyet they often come with a significant trade-off for smaller model size such as 1B. Moreover, they only\nfocus on zero-shot measurement [62, 69]. In production environments, where replicating the original model\u2019s\n\u2217Our corresponding authors are Xiaoxia Wu (xiaoxiawu@microsoft.com) and Leon Song (leonsong@microsoft.com).\n\u2020Haojun Xia is currently a PhD student at The University of Sydney, Australia.\n\u2021Shiyang Chen is currently a PhD student at Rutgers University, USA.\n1Code will be released soon as a part of https://github.com/microsoft/DeepSpeed\n1\narXiv:2312.08583v2  [cs.CL]  18 Dec 2023\nperformance across different tasks is critical, any loss of model quality is a major concern. Existing methods,\nwhile innovative, do not fully address the practical requirements for deploying LLMs in real-world applications.\nContribution. To address these challenges, our contributions are as follows:\n\u2022 Broadened Evaluation Scope and Quantization Analysis. Our study reveals that existing quanti-\nzation methods like GPTQ tend to overfit to calibrated datasets. More significantly, we have broadened\nthe scope of 4-bit quantization analysis in LLMs to include tasks beyond Zero-Shot, such as code\ngeneration and abstractive summarization. We discover that INT4 quantization often underperforms,\nespecially in smaller models, even as large as 13 billion parameters, exemplified by LLaMA-13b. See\nSection 3 for details.\n\u2022 Superior Performance with FP6 Quantization. We illustrate that FP6, employing a basic round-\nto-nearest (RTN) algorithm and a coarse-grain quantization approach, consistently achieves accuracy\non par with full-precision models, proving highly effective across a broad spectrum of generative tasks.\nThe StarCoder-13B model with FP6 quantization matches the performance of its FP16 equivalent in\ncode generation tasks. For smaller models such as the 406M, it aligns closely with baseline results in\nsummarization. These achievements are beyond the capabilities of INT4 quantization. For a more\nin-depth exploration, refer to Section 4.\n\u2022 Innovative 4+2 FP6 Design. We introduce an innovative 4+2 design for FP6 that overcomes\nprior integration and acceleration issues on AI hardware. This design attains latency similar to the\nstate-of-the-art INT4 fine-grain quantization, establishing FP6 as a viable alternative to existing 4-bit\nquantization methods in LLMs. See Section 5 for details.\n2\nRelated Work\nIn this study, we specifically focus on the quantization of Large Language Models (LLMs), diverging from\nother neural network architectures like BERT and ResNet models, which have been extensively explored in\nexisting literature [54, 71, 17, 63, 3, 16, 55, 28].\nQuantization generally refers to employing low-precision weights and activations to leverage faster\narithmetic cores, such as INT8/INT4 tensor cores [26]. However, the distinctive bandwidth constraints of\nLLMs have popularized weight-only quantization methods [71, 19, 68, 62, 61] as a strategy to reduce the\nmemory footprint of these models.\nMost previous research evaluates the impact of quantization using metrics like zero-shot perplexity or\naccuracy [66, 19, 8, 2, 29]. However, given that the main real-world applications of LLMs, such as ChatGPT [5]\nand Codex [21], revolve around generation-based tasks, a more comprehensive evaluation framework for\nquantized LLMs is warranted.\nWhile many studies focus on integer data formats for their ease of simulation and extensive ecosystem\nsupport [31, 15, 19, 8, 29, 27], recent works have also demonstrated the effectiveness of floating-point\nformats [62, 13]. Nonetheless, these investigations typically center on conventional bit precisions such as\n2/4/8 bits. Some research, like GPTQ, delves into 3-bit precision, but number concatenation methods, as\ndiscussed in Section 5, limit their system performance.\nFinally, while the push for lower precision quantization continues, the practicality of deploying a model of\nsize xB quantized to 2y bits over a 2xB model with y bits quantization is often overlooked, despite potential\nquality advantages [14, 69]. Our paper seeks to find an optimal balance where the quantized model retains\nsimilar accuracy to a full-precision model, an aspect largely missing in current literature [50, 69, 53, 70, 34,\n23, 59, 22, 67].\n3\nComprehensive Evaluation is Needed\nFor completeness, we here explain some foundational terminology and concepts in quantization.\n2\nTable 1: Zero-Shot Evaluation (Perplexity\u2193). GPTQ quantization algorihtm for INT4 weight (W4A16)\non LLaMA-1B (Left) and LLaMA-13B (Right). Different calibration datasets result in different results.\nDataset\nLLaMA-1B (4096-seq)\nLLaMA-13B (2048-seq)\nPrecision\nFGQ\nfor GPTQ\nPTB\nPTB-new\nC4\nC4-new\nPTB\nPTB-new\nC4\nC4-new\nFP16\nN/A\nN/A\n37.39\n58.34\n8.91\n9.4\n19.23\n28.10\n6.61\n6.8\nINT4-GPTQ\n\u2717\nPTB\n49.80\n64.01\n10.00\n10.49\n19.68\n28.71\n6.91\n7.17\n\u2717\nC4\n719.21\n693.48\n9.84\n10.37\n21.31\n30.01\n6.84\n7.09\n\u2713\nC4\n1399.89\n1396.76\n9.34\n9.84\n22.14\n29.83\n6.74\n6.95\nInteger Quantization. Consider a full-precision x \u2208 Rd and its quantized counterpart xq \u2208 Rd. The\nmapping strategy from x to xq is\nxq = S\n\u0006\nclamp((x \u2212 xzero1)/S; 0, 2b\u22121 \u2212 1)\n\u0007\n+ xzero1,\n(1)\nwhere clamp the function map its input values to a given range from \u22122b\u22121 to 2b\u22121 \u2212 1. xzero serves as a\nreference point, aiming to minimize any bias and can be defined as the minimum value, such as xzero = min(x).\nThe parameter b represents the number of bits used to represent the quantized value. \u2308\u00b7\u2309 is the rounding\noperator, and S \u2208 R is the scaling factor. For example, S can be computed as the distance between the\nmaximum and minimum of the absolute elements in the x vector, i.e., S = max(x) \u2212 min(x). For 4-bit\ninteger (INT4) quantization on LLMs, this algorithm with xzero strictly greater zero (called asymmetric\nquantization) has been widely adopted and proved to be much better than that by setting xzero = 0 based on\nprior works [61, 62, 68, 69]. For history and details on how to set the parameters, see [20].\nFine-grain Quantization (FGQ) and Coarse-grain Quantization (CGQ) relates to the value\nof d in previous paragraph. Suppose a matrix of dimension n \u00d7 n is vectorized into n2 elements which are\nthen grouped into blocks of size d, yielding n2/d groups. When the block size d is set to 1, it equates to the\nmatrix\u2019s original precision. FGQ comes into play when the block size is smaller than specific thresholds like\n64, 128, or 256, given a matrix size of n \u2265 1024. This method focuses on adjusting the precision within each\nrow or column of the matrix at a finer level. CGQ, in contrast, uses a block size equivalent to the row size\n(d \u2265 1024 in this case), resulting in a coarser quantization approach. FGQ gained significant attention in the\nrealm of LLMs because the values in the weight matrices tend to have a wider distribution, as noted in [69].\nAlongside FGQ or CGQ, specific algorithms are employed for precision mapping in quantization. Given\nthe focus on 4-bit quantization and the demonstrated efficacy of the INT4 format over FP4 (as detailed in the\nappendix) [62], the investigation primarily centers on a straightforward method, RTN, and the increasingly\nrecognized and impactful algorithm, GPTQ [18, 19], with a solid foundation background [33, 24]. We now\nexplain them briefly below:\n\u2022 RTN. Round-to-nearest neighborhood simply map the weight matrices to its low-precision counterpart\nbased on Equation 1.\n\u2022 GPTQ. Generative Pre-trained Transformer Quantization is a more advanced method of leveraging\nthe activation information, which requires the inverse of the second-order input information. According\nto [18, 19], it reduces the precision of the model\u2019s weights to a lower bit representation (down to 3\nor 4 bits per weight) without significant accuracy loss. Their code implementation is structured in a\nlayer-by-layer manner, transferring the computational burden to the CPU when it\u2019s not in use. This\nstrategy allows for the execution of massive models, like those with 175B parameters, on a single GPU,\novercoming previous limitations of scale and complexity. GPTQ enhances the practical deployment of\nthese models, particularly in memory and computationally constrained environments.\nIn addition to the algorithms previously mentioned, there has been significant progress in Post-Training\nQuantization (PTQ) for LLMs, highlighted by innovations such as SmoothQuant [66], AWQ [38], Quip [8],\nSqueezeLLM [29], QUIK [2], and LLM-FP4 [40] and many more [69, 14]. These methodologies, however,\noften necessitate the use of additional sparsity matrices or extra procedures to pinpoint sensitive weights.\n3\nTable 2: Zero-Shot Evaluation (Perplexity\u2193). Compare between GPTQC4 and RTN quantization\nalgorithms for INT4 weight (W4A16) on LLaMA of size 1B, 13B and 65B. We apply fine-grain quantization\n(FGQ) in which the block-size is 256 elements per scale (LLaMA-1B\u2019s block-size is 128). We also report\nresults for coarse-grain quantization (CGQ) (per row per scale). The evaluation datasets are Wikitext2, PTB,\nC4, PTB-new, and C4-new.\nQuant\nPrecision\nLLaMA-1B (4096-seq)\nLLaMA-13B (2048-seq)\nLLaMA-65B (2048-seq)\nFP16\n24.317.53/37.39/8.91/58.34/9.40\n13.165.09/19.23/6.61/28.10/6.80\n6.413.56/8.00/5.62/8.88/5.98\nFGQ\nINT4-GPTQ\n564.737.83/1399.89/9.34/1396.76/9.84\n14.195.28/22.14/6.74/29.83/6.95\n6.613.81/8.17/5.73/9.20/6.13\nINT4-RTN\n22.767.98/34.03/9.50/52.28/10.00\n14.325.35/22.49/6.80/29.96/7.00\n7.303.79/10.13/5.74/10.54/6.31\nCGQ\nINT4-GPTQ\n288.228.20/719.21/9.84/693.48/10.37\n14.135.37/21.31/6.84/30.01/7.09\n7.174.12/10.50/5.83/9.16/6.21\nINT4-RTN\n34.298.33/55.52/10.15/86.85/10.62\n14.325.55/20.95/6.97/30.91/7.22\n7.724.20/10.59/5.90/11.44/6.47\nFurthermore, the majority of these studies concentrate predominantly on zero-shot perplexity and accuracy\nperformance [69, 19, 62]. Yet, the extent to which these findings can be generalized to other generative tasks\nremains to be fully explored.\nExperiment Settings\nWe assess performance across three metrics: Zero-Shot tasks, Code Generation,\nand Summarization. We also perform try to implement comparative experiments for those chat-based models\nand judged by GPT-4 based on the FastChat codes [75]. Despite this, due to significant variability in our\nfindings, we concluded that there is no clear link between bit precision and performance. These results are\ndetailed further in the Appendix of our study.\n\u2022 Zero-Shot Tasks. Leveraging open-source repositories2, we applied GPTQ quantization algorithms\nto measure both perplexity and accuracy in zero-shot contexts. The datasets used for perplexity\nmeasurement include PTB [42], Wikitext [43], and C4 [51].3 For accuracy, we randomly pick ten\ntasks: ARC (Challenge/Easy) [4], BoolQ [9], CB [12], Copa [1], PIQA [56], RTE [11], WSC [35],\nStorycloze [45]). Calibration for GPTQ used 128 (32) samples for LLaMa-1B/13B (-65B) models.4.\nWe believe the results generalize to other models family such sh OPT [72] and BLOOM [52], The\nexperiments were deterministic, using the seed 123.\n\u2022 Code Generation. Following [76] and their open-source implementation5, we adapted non-greedy\ngeneration settings (n=20, t=0.2, top_p=0.95). To mitigate variance, nine random seeds {111,222,. . . ,\n888, 1111} were employed. The models evaluated included CodeGeeX2-6B, StarCoder-15B [36], and\nCodeLLaMA-34B [39].6 We focused on Python and JavaScript, noting instability in other programming\nlanguages.\n\u2022 Summarization Tasks. Based on [37, 61] and their open-source codes,7 we utilized BART-large,\nfine-tuned for CNNDailyMail [25] and XSum [47] summarization tasks.8 Default settings were applied\nfor all other parameters.\nWe focus on thes experiments with FP16 activation and INT4 weights on LLMs. Our experimental setup\n2https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/compression, https://github.com/qwopqwop200/G\nPTQ-for-LLaMa, and https://github.com/jerry-chee/QuIP\n3Following the approach in gptq-for-llama, we added two new validation sets: PTB-new, using the PTB test dataset, and\nC4-new, comprising the first 256\u00d7seqlength. These new sets are implemented as per QuIP.\n4LLaMA-13B/65B are from [57] and LLaMA-1B is from [65]. They can be downloaded from HuggingFace with names:\n\u2018princeton-nlp/Sheared-LLaMA-1.3B\u2019, \u2018huggyllama/llama-13b\u2019, \u2018huggyllama/llama-65b\u2019.\n5https://github.com/THUDM/CodeGeeX2\n6Available as \u2018THUDM/codegeex2-6b\u2019, \u2018bigcode/starcoder\u2019, and \u2018codefuse-ai/CodeFuse-CodeLlama-34B\u2019 on HuggingFace.\n7https://github.com/amazon-science/dq-bart\n8Models available as \u2018facebook/bart-large-cnn\u2019 and \u2018facebook/bart-large-xsum\u2019 on HuggingFace.\n4\nTable 3: Zero-Shot Evaluation (Accuracy\u2191).\nCompare between GPTQC4 and RTN quantization\nalgorithms for INT4 weight (W4A16) on LLaMA-1B (Top) and LLaMA-13B (Bottom). We apply fine-grain\nquantization (FGQ) in which the block-size is 256 elements per scale except for LLaMA-1B\u2019s (which is 128).\narcC (arcE) stands for arc_challenges (arc_easy).\nModels\nPrecision (FGQ)\narcC\narcE\nboolq\ncb\ncopa\npiqa\nrte\nwic\nwsc\nstorycloze\nMEAN\nLLaMA-1B\nFP16\n26.71\n53.11\n61.13\n39.29\n76.00\n73.83\n50.18\n50.00\n36.54\n69.64\n53.64\n(4096-seq)\nINT4-GPTQ\n26.37\n50.59\n61.59\n46.43\n79.00\n73.34\n48.01\n50.00\n36.54\n68.24\n54.01\nINT4-RTN\n26.11\n51.09\n58.07\n50.00\n74.00\n72.91\n48.38\n50.00\n36.54\n68.36\n53.55\nLLaMA-13B\nFP16\n43.86\n74.58\n68.53\n50.00\n90.00\n79.00\n65.34\n50.00\n35.58\n78.23\n63.51\n(2048-seq)\nINT4-GPTQ\n43.00\n73.44\n67.83\n41.07\n93.00\n78.78\n62.45\n50.16\n36.54\n78.17\n62.44\nINT4-RTN\n44.03\n74.45\n67.37\n44.64\n91.00\n78.84\n63.18\n49.84\n36.54\n78.42\n62.83\nLLaMA-65B\nFP16\n47.01\n75.08\n82.32\n64.29\n91.00\n81.61\n71.48\n58.31\n60.58\n79.57\n71.13\n(2048-seq)\nINT4-GPTQ\n46.84\n75.08\n80.76\n58.93\n94.00\n81.18\n72.92\n56.27\n60.58\n79.31\n70.59\nINT4-RTN\n47.10\n75.25\n81.47\n62.50\n95.00\n81.23\n69.68\n57.21\n62.50\n79.63\n71.16\nTable 4: Generation Tasks (Rouge\u2191 or Pass@1\u2191). INT4 weight (W4A16) quantization on BART406M,\nCodeGeeX2-6B, StarCoder-15B, and CodeLLaMA-34B models (left to right) Using RTN. Note that summa-\nrization tasks use two separate BART406M versions fine-tuned by CNN/XSUM and code generation tasks in\nHuman-X including Python and JavaScript (as the variances of other tasks such as CPP, Go and RUST are\nhigher and so not included), averaged over at least 8 repeated experiments with standard deviation.\nBART406M(R1/R2/RL)\nCodeGeeX2-6B (pass@1)\nStarCoder-15B (pass@1)\nCodeLLaMA-34B (pass@1)\nPrecision\nXSUM (CNN)\nPython\nJava-Scrpit\nPython\nJava-Scrpit\nPython\nJava-Scrpit\nFP16\n45.49/22.39/37.28 (44.07/21.09/30.65)\n34.04\u00b11.70\n31.50\u00b12.62\n35.43\u00b12.21\n33.67\u00b12.02\n43.22\u00b12.21\n45.05\u00b11.60\nINT4 (CGQ)\n43.76/20.76/35.82 (43.67/20.68/30.07)\n33.08\u00b12.07\n25.15\u00b11.97\n33.20\u00b11.40\n32.18\u00b11.29\n39.84\u00b11.64\n43.45\u00b12.05\nINT4 (FGQ)\n44.82/21.63/36.48 (43.73/20.72/30.24)\n29.80\u00b11.76\n28.35\u00b12.36\n35.64\u00b12.52\n32.32\u00b12.01\n46.88\u00b11.87\n43.22\u00b11.36\nincludes a single-GPU environment, utilizing either a V100-32g or H100-80g GPU. Based on the results\nTable 1, Table 2, and Table 3, we make the following two key observations.\nGPTQ\u2019s Tendency to Overfit.\nAlthough GPTQ is innovative in post-training quantization, it tends\nto overfit to particular datasets, especially noticeable in its fine-grain quantization results. As indicated in\nTable 1, we see that if we calibrate with specific dataset such as C4 for GPTQ, then the performance for this\nC4 dataset would be much better (see 9.34 or 6.74 using FGQ), while other datasets such as PTB would\nresult in much worse performance (see 1399.89 and 22.14 using FGQ). Independently, [60] also notices this\nissue while examining LLaMA-7B.\nIt is admitted that the over-fitting phenomena is less severe for larger models (moving from 1B to 13B or\n65B). Indeed, as shown in Table 2, we see that LLaMA-65B using GPTQ on FGQ for INT4-weight results in\nthe best average perplexity 6.61 comparing to RTN (7.17), much closer to the baseline 6.41. However, its\neffectiveness in enhancing Zero-Shot performance is somewhat limited (detailed in Table 3), suggesting a gap\nin its adaptability across various language modeling scenarios and highlighting the need for robustness in\nmodel evaluation. In particularly, we presents in Table 3 the comparison between RTN and GPTQ on INT4\nweight while keep the activation untouched, we can not claim that GPTQ and RTN are better than another\nbased on zero-shot performance. In fact, for LLaMA-65B, the performance for RTN is surprisingly better\nthan the one of FP16.\nExpanding Evaluation Methods for Generative Models.\nOur current analysis, mainly centered on\nzero-shot performance as shown in Table 2 and Table 3, highlights the need for a broader scope in evaluation\ntechniques. The core strength of LLMs lies in their ability to generate sequences. Therefore, this paper\n5\nfocuses on assessing summarization and code generation, as elaborated in Table 4.9 This strategy underlines\nthe importance of comprehensive and detailed testing methods that extend beyond zero-shot learning, aiming\nto fully evaluate the generative capabilities of LLMs. The data in Table 4 show a notable difference in\nperformance with INT4, especially when compared to standard benchmarks. For example, the performance of\nthe CodeLLaMA-34B model in Java-Script drops from 45.05 (FP16) to 43.45 (INT4, CGQ) or 43.22 (INT4,\nFGQ), a decrease of 1.6 and 1.83 points, respectively. While FGQ on INT4 offers considerable improvements\nover CGQ, gaps compared to FP16 persist, particularly for smaller models and in Java Scripts. Interestingly,\nthe INT4 CodeLLaMA-34B on FGQ achieves 46.88 in Python code, surpassing its baseline, whereas the INT4\nCodeGeeX2-6B on FGQ scores only 29.8, falling behind even its INT4-CGQ performance. This highlights\nthe inconsistency of INT4.\nThese results emphasize the need for research into the effectiveness of INT4 in complex generative tasks.\n4\nSweet Spot Solution: FP6\nBuilding on previous discussions around the challenges and limitations associated with INT4 quantization,\nparticularly its instability and subpar outcomes in code generation and summarization tasks, this section\ndelves into an emerging area of interest in floating point quantization research.\nRecent studies have\nincreasingly focused on the use of floating point quantization for handling weights or activations within LLMs\n[62, 40, 74, 44, 7, 32, 58]. Notably, a simple FP8\u2019s application in activation processes has shown remarkable\nimprovements over the use of INT8 [62]. Inspired by these advancements, a critical question arises: Could\nincreasing the bit precision, for instance to 5 or 6 bits, offer more stable and robust outcomes in generative\ntasks? This section aims to explore the extent of FP6\u2019s (FP5\u2019s) effectiveness and its resilience to different\nquantization algorithms, offering a potential solution to the dilemma posed by previous INT4 quantization\nchallenges.\nFor completeness, we provide a simplified overview of the floating-point format. For a detailed explanation,\nplease refer to [10]. A standard floating point number comprises three parts: the sign bit, the exponent bits,\nand the mantissa bits. This can be simplified as:\nx = S \u00d7 2E\u2212b \u00d7 M,\n(2)\nwhere S represents the sign (\u00b11), E denotes the exponent ([0, 2e \u2212 1], with e being the count of exponent\nbits), b is the bias for the exponent (usually 2e\u22121 \u2212 1), and M signifies the mantissa ([0, 2)).\nFollowing the implementation of [73], the maximum/minimum achievable value in FP6E3M2 is \u00b128\n(\u00b11 \u00d7 24 \u00d7 1.75).10 The FP16 (or BF16) weight matrix undergoes quantization as follows:\n\u02c6Wfp16 \u2248 Quant(Wfp16) = Sfp16 \u00d7 Wfp6,\n(3)\nwhere Wfp16 is the original full precision weight, Quant(\u00b7) symbolizes the quantization function, Sfp16 is\nthe scaling factor, and Wfp6 is the FP6E3M2 number. The scaling factor Sfp16 is computed by: Sfp16 =\nmax(abs(Wfp16))/28, thereby ensuring optimal use of Wfp16\u2019s range without compromising on precision.\nPlease see Section 5 for additional customizations in FP6E3M2. Similar settting is defined for F5E3M1.\nWhy not INT6 instead of FP6. The choice of FP6 over INT6 is driven by two key factors: firstly, the FP\nformat simplifies conversion processes, as final computations are typically performed using FP16 or BF16.\nSecondly, there is no observed difference in accuracy between these formats, as supported by findings in [62] ,\neliminating the need for additional experimental validation.\n9It should be noted that GPTQ tends to overfit to the calibrated dataset and poses implementation challenges, leading us to\nsolely utilize RTN for our evaluations.\n10Note that for some other work such as [44] has specific configuration defined for exceptional values like NaN and \u00b1\u221e.\nHowever, these are not included in our weight quantization process using FP6E3M2. We do not think this slight difference will\ngreatly impact the model performance.\n6\n4.1\nResults of FP6 and FP5 on all tasks\nTable 5: Generation Tasks (Rouge or Pass@1 \u2191). Comparative results of quantizations on BART406M,\nCodeGeeX2-6B, StarCoder-15B, and CodeLLaMA-34B models (left to right). Summarization tasks using\ntwo BART406M versions fine-tuned by CNN/XSUM and code generation tasks in Python and JavaScript,\naveraged over 8 iterations with standard deviation. FP6 (FP5) format is E3M2 (E3M1).\nBART406M(R1/R2/RL)\nCodeGeeX2-6B (pass@1)\nStarCoder-15B (pass@1)\nCodeLLaMA-34B (pass@1)\nPrecision (RTN)\nXSUM (CNN)\nPython\nJava-Scrpit\nPython\nJava-Scrpit\nPython\nJava-Scrpit\nFP16\n45.49/22.39/37.28 (44.07/21.09/30.65)\n34.04\u00b11.70\n31.50\u00b12.62\n35.43\u00b12.21\n33.67\u00b12.02\n43.22\u00b12.21\n45.05\u00b11.60\nINT4 (CGQ)\n43.76/20.76/35.82 (43.67/20.68/30.07)\n33.08\u00b12.07\n25.15\u00b11.97\n33.20\u00b11.40\n32.18\u00b11.29\n39.84\u00b11.64\n43.45\u00b12.05\nINT4 (FGQ)\n44.82/21.63/36.48 (43.73/20.72/30.24)\n29.80\u00b11.76\n28.35\u00b12.36\n35.64\u00b12.52\n32.32\u00b12.01\n46.88\u00b11.87\n43.22\u00b11.36\nFP5 (CGQ)\n45.12/22.11/36.96 (44.13/21.19/30.68)\n30.56\u00b12.12\n28.43\u00b12.39\n34.30\u00b11.29\n33.00\u00b11.54\n40.55\u00b10.92\n43.29\u00b11.93\nFP6 (CGQ)\n45.37/22.20/37.11 (44.06/21.07/30.67)\n34.10\u00b12.09\n31.61\u00b11.74\n35.64\u00b11.26\n33.60\u00b11.91\n44.31\u00b11.88\n44.51\u00b11.30\nTable 6: Zero-Shot Evaluation (Perplexity\u2193). Compare between GPTQC4 and RTN quantization\nalgorithms for INT4 weight (W4A16) on LLaMA of size 1B, 13B and 65B. We apply fine-grain quantization\n(FGQ) in which the block-size is 256 elements per scale (LLaMA-1B\u2019s block-size is 128). We also report\nresults for coarse-grain quantization (CGQ) (per row per scale). The evaluation datasets are Wikitext2, PTB,\nC4, PTB-new, and C4-new. FP6 (FP5) format is E3M2 (E3M1)\nPrecision\nFGQ\nLLaMA-1B (4096-seq)\nLLaMA-13B (2048-seq)\nLLaMA-65B (2048-seq)\nFP16\nN/A\n24.317.53/37.39/8.91/58.34/9.40\n13.165.09/19.23/6.61/28.10/6.80\n6.413.56/8.00/5.62/8.88/5.98\nINT4-GPTQC4\n\u2713\n564.737.83/1399.89/9.34/1396.76/9.84\n14.195.28/22.14/6.74/29.83/6.95\n6.613.81/8.17/5.73/9.20/6.13\nINT4-RTN\n\u2713\n22.767.98/34.03/9.50/52.28/10.00\n14.325.35/22.49/6.80/29.96/7.00\n7.303.79/10.13/5.74/10.54/6.31\nINT4-GPTQC4\n\u2717\n288.228.20/719.21/9.84/693.48/10.37\n14.135.37/21.31/6.84/30.01/7.09\n7.174.12/10.50/5.83/9.16/6.21\nINT4-RTN\n\u2717\n34.298.33/55.52/10.15/86.85/10.62\n14.325.55/20.95/6.97/30.91/7.22\n7.724.20/10.59/5.90/11.44/6.47\nFP5-GPTQC4\n\u2713\n44.297.74/72.63/9.19/122.20/9.68\n13.765.22/20.43/6.71/29.53/6.92\n6.503.67/8.15/5.68/8.95/6.04\nFP5-RTN\n\u2713\n28.527.78/44.09/9.23/71.79/9.71\n13.955.20/21.01/6.71/29.92/6.92\n6.833.70/9.73/5.69/8.98/6.06\nFP5-GPTQC4\n\u2717\n32.037.77/50.66/9.23/82.73/9.76\n13.905.22/21.02/6.71/29.64/6.93\n6.463.68/7.83/5.70/9.13/5.98\nFP5-RTN\n\u2717\n27.927.83/41.80/9.27/70.94/9.77\n14.105.22/21.52/6.72/30.13/6.93\n6.503.68/8.08/5.69/8.98/6.06\nFP6-GPTQC4\n\u2713\n22.777.59/34.04/8.98/53.76/9.47\n13.365.13/19.67/6.63/28.53/6.83\n6.473.59/8.12/5.63/8.91/6.10\nFP6-RTN\n\u2713\n23.427.60/35.84/8.99/55.20/9.49\n13.245.12/19.43/6.63/28.19/6.83\n6.443.58/8.04/5.63/8.92/6.05\nFP6-GPTQC4\n\u2717\n23.587.59/35.76/8.98/56.10/9.47\n13.235.12/19.34/6.64/28.20/6.83\n6.423.61/8.01/5.63/8.87/6.00\nFP6-RTN\n\u2717\n24.837.60/38.79/8.99/59.26/9.49\n13.095.12/19.06/6.64/27.81/6.83\n6.423.59/8.01/5.63/8.89/5.99\nWe conduct the experiments described in Section 3, namely code generation, summarization and zero-shot\nexperiments. The results are shown in Table 5 and Table 6. Additional results are presented in Table A.1 and\nTable A.2, which we defer to appendix as it we do not find a clear link between bit precision and performance.\nIn general, the FP6 quantization method, particularly with CGQ, stands out in this analysis, offering a\nbalance of high performance and robustness across different tasks, models and even quantiztion algorithms\n(RTN and GPTQ), a notable improvement over both FP5 and INT4 quantizations. In addition, we make the\nfollowing observation:\nFP5 Performance. FP5 with CGQ shows an improvement over INT4 quantization but still does\nnot reach the high performance levels of FP16. The gap between FP5 and its baseline is particularly\nnoticeable in the Python and JavaScript code generation tasks across CodeGeeX2-6B, StarCoder-15B, and\nCodeLLaMA-34B.\nFP6 Robustness. FP6 quantization, especially with CGQ, demonstrates a significant advancement,\nnearly matching the FP16 baseline across various tasks and models. This quantization method not only\nnarrows the performance gap seen in FP5 and INT4 but also shows robustness in handling different tasks.\nThe robustness is further accentuated when comparing CGQ and FGQ within the FP6 framework (as there\n7\nis little difference between CGQ and FGQ), where FP6 with CGQ consistently maintains high performance\nclose to baseline, indicating its effectiveness and stability across different scenarios. Moreover, FP6 is robust\nto quantization algorithms: either RTN or GPTQ results in similar results, particularly for CodeLLaMA-34B,\nas shwon in Table 6.\n5\nSystem Support Discussion\n5.1\n4+2 format for FP6\nIn addressing the challenges of utilizing a non-standard 6-bit number format, which deviates from the\nconventional power-of-2 numerical systems (termed as \"odd bit precision setting\"), we propose a novel\napproach. This method is distinct from the two commonly considered strategies:\n1. The first approach involves directly converting the 6-bit format into an 8-bit floating-point (FP8) format.\nWhile this is a straightforward solution, it unfortunately negates the primary benefit of the 6-bit format,\nwhich is to conserve memory.\n2. The second approach entails grouping several 6-bit numbers together in a continuous memory block and\nrepresenting them using either 32-bit integers (INT32) or a 32-bit floating-point (FP32) format. This\nmethod maintains the memory-saving advantage but adds complexity to the dequantization process.\nOur unique strategy, however, focuses on dividing the 6-bit number into two distinct sub-numbers: the\nfirst sub-number representing the initial 4 bits, and the second sub-number accounting for the remaining 2\nbits. Our poposed \"4+2\" method can be seen as an advanced variation of the second standard approach.\nThe 4+2 bit division is based on the fundamental principle that any positive integer can be expressed as a\nsum of powers of 2. With this foundation, we divide the 6-bit number into two components:\n\u2022 The first part, comprising the initial 4 bits, handles elements such as the sign bit and a 3-bit exponent.\n\u2022 The second part, containing the remaining 2 bits, is dedicated to the 2-bit mantissa.\nThis division into 4+2 bits facilitates simultaneous loading and dequantization of these sub-numbers,\nculminating in the generation of the final 16-bit floating-point (FP16) weight. Our approach innovatively\nbalances the need for reduced memory footprint with the practicalities of dequantization, particularly in\naddressing the challenges of memory access across segmented numbers.\n5.2\nBias Shift\nDequantizing FP6 to FP16 during runtime on GPUs can be significantly resource-intensive, primarily due to\nthe complexity involved in manipulating the exponent field, as detailed in Section 4.\nThe bias term for the exponent, typically determined by the exponent bits, is 3 for FP6 and 15 for FP16.\nMathematically, the process of dequantizing FP6 to FP16 (excluding the sign) is represented as:\n2EFP16\u221215 \u00d7 MFP16 = 2EFP6\u22123 \u00d7 MFP6,\n(4)\nwhere the superscripts FP16/FP6 indicate the respective format. It is noteworthy that the scaling factor\ndequantization can be done after matrix multiplication before the accumulation for fine-grained (sub-row)\nquantization schemes or after accumulation for coarse-grained (row-wise) quantization schemes.\nWhile padding can easily adjust the mantissa, aligning the exponents requires more effort due to the\ndifference in biases. An analogy can be drawn with converting an INT4 number back to a symmetric INT8\nformat: if INT4 employs a symmetric format (for the mantissa), zero padding suffices. However, in an\nasymmetric format, padding alone is inadequate, and additional steps are necessary.\n8\nTo address this, we have customized our FP6 format with a non-standard exponent bias of 15. This\nmodification does not affect precision or accuracy because:\nSFP16 \u00d7 2E\u22123 \u00d7 M = (SFP16 \u00d7 212) \u00d7 2E\u221215 \u00d7 M,\n(5)\nmeaning the bias shift can be seamlessly integrated into the scaling factor. Crucially, since SFP16 is less than\n1, multiplying it with 212 still allows for accurate representation in FP16 format through simple exponent bit\nshifting, avoiding numerical errors.\nE2E3\nE1\nE2E3E4E5\nE1\nWfp6\n12\n+\nStep-1:\n(1)Sfp16 = Sfp6\n(2)Efp16 = Efp6 + 12\n\uff083)Mfp16 = padding(Mfp6)\nE1E2E3E4E5\nSfp16\nx\nStep-2: \nWfp16 = Wtmp * Sfp16\nOutput\nWtmp\n(a) Before Bias Shift.\nE2E3\nE1\nE2E3E4E5\nE1\nWfp6\n0\n+\nStep-1:\n(1)Sfp16 = Sfp6\n(2)Efp16 = padding(Efp6)\n\uff083)Mfp16 = padding(Mfp6)\nE1E2E3E4E5\nSfp16\nx Step-2: \nWfp16 = Wtmp* (Sfp16* 212)\nOutput\nWtmp\n(b) After Bias Shift.\nFigure 1: Comparison of the Traditional Method (Left) versus Our Proposed Method (Right): Demonstrating\nthe Significant Run-time Advantages of Bias Shift.\nOur bias shift method greatly simplifies the FP6-FP16 de-quantization process during runtime. To\ndemonstrate this, we provide a side-by-side comparison in Figures 1a and 1b.\nFigure 1a outlines the original two-step process for de-quantizing each FP6 weight. The first step involves\ncasting Wfp6 to Wtmp, and the second step requires multiplying by the quantization scale Sfp16. The most\ndemanding part of this procedure is recalculating the exponent for Wtmp, which involves extracting Efp6\nfrom Wfp6, adding 12, and then incorporating this back into Wtmp. Additionally, the process to de-quantize\nsubnormal FP6 numbers11 adds further complexity to the de-quantization during runtime.\nHowever, with our bias shift strategy, as illustrated in Figure 1b, the exponent adjustment becomes\na straightforward bit-level padding process. The addition of the constant integer 12, initially required in\nStep 1, can now be deferred to Step 2, eliminating any runtime overhead. This is possible because the\nmultiplication of the quantization scales with the constant integer can be performed statically after the model\nis quantized and before runtime. Moreover, this streamlined approach also efficiently accommodates the\nde-quantization of subnormal numbers.\n5.3\nSystem evaluation\nWe conducted an evaluation of the latest GPU kernels across various weight-only quantization techniques,\nfocusing on their system performance. This kernel-level assessment was carried out on the NVIDIA A100-40GB\nplatform, running Linux 5.3.18 and CUDA 11.8. Our primary focus was on the performance of feed-forward\n(FFN) layers within the LLaMA models, specifically during the token generation phase, as detailed in\n[57]. Comprehensive data on matrix shapes and kernel latency is available in Appendix B. We employed\ncuBLAS [48] as our benchmark for non-quantized performance (W16A16). We also included cutting-edge\nkernel support for F INT4 FGQ quantization (W4A16) from TensorRT-LLM [49] for comparative analysis.12\n11Subnormal numbers in floating-point formats are very small values, including zero, characterized by an exponent field of all\nzeros. When de-quantizing subnormal FP6 numbers, the new exponent should remain at zero, not adding 12, to maintain its\nsubnormal status. Subsequently, FP16 is multiplied by 1.0 \u00d7 212.\n12Between the supported block-size: 64 and 128, we chose 128.\n9\n0\n1\n2\n3\nFFN1-1b\nFFN2-1b\nFFN1-13b\nFFN2-13b\nFFN1-65b\nFFN2-65b\ncuBLAS\nFine-grained INT4\nFP6 (w/o Bias-Shift)\nFP6 (w/ Bias-Shift)\nFigure 2: GPU Kernel Speedups compared to cuBLAS. FFN1 and FFN2 are defined for matrices of 4H \u00d7 H\nand H \u00d7 4H size, where H are defined by the size of the LLaMA models. LLaMA-1B, 13B and 65B are\nrespectively 5504, 13824 and 22016. See detailed number in Table B.1\nTo elucidate the advantages of our proposed Bias Shift technique, detailed in Section 5.2, we also developed\nand tested an FP6 GPU kernel without Bias Shift.\nAccording to the results presented in Figure 2, our FP6 kernel, enhanced with Bias-Shift, achieved speeds\nup to 2.37\u00d7 faster and on average 1.92\u00d7 faster than cuBLAS. Given that LLM inference is commonly\nconstrained by limited GPU DRAM [64, 30], our approach effectively alleviates this bottleneck by minimizing\nmodel weight memory access. Moreover, our FP6 kernel outperforms the state-of-the-art fine-grained INT4\nimplementation in speed, being 1.06\u00d7 faster on average for the first feed-forward layers (FFN1) and 2.05\u00d7\nfaster for the second feed-forward layers (FFN2). It should be noted that Figure 2 provides only a snapshot\nof key outcomes, with a comprehensive system implementation and detailed performance analysis reserved\nfor future work. Importantly, our FP6 kernel with Bias-Shift is on average 1.36\u00d7 faster compared to the\nsame FP6 kernel without Bias Shift, underscoring the crucial role of Bias-Shift as discussed in Section 5.2.\n6\nDiscussion and Conclusion\nOur paper introduces a novel approach to GPU kernel optimization, specifically targeting weight-only\nquantization methods. Despite the significance of our findings, they pave the way for further research and\ndevelopment in several areas.\nEvaluation Scope. A primary limitation of our study is still the narrow scope of evaluation although we\nhave span to code generation and summarization. We suggest a vast field for future research. Expanding the\nscope to include diverse tasks and a focus on both performance and accuracy could enhance the robustness of\nour methods.\nComparative Analysis. While our system evaluation provides valuable insights, it lacks in-depth\ncomparison with state-of-the-art (SOTA) frameworks. A more comprehensive benchmarking against advanced\nframeworks would offer a clearer perspective on our approach\u2019s efficacy and areas for improvement.\nTechnique Adaptability. A notable aspect of our work is the adaptability of our techniques, particularly\nin bit-precision and bias-shift. The potential to adapt our methods to emerging standards, such as 5-bit\nquantization, demonstrates their flexibility and future applicability in various contexts.\nFuture Directions. The advancement of Post-Training Quantization (PTQ) methods and their integra-\ntion with other techniques presents exciting future possibilities. Our research lays the foundation for further\nadvancements in model optimization, such as the 3-bit precision from evolving quantization techniques.\nIn conclusion, our study marks a significant contribution to odd-bit GPU kernel optimization. It also\nopens avenues for broader research, exploring the full potential of model optimization and quantization across\ndiverse applications.\n10\nContributions\nXiaoxia Wu led model quality measurement and paper writing. Haojun Xia, contributing to system section\nwriting, designed the FP6 acceleration strategy and system support. Stephen Youn, Zhen Zheng, and Shiyang\nChen handled DeepSpeed integration. Arash Bakhtiari and Michael Wyatt managed framework testing and\nexternal integration. Reza Yazdani Aminabadi, Yuxiong He and Olatunji Ruwase provided key discussions.\nLeon Song is the overall leader for algorithm and system design. Zhewei Yao initiated the project and led the\nFP6/5 format design.\nReferences\n[1] Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles, Joyce Ho, and Jimeng Sun.\nCopa: Constrained parafac2 for sparse & large datasets. In Proceedings of the 27th ACM International\nConference on Information and Knowledge Management, pages 793\u2013802, 2018.\n[2] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler,\nand Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv\npreprint arXiv:2310.09259, 2023.\n[3] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.\n[4] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew\nMcCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, et al. A\nsystematic classification of knowledge, reasoning, and context within the arc dataset. arXiv preprint\narXiv:1806.00358, 2018.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[6] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq:\nA novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13169\u201313178, 2020.\n[7] L\u00e9opold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz H Elibol, and Hanlin Tang.\nShifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. arXiv\npreprint arXiv:2001.05674, 2020.\n[8] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large\nlanguage models with guarantees. arXiv preprint arXiv:2307.13304, 2023.\n[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\n[10] Wikipedia contributors. Floating-point arithmetic \u2014 Wikipedia, the free encyclopedia, last edited 2023.\n[Online; accessed 8-December-2023].\n[11] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing textual entailment:\nModels and applications. Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220, 2013.\n[12] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigat-\ning projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages\n107\u2013124, 2019.\n11\n[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of\nquantized llms. arXiv preprint arXiv:2305.14314, 2023.\n[14] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\npreprint arXiv:2212.09720, 2022.\n[15] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ: Hessian\naware quantization of neural networks with mixed-precision. In Proceedings of the IEEE International\nConference on Computer Vision, pages 293\u2013302, 2019.\n[16] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S\nModha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and\nArmand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint\narXiv:2004.07320, 2020.\n[18] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training\nquantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\n[19] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[20] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey\nof quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n[21] GitHub. Github copilot. https://github.com/features/copilot/, 2021.\n[22] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo,\nand Yuhao Zhu. Olive: Accelerating large language models via hardware-friendly outlier-victim pair\nquantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture,\npages 1\u201315, 2023.\n[23] Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix\ndecomposition for efficient language model finetuning. arXiv preprint arXiv:2311.12023, 2023.\n[24] Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon.\nIn Advances in neural information processing systems, pages 164\u2013171, 1993.\n[25] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. Teaching machines to read and comprehend. arXiv preprint arXiv:1506.03340, 2015.\n[26] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized\nneural networks: Training neural networks with low precision weights and activations. The Journal of\nMachine Learning Research, 18(1):6869\u20136898, 2017.\n[27] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and\nDongsoo Lee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer\nquantization. arXiv preprint arXiv:2305.14152, 2023.\n[28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only\nbert quantization. In International conference on machine learning, pages 5506\u20135518. PMLR, 2021.\n[29] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney,\nand Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.\n12\n[30] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace\nDinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer\ninference: a survey. arXiv preprint arXiv:2302.14017, 2023.\n[31] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepa-\nper. arXiv preprint arXiv:1806.08342, 2018.\n[32] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort.\nFp8 quantization: The power of the exponent. arXiv preprint arXiv:2208.09225, 2022.\n[33] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information\nprocessing systems, pages 598\u2013605, 1990.\n[34] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from\nactivation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272,\n2023.\n[35] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning. Citeseer, 2012.\n[36] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh\nShliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi,\nJian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman,\nSiva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy,\nUrvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson,\nBrendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite,\nCarlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm\nde Vries. Starcoder: may the source be with you! 2305.06161, 2023.\n[37] Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang,\nand Dan Roth. Dq-bart: Efficient sequence-to-sequence model via joint distillation and quantization. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:\nShort Papers), pages 203\u2013211, 2022.\n[38] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware\nweight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n[39] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen,\nMin Shen, Hailian Zhou, Hang Yu, and Jianguo Li. Mftcoder: Boosting code llms with multitask\nfine-tuning. arXiv preprint arXiv, 2023.\n[40] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. Llm-fp4: 4-bit\nfloating-point quantized transformers. arXiv preprint arXiv:2310.16836, 2023.\n[41] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization\nfor vision transformer. Advances in Neural Information Processing Systems, 34, 2021.\n[42] Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using\nLarge Corpora, page 273, 1994.\n[43] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\nIn International Conference on Learning Representations, 2017.\n13\n[44] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning.\narXiv preprint arXiv:2209.05433, 2022.\n[45] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017\nshared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical,\nSentential and Discourse-level Semantics, pages 46\u201351, 2017.\n[46] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up\nor down? adaptive rounding for post-training quantization. In International Conference on Machine\nLearning, pages 7197\u20137206. PMLR, 2020.\n[47] Sameer Narayan, Andre Martins, Alessandro Sordoni, Philip Bachman, Aaron Courville, and Yoshua\nBengio. Don\u2019t give me the details, just the summary!: topic-aware convolutional neural networks\nfor extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3706\u20133716, 2018.\n[48] NVIDIA. cublas. \"https://developer.nvidia.com/cublas\", 2023.\n[49] NVIDIA. Tensorrt-llm. \"https://github.com/NVIDIA/TensorRT-LLM/\", 2023.\n[50] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm:\nQuantized matmul for efficient inference of large-scale generative language models. arXiv preprint\narXiv:2206.09557, 2022.\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[53] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially binarized large language\nmodels. arXiv preprint arXiv:2310.00034, 2023.\n[54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In AAAI, pages\n8815\u20138821, 2020.\n[55] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong.\nCompression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705,\n2022.\n[56] Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein data sets. In 15th International\nConference on Scientific and Statistical Database Management, 2003., pages 141\u2013150. IEEE, 2003.\n[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[58] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar\nSubramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. Fp8 versus int8 for efficient deep\nlearning inference. arXiv preprint arXiv:2303.17951, 2023.\n[59] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong\nLiu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal\nshifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\n14\n[60] Miles Williams and Nikolaos Aletras. How does calibration data affect the post-training pruning and\nquantization of large language models? arXiv preprint arXiv:2311.09755, 2023.\n[61] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4\nquantization for transformer models: Latency speedup, composability, and failure cases. arXiv preprint\narXiv:2301.12017, 2023.\n[62] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8\nquantization using floating-point formats. arXiv preprint arXiv:2307.09782, 2023.\n[63] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for\npre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022.\n[64] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin,\nand Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large generative model\ninference with unstructured sparsity. arXiv preprint arXiv:2309.10285, 2023.\n[65] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language\nmodel pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.\n[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and\nefficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n[67] Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, and Yuxiong He.\nZeroquant-hero: Hardware-enhanced robust optimized post-training quantization framework for w8a8\ntransformers. arXiv preprint arXiv:2310.17723, 2023.\n[68] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint\narXiv:2206.01861, 2022.\n[69] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-\ntraining quantization in llms from comprehensive study to low rank compensation. arXiv preprint\narXiv:2303.08302, 2023.\n[70] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang\nWu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language\nmodels. arXiv preprint arXiv:2304.01089, 2023.\n[71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert. arXiv\npreprint arXiv:1910.06188, 2019.\n[72] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[73] Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision arithmetic\nsimulation framework, 2019.\n[74] Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang\nZhang, and Ningyi Xu. Integer or floating point? new outlooks for low-bit quantization on large language\nmodels. arXiv preprint arXiv:2305.12356, 2023.\n[75] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging\nllm-as-a-judge with mt-bench and chatbot arena, 2023.\n15\n[76] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi\nWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation\nwith multilingual evaluations on humaneval-x. In KDD, 2023.\nA\nBackground of Quantization\nThroughout this work, we focus on post-training quantization (PTQ), i.e., no or minimal training effort\nis applied after quantization, for which large accuracy degradation usually exhibits for coarse-grained\nquantization (per matrix/tensor) due to their large quantization error. Particularly, we use the per-row\nquantization (one row of the weight matrix) from [68] as our coarsest-grained quantization method, and we\nuse block-k quantization (for every k elements, they have their own scaling factor and/or zero point) as our\nfiner-grained quantization scheme.\nTable A.1: GPT4-evaluation for the same model with different precision [75]. There is no clear relation\nbetween different bits and rating performance.\nModel (Precision)\nWriting\nRoleplay\nCoding\nSTEM\nHumanities\nReasoning\nMath\nExtraction\nvicuna-7b-v1.5 (Baseline)\n7.55\n7.95\n3.21\n8.36\n9.68\n4.6\n3.6\n6.4\nvicuna-7b-v1.5 (INT4)\n7.37\n7.60\n3.21\n8.61\n9.34\n5.7\n2.6\n6.2\nvicuna-7b-v1.5 (FP6)\n7.55\n7.88\n3.67\n8.55\n9.21\n4.8\n2.2\n6.1\nvicuna-13b-v1.5 (Baseline)\n7.90\n7.75\n2.73\n7.32\n9.24\n5.2\n2.2\n6.0\nvicuna-13b-v1.5 (INT4)\n8.42\n7.56\n2.91\n7.93\n9.58\n4.7\n2.7\n6.4\nvicuna-13b-v1.5 (FP6)\n8.06\n7.60\n2.94\n7.76\n9.31\n4.8\n2.8\n5.9\nTable A.2: Zero-Shot Evaluation (Accuracy). Compare between GPTQC4 and RTN quantization\nalgorithms for INT4 weight (W4A16) on LLaMA-1B (Top) and LLaMA-13B (Bottom). We apply fine-grain\nquantization (FGQ) in which the block-size is 256 elements per scale except for LLaMA-1B\u2019s (which is 128).\narcC (arcE) stands for arc_challenges (arc_easy).\nModels\nPrecision\nFGQ\narcC\narcE\nboolq\ncb\ncopa\npiqa\nrte\nwic\nwsc\nstorycloze\nMEAN\nLLaMA-1B\nFP16\nN/A\n26.71\n53.11\n61.13\n39.29\n76.00\n73.83\n50.18\n50.00\n36.54\n69.64\n53.64\n(4096-seq)\nINT4-GPTQ\n\u2713\n26.37\n50.59\n61.59\n46.43\n79.00\n73.34\n48.01\n50.00\n36.54\n68.24\n54.01\nINT4-RTN\n\u2713\n26.11\n51.09\n58.07\n50.00\n74.00\n72.91\n48.38\n50.00\n36.54\n68.36\n53.55\nFP5-GPTQ\n\u2717\n25.51\n51.85\n61.22\n32.14\n80.00\n73.01\n48.38\n50.00\n36.54\n68.94\n52.76\nFP5-RTN\n\u2717\n26.62\n50.67\n61.41\n39.29\n79.00\n72.85\n50.18\n50.00\n36.54\n69.19\n53.58\nFP6-GPTQ\n\u2717\n26.37\n52.53\n61.19\n42.86\n75.00\n73.5\n51.26\n50.00\n36.54\n69.76\n53.90\nFP6-RTN\n\u2717\n26.37\n52.95\n60.95\n37.50\n78.00\n73.39\n54.51\n50.00\n36.54\n69.64\n53.99\nLLaMA-13B\nFP16\nN/A\n43.86\n74.58\n68.53\n50.00\n90.00\n79.00\n65.34\n50.00\n35.58\n78.23\n63.51\n(2048-seq)\nINT4-GPTQ\n\u2713\n43.00\n73.44\n67.83\n41.07\n93.00\n78.78\n62.45\n50.16\n36.54\n78.17\n62.44\nINT4-RTN\n\u2713\n44.03\n74.45\n67.37\n44.64\n91.00\n78.84\n63.18\n49.84\n36.54\n78.42\n62.83\nFP5-GPTQ\n\u2717\n42.92\n73.70\n65.81\n44.64\n90.00\n78.67\n64.62\n50.00\n36.54\n78.23\n62.51\nFP5-RTN\n\u2717\n41.72\n74.03\n68.47\n39.29\n90.00\n78.56\n62.45\n50.31\n36.54\n78.61\n62.00\nFP6-GPTQ\n\u2717\n43.69\n73.99\n67.28\n48.21\n90.00\n78.84\n64.98\n50.31\n36.54\n78.42\n63.23\nFP6-RTN\n\u2717\n43.77\n74.20\n68.38\n46.43\n91.00\n78.84\n65.34\n49.84\n36.54\n78.23\n63.26\nLLaMA-65B\nFP16\nN/A\n47.01\n75.08\n82.32\n64.29\n91.00\n81.61\n71.48\n58.31\n60.58\n79.57\n71.13\n(2048-seq)\nINT4-GPTQ\n\u2713\n46.84\n75.08\n80.76\n58.93\n94.00\n81.18\n72.92\n56.27\n60.58\n79.31\n70.59\nINT4-RTN\n\u2713\n47.10\n75.25\n81.47\n62.50\n95.00\n81.23\n69.68\n57.21\n62.50\n79.63\n71.16\nFP5-GPTQ\n\u2717\n46.50\n75.51\n82.35\n69.64\n93.00\n81.28\n71.84\n57.05\n57.69\n79.76\n71.46\nFP5-RTN\n\u2717\n46.50\n75.59\n82.87\n60.71\n94.00\n81.39\n73.65\n57.21\n60.58\n80.08\n71.26\nFP6-GPTQ\n\u2717\n46.84\n74.96\n82.51\n64.29\n91.00\n81.23\n70.04\n59.72\n61.54\n79.63\n71.18\nFP6-RTN\n\u2717\n47.10\n74.66\n82.69\n64.29\n92.00\n81.99\n70.76\n58.15\n57.69\n79.31\n70.86\n16\nB\nDetailed Performance of GPU Kernels\nThe shapes of weight matrices are set according to the model specification of LLaMA-1B, LLaMA-13B, and\nLLaMA-65B, respectively. We mainly evaluate the kernel performance when the inference batch size is 8. As\nfor the Fine-grained INT4 kernel, we set its quantization group size to 128 for the best of its performance.\nAll the kernel latency shown here is measured in milliseconds.\nTable B.1: The corresponding number for Figure 2.\nLayer Name\nWeight Shape\nInput Shape\ncuBLAS\nFine-grained INT4\nFP6 (w/o Bias-Shift)\nFP6 (w/ Bias-Shift)\nFFN1-1b\n5504*2048\n2048*8\n0.016\n0.019\n0.021\n0.013\nFFN2-1b\n2048*5504\n5504*8\n0.02\n0.043\n0.022\n0.016\nFFN1-13b\n13824*5120\n5120*8\n0.118\n0.044\n0.063\n0.052\nFFN2-13b\n5120*13824\n13824*8\n0.109\n0.106\n0.068\n0.053\nFFN1-65b\n22016*8192\n8192*8\n0.263\n0.098\n0.145\n0.111\nFFN2-65b\n8192*22016\n22016*8\n0.266\n0.167\n0.157\n0.114\n17\n"
  },
  {
    "title": "LIME: Localized Image Editing via Attention Regularization in Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.09256.pdf",
    "upvote": "8",
    "text": "LIME: Localized Image Editing via Attention Regularization\nin Diffusion Models\nEnis Simsar1\nAlessio Tonioni3,\u2020\nYongqin Xian3,\u2020\nThomas Hofmann1\nFederico Tombari2,3\n1ETH Z\u00a8urich - DALAB\n2Technical University of Munich\n3Google Switzerland\nInput\nIP2P [6]\n+ LIME\nInput\nIP2P [6] w/MB [52]\n+ LIME\n(a) What if the horse were wearing a hat?\n(b) Put a lighthouse under UFO.\nInput\nIP2P [6]\n+ LIME\nInput\nHIVE [53]\n+ LIME\n(c) Change to a ros\u00b4e.\n(d) Change the plant color to blue.\nFigure 1. LIME: Localized IMage Editing. Our method edits an image based on an edit instruction without needing customized datasets\nor fine-tuning. The four examples are taken from established papers [29, 52, 53] and compare our edits with the respective state-of-the-art\nmodels. The addition of LIME improves all models and allows localized edits that preserve the rest of the image untouched.\nAbstract\nDiffusion models (DMs) have gained prominence due to\ntheir ability to generate high-quality, varied images, with\nrecent advancements in text-to-image generation. The re-\nsearch focus is now shifting towards the controllability of\nDMs. A significant challenge within this domain is localized\nediting, where specific areas of an image are modified with-\nout affecting the rest of the content. This paper introduces\nLIME for localized image editing in diffusion models that\ndo not require user-specified regions of interest (RoI) or ad-\nditional text input. Our method employs features from pre-\ntrained methods and a simple clustering technique to obtain\nprecise semantic segmentation maps. Then, by leveraging\ncross-attention maps, it refines these segments for localized\nedits. Finally, we propose a novel cross-attention regular-\n\u2020Equal advising.\nThe project page can be found at https://enisimsar.github.io/LIME/.\nization technique that penalizes unrelated cross-attention\nscores in the RoI during the denoising steps, ensuring lo-\ncalized edits. Our approach, without re-training and fine-\ntuning, consistently improves the performance of existing\nmethods in various editing benchmarks.\n1. Introduction\nDiffusion models (DMs) have recently achieved remarkable\nsuccess in generating images that are not only high-quality\nbut also richly varied, thanks to advancements in text-to-\nimage conversion [19, 36, 38, 40]. Beyond their generative\ncapabilities, there is a growing research interest in the con-\ntrollability aspect of these models [2, 6, 8, 17, 33, 53]. This\nhas led to the exploration of a variety of editing techniques,\nleveraging the power of DMs for tasks such as personal-\nized image creation [14, 39, 47], context-aware inpainting\n1\narXiv:2312.09256v1  [cs.CV]  14 Dec 2023\n[26, 31, 50], and image transformation in response to textual\nedits [2, 6, 8, 17, 21, 27]. These developments underscore\nthe versatility of DMs and their potential to serve as foun-\ndational tools for various image editing applications.\nIn this paper, we address the task of text-guided im-\nage editing, explicitly focusing on localized editing, which\nrefers to identifying and modifying any region of interest\nin an image. This is done regardless of its size and based\non textual instructions while preserving the context of the\nsurrounding regions. The difficulty arises from the inter-\ntwined nature of image representations within these mod-\nels, where changes intended for one area can inadvertently\naffect others [6, 17, 27, 53]. Existing methods often depend\non additional user input, such as masking the target area,\ni.e., Region of Interest (RoI), or providing additional text\ninformation, e.g., objects of interest, to pinpoint the editing\nregion [2, 8]. However, these approaches introduce com-\nplexity and do not guarantee the precision necessary for\nseamless editing. Figure 1 highlights localized edits with-\nout altering the overall image, a balance that current meth-\nods have not yet struck. Advancing localized editing to be\nmore intuitive and effective remains a pivotal direction.\nWe address the challenge of localized image editing\nby introducing LIME, that leverages pre-trained Instruct-\nPix2Pix [6] without the need for additional supervision,\nuser inputs, or model re-training/fine-tuning. Recent stud-\nies [34, 44, 49] have demonstrated that diffusion models\nare capable of encoding semantics within their intermedi-\nate features. LIME utilizes those features to identify seg-\nments, then extracts RoI by harnessing attention scores de-\nrived from instructions. Other research [1, 7] has shown the\nsignificant impact of attention-based guidance on the com-\nposition of an image. Accordingly, LIME aims to restrict\nthe scope of edits by regularizing attention scores to enable\ndisentangled and localized edits. By improving these two\nlines of work, LIME not only offers more effective localized\nediting as shown in Fig. 1 but also demonstrates a notable\nadvancement by quantitatively outperforming current state-\nof-the-art methods on four different benchmark datasets.\nOur pipeline contains two steps. It first finds semantic\nsegments of the input image. This is achieved based on se-\nmantic information encoded in intermediate features. Then,\nwe identify the area to be edited by combining the segments\nwith large cross-attention scores toward the edit instruction.\nOnce we isolate the area to be edited, i.e., RoI, the proposed\nattention regularization technique is applied to the text to-\nkens to selectively target the RoI to ensure that subsequent\nediting is accurately focused, avoiding unintended changes\nto other parts of the image. This two-step approach, first\nrefining targeted areas and then editing within the RoI, en-\nsures that our modifications are accurate and contextually\ncoherent, simplifying the editing process while avoiding un-\nintended alterations to the rest of the image.\nThe core contributions of this study are:\n\u2022 We introduce a localized image editing technique that\neliminates the need for fine-tuning or re-training, ensur-\ning efficient and precise localized edits.\n\u2022 Our approach leverages the pre-trained model\u2019s interme-\ndiate features to segment the image and to identify the\nregions where modifications will be applied.\n\u2022 An attention regularization strategy is proposed, which\nis employed to achieve disentangled and localized edits\nwithin the RoI, ensuring contextually coherent edits.\nThe experimental evaluation demonstrates that our ap-\nproach outperforms existing methods in localized editing\nboth qualitatively and quantitatively on four benchmark\ndatasets [5, 6, 20, 52].\n2. Related Work\nText-guided image generation.\nText-to-image synthesis\nsignificantly advanced thanks to diffusion models that sur-\npassed prior generative adversarial networks (GANs) [16,\n37, 51]. Key developments [10, 19, 43] have resulted in\ndiffusion models that generate highly realistic images from\ntextual inputs [31, 36, 40]. Notably, the introduction of la-\ntent diffusion models has significantly increased the com-\nputational efficiency of previous methods [38].\nImage editing with Diffusion Models.\nOne direction\nfor image editing is utilizing pre-trained diffusion mod-\nels by first inverting the input image in the latent space\nand then applying the desired edit by altering the text\nprompt [8, 17, 20, 27, 30, 32, 45, 46, 48]. For instance,\nDirectInversion [20] inverts the input image and then ap-\nplies Prompt2Prompt [17] to obtain the desired edit, but\nit may lose details of the input image during inversion.\nDiffEdit [8], on the other hand, matches the differences\nin predictions for input and output captions to localize the\nedit yet struggles with complex instructions. It works in\nthe noise space to edit. Another direction for image edit-\ning by using instructions is training diffusion models on\ntriplet data, which contains input image, instruction, and\ndesired image [6, 13, 52, 53]. The latest approach, Instruct-\nPix2Pix (IP2P) [6] uses a triplet dataset to train a model for\nediting images by using instructions. It performs better than\nprevious methods but sometimes generates entangled edits.\nTo tackle this problem, HIVE [53] relies on human feed-\nback on edited images to learn what users generally pre-\nfer and uses this information to fine-tune IP2P, aiming to\nalign more closely with human expectations. Alternatively,\nour method leverages the pre-trained IP2P to localize the\nedit instruction. Then, instead of manipulating the noise\nspace [2, 8, 29], our method employs attention regulariza-\ntion to achieve localized editing, ensuring the edits are re-\nstricted within the RoI. The entire process is done without\nneeding additional data, re-training, or fine-tuning.\n2\nSemantics in Diffusion Models.\nIntermediate features of\ndiffusion models, as explored in studies like [33, 34, 44, 49],\nhave been shown to encode semantic information.\nRe-\ncent research such as LD-ZNet [34] and ODISE [49] lever-\nages intermediate features of these models for training net-\nworks for semantic segmentation. Localizing Prompt Mix-\ning (LPM) [33], on the other hand, utilizes clustering on\nself-attention outputs for segment identification. Motivated\nby this success, our method leverages pre-trained interme-\ndiate features to achieve semantic segmentation and apply\nlocalized edits using edit instructions.\n3. Background\nLatent Diffusion Models.\nStable Diffusion (SD) [38] is\na Latent Diffusion Model (LDM) designed to operate in a\ncompressed latent space. This space is defined at the bot-\ntleneck of a pre-trained variational autoencoder (VAE) to\nenhance computational efficiency. Gaussian noise is intro-\nduced into the latent space, generating samples from a latent\ndistribution zt. A U-Net-based denoising architecture [10]\nis then employed for image reconstruction, conditioned on\nnoise input (zt) and text conditioning (cT ).\nThis recon-\nstruction is iteratively applied over multiple time steps, each\ninvolving a sequence of self-attention and cross-attention\nlayers. Self-attention layers transform the current noised\nimage representation, while cross-attention layers integrate\ntext conditioning.\nEvery attention layer comprises three components:\nQueries (Q), Keys (K), and Values (V ). For cross-attention\nlayers, Qs are obtained by applying a linear transformation\nfQ to the result of the self-attention layer preceding the\ncross-attention layer (i.e., image features). Similarly, Ks\nand V s are derived from text conditioning cT using linear\ntransformations fK and fV . Equation (1) shows the math-\nematical formulation of an attention layer where P denotes\nthe attention maps and is obtained as the softmax of the dot\nproduct of K and Q normalized by the square root of di-\nmension d of Ks and Qs.\nAttention(Q, K, V ) = P \u00b7 V,\nwhere P = Softmax\n\u0012QKT\n\u221a\nd\n\u0013\n.\n(1)\nIntuitively, P denotes which areas of the input features\nwill be modified in the attention layer. For cross-attention,\nthis is the area of the image that is affected by one of the\nconditioning text tokens that define cT . Beyond these at-\ntention maps, our approach also leverages the output of\ntransformer layers, noted as intermediate features \u03d5(zt),\nwhich contain rich semantic content, as highlighted in re-\ncent studies [34, 44, 49]. In this work, we modify the cross-\nattention\u2019s P and leverage the intermediate features \u03d5(zt)\nto localize edits in pre-trained LDMs.\nInstructPix2Pix.\nOur\nmethod\nrelies\non\nInstruct-\nPix2Pix (IP2P) [6], an image-to-image transformation\nnetwork trained for text-conditioned editing. IP2P builds\non top of Stable Diffusion and incorporates a bi-conditional\nframework, which simultaneously leverages an input image\nI, and an accompanying text-based instruction T to steer\nthe synthesis of the image, with the conditioning features\nbeing cI for the image and cT for the text.\nThe image\ngeneration workflow is modulated through a classifier-free\nguidance (CFG) strategy [18] that employs two sepa-\nrate coefficients, sT for text condition and sI for image\ncondition.\nThe noise vectors predicted by the learned\nnetwork e\u03b8, which corresponds to the individual U-Net\nstep, with different sets of inputs, are linearly combined as\nrepresented in Eq. (2) to achieve score estimate \u02dce\u03b8. Our\nmethod utilizes and modifies the processes for the terms\nwith cI in Eq. (2) to apply localized image editing.\n\u02dce\u03b8(zt, cI, cT ) = e\u03b8(zt, \u2205, \u2205)\n+ sI \u00b7 (e\u03b8(zt, cI, \u2205) \u2212 e\u03b8(zt, \u2205, \u2205))\n+ sT \u00b7 (e\u03b8(zt, cI, cT ) \u2212 e\u03b8(zt, cI, \u2205)).\n(2)\n4. Method\nWe aim to develop a localized editing method for a pre-\ntrained IP2P without re-training or fine-tuning. The pro-\nposed method contains two components: (i) edit localiza-\ntion finds the RoI by incorporating the input image and the\nedit instruction, and (ii) edit application applies the instruc-\ntion to RoI in a disentangled and localized manner.\n4.1. Edit Localization\nSegmentation:\nOur study extends the established under-\nstanding that intermediate features of diffusion models en-\ncode essential semantic information. In contrast to previous\nmethods that build upon Stable Diffusion [34, 44, 49], our\napproach works on IP2P and focuses on the features condi-\ntioned on the original image (zt, cI, and \u2205) for segmenta-\ntion as indicated in Eq. (2). Through experimental observa-\ntion, we show that these features align well with segmen-\ntation objectives for editing purposes. To obtain segmenta-\ntion maps, we extract features from multiple layers of the\nU-Net architecture, including both down- and up-blocks, to\nencompass a variety of resolutions and enhance the seman-\ntic understanding of the image. Our preference for interme-\ndiate features over attention maps is based on their superior\ncapability to encode richer semantic information, as verified\nby studies such as [34, 44, 49].\nWe implement a multi-resolution fusion strategy to refine\nthe feature representations within our proposed model. This\ninvolves (i) resizing feature maps from various resolutions\nto a common resolution by applying bi-linear interpolation,\n3\n(ii) concatenating and normalizing them along the channel\ndimension, and (iii) finally, applying a clustering method,\nsuch as the K-means algorithm, on fused features. We aim\nto retain each feature set\u2019s rich, descriptive qualities by fol-\nlowing these steps. Moreover, each resolution in the U-Net\nstep keeps different granularity of the regions in terms of\nsemantics and sizes.\nFigure 2 demonstrates segmentation maps from different\nresolutions and our proposed fused features. Each resolu-\ntion captures different semantic components of the image,\ne.g., field, racket, hat, dress.... Although Resolution 64 can\ndistinguish objects, e.g., skin and outfit, it does not provide\nconsistent segment areas, e.g., two distinct clusters for lines\nin the field. On the other hand, lower resolutions, Resolu-\ntion 16 and 32, can capture coarse segments like lines in the\nfield and the racket. Fusing those features from different\nresolutions yields more robust feature representations, en-\nhancing the segmentation; see Fig. 2 - Ours. For the extrac-\ntion of intermediate features, we use time steps between 30\nand 50 out of 100 steps, as recommended by LD-ZNet [34].\nInput\nResolution 16\nResolution 32\nResolution 64\nOurs\nAttention\nRoI\nInstruction:\nMake her outfit\nblack\n# of clusters: 8\nFigure 2. Segmentation and RoI finding. Resolution Xs demon-\nstrates segmentation maps from different resolutions, while Ours\nshows the segmentation map from our method. For the cross-\nattention map, the color yellow indicates high probability, and blue\ndots mark the 100 pixels with the highest probability. The last im-\nage shows the extracted RoI using blue dots and Ours.\nLocalization:\nUpon identifying the segments within the\ninput image, the proposed method identifies the RoI for\nthe edit using cross-attention maps conditioned on the in-\nput image and instruction (zt, cI, and cT ) as indicated in\nEq. (2). These maps have dimensions of Hb \u00d7 Wb \u00d7 D,\nwhere Hb and Wb represent the height and width of the fea-\ntures of block bth (up and down blocks), respectively, and\nD denotes the number of text tokens. Following our seg-\nmentation strategy, the cross-attention maps are resized to\na common resolution, combined among the spatial dimen-\nsions, namely H and W, and normalized among the token\ndimension, D. After merging attention maps from different\nresolutions, the method ignores the <start of text>, stop\nwords, and padding tokens to ignore noisy attention values\nfrom unrelated parts of the conditioning text and focuses\non the remaining tokens to identify the area that is related\nto the edit instruction. Then, we get the mean attention\nscore among the tokens to generate a final attention map;\nsee Fig. 2 - Attention. Subsequently, the top 100 pixels, ab-\nlated in Tab. 4, marked by highest probability scores, are\nidentified. Then, all segments that overlap at least one of\nthose pixels are combined to obtain the RoI; see Fig. 2 -\nOurs, Attention, and RoI.\n4.2. Edit Application\nLeveraging the strength of the pre-trained models, we intro-\nduce a novel localized editing technique within IP2P. This\nmodule manipulates attention scores corresponding to the\nRoI while ensuring the rest of the image remains the same,\nthus preventing any unintended alterations outside the RoI.\nSpecifically, this procedure uses the terms with zt, cI, and\ncT using the notation of Eq. (2).\nToken-based cross attention probabilites\nBefore\nAfter\nFigure 3. Attention Regularization. Our method selectively reg-\nularizes unrelated tokens within the RoI, ensuring precise, context-\naware edits without the need for additional model training or extra\ndata. After attention regularization, the probabilities for the related\ntokens are attending the RoI, as illustrated in the second row.\nAttention Regularization:\nPrevious methods [2, 8, 29]\nuse the noise space instead of attention scores. In contrast,\nour method introduces targeted attention regularization for\nselectively reducing the influence of unrelated tokens within\nthe RoI during editing. This approach regularizes attention\nscores for tokens that are unrelated to the editing task, such\nas <start of text>, padding, and stop words (denoted as S).\nBy adjusting the attention scores (QKT ) within the RoI,\nwe aim to minimize the impact of these unrelated tokens\nduring the softmax normalization process. As a result, the\nsoftmax function is more likely to assign higher attention\nprobabilities within the RoI to tokens that align with the\nediting instructions. This targeted approach ensures that ed-\nits are precisely focused on the desired areas, enhancing the\naccuracy and effectiveness of the edits while preserving the\nrest. Given the binary mask for RoI M, we modify the re-\nsult of the dot product QKT of cross-attention layers for\nunrelevant tokens to a regularization version R(QKT , M)\nas follows:\n4\nInput Image\nIP2P [6]\nRoI\n+ LIME\nInput Image\nIP2P [6]\nRoI\n+ LIME\n(a)\nMake her outfit black.\n(b)\nTurn the real birds into origami birds.\n(c)\nPut blue glitter on fingernails.\n(d)\nAdd a pond.\nInput Image\nIP2P [6] w/MB [52]\nRoI\n+ LIME\nInput Image\nIP2P [6] w/MB [52]\nRoI\n+ LIME\n(e)\nReplace the ground with a forest.\n(f)\nRemove mint leaves.\nFigure 4. Qualitative Examples. We test our method on different tasks: (a) editing a large segment, (b) altering texture, (c) editing multiple\nsegments, (d) adding, (e) replacing, and (f) removing objects. Examples are taken from established papers [20, 52, 53]. The integration of\nLIME enhances the performance of all models, enabling localized edits while maintaining the integrity of the remaining image areas.\nR(QKT , M) =\n(\nQKT\nijt \u2212 \u03b1,\nif Mij = 1 and t \u2208 S\nQKT\nijt,\notherwise,\n(3)\nwhere \u03b1 is a large value. Intuitively, we prevent unrelated\ntokens from attending to the RoI, as shown in Fig. 3. In\ncontrast, related tokens will be more likely to be selected\nin the RoI, leading to more accurate, localized, and focused\nedits. This method achieves an optimal balance between\ntargeted editing within the intended areas and preserving\nthe surrounding context, thus enhancing the overall effec-\ntiveness of the instruction.\nBy employing this precise regularization technique\nwithin the RoI, our method significantly enhances IP2P. It\nelevates the degree of disentanglement and improves the lo-\ncalization of edits by tapping into the already-learned fea-\ntures of the model.\nThis targeted approach circumvents\nthe need for re-training or fine-tuning, preserving computa-\ntional resources and time. It harnesses the inherent strength\nof the pre-trained IP2P features, deploying them in a fo-\ncused and effective manner. This precision ensures that ed-\nits are contained within the intended areas, underpinning the\nmodel\u2019s improved capability to execute complex instruc-\ntions in a localized and controlled way without the necessity\nfor additional rounds of training or fine-tuning.\n5. Experiments\n5.1. Evaluation Datasets and Metrics\nCombining diverse datasets and metrics ensures a thorough\nevaluation of our proposed method. For each dataset, we\nreport the metrics proposed in the corresponding work.\nMagicBrush [52].\nThe test split offers a comprehensive\nevaluation pipeline with 535 sessions and 1053 turns. Ses-\nsions refer to the source images used for iterative editing\ninstructions, and turns denote the individual editing steps\nwithin each session. It employs L1 and L2 norms to mea-\nsure pixel accuracy, CLIP-I, and DINO embeddings for as-\nsessing image quality via cosine similarity, and CLIP-T to\nensure that the generated images align accurately with local\ntextual descriptions.\nInstructPix2Pix [6].\nWe evaluate our method on Instruct-\nPix2Pix test split with 5K image-instruction pairs. Metrics\ninclude CLIP image similarity for visual fidelity and CLIP\ntext-image direction similarity to measure adherence to the\nediting instructions.\nPIE-Bench [20].\nThe benchmark includes 700 images in\n10 editing categories with input/output captions, editing\ninstructions, input images, and RoI annotations. Metrics\nfor structural integrity and background preservation are de-\nrived from cosine similarity measures and image metrics\nlike PSNR, LPIPS, MSE, and SSIM, while text-image con-\nsistency is evaluated via CLIP Similarity.\n5\nMethods\nSingle-turn\nMulti-turn\nMB\nL1 \u2193\nL2 \u2193\nCLIP-I \u2191\nDINO \u2191\nCLIP-T \u2191\nL1 \u2193\nL2 \u2193\nCLIP-I \u2191\nDINO \u2191\nCLIP-T \u2191\nOpen-Edit [25]\n\u2717\n0.143\n0.043\n0.838\n0.763\n0.261\n0.166\n0.055\n0.804\n0.684\n0.253\nVQGAN-CLIP [9]\n\u2717\n0.220\n0.083\n0.675\n0.495\n0.388\n0.247\n0.103\n0.661\n0.459\n0.385\nSDEdit [27]\n\u2717\n0.101\n0.028\n0.853\n0.773\n0.278\n0.162\n0.060\n0.793\n0.621\n0.269\nText2LIVE [4]\n\u2717\n0.064\n0.017\n0.924\n0.881\n0.242\n0.099\n0.028\n0.880\n0.793\n0.272\nNull-Text Inv. [30]\n\u2717\n0.075\n0.020\n0.883\n0.821\n0.274\n0.106\n0.034\n0.847\n0.753\n0.271\nHIVE [53]\n\u2717\n0.109\n0.034\n0.852\n0.750\n0.275\n0.152\n0.056\n0.800\n0.646\n0.267\nHIVE [53] + LIME\n\u2717\n0.051\n0.016\n0.940\n0.909\n0.293\n0.080\n0.029\n0.894\n0.829\n0.283\nHIVE [53]\n\u2713\n0.066\n0.022\n0.919\n0.866\n0.281\n0.097\n0.037\n0.879\n0.789\n0.280\nHIVE [53] + LIME\n\u2713\n0.053\n0.016\n0.939\n0.906\n0.300\n0.080\n0.028\n0.899\n0.829\n0.295\nIP2P [6]\n\u2717\n0.112\n0.037\n0.852\n0.743\n0.276\n0.158\n0.060\n0.792\n0.618\n0.273\nIP2P [6] + LIME\n\u2717\n0.058\n0.017\n0.935\n0.906\n0.293\n0.094\n0.033\n0.883\n0.817\n0.284\nIP2P [6]\n\u2713\n0.063\n0.020\n0.933\n0.899\n0.278\n0.096\n0.035\n0.892\n0.827\n0.275\nIP2P [6] + LIME\n\u2713\n0.056\n0.017\n0.939\n0.911\n0.297\n0.088\n0.030\n0.894\n0.835\n0.294\nTable 1. Evaluation on MagicBrush Dataset [52]. Results for single-turn and multi-turn settings are presented for each method and MB\nstands for models fine-tuned on MagicBrush. The benchmark values for other approaches are sourced from [52], while values for our\nproposed method are computed following the same protocol. Across both settings, our method surpasses the base models performance of\nthe compared models. The top-performing is highlighted in bold, while the second-best is denoted with underline for each block.\nEditVal [5].\nThe benchmark offers 648 image edit-\ning operations spanning 19 classes from the MS-COCO\ndataset [24]. The benchmark assesses the success of each\nedit with a binary score that indicates whether the edit type\nwas successfully applied. The OwL-ViT [28] model is uti-\nlized to detect the object of interest, and detection is used to\nassess the correctness of the modifications.\n5.2. Implementation Details\nOur method adopts InstructPix2Pix [6] as its base model\nand runs the model for 100 steps for each stage explained\nin Secs. 4.1 and 4.2. Specifically, during Edit Localization,\nintermediate representations are extracted between 30 and\n50 out of 100 steps, as suggested in LD-ZNet [34]. More-\nover, those intermediate features are resized to 256 \u00d7 256.\nThe number of clusters for segmenting is 8 across all ex-\nperiments, motivated by an ablation study. Concurrently,\nwe gather features from steps 1 to 75 for the cross-attention\nmaps and retain only related tokens. We extract 100 pix-\nels with the highest probabilities from the attention maps to\nidentify RoI and determine overlapping segments. For Edit\nLocalization, the image scale sI and the text scale sT , the\nparameters are 1.5 and 7.5, respectively. During Edit Ap-\nplication, the attention regularization is employed between\nsteps 1 and 75, targeting only unrelated tokens. Throughout\nthe editing process, the image scale, sI, and the text scale,\nsT , parameters are set to 1.5 and 3.5, respectively.\n5.3. Qualitative Results\nFigure 4 presents qualitative examples for various editing\ntasks. These tasks include editing large segments, altering\ntextures, editing multiple small segments simultaneously,\nand adding, replacing, or removing objects. The first col-\numn displays the input images, with the corresponding edit\ninstructions below each image. The second column illus-\ntrates the results generated by the base models without our\nproposed method. The third and fourth columns report the\nRoI identified by our method and the edited output pro-\nduced by the base models when our regularization method\nis applied to these RoIs. As shown in Fig. 4, our method\neffectively implements the edit instructions while preserv-\ning the overall scene context. In all presented results, our\nmethod surpasses current state-of-the-art models, including\ntheir fine-tuned versions on manually annotated datasets,\ne.g., MagicBrush [52]. Furthermore, as also claimed and\nreported in HIVE [53], without additional training, IP2P\ncannot perform a successful edit for (d) in Fig. 4. However,\nour proposed method achieves the desired edit without any\nadditional training on the base model as shown Fig. 4 - (d).\n5.4. Quantitative Results\nResults on MagicBrush.\nOur method outperforms all\nother methods on both the single- and multi-turn editing\ntasks on MagicBrush (MB) [52] benchmark, as seen in\nTab. 1. Compared to the base models, our approach pro-\nvides significant improvements and best results in terms of\nL1, L2, CLIP-I, and DINO. For the CLIP-T metric, which\ncompares the edited image and caption to the ground truth,\nour method comes very close to the oracle scores of 0.309\nfor multi-turn and 0.307 for single-turn.\nThis indicates\nthat our edits accurately reflect the ground truth modifica-\ntions. VQGAN-CLIP [9] achieves the highest in CLIP-T\n6\nMetrics\nStructure\nBackground Preservation\nCLIP Similarity\nMethods\nGT Mask\nDistance\u00d7103 \u2193\nPSNR \u2191\nLPIPS\u00d7103 \u2193\nMSE\u00d7104 \u2193\nSSIM\u00d7102 \u2191\nWhole \u2191\nEdited \u2191\nInstructDiffusion [15]\n\u2717\n75.44\n20.28\n155.66\n349.66\n75.53\n23.26\n21.34\nBlendedDiffusion [3]\n\u2713\n81.42\n29.13\n36.61\n19.16\n86.96\n25.72\n23.56\nDirectInversion + P2P [20]\n\u2717\n11.65\n27.22\n54.55\n32.86\n84.76\n25.02\n22.10\nIP2P [6]\n\u2717\n57.91\n20.82\n158.63\n227.78\n76.26\n23.61\n21.64\nIP2P [6] + LIME\n\u2717\n32.80\n21.36\n110.69\n159.93\n80.20\n23.73\n21.11\nIP2P [6] + LIME\n\u2713\n26.33\n24.78\n89.90\n105.19\n82.26\n23.81\n21.10\nIP2P [6] w/MB [52]\n\u2717\n22.25\n27.68\n47.61\n40.03\n85.82\n23.83\n21.26\nIP2P [6] w/MB [52] + LIME\n\u2717\n10.81\n28.80\n41.08\n27.80\n86.51\n23.54\n20.90\nIP2P [6] w/MB [52] + LIME\n\u2713\n10.23\n28.96\n39.85\n27.11\n86.72\n24.02\n21.09\nHIVE [53]\n\u2717\n56.37\n21.76\n142.97\n159.10\n76.73\n23.30\n21.52\nHIVE [53] + LIME\n\u2717\n37.05\n22.90\n112.99\n107.17\n78.67\n23.41\n21.12\nHIVE [53] + LIME\n\u2713\n33.76\n24.14\n103.63\n94.01\n81.18\n23.62\n21.21\nHIVE [53] w/MB [52]\n\u2717\n34.91\n20.85\n158.12\n227.18\n76.47\n23.90\n21.75\nHIVE [53] w/MB [52] + LIME\n\u2717\n26.98\n26.09\n68.28\n63.70\n84.58\n23.96\n21.36\nHIVE [53] w/MB [52] + LIME\n\u2713\n25.86\n28.43\n50.33\n43.25\n86.67\n24.23\n21.43\nTable 2. Evaluation on PIE-Bench Dataset [20]. Comparison across ten edit types shows our method outperforming base models on text-\nguided image editing models. The numbers for the first block are taken from the benchmark paper [20]. The top-performing is highlighted\nin bold, while the second-best is denoted with underline for each block. GT Mask stands for ground-truth masks as regions of interest.\nby directly using CLIP [35] for fine-tuning during infer-\nence.\nHowever, this can excessively alter images, lead-\ning to poorer performance in other metrics. Overall, the\nperformance across metrics shows that our approach gen-\nerates high-quality and localized image edits based on in-\nstructions, outperforming prior state-of-the-art methods.\nResults on PIE-Bench.\nOur quantitative analysis on the\nPIE-Bench [20] demonstrates the effectiveness of our pro-\nposed method. Compared to baseline models like Instruct-\nPix2Pix [6] and fine-tuned versions on MagicBrush [52]\nand HIVE [53], our method achieves significantly bet-\nter performance on metrics measuring structure and back-\nground preservation.\nThis indicates that our approach\nmakes localized edits according to the instructions while\navoiding unintended changes to unaffected regions. At the\nsame time, our method obtains comparable results to base\nmodels on the CLIP similarity score, showing it applies ed-\nits faithfully based on the textual instruction. A compre-\nhensive comparison is presented in Tab. 2. Overall, the\nquantitative results validate that our method can enable text-\nguided image editing by making precise edits solely based\non the given instruction without altering unrelated parts.\nResults on EditVal.\nIn evaluation using the EditVal\nbenchmark dataset [5], our method exhibits superior per-\nformance across various edit types, particularly excelling\nin Object Addition (O.A.), Position Replacement (P.R.), and\nPositional Addition (P.A.), while achieving second-best in\nObject Replacement (O.R.). In particular, it performs com-\nparable to other methods for edits involving Size (S.) and Al-\nter Parts (A.P.). A comprehensive comparison is presented\nin Tab. 3. Overall, the method advances the state-of-the-art\nby improving the average benchmark results by a margin of\n5% over the previous best model.\nMethod\nO.A.\nO.R.\nP.R.\nP.A.\nS.\nA.P.\nAvg.\nSINE [54]\n0.47\n0.59\n0.02\n0.16\n0.46\n0.30\n0.33\nNText. [30]\n0.35\n0.48\n0.00\n0.20\n0.52\n0.34\n0.32\nIP2P [6]\n0.38\n0.39\n0.07\n0.25\n0.51\n0.25\n0.31\nImagic [21]\n0.36\n0.49\n0.03\n0.08\n0.49\n0.21\n0.28\nSDEdit [27]\n0.35\n0.06\n0.04\n0.18\n0.47\n0.33\n0.24\nDBooth [39]\n0.39\n0.32\n0.11\n0.08\n0.28\n0.22\n0.24\nTInv. [14]\n0.43\n0.19\n0.00\n0.00\n0.00\n0.21\n0.14\nDiffEdit [8]\n0.34\n0.26\n0.00\n0.00\n0.00\n0.07\n0.11\nIP2P + LIME\n0.48\n0.49\n0.21\n0.34\n0.49\n0.28\n0.38\nTable 3. Evaluation on EditVal Dataset [5]. Comparison across\nsix edit types shows our method outperforming eight state-of-the-\nart text-guided image editing models. The numbers for other meth-\nods are directly taken from the benchmark paper [5] and the same\nevaluation setup is applied to our method. The top-performing\nis highlighted in bold, while the second-best is denoted with\nunderline for each block.\nResults on InstructPix2Pix.\nWe evaluate our method uti-\nlizing the same setup as InstructPix2Pix, presenting results\non a synthetic evaluation dataset [6] as shown in Fig. 5.\nOur approach notably improves the base model, IP2P, op-\ntimizing the trade-off between the input image and the\ninstruction-based edit. Additionally, while an increase in\ntext scale, sT , enhances the CLIP Text-Image Direction\nSimilarity, it adversely impacts CLIP Image Similarity. For\nboth metrics, the higher, the better. The black arrow indi-\ncates the selected configuration for the results in this paper.\n7\n0.05\n0.10\n0.15\nCLIP Text-Image Direction Similarity\n0.75\n0.85\n0.95\nCLIP Image Similarity\nOurs (T=5.5, C=8)\nOurs (T=5.5, C=16)\nOurs (T=3.5, C=8)\nOurs (T=3.5, C=16)\nInstructPix2Pix\nFigure 5. InstructPix2Pix Test. Trade-off between input image\n(Y-axis) and edit (X-axis) is showed. T and C denotes sT and # of\nclusters, respectively. For all experiments, sI \u2208 [1.0, 2.2] is fixed.\nThe arrow points to the chosen configuration for our results.\n5.5. Ablation Study\nAblation studies analyze the impact of three key compo-\nnents: the RoI finding method, the number of points from\nattention maps, and the number of clusters. InstructPix2Pix\nis the base architecture. Evaluation is on the MagicBrush\ndataset. Each parameter is modified separately, while other\nparameters are kept fixed to isolate their impact.\nRoI finding methods.\nThe ground truth masks of Mag-\nicBrush [52] are not very tight around the edit area, see Sup-\nplementary Material for visualizations. For this reason, our\nmethod with predicted masks achieves the best performance\nfor the L1, L2, CLIP-I, and DINO metrics while having on-\npar results with CLIP-T compared to the use of ground truth\nmasks, as shown in Tab. 4. We also compare the segmenta-\ntion predicted by adapting the state-of-the-art LPM [33] to\nIP2P by utilizing the official code base1. Even in this case,\nour method achieves better results.\nNumber of points from attention maps.\nUsing only 25\npoints worsens performance, as it cannot capture multiple\ndistinct segments within RoI. However, having more points\nincludes excessive noise, causing more segments to improp-\nerly merge and expanding the RoI area. 100 points provide\nbetter RoI, as shown in Tab. 4.\nNumber of clusters.\nA few clusters like 4 led to large\nsegments and an expanded RoI, preventing localized edits.\nIncreasing the number of clusters, like 16 or 32, causes the\nseparation of a single RoI into multiple clusters. As shown\nin Tab. 4, 8 achieves the best results.\nEdit Application.\nInstead of attention regularization,\nediting can also be performed in noise space [2, 8, 29]. This\n1https://github.com/orpatashnik/local-prompt-mixing\nMethod\nL1 \u2193\nL2 \u2193\nCLIP-I \u2191\nDINO \u2191\nCLIP-T \u2191\nIP2P [6]\n0.112\n0.037\n0.852\n0.743\n0.276\nMask\nGT\n0.063\n0.017\n0.935\n0.902\n0.297\nLPM [33]\n0.072\n0.019\n0.924\n0.886\n0.291\nOurs\n0.058\n0.017\n0.935\n0.906\n0.293\n# Points\n25\n0.079\n0.023\n0.917\n0.874\n0.290\n100\n0.058\n0.017\n0.935\n0.906\n0.293\n225\n0.065\n0.018\n0.932\n0.901\n0.295\n400\n0.070\n0.020\n0.925\n0.889\n0.295\n# Clusters\n4\n0.080\n0.022\n0.923\n0.885\n0.295\n8\n0.058\n0.017\n0.935\n0.906\n0.293\n16\n0.062\n0.018\n0.933\n0.903\n0.294\n32\n0.064\n0.018\n0.932\n0.901\n0.291\nEdit\nNoise\n0.076\n0.022\n0.914\n0.864\n0.291\nOurs\n0.058\n0.017\n0.935\n0.906\n0.293\nTable 4. Ablation Study. For fair comparison, all parameters\nare the same for all settings except the ablated parameter. The\ntop-performing is highlighted in bold, while the second-best is de-\nnoted with underline for each block.\ncorresponds to a linear blending of the input image and a\nreference image derived from the edit text in noise space,\naccording to the RoI. However, alignment between the ref-\nerence and input images in the edited area is crucial for tar-\ngeting the RoI effectively. As shown in Tab. 4 - Edit, our\nmethod enhances editing precision by employing attention\nregularization.\n6. Conclusion\nIn this paper, we introduce, LIME, a novel localized im-\nage editing technique using IP2P modified with explicit seg-\nmentation of the edit area and attention regularization. This\napproach effectively addresses the challenges of precision\nand context preservation in localized editing, eliminating\nthe need for user input or model fine-tuning/retraining. The\nattention regularization step of our method can also be uti-\nlized with a user-specified mask, offering additional flexi-\nbility. Our method\u2019s robustness and effectiveness are vali-\ndated through empirical evaluations, outperforming existing\nstate-of-the-art methods. This advancement contributes to\nthe continuous evolution of LDMs in image editing, point-\ning toward exciting possibilities for future research.\nInput\nIP2P [6]\nOurs\nInput\n. . . it . . .\n. . . zebra . . .\nColor the tie blue.\nMake \u27e8 word \u27e9 drink from a bucket.\nFigure 6. Failure Cases & Limitations. Left: Base model entan-\nglement. Right: Feature mixing issue.\n8\nLimitations.\nFigure 6 shows limitations of our method:\n(i) shows the limitation due to the pre-trained base model\u2019s\ncapabilities. Our method can focus on the RoI and success-\nfully apply edits but may alter the scene\u2019s style, particularly\nin color, due to the base model entanglement. However, our\nproposal significantly improves the edit compared to IP2P.\n(ii) illustrates how prompt content impacts edit quality. Dur-\ning editing, all tokens except <start of text>, stop words,\nand padding, affect the RoI, leading to feature mixing.\nReferences\n[1] Aishwarya Agarwal, Srikrishna Karanam, K J Joseph,\nApoorv Saxena, Koustava Goswami, and Balaji Vasan Srini-\nvasan. A-star: Test-time attention segregation and retention\nfor text-to-image synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n2283\u20132293, 2023. 2\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-\n24, 2022, pages 18187\u201318197. IEEE, 2022. 1, 2, 4, 8\n[3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics (TOG), 42\n(4):1\u201311, 2023. 7, 12, 13\n[4] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In Computer Vision - ECCV 2022 - 17th\nEuropean Conference, Tel Aviv, Israel, October 23-27, 2022,\nProceedings, Part XV, pages 707\u2013723. Springer, 2022. 6, 12\n[5] Samyadeep Basu,\nMehrdad Saberi,\nShweta Bhardwaj,\nAtoosa Malemir Chegini, Daniela Massiceti, Maziar San-\njabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking\ndiffusion based text-guided image editing methods. arXiv\npreprint arXiv:2310.02426, 2023. 2, 6, 7\n[6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nCoRR, abs/2211.09800, 2022. 1, 2, 3, 5, 6, 7, 8, 12, 14, 15,\n17, 18\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 2\n[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. Diffedit: Diffusion-based semantic image\nediting with mask guidance. In The Eleventh International\nConference on Learning Representations, 2023. 1, 2, 4, 7, 8,\n12\n[9] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. VQGAN-CLIP: open domain image generation\nand editing with natural language guidance. In Computer Vi-\nsion - ECCV 2022 - 17th European Conference, Tel Aviv, Is-\nrael, October 23-27, 2022, Proceedings, Part XXXVII, pages\n88\u2013105. Springer, 2022. 6, 12, 13\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2, 3\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 12\n[12] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2021, virtual, June 19-25, 2021, pages 12873\u201312883.\nComputer Vision Foundation / IEEE, 2021. 12\n[13] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang,\nYinfei Yang, and Zhe Gan. Guiding instruction-based im-\nage editing via multimodal large language models.\narXiv\npreprint arXiv:2309.17102, 2023. 2\n[14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion, 2022. 1, 7, 12\n[15] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang\nGu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong\nChen, et al.\nInstructdiffusion: A generalist modeling in-\nterface for vision tasks. arXiv preprint arXiv:2309.03895,\n2023. 7\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. CoRR, abs/2208.01626,\n2022. 1, 2, 12\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 3\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1, 2\n[20] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and\nQiang Xu. Direct inversion: Boosting diffusion-based edit-\ning with 3 lines of code. arXiv preprint arXiv:2310.01506,\n2023. 2, 5, 7, 12, 13, 15, 17\n[21] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. CoRR,\nabs/2210.09276, 2022. 2, 7, 12\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chlo\u00b4e Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar,\nand Ross B. Girshick.\nSegment anything.\nCoRR,\nabs/2304.02643, 2023. 14\n[23] Pavel Korshunov and S\u00b4ebastien Marcel. Deepfakes: a new\nthreat to face recognition? assessment and detection. arXiv\npreprint arXiv:1812.08685, 2018. 16\n9\n[24] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC. Lawrence Zitnick.\nMicrosoft COCO: common objects\nin context.\nIn Computer Vision - ECCV 2014 - 13th Eu-\nropean Conference, Zurich, Switzerland, September 6-12,\n2014, Proceedings, Part V, pages 740\u2013755. Springer, 2014.\n6\n[25] Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan\nTran, Xiaogang Wang, and Hongsheng Li. Open-edit: Open-\ndomain image manipulation with open-vocabulary instruc-\ntions. In Computer Vision - ECCV 2020 - 16th European\nConference, Glasgow, UK, August 23-28, 2020, Proceed-\nings, Part XI, pages 89\u2013106. Springer, 2020. 6, 12\n[26] Andreas Lugmayr, Martin Danelljan, Andr\u00b4es Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR 2022, New Orleans, LA, USA, June 18-24, 2022,\npages 11451\u201311461. IEEE, 2022. 2\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2022. 2, 6, 7, 12\n[28] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil\nHoulsby. Simple open-vocabulary object detection. In Com-\nputer Vision \u2013 ECCV 2022: 17th European Conference,\n2022, page 728\u2013755, 2022. 6\n[29] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G\nDerpanis, and Igor Gilitschenski. Watch your steps: Local\nimage and scene editing by text instructions. arXiv preprint\narXiv:2308.08947, 2023. 1, 2, 4, 8\n[30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models. CoRR, abs/2211.09794,\n2022. 2, 6, 7, 12\n[31] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob McGrew,\nIlya\nSutskever, and Mark Chen.\nGLIDE: towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\n2022, pages 16784\u201316804. PMLR, 2022. 2\n[32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 2\n[33] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-\nElor, and Daniel Cohen-Or. Localizing object-level shape\nvariations with text-to-image diffusion models. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), 2023. 1, 3, 8, 14\n[34] Koutilya PNVR, Bharat Singh, Pallabi Ghosh, Behjat Sid-\ndiquie, and David Jacobs. Ld-znet: A latent diffusion ap-\nproach for text-based image segmentation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 4157\u20134168, 2023. 2, 3, 4, 6\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\n2021, pages 8748\u20138763. PMLR, 2021. 7, 12\n[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with CLIP latents. CoRR, abs/2204.06125, 2022. 1,\n2\n[37] Scott E. Reed, Zeynep Akata, Honglak Lee, and Bernt\nSchiele. Learning deep representations of fine-grained vi-\nsual descriptions. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition, 2016, pages 49\u201358. IEEE\nComputer Society, 2016. 2\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022, pages\n10674\u201310685. IEEE, 2022. 1, 2, 3, 12, 14\n[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n22500\u201322510, 2023. 1, 7, 12\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Advances in Neural Infor-\nmation Processing Systems, 2022. 1, 2\n[41] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain,\nAmit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman.\nEmu edit: Precise image editing via recognition and genera-\ntion tasks. arXiv preprint arXiv:2311.10089, 2023. 15, 18\n[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models.\nIn 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021. 12\n[43] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 2\n[44] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 2, 3\n[45] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and\nJoost van de Weijer. Dynamic prompt learning: Address-\ning cross-attention leakage for text-based image editing. In\nThirty-seventh Conference on Neural Information Process-\ning Systems, 2023. 2\n[46] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.\n10\nMdp: A generalized framework for text-guided image edit-\ning by manipulating the diffusion path, 2023. 2\n[47] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 1\n[48] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu\nChang. Uncovering the disentanglement capability in text-\nto-image diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1900\u20131910, 2023. 2, 12, 13, 14\n[49] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 2, 3,\n14\n[50] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 2\n[51] Han Zhang, Tao Xu, and Hongsheng Li. Stackgan: Text to\nphoto-realistic image synthesis with stacked generative ad-\nversarial networks.\nIn IEEE International Conference on\nComputer Vision, ICCV 2017, Venice, Italy, October 22-29,\n2017, pages 5908\u20135916. IEEE Computer Society, 2017. 2\n[52] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing. In Advances in Neural Information\nProcessing Systems, 2023. 1, 2, 5, 6, 7, 8, 12, 13, 15, 17, 18\n[53] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih\nChen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese,\nStefano Ermon, Caiming Xiong, and Ran Xu. HIVE: har-\nnessing human feedback for instructional visual editing.\nCoRR, abs/2303.09618, 2023. 1, 2, 5, 6, 7, 12\n[54] Zhixing Zhang,\nLigong Han,\nArnab Ghosh,\nDimitris\nMetaxas, and Jian Ren. Sine: Single image editing with text-\nto-image diffusion models, 2022. 7, 12\n11\nLIME: Localized Image Editing via Attention Regularization\nin Diffusion Models\nSupplementary Material\n7. Implementation Details\nWe obtain the results using an NVIDIA A100 40GB GPU\nmachine with 8 cores. For 512 \u00d7 512 images the IP2P-\nbased baselines (e.g., IP2P, IP2P w/MB, HIVE, and HIVE\nw/MB) take approximately 15 seconds per edit, while for\nLIME integrated models, it takes \u224825 seconds.\n7.1. Baselines\nOpen-Edit [25]:\nThis GAN-based approach uses a recon-\nstruction loss for pre-training and incorporates a consis-\ntency loss during fine-tuning on specific images. Its unique\nfeature is the arithmetic manipulation of word embeddings\nwithin a shared space of visual and textual features.\nVQGAN-CLIP [9]:\nEnhancing the VQGAN [12] frame-\nwork with CLIP embeddings [35], this method fine-tunes\nVQGAN using the similarity of CLIP embeddings between\nthe generated image and the target text, leading to optimized\nimage generation.\nSDEdit [27]:\nLeveraging the capabilities of Stable Diffu-\nsion [38], SDEdit introduces a tuning-free approach. It uses\nstochastic differential equation noise, adding it to the source\nimage and subsequently denoising to approximate the target\nimage, all based on the target caption.\nText2LIVE [4]:\nIt propose a Vision Transformer [11] for\ngenerating edited objects on an additional layer. It incor-\nporates data augmentation and CLIP [35] supervision, ulti-\nmately alpha-blending the edited layer with the original to\ncreate the target image.\nNull Text Inversion [30]:\nBy optimizing the DDIM [42]\ntrajectory, this method initially inverts the source image.\nAfter, it performs image editing during the denoising pro-\ncess guided by cross-attention [17] between text and image.\nSINE [54]:\nReal images are edited using model-based\nguidance and patch-based fine-tuning process.\nDreamBooth [39]:\nIt fine-tunes a diffusion model by\nlearning special text tokens and adjusting model parameters\non a set of images for editing.\nTextual-Inversion [14]:\nIt fine-tunes a token embedding\nwithin the text-encoder space using a set of images.\nImagic [21]:\nIt edits images through a three-step process:\nfirst fine-tuning a token embedding, then fine-tuning the pa-\nrameters of a text-guided image diffusion model using the\nfine-tuned token embedding, and finally performing inter-\npolation to generate various edits based on a target prompt.\nDiffEdit [8]:\nIt identifies the region to edit in images by\ncontrasting between a conditional and unconditional diffu-\nsion model based on query and reference texts. Then, it re-\nconstructs the edited image by collecting the features from\nthe text-query by combining the features in the noise/latent\nspace, considering the region to edit.\nBlended Latent Diffusion [3]:\nThis method uses a text-\nto-image Latent Diffusion Model (LDM) to edit the user-\ndefined mask region. It extracts features for the mask re-\ngion from the edit text, and for the rest of the image, it uses\nfeatures from the original image in the noise/latent space.\nDirectDiffusion [20]:\nIt inverts the input image into the\nlatent space of Stable Diffusion [38] and then applies\nPrompt2Prompt [17] to obtain the desired edit without mak-\ning any changes to the edit diffusion branch.\nDiffusion Disentanglement [48]:\nIt finds the linear com-\nbination of the text embeddings of the input caption and the\ndesired edit to be performed. Since it does not fine-tune Sta-\nble Diffusion parameters, they claim that the method per-\nforms disentangled edits.\nInstructPix2Pix (IP2P) [6]:\nStarting from the founda-\ntion of Stable Diffusion [38], the model is fine-tuned for\ninstruction-based editing tasks. It ensures that the edited im-\nage closely follows the given instructions while maintaining\nthe source image without the need for test-time tuning.\nInstructPix2Pix\nw/MagicBrush\n[52]:\nA\nversion\nof\nIP2P [6] trained on MagicBrush train set [52]. Since the\nMagicBrush dataset has more localized edit examples, the\nfine-tuned version has better results, as seen in Tab. 1.\nHIVE [53]:\nIt extends IP2P [6] by fine-tuning it with an\nexpanded dataset. Further refinement is achieved through\nfine-tuning with a reward model, which is developed based\non human-ranked data.\nHIVE w/MagicBrush [52]:\nHIVE [53] fine-tuned on\nMagicBrush train set [52]. Since the MagicBrush dataset\nhas more localized edit examples, the fine-tuned version has\nbetter results, as seen in Tab. 1.\n8. Additional Experiments\n8.1. MagicBrush Annotations\nAs mentioned in Sec. 5.5, mask annotations for MagicBrush\ndataset [52] are not very tight around the edit area, as shown\nin Fig. 7. Our method directly uses the identified mask\nduring the editing process, therefore, it is important for the\n12\nmasks to be as tight as possible around the edit area to apply\nlocalized edits. The loose GT masks of MagicBrush explain\nwhy our model achieves worse performance in Tab. 4 when\nusing GT masks. To highlight this, we evidentiate with red\ncircles in Fig. 7 precise edits when precise masks are pro-\nvided to LIME. For the first row - (a), the handle of the\nracket can be preserved if the mask has a precise boundary\nbetween the handle and outfit in the occluded area. More-\nover, the second row - (b) shows that if the mask in the Mag-\nicBrush dataset is used during the edit, the method changes\nthe color of the blanket as well. However, with the precise\nmask extracted by our method, the edit can distinguish the\nobjects in the area and apply localized edits.\nInput Image\nGT\n+ Edit\nRoI\n+ Edit\n(a)\nMake her outfit black.\n(b)\nCan the bed be blue?\nFigure 7. MagicBrush Mask Annotations. Ground truth (GT)\nrefers to mask annotations in MagicBrush [52]. RoI indicates in-\nferred masks from our proposed method. Red circles on the edited\nimages (+ Edit) highlight area where the precise localization of the\nedits can be appreciated.\n8.2. Visual Comparison\nVQGAN-CLIP.\nAs shown in Tab. 1, VQGAN-CLIP [9]\nhas better results on the CLIP-T metric. This is expected\nsince it directly fine-tunes the edited images using CLIP\nembeddings. However, as seen in Fig. 8, the edited images\nfrom VQGAN-CLIP fail to preserve the details of the input\nimage. On the other hand, our method successfully per-\nforms the desired edit by preserving the structure and fine\ndetails of the scene. It results in similar CLIP-T values with\nones for the ground truth edited images in the MagicBrush\ndataset.\nBlended Latent Diffusion.\nAs shown in Tab. 2, Blended\nLatent Diffusion [3] has better results than baselines and our\nmethod. However, as shown in Fig. 9, even if their method\ncan perform the desired edit on the given mask (RoI) from\nthe user, (a) it distorts the location of the features, e.g., heads\nof the birds, and (b) it loses the information of the object in\nthe input image and creates a new object in the RoI, e.g.,\nblanket in (b). On the other hand, our method performs\nInput Image\nGround Truth\nVQGAN-CLIP [9]\nOurs\nMake her outfit\nblack.\nCLIP-T: 0.306\nCLIP-T: 0.486\nCLIP-T: 0.314\nReplace the\nground with a\nforest.\nCLIP-T: 0.311\nCLIP-T: 0.420\nCLIP-T: 0.318\nFigure 8. VQGAN-CLIP [9] Comparison. CLIP-T metrics are\nreported below each image and calculated between the output cap-\ntion and the corresponding image. Input images and edit instruc-\ntions are pictured in the first column. Ground truth edit images are\ntaken from the MagicBrush dataset.\nvisually appealing edits on the input images considering the\ngiven edit instructions by preserving as many details from\nthe input image as possible. This is also highlighted by a\nsignificantly lower Distance metric for our method in Tab. 2.\nInput Image\nBlendedDiffusion [3]\nOurs\n(a)\nTurn the real birds into origami birds.\n(b)\nChange the animal from a cat to a dog.\nFigure 9. BlendedDiffusion [3] Qualitative Comparison. Edited\nimages based on input images and edit instructions reported below\neach row. The images for BlendedDiffusion are taken from the\nPIE-Bench evaluation [20].\nDiffusion Disentanglement.\nWu et al. [48] propose a dis-\nentangled attribute editing method. Figure 10 shows edit\ntypes such as (a) texture editing and (b) replacing the ob-\nject with a similar one. Diffusion Disentanglement on (a)\n13\nalters the background objects in the image, e.g., adding\nsnow on and changing the shape of the branch, and also\nchanges the features of the object of interest, e.g., removing\nthe tail of the bird. On (b), it fails to perform the desired\nedit altogether. Moreover, it requires a GPU with > 48 GB\nRAM2, and one image takes approximately 10 minutes on\nan NVIDIA A100 80GB to generate the edited version. In\ncomparison, our method achieves higher visual quality and\ntakes 25 seconds to complete on NVIDIA A100 40GB with\na GPU RAM usage of 25 GB.\nInput Image\nDiffusionDisent. [48]\nOurs\n(a)\nChange the robin to a silver robin sculpture.\n(b)\nTurn the brown horse into a pink unicorn.\nFigure 10. Diffusion Disentanglement [48] Qualitative Com-\nparison. Edits are obtained by using the global description of the\ninput image and the desired edit by concatenating them with \u2019,\u2019.\n8.3. Qualitative comparison on segmentation maps\nOur method proposes an alternative segmentation method\nbased on the clustering of intermediate features of the dif-\nfusion process.\nIn this section, we provide a qualitative\ncomparison to other segmentation methods.\nLPM [33]\nuses self-attention features from one resolution, such as\n32 \u00d7 32, while our method leverages the intermediate fea-\ntures from different resolutions to enhance the segmentation\nmap. Then, both apply a clustering method to find the seg-\nments in the input image. Another way to find segments\nis by using large segmentation models, e.g., SAM [22],\nODISE [49] . . . , but they require supervised data and train-\ning or fine-tuning. As seen in Fig. 11 (i), large segmentation\nmodels cannot detect the transparent fin of the fish, while\nLPM and ours can. Moreover, LPM utilizes only one res-\nolution, so it cannot find rocks in the river separately. As\nseen in Fig. 11 (ii), ODISE [49] and SAM [22] fail to seg-\nment minute object parts, like fingernails, while LPM and\nours can find those segments. Furthermore, our method pro-\nvides precise boundaries and segments in higher resolutions\n2https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement\nthan LPM. Moreover, LPM uses Stable Diffusion [38] and\nrequires real image inversion to find segments, while our\nmethod does not since it is based on IP2P [6]. For this rea-\nson, LPM requires more than 1 minute to perform, while\nour proposal takes only 10-15 seconds per image. As a re-\nsult, in a direct comparison to LPM, our method has the\nadvantage of having higher-resolution segmentation maps\nsegmentation of more details, and it is significantly faster.\nThe publicly available official implementations of LPM3,\nSAM4 and ODISE5 are used for the results in Fig. 11. Ad-\nditionally, the same number of clusters is used for LPM and\nours to achieve a fair comparison.\nInput Image\nLPM [33]\nSAM [22]\nODISE [49]\nOurs\nFigure 11. Segmentation Qualitative. Comparison between the-\nstate-of-art segmentation methods on challenging examples.\n8.4. Ablation study\nIn addition to the ablation study in Sec. 5.5, we also ana-\nlyze token selection during cross-attention regularization as\ndefined in Sec. 4.2. Instead of regularizing the attention of\nunrelated tokens, such as <start of text>, padding, and stop\nwords, by penalizing it. We could think of doing the oppo-\nsite and give high values to relevant tokens (denoted as \u02dcS)\nwithin the RoI as reported in the following equation:\nR(QKT , M) =\n(\nQKT\nijt + \u03b1,\nif Mij = 1 and t \u2208 \u02dcS\nQKT\nijt,\notherwise,\n(4)\nwhere \u03b1 is a large value. This assignment guarantees that\nthe relevant tokens related to edit instructions will have high\nattention scores after the softmax operation.\nAs seen in\nTab. 5, there is no significant improvement if the unrelated\ntokens are penalized instead of awarding the related tokens.\nHowever, penalizing the unrelated tokens gives the free-\ndom to distribute the attention scores among relevant tokens\nto the process unequally. Thus, it means soft assignment\namong the related tokens.\n3https://github.com/orpatashnik/local-prompt-mixing\n4https://segment-anything.com/demo\n5https://github.com/NVlabs/ODISE\n14\nMethod\nL1 \u2193\nL2 \u2193\nCLIP-I \u2191\nDINO \u2191\nCLIP-T \u2191\nIP2P [6]\n0.112\n0.037\n0.852\n0.743\n0.276\nRelated\n0.065\n0.018\n0.930\n0.897\n0.292\nUnrelated\n0.058\n0.017\n0.935\n0.906\n0.293\nTable 5. Ablation Study on Token Selection. For fair compari-\nson, all parameters are the same for all settings except the ablated\nparameter. The top-performing is highlighted in bold, while the\nsecond-best is denoted with underline for each block.\n8.5. More Qualitative Results\nThis section presents additional qualitative results derived\nfrom our method, emphasizing its improved effectiveness\nagainst established baselines, such as IP2P [6] and IP2P\nw/MB [52].\nFigure 12 illustrates the application of our\nmethod in localized image editing tasks.\nSpecifically, it\ndemonstrates our method\u2019s proficiency in altering the color\nof specific objects: (a) ottoman, (b) lamp, (c) carpet, and\n(d) curtain. Unlike the baseline methods, which tend to en-\ntangle the object of interest with surrounding elements, our\napproach achieves precise, disentangled edits. This is not\nachieved by the baseline that tends to alter multiple objects\nsimultaneously rather than isolating changes to the targeted\nregion. The disentangled and localized edits showcased in\nFig. 12 highlight the potential of LIME in end-user applica-\ntions where object-specific edits are crucial.\nFigure 13 demonstrates additional examples of our\nmethod\u2019s performance on the MagicBrush [52] test set and\nthe PIE-Bench [20] dataset. Our approach effectively ex-\necutes various tasks, such as (a) replacing an animal, (b)\nmodifying multiple objects, (c) altering the texture of an\nanimal, and (d) changing the color of multiple objects. As\nillustrated in Fig. 13, our method demonstrates significant\nimprovements over existing baselines. For instance, while\nbaseline models like IP2P w/MB in (a) achieve reasonable\nedits, they often inadvertently modify areas outside the RoI,\nas observed in cases (b) and (c). Notably, our method helps\nfocus the baseline models on the RoI, as seen in (b), (c), and\n(d), where baselines struggle to preserve the original image.\nAlthough our method is dependent on the baseline and may\noccasionally induce unintended changes in peripheral areas,\ne.g., the floor\u2019s color, it consistently outperforms the base-\nline models in terms of targeted and localized editing.\nFigure 14 provides further comparative analyses using\nthe Emu-Edit test set [41]. Our method successfully han-\ndles diverse tasks, including (a) modifying parts of ani-\nmals, (b) altering the color of specific objects, (c) adding,\nand (d) removing objects. As depicted in Fig. 14, our ap-\nproach significantly surpasses existing baselines in perfor-\nmance. Notably, while baseline models tend to alter entire\n1\u00a9 La Redoute Interieurs (https://www.laredoute.ch/).\nInput Image1\nBaseline\n+ LIME\n(a)\nChange color of ottoman to dark green.\n(b)\nChange color of lamp to dark green.\n(c)\nChange color of carpet to dark blue.\n(d)\nChange color of curtain to purple.\nFigure 12. A use-case of the proposed method. Changing the\ncolor of different objects is shown by comparing baselines and our\nmethod. Our method performs disentangled and localized edits for\ndifferent colors and different objects in the scene.\nobjects rather than their individual parts, e.g., animal legs,\nour method targets and modifies the specific sections as in-\nstructed in scenario (a). Furthermore, baseline models often\ninadvertently affect areas beyond the intended RoI, as seen\nin cases (b) and (c). In contrast, our method demonstrates\nprecision by confining its operations within the RoI. Partic-\nularly in scenario (d), while baseline models such as IP2P\nstruggle to maintain the integrity of the original image or,\nas in the case of IP2P w/MB, fail to effectively remove ob-\njects, our method excels by accurately removing the speci-\nfied objects, underscoring its superiority in targeted image\nmanipulation tasks.\n15\n9. Broader Impact & Ethical Considerations\nThe advancement in localized image editing technology\nholds significant potential for enhancing creative expression\nand accessibility in digital media and virtual reality appli-\ncations. However, it also raises critical ethical concerns,\nparticularly regarding its misuse for creating deceptive im-\nagery like deepfakes [23] and the potential impact on job\nmarkets in the image editing sector. Ethical considerations\nmust focus on promoting responsible use, establishing clear\nguidelines to prevent abuse, and ensuring fairness and trans-\nparency, especially in sensitive areas like news media. Ad-\ndressing these concerns is vital for maximizing the technol-\nogy\u2019s positive impact while mitigating its risks.\n16\nInput Image\nIP2P [6]\n+ LIME\nIP2P [6] w/MB [52]\n+ LIME\n(a)\nTurn the brown horse into a pink unicorn.\n(b)\nChange the alcohol shelf into a bookshelf.\n(c)\nChange the robin to a silver robin sculpture.\n(d)\nChange the color of the tulips to yellow.\nFigure 13. More Qualitative Examples. We test our method on different tasks: (a) replacing an animal, (b) changing existing objects, (c)\nchanging texture, and (d) changing the color of multiple objects. Examples are taken from established benchmarks [20, 52]. The integration\nof LIME enhances the performance of all models, enabling localized edits while maintaining the integrity of the remaining image areas.\n17\nInput Image\nIP2P [6]\n+ LIME\nIP2P [6] w/MB [52]\n+ LIME\n(a)\nChange the legs to be bionic.\n(b)\nChange the color of the lighthouse into completely red.\n(c)\nPut Stone Henge as the background of the scene.\n(d)\nRemove the forks from the shelf.\nFigure 14. More Qualitative Examples. We test our method on different tasks: (a) editing animal parts, (b) changing color, (c) adding,\nand (d) removing objects. Examples are taken from established papers [41]. The integration of LIME enhances the performance of all\nmodels, enabling localized edits while maintaining the integrity of the remaining image areas.\n18\n"
  },
  {
    "title": "General Object Foundation Model for Images and Videos at Scale",
    "link": "https://arxiv.org/pdf/2312.09158.pdf",
    "upvote": "7",
    "text": "General Object Foundation Model for Images and Videos at Scale\nJunfeng Wu1*, Yi Jiang2*, Qihao Liu3, Zehuan Yuan2, Xiang Bai1\u2020, Song Bai2\u2020\n1Huazhong University of Science and Technology, 2ByteDance Inc., 3Johns Hopkins University\nAbstract\nWe present GLEE in this work, an object-level founda-\ntion model for locating and identifying objects in images\nand videos. Through a unified framework, GLEE accom-\nplishes detection, segmentation, tracking, grounding, and\nidentification of arbitrary objects in the open world sce-\nnario for various object perception tasks. Adopting a cohe-\nsive learning strategy, GLEE acquires knowledge from di-\nverse data sources with varying supervision levels to formu-\nlate general object representations, excelling in zero-shot\ntransfer to new data and tasks. Specifically, we employ an\nimage encoder, text encoder, and visual prompter to han-\ndle multi-modal inputs, enabling to simultaneously solve\nvarious object-centric downstream tasks while maintain-\ning state-of-the-art performance.\nDemonstrated through\nextensive training on over five million images from di-\nverse benchmarks, GLEE exhibits remarkable versatility\nand improved generalization performance, efficiently tack-\nling downstream tasks without the need for task-specific\nadaptation. By integrating large volumes of automatically\nlabeled data, we further enhance its zero-shot generaliza-\ntion capabilities.\nAdditionally, GLEE is capable of be-\ning integrated into Large Language Models, serving as a\nfoundational model to provide universal object-level infor-\nmation for multi-modal tasks. We hope that the versatil-\nity and universality of our method will mark a significant\nstep in the development of efficient visual foundation mod-\nels for AGI systems. The model and code will be released at\nhttps://glee-vision.github.io/.\n1. Introduction\nFoundation model [7] is an emerging paradigm for build-\ning artificial general intelligence (AGI) systems, signify-\ning a model trained on broad data that is capable of be-\ning adapted to a wide range of downstream tasks in an\ngeneral paradigm. Recently, NLP foundation models such\nas BERT [22], GPT-3 [9], T5 [78] developed with unified\n*Equal Technical Contribution.\n\u2020Correspondence to Xiang Bai\n<xbai@hust.edu.cn> and Song Bai <songbai.site@gmail.\ncom>.\nObject\nDetection(COCO)\nInstance\nSegmentation\n(COCO)\nOpenWorld (UVO)\nREC (RefCOCO)\nRES (RefCOCO)\nVIS\n(YouTubeVIS19)\nOVIS (OVIS)\nVOS (YouTube-\nVOS)\nOV MOT (TAO)\nOV MOTS (BURST)\nOV VIS (LV-VIS)\nOV Detection\n(OmniLabel)\n37.5\n50.0\n62.5\n38.25\n46.5\n54.75\n55.5\n61.0\n66.5\n70.5\n81.0\n91.5\n53.25\n66.5\n79.75\n62.25\n64.5\n66.75\n46.75\n48.5\n50.25\n73.25\n76.5\n79.75\n34.25\n40.5\n46.75\n6.25\n12.5\n18.75\n20.0\n25.0\n30.0\n22.25\n24.5\n26.75\nN/A\nPrevious SOTA\nUNINEXT\nFlorence-2\nOV2Seg\nGLEE (This Work)\nFigure 1. The performance of GLEE on a broad range of object-\nlevel tasks compared with existing models.\ninput-output paradigms and large-scale pre-training, have\nachieved remarkable generalization capabilities to address\nnearly all NLP tasks.\nIn computer vision, the diversity of task types and the\nlack of a unified from makes visual foundation models only\nserve specific subdomains, such as CLIP [77] for multi-\nmodal visual model, MAE [35] for visual representations\nmodel, SAM [43] for segmentation model. Despite being\nwidely studied, current visual foundation models are still\nfocusing on establishing correlations between global image\nfeatures and language descriptions or learning image-level\nfeature representations. However, locating and identifying\nobjects constitute foundational capabilities in computer vi-\nsion systems, serves as a basis for solving complex or high\nlevel vision tasks such as segmentation, scene understand-\ning, object tracking, event detection, and activity recogni-\ntion and support a wide range of applications.\nIn this work, we advance the development of object-level\nfoundation models within the visual domain. To address the\naforementioned limitation, providing general and accurate\nobject-level information, we introduce a general object vi-\nsual foundation model, coined as GLEE, which simultane-\nously solve a wide range of object-centric tasks while ensur-\ning SOTA performance, including object detection, instance\nsegmentation, grounding, object tracking, interactive seg-\nmentation and tracking, etc., as shown in Figure 1. Through\n1\narXiv:2312.09158v1  [cs.CV]  14 Dec 2023\na unified input and output paradigm definition, our model is\ncapable of learning from a wide range of diverse data and\npredicting general object representations, which masks it to\ngeneralize well to new data and tasks in a zero-shot manner\nand achieve amazing performance. In addition, thanks to\nthe unified paradigm, the training data can be scaled up at\nlow cost by introducing a large amount of automatically la-\nbeled data, and further improve the zero-shot generalization\nability of the model.\nA general object foundation model framework. Our\nobjective is to build an object visual foundation model ca-\npable of simultaneously addressing a wide range of object-\ncentric tasks. Specifically, we employ an image encoder, a\ntext encoder, and a visual prompter to encode multi-modal\ninputs. They are integrated into a detector to extract ob-\njects from images according to textual and visual input.\nThis unified approach to handle multiple modalities en-\nables us to concurrently solve various object-centric tasks,\nincluding detection [11, 58, 90, 132], instance segmenta-\ntion [16, 34], referring expression comprehension [38, 62,\n104, 131], interactive segmentation [1, 13, 135], multi-\nobject tracking [21, 68, 111, 126, 129], video object seg-\nmentation [17, 18, 73, 110], video instance segmenta-\ntion [37, 98, 101, 103, 113], and video referring segmen-\ntation [86, 102, 104], all while maintaining state-of-the-art\nperformance.\nA multi-granularity joint supervision and scaleable\ntraining paradigm.\nThe design of the unified framework\ncapable of addressing multiple tasks enables joint train-\ning on over five million images from diverse benchmarks\nand varying levels of supervision.\nExisting datasets dif-\nfer in annotation granularity: detection datasets like Ob-\njects365 [88] and OpenImages [46] offer bounding boxes\nand category names; COCO [58] and LVIS [32] provide\nfiner-grained mask annotations; RefCOCO [72, 120] and\nVisual Genome [44] provide detailed object descriptions.\nAdditionally, video data enhance the temporal consistency\nof model, while open-world data contribute class-agnostic\nobject annotations. An intuitive display of the supervision\ntypes and data scales of the datasets employed is presented\nin Figure 2. The unified support for multi-source data in\nour approach greatly facilitates the incorporation of addi-\ntional manually or automatically annotated data, enabling\neasy scaling up of the dataset. Furthermore, the alignment\nof model optimization across tasks means that joint training\nserves not only as a unifying strategy but also as a mecha-\nnism to boost performance across individual tasks.\nStrong zero-shot transferability to a wide range of\nobject level image and video tasks. After joint training\non data from diverse sources, GLEE demonstrates remark-\nable versatility and zero-shot generalization abilities. Ex-\ntensive experiments demonstrate that GLEE achieves state-\nof-the-art performance compared to existing specialist and\ngeneralist models in object-level image tasks such as detec-\ntion, referring expression comprehension, and open-world\ndetection, all without requiring any task-specific designs\nor fine-tuning. Furthermore, we showcase the extraordi-\nnary generalization and zero-shot capabilities of GLEE in\nlarge-vocabulary open-world video tracking tasks, achiev-\ning significantly superior performance over existing models\neven in a zero-shot transfer manner. Additionally, by incor-\nporating automatically annotated data like SA1B [43] and\nGRIT [75], we are able to scale up our training dataset to an\nimpressive size of 10 million images at a low cost, which is\ntypically challenging to achieve for object-level tasks and\nfurther enhances the generalization performance.\nMore-\nover, we replace the SAM [43] component with GLEE in\na multimodal Large Language Model (mLLM) [47] and ob-\nserve that it achieves comparable results. This demonstrates\nthat GLEE is capable of supplying the visual object-level in-\nformation that modern LLMs currently lack, thus laying a\nsolid foundation for an object-centric mLLMs.\n2. Related Work\n2.1. Visual Foundation Model\nAs foundation models [9, 19, 22, 78, 91] in the NLP field\nhave achieved remarkable success, the construction of vi-\nsual foundation models attracts increasing attention. Unlike\nNLP tasks that are predominantly unified under a text-to-\ntext paradigm, tasks in Computer Vision still exhibit sig-\nnificant differences in form and definition. This disparity\nleads to visual foundation models typically being trained\nin a single-task learning frameworks, limiting their appli-\ncability to tasks within certain sub-domains. For instance,\nmulti-modal visual foundation models like CLIP [77],\nALIGN [41], Florence [121], BEIT3 [97], Flamingo[2]\nmake significant advancements in efficient transfer learn-\ning and demonstrate impressive zero-shot capabilities on\nvision-language tasks by employing contrastive learning\nand masked data modeling on large-scale image-text pairs.\nDALL-E [79, 80] and Stable Diffusion [83] are trained on\nmassive pairs of images and captions, enabling them to gen-\nerate detailed image content conditioned on textual instruc-\ntion. DINO [12], MAE [35], EVA [27], ImageGPT [14]\nobtain strong visual representations through self-supervised\ntraining on large-scale image data, which are then employed\nto transfer to downstream tasks. These foundation models\nlearn image-level features, which are not directly applica-\nble to object-level tasks. The recently proposed SAM [43],\ncapable of segmenting any object of a given image based\non visual prompt such as points and boxes, provides rich\nobject-level information and demonstrates strong general-\nization capabilities. However, the object information lacks\nsemantic context, limiting its application in object-level\ntasks. Unlike existing visual foundation models, we aim\n2\nVisual Objects\nclass-agnostic\nobjects, e.g. SA1B\ncategory objects, e.g.\nCOCO, Objects365,\nOpen Images\ndescribed objects,\ne.g. RefCOCO,\nVisual Genome\n\u2026\n\u2026\n\u2026\n\u2026\nvideo class-agnostic\nobjects, e.g. UVO\nvideo category objects,\ne.g. YTVIS, OVIS\nvideo described objects,\ne.g. Ref-YTVOS\ncategory: [person,\nskateboard, car, dog, cat,\nbird, motorcycle, shoe, \u2026]\nexpression:\n1.Black motorcycle parked on the sidewalk\n2.Motorcycle parked under the sign\n3.Black motorcycle with no one riding it\nobject caption:\nwhite car on the road\nair conditioner outdoor \nunit\narbitrary name:\nbollards\nmanhole cover\nclass agnostic masks\nVideo Data\nCategory: [person, dog, car,\nbird, cat, train, bag \u2026]\nExpression:\n1.the black car on the road\n2. the black car on zebra \ncrossing\nvideo example objects,\ne.g. YTVOS\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nFigure 2. An illustrative example showcasing annotations of varying granularities from different datasets, along with the scale of data we\nutilized. Training on datasets from multiple sources endows the model with more universal representations.\nto develop an object foundation model that directly solve\ndownstream tasks without the need for additional parame-\nters or fine-tuning.\n2.2. Unified and General Model\nUnified models share similarities with foundation models\nin the aspect of multi-task unification for their ability to\nhandle multiple vision or multi-modal tasks within a single\nmodel. MuST [30] and Intern [87] propose to train across\nmultiple vision tasks and solving them simultaneously. In-\nspired by the success of sequence-to-sequence NLP mod-\nels [9, 78], models such as Uni-Perceiver [133], OFA [94],\nUnified-IO [66], Pix2Seq v2 [15], and UniTAB [114] pro-\npose modeling various tasks as sequence generation tasks\nwithin a unified paradigm. While these approaches have\ndemonstrated promising cross-task generalization capabili-\nties, they focus mainly on image-level understanding tasks.\nIn addition, their auto-regressive generation of boxes and\nmasks results in significantly slower inference speeds and\nthe performance still falls short of state-of-the-art task-\nspecific models. Building upon on detectors [50, 132], Uni-\nPerceiver v2 [51] and UNINEXT [112] utilize unified max-\nimum likelihood estimation and object retrieval to support\nvarious tasks, effectively resolves the challenges of local-\nization. Nonetheless, they are trained on closed-set data,\nthereby not exhibiting zero-shot generalization capabilities.\nX-decoder [134] and SEEM [135] construct a generalized\ndecoding model capable of predicting pixel-level segmen-\ntation and language tokens. Diverging from unified models,\nthe proposed GLEE not only directly addresses object-level\ntasks in a unified manner but also provides universal ob-\nject representations, which generalize well to new data and\ntasks, serving as a cornerstone for a broader range of tasks\nthat require detailed object information.\n2.3. Vision-Language Understanding\nOpen-vocabulary detection (OVD) and Grounding models\nboth necessitate the localization and recognition of as many\nobjects as possible. With the recent advancements in vision-\nlanguage pre-training [41, 77, 119, 121], a commonly em-\nployed strategy for OVD involves transferring the knowl-\nedge from pre-trained vision-language models (VLMs) to\nobject detectors [31, 45, 71].\nAnother group of studies\nleverages extensive image-text pair datasets to broaden the\ndetection vocabulary [28, 52, 57, 116, 122, 128]. However,\nthese language-based detectors are inherently constrained\nby the capabilities and biases of language models, making\nit challenging to excel simultaneously in both localization\n3\ncategory: person, car \nmotorcycle,dog, cat \u2026\narbitrary name\nexpression:\nmotorcycle parked \nunder the sign\ninteractive \nsegment\nbollard\nmanhole \ncover\n(a) Object foundation model framework\n(b) Applied to image tasks\n(c) Applied to video tasks\nvideo tasks: VIS\u3001MOT\u3001VOS\u3001RVOS\u3001\nopen world/vocabulary tracking\ninteractive tracking\ncategory\narbitrary name\nexpression\ncategory: [person, car, motorcycle, dog \u2026] \narbitrary name: bollard, manhole cover\nexpression: motorcycle parked under the sign\nobject caption: air conditioner outdoor unit\nText Encoder\nImage Backbone\nVisual Prompter\nObject\nDecoder\nFigure 3. Framework of GLEE. The text encoder accepts textual descriptions in various forms from diverse data sources, including\nobject categories, names, captions, and referring expressions. The visual prompter encodes points, bounding boxes, or scribbles into\ncorresponding visual representations.The object decoder take them and image features to predict objects in images. (b) illustrates the\napplication of GLEE to image tasks tailored for different language descriptions and visual prompts. (c) demonstrates the application across\nvarious object-level video tasks.\nand recognition. Our objective is to optimally utilize exist-\ning datasets to construct a general object-level foundation\nmodel, aims to not only detect and identify objects effec-\ntively but also to offer universal object representations for a\nwide range of downstream tasks\n3. Method\n3.1. Formulation\nThe proposed GLEE consists of an image encoder, a text\nencoder, a visual prompter, and an object decoder, as il-\nlustrated in Figure 3. The text encoder processes arbitrary\ndescriptions related to the task, including object categories,\nnames in any form, captions about objects, and referring ex-\npressions. The visual prompter encodes user inputs such as\npoints, bounding boxes, or scribbles during interactive seg-\nmentation into corresponding visual representations of tar-\nget objects. Then they are integrated into a detector for ex-\ntracting objects from images according to textual and visual\ninput. We build the object decoder upon MaskDINO [50]\nwith a dynamic class head by compute similarity between\nobject embedding from detector and text embedding from\nthe text encoder. Given an input image I \u2208 R3\u00d7H\u00d7W , we\nfirst extract multi-scale features Z with backbones such as\nResNet [33]. Then we feed them into the object decoder\nand adopt three prediction heads (classification, detection,\nand segmentation) on the output embedding qd \u2208 RN\u00d7C\nfrom decoder. Following other object segmentation mod-\nels [16, 50, 55], we construct a 1/4 resolution pixel embed-\nding map Mp \u2208 RC\u00d7 H\n4 \u00d7 W\n4 which is obtained by upsam-\npling and fusing multi-scale feature maps from the back-\nbone and Transformer encoder. Finally, we obtain each bi-\nnary mask prediction m \u2208 RN\u00d7 H\n4 \u00d7 W\n4 via a dot product be-\ntween the N mask embeddings and pixel embedding map:\nm = FFN(qd) \u2297 Mp,\n(1)\nwhere FFN is a 3-layer feed forward head with ReLU acti-\nvation function and a linear projection layer.\nTo support arbitrary vocabularies and object descrip-\ntions, we replace the FFN classifier with text embeddings\nfollowing DetCLIP [115]. Specifically, we feed K category\nnames as separate sentences into the text encoder EncL and\nuse the average of each sentence tokens as the output text\nembedding et \u2208 RK\u00d7D for each category or description.\nThen we compute the alignment scores Salign \u2208 RN\u00d7K\nbetween object embedding and text embedding:\nSalign = qd \u00b7 Wi2t \u2297 et,\n(2)\nwhere Wi2t \u2208 RC\u00d7D is image-to-text projection weights.\nWe use logits Salign to replace traditional classification log-\nits to compute Hungarian matching cost during training and\nassign categories to objects during inference.\nTo make\nthe original visual features prompt-aware, an early fusion\nmodule is adopted before Transformer encoder following\n4\nUNINEXT [112], which tasks image feature from backbone\nand prompt embedding as input and perform bi-directional\ncross-attention between them.\n3.2. Task Unification\nBased on the above designs, GLEE can be used to seam-\nlessly unify a wide range of object perception tasks in im-\nages and videos, including object detection, instance seg-\nmentation, grounding, multi-target tracking (MOT), video\ninstance segmentation (VIS), video object segmentation\n(VOS), interactive segmentation and tracking, and sup-\nports open-world/large-vocabulary image and video detec-\ntion and segmentation tasks.\nDetection and Instance Segmentation. For detection\ntask, a fixed-length category list is given and all objects in\nthe category list are required to be detected. For a dataset\nwith category list length K, the text input can be formulated\nas {pk}K\nk=1 where pk represents for the k-th category name,\ne.g., P = [\u201cperson\u201d, \u201cbicycle\u201d, \u201ccar\u201d, ... , \u201ctoothbrush\u201d] for\nCOCO [58]. For datasets with large vocabulary, calculating\nthe text embedding of all categories is very time-consuming\nand redundant. Therefore, for datasets with a category num-\nber greater than 100, such as objects365 [88] and LVIS [32],\nsuppose there are \u02c6K positive categories in an image, we take\nthe \u02c6K positive categories and then pad the category number\nto 100 by randomly sampling from the negative categories.\nFor instance segmentation, we enable the mask branch and\nadd mask matching cost with mask loss.\nGrounding and Referring Segmentation. These tasks\nprovide reference textual expressions, where objects are de-\nscribed with attributes, for example,Referring Expression\nComprehension (REC) [120, 131], Referring Expression\nSegmentation (RES) [62, 120], and Referring Video Ob-\nject Segmentation (R-VOS) [86, 102] aim at finding ob-\njects matched with the given language expressions like \u201cThe\nfourth person from the left\u201d. For each image, we take the\nall the object expressions as text prompt and feed the them\ninto the text encoder. For each expressions, we apply global\naverage pooling along the sequence dimension to get text\nembedding et. The text embeddings are feed into early fu-\nsion module and additionally interacte with object queries\nthrough self-attention module in the decoder.\nMOT and VIS. Both Multi-object Tracking (MOT)[4,\n21, 68, 126, 129] and Video Instance Segmentation\n(VIS)[37, 76, 103, 113] need to detect and track all the ob-\njects in the predefined category list, and VIS requires addi-\ntional mask for the objects. These two tasks can be consid-\nered as extended tasks of detection and instance segmenta-\ntion on videos. We found that with sufficient image expo-\nsure, object embeddings from the decoder effectively dis-\ntinguish objects in a video, showing strong discriminability\nand temporal consistency. As a result, they can be directly\nemployed for tracking without the need for an additional\ntracking head. Training on image-level data can address\nstraightforward tracking scenarios, but in cases of severe oc-\nclusion scenes, such as OVIS [76], image-level training can-\nnot guarantee that the model exhibits strong temporal con-\nsistency under occlusion conditions. Therefore, for occlu-\nsion scenarios, it is essential to utilize video data for train-\ning. Following IDOL [103], we sample two frames from a\nvideo and introduce contrastive learning between frames to\nmake the embedding of the same object instance closer in\nthe embedding space, and the embedding of different ob-\nject instances farther away. During Inference, the detected\nobjects are tracked by simple bipartite matching of the cor-\nresponding object queries following MinVIS [39].\nVisual Prompted Segmentation. Interactive segmenta-\ntion [8, 13, 63, 84, 89, 100, 109] takes various forms of vi-\nsual prompt, such as points, boxes, or scribbles, to segment\nthe specified objects within an image. On the other hand,\nVOS aims to segment the entire object throughout the en-\ntire video based on a mask provided in the first frame of the\nvideo. We extract visual prompt embeddings twice in the\nmodel. First, we crop the prompt square area from RGB im-\nage and send it to the backbone to obtain the visual prompt\nfeature of the corresponding area, and send it to the early\nfusion module before the Transformer encoder. Second, we\nsample fine-grained visual embeddings from the pixel em-\nbedding map Mp according to visual prompt and make them\ninteracted with object queries through self-attention module\nin the Transformer decoder layer, as the same with text em-\nbeddings.\n3.3. Training Unification\nTasks with Dynamic Loss. We jointly train GLEE in an\nend-to-end manner on over 5 million images from diverse\nbenchmarks with various levels of supervision. Different\nloss functions are selected for training on various datasets.\nThere are six types of losses for our GLEE: semantic loss,\nbox loss, mask loss, confidence loss, contrastive tracking\nloss, and distillation loss. For all tasks with category list or\nobject expressions, we apply focal loss [59] as semantic loss\non logits Salign to align the text concepts with object fea-\ntures. For box prediction, we use a combination of L1 loss\nand generalized IoU loss [81]. The mask loss is defined as\na combination of the Dice loss [70] and Focal loss. For the\nVisual Prompt Segmentation tasks, we employ an additional\nFFN to predict the confidence score for each object queries\nsupervised by focal loss. Following IDOL [103], for video\ntasks, we sample two frames and apply contrastive tracking\nloss on the object query from the last layer of decoder:\nLembed = log[1 +\nX\nk+\nX\nk\u2212\nexp(v \u00b7 k\u2212 \u2212 v \u00b7 k+)],\n(3)\n5\nMethod\nType\nGeneric Detection & Segmentation\nReferring Detection & Segmentation\nOpenWorld\nCOCO-val\nCOCO-test-dev\nLVIS\nRefCOCO\nRefCOCO+\nRefCOCOg\nUVO\nAPbox\nAPmask\nAPbox\nAPmask\nAPbox\nAPr\u2212box\nAPmask\nAPr\u2212mask\nP@0.5\noIoU\nP@0.5\noIoU\nP@0.5\noIoU\nARmask\nMDETR [42]\nSpecialist\n-\n-\n-\n-\n-\n-\n-\n-\n87.5\n-\n81.1\n-\n83.4\n-\n-\nSeqTR [131]\nModels\n-\n-\n-\n-\n-\n-\n-\n-\n87.0\n71.7\n78.7\n63.0\n82.7\n64.7\n-\nPolyFormer (L) [62]\n-\n-\n-\n-\n-\n-\n-\n-\n90.4\n76.9\n85.0\n72.2\n85.8\n71.2\n-\nViTDet-L [55]\n57.6\n49.8\n-\n-\n51.2\n-\n46.0\n34.3\n-\n-\n-\n-\n-\n-\n-\nViTDet-H [55]\n58.7\n50.9\n-\n-\n53.4\n-\n48.1\n36.9\n-\n-\n-\n-\n-\n-\n-\nEVA-02-L [26]\n64.2\n55.0\n64.5\n55.8\n65.2\n-\n57.3\n-\nODISE [107]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n57.7\nMask2Former (L) [16]\n-\n50.1\n-\n50.5\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nMaskDINO (L) [50]\n-\n54.5\n-\n54.7\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nUniTAB (B) [114]\nGeneralist\n-\n-\n-\n-\n-\n-\n-\n-\n88.6\n-\n81.0\n-\n84.6\n-\n-\nOFA (L) [94]\nModels\n-\n-\n-\n-\n-\n-\n-\n-\n90.1\n-\n85.8\n-\n85.9\n-\n-\nPix2Seq v2 [15]\n46.5\n38.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nUni-Perceiver-v2 (B) [51]\n58.6\n50.6\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nUni-Perceiver-v2 (L) [51]\n61.9\n53.6\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nUNINEXT (R50) [112]\n51.3\n44.9\n-\n-\n36.4\n-\n-\n-\n89.7\n77.9\n79.8\n66.2\n84.0\n70.0\n-\nUNINEXT (L) [112]\n58.1\n49.6\n-\n-\n-\n-\n-\n-\n91.4\n80.3\n83.1\n70.0\n86.9\n73.4\n-\nUNINEXT (H) [112]\n60.6\n51.8\n-\n-\n-\n-\n-\n-\n92.6\n82.2\n85.2\n72.5\n88.7\n74.7\n-\nGLIPv2 (B) [123]\n-\n-\n58.8\n45.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGLIPv2 (H) [123]\n-\n-\n60.6\n48.9\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nX-Decoder (B) [134]\n-\n45.8\n-\n45.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nX-Decoder (L) [134]\n-\n46.7\n-\n47.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nFlorence-2 (L) [106]\n43.4\n-\n-\n-\n-\n-\n-\n-\n93.4\n-\n88.3\n-\n91.2\n-\n-\nGLEE-Lite\nFoundation\n55.0\n48.4\n54.7\n48.3\n44.2\n36.7\n40.2\n33.7\n88.5\n77.4\n78.3\n64.8\n82.9\n68.8\n66.6\nGLEE-Plus\nModels\n60.4\n53.0\n60.6\n53.3\n52.7\n44.5\n47.4\n40.4\n90.6\n79.5\n81.6\n68.3\n85.0\n70.6\n70.6\nGLEE-Pro\n62.0\n54.2\n62.3\n54.5\n55.7\n49.2\n49.9\n44.3\n91.0\n80.0\n82.6\n69.6\n86.4\n72.9\n72.6\nTable 1. Comparison of GLEE to recent specialist and generalist models on object-level image tasks. For REC and RES tasks, we report\nPrecision@0.5 and overall IoU (oIoU). For open-world instance segmentation task, we reported the average recall of 100 mask proposals\n(AR@100) on the UVO [96].\nwhere k+ and k\u2212 are the object queries belong to the same\nobject and other objects from the reference frame, respec-\ntively. For the text encoder, we distill the knowledge from\nthe teacher CLIP text encoder to ensure the text embedding\nin pre-trained vison-language embedding space. We apply\nan L1 loss between our text encoder and CLIP text encoder\nto minimize their distance:\nLtext = 1\nK\nK\nX\ni=0\n\u2225EncL(pi) \u2212 EncCLIP (pi)\u2225 .\n(4)\nData Scale Up. A visual foundation model should be\nable to easily scale up the training data and achieve bet-\nter generalization performance. Thanks to the unified train-\ning paradigm, the training data can be scaled up at low cost\nby introducing a large amount of automatically labeled data\nfrom SA1B [43] and GRIT [75]. SA1B provides large and\ndetailed mask annotations, which enhance the object per-\nception capabilities of model, while GRIT offers a more\nextensive collection of referring-expression-bounding-box\npairs, improving the object identification abilities and the\nunderstanding capability of descriptions. Ultimately, we in-\ntroduced 2 million SA1B data points and 5 million GRIT\ndata points into the training process, bringing the total train-\ning data to 10 million.\n4. Experiments\n4.1. Experimental Setup\nDatasets and Training Strategy.\nWe conducted train-\ning in three stages.\nInitially, we performed pretraining\nfor object detection task on Objects365 [88] and Open-\nImages [46], initializing the text encoder with pretrained\nCLIP [77] weights and keeping the parameters frozen. In\nthe second training step, we introduced additional instance\nsegmentation datasets, including COCO [58], LVIS [32],\nand BDD [118]. Furthermore, we treat three VIS datasets:\nYTVIS19 [113], YTVIS21 [108], and OVIS [76], as in-\ndependent image data to enrich the scenes. For datasets\nthat provide descriptions of objects, we included Ref-\nCOCO [120], RefCOCO+ [120], RefCOCOg [72], Visual\nGenome [44], and RVOS [86]. Since Visual Genome con-\ntains multiple objects in a single image, we treat it as detec-\ntion task and used both object descriptions and object noun\nphrases as categories, with a total of 200 dynamic category\nlists per batch. Additionally, we introduced two open-world\ninstance segmentation datasets, UVO [96] and a subset of\nSA1B [43]. For these two datasets, we set the category\nname for each object to be \u2019object\u2019 and train in instance seg-\nmentation paradigm. During the second step, text encoder\nis unfrozen but supervised by distillation loss to ensure the\npredicted text embedding in CLIP embedding space. After\nthe second step, GLEE demonstrated state-of-the-art per-\nformance on a range of downstream image and video tasks\nand exhibited strong zero-shot generalization capabilities,\nunless otherwise specified, all the experimental results pre-\nsented below were obtained by the model at this stage.\nBuilding upon this, we introduce the SA1B and GRIT\ndatasets to scale up the training set, resulting in a model\nnamed GLEE-scale, which exhibited even stronger zero-\nshot performance on various downstream tasks. Since im-\n6\nMethod\nTracking Any Object (TAO [20])\nBURST [3]\nLV-VIS [93]\nTETA\nLocA\nAssocA\nClsA\nALL\nCommon\nUncommon\nAP\nAPb\nAPn\nHOTA\nmAP\nHOTA\nmAP\nHOTA\nmAP\nTracktor [5]\n24.2\n47.4\n13.0\n12.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDeepSORT [99]\n26.0\n48.4\n17.5\n12.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nTracktor++ [20]\n28.0\n49.0\n22.8\n12.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nQDTrack [74]\n30.0\n50.5\n27.4\n12.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nTETer [53]\n33.3\n51.6\n35.0\n13.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOVTrack\u2020 [54]\n34.7\n49.3\n36.7\n18.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSTCN Tracker\u2020 [3]\n-\n-\n-\n-\n5.5\n0.9\n17.5\n0.7\n2.5\n0.6\n-\n-\n-\nBox Tracker\u2020 [3]\n-\n-\n-\n-\n8.2\n1.4\n27.0\n3.0\n3.6\n0.9\n-\n-\n-\nDetic [130]-SORT\u2020 [6]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n12.8\n21.1\n6.6\nDetic [130]-XMem \u2020[17]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n16.3\n24.1\n10.6\nOV2Seg-R50\u2020 [93]\n-\n-\n-\n-\n-\n3.7\n-\n-\n-\n-\n14.2\n17.2\n11.9\nOV2Seg-B\u2020 [93]\n-\n-\n-\n-\n-\n4.9\n-\n-\n-\n-\n21.1\n27.5\n16.3\nUNINEXT (R50) [112]\n31.9\n43.3\n35.5\n17.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGLEE-Lite\u2020\n40.1\n56.3\n39.9\n24.1\n22.6\n12.6\n36.4\n18.9\n19.1\n11.0\n19.6\n22.1\n17.7\nGLEE-Plus\u2020\n41.5\n52.9\n40.9\n30.8\n26.9\n17.2\n38.8\n23.7\n23.9\n15.5\n30.3\n31.6\n29.3\nGLEE-Pro\u2020\n47.2\n66.2\n46.2\n29.1\n31.2\n19.2\n48.7\n24.8\n26.9\n17.7\n23.9\n24.6\n23.3\nTable 2. Comparison of GLEE to recent specialist and generalist models on object-level video tasks in a zero-shot manner. Evaluation\nmetrics of BURST are reported separately for \u2018common\u2019, \u2018uncommon\u2019 and \u2018all\u2019 classes. The mAP computes mask IoU at the track\nlevel, HOTA is a balance of per-frame detection accuracy (DetA) and temporal association accuracy (AssA), and TETA that deconstructs\ndetection into localization and classification components. The AP, APb, and APn in LV-VIS mean the average precision of overall\ncategories, base categories, and novel categories. \u2020 does not use videos for training. The under-performance of Pro relative to Plus on\nLV-VIS is due to Pro employing larger training and inference resolutions, which prove to be sub-optimal for this specific dataset\nage data alone is insufficient for the model to learn temporal\nconsistency features, we incorporated sequential video data\nfrom YTVIS, OVIS, RVOS, UVO, and VOS to improve its\nperformance if specifically note.\nImplementation Details.\nIn our experiments, we\ndeveloped GLEE-Lite, GLEE-Plus, and GLEE-Pro us-\ning ResNet-50 [33],\nSwin-Large [64],\nand EVA-02\nLarge [26] as the vision encoder respectively. Following\nMaskDINO [50], we adopt deformable transformer in ob-\nject decoder, and use 300 object queries. Query denois-\ning and Hybrid matching are kept to accelerate conver-\ngence and improve performance. During pretraining, we\nset a minibatch to 128 on 64 A100 GPUs, for 500,000 it-\nerations. For joint-training, we train GLEE on 64 A100\nGPUs for 500,000 iterations, further training details, data\npre-processing methods, and data sampling strategies can\nbe found in the supplementary materials. More detailed in-\nformation on data usage and model training is available in\nthe supplementary materials.\n4.2. Comparison with Generalist Models\nWe demonstrate the universality and effectiveness of our\nmodel as an object-level visual foundation model, directly\napplicable to various object-centric tasks while ensuring\nstate-of-the-art performance without needing fine-tuning.\nWe compare our approach with existing specialist and gen-\neralist models in image-level tasks, including detection, re-\nferring expression comprehension, and open-world instance\nsegmentation. We report detection and instance segmen-\ntation results on both the COCO validation [58] set and\nLVIS val v1.0 [32].\nWhile sharing almost identical im-\nage sets, LVIS is distinguished by its annotations of over\n1,200 object categories, showcasing a long-tail distribu-\ntion. This distinction makes LVIS more representative of\nchallenging real-world scenarios due to its broader cate-\ngory coverage. As indicated in Table 1, our model outper-\nforms all generalist models on both COCO and LVIS bench-\nmarks. Even when compared to other state-of-the-art spe-\ncialist approaches, which are tailored with specific design,\nour model remains highly competitive. This demonstrates\nthat GLEE, while mastering universal and general object\nrepresentations, concurrently maintains advanced capabili-\nties in object detection and segmentation. This characteris-\ntic is vitally important for adapting to a broad spectrum of\ndownstream tasks requiring precise object localization. For\nthe REC and RES tasks, we evaluated our model on Ref-\nCOCO [120], RefCOCO+ [120], and RefCOCOg [72], as\nshow in Table 1, GLEE achieved comparable results with\nSOTA specialist methods PolyFormer [62], demonstrating\nstrong capability to comprehend textual descriptions and\nshowcasing potential to adapt to a broader range of multi-\nmodal downstream tasks. In open-world instance segmen-\ntation tasks, we treated \u201dobject\u201d as the category name, in-\nstructing the model to identify all plausible instance in an\nimage in a class-agnostic manner. GLEE outperforms pre-\nvious arts ODISE [107] by 8.9 points, demonstrating the\n7\nModel\nPascalVOC\nAerialDrone\nAquarium\nRabbits\nEgoHands\nMushrooms\nPackages\nRaccoon\nShellfish\nVehicles\nPistols\nPothole\nThermal\nAvg\nGLIP-T\n56.2\n12.5\n18.4\n70.2\n50.0\n73.8\n72.3\n57.8\n26.3\n56.0\n49.6\n17.7\n44.1\n46.5\nGLIP-L\n61.7\n7.1\n26.9\n75.0\n45.5\n49.0\n62.8\n63.3\n68.9\n57.3\n68.6\n25.7\n66.0\n52.1\nGLEE-Lite\n61.7\n7.9\n23.2\n72.6\n41.9\n51.6\n32.9\n51.1\n35.0\n59.4\n45.6\n21.8\n56.9\n43.2\nGLEE-Lite-Scale\n61.2\n5.0\n23.9\n71.9\n46.2\n57.8\n25.6\n56.8\n33.1\n60.6\n57.1\n25.3\n52.5\n44.4\nGLEE-Plus\n67.8\n10.8\n38.3\n76.1\n47.4\n19.2\n29.4\n63.8\n66.7\n63.8\n62.6\n15.3\n66.5\n48.3\nGLEE-Plus-Scale\n67.5\n12.1\n39.7\n75.8\n50.3\n41.1\n42.4\n66.4\n64.0\n62.8\n61.8\n17.5\n63.8\n51.2\nGLEE-Pro\n68.9\n16.5\n37.6\n77.2\n23.3\n40.1\n44.7\n68.2\n66.2\n66.1\n63.2\n18.1\n65.8\n50.5\nGLEE-Pro-Scale\n69.1\n13.7\n34.7\n75.6\n38.9\n57.8\n50.6\n65.6\n62.7\n67.3\n69.0\n30.7\n59.1\n53.4\nTable 3. Zero-shot performance on 13 ODinW datasets.\nMethod\nBackbone\nYTVIS 2019 val [113]\nOVIS val [76]\nAP AP50\nAP75\nAP\nAP50\nAP75\nIFC [40]\nResNet-50\n42.8 65.8\n46.8\n13.1\n27.8\n11.6\nSeqFormer [101]\n47.4 69.8\n51.8\n15.1\n31.9\n13.8\nIDOL [103]\n49.5 74.0\n52.9\n30.2\n51.3\n30.0\nVITA [36]\n49.8 72.6\n54.5\n19.6\n41.2\n17.4\nGenVIS [37]\n51.3 72.0\n57.8\n34.5\n59.4\n35.0\nDVIS [124]\n52.6 76.5\n58.2\n34.1\n59.8\n32.3\nNOVIS [69]\n52.8 75.7\n56.9\n32.7\n56.2\n32.6\nUNINEXT\n53.0 75.2\n59.1\n34.0\n55.5\n35.6\nGLEE-Lite\n53.1 74.0\n59.3\n27.1/32.3 45.4/52.2 26.3/33.7\nSeqFormer [101]\nSwin-L\n59.3 82.1\n66.4\n-\n-\n-\nVITA [36]\n63.0 86.9\n67.9\n27.7\n51.9\n24.9\nIDOL [103]\n64.3 87.5\n71.0\n42.6\n65.7\n45.2\nGenVIS [37]\n63.8 85.7\n68.5\n45.4\n69.2\n47.8\nDVIS [124]\n64.9 88.0\n72.7\n49.9\n75.9\n53.0\nNOVIS [69]\n65.7 87.8\n72.2\n43.5\n68.3\n43.8\nGLEE-Plus\n63.6 85.2\n70.5\n29.6/40.3 50.3/63.8 28.9/39.8\nUNINEXT\nConvNeXt-L 64.3 87.2\n71.7\n41.1\n65.8\n42.0\nUNINEXT\nViT-H\n66.9 87.5\n75.1\n49.0\n72.5\n52.2\nGLEE-Pro\nEVA02-L\n67.4 87.1\n74.1\n38.7/50.4 59.4/71.4 39.7/55.5\nTable 4. Performance comparison of our GLEE on video instance\nsegmentation tasks.\ncapability of identifying all plausible instance that might be\npresent in an open-world scenario.\n4.3. Zero-shot Evaluation Across Tasks\nZero-shot Transfer to Video Tasks. The proposed GLEE\nis capable of adapting to new data and even new tasks in\na zero-shot manner, without the need for additional fine-\ntuning. We evaluate its zero-shot capability on three large-\nscale, large-vocabulary open-world video tracking datasets:\nTAO [20], BURST [3], and LV-VIS [93]. TAO comprises\n2,907 high-resolution videos across 833 categories. BURST\nbuilds upon TAO, encompassing 425 base categories and\n57 novel categories.\nLV-VIS offers 4,828 videos within\n1,196 well-defined object categories. These three bench-\nmarks require the model to detect, classify, and track all\nobjects in videos, while BURST and LV-VIS additionally\nrequire segmentation results from the model. In Table 2, we\ncompare the performance of our proposed model with ex-\nisting specialist models. Notably, the GLEE here is from\nthe second training stage, which has not been exposed to\nimages from these three datasets nor trained on video-\nlevel data. Despite these constraints, GLEE achieves state-\nof-the-art performance that significantly exceeds existing\nmethodologies. Specifically, GLEE surpasses the previous\nbest method OVTrack by 36.0% in TAO, nearly triples the\nperformance of the best baseline in BURST, and outper-\nforms OV2Seg [93] by 43.6% in LV-VIS. This outstanding\nperformance strongly validates the exceptional generaliza-\ntion and zero-shot capabilities of GLEE in handling object-\nlevel tasks across a range of benchmarks and tasks.\nWe additionally provide performance comparison on\nclassic video segmentation tasks, including VIS, VOS, and\nRVOS. As shown in Table 4, on the YTVIS2019 [113]\nbenchmark, our model achieves SOTA results across vari-\nous model sizes, surpassing all specialist models with com-\nplex designs to enhance temporal capabilities and the video\nunified model UNINEXT [112]. On the OVIS [76] bench-\nmark, which features lengthy videos with extensive object\nocclusions where temporal capabilities of object features\nare particularly crucial, our model does not directly reach\nSOTA. However, after a few hours of simple fine-tuning,\nit still achieves SOTA performance. This further validates\nthe versatility and generalization capabilities of our model.\nMore details on zero-shot evaluations for video tasks and\ndemonstrations of interactive segmentation and tracking can\nbe found in the Sec 7 of supplementary materials.\nZero-shot Transfer to Real-world Downstream Tasks.\nTo measure generalization on diverse real-world tasks, we\nevaluate zero-shot performance on OmniLabel [85], which\nis a benchmark for evaluating language-based object de-\ntectors and encourages complex and diverse free-form text\ndescriptions of objects.\nAs show in Table 5, compared\nto language-based detectors trained on large-scale caption\ndata, GLEE significantly outperforms previous models in\nP-categ. However, due to the limited captions in our train-\ning dataset, it scores lower in AP-descr. By incorporating\na more diverse set of box-caption data from the GRIT [75]\nto sclae up our training set, the AP-descr can be elevated to\na level comparable with existing models. We conduct ad-\nditional experiments on the \u201cObject Detection in the Wild\u201d\n(ODinW) benchmark [48], which is a suite of datasets cov-\nering a wide range of domains. We report the average mAP\non the subset of 13 ODinW detection datasets introduced\nin [52], and report the per-dataset performance in a zero-\nshot manner, as shown in Table 3. GLEE performs better\nthan GLIP [52] on the average of 13 public datasets, show-\n8\n76.95\n79.25\n73.78\n67.65\n72.42\n61.38\n70.83\n71.96\n76.35\n78.24\n73.81\n67.31\n71.25\n62.33\n71.56\n72.37\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nwith SAM\nwith GLEE\nRefCOCO\nRefCOCO+\nRefCOCOg\nFigure 4. The performance comparison of replacing SAM with\nGLEE in LISA, GLEE achieves the same effectiveness as SAM in\nextracting objects.\nTAO\nBURST\nOVIS\nYTVIS\n10%\n20%\n50%\n100%\n26.9\n29.5\n30.6\n31.2\nData Usage\nPerformance\n34.1\n35.0\n35.9\n38.7\n45.6\n45.2\n45.5\n47.2\n65.0\n66.5\n67.0\n67.4\nFigure 5. Data scaling. The performance of GLEE-Pro after train-\ning on 10%, 20%, 50%, 100% of the total data on TAO, BURST,\nOVIS, YTVIS19. Increased scale of training data result in en-\nhanced zero-shot performance across diverse downstream tasks.\ncasing its robust generalization capability. Furthermore, it\nis evident that by introducing automatically labeled data at\na low cost for scaling up the training data, the zero-shot ca-\npabilities can be further enhanced, this reveals that GLEE\nhas greater potential through scale-up.\nA more compre-\nhensive report on the per-dataset few-shot performance on\nODinW is available in the supplementary materials to as-\nsess the adaptability of GLEE to other datasets.\n4.4. Serve as Foundation Model\nTo explore whether GLEE can serve as a foundation model\nfor other architectures, we selected LISA [47] for analy-\nsis, a mVLLM that combines LLAVA [61] with SAM [43]\nfor reasoning segmentation. We substituted its vision back-\nbone with a frozen, pretrained GLEE-Plus and fed the ob-\nject queries from GLEE into LLAVA and remove decoder of\nLISA. We directly dot product the output SEG tokens with\nImages\nMethod\nAP\nAP-categ\nAP-descr\nAP-descr-pos\nAP-descr-S\nAP-descr-M\nAP-descr-L\nAll\nRegionCLIP [127] 2.7 2.7 2.6 3.2 3.6 2.7 2.3\nDetic [130]\n8.0 15.6 5.4 8.0 5.7 5.4 6.2\nMDETR [42]\n-\n-\n4.7 9.1 6.4 4.6 4.0\nGLIP-T [52]\n19.3 23.6 16.4 25.8 29.4 14.8 8.2\nGLIP-L [52]\n25.8 32.9 21.2 33.2 37.7 18.9 10.8\nFIBER-B [25]\n25.7 30.3 22.3 34.8 38.6 19.5 12.4\nGLEE-Lite\n20.3 37.5 14.0 19.1 23.0 12.7 10.0\nGLEE-Lite-Scale 22.7 35.5 16.7 22.3 33.7 14.3 10.2\nGLEE-Plus\n25.4 46.7 17.5 23.9 28.4 16.3 12.5\nGLEE-Plus-Scale 27.0 44.5 19.4 25.9 36.0 17.2 12.4\nTable 5. Evaluation on the OmniLabel benchmark. The final AP\nvalue is the geometric mean of categories (AP-categ) and free-\nform descriptions (AP-descr).\nGLEE feature map to generate masks. As shown in Fig-\nure 4, after training for the same number of steps, our mod-\nified LISA-GLEE achieved comparable results to the origi-\nnal version, demonstrating the versatility of representations\nfrom GLEE and its effectiveness in serving other models.\n4.5. Ablation\nWe conducted experiments to investigate the impact of\ntraining data scale on zero-shot performance across vari-\nous tasks. To this end, we trained GLEE-Pro with 10%,\n20%, 50%, 100% of the training data to evaluate the perfor-\nmance on zero-shot transfer tasks, including TAO, BURST,\nOVIS, and YTVIS as illustrated in the Figure 5. Our data\nscaling experiments reveal that increased sizes of training\ndatasets result in enhanced zero-shot performance across di-\nverse downstream tasks. This outcome implies that larger\npre-training datasets are a valuable investment, offering a\nmore effective and adaptable basis for a broad spectrum of\ndownstream tasks. Thanks to the unified training approach\nof GLEE, we can efficiently incorporate any manually or\nautomatically annotated data into our training process to\nachieve enhanced generalization capabilities.\n5. Conclusion\nWe introduce GLEE, a cutting-edge object-level foundation\nmodel designed to be directly applicable to a wide range of\nobject-level image and video tasks. Crafted with a unified\nlearning paradigm, GLEE learns from diverse data sources\nwith varying levels of supervisions. GLEE achieves state-\nof-the-art performance on numerous object-level tasks and\nexcels in zero-shot transfer to new data and tasks, showing\nits exceptional versatility and generalization abilities.\nAdditionally, GLEE provides general visual object-level\ninformation, which is currently missing in modern LLMs,\nestablishing a robust foundation for object-centric mLLMs.\n9\nReferences\n[1] David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Ef-\nficient interactive annotation of segmentation datasets with\npolygon-rnn++. 2018. 2\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nAdvances in Neural Information Processing Systems, 35:\n23716\u201323736, 2022. 2\n[3] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha\nKhurana, Achal Dave, Bastian Leibe, and Deva Ramanan.\nBurst: A benchmark for unifying object recognition, seg-\nmentation and tracking in video.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 1674\u20131683, 2023. 7, 8\n[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe.\nTracking without bells and whistles. In ICCV, 2019. 5\n[5] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe.\nTracking without bells and whistles. In ICCV, 2019. 7\n[6] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In 2016\nIEEE international conference on image processing (ICIP),\npages 3464\u20133468. IEEE, 2016. 7\n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 1\n[8] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for\noptimal boundary & region segmentation of objects in nd\nimages.\nIn Proceedings eighth IEEE international con-\nference on computer vision. ICCV 2001, pages 105\u2013112.\nIEEE, 2001. 5\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neu-\nral information processing systems, 33:1877\u20131901, 2020.\n1, 2, 3\n[10] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,\nLaura Leal-Taix\u00b4e, Daniel Cremers, and Luc Van Gool. One-\nshot video object segmentation.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 221\u2013230, 2017. 1\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV,\n2020. 2\n[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn Proceedings of the IEEE/CVF international conference\non computer vision, pages 9650\u20139660, 2021. 2\n[13] Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, and\nSanja Fidler. Annotating object instances with a polygon-\nrnn. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 5230\u20135238, 2017. 2,\n5\n[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In International conference on ma-\nchine learning, pages 1691\u20131703. PMLR, 2020. 2\n[15] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J\nFleet, and Geoffrey E Hinton. A unified sequence interface\nfor vision tasks. Advances in Neural Information Process-\ning Systems, 35:31333\u201331346, 2022. 3, 6\n[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. In CVPR,\n2022. 2, 4, 6\n[17] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In European Conference on Computer Vi-\nsion, pages 640\u2013658. Springer, 2022. 2, 7\n[18] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Re-\nthinking space-time networks with improved memory cov-\nerage for efficient video object segmentation. Advances in\nNeural Information Processing Systems, 34:11781\u201311794,\n2021. 2\n[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2\n[20] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia\nSchmid, and Deva Ramanan. Tao: A large-scale benchmark\nfor tracking any object. In ECCV, 2020. 7, 8\n[21] Patrick Dendorfer, Aljosa Osep, Anton Milan, Konrad\nSchindler, Daniel Cremers, Ian Reid, Stefan Roth, and\nLaura Leal-Taix\u00b4e. Motchallenge: A benchmark for single-\ncamera multiple target tracking. International Journal of\nComputer Vision, 129(4):845\u2013881, 2021. 2, 5\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 1, 2\n[23] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,\nPhilip HS Torr, and Song Bai.\nMose: A new dataset\nfor video object segmentation in complex scenes.\narXiv\npreprint arXiv:2302.01872, 2023. 1, 2\n[24] Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo,\nJunshi Huang, and Xiaoming Wei. Progressive multimodal\ninteraction network for referring video object segmentation.\nThe 3rd Large-scale Video Object Segmentation Challenge,\n2021. 2\n[25] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan\nZhang,\nJianfeng Wang,\nLinjie Li,\nZicheng Liu,\nCe\nLiu, Yann LeCun, Nanyun Peng, et al.\nCoarse-to-fine\nvision-language pre-training with fusion in the backbone.\nNeurIPS, 35:32942\u201332956, 2022. 9\n[26] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,\nXinlong Wang, and Yue Cao. Eva-02: A visual represen-\ntation for neon genesis. arXiv preprint arXiv:2303.11331,\n2023. 6, 7\n10\n[27] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual represen-\ntation learning at scale. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 19358\u201319369, 2023. 2\n[28] Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu,\nHaibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma. Prompt-\ndet: Towards open-vocabulary detection using uncurated\nimages.\nIn Computer Vision \u2013 ECCV 2022: 17th Euro-\npean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part IX, page 701\u2013717, 2022. 3\n[29] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian,\nTsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph.\nSimple copy-paste is a strong data augmentation method for\ninstance segmentation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 2918\u20132928, 2021. 1\n[30] Golnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le,\nand Tsung-Yi Lin. Multi-task self-training for learning gen-\neral representations. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 8856\u2013\n8865, 2021. 3\n[31] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation.\nIn International Conference on\nLearning Representations, 2022. 3\n[32] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 2, 5, 6, 7,\n1\n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 4, 7\n[34] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 2\n[35] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scal-\nable vision learners. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n16000\u201316009, 2022. 1, 2\n[36] Miran Heo, Sukjun Hwang, Seoung Wug Oh, Joon-Young\nLee, and Seon Joo Kim. Vita: Video instance segmenta-\ntion via object token association. In Advances in Neural\nInformation Processing Systems, 2022. 8\n[37] Miran Heo, Sukjun Hwang, Jeongseok Hyun, Hanjung\nKim, Seoung Wug Oh, Joon-Young Lee, and Seon Joo\nKim. A generalized framework for video instance segmen-\ntation.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n14623\u201314632, 2023. 2, 5, 8\n[38] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-\nmentation from natural language expressions. In Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11\u201314, 2016, Proceedings,\nPart I 14, pages 108\u2013124. Springer, 2016. 2\n[39] De-An Huang, Zhiding Yu, and Anima Anandkumar. Min-\nvis: A minimal video instance segmentation framework\nwithout video-based training. In NeurIPS, 2022. 5\n[40] Sukjun Hwang, Miran Heo, Seoung Wug Oh, and Seon Joo\nKim. Video instance segmentation using inter-frame com-\nmunication transformers. In NeurIPS, 2021. 8\n[41] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig.\nScaling up visual and vision-language\nrepresentation learning with noisy text supervision. In In-\nternational conference on machine learning, pages 4904\u2013\n4916. PMLR, 2021. 2, 3\n[42] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal under-\nstanding. In ICCV, pages 1780\u20131790, 2021. 6, 9\n[43] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. In ICCV, 2023. 1, 2, 6, 9\n[44] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vi-\nsion, 123:32\u201373, 2017. 2, 6, 1\n[45] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and\nAnelia Angelova. Open-vocabulary object detection upon\nfrozen vision and language models. In The Eleventh Inter-\nnational Conference on Learning Representations, 2023. 3\n[46] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, Tom\nDuerig, and Vittorio Ferrari. The open images dataset v4:\nUnified image classification, object detection, and visual re-\nlationship detection at scale. IJCV, 2020. 2, 6, 1\n[47] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia.\nLisa:\nReasoning seg-\nmentation via large language model.\narXiv preprint\narXiv:2308.00692, 2023. 2, 9\n[48] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang,\nJyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng\nLiu, Yong Jae Lee, et al.\nElevater: A benchmark and\ntoolkit for evaluating language-augmented visual models.\nIn NeurIPS, 2022. 8, 1, 2\n[49] Dezhuang Li, Ruoqi Li, Lijun Wang, Yifan Wang, Jinqing\nQi, Lu Zhang, Ting Liu, Qingquan Xu, and Huchuan Lu.\nYou only infer once: Cross-modal meta-transfer for refer-\nring video object segmentation. In AAAI, 2022. 2\n[50] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang,\nLionel M Ni, and Heung-Yeung Shum. Mask dino: To-\nwards a unified transformer-based framework for object de-\ntection and segmentation. In CVPR, 2023. 3, 4, 6, 7, 1\n[51] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\nLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\nWenhai Wang, et al. Uni-perceiver v2: A generalist model\nfor large-scale vision and vision-language tasks. In Pro-\n11\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2691\u20132700, 2023. 3, 6\n[52] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn CVPR, pages 10965\u2013\n10975, 2022. 3, 8, 9, 2\n[53] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E\nHuang, and Fisher Yu. Tracking every thing in the wild.\nIn ECCV, 2022. 7\n[54] Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin\nDanelljan, and Fisher Yu. Ovtrack: Open-vocabulary mul-\ntiple object tracking. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5567\u20135577, 2023. 7\n[55] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object\ndetection. In European Conference on Computer Vision,\npages 280\u2013296. Springer, 2022. 4, 6\n[56] Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang,\nZongxin Yang, Yunchao Wei, and Yi Yang.\nRethink-\ning cross-modal interaction from a top-down perspective\nfor referring video object segmentation.\narXiv preprint\narXiv:2106.01061, 2021. 2\n[57] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu,\nGholamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learn-\ning object-language alignments for open-vocabulary object\ndetection.\nIn The Eleventh International Conference on\nLearning Representations, 2023. 3\n[58] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV, 2014. 2, 5, 6, 7, 1\n[59] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Doll\u00b4ar. Focal loss for dense object detection. In\nICCV, 2017. 5\n[60] Zhihui Lin, Tianyu Yang, Maomao Li, Ziyu Wang,\nChun Yuan, Wenhao Jiang, and Wei Liu.\nSwem: To-\nwards real-time video object segmentation with sequen-\ntial weighted expectation-maximization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1362\u20131372, 2022. 2\n[61] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee.\nVisual\ninstruction\ntuning.\narXiv\npreprint\narXiv:2304.08485, 2023. 9\n[62] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-\nmar Satzoda, Vijay Mahadevan, and R Manmatha. Poly-\nformer: Referring image segmentation as sequential poly-\ngon generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n18653\u201318663, 2023. 2, 5, 6, 7\n[63] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Ni-\nethammer.\nSimpleclick: Interactive image segmentation\nwith simple vision transformers.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 22290\u201322300, 2023. 5\n[64] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 7\n[65] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 1\n[66] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-io: A unified\nmodel for vision, language, and multi-modal tasks. arXiv\npreprint arXiv:2206.08916, 2022. 3\n[67] Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yi-\ntong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc:\nSemantic-assisted object cluster for referring video object\nsegmentation. In NeurIPS, 2023. 2\n[68] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. TrackFormer: Multi-object track-\ning with transformers. arXiv preprint arXiv:2101.02702,\n2021. 2, 5\n[69] Tim Meinhardt, Matt Feiszli, Yuchen Fan, Laura Leal-\nTaixe, and Rakesh Ranjan. Novis: A case for end-to-end\nnear-online video instance segmentation.\narXiv preprint\narXiv:2308.15266, 2023. 8\n[70] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 2016 fourth international\nconference on 3D vision (3DV), 2016. 5\n[71] Matthias Minderer,\nAlexey Gritsenko,\nAustin Stone,\nMaxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,\nAravindh Mahendran, Anurag Arnab, Mostafa Dehghani,\nZhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf,\nand Neil Houlsby. Simple open-vocabulary object detec-\ntion. In Computer Vision \u2013 ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Pro-\nceedings, Part X, page 728\u2013755, 2022. 3\n[72] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling context between objects for referring expression\nunderstanding. In ECCV, 2016. 2, 6, 7, 1\n[73] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9226\u20139235, 2019.\n2\n[74] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li,\nTrevor Darrell, and Fisher Yu. Quasi-dense similarity learn-\ning for multiple object tracking. In CVPR, 2021. 7\n[75] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2, 6, 8, 1\n[76] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu,\nXiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and\nSong Bai. Occluded video instance segmentation: A bench-\nmark. International Journal of Computer Vision, pages 1\u2013\n18, 2022. 5, 6, 8, 1, 2\n[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\n12\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 1, 2, 3, 6\n[78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. The Journal of Ma-\nchine Learning Research, 21(1):5485\u20135551, 2020. 1, 2, 3\n[79] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Interna-\ntional Conference on Machine Learning, pages 8821\u20138831.\nPMLR, 2021. 2\n[80] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 2\n[81] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In CVPR, 2019. 5\n[82] Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan,\nFahad Shahbaz Khan, and Michael Felsberg. Learning fast\nand robust target models for video object segmentation. In\nCVPR, 2020. 2\n[83] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[84] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n\u201d grabcut\u201d interactive foreground extraction using iterated\ngraph cuts. ACM transactions on graphics (TOG), 23(3):\n309\u2013314, 2004. 5\n[85] Samuel Schulter, Vijay Kumar B G, Yumin Suh, Konstanti-\nnos M. Dafnis, Zhixing Zhang, Shiyu Zhao, and Dim-\nitris Metaxas.\nOmnilabel: A challenging benchmark for\nlanguage-based object detection. In ICCV, 2023. 8\n[86] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos:\nUnified referring video object segmentation network with a\nlarge-scale benchmark. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XV 16, pages 208\u2013223. Springer,\n2020. 2, 5, 6, 1\n[87] Jing Shao, Siyu Chen, Yangguang Li, Kun Wang, Zhenfei\nYin, Yinan He, Jianing Teng, Qinghong Sun, Mengya Gao,\nJihao Liu, et al. Intern: A new learning paradigm towards\ngeneral vision. arXiv preprint arXiv:2111.08687, 2021. 3\n[88] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 8430\u20138439, 2019. 2, 5, 6, 1\n[89] Konstantin Sofiiuk, Ilya A Petrov, and Anton Konushin.\nReviving iterative training with mask guidance for inter-\nactive segmentation. In 2022 IEEE International Confer-\nence on Image Processing (ICIP), pages 3141\u20133145. IEEE,\n2022. 5\n[90] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, et al.\nSparse r-cnn: End-to-end object\ndetection with learnable proposals. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14454\u201314463, 2021. 2\n[91] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 2\n[92] Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and\nBastian Leibe.\nSiam R-CNN: Visual tracking by re-\ndetection. In CVPR, 2020. 2\n[93] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang,\nXu Tang, Yao Hu, Weidi Xie, and Efstratios Gavves. To-\nwards open-vocabulary video instance segmentation.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4057\u20134066, 2023. 7, 8\n[94] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learn-\ning framework. In International Conference on Machine\nLearning, pages 23318\u201323340. PMLR, 2022. 3, 6\n[95] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip H. S. Torr. Fast online object tracking and segmen-\ntation: A unifying approach. In CVPR, 2019. 2\n[96] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.\nUnidentified video objects: A benchmark for dense, open-\nworld segmentation. In ICCV, 2021. 6, 1, 2\n[97] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022. 2\n[98] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua\nShen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-\nto-end video instance segmentation with transformers. In\nCVPR, 2021. 2\n[99] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple\nonline and realtime tracking with a deep association metric.\nIn ICIP, 2017. 7\n[100] Jiajun Wu, Yibiao Zhao, Jun-Yan Zhu, Siwei Luo, and\nZhuowen Tu. Milcut: A sweeping line multiple instance\nlearning paradigm for interactive image segmentation. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 256\u2013263, 2014. 5\n[101] Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, and Xi-\nang Bai. Seqformer: Sequential transformer for video in-\nstance segmentation. In ECCV, 2022. 2, 8\n[102] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping\nLuo. Language as queries for referring video object seg-\nmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4974\u2013\n4984, 2022. 2, 5\n13\n[103] Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille,\nand Xiang Bai. In defense of online models for video in-\nstance segmentation. In ECCV, pages 588\u2013605. Springer,\n2022. 2, 5, 8\n[104] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan,\nand Ping Luo. Segment every reference object in spatial and\ntemporal spaces. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2538\u20132550,\n2023. 2\n[105] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 1\n[106] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong\nHu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan.\nFlorence-2: Advancing a unified representation for a vari-\nety of vision tasks. arXiv preprint arXiv:2311.06242, 2023.\n6\n[107] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 6,\n7\n[108] Ning Xu, Linjie Yang, Jianchao Yang, Dingcheng Yue,\nYuchen Fan, Yuchen Liang, and Thomas S. Huang.\nYoutubevis dataset 2021 version. https://youtube-\nvos.org/dataset/vis/. 6, 1, 2\n[109] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and\nThomas S Huang. Deep interactive object selection. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 373\u2013381, 2016. 5\n[110] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue,\nYuchen Liang,\nJianchao Yang,\nand Thomas Huang.\nYoutube-vos:\nA large-scale video object segmentation\nbenchmark. arXiv preprint arXiv:1809.03327, 2018. 2, 1\n[111] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Towards grand unification of\nobject tracking. In ECCV, 2022. 2\n[112] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-\nhuan Yuan, and Huchuan Lu. Universal instance perception\nas object discovery and retrieval. In CVPR, 2023. 3, 5, 6,\n7, 8, 2\n[113] Linjie Yang, Yuchen Fan, and Ning Xu.\nVideo instance\nsegmentation. In ICCV, 2019. 2, 5, 6, 8, 1\n[114] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling. In European Conference on Computer\nVision, pages 521\u2013539. Springer, 2022. 3, 6\n[115] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang,\nDan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang\nXu. Detclip: Dictionary-enriched visual-concept paralleled\npre-training for open-world detection. In NeurIPS, 2022. 4\n[116] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\nZhang, Zhenguo Li, and Hang Xu.\nDetclipv2: Scalable\nopen-vocabulary object detection pre-training via word-\nregion alignment. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n23497\u201323506, 2023. 3\n[117] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.\nCross-modal self-attention network for referring image seg-\nmentation. In CVPR, 2019. 2\n[118] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingy-\ning Chen, Fangchen Liu, Vashisht Madhavan, and Trevor\nDarrell. Bdd100k: A diverse driving dataset for heteroge-\nneous multitask learning. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 2636\u20132645, 2020. 6, 1\n[119] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. Transactions\non Machine Learning Research, 2022. 3\n[120] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions. In ECCV, 2016. 2, 5, 6, 7, 1\n[121] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, et al.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 2, 3\n[122] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and\nShih-Fu Chang.\nOpen-vocabulary object detection using\ncaptions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 14393\u2013\n14402, 2021. 3\n[123] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan,\nJenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying lo-\ncalization and vision-language understanding. In Advances\nin Neural Information Processing Systems, pages 36067\u2013\n36080, 2022. 6\n[124] Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo\nWang, Yuan Zhang, and Pengfei Wan. Dvis: Decoupled\nvideo instance segmentation framework. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 1282\u20131291, 2023. 8\n[125] Yizhuo Zhang, Zhirong Wu, Houwen Peng, and Stephen\nLin. A transductive approach for video object segmenta-\ntion. In CVPR, 2020. 2\n[126] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Byte-\ntrack: Multi-object tracking by associating every detection\nbox. arXiv preprint arXiv:2110.06864, 2021. 2, 5\n[127] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\nLi, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\nDai, Lu Yuan, Yin Li, et al.\nRegionclip: Region-based\nlanguage-image pretraining. In CVPR, pages 16793\u201316803,\n2022. 9\n[128] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-\nyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,\nXiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-\nbased language-image pretraining. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16793\u201316803, 2022. 3\n14\n[129] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00a8ahenb\u00a8uhl.\nTracking objects as points. In ECCV, 2020. 2, 5\n[130] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra. Detecting twenty-thousand\nclasses using image-level supervision.\nIn ECCV, pages\n350\u2013368. Springer, 2022. 7, 9\n[131] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,\nXingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-\naoshuai Sun, and Rongrong Ji. Seqtr: A simple yet univer-\nsal network for visual grounding. In European Conference\non Computer Vision, pages 598\u2013615. Springer, 2022. 2, 5,\n6\n[132] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 2, 3\n[133] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\nLi, Xiaohua Wang, and Jifeng Dai.\nUni-perceiver: Pre-\ntraining unified architecture for generic perception for zero-\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16804\u201316815, 2022. 3\n[134] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,\nLu Yuan, et al. Generalized decoding for pixel, image, and\nlanguage. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 15116\u2013\n15127, 2023. 3, 6\n[135] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie\nLi, Jianfeng Gao, and Yong Jae Lee. Segment everything\neverywhere all at once. In NeurIPS, 2023. 2, 3\n15\nGeneral Object Foundation Model for Images and Videos at Scale\nSupplementary Material\nIn this supplementary material, we first provide more\ndetailed information on data usage and model training in\nSec 6. Subsequently, in Sec 7, we supplement additional\nzero-shot and fine-tuning results on classic object-level\nvideo tasks, such as VOS and RVOS. In Sec 8, detailed\nfew-shot experimental results on the ODinW [48] bench-\nmark are provided to validate the transferability of GLEE to\nvarious real-world tasks. Finally, in Sec 9, we showcase the\nresults in interactive segmentation and tracking for images\nand videos.\n6. Datasets and Implementation Details\nData Preparation. To ensure the generalization of GLEE\nas an object-level foundation model, we conduct joint\ntraining using a substantial amount of data with region-\nlevel annotations from both images and videos.\nExist-\ning datasets exhibit variations in annotation granularity:\ndetection datasets such as Objects365 [88] and OpenIm-\nages [46] provide bounding boxes and category names;\nCOCO [58] and LVIS [32] offer more detailed mask annota-\ntions; RefCOCO [72, 120] and Visual Genome [44] include\ncomprehensive object descriptions.\nFurthermore, video\ndatasets [76, 86, 96, 108, 110, 113] contribute to the tem-\nporal consistency of models, and open-world data [43, 96]\nenrich the annotations with class-agnostic object informa-\ntion. A comprehensive list of the datasets we utilized, along\nwith their respective sizes and annotation granularities, is\npresented in Table 6.\nWe extracted subsets of 500,000\nand 2,000,000 images from the SA1B [43] dataset for joint\ntraining in stage 2 and scale-up training respectively. To en-\nsure that objects from SA1B are at the object-level rather\nthan the part-level, we apply mask IoU based NMS and use\narea as NMS score to eliminate part-level object annota-\ntions. For GRIT [75] data, we extract 5,000,000 samples\nfor scale-up training to enhance the richness of object de-\nscriptions.\nModel and Training Details.\nFollowing the image\nbackbone, text encoder, and visual prompter, we incorpo-\nrate a 6-layer deformable transformer encoder and a 9-\nlayer decoder to serve as our Object Decoder following\nMaskDINO [50]. We adopt 300 object queries, query de-\nnoising, and hybrid matching to accelerate convergence and\nimprove performance. During the pretraining phase of stage\n1, we sample data from Objects365 and OpenImages in a\n1:1 ratio, with the batch size of 128 for 500,000 training\niterations. Moving to stage 2, we train GLEE for 500,000\niterations on all image-level data jointly according to the ra-\ntios outlined in Table 7. For the scale-up training, we set the\nSizes\nAnnotations\ndataset\nimages\nobjects\nsemantic\nbox\nmask\ntrack id\nDetection Data\nObjects365 [88]\n1817287\n26563198\ncategory\n\u2713\n-\n-\nOpenImages [46]\n1743042\n14610091\ncategory\n\u2713\n-\n-\nLVIS [32]\n100170\n1270141\ncategory\n\u2713\n\u2713\n-\nCOCO [58]\n118287\n860001\ncategory\n\u2713\n\u2713\n-\nBDD [118]\n69863\n1274792\ncategory\n\u2713\n\u2713\n-\nGrounding Data\nRefCOCO [120]\n16994\n42404\ndescription\n\u2713\n\u2713\n-\nRefCOCOg [72]\n21899\n42226\ndescription\n\u2713\n\u2713\n-\nRefCOCO+ [120]\n16992\n42278\ndescription\n\u2713\n\u2713\n-\nVisualGenome [44]\n77396\n3596689\ndescription\n\u2713\n-\n-\nGRIT [75]\n5117307\n9090607\ndescription\n\u2713\n-\n-\nOpenWorld Data\nUVO [96]\n16923\n157624\n-\n\u2713\n\u2713\n-\nSA1B [43]\n2147712\n99427126\n-\n\u2713\n\u2713\n-\nVideo Data\nYTVIS19 [113]\n61845\n97110\ncategory\n\u2713\n\u2713\n\u2713\nYTVIS21 [108]\n90160\n175384\ncategory\n\u2713\n\u2713\n\u2713\nOVIS [76]\n42149\n206092\ncategory\n\u2713\n\u2713\n\u2713\nUVO-dense [96]\n45270\n657990\n-\n\u2713\n\u2713\n\u2713\nVOS [110]\n94588\n156310\n-\n\u2713\n\u2713\n\u2713\nRefVOS [86]\n93857\n159961\ndescription\n\u2713\n\u2713\n\u2713\nTable 6. The tasks GLEE learns to complete and the datasets used\nin training.\nsampling ratios for SA1B and GRIT to 5.0 in Table 7, and\ntrain for an extra 500,000 iterations. We used AdamW [65]\noptimizer with base learning rate of 1 \u00d7 10\u22124, and weight\ndecay of 0.05, learning rate is decayed at the 400,000 itera-\ntions by a factor of 0.1. Learning rates of the image back-\nbone and text encoder are multiplied by a factor of 0.1. For\nthe ResNet-50 backbone and Swin backbone, we use scale\naugmentation [105], resizing the input images such that the\nshortest side is at least 480 and at most 800 pixels while the\nlongest at most 1333. For EVA02-L backbone, we use the\nlarge-scale jittering (LSJ) [29] augmentation with a random\nscale sampled from range 0.1 to 2.0 followed by a fixed size\ncrop to 1536\u00d71536.\n7. Transfer to Video Tasks\nTo substantiate the effectiveness of GLEE across diverse\nobject-level video tasks, we present the performance on\nVOS and RVOS tasks in Table 8 and Table 9 respectively.\nVOS. Video object segmentation (VOS) aims at seg-\nmenting a particular object throughout the entire video\nclip sequence.\nWe evaluate GLEE on semi-supervised\nVOS [10] that gives the first-frame mask of the target ob-\nject on YouTube-VOS 2018 [110] and MOSE [23]. Given\nthe first-frame mask of the target object, we first crop the\nprompt square area from RGB image and send it to the im-\nage backbone to obtain the visual prompt feature of the cor-\nresponding area, and send it to the early fusion module be-\n1\nDatasets\nOpenImages\nObjects365\nLVIS\nVisualGenome\nCOCO\nRefCOCO-mixed\nSA1B\nUVO-frame\nBDD\nYTVIS19\nYTVIS21\nOVIS\nRef-YTBVOS\nRatio\n1.5\n1.5\n1.5\n2\n1.5\n2.5\n2.5\n0.2\n0.15\n0.3\n0.3\n0.3\n0.3\nTable 7. The data sampling ratios during the joint-training of stage 2. RefCOCO-mixed refers to the mixed dataset of RefCOCO [120],\nRefCOCO+ [120], RefCOCOg [72], and the last four video datasets are treated as independent image data for training.\nMethod\nYT-VOS 2018 val [110]\nMOSE val [23]\nG\nJs\nFs\nJu\nFu J &F\nJ\nF\nMemory\nSTM [73]\n79.4 79.7 84.2 72.8 80.9\n-\n-\n-\nSWEM [60]\n82.8 82.4 86.9 77.1 85.0 50.9 46.8 64.9\nSTCN [18]\n83.0 81.9 86.5 77.9 85.7 50.8 46.6 55.0\nXMem [17]\n86.1 85.1 89.8 80.3 89.2 57.6 53.3 62.0\nNon-Memory\nSiamMask [95]\n52.8 60.2 58.2 45.1 47.7\n-\n-\n-\nSiam R-CNN [92]\n73.2 73.5\n-\n66.2\n-\n-\n-\n-\nTVOS [125]\n67.8 67.1 69.4 63.0 71.6\n-\n-\n-\nFRTM [82]\n72.1 72.3 76.2 65.9 74.1\n-\n-\n-\nUNINEXT-R50 [112] 77.0 76.8 81.0 70.8 79.4\n-\n-\n-\nUNINEXT-L [112]\n78.1 79.1 83.5 71.0 78.9\n-\n-\n-\nUNINEXT-H [112]\n78.6 79.9 84.9 70.6 79.2\n-\n-\n-\nGLEE-Lite\n80.4 80.2 85.5 74.3 81.4 56.1 51.8 60.4\nTable 8. Performance comparison of our GLEE on video object\nsegmentation tasks.\nfore the Transformer encoder. Then we sample fine-grained\nvisual embeddings from the pixel embedding map Mp in-\nside the given mask area and make them interacted with\nobject queries through self-attention module in the Trans-\nformer decoder layer. We conduct fine-tuning of GLEE-\nLite jointly on YouTube-VOS [110], YTVIS2019 [113],\nYTVIS2021 [108], OVIS [76], and UVO-video [96] for\n40,000 iterations. The evaluation is performed on YouTube-\nVOS and MOSE, as shown in the Table 8. It is noteworthy\nthat semi-supervised VOS is almost dominated by space-\ntime memory networks [17, 18, 60, 73] which construct a\nmemory bank for each object in the video. GLEE achieves\nthe best results among all non-memory-based methods on\nYouTube-VOS and even demonstrating competitive results\ncompared to memory-based methods on the more challeng-\ning MOSE dataset.\nRVOS. Referring Video Object Segmentation (R-VOS)\naims at finding objects matched with the given language ex-\npressions in a given video and segment them. Ref-YouTube-\nVOS [86] is a popular R-VOS benchmarks, which are con-\nstructed by introducing language expressions for the objects\nin the original YouTube-VOS [110] dataset. As same as\nsemi-supervised VOS, region similarity J , contour accu-\nracy F, and the averaged score J &F are adopted as the\nmetrics. Given an object expression and a video, we send\nthe description into the text encoder, select the object query\nwith the highest confidence score and compute its mask.\nAdditionally, we introduce temporal consistency by adding\nthe similarity between the 300 object queries of the cur-\nrent frame and the object query selected in the previous\nframe to the current confidence score. We directly evaluate\nthe GLEE trained from stage 2 on Ref-YouTube-VOS. As\nshown in Table 9, GLEE outperforms all previous R-VOS\napproaches and unified method.\nMethod\nBackbone\nJ &F\nJ\nF\nCMSA [117]\nResNet-50\n36.4\n34.8\n38.1\nYOFO [49]\n48.6\n47.5\n49.7\nReferFormer [102]\n58.7\n57.4\n60.1\nUNINEXT [112]\n61.2\n59.3\n63.0\nPMINet + CFBI [24]\nEnsemble\n54.2\n53.0\n55.5\nCITD [56]\n61.4\n60.0\n62.7\nReferFormer [102]\nVideo-Swin-B\n64.9\n62.8\n67.0\nSOC [67]\n67.3\n65.3\n69.3\nUNINEXT [112]\nConvNext-L\n66.2\n64.0\n68.4\nUNINEXT [112]\nViT-H\n70.1\n67.6\n72.7\nGLEE-Plus\nSwin-L\n67.7\n65.6\n69.7\nGLEE-Pro\nEVA02-L\n70.6\n68.2\n72.9\nTable 9. Performance comparison of our GLEE on Ref-YouTube-\nVOS task.\n8. Object Detection in the Wild\nTo further validate transferability of GLEE on diverse real-\nworld detection tasks, we assess its few-shot transfer abil-\nity on the ODinW [48] dataset.\nWe vary the amount\nof task-specific annotated data from X-shot, providing at\nleast X examples per category, to using all the available\ndata in the training set, following the procedure estab-\nlished by GLIP [52]. We fine-tune the models on the pro-\nvided data using the same hyper-parameters across all mod-\nels in a full-model tuning regime. For manually designed\nprompts, we revise the category names for the two datasets\n(\u201cCottontail-Rabbit\u201d to \u201crabbit\u201d and \u201cCow/Chanterelle\u201d to\n\u201cCow/Chanterelle mushroom\u201d) to provide language guid-\nance. Models train with a batch size of 4 and a learning\nrate of 1 \u00d7 10\u22124, undergoing 200, 300, 400, 600, and 2000\niterations for the 1, 3, 5, 10, and ALL shot splits, respec-\ntively. The optimal model is selected based on the validation\nsplit for each train/val split. For each few-shot setting, we\ntrain the models three times using different random seeds\nfor train/val splits, and the average score and standard devi-\nation on the test split are reported, as shown in the Table 10.\n2\nModel\nShot\nTune\nPascalVOC\nAerialDrone\nAquarium\nRabbits\nEgoHands\nMushrooms\nPackages\nRaccoon\nShellfish\nVehicles\nPistols\nPothole\nThermal\nAvg\nDyHead COCO\n1\nFull\n31.7\u00b13.1\n14.3\u00b12.4\n13.1\u00b12.0\n63.6\u00b11.4\n40.9\u00b17.0\n67.0\u00b13.6\n34.6\u00b112.1\n45.9\u00b13.8\n10.8\u00b15.0\n34.0\u00b13.3\n12.0\u00b110.4\n6.1\u00b11.3\n40.9\u00b17.4\n31.9\u00b13.3\nDyHead COCO\n3\nFull\n44.1\u00b10.7\n19.2\u00b13.0\n22.6\u00b11.3\n64.8\u00b11.7\n54.4\u00b12.5\n78.9\u00b11.3\n61.6\u00b110.3\n50.0\u00b12.1\n20.8\u00b13.5\n44.9\u00b11.9\n34.4\u00b111.1\n20.6\u00b12.4\n57.9\u00b12.3\n44.2\u00b10.3\nDyHead COCO\n5\nFull\n44.9\u00b11.5\n22.2\u00b13.0\n31.7\u00b11.0\n65.2\u00b11.5\n55.6\u00b13.7\n78.7\u00b13.9\n50.1\u00b113.7\n48.7\u00b14.8\n22.8\u00b13.3\n52.0\u00b11.2\n39.8\u00b16.7\n20.9\u00b11.5\n48.0\u00b12.8\n44.7\u00b11.7\nDyHead COCO\n10\nFull\n48.4\u00b11.2\n27.5\u00b11.4\n39.3\u00b12.7\n62.1\u00b15.9\n61.6\u00b11.4\n81.7\u00b13.4\n58.8\u00b19.0\n52.9\u00b13.2\n30.1\u00b13.2\n54.1\u00b13.3\n44.8\u00b14.9\n26.7\u00b12.4\n63.4\u00b12.8\n50.1\u00b11.6\nDyHead COCO\nAll\nFull\n60.1\n27.6\n53.1\n76.5\n79.4\n86.1\n69.3\n55.2\n44.0\n61.5\n70.6\n56.6\n81.0\n63.2\nDyHead O365\n1\nFull\n25.8\u00b13.0\n16.5\u00b11.8\n15.9\u00b12.7\n55.7\u00b16.0\n44.0\u00b13.6\n66.9\u00b13.9\n54.2\u00b15.7\n50.7\u00b17.7\n14.1\u00b13.6\n33.0\u00b111.0\n11.0\u00b16.5\n8.2\u00b14.1\n43.2\u00b110.0\n33.8\u00b13.5\nDyHead O365\n3\nFull\n40.4\u00b11.0\n20.5\u00b14.0\n26.5\u00b11.3\n57.9\u00b12.0\n53.9\u00b12.5\n76.5\u00b12.3\n62.6\u00b113.3\n52.5\u00b15.0\n22.4\u00b11.7\n47.4\u00b12.0\n30.1\u00b16.9\n19.7\u00b11.5\n57.0\u00b12.3\n43.6\u00b11.0\nDyHead O365\n5\nFull\n43.5\u00b11.0\n25.3\u00b11.8\n35.8\u00b10.5\n63.0\u00b11.0\n56.2\u00b13.9\n76.8\u00b15.9\n62.5\u00b18.7\n46.6\u00b13.1\n28.8\u00b12.2\n51.2\u00b12.2\n38.7\u00b14.1\n21.0\u00b11.4\n53.4\u00b15.2\n46.4\u00b11.1\nDyHead O365\n10\nFull\n46.6\u00b10.3\n29.0\u00b12.8\n41.7\u00b11.0\n65.2\u00b12.5\n62.5\u00b10.8\n85.4\u00b12.2\n67.9\u00b14.5\n47.9\u00b12.2\n28.6\u00b15.0\n53.8\u00b11.0\n39.2\u00b14.9\n27.9\u00b12.3\n64.1\u00b12.6\n50.8\u00b11.3\nDyHead O365\nAll\nFull\n53.3\n28.4\n49.5\n73.5\n77.9\n84.0\n69.2\n56.2\n43.6\n59.2\n68.9\n53.7\n73.7\n60.8\nGLIP-T\n1\nFull\n54.8\u00b12.0\n18.4\u00b11.0\n33.8\u00b11.1\n70.1\u00b12.9\n64.2\u00b11.8\n83.7\u00b13.0\n70.8\u00b12.1\n56.2\u00b11.8\n22.9\u00b10.2\n56.6\u00b10.5\n59.9\u00b10.4\n18.9\u00b11.3\n54.5\u00b12.7\n51.1\u00b10.1\nGLIP-T\n3\nFull\n58.1\u00b10.5\n22.9\u00b11.3\n40.8\u00b10.9\n65.7\u00b11.6\n66.0\u00b10.2\n84.7\u00b10.5\n65.7\u00b12.8\n62.6\u00b11.4\n27.2\u00b12.7\n61.9\u00b11.8\n60.7\u00b10.2\n27.1\u00b11.2\n70.4\u00b12.5\n54.9\u00b10.2\nGLIP-T\n5\nFull\n59.5\u00b10.4\n23.8\u00b10.9\n43.6\u00b11.4\n68.7\u00b11.3\n66.1\u00b10.6\n85.4\u00b10.4\n72.3\u00b10.0\n62.1\u00b12.0\n27.3\u00b11.2\n61.0\u00b11.8\n62.7\u00b11.6\n34.5\u00b10.5\n66.6\u00b12.3\n56.4\u00b10.4\nGLIP-T\n10\nFull\n59.1\u00b11.3\n26.3\u00b11.1\n46.3\u00b11.6\n67.3\u00b11.5\n67.1\u00b10.7\n87.8\u00b10.5\n72.3\u00b10.0\n57.7\u00b11.7\n34.6\u00b11.7\n65.4\u00b11.4\n61.6\u00b11.0\n39.3\u00b11.0\n74.7\u00b12.3\n58.4\u00b10.2\nGLIP-T\nAll\nFull\n62.3\n31.2\n52.5\n70.8\n78.7\n88.1\n75.6\n61.4\n51.4\n65.3\n71.2\n58.7\n76.7\n64.9\nGLIP-L\n1\nFull\n64.8\u00b10.6\n18.7\u00b10.6\n39.5\u00b11.2\n70.0\u00b11.5\n70.5\u00b10.2\n69.8\u00b118.0\n70.6\u00b14.0\n68.4\u00b11.2\n71.0\u00b11.3\n65.4\u00b11.1\n68.1\u00b10.2\n28.9\u00b12.9\n72.9\u00b14.7\n59.9\u00b11.4\nGLIP-L\n3\nFull\n65.6\u00b10.6\n22.3\u00b11.1\n45.2\u00b10.4\n72.3\u00b11.4\n70.4\u00b10.4\n81.6\u00b113.3\n71.8\u00b10.3\n65.3\u00b11.6\n67.6\u00b11.0\n66.7\u00b10.9\n68.1\u00b10.3\n37.0\u00b11.9\n73.1\u00b13.3\n62.1\u00b10.7\nGLIP-L\n5\nFull\n66.6\u00b10.4\n26.4\u00b12.5\n49.5\u00b11.1\n70.7\u00b10.2\n71.9\u00b10.2\n88.1\u00b10.0\n71.1\u00b10.6\n68.8\u00b11.2\n68.5\u00b11.7\n70.0\u00b10.9\n68.3\u00b10.5\n39.9\u00b11.4\n75.2\u00b12.7\n64.2\u00b10.3\nGLIP-L\n10\nFull\n66.4\u00b10.7\n32.0\u00b11.4\n52.3\u00b11.1\n70.6\u00b10.7\n72.4\u00b10.3\n88.1\u00b10.0\n67.1\u00b13.6\n64.7\u00b13.1\n69.4\u00b11.4\n71.5\u00b10.8\n68.4\u00b10.7\n44.3\u00b10.6\n76.3\u00b11.1\n64.9\u00b10.7\nGLIP-L\nAll\nFull\n69.6\n32.6\n56.6\n76.4\n79.4\n88.1\n67.1\n69.4\n65.8\n71.6\n75.7\n60.3\n83.1\n68.9\nGLEE-Lite\n1\nFull\n61.3\u00b10.5\n19.2\u00b13.1\n27.2\u00b13.4\n70.8\u00b13.3\n52.8\u00b115.1\n70.7\u00b17.5\n49.2\u00b122.0\n58.1\u00b15.4\n28.8\u00b111.0\n57.9\u00b110.0\n57.7\u00b10.6\n22.2\u00b17.9\n57.0\u00b14.5\n48.7\u00b10.9\nGLEE-Lite\n3\nFull\n62.6\u00b10.1\n25.5\u00b13.8\n29.1\u00b11.5\n72.9\u00b14.1\n65.8\u00b11.7\n83.0\u00b14.4\n66.8\u00b13.4\n61.7\u00b110.4\n40.0\u00b13.0\n61.2\u00b13.5\n44.9\u00b112.9\n26.7\u00b13.5\n64.5\u00b16.8\n54.2\u00b12.3\nGLEE-Lite\n5\nFull\n62.8\u00b10.4\n28.0\u00b13.1\n33.8\u00b12.2\n71.7\u00b12.7\n64.0\u00b14.4\n81.6\u00b14.1\n64.9\u00b15.2\n60.1\u00b112.4\n39.1\u00b11.0\n59.7\u00b13.0\n49.2\u00b114.5\n30.8\u00b11.3\n69.2\u00b17.8\n55.0\u00b13.7\nGLEE-Lite\n10\nFull\n62.1\u00b10.9\n32.0\u00b11.6\n39.3\u00b12.0\n71.2\u00b11.5\n64.4\u00b12.7\n88.0\u00b12.7\n64.3\u00b19.8\n65.5\u00b11.5\n36.4\u00b14.2\n62.1\u00b13.4\n54.8\u00b110.9\n38.8\u00b11.2\n70.6\u00b14.0\n57.7\u00b10.6\nGLEE-Lite\nAll\nFull\n62.8\n37.9\n52.9\n73.6\n76.5\n88.9\n69.7\n65.0\n51.1\n58.9\n67.4\n57.2\n82.3\n64.9\nGLEE-Plus\n1\nFull\n68.2\u00b12.2\n20.4\u00b10.2\n43.9\u00b14.1\n75.5\u00b11.6\n68.4\u00b12.7\n50.6\u00b129.0\n47.3\u00b10.8\n70.4\u00b14.0\n64.6\u00b10.5\n67.7\u00b11.5\n62.3\u00b11.0\n30.0\u00b19.2\n71.6\u00b17.7\n57.0\u00b10.8\nGLEE-Plus\n3\nFull\n70.6\u00b10.9\n24.8\u00b12.1\n47.6\u00b10.8\n79.5\u00b10.7\n69.0\u00b12.0\n83.1\u00b15.9\n66.2\u00b11.3\n75.6\u00b13.5\n65.3\u00b11.1\n69.0\u00b10.8\n65.7\u00b14.2\n38.1\u00b13.1\n76.3\u00b14.6\n63.9\u00b11.2\nGLEE-Plus\n5\nFull\n69.9\u00b10.9\n29.6\u00b12.9\n48.8\u00b11.2\n75.0\u00b11.7\n67.7\u00b15.1\n83.6\u00b19.9\n68.5\u00b13.2\n71.6\u00b15.9\n61.6\u00b14.0\n67.7\u00b10.8\n66.8\u00b14.5\n38.8\u00b11.9\n78.9\u00b11.0\n63.7\u00b11.0\nGLEE-Plus\n10\nFull\n69.3\u00b11.2\n32.5\u00b11.9\n50.8\u00b10.9\n76.4\u00b10.6\n70.7\u00b10.9\n88.2\u00b11.2\n68.9\u00b13.3\n68.2\u00b13.0\n60.0\u00b11.9\n69.3\u00b11.5\n62.6\u00b110.3\n41.7\u00b13.1\n81.7\u00b11.7\n64.6\u00b11.7\nGLEE-Plus\nAll\nFull\n70.4\n34.8\n54.1\n76.4\n74.5\n89.7\n68.6\n67.6\n57.8\n69.2\n71.4\n57.1\n82.9\n67.3\nGLEE-Pro\n1\nFull\n70.9\u00b11.7\n24.5\u00b12.3\n46.7\u00b10.4\n76.4\u00b10.8\n68.2\u00b13.8\n60.4\u00b17.8\n58.9\u00b12.7\n68.2\u00b14.5\n58.5\u00b18.8\n67.6\u00b10.8\n69.2\u00b10.2\n31.8\u00b12.6\n70.8\u00b17.6\n59.4\u00b11.5\nGLEE-Pro\n3\nFull\n72.3\u00b10.4\n28.4\u00b10.5\n49.6\u00b12.2\n76.1\u00b11.3\n69.3\u00b13.9\n79.4\u00b19.5\n67.4\u00b13.5\n74.1\u00b14.9\n63.7\u00b12.0\n68.4\u00b10.6\n68.3\u00b12.1\n42.1\u00b15.3\n76.9\u00b11.6\n64.3\u00b11.3\nGLEE-Pro\n5\nFull\n71.4\u00b10.9\n33.4\u00b11.5\n50.6\u00b14.3\n73.8\u00b13.9\n71.9\u00b10.3\n83.6\u00b16.8\n66.6\u00b11.8\n72.5\u00b14.3\n59.1\u00b14.8\n68.7\u00b11.4\n69.7\u00b11.3\n39.5\u00b14.8\n77.4\u00b13.2\n64.5\u00b10.9\nGLEE-Pro\n10\nFull\n71.1\u00b11.9\n37.8\u00b12.1\n54.2\u00b11.2\n73.9\u00b17.2\n70.7\u00b11.3\n90.9\u00b11.4\n66.0\u00b19.4\n73.9\u00b16.8\n57.8\u00b13.9\n69.4\u00b10.9\n62.9\u00b16.3\n44.3\u00b13.8\n79.8\u00b10.6\n65.6\u00b10.4\nGLEE-Pro\nAll\nFull\n72.6\n36.5\n58.1\n80.5\n74.1\n92.0\n67.0\n76.5\n66.4\n70.5\n66.4\n55.7\n80.6\n69.0\nTable 10. Per-dataset performance compared with DyHead, GLIP-T, and GLIP-L. For PascalVOC, we report the mAP (IoU=0.50:0.95)\nusing the COCO evaluation script, to be consistent with other 12 datasets. \u201cFull\u201d denotes full-model tuning.\n9. Interactive Segmentation and Tracking\nAs described in Sec 7, GLEE achieves interactive segmen-\ntation and tracking by introducing a visual prompt. Send-\ning points, boxes, or scribbles along with the image to\nthe model enables the segmentation of specified objects.\nMoreover, by feeding the mask from the previous frame\nand its corresponding prompt feature into early fusion and\nself-attention, GLEE performs segmentation in the current\nframe based on the segmentation results from the previous\nframe. The features of objects in the previous frame serve as\nreferring features at this point. As illustrated in the Figure 6,\nwe showcase the interactive segmentation results of differ-\nent prompts on images and videos. Please visit our project\nhomepage to experience more custom interactive image and\nvideo segmentation effects through our online demo.\n3\nVisual Prompt: Point\nVisual Prompt: Scribble\nVisual Prompt: Box\nTemporal\nFigure 6. The visualization results of interactive segmentation and tracking. For image-level interactive segmentation, GLEE supports\nsending points, boxes, or scribbles as a visual prompts to the model, enabling direct segmentation of the specified object. In the case\nof video object segmentation, using the masked feature from the first frame as a prompt referring features allows segmentation of the\ncorresponding object in subsequent frames of the video.\n4\n"
  },
  {
    "title": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent",
    "link": "https://arxiv.org/pdf/2312.08926.pdf",
    "upvote": "7",
    "text": ""
  },
  {
    "title": "UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2312.08754.pdf",
    "upvote": "6",
    "text": "UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation\nZexiang Liu1\u2217\nYangguang Li1\u2217\nYoutian Lin1\u2217\nXin Yu2\nSida Peng3\nYan-Pei Cao1\nXiaojuan Qi2\nXiaoshui Huang4\nDing Liang1\nWanli Ouyang4\n1VAST\n2The University of Hong Kong\n3Zhejiang University\n4Shanghai AI Laboratory\nhttps://www.UniDream.com/\nOurs (Albedo)\nDreamFusion\nMagic3D\nProlificDreamer\nMVDream\n\u201cDarth Vader helmet, highly detailed\u201d\n\u201cA DSLR photo of a frog wearing a sweater\u201d\n\u201cA zoomed out DSLR photo of a 3d model of an adorable cottage with a thatched roof\u201d\n\u201cA DSLR photo of a steam engine train, high resolution\u201d\n\u201cA DSLR photo of a tiger made out of yarn\u201c\nOurs (Relighting-I)\nOurs (Relighting-II)\nFigure 1. Comparison with baselines. UniDream presents clear albedo textures, completely smooth surfaces, and advanced relighting ca-\npabilities. The \u2018Albedo\u2019 column demonstrates the albedo and normal properties of the 3D objects generated using our method. Meanwhile,\nthe \u2018Relighting-I\u2019 and \u2018Relighting-II\u2019 columns demonstrate the effect of relighting on the generated PBR materials under white and purple\nlighting conditions, respectively.\nAbstract\nRecent advancements in text-to-3D generation technol-\nogy have significantly advanced the conversion of textual\n* Equal contributions\ndescriptions into imaginative well-geometrical and finely\ntextured 3D objects. Despite these developments, a preva-\nlent limitation arises from the use of RGB data in diffusion\nor reconstruction models, which often results in models with\ninherent lighting and shadows effects that detract from their\nrealism, thereby limiting their usability in applications that\ndemand accurate relighting capabilities. To bridge this gap,\n1\narXiv:2312.08754v1  [cs.CV]  14 Dec 2023\nwe present UniDream, a text-to-3D generation framework\nby incorporating unified diffusion priors.\nOur approach\nconsists of three main components: (1) a dual-phase train-\ning process to get albedo-normal aligned multi-view diffu-\nsion and reconstruction models, (2) a progressive genera-\ntion procedure for geometry and albedo-textures based on\nScore Distillation Sample (SDS) using the trained recon-\nstruction and diffusion models, and (3) an innovative appli-\ncation of SDS for finalizing PBR generation while keeping\na fixed albedo based on Stable Diffusion model. Extensive\nevaluations demonstrate that UniDream surpasses existing\nmethods in generating 3D objects with clearer albedo tex-\ntures, smoother surfaces, enhanced realism, and superior\nrelighting capabilities.\n1. Introduction\nThe creation of high-quality 3D content, characterized by\nintricate geometric and textural details, holds important ap-\nplications in various domains, including gaming, AR/VR,\nand artistic content creation. However, these applications\ngenerally require generated 3D objects to be relightable\nunder particular lighting conditions, which is essential for\ntheir realism. The current 3D models production methods\nthat can meet these application requirements mainly rely on\n3D artists, which brings a huge workload.\nRecent methods [4, 16, 21, 31, 39] have been exploring\nthe generation of 3D assets from textual descriptions under\nthe supervision of 2D diffusion model [33\u201335]. For exam-\nple, DreamFusion [31] defines a learnable Neural Radiance\nFields (NeRF) [22] and optimizes it based on the Score Dis-\ntillation Sampling (SDS). To enhance generation quality,\nsubsequent studies have diversified the pipeline, focusing\non aspects like 3D representations [16], loss functions [39],\n3D prior [44], and 2D diffusion models [37, 47]. Although\nthese methods achieve impressive results, they cannot gen-\nerate relightable objects, as they typically represent the un-\nderlying illumination and texture of an object as a holis-\ntic appearance, as shown in the first four columns in Fig.1,\nwhich results in inherent lighting and shadows baked into\nthe texture of the generated object. When relighting, the\ninherent highlights and shadows on these textured surfaces\ncan affect the realism of the object.\nIn this paper, we present UniDream, a novel framework\nthat allows generating relightable objects from textual de-\nscriptions. Fig.2 shows the fundamental difference between\nour method and other existing methods.\nOur key idea is training a diffusion model that can pro-\nvide both Physically-Based Rendering (PBR) material prior\nand multi-view geometry prior. Specifically, we first de-\nvelop an albedo-normal aligned multi-view diffusion model\n(AN-MVM) for consistent multi-view image generation,\nwhich is trained on a set of paired albedo and normal data\nRGB Images\nText\nText\nT2MAN\nAlbedo \nMaps\nNormal \nMaps\n3D\n3D\nSDS\n(b)\n(a)\nT2I Model\nT2AN-MVM\nTRM\nPBR \nDecomposition\nSDS\nRoughness\nMetallic\nFigure 2.\nComparison of UniDream with other methods.\n(a)\nThe existing RGB-based text-to-3D generation process;\n(b)\nUniDream\u2019s multi-stage generation process.\nrendered from 3D object datasets. Then, following the sim-\nplified Diesney BRDF model [2], we define a 3D represen-\ntation that includes albedo, normal, roughness, and metallic\nproperties, which are optimized based on the trained dif-\nfusion model and Stable Diffusion [34] model. Compared\nwith previous text-to-3D methods [16, 31, 37, 39], our ap-\nproach is able to disentangle the illumination and PBR ma-\nterial, achieving high-quality relightable objects under dif-\nferent ambient lighting conditions, as shown in last three\ncolumns of Fig. 1.\nTo robustly obtain 3D objects with PBR from 2D diffu-\nsion models, we have further developed a three-stage gen-\neration pipeline.\nInitially, we utilize the albedo-normal\naligned diffusion model (AN-MVM) to generate multi-\nview albedo and normal images. Subsequently, we adapt a\ntransformer-based reconstruction model (TRM) to convert\nmulti-view albedo images to a 3D coarse model, and per-\nform SDS refinement based on AN-MVM. Finally, we fix\nthe albedo and normal properties of the 3D model and op-\ntimize its roughness and metallic characteristics using the\nStable Diffusion [34] model. Experiments in Sec.4, demon-\nstrate the superior capability of our three-stage pipeline in\nproducing 3D objects with PBR property.\nOverall, we propose a novel text-to-3D generation\nframework that can stably generate high-quality 3D ob-\njects through a multi-stage generation strategy utilizing\nSDS refinement based on multi-view diffusion and recon-\nstruction models. Extensive experiments have demonstrated\nUniDream\u2019s superiority in three critical areas: (1) Real-\nistic Materials: By disentangling lighting from textures,\nUniDream accurately generates PBR materials that approx-\nimate real-world textures and can be relit in various lighting\nconditions, greatly enhancing realism. (2) Complete Geom-\netry: Incorporating normal supervision into our optimiza-\ntion process, UniDream excels at generating more compre-\nhensive geometric details than other existing methods, lead-\ning to more geometrically complete 3D objects. (3) Stable\nGeneration: Due to the introduction of 3D prior from re-\nconstruction model and normal supervision in SDS process,\nUniDream\u2019s effectiveness in generating 3D objects is ahead\nof other methods.\n2\n2. Related Works\n2.1. Text-to-3D Generation\nIn recent years, the field of text-to-3D content generation\nhas seen significant advancements, largely inspired by the\nadvancements in text-to-image generation. These advances\nhave particularly been driven by methods employing CLIP-\nbased guidance or score distillation. Methods like [12, 23,\n42] utilize the text-image relationship inherent in CLIP [32]\nto facilitate general text-to-3D content creation. Another\ninnovative approach, pioneered by DreamFusion [31], em-\nploys score distillation to enhance the robustness of text-\nto-3D generation, which also has been further developed\nand expanded in subsequent works [16, 39, 45]. The cen-\ntral to both of these paradigms is the use of pre-trained\ntext-to-image diffusion models as a foundation, enabling\nthe creation of diverse and imaginative 3D content. Fur-\nthermore, recent advancements have been developed by\nworks [18, 37, 47], which innovatively employ models de-\nrived from the Objaverse [6] 3D dataset. These models are\nused to render 2D RGB images, aiming to train text-to-\nmulti-view image diffusion models. This approach repre-\nsents a significant stride in optimization-based 3D model\ngeneration, as it enables the simultaneous generation of\nmultiple coherent images. Such a technique effectively ad-\ndresses the challenge of inconsistent directions in geometric\noptimization, thereby enhancing the consistency and quality\nof 3D model generation.\nIn a different paradigm, some methods have shifted to-\nwards training diffusion or reconstruction models directly\non paired text-3D data.\nThis strategy enables the cre-\nation of 3D models that inherently possess text-like seman-\ntics.\nA variety of techniques [5, 8, 11, 13, 27, 48] fall\nunder this category of 3D diffusion generation. In these\nmodels, textual information serves as a conditional input,\nguiding the generation process. This approach emphasizes\nthe manipulation of the underlying 3D data representation,\nensuring that the resulting models are both semantically\nrich and accurate representations of the text descriptions.\nMoreover, innovative strides have been taken in the realm\nof 3D reconstruction methods, particularly those grounded\nin transformer models, exemplified by LRM [10] and In-\nstant3D [15]. These methods introduce a novel perspective,\nenabling the generation of high-quality 3D models from text\nor images within seconds, courtesy of their efficient recon-\nstruction networks. The adoption of 3D diffusion and re-\nconstruction methodologies has gained prominence due to\ntheir impressive speed in generating 3D objects.\nThe 2D multi-view diffusion approach and the 3D recon-\nstruction technique utilizing 3D data have provided substan-\ntial inspiration. UniDream is constructed with integrating\nthe strengths of both these methods.\n2.2. Materials Generation\nEstimating surface materials proposes a fundamental chal-\nlenge in the field of computer vision. The Bidirectional\nReflection Distribution Function (BRDF), as the predomi-\nnant model, characterizes how the light is reflected off sur-\nfaces [28]. Early work focused on BRDF recovery concen-\ntrated on controlled lighting [1, 26], yet they were less effec-\ntive in real-world applications. However, recent advances\nin neural implicit methods [7, 38, 46] have demonstrated\npotential in accurately estimating lighting and BRDF from\nimage sets. These methods utilize neural 3D reconstruction\ntechniques to model complex lighting effects, simultane-\nously estimating shape, BRDF, and lighting. This achieves\na more comprehensive decomposition of these elements.\nNevertheless, the implicit representation of materials still\nposes limitations in their application. The recent advance-\nment in differentiable rendering methods [25] addresses this\nissue by incorporating an explicit surface mesh optimization\npipeline, allowing for the simultaneous estimation of BRDF\nand lighting.\nDrawing inspiration from recent material estimation\ntechniques, newer research has focused on generating sur-\nface materials for 3D objects. For instance, Fantasia3D [3]\ncombines a physical differential rendering pipeline with\nSDS to produce detailed 3D objects with realistic surface\nmaterials. However, this approach sometimes mixes albedo\nwith reflected light, resulting in blurred material property\ndistinctions. To address this, MATLABER [43] employs a\nlatent BRDF auto-encoder, trained on an extensive dataset,\nto more effectively differentiate these properties. Building\nupon this, our method initiates with fixed albedo and nor-\nmal generation and progressively incorporates other BRDF\nparameters, achieving a more natural and effective decom-\nposition of surface materials in 3D objects.\n3. Methodology\nOverview. As illustrated in Fig. 3, UniDream can be struc-\ntured into three stages and four modules. Firstly, upon re-\nceiving a text input, the pre-trained albedo-normal aligned\nmulti-view diffusion model generates four view consistent\nalbedo and normal maps (detailed in Section 3.1); secondly,\nthese albedo maps are then fed into a transformer-based re-\nconstruction model, which reconstructs a coarse 3D model\nto serve as an initial prior (as described in Section 3.2);\nthirdly, building on this preliminary coarse model, SDS op-\ntimization is employed, using the albedo-normal aligned\nmulti-view diffusion model to refine and produce a fine 3D\nobject with detailed mesh and albedo texture (explained\nin Section 3.3); finally, we fix the albedo and geometric\nshapes, and then use a Stable Diffusion [34] model to gen-\nerate the corresponding materials (outlined in Section 3.4).\n3\nViT-DINO-v2 Encoder with\nCamera Modulation\nCross-\nAttention\nSelf-\nAttention\nMLP\n\u2026\n\u2026\nX N\nLearnable Tokens Image-to-Triplane Decoder\nTriplane\nTriplane Tokens\n3D Prior\nAlbedo Domain\nNormal Domain\nMulti-View Self-Attention\nCross-Domain Self-Attention\nMulti-View Multi-\nDomain Self-\nAttention\nCross-Attention\nClass-Label,\nCamera, Time-Step\nDown/Up Sample\nResnet-Block\nMulti-view\nNoise X 2\nNormal\nAlbedo\nText: Darth Vader helmet, highly detailed\nCamera\nTRM\n3D Generation\nSDS\nPBR Generation\nAlbedo-Normal Aligned Multi-View Diffusion\nCamera\nNormal\nAlbedo\nRoughness\nMetallic\nText\nDecomposition\nStable Diffusion\nModel\nAN-MVM\nFigure 3. Overview of UniDream. Left: the multi-view diffusion model generates multi-view images based on input text. Middle: first,\nfour view albedo maps obtain 3D prior by the reconstruction model, and then the multi-view diffusion model performs SDS optimization\nbased on the 3D prior to generate a 3D object with albedo texture. Right: using Stable Diffusion model to generate PBR material.\n3.1. Albedo-Normal Aligned Multi-view Consistent\nDiffusion Model\nOur approach represents a departure from traditional meth-\nods that utilize RGB data to train diffusion models. We\ntrain an albedo-normal aligned text-to-multi-view diffusion\nmodel (AN-MVM) using albedo and normal maps rendered\nfrom 3D data. Based on the Stable Diffusion [34] model\nframework, we perform multi-view and multi-domain dif-\nfusion modeling in the UNet module to establish multi-\nview consistency and multi-domain consistency. Specifi-\ncally, within the UNet module design, we address three crit-\nical aspects: ensuring multi-view consistency, aligning the\nalbedo with normal domains, and maintaining the semantic\nintegrity of information from text to generated images.\nMulti-view Self-Attention. To ensure robust generaliza-\ntion, our AN-MVM model expands upon the capabilities\nof the pre-trained Stable Diffusion [34] model by adapt-\ning it for multi-view training.\nThis process, inspired by\nMVDream [37], initiates with the randomly selected four\northogonal views x \u2208 RN\u00d7H\u00d7W \u00d7Cfrom the rendered\nmulti-view dataset. We then encode the camera parame-\nters c \u2208 RN\u00d712 of these views using a two-layer MLP net-\nwork. This procedure generates feature vectors Fc with the\nsame dimensions as time-step. These camera features are\nthen added to the time-step features Ft, facilitating effec-\ntive modulation of variance across different views.\nIn the architecture of our UNet module, we consolidate\nmulti-view data within an additional dimension and perform\nself-attention mechanism between multiple views just be-\nfore the cross-attention layer. This strategic design enables\nmutual constraints among the various multi-view inputs, ef-\nfectively reinforcing consistency across multiple views dur-\ning the diffusion process.\nMulti-Domain Self-Attention. Based on multi-view con-\nsistency and sharing a similar perspective to recent work\nWonder3D [19], we further introduce multi-domain consis-\ntency. Specifically, we introduce a distinct class label L for\nthe normal domain, and use a two-layer multi-layer percep-\ntron (MLP) to encode this class label L to obtain feature Fl\nwith the same dimension with the time-step features Ft, and\nFl is added to Ft to control the generation process within the\nnormal domain. Subsequently, we apply the self-attention\nmechanism to the corresponding views between the albedo\nand normal domains to ensure domain consistency.\nIt is important to highlight that achieving multi-view\nconsistency in normal maps is notably straightforward, pri-\nmarily due to the simplicity of their semantic content and\nthe consistency of values at identical positions across var-\nious views in the world coordinate system. This inherent\nconsistency in normal maps significantly facilitates the con-\nvergence process. Furthermore, the constraints we apply\nbetween the albedo and normal maps contribute to a quickly\nconvergence in controlling multi-view albedo, streamlining\nthe overall generation process.\nText and Image Semantic Alignment. In order to solve the\nproblem of potential semantic generalization loss caused by\nonly using less 3D synthetic data during AN-MVM training,\n4\nwe use joint training to combine the 2D LAION-Aesthetics\ndata with the 3D data we collected. Specifically, in line with\nthe approach used by MVDream [37] during our training\nprocess, each batch is randomly composed by 3D data or\n2D data based on a probability distribution: there\u2019s a 70%\nchance of using 3D data and a 30% chance of incorporating\n2D LAION-Aesthetics data. In this setting, while the nor-\nmal domain is distinguished by class label, the albedo and\n2D LAION-Aesthetics data are not. Moreover, to further\ndifferentiate between 3D and 2D data, we add the phrase \u201d,\n3D asset\u201d to the captions of 3D data. This strategic inclu-\nsion of a significant proportion of 2D data plays a crucial\nrole in enhancing the semantic alignment between the gen-\nerated image content and the corresponding input text.\n3.2. Transformer-Based Reconstruction Model\nInspired by LRM [10] and Instant3D [15], we have inte-\ngrated reconstruction models into our text-to-3D generation\npipeline, aiming to provide an initial 3D prior for enhanc-\ning text-to-3D generation performance. As illustrated in the\nTRM module of Fig.3, for each object in the AN-MVM\ntraining dataset, we randomly select four views Ii, identi-\ncal elevation but orthogonal views. Along with these views,\nthe corresponding camera parameters, Ci, are used as in-\nputs to the model. In this framework, the pre-trained ViT-\nbased DINO-v2 [29] model, denoted as F is employed to\nextract image features, Fi, from the albedo images of the\nfour selected views. Concurrently, a learnable camera mod-\nulation module processes the camera parameters for each\nview using MLPmod(Ci) and seamlessly integrates these en-\ncoded parameters into the image features. Then we em-\nploy learnable tokens, denoted as Th, as input to the trans-\nformer model. These tokens are designed to undergo cross-\nattention with the image features, allowing for an effective\nfusion of the input image information. Subsequently, this\nis followed by the integration of cross-attention and self-\nattention modules and a multi-layer perceptron (MLP) to\nform a transformer block. Multiple such transformer blocks\nwork in sequence to decode the input tokens Th into refined\ntriplane representations T \u2032\nh. The representations correspond\nto the semantic information of the input image. Finally,\nwe decode these triplane representations using a MLP de-\ncoder of NeRF to reconstruct the final 3D model, denoted\nas Gen3D. This entire process is detailed in Eq.1. M is the\nnumber of transformer layers.\nFi = MLPmod(Ci) \u2297 F(Ii)\nT \u2032\nh = MLP(SelfAttn(CrossAttn(Th, Fi))) \u00d7 M\nGen3D \u21d0 MLP(T \u2032\nh)\n(1)\nOur approach differs from the configurations used in In-\nstant3D in several key aspects to better adapt our model\u2019s\nrequirements. Firstly, we remove the intrinsic camera pa-\nrameters of the input multi-view images and only normalize\nand encode the extrinsic parameters to adapt the multi-view\nimages output by AN-MVM. Furthermore, we use albedo\ninstead of RGB for training to prevent the impact of light-\ning and shadows in RGB images on the triplane-NeRF re-\nconstruction results. Additionally, in order to increase the\nresolution of reconstructed model and save training cost, we\nresize the reference views between 128\u00d7128 and 256\u00d7256\nresolution, and randomly crop 128 \u00d7 128 images from the\nresized views to supervise the aligned region of rendered\nimages.\nFinally, in the supervision, we not only render\nalbedo for supervision but also incorporate normal super-\nvision to significantly accelerate the model\u2019s convergence\nspeed and promote the geometry details. These strategic\nenhancements enable our TRM to deliver superior recon-\nstruction results while reducing the training cost.\n3.3. Score Distillation Sample (SDS) Refinement\nAfter acquiring the triplane-NeRF representation of the 3D\nmodel from TRM, we further refine it using our AN-MVM\nin conjunction with score distillation sample (SDS) to ob-\ntain higher quality 3D results.\nWhen given a text input, a cascaded inference using AN-\nMVM and TRM produces a 3D coarse model represented\nby a triplane-NeRF x = g(\u03b8), where \u03b8 is MLP network of\nNeRF, g(\u00b7) is the renderer, and x is the generated view at\na given camera pose. Subsequently, we employ AN-MVM\nwith SDS to refine the 3D coarse model. In details, the\nalbedo and normal maps of four orthogonal views xanmv\nare rendered from the coarse model each iteration.\nAf-\nter adding noise \u03f5anmv, the frozen albedo-normal aligned\nmulti-view diffusion model \u03d5AN\u2212MV M is used to predict\nthe noise \u02c6\u03f5\u03d5AN-MVM(xanmv,t; y, t) for all views across both do-\nmains simultaneously, where t is the time-step representing\nnoisy level, y is the text condition, and xanmv,t is the noised\nimage. Subtracting the predicted noise, \u02c6\u03f5 \u2212 \u03f5, offers a sig-\nnal for aligning the rendered view xanmv with the text in-\nput y, as perceived by the AN-MVM. UniDream updates\nthe NeRF\u2019s MLP parameters by backpropagating the gradi-\nent through the rendering process using Score Distillation\nSampling (SDS), as depicted in Eq. 2.\n\u2207\u03b8LSDS(\u03d5AN-MVM, g(\u03b8)) =\nEt,\u03f5\n\u0014\nw(t)(\u02c6\u03f5\u03d5AN-MVM(xanmv,t; y, t) \u2212 \u03f5anmv)\u2202x\n\u2202\u03b8\n\u0015\n(2)\nHere, w(t) is a weighting function that depends on the\ntimestep t. When calculating the final loss, we use weights\nof 0.8 and 0.2 for the two domains of albedo and normal\nrespectively for weighted summation to ensure that fine ge-\nometry can be quickly optimized without neglecting the op-\ntimization of texture.\nIn order to get a better mesh, we adopt a strategy simi-\nlar to Magic3D [16], incorporating DMTet [36] refinement\nfrom NeRF representation to enhance mesh quality.\n5\nAlbedo\nNormal\nPBR\n\u201ca pig wearing a back pack\u201d\n\u201ca statue of angel blender\u201d\n\"Daenerys Targaryen from game of throne, full body, blender 3d, artstation\nand behance, Disney Pixar, Mobile game character, clash royale, cute\"\n\u201ca DSLR photo of a terracotta bunny\u201d\n\u201cViking axe fantasy weapon blender 8k HD\u201d\n\u201ca DSLR photo of a football helmet\u201d\n\u201ca DSLR photo of a group of dogs playing poker\u201d\n\u201ca DSLR photo of an overstuffed pastrami sandwich\u201d\n\u201ca DSLR photo of a humanoid robot using a laptop\u201d\n\u201ca DSLR photo of edible typewriter made out of vegetables\u201d\n\u201ca DSLR photo of a kitten standing on top of a giant tortoise\u201d\n\u201ca DSLR photo of a lion reading the newspaper\u201d\n\u201ca wide angle zoomed out DSLR photo of \na skiing penguin wearing a puffy jacket\u201d\n\u201ca DSLR photo of a plush triceratops toy studio lighting \nhigh resolution\u201d\n\u201ca zoomed out DSLR photo of a red rotary telephone\u201d\nAlbedo\nNormal\nAlbedo\nNormal\nPBR\nPBR\nFigure 4. Illustrative overview of our method\u2019s capabilities. We demonstrate the performance of 3D objects generated by our method in\nthree dimensions: albedo, PBR, and normal.\n3.4. Physically-Based Rendering (PBR) Material\nGeneration\nBased on the geometry and albedo from the DMTet refine-\nment, we employ the Stable Diffusion [34] model to gen-\nerate the PBR material. We adopt the PBR material and\nillumination representations in Nvdiffrec [24], which is rec-\nognized for its speed and efficiency.\nFor any 3D point, we predict its BRDF parameters, in-\ncluding the diffuse kd, roughness kr, and metalness km.\nIn our approach, following the DMTet refinement, where\na hash grid and a multi-layer perceptron (MLP) are used to\npredict kd, we introduce an additional hash grid and MLP\nto predict the roughness and metalness parameters, kr and\nkm, respectively. In accordance with Nvdiffrec [24], we\ncalculate the final rendering using:\nL = kd(1 \u2212 km)Ld(\u03c9o, n) + ksLs(kr, \u03c9o, n)\n(3)\nwhere kd(1 \u2212 km)Ld corresponds to the diffuse shading,\nwhile ksLs means the specular shading. The terms Ld and\nLs in the equation represent the diffuse and specular light\ncomponents. Please refer to [14] for more details.\nDue to the high quality performance of the previous\nstage, we fixed the albedo and normals. The model parame-\nters are optimized based on the SDS loss. To better accom-\nmodate the supervision from the Stable Diffusion model,\nwe allow the ambient light to be optimized alongside the\nBRDF parameters, which is different from Fantasia3D [3]\nthat fixes the lighting and MATLABER [43] that uses a set\nof ambient light during training. To avoid potential color in-\nterference that might arise from the Stable Diffusion model,\nwe constrain the ambient light optimization to a single chan-\nnel. This channel only represents the magnitude of the light-\ning, effectively circumventing the introduction of any mis-\nleading color information.\n4. Experiments\n4.1. Implementation Details\nTraining Dataset Setup. Like most of previous work [17,\n18, 37], we employed the public available 3D dataset Ob-\n6\nTable 1. Quantitatively compare UniDream with text-to-3D base-\nline methods by CLIP Score, CLIP R-Precision and user study.\nMethods\nUser study\nCLIP\nCLIP R-Precision (%) \u2191\n(%) \u2191\nScore (\u2191)\nR@1\nR@5\nR@10\nDreamFusion [31]\n7.1\n71.0\n54.2\n82.2\n91.5\nMagic3D [16]\n10.5\n75.1\n75.9\n93.5\n96.6\nMVDream [37]\n32.1\n75.7\n76.8\n94.3\n96.9\nOurs\n50.3\n77.9\n80.3\n97.4\n98.5\njaverse [6] for training. To enhance data quality, we imple-\nment a series of filtering rules as follows: no texture map;\nnot a single object; accounting for less than 10% of the\npicture; low quality; no caption information is provided in\nCap3D [20]. After filtering, approximately 300K object re-\nmain, and then we follow the MVDream [37] method to\nrender multi-view albedo and normal data.\nMulti-View Diffusion Model Training Details. We fol-\nlow Tune-A-Video [41] to implement our multi-view dif-\nfusion model. During training, we use 32 A800 GPUs with\n256\u00d7256 image resolution and a per-GPU image batch size\nof 128 (16 objects \u00d7 2 domains \u00d7 4 views) to train 50k iter-\nations, which takes about 19 hours. In addition, the learning\nrate used is 1 \u00d7 10\u22124, and 10 times the learning rate is used\nfor camera encoder\u2019s parameters.\n3D Reconstruction Model Training Details. We use ran-\ndom four views of 256\u00d7256 images as input and produce\n1, 300 \u00d7 768 image features. The learnable tokens are a\nsequence of (3\u00d732\u00d732)\u00d7512. The image-to-triplane de-\ncoder are of 10 layers transformer with hidden dimentions\n512. We train the reconstruction model on 32 A800 GPUs\nwith batch size 96 for 70,000 steps, taking about 3 days to\ncomplete. We set the coefficient \u03bb = 2.0 for Llpips and use\nthe AdamW optimizer with a weight decay of 0.05 to train\nour model. And we use a peak learning rate of 4 \u00d7 10\u22124\nwith a linear 3K steps warm-up and a cosine scheduler.\nScore Distillation Sample(SDS) Refining Details. We im-\nplement the refinement stages of NeRF and DMTet based on\nthe Threestudio2. Specifically, we render four-view albedo\nand normal maps at the same time for SDS training. In\nthe NeRF and DMTet refinement stages, we train 5,000\nand 2,000 iterations respectively. During training, the loss\nweights of the albedo and normal domains are 0.8 and 0.2,\nand we use an \u2018unsharp\u2019 operation [40] is used in the last\n500 iterations of each stage.\nPBR Material Generation Details.\nIn the PBR mate-\nrial generation stage, the texture hash grid, derived from\nDMTet refinement, is repurposed and duplicated for the iso-\n2https://github.com/threestudio-project/threestudio\n\u201carmy Jacket, 3D scan\u201d\n\u201cA product photo of a toy tank\u201d\n\u201d pig wearing a back pack\u201d\n\u201csaber from fate stay night, 3D, girl, anime\u201d\nMVDream\nRGB\nUniDream\nAlbedo\nUniDream\nNormal\nMVDream\nRGB\nUniDream\nAlbedo\nUniDream\nNormal\nFigure 5. Comparison of multi-view results generated by MV-\nDream and UniDream.\nlated learning of the parameters {kr, km}, while maintain-\ning a fixed parameter for the albedo texture. A new MLP\nis initialized for {kr, km} learning. The value ranges of\nthese parameters are constrained within [0.0, 0.9] for kr and\n[0.08, 0.9] for km to prevent erroneous PBR properties. We\nset the learning rate for the hash grid to 1\u00d710\u22124, while that\nof MLP is 0.1. Additionally, the learning rate for the envi-\nronment map is also set to 0.01, coupled with total varia-\ntion regularization. The environment map commences from\nan initialized studio lighting High-Dynamic-Range Imaging\n(HDRI) map. Image rendering resolution is 512\u00d7512, and\nthe model is trained for 2,000 iterations.\n4.2. Qualitative Comparisons\nWe present representative results of UniDream in Fig.4,\nshowcasing the albedo, PBR, and normal maps of the\ngenerated 3D objects. The text-to-3D objects created by\nUniDream, exhibit more complete and smoother geomet-\nric surfaces, clearer albedo in texture color distribution, and\nmore realistic lighting effects. These features represent sig-\nnificant advancements over many previous methods.\nIn Fig.1, we compare the results generated by Dream-\nFusion [31], Magic3D [16], ProlificDreamer [39], MV-\nDream [37], and UniDream. We utilized the results from\nDreamFusion\u2019s official website for its first four cases. For\nthe other cases, including those from Magic3D and Prolific-\nDreamer, we employed Threestudio\u2019s implementation to ac-\nquire the results. The comparison reveals that UniDream\nproduces semantically clearer 3D geometries and does\nnot exhibit the \u2019Janus problem\u2019. In comparison with the\nmethods, especially MVDream [37], illustrated in Fig.1,\nUniDream demonstrates a more complete and smoother ge-\nometric surface, attributed to the implementation of normal\nsupervision. Moreover, an analysis of the last three columns\n7\n\u201carmy Jacket, 3D scan\u201d\n\u201cA product photo of a toy tank\u201d\n\u201d pig wearing a back pack\u201d\n\u201csaber from fate stay night, 3D, girl, anime\u201d\nAlbedo\nNormal\nFigure 6. 3D reconstruction performance of our UniDream.\nin Fig.1 reveals that UniDream possesses unique capabili-\nties not typically found in existing methods. These include\nthe ability to disentangle lighting and texture, exhibit re-\nlighting effects under various lighting conditions, and en-\nhance the realism of the generated 3D objects.\n4.3. Quantitative Evaluations\nWe conducted a quantitative evaluation of text-to-3D\ngeneration quality using CLIP Score [9, 32] and CLIP\nR-Precision [30] following methodologies from Dream\nFileds [12], DreamFusion [31], and Cap3D [20]. Specifi-\ncally, we generated 3D objects using 68 different prompts\nsourced from the DreamFusion and MVDream websites,\nemploying DreamFusion, Magic3D, MVDream, and our\nUniDream. For the evaluation, four views (front, back, left,\nright) of each generated 3D object were rendered. We ex-\ntracted text and image features using the CLIP ViT-B/32\nmodel [32] and calculated the CLIP score by averaging the\nsimilarity between each view and the corresponding text\nprompt. The detailed results, presented in Tab.1, demon-\nstrate that UniDream significantly surpasses DreamFusion,\nMagic3D, and MVDream in terms of CLIP Score and CLIP\nR-Precision. This indicates that UniDream is more effec-\ntive at producing 3D results that are consistent with the text\nprompts.\nAdditionally, we performed a user study evaluating 68\nresults generated by each method, focusing on geometric\ntexture quality and realism to discern the visual quality dif-\nferences among the methods.\nInvolving 22 participants,\nUniDream was distinguished as the preferred choice, secur-\ning 50.3% of the votes. This outcome highlights the supe-\nrior overall quality of our approach.\n5. Ablation and Analysis\nComparison of Multi-view Diffusion Models. Generat-\ning multi-view images is a fundamental aspect of 3D con-\ntent creation. To evaluate this, we compared the multi-view\nresults produced by UniDream\u2019s AN-MVM with those of\nMVDream\u2019s multi-view diffusion model. For MVDream,\nwe utilized the \u2018sd-v2.1-base-4view\u2019 model based on its\nopen-source inference code3. In addition, the same nega-\ntive prompt used in MVDream\u2019s SDS optimization was ap-\nplied for the multi-view inference in both MVDream and\nUniDream. As depicted in Fig.5, UniDream successfully\nmaintains light and texture disentangling in its 2D output\nand produces normal maps with impressive consistency.\nMulti-view Reconstruction Visualization. By employing\nmultiple views (as depicted in Fig.5) as input, TRM recon-\nstructs the corresponding 3D model, subsequently render-\ning albedo and normal maps from various views, as shown\nin Fig.6. The results demonstrate that our TRM successfully\nreconstructs 3D models closely resembling the input images\nin terms of geometry and texture semantics. This provides\na robust prior for SDS refinement, thereby enhancing the\nstability and convergence speed of SDS-based training.\nComparison of Generated PBR Materials. As shown in\nFig.7, we present the results of PBR material generation in\ncomparison with Fantasia3D [4]. It is evident that Fanta-\nsia3D struggles to disentangle lighting and textures, often\nresulting in lighting and shadows being baked into the ap-\npearance of 3D objects. Conversely, our method effectively\ndisentangle lighting and textures, enabling the generation of\nrealistic, relightable 3D objects.\nEnvironment Maps.\nFig.8 shows the different environ-\nment maps used by UniDream. Fig.8(a) is the environment\nmap used to Fig.7 and the visualize of overall capabilities of\nUniDream. Fig.8(b) and Fig.8(c) are the environment maps\nused by \u2019Relighting-I\u2019 and \u2019Relighting-II\u2019 in the teaser of\nUniDream respectively.\n6. Conclusion\nIn this paper, we propose for the first time a relightable text-\nto-3D generation paradigm, UniDream, which is based on\nan albedo and normal aligned multi-view diffusion model.\nThanks to the disentangling of lighting and textures, the 3D\nmodels generated by our method can be relit, thereby en-\nhancing their realism and usability. We provide a detailed\ndiscussion and analysis of each module in UniDream, and\nextensive results underscore the superiority of our approach.\nLimitations and future work. While UniDream demon-\nstrates clear advantages from multiple perspectives, it also\nhas certain limitations due to being trained on only approx-\nimately 300k Objaverse [6] data. Primarily, there may be\nconstraints in semantic generalization, leading to potential\n3https://github.com/bytedance/MVDream\n8\nchallenges with complex combinational concepts.\nAddi-\ntionally, issues in material generalization could arise, such\nas in accurately simulating materials with transparent prop-\nerties. Our subsequent work will primarily concentrate on\nenhancing the generalization of the pipeline.\nMoreover,\nthere is a critical demand to upgrade our rendering pipeline,\naiming to boost the realism and visual fidelity of the gener-\nated 3D models. By incorporating path tracing, renowned\nfor its realistic simulation of lighting and shadow effects, we\nanticipate a substantial improvement in rendering quality.\nNormal \nAlbedo\nPBR\nRoughness\nMetallic\nAlbedo\nPBR\nRoughness\nMetallic\nFantasia3D-PBR\nUniDream-PBR\nInput\n\u201cDarth Vader helmet, highly detailed\u201d\n\u201cA DSLR photo of a frog wearing a sweater\u201d\n\u201cA zoomed out DSLR photo of a 3d model of an adorable cottage with a thatched roof\u201d\n\u201cA DSLR photo of a steam engine train, high resolution\u201d\n\u201cA DSLR photo of a tiger made out of yarn\u201c\nFigure 7. Results comparison of generated PBR materials. Left: the same geometry input; Middle: the PBR material components generated\nby Fantasia3D; Right: the PBR material components generated by UniDream.\n(a)\n(b)\n(c)\nFigure 8. Different environment maps used by UniDream.\nReferences\n[1] Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Practical\nsvbrdf capture in the frequency domain. ACM Trans. Graph.,\n32(4):110\u20131, 2013. 3\n[2] Brent\nBurley\nand\nWalt\nDisney\nAnimation\nStudios.\nPhysically-based shading at disney.\nIn Acm Siggraph,\npages 1\u20137. vol. 2012, 2012. 2\n[3] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 3, 6\n[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 8\n[5] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4456\u20134465, 2023. 3\n[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi.\nObja-\nverse: A universe of annotated 3d objects. arXiv preprint\narXiv:2212.08051, 2022. 3, 7, 8\n9\n[7] Duan Gao, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, and\nXin Tong. Deep inverse rendering for high-resolution svbrdf\nestimation from an arbitrary number of images. ACM Trans.\nGraph., 38(4):134\u20131, 2019. 3\n[8] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 3\n[9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning, 2022. 8\n[10] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d. arXiv preprint arXiv:2311.04400, 2023. 3, 5\n[11] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. In SIG-\nGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 3\n[12] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 3, 8\n[13] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 3\n[14] Brian Karis and Epic Games. Real shading in unreal engine\n4. Proc. Physically Based Shading Theory Practice, 4(3):1,\n2013. 6\n[15] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model.\narXiv preprint arXiv:2311.06214, 2023. 3, 5\n[16] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3, 5, 7\n[17] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 6\n[18] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n3, 6\n[19] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 4\n[20] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models, 2023. 7,\n8\n[21] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[23] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 3\n[24] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting Triangular 3D Models, Materials, and Light-\ning From Images. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 8280\u20138290, 2022. 6\n[25] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 3\n[26] Giljoo Nam, Joo Ho Lee, Diego Gutierrez, and Min H Kim.\nPractical svbrdf acquisition of 3d objects with unstructured\nflash photography. ACM Transactions on Graphics (TOG),\n37(6):1\u201312, 2018. 3\n[27] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3\n[28] Fred E Nicodemus. Directional reflectance and emissivity of\nan opaque surface. Applied optics, 4(7):767\u2013775, 1965. 3\n[29] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 5\n[30] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,\nand Anna Rohrbach. Benchmark for compositional text-to-\nimage synthesis. In Thirty-fifth Conference on Neural Infor-\nmation Processing Systems Datasets and Benchmarks Track\n(Round 1), 2021. 8\n[31] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3, 7, 8\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 8\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\n10\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 2, 3, 4, 6\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[36] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 5\n[37] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 2, 3, 4, 5, 6, 7\n[38] Pratul P Srinivasan,\nBoyang Deng,\nXiuming Zhang,\nMatthew Tancik, Ben Mildenhall, and Jonathan T Barron.\nNerv: Neural reflectance and visibility fields for relighting\nand view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7495\u20137504, 2021. 3\n[39] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 7\n[40] Trygve Wiig.\nBlur latent noise.\nhttps://gist.github.com/\ntrygvebw/e51573d40841d22c11fc32df6863ef58, 2023. 7\n[41] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 7\n[42] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20908\u201320918, 2023. 3\n[43] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. Mat-\nlaber: Material-aware text-to-3d via latent brdf auto-encoder.\narXiv preprint arXiv:2308.09278, 2023. 3, 6\n[44] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin\nWang, and Fan Wang. Points-to-3d: Bridging the gap be-\ntween sparse points and shape-controllable text-to-3d gener-\nation. In Proceedings of the 31st ACM International Confer-\nence on Multimedia, pages 6841\u20136850, 2023. 2\n[45] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-\nHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score\ndistillation. arXiv preprint arXiv:2310.19415, 2023. 3\n[46] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Ner-\nfactor: Neural factorization of shape and reflectance under\nan unknown illumination. ACM Transactions on Graphics\n(ToG), 40(6):1\u201318, 2021. 3\n[47] Minda Zhao,\nChaoyi Zhao,\nXinyue Liang,\nLincheng\nLi, Zeng Zhao, Zhipeng Hu, Changjie Fan, and Xin\nYu.\nEfficientdreamer:\nHigh-fidelity and robust 3d cre-\nation via orthogonal-view diffusion prior.\narXiv preprint\narXiv:2308.13223, 2023. 2, 3\n[48] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum. Locally attentional sdf\ndiffusion for controllable 3d shape generation. arXiv preprint\narXiv:2305.04461, 2023. 3\n11\n"
  },
  {
    "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
    "link": "https://arxiv.org/pdf/2312.09251.pdf",
    "upvote": "6",
    "text": "VL-GPT: A Generative Pre-trained Transformer\nfor Vision and Language Understanding and Generation\nJinguo Zhu1*\nXiaohan Ding2*\nYixiao Ge2,3\nYuying Ge2\nSijie Zhao2\nHengshuang Zhao4\nXiaohua Wang1\nYing Shan2,3B\n1 Xi\u2019an Jiaotong University\n2 Tencent AI Lab\n3 ARC Lab, Tencent PCG\n4 The University of Hong Kong\nAbstract\nIn this work, we introduce Vision-Language Generative\nPre-trained Transformer (VL-GPT), a transformer model\nproficient at concurrently perceiving and generating visual\nand linguistic data. VL-GPT achieves a unified pre-training\napproach for both image and text modalities by employing a\nstraightforward auto-regressive objective, thereby enabling\nthe model to process image and text as seamlessly as a lan-\nguage model processes text. To accomplish this, we initially\npropose a novel image tokenizer-detokenizer framework for\nvisual data, specifically designed to transform raw images\ninto a sequence of continuous embeddings and reconstruct\nthem accordingly. In combination with the existing text tok-\nenizer and detokenizer, this framework allows for the en-\ncoding of interleaved image-text data into a multimodal\nsequence, which can subsequently be fed into the trans-\nformer model. Consequently, VL-GPT can perform large-\nscale pre-training on multimodal corpora utilizing a uni-\nfied auto-regressive objective (i.e., next-token prediction).\nUpon completion of pre-training, VL-GPT exhibits remark-\nable zero-shot and few-shot performance across a diverse\nrange of vision and language understanding and genera-\ntion tasks, including image captioning, visual question an-\nswering, text-to-image generation, and more. Additionally,\nthe pre-trained model retrains in-context learning capabil-\nities when provided with multimodal prompts. We further\nconduct instruction tuning on our VL-GPT, highlighting its\nexceptional potential for multimodal assistance.\n1. Introduction\nDriven by the remarkable success of large language mod-\nels (LLMs) in the field of natural language processing\n*Equal contribution. This work is done when Jinguo Zhu is an intern\nat Tencent AI Lab. The source code and model weights shall be released\nat https://github.com/AILab-CVC/VL-GPT. BCorresponding\nauthor.\n(NLP) [40, 41, 54], there has been a surge of interest within\nmultimodal community to develop large vision-language\n(VL) models. One of the promising approaches, exempli-\nfied by Flamingo [1], BLIP2 [24], LLAVA [25], have ex-\nplored how to build large VL models based on powerful\npre-trained LLMs. These studies typically adopted a similar\narchitecture: a pre-trained image encoder and an LLM are\nconnected via a trainable connection module, which aligns\nthe image feature and text embeddings, thereby enabling\nlanguage models to accept images and text as inputs and\ngenerate a text sequence.\nTo expand the capabilities of generating image in a mul-\ntimodal context, certain efforts, e.g., Visual ChatGPT [47],\nattempt to connect LLMs with image generation tools in a\ncascaded pipeline by transferring text messages, which in-\nevitably introduce instability and noise. Alternatively, an-\nother line of research achieves it by optimizing models in\nan end-to-end manner [9, 18, 23, 30, 48]. By aligning the\noutput space with the image diffusion models, VL models\ncan not only perceive but also generate images and text.\nA crucial characteristic of large language models is auto-\nregressive modeling [31], i.e., predicting next token, which\nfacilitates language understanding and generation in a uni-\nfied manner. However, in the aforementioned studies, the\ninconsistency of image embeddings between LLM\u2019s input\nand output sides compels the model to treat input images\nand generated images differently, resulting in separate mod-\neling for image understanding and generation. Meanwhile,\nthis discrepancy also obstructs the implementation of auto-\nregressive training loss on image embeddings.\nIn this study, we introduce VL-GPT, a large vision-\nlanguage generative pre-trained transformer that enables the\nunified training of both visual and linguistic data using an\nauto-regressive objective, as depicted in Fig. 1. To achieve\nthis, we propose an image tokenizer-detokenizer framework\nfor the conversion between raw image pixels and contin-\nuous visual embeddings, analogous to the role of the text\ntokenization [19, 43] in language models. The framework\n1\narXiv:2312.09251v1  [cs.CV]  14 Dec 2023\nImage Tokenizer\nImage Detokenizer\nImage \nTokenizer\nText \nTokenizer\nLarge \nVision-Language \nTransformer\nModel\nImage \nDetokenizer\nText \nDetokenizer\nVL-GPT\nInterleaved\nImage-text\n Input\nInterleaved\nImage-text \nGeneration\nMultimodal  \nSequence \nMultimodal  \nSequence \nVisual  \nEmbeddings\nCausal \nTransformer\nTransformer\nDecoder\nDiffusion Decoder\nVisual Encoder\nFigure 1. Overview of our proposed approach. The upper part delineates the image tokenizer-detokenizer framework, designed for encoding\nimages into continuous visual embeddings and reconstructing them in the pixel space. The lower part demonstrates the implementation\nof our VL-GPT, where interleaved image-text data are encoded into multimodal sequence using image and text tokenizers, subsequently\nprocessed by a transformer model auto-regressively. The image and text detokenizers are employed for generating respective outputs.\ncomprises an image tokenizer and an image detokenizer,\nwhere the tokenizer encodes raw images into a sequence of\ncontinuous visual embeddings, and the detokenizer decodes\nthe continuous embeddings into pixel space. To obtain vi-\nsual continuous embeddings that are rich in both image de-\ntails and semantic information, we employ the image em-\nbeddings and their corresponding caption embeddings ex-\ntracted by pre-trained encoders (i.e., CLIP [32]) as the su-\npervision for training of the framework. Furthermore, the\nefficiency of the framework training is enhanced through\nweight initialization from pre-trained image encoders and\nhigh-quality image diffusion models.\nBy employing the image tokenizer-detokenizer frame-\nwork, visual embeddings can achieve consistency on both\nthe input and output sides of the transformer model. Con-\nsequently, interleaved image-text data can be trained in\na unified auto-regressive manner. Specifically, the image\ntokenizer and the existing text tokenizer (i.e., BPE tok-\nenizer [43]) first convert the image and text into a multi-\nmodal sequence consisting of interleaved continuous visual\nembeddings and discrete text tokens. The transformer can\nthen be trained to predict the next embedding or token in\nthis multimodal sequence, employing mean squared error\n(MSE) loss for continuous visual embeddings and cross-\nentropy loss for discrete text tokens. Contrary to previous\nworks [9, 18, 30, 48], all embeddings in the multimodal\nsequence can receive supervision from the auto-regressive\nloss. During the generation stage, visual embeddings and\ntext tokens can be generated auto-regressively without dis-\ntinction, and subsequently decoded into raw images and text\nby the image detokenizer and text detokenizer, respectively.\nOwing to the unified modeling, the pre-training of the\nVL model can be conducted on large-scale image-text pairs\nand interleaved image-text data. Upon completion of pre-\ntraining, the model is capable of perceiving arbitrary multi-\nmodal input and generating responses varying in modalities\n(e.g., text, images or their interleaved contents), allowing\nit to generalize to a wide range of vision and language un-\nderstanding and generation tasks in a zero-shot or few-shot\nmanner. Moreover, the pre-trained model exhibits appeal-\ning emergent properties for multimodal in-context learning,\nas it can effectively tackle new unseen tasks when provided\nwith multimodal prompts. The VL generative pre-trained\ntransformer model, referred to as VL-GPT, holds the poten-\ntial to serve as a powerful foundation model for the multi-\nmodal community, similar to the role of GPT family [4, 29]\nin NLP. Our contributions are summarized as follows:\n\u2022 We propose an image tokenizer-detokenizer framework to\nconvert images into continuous embeddings and recon-\nstruct them, while exploring effective training methods\nfor this framework.\nThrough efficient training that requires an affordable\ncomputational cost, the image tokenizer and detokenizer\ncan effectively retain both semantic information and pixel\ndetails of the original image.\n\u2022 We introduce VL-GPT, a generative pre-trained trans-\n2\nformer model for vision and language (VL) understand-\ning and generation tasks. The model can be pre-trained\non large-scale multimodal corpora in a unified auto-\nregressive manner, i.e., predicting the next token in a mul-\ntimodal sequence containing continuous visual embed-\ndings and discrete text tokens without any discrimination.\n\u2022 VL-GPT exhibits competitive performance on various VL\nunderstanding and generation benchmarks under zero-\nshot and few-shot settings, including image captioning,\nvisual question answering, and text-to-image generation.\nIt also demonstrates an appealing multimodal in-context\nlearning ability when provided with multimodal prompts.\nFurthermore, it shows promising potential to serve as a\ngeneral multimodal assistant through instruction tuning.\n2. Related Work\nMultimodal Pre-training in the Pre-LLM Era. Prior re-\nsearch efforts primarily concentrated on model architec-\nture to facilitate the fusion and interaction of cross-model\ndata [6, 50, 52]. The success of transformers in language\nmodels [42] and ViT [10] inspired the development of uni-\nfied multi-modal modeling [27, 44]. Although images and\nlanguage can be processed by a unified model with shared\nparameters, they often have distinct training objectives. It\nis worth mentioning that the BEiT series [2, 45] success-\nfully adapted the masked language modeling objective from\nBERT [8] to vision and multimodal pre-training.\nMultimodal Pre-training in the LLM Era. Building upon\npre-trained large language models (LLMs) [33, 40, 41, 54],\nrecent studies have effectively developed multimodal lan-\nguage models capable of processing image and text inputs\nto generate text outputs [1, 22, 24, 25, 56]. Another chal-\nlenge for large multimodal models is generating multimodal\ncontent beyond language. Several efforts, such as Visual\nChatGPT [47] and HuggingGPT [38], have achieved this\nby connecting LLMs with other generation tools within an\nLLM integration framework, e.g., LangChain. However,\nthese systems exhibit instability and limited room for fur-\nther optimization. To enable LLMs to generate images with\noptimization, M-VADER [46] aligns the semantic consis-\ntence between an LLM and a diffusion decoder by train-\ning them on image-text pair data. GILL [18] achieves more\ncomplex interleaved image-text generation by mapping the\nembedding spaces of the LLM to text-to-image generation\nmodels. NExT-GPT [48] extends this concept to additional\nmodalities, such as audio and video. DreamLLM [9] fa-\ncilitates passing the differential gradient from image diffu-\nsion models to language models, enabling the generation of\nfree-form interleaved content. Following similar methods,\nKosmos-G [30] enhances the fidelity of generated images in\ncontext through a compositional instruction tuning task.\nIn contrast to our VL-GPT, these studies mainly focus on\nImage \nEncoder\nText \nEncoder\na brown dog is \nsleeping on a bed\nRec Loss\nRec Loss\nInput image\nImage caption\nVisual continuous embedding xv\nImage embedding ev\nText embedding et\nEstimated image embedding zv\nEstimated text embedding zt\nImage Tokenizer\nReconstruction \nImage !x\nUnused During\nTraining\nImage Detokenizer\nCausal \nTransformer\nTransformer\nDecoder\nVisual Encoder\nDiffusion Decoder\nFigure 2. The training scheme of our image tokenizer-detokenizer\nframework, which is supervised by the frozen image and text en-\ncoders of our adopted pre-trained image diffusion model. Only\nthe causal transformer in tokenizer and the transformer decoder\nin detokenizer necessitate training, while the diffusion decoder in\ndetokenizer remains unused during training.\nleveraging existing LLMs and exploring the integration of\ncurrent image encoders and image generation models into\nLLMs.\nHowever, these methods do not achieve unified\nmodeling for images and language, nor unified modeling\nfor image understanding and generation. For instance, spe-\ncial queries are typically needed to encapsulate the context\ninformation for image generation, but they are deemed un-\nnecessary when images serve as input for LLMs. Moreover,\napplying an auto-regressive training objective on visual em-\nbeddings is challenging due to the inconsistency of image\nembedding space. Consequently, these approaches are lim-\nited in expanding the scalable pre-training paradigm for\nthe GPT family, i.e., next-token prediction, to large vision-\nlanguage models on web-scale multimodal corpora.\nRecently, Emu [39] proposes a multimodal pre-trained\nmodel that enables the auto-regressive training for both vi-\nsual and text embeddings. However, it requires an costly\nsecond-stage fine-tuning of the Stable Diffusion [35] to con-\nvert the visual embeddings into pixel space. In contrast, our\nmethod utilizes a novel image tokenizer-detokenizer frame-\nwork that can fully leverage a pre-trained image diffusion\nmodel (see Fig. 2). This approach not only simplifies the\nprocess but also enhances training efficiency. Similar to our\napproach, SEED [11] initially trains an image tokenizer, fol-\nlowed by a multi-modal training. Nevertheless, its tokenizer\nencodes images into discrete tokens via quantization opera-\ntions, potentially losing partial image information. In con-\ntrast, our tokenizer converts images into continuous visual\nembeddings, preserving both semantic information and ap-\npearance details, resulting in improved performance across\n3\ndiverse benchmarks.\n3. Method\nAs illustrated in Fig. 1, the implementation of our VL-GPT\ncan be separated into two consecutive stages. In the first\nstage, we learn an image tokenizer-detokenizer framework,\ncapable of encoding images into continuous visual embed-\ndings and decoding them back. The second stage is the pre-\ntraining and instruction tuning of our VL-GPT, which facil-\nitates a unified modeling approach for vision and language\nunderstanding and generation. In the following sections, we\nwill provide a detailed description of these two stages.\n3.1. Image Tokenizer-Detokenizer Framework\nTo implement an auto-regressive training objective on vi-\nsual embeddings and text tokens concurrently, we de-\nvelop an image tokenizer-detokenizer framework for vision-\nlanguage models.\nThe framework, inspired by text tok-\nenizers utilized in language models [43], can realize bi-\ndirectional conversion between original images and contin-\nuous visual embeddings, thereby enabling the transformer\nmodel to process vision data akin to processing text data.\nArchitecture The overall architecture of our image\ntokenizer-detokenizer framework is depicted in Fig. 1. It\ncomprises two primary components: a tokenizer E respon-\nsible for encoding the image into continuous visual embed-\ndings, and a detokenizer D dedicated to decoding the visual\nembeddings back to raw images.\nFormally, the image tokenizer E employs an image en-\ncoder (e.g., ViT [10]) to extract spatial patched features xp\nfrom the given image x. Subsequently, a standard decoder-\nonly causal transformer is utilized to convert the patched\nfeatures xp to 1D (one-dimensional) visual embeddings\nxv \u2208 RN\u00d7d, where N represents the number of visual em-\nbeddings, and d denotes the embedding dimension. The\n1D continuous visual embeddings xv serve as input embed-\ndings to our vision-language model, analogous to word to-\nkens in language models.\nInspired by current image diffusion models with excel-\nlent performance and accessibility [34, 35, 49], our image\ndetokenizer D learns a latent diffusion model to decode\nvisual embeddings xv into images. Specifically, a trans-\nformer decoder is employed to estimate condition embed-\nding z from xv. Then a diffusion decoder, initialized from\na pre-trained image diffusion models, can generate images\n\u02c6x based on estimated condition embedding z.\nTraining Despite the initialization with pre-trained models,\nconducting a full-scale end-to-end optimization of the im-\nage tokenizer and detokenizer demands large-scale data and\nconsiderable training costs. To pursue efficient training, we\nopt to train the transformer decoder in image detokenizer\nto estimate the condition embedding utilized for the diffu-\nsion decoders, as illustrated in Fig. 2. Notably, the diffusion\ndecoder, including its U-Net and VAE modules, is not em-\nployed during framework training, substantially enhancing\nthe efficiency of training procedure.\nAs Fig. 2 shows, the training objective of our framework\naims to concurrently reconstruct the image condition em-\nbedding ev and text condition embedding et. This design\ndistinguishes our framework from previous works [11, 18,\n48], which only align their intermediate outputs with text\nembedding produced by the text encoder of the diffusion\nmodel. Specifically, we optimize the framework by mini-\nmizing the following loss function (with weight \u03bb1 and \u03bb2):\nL(z) = \u03bb1 \u2217 MSE(zv, ev) + \u03bb2 \u2217 MSE(zt, et)\n(1)\nwhere MSE (\u00b7) denotes the mean squared error loss, and\nzv and zt represent the estimated image condition em-\nbedding and estimated text condition embedding, respec-\ntively. During inference, both types of condition embed-\nding contribute collectively to generate images. Our image\ntokenizer-detokenizer framework can also work when re-\nconstructing only image condition embedding (if \u03bb2=0) or\nonly text condition embedding (if \u03bb1=0). Moreover, the\ntraining for estimating image embedding only requires vi-\nsual data, which is more training-friendly than estimating\ntext embedding. However, our experiments in Sec. 4.5 re-\nveal that these two types of embedding complement each\nother: text embedding contain rich semantic information\nwhile image embedding effectively persevere image details.\n3.2. VL-GPT\nVL-GPT aims to process the vision and language under-\nstanding and generation within a single transformer model\nin a unified way, similar to GPT handles language tasks.\nIt is capable of perceiving the interleaved multi-modal data\nand generating content across various modalities. By em-\nploying unified modeling, our VL-GPT can conduct auto-\nregressive pre-training on web-scale multimodal corpora,\nthereby holding the potential to serve as a powerful foun-\ndation model in the multimodal research community.\nArchitecture As depicted at the bottom of Fig. 1, our VL-\nGPT comprises five components: a large vision-language\ntransformer model M, an image tokenizer Ev, a text tok-\nenizer Et, an image detokenizer Dv and a text detokenizer\nDt. In comparison to a language model, VL-GPT incor-\nporates additional image tokenizer and image detokenizer\nelements.\nGiven any interleaved image-text data, the image tok-\nenizer and the text tokenizer initially encode them into a\nmultimodal sequence.\nMore specifically, the image tok-\nenizer Ev converts each image into N continuous visual em-\nbeddings xv. Additionally, two special tokens [IMG] and\n[/IMG] are appended at the beginning and end of the vi-\nsual embeddings, respectively. The visual embeddings are\n4\nthen combined with the discrete text tokens encoded by the\ntext tokenizer Et to form a interleaved multimodal sequence\nv = (v1, v2, . . . , vn), where vi can be either a discrete text\ntoken or a continuous visual embedding. The multimodal\nsequence v is then fed into the large VL model M for uni-\nfied auto-regressive modeling.\nThe output embedding M(vi) can be flexibly trans-\nformed into a text embedding through a language model-\ning head for the predefined vocabulary or into a visual em-\nbedding with a separate regression head. During training,\nthe selection of the transformed head depends on whether\nthe target for the current embedding is a text token or a vi-\nsual embedding. During inference, if [IMG] is predicted, the\nvisual regression head will be utilized to transform output\nembeddings in the subsequent N prediction; otherwise, the\nlanguage modeling head will be used. The prediction em-\nbeddings are subsequently decoded to raw images or text\nvia the image detokenizer Dv or the text detokenizer Dt .\nMultimodal Pre-training.\nBenefiting from the unified\nmodeling of both visual and text embeddings, we can apply\nthe unsupervised pre-training paradigm of GPT [31] to our\nVL-GPT on a large corpus of multimodal data with minimal\nmodifications.\nGiven an interleaved multimodal sequence v\n=\n(v1, v2, . . . , vn) in a large-scale corpora, we employ the\nstandard auto-regressive modeling objective in language\nmodels to maximize the following likelihood:\nL(v) =\nn\nX\ni\nlog P (vi | v1, v2, . . . , vi\u22121; \u0398)\n(2)\nwhere \u0398 represents the parameters of our VL-GPT. We ap-\nply cross-entropy loss with a language modeling head on the\ndiscrete text tokens and utilize MSE loss with a regression\nhead for continuous visual embeddings.\nInstruction Tuning To enhance the ability of the pre-\ntrained VL-GPT to follow human instructions faithfully and\ngenerate multimodal contents creatively, we perform fur-\nther instruction tuning of VL-GPT using publicly available\ninstruction-tuning datasets.\nBriefly, the data from these\ndatasets will be restructured into a conversational format,\ni.e., pairs of multimodal human instructions and their re-\nsponses for single or multiple rounds, and subsequently em-\nployed for model tuning in a manner similar to the pre-\ntraining corpora. A minor deviation from pre-training pro-\ncess is that the training objective will be applied exclusively\nto the embeddings tokenized from answer responses.\n4. Experiments\nThe training of our VL-GPT consists of three phases: train-\ning for the tokenizer-detokenizer framework, unified mul-\ntimodal pre-training for the vision-language transformer\nmodel, and instruction tuning for the pre-trained VL-GPT.\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nFigure 3.\nReconstruction images of our image tokenizer-\ndetokenizer framework by utilizing image condition embedding\n(zv), or text condition embedding (zt), or both types of condition\nembedding (zv+zt). More examples are included in the appendix.\nModel\nCOCO\nFlickr30k\nGILL [18]\n67.45\n65.16\nSD v1.5 [35]\n68.43\n65.40\nSEED [11]\n68.23\n65.22\nunCLIP [34]\n79.30\n79.55\nOur tokenizer-detokenizer\n80.22\n79.14\nTable 1. Evaluation of image reconstruction with CLIP similarity.\n4.1. Datasets\nPublicly available datasets are utilized for different phrase\nof the VL-GPT training. The image tokenizer-detokenizer\nframework is trained on image-text pairs from CC3M [37],\nLAION-Aestheics [20], and LAION-COCO [36].\nDur-\ning the unified multimodal pre-training of VL-GPT, a\ncombination of paired and interleaved image-text data is\nemployed.\nThe image-text pairs remain consistent with\nthe preview phase, while the interleaved image-text se-\nquences are acquired from Multimodal-C4 (MMC4) [57]\nand OBELICS [21]. We adopt similar preprocessing tech-\nniques for interleaved data implemented in Flamingo [1].\nFor each document, a maximum of 5 images and their as-\nsociated captions are randomly sampled to construct a sub-\nsequence with a token length of up to 512. Additionally,\nfor paired and interleaved image-text data, each image is\nrandomly placed before or after its corresponding caption.\nFor the instruction tuning of VL-GPT, a compositional in-\nstruction tuning dataset is constructed from various sources,\nencompassing conversational data from LLAVA [25] and\nSVIT [55], image-text pair data from COCO Caption [5],\nand image editing data from InstructPix2Pix [3] and Mag-\nicbrush [53]. These datasets are restructured into a conver-\n5\nModels\nImage-Text understanding\nText-to-image generations\nCOCO\nVQAv2\nGQA\nOKVQA\nVizWiz\nVisDial\nCOCO FID (\u2193)\n\u25b6 VL Understanding or generation Models\nMetaLM [14]\n82.2\n41.1\n-\n11.4\n-\n-\n-\nKosmos-1 [16]\n84.7\n51.0\n-\n-\n29.2\n-\n-\nFlamingo-9B\u00b6 [1]\n79.4\n51.8\n-\n44.7\n28.8\n48.0\n-\nSD v1.5 [35]\n-\n-\n-\n-\n-\n-\n9.22\n\u25b6 Unified VL understanding and generation Pre-trained Models\nGILL [18]\n-\n-\n-\n-\n-\n-\n12.2\nKosmos-G-1.9B [30]\n-\n-\n-\n-\n-\n-\n10.99\nSEED-OPT2.7B [11]\n119.0\n42.8\n28.8\n-\n-\n-\n-\nEmu [39]\n112.4\n52.0\n-\n38.2\n34.2\n47.4\n11.66\nEmu\u2020 [39]\n-\n52.9\n-\n42.8\n34.4\n47.8\n-\nVL-GPT\n116.4\n51.7\n34.6\n35.8\n34.7\n49.9\n12.25\nVL-GPT\u2020\n119.2\n55.3\n38.1\n41.5\n35.2\n49.6\n-\n\u25b6 Unified VL understanding and generation Models with Instruction-tuning or Fine-tuning\nCM3Leon-7B [51]\n61.6\n47.6\n-\n23.8\n37.6\n22.6\n10.82\nEmu-I [39]\n-\n57.5\n-\n46.2\n38.1\n50.1\n-\nNExT-GPT\u00a7 [48]\n156.7\n-\n-\n-\n-\n-\n11.28\nDreamLLM-7B [9]\n115.4\n56.6\n-\n44.3\n38.1\n-\n8.46\nVL-GPT-I\n133.7\n67.2\n51.5\n50.3\n38.9\n51.8\n11.53\nTable 2. Evaluation comparison between our VL-GPT and other models.\n\u2020 denotes that the zero-shot prompt is built by sampling two\ntask-specific examples with their associated images removed. \u00a7 represents that the dataset employed for instruction tuning is private.\nsational format using the template provided in the appendix.\nFor further details regarding preprocessing and construction\nof our training dataset, please refer to the appendix as well.\n4.2. Training setup\nTo efficiently train the image tokenizer-detokenizer frame-\nwork, the visual encoder in the image tokenizer and the dif-\nfusion decoder in the image detokenizer are initialized with\nCLIP-L image encoder [32] and IP-Adapter [49], respec-\ntively. Moreover, these two modules remain frozen through-\nout the entire training, and only the causal transformer and\nthe transformer decoder necessitate optimization. Unless\nspecified otherwise, the weight coefficients \u03bb1 and \u03bb2 in\nEq. 1 are assigned a value of 1.0 during both training and\nevaluation. The AdamW optimizer [26] is employed for\ntraining, with a learning rate of 2e-4 and a cosine schedule.\nThe framework is trained using a total batch size of 1024 on\n8 NVIDIA 40G-A100 GPUs for 10,000 iterations.\nFor the multimodal pre-training of our VL-GPT, the pre-\ntrained LLaMA 7B [40], its text tokenizer, and its text deto-\nkenizer are integrated with our trained image tokenizer and\ndetokenizer to establish the VL-GPT model with a total of\n7.5 billion parameters. LoRA [15] module is incorporated\ninto the LLaMA model, resulting in relatively low demand\nfor computational resources. AdamW optimizer is also uti-\nlized with a learning rate of 2e-4.\nThe multimodal pre-\ntraining is conducted with a batch size of 4096 on 32 GPUs\nfor 20,000 iterations. Instruction tuning is performed on the\npre-trained VL-GPT, adopting similar training settings used\nduring pre-training. LoRA is also employed, and the learn-\ning rate is reduced to 5e-5. The model is trained for 10,000\niterations with batch size of 512 on 4 GPUs. Additional\ntraining settings are included in the appendix.\n4.3. Image Tokenizer and Detokenizer Performance\nThe image tokenizer-detokenizer framework is designed to\nconvert images between pixel space and continuous visual\nembeddings.\nTo assess its effectiveness, we employ the\nmethod of calculating the CLIP similarity as the evaluation\nmetric for our framework, as implemented in SEED [11].\nAs demonstrated in Tab. 1, our framework achieves notably\nsuperior semantic consistency compared to SEED, which\nutilized quantized visual tokens.\nFurthermore, we present visualizations of the recon-\nstructed images generated by our framework in Fig. 3. By\nestimating both image condition embedding and text condi-\ntion embedding and utilizing them to guide the generation\nprocess of diffusion decoder, our image detokenizer is ca-\npable of generating images with high consistency in terms\nof spatial appearance and semantic information.\n4.4. Evaluation of our VL-GPT\nBenchmark Performance We first evaluate the zero-shot\nperformance of VL-GPT on a variety of vision-language\ntasks, including image captioning on MSCOCO [5], vi-\nsual question answering on VQAv2 [12], GQA [17],\nOKVQA [28], and VizWiz [13], visual dialog on Vis-\nDial [7], and text-to-image generation on MSCOCO. Com-\nprehensive details regarding these benchmarks and their\nmetrics can be found in the appendix. As results in Tab. 2\nindicate, VL-GPT achieves competitive performance on\n6\nA group of white daisies with \nyellow centers and green leaves \nin the background.\nIt  also wears a red scarf.\nPlease generate an image of a Labrador\ndog wearing glasses.\nThe office staff is not here, he may be taking a rest \nat the moment.\nA photo of \nthree giraffe. \nA photo of \none swan. \nA photo of \ntwo dogs. \nDscribe the image in \ndetail in one sentence. \nA stream flowing \nthrough a forest.\nA wedding cake \nwith pink roses \nand greenery.\nA yellow and \nblack football.\nA dog is standing in \nthe snow mountain.\nA modern architectural building \nwith large glass windows, situated \non a cliff overlooking a serene \nocean at sunset.\nA woman is eating \npizza in the park.\nA fire is burning in the \nmiddle of a blue sea.\nAn antique botanical illustration drawn with fine \nlines and a touch of watercolour whimsy, depicting \na strange lily crossed with a Venus flytrap, its \npetals poised as if ready to snap shut on any \nunsuspecting insects.\nAn image shows that a cat \nis sleeping on these plants.\nYes, there is a banana on the table.\nWhat does this picture shows as a whole?\nAre there any items on the table that are \nnot for office use?\nAre the people at the office here?\nChange the scarf to blue.\nQuestion: \nwhat is she \nplaying with?\nShort answer:\n(2) \n(3) \n(4) \n(5) \n(6) \n(7) \n(8) \n(9) \n(10) \n(11) \n(12) \nThe image depicts a spacious, well-lit office with a wooden \ndesk as its centerpiece. The desk is adorned with a variety of \nitems, including a computer monitor, a keyboard, a mouse, and \na phone. There is a chairs in the office, positioned near the \ndesk. \n(1) \nFigure 4. Illustrative samples of our VL-GPT across various vision and language understanding and generation tasks. These tasks en-\ncompass: (1)-(2) image captioning, (3) visual question answering (VQA), (4)-(8) text-to-image generation, (9)-(10) multimodal in-context\ngeneration, and (11)-(12) multimodal dialogue. Examples (1)-(10) are generated by our pre-trained VL-GPT, while (11)-(12) are produced\nby our instruction-tuned VL-GPT. Blue boxes represent multimodal inputs and yellow boxes indicate VL-GPT outputs.\n7\nModels\nVQAv2\nVizWiz\nk\n2\n4\n8\n2\n4\n8\nKosmos-1 [16]\n51.4\n51.8\n51.4\n31.4\n35.3\n39.0\nFlamingo-9B [1]\n-\n56.3\n58.0\n-\n34.9\n39.4\nEmu [39]\n56.4\n58.4\n59.0\n37.8\n41.3\n43.9\nVL-GPT\n57.2\n58.6\n58.9\n38.9\n41.8\n44.2\nTable 3. Few-shot performance on visual question answering.\nEstimation target\nReconstruction\nVL-GPT\nCLIP\nSimilarity (\u2191)\nCaptioning\nCIDEr (\u2191)\nGeneration\nFID (\u2193)\net\n73.59\n131.1\n12.79\nev\n80.05\n123.6\n13.61\net + ev\n80.22\n133.7\n12.25\nTable 4. Ablation of condition embedding types. Text embedding\n(et), image embedding (ev), or their combination (et + ev) are\nemployed to guide the training of the tokenizer-detokenizer frame-\nwork. We evaluate the effectiveness of reconstructing images and\nthe performance of VL-GPT when adopting different image tok-\nenizer and detokenizer.\nboth image-text understanding and text-to-image generation\ntasks, thereby validating the effectiveness of unified multi-\nmodal pre-training. Notably, VL-GPT attains an impres-\nsive CIDEr score of 116.4 or 119.2 on MSCOCO caption-\ning without or with text-only prompts, surpassing other uni-\nfied VL pre-trained models. With further instruction tuning,\nVL-GPT-I, the instruction-tuned VL-GPT, significantly en-\nhances model performance, achieving the best or near-best\nresults in all tasks.\nMultimodal In-context Learning Similar to the behav-\nior of LLMs, our VL-GPT can be prompted to address\nnew vision-language tasks when provided with a few mul-\ntimodal examples from training data composed in the mul-\ntimodal prompt. To quantitatively evaluate its multimodal\nin-context learning capability, we examine the few-shot per-\nformance of VL-GPT when varying the number of exam-\nples in the given prompt, as shown in Tab. 3. Our VL-GPT\noutperforms other works under almost all few-shot setting\n(k=2, 4, 8) on two datasets for the visual question answer-\ning task. Moreover, a positive correlation is observed be-\ntween the number of the examples in the given prompt and\nthe performance on these two datasets.\nQualitative Results Fig. 4 showcases a series of gener-\nated visualizations using our VL-GPT model, encompass-\ning various tasks such as image captioning, visual ques-\ntion answering, text-to-image generation, multimodal gen-\neration with in-context learning, and multimodal multi-turn\ndialogue. Intriguingly, VL-GPT demonstrates remarkable\ncapabilities that are not readily assessed through existing\nacademic benchmarks. For instance, in Fig. 4 (7-8), VL-\nGPT generates highly realistic images in response to long-\ntext prompts containing complex concepts. In Fig. 4 (10),\nVL-GPT exhibits the ability to generate images and texts in\na flexible manner, conditioned on the provided multimodal\ncontext. Fig. 4 (11-12) illustrates the multi-turn dialogue\ncapabilities of the instruction-tuned VL-GPT, wherein the\nmodel generates multimodal contents consistent with the\nexisting context based on user instructions. This suggests\nthe promising potential of the VL-GPT as a versatile and\neffective multimodal general assistant.\n4.5. Ablation Studies\nPrevious studies typically generate images by converting\ntheir output into text condition embedding for image dif-\nfusion models. In contrast, our detokenizer estimates both\ntext condition embedding and image condition embedding\nfrom visual continuous embeddings, as depicted in Sec. 3.1.\nThe advantage of this design will be discussed next.\nFig. 3 displays the images reconstructed by our\ntokenizer-detokenizer using different estimated condition\nembedding, i.e., only using image condition embedding,\nonly using text condition embedding, or using both. These\nexamples reveal that these two type of embedding comple-\nment each other: image embedding effectively preserve im-\nage appearance details while text embedding assists in im-\nage reconstruction, e.g., determining the number of people.\nAs evidenced in Tab. 4, although it is feasible to train im-\nage tokenizer-detokenizer framework by estimating solely\none type of condition embedding (when \u03bb1=0 or \u03bb2=0 in\nEq. 1), the simultaneous estimation of both types of con-\ndition embedding leads to optimal performance for both\nthe tokenizer-detokenizer framework and VL-GPT. We hy-\npothesize that estimating image condition embedding en-\nables our tokenizer to retain more pixel information from\nthe input image, which is beneficial for image reconstruc-\ntion. Meanwhile, estimating text condition embedding al-\nlows the visual embeddings to contain more high-level se-\nmantics, leading to improved performance in subsequent vi-\nsion and language tasks.\n5. Conclusion\nWe propose VL-GPT, a generative pre-trained transformer\nmodel for vision and language understanding and gen-\neration.\nThe model incorporates an innovative image\ntokenizer-detokenizer framework, enabling it to be pre-\ntrained on large-scale multimodal corpora with a unified\nauto-regressive objective.\nUpon completion of the pre-\ntraining, VL-GPT exhibits competitive performance across\nvarious academic benchmarks and manifests several appeal-\ning emergent capabilities. As for limitations, the effective-\nness of our method has not been verified through the scaling\nup of model parameters. We hope that our work will stimu-\nlate further exploration in the pursuit of general intelligence\nwithin the multimodal research community.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022.\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021.\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020.\n[5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015.\n[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning, 2020.\n[7] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos\u00b4e MF Moura, Devi Parikh, and Dhruv Ba-\ntra. Visual dialog. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 326\u2013335,\n2017.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[11] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023.\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017.\n[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018.\n[14] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen\nChi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLan-\nguage models are general-purpose interfaces. arXiv preprint\narXiv:2206.06336, 2022.\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[16] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023.\n[17] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700\u20136709, 2019.\n[18] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023.\n[19] Taku Kudo and John Richardson.\nSentencepiece:\nA\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing.\narXiv preprint\narXiv:1808.06226, 2018.\n[20] LAION. Laion-aesthetics. https://laion.ai/blog/\nlaion-coco/.\n[21] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M. Rush, Douwe Kiela,\nMatthieu Cord, and Victor Sanh. Obelics: An open web-\nscale filtered dataset of interleaved image-text documents,\n2023.\n[22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023.\n[23] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023.\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023.\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023.\n[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[27] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-\ntaghi, and Aniruddha Kembhavi.\nUnified-io: A unified\nmodel for vision, language, and multi-modal tasks. arXiv\npreprint arXiv:2206.08916, 2022.\n9\n[28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195\u20133204, 2019.\n[29] OpenAI. Gpt-4 technical report, 2023.\n[30] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023.\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. 2018.\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020.\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022.\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022.\n[36] Christoph Schuhmann, K\u00a8opf, Andreas, Theo Coombes,\nRichard Vencu, Benjamin Trom, and Romain Beaumont.\nLaion coco:\n600m synthetic captions from laion2b-en.\nhttps://laion.ai/blog/laion-coco/.\n[37] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018.\n[38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang.\nHugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv\npreprint arXiv:2303.17580, 2023.\n[39] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023.\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n[43] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural\nmachine translation with byte-level subwords. In Proceed-\nings of the AAAI conference on artificial intelligence, pages\n9154\u20139160, 2020.\n[44] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In International Conference on Machine Learn-\ning, pages 23318\u201323340. PMLR, 2022.\n[45] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022.\n[46] Samuel Weinbach, Marco Bellagente, Constantin Eichen-\nberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Bj\u00a8orn\nDeiseroth, Koen Oostermeijer, Hannah Teufel, and An-\ndres Felipe Cruz-Salinas. M-vader: A model for diffusion\nwith multimodal context. arXiv preprint arXiv:2212.02936,\n2022.\n[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023.\n[48] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv:2309.05519, 2023.\n[49] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023.\n[50] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models, 2022.\n[51] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023.\n[52] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,\nYumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,\nZhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and\nPengchuan Zhang. Florence: A new foundation model for\ncomputer vision, 2021.\n[53] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing.\narXiv preprint arXiv:2306.10012,\n2023.\n10\n[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022.\n[55] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up\nvisual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023.\n[56] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023.\n[57] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023.\n11\nDataset\nTask\nSplit\nMetric\nCOCOCap\nScene Description\ntest\nCIDEr (\u2191)\nVQAv2\nScene Understanding QA\ntest-dev\nVQA acc. (\u2191)\nGQA\nScene Understanding QA\ntest-dev\nVQA acc. (\u2191)\nOKVQA\nExternal Knowledge QA\nval\nVQA acc. (\u2191)\nVizWiz\nScene Understanding QA\ntest-dev\nVQA acc. (\u2191)\nVisDial\nImage Dialogue\nval\nNDCG (\u2191)\nCOCO\nText-to-Image Generation\nval test\nFID (\u2193)\nTable 5. Summary of the evaluation benchmarks.\n6. Training Details\n6.1. Training of image tokenizer and detokenizer\nDatasets.\nThe image-text pairs from CC3M, Laion-\nAestheics, and LAION-COCO are utilized for the train-\ning of our image tokenizer-detokenizer framework. Specif-\nically, CC3M dataset comprises 3.3 million image-text\npairs crawled from the Web. Both Laion-Aesthetics and\nLAION-COCO are subsets of the larger LAION-5B dataset.\nLAION-Aesthetics is characterized by its high aesthetic\nquality, while LAION-COCO is composed of images sam-\npled from LAION-5B and their corresponding captions\ngenerated by existing vision-language models, e.g., BLIP.\nDue to the efficient design of the framework, a relatively\nsmall subset of 10 million samples from these two datasets\nwas found to be sufficient for model convergence in our ex-\nperiments. Further exploration of experiments with larger\ndatasets remains a prospect for future research.\nDuring\nthe training process, data were randomly sampled from the\nmixture of these three datasets in a ratio proportional to their\nrespective sizes.\nOptimization. The visual encoder in the image tokenizer is\ninitialized with CLIP-L, while the diffusion decoder in the\nimage detokenizer incorporates the U-Net and VAE mod-\nules from IP-adapter Plus. These components remain frozen\nduring the training process. The causal transformer in the\nimage tokenizer and the transformer decoder in the image\ndecoder are constructed using the standard transformer de-\ncoder, which consisting of 12 transformer blocks with ran-\ndom initialization.\nEach block comprises a causal self-\nattention layer, a cross attention layer, and a multilayer per-\nception (MLP) layer. The causal attention layer plays a vi-\ntal role in capturing causal dependencies among 1D visual\ncontinual embeddings, which is proved to be effective for\nfurther modeling in large vision-language models like Emu\nand SEED. In all experiments, the number of visual embed-\nding N is set to 32 in our all experiments, and its dimension\nd is set to 4096. Image augmentation techniques employed\nin CLIP models are applied, which involve resizing the in-\nput image with its shorter side to 224 pixels and cropping\nthe image to a fixed size of 224\u00d7224 pixels.\n6.2. Pre-training of VL-GPT\nDatasets. In addition to the datasets utilized for training the\nimage tokenizer-detokenizer framework, publicly available\ninterleaved image-text data, i.e., MMC4 and OBELICS,\nare employed for the pre-training of our vision-language\ntransformer.\nDuring pre-training, multimodal sequences\nwith interleaved image-text data are obtained from these\ntwo datasets. For MMC4, the core split is used, and low-\nquality samples with a CLIP similarity between the image\nand its caption below 0.24 are filtered out. For OBELICS,\na sequence comprising 512 tokens is randomly sampled\nbased on the arrangement of image and text data within the\noriginal document. To augment the probability of procur-\ning sequences containing multiple images, single-image se-\nquences are discarded with a likelihood of 0.8. Throughout\nthe training process, these two datasets maintain equivalent\nsampling probabilities, as do the sampling probabilities for\ndatasets comprising image-text pairs and datasets contain-\ning interleaved image and text data.\nOptimization. The large vision-language model, VL-GPT,\nis constructed by integrating the pre-trained language model\nLLaMA 7B with our image tokenizer-detokenizer frame-\nwork. LoRA modules are attached to all linear layers in the\nvision-language transformer, with a LoRA rank of 32. An\nadditional linear head is employed as a separate regression\nhead to predict the subsequent visual continuous embedding\nfor the current embedding. During multimodal pre-training,\nonly the parameters in LoRA modules and the regression\nhead are tuned, while all parameters of pre-trained LLaMA,\nimage tokenizer, and image detokenizer remain frozen to re-\nduce training costs. The data augmentation techniques used\nin the previous stage are also utilized in this phase.\n6.3. Instruction tuning of VL-GPT\nDatasets. To align the VL-GPT model with human instruc-\ntions, multimodal instruction tuning is applied to the model\nusing a combination of publicly available datasets, such\nas LLAVA, SVIT, MSCOCO Caption, InstructPix2Pix, and\nMagicbrush. All dataset are restructured into a conversa-\ntional formulation, consisting of a system message followed\nby a single-turn or multi-turn conversation dialogue be-\ntween a user and an assistant. The system message and con-\nversational template employed in our method are presented\nin Tab. 6. Furthermore, the MSCOCO caption dataset is\nemployed for both image captioning task and image gen-\neration task by altering the order of the image and its cor-\nresponding caption. The InstructPix2Pix and Magicbrush\ndatasets are utilized for prompt-based image editing task.\nDuring instruction tuning, data in these datasets are sam-\npled to construct a batch of data for model optimization in\na ratio proportional to the dataset size.\nOptimization. Instruction tuning is carried out on the pre-\n12\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nFigure 5. Reconstruction examples of our image tokenizer and detokenizer by employing different condition embedding.\nA small cactus \nwearing a straw hat \nand neon sunglasses \nin the Sahara desert.\nA silhouette of a grand piano \noverlooking a dusky cityscape \nviewed from a top-floor penthouse, \nrendered in the bold and vivid style \nof a vintage travel poster.\nA paper craft art depicting a girl giving her cat \na gentle hug. Both sit amidst potted plants, with \nthe cat purring contentedly while the girl smiles. \nThe scene is adorned with handcrafted paper \nflowers and leaves.\nA cat wearing \nsunglasses\nA woman in a red \ndress is dancing in \nthe park\nYellow flowers in a \nvase on the desk\nA bird is \nstanding in the \nsnow \nA fire is burning in \nthe middle of a blue \nsea\nA large house new\nthe lake under the \nsunlight\nA group of boys \nare playing \nfootball on the \ngreen grass\nA  cat is sitting in a \nblue car in the street\nA girl is playing the \npiano on the stage\nA cat with a \nscarf is sitting in \nthe snow\nA brown dog and a \nwhite cat are lying \non the couch\nFigure 6. Examples of text-to-image generation. Blue boxes denotes the text prompt, and yellow boxes represents the generated image.\n13\nSystem Message\nYou are a helpful, respectful and honest assistant.\nAlways\nanswer as helpfully as possible, while being safe.\nYour answers\nshould not include any harmful, unethical, racist, sexist, toxic,\ndangerous, or illegal content.\nPlease ensure that your responses\nare socially unbiased and positive in nature.\nConversation Template\nImage captioning\nUSER: Provide a brief description of the given image <image>\nASSISTANT: <caption>.\nImage generation\nUSER: Create an image that visually represents the description:\n<caption>.\nASSISTANT: Here\u2019s the image:\n<image>\nImage editing\nUSER:<image> <editing prompt>.\nASSISTANT: Here is the edited\nimage:\n<image>\nTable 6. Summary of prompt templates employed in instruction tuning. The notation \u201c<image>\u201d will be replaced with the image data.\n\u201c<caption>\u201d and \u201c<editing prompt>\u201d will be substituted with the corresponding caption and editing instruction, respectively.\nModel\nTask\nTemplate\nVL-GPT\nImage captioning\n<image> Please describe this image in detail in one sentence.\nIt shows\nImage generation\nAn image of <caption>.\n[IMG]\nImage QA\n<image> Based on the image, <question>?\nShort answer:\nImage dialog\n<image> an image of <caption>.\nBased on the image, <question1>?\nShort\nanswer:\n<answer1>.\n\u00b7 \u00b7 \u00b7 Based on the image, <questionn>?\nShort answer:\nVL-GPT-I\nImage captioning\nUSER: Provide a brief description of the given image.<image> ASSISTANT:\nImage generation\nUSER: Create an image that visually represents the description:\n<caption>.\nASSISTANT:\nImage QA\nUSER: answer the question with the shortest answer <question>?\nASSISTANT:\nImage dialog\nUSER: <image> ASSISTANT: an image of <caption>.\nUSER: <question1>?\nASSISTANT: <answer1>.\n\u00b7 \u00b7 \u00b7 USER: <questionn>?\nASSISTANT:\nTable 7. Summary of the prompting template utilized during model evaluation.The terms \u201c<image>\u201d and \u201c<caption>\u201d shall be substituted\nwith the corresponding image and its caption. Additionally, the notations \u201c<questioni> and \u201c <answeri>\u201d will be replaced with the i-th\nquestion and answer pair in the dialogue. [IMG] denotes the special token indicating the start of visual continuous embeddings.\ntrained VL-GPT, with the training hyper-parameters primar-\nily following those used in the pre-training phase. As the\ntraining data for instruction tuning is significantly smaller\nthan that employed for pre-training, the batch size is set\nto a smaller number, i.e. 512, and only four GPUs are uti-\nlized. To prevent catastrophic forgetting of the pre-trained\nmodel, the model is optimized with a reduced learning rate.\nFurthermore, LoRA modules are applied in the transformer\nmodel, while all other parameters remain frozen.\n7. Evaluation Details\n7.1. Benchmarks\nTo evaluate the vision and language understanding and gen-\neration ability of VL-GPT, we evaluate it on a variety of\nbenchmarks, whose details and metrics are summarized in\nTab. 5. Specifically, the test sample from any benchmark is\nfirst packaged with a task-specific prompt template and then\ntokenized into an incomplete multimodal sequence. Then\nthe VL-GPT model and its instruction tuned version, VL-\nGPT-I, are required to complete the multimodal sequence\nin an auto-regressive and open-ended manner. Evaluation\nresults can be obtained by either using the official evalua-\ntion code or submitting our prediction on the official server.\nIt should be noted that not all results reported in Tab. 2\nare zero-shot evaluation; for instance, VL-GPT-I has been\ntrained on COCO Caption.\n7.2. Prompt Templates\nTo thoroughly capitalize on the knowledge acquired dur-\ning pre-training while generating outputs that adhere to the\nstyle of the benchmark under evaluation, we design task-\nspecific prompt templates for the VL-GPT and VL-GPT-I.\nThese templates are comprehensively outlined in Tab. 7.\n14\nA pair of horses \nwith red and \nwhite decorations \non their heads.\nA photo of a \nwoman holding a \ntennis racket\nA group of men \nsitting at desks \nwith computers.\nA photo of three \nmen playing \ncomputers.\nQuestion: what is \nhe doing? Answer: \nplaying computers \nA woman \nstanding next to a \ntable holding a \nsheet cake.\nA photo of a \nwoman is \ncooking.\nQuestion: what is \nshe doing? Answer: \ncooking \nA woman \nholding a tennis \nracket and \nstanding next to a \ntable.\nQuestion: what is \nshe doing? \nAnswer: playing\ntennis.\nAn image of \nthree elephants\nQuestion: how \nmany? answer: \nthree \nAn image of one \ndog\nAn adorable dot \nresting on a \nbeach near the \nocean. \nQuestion: how \nmany? answer: one \nAn image of two \nhorses.\nQuestion: how \nmany? answer: \ntwo. \nA group of \nelephants with one \nelephant raising \nit's trunk and dust \nand dirt coming up \nbehind it.\nThis is an animal. \nThis is a flower.\nThis animal is\nsurrounded with \nthis flower.\nFigure 7. Three groups of Multimodal In-context learning. In each group, partial content in the multimodal prompts is changed, and our\nVL-GPT can generate desired outputs. Blue boxes represent multimodal prompts and yellow boxes indicate VL-GPT outputs.\n8. Qualitative Cases\nAdditional reconstruction examples of our image tokenizer-\ndetokenizer framework are illustrated in Fig. 5. Further-\nmore, the qualitative examples presented in Fig. 6, Fig. 7,\nand Fig. 8 demonstrate the superior performance of VL-\nGPT in various tasks, including text-to-image generation,\nmultimodal in-context learning, and multimodal dialogue.\n15\nDescribe this image in one sentence.\nSure, here's a poem about the image: \nA wooden dock extends into the lake, \nA place to sit and watch the world go by. \nThe water is calm and clear, reflecting the trees, \nA serene place to sit and watch the world go by.\nWhen visiting this lake, you should be cautious about the \nweather conditions. The image shows a cloudy sky, which \ncould indicate that it might rain or become windy. It's \nimportant to dress appropriately and bring an umbrella or a \nraincoat if necessary.\nPlease describe this image in detail.\nWhat are the things I should be cautious \nabout when I visit here?\nTwo white puppies sit together on \nthe grass\nThe image features a serene lake scene with a wooden dock \nextending into the water. The dock is made of wood and is \nquite long, stretching across the entire width of the image. The \ndock is surrounded by calm, clear water, which reflects the \nsurrounding trees and sky.\nCan you write a poem about the image?\nGenerate image that they\nwith pink hair\nThis is the generated image\nChange to yellow glasses\nHere is the edited image\nFigure 8. Qualitative examples of multimodal dialogue by our instruction tuned VL-GPT. Blue boxes denotes the user instructions, and\nyellow boxes represents the assistant responses.\n16\n"
  },
  {
    "title": "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds",
    "link": "https://arxiv.org/pdf/2312.09246.pdf",
    "upvote": "5",
    "text": "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds\nMinghao Chen\nJunyu Xie\nIro Laina\nAndrea Vedaldi\nVisual Geometry Group, University of Oxford\n{minghao, jyx, iro, vedaldi}@robots.ox.ac.uk\nsilent-chen.github.io/Shap-Editor\n\u201cAdd a Santa hat to it\u201d\n\u201cMake it wear a blue sweater \u201d\n\u201cMake its color look like rainbow\u201d\n\u201cMake it look like a panda\u201d\n\uff5e1 sec\nFigure 1. Given 3D assets as inputs, SHAP-EDITOR achieves fast editing within one second by learning a feed-forward mapping in the\nlatent space of a 3D asset generator.\nAbstract\nWe propose a novel feed-forward 3D editing framework\ncalled SHAP-EDITOR. Prior research on editing 3D ob-\njects primarily concentrated on editing individual objects\nby leveraging off-the-shelf 2D image editing networks. This\nis achieved via a process called distillation, which transfers\nknowledge from the 2D network to 3D assets. Distillation\nnecessitates at least tens of minutes per asset to attain sat-\nisfactory editing results, and is thus not very practical. In\ncontrast, we ask whether 3D editing can be carried out di-\nrectly by a feed-forward network, eschewing test-time op-\ntimization. In particular, we hypothesise that editing can\nbe greatly simplified by first encoding 3D objects in a suit-\nable latent space. We validate this hypothesis by building\nupon the latent space of Shap-E. We demonstrate that di-\nrect 3D editing in this space is possible and efficient by\nbuilding a feed-forward editor network that only requires\napproximately one second per edit. Our experiments show\nthat SHAP-EDITOR generalises well to both in-distribution\nand out-of-distribution 3D assets with different prompts, ex-\nhibiting comparable performance with methods that carry\nout test-time optimisation for each edited instance.\n1. Introduction\nWe consider the problem of generating and editing 3D ob-\njects based on instructions expressed in natural language.\nWith the advent of denoising diffusion models [20, 55, 61,\n63], text-based image generation [55] and editing [3, 18, 48]\nhave witnessed remarkable progress. Many authors have\nsince attempted to transfer such capabilities to 3D via test-\ntime optimisation, where a 3D model is optimised from\nscratch until its rendered 2D appearance satisfies an under-\nlying prior in the pre-trained 2D models [17, 24, 58].\nWhile optimisation-based methods obtain encouraging\nresults, they are not scalable \u2014 in fact, a single 3D gener-\nation or edit can take from minutes to hours. It is then nat-\nural to seek for more efficient generators and editors that\ncan directly work in 3D. We hypothesise that this can be\ngreatly facilitated by first learning a suitable latent space\nfor 3D models. For instance, Shape-E [23] has recently\nlearned an auto-encoder that maps 3D objects into vectors\n(latents). These vectors can be generated directly by a dif-\nfusion model, eschewing test-time optimisation entirely.\nIn this paper, we thus ask whether such a 3D latent space\ncan support not only efficient 3D generation, but also effi-\narXiv:2312.09246v1  [cs.CV]  14 Dec 2023\ncient 3D editing. We answer affirmatively by developing\na method, SHAP-EDITOR, that can apply semantic, text-\ndriven edits directly in the latent space of 3D asset gener-\nators. Because of the properties of the latent space, once\nlearned, the editor function is capable of applying the edit\nto any new object in just one second vs. minutes to hours\nrequired by optimisation-based approaches.\nIn more detail, our method starts from a 3D auto-\nencoder \u2014 e.g., the off-the-shelf Shape-E encoder. It also\ntakes as input a 2D image editor that can understand instruc-\ntions in natural language. For any such instruction, SHAP-\nEDITOR learns a function that can map, in a feed-forward\nmanner, the latent of any 3D object into the latent of the cor-\nresponding edit \u2014 we call this a latent editor. The fact that\nthe latent editor can be learned relatively easily is a strong\nindication that the 3D latent space has a useful structure for\nthis type of operations. Empirically, we further explore and\ndemonstrate the partial linearity of such edits when they are\ncarried out in this space.\nOur method has several interesting practical properties.\nFirst, we learn a single latent editor that works universally\nfor any input object. This function lifts to 3D space the\nknowledge contained in the 2D image editor via distillation\nlosses. In fact, we show that we can distill simultaneously\nseveral different 2D editors of complementary strengths. In\nour student-teacher framework, the combined knowledge of\nthe editors is then transferred to the latent editor.\nSecond, we note that the latent editor is able to cap-\nture certain semantic concepts, and in particular complex\ncompositions of concepts, better than the original text-to-\n3D generator. Moreover, it allows the application of several\nedits sequentially, with cumulative effects.\nThird, while our method learns an editor function for\neach type of edit, at test time it can be applied to any num-\nber of objects very quickly, which could be used to modify\nlibraries of thousands of 3D assets (e.g., to apply a style to\nthem). In this sense, it can be seen as an amortised coun-\nterpart to methods that use test-time optimisation. We also\ndemonstrate that by conditioning the latent editor on text,\nseveral different edits can be learned successfully by a sin-\ngle model. This suggests that, given sufficient training re-\nsources, it might be possible to learn an open-ended editor.\nTo summarise, our contributions are: (1) We show that\n3D latent representations of objects designed for generation\ncan also support semantic editing; (2) We propose a method\nthat can distill the knowledge of one or more 2D image gen-\nerators/editors in a single latent editor function which can\napply an edit in seconds, significantly reducing the compu-\ntational costs associated with test-time optimisation; (3) We\nshow that this latent function does better at compositional\ntasks than the original 3D generator; (4) We further show\nthat it is possible to extend the latent editor to understand\nmultiple editing instructions simultaneously.\n2. Related work\nDiffusion-based image manipulation.\nRecent advances\nin text-guided diffusion models have greatly improved 2D\nimage generation. Yet, these models typically offer lim-\nited control over the generated content. To enable control-\nlable generation, researchers have explored concept person-\nalisation [13, 31, 56], layout control [4, 6, 10, 37], and\nother conditionings [85]. Other recent works [2, 18, 27,\n41, 48, 51, 67] have extended text-guided diffusion models\nto image-to-image translation tasks and image editing. In-\nstructPix2Pix (IP2P) [3] finetunes a diffusion model to ac-\ncept image conditions and instructional prompts as inputs,\nby training on a large-scale synthetic dataset. Subsequent\nresearch [84, 86] has sought to further finetune Instruct-\nPix2Pix with manually annotated datasets.\nNeural field manipulation.\nSeveral attempts have been\nmade to extend neural fields, such as NeRFs [45], with edit-\ning capabilities. EditNeRF [39] was the first approach to\nedit the shape and color of a NeRF given user scribbles. Ap-\nproaches that followed include 3D editing from just a single\nedited view [1], or via 2D sketches [44], keypoints [87], at-\ntributes [25], meshes [22, 52, 76, 78, 80] or point clouds [5].\nOthers focus on object removal with user-provided points\nor masks [46, 65, 73], object repositioning [77], recolor-\ning [15, 30, 32] and style transfer [75, 83].\nText-to-3D generation.\nGiven the success of diffusion-\nbased generation and vision-language models such as\nCLIP [54], several methods have been proposed for gen-\nerating 3D scenes using text prompts [21, 47]. A pioneer-\ning work is DreamFusion [53], which proposes the Score\nDistillation Sampling (SDS) loss. They use it to optimise a\nparametric model, such as NeRF, with the supervision of an\noff-the-shelf 2D diffusion model. DreamFusion has since\nbeen improved by followups [7, 38, 42, 72], but these meth-\nods are generally not directly applicable to 3D editing tasks.\nAnother direction is to train auto-encoders on explicit 3D\nrepresentation, such as point cloud [81] or voxel grid [57],\nor on implicit functions, such as singed distance function\n[11] or neural radiance field [29]. The generative models\nare trained on the latent space [8, 49] conditioned on text\ninputs. The most related work is Shap-E [23] that trained\non a very large-scale dataset (several million). It encodes\n3D assets into latents and can directly output implicit func-\ntions, such as NeRFs, signed distance functions and texture\nfields [14, 60]. It also incorporates a diffusion model [20]\nfor the conditional 3D asset generation part.\nText-based 3D editing.\nDifferently from text-to-3D gen-\neration, editing methods start from a given 3D object\nor scene (usually represented by a NeRF [45] or voxel\ngrid [64]).\nSome authors leverage CLIP embeddings or\nsimilar models [34, 35, 40] to perform text-driven se-\nmantic editing/stylisation globally [33, 43, 69, 70] or lo-\ncally [16, 28, 62, 71].\nMost recent and concurrent approaches leverage diffu-\nsion priors. Starting with InstructNeRF2NeRF [17], one\nline of research employs pre-trained 2D models to edit im-\nage renderings of the original model and uses these to grad-\nually update the underlying 3D representation [17, 68, 79].\nInstead of editing images, others optimise the 3D repre-\nsentation directly with different variants of score distilla-\ntion sampling [9, 24, 36, 50, 58, 82, 88, 89]. They often\ndiffer in their use of the diffusion prior; e.g., [17, 24] use\nInstructPix2Pix [3], while most others rely on Stable Dif-\nfusion [20]. Many existing methods edit scenes globally,\nwhich may sometimes affect unintended regions. To ad-\ndress this issue, approaches such as Vox-E [58] and Focal-\nDreamer [36], introduce mechanisms for local 3D editing.\nWe note, however, that, due to their inherent design, most\nmethods cannot handle global and local edits equally well.\nIn contrast, we show that we can train a single network\nfor both types of edits with a loss tailored to each edit type.\nWe also note that all these methods perform editing via test-\ntime optimisation, which does not allow interactive editing\nin practice; [68, 79] focus on accelerating this process, but\nthey still use an optimisation-based approach. Instead, our\nfeed-forward network applies edits instantaneously.\n3. Method\nLet \u03b8 be a model of a 3D object, specifying its shape and ap-\npearance. Common choices for \u03b8 include textured meshes\nand radiance fields, but these are often difficult to use di-\nrectly in semantic tasks such as text-driven 3D generation\nand editing. For images, generation and editing are often\nsimplified by adopting a latent representation. In this paper,\nwe thus ask whether replacing \u03b8 with a corresponding latent\ncode r can result in similar benefits for 3D editing.\nMore formally, we consider the problem of constructing\nan editor function f : (\u03b8s, y) 7\u2192 \u03b8e which takes as input a\n3D object \u03b8s (source) and produces as output a new version\nof it \u03b8e (edit) according to natural-language instructions y.\nFor example, \u03b8s could be the 3D model of a corgi, y could\nsay \u201cGive it a Christmas hat\u201d, then \u03b8e would be the same\ncorgi but with the hat. Learning the map f directly is chal-\nlenging because interpreting natural language in an open-\nended manner requires large models trained on billions of\ndata samples, which are generally not available in 3D.\nSome authors have approached this problem by starting\nfrom existing 2D image models, trained on billions of im-\nages. We can think of a 2D editor as a conditional distri-\nbution p(xe | xs, y) of possible edits xe given the source\nimage xs. Then, one can obtain \u03b8e by optimising the log-\nposterior E\u03c0 [log p(R(\u03b8e, \u03c0) | xs, y)] where R(\u03b8e, \u03c0) is the\nimage obtained by rendering \u03b8e from a random viewpoint \u03c0\nwith a differentiable renderer R. This, however, requires\nper-instance optimisation at test time, so obtaining \u03b8e may\ntake minutes to hours in practice.\nHere, we thus study the problem of learning a much\nfaster feed-forward editor function f. To do so, we first\nconsider a pair of encoder-decoder functions h : \u03b8 7\u2192 r\nand h\u2217 : r 7\u2192 \u03b8, mapping the 3D object \u03b8 to a correspond-\ning latent representation r. We then reduce the problem to\nlearning a latent editor g : (rs, y) 7\u2192 re which performs\nthe edit directly in latent space. Hence, we decompose the\neditor as f|y = h\u2217 \u25e6g|y \u25e6h. This can be advantageous if, by\nexploiting the compactness and structure of the latent space,\nthe latent editor g|y can be fast, efficient, and easy to learn.\nIn the rest of the section, we review important back-\nground (Section 3.1), explain how we build and train the\nlatent editor (Section 3.2), and finally describe a combina-\ntion of 2D priors for global and local edits (Section 3.3).\n3.1. Background\nShap-E: an off-the-shelf 3D latent space.\nInstead of\nlearning a latent space from scratch, we turn to a pre-trained\noff-the-shelf model, Shap-E [23], which is a conditional\ngenerative model of 3D assets that utilises a latent space.\nIt comprises an auto-encoder that maps 3D objects to latent\ncodes as well as a diffusion-based text/image-conditioned\ngenerator that operates in said space.\nIn our work, we\nmainly use the encoder/decoder components, denoted as h\nand h\u2217, respectively, mapping the 3D object from/to a latent\nvector r \u2208 R1024\u00d71024. In an application, the source latent\nrs can be either obtained using h starting from a mesh, or\ncan be sampled from a textual description using the Shape-E\ngenerator. For more details, please refer to the Appendix B.\nScore Distillation Sampling (SDS).\nSDS [53] is a loss\nuseful for distilling diffusion probabilistic models (DPMs).\nRecall that a DPM models a data distribution p(x) by learn-\ning a denoising function \u03f5 \u2248 \u02c6\u03f5(xt; y, t), where xt = \u03b1tx+\n\u03c3t\u03f5 is a noised version of the data sample x. Here (\u03b1t, \u03c3t)\ndefine the noise schedule, \u03f5 \u223c N(0, I) is normally dis-\ntributed, and t = 0, 1, . . . , T are noising steps. The SDS en-\nergy function is given by LSDS(x) = Et,\u03f5\nh\n\u2212 \u03c3t log p(xt)\ni\n,\nwhere p(xt) is the noised version of the data distribution\np(x) and the noise level is picked randomly according to\na distribution w(t). The reason for choosing this distribu-\ntion is that the denoising function is also an estimator of\nthe gradient of the log-posterior log p(xt; y, t), in the sense\nthat \u02c6\u03f5(xt; y, t) = \u2212\u03c3t log p(xt; y, t). Hence, one obtains\nthe gradient estimator\n\u2207xLSDS(x) = Et,\u03f5\nh\n\u02c6\u03f5 (xt; y, t) \u2212 \u03f5\ni\n(1)\nFor 3D distillation, x = R(\u03b8, \u03c0), so the chain rule is used\nto compute the gradient w.r.t. \u03b8 and the loss is also averaged\nw.r.t. random viewpoints \u03c0.\nSHAP-\nEDITOR \nSDS \nLoss\n3D Object\n3D\nEncoder\nSource NeRF\n3D\nDecoder\nRender\nRender\nSource \nLatent\nEdited NeRF\n3D\nDecoder\nEdited \nLatent\nEdited 2D view\nSource 2D view\nTraining only\n\u201cAdd$a$party$hat$\nto$it\u201d\n\u201cAdd$a$party$hat$\nto$the$corgi\u201d\n\u201cA$corgi$wearing$\na$party$hat\u201d\nfor T2I SDS\nfor TI2I SDS\nFigure 2. Latent 3D editing with SHAP-EDITOR. During training, we use the Shap-E encoder to map a 3D object into the latent space.\nThe source latent and a natural language instruction are then fed into an editing network that produces an edited latent. The edited latent\nand original latent are decoded into NeRFs and we render a pair of views (RGB images and depth maps) with the same viewpoint for the\ntwo different NeRF. The paired views are used for distilling knowledge from the pre-trained 2D editors with our design training objective\nto our SHAP-EDITOR. During inference, one only needs to pass the latent code to our SHAP-EDITOR, resulting in fast editing.\n3.2. 3D editing in latent space\nWe now consider the problem of learning the latent editor g\n(i.e., our SHAP-EDITOR), using the method summarised in\nFigure 2 and Algorithm 1. Learning such a function would\nrequire suitable triplets (\u03b8s, \u03b8e, y) consisting of source and\ntarget 3D objects and the instructions y, but there is no such\ndataset available. Like prior works that use test-time opti-\nmisation, we start instead from an existing 2D editor, im-\nplementing the posterior distribution p(xe | xs, y), but we\nonly use it for supervising g at training time, not at test time.\nA benefit is that this approach can fuse the knowledge con-\ntained in different 2D priors into a single model, which, as\nwe show later, may be better suited for different kinds of\nedits (e.g., local vs global).\nTraining the latent editor.\nTraining starts from a dataset\n\u0398 of source 3D objects \u03b8s which are then converted in cor-\nresponding latent codes rs = h(\u03b8s) by utilising the encoder\nfunction h or sampling the text-to-3D generator p(rs | ys)\ngiven source descriptions ys.\nThe latent editor re = g(rs, y) is tasked with mapping\nthe source latent rs to an edited latent re based on instruc-\ntions y. We supervise this function with a 2D editor (or\nmixture of editors) providing the conditional distribution\np(xe |xs, y). Specifically, we define a loss of the form:\nLSDS-E(xe |xs, y) = Et,\u03f5 [\u2212\u03c3t log p(xe\nt | xs, y)] ,\n(2)\nwhere xe\nt = \u03b1txe + \u03c3t\u03f5, and xs = R(h\u2217(rs), \u03c0) and\nxe = R(h\u2217(re), \u03c0) are renders of the object latents rs\nand re, respectively, from a randomly-sampled viewpoint\n\u03c0. Importantly, the rendering functions are differentiable.\nWe choose this loss because its gradient can be computed\ndirectly from any DPM implementation of the 2D editor\n(Section 3.1). At every learning iteration, a new source la-\ntent rs is considered, the edited image xe = R(g(rs, y), \u03c0)\nis obtained, and the gradient \u2207xeLSDS-E(xe |xs, y) is back-\npropagated to g to update it.\nIn practice, we utilise a loss that combines gradients\nfrom one or more 2D image editors, thus combining their\nstrengths. Likewise, we can incorporate in this loss addi-\ntional regularisations to improve the quality of the solution.\nHere we consider regularising the depth of the edited shape\nand appearance of the rendered image. We discuss this in\ndetail in the next Section 3.3.\nThe choice of g.\nRather than learning the function g from\nscratch, we note that Shape-E provides a denoising neural\nnetwork that maps a noised code rs\n\u03c4 = \u03b1\u03c4rs + \u03c3\u03c4\u03f5 to an\nestimate rs \u2248 \u02c6rSE(rs\n\u03c4; y, \u03c4) of the original latent. We thus\nset g(re | rs, y) = \u02c6rSE(r, \u03c4, y), as an initialisation, where\nr = (\u03c3\u03c4rs + \u03b1\u03c4\u03f5, rs) is obtained by stacking the noised\ninput rs with the original latent for a fixed noise level (\u03c3\u03c4 =\n0.308). This encoding is only useful because the network g\nis initialized from Shape-E, and it expects a noisy input. In\nfact, the learned distribution in the original Shap-E is very\ndifferent from the desired editing distribution.\n3.3. 2D editors\nWe consider two types of edits: (i) global edits (e.g., \u201cMake\nit look like a statue\u201d), which change the style of the object\nbut preserve its overall structure, and (ii) local edits (e.g.,\n\u201cAdd a party hat to it\u201d), which change the structure of the\nobject locally, but preserve the rest. To achieve these, we\nlearn our model from a combination of complementary 2D\neditors and regularisation losses. For both edit kinds, we\nadopt a text-guided image-to-image (TI2I) editor for distil-\nlation and consider further edit-specific priors.\nAlgorithm 1 SHAP-EDITOR training\nInput: \u0398: training 3D objects\ng: latent editor initialization (h, h\u2217): auto-encoder,\nL: distillation loss Y: instruction set\nOutput: g: optimized editor\nwhile not converged do\nrs \u2190 h(\u03b8s), \u03b8s \u2208 \u0398\nre \u2190 g(rs, y), y \u2208 Y\n\u25b7 Render objects to RGB and depth\n\u03c0 \u2190 random viewpoint\n(xs, ds) \u2190 R(h\u2217(rs), \u03c0)\n(xe, de) \u2190 R(h\u2217(re), \u03c0)\nUpdate g using the gradient \u2206gL(xs, xe, ds, de)\nend while\n3.3.1\nGlobal editing\nTI2I loss.\nIn order to learn from a pre-trained TI2I model\n(e.g., InstructPix2Pix [3]), we obtain the SDS gradient\n\u2207xeLSDS-TI2I(xe | xs, y) from the TI2I denoising network\n\u02c6\u03f5TI2I(xe\nt; xs, y, t). Note that the latter is conditioned on the\nsource image xs and the editing instructions y. We also use\nclassifier-free guidance (CFG) [19] to enhance the signal of\nthis network for distillation purposes. Please refer to the\nAppendix B for details.\nDepth regularisation for global editing.\nGlobal edits are\nexpected to change the style of an object, but to retain its\noverall shape. We encourage this behaviour via an addi-\ntional depth regularisation loss:\nLreg-global(de, ds) = E\u03c0\n\u0002\n\u2225de \u2212 ds\u22252\u0003\n,\n(3)\nwhere de and ds are the rendered depth maps from a view-\npoint \u03c0 for edited and source objects, respectively.\nOverall\nloss.\nFor\nLglobal(xs, xe, ds, de),\nwe\nuse\na\nweighted combination of LSDS-TI2I and Lreg-global.\n3.3.2\nLocal editing\nFor local edits, we use LSDS-TI2I as before, but also consider\nadditional inductive priors, as follows.\nT2I loss.\nCurrent 2D editors often struggle to edit images\nlocally, sometimes failing to apply the edit altogether. To\nencourage semantic adherence to the edit instruction, we\nfurther exploit the semantic priors in a text-to-image (T2I)\nmodel, obtaining the SDS gradient \u2207xeLT2I(xe | ye) from\nthe denoising network \u02c6\u03f5T2I(xe\nt; ye, t). Here, the text prompt\nye contains a full description of the edited object (e.g., \u201cA\ncorgi wearing a party hat\u201d), instead of an instruction based\non a reference image. We use CFG for this gradient as well.\nMasked regularisation for local editing.\nTo further en-\nhance the locality of the edits, inspired by the cross-\nattention guidance proposed for controllable generations [6,\n10], we extract the cross-attention maps from the pre-\ntrained TI2I model during the SDS loss calculation. For\ninstance, given a local editing instruction \u201cAdd a party hat\nto the corgi\u201d, we compute the cross-attention maps between\nU-Net features and the specific text embedding for the word\n\u201chat\u201d. These maps are then processed to yield a mask m,\nwhich represents an estimation of the editing region.\nWe can then use the complement of the mask to encour-\nage the appearance of source and edited object to stay con-\nstant outside of the edited region:\nLreg-local(xs, xe, ds, de, m) = E\u03c0\nh\n(1 \u2212 m)\n\u2299\n\u0000\u03bbphoto\u2225xe \u2212 xs\u22252+\u03bbdepth\u2225de \u2212 ds\u22252\u0001i\n,\n(4)\nwhere \u03bbphoto and \u03bbdepth denote corresponding weight factors\nfor the photometric loss \u2225xe \u2212 xs\u22252 and the depth map dif-\nferences \u2225de \u2212 ds\u22252 between source and edited views.\nOverall loss.\nFor Llocal(xs, xe, ds, de, m), we use a\nweighted combination of the LSDS-TI2I, LSDS-T2I and local\nregularisation losses Lreg-local.\n4. Experiments\nIn this section, we provide details of our implementation\nand the evaluation dataset, compare different variants of our\napproach to state-of-the-art instruction-based editing meth-\nods, and study the effect of the various losses in our ap-\nproach via ablation.\n4.1. Dataset and implementation details\nDataset.\nWe construct our 3D object dataset from two\nsources: (i) scanned 3D objects from OmniObject3D [74],\nand (ii) 3D objects generated by Shap-E for specific ob-\nject categories. To ensure the high quality of synthetic 3D\nobjects, we apply additional filtering based on their CLIP\nscores. The resultant training dataset encompasses approx-\nimately 30 classes, each containing up to 10 instances. For\nevaluation, we set up 20 instance-instruction pairs. These\npairs are composed of 5 editing instructions (3 for global\nediting and 2 for local editing) and 15 high-quality 3D ob-\njects which are not included in the training set (8 objects\nfrom Shap-E generation, and 7 from OmniObject3D).\nEvaluation metrics.\nFollowing common practice [36,\n58], we assess edits by measuring the alignment between\ngenerated results and the editing instructions using CLIP\nsimilarity (CLIPsim) and CLIP directional similarity [12]\n(CLIPdir). CLIPsim is the cosine similarity between the\nedited output and the target text prompts.\nCLIPdir first\ncalculates the editing directions (i.e., {target vectors minus\nsource vectors}) for both rendered images and text descrip-\ntions, followed by the evaluation of the cosine similarity\nbetween these two directions. Additionally, to assess struc-\ntural consistency in global editing, we utilise the Structure\nModel\nPer-instance\noptimization\nLocal editing\nGlobal editing\nInference time \u2193\nCLIPsim \u2191\nCLIPdir \u2191\nCLIPsim \u2191\nCLIPdir \u2191\nStructure Dist. \u2193\nText2Mesh [43]\n\u2713\n0.239\n0.058\n0.248\n0.057\n0.073\n\u223c 14 min\nInstruct-NeRF2NeRF [17]\n\u2713\n0.253\n0.051\n0.239\n0.057\n0.095\n\u223c 36 min\nVox-E [58]\n\u2713\n0.277\n0.075\n0.271\n0.066\n0.026\n\u223c 40 min (+ 13 min)\nOurs (Test-time Opt.)\n\u2713\n0.290\n0.087\n0.268\n0.072\n0.013\n\u223c 19 min\nOurs (Single-prompt)\n\u2717\n0.292\n0.097\n0.272\n0.070\n0.008\n\u223c 1 sec\nOurs (Multi-prompt)\n\u2717\n0.279\n0.085\n0.255\n0.062\n0.009\n\u223c 1 sec\nTable 1. Quantitative comparison of our SHAP-EDITOR with other per-instance editing methods. The measured inference time\nexcludes both the rendering process and the encoding of 3D representations. The time inside the bracket indicates the extra time required\nby Vox-E for its refinement step in local editing. Our method achieves superior results within one second on the evaluation dataset.\nVox-E\nIN2N\nText2Mesh\nOurs \n(Single-prompt)\nSource \n\u201cAdd a Santa \nhat to it \u201d\n\u201cMake its color \nlook like rainbow\u201d\n\u201cMake it look like \nmade of gold\u201d\nOurs \n(Test-time Opt.)\nOurs \n(Multi-prompt)\nFigure 3. Qualitative comparison with text-guided 3D editing methods. Both the single-prompt and multi-prompt versions of our\nmethod achieve superior local and global editing results. Our SHAP-EDITOR can preserve the identity of the original assets, such as the\nappearance and shape of the \u201cpenguin\u201d, the fine geometric details of the \u201cvase\u201d, and the structure of the \u201cchair\u201d.\nDistance proposed by [66]. This is the cosine similarity be-\ntween the self-attention maps generated by two images.\nImplementation details.\nWhile training SHAP-EDITOR,\nwe use IP2P [3] as \u02c6\u03f5TI2I for global editing. For local editing,\nwe employ the Stable Diffusion v1-5 model [55] for \u02c6\u03f5T2I\nand MagicBrush [84] (i.e., a fine-tuned version of IP2P with\nenhanced editing abilities for object additions) for \u02c6\u03f5TI2I. All\n3D objects used for evaluation, including those in quantita-\ntive and qualitative results, are \u201cunseen\u201d, i.e., not used to\ntrain and thus optimise the editor. This differs from previ-\nous methods that perform test-time optimisation. Further\nimplementation details are provided in the Appendix B.\n4.2. Comparison to the state of the art\nWe compare our method to other text-driven 3D editors\nsuch as Instruct-NeRF2NeRF (IN2N) [17], Vox-E [59], and\nText2Mesh [43]. Specifically, Instruct-NeRF2NeRF itera-\ntively updates images rendered from a NeRF with a 2D im-\nage editing method (IP2P) and uses the edited images to\ngradually update the NeRF. Vox-E optimises a grid-based\nrepresentation [26] by distilling knowledge from a 2D text-\nto-image model (Stable Diffusion) with volumetric regular-\nisation; a refinement stage is added to achieve localised ed-\nits. Text2Mesh optimises meshes with CLIP similarity be-\ntween the mesh and the target prompt. Since different meth-\nods receive different input formats (NeRF, mesh, and voxel\ngrid), we provided many (\u223c 200) rendered images at 512 \u00d7\n512 resolution for initialising their 3D representations.\nWe consider two variants of SHAP-EDITOR: (i) Ours\n(Single-prompt):\nSHAP-EDITOR trained with a single\nprompt at a time and multiple classes (this is the default set-\nting for our experiments), and (ii) Ours (Multi-prompt):\nSHAP-EDITOR trained with multiple prompts and multiple\nclasses. Finally, we also consider a test-time optimisation\nbaseline (Ours (Test-time Optimisation)), where, instead\nof training an editor function, the Shape-E latent is opti-\nmised directly to minimise the same set of losses.\nQuantitative comparison.\nTable 1 compares methods\nquantitatively.\nBoth the single-prompt and multi-prompt\nvariants of our approach are superior to optimisation-based\n3D editing methods, despite addressing a harder problem,\ni.e., the test 3D assets are not seen during training. The in-\nference of SHAP-EDITOR is near-instantaneous (within one\nsecond) since editing requires only a single forward pass.\nQualitative comparison.\nFigure 3 compares methods\nqualitatively. All prior works struggle with global edits.\nSeen categories\nUnseen categories\n\u201cMake it look \nlike a statue\u201d\n\u201cMake it look \nlike a tiger\u201d\n\u201cMake it wear \na blue sweater\u201d\n\u201cAdd a party \nhat to it\u201d\nFigure 4. Generalisation to unseen categories. \u201cSeen categories\u201d refer to object classes included in the training dataset; the specific\ninstances shown were not used for training. \u201cUnseen categories\u201d represent the object classes that were never encountered during training.\nText2Mesh results in noisy outputs and structural changes.\nIN2N is able to preserve the shape and identity of the orig-\ninal objects but fails to converge for some prompts, such\nas \u201cMake its color look like rainbow\u201d. The reason is that\nedited images produced by IP2P share almost no consis-\ntency under this prompt, which cannot be integrated co-\nherently into 3D. On the other hand, Vox-E successfully\nchanges the appearance of the objects, but due to distilla-\ntion from a T2I model rather than a TI2I model, it fails to\npreserve the geometry.\nWhen local edits are desired, such as \u201cAdd a Santa hat\nto it\u201d (Figure 3, bottom row), Text2Mesh and IN2N do not\nproduce meaningful changes. Text2Mesh mainly changes\nthe texture, and IN2N ignores the instruction entirely. This\ncan be attributed to the inability of their underlying 2D mod-\nels to add or remove objects. Vox-E adds the hat to the\npenguin, but other regions (e.g., nose) also change uninten-\ntionally, despite their spatial refinement stage.\nThe combination of training objectives in our approach\nleverages the complementary aspects of different 2D dif-\nfusion priors, overcoming these problems even while us-\ning feed-forward prediction. Furthermore, the learned ed-\nitor also improves over test-time optimisation results with\nthe same prompt and optimisation objectives. We hypoth-\nesise that this is because learning an editor can regularise\nthe editing process too. Finally, while a single-prompt ed-\nitor achieves the best results, we show that it is possible to\ntrain an editor with multiple prompts (last column) without\ncompromising fidelity or structure.\nFigure 4 provides additional results for various instruc-\ntions, each associated with a single-prompt editor.\nOur\ntrained editors are capable of performing consistent edits\nModel\nCLIPsim \u2191\nCLIPdir \u2191\nStructure Dist. \u2193\nOurs w/o Lreg-global\n0.218\n0.058\n0.138\nOurs\n0.272\n0.070\n0.008\n(a) Ablation study for global editing.\nModel\nCLIPsim \u2191\nCLIPdir \u2191\nOurs w/o cross-attn masks\n0.261\n0.064\nOurs w/o Lreg-local\n0.282\n0.092\nOurs w/o LSDS-T2I\n0.263\n0.067\nOurs w/o LSDS-TI2I\n0.278\n0.096\nOurs\n0.292\n0.097\n(b) Ablation study for local editing.\nTable 2. Quantitative ablation study on loss components.\nacross diverse objects, and, importantly, generalise to un-\nseen categories not included in the training dataset.\n4.3. Ablation study\nQuantitative analysis.\nTable 2a presents the quantitative\nresults for global editing, where the omission of depth regu-\nlarisation leads to a noticeable degradation in performance,\nreflected by high Structure Dist. Likewise, the removal of\nloss components for local editing impairs the model to vary-\ning extents (Table 2b), which we analyse next.\nQualitative analysis.\nIn Figure 5, we illustrate the ef-\nfect of the different model components. For global edit-\ning, eliminating the depth regularisation term (i.e., Ours w/o\nLreg-global) can lead to significant alterations of the source\nshape. For local editing, we observe the following: (i) the\ncross-attn masks specify the editable region where regu-\nlarisation is not applied. If such a region is not defined,\nthe depth and photometric regularisers would be applied to\nOurs \nw/o reg-glo \n\u201cMake it look \nlike a tiger\u201d\n\u201cAdd a \nSanta hat \nto it\u201d\nOurs \nSource \nOurs \nw/o reg-glob \nOurs w/o \ncross-attn  \nSource \n\u201cMake its color \nlook like rainbow\u201d\nOurs \nOurs \nw/o      g-gl \nOurs \nw/o       g-gl \nFigure 5. Qualitative ablation results, where the left and right parts correspond to global and local editing, respectively.\nEffect Strength\n\u201cMake it look like made of steel \u201d\n\u201cMake it wooden\u201d\nNetwork Output\n\u201cMake it look like made of gold \u201d\n\u201cAdd a Santa hat to it\u201d\n\u201cA golden deer wearing a Santa hat\u201d\n\u201cA golden deer\u201d\n\u201cA deer\u201d\n\u201cA deer\u201d\nFigure 6. Top: the strength of the editing effects can be controlled\nvia linear interpolation and extrapolation in latent space. Bottom:\nthe examples in the first row are directly generated by Shap-E and\nthe second row is generated by progressively adding multiple ef-\nfects to the unseen category \u201cdeer\u201d.\nthe whole object, thereby forbidding the formation of lo-\ncal shapes (in this case, the Santa hat); (ii) the regular-\nisation loss (Lreg-local) helps the model to maintain the ob-\nject\u2019s identity (both appearance and shape); (iii) the T2I loss\n(LSDS-T2I) significantly improves the quality of local edit-\ning. When omitted (i.e., Ours w/o LSDS-T2I), only the TI2I\nprior is used, which struggles with localised edits (same is-\nsues that [17, 24] exhibit); (iv) the TI2I loss (LSDS-TI2I) uses\nsource images as references, which greatly helps with un-\nderstanding the layout of edits. Thus, Ours w/o LSDS-TI2I\nleads to spatial inaccuracy in editing (same as [58]).\n4.4. Discussion\nIn Figure 6 (top), we observe that the latent space of Shap-\nE is partially linear. After training the editor to produce\nFigure 7. Unified editing vector. The editing effects can be trans-\nferred via simple vector arithmetic operations in latent space.\nthe desired effects, we can further control the strength of\nthe effects. This could be done by scaling to residual of\nupdated latent and source latent by a factor \u03b7. The editor\u2019s\noutput corresponds to \u03b7 = 1. Increasing (decreasing) \u03b7\nweakens (strengthens) the effects. In Figure 6 (bottom), we\nshow that edits can be accumulated progressively until the\ndesired effect is achieved. Furthermore, as noted in [23]\nand shown in the figure, Shap-E (the first row of the bottom\npart) itself fails at compositional object generation, but our\napproach can largely remedy that by decomposing complex\nprompts into a series of edits. Finally, in Figure 7, we also\nshow that some of the edits, once expressed in latent space,\nare quite linear. By this, we mean that we can find a single\nvector for effects like \u201cMake its color look like rainbow\u201d\nor \u201cTurn it into pink\u201d that can be used to edit any object by\nmere addition regardless of the input latent. This is a strong\nindication that the latent space is well structured and useful\nfor semantic tasks like editing.\nLimitations.\nOur work is based on the latent space of\nShap-E and pre-trained 2D editors, which pose an upper\nbound on quality and performance. Furthermore, while we\nshow that we can learn a latent editor that understands mul-\ntiple instructions, we could not yet achieve a fully open-\nended editor. We conjecture that this might require training\nat a much larger scale than we can afford (i.e., hundreds of\nGPUs vs. a handful).\n5. Conclusion\nWe have introduced SHAP-EDITOR, a universal editor for\ndifferent 3D objects that operates efficiently in latent space.\nIt eschews costly test-time optimisation and runs in a feed-\nforward fashion within one second for any object. SHAP-\nEDITOR is trained from multiple 2D diffusion priors and\nthus combines their strengths, achieving compelling results\nfor both global and local edits, even when compared to\nslower optimisation-based 3D editors.\nEthics.\nWe use the OmniObject3D dataset following their\nterms and conditions. This data contains no personal data.\nFor further details on ethics, data protection, and copy-\nright, please see https://www.robots.ox.ac.uk/\n\u02dcvedaldi/research/union/ethics.html.\nAcknowledgements.\nThis research is supported by ERC-\nCoG UNION 101001212. I. L. is also partially supported\nby the VisualAI EPSRC grant (EP/T028572/1). J. X. is\nsupported by the Clarendon Scholarship. We also appre-\nciate the valuable discussions and support from Paul En-\ngstler, Tengda Han, Laurynas Karazija, Ruining Li, Luke\nMelas-Kyriazi, Christian Rupprecht, Stanislaw Szymanow-\nicz, Jianyuan Wang, Chuhan Zhang, Chuanxia Zheng, and\nAndrew Zisserman.\nReferences\n[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,\nZesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng\nCui. Sine: Semantic-driven image-based nerf editing with\nprior-guided editing field. In CVPR, 2023. 2\n[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In ECCV, 2022. 2\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, pages 18392\u201318402, 2023. 1, 2, 3, 5, 6, 14, 15, 16\n[4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. In SIG-\nGRAPH, 2023. 2\n[5] Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. Neuraled-\nitor: Editing neural radiance fields via manipulating point\nclouds. In CVPR, 2023. 2\n[6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-\nfree layout control with cross-attention guidance. In WACV,\n2023. 2, 5\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2\n[8] Yinbo Chen and Xiaolong Wang.\nTransformers as meta-\nlearners for implicit neural representations. In ECCV, 2022.\n2\n[9] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei\nZhang, Jian Zhang, and Li Yuan. Progressive3d: Progres-\nsively local editing for text-to-3d content creation with com-\nplex semantic prompts. arXiv preprint arXiv:2310.11784,\n2023. 3\n[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. In NeurIPS, 2023. 2, 5\n[11] Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath\nSridhar. Shapecrafter: A recursive text-conditioned 3d shape\ngeneration model. NeurIPS, 2022. 2\n[12] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. Stylegan-nada: Clip-guided domain adap-\ntation of image generators. In SIGGRAPH, 2021. 5\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In ICLR, 2023. 2\n[14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. NeurIPS, 2022. 2\n[15] Bingchen Gong, Yuehao Wang, Xiaoguang Han, and Qi\nDou.\nRecolornerf:\nLayer decomposed radiance field\nfor efficient color editing of 3d scenes.\narXiv preprint\narXiv:2301.07958, 2023. 2\n[16] Ori Gordon, Omri Avrahami, and Dani Lischinski. Blended-\nnerf: Zero-shot object generation and blending in existing\nneural radiance fields. ICCV, 2023. 3\n[17] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions. In ICCV, 2023. 1, 3, 6, 8,\n16\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In ICLR, 2023. 1, 2\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. abs/2207.12598, 2022. 5, 12\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 1,\n2, 3, 15, 16\n[21] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In CVPR, 2022. 2\n[22] Cl\u00b4ement Jambon,\nBernhard Kerbl,\nGeorgios Kopanas,\nStavros\nDiolatzis,\nGeorge\nDrettakis,\nand\nThomas\nLeimk\u00a8uhler.\nNerfshop:\nInteractive editing of neural\nradiance fields.\nProceedings of the ACM on Computer\nGraphics and Interactive Techniques, 6(1), 2023. 2\n[23] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 1, 2, 3, 8, 13\n[24] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato\nIshii, and Takuya Narihira.\nInstruct 3d-to-3d:\nText in-\nstruction guided 3d-to-3d conversion.\narXiv preprint\narXiv:2303.15780, 2023. 1, 3, 8, 16\n[25] Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz\nTrzci\u00b4nski, and Andrea Tagliasacchi. Conerf: Controllable\nneural radiance fields. In CVPR, 2022. 2\n[26] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy\nMitra. Relu fields: The little non-linearity that could. In\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139,\n2022. 6\n[27] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, 2023. 2\n[28] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field dis-\ntillation. NeurIPS, 35:23311\u201323330, 2022. 3\n[29] Adam\nR\nKosiorek,\nHeiko\nStrathmann,\nDaniel\nZo-\nran, Pol Moreno, Rosalia Schneider, Sona Mokr\u00b4a, and\nDanilo Jimenez Rezende. Nerf-vae: A geometry aware 3d\nscene generative model. In ICML. PMLR, 2021. 2\n[30] Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon\nWetzstein, and Kalyan Sunkavalli. Palettenerf: Palette-based\nappearance editing of neural radiance fields. In CVPR, 2023.\n2\n[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 2\n[32] Jae-Hyeok Lee and Dae-Shik Kim.\nIce-nerf: Interactive\ncolor editing of nerfs via decomposition-aware weight op-\ntimization. In ICCV, 2023. 2\n[33] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. NeurIPS, 2022. 3\n[34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Rene Ranftl.\nLanguage-driven semantic seg-\nmentation. In ICLR, 2022. 2\n[35] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training. In CVPR, 2022. 2\n[36] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi\nZhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-\ndriven 3d editing via focal-fusion assembly. arXiv preprint\narXiv:2308.10608, 2023. 3, 5\n[37] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023. 2\n[38] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 2\n[39] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance fields. In ICCV, 2021. 2\n[40] Timo L\u00a8uddecke and Alexander Ecker. Image segmentation\nusing text and image prompts. In CVPR, 2022. 2\n[41] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 2\n[42] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures. In CVPR, 2023. 2\n[43] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In CVPR, 2022. 3, 6\n[44] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or,\nand Ali Mahdavi-Amiri. Sked: Sketch-guided text-based 3d\nediting. In ICCV, 2023. 2\n[45] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[46] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-\nnos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor\nGilitschenski, and Alex Levinshtein. Spin-nerf: Multiview\nsegmentation and perceptual inpainting with neural radiance\nfields. In CVPR, 2023. 2\n[47] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2\n[48] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, 2023. 1, 2\n[49] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 12\n[50] Jangho Park, Gihyun Kwon, and Jong Chul Ye.\nEd-nerf:\nEfficient text-guided editing of 3d scene using latent space\nnerf. arXiv preprint arXiv:2310.02712, 2023. 3\n[51] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In SIGGRAPH, 2023. 2\n[52] Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng,\nShanyan Guan, Bowen Pan, Guangtao Zhai, and Xiaokang\nYang. Cagenerf: Cage-based neural radiance field for gener-\nalized 3d deformation and animation. NeurIPS, 2022. 2\n[53] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,\n2023. 2, 3, 14\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 2\n[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 6\n[56] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 2\n[57] Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman\nShayani, Amir H Khasahmadi, Srinath Sridhar, and Daniel\nRitchie. Clip-sculptor: Zero-shot generation of high-fidelity\nand diverse shapes from natural language. In CVPR, 2023. 2\n[58] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects. In ICCV, 2023. 1, 3, 5, 6, 8\n[59] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects. In ICCV, 2023. 6, 16\n[60] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid representa-\ntion for high-resolution 3d shape synthesis. NeurIPS, 2021.\n2\n[61] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn ICML, pages 2256\u2013\n2265. PMLR, 2015. 1\n[62] Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, and\nTaehyeong Kim. Blending-nerf: Text-driven localized edit-\ning in neural radiance fields. In ICCV, 2023. 3\n[63] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 1\n[64] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 2\n[65] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea\nVedaldi.\nNeural feature fusion fields: 3d distillation of\nself-supervised 2d image representations.\nIn 2022 Inter-\nnational Conference on 3D Vision (3DV), pages 443\u2013453.\nIEEE, 2022. 2\n[66] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali\nDekel. Splicing vit features for semantic appearance transfer.\nIn CVPR, 2022. 6\n[67] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In CVPR, 2023. 2\n[68] Binglun Wang, Niladri Shekhar Dutt, and Niloy J Mitra. Pro-\nteusnerf: Fast lightweight nerf editing using 3d-aware image\ncontext. arXiv preprint arXiv:2310.09965, 2023. 3\n[69] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipula-\ntion of neural radiance fields. In CVPR, 2022. 3\n[70] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,\nDongdong Chen, and Jing Liao. Nerf-art: Text-driven neural\nradiance fields stylization. IEEE TVCG, 2023. 3\n[71] Dongqing Wang, Tong Zhang, Alaa Abboud, and Sabine\nS\u00a8usstrunk.\nInpaintnerf360:\nText-guided 3d inpainting\non unbounded neural radiance fields.\narXiv preprint\narXiv:2305.15094, 2023. 3\n[72] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2\n[73] Silvan Weder, Guillermo Garcia-Hernando, Aron Monsz-\npart, Marc Pollefeys, Gabriel J Brostow, Michael Firman,\nand Sara Vicente. Removing objects from neural radiance\nfields. In CVPR, 2023. 2\n[74] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\nDahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary\n3d object dataset for realistic perception, reconstruction and\ngeneration. In CVPR, 2023. 5, 12\n[75] Shiyao Xu, Lingzhi Li, Li Shen, and Zhouhui Lian. Desrf:\nDeformable stylized radiance field. In CVPR, 2023. 2\n[76] Tianhan Xu and Tatsuya Harada. Deforming radiance fields\nwith cages. In ECCV, 2022. 2\n[77] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.\nLearning object-compositional neural radiance field for ed-\nitable scene rendering. In ICCV, 2021. 2\n[78] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda\nZhang, Zhaopeng Cui, and Guofeng Zhang.\nNeumesh:\nLearning disentangled neural mesh-based implicit field for\ngeometry and texture editing. In ECCV, 2022. 2\n[79] Lu Yu, Wei Xiang, and Kang Han. Edit-diffnerf: Editing\n3d neural radiance fields using 2d diffusion model. arXiv\npreprint arXiv:2306.09551, 2023. 3\n[80] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: geometry editing\nof neural radiance fields. In CVPR, 2022. 2\n[81] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\ntent point diffusion models for 3d shape generation. arXiv\npreprint arXiv:2210.06978, 2022. 2\n[82] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Jus-\ntus Thies, and Michael J Black.\nText-guided generation\nand editing of compositional 3d avatars.\narXiv preprint\narXiv:2309.07125, 2023. 3\n[83] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu,\nEli Shechtman, and Noah Snavely. Arf: Artistic radiance\nfields. In ECCV, 2022. 2\n[84] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing. In NeurIPS, 2023. 2, 6, 15\n[85] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2\n[86] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih\nChen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese,\nStefano Ermon, Caiming Xiong, and Ran Xu. Hive: Har-\nnessing human feedback for instructional visual editing.\narXiv preprint arXiv:2303.09618, 2023. 2\n[87] Chengwei Zheng, Wenbin Lin, and Feng Xu. Editablenerf:\nEditing topologically varying neural radiance fields by key\npoints. In CVPR. 2, 16\n[88] Xingchen Zhou, Ying He, F Richard Yu, Jianqiang Li, and\nYou Li. Repaint-nerf: Nerf editting via semantic masks and\ndiffusion models. arXiv preprint arXiv:2306.05668, 2023. 3\n[89] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. arXiv preprint arXiv:2306.13455, 2023. 3\nAppendix\nA. Overview\nThis Appendix contains the following parts:\n\u2022 Implementation details (Appendix B). We provide full\ndetails regarding the dataset formulation and experimen-\ntal settings.\n\u2022 Additional results (Appendix C). We provide additional\nexamples of our method, including additional global and\nlocal prompts, and we further demonstrate the potential\nof our multi-prompt editor to handle a large number of\nprompts.\n\u2022 Additional ablation study (Appendix D). We provide\nadditional ablation studies of SHAP-EDITOR on the ini-\ntialisation method, the choice of \u03c3\u03c4, and the attention\nmaps used to guide the regularisation loss for local edit-\ning.\n\u2022 Extended discussion on prior methods (Appendix E).\nWe discuss the difference between our SHAP-EDITOR\nand other related 3D editing methods.\n\u2022 Failure cases (Appendix F). We provide failure cases of\nour method qualitatively.\nB. Implementation details\nB.1. Dataset formulation\nIn this section, we provide more details regarding the con-\nstruction of the training and evaluation datasets.\nTraining dataset.\nThe training dataset specifies a set of\n3D objects used to train different instructions. There are in\ntotal 33 object classes, each containing up to 10 instances.\nSpecifically, we use Shap-E-generated objects spanning\n20 object classes: apple, banana, candle, cat, chair, corgi,\ndinosaur, doctor, duck, guitar, horse, microphone, penguin,\npineapple, policeman, robot, teapot, teddy bear, toy plane,\nvase\nIn addition, we use 3D objects from OmniObject3D [74]\nspanning 21 object classes:\nbear, cat, cow, dinosaur,\ndog, duck, elephant, giraffe, guitar, hippo, mouse, panda,\npineapple, rabbit, rhino, scissor, teapot, teddy bear, toy\nplane, vase, zebra\nNote that, we manually filter out the invalid instruction-\ninstance pairs during training. For example, we consider it\nunreasonable to \u201cadd a Santa hat\u201d to \u201cchairs\u201d, thereby dis-\ncarding such pairs. Consequently, we obtain a set of valid\nobject classes for each editing instruction during training,\nas summarised in Table 3.\nEvaluation dataset.\nThe evaluation dataset consists of\n20 high-quality instance-instruction pairs (12 global editing\npairs and 8 local editing pairs), with the details listed in Ta-\nble 4. In summary, there are 3 and 2 editing instructions for\nglobal and local editing, respectively, with 8 Shap-E gener-\nated objects and 7 instances sourced from OmniObject3D.\nNote that none of the instances in the evaluation dataset are\nutilised for training purposes.\nB.2. Experimental details\nShap-E settings.\nThe encoder h takes as input an RGB\npoint cloud (16384 points) and different views (20) of the\n3D asset from random camera angles at 256 \u00d7 256 reso-\nlution. The outputs of the encoder are latents with shape\n1024 \u00d7 1024.\nThe decoder outputs the parameters of a neural field rep-\nresented as a 6-layer MLP. The weights of the first four\nlayers are linear transformations of the latent, while the\nweights of the last two layers are fixed. The output fea-\nture vector computed through the MLP is then mapped to\nthe neural field\u2019s density and RGB values (or alternatively,\nSDF and texture color) using different heads.\nFinally, Shap-E uses a generative latent-space model\nfor which it employs a transformer-based diffusion archi-\ntecture akin to Point-E [49], with latent dimensions of\n1024 \u00d7 1024. It offers two pre-trained conditional diffu-\nsion models: image-conditional and text-conditional. The\nimage-conditional approach, paralleling Point-E, augments\nthe transformer context with a 256-token CLIP embedding.\nThe text-conditional model introduces a single token to the\ntransformer context. We use the text-conditional model in\nour paper.\nSDS with classifier guidance.\nDuring the Score Distilla-\ntion Sampling (SDS) process, we adopt the classifier-free\nguidance [19] to enhance the signal of each underlying 2D\nmodel for distillation purposes. Specifically, for the text-\nguided image-to-image (TI2I) SDS, we define:\n\u02c6\u03f5\u2217\nTI2I(xe\nt; xs, y, t) = \u02c6\u03f5TI2I(xe\nt; \u2205\u2205\u2205, \u2205, t)\n+ \u03b3I \u00b7\n\u0000\u02c6\u03f5TI2I(xe\nt; xs, \u2205, t) \u2212 \u02c6\u03f5TI2I(xe\nt; \u2205\u2205\u2205, \u2205, t)\n\u0001\n+ \u03b3T \u00b7\n\u0000\u02c6\u03f5TI2I(xe\nt; xs, y, t) \u2212 \u02c6\u03f5TI2I(xe\nt; xs, \u2205, t)\n\u0001\n, (5)\nwhere \u03b3I and \u03b3T correspond to image and text guidance\nscales, respectively. Then:\n\u2207xeLSDS-TI2I(xe | xs, y) = Et,\u03f5\nh\n\u02c6\u03f5\u2217\nTI2I(xe\nt; xs, y, t) \u2212 \u03f5\ni\n(6)\nSimilarly, for text-to-image (T2I) SDS,\n\u02c6\u03f5\u2217\nT2I(xe\nt; ye, t) = \u02c6\u03f5T2I(xe\nt; \u2205, t)\n+ \u03b3\u2032\nT \u00b7\n\u0000\u02c6\u03f5T2I(xe\nt; ye, t) \u2212 \u02c6\u03f5T2I(xe\nt; \u2205, t)\n\u0001\n,\n(7)\nwhere \u03b3\u2032\nT denotes the text guidance scale, and\n\u2207xeLSDS-T2I(xe |ye) = Et,\u03f5\nh\n\u02c6\u03f5\u2217\nT2I(xe\nt; ye, t) \u2212 \u03f5\ni\n(8)\nFor global editing, where only TI2I SDS is applied, we\nconsider a default setting of guidance scales (\u03b3I, \u03b3T ) =\n(2.5, 50). For local editing, we adopt the guidance scales\n(\u03b3I, \u03b3T , \u03b3\u2032\nT ) = (2.5, 7.5, 50).\nEditing type\nInstruction\nObject class\nGlobal\n\u201cMake it look like made of gold\u201d\napple, banana, candle, cat, chair, corgi, dinosaur, doctor, duck, guitar, horse,\nmicrophone, penguin, pineapple, policeman, robot, teapot, teddy bear, toy plane, vase;\nbear, cat, cow, dinosaur, dog, duck, elephant, giraffe, guitar, hippo, mouse,\npanda, pineapple, rabbit, rhino, scissor, teapot, teddy bear, toy plane, vase, zebra\nGlobal\n\u201cMake it look like a tiger\u201d\ncat, corgi, dinosaur, duck, horse, penguin, teddy bear; bear, cat, cow, dinosaur,\ndog, duck, elephant, giraffe, hippo, mouse, panda, rabbit, rhino, teddy bear, zebra\nGlobal\n\u201cMake its color look like rainbow\u201d\napple, banana, candle, cat, chair, corgi, dinosaur, doctor, duck, guitar, horse,\nmicrophone, penguin, pineapple, policeman, robot, teapot, teddy bear, toy plane, vase;\nbear, cat, cow, dinosaur, dog, duck, elephant, giraffe, guitar, hippo, mouse,\npanda, pineapple, rabbit, rhino, scissor, teapot, teddy bear, toy plane, vase, zebra\nLocal\n\u201cAdd a Santa hat to it\u201d\ncat, corgi, dinosaur, doctor, duck, horse, penguin, policeman, teddy bear; bear, cat, cow,\ndinosaur, dog, duck, elephant, giraffe, hippo, mouse, panda, rabbit, rhino, teddy bear, zebra\nLocal\n\u201cMake it wear a blue sweater\u201d\ncat, corgi, dinosaur, doctor, duck, horse, penguin, policeman, teddy bear; bear, cat, cow,\ndinosaur, dog, duck, elephant, giraffe, hippo, mouse, panda, rabbit, rhino, teddy bear, zebra\nTable 3. Training dataset formulation. The object classes in bold are sourced from OmniObject3D, whereas the remaining classes are\ngenerated from text prompts using Shap-E.\nEditing type\nInstruction\nInstance\n\u201cMake it look like made of gold\u201d\n\u201cA bird\u201d, \u201cAn apple\u201d, \u201cA scissor\u201d, \u201cA vase\u201d\nGlobal\n\u201cMake it look like a tiger\u201d\n\u201cA cat\u201d, \u201cA corgi\u201d, \u201cA dinosaur\u201d, \u201cA zebra\u201d\n\u201cMake its color look like rainbow\u201d\n\u201cA chair\u201d, \u201cA teapot\u201d, \u201cA guitar\u201d, \u201cA pineapple\u201d\nLocal\n\u201cAdd a Santa hat to it\u201d\n\u201cA corgi\u201d, \u201cA penguin\u201d, \u201cA robot\u201d, \u201cA dinosaur\u201d, \u201cA teddy bear\u201d\n\u201cMake it wear a blue sweater\u201d\n\u201cA corgi\u201d, \u201cA penguin\u201d, \u201cA teddy bear\u201d\nTable 4. Evaluation dataset formulation. The instances in bold are sourced from OmniObject3D, whereas the remaining instances are\ngenerated from text prompts using Shap-E. The specific object instances used for evaluation are not seen during training.\nLoss configuration.\nIn terms of the overall loss for global\nediting, we consider a weighted combination of TI2I and\nglobal regularisation losses,\nLglobal(xs, xe, ds, de) = \u03bbTI2I \u00b7 LSDS-TI2I(xe |xs, y)\n+ \u03bbreg-global \u00b7 Lreg-global(de, ds), (9)\nwith loss scales indicated by \u03bbTI2I and \u03bbreg-global, respec-\ntively.\nFor local editing, we use a weighted combination of the\nTI2I, T2I, and local regularisation losses:\nLlocal(xs, xe, ds, de, m) = \u03bbTI2I \u00b7 LSDS-TI2I(xe |xs, y)\n+\u03bbT2I \u00b7 LSDS-T2I(xe |ye)\n+Lreg-local(xs, xe, ds, de, m), (10)\nwhere \u03bbTI2I and \u03bbT2I denote corresponding loss scales, and\nLreg-local(xs, xe, ds, de, m) is defined by Eq. (4).\nEstimation of local editing region.\nAn estimate of lo-\ncal editing regions can be obtained by extracting the cross-\nattention maps from pre-trained 2D models (i.e., Mag-\nicBrush).\nSpecifically, given an example editing prompt\n\u201cAdd a Santa hat to it\u201d, we first calculate the cross-attention\nmaps between the image features and the word token \u201chat\u201d.\nWe then average all cross-attention maps corresponding to\nfeature resolution 32 \u00d7 32 at a particular timestep t = 600.\nThe averaged map undergoes a series of post-processing\nsteps, including (i) bilinear upsampling to a higher resolu-\ntion at 128 \u00d7 128; (ii) hard thresholding at 0.5; (iii) spatial\ndilation by 10 pixels; (iv) Gaussian blurring by 5 pixels.\nThe final mask m is then adopted as an approximation of\nthe editable region, and used in Eq. (4).\nModel settings.\nAs mentioned previously, we consider\ntwo variants in our method, namely Ours (Single-prompt)\nand Ours (Multi-prompt). We train both methods on ob-\njects from the entire training dataset, to ensure their appli-\ncability across multiple instances of various categories.\nIn terms of the editing instructions, Ours (Single-\nprompt) is considered as our default model and designed\nto handle one prompt at one time. Consequently, it requires\n5 independent models for each of the editing instructions in\nthe evaluation dataset. In contrast, Ours (Multi-prompt) is\ntrained on a combination of editing instructions and is capa-\nble of performing different edits according to the input text\nprompt. We train this multi-prompt model on all 5 instruc-\ntions from the evaluation dataset simultaneously.\nArchitecture details.\nFor SHAP-EDITOR\u2019s architecture,\nwe use a similar network architecture as the text-conditional\ndiffusion model in Shap-E [23], a transformer-based net-\nwork. The original Shap-E text-to-3D network takes a noisy\nlatent \u03c3\u03c4rs + \u03b1\u03c4\u03f5 as input and directly predicts the original\nclean latent rs. Instead, the goal of our editor is to trans-\nform the original latent rs to an edited one. Therefore, to\nsupport (\u03c3\u03c4rs+\u03b1\u03c4\u03f5, rs) as our input, we add additional in-\nput channels to the first linear projection layer. All weights\nare initialised by the weights of the pre-trained Shap-E text-\nto-3D diffusion model, while the weights that apply to the\nadditional input channels are initialized to zeros following\na similar setting to [3].\nRendering details.\nDuring the training phase, camera po-\nsitions are randomly sampled using a circular track. This\ntrack has a radius of 4 units and a constant elevation angle of\n30\u25e6. The azimuth angle varies within the range of [\u2212180\u25e6,\n180\u25e6]. For loss computation, images of the source NeRF\nand edited NeRF are rendered at a resolution of 128 \u00d7 128.\nTraining details.\nWe adopt a constant learning rate of\n1e\u22124, utilising the Adam optimiser with \u03b2 = (0.9, 0.999), a\nweight decay of 1e\u22122, and \u03f5 = 1e\u22128. The batch size is 64.\nWe train our single-prompt and multi-prompt models for\n150 and 500 epochs, respectively. We use the same timestep\nfor \u02c6\u03f5TI2I(xe\nt; xs, y, t) and \u02c6\u03f5T2I(xe\nt; ye, t), which is randomly\nsampled from [0.02, 0.98] following the setting in [53]. We\nalso adopt an annealing schedule for the max timestep. Af-\nter 100 and 300 epochs for the single- and multi-prompt\nversions, respectively, the max timestep of t decreases with\na ratio of 0.8 for every 10 epochs and 50 epochs. The an-\nnealing scheduler helps the model capture more details in\nthe late training.\nWe set \u03bbT2I = \u03bbTI2I = 1, and the regularisation terms\n\u03bbphoto = 1.25 and \u03bbdepth = 0.8 for local editing and\n\u03bbreg-global = 5 for global editing in order to better preserve\nstructure. We also employ a linear warmup schedule for the\nphotometric loss in the early epochs. This helps the model\nto first focus on generating correct semantics, such as \u201ca\npenguin wearing a Santa hat\u201d, and then gradually recon-\nstructing the appearance and shape of the original object\n(e.g., the \u201cpenguin\u201d) with the help of masked loss, i.e. re-\ncovering the identity of the original object. The training\nof each single-prompt model takes approximately 10 GPU\nhours, and the multi-prompt model takes 30 GPU hours, on\nthe NVIDIA RTX A6000.\nEvaluation details.\nDuring evaluation, to compute the\nCLIP metrics and the Structure Distance, we uniformly\nsample 20 viewpoints following the same recipe as in the\ntraining phase. All rendered images are resized to 256\u00d7256\nto ensure a fair comparison across different methods.\nC. Additional Results\nAdditional visualisations.\nFigure 8 provides additional\nvisualised results for our SHAP-EDITOR, with each editing\nprompt associated with a distinct model, i.e., Ours (Single-\nprompt). It is evident that our method is capable of perform-\ning accurate edits across diverse object classes and demon-\nstrates reasonable generalisation to unseen categories.\nScaling up to more prompts.\nWe further explore the pos-\nsibility of learning more prompts within one editor model.\nWe first train a 10-prompt model using the five instruc-\ntions included in the original dataset plus extra five prompts:\n\u201cMake it look like a statue\u201d, \u201cMake it look like made of\nsteel\u201d, \u201cMake it look like made of lego\u201d, \u201cAdd a party hat\nto it\u201d, \u201cAdd rollerskates to it\u201d. We also expand the instruc-\ntions to train a 20-prompt model (which includes the previ-\nous 10 prompts plus \u201cMake it look like a panda\u201d, \u201cMake it\nlook like made of bronze\u201d, \u201cTurn it into blue\u201d, \u201cTurn it into\nyellow\u201d, \u201cTurn it into pink\u201d, \u201cTurn it into green\u201d, \u201cTurn\nit into red\u201d, \u201cAdd a snowboard to it\u201d, \u201cAdd sunglasses\nto it\u201d, \u201cAdd a crown to it\u201d). As shown in Figure 9, the\nperformance decreases slightly when moving from a single\nprompt to more prompts. However, the difference between\n10 prompts and 20 prompts is marginal. This indicates the\npotential of our SHAP-EDITOR to scale to tens of prompts\nand even arbitrary prompts as inputs.\nD. Additional ablation studies\nNetwork initialisation.\nWe compare the editing results\nof SHAP-EDITOR with and without the pre-trained weights\nof the text-to-3D Shap-E diffusion network quantitatively\nand qualitatively under the multi-prompt setting, i.e., Ours\n(Multi-prompt). As shown in Figure 10, the editor trained\nwith Shap-E initialisation can successfully generate differ-\nent effects given different prompts. However, if instead we\nrandomly initialise the network, the editor is unsuccessful,\nyielding similar outputs regardless of the prompt. We as-\nsume this is because the model initialised with Shap-E pre-\ntrained weights inherits its partial abilities to understand the\nnatural language, and the randomly initialised one reaches\na local optimum, ignoring the textual instructions.\nEffects of \u03c3\u03c4.\nNext, we study the value \u03c3\u03c4 that is used\nwhen noising the source latent to be able to initialise the\neditor with the pre-trained weights of the Shap-E diffusion\nmodel. To keep the full information and details of the orig-\ninal 3D asset, we follow [3] and concatenate the noised la-\ntent with the original latent (\u03c3\u03c4rs + \u03b1\u03c4\u03f5, rs). Here, \u03c3\u03c4 is a\nhyperparameter that controls the information we keep from\nthe original latent in the noised counterpart. A smaller \u03c3\u03c4\nmeans we keep less information. The \u03c3\u03c4 value in the main\ntext corresponds to \u03c4 = 200 in the original total 1024 Shap-\nE diffusion steps. A higher \u03c4 corresponds to a smaller \u03c3\u03c4,\nand when \u03c4 = 1024, the noised input can be considered as\na random Gaussian noise. As illustrated in the Figure 11,\ntime steps in the range [200, 600] result in only marginal\ndifferences in performance. A large noise or no noise will\nSeen categories\nUnseen categories\n\u201cMade it look like \nmade of steel\u201d\n\u201cMade it wooden\u201d\n\u201cMake it in \ncyberpunk style\u201d\n\u201cMake it look like \nmade of lego\u201d\n\u201cAdd a crown \nto it\u201d\n\u201cAdd a snowboard \nto it\u201d\nFigure 8. Additional visualisations. We apply different editing instructions (including both global and local edits) across various instances,\nalso demonstrating the generalisability of our method to multiple unseen categories.\n0\n5\n10\n15\n20\nPrompt Amount\n0.20\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP Similarity\nlocal\nglobal\n0\n5\n10\n15\n20\nPrompt Amount\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nCLIP Directional Similarity\nlocal\nglobal\n0\n5\n10\n15\n20\nPrompt Amount\n0.000\n0.004\n0.008\n0.012\n0.016\n0.020\nStructure Distance\nglobal\nFigure 9. Scale up the amount of prompts. We explore the pos-\nsibility of learning a single editor function with tens of prompts.\nAs the amount of prompts increases, the CLIP similarly and CLIP\ndirectional similarity scores decrease. However, both scores reach\na plateau when more prompts are introduced.\nlead to a drop in the CLIP similarity and CLIP directional\nsimilarity scores. Increasing the noise also leads to a larger\nStructure Distance.\nChoice of cross-attention maps.\nTo accurately estimate\nthe local editing region, we assess the quality of cross-\nattention maps extracted at various timesteps from several\npre-trained 2D models, including InstructPix2Pix [3], Mag-\nicBrush [84], and Stable Diffusion v1.5 [20]. As demon-\nstrated in Figure 12, most cross-attention maps either fail\nto disentangle the region of interest from the main ob-\nject or suffer from excessive background noise.\nIn con-\ntrast, the cross-attention map extracted from MagicBrush\nat t = 600 (indicated by red boxes) effectively highlights\nthe region associated with the attention word tokens (i.e.,\n\u201chat\u201d, \u201csweater\u201d). Therefore, we adopt this setting as the\ndefault in our experiments.\nw/ init.\nw/o Init.\nSource\n\u201cAdd$a$Santa$\nhat$to$it\u201d\n\u201cMake$the$color$of$it$\nlook$like$rainbow\u201d\n\u201cMake$it$look$like\nmade$of$gold\u201d\nw/o Init.\nw/ init.\nFigure 10. Comparison with different initialisation method.\nWe use the multi-prompt setting in our experiments. \u201cw/ init.\u201d\ndenotes initialisation using the pre-trained weights of the Shap-\nE text-to-3D diffusion model, and \u201cw/o init.\u201d indicates random\ninitialisation. With random initialisation, the network loses the\nability to distinguish across different prompts and produces simi-\nlar results despite different instructions.\n0\n200\n400\n600\n800\n1000\nTime Step\n0.20\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP Similarity\nlocal\nglobal\n0\n200\n400\n600\n800\n1000\nTime Step\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\nCLIP Directional Similarity\nlocal\nglobal\n0\n200\n400\n600\n800\n1000\nTime Step\n0.000\n0.004\n0.008\n0.012\n0.016\n0.020\nStructure Distance\nglobal\nFigure 11. Ablation study on the timestep \u03c4 for \u03c3\u03c4. We analyse\nthe level of noise introduced to the input, with a large timestep \u03c4\ncorresponding to a large noise. As the timestep \u03c4, thereby the\nnoise level, increases, the Structure Distance rises consistently.\nWhen the input is the original latent (\u03c4 = 0) or has very large noise\n(\u03c4 \u2192 1024), we observe degraded performance in both CLIP sim-\nilarity score and CLIP directional similarity score.\nE. Extended discussion on prior methods\nOur method differs from prior work employing test-time op-\ntimisation techniques in two main ways: (i) We use a direct,\nfeed-forward approach to function on the 3D (latent) repre-\nsentation, unlike others that gradually optimise the 3D rep-\nresentation at test time to align it with a 2D prior. Our ap-\nproach significantly reduces the inference time from tens of\nminutes to less than one second; (ii) Our method learns edit-\ning in a simpler, more structured latent space, avoiding the\ncomplexity of spaces like NeRF\u2019s weight matrix space. This\nsimplification reduces learning difficulty and cost, allowing\nour model to generalise to novel objects at test time. Re-\ncently, EDNeRF [87] tries to edit NeRFs that are trained on\nthe latent space of Stable Diffusion [20]. The loss changes\nt = 200\n  t = 400\n   t = 600\n  t = 800\nSource\nEdited\nTI2I\nInstruct-\nPix2Pix\nTI2I\nMagic-\nBrush\nT2I\nStable \nDiffusion \nv1.5\n \nSource\nEdited\nt = 200\n  t = 400\n   t = 600\n  t = 800\nTI2I\nInstruct-\nPix2Pix\nTI2I\nMagic-\nBrush\nT2I\nStable \nDiffusion \nv1.5\n \n\u201cAdd a Santa hat to it\u201d\n\u201cMake it wear a blue sweater\u201d\nFigure 12. Visualisation of cross-attention maps that correspond\nto particular attention word tokens (labeled in red). These maps are\nextracted at different timesteps (t \u2208 [0, 1000]) from various pre-\ntrained 2D models (including InstructPix2Pix, MagicBrush, and\nStable Diffusion v1.5). In this work, we consider the default cross-\nattention maps at t = 600 from MagicBrush, which are indicated\nby the red boxes.\nfrom the image space to the VAE latent space of Stable Dif-\nfusion. In this context, the use of the term \u201clatent\u201d is differ-\nent from ours since it still requires NeRF as a representation\nof the 3D model and test-time optimisation.\nAnother key difference in our work is the use of com-\nplementary 2D diffusion priors for the training objectives.\nOther methods, such as IN2N [17], Vox-E [59], Instruct\n3D-to-3D [24] typically distill knowledge from one net-\nwork (e.g., Stable Diffusion [20] for Vox-E [3] and Instruct-\nPix2Pix [3] for IN2N and Instruct 3D-to-3D) with different\nregularisation due to different 3D representations.\nAs shown in our ablation studies in the main text, distill-\ning from only one network usually inherits the drawbacks of\nthe original 2D model, such as the inability to edit locally\nor preserve the original appearance and structure. Instead,\none can distill from multiple 2D editors to overcome these\npitfalls and achieve better editing results.\nFinally, we also experiment with the training objective of\nIN2N, i.e., editing images directly and updating our editor\nfunction with a photometric loss. However, this led to di-\nSource \n\u201cMake&it&wear&a&blue&sweater\u201d&&&\u201cAdd&a&Santa&hat&to&it\u201d&&&\nFigure 13. Failure case. When encountering a new class, such\nas \u201cturtle\u201d, which significantly differs from those in the training\ndataset, our model struggles to identify the correct position for\nlocal editing.\nvergence, likely due to the greater inconsistency introduced\nby training with multiple instances compared to optimising\na single NeRF.\nF. Failure case\nIn Figure 13, we present two failure cases. These occur par-\nticularly when the model encounters an unseen class that\nsignificantly differs from the classes the editor was trained\non. This disparity leads to difficulties in accurately deter-\nmining the position for local editing, ultimately resulting in\nediting failures. We conjecture that such failure cases can be\neliminated by training the editor on an even larger number\nof object categories.\n"
  },
  {
    "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking",
    "link": "https://arxiv.org/pdf/2312.09244.pdf",
    "upvote": "3",
    "text": "Helping or Herding?\nREWARD MODEL ENSEMBLES MITIGATE BUT DO NOT\nELIMINATE REWARD HACKING\nJacob Eisenstein1,\u2217\nChirag Nagpal2,\u2217\nAlekh Agarwal2,\u2217\nAhmad Beirami2\nAlex D\u2019Amour1\nDJ Dvijotham1\nAdam Fisch1\nKatherine Heller2\nStephen Pfohl2\nDeepak Ramachandran2\nPeter Shaw1\nJonathan Berant1,\u2217\n1. Google DeepMind\n2. Google Research\n* Core contributors\nreward-ensembles-helping-or-herding@google.com\nABSTRACT\nReward models play a key role in aligning language model applications towards hu-\nman preferences. However, this setup creates an incentive for the language model to\nexploit errors in the reward model to achieve high estimated reward, a phenomenon\noften termed reward hacking. A natural mitigation is to train an ensemble of reward\nmodels, aggregating over model outputs to obtain a more robust reward estimate.\nWe explore the application of reward ensembles to alignment at both training time\n(through reinforcement learning) and inference time (through reranking). First, we\nshow that reward models are underspecified: reward models that perform similarly\nin-distribution can yield very different rewards when used in alignment, due to\ndistribution shift. Second, underspecification results in overoptimization, where\nalignment to one reward model does not improve reward as measured by another\nreward model trained on the same data. Third, overoptimization is mitigated by\nthe use of reward ensembles, and ensembles that vary by their pretraining seeds\nlead to better generalization than ensembles that differ only by their fine-tuning\nseeds, with both outperforming individual reward models. However, even pretrain\nreward ensembles do not eliminate reward hacking: we show several qualitative\nreward hacking phenomena that are not mitigated by ensembling because all reward\nmodels in the ensemble exhibit similar error patterns.\n1\nINTRODUCTION\nTo align machine learning systems with human preferences, it is common to use reward models\nthat are finetuned on preference annotations to score potential outputs by how likely they are to\nbe preferred by human raters (Christiano et al., 2017; Stiennon et al., 2020; Bai et al., 2022; Roit\net al., 2023). There are many ways to use reward models to align policy models: they can act as\ntraining signals in reinforcement learning (Christiano et al., 2017; Stiennon et al., 2020), they can\nselect examples for further imitation learning (Gulcehre et al., 2023; Liu et al., 2023; Dong et al.,\n2023; Touvron et al., 2023), or they can be applied at inference time to steer the output distribution\ntoward higher expected reward (e.g., Yang & Klein, 2021; Gao et al., 2023). Such procedures create\na semi-adversarial dynamic in which the language model is encouraged to produce outputs that\nobtain high reward by exploiting errors in the reward model. Furthermore, while the reward model\n1\narXiv:2312.09244v2  [cs.LG]  21 Dec 2023\nSteak is a high-fat steak, which \nmeans that it\u2019s a lot of calories \nand fat, but that it makes for a \ngood meal. Steak can sit as a side \ndish to a lot of different foods. \nHere\u2019s some popular suggestions: \n- Grilled tomatoes \n- Fried eggs on toast\n- Cheese with mashed potatoes\n- Mushroom and spinach\n- [...]\ny: sampled response\n= +1.85\n= -0.86\n= -2.03\n= -0.88\n= -0.10\n= -0.40\nReward ensemble is e\ufb00ective\n= -1.13 \u2248\nOf \ncourse! \nThere \nare \nlots \nof \ndifferent greetings, and it will be \nreally useful to remember them when \nyou get to know people there. For \nexample, if someone says \"goodbye\" \nor \"thank you,\" you can use these \nexpressions \nto \npolitely \nsay \n\"goodbye\" or \"thank you,\" which \nmeans \"thank you\" or \"thank you \nvery much\" in Dutch. If someone \nthanks you, you can [...]\ny: sampled response\n= +2.83\n= +1.89\n= +2.60\n= +2.60\n= +2.34\n= +2.31\nReward ensemble is ine\ufb00ective\n= -1.04  <<\nHuman: I want to make a nice steak dinner, but I don't know the \ndi\ufb00erence between the various cuts of steak. Assistant: OK, I\u2019m \nhappy to help! I think you want to know which steaks are most \ntender [...] Human: What should we drink with it? Assistant:\nx: context\nHuman: I'm going to the Netherlands and would like to learn a \nfew common phrases in Dutch. Assistant: Sure, would you like \nto learn how to say [...] Human: Could you teach me how to say \n\"Goodbye\" and \"Thank you\"? Assistant:\nx: context\nFigure 1: Left: reward model ensembles can attenuate errors made by individual reward models, in\nthis case the positive r1 for this off-topic response from the policy model \u03c0(y | x), which gets a low\ntrue reward (r\u2217). Right: insufficiently diverse reward models unanimously rate this overly-verbose\nand non-responsive reply from \u03c0(y | x) as positive, but it too gets a low true reward. Both examples\nare real outputs and rewards (here, represented as normalized Z-scores) from best-of-n reranking on\na dataset of dialogue turns rated for helpfulness (Bai et al., 2022); see the paper for details.\nis trained on a fixed set of human preference data, the process of alignment shifts the distribution\nof its inputs, increasing the likelihood of such errors. This phenomenon where the policy language\nmodel exploits reward model errors is often termed reward hacking (Amodei et al., 2016), reward\ngaming (Skalse et al., 2022; Pang et al., 2023), or reward over-optimization (Gao et al., 2023).\nReward hacking has been investigated from several perspectives in prior work (e.g., Krakovna et al.,\n2020; Skalse et al., 2022; Pan et al., 2022). Bai et al. (2022) used reinforcement learning with human\nfeedback (RLHF) and trained two reward models on non-overlapping splits of preference data, using\none to drive alignment, and the other to measure the quality of the outputs. They find that RLHF\nincreases performance according to both the driver and measurement models, but that a performance\ngap emerges as the policy is allowed to diverge from the initial distribution. However, both reward\nmodels were built on base models trained on the same pretraining data, which, as we will show,\nlimits their diversity (as hypothesized by Gleave & Irving (2022)) and thus may understate the effect\nof reward hacking. Other work has simulated the relationship between a \u201ctrue\u201d reward and a learned\nproxy, showing that it is possible to over-optimize the proxy to such an extent that the true reward starts\nto decrease (Gao et al., 2023; Coste et al., 2023). This has been replicated in more realistic settings\nby examining (and creating) spurious correlations in reward model training data (Pang et al., 2023).\nIn this work, we first analyze reward model distribution shift from the perspective of underspec-\nification (D\u2019Amour et al., 2022), which occurs when a machine learning pipeline yields reliable\nperformance on held-out data from the training distribution, but variable performance on out-of-\ndistribution data. When applied to learning reward models from human preference data, we show\nthat reward models that agree in-distribution often disagree when transferred out-of-distribution.\nFurthermore, such disagreements are more pronounced when the reward models are built on different\npretrainings, even when that difference is induced merely by varying the pretraining random seed.\nThese disagreements become increasingly severe when evaluated on outputs of a policy model that\nhas been aligned to a specific reward model. This occurs both when using reward models in RLHF,\nas well as when using an inference-time alignment procedure, best-of-n reranking, where n samples\nare drawn from the policy and then reranked with a reward model.\nMotivated by these findings, we systematically investigate reward model ensembles as a possible\nremedy for reward hacking. Assuming different models err in different ways, ensembling can leverage\n2\nreward uncertainty across the ensemble during alignment (see Figure 1, Left). We explore several\ntechniques for aggregating scores across the ensemble, e.g., taking the median score as a robust\nestimate of the true reward of the policy. We also consider two types of ensembles: pretrain ensembles,\nwhere different members of the ensemble differ in the random seed used during the pretraining phase,\nand finetune ensembles, where members differ only in the random seed used during finetuning. These\nensembles are then evaluated across several types of policies and preference annotations: dialogue\npreferences for a helpful assistant (Bai et al., 2022), summarization quality (Stiennon et al., 2020),\nand whether a single-document summary is grounded in its source text (Roit et al., 2023).\nWe find that pretrain ensembles substantially outperform finetune ensembles. Moreover, they con-\nsistently outperform single reward models, unlike finetune ensembles, which in many cases are\ncomparable to single reward models. However, our analysis also reveals that policies trained with\nensembles are still susceptible to reward hacking: different reward models sometimes share similar\nerror patterns, which in turn propagate to the ensemble (see Figure 1, Right). This is exploited and\namplified by the policy, leading, for example, to outputs that are too short when tuning for factuality,\ntoo verbose when tuning for summarization quality, or responses that follow a particular format that\nis often unsuitable, when training a helpful assistant. Thus, it is possible that methods that, unlike\nensembles, are aware of the distance of outputs from the reward data distribution (Liu et al., 2020)\ncould provide more reliable estimates of uncertainty.\nIn concurrent work, Coste et al. (2023) argue that reward model ensembles effectively mitigate\nreward hacking. Our work shares a similar research question, but differs in several ways, leading\nto more nuanced conclusions. First, we investigate the difference between pretrain and finetune\nensembles, finding that pretrain ensembles are considerably more effective.\nSecond, we use\nhuman-annotated preference data rather than synthetically-generated labels, which provides a more\nrealistic experimental setup. Third, we perform analysis that demonstrates the limitations of reward\nensembles, showing reward ensembles are still susceptible to reward hacking. Last, our experimental\nsetup covers a wider range of tasks, larger reward models, and more extensive policy optimization.\n2\nPRELIMINARIES\nReward models have become the primary tool for aligning LMs towards user-facing applications. We\nnow briefly review how reward models are trained (\u00a72.1) and how they are used for alignment (\u00a72.2).\nWe then describe the experimental setup that we will use for the remainder of the paper (\u00a72.3).\n2.1\nREWARD MODEL TRAINING\nWe focus on the the typical setup where reward models are trained from preference data, (x, y+, y\u2212) \u2208\nD, where y+ is annotated to be preferred over y\u2212 for prompt x.\nUnder the Bradley-Terry\nmodel (Bradley & Terry, 1952), the probability that response y2 is preferred over y1 given a re-\nward function r and a prompt x is p(y1 \u227a y2 | x) = \u03c3(r(x, y2) \u2212 r(x, y1)), where \u03c3(\u00b7) is the\nsigmoid function. Then, we can use preference data to train a reward model by maximizing\nJ (r) = E(x,y+,y\u2212)\u223cD\n\u0002\nlog p(y\u2212 \u227a y+ | x)\n\u0003\n.\n(1)\nThe Bradley-Terry model is underdetermined: for any reward model r\u2217, we can define an equivalent\nreward model, r\u2032(x, y) = r\u2217(x, y) + C(x) where C(x) is a prompt-dependent constant, obtaining\nthe same objective value as r\u2217, i.e., J (r\u2217) = J (r\u2032). This is problematic for ensembling: if different\nreward models choose different values for C(x), then order statistics like median and minimum\nare meaningless. We therefore modify the objective function by adding a regularization term to\nencourage the sum of reward values per preference pair to stay close to zero, i.e.,\nJreg(r) = J (r) + \u03b7 \u00b7 E(x,y+,y\u2212)\u223cD\n\u0002\n(r(x, y+) + r(x, y\u2212))2\u0003\n,\n(2)\nwhere \u03b7 is a small positive value, thereby resolving the issue of underdetermination.\nNote that reward models can also be trained from \u201cpointwise\u201d data, such as toxicity or factuality\nannotations on individual examples (Yang & Klein, 2021; Roit et al., 2023). Such reward models are\nnot underdetermined and so can be aggregated without adjustment.\n3\nTask\nPrompt\nOutput\nTL;DR\nI\u2019ll try to keep this short! **Background** * I\u2019ve always been an on again/off again (very\ncasual!) jogger, typically doing 3 - 5 k * My knees have always been finicky, and I went to a\nphysio who thought I had \u201crunner\u2019s knee\u201d [...] I had my baby a year ago, so all in all I haven\u2019t\nrun for about 1.5 years [...] However, when I run even the tinyiest bit, or even go on a long\nwalk or a hike, my pelvis gets very sore and tight [...]\nI think pregnancy messed with my body, now I\ncan\u2019t even run even the smallest amount with-\nout pain in my pelvis and knees. I\u2019m fairly\ncertain the problem isn\u2019t just that I\u2019m com-\npletely out of shape.\nHelpfulness\nHuman: Assistant, can you help me find books that have really popular antagonists? Assistant:\nSure! I know many books that feature good antagonists, such as those in the Lord of the Rings\nbooks. Human: Who specifically from Lord of the Rings? Assistant:\nWell, Sauron, for example. If you want, I can\nexplain who he was.\nXSum/NLI\nThe ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foun-\ndation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder\nbrothers Efe, 44, Bright, 50 and Stephen, 42. Appearing at the Old Bailey earlier, all four\ndenied the offence. The charge relates to offences which allegedly took place between 2008\nand 2014. Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley,\nare due to stand trial in July. They were all released on bail.\nFormer Premier League footballer Sam Sodje\nhas appeared in court alongside three brothers\naccused of charity fraud.\nTable 1: Prompt-output pairs for the three benchmarks we consider. See \u00a72.3.\n2.2\nALIGNING LANGUAGE MODELS USING REWARD MODELS\nBest-of-n reranking (BoN) is an inference-time alignment strategy, where given a prompt x, we\nsample n generations y1, . . . , yn from a policy language model \u03c0(y | x) and return the generation that\nhas the highest reward according to a reward model r, i.e., y\u2217 = arg maxyk\u2208{y1,...,yn} r(x, yk). The\nKullback\u2013Leibler (KL) divergence of BoN from the initial policy is upper bounded by log n \u2212 n\u22121\nn .\nBoN tends to outperform more elaborate alignment techniques like RLHF in the low-KL regime (Gao\net al., 2023), albeit with the cost of generating multiple samples at inference time.\nReinforcement Learning from Human Feedback (RLHF) is an online reinforcement learning\nmethod that trains a policy language model \u03c0 to maximize expected reward, while staying close to an\ninitial policy, \u03c0sft, which is typically finetuned on supervised data (prompt-output pairs). Distance\nfrom the initial policy is measured with KL divergence, which leads to the regularized objective\nmax\n\u03c0\nEx\u223c\u03c1\ny\u223c\u03c0[r(x, y)] \u2212 \u03bbKL(\u03c0\u2225\u03c0sft),\n(3)\nwhere r is a reward model, \u03c1 is a distribution over prompts, and \u03bb is a hyper-parameter. Typically,\nthis objective is optimized using PPO (Schulman et al., 2017), which we also use in this work.\n2.3\nEXPERIMENTAL SETUP\nDatasets\nWe will examine the performance of reward models (both single models and ensembles)\nacross three tasks. An example from each task is provided in Table 1.\n\u2022 TL;DR: A summarization benchmark where authors summarize their own reddit posts (V\u00f6lske\net al., 2017). We use the preference data created by Stiennon et al. (2020). This benchmark has\nbeen commonly used to evaluate finetuning of policy LMs (Rafailov et al., 2023; Zhao et al., 2023).\n\u2022 HELPFULNESS: A helpful assistant benchmark (Bai et al., 2022), where given a partial conversation\nbetween a human and a digital assistant the goal is to complete the next turn of the assistant. This\nbenchmark has also been commonly used for evaluating finetuned policy LMs (Bai et al., 2022;\nRafailov et al., 2023). We use the base dataset (44K examples), where responses are generated\nfrom a 52B context-distilled LM, and split the training set into two: half for training the reward\nmodel, and half for training the policy model.\n\u2022 XSUM/NLI: We adopt the setup of factually-consistent summarization (Roit et al., 2023), where a\nmodel trained on XSum (Narayan et al., 2018) is finetuned to generate summaries that are consistent\nwith the source document according to a Natural Language Inference (NLI) reward model.\nTraining reward models\nTo examine the effect of pretraining on reward models, we pretrain five\nT5 models from scratch with the base (220M parameters), large (770M), and XL (3B) architectures,\nusing the standard denoising objective over the C4 corpus (Raffel et al., 2020). The pretrained\ncheckpoints differ only in their random seed, which controls parameter initialization and the sample\nfrom the pretraining data. The same pretrained models are used for finetuning across all tasks.\nWe finetune each pretrained model five times using different random seeds across all three benchmarks.\nIn TL;DR and HELPFULNESS we use the aforementioned preference data. For XSUM/NLI, we finetune\n4\nModel Size\nTL;DR\nHELPFULNESS\nXSum/NLI\nT5-BASE\n65.8 \u00b1 0.3\n66.7 \u00b1 0.7\n86.7 \u00b1 0.9\nT5-LARGE\n69.3 \u00b1 0.7\n68.5 \u00b1 0.4\n88.3 \u00b1 1.2\nT5-XL\n71.4 \u00b1 0.8\n69.2 \u00b1 0.6\n91.3 \u00b1 0.5\nT5-XXL\n79.5\n71.5\n92.9\nTable 2: Mean in-distribution accuracy of 25 trained reward models on validation data for TL;DR,\nHELPFULNESS, and XSUM/NLI. Standard deviation is also reported, and observed to be small in-\ndistribution. The single T5-XXL reward model is used for evaluation purposes only.\nNLI models on the ANLI dataset (Nie et al., 2020). Overall we obtain 25 reward models per task (5\npretrain \u00d7 5 finetune). This makes it possible to evaluate the effect of pretraining and finetuning on\nunderspecfication (\u00a73) by constructing ensembles that differ in either pretrain or finetune seed (\u00a74).\nAlignment strategy\nWe use the publicly available T5-large model (Raffel et al., 2020) as a policy\nfor the two summarization tasks. For helpfulness, the task requires substantial background knowledge,\nand thus we use the instruction-tuned PALM-2-XXS model (Anil et al., 2023). Prior to alignment, we\ncreate a finetuned policy \u03c0sft by finetuning on supervised data in the standard manner. We finetune on\nannotated summaries from TL;DR and XSUM/NLI for the corresponding tasks, and on the preferred\nresponses, (x, y+), from the preference data in HELPFULNESS.\nIn BoN reranking, we rerank sampled sets of size n \u2208 {21, 22, . . . , 25} for HELPFULNESS and\n{21, . . . , 26} for TL;DR. Larger sets lead to higher reward at a cost of more expensive inference\nand larger deviation from \u03c0sft. In RLHF, we obtain a trade-off between the KL from \u03c0sft and the\nexpected reward by training multiple times, varying the value of \u03bb. Low values of \u03bb correspond\nto high KL and high reward, while high values of \u03bb entail low KL and low reward. For each value of\n\u03bb we train roughly to convergence using a predetermined fixed number of steps (all hyperparameter\nvalues, including \u03bb and the number of steps, are in Appendix C). Coste et al. (2023) trade-off KL and\nreward by tracking their values during training; however, for any particular value of KL the reward\nmight still be underoptimized during training (i.e., there can exist a different policy \u03c0(y | x) with\nbetter reward, but the same KL(\u03c0(y | x)\u2225\u03c0sft(y | x)), which can be found with longer training).\nEvaluation\nWe use two metrics to quantify generalization of reward models\u2014reward by a larger\nmodel and win rate. Similar to past work (Gao et al., 2023; Coste et al., 2023), we use a larger reward\nmodel to evaluate the generalization of models trained with a smaller reward model. We train a\nT5-XXL reward model by taking the publicly available T5-XXL (Raffel et al., 2020) and finetuning\nit as described above. Table 2 details the performance of reward models of different sizes on the three\ntasks, and it can be seen that T5-XXL outperforms the best T5-XL model. We report both average\nreward of the T5-XXL evaluator as well as win rate, which is the fraction of prompts for which the\nresponse sampled from the aligned policy \u03c0 has higher reward compared to \u03c0sft.\nThe errors of the T5-XXL autoeval model might correlate with errors of the smaller T5 models\nbecause they are trained on the same preference data. For this reason, we also evaluate win rate\naccording to a prompted PALM-2-Large model, which was not exposed to the reward training data\nbut was instruction-tuned on FLAN (Wei et al., 2022). Given a prompt x, we sample a response ysft\nfrom \u03c0sft and yrlhf from \u03c0. We then ask PALM-2 which response is better, using a hand-engineered\nprompt proposed by Rafailov et al. (2023). To avoid position bias we run PALM-2 on the two\npossible orderings (ysft, yrlhf) and (ysft, yrlhf), sample K = 8 outputs for each order and determine\nthe winner on this prompt through majority voting. This style of evaluation has become common\nrecently (Dubois et al., 2023; Singhal et al., 2023) and was shown to correlate well with human\njudgements (Rafailov et al., 2023).\n3\nUNDERSPECIFICATION IN REWARD MODELS\nWe now analyze alignment strategies that use a single reward model, and demonstrate that reward\nmodels are underspecified. First, Table 2 shows the average in-distribution accuracy across the 25\ndifferent reward models, together with the standard deviation (which is low in-distribution).\n5\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nreward\nbase\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\nlarge\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\nxl\nsame pretrain\nself\ndiff pretrain\n(a) TL;DR\n1\n2\n4\n8\n16\n32\nreranking candidates\n1.0\n1.5\n2.0\nreward\nbase\n1\n2\n4\n8\n16\n32\nreranking candidates\nlarge\n1\n2\n4\n8\n16\n32\nreranking candidates\nxl\nself\ndiff pretrain\nsame pretrain\n(b) HELPFULNESS\nFigure 2: Average reward of the best-of-n output, as judged by: the same reward model used for\nranking (self); reward models fine-tuned from the same pretrain as the ranker (same pretrain); reward\nmodels fine-tuned from different pretrains from the ranker (diff pretrain). The reward models that\ndo not share a pretrain with the ranker regard the ranker\u2019s preferred outputs as significantly worse.\nThe story changes, however, when we move to out-of-distribution data. Figure 2 shows the expected\nreward achieved by BoN as a function of the number of sampled candidates, n, for three reward\nmodel scales (KL is approximately log n \u2212 n\u22121\nn ). The dotted green line shows the expected reward\nof the top-ranked output according to the reranker itself, while the dashed orange line shows the\nexpected reward of the same output according to reward models that share a pretrain seed. The solid\nblue line shows the expected reward according to reward models that do not share a pretrain seed.\nUnsurprisingly, the reranker scores its own top outputs more favorably than the other reward models\ndo. However, the reranker\u2019s outputs are scored significantly less favorably by reward models which\ndo not share a pretrain with the ranker. Reward models that share a pretrain seed with the ranker\nmodel overestimate the true reward of the top-ranked output\u2014suggesting that finetune ensembles are\nnot sufficiently diverse because of the shared pretraining state of each of the ensemble\u2019s members.\nNotably, this gap does not disappear with scale, and is present for base, large, and XL models.\nMoving to alignment, differences in estimated rewards induce different policies from the BoN\nstrategy: Figure 3 shows the effects on agreement of the top-ranked summary when reward models\ndo (crosses) or do not (circles) share pretraining seeds. Different reward models tend to produce\ndifferent 1-best outputs. Again these differences are strongly associated with the pretraining seed: for\nexample, two reward models from different pretrains will choose a different best-of-16 output more\nthan half the time for both TL;DR and HELPFULNESS and in all scales.\nLast, Figure 4 analyzes the evolution of agreement of the estimated reward scores when performing\nRLHF on TL;DR for reward models of various scales. Specifically, we align a policy using a single\nreward model, and then measure how well pairs of reward models agree on the ranking of samples\nfrom that policy using Spearman rank correlation. To compute Spearman, we sample 5 completions\nfor each prompt in the validation set from a policy model, at 2K step intervals during RLHF. We\ncompare the agreement between a set of 5 reward models that share the same pre-training seed and a\nset of 5 that do not (both sets include the reward model used to drive RLHF). For each prompt, we\ncompute Spearman correlation across all ten pairs in each set and report the mean correlation over\nthe pairs. The correlation of models that do not share a pretrain is lower compared to models that\nshare a pretrain seed. Moreover, correlation goes down during RLHF, indicating that the uncertainty\nabout the true reward increases as a result of alignment.\n6\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntop-1 agreement\nbase\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\nlarge\n1\n2\n4\n8\n16\n32\n64\nreranking candidates\nxl\nsame pretrain\nchance\ndiff_pretrain\n(a) TL;DR\n1\n2\n4\n8\n16\n32\nreranking candidates\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntop-1 agreement\nbase\n1\n2\n4\n8\n16\n32\nreranking candidates\nlarge\n1\n2\n4\n8\n16\n32\nreranking candidates\nxl\nsame pretrain\nchance\ndiff_pretrain\n(b) HELPFULNESS\nFigure 3: Agreement of the top-ranked output between reward models that do (crosses) and do not\n(circles) share pretraining seeds. Underspecification of reward models directly affects the behavior of\nthe aligned policy. Chance agreement is 1/n.\n10000\n20000\n30000\nRLHF Step\n0.2\n0.3\n0.4\n0.5\n0.6\nMean Rank Correlation\nbase\n10000\n20000\n30000\nRLHF Step\nlarge\n10000\n20000\n30000\nRLHF Step\nxl\nsame ( =0.1)\nsame ( =0.03)\nsame ( =0.01)\ndiff. ( =0.1)\ndiff. ( =0.03)\ndiff. ( =0.01)\nFigure 4: Rank correlation of reward scores for TL;DR reward models that share a pretraining seed\nand models that do not. RLHF alignment increases disagreements between reward models (lower\ncorrelation), particularly at low values of \u03bb and for reward models that do not share a pretrain.\nOverall, our analysis demonstrates that (1) different reward models tend to disagree on out-of-\ndistribution data, particularly when the reward models have different pretraining seeds; (2) this\npropagates to the trained policy model, in the sense that the resulting policy is highly tuned to\nthe preferences of the specific reward model used to drive it; and (3) as a result, the disagreement\nbetween reward models tends to increase during alignment. These findings suggest that reward model\nensembles might mitigate reward hacking, which we turn to next.\n4\nREWARD MODEL ENSEMBLES\nWe describe how to construct reward model ensembles (\u00a74.1), and evaluate their performance (\u00a74.2).\n4.1\nPRETRAIN AND FINETUNE REWARD ENSEMBLES\nWe showed that reward models are underspecified\u2014as they are used more in alignment, they induce\na stronger distribution shift in the outputs of the policy, which in turns leads to higher disagreement\nacross reward models. Thus, a natural mitigation strategy is to ensemble multiple reward models,\nunder the assumption that different models will have different errors. Aggregating over the scores\n7\n2\n4\n8\n16\n32\n64\nreranking candidates\n0.5\n0.0\n0.5\nT5-XXL reward (finetuned)\nbase\n2\n4\n8\n16\n32\n64\nreranking candidates\nlarge\n2\n4\n8\n16\n32\n64\nreranking candidates\nxl\nensemble\npretrain\nfinetune\nsingle RM\naggregator\nmean\nmean_minus_std\nsingle RM\n(a) TL;DR\n2\n4\n8\n16\n32\nreranking candidates\n0.0\n0.2\n0.4\n0.6\n0.8\nT5-XXL reward (finetuned)\nbase\n2\n4\n8\n16\n32\nreranking candidates\nlarge\n2\n4\n8\n16\n32\nreranking candidates\nxl\nensemble\npretrain\nfinetune\nsingle RM\naggregator\nmean\nmean_minus_std\nsingle RM\n(b) HELPFULNESS\nFigure 5: In best-of-n reranking, pretrain ensemble reward models significantly improve the quality\nof outputs in the TL;DR summarization task (top) and the HELPFULNESS task, as measured by a\nT5-XXL model. Full numerical results are in Appendix A.\nof the ensemble members will help when some of the ensemble members erroneously assign high\nreward to a bad output.\nGiven a set of reward models\nM,\nwe define the reward of the ensemble to be\nr(x, y) = agg({rm(x, y)}m\u2208M),\nwith agg indicating an aggregation function (Dietterich,\n2000; Lakshminarayanan et al., 2017; Raffel et al., 2020; Zaidi et al., 2021). Intuitively, the aggrega-\ntion function should be conservative, and return a lower score when there is disagreement between the\nensemble members. We consider the following simple aggregation function: MEAN, MEDIAN, and\nMEAN_MINUS_STD, which subtracts the standard deviation of the reward from the mean to penalize\nhigh variance. We also experiment with MIN, but overall find it to be inferior to the alternatives.\nWe evaluate two types of reward ensembles: pretrain ensembles, where each member was pretrained\nusing a different random seed,1 and finetune ensembles, where all members share the same pretraining\nseed, but use a different seed when finetuned on the reward data (which typically includes preference\npairs, where one output is preferred over another). In all cases the ensemble contains exactly 5\nindividual reward models. Pretrain ensembles are significantly more expensive to train, but are more\ndiverse and hence likely to lead to a more robust reward estimate. In fact, Gleave & Irving (2022)\nreported negative results when using reward ensembles and hypothesized this is due to ensemble\nmembers sharing the same underlying pretrained model.\n4.2\nEXPERIMENTS\nWe now evaluate reward model ensembles across all tasks. Figure 5 shows the results of ensembling\nin best-of-n reranking, as measured by an XXL-scale fine-tuned reward model. Pretrain ensembles\nconsistently improve performance over individual reward models, especially for higher values of n\nfor both TL;DR and HELPFULNESS. Finetune ensembles, conversely, improve performance in some\ncases and are comparable in others. For example, on TL;DR a pretrain ensemble with the MEAN\naggregator achieves a win rate of 90% over the SFT outputs at the XL scale, while the win rate of a\nfinetune ensemble with the same MEAN aggregator is 87.3%. The win rate of the average individual\nXL-scale reward model is 85.3% (see Table 7). For visual clarity, in Figure 5 we show only two\n1Pretraining does not complete a single epoch over the pretraining data, and thus the data observed by each\nmember of a pretrain ensemble is different (but sampled from the same distribution).\n8\n20\n40\n60\n80 100\nKL\n2\n1\n0\n1\n2\nT5-XXL reward (finetuned)\nbase\n20\n40\n60\n80 100\nKL\nlarge\n20\n40\n60\n80 100\nKL\nensemble_type\nft\npt\nsingle RM\naggregator\nmean\nmedian\nn/a\n(a) TL;DR\n20\n40\n60\n80\nKL\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nT5-XXL reward (finetuned)\nbase\n20\n40\n60\n80\nKL\nlarge\n20\n40\n60\n80\nKL\nxl\nensemble_type\nft\npt\nsingle RM\naggregator\nmean\nmedian\nn/a\n(b) HELPFULNESS\nFigure 6: In RLHF, pretrain ensemble reward models lead to significantly more favorable reward-KL\ntradeoffs, as judged by a T5-XXL autoeval model. Each point corresponds to training of models\nto convergence with a particular value of \u03bb. We show the MEDIAN aggregator here, full numerical\nresults are in Appendix B.\naggregators: MEAN and MEAN_MINUS_STD; see Appendix A for results with other aggregators. In\ngeneral, the differences between aggregators are small, with MEAN usually performing at, or near, the\ntop. More conservative aggregators (MIN and MEAN_MINUS_STD) come out slightly ahead of MEAN\nat the smaller scales on TL;DR, suggesting that high variance may be a bigger issue in this setting.\nFigure 6 shows the KL-reward trade-off of ensemble reward models in RLHF for TL;DR and HELPFUL-\nNESS (evaluated with the finetuned T5-XXL model). In such plots, a better model is one that improves\nreward and/or reduces the value of KL from the original SFT policy (Gao et al., 2023; Coste et al.,\n2023). Indeed, similar to BoN, pretrain ensembles consistently outperform both finetune ensembles\nas well as the average individual model. We present results for the MEDIAN and MEAN aggregators\nfor visual clarity, and report full numerical results in Appendix B. In RLHF, KL values are much\nhigher than BoN (which is bounded by \u2248 3.17 for n = 64). Consequently, in this setting we witness\nexplicit reward hacking, in which the T5-XXL rewards decrease even as the RLHF objective improves.\nThis happens most prominently for individual models, in many cases for finetune ensembles, and\nmost rarely for pretrain ensembles\u2014where T5-XXL reward scores decrease only when RLHF uses\na T5-Base reward model. Thus, our experiments on real data yield more negative conclusions than\nCoste et al. (2023) about the potential of ensembles to eliminate reward overoptimization.\nBecause the T5-XXL autoeval model is trained on the same data distribution as the reward models\nused for best-of-n and RLHF, it may overstate their performance. For this reason, we also use a\nzero-shot autoeval model (PALM-2-Large), as described in Section 2.3. Because this evaluation is\nmore computationally expensive, we apply it only to the largest-scale reward models (XL). Results\nare shown in Figure 7. Ensemble reward models consistently achieve higher win rates on both\ntasks and with both alignment techniques. For best-of-n, pretrain ensembles get significantly higher\nwin rates on TL;DR at n = 64 (p < .001 by a permutation test); on HELPFULNESS the differences\n9\n2\n4\n8\n16\n32\n64\nreranking candidates\n0.60\n0.65\n0.70\n0.75\n0.80\nPalm-2 win rate\nranker\npt ensemble\nft ensemble\nsingle RM\n(a) BoN + TL;DR\n2\n4\n8\n16\n32\nreranking candidates\n0.60\n0.65\n0.70\n0.75\n0.80\nPalm-2 win rate\nranker\npt ensemble\nft ensemble\nsingle RM\n(b) BoN + HELPFULNESS\n10\n20\n30\n40\n50\n60\n70\nKL\n60\n65\n70\n75\n80\n85\n90\n95\nPalm-2 win rate\nensemble_type\nft\npt\nsingle RM\naggregator\nmean\nn/a\n(c) RLHF + TL;DR\n10\n20\n30\n40\n50\n60\n70\n80\nKL\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\nPalm-2 win rate\nensemble_type\nft\npt\nsingle RM\naggregator\nmedian\nn/a\n(d) RLHF + HELPFULNESS\nFigure 7: Using a prompted autoevaluator (PALM-2-FLAN), ensemble reward models offer signifi-\ncantly better win rates on both TL;DR and HELPFULNESS. Here all reward models are XL-scale.\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0 17.5\nKL\n2.00\n1.75\n1.50\n1.25\n1.00\n0.75\n0.50\n0.25\nT5-XXL reward (finetuned)\nft\npt\nsingle RM\nFigure 8: XSUM/NLI KL-reward tradeoff for pretrain ensembles, finetune ensembles, and individual\nmodels. Reward is measured with T5-XXL. Both pretrain and finetune ensembles slightly improve\nover individual models.\nbetween ensembling techniques are not significant at n = 32. On both tasks, single reward models\nare significantly worse, p < .001. For RLHF, pretrain ensembles generally achieve better or equal\nwin rates at lower KL divergence from the reference policy, with particularly strong performance on\nHELPFULNESS. Overall, these results mirror the T5-XXL evaluation, with one interesting difference:\nthe PALM-2 autoeval model reveals more reward hacking for RLHF, where win rate decreases with\nKL. This suggests that fine-tuned autoevaluators can overestimate performance when they are trained\non the same preference data as the alignment reward models.\nFigure 8 shows RLHF results for XSUM/NLI. Here we see a relatively small improvement for ensem-\nbles compared to individual models, and a very small difference between pretrain and finetune ensem-\nbles. We conjecture this is because XSUM/NLI optimizes for a particular aspect of the response, namely\nits factuality. This allows all models to find simple and similar strategies that lead to high reward (for\nexample, emitting short responses with limited content), and thus ensembling does not lead to large\ngains in performance. We further elaborate on this when discussing limitations of ensembles in \u00a75.\n10\n5\nWHEN DO REWARD MODEL ENSEMBLES FAIL?\nWe saw that ensembles improve performance according to automatic evaluation metrics. We now con-\nduct a complementary analysis that illustrates that, for some types of errors, ensembling is ineffective.\nWhen all reward models share a similar error pattern, this error propagates to the ensemble. Systematic\nerrors across ensemble members can arise due to biases in the finite reward model training data.\nTo demonstrate this, we manually analyze ensemble outputs to detect frequent errors, and then\nperform a qualitative analysis. Figure 9 shows the results of this analysis on all three benchmarks.\nThe x-axis corresponds to outputs of the model after training for a certain number of steps, and the\ny-axis is a statistic of interest (e.g., average output length). We plot the statistic value for the pretrained\nensemble (using MEAN as a representative aggregation function) and for its members. In addition, for\nTL;DR and HELPFULNESS, where the reward model is trained on the preference data, we show the\nstatistic value on the preference data validation set, conditioned on the label \u2018Preferred\u2019 or \u2018Rejected\u2019.\n\u2022 For HELPFULNESS (Figure 9a), outputs tend to be in a format of a list, and thus we write a regular\nexpression that captures this format. The fraction of outputs that have this pattern increases to\nroughly 50% for 3 members of the ensemble and to the ensemble itself. Looking at the preference\ndata, we do not detect a tendency to produce list outputs in the preferred responses, as the fraction\nof outputs that matches this format is roughly 8% for both the preferred and rejected responses.\n\u2022 For TL;DR (Figure 9b), RLHF alignment leads to longer summaries (Singhal et al., 2023) and also\noutputs that are more extractive, i.e., copy more from the input. Summary length in characters\ngrows substantially for the ensemble and all its members, where for the ensemble, length increases\nby a factor of two. On the preference data, indeed preferred responses are slightly longer than\nrejected responses, but much shorter than outputs post-RLHF. We also compute the longest common\nsubsequence (in characters) between the document and the summary and find that it increases for\nthe ensemble from 28.2 to 49.1. Again, the tendency for copying from the document already occurs\nin the preference data to a small degree, but is amplified by RLHF.2\n\u2022 For XSUM/NLI (Figure 9c), training for factuality tends to make summaries shorter. Additionally,\nprecise numbers are typically omitted from the summaries. Figure 9 shows how all members of the\nensemble and the ensemble itself exhibit this phenomenon, with length in characters decreasing\nrapidly, as well as the fraction of examples that contain any numeric value whatsoever.\nOverall, these qualitative findings are symptoms of the tendency for different pretrain reward models\nto learn to associate certain features with high reward. Policy models can then exploit this association,\nand use these features to produce outputs that are dramatically different from the reward training data,\nand that achieve (spuriously) high reward for both single reward models and the ensemble.\nWhy does this happen for both single reward models and reward model ensembles? As one indication,\nLakshminarayanan et al. (2017) have proposed distance-awareness, i.e., the ability to quantify the\ndistance of an example from the training set, as a necessary condition for achieving good uncertainty\nestimates. They showed in a synthetic binary classfication setup that deep ensembles provide good\nestimates when examples are on the decision boundary, but underestimate uncertainty in areas that are\nfar from the training distribution. In LM alignment, the policy can shift the output distribution away\nfrom the decision boundary to areas where all reward models erroneously extrapolate in the same\nmanner. While we focus on ensembles in this work, we hypothesize that the same phenomenon will\noccur in other approaches for uncertainty estimation that are not distance-aware, such as Monte-Carlo\nDropout (Gal & Ghahramani, 2016) and Epistemic Neural Networks (Osband et al., 2021).\n6\nCONCLUSION\nIn this work, we investigate reward model ensembles as a method for mitigating reward hacking.\nWe find that diversity of the reward ensemble is crucial, and that a pretrain ensemble that contains\nmembers that do not share a pretrain seed leads to stronger generalization during alignment when\ncompared to an ensemble whose members share a pretrain seed. However, reward ensembles are not\nalways effective\u2014for example, we find that they can still assign reward based on spurious correlations\n2The distribution of outputs in the preference data is not identical to the distribution of outputs before RLHF,\nand therefore the statistics after zero training steps do not necessarily match those of the preference data.\n11\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nThousand of traning steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nList fraction\nEnsemble\nMember 1\nMember 2\nMember 3\nMember 4\nMember 5\nPreferred\nRejected\n(a) HELPFULNESS. Fraction of answers containing lists (as matched by a regular expression).\n0\n5\n10\n15\n20\n25\n30\nThousand of traning steps\n30\n40\n50\n60\n70\nLongest common subsequence\nEnsemble\nMember 1\nMember 2\nMember 3\nMember 4\nMember 5\nPreferred\nRejected\n0\n5\n10\n15\n20\n25\n30\nThousand of traning steps\n150\n200\n250\n300\n350\n400\nLength\nEnsemble\nMember 1\nMember 2\nMember 3\nMember 4\nMember 5\nPreferred\nRejected\n(b) TL;DR. Left: extractiveness, as measured by average longest common substring between the summary and\nthe context document. Right: length.\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nThousand of traning steps\n20\n40\n60\n80\n100\n120\nLength\nEnsemble\nMember 1\nMember 2\nMember 3\nMember 4\nMember 5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nThousand of traning steps\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nFraction with number\nEnsemble\nMember 1\nMember 2\nMember 3\nMember 4\nMember 5\n(c) XSUM/NLI. Left: length. Right: specificity, as measured by fraction of numerical tokens in the output.\nFigure 9: Limitations of reward model ensembles. The x-axis is number of RLHF steps, the y-axis\nplots different statistics of the average validation output at that step, and the curves correspond to the\npretrain ensemble (solid blue) and its members (dashed orange). For preference data, we plot the\nsame statistics conditioned on the preference data label (Preferred vs. Rejected). On HELPFULNESS\n(\u03bb = 0.05, top), the ensemble tends to return a list of items. On TL;DR (center, \u03bb = 0.01), summaries\nbecome longer and copy longer spans from the original document. For XSUM/NLI (\u03bb = 0.03, bottom),\nresponses are short and less specific, as measured by lack of numerical information. In HELPFULNESS\nand TL;DR, the statistics of the \u201caligned\u201d outputs are far from their values in the preference data.\n12\nbetween the input and the label. If all members of the ensemble capture the same correlations, the\nensemble will inherit the same undesirable behaviour. In such cases, the policy can exploit this\nvulnerability and shift the distribution towards outputs that overuse this correlation, which results in\nreward hacking. Consequently, reward model ensembles mitigate, but do not fully eliminate, reward\nhacking. Future work should examine methods for uncertainty estimation that are more robust to\nthe type of distribution shift that occurs during alignment, particularly those that are aware of how\ndifferent model policy outputs are from the preference data\u2014such as Gaussian processes (Kuss\n& Rasmussen, 2003; Chu & Ghahramani, 2005; Liu et al., 2020) and conformal prediction under\ncovariate shift (Tibshirani et al., 2019).\nAcknowledgments\nThanks to Sharat Chikkerur, Mohammad Havaei, and the anonymous reviewers\nfor feedback on this paper. The research also benefited from feedback from David Bruns-Smith,\nMing-Wei Chang, Michael Collins, Patrick Fernandez, Mandar Joshi, Rishabh Joshi, Balaji Lak-\nshminarayanan, Kenton Lee, Kristina Toutanova, Victor Veitch, and Zihao Wang. Finally, we\nthank the people who built the infrastructure used in our experiments, including the T5X team and\nL\u00e9onard Hussenot, Johan Ferret, Robert Dadashi, Geoffrey Cideron, Alexis Jacq, Sabela Ramos,\nPiotr Stanczyk, Sertan Girgin, Danila Sinopalnikov, Am\u00e9lie H\u00e9liou, Bobak Shahriari, Bilal Piot, Matt\nHoffmann, Nikola Momchev, and Olivier Bachem.\nREFERENCES\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9.\nConcrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,\nSiddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad\nFienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang\nLi, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\nRalph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. ISSN 00063444. URL\nhttp://www.jstor.org/stable/2334029.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\nWei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In Proceedings of\nthe 22nd international conference on Machine learning, pp. 137\u2013144, 2005.\n13\nThomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help\nmitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.\nAlexander D\u2019Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification\npresents challenges for credibility in modern machine learning. The Journal of Machine Learning\nResearch, 23(1):10237\u201310297, 2022.\nThomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple\nclassifier systems, pp. 1\u201315. Springer, 2000.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and\nTong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. arXiv\npreprint arXiv:2304.06767, 2023.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A simulation framework for methods that\nlearn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model\nuncertainty in deep learning. In international conference on machine learning, pp. 1050\u20131059.\nPMLR, 2016.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, pp. 10835\u201310866. PMLR, 2023.\nAdam Gleave and Geoffrey Irving. Uncertainty estimation for language reward models. arXiv\npreprint arXiv: 2203.07472, 2022.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(ReST) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nVictoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar,\nZac Kenton, Jan Leike, and Shane Legg. Specification gaming: the flip side of ai ingenuity.\nDeepMind Blog, 3, 2020.\nMalte Kuss and Carl Rasmussen. Gaussian processes in reinforcement learning. Advances in neural\ninformation processing systems, 16, 2003.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. Advances in neural information processing systems,\n30, 2017.\nJeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan.\nSimple and principled uncertainty estimation with deterministic deep learning via distance aware-\nness. Advances in Neural Information Processing Systems, 33:7498\u20137512, 2020.\nTianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu.\nStatistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657,\n2023.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, 2018.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nNLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 2020.\nIan Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi,\nXiuyuan Lu, and Benjamin Van Roy. Epistemic neural networks. arXiv preprint arXiv:2107.08924,\n2021.\n14\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping\nand mitigating misaligned models. In International Conference on Learning Representations\n(ICLR), 2022.\nRichard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur Parikh, and He He. Reward\ngaming in conditional text generation. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Vlume 1: Long Papers), 2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, Leonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea,\nPiotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin,\nand Idan Szpektor. Factually consistent summarization via reinforcement learning with textual en-\ntailment feedback. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nPrasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating\nlength correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing\nreward gaming. Advances in Neural Information Processing Systems, 35:9460\u20139471, 2022.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020.\nRyan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal\nprediction under covariate shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u00c1lch\u00e9 Buc,\nE. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2019.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023.\nMichael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining reddit to learn\nautomatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pp.\n59\u201363, 2017.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\nid=gEZrGCozdqR.\n15\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In\nKristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2021.\nSheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, and Yee Teh. Neural\nensemble search for uncertainty estimation and dataset shift. Advances in Neural Information\nProcessing Systems, 34:7898\u20137911, 2021.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiC-HF:\nSequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\n16\nA\nNUMERICAL RESULTS FOR BEST-OF-N RERANKING\nAverage agreement between reward models are shown in Tables 3-6. Autoevaluation results are\nshown in Table 7 and 8.\nk\ndiff pretrain\nsame pretrain\nself\nbase\n1\n0.599\n0.599\n0.599\n2\n0.915\n0.963\n0.981\n4\n1.155\n1.243\n1.275\n8\n1.340\n1.462\n1.507\n16\n1.486\n1.640\n1.696\n32\n1.605\n1.787\n1.854\n64\n1.708\n1.914\n1.991\nlarge\n1\n0.785\n0.785\n0.785\n2\n1.228\n1.328\n1.368\n4\n1.556\n1.732\n1.805\n8\n1.830\n2.069\n2.168\n16\n2.031\n2.330\n2.454\n32\n2.203\n2.552\n2.697\n64\n2.348\n2.744\n2.907\nxl\n1\n0.673\n0.673\n0.673\n2\n1.159\n1.245\n1.309\n4\n1.513\n1.663\n1.780\n8\n1.806\n2.001\n2.157\n16\n2.023\n2.256\n2.449\n32\n2.203\n2.463\n2.686\n64\n2.349\n2.631\n2.881\nTable 3: TL;DR best-of-n agreement.\nk\ndiff pretrain\nsame pretrain\nself\nbase\n1\n0.662\n0.662\n0.662\n2\n1.081\n1.144\n1.178\n4\n1.446\n1.560\n1.621\n8\n1.609\n1.770\n1.855\n16\n1.776\n1.972\n2.078\n32\n1.910\n2.139\n2.267\nlarge\n1\n0.727\n0.727\n0.727\n2\n1.139\n1.190\n1.231\n4\n1.492\n1.580\n1.656\n8\n1.670\n1.791\n1.896\n16\n1.832\n1.979\n2.112\n32\n1.962\n2.134\n2.291\nxl\n1\n0.588\n0.588\n0.588\n2\n1.037\n1.079\n1.134\n4\n1.441\n1.513\n1.609\n8\n1.635\n1.731\n1.866\n16\n1.817\n1.932\n2.098\n32\n1.963\n2.097\n2.293\nTable 4: HELPFULNESS best-of-n agreement.\n17\nk\ndiff pretrain\nsame pretrain\nbase\n1\n1.000\n1.000\n2\n0.811\n0.904\n4\n0.667\n0.825\n8\n0.546\n0.756\n16\n0.447\n0.695\n32\n0.366\n0.637\n64\n0.303\n0.589\nlarge\n1\n1.000\n1.000\n2\n0.780\n0.886\n4\n0.616\n0.794\n8\n0.497\n0.720\n16\n0.394\n0.651\n32\n0.319\n0.593\n64\n0.260\n0.546\nxl\n1\n1.000\n1.000\n2\n0.781\n0.859\n4\n0.618\n0.743\n8\n0.503\n0.655\n16\n0.400\n0.567\n32\n0.323\n0.497\n64\n0.262\n0.433\nTable 5: TL;DR top 1 agreement.\nk\ndiff pretrain\nsame pretrain\nbase\n1\n1.000\n1.000\n2\n0.805\n0.885\n4\n0.650\n0.789\n8\n0.506\n0.695\n16\n0.406\n0.620\n32\n0.318\n0.548\nlarge\n1\n1.000\n1.000\n2\n0.810\n0.874\n4\n0.656\n0.766\n8\n0.522\n0.668\n16\n0.413\n0.579\n32\n0.324\n0.506\nxl\n1\n1.000\n1.000\n2\n0.812\n0.860\n4\n0.666\n0.746\n8\n0.536\n0.635\n16\n0.436\n0.547\n32\n0.345\n0.466\nTable 6: HELPFULNESS top-1 agreement.\n18\nscale\nensemble\nmethod\nreward\nwin rate\nbase\nfinetune\nmean\n\u22120.220\n0.700\nmean minus std\n\u22120.186\n0.703\nmedian\n\u22120.231\n0.700\nmin\n\u22120.177\n0.710\npretrain\nmean\n\u22120.130\n0.721\nmean minus std\n\u22120.086\n0.731\nmedian\n\u22120.155\n0.715\nmin\n\u22120.086\n0.727\nsingle RM\nsingle RM\n\u22120.244\n0.685\nlarge\nfinetune\nmean\n0.342\n0.814\nmean minus std\n0.343\n0.816\nmedian\n0.309\n0.809\nmin\n0.348\n0.813\npretrain\nmean\n0.549\n0.850\nmean minus std\n0.513\n0.847\nmedian\n0.510\n0.846\nmin\n0.475\n0.841\nsingle RM\nsingle RM\n0.280\n0.792\nxl\nfinetune\nmean\n0.695\n0.873\nmean minus std\n0.644\n0.872\nmedian\n0.625\n0.867\nmin\n0.638\n0.868\npretrain\nmean\n0.831\n0.900\nmean minus std\n0.781\n0.895\nmedian\n0.757\n0.889\nmin\n0.735\n0.883\nsingle RM\nsingle RM\n0.585\n0.853\nTable 7: TL;DR BoN (n = 64) autoeval results (T5-XXL fine-tuned evaluator).\n19\nscale\nensemble\nmethod\nreward\nwin rate\nbase\nfinetune\nmean\n0.635\n0.741\nmean minus std\n0.615\n0.735\nmedian\n0.627\n0.738\nmin\n0.604\n0.725\npretrain\nmean\n0.691\n0.752\nmean minus std\n0.661\n0.748\nmedian\n0.683\n0.749\nmin\n0.624\n0.741\nsingle RM\nsingle RM\n0.600\n0.727\nlarge\nfinetune\nmean\n0.778\n0.776\nmean minus std\n0.758\n0.772\nmedian\n0.771\n0.770\nmin\n0.738\n0.766\npretrain\nmean\n0.847\n0.792\nmean minus std\n0.802\n0.779\nmedian\n0.813\n0.784\nmin\n0.770\n0.776\nsingle RM\nsingle RM\n0.730\n0.759\nxl\nfinetune\nmean\n0.884\n0.805\nmean minus std\n0.837\n0.788\nmedian\n0.859\n0.790\nmin\n0.814\n0.788\npretrain\nmean\n0.932\n0.816\nmean minus std\n0.876\n0.797\nmedian\n0.892\n0.798\nmin\n0.858\n0.792\nsingle RM\nsingle RM\n0.811\n0.779\nTable 8: HELPFULNESS BoN (n = 32) autoeval results (T5-XXL fine-tuned evaluator).\n20\nB\nNUMERICAL RESULTS FOR RLHF\nFull RLHF numerical results for HELPFULNESS and TL;DR are shown in Table 9 and Table 10.\nEnsemble\nMethod\n\u03bb\nReward xl\nReward large\nReward base\nft\nmean\n0.010\n2.562\nft\nmean\n0.025\n2.476\n2.204\n0.041\nft\nmean\n0.050\n2.148\n2.089\n1.503\nft\nmean\n0.100\n1.652\n1.591\n1.497\nft\nmean\n0.150\n1.328\n1.258\n1.212\nft\nmean\n0.200\n1.079\n1.032\n0.980\nft\nmean\n0.300\n0.764\n0.688\n0.666\nft\nmean subtract std\n0.010\n2.478\nft\nmean subtract std\n0.025\n2.401\n2.188\n0.240\nft\nmean subtract std\n0.050\n2.118\n1.978\n1.585\nft\nmean subtract std\n0.100\n1.620\n1.525\n1.432\nft\nmean subtract std\n0.150\n1.315\n1.207\n1.152\nft\nmean subtract std\n0.200\n1.089\n0.998\n0.949\nft\nmean subtract std\n0.300\n0.746\n0.667\n0.648\nft\nmedian\n0.010\n2.466\nft\nmedian\n0.025\n2.425\n2.088\n0.153\nft\nmedian\n0.050\n2.154\n2.051\n1.445\nft\nmedian\n0.100\n1.662\n1.585\n1.489\nft\nmedian\n0.150\n1.318\n1.255\n1.197\nft\nmedian\n0.200\n1.096\n1.022\n0.976\nft\nmedian\n0.300\n0.750\n0.699\n0.676\npt\nmean\n0.010\n2.651\npt\nmean\n0.025\n2.551\n2.293\n1.220\npt\nmean\n0.050\n2.196\n2.099\n1.750\npt\nmean\n0.100\n1.724\n1.498\n1.506\npt\nmean\n0.150\n1.319\n1.225\n1.191\npt\nmean\n0.200\n1.106\n1.014\n0.958\npt\nmean\n0.300\n0.759\n0.643\n0.680\npt\nmean subtract std\n0.010\n2.688\npt\nmean subtract std\n0.025\n2.529\n2.293\n0.636\npt\nmean subtract std\n0.050\n2.167\n2.025\n1.696\npt\nmean subtract std\n0.100\n1.695\n1.450\n1.342\npt\nmean subtract std\n0.150\n1.301\n1.188\n1.117\npt\nmean subtract std\n0.200\n1.099\n1.007\n0.932\npt\nmean subtract std\n0.300\n0.732\n0.688\n0.641\npt\nmedian\n0.010\n2.611\npt\nmedian\n0.025\n2.540\n2.383\n1.166\npt\nmedian\n0.050\n2.141\n2.064\n1.662\npt\nmedian\n0.100\n1.674\n1.488\n1.537\npt\nmedian\n0.150\n1.365\n1.181\n1.220\npt\nmedian\n0.200\n1.117\n0.973\n1.014\npt\nmedian\n0.300\n0.743\n0.669\n0.661\nsingle RM\nn/a\n0.010\n2.245\nsingle RM\nn/a\n0.025\n2.321\n1.511\n-0.349\nsingle RM\nn/a\n0.050\n2.024\n1.834\n1.028\nsingle RM\nn/a\n0.100\n1.594\n1.478\n1.432\nsingle RM\nn/a\n0.150\n1.297\n1.194\n1.148\nsingle RM\nn/a\n0.200\n1.069\n0.988\n0.937\nsingle RM\nn/a\n0.300\n0.759\n0.661\n0.636\nTable 9: HELPFULNESS RLHF numerical results.\n21\nEnsemble\nMethod\n\u03bb\nReward xl\nReward large\nReward base\nft\nmean\n0.010\n2.356\n1.562\n-1.310\nft\nmean\n0.030\n2.088\n1.659\n-0.456\nft\nmean\n0.100\n1.073\n0.779\n-0.389\nft\nmean\n0.300\n-0.217\n-0.380\n-0.785\nft\nmean\n0.500\n-0.707\n-0.785\n-0.964\nft\nmean subtract std\n0.010\n2.171\n1.579\n-1.185\nft\nmean subtract std\n0.030\n1.760\n1.533\n-0.392\nft\nmean subtract std\n0.100\n0.811\n0.658\n-0.359\nft\nmean subtract std\n0.300\n-0.303\n-0.409\n-0.777\nft\nmean subtract std\n0.500\n-0.735\n-0.791\n-0.960\nft\nmedian\n0.010\n2.202\n1.480\n-1.939\nft\nmedian\n0.030\n1.943\n1.596\n-0.509\nft\nmedian\n0.100\n0.977\n0.809\n-0.376\nft\nmedian\n0.300\n-0.249\n-0.370\n-0.789\nft\nmedian\n0.500\n-0.715\n-0.788\n-0.963\npt\nmean\n0.010\n2.366\n2.037\n-0.817\npt\nmean\n0.030\n1.997\n1.852\n-0.343\npt\nmean\n0.100\n0.964\n0.858\n-0.366\npt\nmean\n0.300\n-0.309\n-0.377\n-0.776\npt\nmean\n0.500\n-0.744\n-0.786\n-0.962\npt\nmean subtract std\n0.010\n2.398\n2.019\n-0.957\npt\nmean subtract std\n0.030\n1.997\n1.710\n-0.250\npt\nmean subtract std\n0.100\n1.002\n0.768\n-0.328\npt\nmean subtract std\n0.300\n-0.277\n-0.357\n-0.767\npt\nmean subtract std\n0.500\n-0.752\n-0.774\n-0.958\npt\nmedian\n0.010\n2.431\n2.009\n-0.868\npt\nmedian\n0.030\n2.030\n1.903\n-0.317\npt\nmedian\n0.100\n1.086\n0.850\n-0.347\npt\nmedian\n0.300\n-0.308\n-0.388\n-0.778\npt\nmedian\n0.500\n-0.746\n-0.792\n-0.962\nsingle RM\nn/a\n0.010\n1.728\n1.429\n-1.784\nsingle RM\nn/a\n0.030\n1.590\n1.511\n-0.458\nsingle RM\nn/a\n0.100\n0.787\n0.758\n-0.397\nsingle RM\nn/a\n0.300\n-0.299\n-0.387\n-0.783\nsingle RM\nn/a\n0.500\n-0.736\n-0.791\n-0.966\nTable 10: TL;DR RLHF numerical results.\nC\nHYPERPARAMETERS\nWe provide the hyperparameters for reward model and RLHF training in Table 11 and Table 12. For\nreward models, we use the validation set to choose the best checkpoint along training. For RLHF, we\ntake the last checkpoint.\n22\nTask\nParameter\nvalue\nHelpfulness\nLearning rate\n10\u22124\nLearning schedule\nConstant (linear warm-up)\nWarm-up steps\n500\nDropout\n0.05\nBatch size\n64\n\u03b7 (regularization coefficient)\n0.01\nTL;DR\nLearning rate\n10\u22124\nLearning schedule\nConstant (linear warm-up)\nWarm-up steps\n1000\nDropout\n0.05\nBatch size\n32\n\u03b7 (regularization coefficient)\n0.01\nXSum/NLI\nLearning rate\nbase/large: 10\u22123, xl: 3 \u00b7 10\u22123\nLearning schedule\nconstant\nWarm-up steps\n-\nDropout\n0.01\nBatch size\nbase/large: 128, xl: 32\nTable 11: Hyper-parameters for reward model training.\nTask\nParameter\nvalue\nHelpfulness\nPolicy learning rate\n5 \u00b7 10\u22126\nValue learning rate\n10\u22125\nLearning schedule\nConstant (linear warm-up)\nTraining steps\n20000\nWarm-up steps\n2000\nBatch size\nbase/large: 32, xl: 16\ninput length\n1024\noutput length\n256\n\u03bb\n[0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.3]\nTL;DR\nPolicy learning rate\n5 \u00b7 10\u22126\nValue learning rate\n10\u22125\nLearning schedule\nConstant (linear warm-up)\nTraining steps\n20000\nWarm-up steps\n2000\nBatch size\n32\ninput length\n1024\noutput length\n128\n\u03bb\n[0.01, 0.03, 0.1, 0.3, 0.5]\nXSum/NLI\nPolicy learning rate\n5 \u00b7 10\u22126\nValue learning rate\n10\u22125\nLearning schedule\nConstant (linear warm-up)\nTraining steps\n20000\nWarm-up steps\n2000\nBatch size\n32\ninput length\n1024\noutput length\n64\n\u03bb\n[0.01, 0.03, 0.05, 0.1, 0.3, 0.5]\nTable 12: Hyper-parameters for RLHF.\n23\n"
  },
  {
    "title": "TigerBot: An Open Multilingual Multitask LLM",
    "link": "https://arxiv.org/pdf/2312.08688.pdf",
    "upvote": "3",
    "text": "TigerBot: An Open Multilingual Multitask LLM\nYe Chen\nTiger Research\nShanghai, China\nyechen@tigerbot.com\nWei Cai\nTiger Research\nShanghai, China\nwei.cai@tigerbot.com\nLiangmin Wu\nTiger Research\nShanghai, China\nliangmin.wu@tigerbot.com\nXiaowei Li\nTiger Research\nShanghai, China\nxiaowei.li@tigerbot.com\nZhanxuan Xin\nTiger Research\nShanghai, China\nzhanxuan.xin@tigerbot.com\nCong Fu\nTiger Research\nShanghai, China\ncong.fu@tigerbot.com\nAbstract\nWe release and introduce the TigerBot family of large language models (LLMs) 1,\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters.\nWe develop our models embarking from Llama-2 and BLOOM, and push the\nboundary further in data, training algorithm, infrastructure, and application tools.\nOur models yield meaningful performance gain over SOTA open-source models,\ne.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot\nmodel family also achieves leading performance in major academic and industrial\nbenchmarks and leaderboards 2. We believe that TigerBot represents just a snapshot\nof lightning-fast progression in LLM open-source community. Therefore, we are\nthrilled to give back by publicly releasing our models and reporting our approach\nbehind, with additional emphases on building SOTA LLMs in a democratized way\nand making LLMs of use in real-world applications.\n1\nIntroduction\nLarge language models (LLMs) has shown unprecedented promise in a wide range of tasks. Since\nthe phenomenal launch of ChatGPT, there have been breathtaking development in the community,\nmainly following three themes:\n1. Fundamental capabilities, pushed forward by both proprietary models (GPT [4, 19],\nBARD [22], Claude [2]) and open-source models (BLOOM [27], Llama [32,33]).\n2. Computational economics, from data collection (e.g. Alpaca [31]), training (e.g. LoRA [11]),\nquantization (ExLlama [34]), and inference (e.g. TGI [12] and TensorRT [18]).\n3. Application readiness, from APIs, plug-ins, function calling and agents, retrieval-augmented\ngeneration (RAG), long context window, to recently multimodality and role-playing.\nThe mainstream approach to building LLMs has been pretraining decoder-only transformers [35]\non an extensive corpus of unsupervised textual data, followed by alignment with human preferences\nwith labelled demonstration or comparison data, using supervised fine-tuning (SFT) or reinforcement\nlearning with human feedback (RLHF). We have followed the same methodology, albeit made the\nfollowing contributions:\n1web:\nhttps://www.tigerbot.com/chat;\ngithub:\nhttps://github.com/TigerResearch/\nTigerBot\n2As of this writing, TigerBot ranked top-tier open-source models in OpenCompass LLM Leaderboard , and\nCLiB Chinese LLM benchmark leaderboard.\nPreprint. Under review.\narXiv:2312.08688v2  [cs.CL]  15 Dec 2023\n1. A new training data mix with thorough experimental assessment and cleaning.\n2. A stack of novel algorithmic and infrastructural implementations to make our models\nstate-of-the-art (SOTA) in both performance and computational efficiency.\n3. A thorough description of our implementations and observations from the field, in deploying\nour models to real-world applications, which help us prioritize research endeavors.\nBesides achieving superior fundamental capabilities, we are dedicated to democratizing LLM devel-\nopment. To the best of our knowledge, TigerBot only incurs the least amount of computational costs\n(less than two million dollars over the time period April\u2013December, 2023) and carbon footprint to\nproduce probably one of the most comprehensive model families (from 7B to 180B, base and chat,\nwith full stack of tools). This can only happen with an open-source spirit, hence we contribute a\ndetailed elaboration on our methodology and experiences by the same token. Furthermore, we have\ntaken measures to ensure the safety of our models.\n2\nTigerBot models\nWe are open-source releasing our models for free research and commercial use 3, as summarized in\nTable 1, along with a suite of developer tools. Figure 1 shows the training loss for pretraining.\nTable 1: Tigerbot model family\nModel\nBase\nChat\nAPI\nPlug-in\nMulti-modal\nContext-length\n7B\n\u2713\n\u2713\nchat, fine-tune\nsearch\nimage out\n2k\n13B\n\u2713\n\u2713\nchat, fine-tune\nsearch, doc\nimage in/out\n32k\n70B\n\u2713\n\u2713\nchat, fine-tune\nsearch, doc\nimage in/out\n32k\n180B\n\u2713\n\u2713\nchat\nsearch, doc\nimage in/out\n2k\n(a) Training loss for Tigerbot-70b etc.\n(b) Training loss for Tigerbot-180b\nFigure 1: Training loss for Tigerbot models\nBase model is a manifestation of world knowledge, serving as a foundation for downstream fine-\ntuning and applications. Chat model is fine-tuned to perform general-purpose tasks such as chat,\nquestion-answering (QA), generation, and so forth. API is a quick way to tap into TigerBot SOTA\nmodel capabilities from cloud with few lines of codes. Moreover, plug-in\u2019s allow developers and\nusers to leverage the entire internet through modern search engines (search), or their own proprietary\nknowledge base (document).\n2.1\nTraining data\nIn the pretraining stage, our training data consists of about 500 billion tokens, or 1.8TB plaintext data,\nwhich in turn was cleaned, deduped and down-sampled from 5.6TB data. Our data are chosen from\n3TigerBot is released under Apache-2.0 license. However, since we continual pretrained from Llama-2 (7,\n13, 70B) and BLOOM (180B), one shall consult with their open-source licenses respectively.\n2\n25 public and proprietary datasets, based on the following design considerations: (1) good quality,\nin terms of factuality, diversity, and format, based on our months\u2019 iterations and user feedback; (2)\nmultilingual coverage, especially Chinese (e.g., WuDao and WanJuan corpus) and major eastern asian\nlanguages besides English (with zh:en roughly 5:5); and (3) multitask coverage, such as web (C4 and\nRefinedWeb), books (BookCorpus and Clibrary), wikipedia, codes (GitHub and Stack Overflow),\nacademics (arXiv), and domain data (e.g., legal and patent). Table 2 shows our training data mixture\nand their sources, and Figure 2 illustrates proportions of datasets.\nTable 2: Tigerbot training data\nDataset\nSize (GB)\nTokens (B)\nSource\nBooks\nen-books\n100.00\n25.06\nBookCorpus\nzh-books\n154.00\n39.23\nClibrary\nzh-textbook\n2.20\n0.74\nWanJuan\nWebTexts\nen-c4\n80.30\n19.92\nC4\nen-refinedweb\n345.15\n86.80\nRefinedWeb\nen-webtext\n39.00\n10.14\nOpenWebText\nzh-news\n121.00\n27.38\nTigerbot and WanJuan\nzh-webtext\n614.00\n147.59\nWuDao and WanJuan\nPapers\nen-arxiv\n38.00\n12.52\narXiv\nCodes\nen-github\n117.13\n42.84\nGithub\nen-stackoverflow\n24.80\n7.97\nStack Overflow\nWiki\nen-wiki\n21.00\n6.68\nEnglish wikipedia\nzh-wiki\n2.79\n1.72\nChinese wikipedia\nzh-baike\n87.50\n23.00\nTigerbot and WuDao\nja-wiki\n6.80\n2.00\nJapanese wikipedia\nko-wiki\n1.50\n0.53\nKorean wikipedia\nDomain\nen-stackexchange\n6.80\n1.91\nStack Exchange\nzh-law\n35.03\n9.42\nTigerbot and WanJuan\nzh-patent\n17.00\n4.66\nWanJuan\nzh-sentiment\n0.02\n0.01\nCantonese sentiment\nTotal\n1,814.02\n470.12 4\nIn the alignment learning stage, we collected 5 million instruction completion data for SFT, and\n15k human annotated comparison data for RLHF, where 10k examples for rejection sampling and\n5k for direct preference optimization (DPO). Our fine-tuning training data was chosen from about\n30 open and proprietary datasets, and months of human annotation from real user prompts. The\ndata mix is designed to represent a broad range of general-purpose tasks (e.g., generation, QA, and\nbrainstorming), following a similar category in [19] and we further expand to about 100 subcategories\n(e.g., extract\u2013from text to json, classification\u2013sentiment classification).\nWe believe that data quality has a pivotal role in model quality, hence we have taken a systematic\napproach to ensuring data quality. Data from web is usually skewed in quality, even more so for\ncorpus outside English. On the other hand, the mechanism underlying LLMs makes models have\ngood memory. Our experiments find that less than a dozen low-quality examples would have the\nmodel learn and generate suboptimal results. Examples of low-quality data include: casual or oral\nlanguage on the web, jargons in social media, ads, harmful contents, messy format and style, and so\nforth. We developed a set of rules and models to filter out about 10% low-quality data as follows.\nGiven the large volume of the data, we designed the approach to have a complexity of O(n2).\n1. Filter out ill-format data using a set of rules, e.g., too many symbols or digits in prompts.\n2. Dedup using exact string match.\n3. Dedup using sequence simhash + longest common substring.\n4later adding 5% holistic training data to make \u223c500B tokens.\n3\nWebTexts, 1,199.45 , 66%\nBooks, 256.20 , 14%\nCodes, 141.93 , 8%\nWiki, 119.59 , 7%\nPapers, 38.00 , 2%\nDomain, 58.85 , 3%\nDataset in training mix (GB, %)\nWebTexts\nBooks\nCodes\nWiki\nPapers\nDomain\nFigure 2: Training data proportions\n4. Filter out harmful content using an ensemble of three SOTA safety models and a dictionary\nof sensitive words.\n2.2\nTraining method\nWe innovate on the shoulders of our precedents, to fully inherit the data, computational resources,\nand intellectuality laid upfront. Models with 7, 13, and 70B parameters were continual pretrained\nfrom Llama-2 respective variants, whereas 180B was pretrained from BLOOM. We adopt most of the\ntraining setting and model architecture from our precedents, mainly decoder-only transformers [35],\nRoPE [29] and ALiBi [23] positional embedding, SwiGLU [28] and GeLU activation functions,\nrespectively. We made further contributions as elaborated below. Our design objectives are: (1) to put\nforward a SOTA training infrastructure that can yield superior models in a computational economic\nmanner, (2) multilingual coverage especially for Chinese, and (3) instrumental for application\ndevelopment.\nTokenizer\nLlama models have been lacking language representation other than English (e.g.,\nChinese only accounts for 0.13% in their training data), hence we expand tokenizer vocabulary\nin Tigerbot. First, we sample a 100GB corpus of Chinese and major eastern asian languages\n(mainly Japanese and Korean) from our pretraining data. Second, we train a Byte-Pair Encoding\n(BPE) SentenePiece tokenizer [10] using this non-English data to ensure representativeness in final\ntokenizer. Third, we merge the non-English tokenizer with the original Llama-2 one to make our\nfinal tokenizer [8]. The original vocabulary size is 32k, we expand to near but not exceeding 65k, to\navoid doubling storage and IO in downstream tokenized binarized data. We also found that more than\n100GB training data is unnecessary, the character coverage is almost identical; but the peak CPU\nmemory would exceed 2TB, which is beyond mainstream hardware. For our 180B-parameter model,\nwe keep the vocabulary size same as 250k, since BLOOM already has good multilingual coverage.\nTraining framework\nTigerBot model family has been trained using our private fork of Megatron-\nDeepSpeed [16], which implements 3D parallelism by combining ZeRO sharding, data parallelism\n(DP) and pipeline parallelism (PP) from DeepSpeed [25] with tensor parallelism (TP) from Megatron-\nLM [17]. Our fork made several upgrades as follows:\n1. Bring Megatron-DeepSpeed Modeling class up to speed with SOTA architectural ingredients\nincluding CoreAttention, SwiGLU, grouped-query attention (GQA) [1], RoPE [29], and\nflash attention [9] adapted to Llama-2 architecture.\n4\n2. Design a better yet simple algorithm for pipeline partition. Given that a model with N\nattention blocks is divided into M stages, first N mod M stages contain \u2308N/M\u2309 blocks and\nremaining blocks each has \u230aN/M\u230b blocks. Compared to the original implementation where\ntotal stage is limited to several special numbers, our method is more flexible to alleviate the\nproblem of skewed partition.\n3. A set of scripts converting Megatron-DeepSpeed sharded weights to transformers weights\nand vice versa.\nTensor parallelism is particularly critical for training models over 100 billion parameters, since model\nsize over 70B cannot fit into a single GPU while CPU offloading is slow and we want to avoid.\nOn the other hand, more TPs introduce heavier communication and TP across nodes is impractical\ndue to huge inter-node communication. Given a fixed value of TP \u00d7 PP \u00d7 DP, empirically we\nfound smaller TP yields better global efficiency, largely because of heavier communications among\ntensor partitions (parallelizing matrix multiplication) than those of pipeline partitions (layers). Along\ntogether with techniques of GQA, flash attention, gradient accumulation and checkpointing, we have\nbeen able to find optimal configurations for different model sizes under various cluster resources. A\nback-of-the-envelop calculation follows.\nLlama-2-13B pretrain GPU hours is 368,640 with 2TB tokens data, per the Llama-2 paper [33], thus\nwe have: training-tokens/gpu-sec = 1, 507. On the other hand, TigerBot-13B training throughput\nreaches 25.7 examples/sec or equivalently 74.6 sec/iteration, as shown in Figure 3, on a 32\u00d7 A100-\n40G GPU cluster, with a sequence length of 2, 048. Only after a preliminary geometric search of\naforementioned parallelism configurations, we found an optimal setting is: TP=2, PP=8, DP=2,\nper-device-batch-size=2, and global-batch-size=1,920, to reach about 4M tokens global batch. Our\nefficiency reads: training-tokens/gpu-sec = 1, 645 (109% of Llama-2 training). Also considering\nthat Llama-2 used higher-end Meta\u2019s Research Super Cluster (A100-80G, 2TB CPU memory, RDMA\ninter-connection) [20], we believe that TigerBot\u2019s codebase has reached cutting-edge computational\neconomics world wide.\n(a) iteration time vs. tokens\n(b) batch size vs. tokens\n(c) seqlen vs. tokens\nFigure 3: Training efficiency for Tigerbot models\nHolistic training\nIn the pretraining stage, we mix in 2-5% (of pretraining data) instruction comple-\ntion data, preprocessed into an unsupervised format, e.g., {instruction}+\"\\n\"+{response}, and\nremove duplicates from the original pretraining data that overlaps with SFT data knowledge. The\nrationale behind is: instruction completion (e.g., question-answering, generation) is essentially still a\nkind of human language continuation. We want models to learn some patterns of instruction following,\nholistically along with base knowledge during pretraining. The incurred additional computational\ncost is negligible, but the advantages are two folds:\n1. Base models exhibit strong capability to follow instructions, right away before alignment.\nWe performed a quick evaluation on SQuAD2.0 benchmark, and found that Tigerbot-13b-\nbase reached 86% of the next token prediction accuracy as Tigerbot-13b-chat.\n2. Since foundational capabilities (knowledge and instruction following) has been learned\nduring pretraining, alignment learning can be lightweight. This further benefits rapid and\neconomic application deployment in various verticals. Our experiments show that loss\nreaches 95% of convergence after one million examples of SFT training.\nSupervised fine-tuning (SFT)\nThe models are only trained on the response portion of supervised\ntraining data, using largely the same training routine, except data preprocessing. We first introduce\n5\na pair of special tokens to markup instruction and response, respectively, for each example. We\nthen process data examples individually (truncate or pad to maximum sequence length) or grouped\n(concatenate into maximum sequence length) into trainer. For the grouped method, we implement\nattention mask such that attention would not be computed cross examples. SFT data is usually quite\nsparse (few long examples and most within max-seq-length). The grouped method gives 5 to 10\u00d7\nspeedup, albeit the individual way should yield higher accuracy intuitively. However, our experiments\nshow that there is no noticeable performance difference between two methods, we then choose the\ngrouped method in our production runs. We also notice that grouping method may introduce some\ndegree of robustness and generality into the model, analogous to that human also learn from noises.\nReinforcement learning with human feedback (RLHF)\nIn the RLHF stage, we first adopt\nrejection-sampling fine-tune with human-in-the-loop as follows:\n1. Sample 10k prompts from SFT data and real user interactions.\n2. Generate 10 random responses (e.g., with temperature=0.6) for each prompt, using our best\ncandidate chat model.\n3. Rank the generations, of all prompts 90% using reward model (a 13B model fine-tuned for\ntext classification), 5% using human ranking, and 5% using human editing (gold set).\n4. Mix the top-ranked generations into SFT data and perform fine-tuning.\n5. Iterate the above process on a monthly basis, as we collect real user prompts.\nWe further apply the direct preference optimization (DPO) algorithm [24] on top of the rejection-\nsampling fine-tuned weights, using 5k gold comparison data. We choose DPO for it is simple to\nimplement, efficient to train, while performing as well as or better than PPO-based methods. By\nintrinsically treating the LLM to be fit as a reward model, DPO essentially formulates a classification\nproblem for pairwise comparison, with a simple cross-entropy loss as follows [24]:\nLDPO(\u03c0\u03b8; \u03c0ref) = \u2212E(x,yw,yl)\u223cD\n\u0014\nlog \u03c3\n\u0012\n\u03b2 log \u03c0\u03b8(yw|x)\n\u03c0ref(yw|x) \u2212 \u03b2 log \u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)\n\u0013\u0015\n(1)\nwhere \u03c3 is a logistic function, and \u03b2 is a hyperparameter that controls the deviation from the reference\npolicy \u03c0ref. Also as in [24], empirically we found DPO is more stable to train, likely due to its elegant\nformulation and does not need separate networks as in actor-critic style PPO algorithms. Efficient\nand stable training leads to rapid iteration in application deployment.\nLong sequence\nLong sequence capability of a model tends to be important in applications, e.g.,\nlong context window to read a novel, or long generation length to write a book, all at once. Since\ntext data follows a power law, i.e., most is short and few is long, inference with long sequence can\nbe analogous to magnifying a picture. There are two key factors: (1) the resolution of the original\nimage, and (2) the interpolation algorithm. The former is the length of training samples and the latter\nis RoPE extrapolation method.\nDuring training TigerBot, we increase the RoPE base frequency to 500k [37] and group training\nsamples to 4k. Attention parallelism could also be done along the sequence length dimension to allow\ntraining length exceed the limit of total GPU memory on a single node. This may yield extra-long\nsequence, e.g., several hundred thousand for certain applications, but at a higher cost of speed. Tensor\nparallelism between nodes becomes extremely slow due to huge communication, which indirectly\nlimits the sample length in each node. We choose not to mix in special-purpose long-sequence data\nto preserve the generality of our models, but in the alignment stage we observed about 2\u2030 examples\nexceeding 2k tokens.\nDuring the inference phase, length extrapolation is achieved through interpolating RoPE position\nembeddings [29]. The main difference among popular methods like Dynamic and YaRN [21] lies\nin their interpolation techniques. The challenge is how to maintain output consistency. In the\nimplementations of Dynamic and YaRN by Transformers [13] and TGI [12], the approach involves\n\"caching the position embeddings of the longest sequences seen\". Under this implementation, even if\nthe model observes the same input, the output may differ due to variations in the length of the cached\nposition embeddings. Tigerbot addresses this by calculating the sum of the input-token-length\n6\nand max-new-token-length per request. This value is used as a reference when computing scaled\nposition embeddings. This ensures the model\u2019s performance remains consistent when extrapolating\nlengths. The model\u2019s behavior for sequences not exceeding the training length is also preserved. We\nextrapolate the max sequence length to 32k using a RoPE scaling factor of 8.\nQuantization\nQuantizing a LLM involves using a reduced-precision integer representation for\nweights and activations, which might be important for practical considerations of GPU memory\nlimitation and fast inference. We implemented both static and dynamic quantizations.\nIn static quantization, the weights and activations of the model are computed using a calibration\ndataset in advanced. TigerBot models are quantized using ExLlamaV2 [34], which is based on the\nsame optimization method as GPTQ. We demonstrated up to 3\u00d7 speedup and 4\u00d7 memory reduction\nfor Tigerbot-4bit quantized models with negligible loss in accuracy.\nIn dynamic quantization, the weights are still quantized ahead of time post training, but the activations\nare quantized during inference on the fly. In particular, we use 8-bit weight and 16-bit activation\nquantization (W8A16). Our experiments show that 8-bit activation may incur significant accuracy\ndegradation similar to [38], while W8A16 yields a good balance between accuracy and speedup.\nThe dynamic approach has advantages in adapting to various serving hardwares, especially for those\nbottlenecked more by memory bandwidth than compute.\nSafety\nWe have performed safety filtering on the training data, we also take measures to mitigate\nsafety risk during training and inference at runtime for any user-interfacing TigerBot products. We\nadopt a safety category consisting of 5 categories and 31 subcategories. Main categories include:\n1. Violating core values of national and social security\n2. Discriminatory content\n3. Commercial illegal and unregulated activities\n4. Infringement of others\u2019 legitimate rights and interests\n5. Inability to meet the security requirements for some special-purpose services, e.g., medical\ninformation services and critical information infrastructure.\nDuring training, we use human annotation to collect about 40k safety demonstration data, in consul-\ntation with administration guidelines and domain experts. This data is then fed into our alignment\nlearning as well as pretraining per holistic training. The safety training data is refreshed on a monthly\nbasis and reflected into our iterative alignment process. Both data and training level safety measures\nare preventive, while runtime-level safety check is protective.\nDuring runtime inference, user input is examined safety first before feeding into model to generate.\nWould either user input or model generation be detected harmful, our products provide a default yet\nsuggestive response to users. All contents undergo a two-stage examination, first a dictionary of about\n120k sensitive vocabulary, followed by an ensemble of three BERT-based classifiers. These safety\nclassifiers are trained on millions of annotated positive (violating) samples and focused on different\naspects in the aforementioned safety categories. The dictionary is designed to be comprehensive to\nensure high recall, whereas a good precision is achieved by tuning the positive threshold from safety\nclassifiers. The final safety label is a parameterized function of dictionary detection and classifier\noutput, and may well be varied for different domains and applications. Moreover, we have safety team\nto keep our dictionary and classifiers up to date with emerging topics and administration guidelines.\nHyperparameters\nWe pretrained TigerBot models using a global batch size (GBS) of 4M tokens,\nwhile fine-tuned models with a GBS as small as 100\u2013400k tokens. Our experiments show that, given\nhigh quality data, smaller batch for finer-grained updates can yield lower loss, as shown in Figure 4.\nWe pretrained models for one epoch over training data, fine-tuned for two epochs, then followed by\nalignment learning for one epoch.\nWe used the adamW optimizer, with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22125. We applied a cosine decaying\nlearning rate (LR) schedule between [2.0\u22125, 2.0\u22124] for 7B and 13B, and [1.0\u22125, 1.0\u22124] for 70B and\n180B model pretraining. For fine-tuning, the LR is [2.0\u22126, 2.0\u22125]. We used warmup steps close to 1%\nof training steps, a weight decay rate of 0.1, and gradient clipping to 1.0. All trainings were performed\nunder bfloat16 precision and flash attention, except for 180B we used gradient checkpointing instead.\n7\n(a) training loss\n(b) validation loss\nFigure 4: Tigerbot-70b-SFT loss with different batch size\nTraining hardware\nOur training cluster consists of 512\u00d7 A100-40G GPUs (64 nodes \u00d7 8 GPUs),\nequipped with NVLink intra-node GPU interconnects, and RoCE (RDMA over Converged Ethernet)\ninter-node communications. Each node has 64-core Intel Platinum CPU and 1024GB of RAM.\n2.3\nEvaluation\nEvaluating LLMs is a challenging yet critical problem. It serves as a systematic basis for model\nselection, and reveals areas for improvements. However, performing evaluation only at the end of the\ntraining pipeline puts expensive computations at a risk of sinking, possibly just for a small negligence\nin data or bugs in codes. Moreover, developers have found that often there are discrepancies between\nbenchmark metrics and human perception. To address these challenges, we develop a three-stage\nevaluation approach as below.\nTable 3: Tigerbot base model evaluation results\nLang.\nTask\nBenchmark\nTigerBot\nLlama-2\n70B-base\n13B-base\n70B-base\n13B-base\nEn\nCode\nHumanEval\n28.66\n18.29\n31.10\n15.27\nCommonsense\nReasoning\nPIQA\n83.30\n79.33\n82.21\n79.21\nSIQA\n48.77\n48.52\n46.01\n46.32\nHellaSwag\n78.62\n72.64\n79.46\n74.96\nWinoGande\n69.38\n64.17\n69.69\n64.09\nOpenBookQ\n88.60\n72.40\n57.40\n57.00\nReading\nComprehension\nBoolQ\n67.52\n63.18\n69.69\n71.50\nMath\nGSM8K\n65.66\n35.86\n63.99\n28.81\nMulti-choice\nQuestions\nMMLU\n68.68\n55.35\n69.58\n55.81\nAverage (En)\n66.58\n56.64\n63.24\n54.77\nZh\nReading\nComprehension\nCMRC\n85.93\n66.57\n68.97\n76.73\nC3\n77.37\n67.01\n60.16\n47.51\nNatural Language\nInference\nOCNLI\n30.00\n30.60\n30.03\n30.00\nMulti-choice\nQuestions\nC-EVAL\n67.75\n48.46\n49.90\n38.67\nAverage (Zh)\n65.26\n53.16\n52.27\n48.23\n8\nTable 4: Tigerbot chat model evaluation results\nLang.\nTask\nBenchmark\nTigerBot\nLlama-2\n70B-chat\n13B-chat\n70B-chat\n13B-chat\nEn\nCode\nHumanEval\n31.10\n26.83\n26.22\n11.59\nCommonsense\nReasoning\nPIQA\n83.57\n80.09\n80.25\n78.67\nSIQA\n51.89\n49.44\n51.59\n50.97\nHellaSwag\n76.68\n70.63\n77.63\n74.71\nWinoGande\n67.01\n63.30\n68.11\n65.82\nOpenBookQ\n85.00\n67.40\n85.00\n80.00\nReading\nComprehension\nBoolQ\n80.67\n78.87\n78.00\n78.32\nMath\nGSM8K\n84.91\n51.25\n58.91\n54.62\nMulti-choice\nQuestions\nMMLU\n68.03\n55.94\n64.84\n54.61\nAverage (En)\n69.87\n60.42\n65.62\n59.43\nZh\nReading\nComprehension\nCMRC\n85.37\n76.17\n80.06\n74.62\nC3\n75.34\n69.42\n54.85\n51.01\nNatural Language\nInference\nOCNLI\n38.07\n40.17\n36.23\n30.00\nMulti-choice\nQuestions\nC-EVAL\n60.40\n48.89\n44.17\n39.22\nAverage (Zh)\n64.80\n58.66\n53.83\n48.71\n1. During training for major checkpoints, we perform lightweight evaluations for a quick\npreview. We first draw a random sample of 100k train and validation examples from 10 major\nbenchmarks including ARC [6], CommonsenseQA [30], SQuAD 2.0 [26], WebQuestions [3],\nand so forth. We then implemented a next-token prediction accuracy in Transformers\u2019 trainer,\nand a run of evaluation only takes several minutes, on one node, even for 70B and 180B\nmodels.\n2. After training, we conduct a comprehensive evaluation on 13 mainstream benchmarks,\ncovering 8 tasks and languages mainly of English and Chinese. The evaluation datasets\nare designed to cover a broad range of tasks such as math, reasoning, codes, and reading\ncomprehension, and so forth. We report the results for both base and chat models in Tables 3\nand 4, respectively. Base models are tested using 0-shot, and otherwise we follow the\nimplementation of OpenCompass [7] to promote reproducibility.\n3. Furthermore, we carry out human evaluations on our top candidates. Human evaluation\nis usually considered gold standard for assessing natural language generation systems.\nHowever, since LLMs can perform a very wide range of tasks, it is impractical to collect\na comprehensive dataset to yield statistically significant results. The subjective biases\namong human annotators and the nontrivial cost if evaluating iteratively are also important\nconsiderations. We employ human evaluation as a gatekeeper, in conjunction with automated\nbenchmark to select models. We first collect 5k gold set prompts, largely from real user\nquestions and unseen from any upstream process. The gold set is sampled to cover a broad\nrange of tasks as described in Section 2.1, yet reflect real user behaviors (tones, typos, oral\nvocabulary, etc.). We then ask our human annotators to rate on helpfulness and safety using a\n1-5 Likert scale. We develop an evaluation guideline to train our annotators, which includes\ndetailed rules and examples for various types of generations. e.g., factuality for objective\nquestions, diversity for subjective generations. We occasionally found that human evaluation\nresults were not aligned tightly with benchmark metrics, we choose production models\nas a holistic tradeoff, also considering model degradation, serendipity and downstream\napplication desiderata.\n9\n3\nApplications\nIn this section, we elaborate on our implementations of a spectrum of applications, some are tools\nand consumer products, and some are real world deployed applications.\nLong-context question answering\nMany applications involve reading comprehension (summariza-\ntion and question-answering) based on a long context, e.g., reading papers, studying laws, QA based\non an in-house knowledge base, and so forth. Most knowledge-based QA systems are implemented\nas a two-stage retrieval-reader pipeline. A dense retrieval narrows the context down to a small subset\nof passages, followed by a LLM reader to generate answers based on the retrieved contexts [14].\nThis setup has two limitations: (1) it is not suitable for summarizing and inductive questions, where\ncontext should be comprehensive; and (2) a large portion of errors may already occur at the retrieval\nstage, due to practical reasons e.g., noisy and ill-format data.\nAs described in Section 2.2, we have extrapolated the context length to 32k tokens, or approximately\n50k characters as in a 50-page pdf or word document (Tigerbot\u2019s tokenizer has a character-to-token\ncompression rate of 1.5\u00d7 for Chineses and 5\u00d7 for English). This context window is big enough\nfor most ad-hoc knowledge-based QA tasks, therefore we skip the dense retrieval part and take a\none-stop approach as follows.\n1. Segmentation: if input text exceeds the max input length (e.g., 32k), segment it into chunks\nof max length, using line breaks to preserve semantics.\n2. Filtering: upon receiving a user query, zero-shot prompt the LLM as a binary classifier to\nfilter out irrelevant segments, similar to the approach in [5]. We compose the prompt as:\nC:{context}+\"\\n\"+Q:{query}+\"\\n\"+\"Can the above Q be answered by C?\"\n3. Generation:\nfirst generate a response by each candidate segment as intermedi-\nate results, from which then generate the final response using a prompt like:\n{intermediate results}+\"\\n\"+{query}. This recursive approach has been used in\nsummarization task and shown superior performance [36].\nRecursive summarization\nSummarization has been one major NLP task, and now can be seam-\nlessly solved by LLMs. To effectively handle extensively long texts, we employ a recursive summa-\nrization approach, similar to [36] but we solely rely on LLM. We first chunk the input text into smaller\nand manageable segments (within max-input-length), following natural semantic boundaries such\nas section endings and line breaks. We then summarize each segment independently. In the final\nstep, we aggregate these individual summaries, to generate a comprehensive and cohesive final\nsummary. Domain specifics and desired length can be naturally guided by prompts, e.g., with prompt:\n\"Summarize the above article into 200 words, preserving key financials.\"\nFunction calling\nNatural language interface is attractive to many applications, where users give\ninstructions in natural language and systems can understand and perform tasks otherwise requiring\nstructured inputs. The intrinsic capability of natural language understanding of LLMs can be leveraged\nto extract structured data from natural language query, and then to perform downstream tasks, namely\nfunction calling. We design the function calling capabilities relying on the underlying TigerBot\nmodels, and as three steps as follows.\n1. Extraction:\ngiven a function definition,\nthe underlying LLM is prompted to\nextract function arguments from a user query.\nWe compose the prompt as:\nF:{func}+\"\\n\"+Q:{query}+\"\\n\"+\"Extract args from Q per F.\"+\"\\n\"+JSON:.\n2. Calling: we then call the target function with the arguments extracted to get a response.\nFunctions can be in-house systems or third-party APIs from web, e.g., stock quotes and\nweather lookup APIs.\n3. Generation:\nwith the returned function response, we prompt the LLM again as:\n{func response}+\"\\n\"+{query} to get the final answer in natural language.\nThe end-to-end performance of function calling largely relies on the LLM\u2019s capabilities in natural\nlanguage understanding and structural extraction. For this purpose, we intentionally mixed a mild\nportion of general-purpose extraction data in pretraining and fine-tuning. We have observed quite\n10\nsatisfactory performance for some basic function calling tasks, e.g., math calculation and stock quotes\nlookup. Therefore, we believe that with further fine-tuning on domain-specific extraction data, the\nfunction calling capabilities can be of use in many real world applications, particularly in place of\nthose costly and complex legacy systems just for structural extraction.\nOnline search\nLLMs can be augmented by search, to get factoid and real time context, also to\nsome degree to alleviate the hallucination problem. We implement search augmentation as follows.\n1. Preprocess and search: we first preprocess user query to be suitable for modern search\nengines, e.g., removing oral interjections and time reference resolution, then issue the query\nto search engines to get results.\n2. Quality filtering and parsing: we then filter the search results into top 1-3 candidates based\non some quality and timeliness conditions, e.g., site quality and if results within a week\npresent, remove those older than one month. We also parse relevant contents from those\nstructured results, e.g., weather and stock prices.\n3. Generation: we finally generate the response to users by prompting the underlying LLM\nwith: {top search results}+\"\\n\"+{query}.\nRole playing\nGiven its rich embodiment of general knowledge and conversational capability, NPC\n(non-player character) in RPG games can be equipped with LLM to become more entertaining. One\ncommon requirement from gaming applications is to make LLMs act as some roles, e.g., dialogues\nand personal memory. To develop a role-playing LLM, there are two design objectives: (1) to train a\nLLM with role-playing capability, and (2) to adapt the LLM into the context of the game. Meanwhile,\ndevelopers usually want to keep the general capabilities of the LLM to make the NPC more humanlike.\nFurthermore, in practice the approach needs to be somewhat certain, lightweight, and scalable. Our\napproach to role-playing LLM combines fine-tuning and retrieval-augmented generation (RAG) as\nfollows, and the process is illustrated in Figure 5. Our approach was inspired by [15], but we added a\nfine-tuning step to get a gamified foundation.\n1. Fine-tuning: we continual fine-tune a TigerBot chat model on a dataset of general-purpose\nmulti-round role-playing dialogues, e.g., acting as a hero in a novel. Fine-tuning can gauge\nthe model to have role-playing capability, albeit is time consuming and stochastic in nature.\n2. Extraction: given a novel or plot as the background context of a game, we extract dialogues\nbetween characters and summarize their profiles, both using a general-purpose TigerBot\nchat model. Extracted dialogues and profiles are fed into an embedding index as the game\nknowledge base, under a hierarchy beginning with \"role\".\n3. Inference: during runtime inference, given a user\u2019s role and question, we first dense-\nretrieve reference dialogues and profiles from the knowledge base, and then use the\nabove fine-tuned role-playing LLM to generate response. The prompt is composed as:\n{role profiles}+\"\\n\"+{reference dialogues}+\"\\n\"+{question}.\nOur approach is designed such that it can be scaled to different games rapidly and economically. The\nrole-playing capability is general and learned through fine-tuning, while knowledge base extraction\nis specific to game and fast. An example shown in Figure 6 is from a role-playing game developed\nwithin 2-3 days and took one node of 8\u00d7 A100 GPUs.\nIntelligent hardware\nIntelligent hardwares can be equipped with LLMs to first have natural\nlanguage interfaces and potentially to orchestrate other applications using capabilities like function\ncalling. In our practice at this stage, there are three typical requirements as follows.\n1. Instantaneous response: intelligent hardwares, such as speakers, wearables, and electric\nvehicles, are mainly used on-the-go, thus require response time to be near real time. The\nsystem end-to-end response time usually cannot exceed 2s, while there are other processes\ninvolved including ASR, TTS, etc. To achieve fast response, we use streaming generation,\nalong with our optimized inference engine TGI (vLLM and KV cache) as described in\nSection 2.2.\n2. Frequently asked questions (FAQ): intelligent hardwares are purposed for specific scenarios.\nTo get users familiar with the setting, LLMs need to be able to answer a set of FAQs, e.g.,\n11\nFigure 5: A scalable approach to Tigerbot\u2019s role-playing LLM\nFigure 6: An example screenshot of a Tigerbot\u2019s role-playing with the novel \"Genshin Impact\"\nWhat are the make and model of the car?. We first continual fine-tune a TigerBot chat model\non domain data, e.g., user manual. Then during inference, we generate answers augmented\nby retrieval from a knowledge base filled with annotated FAQs. The knowledge base can be\neasily kept up to date.\n3. Personification: hardwares become more vivid and entertaining would have been portrayed\nas a real-world character. Personification is manifested by a set of annotated data describing\nher profile (name and gender) and style (fun or professional). We then fine-tune a TigerBot\nchat model on the personification dataset for one to two epochs. To preserve the original\ngeneral-purpose capabilities, the fine-tuning data is mixed with some general data. One rule\nof thumb for the mixture rate is that general data accounts for more than half.\n12\n4\nConclusion\nIn this work, we have introduced TigerBot, a family of pretrained and chat LLMs with parameter\nsizes of 7B to 180B. TigerBot has achieved SOTA performance with its high-quality training data\nand a stack of cutting-edge training methods and systems. We also placed an extra emphasis on\npractical applications, with our detailed implementations and observations for a spectrum of tools\nand real-world scenarios. We are strong believer of open innovations, and our work has benefited\nhugely from the LLM open-source community. Likewise, we hope that our work can contribute to\nthe community for future theoretical and practical research and development.\nThe emergence of LLM has marked one of the most heart-bumping moments in decades of AI\ndevelopment history, largely in view of its overwhelming performance with extreme generality while\nbeing straightforward to build. We feel no less awe-inspiring. From our tremendous amount of\nexperiments and implementations, however, we believe that we are still in the nascent stage of LLM\nand more broadly AGI evolution. There are challenges in reliable solutions to mission-critical tasks,\nsustainable infrastructure, disruptive not incremental user values from practical applications, to name\njust a few. The journey ahead is exciting yet equally arduous. Stay calm and happy coding.\nReferences\n[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00f3n, and S. Sanghai. Gqa: Training generalized\nmulti-query transformer models from multi-head checkpoints. arXiv:2305.13245 [cs.CL], 05 2023.\n[2] Anthropic. Claude 2. https://www.anthropic.com/index/claude-2, 06 2023.\n[3] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs.\nProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner,\nS. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners.\narXiv:2005.14165v4 [cs.CL], 05 2020.\n[5] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz. Walking down the memory maze: Beyond context\nlimit through interactive reading. arXiv:2310.05029 [cs.CL], 10 2023.\n[6] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have\nsolved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457 [cs.AI], 03 2018.\n[7] O. Contributors. Opencompass: A universal evaluation platform for foundation models. GitHub repository,\n2023.\n[8] Y. Cui, Z. Yang, and X. Yao.\nEfficient and effective text encoding for chinese llama and alpaca.\narXiv:2304.08177 [cs.CL], 04 2023.\n[9] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention\nwith io-awareness. arXiv:2205.14135 [cs.LG], 05 2022.\n[10] Google. Sentencepiece. GitHub repository, 2023.\n[11] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\nadaptation of large language models. arXiv:2106.09685 [cs.CL], 06 2021.\n[12] Huggingface. Text generation inference. GitHub repository, 2023.\n[13] Huggingface. Transformers. GitHub repository, 2023.\n[14] V. Karpukhin, B. O\u02d8guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W. tau Yih. Dense passage\nretrieval for open-domain question answering. EMNLP 2020, 04 2020.\n[15] C. Li, Z. Leng, C. Yan, J. Shen, H. Wang, W. MI, Y. Fei, X. Feng, S. Yan, H. Wang, L. Zhan, Y. Jia, P. Wu,\nand H. Sun. Chatharuhi: Reviving anime character in reality via large language model. arXiv:2308.09597\n[cs.CL], 2023.\n[16] Microsoft. Megatron-deepspeed. GitHub repository, 2023.\n[17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand,\nP. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale language\nmodel training on gpu clusters using megatron-lm. arXiv:2104.04473 [cs.CL], 04 2021.\n[18] NVIDIA. Tensorrt open source software. GitHub repository, 2023.\n13\n[19] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Chris-\ntiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback.\narXiv:2203.02155v1 [cs.CL], 03 2022.\n[20] O. Peckham.\nMeta completes research supercluster, announces next-gen datacenter.\nHPCwire:\nhttps://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-\ndatacenter/, 05 2023.\n[21] B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language\nmodels. arXiv:2309.00071 [cs.CL], 09 2023.\n[22] S. Pichai. An important next step on our ai journey. https://blog.google/technology/ai/bard-google-ai-\nsearch-updates/, 02 2023.\n[23] O. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input\nlength extrapolation. arXiv:2108.12409 [cs.CL], 08 2021.\n[24] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:\nYour language model is secretly a reward model. arXiv:2305.18290 [cs.LG], 05 2023.\n[25] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion\nparameter models. arXiv:1910.02054 [cs.LG] and In Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and Analysis (SC \u201920), 10 2019.\n[26] P. Rajpurkar, R. Jia, and P. Liang. Know what you don\u2019t know: Unanswerable questions for squad.\narXiv:1806.03822 [cs.CL], 06 2018.\n[27] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon,\nM. Gall\u00e9, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot,\nN. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy,\nH. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Lauren\u00e7on, Y. Jernite, J. Launay, M. Mitchell,\nC. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou,\nC. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkovizh,\nE. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar, H. Benyamina, H. Tran,\nI. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang,\nJ. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan,\nL. B. allal, L. Tanguy, M. Dey, M. R. Mu\u00f1oz, M. Masoud, M. Grandury, M. \u0160a\u0161ko, M. Huang, M. Coavoux,\nM. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis,\nO. Nguyen, O. Espejel, O. de Gibert, P. Villegas, and et al. (293 additional authors not shown). Bloom: A\n176b-parameter open-access multilingual language model. arXiv:2211.05100 [cs.CL], 11 2022.\n[28] N. Shazeer. Glu variants improve transformer. arXiv:2002.05202 [cs.LG], 02 2020.\n[29] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. arXiv:2104.09864 [cs.CL], 04 2021.\n[30] A. Talmor, J. Herzig, N. Lourie, and J. Berant. Commonsenseqa: A question answering challenge targeting\ncommonsense knowledge. arXiv:1811.00937 [cs.CL], 11 2018.\n[31] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford\nalpaca: An instruction-following llama model. GitHub repository, 2023.\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language\nmodels. arXiv:2302.13971 [cs.CL], 02 2023.\n[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,\nB. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,\nM. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu,\nY. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,\nK. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams,\nJ. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\nS. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288\n[cs.CL], 07 2023.\n[34] Turboderp. Exllamav2. GitHub repository, 2023.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long\nBeach, CA, USA., 06 2017.\n[36] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano.\nRecursively\nsummarizing books with human feedback. arXiv:2109.10862 [cs.CL], 09 2021.\n14\n[37] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman,\nB. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis,\nS. Wang, and H. Ma. Effective long-context scaling of foundation models. arXiv:2309.16039 [cs.CL], 09\n2023.\n[38] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable\npost-training quantization for large-scale transformers. arXiv:2206.01861 [cs.CL], 06 2022.\n15\n"
  }
]