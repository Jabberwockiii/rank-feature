[
  {
    "title": "FreeU: Free Lunch in Diffusion U-Net",
    "link": "https://arxiv.org/pdf/2309.11497.pdf",
    "upvote": "63",
    "text": "FreeU: Free Lunch in Diffusion U-Net\nChenyang Si\nZiqi Huang\nYuming Jiang\nZiwei LiuB\nS-Lab, Nanyang Technological University\n{chenyang.si, ziqi002, yuming002, ziwei.liu}@ntu.edu.sg\nSD1.4\nFreeU\nSDXL\nFreeU\nSD1.4\nFreeU\nFigure 1. We propose FreeU, a method that substantially improves diffusion model sample quality at no costs: no training, no additional\nparameter introduced, and no increase in memory or sampling time.\nAbstract\nIn this paper, we uncover the untapped potential of dif-\nfusion U-Net, which serves as a \u201cfree lunch\u201d that substan-\ntially improves the generation quality on the fly. We initially\ninvestigate the key contributions of the U-Net architecture\nto the denoising process and identify that its main backbone\nprimarily contributes to denoising, whereas its skip connec-\ntions mainly introduce high-frequency features into the de-\ncoder module, causing the network to overlook the back-\nbone semantics. Capitalizing on this discovery, we propose\na simple yet effective method\u2014termed \u201cFreeU\u201d \u2014 that en-\nhances generation quality without additional training or\nfinetuning. Our key insight is to strategically re-weight the\ncontributions sourced from the U-Net\u2019s skip connections\nand backbone feature maps, to leverage the strengths of\nboth components of the U-Net architecture. Promising re-\nsults on image and video generation tasks demonstrate that\nour FreeU can be readily integrated to existing diffusion\nmodels, e.g., Stable Diffusion, DreamBooth, ModelScope,\nRerender and ReVersion, to improve the generation qual-\nity with only a few lines of code. All you need is to ad-\njust two scaling factors during inference. Project page:\nhttps://chenyangsi.top/FreeU/.\n1. Introduction\nDiffusion probabilistic models, a cutting-edge category of\ngenerative models, have become a focal point in the re-\nsearch landscape, particularly for tasks related to computer\nvision [5, 6, 8, 10, 12, 20, 22, 26, 28, 29, 32]. Distinct from\nother classes of generative models [3, 7, 9, 16\u201319, 21, 25,\n34, 35] such as Variational Autoencoder (VAE) [21], Gener-\native Adversarial Networks (GANs) [3, 9, 16\u201319, 25], and\nvector-quantized approaches [7, 34], diffusion models in-\ntroduce a novel generative paradigm. These models employ\na fixed Markov chain to map the latent space, facilitating\nintricate mappings that capture latent structural complex-\nities within a dataset. Recently, its impressive generative\ncapabilities, ranging from the high level of details to the\ndiversity of the generated examples, have fueled ground-\nbreaking advancements in a variety of computer vision ap-\nplications such as image synthesis [12, 29, 32], image edit-\ning [1, 4, 14, 24], image-to-image translation [4, 31, 36],\nand text-to-video generation [2, 11, 13, 23, 33, 37, 38, 40].\nThe diffusion models are comprised of the diffusion pro-\ncess and the denoising process. During the diffusion pro-\ncess, Gaussian noise is gradually added to the input data\nand eventually corrupts it into approximately pure Gaussian\nnoise. During the denoising process, the original input data\n1\narXiv:2309.11497v2  [cs.CV]  18 Oct 2023\nGenerated image\nLow frequency\nHigh frequency\nA squirrel eating a burger.\nFigure 2. The denoising process. The top row illustrates the image\u2019s progres-\nsive denoising process across iterations, while the subsequent two rows display\nlow-frequency and high-frequency components after the inverse Fourier Trans-\nform, matching each step. It\u2019s evident that low-frequency components change\nslowly, whereas high-frequency components exhibit more significant variations\nduring the denoising process.\nStep 1\nStep 4\nStep 7\nStep 10\nStep 13\nStep 16\nStep 19\nStep 22\nStep 25\nFigure 3. Relative log amplitudes of Fourier with vari-\nations of the backbone scaling factor b.\nIncreasing\nin b correspondingly results in a suppression of high-\nfrequency components in the images generated by the dif-\nfusion model.\nis recovered from its noise state through a learned sequence\nof inverse diffusion operations. Usually, a U-Net is trained\nto iteratively predict the noise to be removed at each denois-\ning step. Existing works focus on utilizing pre-trained dif-\nfusion U-Nets for downstream applications, while the inter-\nnal properties of the diffusion U-Net, remain largely under-\nexplored.\nBeyond the application of diffusion models, in this pa-\nper, we are interested in investigating the effectiveness of\ndiffusion U-Net for the denoising process. To better un-\nderstand the denoising process, we first present a paradigm\nshift toward the Fourier domain to perspective the gener-\nated process of diffusion models, a research area that has\nreceived limited prior investigation. As illustrated in Fig. 2,\nthe uppermost row provides the progressive denoising pro-\ncess, showcasing the generated images across successive it-\nerations. The subsequent two rows exhibit the associated\nlow-frequency and high-frequency spatial domain informa-\ntion after the inverse Fourier Transform, aligning with each\nrespective step.\nEvident from Fig. 2 is the gradual modulation of low-\nfrequency components, exhibiting a subdued rate of change,\nwhile their high-frequency components display more pro-\nnounced dynamics throughout the denoising process. These\nfindings are further corroborated in Fig.3. This can be intu-\nitively explained: 1) Low-frequency components inherently\nembody the global structure and characteristics of an im-\nage, encompassing global layouts and smooth color. These\ncomponents encapsulate the foundational global elements\nthat constitute the image\u2019s essence and representation. Its\nrapid alterations are generally unreasonable in denoising\nprocesses. Drastic changes to these components could fun-\ndamentally reshape the image\u2019s essence, an outcome typ-\nically incompatible with the objectives of denoising pro-\ncesses. 2) Conversely, high-frequency components contain\nthe rapid changes in the images, such as edges and textures.\nThese finer details are markedly sensitive to noise, often\nmanifesting as random high-frequency information when\nnoise is introduced to an image. Consequently, denoising\nprocesses need to expunge noise while upholding indispens-\nable intricate details.\nIn light of these observations between low-frequency\nand high-frequency components during the denoising pro-\ncess, we extend our investigation to ascertain the specific\ncontributions of the U-Net architecture within the diffusion\nframework. In each stage of the U-Net decoder, the skip\nfeatures from the skip connection and the backbone fea-\ntures are concatenated together. Our investigation reveals\nthat the main backbone of the U-Net primarily contributes\nto denoising. Conversely, the skip connections are observed\nto introduce high-frequency features into the decoder mod-\nule. These connections propagate fine-grained semantic in-\nformation to make it easier to recover the input data. How-\never, an unintended consequence of this propagation is the\npotential weakening of the backbone\u2019s inherent denoising\ncapabilities during the inference phase. This can lead to the\ngeneration of abnormal image details, as illustrated in the\nfirst row of Fig. 1.\nBuilding upon this revelation, we propel forward with\nthe introduction of a novel strategy, denoted as \u201cFreeU\u201d,\nwhich holds the potential to improve sample quality with-\nout necessitating the computational overhead of additional\n2\ntraining or fine-tuning.\nDuring the inference stage, we\ninstantiate two specialized modulation factors designed to\nbalance the feature contributions from the U-Net architec-\nture\u2019s primary backbone and skip connections. The first,\ntermed the backbone feature factors, aims to amplify the\nfeature maps of the main backbone, thereby bolstering the\ndenoising process. However, we find that while the inclu-\nsion of backbone feature scaling factors yields significant\nimprovements, it can occasionally lead to an undesirable\noversmoothing of textures. To mitigate this issue, we intro-\nduce the second factor, skip feature scaling factors, aiming\nto alleviate the problem of texture oversmoothing.\nOur FreeU framework exhibits seamless adaptability\nwhen integrated with existing diffusion models, encom-\npassing applications like text-to-image generation and text-\nto-video generation.\nWe conduct a comprehensive ex-\nperimental evaluation of our approach, employing Stable\nDiffusion [29], DreamBooth [30], ReVersion [15], Mod-\nelScope [23], and Rerender [39] as our foundational mod-\nels for benchmark comparisons. By employing FreeU dur-\ning the inference phase, these models indicate a discernible\nenhancement in the quality of generated outputs. The vi-\nsualization illustrated in Fig. 1 substantiates the efficacy of\nFreeU in significantly enhancing both intricate details and\noverall visual fidelity within the generated images. Our con-\ntributions are summarized as follows:\n\u2022 We investigate and uncover the potential of U-Net ar-\nchitectures for denoising within diffusion models and\nidentify that its main backbone primarily contributes to\ndenoising, whereas its skip connections introduce high-\nfrequency features into the decoder module.\n\u2022 We further introduce a simple yet effective method, de-\nnoted as \u201cFreeU\u201d, which enhances U-Net\u2019s denoising ca-\npability by leveraging the strengths of both components\nof the U-Net architecture. It substantially improves the\ngeneration quality without requiring additional training or\nfine-tuning.\n\u2022 The proposed FreeU framework is versatile and seam-\nlessly integrates with existing diffusion models.\nWe\ndemonstrate significant sample quality improvement\nacross various diffusion-based methods, showing the ef-\nfectiveness of FreeU at no extra cost.\n2. Methodology\n2.1. Preliminaries\nDiffusion models such as Denoising Diffusion Probabilis-\ntic Models (DDPM) [12], encompass two fundamental pro-\ncesses for data modeling: a diffusion process and a denois-\ning process. The diffusion process is characterized by a se-\nquence of T steps. At each step t, Gaussian noise is incre-\nmentally introduced into the data distribution x0 \u223c q(x0)\nvia a Markov chain, following a prescribed variance sched-\nule denoted as \u03b21, . . . , \u03b2T :\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI)\n(1)\nThe denoising process reverses the above diffusion process\nto the underlying clean data xt\u22121 given the noisy input xt:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t))\n(2)\nThe \u00b5\u03b8 and \u03a3\u03b8 determined through estimation procedures\ninvolving a denoising model denoted as \u03f5\u03b8. Typically, this\ndenoising model is implemented using a time-conditional\nU-Net architecture. It is trained to eliminate noise from data\nsamples while concurrently enhancing the overall fidelity of\nthe generated samples.\n2.2. How does diffusion U-Net perform denoising?\nBuilding upon the notable disparities observed between\nlow-frequency and high-frequency components throughout\nthe denoising process illustrated in Fig. 2 and Fig. 3, we\nextend our investigation to delineate the specific contribu-\ntions of the U-Net architecture within the denoising process,\nto explore the internal properties of the denoising network.\nAs depicted in Fig. 4, the U-Net architecture comprises a\nprimary backbone network, encompassing both an encoder\nand a decoder, as well as the skip connections that facili-\ntate information transfer between corresponding layers of\nthe encoder and decoder.\nThe backbone of U-Net. To evaluate the salient char-\nacteristics of the backbone and lateral skip connections\nin the denoising process, we conduct a controlled experi-\nment wherein we introduce two multiplicative scaling fac-\ntors\u2014denoted as b and s\u2014to modulate the feature maps\ngenerated by the backbone and skip connections, respec-\ntively, prior to their concatenation. As shown in Fig. 5, it\nis evident that elevating the scale factor b of the backbone\ndistinctly enhances the quality of generated images. Con-\nversely, variations in the scaling factor s, which modulates\nthe impact of the lateral skip connections, appear to exert a\nnegligible influence on the quality of the generated images.\nBuilding upon these observations, we subsequently\nprobed the underlying mechanisms that account for the en-\nhancement in image generation quality when the scaling\nfactor b associated with the backbone feature maps is aug-\nmented. Our analysis reveals that this quality improvement\nis fundamentally linked to an amplified denoising capabil-\nity imparted by the U-Net architecture\u2019s backbone. As de-\nlineated in Fig. 6, a commensurate increase in b correspond-\ningly results in a suppression of high-frequency components\nin the images generated by the diffusion model. This im-\nplies that enhancing backbone features effectively bolsters\nthe denoising capability of the U-Net architecture, thereby\ncontributing to a superior output in terms of both fidelity\nand detail preservation.\n3\n\u2295\ns\n\u2295\nb\nbackbone features (x)\nskip features (h)\nFFT\nIFFT\nskip connections\nskip \nfeatures\nbackbone \nfeatures\n(a) UNet Architecture\n(b) FreeU Operations\nskip connection\nFigure 4. FreeU Framework. (a) U-Net Skip Features and Backbone Features. In U-Net, the skip features and backbone features are\nconcatenated together at each decoding stage. We apply the FreeU operations during concatenation. (b) FreeU Operations. The factor b\naims to amplify the backbone feature map x, while factor s is designed to attenuate the skip feature map h.\nb=0.6, s=1.0\nb=0.8, s=1.0\nb=1.0, s=1.0\nb=1.2, s=1.0\nb=1.4, s=1.0\nb=1.0, s=0.6\nb=1.0, s=0.8\nb=1.0, s=1.0\nb=1.0, s=1.2\nb=1.0, s=1.4\nFigure 5. Effect of backbone and skip connection scaling factors (b and s).\nIncreasing the backbone scaling factor b significantly enhances image quality, while\nvariations in the skip scaling factor s have a negligible influence on image synthesis\nquality.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n Log amplitude\n0.6\n0.8\n1.0\n1.2\n1.4\nFigure 6. Relative log amplitudes of Fourier with\nvariations of the backbone scaling factor b. Increas-\ning in b correspondingly results in a suppression of\nhigh-frequency components in the images generated\nby the diffusion model.\nThe skip connections of U-Net. Conversely, the skip\nconnections serve to forward features from the earlier lay-\ners of encoder blocks directly to the decoder. Intriguingly,\nas evidenced in Fig. 7, these features primarily constitute\nhigh-frequency information. Our conjecture, grounded in\nthis observation, posits that during the training of the U-Net\narchitecture, the presence of these high-frequency features\nmay inadvertently expedite the convergence toward noise\nprediction within the decoder module.\nFurthermore, the\nlimited impact of modulating skip features in Fig. 5 also\nindicates that the skip features predominantly contribute\nto the decoder\u2019s information. This phenomenon, in turn,\ncould result in an unintended attenuation of the efficacy of\nthe backbone\u2019s intrinsic denoising capabilities during infer-\nence. Thereby, this observation prompts pertinent questions\nabout the counterbalancing roles played by the backbone\nand the skip connections in the composite denoising perfor-\nmance of the U-Net framework.\n2.3. Free lunch in diffusion U-Net\nCapitalizing on the above discovery, we propel forward with\nthe introduction of simple yet effective method, denoted as\n\u201cFreeU\u201d, which effectively bolsters the denoising capabil-\nity of the U-Net architecture by leveraging the strengths\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\n-6.0\n-4.0\n-2.0\n0.0\n Log amplitude\nbackbone\nskip\nfusion\nFigure 7. Fourie relative log amplitudes of backbone, skip, and\ntheir fused feature maps. The features, forwarded by skip con-\nnections directly from earlier layers of the encoder block to the\ndecoder contain a large amount of high-frequency information.\nGenerated image\nFeature map\nGenerated image\nFeature map\nFigure 8. Visualization of the average feature maps from the\nsecond stage in the decoder.\nof both components of the U-Net architecture. It substan-\ntially improves the generation quality without requiring ad-\nditional training or fine-tuning.\nTechnically, for the l-th block of the U-Net decoder, let\nxl represent the backbone feature map from the main back-\nbone at the preceding block, and let hl denote the feature\nmap propagated through the corresponding skip connec-\ntion. To modulate these feature maps, we introduce two\nscalar factors: a backbone feature scaling factor bl for xl\nand a yet-to-be-defined skip feature scaling factor sl for hl.\nSpecifically, the factor bl aims to amplify the backbone fea-\nture map xl, while factor sl is designed to attenuate the skip\nfeature map hl.\nFor the backbone features, we introduce a novel method\nknown as structure-related scaling, which dynamically ad-\njusts the scaling of backbone features for each sample. Un-\nlike a fixed scaling factor applied uniformly to all samples\nor positions within the same channel, our approach adjusts\nthe scaling factor adaptively based on the specific character-\nistics of the sample features. We first computer the average\nfeature map along the channel dimension:\n\u00afxl = 1\nC\nC\nX\ni=1\nxl,i,\n(3)\nwhere xl,i represents the i-th channel of the feature map xl.\nC denotes the total number of channels in xl. Subsequently,\nthe backbone factor map is determined as follows:\n\u03b1l = (bl \u2212 1) \u00b7\n\u00afxl \u2212 Min(\u00afxl)\nMax(\u00afxl) \u2212 Min(\u00afxl) + 1,\n(4)\nwhere \u03b1l represents the backbone factor map.\nbl is a\nscalar constant.\nThen, upon experimental investigation,\nwe discern that indiscriminately amplifying all channels\nof xl through multiplication with \u03b1l engenders an over-\nsmoothed texture in the resulting synthesized images. The\nreason is the enhanced U-Net compromises the image\u2019s\nhigh-frequency details while denoising. Consequently, we\nconfine the scaling operation to the half channels of xl as\nfollows:\nx\n\u2032\nl,i =\n(\nxl,i \u2299 \u03b1l,\nif i < C/2\nxl,i,\notherwise\n(5)\nIndeed, as illustrated in Fig. 8, the average feature map \u00afxl\ninherently contains valuable structural information. Conse-\nquently, the backbone factor map \u03b1l is instrumental in am-\nplifying the backbone feature map xl in a manner that aligns\nwith its structural characteristics. This strategic approach\nserves to mitigate the issue of oversmoothing. Importantly,\nthis strategy offers a dual benefit. Firstly, it enhances the\ndenoising capabilities of the backbone feature map, allow-\ning it to filter out noise more effectively. Secondly, it avoids\nthe adverse effects associated with the indiscriminate ap-\nplication of scaling across the entire feature map, thereby\nachieving a more nuanced equilibrium between noise re-\nduction and texture preservation.\nTo further mitigate the issue of oversmoothed texture due\nto enhancing denoising, we further employ spectral modu-\nlation in the Fourier domain to selectively diminish low-\nfrequency components for the skip features. Mathemati-\ncally, this operation is performed as follows:\nF(hl,i) = FFT(hl,i)\n(6)\nF\u2032(hl,i) = F(hl,i) \u2299 \u03b2l,i\n(7)\nh\u2032\nl,i = IFFT(F\u2032(hl,i))\n(8)\nwhere FFT(\u00b7) and IFFT(\u00b7) are Fourier transform and inverse\nFourier transform. \u2299 denotes element-wise multiplication,\nand \u03b2l,i is a Fourier mask, designed as a function of the\nmagnitude of the Fourier coefficients, serving to implement\nthe frequency-dependent scaling factor sl:\n\u03b2l,i(r) =\n(\nsl\nif r < rthresh,\n1\notherwise.\n(9)\nwhere r is the radius.\nrthresh is the threshold frequency.\nThen, the augmented skip feature map h\u2032\nl is then concate-\nnated with the modified backbone feature map x\u2032\nl for sub-\nsequent layers in the U-Net architecture, as shown in Fig. 4.\n5\na blue car is being filmed\nMother rabbit is raising baby rabbits\na baby in a red shirt\na attacks an upset cat and is then chased off\nA bridge is depicted in the water\nA panda standing on a surfboard in the ocean \nA boy is playing pokemon\nA cat riding a motorcycle.\nA teddy bear walking in the snowstorm\nSD + FreeU\nSD\nSD + FreeU\nSD\nSD + FreeU\nSD\nFigure 9. Samples generated by Stable Diffusion [29] with or without FreeU.\nRemarkably, the proposed FreeU framework does not re-\nquire any task-specific training or fine-tuning. Adding the\nbackbone and skip scaling factors can be easily done with\njust a few lines of code. Essentially, the parameters of the\narchitecture can be adaptively re-weighted during the in-\nference phase, which allows for a more flexible and potent\ndenoising operation without adding any computational bur-\nden. This makes FreeU a highly practical solution that can\nbe seamlessly integrated into existing diffusion models to\nimprove their performance.\n3. Experiments\n3.1. Implementation details\nTo assess the effectiveness of the proposed FreeU, we sys-\ntematically conduct a series of experiments, aligning our\nbenchmarks with state-of-the-art methods such as Stable\nDiffusion [29], DreamBooth [30], ModelScope [23], and\nRerender [39]. Importantly, our approach seamlessly inte-\ngrates with these established methods without imposing any\nadditional computational overhead associated with supple-\nmentary training or fine-tuning. We meticulously adhere to\nthe prescribed settings of these methods and exclusively in-\ntroduce the backbone feature factors and skip feature factors\nduring the inference.\n3.2. Text-to-image\nStable Diffusion [29] is a latent text-to-image diffusion\nmodel renowned for its capability to generate photorealistic\nimages based on textual input. It has consistently demon-\nstrated exceptional performance in various image synthesis\ntasks. With the integration of our FreeU augmentation into\nStable Diffusion, the results, as exemplified in Fig. 9, ex-\nhibit a notable enhancement in the model\u2019s generative ca-\npacity.\nTo elaborate, the incorporation of FreeU into Stable Dif-\nfusion [29] yields improvements in both entity portrayal\nand fine-grained details. For instance, when provided with\nthe prompt \u201ca blue car is being filmed\u201d, FreeU refines the\nimage, eliminating rooftop irregularities and enhancing the\ntextural intricacies of the surrounding structures. In the case\nof \u201cMother rabbit is raising baby rabbits\u201d, FreeU ensures\nthat the generated image portrays a mother rabbit in a nor-\nmal appearance caring for baby rabbits. Furthermore, In\nscenarios like \u201ca attacks an upset cat and is then chased\noff\u201d and \u201cA teddy bear walking in the snowstorm\u201d, FreeU\nhelps generate more realistically posed cats and teddy bears.\nImpressively, in response to the complex prompt \u201cA cat\nriding a motorcycle\u201d, FreeU not only accurately renders\nthe individual entities but also expertly captures the nu-\n6\nSDXL + FreeU\nSDXL\nSDXL + FreeU\nSDXL\nSDXL + FreeU\nSDXL\nFigure 10. Samples generated by Stable Diffusion-XL [27] with or without FreeU.\nanced relationship between them, ensuring that the cat is\nactively engaged in riding. In Figure 10, we present the\ngenerated images based on the SDXL framework [27]. It\nbecomes evident that our proposed FreeU consistently ex-\ncels in generating realistic images, especially in detail gen-\neration. These compelling results serve as a testament to\nthe substantial qualitative enhancements engendered by the\nsynergy of FreeU with the SD[29] or SDXL[27] frame-\nworks.\nQuantitative evaluation. We conduct a study with 35 par-\nticipants to assess image quality and image-text alignment.\nEach participant receives a text prompt and two correspond-\ning synthesized images, one from SD and another from\nSD+FreeU. To ensure fairness, we use the same randomly\nsampled random seed for generating both images. The im-\nage sequence is randomized to eliminate any bias.\nPar-\nticipants then select the image they consider superior for\nimage-text alignment and image quality, respectively. We\ntabulate the votes for SD and SD+FreeU in each category in\nTable 1. Our analysis reveals that the majority of votes go\nto SD+FreeU, indicating that FreeU significantly enhances\nthe Stable Diffusion text-to-image model in both evaluated\naspects.\n3.3. Text-to-video\nModelScope [23], an avant-garde text-to-video diffusion\nmodel, stands at the forefront of video generation from tex-\ntual descriptions. The infusion of our FreeU augmentation\nTable 1.\nText-to-Image Quantitative Results.\nWe count the\npercentage of votes for the baseline and our method respectively.\nImage-Text refers to Image-Text Alignment.\nMethod\nImage-Text\nImage Quality\nSD [29]\n14.12%\n14.66%\nSD+FreeU\n85.88%\n85.34%\nTable 2. Text-to-Video Quantitative Results. We count the per-\ncentage of votes for the baseline and our method respectively.\nVideo-Text refers to Video-Text Alignment.\nMethod\nVideo-Text\nVideo Quality\nModelScope [23]\n15.29%\n14.33%\nModelScope+FreeU\n84.71%\n85.67%\ninto ModelScope [23] serves to further hone its video syn-\nthesis prowess, as substantiated by Fig. 11. For instance,\nwhen presented with the prompt \u201cA cinematic view of the\nocean, from a cave\u201d, FreeU enables ModelScope [23] to\ngenerate the perspective \u201cfrom a cave\u201d, enriching the visual\nnarrative. In the case of \u201cA cartoon of an elephant walk-\ning\u201d, ModelScope [23] initially generates an elephant with\ntwo trunks, but with the incorporation of FreeU, it rectifies\nthis anomaly and produces a correct depiction of an ele-\nphant in motion. Moreover, in response to the prompt \u201cAn\nastronaut flying in space\u201d, ModelScope [23], with the as-\nsistance of FreeU, can generate a clear and vivid portrayal\nof an astronaut floating in the expanse of outer space.\n7\nA cinematic view of the ocean, from a cave.\nA cartoon of an elephant walking.\nAn astronaut flying in space.\nModelScope+FreeU\nModelScope\nModelScope+FreeU\nModelScope\nModelScope+FreeU\nModelScope\nFigure 11. Samples generated by ModelScope [23] with or without FreeU.\nThese results underscore the significant improvements\nachieved through the synergistic application of FreeU with\nModelScope [23], resulting in high-quality generated con-\ntent characterized by clear motion, rich detail, and semantic\nalignment.\nQuantitative evaluation. We conduct the quantitative eval-\nuation for FreeU on the text-to-video task in a similar way\nas text-to-image.\nThe results displayed in Table 2 indi-\ncate that most participants prefer the video generated with\nFreeU.\n3.4. Downstream tasks\nFreeU presents substantial enhancements in the quality of\nsynthesized samples across various diffusion model appli-\ncations. Our evaluations extend from foundational image\nand video synthesis models to more specialized downstream\n8\nDreamBooth + FreeU\nDreamBooth\nInput images\na photo of action figure riding a motorcycle\nA toy on a beach\nFigure 12. Samples generated by DreamBooth [30] with or\nwithout FreeU.\ndog <R> basket\n<R> = \u201cis contained inside of\u201d\n<R> = \u201csits back-to-back with\u201d\n\u201ccat <R> motorbike\u201d\nSpiderman <R> basket\n<R> = \u201cride on\u201d\nchild <R> child\nReVersion\nReVersion+FreeU\ncat <R> motorbike\n<R> = \u201cride on\u201d\n<R> = \u201cis contained inside of\u201d\nReVersion\nReVersion+FreeU\nFigure 13. Samples generated by ReVersion [15] with or with-\nout FreeU.\napplications.\nWe incorporate FreeU into Dreambooth [30], a diffusion\nmodel specialized in personalized text-to-image tasks. The\nenhancements are evident, as demonstrated in Fig. 12, the\nsynthesized images present marked improvements in real-\nism. For instance, while the base DreamBooth [30] model\nstruggles to synthesize the appearance of the action figure\u2019s\nlegs from the prompt \u201ca photo of action figure riding a mo-\ntorcycle\u201d, the FreeU-augmented version deftly overcomes\nthis hurdle. Similarly, for the prompt \u201cA toy on a beach\u201d,\nthe initial output exhibited body shape anomalies. FreeU\u2019s\nintegration refines these imperfections, providing a more\naccurate representation and improving color fidelity.\nWe also integrate FreeU into ReVersion [15], a Stable\nDiffusion based relation inversion method, enhancing its\nquality as shown in Fig. 13. For example, when the rela-\ntion \u201cback to back\u201d is to be expressed between two children,\nFreeU enhances ReVersion\u2019s ability to accurately represent\nRerender\nRerender+FreeU\nA dog wearing sunglasses\nFigure 14. Samples generated by Rerender [39] with or without\nFreeU.\nthis relationship. For the \u201cinside\u201d relation, when a dog is\nsupposed to be placed inside of a basket, ReVersion some-\ntimes generates a dog with artifacts, and introducing FreeU\nhelps eliminate these artifacts. While ReVersion effectively\ncaptures relational concepts, Stable Diffusion might occa-\nsionally struggle to synthesize the relation concept due to\nexcessive high-frequency noises in the U-Net skip features.\nAdding FreeU allows better entity and relation synthesis\nquality by using exactly the same relation prompt learned\nby ReVersion.\nFurthermore, we evaluated FreeU\u2019s impact on Reren-\nder [39], a diffusion model tailored for zero-shot text-\nguided video-to-video translations. Fig. 14 depicts the re-\nsults: clear improvements in the detail and realism of syn-\nthesized videos.\nFor instance, when provided with the\nprompt \u201cA dog wearing sunglasses\u201d and an input video,\nRerender [39] initially produces a dog video with artifacts\nrelated to the \u201csunglasses\u201d. However, the incorporation of\nFreeU successfully eliminates such artifacts, resulting in a\nrefined output.\nIn summation, these outcomes substantiate that the in-\ncorporation of FreeU leads to enhanced entity representa-\ntion and synthesis quality, employing precisely the same\nlearned prompt.\n3.5. Ablation study\nEffects of FreeU. FreeU is introduced with the primary aim\nof enhancing the denoising capabilities of the U-Net archi-\ntecture within the diffusion model. To assess the impact\nof FreeU, we conducted analytical experiments using Sta-\n9\nFigure 15. Fourier relative log amplitudes of Stable Diffusion [29] with or without FreeU within the denoising process.\nSD + FreeU\nSD\nSD + FreeU\nSD\nSD + FreeU\nSD\nSD + FreeU\nSD\nFigure 16. The visualization of feature maps for Stable Diffusion [29] with or without FreeU.\nA fat rabbit wearing a purple robe walking through a fantasy landscape\nA synthwave style sunset above the reflecting water of the sea, digital art\na teddy bear walking down the road in the sunset\nSD\nSD+FreeU (b)\nSD+FreeU (b&s)\nFigure 17. The ablation study of backbone scaling factor and\nskip scaling factor.\nble Diffusion [29] as the base framework. In Fig. 15, we\npresent visualizations of the relative log amplitudes of the\nFourier transform of Stable Diffusion [29], comparing cases\nwith and without the incorporation of FreeU. These visual-\nizations illustrate that FreeU exerts a discernible influence\nin reducing high-frequency information at each step of the\ndenoising process, which indicates FreeU\u2019s capacity to ef-\n(a)\n(b)\n(c)\nFigure 18. The ablation study of backbone scaling factor. (a)\nThe generated images of SD. (b) The generated images of\nFreeU with a constant factor.\n(c) The generated images of\nFreeU with the structure-related scaling factor map.\nfectively denoising. Furthermore, we extended our analysis\nby visualizing the feature maps of the U-Net architecture.\nAs shown in Fig. 16, we observe that the feature maps gen-\nerated by FreeU contain more pronounced structural infor-\nmation. This observation aligns with the intended effect of\nFreeU, as it preserves intricate details while effectively re-\nmoving noise, harmonizing with the denoising objectives of\n10\nthe model.\nEffects of components in FreeU. We evaluate the ef-\nfects of the proposed FreeU strategy, i.e. introducing back-\nbone feature scaling factors and skip feature scaling factors\nto intricately balance the feature contributions from the U-\nNet architecture\u2019s primary backbone and skip connections.\nIn Fig. 17, we present the results of our evaluations. In the\ncase of SD+FreeU(b), where backbone scaling factors are\nintegrated during inference, we observe a noticeable im-\nprovement in the generation of vivid details compared to\nSD [29] alone. For instance, when given the prompt \u201cA\nfat rabbit wearing a purple robe walking through a fan-\ntasy landscape\u201d, SD+FreeU(b) generates a more realistic\nrabbit with normal arms and ears, as opposed to SD [29].\nHowever, it is imperative to note that while the inclusion\nof feature scaling factors yields significant improvements,\nit can occasionally lead to an undesirable oversmoothing of\ntextures. To mitigate this issue, we introduce skip feature\nscaling factors, aiming to reduce low-frequency informa-\ntion and alleviate the problem of texture oversmoothing. As\ndemonstrated in Fig. 17, the combination of both backbone\nand skip feature scaling factors in SD+FreeU(b & s) leads\nto the generation of more realistic images. For instance, in\nthe prompt \u201cA synthwave style sunset above the reflecting\nwater of the sea, digital art\u201d, the generated sunset sky in\nSD+FreeU(b & s) exhibits enhanced realism compared to\nSD+FreeU(b). This highlights the efficacy of the compre-\nhensive FreeU strategy in balancing features and mitigating\nissues related to texture smoothing, ultimately resulting in\nmore faithful and realistic image generation.\nEffects of backbone structure-related factor. We eval-\nuate the effects of the proposed backbone scaling strategy,\nstructure-related scaling, on the delicate balance between\nnoise reduction and texture preservation. Illustrated in Fig-\nure 18, when compared to the results generated by SD [29],\nwe observe a substantial enhancement in the image quality\ngenerated by FreeU when utilizing a constant scaling factor.\nHowever, it is pertinent to highlight that the utilization of\na fixed scaling factor can engender adverse consequences,\nmanifesting as pronounced oversmoothing of textures and\nundesirable color oversaturation. Conversely, FreeU with\nthe structure-related scaling factor map employs an adaptive\nscaling approach, leveraging structural information to guide\nthe assignment of the backbone factor map. Our observa-\ntions indicate that FreeU with the structure-related scaling\nfactor map effectively mitigates these issues and achieves\nsignificant improvements in generating vivid and intricate\ndetails.\n4. Conclusion\nIn this study, we introduce the elegantly simple yet highly\neffective approach, termed FreeU, which substantially en-\nhances the sample quality of diffusion models without in-\ncurring any additional computational costs. Motivated by\nthe fundamental role played by both skip connections and\nbackbone features in U-Net architectures, we conduct an\nin-depth analysis of their effects in diffusion U-Net. Our\ninvestigation reveals that the primary backbone primarily\ncontributes to denoising, while the skip connections pre-\ndominantly introduce high-frequency features into the de-\ncoder, potentially leading to a neglect of essential backbone\nsemantics. To address this, we strategically re-weight the\ncontributions originating from the U-Net\u2019s skip connections\nand backbone feature maps. This re-weighting process cap-\nitalizes on the unique strengths of both U-Net components,\nresulting in a substantial improvement in sample quality\nacross a wide range of text prompts and random seeds. Our\nproposed FreeU can be seamlessly integrated into various\ndiffusion foundation models and their downstream tasks, of-\nfering a versatile means of enhancing sample quality.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\n2022. 1\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 1\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 1\n[4] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. ILVR: Conditioning method for\ndenoising diffusion probabilistic models. In ICCV, 2021. 1\n[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. In NeurIPS, 2021. 1\n[6] Patrick Esser, Robin Rombach, Andreas Blattmann, and\nBjorn Ommer.\nImageBART: Bidirectional context with\nmultinomial diffusion for autoregressive image synthesis. In\nNeurIPS, 2021. 1\n[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 1\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In ICLR, 2023. 1\n[9] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C Courville,\nand Yoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 1\n[10] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, 2022. 1\n[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\n11\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 1\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 1, 3\n[13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogVideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 1\n[14] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei\nLiu. Collaborative diffusion for multi-modal face generation\nand editing. In CVPR, 2023. 1\n[15] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan,\nand Ziwei Liu. ReVersion: Diffusion-based relation inver-\nsion from images. arXiv preprint arXiv:2303.13495, 2023.\n3, 9\n[16] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of GANs for improved quality, stability,\nand variation. In ICLR, 2018. 1\n[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019.\n[18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In CVPR, 2020.\n[19] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In NeurIPS, 2021. 1\n[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. arXiv\npreprint arXiv:2210.09276, 2022. 1\n[21] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 1\n[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. arXiv preprint arXiv:2212.04488,\n2022. 1\n[23] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan. VideoFusion: Decomposed diffusion models for\nhigh-quality video generation. In CVPR, 2023. 1, 3, 6, 7, 8\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 1\n[25] Mehdi Mirza and Simon Osindero. Conditional generative\nadversarial nets. arXiv preprint arXiv:1411.1784, 2014. 1\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards photorealistic image gener-\nation and editing with text-guided diffusion models. arXiv\npreprint arXiv:2112.10741, 2021. 1\n[27] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 7\n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with CLIP latents. arXiv preprint arXiv:2204.06125,\n2022. 1\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 3, 6,\n7, 10, 11\n[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 3, 6, 9\n[31] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH, 2022. 1\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1\n[33] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1\n[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. In NeurIPS, 2017. 1\n[35] Pei Wang, Yijun Li, and Nuno Vasconcelos. Rethinking and\nimproving the robustness of image style transfer.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 124\u2013133, 2021. 1\n[36] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is all\nyou need for image-to-image translation.\narXiv preprint\narXiv:2205.12952, 2022. 1\n[37] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 1\n[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 1\n[39] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 3, 6, 9\n[40] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2023. 1\n12\n"
  },
  {
    "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
    "link": "https://arxiv.org/pdf/2309.11499.pdf",
    "upvote": "57",
    "text": "Published as a conference paper at ICLR 2024\nDREAMLLM: SYNERGISTIC MULTIMODAL\nCOMPREHENSION AND CREATION\nRunpei Dong 12\u22c6\u2020\nChunrui Han 3\u22c6\nYuang Peng 4\u2020\nZekun Qi 12\u2020\nZheng Ge 3\nJinrong Yang 5\u2020\nLiang Zhao 3\nJianjian Sun 3\nHongyu Zhou 3\nHaoran Wei 3\nXiangwen Kong 3\nXiangyu Zhang 3\u2021\nKaisheng Ma 4\u00b6\nLi Yi 467\u00b6\u2021\n1Xi\u2019an Jiaotong University\n2Institute for Interdisciplinary Information Core Technology (IIISCT)\n3MEGVII Technology\n4Tsinghua University\n5HUST\n6Shanghai Artificial Intelligence Laboratory\n7Shanghai Qi Zhi Institute\nABSTRACT\nThis paper presents DREAMLLM, a learning framework that first achieves versatile\nMultimodal Large Language Models (MLLMs) empowered with frequently over-\nlooked synergy between multimodal comprehension and creation. DREAMLLM\noperates on two fundamental principles. The first focuses on the generative model-\ning of both language and image posteriors by direct sampling in the raw multimodal\nspace. This approach circumvents the limitations and information loss inherent to\nexternal feature extractors like CLIP, and a more thorough multimodal understand-\ning is obtained. Second, DREAMLLM fosters the generation of raw, interleaved\ndocuments, modeling both text and image contents, along with unstructured layouts.\nThis allows DREAMLLM to learn all conditional, marginal, and joint multimodal\ndistributions effectively. As a result, DREAMLLM is the first MLLM capable of\ngenerating free-form interleaved content. Comprehensive experiments highlight\nDREAMLLM\u2019s superior performance as a zero-shot multimodal generalist, reaping\nfrom the enhanced learning synergy. Project page: dreamllm.github.io.\n1\nINTRODUCTION\n\u201cWhat I cannot create, I do not understand.\u201d\nRichard P. Feynman, on his blackboard at the time of his death, 1988\nContent comprehension and creation in multimodality are crucial and among the ultimate courses\nof machine intelligence (Sternberg, 1985; Legg & Hutter, 2007). To this end, Multimodal Large\nLanguage Models (MLLMs) (Alayrac et al., 2022; Hao et al., 2022; Huang et al., 2023) have emerged\nas extensions of the successful GPT-style Large Language Models (LLMs) (Brown et al., 2020; Zhang\net al., 2022; OpenAI, 2022; 2023a;b; Chen et al., 2023b; Touvron et al., 2023a;b) into visual realm.\nRecognized as foundation models (Bommasani et al., 2021), MLLMs have achieved unprecedented\nprogress in multimodal comprehension capabilities. These advanced models typically enhance LLMs\nby incorporating images as multimodal inputs, such as CLIP features (Radford et al., 2021), to\nfacilitate language-output multimodal comprehension. Their aim is to capture multimodal conditional\nor marginal distributions via a language posterior. However, multimodal creation, which involves\ngenerating images, texts, or both, necessitates a universal generative model that simultaneously learns\nlanguage and image posteriors\u2014currently underexplored.\nUntil very recently, some concurrent works have shown success in conditional image generation using\nMLLMs (Koh et al., 2023; Sun et al., 2023b). As depicted in Fig. 1, these methods compel MLLMs\nto produce either discrete or continuous conditional embeddings that explicitly align with a pretrained\nCLIP encoder, which could later be used by a pretrained Stable Diffusion (SD) (Rombach et al., 2022)\nmodel for image generation. However, due to an inherent modality gap (Liang et al., 2022), CLIP\n\u22c6Equal contribution. \u2020Work partially done during the internship at IIISCT and MEGVII. \u2021Project leaders.\n\u00b6Corresponding authors.\n1\narXiv:2309.11499v2  [cs.CV]  15 Mar 2024\nPublished as a conference paper at ICLR 2024\n(a) CLIP-like\nText\nImage\nContrastive \nAlignment\nText \nEncoder\nImage \nEncoder\nVisual\nEmbedding\nText \nEmbedding\n,\n:  Raw Image\n\ud835\udc701\n,\n\ud835\udc7b2 : Raw Text\n\ud835\udc7b1\n(b) Flamingo/BLIP-like\nMLLM\n\ud835\udc701\n\ud835\udc7b1\n\ud835\udc7b2\n\ud835\udc7b1\nMLLM\n(d) DreamLLM (Ours)\n\ud835\udc701\n\ud835\udc7b2\nInference \nStream\n(c) GILL/Emu-like\nMLLM\nor\nCLIP Alignment\n\ud835\udc701\n\ud835\udc7b2\n\ud835\udc7b1\nCLIP\n\ud835\udc702\n\ud835\udc702\n\ud835\udc702\n\ud835\udc702\n\ud835\udc7b1\n\ud835\udc7b1\n\ud835\udc702\n\ud835\udc7b1\n\ud835\udc7b1\nprojector\nFigure 1: Conceptual comparison of vision-language (VL) foundation models. (a) CLIP-like\nmodels (Radford et al., 2021; Yu et al., 2022a; Li et al., 2023e) take advantage of two towers\nthat explicitly align VL representations. (b) Flamingo/BLIP-like models (Alayrac et al., 2022; Li\net al., 2022; 2023d; Huang et al., 2023) encode VL representations into a unified manifold space\nusing a singular MLLM. However, these models lack full autoregressivity, as they only output\nlanguage. (c) Concurrent MLLMs (Koh et al., 2023; Sun et al., 2023b) align visual outputs with CLIP\nrepresentations, but this alignment occurs in an intermediate space, not a raw data space. Consequently,\nmodels such as Emu necessitate a second-stage fine-tuning of Stable Diffusion (Rombach et al., 2022)\nfor raw image generation. These models also fall short in generating raw interleaved documents. (d)\nOur DREAMLLM, instead, generates raw language and image inputs in a unified auto-regressive\nmanner, inherently enabling interleaved generation. Only non-autoregressive generation loss is noted.\nsemantics focus predominantly on modality-shared information, often overlooking modality-specific\nknowledge that could enhance multimodal comprehension. Consequently, these studies have not fully\nrealized the potential learning synergy between multimodal creation and comprehension, have shown\nonly marginal improvements in creativity, and remain deficient in multimodal comprehension.\nIn this work, we introduce DREAMLLM, universally learning image and text posteriors with expected\ncreation & comprehension synergy, based on the following two de-facto designing principles:\ni. Generate Everything as It Is Different from existing works that generate intermediate image\nrepresentations like CLIP embeddings during training, DREAMLLM not only takes all modali-\nties raw data as inputs but also as outputs in a truly end-to-end fashion (i.e., outputs are identical\nto inputs, see Fig. 1). The challenge lies in enabling MLLMs to learn the image posterior\nwithout compromising their comprehension capabilities. To address this, we introduce dream\nqueries, a set of learnable embeddings that encapsulate the semantics encoded by MLLMs. This\napproach avoids altering the output space of MLLMs. Raw images are then decoded by the SD\nimage decoder conditioned on these semantics. In this fashion, the pretrained SD acts as the\nscore function (Ho et al., 2020). The image posterior is thus modeled by direct sampling in the\npixel space, facilitated by score distillation (van den Oord et al., 2018; Poole et al., 2023).\nii. Interleaved Generative Pre-Training (I-GPT) DREAMLLM is trained to generate interleaved\nmultimodal corpora from the internet (Zhu et al., 2023b), both encoding and decoding interleaved\nimage-text multimodal inputs. Unlike encoding multimodal inputs as in existing methods,\ndecoding interleaved multimodal outputs is challenging due to the complex interleaving layout\nstructures and the long-context requirement of images. Our approach tackles the interleaved\nlayout learning using a unique <dream> token that predicts the placement of images within texts.\nHarnessing DREAMLLM\u2019s causal nature, all contents are generated with history multimodal\ncontexts of any length. This interleaved generative pretraining (I-GPT) inherently forms all\njoint, marginal, and conditional distributions of images and texts in the document, leading to a\nlearning synergy that grounds DREAMLLM\u2019s comprehension in creation and vice versa.\nExtensive experiments across various vision-language comprehension, content creation, and language-\nonly tasks demonstrate DREAMLLM\u2019s superior performance as a zero-shot multimodal generalist.\nFor instance, DREAMLLM-7B achieves an 8.46 FID on MS-COCO and sets a new standard with\n49.1/35.9 scores on MMBench and MM-Vet evaluations, respectively. Moreover, we delve into\nthe learning synergy between comprehension and creation, revealing decent in-context generation\ncapabilities. With I-GPT pretraining, DREAMLLM generates interleaved documents following\nhuman prompts after supervised fine-tuning on instruction-following data curated with GPT-4. To\nour knowledge, this work is the first to enable MLLMs to create free-form interleaved content with a\nlearning synergy on both sides. As a foundational learning framework, DREAMLLM is adaptable\nacross all modalities, laying a promising foundation for future multimodal learning research.\n2\nPublished as a conference paper at ICLR 2024\n2\nBACKGROUND & PROBLEM STATEMENT\nAutoregressive Generative Modeling Given the joint probability distribution p\u03b8(w) over a sequence\nw = {wt}T\nt=1 with length T, the canonical causal generation (Mikolov et al., 2010; Radford\net al., 2018; 2019) of every token wt by a \u03b8-parameterized language model F is modeled as\np\u03b8(w) = QT\nt=1 p\u03b8(wt|w<t). For multimodal comprehension, the sequence could contain K ordered\nimages I = {Ik}K\nk=1 interleaved with words. The k-th image is processed as patch embeddings\nwith visual encoders H\u03d5(\u00b7) like CLIP, which will then be encoded by a projector M\u03b6 (e.g., a\nlinear layer (Huang et al., 2023) or DETR- (Carion et al., 2020)/Perceiver-like (Jaegle et al., 2021)\nResampler (Alayrac et al., 2022)) into L-length visual embeddings Vk = {v\u2113}L\n\u2113=1. Let K(t) be the\nimage number before the t-th word token. The maximum likelihood estimation (MLE) is to minimize\nLMLLM(\u0398 = {\u03b8, \u03b6}, w, I) := \u2212Et\n\u0002\nlog p\u0398(wt|w<t, V<K(t))\n\u0003\n, VK(t) = M\u03b6 \u25e6 H\u03d5(IK(t)). (1)\nDiffusion Models\nDiffusion Models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) are\nprobabilistic generative models that learn the latent structure of data z = {zt}T\nt=1 through continuous-\nT-timestamps information diffusion. DMs involve a forward or diffusion process q that smoothly\nconverts data to Gaussian noise. Given the initial datapoint z1 \u223c q(z1) and diffusion rate \u03b2t := 1\u2212\u03b1t,\nthis process can be defined as a marginal distribution q(zt|z1) := N(\u221a\u03b1tz1, \u03b2tI), and the perturbed\ndata distribution is q(zt) :=\nR\nq(zt|z)q(z)dz by integrating out data density q(z). A reversed\ndenoising probability flow p is used for generating data from noise zT \u223c N(0, I) as a Markov Chain\nwith transition approximated by a Gaussian model p\u03be(zt\u22121|zt) := N(\u00b5\u03be(zt), \u03c32\nt I), which relates to\nan optimal MSE denoiser since q(zt\u22121|zt, z1) is Gaussian with enough timestamps (Feller, 1949;\nSohl-Dickstein et al., 2015). Ho et al. (2020) show that the optimization with the evidence lower\nbound (ELBO) can be simplified by training a denoising U-Net \u03f5\u03be(zt, t) parameterized with \u03be that\nestimates the conditional expectation E[\u03f5 \u223c N(0, I)|zt] (Bao et al., 2022). Let C be the conditional\nembeddings, and the perturbed data zt = \u221a\u03b1tz1 + \u221a1 \u2212 \u03b1t\u03f5, the minimization objective is\nLDM(\u03be, z) := Et\u223cU(0,1),\u03f5\u223cN (0,I)\n\u0002\n\u2225\u03f5\u03be(zt; C, t) \u2212 \u03f5\u22252\u0003\n.\n(2)\nSince \u03f5\u03be(zt; t) = \u2212\u03c3ts\u03be(zt; t) as derived from Tweedie\u2019s (Efron, 2011; Luo, 2022), it is equivalent\nto denoising score matching of \u2207zt log p\u03be(zt) (Hyv\u00e4rinen, 2005; Vincent, 2011), thus DMs are also\ncalled score-function based generative models (Song & Ermon, 2019; 2020; Song et al., 2021; 2023).\n2.1\nHOW CAN WE USE MLLMS FOR DIFFUSION SYNTHESIS THAT SYNERGIZES BOTH SIDES?\nMultimodal signals typically exhibit modality-specific information that has distinct structure but\ncomplementary semantics (Dong et al., 2023). This complementary property allows us to utilize deep\nlanguage comprehension to enhance cross-modal image generation (Saharia et al., 2022). However,\nthe potential of multimodal creation to improve comprehension remains largely unexplored.\nExisting strategies (Koh et al., 2023; Sun et al., 2023b; Ge et al., 2023) integrate successful Diffusion\nModels with MLLMs by aligning the semantic spaces of conditional embeddings between CLIP CCLIP\nand MLLMs CMLLM. The objective is to minimize alignment loss Lalign = D(M\u03c8 \u25e6 CMLLM, CCLIP),\nemploying a distance metric D(\u00b7, \u00b7) and a condition projector M\u03c8. However, CLIP models primarily\nlearn modality-shared semantics, often overlooking modality-specific information due to a modality\ngap (Liang et al., 2022; Liu et al., 2023f). This explicit alignment with CLIP\u2019s intermediate output\nspace may induce more conflicts than synergies, as MLLMs are forced to generate semantically\nreduced information, deviating from their original output space. To circumvent these issues, we\npropose alternative learning methodologies (See Fig. 2), which we elaborate in the ensuing sections.\nLearning Objective Our aim is to leverage MLLMs to model distributions via direct pixel space\nsampling. Here, the pretrained SD functions as a score metric, distilling the learned data distribution.\nThis approach is similar to Score Distillation Sampling (Poole et al., 2023) (SDS, also known as\nScore Jacobian Chaining (Wang et al., 2023a)). In this context, image posterior is learned in a\nDeepDream-like manner (Mordvintsev et al., 2015), using MLLMs\u2019 conditional parameterization.\nConditional Embeddings Rather than converting the output space of MLLMs to align with CLIP,\nwe propose to query MLLMs using learned embeddings. Consequently, MLLMs-enriched semantics\nserve as diffusion conditioning, and the distribution is implicitly modeled through synthesis sampling.\n3\nPublished as a conference paper at ICLR 2024\nInterleaved Documents\n\u201cI like my cute Siamese \ncat.\u201d,\n\u201cShe has beautiful blue \neyes, and she likes to \nlie on her cozy \nnest.\u201d, ...\ndream queries\n<s> I like my \ncute Siamese cat.\nWord \nEmbedding\nProjection\nScore \nDistillation\n<dream>\n<s> I like my \ncute Siamese cat.\nShe has \nbeautiful ... </s>\nVisual \nEncoder\nProjection\nvisual embeddings\nword \nembeddings\nspecial \n<dream> token\nU-Net\nShe has \nbeautiful ... </s>\nWord \nEmbedding\nCausal Multimodal Large Language Model (MLLM)\nInference Stream\nFigure 2: Overview of of our DREAMLLM framework. Interleaved documents serve as input,\ndecoded to produce outputs. Both text and images are encoded into sequential, discrete token\nembeddings for the MLLM input. A special <dream> token predicts where to generate images.\nSubsequently, a series of dream queries are fed into the MLLM, capturing holistic historical semantics.\nThe images are synthesized by the SD image decoder conditioned on queried semantics. The\nsynthesized images are then fed back into the MLLM for subsequent comprehension.\n3\nDREAMLLM\nWe introduce DREAMLLM, a universal learning framework that facilitates both MLLM\u2019s compre-\nhension and creation capabilities. Our DREAMLLM is built with a causal decoder-only LLM F\u03b8 as\nthe model foundation, i.e., Vicuna (Chiang et al., 2023) based on LLaMA (Touvron et al., 2023a)\ntrained on ShareGPT (Zheng et al., 2023). We adopt OpenAI\u2019s CLIP-Large (Radford et al., 2021) as\nthe visual encoder H\u03d5, followed by a linear layer M\u03b6 for visual embedding projection. To synthesize\nimages, we use Stable Diffusion (SD) (Rombach et al., 2022) as the image decoder, and the condition\nprojector M\u03c8 is also a linear layer. An overview of the architecture is depicted in Fig. 2.\n3.1\nEND-TO-END INTERLEAVED GENERATIVE PRETRAINING (I-GPT)\nAll natural documents can be regarded as carriers of text-image interleaved information. Text-only,\nimages-only, and text-image pairs data, on the other hand, can be seen as special cases of interleaved\ncorpora with different modality compositions. Thus, it is critical to empower the model with the\ncapability to learn and generate free-form interleaved documents that form all possible distributions.\nInterleaved Structure Learning To model the interleaved structure, the interleaved sequence is\noperated by extending a new special <dream> token before images. During training, DREAMLLM\nis trained to predict this <dream> token that indicates where an image emerges, and the conditional\nimage synthesis is performed afterward, as introduced next. During inference, DREAMLLM will\ngenerate an image on its \u201cfree will\u201d when this token is predicted.\nConditional Synthesis through Score Distillation To avoid the possible conflicts of CLIP semantics\nand MLLMs stated in Sec. 2.1, we carefully design a different learning objective and conditional\nembeddings. Formally, we introduce a series of learnable dream queries with length Q: d = {dq}Q\nq=1.\nConsidering the t-th token is predicted as <dream> token, the conditional embeddings CDREAMLLM\nK(t)+1\nfor\nthe (K(t) + 1)-th image synthesis can be obtained by causally querying the previous sequences:\nCDREAMLLM\nK(t)+1\n:= F\u03b8(d, x<t+1, V<K(t)+1).\n(3)\nThus, the denoising score matching with latent z is motivated in the similar formulation to Eq. (2):\nLDREAMLLM\nDM\n(\u03b8, d, \u03b6, \u03c8, z) := Et\u223cU(0,1),\u03f5\u223cN (0,I)\n\u0002\n\u2225\u03f5\u03be(zt; CDREAMLLM, t) \u2212 \u03f5\u22252\u0003\n,\n(4)\nwhere \u03be is not updated since the SD is frozen. Eq. (4) can also be viewed as a generalized formulation\nof textual inversion (Gal et al., 2023), but all condition embeddings are learnable by model-seeking.\nFrom the perspective of score distillation (van den Oord et al., 2018), the KL divergence defined by\nconditions and the pre-learned score function is equivalently minimized for distilling (Hinton et al.,\n4\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-shot multimodal comprehension evaluation of image-to-text captioning, general\nVQA, text-related VQA, and comprehensive benchmarks. \u2217 denotes non-zero-shot results for VQA.\nDREAMLLM-7B\u2217 is trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nCaptioning\nVQA\nComprehensive\nCOCO\nI2Paragraph\nVQAv2\nOKVQA\nVizWiz\nTextVQA\nMMBench\nMM-Vet\nComprehension Only MLLMs\nMetaLM (Hao et al., 2022)\n-\n-\n41.1\n11.4\n-\n-\n-\n-\nKosmos-1 (Huang et al., 2023)\n-\n-\n51.0\n-\n29.2\n-\n-\n-\nFlamingo-9B (Alayrac et al., 2022)\n79.4\n-\n51.8\n44.7\n28.8\n-\n-\n-\nOF-9B (Awadalla et al., 2023)\n65.5\n-\n52.7\n37.8\n27.5\n29.1\n4.6\n21.8\nLLaVA-7B (Liu et al., 2023c)\n-\n-\n-\n-\n-\n28.9\n38.7\n23.8\nMLLMs for Comprehension & Creation\nCM3Leon-7B\u2217 (Yu et al., 2023a)\n61.6\n10.5\n47.6\n23.8\n37.6\n-\n-\n-\nEmu-14B (Sun et al., 2023b)\n117.7\n-\n40.0\n34.7\n35.4\n-\n-\n-\nDREAMLLM-7B (Ours)\n115.4\n17.4\n56.6\n44.3\n45.8\n34.9\n49.9\n35.9\nDREAMLLM-7B\u2217 (Ours)\n103.7\n8.4\n72.9\n52.2\n49.3\n41.8\n58.2\n36.6\n2015) learned probability density in conditional image synthesis:\nmin\n\u03b8,d,\u03b6,\u03c8 LDREAMLLM\nDM\n:= Et,CDREAMLLM\nh\nDKL\n\u0000q(zt\u22121|zt, z1, CDREAMLLM) \u2225 p\u03be(zt\u22121|zt)\n\u0001i\n.\n(5)\nUniversal Multimodal Generative Modeling An interleaved document sequence x = {xt}T\nt=1\ncontains both words w = {wi}N\ni=1 and images I = {Ik}K\nk=1. The autoregressive nature forms all\npossible conditional distributions, such as image conditional multimodal comprehension p(w|I)\nor text-to-image synthesis p(I|w). The images are processed as visual embeddings V for causal\ncomprehension. Assuming that the pretrained SD is an optimal score function, Eq. (5) thus could\nbe viewed as an MLE optimization for the synthesis posterior. Different from Eq. (1), the targeted\nsequence xt now could be both encoded images or words. The objective is thus unified to the MLE\nof all causally-conditioned posteriors in arbitrary forms:\nLDREAMLLM\nMLLM\n(\u0398 = {\u03b8, d, \u03b6, \u03c8}, x) := \u2212Et [log p\u0398(xt|x<t)] .\n(6)\n3.2\nMODEL TRAINING\nIn this work, we consider a three-stage training procedure. It can be summarized as follows, and the\nimplementation details, like training data, can be found in Table 13 in Appendix C.\nI Alignment Training This stage is used to alleviate the gap in multimodality, facilitating the\nadaptation of multimodal inputs to LLMs. The linear visual projector, linear condition projector,\nand learnable dream embeddings are pretrained for cross-modal manifold alignment among frozen\nLLMs, visual encoder, and SD. We use approximately 30M image-text pairs data, training both\nimage-to-text comprehension and text-to-image synthesis.\nII I-GPT Pretraining Following alignment, the LLM undergoes an unfrozen process for I-GPT\npretraining (detailed in Sec. 3.1). This critical stage facilitates the learning of joint vision-language\ndistributions via generative modeling. Training incorporates approximately 2M selectively filtered\ndocuments from MMC4-Core (Zhu et al., 2023b), adhering to a CLIP score threshold of 0.25.\nFurthermore, we use 2M paired data samples from LAION400M (Schuhmann et al., 2021),\ncaptioned by BLIP (Li et al., 2022) (i.e., BLIP-LAION), to enhance text-to-image training and\npotentially mitigate the impact of some low-quality noisy images and texts from sMMC4.\nIII Supervised Fine-tuning This stage enables the model to perform general multimodal com-\nprehension and creative tasks following human instructions (Ouyang et al., 2022). We utilize\napproximately 80K visual instruction tuning data collected by Liu et al.. For instruction-following\ncontent creation, GPT-4 is prompted with document summaries or image captions, collecting\napproximately 20K instruction-following document synthesis from MMC4 (InstructMMC4) and\n20K image synthesis data from BLIP captioned LAION400M (Instruct-BLIP-LAION).\n4\nEXPERIMENTS\nDREAMLLM is a versatile multimodal generalist that excels at zero-shot or in-context vision-\nlanguage comprehension and synthesis tasks. In this section, we conduct systematic evaluations for\ndemonstration. See qualitative results in Appendix B and implementation details in Appendix C.\n5\nPublished as a conference paper at ICLR 2024\n4.1\nMULTIMODAL COMPREHENSION\nMultimodal comprehension enables humans to interact with agents conditioned on both words and\nvisual content. We evaluate the multimodal vision and language capabilities of DREAMLLM across\nseveral benchmarks, including image-to-text captioning on COCO (Karpathy & Fei-Fei, 2017) and\nImage2Paragraph (Krause et al., 2017), general visual question answering (VQA) on VQAv2 (Goyal\net al., 2019), OKVQA (Marino et al., 2019), VizWiz (Gurari et al., 2018), and text-related VQA on\nTextVQA (Singh et al., 2019). Additionally, we conducted a zero-shot evaluation on the recently\ndeveloped benchmarks of MMBench and MM-Vet to assess the model\u2019s performance in complex\nmultimodal tasks. The results are presented in Table 1 (See Table 5, and Table 6 in Appendix A).\nAll metrics and data splits are listed in Table 14 in Appendix C. We find that i) DREAMLLM\noutperforms other MLLMs across all benchmarks. Notably, DREAMLLM-7B surpasses concurrent\nMLLMs with image synthesis capabilities by a significant margin, achieving +16.6 higher accuracy\non VQAv2 compared to Emu-13B. ii) On comprehensive benchmarks like MMBench and MM-Vet,\nDREAMLLM achieves state-of-the-art performance against all 7B counterparts. Detailed analysis\nrevealed superior spatial/relation reasoning capabilities in DREAMLLM compared to other MLLMs,\nlikely a result of its image synthesis learning. See qualitative results and comparisons on multimodal\ndialogue in Table 11, Table 12, Fig. 10, Fig. 11, and Fig. 12, in Appendix B.\n4.2\nTEXT-CONDITIONAL IMAGE SYNTHESIS\nTable 2: Zero-shot text-to-image generation FID on\nMS-COCO LN-COCO. LM denotes language model\nbased methods, MG denotes multimodal generation meth-\nods, and FIG denotes free-form interleaved generation\nmethods. \u2020 is fine-tuned SDv2.1 on our state I data. \u2217\ndenotes retrieval-augmentation (Sheynin et al., 2023). \u25b7\ndenotes results after stage I alignment training.\nMethod\nLM MG FIG MS-COCO LN-COCO\nText2Image Specialists\nRetrieval Result (Yu et al.)\n\u2717\n\u2717\n\u2717\n17.97\n33.59\nDALL-E (Ramesh et al.)\n\u2717\n\u2717\n\u2717\n\u223c28\n-\nCogView (Ding et al.)\n\u2717\n\u2717\n\u2717\n27.1\n-\nCogView2 (Ding et al.)\n\u2717\n\u2717\n\u2717\n24.0\n-\nSDv2.1 (Rombach et al.)\n\u2717\n\u2717\n\u2717\n12.43\n34.26\nSDv2.1\u2020 (Rombach et al.)\n\u2717\n\u2717\n\u2717\n11.91\n25.35\nGLIDE (Nichol et al.)\n\u2717\n\u2717\n\u2717\n12.24\n-\nMake-A-Scene (Gafni et al.)\n\u2717\n\u2717\n\u2717\n11.84\n-\nDALL-E 2 (Ramesh et al.)\n\u2717\n\u2717\n\u2717\n10.39\n-\nMuse-3B (Chang et al.)\n\u2713\n\u2717\n\u2717\n7.88\n-\nImagen-3.4B (Saharia et al.)\n\u2713\n\u2717\n\u2717\n7.27\n-\nParti-20B (Yu et al.)\n\u2713\n\u2717\n\u2717\n7.23\n15.97\nMultimodal Large Language Models\nCM3-13B (Aghajanyan et al.) \u2713\n\u2713\n\u2717\n29.56\n-\nGILL-8B (Koh et al.)\n\u2713\n\u2713\n\u2717\n12.20\n-\nEmu-13B (Sun et al.)\n\u2713\n\u2713\n\u2717\n11.66\n-\nCM3Leon-7B\u2217 (Yu et al.)\n\u2713\n\u2713\n\u2717\n10.82\n-\nDREAMLLM-7B\u25b7 (Ours)\n\u2713\n\u2713\n\u2713\n8.76\n22.42\nDREAMLLM-7B (Ours)\n\u2713\n\u2713\n\u2713\n8.46\n20.53\nText2Image is one of the most commonly\nused techniques for creative content gener-\nation that follows human\u2019s fabulous imag-\ninations through free-form languages.\nWe assess text-conditional image synthe-\nsis on the MS-COCO validation set (Lin\net al., 2014) and LN-COCO, the COCO\nsubset of Localized Narratives (Pont-\nTuset et al., 2020),\nfollowing prior\nworks (Xu et al., 2018; Yu et al., 2022b).\nThe MS-COCO dataset primarily con-\ntains high-level image abstractions with\nshorter captions, whereas LN-COCO pro-\nvides more comprehensive image descrip-\ntions (Yu et al., 2022b). DREAMLLM\nsamples 8 images per text prompt on MS-\nCOCO by CLIP score ranking, following\nprevious works (Ramesh et al., 2022). On\nLN-COCO, DREAMLLM samples one\nimage per prompt without CLIP ranking\nsince the text is too long and exceeds the\nCLIP length limit. Note that Parti sam-\nples 16 images per prompt with CoCa (Yu\net al., 2022a). Our evaluation metric is\nthe zero-shot Fr\u00e9chet Inception Distance\n(FID) (Heusel et al., 2017), the results of\nwhich are presented in Table 2. We note three key observations: i) Our DREAMLLM shows a signifi-\ncant FID improvement over the StableDiffusion baseline after stage-I alignment, reducing the score\nby 3.67 and 11.83 on MS-COCO and LN-COCO, respectively. Further, FID improvements of 3.97\nand 13.73 are achieved after pretraining and supervised fine-tuning. The substantial improvement on\nLN-COCO underscores DREAMLLM\u2019s superior capability in processing long-context information.\nii) When compared to prior specialist models, DREAMLLM delivers competitive results based on\nthe SD image decoder. iii) DREAMLLM consistently outperforms concurrent MLLMs-based image\nsynthesis methods. For instance, DREAMLLM-7B surpasses Emu-13B by a significant 3.20 FID on\nMS-COCO. See qualitative results on text-to-image synthesis in Fig. 13 and Fig. 14 in Appendix B.\n4.3\nMULTIMODAL JOINT CREATION & COMPREHENSION\nFree-form Interleaved Document Creation\nLeveraging the interleaved generative modeling\nfrom I-GPT, DREAMLLM can now generate interleaved documents in a free-form manner. In\n6\nPublished as a conference paper at ICLR 2024\nHelp me write the introduction and illustrations about Vienna city, show several high-quality photographs\nabout the beautiful scenes and buildings such as Vienna International Opera House.\nThe Austrian capital, Vienna, has long been known as the City\nof Dreams, and there is no better place to experience than this\nromantic city during the holiday season. The city\u2019s historic\ncenter is surrounded by a collection of beautiful buildings on\na\nsquare\nmile.\nStroll\nalong\nthe\nfamous\nRing\nBoulevard\nand\nadmire the architecture. Some of the buildings date back to the\n13th century. The Vienna State Opera House is one of the most\nbeautiful buildings. Founded in 1869, the opera house is the\nImagine you are a robot agent in the house. There is a mobile phone on the dining table in the dining\nroom, and you are in the living room. Write a short article about how would you walk through the house to\ngrab the mobile phone with your\nrobotic arm step by step. Include several high-quality photographs to\nillustrate your procedure.\nAs a robot agent in the house, I would walk through the house to grab the mobile\nphone in the dining room as shown in the image below. <dream_1>\nFirst, I would look around the living room to make sure the area is safe and there\nare no obstacles or people that I need to avoid. I would then walk towards the dining\nroom and stop before entering the room.\nNext, I would look for the mobile phone on the table, as shown in the image below.\n<dream_2>\nOnce I have identified the phone, I would then move closer to\nit and extend my robot arm to grab it, as shown in the image\nbelow. <dream_3>\nOnce the phone is grabbed, I would then bring it back to the\nliving room and place it on the table where it belongs.\nhome\nof\nthe\nVienna\nState\nOpera\nand\nthe\nannual\nVienna\nOpera\nBall.\n<dream_1>\nCosmopolitan\nflair,\nan\neclectic mix of architectural styles and a rich cultural heritage come together in Vienna. The city is\nhome to a number of museums, palaces and churches. The Sacher Torte and the Wiener Schnitzel are local\nspecialties. A visit to the romantic Christmas Market in Vienna is also a unique experience. <dream_2>\n<dream_1>\n<dream_2>\nDreamLLM\nDreamLLM\n<dream_1>\n<dream_3>\n<dream_2>\nDreamLLM\nDreamLLM\nDreamLLM\nHuman Instruction\nDreamLLM Generation\nFigure 3: Selected DREAMLLM instruction following interleaved content creation examples.\nNote that each image is created automatically at the location decided by DREAMLLM, and then it\nwill be fed back as multimodal comprehension input for the following content generation.\nFig. 3, we showcase the generated interleaved contents based on human instructions. It demonstrates\nthat: i) DREAMLLM can generate meaningful content per the instructions. ii) The system can\nautonomously create images at any specified location by predicting the proposed <dream> tokens,\nthereby eliminating the need for additional human intervention. This is a more user-friendly approach\ncompared to systems like Emu, which necessitate human input for image generation locations.\nImage Quality Document quality can be influenced by factors such as text content, image quality\n(including image-text alignment), and illustration positioning. To assess the quality of generated\ndocuments, we utilized a held-out instruction-following subset from the constructed InstrcutMMC4\nas a demonstrative tool. This subset comprises 15K documents across 30 MMC4-defined topics, with\n500 samples per topic. We began by evaluating image quality using FID on this subset, generating\neach image based on the corresponding ground truth texts. The results revealed that when using\nonly matched text inputs for image synthesis, SD achieved an FID score of 74.77. In contrast, our\nDREAMLLM significantly outperforms SD with an FID score of 36.62.\nHuman Evaluation\nWe perform a comprehensive human evaluation to assess the quality of the\ngenerated samples. We randomly selected 150 samples (5 per topic) for instruction-following docu-\nment generation, mixing the generated and ground truth MMC4 documents without any identifying\ninformation. Five unbiased volunteers were then asked to determine whether the given samples were\nsupported. Given the presence of duplicate and low-quality images in MMC4, the supportive rate for\nMMC4 was only 77.24%. In contrast, our DREAMLLM model achieves a supportive rate of 60.68%,\nsurpassing the 30% Turing test requirement. This result indicates that the generated documents\ncontain high-quality images placed logically, demonstrating the effectiveness of our model.\n7\nPublished as a conference paper at ICLR 2024\n5\nDISCUSSIONS\n5.1\nSYNERGY BETWEEN CREATION & COMPREHENSION?\nTable 3: Concrete analysis of the synergy between mul-\ntimodal comprehension and creation (image synthesis).\nID denotes whether the interleaved dataset is used during\nthe second stage of pretraining.\nID\nLalign\nMM-Vet\nVQAv2\nCOCO\n0\nStable Diffusion\n\u2717\n-\n-\n-\n12.43\n1\nCreation-only\n\u2717\n\u2717\n-\n-\n8.50\n2\nCreation-only\n\u2713\n\u2717\n-\n-\n8.57\n3\nComprehension-only\n\u2717\n\u2717\n31.0\n55.1\n-\n4\nComprehension-only\n\u2713\n\u2717\n34.4\n54.3\n-\n5\nJoint-learning\n\u2713\n\u2717\n35.9\n56.6\n8.46\n6\nJoint-learning\n\u2713\n\u2713\nN/A\nN/A\nN/A\nTo elucidate the synergy between multi-\nmodal creation and comprehension, we\nmake the comparison among three meth-\nods with DREAMLLM architecture, each\nutilizing identical training data yet differ-\ning in their learning objectives: a) the\nCreation-only baseline, focused solely on\ntext/document-conditional image synthe-\nsis; b) the Comprehension-only baseline,\ndedicated to word generation exclusively;\nc) the Joint-learning method, which is the\ndefault setting of DREAMLLM learning\nboth image and language modeling.\nQuantitative Analysis\nAs per Table 3, the following observations are made: i) The powerful\nlanguage comprehension of LLMs significantly enhances the performance of text-to-image specialists\nlike SD, as evidenced by the impressive 8.50 FID (line 1). ii) The use of interleaved data, such as\nMMC4, can potentially boost multimodal comprehension performance (line 4). iii) The proposed\nI-GPT further synergizes comprehension and creation with improved performance (line 5). iv) When\nincorporating CLIP alignment loss Lalign stated in Section 2.1, our DREAMLLM fails to converge but\nrather ends in a collapsing point (line 6). This indicates that the queries are adaptively learning the\ntrue data distributions, where CLIP semantics are in conflict with MLLM-encoded semantics.\nQuestion: Which animal in\nthe\npicture\nis\nlarger,\nthe cat or the dog?\nAnswer A: The dog in the\npicture\nis\nlarger\nthan\nthe cat.\nAnswer B: The cat in the\npicture\nis\nlarger\nthan\nthe dog.\nQuestion: What's the difference between these two animals?\nAnswer A: In Photo A, there is a large bear sitting on a tree branch,\nleaning against it, and looking over its shoulder. The bear appears to be\na sleeping or resting pose. In Photo B, there is an image of a koala that\nis sitting on a tree branch and leaning over to one side. The koala is in\na different posture compared to the bear in Photo A; it is not asleep or\nresting,\nbut\nrather\nsitting\nand\nleaning.\nThe\nmain\ndifference\nbetween\nthese two animals is their body position and the fact that the bear is\nmuch larger than the koala.\nAnswer B: In the two images, A is a bear and B is a koala. The difference lies\nin\ntheir\nphysical\ncharacteristics,\nhabitat,\nand\ndiet.\nBears\nare\nlarge,\nomnivorous mammals known for their strength and ability to climb trees. They\nhave a distinctive appearance with a large body, short limbs, and a broadsnout.\nBears\nare\nfound\nin\nvarious\nhabitats,\nwith\nsome\nspecies\nliving\nin\nforests,\nmountains, or grasslands. Koalas, on the other hand, are small marsupials native\nto Australia, known for their unique appearance with a fluffy, teddy-bear-like\nfur, a short tail, and large, round ears. Koalas primarily feed on the leaves of\neucalyptus trees and have a specialized dental structure to efficiently process\nthe fibrous plant material. These differences in physical features, habitat, and\ndiet highlight the distinct characteristics of each animal.\nPhoto A:\nPhoto B:\nPhoto:\nFigure 4: Qualitative comparison. Answer A: answer\nfrom comprehension-only models w/o interleaved train-\ning; Answer B: answer from joint-learning models.\nQualitative Analysis\nIn Fig. 4, we\ncompare answers to some examplar VQA\ntasks from comprehension-only and joint\nlearning modules, respectively. It can be\nseen that: i) The joint-learning method\nexhibits superior multimodal comprehen-\nsion, particularly in identifying subject re-\nlationships and attributes like object size.\nii) In multimodal comprehension scenar-\nios involving multiple image inputs, the\njoint-learning approach demonstrates en-\nhanced precision. This improved perfor-\nmance is a natural outcome of I-GPT pretraining, allowing better modeling of multimodal correlations\nin various interleaved documents.\nMultimodal In-Context Generation Multimodal in-context generation is a critical emerging capa-\nbility for MLLMs (Bommasani et al., 2021; Alayrac et al., 2022). While significant strides have been\nmade in in-context visual question answering, in-context image synthesis remains relatively lacking\nin exploration. The multimodal context-conditional image synthesis capabilities of DREAMLLM, as\ndemonstrated in Fig. 5, offer promising insights into this domain. Tasks such as in-context image\nedition, subject-driven image generation, and compositional generation, however, pose significant\nTeddy bear\nMultimodal Input\nDelicious food.\nX.\nDreamLLM\nswims in water\nMultimodal Input\non the beach\nA black dog.\nA dog X.\nin the snow\nlies on sofa\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\n(a) In-context Image Edition\n(b) In-context Subject-Driven Generation \nDreamLLM\nDreamLLM\n(c) In-context Compositional Generation\nA bear.\nA pod of dolphins leaping out of the water in \nan ocean, with a ship on the background.\nA salmon. X.\nA grizzly bear catching a salmon in a \ncrystal clear river surrounded by a forest\nDreamLLM\nMultimodal Input\nDreamLLM\nSD\nA ship on the ocean. X.\nDreamLLM\nFigure 5: Selected DREAMLLM in-context image generation examples. The X in multimodal\ninputs are replaced accordingly by the text prompts shown under the generated images. We show the\nresults of the SD baseline in (c) with only the text prompt X for a comparison.\n8\nPublished as a conference paper at ICLR 2024\nchallenges in a zero-shot setting, particularly without downstream fine-tuning as in DreamBooth (Ruiz\net al., 2023) or attention modification techniques as in Prompt2Prompt (Hertz et al., 2023). Despite\nthese hurdles, Fig. 5 illustrates DREAMLLM\u2019s ability to generate images conditioned on the provided\nimage context. This capability suggests promising potential for DREAMLLM in maintaining subject,\nidentity, and semantic context, thereby paving a new way for resolving these complex tasks.\n5.2\nWHAT IS LEARNED BY DREAMLLM?\nA cat and \na glass \nof whisky.\nA polar \nbear in \nthe forest.\n(b)\nimage\ncross-attention maps of \ndream queries & U-Net latent\ncross-attention maps of \ndream queries & U-Net latent\nimage\n(a)\nFigure 6: Cross-attention of dream queries and the dif-\nfusion U-Net latent. Similar to (Hertz et al., 2023), the\n64 queries can be viewed as 64 \u201cwords\u201d. Each attention\nmap is computed as the cross-attention between each\nquery and the latent feature in the U-Net. The 64 queries\nare ordered as 8\u00d78 grid sequentially, and each attention\nmap is the result averaged across all timestamps.\nDream Query Attention\nIn DREAM-\nLLM, the conditional embedding is de-\nrived from MLLMs with some learned\ndream queries. Fig. 6 demonstrates a vi-\nsualization of the learned cross-attention\nmechanism between these queries and the\ndiffusion latent. Similar to (Hertz et al.,\n2023), we visualize the attention map av-\neraged across all timestamps. It is seen\nthat: i) The query attention is structured,\ndisentangled, and semantically-oriented.\nThis is evidenced by the fact that distinct\nqueries adeptly capture different subject\nand background semantics. ii) Despite\nvarying prompts, attention patterns exhibit\nremarkable similarity as shown in Fig. 6\n(a) and (b). This contrasts with the token\nattentions from the original SD, which are typically text-token dependent. We postulate that this\narises from the model\u2019s causal nature, leading to a consistent semantic structure order.\n6\nRELATED WORKS\nRapid developments have been witnessed in extending LLMs like LLaMA (Touvron et al., 2023a) to\nmultimodal comprehension that enables human interaction with both words and visual content. One\nline of work is built by system integration of LLMs with various functioning agents where language\nacts as general interface (Wu et al., 2023; Gupta & Kembhavi, 2023; Yang et al., 2023b; Liang\net al., 2023; Shen et al., 2023; Yang et al., 2023a; Sur\u00eds et al., 2023), and remarkable success has\nbeen demonstrated in such plugin-style frameworks. Another line of work instead explores training\nLLMs to consume and understand multimodal inputs (Hao et al., 2022; Huang et al., 2023; Chen\net al., 2023b) with parameter-efficient tuning (Hu et al., 2022; Alayrac et al., 2022; Li et al., 2023d;\nZhang et al., 2023e; Zhu et al., 2023a; Ye et al., 2023) and instruction tuning (Xu et al., 2023b;\nLiu et al., 2023c; Dai et al., 2023a). More recently, some approaches have been developed towards\nvisual-interactive multimodal comprehension by precise referring instruction tuning (Zhao et al.,\n2023a; Peng et al., 2023; Chen et al., 2023a; Zhang et al., 2023g). For cross-modal creation, early\nworks generally tokenize the visual contents into discrete VQ codebooks (van den Oord et al., 2017;\nWang et al., 2022; Sun et al., 2022; Lu et al., 2023; Diao et al., 2023; Yu et al., 2023a). Recent works\ninstead explore incorporating MLLMs for image synthesis using text-to-image models such as Stable\nDiffusion, and the objective is to generate conditional embeddings that align pretrained CLIP text\n(i.e., CLIP) or CLIP variant embeddings (Koh et al., 2023; Ge et al., 2023; Sun et al., 2023a;b).\n7\nCONCLUSIONS\nHow can the learning synergy between multimodal content understanding and creation emerge? In\nthis paper, we present DREAMLLM, a learning framework for developing MLLMs that not only\ncomprehends but also creates multimodal content via diffusion models. Through score distillation of\nconditional-image synthesis distributions, we avoid the need for intermediate representation targets\nthat may bring information loss. The employment of interleaved documents further enriches the\nmultimodal distributions, fostering the learning of multimodal encoding and decoding. Our extensive\nempirical evaluations across diverse VL benchmarks demonstrate the effectiveness of DREAMLLM\nand the emerging learning synergy between multimodal content understanding and creation. Besides,\nthis work initiates the first step towards free-form interleaved content creation. As a general learning\nframework, we hope it will spur further research in the multimodal machine learning field.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENT\nThis research is supported by the National Natural Science Foundation of China (20211710187).\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\nMandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model\nof the internet. CoRR, abs/2201.07520, 2022. 6\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao\nGong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida\nNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman,\nand Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2022. 1, 2, 3, 5, 8, 9, 24\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and\nDevi Parikh. VQA: visual question answering. In Int. Conf. Comput. Vis. (ICCV), 2015. 30\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco,\nMitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large\nautoregressive vision-language models. CoRR, abs/2308.01390, 2023. 5, 22, 23, 26, 27, 28\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse\nvariance in diffusion probabilistic models. In Int. Conf. Learn. Represent. (ICLR), 2022. 3\nJames Betker, Goh Gabriel, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\nJoyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh.\nImproving image generation with better captions. 2023. 26, 31\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical\ncommonsense in natural language. In AAAI Conf. Artif. Intell. (AAAI), 2020. 22, 30\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle\nHsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,\nand et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. 1, 8\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. 1, 22, 30\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In Eur. Conf. Comput. Vis. (ECCV), 2020. 3\nHuiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jos\u00e9 Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin\nMurphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image\ngeneration via masked generative transformers. In Int. Conf. Mach. Learn. (ICML), 2023. 6\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based\nsemantic guidance for text-to-image diffusion models. ACM Trans. Graph., 42(4):148:1\u2013148:10, 2023. 31\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. CoRR, abs/2306.15195, 2023a. 9\n10\nPublished as a conference paper at ICLR 2024\nXi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong,\nHassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. Pali:\nA jointly-scaled multilingual language-image model. In Int. Conf. Learn. Represent. (ICLR), 2023b. 1, 9\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impress-\ning gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n4, 22, 26, 29, 31\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR,\nabs/2204.02311, 2022. 22, 30\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. In Adv. Neural Inform. Process. Syst. (NIPS), 2017. 30\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers),\n2019. 22, 30\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\nPascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023a. 9, 22, 23\nWenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object\nhallucination in vision-language pre-training. In Proceedings of the 17th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, 2023b. 23\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-\nefficient exact attention with IO-awareness. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 29\nShizhe Diao, Wangchunshu Zhou, Xinsong Zhang, and Jiawei Wang. Write and paint: Generative vision-\nlanguage models are unified modal learners. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In Adv. Neural\nInform. Process. Syst. (NeurIPS), 2021. 6\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via\nhierarchical transformers. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 6\nRunpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, and Kaisheng Ma. Finding the task-optimal low-bit\nsub-distribution in deep neural networks. In Int. Conf. Mach. Learn. (ICML), 2022. 25\nRunpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma.\nAutoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?\nIn Int. Conf. Learn. Represent. (ICLR), 2023. 3, 32\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke,\nLucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling\nof language models with mixture-of-experts. In Int. Conf. Mach. Learn. (ICML), 2022. 30\nBradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):\n1602\u20131614, 2011. 3\n11\nPublished as a conference paper at ICLR 2024\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. J. Mach. Learn. Res. (JMLR), 23:120:1\u2013120:39, 2022. 30\nWilliam Feller. On the theory of stochastic processes, with particular reference to applications. In Proceedings\nof the [First] Berkeley Symposium on Mathematical Statistics and Probability. The Regents of the University\nof California, 1949. 3\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis. In Int. Conf. Learn. Represent. (ICLR), 2023. 31\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In Eur. Conf. Comput. Vis. (ECCV), 2022. 6\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. In Int. Conf.\nLearn. Represent. (ICLR), 2023. 4, 25\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter V2: parameter-efficient visual instruction model.\nCoRR, abs/2304.15010, 2023. 22, 23\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language\nmodel. CoRR, abs/2307.08041, 2023. 3, 9\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and\nIshan Misra. Imagebind one embedding space to bind them all. In IEEE/CVF Conf. Comput. Vis. Pattern\nRecog. (CVPR), 2023. 32\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,\nPing Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. CoRR,\nabs/2305.04790, 2023. 22, 23\nYash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe V in VQA matter: Elevating the role of image understanding in visual question answering. Int. J. Comput.\nVis. (IJCV), 127(4):398\u2013414, 2019. 6, 30\nAnisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language\nmodels. CoRR, abs/2308.06394, 2023. 23\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training.\nIn IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 9\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In IEEE/CVF Conf. Comput.\nVis. Pattern Recog. (CVPR), 2018. 6, 30\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. CoRR, abs/2206.06336, 2022. 1, 5, 9, 22\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. In Int. Conf. Learn. Represent. (ICLR), 2021. 22, 30\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt\nimage editing with cross-attention control. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst.\n(NIPS), 2017. 6, 30\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR,\nabs/1503.02531, 2015. 4\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep\nGenerative Models and Downstream Applications, 2021. 30\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2020. 2, 3\n12\nPublished as a conference paper at ICLR 2024\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma,\nBen Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video\ngeneration with diffusion models. CoRR, abs/2210.02303, 2022a. 31\nJonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.\nVideo diffusion models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022b. 31\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie\nMillican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models.\nCoRR, abs/2203.15556, 2022. 30\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm:\nInjecting the 3d world into large language models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 32\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent. (ICLR), 2022. 9\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav\nChaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with\nlanguage models. CoRR, abs/2302.14045, 2023. 1, 2, 3, 5, 9, 22, 24\nHuggingface.\nTransformers agent,\n2023.\nURL https://huggingface.co/docs/transformers/\ntransformers_agents. Accessed: 2023-07-20. 23\nAapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.\n(JMLR), 6:695\u2013709, 2005. 3\nBrian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel\nHo, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina\nParada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng\nXu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu,\nDiego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng\nKuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana\nGopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding\nlanguage in robotic affordances. In Annu. Conf. Robot. Learn. (CoRL), 2022. 32\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional\nadversarial networks. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 31\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Jo\u00e3o Carreira. Perceiver:\nGeneral perception with iterative attention. In Int. Conf. Mach. Learn. (ICML), 2021. 3\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision.\nIn Int. Conf. Mach. Learn. (ICML), 2021. 31\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361,\n2020. 30, 31\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE\nTrans. Pattern Anal. Mach. Intell. (TPAMI), 39(4):664\u2013676, 2017. 6, 30\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. In\nAdv. Neural Inform. Process. Syst. (NeurIPS), 2023. 1, 2, 3, 6, 9\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. In Int. Conf. Learn. Represent. (ICLR), 2021. 32\nJonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for generating\ndescriptive image paragraphs. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 6, 30\nShane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. Minds Mach., 17\n(4):391\u2013444, 2007. 1\n13\nPublished as a conference paper at ICLR 2024\nAlexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is\nsecretly a zero-shot classifier. In Int. Conf. Comput. Vis. (ICCV), 2023a. 32\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. CoRR, abs/2305.03726, 2023b. 23\nDongxu Li, Junnan Li, and Steven C. H. Hoi. Blip-diffusion: Pre-trained subject representation for controllable\ntext-to-image generation and editing. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023c. 25\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In Int. Conf. Mach. Learn. (ICML), 2022. 2, 5, 26,\n29\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In Int. Conf. Mach. Learn. (ICML), 2023d. 2,\n9, 23, 27, 28\nYanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image\npre-training via masking. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023e. 2\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. CoRR, abs/2305.10355, 2023f. 23, 30\nLong Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding\nof text-to-image diffusion models with large language models. CoRR, abs/2305.13655, 2023. 31\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou. Mind the gap: Understanding\nthe modality gap in multi-modal contrastive representation learning. In Adv. Neural Inform. Process. Syst.\n(NeurIPS), 2022. 1, 3\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang\nMao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting\nfoundation models with millions of apis. CoRR, abs/2303.16434, 2023. 9\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE/CVF\nConf. Comput. Vis. Pattern Recog. (CVPR), 2023. 31\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in context. In Eur. Conf. Comput. Vis. (ECCV),\n2014. 6, 30\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal\nmodel with robust instruction tuning. CoRR, abs/2306.14565, 2023a. 23\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.\nCoRR, abs/2310.03744, 2023b. 5, 22, 23, 24, 29\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2023c. 5, 9, 22, 23, 26, 27, 28, 29\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Int. Conf. Comput. Vis. (ICCV), 2023d. 31, 32\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around\nplayer? CoRR, abs/2307.06281, 2023e. 22, 30\nZhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. ISS: image as stepping stone for text-guided\n3d shape generation. In Int. Conf. Learn. Represent. (ICLR), 2023f. 3\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic repre-\nsentations for vision-and-language tasks. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume 32, 2019.\n29\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UNIFIED-IO: A\nunified model for vision, language, and multi-modal tasks. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nCalvin Luo. Understanding diffusion models: A unified perspective. CoRR, abs/2208.11970, 2022. 3\n14\nPublished as a conference paper at ICLR 2024\nHaley MacLeod, Cynthia L. Bennett, Meredith Ringel Morris, and Edward Cutrell. Understanding blind people\u2019s\nexperiences with computer-generated captions of social media images. In Proceedings of the 2017 CHI\nConference on Human Factors in Computing Systems, CHI \u201917, pp. 5988\u20135999, New York, NY, USA, 2017.\nAssociation for Computing Machinery. ISBN 9781450346559. 23\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question\nanswering benchmark requiring external knowledge. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR),\n2019. 6, 30\nTom\u00e1s Mikolov, Martin Karafi\u00e1t, Luk\u00e1s Burget, Jan Cernock\u00fd, and Sanjeev Khudanpur. Recurrent neural\nnetwork based language model. In Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH), 2010. 3\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99\u2013106, 2022.\n31\nAlexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks.\n2015. URL https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html.\n3\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided\ndiffusion models. In Int. Conf. Mach. Learn. (ICML), 2022. 6, 26, 36\nOpenAI. Introducing chatgpt. 2022. URL https://openai.com/blog/chatgpt. 1, 30\nOpenAI. Gpt-4v(ision) system card. 2023a. URL https://openai.com/research/gpt-4v-system-card. 1,\n26, 27, 28, 30\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023b. URL https://openai.com/research/gpt-4.\n1, 26, 27, 28, 31\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned\nphotographs. In Adv. Neural Inform. Process. Syst. (NIPS), 2011. 29\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In Adv. Neural Inform. Process. Syst. (NeurIPS),\n2022. 5, 30\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot\nimage-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH 2023, Los\nAngeles, CA, USA, August 6-10, 2023, pp. 11:1\u201311:11, 2023. 31\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. CoRR, abs/2212.09748, 2022.\n31\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM:\noutperforming curated corpora with web data, and web data only. CoRR, abs/2306.01116, 2023. 31\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. CoRR, abs/2306.14824, 2023. 9\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR,\nabs/2307.01952, 2023. 31\nJordi Pont-Tuset, Jasper R. R. Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting\nvision and language with localized narratives. In Eur. Conf. Comput. Vis. (ECCV), 2020. 6, 30\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In\nInt. Conf. Learn. Represent. (ICLR), 2023. 2, 3, 31, 32\nZekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with\nreconstruct: Contrastive 3d representation learning guided by generative pretraining. In Int. Conf. Mach.\nLearn. (ICML), 2023a. 32\n15\nPublished as a conference paper at ICLR 2024\nZekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma. VPP: efficient conditional 3d generation via voxel-point\nprogressive representation. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023b. 32\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, Li Yi, and\nKaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. CoRR, abs/2402.17766,\n2024. 32\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018. 3, 30\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3, 30\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable\nvisual models from natural language supervision. In Int. Conf. Mach. Learn. (ICML), 2021. 1, 2, 4, 29\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick,\nAlbin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-\nSen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John\nMellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar,\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\nde Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura\nWeidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey\nIrving. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446,\n2021. 30\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Int. Conf. Mach. Learn. (ICML), 2021. 6, 26, 36\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with CLIP latents. CoRR, abs/2204.06125, 2022. 6, 26, 31, 36\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination\nin image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, 2018. 23\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 1, 2,\n4, 6, 30, 31\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In IEEE/CVF Conf. Comput. Vis.\nPattern Recog. (CVPR), 2023. 9, 25\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and\nMohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In\nAdv. Neural Inform. Process. Syst. (NeurIPS), 2022. 3, 6, 26, 31, 37\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd\nschema challenge at scale. Commun. ACM, 64(9):99\u2013106, 2021. 22, 30\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense\nreasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019. 22, 30\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn\u00e9,\nAlexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Bider-\nman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff,\nAlbert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major,\n16\nPublished as a conference paper at ICLR 2024\nIz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on,\nYacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa,\nAlham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris\nEmezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM:\nA 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. 31\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,\nTheo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million\nimage-text pairs. CoRR, abs/2111.02114, 2021. 5, 29\nChristoph Schuhmann, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco:\n600m synthetic captions from laion2b-en, 2023. URL https://laion.ai/blog/laion-coco/. 29\nYuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion\nmodels. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 25\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. In ACL, 2018. 29\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving\nAI tasks with chatgpt and its friends in huggingface. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 9\nShelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman.\nknn-diffusion: Image generation via large-scale retrieval. In Int. Conf. Learn. Represent. (ICLR), 2023. 6\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\nAshual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation\nwithout text-video data. In Int. Conf. Learn. Represent. (ICLR), 2023a. 31\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal,\nAndrea Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. Text-to-4d dynamic scene generation. In\nInt. Conf. Mach. Learn. (ICML), 2023b. 31\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards VQA models that can read. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR),\n2019. 6, 30\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In Int. Conf. Mach. Learn. (ICML), 2015. 3\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Adv.\nNeural Inform. Process. Syst. (NeurIPS), 2019. 3\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Adv. Neural\nInform. Process. Syst. (NeurIPS), 2020. 3\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In Int. Conf. Learn. Represent.\n(ICLR), 2021. 3\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Int. Conf. Mach. Learn.\n(ICML), 2023. 3, 25\nRobert J Sternberg. Beyond IQ: A triarchic theory of human intelligence. CUP Archive, 1985. 1\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng,\nand Daxin Jiang. Multimodal dialogue response generation. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL, 2022. 9\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: improved training techniques for\nCLIP at scale. CoRR, abs/2303.15389, 2023a. 9\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,\nTiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. CoRR, abs/2307.05222, 2023b.\n1, 2, 3, 5, 6, 9, 24\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.\nCoRR, abs/2303.08128, 2023. 9\n17\nPublished as a conference paper at ICLR 2024\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d:\nHigh-fidelity 3d creation from A single image with diffusion prior. In Int. Conf. Comput. Vis. (ICCV), 2023.\n31\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/\ntatsu-lab/stanford_alpaca, 2023. 31\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971,\n2023a. 1, 4, 9, 22, 29, 31\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela\nFan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. 1, 31\nA\u00e4ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Adv.\nNeural Inform. Process. Syst. (NIPS), 2017. 9\nA\u00e4ron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George\nvan den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe,\nSeb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom\nWalters, Dan Belov, and Demis Hassabis. Parallel wavenet: Fast high-fidelity speech synthesis. In Int. Conf.\nMach. Learn. (ICML), 2018. 2, 4\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2015. 30\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Comput., 23(7):\n1661\u20131674, 2011. 3\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining:\nLifting pretrained 2d diffusion models for 3d generation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023a. 3, 31\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence\nlearning framework. In Int. Conf. Mach. Learn. (ICML), 2022. 9\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In Int. Conf.\nLearn. Represent. (ICLR), 2023b. 31\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer:\nHigh-fidelity and diverse text-to-3d generation with variational score distillation. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2023c. 31\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Int. Conf. Learn. Represent.\n(ICLR), 2022a. 30\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2022b. 31\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. CoRR, abs/2303.04671, 2023. 9\n18\nPublished as a conference paper at ICLR 2024\nJiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary\npanoptic segmentation with text-to-image diffusion models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023a. 32\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan:\nFine-grained text to image generation with attentional generative adversarial networks. In IEEE/CVF Conf.\nComput. Vis. Pattern Recog. (CVPR), 2018. 6\nZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction\ntuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)\n(Volume 1: Long Papers), 2023b. 9\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large\nlanguage model to use tools via self-instruction. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023a. 9\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of GPT-3 for few-shot knowledge-based VQA. In AAAI Conf. Artif. Intell. (AAAI), 2022. 24\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang. MM-REACT: prompting chatgpt for multimodal reasoning and action.\nCoRR, abs/2303.11381, 2023b. 9, 23\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant:\nEfficient and affordable post-training quantization for large-scale transformers. In Adv. Neural Inform. Process.\nSyst. (NeurIPS), 2022. 25\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.\nmplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178,\n2023. 9, 23\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. T. Mach. Learn. Res. (TMLR), 2022a. 2, 6\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason\nBaldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. T.\nMach. Learn. Res. (TMLR), 2022, 2022b. 6, 26, 37\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu,\nHovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh,\nYaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling\nautoregressive multi-modal models: Pretraining and instruction tuning. CoRR, abs/2309.02591, 2023a. 5, 6, 9\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan\nWang. Mm-vet: Evaluating large multimodal models for integrated capabilities. CoRR, abs/2308.02490,\n2023b. 22, 23, 30\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, 2019. 22, 30\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng\nZhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In Int. Conf. Learn.\nRepresent. (ICLR), 2023. 22, 31\nJunbo Zhang, Runpei Dong, and Kaisheng Ma. CLIP-FO3D: learning free open-world 3d scene representations\nfrom 2d dense CLIP. In Int. Conf. Comput. Vis. Worksh. (ICCV Workshop), 2023a. 32\nJunbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma, and Li Yi. Language-assisted 3d\nfeature learning for semantic scene understanding. In AAAI Conf. Artif. Intell. (AAAI), 2023b. 32\nLinfeng Zhang, Xin Chen, Runpei Dong, and Kaisheng Ma. Region-aware knowledge distillation for efficient\nimage-to-image translation. In Brit. Mach. Vis. Conf. (BMVC), 2023c. 31\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\nIn Int. Conf. Comput. Vis. (ICCV), 2023d. 31\n19\nPublished as a conference paper at ICLR 2024\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR,\nabs/2303.16199, 2023e. 9\nRenrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d\npre-trained models via image-to-point masked autoencoders. In IEEE/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023f. 32\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-of-interest. CoRR, abs/2307.03601, 2023g. 9\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer\nlanguage models. CoRR, abs/2205.01068, 2022. 1, 31\nLiang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei\nDong, Chunrui Han, and Xiangyu Zhang. Chatspot: Bootstrapping multimodal llms via precise referring\ninstruction tuning. CoRR, abs/2307.09474, 2023a. 9, 31\nYanli Zhao, Rohan Varma, Chien-Chin Huang, Shen Li, Min Xu, and Alban Desmaison.\nIntro-\nducing pytorch fully sharded data parallel (fsdp) api, 2023b.\nURL https://pytorch.org/blog/\nintroducing-pytorch-fully-sharded-data-parallel-api/. Accessed: 2022-03-14. 29\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with\nmt-bench and chatbot arena. CoRR, abs/2306.05685, 2023. 4, 29\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in\nlarge language models. In Int. Conf. Learn. Represent. (ICLR), 2023. 31\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. CoRR, abs/2304.10592, 2023a. 9, 22,\n23\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: an open, billion-scale corpus of\nimages interleaved with text. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023b. 2, 5, 29, 30\n20\nPublished as a conference paper at ICLR 2024\nCONTENTS\n1\nIntroduction\n1\n2\nBackground & Problem Statement\n3\n2.1\nHow can we use MLLMs for Diffusion synthesis that synergizes both sides? . . . .\n3\n3\nDREAMLLM\n4\n3.1\nEnd-to-End Interleaved Generative Pretraining (I-GPT) . . . . . . . . . . . . . . .\n4\n3.2\nModel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4\nExperiments\n5\n4.1\nMultimodal Comprehension\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nText-Conditional Image Synthesis\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.3\nMultimodal Joint Creation & Comprehension . . . . . . . . . . . . . . . . . . . .\n6\n5\nDiscussions\n8\n5.1\nSynergy between creation & comprehension? . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nWhat is learned by DREAMLLM? . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n6\nRelated Works\n9\n7\nConclusions\n9\nA Additional Experiments\n22\nA.1 Additional Natural Language Understanding Results\n. . . . . . . . . . . . . . . .\n22\nA.2 Additional Multimodal Comprehension Results . . . . . . . . . . . . . . . . . . .\n22\nA.3\nIn-Context Multimodal Comprehension\n. . . . . . . . . . . . . . . . . . . . . . .\n24\nA.4\nSubject-Driven Image Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.5 Additional Ablation Study\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.6 Additional Discussions on Prompt Rewriting Strategy . . . . . . . . . . . . . . . .\n26\nB Additional Qualitative Examples\n26\nC Implementation Details\n29\nC.1\nTraining Data & Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.2\nDREAMLLM Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.3\nEvaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nD Additional Related Works\n30\nE\nLimitations, Failure Cases & Future Works\n31\n21\nPublished as a conference paper at ICLR 2024\nTable 4: Zero-shot natural language processing evaluation. We report the 5-shot result on MMLU\nand the relative performance of DREAMLLM compared to base LLM Vicuna-7B.\nMethod\nCommonsense Reasoning\nReading\nMultitask\nPIQA\nSIQA\nHellaSwag\nWinoGrande\nBoolQ\nMMLU\nLanguage Only Large Language Models (LLMs)\nGPT-3 (Brown et al., 2020)\n81.0\n-\n78.9\n70.2\n60.5\n43.9\nPaLM-540B (Chowdhery et al., 2022)\n82.3\n-\n83.4\n81.1\n88.0\n69.3\nLLaMA-7B (Touvron et al., 2023a)\n79.8\n48.9\n76.1\n70.1\n76.5\n35.1\nVicuna-7B (Chiang et al., 2023)\n77.7\n47.5\n75.7\n67.5\n73.9\n45.0\nMultimodal Large Language Models (MLLMs)\nMetaLM (Hao et al., 2022)\n72.3\n-\n53.5\n56.1\n62.2\n-\nKosmos-1 (Huang et al., 2023)\n72.9\n-\n50.0\n54.8\n56.4\n-\nDREAMLLM-7B (Ours)\n78.6+1.5\n48.8+1.3\n77.4+1.7\n68.5+1.0\n75.2+1.3\n41.8\u22123.2\nTable 5: Zero-shot multimodal comprehension evaluation on MMBench (Liu et al., 2023e) dev\nset. LR: Logical Reasoning, AR: Attribute Reasoning, RR: Relation Reasoning, FP-C: Fine-grained\nPerception (Cross Instance), FP-S: Fine-grained Perception (Single Instance), CP: Coarse Perception.\nDREAMLLM \u2217 is trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nLR\nAR\nRR\nFP-S\nFP-C\nCP\nOverall\nOpenFlamingo-9B (Awadalla et al., 2023)\n4.2\n15.4\n0.9\n8.1\n1.4\n5.0\n6.6\nMMGPT-7B (Gong et al., 2023)\n2.5\n26.4\n13.0\n14.1\n3.4\n20.8\n15.3\nMiniGPT-4-7B (Zhu et al., 2023a)\n7.5\n31.3\n4.3\n30.3\n9.0\n35.6\n24.3\nInstructBLIP-7B (Dai et al., 2023a)\n14.2\n46.3\n22.6\n37.0\n21.4\n49.0\n36.0\nVisualGLM (Zeng et al., 2023)\n10.8\n44.3\n35.7\n43.8\n23.4\n47.3\n38.1\nLLaVA-7B (Liu et al., 2023c)\n16.7\n48.3\n30.4\n45.5\n32.4\n40.6\n38.7\nLLaMA-Adapter V2 (Gao et al., 2023)\n11.7\n35.3\n29.6\n47.5\n38.6\n56.4\n41.2\nMiniGPT-4-13B (Zhu et al., 2023a)\n20.8\n50.7\n30.4\n49.5\n26.2\n50.7\n42.3\nDREAMLLM-7B (Ours)\n15.8\n53.7\n60.9\n53.2\n40.0\n58.3\n49.9\nDREAMLLM-7B\u2217 (Ours)\n23.3\n67.2\n47.8\n58.6\n54.4\n70.5\n58.2\nA\nADDITIONAL EXPERIMENTS\nA.1\nADDITIONAL NATURAL LANGUAGE UNDERSTANDING RESULTS\nWe evaluate the natural language processing capabilities of DREAMLLM post-multimodal adap-\ntation learning via zero-shot experiments on language-only tasks. These included commonsense\nreasoning (PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), Wino-\nGrande (Sakaguchi et al., 2021)), reading comprehension (BoolQ (Clark et al., 2019)), and a general\nmulti-task benchmark (MMLU 5-shot (Hendrycks et al., 2021)). As Table 4 illustrates, DREAMLLM\noutperforms the Vicuna baseline on most language benchmarks. This suggests that DREAMLLM\u2019s\nmultimodal adaptation does not compromise the language learning model\u2019s (LLM) capabilities. When\ncompared to prior Multimodal Language Learning Models (MLLMs), DREAMLLM demonstrates\nsuperior performance, although this may be attributed to the higher baseline results. This finding\nsuggests that a more robust LLM base model could yield improved results.\nA.2\nADDITIONAL MULTIMODAL COMPREHENSION RESULTS\nDetailed Comprehensive Comparison\nThe evaluation results on MMBench (Liu et al., 2023e) and\nMM-Vet (Yu et al., 2023b) are presented in Table 5 and Table 6, respectively. The key observations\nfrom these results are as follows: i) Our DREAMLLM-7B outperforms all other 7B MLLMs, setting\na new benchmark in overall performance. Notably, it even exceeds the performance of some 13B\nmodels, including LLaVA and MiniGPT-4. ii) A detailed capability evaluation reveals DREAMLLM\u2019s\nsuperior performance in fine-grained understanding and relational/spatial comprehension. This\nadvantage is likely due to DREAMLLM\u2019s unique learning synergy, where image distributions are\ncomprehended not solely through language-posterior comprehension but also through creation.\n22\nPublished as a conference paper at ICLR 2024\nTable 6: Zero-shot multimodal comprehension evaluation of core VL capabilities on MM-Vet (Yu\net al., 2023b). \u2021 denotes compositional systems with OpenAI GPT and various interfaces. Rec:\nGeneral Visual Recognition, OCR: Optical Character Recognition, Know: Knowledge, Gen: Language\nGeneration, Spat: Spatial Awareness, Math: Arithmetic Math. DREAMLLM \u2217 is trained using the\nSFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nTF Agent-GPT-4\u2021 (Huggingface, 2023)\n18.2\n3.9\n2.2\n3.2\n12.4\n4.0\n13.4\u00b10.5\nMM-ReAct-GPT-3.5\u2021 (Yang et al., 2023b)\n24.2\n31.5\n21.5\n20.7\n32.3\n26.2\n27.9\u00b10.1\nMM-ReAct-GPT-4\u2021 (Yang et al., 2023b)\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6\u00b10.2\nLLaMA-Adapter v2-7B (Gao et al., 2023)\n16.8\n7.8\n2.5\n3.0\n16.6\n4.4\n13.6\u00b10.2\nOpenFlamingo-9B (Awadalla et al., 2023)\n24.6\n14.4\n13.0\n12.3\n18.0\n15.0\n21.8\u00b10.1\nMiniGPT-4-8B (Zhu et al., 2023a)\n27.4\n15.0\n12.8\n13.9\n20.3\n7.7\n22.1\u00b10.1\nBLIP-2-12B (Li et al., 2023d)\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4\u00b10.2\nMiniGPT-4-14B (Zhu et al., 2023a)\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4\u00b10.4\nOtter-9B (Li et al., 2023b)\n28.4\n16.4\n19.4\n20.7\n19.3\n15.0\n24.6\u00b10.2\nInstructBLIP-14B (Dai et al., 2023a)\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6\u00b10.3\nInstructBLIP-8B (Dai et al., 2023a)\n32.4\n14.6\n16.5\n18.2\n18.6\n7.7\n26.2\u00b10.2\nLLaVA-7B (LLaMA-2) (Liu et al., 2023c)\n32.9\n20.1\n19.0\n20.1\n25.7\n5.2\n28.1\u00b10.4\nLLaVA-13B (LLaMA-2) (Liu et al., 2023c)\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9\u00b10.1\nDREAMLLM-7B (Ours)\n41.8\n26.4\n33.4\n33.0\n31.0\n11.5\n35.9\u00b10.1\nDREAMLLM-7B (Ours)\n42.0\n28.1\n33.2\n33.8\n32.0\n11.5\n36.6\u00b10.1\nTable 7: Zero-shot visual hallucination evaluation on POPE (Li et al., 2023f) using MS-COCO\nval set. Yes denotes the proportion of answering \u201cYes\u201d to the given question, which is better if it is\nmore close to 50%. Objects that do not exist in the image are sampled with three different strategies.\nRandom: random sampling, Popular: top-k most frequent objects in MS-COCO (k = 3), Adversial:\nobjects are first ranked based on co-occurring frequencies, then top-k frequent ones are sampled.\nPOPE\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\nYes (%)\nRandom\nmPLUG-Owl-7B (Ye et al., 2023)\n53.97\n52.07\n99.60\n68.39\n95.63\nLLaVA-13B (Liu et al., 2023c)\n50.37\n50.19\n99.13\n66.64\n98.77\nMMGPT-7B (Gong et al., 2023)\n50.10\n50.05\n100.00\n66.71\n99.90\nMiniGPT-4-14B (Zhu et al., 2023a)\n79.67\n78.24\n82.20\n80.17\n52.53\nInstructBLIP-14B (Dai et al., 2023a)\n88.57\n84.09\n95.13\n89.27\n56.57\nDREAMLLM-7B (Ours)\n86.36\n85.92\n87.93\n86.91\n52.75\nPopular\nmPLUG-Owl-7B (Ye et al., 2023)\n50.90\n50.46\n99.40\n66.94\n98.57\nLLaVA-13B (Liu et al., 2023c)\n49.87\n49.93\n99.27\n66.44\n99.40\nMMGPT-7B (Gong et al., 2023)\n50.00\n50.00\n100.00\n66.67\n100.00\nMiniGPT-4-14B (Zhu et al., 2023a)\n69.73\n65.86\n81.93\n73.02\n62.20\nInstructBLIP-14B (Dai et al., 2023a)\n82.77\n76.27\n95.13\n84.66\n62.37\nDREAMLLM-7B (Ours)\n80.07\n75.74\n88.47\n81.61\n58.40\nAdversarial\nmPLUG-Owl-7B (Ye et al., 2023)\n50.67\n50.34\n99.33\n66.82\n98.67\nLLaVA-13B (Liu et al., 2023c)\n49.70\n49.85\n99.07\n66.32\n99.37\nMMGPT-7B (Gong et al., 2023)\n50.00\n50.00\n100.00\n66.67\n100.00\nMiniGPT-4-14B (Zhu et al., 2023a)\n65.17\n61.19\n82.93\n70.42\n67.77\nInstructBLIP-14B (Dai et al., 2023a)\n72.10\n65.13\n95.13\n77.32\n73.03\nDREAMLLM-7B (Ours)\n72.63\n67.07\n88.93\n76.47\n66.30\nVisual Hallucination\nVisual hallucination, a phenomenon where MLLMs generate non-existent\nobjects or identities in images, significantly compromises their multimodal comprehension capabili-\nties (Dai et al., 2023b; Liu et al., 2023a; Gunjal et al., 2023) and may pose safety risks (MacLeod\net al., 2017; Rohrbach et al., 2018). We assess the robustness of DREAMLLM against visual\nhallucination using the recently developed POPE benchmark (Li et al., 2023f). Refer to Table 7\nfor a detailed comparison with concurrent comprehension-only MLLMs. Our results indicate that\nDREAMLLM-7B exhibits robustness to visual hallucination, matching or surpassing the performance\nof 13B counterparts. Remarkably, DREAMLLM achieves the best or second-best performance in\nthe most challenging setting. We posit that this robust anti-hallucination property stems from a deep\nunderstanding of object concepts and semantics fostered by multimodal creation learning.\n23\nPublished as a conference paper at ICLR 2024\nTable 8: Few-shot multimodal comprehension evaluation. k is the number of in-context examples.\n\u2020 denotes methods using the RICES sample selection approach (Yang et al., 2022). DREAMLLM-7B\u2217\nis trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nVQAv2\nVizWiz\nk=2\nk=4\nk=8\nk=2\nk=4\nk=8\nComprehension Only MLLMs\nKosmos-1 (Huang et al., 2023)\n51.4\n51.8\n51.4\n31.4\n35.3\n39.0\nFlamingo-9B\u2020 (Alayrac et al., 2022)\n-\n56.3\n58.0\n-\n34.9\n39.4\nMLLMs for Comprehension & Creation\nEmu-14B\u2020 (Sun et al., 2023b)\n56.4\n58.4\n59.0\n37.8\n41.3\n43.9\nDREAMLLM-7B (Ours)\n58.1\n59.2\n59.4\n46.1\n46.7\n46.8\nDREAMLLM-7B\u2217 (Ours)\n73.8\n74.4\n73.8\n49.8\n50.3\n49.7\nA.3\nIN-CONTEXT MULTIMODAL COMPREHENSION\nFew-Shot Evaluation In Table 8, we show the results of few-shot (i.e., k-shot and we set k=2,\n4, 8) evaluation by promoting models with a small number of training examples in context. The\nresults demonstrate the strong in-context learning performance of DREAMLLM compared to Emu\nand Flamingo. It shows that DREAMLLM\u2019s effectiveness in leveraging in-context knowledge.\nQualitative Examples In Fig. 7, we present qualitative instances of in-context comprehension using\nDREAMLLM. The illustrations indicate that DREAMLLM, when prompted with specific examples,\nefficiently executes in-context comprehension in the required formats and logic.\nThis is a Welsh Corgi. \nThis is a Koala. \nThis is\na woolly Mammoth.\nWhat is the title of \nthis painting? Answer: \nThe Son of Man.\nWhen was this\nmasterpiece painted?\nAnswer: 1818.\nWho painted this? \nAnswer:\nClaude Monet.\nFilm name: Harry \nPotter and the \nPrisoner of Azkaban\nFilm name: \nBlade Runner 2049\nJoker\nFilm name:\nInput Prompt\nCompletion\nFigure 7: Selected DREAMLLM in-context multimodal comprehension examples.\n24\nPublished as a conference paper at ICLR 2024\nInput Image\ndog\nswimming in \nthe pool\nWearing \nsunglasses\nwearing a \ntop hat\nin the jungle\non the beach\nteapot\nin blue\nas a lamp\nin the jungle\nfloating in \nthe water\npainted in \ngreen\nruns on the \nmountain\nin front of \nwheat field\ncar\nInput Image\nInput Image\nFigure 8: Selected zero-shot subject-driven image generation examples with DREAMLLM. The\nresults demonstrate that DREAMLLM is able to perform zero-shot subject-driven image generation\nwhile preserving image subject details and following generation instructions.\nTable 9: Ablation studies and inference latency of DREAMLLM. The zero-shot FID on MS-COCO\n30K is reported. The inference latency is tested on NVIDIA A800 devices.\n(a)\nThe number of <dream>\nqueries.\nNo. Queries\nCOCOFID\u2193\n32\n9.56\n64\n8.46\n128\n14.24\n(b) Inference latency versus dif-\nferent number of diffusion steps.\nSteps\nDREAMLLM\nSD\n50\n3.65s\n3.46s\n100\n7.02s\n6.84s\n150\n10.41s\n10.22s\nA.4\nSUBJECT-DRIVEN IMAGE GENERATION\nImage consistency is important when generating interleaved content or performing controllable image\ngeneration tasks (Gal et al., 2023; Ruiz et al., 2023). However, MMC4 does not have such image\nconsistency property, which leads to unsatisfactory image consistency results as shown in Fig. 3. To\nfurther verify the effectiveness and potential of DREAMLLM in generating consistent images that\npreserve subject features, we fine-tune stage II pretrained DREAMLLM on subject representation\nlearning data constructed by following the recently proposed controllable image generation method\nBLIP-Diffusion (Li et al., 2023c). We fine-tune DREAMLLM on this small \u223c270K samples data\nfor 20 epochs, and the results are shown in Fig. 8. It demonstrates the effectiveness and promising\npotential of applying DREAMLLM for image-consistent generation.\nA.5\nADDITIONAL ABLATION STUDY\nQuery Number\nIn Table 9a, we show the results of DREAMLLM using different numbers of the\nproposed learnable queries. i.e., <dream> queries. The results show that 64 queries achieve the best\nresult, while 128 may be too many, which may impact the performance. However, the choice of query\nnumber is also related to the choice of training data size and diffusion model. For example, if given\nmore data and a stronger diffusion model image decoder, queries more than 64 may be better.\nInference Latency\nIn Table 9b, we present a comparison of real-time inference latency between\nDREAMLLM and SD. Relative to SD, DREAMLLM introduces a marginal latency cost of 0.2s on\naverage. This is because the latency primarily stems from the computational demands of the diffusion\nU-Net denoising rather than the text condition embedding. To enhance inference efficiency, potential\nstrategies could include the adoption of Consistency Models (Song et al., 2023) or the implementation\nof model compression techniques such as quantization (Yao et al., 2022; Dong et al., 2022; Shang\net al., 2023).\n25\nPublished as a conference paper at ICLR 2024\nTable 10: Language processing and multimodal comprehension & creation capability comparison\nto the rewrite-then-generate baseline.\nMethod\nLanguage Processing\nMultimodal Processing\nPIQA\nSIQA\nHellaSwag\nWinoGrande\nBoolQ\nMMLU\nVQAv2\nMM-Vet\nCOCO\nVicuna-7B (Chiang et al., 2023)\n77.7\n47.5\n75.7\n67.5\n73.9\n45.0\n-\n-\n-\nrewrite-then-generate\n78.2\n48.5\n75.8\n68.3\n77.4\n43.1\n54.2\n34.1\n11.91\nDREAMLLM-7B (Ours)\n78.6\n48.8\n77.4\n68.5\n75.2\n41.8\n56.6\n35.9\n8.46\nA.6\nADDITIONAL DISCUSSIONS ON PROMPT REWRITING STRATEGY\nVery recently, OpenAI has released DELLE-3 (Betker et al., 2023), which proposes to improve\ngenerated image quality by rewriting descriptive and better prompts with GPT-4. This product has\ndemonstrated great success in leveraging LLMs as language-output agents. However, it generally\nrequires a large amount of high-quality data and is limited when applied to image-conditional genera-\ntion tasks. For instance, DALLE-3 necessitates the initial training of a bespoke image captioning\nspecialist capable of producing high-quality descriptive captions, followed by model training in a\ndata-rich environment featuring these written captions. This process is non-trivial, hinging heavily on\nthe availability of substantial volumes of high-quality data. Moreover, such disjoint systems cannot\nguarantee learning synergy. In contrast, our exploration of DreamLLM has essentially unveiled\nthe significant potential of LLMs to attain a comprehensive understanding of multimodality that\ngenuinely comprehends modalities beyond mere language.\nTo make a comparison regarding language processing and multimodal comprehension capabilities\nto this rewrite-then-generate baseline method, we conduct a preliminary study. Given the absence\nof an optimal dataset holding improved prompts, we modify the original MMC4 by using <dream>\nstart & end tokens before and after the specific text prompt that has the highest CLIP similarity\nto a specific image, which can be used as text prompts for image generation. In this setting, we\nonly train the LLMs to output texts, and no image decoders are involved during training. During\ninference, when the model outputs texts encompassed by the <dream> tokens, the texts are used\nfor an off-the-shelf SD image decoder for generating images. After training, we test the model\u2019s\nlanguage processing and multimodal capabilities. The results show that i) the rewrite-then-generate\nmethod achieves similar performance to DREAMLLM. This demonstrates that both methods won\u2019t\nimpact the language capability, which is as expected. ii) the performance of the rewrite-then-generate\nbaseline falls short when compared to DREAMLLM, particularly in the context of text-to-image\ngeneration on the COCO dataset. This underlines the efficacy of the synergistic learning approach\ninherent in DREAMLLM, suggesting its potential superiority over the baseline methodology.\nB\nADDITIONAL QUALITATIVE EXAMPLES\nMultimodal Dialogue\nIn Tables 11 and 12, we present a qualitative comparative analysis of\nVQA results between our model, DREAMLLM, and other state-of-the-art models: GPT-4 (OpenAI,\n2023a;b), LLaVA (Liu et al., 2023c), BLIP-2 (Li et al., 2022), and OpenFlamingo (Awadalla et al.,\n2023). The key findings are as follows: i) DREAMLLM surpasses GPT-4 in providing more\ndetailed and precise responses to given questions. ii) While LLaVA (Liu et al., 2023c) also offers\ndetailed responses, it frequently introduces imaginary elements not present in the image. In contrast,\nDREAMLLM delivers more accurate answers, effectively avoiding this visual hallucination issue.\nThis observation aligns with our earlier findings in Table 7, which underscore the robustness of\nDREAMLLM against visual hallucination. Furthermore, we showcase additional qualitative results\nof the multimodal dialogue in Fig. 10, Fig. 11, and Fig. 12. These figures illustrate DREAMLLM\u2019s\nproficiency in comprehending and generating long-context multimodal information in arbitrary input\nand output formats.\nText-condition Image Synthesis\nIn Fig. 13 and Fig. 14, we show the image examples of DREAM-\nLLM using the same prompts from previous works for a cross reference and comparison, including\nDALL-E (Ramesh et al., 2021), DALL-E 2 (i.e., unCLIP) (Ramesh et al., 2022), GLIDE (Nichol\net al., 2022), Imagen (Saharia et al., 2022), and Parti (Yu et al., 2022b). Similar to Parti, we have\nextended some prompts with new sub-prompts to construct more examples from different prompts.\n26\nPublished as a conference paper at ICLR 2024\nVisual input example, Extreme Ironing:\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\nUser\nWhat is unusual about this image?\nDREAMLLM (Ours)\nThe unusual aspect of this image is that a man is ironing clothes while sitting on the trunk\nof a car parked in traffic. This is unconventional because ironing clothes is typically done\nindoors, on a flat surface, and away from busy streets where cars are passing by. The man\u2019s\nactions of ironing clothes while on the trunk of the car and amidst traffic create an unusual\nand interesting scene.\n[Start a new conversation, and clear the history]\nUser\nWhat\u2019s happening in the scene?\nDREAMLLM (Ours)\nIn the scene, a man is ironing clothes while standing on the back of a car on a busy city\nstreet. The street is filled with traffic and the man is exposed to the dangers of being\nclose to moving vehicles. This unusual activity of ironing clothes while standing on an\nautomobile also draws attention to the man\u2019s determination to accomplish his task despite\nthe challenging urban environment.\nUser\nWhat is unusual about this image?\nLLaVA (Liu et al., 2023c)\nThe unusual aspect of this image is a man ironing clothes on the back of a minivan or\nvan. This is not a typical place to perform this activity, as one would usually iron clothes\nin a more stationary and safe location, such as a home, using a regular ironing board.\nThe scene depicted in the image is peculiar as it involves a makeshift ironing setup on a\nvehicle, which can be both unsafe and unconventional. Additionally, it is not clear how\nthe man is able to maintain balance and stability while ironing clothes in such an unstable\nenvironment.\n[Start a new conversation, and clear the history]\nUser\nWhat\u2019s happening in the scene?\nLLaVA (Liu et al., 2023c)\nThe image depicts a man standing on top of a yellow SUV in a busy city street. He is\nholding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around\nthe scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are\ntwo other people in the vicinity, one close to the man on the car and another further away.\nThe man on the ladder appears to be attracting attention due to his unconventional choice\nof ironing his clothes on top of a moving car. The city street around him is bustling with\nactivity, adding to the unique nature of the scene.\nUser\nWhat is unusual about this image?\nGPT-4 (OpenAI, 2023a;b)\nThe unusual thing about this image is that a man is ironing clothes on an ironing board\nattached to the roof of a moving taxi.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023d)\na man is sitting on the back of a yellow cab\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nThe man is drying his clothes on the hood of his car.\nTable 11: Visual question answering example comparison of DREAMLLM to LLaVA (Liu et al.,\n2023c), GPT-4 (OpenAI, 2023a;b), BLIP-2 (Li et al., 2023d), and OpenFlamingo (Awadalla et al.,\n2023). This table format follows OpenAI (2023a;b).\n27\nPublished as a conference paper at ICLR 2024\nVisual input example, Chicken Nugget Map:\nUser\nCan you explain this meme in detail?\nDREAMLLM (Ours)\nThe meme presents a humorous visual in which a close-up of a plate of fried chicken,\nspecifically chicken nuggets, appears to be a map of the world. The context suggests that\nsomeone has made this map by arranging the chicken nuggets to resemble the shapes\nand outlines of various countries. The image is accompanied by a caption that reads,\n\u201cSometimes I just look at pictures of the earth from space and I marvel at how beautiful it\nis all is.\u201d This hints that the person who created the \u201cmap\u201d might have been inspired by the\nbeauty of our planet from space, and they have used the similar visual pattern to create an\namusing and creative representation of the world using chicken nuggets.\nUser\nCan you explain this meme in detail?\nLLaVA (Liu et al., 2023c)\nThe meme in the image is a creative and humorous take on food, with a focus on chicken\nnuggets, as the center of the universe. The meme begins with the phrase \u201cSometimes I just\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...\u201d The main\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\nresemble the continents and islands on a world map. The meme playfully suggests that the\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\nactually chicken nugget versions of different places. The punchline of the meme appears\nin smaller text at the bottom, reading \u201cI mean, it\u2019s not the real Earth, but how beautiful it\nis all is.\u201d This lighthearted and entertaining meme combines food and humor, using the\nimage of chicken nuggets to create a funny and imaginative representation of the world.\nUser\nCan you explain this meme?\nGPT-4 (OpenAI, 2023a;b)\nThis meme is a joke that combines two unrelated things: pictures of the earth from space\nand chicken nuggets. The text of the meme suggests that the image below is a beautiful\npicture of the earth from space. However, the image is actually of chicken nuggets\narranged to vaguely resemble a map of the world. The humor in this meme comes from\nthe unexpected juxtaposition of the text and the image. The text sets up an expectation of a\nmajestic image of the earth, but the image is actually something mundane and silly.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023d)\nsometimes i just look at pictures of the earth from space and marvel how beautiful it is\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nIt\u2019s a picture of a chicken nugget on the International Space Station.\nTable 12: Visual question answering example comparison of DREAMLLM to LLaVA (Liu et al.,\n2023c), GPT-4 (OpenAI, 2023a;b), BLIP-2 (Li et al., 2023d), and OpenFlamingo (Awadalla et al.,\n2023). This table format follows OpenAI (2023b).\n28\nPublished as a conference paper at ICLR 2024\nTable 13: Training recipes for DREAMLLM. The three training stages are introduced in Section 3.2.\nStage I: Alignment training, Stage II: I-GPT pretraining, Stage III: Supervised fine-tuning.\nStage I\nStage II\nStage III\nConfig\nAlignment\nI-GPT\nSFT\nTraining Hyper-Parameters\nOptimizer\nAdamW\nAdamW\nAdamW\nLearning Rate\n2e-3\n2e-5\n4e-5\nWeight Decay\n0.0\n0.0\n0.0\nTraining Epochs\n1\n1\n3\nWarmup Ratio\n0.003\n0.003\n0.003\nLearning Rate Scheduler\nCosine\nCosine\nCosine\nBatch Size Per GPU\n8\n8\n8\nMaximum Token Length\n2048\n2048\n2048\nUnfreeze LLM\n\u2717\n\u2713\n\u2713\nTraining Data\nDataset\n\u2780 LLaVAPretrain (558K)\n\u2780 MMC4 (2M)\n\u2780 LLaVAInstruct (80K/665K)\n\u2781 BLIP-LAION (8M)\n\u2781 BLIP-LAION (2M)\n\u2781 InstructMMC4 (20K)\n\u2782 LAION400M (11M)\n\u2782 Instruct-BLIP-LAION (20K)\n\u2783 LAION-COCO (11M)\nData Size\n30M\n4M\n120K\nData Type\nPair\nInterleave/Pair\nInstruction\nTraining Cost\nGPU Device\n128\u00d7NVIDIA A800\n128\u00d7NVIDIA A800\n128\u00d7NVIDIA A800\nTraining Time\n\u223c6h\n\u223c10h\n\u223c1.5h\nC\nIMPLEMENTATION DETAILS\nC.1\nTRAINING DATA & HYPER-PARAMETERS\nIn Table 13, we list the detailed training dataset usage and hyper-parameters. The training data are\nconstructed based on the following datasets: a) LAION400M (Schuhmann et al., 2021), b) LAION-\nCOCO (Schuhmann et al., 2023), c) MMC4 (Zhu et al., 2023b), d) BLIP-LAION (Li et al., 2022)\nwhich is filtered and caption by BLIP (Li et al., 2022), e) LLaVAPretrain (Liu et al., 2023c) which\ncontains 558K image-text pairs from BLIP-captioned CC3M (Sharma et al., 2018), SBU (Ordonez\net al., 2011), and LAION400M filtered by LLaVA, f) LLaVAInstruct, which contains 80K/665K\nvisual instruction-following data constructed by LLaVA (Liu et al., 2023c) and LLaVA-1.5 (Liu\net al., 2023b), and g) InstructMMC4, which is our instruction-following interleaved document\ngeneration data curated by prompting GPT-4 to generate instruction based on the text contents of\nMMC4. h) Instruct-BLIP-LAION, which is our instruction-following image synthesis data. Similar\nto InstructMMC4, it is curated by prompting GPT-4 to generate instructions based on image captions.\nUnless otherwise specified, we randomly sample the indicated number of instances from each dataset\nduring the training process.\nC.2\nDREAMLLM MODEL\nLanguage Model We use LLaMA-1 (Touvron et al., 2023a) trained on ShareGPT (Zheng et al.,\n2023) as as the default LLM (i.e., Vicuna-7B1 (Chiang et al., 2023)) following Liu et al. (2023c) to\nendow its instruction-following capacity. During training, we use Flash Attention (Dao et al., 2022)\nand PyTorch FSDP (Zhao et al., 2023b) to accelerate training efficiency.\nVisual Encoder The visual encoder is the publicly available OpenAI CLIP-L/14 (Radford et al.,\n2021) model, which is frozen during the whole process. The images are resized to 224\u00d7224 resolution\nto align with the CLIP pretraining settings, resulting in a sequence of 256 total tokens for each image.\nFollowing prior VL practice (Lu et al., 2019; Liu et al., 2023c), we append a special <IMG> token\nbefore the image sequence and a special <IMG/> at the end of the sequence.\n1Vicuna-7B v1.1: https://huggingface.co/lmsys/vicuna-7b-v1.1.\n29\nPublished as a conference paper at ICLR 2024\nTable 14: Overall descriptions of the evaluation benchmarks for evaluating capabilities, including\nVL comprehension, content creation, and natural language processing (NLP).\nDataset\nTask description\nEval Split\nMetric\nVL Comprehension\nCOCO (Karpathy & Fei-Fei, 2017)\nScene description\ntest\nCIDEr (Vedantam et al., 2015)\nImage2Paragraph (Krause et al., 2017)\nScene description\ntest\nCIDEr (Vedantam et al., 2015)\nVQAv2 (Goyal et al., 2019)\nScene understanding QA\ntest-dev\nVQA Acc (Antol et al., 2015)\nOKVQA (Marino et al., 2019)\nExternal knowledge QA\nval\nVQA Acc (Antol et al., 2015)\nVizWiz (Gurari et al., 2018)\nScene understanding QA\ntest-dev\nVQA Acc (Antol et al., 2015)\nTextVQA (Singh et al., 2019)\nText reading QA\nval\nVQA Acc (Antol et al., 2015)\nMM-Vet (Yu et al., 2023b)\nMultimodal Comprehension\n-\nGPT-4 Eval (Yu et al., 2023b)\nMMBench (Liu et al., 2023e)\nMultimodal Comprehension\ndev\nGPT-3.5 Eval (Liu et al., 2023e)\nPOPE (Li et al., 2023f)\nVisual Hallucination\n-\nAcc, F1-score, Recall, Precision\nCreation\nMS-COCO (Lin et al., 2014)\nText-Conditional Image Synthesis\nval-30K\nFID (Heusel et al., 2017)\nLN-COCO (Pont-Tuset et al., 2020)\nText-Conditional Image Synthesis\nval\nFID (Heusel et al., 2017)\nMMC4 (Zhu et al., 2023b)\nDoc-Conditional Image Synthesis\nheld-out\nFID (Heusel et al., 2017)\nNLP\nSIQA (Sap et al., 2019)\nCommonsense Reasoning\ndev\nAcc\nPIQA (Bisk et al., 2020)\nCommonsense Reasoning\ndev\nAcc\nHellaSwag (Zellers et al., 2019)\nCommonsense Reasoning\ndev\nAcc\nWinoGrande (Sakaguchi et al., 2021)\nCommonsense Reasoning\ndev\nAcc\nBoolQ (Clark et al., 2019)\nReading Comprehension\ndev\nAcc\nMMLU (Hendrycks et al., 2021)\nAggregated Comprehension\ntest\nAcc\nDiffusion Image Decoder We adopt SDv2.1 (Rombach et al., 2022) trained on 512\u00d7512 resolution\nas the default diffusion image decoder. Same as the visual encoder, the SD model is frozen without\nany modifications or training throughout the whole process. When constructing the SD target to\ncompute the MSE loss, we resize the images to 512 resolution to fit its pretraining configuration.\nDream Query We use dream queries to gather semantic context from MLLMs as introduced before in\nSec. 3. Without specifications, we use 64 learnable query embeddings. It is both efficient and effective\nin generating high-quality images. In order to predict when to generate images, we also introduce\nthe special <dream> token, which is appended before the dream query sequence. A <dream/> is\nappended at the end of the sequence, similar to image inputs.\nClassifier-Free Guidance Classifier-free guidance (CFG) (Ho & Salimans, 2021) has been demon-\nstrated successful in generating photo-realistic contents at the cost of acceptable generation diversity.\nThis technique modifies the objective by \u02c6\u03f5 := (1 + s)\u03f5\u03be(xt, t, C) \u2212 s\u03f5\u03be(xt, t, \u2205), where \u2205 is a special\n\u201cempty\u201d condition representation and s is the condition scale. The larger guidance scale generally\nimproves image authenticity while decreasing diversity. We only adopt CFG during inference, and\nthe scale is set to 7.5 by default and 2.0 for MS-COCO text-conditional image generation.\nC.3\nEVALUATION BENCHMARKS\nSystemic evaluations of DREAMLLM regarding VL comprehension, content creation, and NLP\ncapabilities have been conducted. See the used benchmarks and datasets listed in Table 13. During\nthe evaluation, we use the prompt templates listed in Fig. 9.\nD\nADDITIONAL RELATED WORKS\nLarge Language Models\nA flourishing era of Natural Language Processing (NLP) driven by LLMs\nis being experienced, with the parameter size growing over 100B according to the scaling law (Kaplan\net al., 2020). The GPT series of models, starting with GPT-1 (Radford et al., 2018) and followed\nby GPT-2 (Radford et al., 2019), made significant advancements in few-shot learning by scaling up\nthe number of parameters to 175 billion in GPT-3 (Brown et al., 2020). This breakthrough garnered\na lot of attention and paved the way for further research and development in the field. Since then,\nresearchers have focused on developing LLMs by improving the scaling strategy. Several notable\nefforts include Gopher (Rae et al., 2021), GaLM (Du et al., 2022), FLAN (Wei et al., 2022a), Switch-\nTransformer (Fedus et al., 2022), Chinchilla (Hoffmann et al., 2022), and PaLM (Chowdhery et al.,\n2022). Besides, instruction-based tuning techniques are explored for aligning with human prefer-\nences (Christiano et al., 2017; Ouyang et al., 2022). Such success of LLMs has been further solidified\nby the production release of ChatGPT (OpenAI, 2022) and the highly anticipated GPT-4 (OpenAI,\n30\nPublished as a conference paper at ICLR 2024\n2023a;b). Meanwhile, in the community, the open-source LLMs are achieving remarkable progress\nin language capabilities compared to their close-source counterparts. For example, OPT (Zhang et al.,\n2022), BLOOM (Scao et al., 2022), GLM (Zeng et al., 2023), LLaMA (Touvron et al., 2023a;b), and\nFalcon (Penedo et al., 2023) all raised great attention and are been widely deployed. Other methods\nattempt to learn from distillation, such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023).\nText-Conditional Content Creation with Diffusion Models\nThe recent surge in AI-generated\ncontent (AIGC) has been primarily driven by diffusion-based methods, particularly in the realm of\ntext-conditional content creation. Saharia et al. (2022) have achieved astonishing advancements in\nhigh-resolution image synthesis through large-scale pretrained language models and cascaded DMs.\nAnother paradigm, such as SD, focuses on latent spaces and demonstrates superior efficiency and\nperformance (Rombach et al., 2022; Ramesh et al., 2022; Peebles & Xie, 2022; Podell et al., 2023).\nRecently, Lian et al. (2023) propose to enhance the reasoning capability by constructing layouts with\nLLMs. DALLE-3 (Betker et al., 2023) leverages LLMs as agents and proposes to generate images by\nincorporating GPT-4 for providing high-quality and detailed prompts that facilitate image synthesis.\nMotivated by the great success in 2D, a series of works have significantly propelled the 3D synthesis\ndevelopment (Mildenhall et al., 2022; Liu et al., 2023d; Lin et al., 2023; Wang et al., 2023c; Tang\net al., 2023) based on Score Distillation Sampling (SDS) (Poole et al., 2023; Wang et al., 2023a) that\nutilizes pretrained 2D DMs. For text-to-video/4D synthesis, the expansion of pretrained spatial to a\nspatial-temporal factorized U-Net with joint image and video data training has yielded significant\nsuccess (Ho et al., 2022a;b; Singer et al., 2023a;b).\nE\nLIMITATIONS, FAILURE CASES & FUTURE WORKS\nLimitations\nWhile DREAMLLM has made significant strides toward the development of versatile,\ncreative, and foundational MLLMs, it still has several limitations.\nModel scale. The primary constraint pertains to the scale of the LLMs utilized. Current evaluations\nmainly employ 7B LLMs as the base model, and despite the impressive results garnered, the potential\nbenefits of larger model sizes, such as 65B or 130B (Kaplan et al., 2020), are worth future exploration.\nTraining data. The second challenge relates to the quality and quantity of training data (Jia et al.,\n2021). As the model size and capabilities scale up, a corresponding increase in data is crucial.\nHowever, the procurement and refinement of high-quality training data present substantial logistical\nand financial hurdles. For instance, the open-source interleaved dataset MMC4 contains a significant\namount of noise in the form of text and images, like commercial advertisements. This noise could\nadversely affect the model\u2019s output language and image style.\nPrompt sensitivity. The sensitivity of LLMs to human prompts is a known issue (Wei et al., 2022b;\nWang et al., 2023b; Zhou et al., 2023), a challenge that extends to MLLMs. For instance, MLLMs\u2019\npropensity for detailed responses necessitates tailored prompting to elicit concise and short answers,\nwhich is particularly useful when addressing Visual Question Answering (VQA) tasks.\nFailure Cases\nThe main failure cases of DREAMLLM are observed for multiple image-based\ncontent creations. For instance, when presented with two images and a composite instruction such as\n\u201cA and B\u201d, DREAMLLM sometimes generates a single subject that amalgamates the characteristics\nof A and B. This output aligns more closely with the directive \u201cA like B\u201d. This phenomenon is not\nunique to DREAMLLM, but is also observed in specialized compositional generation methodologies,\nsuch as StructureDiffusion (Feng et al., 2023; Chefer et al., 2023). This recurring issue may be\nattributed to the inherent complexity of compositional generation tasks, compounded by the severe\nscarcity of data specific to this domain.\nFuture Works\nAs a simple and general multimodal learning framework, our future work aims to\nenhance the DREAMLLM framework by integrating fine-grained visual comprehension via methods\nlike precise referring instruction tuning (Zhao et al., 2023a). We also plan to expand beyond visual\nand linguistic content comprehension and generation. Several promising research directions include:\n\u2022 Exploring applications of in-context generation capabilities of DREAMLLM to complex tasks such\nas image-to-image translation (Isola et al., 2017; Zhang et al., 2023c;d; Parmar et al., 2023).\n31\nPublished as a conference paper at ICLR 2024\nBased on the image, give the image caption briefly.\n<IMAGE>\nPlease summarize object in one sentence within 10 words.\nUSER:\nThe image depicts\nASSISTANT:\n<ANSWER>\n(a) Image Captioning (Short)\n(b) Image Captioning (Long)\nBased on the image, please describe the image in detail.\n<IMAGE>\nPlease describe the image in detail.\nUSER:\nThe image depicts\nASSISTANT:\n<ANSWER>\n(c) VQA (Short)\nBased on the image, please answer the question. <IMAGE>\nPlease provide an accurate answer within one word.\nUSER:\nThe answer is:\nASSISTANT:\n<ANSWER>\n(d) VQA (Long)\nThis is an exam, please answer according to the image and question.\n<IMAGE>\nPlease provide an accurate and detailed answer.\nUSER:\nASSISTANT:\n<ANSWER>\n<QUESTION>\n<QUESTION>\n(e) Visual Hallucination\nBased on the image, please objectively and accurately indicate whether the object exists.\n<IMAGE>\nUSER:\nASSISTANT:\nThe answer is:\n<ANSWER>\nIs there a <OBJECT> in the image?\nSystem Prompt\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nFigure 9: Prompt templates. (a) Short image captioning includes COCO captioning, and (b) long\nimage captioning includes Image2Paragraph. (c) Short VQA includes VQAv2, VizWiz, OKVQA,\nand TextVQA. (d) Long VQA includes MMBench and MM-Vet. (e) Visual hallucination includes\nPOPE. <IMAGE> is the image representation, <QUESTION> denotes each specific question, <ANSWER>\nis the generated answer, and <OBJECT> is a specific object name in POPE.\n\u2022 The extension of the DREAMLLM model to video data possessing long-term context is natural,\ncapitalizing on the model\u2019s ability to encapsulate temporal knowledge through context consistency.\nThis would give rise to video comprehension and text-to-video generation capabilities.\n\u2022 Utilizing DREAMLLM\u2019s context consistency feature for geometry-preserving tasks, including\n3D content creation (Poole et al., 2023; Qi et al., 2023b; Liu et al., 2023d), representation learn-\ning (Dong et al., 2023; Qi et al., 2023a; Zhang et al., 2023a;f), scene comprehension (Zhang et al.,\n2023b; Hong et al., 2023), and embodied artificial inteligence (Ichter et al., 2022; Qi et al., 2024).\n\u2022 In addition, since the diffusion model is used as the image decoder, it would be interesting to explore\nDMs-based zero-shot perception such as classification (Li et al., 2023a) and segmentation (Xu\net al., 2023a) with the goal of unifying all perception tasks with image synthesis/generation.\n\u2022 Striving to achieve a unified multimodal zero-shot generalist by extending the scope to various\nmodalities using techniques such as ImageBind (Girdhar et al., 2023) and exploring content creation\nmodels in other modalities like audio (Kong et al., 2021).\n32\nPublished as a conference paper at ICLR 2024\nCan you give a short introduction\nto this movie?\nThe Godfather, also known as Il Padrino, is a classic\nAmerican\ncrime\ndrama\nfilm\ndirected\nby\nFrancis\nFord\nCoppola. It is the first installment in The Godfather\ntrilogy,\nwhich\nalso\nincludes\nThe\nGodfather\nPart\nII\n(1974) and The Godfather Part III (1990). The movie is\nwidely regarded as a masterpiece and a milestone in\nthe mafia genre, exploring the story of the Corleone\nfamily, a fictional Italian-American crime family in\nNew\nYork.\nIt\nfollows\nthe\ntransformation\nof\nMichael\nCorleone, the youngest son of the family's aging don,\nVito\nCorleone,\nas\nhe\nrises\nto\npower\nand\neventually\ntakes on the mantle of his father. The film features a\nstrong\ncast,\nwhich\nincludes\nMarlon\nBrando\nas\nVito\nCorleone, Al Pacino as Michael Corleone, James Caan as\nSonny Corleone, Robert Duvall as Tom Hagen, and Diane\nKeaton as Kay Corleone. The Godfather is known for its\niconic scenes, memorable lines, and its impact on the\nHollywood film industry.\nDo you feel dangerous in the scene in \nthe picture?\nGood. Show me an illustration of a panda wearing a \nsuit and drinking the martini like the Godfather.\nThe image depicts a large painted tiger made from a\nvariety\nof\ncolored\nrocks\nand\nstones,\nwhich\nis\na\npiece of artwork rather than a real animal. As an\ninanimate object, the tiger does not pose any danger.\nWhile\nthe\nrepresentation\nof\nthe\ntiger\nmay\nbe\nfrightening\nto\nsome,\nit\nis\nimportant\nto\nremember\nthat this work of art is not capable of causing harm.\nAn illustration of a panda wearing a\nsuit and drinking a martini like the\nGodfather.\nWhat is the name of this painting?\nThe name of this painting is \u201cGirl with a Pearl \nEarring.\u201d\nCan you give a short introduction to this painting?\n\u201cGirl with a Pearl Earring\u201d is a painting by the\nDutch master Johannes Vermeer, created during the\n17th century. The painting features a girl wearing\na\npearl\nearring\nand\na\nheadscarf,\nwith\nher\nhead\ntilted to the side. The use of light and shadow, as\nwell as the intricate details in her clothing and\nthe surrounding environment, are characteristic of\nVermeer's unique painting style.\nWhat emotional value can this store \nbring to human society?\nThe Disney Store, with its whimsical and colorful\ninterior\nand\na\ndizzying\narray\nof\nDisney-themed\nmerchandise, can bring a sense of joy, wonder, and\nnostalgia to human society. Walking into the store\nevokes\nfeelings\nof\nexcitement\nand\ndelight,\nas\ncustomers\nare\nsurrounded\nby\nfamiliar\ncharacters,\nstories,\nand\nmemories\nassociated\nwith\nthe\niconic\nDisney brand. This emotional value can also promote\nimagination and creativity, as children can engage\nwith the merchandise by pretending and playing out\ntheir favorite Disney stories. The store provides a\nunique and uplifting shopping experience for both\nchildren\nand\nadults,\nfostering\npositive\nemotions\nand\ncreating\nmemorable\nmoments\nthat\ncan\nenrich\npersonal experiences and interactions.\nFantastic. Generate some high-quality illustrations \nof Disney-themed merchandise for me.\nAn image of a bag of Disney branded\nmerchandise,\nwith\na\npicture\nof\na\nDisney character in the background.\nWhat\nabout\na\nbig\nDisney\ncharacter\nlike Mickey Mouse?\nPhotograph A:\nPhotograph B:\nWhat is written in these photographs? What is\nthe difference between these two photos? Answer\nin detail about the difference in written words\nand styles.\nIn the two photographs, A and B, the written words\nare different. In Photograph A, the word \u201cDream\u201d\nis written differently, possibly with a blue and\npink background, while in Photograph B, the word\n\u201cDream\u201d appears in a vintage style. The difference\nin\nthe\nstyle\nof\nthe\nword\nand\nthe\nbackground\nsuggests that each photograph presents a different\napproach, possibly signifying different moods or\nthemes. Additionally, the presence of a \u201cllm\u201d in\nPhotograph\nB\nsuggests\na\nvariation\nin\nthe\ntext\ncompared\nto\nonly\nhaving\nthe\nword\n\u201cDream\u201d\nin\nPhotograph A.\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 10: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n33\nPublished as a conference paper at ICLR 2024\nThe man in photo A is Albert Einstein, and the man in photo B is Lionel Messi.\nWho is the man in photo A, and who is the man in photo B?\nPhotograph of man A:\nPhotograph of man B:\nImagine what kind of conversation will these two people have when they sit in the same room?\nPlease give a possible dialogue script.\nIf Albert Einstein and Lionel Messi were to sit in the same room, they might have a fascinating\nconversation about their experiences, knowledge, and perspectives on various topics such as science,\nphysics, and their respective sports. Here is a possible dialogue script:\nEinstein: Good afternoon, Mr. Messi. How are you?\nMessi: Hello, Dr. Einstein. I am doing well. Thank you for having me.\nEinstein: I am pleased to encounter a fellow genius in the world of sports, and I would love to hear\nyour insights on the game.\nMessi: Thank you, sir. As a soccer player, I feel that the key to success is having a strong mindset,\nan understanding of the game, and the ability to adapt to different situations. I try to learn from\neach game and work on my weaknesses.\nEinstein: Indeed, I have said similar things about the nature of intelligence and knowledge. Learning\nfrom past experiences is crucial to personal growth and understanding. It reminds me of a famous\nquote by the German philosopher Johann Gottlieb Fichte: \u201cIf you would be a genius, think deeply and\nlong, and incessantly about one thing. \u201d\nMessi: I understand the value of continuous learning, and I try to apply it in my training and on the\nfield. I am always trying to improve my skills and learn from my colleagues in the club.\nEinstein: That is a wonderful attitude to have. I am also very impressed by your passion for the\nfield. In my opinion, passion is an integral part of success. It fuels the drive to achieve greatness,\nand it is especially evident in my own field of physics. For instance, I once said, \u201cThe passion for\ntruth is the foundation for the sciences.\u201d\nMessi: It is a great feeling to know that I am playing a sport that is full of passionate individuals.\nI am proud to be a part of that world and do my part in contributing to it.\nEinstein: I am sure that you, Mr. Messi, would agree that having a passion for knowledge can lead to\ndiscoveries that change the world. For example, my theory of relativity has had a profound impact on\nour understanding of space, time, and gravity. It was a result of decades of contemplation and\nresearch.\nMessi: The world of sports and the world of science are not so different when it comes to the pursuit\nof excellence. I am honored to be in the presence of a true genius like you, Dr. Einstein.\nEinstein: I feel the same way, and I would like to thank you for this opportunity to engage in a\nconversation about our respective fields. The world is a wondrous place, and I am grateful to be a\npart of it.\nFigure 11: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n34\nPublished as a conference paper at ICLR 2024\nThis image is funny for a few reasons. First, the brown teddy bear is in a swimming pool, which is an\nunexpected and unusual environment for a stuffed bear. Typically, teddy bears are found indoors, in a child's\nroom or on a couch. Swimming pools are associated with human activities, so seeing a teddy bear in a pool\nevokes a sense of humor, as it's an amusing juxtaposition of the bear's typically inanimate and non-aquatic\nnature with the human-centered environment of a pool. Second, the bear is holding up one of its hands, which\ncould be interpreted as a gesture of surprise or even as an attempt to save itself from drowning, adding to\nthe humor. Lastly, the fact that the bear is swimming with a smile on its face, as if it is enjoying itself,\nfurther contributes to the amusing nature of the scene. The combination of the unfamiliar environment and the\nbear's expressions create a funny and entertaining image.\nWow. That's correct. Can you write an article about teddy bears and swimming? Discuss what\nwill happen if teddy bears can swim. Include some high-quality images.\nIf teddy bears can swim, it's likely that they would try to mimic human behavior in the water. They would\nlikely\nfloat\non\ntheir\nbacks,\nkick\ntheir\nlegs,\nand\neven\nhold\ntheir\nbreath\nunderwater.\nOf\ncourse,\nit's\nimpossible to know for certain what teddy bears would do in such a situation, but that doesn't mean you\nshouldn't imagine! Imagine a scene with a teddy bear floating in a pool, wearing a cap and goggles, and\nlaughing with delight as it floats on its back in the water. Imagine the scene with a lemon drop pool float\nand a swim cap on a teddy bear and it would be even more adorable. In the image, you'll see a brown teddy bear\nwearing a swim cap, floating on its back in a swimming pool.\nThat's interesting. So, what do you think a real bear looks like? Show me a high-quality DSLR\nphotograph of a bear in the forest.\nA real bear, such as a brown or black bear, typically has shaggy hair that can be either brown, blond, or\nblack. The exact color can vary depending on the species of bear and the region where it lives. Bears have\nthick fur and are usually large and muscular with strong limbs, which they use for climbing, digging, and\nforaging for food. In the image, you'll see a bear in a forest, walking through the trees.\nCan you explain why this is funny. Think about it carefully and step-by-step.\nThe image is so amusing and cute that it can evoke a feeling of joy and happiness in anyone who sees the image.\nThe teddy bear is floating on its back in the water, and what's even more amusing is the bear is wearing a red\nand yellow shirt with a large brown belly. This makes the bear look even more furry and cute, and even more\nlikely to make you smile.\nThe photo is taken with a DSLR camera, which allows for a high level of detail, sharpness, and clarity in the\nimage. The bear appears to be staring into the distance, focusing on something in the distance. The image\ncaptures the bear's natural habitat, which is a prime example of the forest environment where these animals\nlive.\nThe\nimage\nhighlights\nthe\nbear's\nsize\nand\nstrength,\nalong\nwith\nits\nuntamed\nand\nwild\nnature.\nThis\nphotograph is an excellent example of wildlife photography, capturing a real-life scene that is both visually\nstriking and informative.\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 12: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n35\nPublished as a conference paper at ICLR 2024\nsmall bird with a pale yellow underside light brown crown and back gray tail and wing tips \ntip of tail feather bright yellow black eyes and black strip over eyes\n(a)\nan astronaut riding a horse X, where X \u2208 {\u201cin a photorealistic style\u201d, \u201cin the style of Pop \nArt\u201d, \u201cas a charcoal sketch\u201d, \u201cas a golden relief\u201d}\n(c)\npanda mad scientist mixing sparkling \nchemicals, art station\n(d)\nan espresso machine that makes coffee X, art station, \nwhere X \u2208 {\u201cin a warm scene\u201d, \u201cfrom human soul\u201d}\nan armchair in the shape of an avocado\n(b)\na futuristic city X, where X \u2208\n{\u201cin a synthwave style\u201d, \u201cin vaporwave style\u201d, \u201cmade of water\u201d, \u201cBeijing opera style\u201d}\n(e)\n(f)\nrobots meditating in a vipassana retreat\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 13: DREAMLLM text-conditional image generation examples with prompts from (a-b) DALL-\nE (Ramesh et al., 2021), (c-d) DALL-E 2 (Ramesh et al., 2022), (e-f) GLIDE (Nichol et al., 2022).\n36\nPublished as a conference paper at ICLR 2024\nA sculpture of a duck X. where X \u2208\n{\u201cmade out of transparent glass\u201d, \u201cmade of wool\u201d, \u201cmade of wood\u201d, \u201cmade of paper\u201d}\n(a)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nA couple of glasses are sitting on a table.\n(b)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDowntown X at sunrise. detailed ink wash. where X \u2208\n{\u201cIstanbul\u201d, \u201cAustin\u201d, \u201cBeijing\u201d, \u201cLA\u201d}\n(e)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\n(c)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nA squirrel is inside a giant bright shiny crystal \nball in on the surface of blue ocean. \nThere are few clouds in the sky.\nAn art gallery displaying Monet paintings. The art \ngallery is flooded. Robots are going around the \nart gallery using paddle boards.\nOil-on-canvas painting of a blue\nnight sky with roiling energy.\nA\nfuzzy\nand\nbright\nyellow\ncrescent moon shining at the top.\nBelow the exploding yellow stars\nand radiating swirls of blue,\na\ndistant\nvillage\nsits\nquietly\non\nthe\nright.\nConnecting\nearth\nand sky is a flame-like cypress\ntree\nwith\ncurling\nand\nswaying\nbranches on the left. A church\nspire\nrises\nas\na\nbeacon\nover\nrolling blue hills.\n(d)\na long wooden bench in \nfront of a brick wall\nDreamLLM\nDreamLLM\nDreamLLM\na hot air balloon landing \nin a corn field\n(f)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\na beat-up truck at the base of the Great Pyramid\na wooden deck overlooking a mountain valley\nFigure 14: DREAMLLM text-conditional image generation examples with prompts from (a-c) Imagen\nand DrawBench (Saharia et al., 2022), (d-f) Parti (i.e., PartiPrompts or P2) (Yu et al., 2022b).\n37\n"
  },
  {
    "title": "LMDX: Language Model-based Document Information Extraction and Localization",
    "link": "https://arxiv.org/pdf/2309.10952.pdf",
    "upvote": "57",
    "text": "LMDX: LANGUAGE MODEL-BASED DOCUMENT\nINFORMATION EXTRACTION AND LOCALIZATION\nVincent Perot1\u2217\nKai Kang2\nFlorian Luisier2\nGuolong Su1\nXiaoyu Sun2\nRamya Sree Boppana2\nZilong Wang3\u2020\nJiaqi Mu1\nHao Zhang1\nNan Hua1\n1Google Research\n2Google Cloud\n3University of California, San Diego\n{vperot,kangkai,fluisier,gsu,sunse}@google.com\n{ramyasreeb,jqmu,haozhangthu,nhua}@google.com\nzlwang@ucsd.edu\nABSTRACT\nLarge Language Models (LLM) have revolutionized Natural Language Processing\n(NLP), improving state-of-the-art on many existing tasks and exhibiting emergent\ncapabilities. However, LLMs have not yet been successfully applied on semi-\nstructured document information extraction, which is at the core of many docu-\nment processing workflows and consists of extracting key entities from a visually\nrich document (VRD) given a predefined target schema. The main obstacles to\nLLM adoption in that task have been the absence of layout encoding within LLMs,\ncritical for a high quality extraction, and the lack of a grounding mechanism ensur-\ning the answer is not hallucinated. In this paper, we introduce Language Model-\nbased Document Information EXtraction and Localization (LMDX), a method-\nology to adapt arbitrary LLMs for document information extraction. LMDX can\ndo extraction of singular, repeated, and hierarchical entities, both with and with-\nout training data, while providing grounding guarantees and localizing the entities\nwithin the document. In particular, we apply LMDX to the PaLM 2-S LLM and\nevaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and\nshowing how LMDX enables the creation of high quality, data-efficient parsers.\n1\nINTRODUCTION\nThe recent advent of transformers (Vaswani et al., 2017) and self-supervised pretraining procedures\nhas led to significant progress in Visually Rich Document (VRD) Understanding. Within that field,\nthe task of document information extraction (IE), which consists of extracting key entities within a\nsemi-structured document (e.g. invoice, tax form, paystub, receipt, etc) given a predefined schema,\nhas received a lot of attention from industry and academia due to its importance and wide applicabil-\nity to intelligent document processing workflows. However, document information extraction still\nremains challenging for current generation systems. In particular, information in semi-structured\nforms is organized in complex layout across many possible templates, which requires understand-\ning of the document context, spatial alignment among the different segments of text, and tabular\narrangement of structured entities (e.g. line items on an invoice, deduction items on a paystub, etc.).\nContent on the document can be printed or handwritten, with scanning artefacts like rotation and\ncontrast issues. Moreover, since some business automation workflows require certain level of ac-\ncuracy, they are often integrated with human-in-the-loop interactions for auditing and correction of\npredictions, requiring knowing the precise location of extracted entities to make it a tractable task for\na human rater. Finally, since a quasi-infinite number of document types exist, and that organizations\nhave limited annotation resources, most parsers are built with very small amount of data.\nFrom those complexities emerge the following desiderata of document information extraction sys-\ntems: they should support high-quality extraction of singular, repeated, and hierarchical entities,\nwhile localizing those entities in the document, and doing so with very low or no amount of training\ndata. So far, no publicly disclosed system has been able to address all of those desiderata.\n\u2217Corresponding author\n\u2020Work done during Google internship\n1\narXiv:2309.10952v1  [cs.CL]  19 Sep 2023\nMany current approaches divide the problem in two stages: a text recognition/serialization step,\nusually achieved by an off-the-shelf Optical Character Recognition (OCR) service, followed by a\nparsing step, which finds the relevant entity values from the recognized text. Since the text serializa-\ntion is imperfect, much attention has been given to fusing the text and layout together in the parsing\nstep (Majumder et al., 2020; Garncarek et al., 2021; Hwang et al., 2021; Katti et al., 2018; Denk &\nReisswig, 2019). Hong et al. (2021) proposes to encode the relative 2D distances of text blocks in\nthe attention of the transformer, and learning from unlabeled documents with an area-masking strat-\negy. Lee et al. (2022) proposes encoding the relative token positions with a graph neural network\nwith edges constructed from a beta-skeleton algorithm. It further frames information extraction as\na NER sequence tagging task with an IOB scheme (Ramshaw & Marcus, 1995; Palm et al., 2017)\nwhich allows them to localize the entities. However, IOB does not support extracting hierarchical\nentities, and is not robust to text serialization errors, where an entity is broken in disjoint segments.\nSince text and layout do not contain all the information in the document (e.g. table boundaries,\nlogos), leveraging the image modality has also been extensively explored (Xu et al., 2021; Lee et al.,\n2023; Appalaraju et al., 2021; 2023; Zhang et al., 2022). Xu et al. (2020) uses a separate image\nencoder before adding the output as feature to the token encodings, while Huang et al. (2022) jointly\nmodels the page image patches alongside the tokens, using a word-patch alignment self-supervised\npretraining task to learn an alignment between the modalities.\nOther approaches treat extraction as a sequence generation problem. Powalski et al. (2021) adds an\nauto-regressive decoder on top of a text-layout-image encoder, all initialized from T5 (Raffel et al.,\n2020). Kim et al. (2022) foregoes the text recognition step completely, using a Vision Transformer\nencoder with an auto-regressive decoder pretrained on a pseudo-OCR task on a large document im-\nage corpora, and finetuned on the final extraction parse tree with XML tags for the target extraction\nschema. While this approach allows to predict hierarchical entities, it does not allow localizing\nentities in the document.\nNone of the previously discussed approaches attempt to understand the semantics of the schema and\nits entity types, and instead opt to encode the schema in the model weights through training, hence\nrequiring training data for unseen schemas and document types. QueryForm (Wang et al., 2023b)\nutilizes a prompt encoding both the schema and entity types, allowing the model to do zero-shot\nextraction. Likewise, PPN (Wei et al., 2023) inputs the raw entity types in the encoder itself, and\nuses a scoring matrix to predict the link classes between document tokens and types, with great\nfew-shot performance. However, both approaches are not able to predict hierarchical entities.\nIn parallel, Large Language Models (OpenAI, 2023; Google et al., 2023; Hoffmann et al., 2022)\nhave revolutionized Natural Language Processing, showing the capabilities to solve a task with sim-\nply an instruction (Wei et al., 2022) or a few examples added to the prompt (Brown et al., 2020).\nThis paradigm opens the possibility of extracting entities with very little to no training data. Wang\net al. (2023a) transforms the NER task to a sequence generation task suitable for LLMs by incorpo-\nrating special tokens in the sequence, marking the entity boundaries, and proposes a self-verification\nstrategy limiting the LLM hallucinations. However, this is applicable to text-only scenarios, with\nhallucinations still a possibility.\nThis motivates us to introduce Language Model-based Document Information EXtraction and Lo-\ncalization (LMDX), a methodology for leveraging off-the-shelf LLMs for information extraction on\nsemi-structured documents. Our contributions can be summarized as follows:\n\u2022 We propose a prompt that enables LLMs to perform the document IE task on leaf and\nhierarchical entities with precise localization, including without any training data.\n\u2022 We also propose a layout encoding scheme that communicate spatial information to the\nLLM without any change to its architecture.\n\u2022 We introduce a decoding algorithm transforming the LLM responses into extracted entities\nand their bounding boxes on the document, while discarding all hallucination.\n\u2022 We systematically evaluate the data efficiency of LMDX on multiple public benchmarks\nand establish a new state-of-the-art on those by a wide margin, especially at low-data\nregimes.\nA comparison of LMDX characteristics and other popular document information extraction systems\ncan be found at Table 1.\n2\nTable 1: Comparison of document information extraction systems.\nDocument Information Extraction Systems\nHierarchical entity\nEntity localization\nZero-shot support\nFormNet(v2), LayoutLM(v2), Docformer, Glean, ...\n\u2717\n\u2713\n\u2717\nQueryForm, PPN\n\u2717\n\u2713\n\u2713\nDonut\n\u2713\n\u2717\n\u2717\nLMDX (Ours)\n\u2713\n\u2713\n\u2713\n2\nMETHODOLOGY\n2.1\nOVERVIEW\nOverall, our pipeline is divided into five stages: OCR, chunking, prompt generation, LLM inference\nand decoding, detailed in the following sections. An overview with a simple example can be found\nin Figure 1, with the input and output of each stage showcased.\nOCR\u2019ed Document \n(multi-page)\nDocument chunks \n( x N )\n{\n \"retailer\":\"\",\n \"line_item\":[{\n  \"product_id\":\"\",\n  \"product_price\":\"\"\n }],\n \"subtotal\":\"\"\n}\nLLM Prompts\n( x N )\nTarget Schema\n<Document>\nApple Store xx|yy\n...\nD8050LLA xx|yy\n...\n</Document>\n<Task>\n{\n  \"retailer\":\"\",\n  \"order_date\":\"\",\n  \"line_item\":[{\n    \"prd_number\":\"\",\n    \"item_price\":\"\"\n  }],\n  \"subtotal\":\"\"\n}\n</Task>\n<Extraction>\n<Document>\nApple Store 38|05\n...\nD8050LLA 25|43\n...\n</Document>\n<Task>\n{\n \"retailer\":\"\",\n \"line_item\":[{\n  \"product_id\":\"\",\n  \"product_price\":\"\"\n }],\n \"subtotal\":\"\"\n}\n</Task>\n<Extraction>\nLLM Completions\n( x NK )\n{\n \"retailer\":\n  \"Apple Store xx|yy\",\n \"order_date\":\n  \"June 29, 2023 \nxx|yy\",\n \"line_item\":[{\n  \"prd_number\":\n   \"D8050LL/A xx|yy\",\n  \"item_price\":\n   \"$499.00 72|58\"\n }],\n \"subtotal\": \n  \"$499.00 78|74\"\n}\n</Extraction>\n{\n \"retailer\":\n  \"Apple 38|05\",\n \"line_item\":[{\n  \"product_id\":\n   \"D8050LL/A 25|43\",\n  \"product_price\":\n   \"$100.00 72|43\"\n }],\n \"subtotal\": null\n}\n</Extraction>\nFinal Extraction\n{\n \"retailer\":{\n  \"value\":\"Apple\",\n  \"location\":{\n   \"page\":0,\n   \"bbox\":[18,04,47,08]\n}},\n \"line_item\":[{\n  \"product_id\":{\n   \"value\":\"D8050LL/A\",\n   \"location\":{\n    \"page\":0, ...\n   }},\n  \"product_price\":{\n   \"value\":\"$100.00\",\n   \"location\":{\n    \"page\":0, ...\n   }},...],\n \"subtotal\":{\n  \"value\":\"$499.00\",\n  \"location\":{\n   \"page\": 1,\n   \"bbox\": [70,73,90,77]\n  }},\n}\nPrompt\nGeneration\nLLM \nInference\nDecoding\nInput: Red   Output: Green\nIntermediate results: Grey     \nChunking\nFigure 1: Overview of the LMDX methodology.\n2.2\nOPTICAL CHARACTER RECOGNITION\nWe first use an off-the-shelf OCR service on the document image to obtain words and lines segments,\nalong with their corresponding spatial position (bounding box) on the document. An example of\noutput from that stage on a sample document is given in Appendix A.6.\n2.3\nCHUNKING\nSince a document can be arbitrarily long and that LLMs have limited input token length, the docu-\nment is divided into document chunks so that each is small enough to be processed by the LLM. To\nachieve this, we first divide the document into individual pages, then we iteratively remove the last\nline segments (coming from OCR) until the prompt containing this chunk is below the maximum\ninput token length of the LLM. Lastly, we group those removed lines as a new document page, and\nrepeat the same logic until all chunks are below the input token limit of the LLM. At the end of this\nstage, we have N chunks. The decision to first divide the document by page stems from the observa-\ntion that entities rarely cross page boundaries, and as such this chunking scheme will have minimal\nimpact on the final extraction quality. The algorithm is described in pseudo-code in Appendix A.1.\n2.4\nPROMPT GENERATION\nThe prompt generation stage takes in the N document chunks and creates a LLM prompt for each\nof them. As seen in Figure 2, our prompt design contains the document representation, a description\n3\nof the task, and the target schema representation containing the entities to extract. XML-like tags\nare used to define the start and end of each component.\n<Document>\n{DOCUMENT_REPRESENTATION}\n</Document>\n<Task>\n{TASK_DESCRIPTION}\n{SCHEMA_REPRESENTATION}\n</Task>\n<Extraction>\nFigure 2: Structure of the LLM prompts.\nDocument Representation.\nThe chunk content is represented in the prompt as the concatenation\nof all its segment texts, suffixed with the coordinates of those segments in the following format:\n< segment text > XX|Y Ysegment. Coordinate tokens are built by normalizing the segment\u2019s X\nand Y coordinate, and quantizing them in B buckets, assigning the index of that bucket as the token\nfor a coordinate.\nThis coordinate-as-tokens scheme allows us to communicate the layout modality to the LLM, with-\nout any change to its architecture. There are many variation to that scheme: using OCR line versus\nOCR words as segment, the granularity of the quantization, and the number of coordinates to use\nper segment (e.g. [xcenter, ycenter] versus [xmin, ymin, xmax, ymax]). Appendix A.4 shows how those\nvariations affect the prompt token length. In practice, since LLM context length is still limited, we\nuse line-level segments with 2 coordinates and B = 100 quantization buckets in all our experiments.\nTask Description.\nThe task description is simply a short explanation of the task to accomplish. In\nour experiments, we hard code it to the following: From the document, extract the text values and\ntags of the following entities:.\nSchema Representation.\nThe schema is represented as a structured JSON object, where the keys\nare the entity types to be extracted, and the values correspond to their occurrence (single or multiple),\nand sub-entities (for hierarchical entities). For instance, {\"foo\": \"\", \"bar\": [{\"baz\": []}]} means that\nthe LLM should extract only a single entity of type foo and multiple hierarchical entities of type bar,\nthat could each hold multiple entities of type baz.\nAfter this step, we have N prompts, one for each document chunk. A full example of a prompt on a\ndocument can be found in Appendix A.6.\n2.5\nCOMPLETION TARGETS\nIn this section, we describe the expected LLM completion format, which can be observed in Figure 1.\nLike the schema, the completion is a JSON structured object with the keys being the entity types,\nand values being the extracted information from the document chunk. JSON was chosen as a format\nfor the completion and schema since it supports hierarchical objects, is very token-efficient, and\nusually present in LLMs training data mixtures. Note that the keys in the completion have the same\nordering, occurrence and class (hierarchical or leaf) as the entity types in the schema. The values of\nleaf entities must follow a specific format:\n< text on segment1 > XX|Y Ysegment1\\n < text on segment2 > XX|Y Ysegment2\\n ...\nAn entity can span multiple (potentially disjoint) OCR segments (lines or words). For each segment\nof the entity, the value contains the entity text on that segment, along with the coordinate tokens of\nthat segment, which act as a segment identifier, allowing us to localize the entities and ground the\nmodel prediction (e.g. making sure the extracted value is not a hallucination), as will be detailed in\nSection 2.7.\n4\nMissing entity types are completed by the model with null for singular types, and [] for repeated\ntypes. Samples of completions can be found in Appendix A.6.\n2.6\nLLM INFERENCE\nIn this stage of the pipeline, we run inference on the LLM with the N prompts. For each prompt,\nwe sample K completions from the LLM (for a total of NK completions for the entire document)\nusing TopK sampling. This randomness in the sampling allows to do error correction (e.g. if a\nresponse is not valid JSON, have hallucinated segment coordinate identifier, etc), and increase the\nextraction quality as will be shown in further sections. Note that we still want the inference to be\nfully deterministic so that LMDX\u2019s extractions are the same across two identical documents. To do\nso, we rely on pseudo-random sampling using a fixed seed.\n2.7\nDECODING\nIn this stage, we parse the raw LLM completions into structured entities and their locations.\nConversion to structured entities.\nWe begin by parsing each model completion as a JSON object.\nCompletions that fail to parse are discarded. For each key-value pair in the JSON object, we interpret\nthe key as the entity type and parse the value to get the entity text and bounding box (as detailed in\nthe next paragraph). Predicted entity types that are not in the schema are discarded. If the model\nunexpectedly predicts multiple values for single-occurrence entity types, we use the most frequent\nvalue as the final predicted value. Hierarchical JSON object are recursively parsed as hierarchical\nentities in a similar manner. This algorithm is described in pseudo-code in Appendix A.3.\nEntity Value Parsing.\nWe expect the JSON value to include both text extractions and segment\nidentifiers for each predicted entity, as described in Section 2.5. We first parse the value into its\n(segment text, segment identifier) pairs. For each pair, we look up the corresponding segment\nin the original document using the segment identifier and verify that the extracted text is exactly\nincluded on that segment. Finally, once we have the entity location on all its segments, we get the\nentity bounding box by computing the smallest bounding box encompassing all the OCR words\nincluded in the entity. Entity values with any segments that fail to ground (invalid entity value\nformat, non-existent segment identifier, or non-matching segment text) in the original document are\ndiscarded. The entity value parsing algorithm is described in pseudo-code in Appendix A.2.\nPrediction Merging.\nWe first merge the predicted entities for the same document chunk from the\nK LLM completions through majority voting (Wang et al., 2022). For each entity type, we gather\nthe predicted entities, including empty predictions, across the K completions. The most common\nprediction(s) are selected as the predicted value for that entity type. We then merge the predictions\namong the N document chunks by concatenating them to obtain the document level predictions.\nPrediction Merging for hierarchical entities.\nFor hierarchical entities, we use the entire pre-\ndicted tree value from a single LLM completion, as this method best preserves the parent-child\nrelationship predicted by the model. For each top-level hierarchical entity type, we perform major-\nity voting on all affiliated leaf, intermediate and top-level entity types among K completions as if\nthey are flattened. We then tally the votes with equal weight to determine which completion to use\nfor the prediction, and select the most common one for that hierarchical entity.\n3\nEVALUATION\nWe evaluate the methodology explained in section 2 on public benchmarks using the PaLM 2-S\nLLM, which we call LMDXPaLM 2-S. Note that we use the small version of this LLM due to limited\naccelerator resources, but larger versions could be used, likely leading to higher extraction quality.\nOur training process is composed of two phases as shown in Figure 3. In the first phase we finetune\nPaLM 2-S on a data mixture containing a variety of (document, schema, extraction) tuples. In par-\nticular, this data mixture contains the Payment dataset (Majumder et al., 2020), along with a diverse\nset of publicly available PDF form templates obtained from government websites that we filled with\n5\nsynthetic data using an internal tool, and annotated for schema and entities to extract. The goal of\nthis phase is to train the model to interpret the semantics of the entity types and extraction hierar-\nchy specified in the schema, and find them within the document, along with learning the extraction\nsyntax. Hence, the variety of schemas and documents in this phase is of utmost importance.\nDuring the second phase, starting from the base entity extractor checkpoint from the previous phase,\nwe finetune the LLM on the target to specialize it to do high quality extraction on the target bench-\nmark. At this stage, only the target benchmark data is included in the training mixture. Note that, for\nzero-shot experiments, this second phase is skipped. Furthermore, no document or schema contained\nin the base extraction training phase overlap with the documents and schemas used in the special-\nization training phase. For all training phases, we follow the input and target syntax described in\nsection 2.4 and 2.5.\n3.1\nPARAMETERS\nFor training, we finetune PaLM 2-S using a batch size of 8, a dropout probability of 0.1 and a learning\nrate of 10\u22126 with a standard cross-entropy loss. Once training is done, we select the checkpoint with\nthe lowest loss on the dev set, and report performance on the test set. For LLM inference, we use a\ntemperature of 0.5 and a TopK of 40, sampling 16 responses for each chunk processed by the LLM,\nas described in section 2.6. Finally, for both training and inference, we use an input token length of\n6144 and output token length of 2048. We use line-level segments and only two coordinates [xcenter,\nycenter] with 100 quantization buckets to save on the number of input and output tokens consumed\nby the coordinate-as-tokens scheme.\n3.2\nDATASETS\nVisually Rich Document Understanding (VRDU).\nWang et al. (2023c) introduces a public\nbenchmark for entity extraction from visually-rich documents that includes two datasets: Regis-\ntration Form, containing 6 semantically rich entity types, and Ad-buy Form, containing 14 entity\ntypes with one hierarchical line_item entity. For each dataset, VRDU proposes samples of 10, 50,\n100 and 200 train documents to evaluate the data efficiency of models. It also offers different tasks\nto evaluate the generalization powers of extraction systems: Single Template Learning (STL) where\ntrain/test share the same single template, Mixed Template Learning (MTL) where train/test con-\ntains overlapping sets of templates, and Unseen Template Learning (UTL) where train/test contains\ndisjoint sets of templates. For our experiments, we finetune LMDXPaLM 2-S for 4000 steps on each\ndataset, training data size, and task setup independently and report Micro-F1 through the provided\nevaluation tool. We then compare LMDXPaLM 2-S to the published state-of-the-art baselines.\nConsolidated Receipt Dataset (CORD).\nPark et al. (2019) introduces a benchmark of Indonesian\nreceipts from shops and restaurants, with a target schema of 30 fine-grained entities, grouped into\nmenu, total and subtotal hierarchical entities. CORD1 does not provide a standard evaluation toolkit,\nso we adopt the normalized Tree Edit Distance accuracy metric (Zhang & Shasha, 1989), previously\nintroduced by Kim et al. (2022) on that benchmark, since it is agnostic to the output scheme used\nand considers the hierarchical entities as part of the metric. For our experiments, we use the official\n800train/100dev/100test split, but also sample the first D = 10/50/100/200 documents from the\ntrain split to assess the data efficiency of LMDX on this benchmark. For each data setup, we fine-\n1https://huggingface.co/datasets/naver-clova-ix/cord-v1\nBase Entity \nExtractor\nPaLM 2\nVanilla PaLM 2\nSpecialized \nEntity Extractor\nPaLM 2\nFinetune on \ndiverse \ndocument and \nschemas \nFinetune on \ndocuments \nwith a single \nschema\nFigure 3: LMDX training phases.\n6\ntune LMDX for 12000 steps. For comparison, we also train and evaluate state-of-the-art baselines\nLayoutLMv3LARGE and Donut. Those baselines are detailed in Appendix A.7.\n3.3\nRESULTS\nTable 2: Results of LMDXPaLM 2-S on the different tasks and train data size setups |D| of VRDU, with\nbest performing model results in bold. Unlike the baselines, LMDX can do zero-shot extraction.\n|D|\nModel\nRegistration Form\nAd-buy Form\nSingle\nMixed\nUnseen\nMixed Template\nUnseen\nMicro-F1\nMicro-F1\nMicro-F1\nMicro-F1\nLine Item F1\nMicro-F1\n0\nLMDXPaLM 2-S\n73.81\n71.65\n74.94\n39.74\n21.21\n39.33\n10\nFormNet\n74.22\n63.61\n50.53\n20.47\n5.72\n20.28\nLayoutLM\n65.91\n36.41\n25.54\n20.20\n6.95\n19.92\nLayoutLMv2\n80.05\n69.44\n54.21\n25.36\n9.96\n25.17\nLayoutLMv3\n72.51\n60.72\n21.17\n10.16\n5.92\n10.01\nLMDXPaLM 2-S\n90.88\n87.72\n86.87\n54.35\n39.35\n54.82\n50\nFormNet\n89.38\n85.38\n68.29\n40.68\n19.06\n39.52\nLayoutLM\n86.21\n80.15\n55.86\n39.76\n19.50\n38.42\nLayoutLMv2\n88.68\n84.13\n61.36\n42.23\n20.98\n41.59\nLayoutLMv3\n87.24\n81.36\n47.85\n39.49\n19.53\n38.43\nLMDXPaLM 2-S\n93.06\n91.42\n88.43\n75.08\n65.42\n75.70\n100\nFormNet\n90.91\n88.13\n72.58\n40.38\n18.80\n39.88\nLayoutLM\n88.70\n86.02\n63.68\n42.38\n21.26\n41.46\nLayoutLMv2\n90.45\n88.36\n65.96\n44.97\n23.52\n44.35\nLayoutLMv3\n89.23\n87.32\n57.69\n42.63\n22.08\n41.54\nLMDXPaLM 2-S\n93.97\n92.41\n89.70\n78.05\n69.77\n75.99\n200\nFormNet\n92.12\n90.51\n77.29\n43.23\n21.86\n42.87\nLayoutLM\n90.47\n87.94\n70.47\n44.66\n23.90\n44.18\nLayoutLMv2\n91.41\n89.19\n72.03\n46.54\n25.46\n46.31\nLayoutLMv3\n90.89\n89.77\n62.58\n45.16\n24.51\n44.43\nLMDXPaLM 2-S\n93.97\n92.78\n90.22\n79.82\n72.09\n78.42\nResults for VRDU are presented in Table 2. For all data regimes and tasks, LMDXPaLM 2-S sets a\nnew state-of-the-art by a wide margin. In particular, we find that LMDXPaLM 2-S can extract decently\nwith no training data: it exhibits similar extraction quality at zero-shot than baselines at 10-100\ntrain dataset size (for instance 39.74% Micro-F1 on Ad-Buy Form Mixed Template vs 40.68% for\nFormNet at 50 train documents, or 73.81% Micro-F1 on Registration Single Template vs 74.22%\nfor FormNet at 10 train documents). LMDXPaLM 2-S is also much more data efficient than the base-\nlines: it is at 5.06% Micro-F1 of its peak performance at 10 training documents for Registration\nForm Mixed Template (87.72% vs 92.78% Micro-F1) while LayoutLMv2, the strongest baseline,\nis within 19.75% of its peak performance (69.44% vs 89.19% Micro-F1). Finally, we notice that\nLMDXPaLM 2-S generalizes much better to unseen templates than baselines: on Registration Form,\nLMDXPaLM 2-S has a drop lesser than 5% Micro-F1 on Unseen Template compared to Single Tem-\nplate across all data regimes, while baselines like LayoutLMv2 sees a drop between 19.38% and\n27.32%.\nOn CORD, with results in Table 3, we observe similar trends, highlighting the generalization of\nthe results. At 10 documents, LMDXPaLM 2-S is 4.03% from its peak performance attained at 800\ndocuments, versus 22.34% for the strongest baseline LayoutLMv3LARGE, showcasing the data\nefficiency of the LMDX methodology.\nPerformance on Hierarchical Entities.\nAs seen on Ad-Buy Form tasks, LMDXPaLM 2-S is capa-\nble of grouping line items much better than the baselines (which are using heuristics) for all data\nregimes. In particular, LMDXPaLM 2-S has similar line_item grouping performance at zero-shot than\nthe best baseline at 200 train documents (21.21% versus 25.46% F1 respectively). With all the train-\ning data, LMDXPaLM 2-S scores a 72.09% F1 on line_item, an absolute improvement of 46.63% over\nthe best baseline LayoutLMv2.\n7\nTable 3: LMDXPaLM 2-S results on CORD. Normalized Tree Edit Distance Accuracy is reported.\n|D|\nModel\nn-TED Accuracy\n0\nLMDXPaLM 2-S\n67.47\n10\nDonut\n33.01\nLayoutLMv3LARGE\n73.87\nLMDXPaLM 2-S\n92.27\n50\nDonut\n75.44\nLayoutLMv3LARGE\n87.29\nLMDXPaLM 2-S\n93.80\n100\nDonut\n82.17\nLayoutLMv3LARGE\n91.83\nLMDXPaLM 2-S\n93.64\n200\nDonut\n84.49\nLayoutLMv3LARGE\n94.44\nLMDXPaLM 2-S\n94.73\n800\nDonut\n90.23\nLayoutLMv3LARGE\n96.21\nLMDXPaLM 2-S\n96.30\n3.4\nABLATIONS\nIn this section, we ablate different facets of the LMDX methodology to highlight their relative\nimportance. The results can be found in Table 4 and are discussed below. For all ablations, we\nevaluate on the VRDU Ad-Buy Form Mixed Template task, only changing the ablated facet.\nTable 4: Ablations of Base Entity Extraction Training, Coordinate Tokens, and Sampling and their\nrelative effects on extraction quality. All ablations are done on VRDU Ad-Buy Mixed Template.\n|D|\nLMDXPaLM 2-S\nWithout Base EE Training\nWithout Coordinate Tokens\nWithout Sampling Strategy\nMicro-F1\nMicro-F1\n\u2206 (%)\nMicro-F1\n\u2206 (%)\nMicro-F1\n\u2206 (%)\n0\n39.74\n0.00\n-39.74\n27.59\n-12.15\n39.53\n-0.21\n10\n54.35\n42.91\n-11.44\n39.37\n-14.98\n52.85\n-1.50\n50\n75.08\n66.51\n-8.57\n62.35\n-12.73\n73.88\n-1.20\n100\n78.05\n68.87\n-9.18\n65.14\n-12.91\n77.30\n-0.75\n200\n79.82\n72.25\n-7.57\n65.70\n-14.12\n78.43\n-1.39\nEffects of Base Entity Extraction Training.\nIn this ablation, we remove the first stage training\non the varied data mixture from Figure 3 and directly finetune on the VRDU target task. As seen\non columns 3-4 of Table 4, ablating that training stage leads to significant drop in extraction quality\nin few-shot scenarios and the complete loss of zero-shot extraction ability due to the model not\nrespecting the extraction format, hence failing decoding. As the train set size increases, the degraded\nperformance lessens, from -11.44% to -7.57%, as the model learns the extraction task and the desired\ncompletion format.\nEffects of Coordinate Tokens.\nIn this ablation, we replace the coordinate tokens, which commu-\nnicate the position of each line within the document, by the index of that line. This index still acts\nas a unique identifier for the line segment (required for entity localization and grounding) but does\nnot communicate any position information. An example of a prompt with line index can be found\nin Appendix A.6. As can be seen on columns 5-6 of Table 4, the coordinate tokens are substantially\nimportant to the extraction quality, ranging from 12.15% to 14.98% absolute micro-F1 improvement\nacross the data regimes.\nEffects of Sampling Strategy.\nIn this ablation, we discard our sampling strategy, and instead\nsample a single response from the model. As seen in columns 7-8 of Table 4, this leads to a 0.21%\nto 1.5% drop in micro-F1. While overall minor for quality, the sampling strategy also allows to\ncorrect extraction format mistakes.\n8\n3.5\nERROR ANALYSIS AND LIMITATIONS\nIn this section, we perform an error analysis on the test set to identify common error patterns of\nLMDX. A very common error type we observe is caused by OCR lines grouping multiple se-\nmantically different segments. We show two instance of those cases observed in LMDXPaLM 2-S\non the VRDU Ad-Buy Form in Figure 4.\nIn the first example, prediction for the entity\nline_item/program_desc includes text from the previous column \"Channel\" along with the value in\nthe column \"Description\". From the OCR line bounding boxes, we can see that these two columns\nare grouped as the same OCR line. In the second example, the model confuses between the adjacent\nkeys \"Invoice Period\" and \"Flight Dates\" and extracts invoice dates as flight dates. Similar to the\nfirst example, OCR line bounding boxes show that the invoice dates and the key \"Flight Dates\" are\ngrouped together in the same line although they are semantically different. As LMDXPaLM 2-S uses\nonly coarse line layout information ([xcenter, ycenter] with 100 quantization buckets), the model fails\nin these cases. This is a current limitation of the LMDX system. We believe that incorporating the\nimage modality will help make LMDX more performant and robust to those OCR errors, addressing\nthe current limitations of the system.\nExample 1:\n     line_item/program_desc\nGroundtruth: \n      Local News 6a-630a\nPrediction: \n     WJZ Local News 6a-630a\nGroundtruth: \n     flight_from: 12/24/19\n     flight_to: 12/30/19\nPrediction: \n     flight_from: 11/25/19\n     flight_to: 12/29/19\nExample 2\nFigure 4: Typical error pattern of LMDXPaLM 2-S. In both examples, the detected OCR lines are\nshown in red, the model predicted entities are shown in blue, and the groundtruth entities are shown\nin green. In both cases, the detected OCR lines merge two semantically distinct segments, causing\nthe model to wrongly associate them in its predictions.\n4\nCONCLUSION\nIn this paper, we have introduced LMDX, a methodology that enables using LLMs for information\nextraction on visually rich documents, setting a new state-of-the-art on public benchmarks VRDU\nand CORD. LMDX is the first methodology to allow the extraction of singular, repeated and hierar-\nchical entities, while grounding its predictions and localizing the entities in the document. LMDX\nis extremely data efficient, and even allows high quality extraction at zero-shot on entirely new doc-\nument types and schemas. Nonetheless, since it relies on a LLM, LMDX is more resource-intensive\nthan previous approaches, and its coordinate-as-tokens scheme requires long inputs and outputs. As\nfuture work, we will explore applying the methodology to open-source LLMs and adding the image\nmodality to the system using Large Vision-Language Models.\n5\nREPRODUCIBILITY STATEMENT\nIn order to increase reproducibility, we\u2019ve provided all details of the LMDX methodology. We\u2019ve\nincluded our LLM prompts and completions in Appendix A.6, along with all our algorithms for\nchunking and decoding in Appendix A.1, A.2 and A.3. Furthermore, we\u2019ve provided the exact tar-\nget schemas used in our experiments in Appendix A.5. For CORD specifically, we\u2019ve used a metric\nwith a public implementation (https://github.com/clovaai/donut/blob/master/\ndonut/util.py) and an easy to reproduce sampling strategy for the data-efficiency splits (first\nD train documents).\nFinally, our baselines are publicly available (https://github.com/\nmicrosoft/unilm/tree/master/layoutlmv3, https://github.com/clovaai/\ndonut) and thoroughly detailed in Appendix A.7.\n9\nREFERENCES\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. Doc-\nformer: End-to-end transformer for document understanding. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pp. 993\u20131003, October 2021.\nSrikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran, Yichu Zhou, and R. Manmatha. Doc-\nformerv2: Local features for document understanding, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nTimo I. Denk and Christian Reisswig. Bertgrid: Contextualized embedding for 2d document repre-\nsentation and understanding, 2019.\n\u0141ukasz Garncarek, Rafa\u0142 Powalski, Tomasz Stanis\u0142awek, Bartosz Topolski, Piotr Halama, Micha\u0142\nTurski, and Filip Grali\u00b4n ski. LAMBERT: Layout-aware language modeling for information ex-\ntraction. In Document Analysis and Recognition \u2013 ICDAR 2021, pp. 532\u2013547. Springer Inter-\nnational Publishing, 2021. doi: 10.1007/978-3-030-86549-8_34. URL https://doi.org/\n10.1007%2F978-3-030-86549-8_34.\nRohan Anil Google, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H.\nClark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Mor-\neira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yu-\njing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\nJames Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,\nChristopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxi-\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,\nGuy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,\nJeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Ke-\nnealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li,\nMusic Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Freder-\nick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Mous-\nsalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Mar-\ntin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley,\nAlex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone,\nDaniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan,\nKiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai\nWu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,\nCe Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report,\n2023.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-\nnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language models, 2022.\nTeakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park.\nBROS: A layout-aware pre-trained language model for understanding documents.\nCoRR,\nabs/2108.04539, 2021. URL https://arxiv.org/abs/2108.04539.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for\ndocument ai with unified text and image masking. In Proceedings of the 30th ACM International\nConference on Multimedia, 2022.\n10\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo.\nSpatial de-\npendency parsing for semi-structured document information extraction.\nIn Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021, pp. 330\u2013343, Online, August\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.28. URL\nhttps://aclanthology.org/2021.findings-acl.28.\nAnoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Jo-\nhannes H\u00f6hne, and Jean Baptiste Faddoul.\nChargrid: Towards understanding 2d documents,\n2018.\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,\nWonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document un-\nderstanding transformer. In European Conference on Computer Vision (ECCV), 2022.\nChen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie,\nRenshen Wang, Yasuhisa Fujii, and Tomas Pfister. FormNet: Structural encoding beyond sequen-\ntial modeling in form document information extraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3735\u20133754,\nDublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nacl-long.260. URL https://aclanthology.org/2022.acl-long.260.\nChen-Yu Lee, Chun-Liang Li, Hao Zhang, Timothy Dozat, Vincent Perot, Guolong Su, Xiang\nZhang, Kihyuk Sohn, Nikolay Glushnev, Renshen Wang, Joshua Ainslie, Shangbang Long,\nSiyang Qin, Yasuhisa Fujii, Nan Hua, and Tomas Pfister. FormNetV2: Multimodal graph con-\ntrastive learning for form document information extraction. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9011\u2013\n9026, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.acl-long.501. URL https://aclanthology.org/2023.acl-long.501.\nBodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, and\nMarc Najork. Representation learning for information extraction from form-like documents. In\nACL, 2020.\nOpenAI. Gpt-4 technical report, 2023.\nRasmus Berg Palm, Ole Winther, and Florian Laws. Cloudscan - a configuration-free invoice anal-\nysis system using recurrent neural networks. In Proceedings of 2017 14th IAPR International\nConference on Document Analysis and Recognition, pp. 406\u2013413, United States, 2017. IEEE.\nISBN 9781538635858. doi: 10.1109/ICDAR.2017.74.\nSeunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwal-\nsuk Lee. Cord: A consolidated receipt dataset for post-ocr parsing. In Workshop on Document\nIntelligence at NeurIPS 2019, 2019.\nRafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, and\nGabriela Pa\u0142ka. Going full-tilt boogie on document understanding with text-image-layout trans-\nformer. In Josep Llad\u00f3s, Daniel Lopresti, and Seiichi Uchida (eds.), Document Analysis and\nRecognition \u2013 ICDAR 2021, pp. 732\u2013747, Cham, 2021. Springer International Publishing. ISBN\n978-3-030-86331-9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\ntext transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http:\n//jmlr.org/papers/v21/20-074.html.\nLance Ramshaw and Mitch Marcus. Text chunking using transformation-based learning. In Third\nWorkshop on Very Large Corpora, 1995. URL https://aclanthology.org/W95-0107.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n11\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and\nGuoyin Wang. Gpt-ner: Named entity recognition via large language models, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nZifeng Wang, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong Su, Hao Zhang, Jennifer Dy,\nVincent Perot, and Tomas Pfister. QueryForm: A simple zero-shot form entity query frame-\nwork. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 4146\u20134159,\nToronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nfindings-acl.255. URL https://aclanthology.org/2023.findings-acl.255.\nZilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep Tata. Vrdu: A benchmark for\nvisually-rich document understanding. In Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, KDD \u201923, pp. 5184\u20135193, New York, NY, USA, 2023c.\nAssociation for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599929.\nURL https://doi.org/10.1145/3580305.3599929.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.\nKaiwen Wei, Jie Yao, Jingyuan Zhang, Yangyang Kang, Fubang Zhao, Yating Zhang, Changlong\nSun, Xin Jin, and Xin Zhang. Ppn: Parallel pointer-based network for key information extraction\nwith complex layouts, 2023.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio,\nCha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi-modal pre-training\nfor visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics (ACL) 2021, 2021.\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-\ntraining of text and layout for document image understanding. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1192\u20131200,\n2020.\nKaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees\nand related problems.\nSIAM Journal on Computing, 18(6):1245\u20131262, 1989.\ndoi: 10.1137/\n0218082. URL https://doi.org/10.1137/0218082.\nZhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, and Jianshu Zhang. Multimodal pre-training\nbased on graph attention network for document understanding, 2022.\n12\nA\nAPPENDIX\nA.1\nCHUNKING ALGORITHM\nAlgorithm 1 Document Chunking\n1: function CHUNK(D, L, F)\n\u25b7 D is a document containing multiple pages. L is token limit.\n2:\n\u25b7 F is a function that outputs prompt token length given some segments (e.g. lines).\n3:\nC = \u03d5\n\u25b7 C is to record all produced chunks.\n4:\nfor i = 1 to |D.pages| do\n5:\nS = D.pages[i].segments\n6:\nwhile S \u0338= \u03d5 do\n7:\nfor j = |S| to 1 do\n\u25b7 Start pruning from the end of the page.\n8:\nif F(S[1 : j]) \u2264 L then\n9:\nC = C \u222a {S[1 : j]}\n10:\nS = S[j + 1 : |S|]\n\u25b7 Continue to work on the rest of the segments.\n11:\nExit for loop\n12:\nend if\n13:\nend for\n14:\nend while\n15:\nend for\n16:\nreturn C\n17: end function\nA.2\nENTITY VALUE PARSING ALGORITHM\nAlgorithm 2 Entity Value Parsing\n1: function PARSEENTITYVALUE(D, E)\n\u25b7 D is a document chunk.\n2:\n\u25b7 E is raw extraction results for one entity type parsed from one LLM sample.\n3:\nG = \u03d5\n\u25b7 G is to record all parsed entity values.\n4:\nR = Regex(\u201c(\\d\\d\\|\\d\\d)\u201d)\n\u25b7 R is a regex that captures the segment identifiers.\n5:\nM = {\u201cs.x|s.y\u201d 7\u2192 s|s \u2208 D.segments}\n\u25b7 M holds a mapping between segment id and segment.\n6:\nfor i = 1 to |E| do\n7:\nW = \u03d5\n\u25b7 W is to hold all words for this entity.\n8:\nP = R.split(E[i])\n\u25b7 P is expected to be interleaved text values and segment ids.\n9:\nfor j = 1 to |P|/2 do\n10:\nif P[j \u2217 2] /\u2208 M then\n11:\nGo to next i\n\u25b7 Segment ID is hallucinated. Grounding failure.\n12:\nend if\n13:\nS = M[P[j \u2217 2]]\n\u25b7 Retrieve the stored segment from M with parsed segment ID.\n14:\nT = P[j \u2217 2 \u2212 1]\n\u25b7 T is to hold the predicted text.\n15:\nif T not substring of S then\n16:\nGo to next i\n\u25b7 Grounding failure, skip the current entity.\n17:\nend if\n18:\nW = W \u222a (S \u2229 T)\n19:\nend for\n20:\nG\u2032.value = S\nw\u2208W w.text_value\n\u25b7 G\u2032 is to hold the entity to return.\n21:\nG\u2032.bounding_box = {min(b.x), min(b.y), max(b.x), max(b.y)}w\u2208W,b=w.bounding_box\n22:\nG = G \u222a {G\u2032}\n23:\nend for\n24:\nreturn G\n25: end function\n13\nA.3\nDECODING ALGORITHM\nAlgorithm 3 Responses Decoding\n1: function DECODEFORTYPE(J, T, D)\n\u25b7 J is one or more JSON objects.\n2:\n\u25b7 T is an entity type.\n3:\n\u25b7 D is a document chunk.\n4:\nE = \u03d5\n\u25b7 E is to record all parsed and grounded entities.\n5:\nfor j = 1 to |J| do\n6:\nJ\u2032 = J[j][T.type]\n\u25b7 J\u2032 is to hold entities for T\u2019s type before grounding.\n7:\nif T.subtypes = \u03d5 then\n\u25b7 T is leaf entity type.\n8:\nE = E \u222a ParseEntityV alue(D, J\u2032)\n9:\nelse\n\u25b7 T is hierarchical entity type.\n10:\nE\u2032.subtypes = S\nT \u2032\u2208T.subtypes DecodeForType(J\u2032, T \u2032, D)\n\u25b7 E\u2032 is hierarchical entity.\n11:\nE = E \u222a {E\u2032}\n12:\nend if\n13:\nend for\n14:\nreturn E\n15: end function\n16:\n17: function MAJORITYVOTING(T, E)\n\u25b7 T is an entity type.\n18:\n\u25b7 E is a 2D vector of entities of type T from all LLM responses.\n19:\nV = [0, 0, ..., 0] \u2208 R|E|\n\u25b7 V is to record all votes.\n20:\nL = {T}\n21:\nwhile L \u0338= \u03d5 do\n22:\nT \u2032 = L[0]\n23:\nE\u2032 = \u03d5\n24:\nfor j = 1 to |E| do\n25:\nE\u2032 = E\u2032 \u222a {e|e \u2208 E[j], e.type = T \u2032}\n\u25b7 E\u2032[j] holds entities with type T \u2032 from E[j].\n26:\nend for\n27:\nfor i = 1 to |E\u2032| - 1 do\n28:\nfor j = i + 1 to |E\u2032| do\n29:\nif E\u2032[i] = E\u2032[j] then\n30:\nV [i] = V [i] + 1\n31:\nV [j] = V [j] + 1\n32:\nend if\n33:\nend for\n34:\nend for\n35:\nL = L[1 : |L|]\n\u25b7 Remove T \u2032 and inject its sub-types for recursion.\n36:\nL = L \u222a T \u2032.subtypes\n37:\nend while\n38:\nreturn E[argmax(V )]\n\u25b7 Return the entity values with the highest votes.\n39: end function\n40:\n41: function DECODEALLSAMPLES(S, T, D)\n\u25b7 S is all LLM response samples on D.\n42:\n\u25b7 T is a list of entity types.\n43:\n\u25b7 D is a document chunk.\n44:\nreturn S\nT \u2032\u2208T MajorityV oting(S\nS\u2032\u2208S DecodeForType(ParseJson(S\u2032), T \u2032, D))\n45: end function\n14\nA.4\nTOKEN LENGTH STATISTICS\nTable 5 details the token length (50th and 99th percentiles) of the prompt and completion targets for\nthe train split of datasets used in our experiments. We select the line level segment, 2 coordinate\nscheme, no JSON indentation so that all datasets fit within our 6144 prompt token length and 2048\noutput token length.\nTable 5: Prompt and target token length of different coordinate-as-tokens schemes on VRDU and\nCORD benchmarks, using the vocabulary of PaLM 2-S. We vary the number of coordinates and their\nquantization buckets in the localization tags, the segment level (e.g. line versus word), chunking\nstyle (e.g. page versus max input tokens) and JSON indentation in the schema and completion\ntargets.\nVRDU Ad-Buy Form\n# Coord.\n# Quant.\nSegment\nChunking\nJSON Indent\nInput\nTarget\n50th\n99th\n50th\n99th\n2\n100\nLine\nPage\nNone\n2377\n3920\n602\n1916\n2\n100\nWord\nPage\nNone\n3865\n13978\n718\n2328\n4\n100\nLine\nPage\nNone\n3329\n5284\n777\n2473\n2\n1000\nLine\nPage\nNone\n2687\n4322\n660\n2095\n2\n100\nLine\nPage\n4\n2417\n3328\n689\n2234\n2\n100\nLine\n6144 tokens\nNone\n2377\n3920\n602\n1916\nVRDU Registration Form\n# Coord.\n# Quant.\nSegment\nChunking\nJSON Indent\nInput\nTarget\n50th\n99th\n50th\n99th\n2\n100\nLine\nPage\nNone\n963\n1578\n79\n147\n2\n100\nWord\nPage\nNone\n3083\n5196\n101\n349\n4\n100\nLine\nPage\nNone\n1232\n2017\n91\n177\n2\n1000\nLine\nPage\nNone\n1052\n1723\n83\n155\n2\n100\nLine\nPage\n4\n977\n1592\n92\n160\n2\n100\nLine\n6144 tokens\nNone\n963\n1578\n79\n147\nCORD\n# Coord.\n# Quant.\nSegment\nChunking\nJSON Indent\nInput\nTarget\n50th\n99th\n50th\n99th\n2\n100\nLine\nPage\nNone\n342\n869\n355\n1495\n2\n100\nWord\nPage\nNone\n396\n1067\n375\n1638\n4\n100\nLine\nPage\nNone\n408\n1139\n422\n1801\n2\n1000\nLine\nPage\nNone\n364\n959\n376\n1957\n2\n100\nLine\nPage\n4\n411\n938\n474\n1997\n2\n100\nLine\n6144 tokens\nNone\n342\n869\n355\n1495\n15\nA.5\nSCHEMAS\nIn this section, we present the schemas used for the experiments of this paper. The schema for\nVRDU Ad-Buy Form, VRDU Registration Form, and CORD can be found in Figure 5, Figure 6 and\nFigure 7 respectively.\n{\n\"advertiser\": \"\",\n\"agency\": \"\",\n\"contract_num\": \"\",\n\"flight_from\": \"\",\n\"flight_to\": \"\",\n\"gross_amount\": \"\",\n\"line_item\": [\n{\n\"channel\": \"\",\n\"program_desc\": \"\",\n\"program_end_date\": \"\",\n\"program_start_date\": \"\",\n\"sub_amount\": \"\"\n}\n],\n\"product\": \"\",\n\"property\": \"\",\n\"tv_address\": \"\"\n}\nFigure 5: VRDU Ad-Buy Form Schema.\n{\n\"file_date\": \"\",\n\"foreign_principle_name\": \"\",\n\"registrant_name\": \"\",\n\"registration_num\": \"\",\n\"signer_name\": \"\",\n\"signer_title\": \"\"\n}\nFigure 6: VRDU Registration Form Schema.\n16\n{\n\"line_item\": [\n{\n\"discount_price\": \"\",\n\"identifier\": \"\",\n\"name\": \"\",\n\"other\": \"\",\n\"quantity\": \"\",\n\"sub_name\": [],\n\"sub_price\": [],\n\"sub_quantity\": [],\n\"subtotal_price\": \"\",\n\"total_price\": \"\",\n\"unit_price\": \"\"\n}\n],\n\"subtotal\": {\n\"discount_price\": \"\",\n\"other\": [],\n\"service_price\": \"\",\n\"subtotal_price\": [],\n\"tax_price\": []\n},\n\"total\": {\n\"cash_price\": [],\n\"change_price\": \"\",\n\"credit_card_price\": \"\",\n\"emoney_price\": \"\",\n\"line_item_quantity_count\": \"\",\n\"line_item_type_count\": \"\",\n\"other\": \"\",\n\"total_price\": []\n}\n}\nFigure 7: CORD Schema. Note that the original entity types have been renamed to more semanti-\ncally meaningful names.\n17\nA.6\nSAMPLE PROMPTS AND COMPLETIONS\nIn this section, we present example of LMDX prompts and completions from the LLM on the VRDU\nAd-Buy dataset to better showcase the format used. Figure 8 shows the original document with the\nline bounding boxes from OCR, Figure 9 shows the corresponding prompt and completion on that\ndocument with coordinate grounding tags, and Figure 10 shows the same prompt and completion,\nbut with line index tags (used in ablation studies to showcase how the LLM can interpret the layout).\nFigure 8: Example document from VRDU Ad-Buy Form. The line bounding boxes detected by\nOCR are shown in red.\n18\n=============================== Prompt ===============================\n<Document>\nPrint Date 02/28/20 14:21:20 75|02\nPage 1 of 1 91|02\nORDER 49|04\nWSIL 77|07\nOrders 04|09\nOrder/Rev: 17|09\n14086 29|09\nHARRISBURG 78|09\nPADUCAH 78|10\nCAPE GIRARDEAU 78|12\nAlt Order #: 17|11\nProduct Desc: 18|13\nMike Carr for Jackson Co States Atty 38|13\nEstimate: 16|15\nWSIL-TV 73|15\nFlight Dates: 17|17\n03/03/20 - 03/03/20 33|17\nPrimary AE: 57|17\nDavid Cisco 73|17\nOriginal Date/ Rev: 19|19\n02/28/20/02/28/20 33|19\n...\nStart Date End Date Weekdays Spots/Week Rate Rating 31|66\nWeek: 03/02/20 03/08/20 -3-- 15|67\n3 $200.00 0.00 47|67\nTotals 79|69\n3 87|69\n$600.00 94|69\n</Document>\n<Task>\nFrom the document, extract the text values and tags of the following\nentities:\n{\"advertiser\": \"\", \"agency\": \"\", \"contract_num\": \"\", \"flight_from\": \"\",\n\"flight_to\": \"\", \"gross_amount\": \"\", \"line_item\": [{\"channel\": \"\",\n\"program_desc\": \"\", \"program_end_date\": \"\", \"program_start_date\": \"\",\n\"sub_amount\": \"\"}], \"product\": \"\", \"property\": \"\", \"tv_address\": \"\"}\n</Task>\n<Extraction>\n============================= Completion =============================\n{\"advertiser\": \"Committee to Elect Mike Carr 37|36\", \"agency\":\n\"Committee to Elect Mike Carr 37|24\", \"contract_num\": \"14086 29|09\",\n\"flight_from\": \"03/03/20 33|17\", \"flight_to\": \"03/03/20 33|17\",\n\"gross_amount\": \"$600.00 77|51\", \"line_item\": [{\"channel\": \"3 31|63\",\n\"program_desc\": \"Tuesday Prime 31|63\\nYour Voice Your Vote 29|64\",\n\"program_end_date\": \"03/03/20 31|63\", \"program_start_date\":\n\"03/03/20 31|63\", \"sub_amount\": \"$600.00 94|63\"}], \"product\":\n\"Mike Carr for Jackson Co States Atty 38|13\", \"property\": \"WSIL 77|07\",\n\"tv_address\": null}\n</Extraction>\nFigure 9: VRDU Ad-Buy Form sample prompt and completion with 2 Coordinates for grounding\ntags. The document is truncated for easier visualization.\n19\n=============================== Prompt ===============================\n<Document>\nPrint Date 02/28/20 14:21:20 <0>\nPage 1 of 1 <1>\nORDER <2>\nWSIL <3>\nOrders <4>\nOrder/Rev: <5>\n14086 <6>\nHARRISBURG <7>\nPADUCAH <8>\nCAPE GIRARDEAU <9>\nAlt Order #: <10>\nProduct Desc: <11>\nMike Carr for Jackson Co States Atty <12>\nEstimate: <13>\nWSIL-TV <14>\nFlight Dates: <15>\n03/03/20 - 03/03/20 <16>\nPrimary AE: <17>\nDavid Cisco <18>\nOriginal Date/ Rev: <19>\n02/28/20/02/28/20 <20>\n...\nStart Date End Date Weekdays Spots/Week Rate Rating <105>\nWeek: 03/02/20 03/08/20 -3-- <106>\n3 $200.00 0.00 <107>\nTotals <108>\n3 <109>\n$600.00 <110>\n</Document>\n<Task>\nFrom the document, extract the text values and tags of the following\nentities:\n{\"advertiser\": \"\", \"agency\": \"\", \"contract_num\": \"\", \"flight_from\": \"\",\n\"flight_to\": \"\", \"gross_amount\": \"\", \"line_item\": [{\"channel\": \"\",\n\"program_desc\": \"\", \"program_end_date\": \"\", \"program_start_date\": \"\",\n\"sub_amount\": \"\"}], \"product\": \"\", \"property\": \"\", \"tv_address\": \"\"}\n</Task>\n<Extraction>\n============================= Completion =============================\n{\"advertiser\": \"Committee to Elect Mike Carr <44>\", \"agency\":\n\"Committee to Elect Mike Carr <29>\", \"contract_num\": \"14086 <6>\",\n\"flight_from\": \"03/03/20 <16>\", \"flight_to\": \"03/03/20 <16>\",\n\"gross_amount\": \"$600.00 <84>\", \"line_item\": [{\"channel\": \"3 <102>\",\n\"program_desc\": \"Tuesday Prime Other CM <102>\", \"program_end_date\":\n\"03/03/20 <102>\", \"program_start_date\": \"03/03/20 <102>\",\n\"sub_amount\": \"$600.00 <100>\"}], \"product\": \"Mike Carr for Jackson Co\nStates Atty <12>\", \"property\": \"WSIL-TV <14>\", \"tv_address\": null}\n</Extraction>\nFigure 10: VRDU Ad-Buy Form sample prompt and completion with line index for grounding tags,\nwhich does not communicate layout information. The document is truncated for easier visualization.\n20\nA.7\nCORD BASELINES DETAILS\nLayoutLMv3LARGE\nBaseline.\nWe\nfollow\nthe\nreleased\nimplementation2\nfor\nthe\nLayoutLMv3LARGE model and the training protocol described in Huang et al. (2022) as\nclosely as possible. In particular, we train the model for 80 epochs for each experiment on CORD\n(namely, 10, 50, 100, 200, and 800-document training sets), on the IOB tags of the leaf entities. One\ndifference in our training is that, due to computational resource constraints, we use batch_size = 8\nand learning_rate = 2 \u00b7 10\u22125.\nAs the LayoutLMv3 model can only extract leaf entities, we design and heavily optimize a heuristic\nalgorithm to group the leaf entities into hierarchical entities menu, subtotal and total. The best\nheuristics we could find are as follows:\n\u2022 For the subtotal and total hierarchical entity types, since they appear only once per doc-\nument, we group all their extracted sub-entities under a single subtotal and total entity,\nrespectively.\n\u2022 For menu hierarchical entity type, we observe that those entities usually occur multiple\ntimes on a document, and each menu has at most one nm, num, unitprice, cnt, discount-\nprice, price, itemsubtotal, etc sub-entities and potentially multiple sub_nm, sub_price and\nsub_cnt sub-entities. We also notice that the sub-entities aligned horizontally overwhelm-\ningly belong to the same menu entity, and a menu entity can sometimes span over two or\nmore consecutive horizontal lines. To leverage those observations, we perform a two-step\ngrouping process for menu entities. First, we merge the extracted leaf sub-entities into hor-\nizontal groups, where a threshold of 0.5 on the intersection-over-union of the Y-axis was\nused for the determination of horizontal alignment. Second, we further merge the consec-\nutive horizontal groups into menu entities, if and only if the horizontal groups do not have\ntype duplication in any of the nm, num, unitprice, cnt, discountprice, price, itemsubtotal,\nand etc sub-entities (namely, those sub-entities only show up in at most one of the con-\nsecutive horizontal groups to be merged). We allow duplication of sub_nm, sub_price and\nsub_cnt sub-entity types. After those two steps, we obtain the final menu entities.\nDonut Baseline.\nWe follow Donut released implementation3 for the Donut benchmarking results\non CORD. We use the default training configuration for all experiments on CORD (namely, 10,\n50, 100, 200, and 800-document training sets), with the following difference: we reduce batch size\nfrom 8 to 4 due to computational resource constraints, and increase the number of train epochs from\n30 to 60. For each experiment, checkpoint with the lowest loss on the dev set is selected and we\nreport performance on test set. Normalized Tree Edit Distance accuracy scores produced by Donut\nevaluation code are reported (similar to all our other models).\n2https://github.com/microsoft/unilm/tree/master/layoutlmv3\n3https://github.com/clovaai/donut\n21\n"
  },
  {
    "title": "Kosmos-2.5: A Multimodal Literate Model",
    "link": "https://arxiv.org/pdf/2309.11419.pdf",
    "upvote": "47",
    "text": "KOSMOS-2.5: A Multimodal Literate Model\nTengchao Lv\u2217, Yupan Huang\u2217, Jingye Chen\u2217, Lei Cui\u2217\u2020, Shuming Ma, Yaoyao Chang,\nShaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang,\nCha Zhang, Furu Wei\u2020\nMicrosoft\naka.ms/GeneralAI\nAbstract\nWe present KOSMOS-2.5, a multimodal literate model for machine reading of text-\nintensive images. Pre-trained on large-scale text-intensive images, KOSMOS-2.5\nexcels in two distinct yet cooperative transcription tasks: (1) generating spatially-\naware text blocks, where each block of text is assigned its spatial coordinates\nwithin the image, and (2) producing structured text output that captures styles and\nstructures into the markdown format. This unified multimodal literate capability\nis achieved through a shared Transformer architecture, task-specific prompts, and\nflexible text representations. We evaluate KOSMOS-2.5 on end-to-end document-\nlevel text recognition and image-to-markdown text generation. Furthermore, the\nmodel can be readily adapted for any text-intensive image understanding task with\ndifferent prompts through supervised fine-tuning, making it a general-purpose tool\nfor real-world applications involving text-rich images. This work also paves the\nway for the future scaling of multimodal large language models.\nKOSMOS 2.5: A Multimodal Literate Model\ntask prompts\n\u2026 [x_119] [y_306] [x_159] [y_319]: Type of \n[x_119] [y_323] [x_163] [y_336]: Disaster \u2026\n\u2026 | Type of Disaster | No. of disasters \noccurred | \u2026\nspatially-aware texts\ntext images\nmarkdown-formatted texts\nFigure 1: KOSMOS-2.5 is a multimodal large language model that takes text images as input and\ngenerates spatially-aware texts (i.e., texts with bounding boxes) or markdown-formatted texts (i.e.,\ntexts with markdown elements), following different task prompts, respectively.\n\u2217 Equal contribution. \u2020 Corresponding author.\nWork in Progress.\narXiv:2309.11419v1  [cs.CL]  20 Sep 2023\n1\nIntroduction\nOver the past several years, large language models (LLMs) have emerged as a critical area of research\nin artificial intelligence. These models are designed to learn from massive amounts of natural language\ndata, allowing them to perform a wide range of language-related tasks with impressive accuracy. This\ndevelopment has been fueled by advancements in model scaling that enabled researchers to create\nmodels with unprecedented complexity. As a result, LLMs have become increasingly prevalent across\nvarious industries and applications, from customer service chatbots to virtual assistants and automated\ncontent creation. One notable trend in recent years has been the focus on building larger and more\ncomplex models, such as GPT-3 [BMR+20] and GPT-4 [Ope23], which has hundreds/thousands\nof billion parameters and can generate compelling language outputs. While these models require\nsignificant computing resources to train and operate, they hold enormous potential for revolutionizing\nhow we interact with and understand natural language.\nCurrent LLMs primarily focus on textual information and cannot understand visual information.\nHowever, advancements in the field of multimodal large language models (MLLMs) aim to address\nthis limitation. MLLMs combine visual and textual information within a single Transformer-based\nmodel, enabling the model to learn and generate content based on both modalities. MLLMs have\nshown promise in a variety of real-world applications, including natural image understanding and text\nimage understanding. These models leverage the power of language modeling as a general interface\nfor multimodal problems, allowing them to process and generate responses based on textual and\nvisual inputs. While existing MLLMs have mainly focused on natural images with lower resolutions,\nthe exploration of text images is an area that requires further investigation. Taking advantage of\nlarge-scale multimodal pre-training for text images is an important direction for MLLM research. By\nincorporating text images into the training process and developing models based on textual and visual\ninformation, we can unlock new possibilities for multimodal applications involving high-resolution\ntext-intensive images.\nIn this study, we present KOSMOS-2.5, a multimodal literate model that takes advantage of KOSMOS-\n2 [PWD+23] designed to tackle machine reading of text-intensive images, which is shown in\nFigure 1. KOSMOS-2.5 performs two closely related transcription tasks in a unified multimodal\nmodel. The first task generates spatially-aware text blocks, assigning text lines their corresponding\nspatial coordinates within the original text-rich image. The second task produces structured text\noutput, capturing styles and structures in the markdown format. Both tasks are conducted under\na unified framework, leveraging a shared Transformer architecture, task-specific prompts, and\nflexible text representations. Specifically, our model architecture combines a ViT-based vision\nencoder and a Transformer-based language decoder linked by a resampler module. Our model is pre-\ntrained on a large corpus of text-intensive images, whose text representations include text lines with\nbounding boxes and plain markdown texts. By employing this dual-task training strategy, KOSMOS-\n2.5 enhances its general-purpose multimodal literate capabilities. We assess the performance of\nKOSMOS-2.5 on two tasks: end-to-end document-level text recognition and markdown-formatted\nimage-to-text generation. Experiment results have demonstrated strong literate performance on\nseveral text-intensive image understanding tasks. In addition, KOSMOS-2.5 also demonstrates\npromising capabilities in few-shot and zero-shot learning scenarios, offering a universal interface for\nreal-world applications that involve text-rich images.\nThe contributions of this work are summarized as follows:\n\u2022 KOSMOS-2.5 represents a significant paradigm shift in text image understanding, transition-\ning from encoder-only/encoder-decoder models to a decoder-only model. It is pre-trained by\nincorporating dual transcription tasks (spatially-aware text block generation and structured\nmarkdown text generation) into a single, unified model architecture.\n\u2022 This innovative method streamlines the application interface by integrating generative\nmultimodal language modeling, simplifying the traditionally complex cascaded pipelines\nused for various downstream tasks.\n\u2022 Furthermore, KOSMOS-2.5 demonstrates impressive multimodal literate capabilities, thus\nsetting the stage for future scaling of multimodal large language models.\n2\nVision Encoder\nResampler\ntext tokens\nimage tokens\n[task prompt 1] \n+\n<bbox> [111,73,447,124] </bbox> \n<text> PEGGY CARTER </text> <bbox> \n\u2026 </bbox>\n\u2026\ntext tokens\nimage tokens\n[task prompt 2] \n+\n<text> \u2026 **List Price:** $499.00 \n**Price:**  $457.38 **You Save:** \n\u2026 </text>\nRender\nRender\nshared decoder-only Transformer\nFigure 2: Model architecture of KOSMOS-2.5. A shared decoder-only Transformer model generates\nthe output text sequence based on the input image from a vision encoder and different task prompts.\n2\nKOSMOS-2.5\n2.1\nModel Architecture\nThe model architecture of KOSMOS-2.5 consists of a pre-trained vision encoder and a language\ndecoder connected with a resampler module, shown in Figure 2. We adopt the pre-trained vision\nencoder based on the Vision Transformer (ViT) [DBK+21]. We further adapt a Perceiver Resampler\nmodule with an attentive pooling mechanism to reduce the size of image embeddings [ADL+22].\nThe language decoder is built upon the Transformer-based decoder to condition on image and text\ncontext for the next token prediction.\n2.2\nImage and Text Representations\nKOSMOS-2.5 takes a composite input consisting of an image and a text representation. The image\nrepresentation is uniform across various configurations and leverages a variable-resolution input\nstrategy following Pix2Struct [LJT+23]. Precisely, we extract the maximum number of fixed-size\npatches (16\u00d716) that can fit within a predefined sequence length L. In addition, Resampler [ADL+22]\nis used as an attentive pooling mechanism to reduce the number of image embeddings. The text\nrepresentation, however, is more versatile and can be one of two types: text lines with bounding\nboxes or plain markdown texts.\nText lines with bounding boxes: For the layout-based document representation, text lines and their\nassociated bounding boxes are extracted. Inspired by KOSMOS-2 [PWD+23], we ground the text lines\nto their spatial positions in images by aligning their representations. The coordinates of these bounding\nboxes are then converted into discrete location tokens. Given that L also represents the maximum\nlength for each image dimension, we introduce a set of 2L + 2 specialized tokens. These tokens,\n<x0>, <x1>, ..., <xL\u22121>, <y0>, ..., <yL\u22121>, <bbox>, and </bbox>, correspond to the coordinates\nand the start and end of a bounding box. The coordinates are obtained by rounding down the actual\nposition after resizing images. Consider a document T that comprises N text lines. Each line is\nrepresented as Tn = {w(n)\n1 , w(n)\n2 , . . . , w(n)\nMn}, where Mn is the number of words in the n-th text line.\nThe bounding box for Tn is then denoted by Bn = <bbox><x(n)\ntl ><y(n)\ntl\n><x(n)\nbr ><y(n)\nbr ></bbox>,\nwhich includes coordinates for its top-left and bottom-right corners.\nMarkdown texts: For the markup-based document representation where the output text is in the\nmarkdown format, the text component captures both content and formatting markup. Unlike layout-\nbased documents, markdown text does not require bounding boxes. Instead, the text is directly\ntokenized, retaining all special characters and formatting indicators.\nTo facilitate these diverse input types, we employ different composite representations.\nFor\nimage-text pairs with text lines and bounding boxes, the input is denoted as <s><image>Image\nEmbedding</image> SN\nn=1 (Bn \u2295 Tn) </s>. The operator \u2295 represents the concatenation of the\n3\ntext line Tn and its bounding box Bn. Conversely, when the text is in the markdown format, the\ninput simplifies to <s><image>Image Embedding</image>Markdown Text</s>. In both cases,\n<s> and </s> signify the sequence boundaries, while <image> and </image> indicate the beginning\nand end of image embeddings. This flexibility in text representation allows KOSMOS-2.5 to apply to\nvarious document analysis tasks.\n2.3\nPre-training Data\nThe pre-training process enables KOSMOS-2.5 to learn versatile representations suitable for various\ntext-intensive image understanding tasks. The model is pre-trained on a rich array of datasets from\ndiverse sources. Traditional Optical Character Recognition (OCR) task is primarily geared towards\ngenerating text content and its 2D positions within an image. However, they often neglect the\nneed to maintain the order and structural integrity of the original document, which is essential for\ntext-intensive image understanding tasks involving structured information.\nTo address this, we steer KOSMOS-2.5 to excel in two distinct yet cooperative transcription tasks: (1)\ngenerating spatially-aware text blocks, where each block of text is assigned its spatial coordinates\nwithin the image, and (2) producing structured text output that captures styles and structures into\nthe markdown format. Markdown provides an advantage over plain text by explicitly distinguishing\ndifferent structural elements, such as tables and lists, with specific tokens. For example, table cells\ncan be denoted with vertical bars (|) and list items with bullets (*, -, or +). It also standardizes the\nrepresentation of typographic emphases like bold (**bold**) and italics (*italics*), integrating the\nlearning of document structure with natural language understanding in a unified model.\nFor spatially-aware text blocks, we use:\n\u2022 IIT-CDIP: The IIT-CDIP dataset is a large-scale public collection comprising scanned\ndocument images. We used approximately 27.6 million pages to train our model.\n\u2022 arXiv papers: arXiv, an open-access research-sharing platform, provides another significant\ndata source, accounting for roughly 20.9 million pages. We downloaded a bulk of data,\nconsisting of PDF and LATEX source files, from the official arXiv repository2.\n\u2022 PowerPoint slides: A corpus of 6.2 million pages is collected from various web pages\ncontaining PowerPoint documents, significantly enhancing the diversity of our training data.\n\u2022 General PDF: Additionally, we crawled the web for diverse open-domain digital PDF files,\nleading to the collection of a large corpus comprising approximately 155.2 million pages.\n\u2022 Web screenshots: A subset of the mC4 webpages is scraped and rendered as screenshots\ncontaining almost 100 million pages.\nFor structured text output in markdown format, we use:\n\u2022 README: We collect 2.9 million \u201cREADME.md\u201d files from open-source GitHub projects,\nprimarily written in markdown format.\n\u2022 DOCX: We also extract 1.1 million DOCX pages from millions of WORD files crawled from\nthe web. The DOCX pages are converted to markdown format, and each page corresponds\nto its markdown information.\n\u2022 LATEX: A subset of the entire arXiv papers is used to extract the mapping of PDF pages and\nits corresponding markdown information converted from the LATEX code, which contains a\ntotal of 3.7 million pages.\n\u2022 HTML: We obtain 6.3 million HTML files from the aforementioned mC4 subset and convert\nthem into markdown format.\n2.4\nData Processing\nThe pre-training data has a wide coverage, and each type of data requires a different processing\nworkflow, which is introduced as follows:\n2https://info.arxiv.org/help/bulk_data/index.html\n4\nIIT-CDIP\nThe IIT-CDIP dataset mainly consists of scanned document images. We use the Microsoft\nRead API 3 to extract text and layout information.\narXiv papers, PowerPoint slides, General PDF\nWe first compile and convert arXiv papers and\nPowerPoint slides into PDF files. Together with other general PDFs, we employed the PyMuPDF\nparser 4 to extract text and layout information efficiently.\nWeb screenshots\nWe also include webpage screenshots in the model pre-training to diversify the\nlayout distribution further. We collect the webpage URLs from the English portion of the mC4 dataset.\nPlaywright 5 is used to access a specified URL and open the webpage. The HTML content of the\npage is extracted and parsed using the lxml library 6 to obtain a Document Object Model (DOM)\ntree representation. This DOM tree is traversed, examining the XPath of each element within it.\nThis traversal aims to determine whether each element is visible and retrieve information about its\nbounding boxes.\nREADME (markdown)\nIn addition to layout-based data, we collect markup-based data for the\npre-training. We collect \u201cREADME.md\u201d files from many GitHub projects and convert these files into\nHTML using Pandoc 7. Then, wkhtmltopdf 8 is used to obtain the images from the generated HTML\ncontent.\nDOCX (markdown)\nThe Microsoft Office WORD files have been extensively used in existing\nresearch like TableBank [LCH+20] and ReadingBank [WXC+21]. We collect WORD DOCX files\nand convert them into texts with markdown. First, we use Pandoc to convert the XML content within\nthe DOCX files into markdown files. As Pandoc keeps the \u201c<table>\u201d tags to represent the tabular\ncells in the generated markdown, we further identify all the tables and use markdownify 9 to convert\nthem into the markdown formats. Finally, the original DOCX files are converted into PDF files, and\neach page is aligned to the corresponding span of the markdown content based on a heuristic method.\nLATEX (markdown)\nLATEX documents from arXiv have been used to generate PDF files to obtain\ntexts with bounding boxes. Meanwhile, we also convert the LATEX content into the markdown texts.\nSimilar to Nougat [BCSS23], LaTeXML 10 is used to convert the LATEX code into the HTML sequence,\nwhich is further transformed into the markdown format. Different from Nougat, we keep all the tables\nat the beginning of the page as most LATEX users prefer to position tables with \u201c[t]\u201d or \u201c[h]\u201d instead of\n\u201c[b]\u201d. Meanwhile, we also convert the table content from the LATEX format into the markdown format.\nHTML (markdown)\nThe most straightforward way to obtain markdown resources from HTML\nwebpages is through web scraping. However, webpages are often cluttered with various layouts and\nstyles, resulting from the misuse of HTML tags. Moreover, HTML pages may include extraneous\nelements, such as advertisements, navigation menus, or formatting elements, making extracting clean\nand meaningful content challenging. To overcome these obstacles, we employ Playwright, a fast\nand reliable end-to-end testing framework for the web. The library allows us to navigate the HTML\nstructure, filter out non-essential elements, and extract the relevant text content. We also apply custom\nrules and regular expressions to further refine the extracted text and format it as markdown, ensuring\nthat the resulting markdown files are coherent and readable.\n2.5\nFiltering and Quality Control\nWe employ fastText for language identification (with a threshold of 0.5) to filter out non-English\ndocuments from the entire pre-training dataset. To ensure content diversity within each source, we\n3https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/\noverview-ocr#read-api\n4https://github.com/pymupdf/PyMuPDF\n5https://github.com/microsoft/playwright-python\n6https://lxml.de/\n7https://pandoc.org/\n8https://wkhtmltopdf.org/\n9https://github.com/matthewwithanm/python-markdownify\n10https://math.nist.gov/~BMiller/LaTeXML/\n5\nTask\nData Source\nNumber of Pages\nSampling Ratio\nLayout-based (texts+bboxes)\nIIT-CDIP\n27.6M\n10%\narXiv papers\n20.9M\n5%\nPowerPoint slides\n6.2M\n5%\nGeneral PDF\n155.2M\n20%\nWeb screenshots\n100.5M\n10%\nMarkup-based (texts+markdown)\nREADME\n2.9M\n15%\nDOCX\n1.1M\n10%\nLATEX\n3.7M\n15%\nHTML\n6.3M\n10%\nTotal\n324.4M\n100%\nTable 1: Summary of Pre-training Data in KOSMOS-2.5\nutilize the MinHash [Bro97] to identify and remove redundant pages. We use the same parameters\nas [LIN+21] and a document pair with similarity 0.8 will be marked as duplicate. A comprehensive\nbreakdown of the pre-training data, along with their respective sampling ratios, is provided in Table 1.\nWhen dealing with image-to-markdown data from README, DOCX, LATEX, and HTML sources,\nwe observe discrepancies between the content in text images and their corresponding markdown\nsequences due to conversion issues. Consequently, we refine the data by evaluating token overlap\nbetween images and markdown files, requiring a token intersection-to-union ratio greater than 0.95\nfor inclusion. Section A.2 shows some of the training samples.\n3\nExperiments\n3.1\nEvaluation\nText Recognition\nWe utilize word-level precision (# or correct matches over the number of detected\nwords), recall (# of correct matches over the number of ground truth words), and f1 as the metrics to\nevaluate the text recognition performance. If there are repeated words in the ground truth, they are\nexpected to be repeated in the prediction. Text recognition is evaluated on three benchmark datasets,\nincluding FUNSD [JET19], SROIE [HCH+19] and CORD [PSL+19]. We compare KOSMOS-2.5 to\nthe text recognition results from Document OCR in Google Document AI 11.\nImage-to-markdown Generation\nIn light of the unique nature of the image-to-markdown con-\nversion task, assessing the quality of the generated markdown necessitates specialized metrics. We\nadopt a two-fold evaluation scheme: Normalized Edit Distance (NED) and Normalized Tree Edit\nDistance (NTED), considering both the lexical accuracy and the preservation of the original structural\nelements.\nThe NED is formulated as\nNED = 1 \u2212 1\nN\nN\nX\ni=1\nD (si, \u02c6si) / max (len(si), len(\u02c6si))\nwhere N, s, and \u02c6s denote the number of samples, prediction, and ground truth, respectively. D(\u00b7, \u00b7)\nand len(\u00b7) represent the edit distance function and the length of a string. The NED value ranges from\n0 to 1, with a higher NED value indicating the prediction is closer to the ground truth.\nHowever, given the hierarchical structure inherent to markdown, relying solely on a string-based\ncomparison metric like NED can be insufficient. Thus, we adopt NTED as an additional evaluation\nmetric for structural differences. NTED is a tree edit distance normalized by the number of nodes\nin the tree, considering the structural discrepancies between parse trees. Specifically, the predicted\nmarkdown sequence is first transformed into an HTML tree. Then, the tree edit distance between\n11https://cloud.google.com/document-ai\n6\nthe prediction and the ground truth is calculated using the ZSS algorithm [ZS89]. The NTED is\nformulated as\nNTED = 1 \u2212 1\nN\nN\nX\ni=1\nTD\n\u0000ti, \u02c6ti\n\u0001\n/ max\n\u0000node(ti), node(\u02c6ti\n\u0001\n)\nwhere N, t, and \u02c6t signify the number of samples, the HTML tree of prediction, and the HTML tree of\nground truth, respectively. Besides, TD(\u00b7, \u00b7) and node(\u00b7) stand for the tree edit distance function and\nthe number of nodes in a tree.\nWe create three datasets to evaluate the image-to-markdown task from different data sources, includ-\ning document-level markdown generation, README markdown generation and table markdown\ngeneration. Each dataset includes 1,000 \u27e8image, markdown\u27e9 pairs, which are held out from the\npre-training data. We compare KOSMOS-2.5 to the markdown generated by the Nougat [BCSS23]\nbase and small models.\n3.2\nImplementation Details\nWe employ the AdamW optimizer [LH17] with \u03b2 = (0.9, 0.98) for optimization, setting the weight\ndecay to 0.01 and the dropout rate to 0.1. The learning rate is warmed up to 2\u00d710\u22124 during the initial\n375 steps, followed by a linear decay to zero throughout the remaining training steps. The batch size\nis adjustable to align with the available computational resources and specific training requirements.\nKOSMOS-2.5 contains a total of 1.3 billion parameters. The vision encoder is initialized from the\nencoder of the Pix2Struct-Large model. The language decoder includes 24 Transformer layers with a\nhidden size of 1,536, an FFN intermediate size of 6,144, and 16 attention heads. Section A.1 shows\nmore details of the training hyperparameters.\nDue to the substantially larger quantity of available layout-based data than markup-based data, we\ninitially trained the model for 100k steps exclusively using the layout-based dataset. Subsequently,\nthe two datasets were combined for further training of 140k steps. Additionally, we incorporate\nthe training split of the evaluation dataset into the entire pre-training data, extending the process\nby an additional 10k steps. For text tokenization, we utilize SentencePiece [KR18] and adopt the\n\u201cfull-sentence\u201d format [LOG+19]. This approach packs each input sequence with full sentences,\ncontinuously sampled from one or multiple documents. Newly added word embeddings of location\ntokens are randomly initialized, with all parameters updated during training. We also leverage the\ndata augmentation approaches from TrOCR [LLC+22] in the training to make models more robust.\nThroughout the evaluation process, model inference is conducted using a single model checkpoint\nacross various evaluation datasets with the corresponding task prompt respectively, demonstrating\nthat our approach does not necessitate individualized model fine-tuning for each dataset.\n3.3\nResults\nKOSMOS-2.5 is a flexible framework that facilitates multitasking, with tasks determined by the\nprovided task prompts. Experimental results are demonstrated in Table 2 and Table 3. Specifically, for\nthe text recognition task, our KOSMOS-2.5 outperforms Google Document OCR by 0.33%, 2.45%,\nand 1.35% in terms of the F1 score, showcasing its effectiveness. For the image-to-markdown task,\nit is worth noting that our method significantly outperforms the Nougat [BCSS23]. For example,\nKOSMOS-2.5 achieves a notable improvement of 33.68% (95.09% vs 61.41%) over Nougat BASE\nin terms of NED on the README dataset. Besides, regarding NTED, KOSMOS-2.5 also boosts\nthe performance by 33.38% (82.08% vs 48.70%) compared with Nougat BASE on the Documents\ndataset. We attribute the performance boost to the increased diversity of our training data compared\nto Nougat, which primarily focuses on the academic paper domain. Notably, the greater diversity\nin our training data significantly enhances our model\u2019s comprehension of different document types\nand strengthens its generalization capabilities. In summary, the experimental results validate the\nremarkable capabilities of KOSMOS-2.5 in various tasks.\n3.4\nDiscussion\nWe illustrate an example in Figure 3, showcasing the model outputs produced by KOSMOS-2.5 with\nvarious task prompts when presented with the same input text image. As shown in the figure, the\n7\nDataset\nFUNSD\nSROIE\nCORD\nP / R / F1\nP / R / F1\nP / R / F1\nCommercial OCR\n85.12 / 80.86 / 82.93\n89.68 / 89.69 / 89.69\n81.95 / 86.87 / 84.34\nKOSMOS-2.5\u2020\n83.88 / 82.66 / 83.26\n91.72 / 92.57 / 92.14\n83.64 / 87.83 / 85.69\nTable 2: Experimental results on text recognition using Precision (%), Recall (%), F1 (%), where\nmodel inference is conducted with the layout task prompt. \u2020KOSMOS-2.5 does not require task-\nspecific fine-tuning.\nDataset\nGeneral Documents\nREADME\nTables\nNED / NTED\nNED / NTED\nNED / NTED\nNougat SMALL [BCSS23]\u2020\n82.80 / 48.96\n58.58 / 35.49\n68.33 / 61.52\nNougat BASE [BCSS23]\u2020\n83.75 / 48.70\n61.41 / 36.41\n68.53 / 61.60\nKOSMOS-2.5\u2021\n91.59 / 82.08\n95.09 / 91.18\n85.14 / 90.64\nTable 3: Experimental results on image-to-markdown using NED (%) and NTED (%), where model\ninference is conducted with the markup task prompt. \u2020Nougat [BCSS23] generates the table content\nin the LATEX format, which is converted to the markdown format for fair comparison. \u2021KOSMOS-2.5\ndoes not require task-specific fine-tuning.\nmodel generates distinct outputs depending on the task prompts it receives. When given the layout\ntask prompt, the model produces the following text sequence, which includes textual content and\ncorresponding bounding boxes:\n[x_52] [y_113] [x_756] [y_145]: NYC Department of Education School Year Calendar 2023-2024\n[x_52] [y_159] [x_826] [y_181]: This is the 2023-24 school year calendar for all 3K-12 NYCDOE public schools. If your child\n,\u2192 attends a private,\n[x_52] [y_180] [x_820] [y_202]: parochial, charter school, NYC Early Education Center (NYCEEC) or Family Childcare Program,\n,\u2192 please contact\n[x_52] [y_201] [x_639] [y_223]: your child\u2019s school for information about their calendar. Please note the following:\n[x_65] [y_223] [x_77] [y_245]: \u2022\n[x_92] [y_223] [x_825] [y_245]: On days when school buildings are closed due to inclement weather or other emergencies, all\n,\u2192 students\n...\nWith the markup task prompt, the model generates another text sequence that follows the markdown\nformat:\n# NYC Department of Education School Year Calendar 2023-2024\nThis is the 2023-24 school year calendar for all 3K-12 NYCDOE public schools. If your child attends a private, parochial,\n,\u2192 charter school, NYC Early Education Center (NYCEEC) or Family Childcare Program, please contact your child\u2019s\n,\u2192 school for information about their calendar. Please note the following:\n...\n- On this schedule, **elementary schools** are defined as programs that serve kindergarten (K) through grade 8, including\n,\u2192 schools with 3-K and Pre-K programs, as well as those that end in grade 5. **Middle schools** are defined as\n,\u2192 programs that serve grades 6-8, and **high schools** are defined as programs that serve grades 9-12.\n...\nIt is apparent that KOSMOS-2.5 excels in precisely identifying text positions and recognizing text\ncontent. Moreover, it adeptly captures the styles and structures present within the text image, including\nelements like titles, bullet points, tables, and bold text. Section A.3 provides the full output sequence\nusing different task prompts for this example.\nKOSMOS-2.5 provides a unified architecture and interface for text image understanding, making\nit versatile for various application scenarios. Firstly, it can be fine-tuned as a single model for a\nwide range of text image understanding tasks, including information extraction, layout detection and\nanalysis, visual question answering, screenshot understanding, UI automation, and many others. This\nunified model interface significantly streamlines downstream task training and enables the model to\neffectively follow instructions in real-world applications. Secondly, our solution is compatible with\nmore powerful LLMs like GPT-3.5 or GPT-4. The output from our model can serve as contexts for\n8\n(a) Input\n(b) Using the layout prompt\n(c) Using the markup prompt\nFigure 3: Model outputs from KOSMOS-2.5 with different task prompts given the same input text\nimage.\nLLMs, enhancing their capabilities through further prompt engineering. This approach empowers\nLLMs with robust text image understanding capabilities. Thirdly, we have the potential to augment\nthe pre-training with textual data, transforming it into a general-purpose MLLM. This expanded\nmodel not only processes visual signals but also possesses strong language understanding capabilities.\n4\nRelated Work\n4.1\nMultimodal Large Language Models\nThe flourishing blossom of large language models (LLM), represented by ChatGPT [Cha22], has\nrevolutionized artificial intelligence and significantly impacted numerous downstream tasks such\nas text translation, code generation, question answering, etc. Despite the rapid development, it is\nsignificant to recognize that the human perception of the world is not limited to language alone but\nencompasses a wide range of modalities, with particular emphasis on the visual modality. Many\nresearch works attempt to \u201cbring eyes\u201d to LLM and develop multimodal large language models\n(MLLM), which can be categorized into LLM-centric scheduling systems and end-to-end trainable\nmultimodal systems.\nThe LLM-centric scheduling system [WYQ+23, YLW+23, LWS+23, SST+23, LHW+23, SMV23,\nCLS+23] takes advantage of many vision foundation models (e.g., Stable Diffusion [RBL+22],\nControlNet [ZA23], BLIP [LLXH22], etc.), and schedules these models in a language-centric manner.\nFor example, Visual ChatGPT [WYQ+23] develops a set of prompts to incorporate visual information\ninto ChatGPT, enabling users to draw or edit images through chatting. MM-REACT [YLW+23]\nleverages vision experts to augment its multimodal capabilities by incorporating a textual prompt\ndesign that can effectively represent various visual signals, including text descriptions, coordinates,\nand aligned file names, for images and videos. HuggingGPT [SST+23] connects LLMs with extensive\nAI models in machine learning communities, tackling user requests through ChatGPT\u2019s task planning,\nmodel selection, and response summarization capabilities. Further, TaskMatrix.AI [LWS+23] largely\nextends the scale and connects foundation models with millions of APIs for solving tasks in both\ndigital and physical domains. Differently, InternGPT [LHW+23] incorporates pointing instructions\n(e.g., clicking and dragging) for better communication between chatbots and users, while also\nimproving the accuracy of chatbots in performing vision-centric tasks. Nevertheless, this approach\nhas several limitations, such as the expenses associated with API calls or the storage space required\nfor the pre-trained weights of foundation models.\nEnd-to-end trainable multimodal system [HSD+22, ADL+22, HDW+23, PWD+23, HZH+21,\nXHL+21, ZCS+23, HML+23, LLSH23, DLL+23, LLWL23, LZR+23, WCC+23, SLL+23,\nZHZ+23, GHZ+23, KSF23, LZC+23] integrates vision and language models into a unified model,\nwhich are further trained on multimodal datasets. For instance, Flamingo [ADL+22] leverages\n9\ngated cross-attention to fuse pre-trained vision and language models, showing impressive ability in\ndownstream multimodal tasks. Besides, BLIP-2 [LLSH23] utilized Q-Former to align the visual\nfeatures with a large language model. Furthermore, Instruct-BLIP improves the training of Q-Former\nby introducing a novel instruction-aware visual feature extraction method. Based on this design,\nMiniGPT-4 [ZCS+23] uses Vicuna [CLL+23] as the text encoder and fine-tunes detailed image\ndescriptions to better match user intent. Sparkles unlocks multimodal instruction-following models\u2019\ncapabilities in open-ended dialogues involving multiple images [HML+23]. LLaVA [LLWL23] in-\njects visual features into the language model by treating image tokens as a foreign language, and uses\nconversation generated by GPT-4 [GPT23] for fine-tuning. KOSMOS-1 [HDW+23] is trained from\nscratch using web-scale corpora while showing impressive performance in zero-shot, few-shot, and\nmultimodal chain-of-thought prompting settings. Analogously, KOSMOS-2 [PWD+23] incorporates\ngrounding and referring abilities and can accept image regions users select using bounding boxes as\ninput. mPLUG-Owl [YXX+23] efficiently fine-tunes the language model using low-rank adaption\nwith multimodal instruction datasets. Otter [LZC+23] is built using Flamingo and aims to explore\nmultimodal in-context learning capabilities.\n4.2\nText Image Understanding\nText image understanding is a cutting-edge technology that harnesses the power of artificial intel-\nligence, including natural language processing and computer vision, to automatically comprehend,\ncategorize, and extract information from documents [CXLW21]. Any file containing written or\nprinted characters can be considered a document, including web pages, slides, posters, and even scene\ntext images. Documents are ubiquitous in our daily lives, so the research on documents is significant.\nBefore the deep learning era, researchers used rule-based heuristic approaches for document anal-\nysis [WCW82, O\u2019G93].\nThey manually observed layout information and summarized heuris-\ntic rules, but these methods are not scalable and require enormous labour costs. Subsequently,\nthe rise of deep learning has led to significant advancements in the field of Document AI\n[XLC+20, XXL+21, XLC+21, HLC+22, CLC+22, LXCW21, LXL+22, LGK+21, AJK+21,\nWJD22, GMW+22, LBY+21, YLZ+23]. For example, LayoutLM series [XLC+20, XXL+21,\nHLC+22] employs large-scale document data for pre-training and incorporates text, layout, and\nimage information into the model, showing impressive performance in downstream tasks like key\ninformation extraction and document question answering. Similarly, DocFormer [AJK+21] intro-\nduces an additional task to reconstruct the document image during pre-training. Donut [KHY+21]\nintroduces an OCR-free document understanding Transformer, directly mapping an input document\nimage to the desired output with OCR. MarkupLM [LXCW21] takes advantage of large-scale web-\npages from Common Crawl and uses node-level hierarchical structure information as the pre-training\nobjective. XDoc [CLC+22] introduces a unified framework for tackling multiple document formats\nin one model for parameter efficiency. UDOP [TYW+23] designs a unified model that integrates text,\nimage, and layout modalities, showing impressive performance on diverse document understanding\ntasks. Pix2Struct [LJT+23] is a pre-trained image-to-text model trained to parse masked screenshots\nof web pages into simplified HTML.\nDespite significant progress in text image understanding, most models are designed for specific tasks\nand lack generalizability. On the contrary, the proposed KOSMOS-2.5 represents an important step\nforward in this field, demonstrating the potential of MLLM in achieving robust and generalizable\nperformance across a wide range of text image types.\n5\nConclusion and Future Work\nWe introduced KOSMOS-2.5, a multimodal literate model built on the strengths of KOSMOS-2,\ndesigned to enhance machine understanding of text-intensive images. This model shifted from\nconventional encoder-only/encoder-decoder models to a more unified, decoder-only architecture. The\nshift to generative multimodal language modeling simplifies task interfaces, eliminating the need for\ncomplex, task-specific pipelines. Moreover, KOSMOS-2.5 demonstrated potential in few-shot and\nzero-shot learning capabilities, laying a foundation for future advances and scalability in multimodal\nliterate models.\nDespite these promising results, our current model faces some limitations, offering valuable future\nresearch directions. For instance, KOSMOS-2.5 currently does not support fine-grained control of\n10\ndocument elements\u2019 positions using natural language instructions, despite being pre-trained on inputs\nand outputs involving the spatial coordinates of text. Instruction tuning could offer a promising\nroute to enhance this aspect of the model, leading to broader application capabilities. Furthermore,\ndocuments spanning multiple pages pose a challenge as they typically demand holistic processing\nand comprehension. Meanwhile, it is also feasible that KOSMOS-2.5 allows for multiple image pages\ninterleaved with text as input; however, managing long context windows remains a vital issue we aim\nto address in future work.\nIn the broader research landscape, a significant direction lies in furthering the development of model\nscaling capabilities. With an expanding spectrum of tasks and rising complexities, scaling up the\nmodel to handle larger volumes of data is crucial for the progression of multimodal literate models.\nUltimately, our goal is to develop a model that effectively interprets both visual and textual data, and\ngeneralizes smoothly across an expanded array of text-intensive multimodal tasks.\nAcknowledgement\nWe would like to acknowledge Zhiliang Peng for the helpful discussions.\nReferences\n[ADL+22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning. Advances in Neural Informa-\ntion Processing Systems, 35:23716\u201323736, 2022.\n[AJK+21] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha.\nDocformer: End-to-end transformer for document understanding. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 993\u20131003, 2021.\n[BCSS23] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural\noptical understanding for academic documents, 2023.\n[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020.\n[Bro97] Andrei Z Broder. On the resemblance and containment of documents. In Proceedings.\nCompression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages\n21\u201329. IEEE, 1997.\n[Cha22] ChatGPT. https://openai.com/blog/chatgpt, 2022.\n[CLC+22] Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Xdoc: Unified pre-\ntraining for cross-format document understanding. arXiv preprint arXiv:2210.02849,\n2022.\n[CLL+23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\n[CLS+23] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor\nDarrell, and Ziwei Liu. Language models are visual reasoning coordinators. In ICLR\n2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,\n2023.\n[CXLW21] Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. Document ai: Benchmarks, models\nand applications, 2021.\n11\n[DBK+21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. In ICLR, 2021.\n[DLL+23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\n[GHZ+23] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\nPan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010, 2023.\n[GMW+22] Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu,\nand Liqing Zhang.\nXylayoutlm: Towards layout-aware multimodal networks for\nvisually-rich document understanding. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4583\u20134592, 2022.\n[GPT23] GPT-4. https://openai.com/gpt-4, 2023.\n[HCH+19] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and\nCV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction.\nIn 2019 International Conference on Document Analysis and Recognition (ICDAR),\npages 1516\u20131520. IEEE, 2019.\n[HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,\nTengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all\nyou need: Aligning perception with language models. arXiv preprint arXiv:2302.14045,\n2023.\n[HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\n[HLC+22] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-\ntraining for document ai with unified text and image masking. In Proceedings of the\n30th ACM International Conference on Multimedia, 2022.\n[HML+23] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Collier Nigel, and Yutong Lu.\nSparkles: Unlocking chats across multiple images for multimodal instruction-following\nmodels. arXiv preprint arXiv:2308.16463, 2023.\n[HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n[HZH+21] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong\nFu. Seeing out of the box: End-to-end pre-training for vision-language representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12976\u201312985, 2021.\n[JET19] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset\nfor form understanding in noisy scanned documents, 2019.\n[KHY+21] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok\nHwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document\nunderstanding transformer without ocr. arXiv preprint arXiv:2111.15664, 7:15, 2021.\n[KR18] Taku Kudo and John Richardson. Sentencepiece: A simple and language indepen-\ndent subword tokenizer and detokenizer for neural text processing. arXiv preprint\narXiv:1808.06226, 2018.\n[KSF23] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to\nimages for multimodal generation. arXiv preprint arXiv:2301.13823, 2023.\n12\n[LBY+21] Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo\nSi.\nStructurallm: Structural pre-training for form understanding.\narXiv preprint\narXiv:2105.11210, 2021.\n[LCH+20] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Table-\nbank: A benchmark dataset for table detection and recognition, 2020.\n[LGK+21] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain,\nVarun Manjunatha, and Hongfu Liu. Selfdoc: Self-supervised document representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5652\u20135660, 2021.\n[LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n[LHW+23] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen,\nQinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving\nvision-centric tasks by interacting with chatbots beyond language. arXiv preprint\narXiv:2305.05662, 2023.\n[LIN+21] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language\nmodels better. arXiv preprint arXiv:2107.06499, 2021.\n[LJT+23] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Mar-\ntin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina\nToutanova. Pix2struct: Screenshot parsing as pretraining for visual language under-\nstanding. In International Conference on Machine Learning, pages 18893\u201318912.\nPMLR, 2023.\n[LLC+22] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha\nZhang, Zhoujun Li, and Furu Wei. Trocr: Transformer-based optical character recogni-\ntion with pre-trained models, 2022.\n[LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023.\n[LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\narXiv preprint arXiv:2304.08485, 2023.\n[LLXH22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and generation. In Inter-\nnational Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[LWS+23] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai\nLu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting\nfoundation models with millions of apis. arXiv preprint arXiv:2303.16434, 2023.\n[LXCW21] Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. Markuplm: Pre-training of text\nand markup language for visually-rich document understanding.\narXiv preprint\narXiv:2110.08518, 2021.\n[LXL+22] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Dit: Self-\nsupervised pre-training for document image transformer. In Proceedings of the 30th\nACM International Conference on Multimedia, pages 3530\u20133539, 2022.\n[LZC+23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei\nLiu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint\narXiv:2305.03726, 2023.\n13\n[LZR+23] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji.\nCheap and quick: Efficient vision-language instruction tuning for large language models.\narXiv preprint arXiv:2305.15023, 2023.\n[O\u2019G93] Lawrence O\u2019Gorman. The document spectrum for page layout analysis. IEEE Transac-\ntions on pattern analysis and machine intelligence, 15(11):1162\u20131173, 1993.\n[Ope23] OpenAI. Gpt-4 technical report, 2023.\n[PSL+19] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo,\nand Hwalsuk Lee. Cord: A consolidated receipt dataset for post-ocr parsing. Document\nIntelligence Workshop at Neural Information Processing Systems, 2019.\n[PWD+23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and\nFuru Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023.\n[RBL+22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695,\n2022.\n[SLL+23] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One\nmodel to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n[SMV23] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python\nexecution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[SST+23] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting\nZhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint arXiv:2303.17580, 2023.\n[TYW+23] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu,\nMichael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for\nuniversal document processing.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 19254\u201319264, 2023.\n[WCC+23] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping\nLuo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an\nopen-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175, 2023.\n[WCW82] Kwan Y. Wong, Richard G. Casey, and Friedrich M. Wahl. Document analysis system.\nIBM journal of research and development, 26(6):647\u2013656, 1982.\n[WJD22] Jiapeng Wang, Lianwen Jin, and Kai Ding. Lilt: A simple yet effective language-\nindependent layout transformer for structured document understanding. arXiv preprint\narXiv:2202.13669, 2022.\n[WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers.\narXiv preprint arXiv:2210.06423, 2022.\n[WXC+21] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. Layoutreader: Pre-\ntraining of text and layout for reading order detection. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing, pages 4735\u20134744,\n2021.\n[WYQ+23] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan\nDuan. Visual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint arXiv:2303.04671, 2023.\n[XHL+21] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li,\nand Jiebo Luo. Probing inter-modality: Visual parsing with self-attention for vision-\nand-language pre-training. In Advances in Neural Information Processing Systems,\nvolume 34, pages 4514\u20134528, 2021.\n14\n[XLC+20] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm:\nPre-training of text and layout for document image understanding. In Proceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining, pages 1192\u20131200, 2020.\n[XLC+21] Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, and Furu Wei. Layoutxlm: Multimodal pre-training for multilingual visually-\nrich document understanding, 2021.\n[XXL+21] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei\nFlorencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2:\nMulti-modal pre-training for visually-rich document understanding. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 2579\u20132591, Online, August 2021. Association for Computational\nLinguistics.\n[YLW+23] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt\nfor multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[YLZ+23] Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan Guo, Xiameng\nQin, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang. Structextv2: Masked visual-\ntextual prediction for document image pre-training. arXiv preprint arXiv:2303.00289,\n2023.\n[YXX+23] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[ZA23] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models. arXiv preprint arXiv:2302.05543, 2023.\n[ZCS+23] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:\nEnhancing vision-language understanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023.\n[ZHZ+23] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nLi, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models\nwith zero-init attention. arXiv preprint arXiv:2303.16199, 2023.\n[ZS89] Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance\nbetween trees and related problems. SIAM journal on computing, 18(6):1245\u20131262,\n1989.\n15\nA\nSupplementary Material\nA.1\nHyperparameters\nThe settings of hyperparameters are demonstrated in Table 4.\nHyperparameters\nNumber of layers\n24\nHidden size\n1,536\nFFN inner hidden size\n6,144\nAttention heads\n16\nActivation function\nGeLU [HG16]\nVocabulary size\n108,481\nSoft tokens V size\n2,048\nMax sequence length\n4,096\nInitialization\nMagneto [WMH+22]\n(a) Basic hyperparameters of KOSMOS-2.5\nHyperparameters\nTraining steps\n200,000\nWarmup steps\n375\nBatch size\n1,024\nOptimizer\nAdamW\nLearning rate\n2e-4\nLearning rate decay\nLinear\nAdam \u03b2\n(0.9, 0.98)\nWeight decay\n0.01\nDropout\n0.1\n(b) Training hyperparameters of KOSMOS-2.5\nTable 4: Hyperparameters of KOSMOS-2.5\nA.2\nData Samples\nWe demonstrate some of the training samples in KOSMOS-2.5, which include the input and output\nfrom IIT-CDIP, arXiv papers, PowerPoint slides, general PDFs, web screenshots, README, DOCX,\nLATEX and HTML.\n(a) Input\n(b) Rendered output\nFigure 4: A training sample for the layout-based task from IIT-CDIP\nA.3\nExamples of Model Inference\n16\n(a) Input\n(b) Rendered output\nFigure 5: A training sample for the layout-based task from arXiv papers (single-column)\n(a) Input\n(b) Rendered output\nFigure 6: A training sample for the layout-based task from arXiv papers (two-column)\n17\n(a) Input\n(b) Rendered output\nFigure 7: A training sample for the layout-based task from PowerPoint slides\n(a) Input\n(b) Rendered output\nFigure 8: A training sample for the layout-based task from PDFs\n18\n(a) Input\n(b) Rendered output\nFigure 9: A training sample for the layout-based task from web screenshots\n(a) Input\n(b) Rendered output\nFigure 10: A training sample for the markup-based task from README\n19\n(a) Input\n(b) Rendered output\nFigure 11: A training sample for the markup-based task from DOCX\n(a) Input\n(b) Rendered output\nFigure 12: A training sample for the markup-based task from LATEX (single-column)\n20\n(a) Input\n(b) Rendered output\nFigure 13: A training sample for the markup-based task from LATEX (two-column)\n(a) Input\n(b) Rendered output\nFigure 14: A training sample for the markup-based task from HTMLs\n21\nListing 1: Model outputs using the layout-based prompt\n[x_52] [y_113] [x_756] [y_145]: NYC Department of Education School Year Calendar 2023\u22122024\n[x_52] [y_159] [x_826] [y_181]: This is the 2023\u221224 school year calendar for all 3K\u221212 NYCDOE public schools. If your child attends a private,\n[x_52] [y_180] [x_820] [y_202]: parochial, charter school, NYC Early Education Center (NYCEEC) or Family Childcare Program, please contact\n[x_52] [y_201] [x_639] [y_223]: your child\u2019s school for information about their calendar. Please note the following:\n[x_65] [y_223] [x_77] [y_245]: \u2022\n[x_92] [y_223] [x_825] [y_245]: On days when school buildings are closed due to inclement weather or other emergencies, all students\n[x_92] [y_244] [x_525] [y_266]: and families should plan on participating in remote learning.\n[x_65] [y_265] [x_77] [y_287]: \u2022\n[x_92] [y_265] [x_846] [y_287]: Individual schools\u2019 Parent\u2212Teacher Conference dates might be different from the dates below. Your child\u2019s\n[x_92] [y_286] [x_491] [y_308]: teacher will work with you to schedule your conference.\n[x_65] [y_308] [x_77] [y_330]: \u2022\n[x_92] [y_307] [x_845] [y_330]: On this schedule, elementary schools are defined as programs that serve kindergarten (K) through grade\n[x_92] [y_329] [x_826] [y_351]: 8, including schools with 3\u2212K and Pre\u2212K programs, as well as those that end in grade 5. Middle schools\n[x_92] [y_350] [x_810] [y_372]: are defined as programs that serve grades 6\u22128, and high schools are defined as programs that serve\n[x_92] [y_371] [x_186] [y_393]: grades 9\u221212.\n[x_60] [y_414] [x_106] [y_436]: DATE\n[x_318] [y_414] [x_399] [y_436]: WEEKDAY\n[x_605] [y_414] [x_659] [y_436]: EVENT\n[x_60] [y_437] [x_155] [y_459]: September 7\n[x_297] [y_437] [x_366] [y_459]: Thursday\n[x_432] [y_437] [x_565] [y_459]: First day of school\n[x_60] [y_470] [x_164] [y_492]: September 14\n[x_297] [y_470] [x_366] [y_492]: Thursday\n[x_432] [y_459] [x_804] [y_481]: Evening Parent\u2212Teacher Conferences for elementary\n[x_432] [y_480] [x_622] [y_503]: schools and Pre\u2212K Centers\n[x_60] [y_514] [x_164] [y_536]: September 21\n[x_297] [y_514] [x_366] [y_536]: Thursday\n[x_432] [y_504] [x_832] [y_526]: Evening Parent\u2212Teacher Conferences for middle schools\n[x_432] [y_525] [x_553] [y_547]: and D75 schools\n[x_60] [y_548] [x_164] [y_570]: September 25\n[x_297] [y_548] [x_360] [y_570]: Monday\n[x_432] [y_548] [x_630] [y_570]: Yom Kippur, schools closed\n[x_60] [y_581] [x_164] [y_603]: September 28\n[x_297] [y_581] [x_366] [y_603]: Thursday\n[x_432] [y_570] [x_818] [y_593]: Evening Parent\u2212Teacher Conferences for high schools,\n[x_432] [y_592] [x_601] [y_614]: K\u221212, and 6\u221212 schools\n[x_60] [y_625] [x_135] [y_647]: October 9\n[x_297] [y_625] [x_360] [y_647]: Monday\n[x_432] [y_614] [x_786] [y_636]: Italian Heritage/Indigenous Peoples\u2019 Day, schools\n[x_432] [y_636] [x_482] [y_658]: closed\n[x_60] [y_679] [x_152] [y_701]: November 2\n[x_297] [y_679] [x_366] [y_701]: Thursday\n[x_432] [y_658] [x_829] [y_680]: Afternoon and Evening Parent\u2212Teacher Conferences for\n[x_432] [y_679] [x_833] [y_701]: elementary schools; students in these schools dismissed\n[x_432] [y_700] [x_556] [y_723]: three hours early\n[x_60] [y_727] [x_152] [y_749]: November 7\n[x_297] [y_727] [x_360] [y_749]: Tuesday\n[x_432] [y_727] [x_745] [y_749]: Election Day, students do not attend school\n[x_60] [y_775] [x_152] [y_797]: November 9\n[x_297] [y_775] [x_366] [y_797]: Thursday\n[x_432] [y_754] [x_829] [y_776]: Afternoon and Evening Parent\u2212Teacher Conferences for\n[x_432] [y_775] [x_793] [y_797]: middle schools and D75 schools; students in these\n[x_432] [y_796] [x_687] [y_818]: schools dismissed three hours early\n[x_60] [y_829] [x_161] [y_851]: November 16\n[x_297] [y_829] [x_366] [y_851]: Thursday\n[x_432] [y_819] [x_818] [y_841]: Evening Parent\u2212Teacher Conferences for high schools,\n[x_432] [y_840] [x_601] [y_862]: K\u221212, and 6\u221212 schools\n[x_60] [y_884] [x_161] [y_906]: November 17\n[x_297] [y_884] [x_344] [y_906]: Friday\n[x_432] [y_863] [x_773] [y_885]: Afternoon Parent\u2212Teacher Conferences for high\n[x_432] [y_884] [x_791] [y_906]: schools, K\u221212, and 6\u221212 schools; students in these\n[x_432] [y_905] [x_687] [y_927]: schools dismissed three hours early\n[x_60] [y_928] [x_186] [y_950]: November 23\u221224\n[x_297] [y_928] [x_416] [y_950]: Thursday\u2212Friday\n[x_432] [y_928] [x_692] [y_950]: Thanksgiving Recess, schools closed\n[x_60] [y_960] [x_234] [y_983]: December 25\u2212January 1\n[x_297] [y_950] [x_368] [y_972]: Monday\u2212\n[x_297] [y_971] [x_360] [y_994]: Monday\n[x_432] [y_960] [x_646] [y_983]: Winter Recess, schools closed\n[x_60] [y_999] [x_140] [y_1021]: January 15\n[x_297] [y_999] [x_360] [y_1021]: Monday\n[x_432] [y_999] [x_789] [y_1021]: Rev. Dr. Martin Luther King Jr. Day, schools closed\n[x_60] [y_1027] [x_170] [y_1049]: January 23\u2212 26\n[x_297] [y_1027] [x_410] [y_1049]: Tuesday\u2212Friday\n[x_432] [y_1027] [x_603] [y_1049]: Regents Administration\n[x_52] [y_1099] [x_311] [y_1118]: NYCDOE School Year Calendar 2023\u221224\n22\nListing 2: Model outputs using the markup-based prompt\n# NYC Department of Education School Year Calendar 2023\u22122024\nThis is the 2023\u221224 school year calendar for all 3K\u221212 NYCDOE public schools. If your child attends a private, parochial, charter school, NYC Early Education\n,\u2192 Center (NYCEEC) or Family Childcare Program, please contact your child\u2019s school for information about their calendar. Please note the following:\n\u2212 On days when school buildings are closed due to inclement weather or other emergencies, all students and families should plan on participating in remote learning.\n\u2212 Individual schools\u2019 Parent\u2212Teacher Conference dates might be different from the dates below. Your child\u2019s teacher will work with you to schedule your conference.\n,\u2192\n\u2212 On this schedule, **elementary schools** are defined as programs that serve kindergarten (K) through grade 8, including schools with 3\u2212K and Pre\u2212K programs,\n,\u2192 as well as those that end in grade 5. **Middle schools** are defined as programs that serve grades 6\u22128, and **high schools** are defined as programs\n,\u2192 that serve grades 9\u221212.\n| DATE | WEEKDAY | EVENT |\n| \u2212\u2212\u2212 | \u2212\u2212\u2212 | \u2212\u2212\u2212 |\n| September 7 | Thursday | First day of school |\n| September 14 | Thursday | Evening Parent\u2212Teacher Conferences for elementary schools and Pre\u2212K Centers |\n| September 21 | Thursday | Evening Parent\u2212Teacher Conferences for middle schools and D75 schools |\n| September 25 | Monday | Yom Kippur, schools closed |\n| September 28 | Thursday | Evening Parent\u2212Teacher Conferences for high schools, K\u221212, and 6\u221212 schools |\n| October 9 | Monday | Italian Heritage/Indigenous Peoples\u2019 Day, schools closed |\n| November 2 | Thursday | Afternoon and Evening Parent\u2212Teacher Conferences for elementary schools; students in these schools dismissed three hours early |\n| November 7 | Tuesday | Election Day, students do not attend school |\n| November 9 | Thursday | Afternoon and Evening Parent\u2212Teacher Conferences for middle schools and D75 schools; students in these schools dismissed three hours\n,\u2192 early |\n| November 16 | Thursday | Evening Parent\u2212Teacher Conferences for high schools, K\u221212, and 6\u221212 schools |\n| November 17 | Friday | Afternoon Parent\u2212Teacher Conferences for high schools, K\u221212, and 6\u221212 schools; students in these schools dismissed three hours early |\n| November 23\u221224 | Thursday\u2212Friday | Thanksgiving Recess, schools closed |\n| December 25\u2212January 1 | Monday\u2212 Monday | Winter Recess, schools closed |\n| January 15 | Monday | Rev. Dr. Martin Luther King Jr. Day, schools closed |\n| January 23\u2212 26 | Tuesday\u2212Friday | Regents Administration |\n23\n"
  },
  {
    "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
    "link": "https://arxiv.org/pdf/2309.11495.pdf",
    "upvote": "37",
    "text": "CHAIN-OF-VERIFICATION REDUCES HALLUCINATION\nIN LARGE LANGUAGE MODELS\nShehzaad Dhuliawala\nMeta AI & ETH Z\u00a8urich\nMojtaba Komeili\nMeta AI\nJing Xu\nMeta AI\nRoberta Raileanu\nMeta AI\nXian Li\nMeta AI\nAsli Celikyilmaz\nMeta AI\nJason Weston\nMeta AI\nABSTRACT\nGeneration of plausible yet incorrect factual information, termed hallucination,\nis an unsolved issue in large language models. We study the ability of language\nmodels to deliberate on the responses they give in order to correct their mistakes.\nWe develop the Chain-of-Verification (COVE) method whereby the model first (i)\ndrafts an initial response; then (ii) plans verification questions to fact-check its\ndraft; (iii) answers those questions independently so the answers are not biased\nby other responses; and (iv) generates its final verified response. In experiments,\nwe show COVE decreases hallucinations across a variety of tasks, from list-based\nquestions from Wikidata, closed book MultiSpanQA and longform text generation.\n1\nINTRODUCTION\nLarge Language Models (LLMs) are trained on huge corpora of text documents with billions of\ntokens of text. It has been shown that as the number of model parameters is increased, performance\nat tasks such as closed book QA improve in accuracy, and larger models can generate more correct\nfactual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can\nstill fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those\nthat occur relatively rarely in the training corpora. In those cases where the model is incorrect, they\ninstead generate an alternative response which is typically plausible looking (e.g., a similar entity, but\nan incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez\net al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the\nhallucination problem can be exacerbated due to the issue of exposure bias (Wang & Sennrich, 2020).\nThe current wave of language modeling research goes beyond next word prediction, and has focused\non their ability to reason. Improved performance in reasoning tasks can be gained by encouraging\nlanguage models to first generate internal thoughts or reasoning chains before responding (Wei et al.,\n2022; Adolphs et al., 2021; Wang et al., 2022; Lanchantin et al., 2023), as well as updating their\ninitial response through self-critique (Press et al., 2022; Madaan et al., 2023). In this work we\nfollow this line of research to study how and when language-model-based reasoning can be used to\nreduce hallucinations. We develop an approach, called Chain-of-Verification (CoVe) which, given\nan initial draft response, first plans verification questions to check its work, and then systematically\nanswers those questions in order to finally produce an improved revised response. We find that\nindependent verification questions tend to provide more accurate facts than those in the original\nlongform answer, and hence improve the correctness of the overall response. We study variations on\nthis recipe across a range of tasks: from list-based questions, closed booked QA and longform text\ngeneration. We first propose a joint approach for generating the entire verification chain left-to-right,\nwhich improves performance and decreases hallucinations compared to the baseline language model.\nHowever, models that attend to existing hallucinations in the context from their own generations tend\nto repeat the hallucinations. Hence we also introduce further improvements with factored variants\nwhich separate out the verification chain steps, in terms of which context is attended to. We show\nhow these factored variants give further performance gains across all three tasks considered.\n1\narXiv:2309.11495v2  [cs.CL]  25 Sep 2023\nQuery\n       1. Baseline Response\n       4. Final Verified Response\n        2. Plan Verifications\nHere are some politicians who were born in NY, New York:\n1. Hillary Clinton - former secretary of state and former Democratic presidential nominee\n2. Donald Trump - former president of the United States\n3. Michael Bloomberg - former Mayor of New York City and former Democratic presidential candidate \n  \u2026 <list continues..>\nName some politicians who were born in NY, New York. \n  - Where was Hillary Clinton born?  \n  - Where was Donald Trump born?   \n  - Where was Michael Bloomberg born?\n  \u2026 <questions continue..>\nHere are some politicians who were born in NY, New York:\n1. Donald Trump - former president of the United States\n2. Alexandria Ocasio-Cortez - Democratic member of the U.S. House of Representatives\n  \u2026 <list continues..>\nHillary Clinton was born in Chicago, Illinois, United   \nStates on October 26, 1947.\nDonald Trump was born on June 14, 1946, in Queens, \nNew York City, New York, United States\n\u2026\nMichael Bloomberg was born on February 14, 1942, in \nBoston, Massachusetts, United States.\n        3. Execute Verifications\nFigure 1: Chain-of-Verification (CoVe) method. Given a user query, a large language model generates\na baseline response that may contain inaccuracies, e.g. factual hallucinations. We show a query here\nwhich failed for ChatGPT (see section 9 for more details). To improve this, CoVe first generates a\nplan of a set of verification questions to ask, and then executes that plan by answering them and hence\nchecking for agreement. We find that individual verification questions are typically answered with\nhigher accuracy than the original accuracy of the facts in the original longform generation. Finally,\nthe revised response takes into account the verifications. The factored version of CoVe answers\nverification questions such that they cannot condition on the original response, avoiding repetition\nand improving performance.\n2\nRELATED WORK\nHallucination is a general problem in language model generations that appears across many tasks,\nfrom summarization (Maynez et al., 2020) to open-domain dialogue (Roller et al., 2020), and has not\nbeen resolved by simply scaling up training data or model size (Zhang et al., 2023). For a survey of\nthe hallucination issue, see Ji et al. (2023). A majority of the methods for reducing hallucination can\nbe divided into roughly three categories: training-time correction, generation-time correction and via\naugmentation (tool-use).\nIn training-time correction methods, an attempt is made to improve the raw left-to-right generations\nof an encoder-decoder or decoder-only language model by either training or otherwise adjusting\nthe model weights to decrease the probability of hallucinated generations. This includes using\nreinforcement learning (Roit et al., 2023; Wu et al., 2023), constrastive learning (Chern et al., 2023b;\nSun et al., 2023b) and other methods (Li et al., 2023).\nIn generation-time correction, a common theme is to make reasoning decisions \u201con top of\u201d the base\nLLM in order to make them more reliable. For example, by considering the probabilities of the\ngenerated tokens (Mielke et al., 2022; Kadavath et al., 2022). In Manakul et al. (2023) multiple\nsamples are drawn from the model to detect hallucinations. In Varshney et al. (2023) hallucinations\nare identified using low confidence scores, and their correctness is checked through a validation\n2\nprocedure, mitigated, and then the generation is continued. An alternative to using the confidence\nscores is to leverage inconsistencies in the LLMs output to detect hallucination. Agrawal et al. (2023)\nuse both multiple samples and consistency detection by asking direct and indirect queries to check for\nhallucinated references. Cohen et al. (2023) introduce a method called LM vs LM which simulates\nan interactive setup between two LLMs where one LLM acts as an examiner and tests if the output\nis consistent via repeated cross-examination. Cohen et al. (2023) shows that using inconsistencies\nfor QA tasks can outperform using confidence scores for hallucination detection. COVE also uses a\nrelated self-consistency approach, but without the multi-agent (multi-LLM) debate concept.\nA third approach is to use external tools to help mitigate hallucinations, rather than relying solely on\nthe abilities of the language model itself. For example, retrieval-augmented generation can decrease\nhallucinations by using factual documents for grounding (Shuster et al., 2021; Jiang et al., 2023b;\nYu et al., 2023) or chain-of-thought verification (Zhao et al., 2023). Other approaches include using\ntools for fact-checking (Chern et al., 2023a; Galitsky, 2023; Peng et al., 2023), or linking to external\ndocuments with attribution (Menick et al., 2022; Rashkin et al., 2023; Gao et al., 2023).\nThere are also a number of related works in improving reasoning for logical and mathematical tasks,\neven if they do not address reducing hallucination explicitly. Several approaches have been shown to\nimprove results with extended reasoning steps by the system, such as chain-of-thought (Wei et al.,\n2022), deductive verification (Ling et al., 2023), and self-verification (Miao et al., 2023; Jiang et al.,\n2023a; Weng et al., 2022). The latter tries to predict the (masked) question given the answer for math\nproblems, and use that as evidence that this is the correct solution.\n3\nCHAIN-OF-VERIFICATION\nOur approach assumes access to a base LLM that \u2013 despite potentially being prone to hallucination \u2013\nis capable of being prompted with general instructions in either a few-shot or zero-shot fashion. A key\nassumption of our method is that this language model, when suitably prompted, can both generate\nand execute a plan of how to verify itself in order to check its own work, and finally incorporate this\nanalysis into an improved response.\nOur overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:\n1. Generate Baseline Response: Given a query, generate the response using the LLM.\n2. Plan Verifications: Given both query and baseline response, generate a list of verification\nquestions that could help to self-analyze if there are any mistakes in the original response.\n3. Execute Verifications: Answer each verification question in turn, and hence check the answer\nagainst the original response to check for inconsistencies or mistakes.\n4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a\nrevised response incorporating the verification results.\nEach of these steps is performed by prompting the same LLM in different ways to obtain the desired\nresponse. While steps (1), (2) and (4) all can be invoked with a single prompt, we investigate\nvariations of step (3) including joint, 2-step and factored versions. These variants either involve a\nsingle prompt, two prompts or else independent prompts per question, where more sophisticated\ndecomposition can yield improved results.\nWe describe these steps in more detail below. An overview of the approach is illustrated in Figure 1,\nand in the Appendix in Figure 3.\n3.1\nBASELINE RESPONSE\nGiven a query, we generate left-to-right as usual using the LLM, with no special tricks. While this is\nthe first step in the CoVe pipeline, it also serves as the baseline we wish to improve in our experiments\n(i.e., we will directly compare this baseline response with the final verified response from our overall\nmethod).\nGiven such baseline generations are typically prone to hallucination, CoVe attempts to identify these\nhallucinations, and correct them, in the following steps.\n3\n3.2\nPLAN VERIFICATIONS\nConditioned on the original query and the baseline response, the model is prompted to generate\na series of verification questions that test the factual claims in the original baseline response. For\nexample if part of a longform model response contains the statement \u201cThe Mexican\u2013American War\nwas an armed conflict between the United States and Mexico from 1846 to 1848\u201d, then one possible\nverification question to check those dates could be \u201cWhen did the Mexican American war start and\nend?\u201d. We note that verification questions are not templated and the language model is free to phrase\nthese in any form it wants, and they also do not have to closely match the phrasing of the original text.\nIn our experiments, we perform such verification planning by providing a few-shot prompt of\n(response, verification) demonstrations to our LLM. See section 8 for the few-shot prompts we will\nuse in our experiments. We note it is also possible with a sufficiently performant instruction-following\nLLM that this could be performed zero-shot.\n3.3\nEXECUTE VERIFICATIONS\nGiven the planned verification questions, the next step is to answer them in order to assess if any\nhallucinations exist. While techniques such as retrieval-augmentation could be used in this process,\nsuch as verification via search engine, in this work we do not explore tool-use. Instead, we consider\nonly using the LLM itself in all steps of CoVe, hence the model is used to check its own work. We\ninvestigate several variants of verification execution, called joint, 2-Step, factored and factor+revise.\nJoint\nIn the joint method, the planning and execution (steps 2 and 3) are accomplished by using a\nsingle LLM prompt, whereby the few-shot demonstrations include both verification questions and\ntheir answers immediately after the questions. In this approach separate prompts are not needed.\n2-Step\nA potential disadvantage of the joint method is that because the verification questions must\ncondition on the baseline response in the LLM context, and the method is joint, the verification\nanswers have to condition on the initial response as well. This may increase the likelihood of\nrepetition, another known issue of modern LLMs (Holtzman et al., 2019). This means the verification\nquestions might hallucinate similarly to the original baseline response, which defeats the purpose.\nWe hence instead separate the planning and execution into separate steps, both with their own LLM\nprompt. The planning prompt conditions on the baseline response in the first step. The verification\nquestions generated from planning are answered in the second step, where crucially the context given\nto the LLM prompt only contains the questions, and not the original baseline response and hence\ncannot repeat those answers directly.\nFactored\nAnother, more sophisticated approach, is to answer all questions independently as separate\nprompts. Again, crucially, those prompts do not contain the original baseline response and are hence\nnot prone to simply copying or repeating it. The factored approach has the further advantage of\nremoving any potential interference not only from the baseline response, but also between answer\ncontexts, and is somewhat related to the recent (concurrent) work of Radhakrishnan et al. (2023)\nfor subquestion answering by factored decomposition, hence we adopt their naming. It can also\npotentially handle more verification questions by virtue of them not all having to fit with the same\nsingle context. While this is potentially more computationally expensive, requiring the execution\nof many more LLM prompts, they can be run in parallel, and hence be batched. In order to do\nthis, we first have to take the set of generated questions from subsection 3.2 and parse them into\nseparate questions, which is a relatively easy task as the few-shot demonstrations we provide indicate\nthey should be generated as a comma-separated list. We can then split them out into separate LLM\nprompts.\nFactor+Revise\nAfter answering the verification questions, the overall CoVe pipeline then has to\neither implicitly or explicitly cross-check whether those answers indicate an inconsistency with the\noriginal responses. In the factor+revise approach, we execute this as a deliberate step via an extra\nLLM prompt, which may make it easier for the final system to reason about this step explicitly.\nDifferently to answering the verification questions, the cross-checking phase needs to condition\non both the baseline response and the verification question and answer. We thus execute this as\nseparate LLM prompts, one \u201ccross-check\u201d prompt for each question, with again a set of few-shot\n4\ndemonstrations showing the desired output. For example if the original baseline response contained\nthe phrase \u201cIt followed in the wake of the 1845 U.S. annexation of Texas. . . \u201d and CoVe generated a\nverification question When did Texas secede from Mexico? which was answered with 1836 then an\ninconsistency should be detected by this step.\n3.4\nFINAL VERIFIED RESPONSE\nFinally, the improved response that takes verification into account is generated. This is executed by a\nfinal few-shot prompt where the context takes into account all of the previous reasoning steps, the\nbaseline response and verification question answer pairs, so that the corrections can take place. If the\nFactor+Revise approach is used from subsection 3.3 then the output of the cross-check inconsistency\ndetection is provided as well.\n4\nEXPERIMENTS\nWe use various experimental benchmarks to measure the efficacy of CoVe in reducing hallucination,\ncomparing against a number of baselines.\n4.1\nTASKS\nThe benchmarks we use range from list-based questions where the required answer is a set of entities,\nto where the answer is a longform generation of multiple freeform sentences.\n4.1.1\nWIKIDATA\nWe start by testing CoVe on a set of automatically generated questions using the Wikidata API1. We\ncreate list questions of the form: \u201cWho are some [Profession]s who were born in [City]?\u201d.\nFor example, \u201cWho are some politicians who were born in Boston?\u201d. The answer to these\nquestions is a set of entities, where the gold list is obtained from the Wikidata knowledge base. This\nresults in a dataset of 56 test questions, each typically containing \u223c600 known gold entities, but\ntypically an LLM will produce a much shorter list. We then use the precision metric (micro-averaged)\nto measure performance, in addition to reporting the averaged number of positive and negative entities\nproduced.\n4.1.2\nWIKI-CATEGORY LIST\nWe then proceed to a harder set-generation task. We use the QUEST (Malaviya et al., 2023) dataset\nthat was created using Wikipedia Category lists. We convert these category names to questions by\nsimply prepending a \u201cName some\u201d. Owing to the varied questions such as Name some Mexican\nanimated horror films or Name some Endemic orchids of Vietnam we believe this task can pose a\ngreater challenge. We collate all examples in the dataset that do not require logical operations to\ncreate a set of 55 test questions each having \u02dc8 answers. Similar to the Wikidata task, we measure\nprecision (micro-averaged) to measure performance, in addition to reporting the averaged number of\npositive and negative entities produced.\n4.1.3\nMULTISPANQA\nWe next test our approach on an reading comprehension benchmark, MultiSpanQA (Li et al., 2022).\nMultiSpanQA comprises of questions that have multiple independent answers (derived from a series\nof multiple discontiguous spans in the text, with questions originally from the Natural Questions\ndataset). We consider a closed-book setting, where we do not provide supporting documents, and\nhence consider a subset of questions which are factoid-based, so that our base LLM is more likely to\nbe able to answer them. We thus use a test set of 418 questions with shorter answers per span (up to\n3 tokens per item). For example, Q: Who invented the first printing press and in what\nyear?, A: Johannes Gutenberg, 1450.\n1https://query.wikidata.org/\n5\nWikidata\n(Easier)\nWiki-Category list\n(Harder)\nLLM\nMethod\nPrec. (\u2191)\nPos.\nNeg.\nPrec. (\u2191)\nPos.\nNeg.\nLlama 2 70B Chat\nZero-shot\n0.12\n0.55\n3.93\n0.05\n0.35\n6.85\nLlama 2 70B Chat\nCoT\n0.08\n0.75\n8.92\n0.03\n0.30\n11.1\nLlama 65B\nFew-shot\n0.17\n0.59\n2.95\n0.12\n0.55\n4.05\nLlama 65B\nCoVe (joint)\n0.29\n0.41\n0.98\n0.15\n0.30\n1.69\nLlama 65B\nCoVe (two-step)\n0.36\n0.38\n0.68\n0.21\n0.50\n0.52\nLlama 65B\nCoVe (factored)\n0.32\n0.38\n0.79\n0.22\n0.52\n1.52\nTable 1: Test Precision and average number of positive and negative (hallucination) entities for\nlist-based questions on the Wikidata and Wiki-Category list tasks.\n4.1.4\nLONGFORM GENERATION OF BIOGRAPHIES\nWe next validate the performance of CoVe on longform text generation. In this setting, we evaluate\nour method on generating biographies, adopting the benchmark proposed in by Min et al. (2023).\nHere the model is simply prompted to generate a biography of a selected entity using the prompt:\n\u201cTell me a bio of <entity>\u201d. We evaluate the efficacy of our approach using the FACTSCORE\nmetric (Min et al., 2023) developed in that work, which uses a retrieval-augmented language model\nto fact-check the response (Instruct-Llama, \u201cLlama + Retrieval + NP\u201d), which they showed correlates\nwell with human judgments.\n4.2\nBASELINES\nWe use Llama 65B, a strong open model as our base LLM (Touvron et al., 2023a), and use greedy\ndecoding for all models. As Llama 65B is not instruction fine-tuned, we employ few-shot examples\nparticular to each task for measuring performance on each of our benchmarks. This serves as our\nmain baseline which CoVe tries to improve upon. CoVe uses the same Llama 65B base, but includes,\nfor the same few-shot examples, demonstrations of verification questions and final verified responses,\nfollowing Figure 1 and section 3. Thus, we measure the ability to improve over the original baseline\nresponse for the same LLM. For CoVe, we compare different variants, particularly the joint and\nfactored versions on all tasks.\nWe also compare to Llama instruction fine-tuned models, for which we use Llama 2 (Touvron et al.,\n2023b). We measure both zero-shot performance on the task, or zero-shot with chain-of-thought by\nadding \u201cLet\u2019s think step by step\u201d to the zero-shot prompt. We find that the instruction fine-tuned\nmodels tend to generate extraneous content when queried. This can especially be a problem for the\nlist-based tasks. To deal with this we add an extra line to our prompt: \u201cList only the answers\nseparated by a comma\u201d. We also add another layer of post-processing to extract the answers by\nusing an off-the-shelf NER model to further avoid this issue as this helped. However, we still expect\nfew-shot to improve over this, especially for tasks like Multi-Span-QA where the answers are not all\nnamed entities, and the few-shot examples effectively show the domain of the task.\nFor the longform generation of biographies we also compare to several existing model results reported\nin Min et al. (2023), in particular InstructGPT (Ouyang et al., 2022), ChatGPT 2 and PerplexityAI 3.\n4.3\nRESULTS\nWe are interested in empirically answering the following research questions:\nRQ1 Can COVE effectively reduce the rate of hallucinatory content produced by the LLM?\nRQ2 Can COVE be used to fix or remove incorrect generations without decreasing the amount of\ncorrect content?\n2https://openai.com/blog/chatgpt\n3www.perplexity.ai\n6\nLLM\nMethod\nF1 (\u2191)\nPrec.\nRec.\nLlama 2 70B Chat\nZero-shot\n0.20\n0.13\n0.40\nLlama 2 70B Chat\nCoT\n0.17\n0.11\n0.37\nLlama 65B\nFew-shot\n0.39\n0.40\n0.38\nLlama 65B\nCoVe (joint)\n0.46\n0.50\n0.42\nLlama 65B\nCoVe (factored)\n0.48\n0.50\n0.46\nTable 2: Closed book MultiSpanQA test performance, comparing CoVe with various baselines.\nLLM\nMethod\nFACTSCORE. (\u2191)\nAvg. # facts\nInstructGPT\u2217\nZero-shot\n41.1\n26.3\nChatGPT\u2217\nZero-shot\n58.7\n34.7\nPerplexityAI\u2217\nRetrieval-based\n61.6\n40.8\nLlama 2 70B Chat\nZero-shot\n41.3\n64.9\nLlama 2 70B Chat\nCoT\n41.1\n49.0\nLlama 65B\nFew-shot\n55.9\n16.6\nLlama 65B\nCoVe (joint)\n60.8\n12.8\nLlama 65B\nCoVe (factored)\n63.7\n11.7\nLlama 65B\nCoVe (factor+revise)\n71.4\n12.3\nTable 3: Longform generation of biographies with metrics defined from Min et al. (2023). Models\nmarked with \u2217 are reported from previous work. FACTSCORE automatically computed using \u201cInstruct-\nLlama\u201d ( Retrieve \u2192 LM + NP), the best open-access model.\nvery rare\nrare\nmedium\nfreq\nvery freq\nRarity\n0.0\n0.2\n0.4\n0.6\n0.8\nFactScore\nLlama Few-shot\nLlama CoVe (joint)\nLlama CoVe (factored)\nLlama CoVe (factor+revise)\nInstructGPT\nChatGPT\nPerplexity.ai\nLlama 2 Chat Zero-shot\nLlama 2 Chat CoT\nvery rare\nrare\nmedium\nfreq\nvery freq\nRarity\n0.0\n0.2\n0.4\n0.6\n0.8\nFactScore\nFigure 2: FACTSCORE performance distribution across head, torso and tail facts for CoVe variants\nand various baselines on longform generation of biographies.\nOur main results across the four benchmark tasks are given in Table 1, Table 2 and Table 3, and our\nmain findings are as follows.\nCoVe improves precision on list-based answer tasks\nWe find that CoVe provides large gains in\nprecision on the list-based tasks, e.g. more than doubles the precision from the Llama 65B few-shot\nbaseline for the Wikidata task (from 0.17 to 0.36). We find from the positive and negative breakdown\nthat there is a large reduction in the number of hallucinated answers (negatives: 2.95 \u2192 0.68) while\nonly a relatively small reduction in the number of non-hallucinations (positives: 0.59 \u2192 0.38).\nCoVe improves performance on closed book QA\nWe also find that CoVe brings improvements in\ngeneral QA problems, as measured on MultiSpanQA. We observe a 23% improvement in F1 over the\nfew-shot baseline (0.39 \u2192 0.48), where the improvements come from gains in both precision and\nrecall.\n7\nVerification Execution\nCoVe (joint)\nCoVe (factored)\nVerification Plan\nPrec.\nPrec.\nRule-based questions\n0.13\n0.16\nGenerated by model:\nyes/no questions\n0.15\n0.19\ngeneral questions\n0.15\n0.22\nTable 4: Comparison of various CoVe verification plan strategies (rows) and verification execution\ntechniques (columns) on the Wiki-Category task.\nCoVe improves precision on longform generation\nThese results also extend to longform genera-\ntion, where we actually see larger gains than in the QA setting. FACTSCORE increases 28% (55.9 \u2192\n71.4) from the few-shot baseline, with again only a relatively small reduction in average number of\nfacts provided (16.6 \u2192 12.3). We also show the breakdown of improvements across facts in Figure 2,\nwhere one can see CoVe improves results for both rare and more frequent facts.\nInstruction-tuning and CoT do not reduce hallucinations\nWe find that the few-shot baseline that\nemploys a pre-trained Llama model outperforms Llama 2 Chat, an instruction tuned model, across all\nthe tasks. The few-shot examples lead the model to give outputs in line with those expected for the\ntask, whereas general instruction tuning produces more hallucinations or incorrect outputs. Standard\nchain-of-thought (CoT) prompting also fails to improve the results for these tasks. While CoT has\nproven to help for reasoning tasks, it seems less appropriate for the issue of hallucination we measure\nin this work.\nFactored and 2-step CoVe improve performance\nWe observe a consistent performance improve-\nment across all tasks from applying the factored CoVe approach compared to joint CoVe. For\nexample improvement from 60.8 \u2192 63.7 in FACTSCORE in longform generation. Similarly, the\n2-step approach also outperforms the joint approach, as tested on the Wikidata and Wiki-Category list\ntasks, with 2-step giving the best results for Wikidata, and factored the best for Wiki-Category. All\nthese results support our hypothesis that verifying questions should not attend to the original baseline\nresponse as they may be prone to repeating it (as the joint method can do).\nFurther explicit reasoning helps remove hallucinations\nIn the longform generation task we also\nexplore more sophisticated reasoning steps in the CoVe \u201cfactor+revise\u201d method, which explicitly\ncross-checks whether verification answers indicate an inconsistency. We see large gains in the\nFACTSCORE metric from this further explicit reasoning from 63.7 (factored) \u2192 71.4 (factor+revise).\nThis gives further indication that appropriate and explicit reasoning in LLMs can bring improvements\nin mitigating hallucinations.\nCoVe-based Llama outperforms InstructGPT, ChatGPT and PerplexityAI\nOn the longform\ngeneration task, our baseline few-shot Llama 65B is outperformed by the ChatGPT and PerplexityAI\nmodels in terms of the FACTSCORE metric. However, applying CoVe to the baseline Llama 65B\nlifts its performance above both ChatGPT and PerplexityAI, as well as outperforming InstructGPT.\nThis is particularly impressive compared to PerplexityAI considering that is a model that can support\nits facts with retrieval-augmentation, whereas CoVe uses only the base language model itself with\nimproved reasoning via deliberation (verification). However, we can see in Figure 2 PerplexityAI still\noutperforms CoVe for very rare facts where retrieval is essential, but CoVe outperforms PerplexityAI\nfor more frequent facts. We note that some models produce less overall facts than others, however\nthe FACTSCORE metric is normalized and hence comparable across models. We verified this\nexperimentally by clipping Llama 2 70B chat\u2019s output to present less facts (as it contains the largest\nnumber in its output out of all models), but this did not change its FACTSCORE substantially, e.g.\nclipping to 10 sentences increased its score from 41.3 \u2192 42.7. We note the length of the generations\nof the few-shot-based models are essentially governed by the few-shot examples, which in-turn are\nconstrained by the context length.\n8\nShortform verification questions are more accurately answered than longform queries\nIn a\nlongform response, LLMs are prone to generate a number of hallucinations. However, it can often\nbe the case that the LLM itself would know these hallucinations are wrong if queried specifically\nfor that individual fact, independent of the rest of the longform generation, see Figure 1, Figure 3,\nand section 9. This can be seen quantitatively on the Wikidata task, where only \u223c17% of the Llama\nfew-shot baseline answer entities are correct in list-based questions. However, when querying each\nindividual entity via a verification question, we find \u223c70% are correctly answered.\nLLM-based verification questions outperforms heuristics\nIn our method, CoVe, the verification\nquestions are generated by the LLM dependent on the task. We compare the quality of these questions\nto heuristically constructed ones in order to measure their quality, by replacing the LLM questions\nwith templated yes/no questions of the form \u201cDoes X answer the question\u201d for list-based questions\nwith elements X in the answer. Results on the Wiki-Category task, given in Table 4, show a reduced\nprecision with rule-based verification questions. We believe this difference would be larger for\nlongform generation where the types of required verification questions can be more diverse, and\nLLM-based verification becomes even more necesary.\nOpen verification questions outperform yes/no-based questions\nIn our main experiments we use\nverification questions where the expected answers are true facts. An alternative setup is to include\nthe fact as part of the verification question and ask it in a yes/no answer format. We evaluate this\ndifference in Table 4, and find that yes/no type questions perform worse for the factored version of\nCoVe. Some anecdotal examples are included in Appendix section 9 for ChatGPT where we find the\nmodel tends to agree with facts in a yes/no question format whether they are right or wrong.\n5\nCONCLUSION\nWe introduced Chain-of-Verification (CoVe), an approach to reduce hallucinations in a large language\nmodel by deliberating on its own responses and self-correcting them. In particular, we showed that\nmodels are able to answer verification questions with higher accuracy than when answering the\noriginal query by breaking down the verification into a set of simpler questions. Secondly, when\nanswering the set of verification questions, we showed that controlling the attention of the model\nso that it cannot attend to its previous answers (factored CoVe) helps alleviate copying the same\nhallucinations. Overall, our method provides substantial performance gains over the original language\nmodel response just by asking the same model to deliberate on (verify) its answer. An obvious\nextension to our work is to equip CoVe with tool-use, e.g., to use retrieval augmentation in the\nverification execution step which would likely bring further gains.\n6\nLIMITATIONS\nWhile our Chain-of-Verification (CoVe) method seeks to reduce hallucinations, it does not remove\nthem completely from generations. This means that CoVe can still generate incorrect or misleading\ninformation for a given query, even if it improves over the baseline. We also note that in our\nexperiments we have only addressed hallucinations in the form of directly stated factual inaccuracies.\nHowever, hallucinations could come in other forms, such as during incorrect reasoning steps, as part\nof opinions, etc. We also note that the generations CoVe produces come with verifications which,\nif viewed by the user, add more interpretability to its decisions, but come at the cost of increased\ncomputational expense due to generating more tokens in the output, similar to other reasoning\nmethods such as Chain-of-Thought.\nOur method seeks to make a large language model produce improved responses by spending more\ntime deliberating to identify its own mistakes. While we have shown this gives clear improvements,\nthe upper bound to the improvement is clearly limited by the overall capabilities of the model, e.g. in\nidentifying and knowing what it knows. In this regard, an orthogonal line of research, as discussed in\nsection 2 is the use of external tools by language models, to gain further information beyond what is\nstored in its weights. While we do not explore that avenue in this work those techniques would likely\nbe fruitful to combine with the findings here.\n9\nREFERENCES\nLeonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. Reason first, then\nrespond: Modular generation for knowledge-infused dialogue. arXiv preprint arXiv:2111.05204,\n2021.\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they\u2019re\nhallucinating references? arXiv preprint arXiv:2305.18248, 2023.\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai\u2013a tool augmented\nframework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023a.\nI-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig, et al.\nImproving factuality of abstractive summarization via contrastive reward learning. arXiv preprint\narXiv:2307.04507, 2023b.\nRoi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm vs lm: Detecting factual errors via\ncross examination. arXiv preprint arXiv:2305.13281, 2023.\nBoris A Galitsky. Truth-o-meter: Collaborating with llm in fighting its hallucinations. 2023.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,\nVincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what\nlanguage models say, using language models. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 16477\u201316508, 2023.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys, 55(12):1\u201338, 2023.\nWeisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T Kwok.\nBackward reasoning in large language models for verification. arXiv preprint arXiv:2308.07758,\n2023a.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig.\nActive retrieval augmented generation.\narXiv preprint\narXiv:2305.06983, 2023b.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly)\nknow what they know. arXiv preprint arXiv:2207.05221, 2022.\nJack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar.\nLearning to reason and memorize with self-notes. arXiv preprint arXiv:2305.00833, 2023.\nHaonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. Multispanqa: A dataset for\nmulti-span question answering. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n1250\u20131260, 2022.\nKenneth Li, Oam Patel, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\nintervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341,\n2023.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.\nDeductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n10\nChaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Quest:\nA retrieval dataset of entity-seeking queries with implicit set operations.\narXiv preprint\narXiv:2305.11694, 2023.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box\nhallucination detection for generative large language models. arXiv preprint arXiv:2303.08896,\n2023.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own\nstep-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023.\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents\u2019\noverconfidence through linguistic calibration. Transactions of the Association for Computational\nLinguistics, 10:857\u2013872, 2022.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars\nLiden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language\nmodels with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\nFabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\n2019.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,\n2022.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAnsh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez,\nEsin Durmus, Evan Hubinger, Jackson Kernion, Kamil\u02d9e Luko\u02c7si\u00afut\u02d9e, et al. Question decomposition\nimproves the faithfulness of model-generated reasoning. arXiv preprint arXiv:2307.11768, 2023.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das,\nSlav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural\nlanguage generation models. Computational Linguistics, pp. 1\u201366, 2023.\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, L\u00b4eonard Hussenot, Orgad Keller, et al. Factually consistent summarization\nvia reinforcement learning with textual entailment feedback. arXiv preprint arXiv:2306.00186,\n2023.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle\nOtt, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint\narXiv:2004.13637, 2020.\n11\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\nreduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledge-\nable are large language models (llm)? aka will llms replace knowledge graphs? arXiv preprint\narXiv:2308.10168, 2023a.\nWeiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. Con-\ntrastive learning reduces hallucination in conversations. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pp. 13618\u201313626, 2023b.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves\nnine: Detecting and mitigating hallucinations of llms by validating low-confidence generation.\narXiv preprint arXiv:2307.03987, 2023.\nChaojun Wang and Rico Sennrich. On exposure bias, hallucination and domain shift in neural\nmachine translation. arXiv preprint arXiv:2005.03642, 2020.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners\nwith self-verification. arXiv preprint arXiv:2212.09561, 2022.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith,\nMari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for\nlanguage model training. arXiv preprint arXiv:2306.01693, 2023.\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language\nmodels via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model\nhallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework. arXiv preprint arXiv:2305.03268, 2023.\n12\n7\nCOVE - FURTHER DETAILS\nQuery\n       1. Baseline Response\n       4. Final Verified Response\n        2. Plan Verifications\nThe Mexican\u2013American War was an armed conflict between the United States and Mexico from 1846 \nto 1848. It followed in the wake of the 1845 U.S. annexation of Texas, which Mexico considered part of \nits territory in spite of its de facto secession in the 1835 Texas Revolution.\nWhat was the primary cause of the Mexican-American war? \n  - When did the Mexican American war start and end?  \n  - When did the US annex Texas?\n  - When did Texas secede from Mexico?  \n  \u2026 <questions continue..>\nThe Mexican\u2013American War was an armed conflict between the United States and Mexico from 1846 \nto 1848. It followed in the wake of the 1845 U.S. annexation of Texas.\n1846-1848\n1845\n\u2026\n1836\n        3. Execute Verifications\n\u2705\n\u274c\n\u2705\nFigure 3: For longform generation, the Chain-of-Verification (CoVe) Factor + Revise method is the\nmost effective in our longform generation experiments. CoVe Factor + Revise has the model indepen-\ndently identify (cross-check) which facts are consistent with its executed verifications (indicated by\ntickmark and crosses in the figure). With this extra step we aim to disregard the inconsistent facts and\nuse the consistent facts to regenerate the response.\n13\n8\nPROMPT TEMPLATES\nWe provide prompt templates for the longform generation of biographies task below for the different\nsteps and variants of CoVe (see section 3). Templates for the other tasks are similar, but using\nfew-shot examples from those tasks instead.\n8.1\nGENERATE BASELINE RESPONSE\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA: <bio of person>\nQ: Tell me a bio of <person>\nA:\nTable 5: Few-shot prompting with 3 few-shot examples for the longform generation of biographies\ntask. Other tasks use the same standard few-shot setup as well (with 3 examples from that particular\ntask).\n8.2\nPLAN VERIFICATIONS\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\n<fact in passage>, Verification Question\n<fact in passage>, Verification Question\nContext: Q: Tell me a bio of <person>.\nA: <passage about person>\nResponse:\nTable 6: Step (2) of CoVe involves planning the verification questions. In the biography task case we\nsplit the longform generation into its individual passages (e.g. sentences in the biography case, this\nwas done due to excessive context length, which we don\u2019t need to do for the other tasks). The model\nthen generates a verification question for each fact it observes in each passage (a passage may have\nmultiple facts).\n14\n8.3\nEXECUTE VERIFICATIONS\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA: Answer\nQ: Verification Question\nA:\nTable 7: In step (3) of CoVe, the model then generates an answer for each of the verification questions.\nAgain we use 3 few-shot examples.\n8.4\nGENERATE FINAL VERIFIED RESPONSE\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original Passage>.\nFrom another source,\n<output of execute verification step: Q + A>\n<output of execute verification step: Q + A>\nResponse: <revised and consistent Passage>\nContext: <Original passage>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse:\nTable 8: In step (4) of CoVe (factored) the model is then presented with its original generation (split\ninto passages, e.g. sentences, in the biography case, due to excessive context length which we do not\nneed to do for the other tasks) along with its own verification step results. The model is told that this\ninformation comes from \u201canother source\u201d. The model is required to synthesize a new final answer\nbased on facts that are consistent between the two sources.\n15\n8.5\nFACTOR+REVISE: IDENTIFY WHICH FACTS ARE CONSISTENT\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: CONSISTENT. <Consistent fact>\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: INCONSISTENT.\nContext: <Original Fact>.\nFrom another source,\n<output of execute verification step: Q + A>\nResponse: PARTIALLY CONSISTENT. <Consistent part>\nTable 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is\nmade to explicitly identify which facts are consistent between the two sources. The consistent facts\ncan then be spliced together.\n16\n9\nCHATGPT EXAMPLE SCREENSHOTS\nFigure 4:\nChatGPT generates several hallucinations for this question, e.g. Hillary Clinton and\nMichael Bloomberg.\nFigure 5:\nEven when the longform answer is provided for a rewritten query (see query from\nFigure 4), while giving a slightly different answer, ChatGPT still generates several hallucinations for\nthis question, e.g. Hillary Clinton and Michael Bloomberg.\n17\nFigure 6: Shortform questions (which could be verification questions) appear to be answered more\nfactually than the longform answers in Figure 4 and Figure 5.\nFigure 7: Another example of hallucinations for a different query, e.g., John F. Kennedy Jr was born\nin Washington D.C.\n18\nFigure 8: Examples where questions asking for a fact are answered correctly, but verifying via a\nyes/no question is incorrect (the model tends to agree with the way the question is stated, even if it\nwas stated incorrectly).\n19\n"
  },
  {
    "title": "A Large-scale Dataset for Audio-Language Representation Learning",
    "link": "https://arxiv.org/pdf/2309.11500.pdf",
    "upvote": "9",
    "text": "A LARGE-SCALE DATASET FOR AUDIO-LANGUAGE REPRESENTATION LEARNING\nLuoyi Sun1\nXuenan Xu2\nMengyue Wu2,\u2020\nWeidi Xie1,3,\u2020\n1CMIC & 2X-LANCE, Shanghai Jiao Tong University\n3Shanghai AI Laboratory\nABSTRACT\nThe AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal\ndatasets.\nHowever, in the audio representation learning commu-\nnity, the present audio-language datasets suffer from limitations\nsuch as insufficient volume, simplistic content, and arduous col-\nlection procedures. To tackle these challenges, we present an in-\nnovative and automatic audio caption generation pipeline based\non a series of public tools or APIs, and construct a large-scale,\nhigh-quality, audio-language dataset, named as Auto-ACD, com-\nprising over 1.9M audio-text pairs. To demonstrate the effectiveness\nof the proposed dataset, we train popular models on our dataset\nand show performance improvement on various downstream tasks,\nnamely, audio-language retrieval, audio captioning, environment\nclassification. In addition, we establish a novel test set and provide\na benchmark for audio-text tasks.\nThe proposed dataset will be\nreleased at https://auto-acd.github.io/.\nIndex Terms\u2014 audio-text dataset, contrastive learning, auto-\nmated audio captioning\n1. INTRODUCTION\nIn the recent literature, foundation models, like CLIP [1], variants\nof GPT [2], DALL-E 2 [3] and Stable Diffusion [4], have shown\ntremendous success.\nDespite being different in architectural or\nalgorithmic designs, they are fundamentally lying on a common\nbasis: large-scale multimodal datasets, for example, MMC4 [5],\nLAION [6], indicating an emerging transition from a model-centric\nto data-centric representation learning. The former considers push-\ning the boundaries of model design within the constraints of a\npredetermined data budget, while the latter focuses on curating\nlarge-scale and high-quality datasets in a scalable manner.\nIn the audio community, there have been recent endeavours fo-\ncused on constructing audio-language datasets, as demonstrated in\nTable. 1. However, existing datasets potentially suffer from two lim-\nitations, for instance, Clotho [7] and AudioCaps [8], which contain\naudios typically comprising 1 to 3 sound events, accompanied by\nhigh-quality text descriptions provided by human annotators. They\nare clearly challenging to scale up. On the other hand, LAION-\nAudio-630K [9] and WavCaps [10] source large amounts of raw\ndata from online foley websites, then employ sentence templates or\nkeyword-to-caption models to convert the original audio labels into\nfree-form sentences. However, it is questionable whether the result-\ning language descriptions indeed offer any additional information\nbeyond simple prompts or sound tags.\nThis paper presents our recent efforts for constructing a large-\nscale, high-quality, audio-language dataset, with minimal manual\nefforts, termed as Auto-ACD. Specifically, building on the prior of\nrobust audio-visual correspondence in existing video datasets, for\nexample, VGGSound [11], AudioSet [12]. A comprehensive visual\n\u2020 Corresponding author.\nDataset\nQuantity\nLength\n# Vocab.\nEnv.\nAuto.\nAudioCaps [8]\n57K\n8.8\n5K\n\u00d7\n\u00d7\nClotho [7]\n30K\n11.3\n4K\n\u00d7\n\u00d7\nLAION-Audio-630K [9]\n630K\n7.3\n311K\n\u00d7\n\u2713\nWavCaps [10]\n400K\n7.8\n29K\n\u00d7\n\u2713\nAuto-ACD (ours)\n1.9M\n18.1\n23K\n\u2713\n\u2713\nTable 1. Comparation with other audio caption datasets. \u201cLength\u201d\nand \u201c# Vocab.\u201d refer to average length and vocabulary. \u201cEnv.\u201d and\n\u201cAuto.\u201d refer to environmental information and automatic pipeline,\nrespectively.\nscene understanding is thus expected to serve as valuable informa-\ntion source for generating high-quality audio descriptions. In fact,\nAudioCaps were exactly constructed by providing annotators both\nword labels (by AudioSet) and the accompanied videos. Particu-\nlarly, we employ a range of publicly available tools or APIs across\nthe general AI community, e.g., vision, language and audio models,\nto generate comprehensive language descriptions for the audio tracks\nof the given video datasets. As a result, these language descriptions\nnot only depict the type of sound and its source, but also describe the\nauditory attributes and the specific location of its occurrence. Due\nto the limited information in audio tags, these pieces of information\nare infrequently present within the existing datasets.\nTo assess the efficacy of Auto-ACD, we conduct experiments\nfrom four aspects: First, we launch a joint audio-language repre-\nsentation learning using InfoNCE loss [13, 14], and evaluate the\nmodel via a retrieval task between audio and language, showing\nnoticeable improvement over existing datasets; Second, we conduct\nzero-shot environment classification experiments both before and af-\nter fine-tuning, thus substantiating the accurate environmental infor-\nmation within our dataset; Third, we benchmark on audio-language\ngeneration task, namely, automatic audio captioning, by training a\nlightweight mapping network between the pre-trained audio back-\nbone and GPT2, showing superior performance on the widely used\nbenchmark, e.g., Clotho; Fourth, we manually filter a test set and\nintroduce a novel benchmark for audio-text tasks. This benchmark\nassesses the model to grasp information beyond mere audio tags, for\nexample, the environment and fine-grained categories of sound, we\nset a baseline for future research in this direction.\n2. DATASET CONSTRUCTION\nTo create a large-scale, audio dataset with rich language description,\nwe base on the assumption that visual scene understanding serves as\na strong prior. For instance, synchronized videos frequently show-\ncase auditory cues, and visual information serves as a precise repre-\nsentation of the acoustic environment in which the sound happens.\nIn an audio caption, it is desirable to incorporate sound at-\ntributes, location, and fine-grained labels. To achieve this, we can\nleverage publicly available tools or APIs to gather the necessary in-\nformation for audio description and mutually verify the results. For\narXiv:2309.11500v3  [cs.SD]  3 Oct 2023\nBLIP-2\nGroundingDINO\nPlace365\nCLIP\nA train horn blows as a train \npasses by, creating a loud \nand distinct sound in a \nrailway station.\nvisual-audio label\naudio\nframe \nvideo\nPANNs\nAudioCaption\na train pulling into a station.\ntrain [\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f: 0.907], rail transport [\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f: 0.858], \nrailroad car, train wagon [\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f: 0.833]\na train horn blows.\ntrain_station [\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f:0.657]\npassenger_car\ntrain [\ud835\udc65!:0.5921, \ud835\udc66!:0.5947, \ud835\udc64:0.7879, \u210e:0.3298]\ntrain horning\nprompt\nFig. 1. Automatic pipeline for Auto-ACD collection.\ninstance, we can employ an object detection model to identify po-\ntential sources of sound, and an environmental classification model\nto extract scene categories. By extracting a wealth of information,\nwe ensure the maximum coverage of accurate details, providing the\nlanguage model with ample references.\n2.1. Tools or APIs\nGiven one sample from existing large-scale video datasets, for exam-\nple, AudioSet, VGGSound [11, 12], i.e., denoted as V = {f; a; y},\nwhere f, a and y correspond to frame sequence, audio stream, and\nvisual or audio labels, respectively. Our goal is to adopt a range of\npublicly available tools or APIs across the general AI community,\ni.e., using off-the-shelf vision, language and audio models to con-\nstruct language descriptions for audios, as shown in Fig. 1. In this\nsection, we describe these tools in details.\nImage Captioning.\nWe employ the off-the-shelf BLIP-2 [15]\nmodel, which obtains competitive results for image captioning. This\ntool has the ability to generate captions for the entire image, and\ndepict the primary subject or environment, in our case, we feed in\nthe video\u2019s middle frame.\nObject Detection.\nWe use the pre-trained Grounding DINO\nmodel [16], to identify objects within the middle frame, and preserve\nall the detected entities along with their corresponding prediction\nconfidence scorces.\nImage Labeling. We adopt the pre-trained OpenAI CLIP model\nfor image classification. Here, we utilize the prompt: \u201ca photo of\na {label}\u201d to generate textual embedding, leveraging the category\nontology from ImageNet [17].\nPlace Recognition. We employ the pre-trained PlaceCNN [18], to\ninfer the environment context captured in videos. We retain the out-\nput results only when the probability exceeds 0.15. Given the robust\ncorrespondence between audio and visual signals, the environment\ndepicted in the video is highly likely to represent the acoustic ambi-\nence in which the sound occurs.\nAudio Tagging. We use the pre-trained PANNs [19] to predict the\ntags of sounds within the audio, and preserve the top three predic-\ntions with their confidence scores.\nAudio Captioning. We use the existing AudioCaption [20] model,\nto generate concise and brief captions. This caption resembles the\nstyle of AudioCaps, with category information audio events.\nExisting Audio-Visual Labels. In addition to the predictions from\nthe model, we also adopt the provided labels of existing datasets, for\ninstance, VGGSound [11] gives a single label for each video, while\nAudioSet [12] provides multiple labels.\nSummary. As for the language model, we use the OpenAI ChatGPT\nAPI1, to assemble the abovementioned descriptions or labels into\ncomprehensive descriptions for audio.\n2.2. Caption Generation\nBased on the visual and acoustic clues in video, we create a struc-\ntured language paragraph, and use it to prompt ChatGPT to generate\ndescriptions for audio. In detail, we incorporate the aforementioned\nclues into the prompt, accompanied by their corresponding confi-\ndence score, we also offer three sentence examples from AudioCaps\nor Clotho as instruction. While generating captions, we explicitly\nask ChatGPT to remove information that is inaudible, i.e., illogical\nand visually oriented elements, for example, colours. As a result,\nthe large language model is able to analyze the scenario from all\nprovided clues, and generate language description for audio, with\nsound category, and environment. The generated caption results are\nshown in Table. 2.\nNo. Generated Caption\n1.\nLoud pops and bangs resonate as timbales are being played, creating\nrhythmic music in a room.\n2.\nWater gurgles and bubbles as a boat glides through, creating a sooth-\ning and peaceful underwater ambience.\n3.\nA woman speaks softly amidst the soothing sound of birds chirping,\ncreating a serene atmosphere in a garden.\n4.\nA motorcycle engine idles before revving up, creating a loud sound\nin an urban environment.\nTable 2. The results of generated captions in Auto-ACD, with accu-\nrate content and ample surrounding information. Green and Yellow\nrefer to \u201cwhere\u201d and \u201chow\u201d the audio sounds like.\n2.3. Dataset Statistics\nAs depicted in Table. 1, we collect 1.9 million audio-language pairs\nfrom AudioSet and VGGSound in total. Auto-ACD stands out as\nthe largest audio-language dataset to date, with train, validation\nand manually filtered test sets. The text descriptions in Auto-ACD\ncontain long texts (18 words) and diverse vocabularies (23K), and\n1https://openai.com/chatgpt\nFrogs croaking\nand with insects\nvocalizing.\n\u22ef\nMapping \nNetwork\n\u22ef\nGPT\nprefix embeddings  \ud835\udcab!\ncaption tokens \nA train running followed by a train horn.\nFrogs croaking\nand with insects\nvocalizing.\nText\nEncoder\nAudio\nEncoder\nFrogs croaking\nand with insects\nvocalizing.\n\ud835\udc52!\n\"\n\ud835\udc52!\n#\n\ud835\udc52!\n$\n\u22ee\n\ud835\udc52!\n%\n\ud835\udc52&\n\"\n\ud835\udc52&\n#\n\ud835\udc52&\n$\n\ud835\udc52&\n%\n\u22ef\n\ud835\udc52\"# $ \ud835\udc52%#\n\ud835\udc52\"& $ \ud835\udc52%#\n\ud835\udc52\"' $ \ud835\udc52%#\n\ud835\udc52\"( $ \ud835\udc52%#\n\ud835\udc52\"# $ \ud835\udc52%&\n\ud835\udc52\"# $ \ud835\udc52%'\n\ud835\udc52\"# $ \ud835\udc52%(\n\u22ef\n\u22ee\n\u22ee\n\u22ee\n\u22ee\n\u22ef\n\u22ef\n\u22ef\n\u22f1\n\ud835\udc52\"& $ \ud835\udc52%&\n\ud835\udc52\"& $ \ud835\udc52%'\n\ud835\udc52\"& $ \ud835\udc52%(\n\ud835\udc52\"' $ \ud835\udc52%&\n\ud835\udc52\"' $ \ud835\udc52%'\n\ud835\udc52\"' $ \ud835\udc52%(\n\ud835\udc52\"( $ \ud835\udc52%&\n\ud835\udc52\"( $ \ud835\udc52%'\n\ud835\udc52\"( $ \ud835\udc52%(\n\u22ef\n\u22ee\nAudio\nEncoder\naudio feature  \ud835\udc52\"\n!\nFig. 2. Audio-text retrieval model and automatic audio captioning model frameworks.\nprovide information about the surrounding auditory environment in\nwhich sounds take place.\n3. ARCHITECTURE\nAudio-Language Constrastive Pre-training. To validate the effi-\ncacy of our proposed dataset, we train an audio-language model with\nstandard contrastive learning, e.g., infoNCE [21] loss, as shown in\nFig.2. Specifically, we employ the pre-trained HTSAT [22] as the\naudio encoder, and the pre-trained RoBERTa [23] as the language\nencoder. Both encoders were initialised from the pre-trained CLAP\nmodel [9], and further finetuned on our dataset. We term our final\nmodel as Audio-Text Retrieval (ATR).\nGiven an audio-text pair (ai, ti), we utilise audio encoder Aenc\nand text encoder Tenc to extract audio embedding ei\na = Aenc(ai)\nand text embedding ei\nt = Tenc(ti), respectively. The model is then\ntrained with contrastive loss, wherein the paired audio and language\nembeddings are treated as positive, and unpaired ones as negative,\nwith the following loss function:\nL =\n1\n2N\nN\nX\ni=1\n(log\nexp\n\u0000ei\na \u00b7 ei\nt/\u03c4\n\u0001\nPN\nj=1 exp\n\u0000eia \u00b7 ej\nt/\u03c4\n\u0001+log\nexp\n\u0000ei\nt \u00b7 ei\na/\u03c4\n\u0001\nPN\nj=1 exp\n\u0000ei\nt \u00b7 ej\na/\u03c4\n\u0001)\nwhere \u03c4 represents the learnable temperature parameters.\nAudio Captioning. To demonstrate the effectiveness of our pre-\ntrained audio backbone, we also use audio captioning for evaluation.\nInspired by ClipCap [24] and AutoAD [25], we adopt a lightweight\naudio captioning model, where both the audio backbone and lan-\nguage model (GPT-2) are fixed, and only a mapping network is\ntrained, as shown in Fig. 2.\nGiven an audio-text pair (ai, ci), we use the pre-trained audio\nencoder to extract audio features ei\na = Aenc(ai), and we convert\nthe caption into a token sequence, ci\n1, . . . , ci\nk, where k indicates the\nmaximal length of text. Then, we design a mapping network Fmap to\ntransform the extracted embedding into a set of prefix embeddings,\ni.e., Pi = fmap(ei\na).\nLike ClipCap and AutoAD, we take the prefix embedding set as\nthe condition for predicting the next token in an auto-regressive lan-\nguage model. Therefore, during training, we minimize the negative\nlog-likelihood of predicting the correct word:\nL = \u2212\nN\nX\ni=1\n\u2113\nX\nj=1\nlog p\u03b8\n\u0010\nci\nj | Pi, ci\n1, . . . , ci\nj\u22121\n\u0011\nwhere \u03b8 represents the trainable parameters.\n4. EXPERIMENTS\nIn this section, we evaluate on three tasks, namely, audio-language\nretrieval, audio captioning, and sound environment classification.\n4.1. Audio-language Retrieval\nDataset. We conduct audio-text retrieval experiments on Audio-\nCaps, Clotho, Auto-ACDVS, and Auto-ACD datasets. AudioCaps,\nClotho and Auto-ACD consist of 50K/495/975, 3.8K/1045/1045 and\n1.9M/2K/1K data pairs in the train, validation, and test sets, respec-\ntively. Auto-ACDVS, a subset of Auto-ACD, contains 190K data\npairs exclusively generated from VGGSound. In the case of Clotho,\nvalidation and test set in AudioCaps, each data pair consists of one\naudio sample and five corresponding captions, while the remaining\ndata pairs only comprise one audio-caption pair. It is worth men-\ntioning that we manually filter and revise the Auto-ACD test set to\nensure the accuracy of the included information.\nAuto-ACD Benchmark. In addition to the Auto-ACD training set,\nwe also randomly selected 2K data samples as the validation set\nand 1K samples as the test set. We manually verified the test set,\nby removing incorrect information from the language descriptions\nand rewriting inappropriate vocabulary expressions. This test set is\nused for evaluating both audio-language retrieval and automatic au-\ndio captioning tasks.\nMetrics. In order to validate the rich and accurate information of\nour dataset, we compare the Recall@k performance on commonly\nused datasets, for example, AudioCaps and Clotho.\nTraining Details. We train our proposed ATR for 20 epochs with\na batch size of 768, using Adam optimizer with a warm-up, and an\ninitial learning rate of 1e-4 with a cosine learning rate decay. We use\nthe same hyperparameters with the existing CLAP model. Addition-\nally, we introduce 25% random masking on words in the text and\nrandomly apply augmentations such as Noise and Gain to 50% of\naudio samples. We further fine-tune the model on specific datasets\nwith an initial learning rate of 2e-5 for 15 epochs.\nResults. As shown in Table.3, we can draw the following obser-\nvations: (i) training on our proposed Auto-ACDVS dataset leads to\na significant improvement in Recall@k. (ii) training on Auto-ACD\nleads to a remarkable performance gain. This improvement is par-\nticularly evident when evaluating the model on the test set of Au-\ndioCaps, as AudioCaps is a subset of AudioSet and shares a similar\ndata distribution with Auto-ACD. This fine-tuning process allows\nTrain Set\nAudioCaps Eval.\nClotho Eval.\nAuto-ACD Eval.\nAudio\u2192Text\nText\u2192Audio\nAudio\u2192Text\nText\u2192Audio\nAudio\u2192Text\nText\u2192Audio\nR@1\nR@10\nR@1\nR@10\nR@1\nR@10\nR@1\nR@10\nR@1\nR@10\nR@1\nR@10\nbasic + LA\n45.0\n88.0\n36.2\n82.5\n24.2\n66.9\n17.2\n55.4\n20.0\n65.0\n17.9\n59.7\nbasic + ACDVS\n50.5\n90.6\n39.8\n86.9\n24.2\n62.9\n20.0\n58.9\n39.2\n86.2\n39.6\n85.7\nbasic + ACD\n53.1\n91.1\n39.9\n85.9\n18.5\n56.7\n14.1\n48.3\n48.1\n92.1\n51.1\n92.1\nbasic + ACD * FT\n56.2\n93.7\n44.4\n89.0\n29.1\n68.2\n24.1\n64.8\n-\n-\n-\n-\nTable 3. The audio-text retrieval results on AudioCaps, Clotho and ACD test sets. \u201cbasic\u201d, \u201cLA\u201d and \u201cACD\u201d refer to the combination of\nAudioCaps and Clotho, LAION-Audio-630K and Auto-ACD, respectively. \u201cACDVS\u201d is a subset of Auto-ACD, curated from VGGSound. \u201c\n* FT\u201d refers to fine-tuning the model on the target dataset.\nthe model to acquire more comprehensive understanding of both au-\ndio and text information, thus better retrieval performance. (iii) on\nthe Auto-ACD benchmark, which contains more diverse lexicon and\nabundant language description, training on Auto-ACD datasets sig-\nnificantly outperforms the model trained on Laion-Audio-630K.\n4.2. Automatic Audio Captioning\nDataset. In addition to the datasets mentioned in Section 4.1, we\nalso use the MACS dataset [26], that consists of 3.9K audio-text data\npairs, with each audio containing two to five captions and several\naudio tags. In total, we train the audio captioning model using a total\nof 58k data pairs from Clotho, AudioCaps and MACS, and evaluate\nthe model on Clotho and Auto-ACD test set.\nMetrics. In addition to conventional captioning metrics, for exam-\nple, Meteor [27], RougeL [28], Spider [29], we incorporate Sen-\ntenceBERT [30] as additional evaluation metrics, that not solely rely\non lexical alignment, but rather prioritize the semantic resemblance\nand accuracy of the captions\u2019 content.\nTraining Details.\nWe devise two mapping networks, MLP and\ntransformer, and selectively fine-tune the parameters of GPT during\nthe training process. We set the number of prefixes to be 8, each\nwith a dimension of 512. We train this audio captioning model on\nthe MACS [26], Clotho and AudioCaps for 15 epochs with a batch\nsize of 128 and an initial learning rate of 5e-4. In this task, we\ncompare the audio encoder from our ATR model and the pre-trained\nCLAP [9], by only training the mapping network of both models on\nthe benchmark datasets, namely, Clotho, and Auto-ACD.\nResults. As shown in Table. 4, we can draw two observations: (i)\nThe automatic audio captioning model, with the audio encoder ini-\ntialised from our pre-trained ATR model, shows improved perfor-\nmance across all evaluation metrics than baseline. (ii) There is a\nmore pronounced outcome when evaluated on Auto-ACD: the base-\nline approach\u2019s performance oversees a sharp decrease on the test set\nof Auto-ACD. We conjecture this is because the baseline features ex-\ntracted from CLAP model lack detailed descriptions of environmen-\ntal information. While captioning model based on our ATR model\nshows a significant improvement in performance, and is able to pre-\ncisely infer where the sound occurs. This observation signifies that\nAuto-ACD showcases an extensive lexicon, enabling the portrayal of\na given audio using various sentence structures. On the other side, it\nillustrates that models trained on our dataset will deduce the context\nin which the sound emanates.\n4.3. Environment Classification\nOne of the distinguishing aspects of Auto-ACD is its environmental\ninformation within the text descriptions. After training on Auto-\nEval Set\nAudio Encoder\nMeteor\nRougeL\nSpider\nS-BERT\nEnv.\nClotho\nCLAP\n15.5\n34.9\n20.6\n46.0\n\u00d7\nOurs\n16.8\n36.5\n21.5\n47.3\n\u00d7\nAuto-ACD\nCLAP\n9.9\n23.0\n19.6\n8.7\n\u00d7\nOurs\n20.8\n37.2\n55.0\n9.4\n\u2713\nTable 4. The automatic audio captioning results on Clotho and Auto-\nACD test sets. \u201cS-BERT\u201d refers to SentenceBERT, \u201cEnv.\u201d refers to\nthe environmental information in predicted captions.\nACD, we conducted environment classification on two scenarios,\none is on urban acoustic scene dataset [31], that has been used in\nthe DCASE 2020 challenge, denoted as DCASE 2020 Mobile, the\nother one is a set of samples from the AudioSet evaluation set that\nare labelled with child classes of \u201cAcoustic environment\u201d in the Au-\ndioSet ontology, denoted as AudioSet Env. To avoid data leakage,\nhere we only employ the model pre-trained on Auto-ACDVS for this\nexperiment. As depicted in Table. 5, the results indicate that our\naudio-language model demonstrates a stronger recognition ability of\nenvironments over CLAP.\nModel\nDCASE 2020 Mobile\nAudioSet Env\nCLAP\n32.2\n19.5\nOurs\n36.5\n39.5*\nTable 5. Zero-Shot Acoustic Environment Classification. \u201c*\u201d refers\nto pre-training model on Auto-ACDVS.\n5. CONCLUSION\nIn this paper, we present an automated pipeline for audio caption\ngeneration, accompanied by a large-scale and comprehensive audio\ncaptioning dataset with 1.9M data pairs. Furthermore, we assess the\nperformance of previous models on our dataset to authenticate the\nefficacy, and provide a manually checked test and a benchmark for\naudio-text tasks. These experimental findings unveil the wealth of\ninformation inherent in our data, facilitating the models to learn a\nmore robust audio representation.\n6. REFERENCES\n[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al.,\n\u201cLearning transferable visual models from natural language su-\npervision,\u201d in Proc. ICML, 2021, pp. 8748\u20138763.\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., \u201cLanguage models are unsupervised multitask learners,\u201d\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[3] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHi-\nerarchical text-conditional image generation with clip latents,\u201d\narXiv preprint arXiv:2204.06125, 2022.\n[4] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Om-\nmer, \u201cHigh-resolution image synthesis with latent diffusion\nmodels,\u201d in Proc. IEEE/CVF CVPR, 2022, pp. 10 684\u201310 695.\n[5] W. Zhu, J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang,\nY. Yu, L. Schmidt, W. Y. Wang, and Y. Choi, \u201cMultimodal C4:\nAn open, billion-scale corpus of images interleaved with text,\u201d\narXiv preprint arXiv:2304.06939, 2023.\n[6] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wight-\nman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Worts-\nman et al., \u201cLaion-5b: An open large-scale dataset for training\nnext generation image-text models,\u201d in Proc. NIPS, vol. 35,\n2022, pp. 25 278\u201325 294.\n[7] K. Drossos, S. Lipping, and T. Virtanen, \u201cClotho: An audio\ncaptioning dataset,\u201d in Proc. IEEE ICASSP.\nIEEE, 2020, pp.\n736\u2013740.\n[8] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudiocaps: Gen-\nerating captions for audios in the wild,\u201d in NAACL, 2019, pp.\n119\u2013132.\n[9] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and\nS. Dubnov, \u201cLarge-scale contrastive language-audio pretrain-\ning with feature fusion and keyword-to-caption augmentation,\u201d\nin Proc. IEEE ICASSP, 2023, pp. 1\u20135.\n[10] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D.\nPlumbley, Y. Zou, and W. Wang, \u201cWavcaps: A chatgpt-assisted\nweakly-labelled audio captioning dataset for audio-language\nmultimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023.\n[11] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, \u201cVggsound: A\nlarge-scale audio-visual dataset,\u201d in Proc. IEEE ICASSP, 2020,\npp. 721\u2013725.\n[12] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio\nset: An ontology and human-labeled dataset for audio events,\u201d\nin Proc. IEEE ICASSP, 2017, pp. 776\u2013780.\n[13] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation\nlearning with contrastive predictive coding,\u201d arXiv preprint\narXiv:1807.03748, 2018.\n[14] T. Han, W. Xie, and A. Zisserman, \u201cVideo representation learn-\ning by dense predictive coding,\u201d in Proc. ICCVW, 2019, pp.\n1\u201313.\n[15] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBLIP-2: bootstrapping\nlanguage-image pre-training with frozen image encoders and\nlarge language models,\u201d in ICML, 2023, pp. 1\u201313.\n[16] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li,\nJ. Yang, H. Su, J. Zhu et al., \u201cGrounding dino: Marrying\ndino with grounded pre-training for open-set object detection,\u201d\narXiv preprint arXiv:2303.05499, 2023.\n[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei, \u201cImagenet: A large-scale hierarchical image database,\u201d in\nProc. IEEE/CVF CVPR, 2009, pp. 248\u2013255.\n[18] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba,\n\u201cPlaces: A 10 million image database for scene recognition,\u201d\nIEEE TPAMI, vol. 40, no. 6, pp. 1452\u20131464, 2017.\n[19] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D.\nPlumbley, \u201cPanns: Large-scale pretrained audio neural net-\nworks for audio pattern recognition,\u201d IEEE/ACM TASLP,\nvol. 28, pp. 2880\u20132894, 2020.\n[20] X. Xu, Z. Zhang, Z. Zhou, P. Zhang, Z. Xie, M. Wu, and\nK. Q. Zhu, \u201cBlat: Bootstrapping language-audio pre-training\nbased on audioset tag-guided synthetic data,\u201d arXiv preprint\narXiv:2303.07902, 2023.\n[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al.,\n\u201cLearning transferable visual models from natural language su-\npervision,\u201d in Proc. ICML.\nPMLR, 2021, pp. 8748\u20138763.\n[22] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and\nS. Dubnov, \u201cHts-at: A hierarchical token-semantic audio trans-\nformer for sound classification and detection,\u201d in Proc. IEEE\nICASSP, 2022, pp. 646\u2013650.\n[23] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A ro-\nbustly optimized bert pretraining approach,\u201d arXiv preprint\narXiv:1907.11692, 2019.\n[24] R. Mokady, A. Hertz, and A. H. Bermano, \u201cClipcap: Clip pre-\nfix for image captioning,\u201d arXiv preprint arXiv:2111.09734,\n2021.\n[25] T. Han, M. Bain, A. Nagrani, G. Varol, W. Xie, and A. Zis-\nserman, \u201cAutoad:\nMovie description in context,\u201d in Proc.\nIEEE/CVF CVPR, 2023, pp. 18 930\u201318 940.\n[26] I. Martin Morato and A. Mesaros, \u201cDiversity and bias in audio\ncaptioning datasets,\u201d in Proc. DCASE, 2021, pp. 90\u201394.\n[27] S. Banerjee and A. Lavie, \u201cMeteor: An automatic metric for mt\nevaluation with improved correlation with human judgments,\u201d\nin ACL Workshop on MT, 2005, pp. 65\u201372.\n[28] C.-Y. Lin and E. Hovy, \u201cAutomatic evaluation of summaries\nusing n-gram co-occurrence statistics,\u201d in NAACL, 2003, pp.\n150\u2013157.\n[29] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, \u201cIm-\nproved image captioning via policy gradient optimization of\nspider,\u201d in Proc. IEEE/CVF CVPR, 2017, pp. 873\u2013881.\n[30] N. Reimers and I. Gurevych, \u201cSentence-bert: Sentence embed-\ndings using siamese bert-networks,\u201d in Proc. EMNLP, 2019,\npp. 3982\u20133992.\n[31] T. Heittola, A. Mesaros, and T. Virtanen, \u201cTau urban acoustic\nscenes 2020 mobile, development dataset,\u201d 2020. [Online].\nAvailable: https://doi.org/10.5281/zenodo.3670167\n7. APPENDIX\n7.1. Dataset Statistics\nAs illustrated in Fig. 3 and Table. 1, Auto-ACD surpasses the other\ndatasets in terms of data volume, average sentence length, and con-\ntains relatively wide verbal vocabulary. To the best of our knowl-\nedge, Auto-ACD is the first million-level audio-language dataset.\nAdditionally, Auto-ACD stands as the only audio-language dataset\nthat encompasses environmental information, not only delineates the\ntype and source of sounds but also specifies the location of their oc-\ncurrence, increasing the richness of contextual details.\nClotho (4K) \nAudioCaps (5K) \nWavCaps* (29K) \nLAION-Audio-630K* (311K)\nAuto-ACD* (23K)\nAverage Length\nDataset Size (K)\nFig. 3. Dataset statistics of audio-language datasets. The X-axis and\nY-axis refer to the quantity and average lengths within per dataset,\nwhile the size of each data point (numbers in parentheses) reflects the\nvocabulary coverage. The data point with shadow refers to descrip-\ntions enriched with environmental context, \u201c*\u201d signifies datasets col-\nlected automatically.\n7.2. Dataset Comparison\nAs indicated in Table. 6, we present a comparison of captions from\nLAION-Audio-630K, WavCaps, and Auto-ACD for the same audio\nsample. Specifically, LAION-Audio-630K employs a keyword-to-\ncaption model to convert the strong labels to captions. WavCaps uti-\nlizes ChatGPT to rephrase the strong labels into simple captions. It\ncan be observed that captions in LAION-Audio-630K and WavCaps\nare concise and contain minimal information beyond the audio tags.\nIn particular, LAION-Audio-630K may include sentences that devi-\nate from common sense, for example, describing \u201crapping a tree\u201d for\nan audio tag of \u201crapping\u201d. WavCaps exhibit a monotonous sentence\nstructure, such as \u201d... sound can be heard.\u201d In contrast, Auto-ACD\nfeatures longer sentences with a richer depiction of the audio scenes.\n7.3. ChatGPT Prompt\nIn this section, we detail the prompt used as input to ChatGPT. As\nshown in Fig. 4, we start by formulating the specific task and criteria\nfor the desired outcome, then give seven distinctive visual-audio cues\ninto the prompt. Lastly, we provide three illustrative examples of\ncaptions for reference purposes. For visualisation purpose, we here\nuse a color-coded system to distinguish various cues.\n7.4. Dataset Visualization\nAs shown in Table. 7, we show more generated captions for audios\nfrom VGGSound and AudioSet. Note that, we present the video\nsequences to demonstrate how visual information can assist the lan-\nguage description for audio. It can be observed that, the captions\nNo. Dataset Generated Caption\n1.\nLA.\nA person is rapping a tree.\nWavC.\nMusic plays with a man rapping.\nACD\nA woman sings while hip hop music plays in the backgrou-\nnd, creating a rapping audio event in a computer room.\n2.\nLA.\na slushy water lily.\nWavC.\nStream noise, crowd and splashing sounds.\nACD\nA crowd of people yells and cheers as water sloshes in the\nbackground at a water park.\n3.\nLA.\na truck with a siren and a fire engine in an emergency.\nWavC.\nA fire engine siren is heard.\nACD\nAn emergency vehicle siren blares loudly as a fire truck ru-\nshes through a residential neighbourhood.\n4.\nLA.\na vehicle with a medium frequency of engine idling.\nWavC.\nA medium engine sound can be heard.\nACD\nA medium-sized engine is idling and vibrating, while an ad-\nult male speaks in the background near a running vehicle.\nTable 6. Caption comparison with LAION-Audio-630K and Wav-\nCaps, \u201cLA.\u201d, \u201cWavC.\u201d and \u201cACD\u201d refer to LAION-Audio-630K,\nWavCaps and Auto-ACD, respectively.\nI will give you some information from a video and an audio, this audio is separated\nfrom the video.\nThere is a caption for an audio, simple audio caption, this sentence simply describe\nwhat happens in the audio.\nThere are some audio tags: multiple audio tags, they indicate the audio events in this\naudio. number indicates the probability.\nThe audio-visual label is dataset visual-audio label.\nI extract a key frame from one video, and this is the image caption of this frame:\nimage caption; this is the image label: image label; this is the object detection: object\ndetection; this is the place detection: place label.\nNow, please help me write one audio caption using common vocabulary and no more\nthan 24 words, providing a description of what happened in the audio, and infer\nwhere the audio happened. You can refer the above information, and some visual\ninformation is inaccurate and can be ignored. please using the audio-visual label\ncheck the audio event in your caption.\nThe sentence you write need to be like these following examples:\nA bell chimes thrice as birds chirp in the background in the forest.\nA lawnmower engine buzzing and stopping to take a few breaks on the lawn.\nA machine being operated intermittently and people talking in the background in a\nfactory.\nPrompting ChatGPT to generate caption for audio \nFig. 4. Detailed prompt provided to ChatGPT. For visualisation pur-\npose, we use different colors to highlight diverse visual-audio cues.\nin Auto-ACD not only accurately depict sound events but also in-\nfer contextual information based on visual priors, that can also be\ninferred from audios, for example, (i) environmental details, for in-\nstance, \u201ca rainy day\u201d and \u201cin a railway environment\u201d, (ii) sound at-\ntributes like \u201ca loud and powerful sound\u201d and \u201cpeople talk in the\ndistance\u201d, (iii) sound variations, for example, \u201cmotorcycle engine\nrevs up and down\u201d and \u201ca train approaches\u201d.\nNo. Generated Caption\n1.\nRain falls hard on a surface as people talk in the distance, creating a soothing ambiance of a rainy day.\n2.\nThe motorcycle engine revs up and down while driving through a residential neighborhood, accompanied by some speech and light engine sounds.\n3.\nA melodic accordion tune fills the air as the musician plays in a music studio, creating a pleasant ambiance.\n4.\nA train horn blares as a train approaches, creating a loud and powerful sound in a railway environment.\n5.\nSheep bleat in the distance as people talk faintly, creating a pastoral atmosphere in a wheat field.\n6.\nA roaring crowd erupts in cheers and battle cries, creating an electrifying atmosphere during a lively event.\n7.\nBird wings flap as rustling and birds chirping in the background create a serene ambiance in a garden.\n8.\nA singing bowl resonates with a gentle gong sound, accompanied by soft music playing in a church.\nTable 7. Data visualization in Auto-ACD. In each sample, the top line showcases the video frame sequence, the bottom line presents the\ncorresponding audio caption. The sound events in the caption are highlighted in bold text.\n"
  },
  {
    "title": "End-to-End Speech Recognition Contextualization with Large Language Models",
    "link": "https://arxiv.org/pdf/2309.10917.pdf",
    "upvote": "8",
    "text": "END-TO-END SPEECH RECOGNITION CONTEXTUALIZATION WITH LARGE\nLANGUAGE MODELS\nEgor Lakomkin, Chunyang Wu, Yassir Fathullah\u2020, Ozlem Kalinli, Michael L. Seltzer, Christian Fuegen\nMeta AI\nABSTRACT\nIn recent years, Large Language Models (LLMs) have\ngarnered significant attention from the research community\ndue to their exceptional performance and generalization ca-\npabilities. In this paper, we introduce a novel method for con-\ntextualizing speech recognition models incorporating LLMs.\nOur approach casts speech recognition as a mixed-modal lan-\nguage modeling task based on a pretrained LLM. We provide\naudio features, along with optional text tokens for context,\nto train the system to complete transcriptions in a decoder-\nonly fashion. As a result, the system is implicitly incentivized\nto learn how to leverage unstructured contextual information\nduring training. Our empirical results demonstrate a signifi-\ncant improvement in performance, with a 6% WER reduction\nwhen additional textual context is provided. Moreover, we\nfind that our method performs competitively and improve by\n7.5% WER overall and 17% WER on rare words against a\nbaseline contextualized RNN-T system that has been trained\non more than twenty five times larger speech dataset. Over-\nall, we demonstrate that by only adding a handful number of\ntrainable parameters via adapters, we can unlock contextu-\nalized speech recognition capability for the pretrained LLM\nwhile keeping the same text-only input functionality.\nIndex Terms\u2014 contextual biasing, large language mod-\nels, speech recognition\n1. INTRODUCTION\nIn recent years, there has been growing interest in Large Lan-\nguage Models (LLMs) due to their remarkable efficacy in\nenhancing performance in tasks like question answering and\nsummarization, surpassing specialized models [1, 2]. LLMs\nare trained on vast quantities of text data, thereby encapsu-\nlating a wealth of world knowledge within the network. This\naccumulated knowledge and contextual understanding prove\nto be particularly beneficial in the field of Automatic Speech\nRecognition (ASR), especially when additional context sur-\nrounding an utterance is available beyond the audio alone.\nFor example, video titles and descriptions can provide in-\nsights into the topic of the video or offer clues about named\nentities that might be mentioned [3, 4]. Such contextual in-\nWork done during internship at Meta AI.\nFig. 1. A speech recognition model with mixed-modal con-\ntext consisting of audio and optional text tokens based on a\npretrained LLM backbone. Speech encoder and LLM decoder\nare both initially pretrained. The LLM weights are frozen\n(orange blocks), while audio encoder and LoRa adapters are\nfine-tuned during training (blue blocks).\nformation can assist in disambiguating challenging pronun-\nciations, as certain words, domain-specific terms, or named\nentities can often be inferred from context alone. Traditional\napproaches to ASR contextualization [4, 3, 5, 6] operate at the\ntoken or phrase level, employing techniques like biasing with\nweighted finite state transducers (WFSTs) or using special-\nized attention networks. These are typically either incorpo-\nrated during the decoding stage or trained as separate compo-\nnents. Consequently, contextualization significantly improves\nthe ASR system\u2019s ability to recognize named entities or spe-\ncialized in-domain terms. However, there are some limita-\ntions to these approaches:\n- The biasing is limited towards individual phrases or\nwords, as opposed to contextualizing based on external infor-\nmation as a whole (for example, topic-based biasing).\n- The biasing strength is usually controlled via a hyper-\nparameter or requires specialized architectural changes and\ntraining procedures to ensure the system is not overbiased.\n- Some of the contextualization methods influence only\nthe decoder state without interacting with the encoder di-\nrectly.\nIn this work, we propose a Speech LLaMA - a decoder-\nonly architecture inspired by recent developments in LLMs\ntailored towards speech recognition. It is trained to use the\ncontextual information end-to-end without any additional hy-\narXiv:2309.10917v1  [eess.AS]  19 Sep 2023\nperparameters. Specifically, 1) we prepend the whole avail-\nable textual context as a prompt to an ASR system along with\naudio tokens.\nThe Speech LLaMA hence have the full flexibility to look\nback and cross-corellate the contextual text tokens and the\nacoustic tokens when decoding the next spoken word. And\n2) we employ the publicly available 7B LLaMA LLM [1] as\na pretrained decoder for the Speech LLaMA. This simplifies\nthe overall design of a contextual ASR as speech recogni-\ntion can be considered as mixed-modal language model with\nnext-token prediciton. Our intuition behind this is the pre-\ntrained LLMs already distill the linguistic information which\nshould be particularly useful when reasoning which part of\nthe context is relevant given the utterance. Our results on a\ncompetitive benchmark suggest a feasibility of this modelling\napproach.\n2. RELATED WORK\nThere have been several works on speech recognition models\ncontextualization including deep and shallow biasing [8, 4].\nLe et al.\n[4] introduced a weighted finite state transducer\n(WFST) composed from biasing strings which is attached dy-\nnamically during decoding and the scores of the RNN-T sys-\ntem and biasing WFST are interpolated. The advantage of\nsuch approaches is that they could be attached to any system\nafter the training is completed. Another line of research is\ndeep biasing methods that incorporate contextualization end-\nto-end during the model training [9, 3, 6, 10, 11, 5]. A com-\nmon limitation of these approaches is that the bias on the\nphrase level, rather than providing on the full context avail-\nable. In addition, these approaches require a specialized bias-\ning modules added to the main ASR architecture.\nIn parallel to this reseach several approaches were pre-\nsented incorporating LLMs for speech related tasks. Wu at\nal. [12] incorporated LLaMA LLM for speech translation by\nconcatenating a textual prompt (\u201dTranslate audio to language\nX\u201d) with audio representations. AudioPalm [13] model was\nproposed mixing audio and text tokens for speech-to-text and\nspeech to speech tasks. Fathullah et al. [14] presented re-\nsults on enabling speech recognition capabilities for LLaMA\nmodel on the multi-lingual data. Recently a Whisper model\n[15] incorporated a biasing approach, where the previous seg-\nment\u2019s transcription was added to the prompt for the long-\nform speech recognition. In difference to their work, we bias\nthe system on the unstructed and sometimes unrelated textual\ncontext as not always video title and description match the\ncontext of speech.\n3. EXPERIMENTAL SETUP\nModel: Figure 1 illustrates the overview of our proposed\nmodel. This speech LLM architecture consists of two main\nblocks: audio encoder and text decoder. The audio encoder\nfirstly applies four downsampling blocks resuling in 16x time\nreduction of audio representations. After that a stack of Con-\nformer [16] blocks with rotary positional embeddings [17] are\napplied with hidden dimensionality of 512 and kernel size of\n9.\nAt the end we add an additional downsampling block.\nAs a result the decoder observes audio tokens sampled ev-\nery 320ms with dimensionality of size 4,096. We pretrained\nthe audio encoder with Connectionist Temporal Classifica-\ntion [18] criterion for 300k training steps on the same train-\ning data. We used a pretrained 7B LLaMA (v1) [1] as a de-\ncoder. To adapt text-only LLaMA to speech recognition task,\nwe have added Low-Rank Adapters [19] to query, key, value,\nand output projection matrices in the self-attention layer of\nevery decoder layer while keeping the rest of LLM param-\neters frozen throughout the training. We used the following\nLoRa parameters: rank of size 32, dropout rate of 5%, and\n0.05 scaling parameter. Overall LoRa parameters add 30 mil-\nlion trainable parameters to the LLM decoder and the rest 6.7\nbillion are kept frozen.\nWe used 80 dimensional log Mel features computed every\n10ms with a window of 25ms. SpecAugment [20] with two\nfrequency masks of width 27 and ten time masks with max-\nimum width of 4% of the length of an utterance. We trained\nour models for 200,000 updates with mixed precision, lin-\nearly increasing the learning rate to 5e-4 in the first 20,000\nupdates and exponentially decaying to 1e-5 over the remain-\ning updates. We use Adam with parameters \u03b21 = 0.9, \u03b22 =\n0.98, weight decay = 1e-5 and clip the gradient norm to 1.\nOur model is trained with 128 A100 GPUs for 3 days using\nFairseq library [21].\nData: The models are trained on an in-house dataset that\nwas de-identified with no personally identifiable information\n(PII) derived from public Facebook and Instagram videos.\nThe data was further augmented with two distortion methods:\nspeed perturbation [22] and randomly sampled additive back-\nground noise. For evaluation, we have sampled 3,200 videos\ncomprising around 34 hours of speech that have context of\nat least 100 characters length with at least one non-frequent\nword from the context occurs in the transcription.\nMetrics: To evaluate our models, we report both the over-\nall Word Error Rate (WER) and Rare WER, which considers\nonly rare words. A word is considered rare if it does not occur\nin the 90% percentile of the most frequent words estimated on\ntraining data.\nTextual context: Similar to Xiao et al. [7] we incorpo-\nrate video title and video description as an external context.\nWe perform basic text post-processing like unicode character\nnormalization and removal of all non-ascii symbols. Overall\napproximately 25% of videos from supervised video dataset\nhave non-empty text context. When video title or description\nare present, we first concatenate and then tokenize them with\nthe LLaMA tokenizer. After that, we prepend the <bos> to-\nken with the textual tokens. When both video title and de-\nscriptions are missing, the input corresponds to a traditional\nASR setup without contextual information. The cross-entropy\nTable 1. Evaluation results of Speech LLaMA compared to large-scale RNN-T baseline on English speech data. We report\noverall WER and Rare WER. Rare WER specifically focuses on the accuracy of recognizing rare words in the dataset.\nModel\nSpeech\ndata (h)\nTrainable\nparams (M)\nContext presence\nWER (%)\nSUB\nINS\nDEL\nRare WER (%)\nTraining\nEvaluation\n1B RNN-T [7]\n4M\n1000\n-\n-\n12.34\n6.53\n3.21\n2.60\n30.80\n1B RNN-T [7]\n4M\n1000\n-\n\u2713\n12.13\n6.23\n3.05\n2.85\n28.96\nSpeech LLaMa\n150k\n130\n-\n-\n11.70\n6.09\n3.20\n2.38\n27.33\nSpeech LLaMa\n150k\n130\n\u2713\n-\n11.98\n6.28\n3.07\n2.63\n28.64\nSpeech LLaMa\n150k\n130\n\u2713\n\u2713\n11.22\n5.76\n3.14\n2.32\n23.88\nloss is masked for the contextual tokens and only computed\nfor spoken tokens. In these experiments we limit the textual\ncontent to a maximum of 50 tokens for computational rea-\nsons. If the context is longer than the threshold, we perform\na random crop of size 50 during training and crop the leading\ntokens during inference.\nBaseline: As a baseline we used a transformer based\nRNN-T system with one billion parameters [7], which is\ntrained on four million hours of supervised and semi-supervised\nspeech data. The RNN-T system architecture consists of 60\ntransformer layers in the encoder and 3 LSTM layers in\nthe decoder.\nFor contextualization it uses an WFST bias-\ning method with neural language modelling shallow fusion\n[4], where the biasing FST is composed from video title and\ndescription. We are using exactly the same contextual in-\nformation during decoding for the RNN-T baseline and our\nSpeech LLaMA.\n4. RESULTS\nTable 1 presents a summary of our decoding results on the\nevaluation set. We compare the Speech LLaMA against the\noffline RNN-T 1b model, considering two scenarios: with\nand without presenting contextualization information during\ndecoding. The WER scores obtained for these scenarios us-\ning RNN-T are 12.34% and 12.13% respectively. Contextual\nbiasing resuts in a relative WER reduction of approximately\n1.7%.\nEven without the use of contextual information during\ntraining and evaluation, Speech LLaMA achieves a WER of\n11.70%, a relative reduction of 5.2% over the RNN-T system\ntrained on much more data.\nBy incorporating context during training and evaluation,\nwe achieve a significant improvement reaching an overall\nWER of 11.22% and resulting in 17% relative improvement\nin Rare WER, surpassing the performance of the RNN-T\nmodel with contextualization.\nIt is worth noting that when we evaluate the Speech\nLLaMA trained with context but do not provide the context\nduring inference, we obtain a WER of 11.98%. This corre-\nsponds to a slight WER gap compared to the model trained\nwithout context. We leave to address this minor performance\ndifference to the future work, where adding a certain jitter\nto the context may improve the generalization of a model\ntowards presence of the context.\n4.1. Ablation studies\n4.1.1. Context sensitivity\nTo better understand how the model learns to use the context,\nwe studied how receptive the model is to context perturba-\ntions. For this we tried a few ways to modify the prompt and\nmeasure its effect on the decoding. Specifically, we experi-\nmented with:\n1. Replacing the actual context with words randomly sam-\npled from the training data.\n2. Replacing the context with the ground truth words. We\nfilter out frequent words in this experiment as we as-\nsume that the model should not have significant issues\nin transcribing them. We expect a significant reduc-\ntion of WER if the model is capable of copy-pasting\nthe words from the context.\n3. Replacing the contextual words with phonetical re-\nspellings of the words that appear in the transcripts.\nOur intuition is that such replacements are particularly\nchallenging for the model and we should expect a big-\nger WER change compared to random substitutions. To\ngenerate re-spellings we employed a G2G [23] model.\nFor every rare word in the ground truth we sample an\nalternative spelling from the G2G model and add it to\nthe context. For example, if the word ball is present in\nthe context and ground truth we replace it by bawl and\nuse that as context instead of the original token.\n4. In addition to the previous perturbation we probe ap-\npending a similar sounding word to the context (e.g.\nboth tokens ball and bawl will be present in the con-\ntext). This tests the ability of an ASR system to dis-\nambiguate the actual spoken word given a competitive\nword in context.\nTable 2. WER under different context perturbations during\ndecoding stage.\nContext noise\nWER (%)\nRare WER (%)\n(Original context)\n11.22\n23.88\n(Remove all context)\n11.98\n28.64\nRandom\n12.07\n28.85\nRespellings\n11.89\n28.31\nRespellings (append)\n11.46\n25.59\nGround Truth\n10.50\n19.54\nWe present our results in Table 2. We note that replacing\nthe whole context with random words sampled from the train-\ning data results in only a marginal difference in WER com-\npared to removing all external context (11.98% vs. 12.07%).\nThis indicates that the model is robust against some contex-\ntual noise and can distinguish relevant from irrelevant context.\nSubstituting rare words that match both the context and the\nground truth with G2G respellings results in a significant drop\nin WER (11.22% \u2192 11.89%), almost matching with not us-\ning any context. This hints that the majority of gains observed\nare due to the model being able to copy certain words from\nthe context. In contrast, when we instead of replacing the\nmatching contextual word rather append a competing similar-\nsounding word, we observe a smaller WER drop (11.22% \u2192\n11.46%). This indicates that the model does not necessar-\nily get confused by similarly pronounced words with differ-\nent meanings. Furthermore, when we take the rare words\nfrom the ground truth into the context, the WER improves\nto 10.50% (6% relative change) and Rare WER improves by\n18% relative. This further proves the ability of the model to\nutilize contextual information when present in order to better\nrecognize rare entities.\nTable 3. Impact of the context masking structure on the WER.\nMasking\nWER (%)\nCausal\n11.22\nFull-Mask\n11.15\n4.1.2. Causal vs Full Masking\nTraditionally causal masking is used in all self-attention lay-\ners for decoder-only language models to prevent future infor-\nmation leakage. However for offline speech recognition we\nhave full audio and text context observed at the time of decod-\ning and only transcription tokens are necessary to be masked\ncausally. In this section we experiment the impact of apply-\ning causal masking on all input tokens and contrast it with\napplying full mask on the text and audio context followed by\ncausal masking on transcription tokens. While the audio rep-\nresentations are fully contextualized already, we hypothesize\nTable 4. Performance comparison of decoder-only Speech\nLLM and cross-attention Speech LLM.\nDecoder\nWER (%)\nDecoder-only\n11.22\nEncoder-decoder\n11.18\nthat textual tokens may benefit from full masking.\nWe present our results in Table 3. The full-mask shows\nonly marginally better WER then causal masking (improving\nfrom 11.22% \u2192 to 11.15%). This comes at a cost as effi-\ncient self-attention implementations are currently tailored to-\nwards causal masking (Flash-Attention v2) and using a cus-\ntom masking slows down training by 10%.\n4.1.3. Decoder-only vs Cross-attention\nFurthermore, we compared the decoder-only approach to a\ntraditional encoder-decoder model by converting the Speech\nLLM architecture to Listen-Attend-Spell architecture [24].\nTo achieve that, instead of concatenating audio and text\ntokens we treated them separaterely.\nWe added trainable\ncross-attention matrices to every LLM decoder layer. Table\n3 presents the results of this study. We observed that the two\napproaches perform similarly, with only minor improvement\nfor the Encoder-Decoder architecture (11.22% \u2192 11.18%).\nThis indicates that the decoder-only approach is a viable and\nstraightforward method for performing ASR with or without\ncontextualization.\nHowever, one limitation of the decoder-only approach is\nthe quadratic attention complexity, which can impose restric-\ntions on the overall sequence length. This limitation becomes\nsignificant as the context grows. To address this issue, we can\nemploy techniques such as lower precision training (8 or 4\nbits) and linear attention approximation methods [25, 26].\n5. CONCLUSIONS AND FUTURE WORK\nIn this work, we have presented to our knowledge the first\nresults on utilizing pretrained LLMs to leverage contextual\ninformation in order to improve speech recognition. We have\ndemonstrated that with a simple decoder-only architecture we\ncan condition the ASR output on the unstructured text. Our\napproach shows superior performance against a strong base-\nline, proving the feasability of the proposed method at scale.\nEnd-to-end contextualization via text promping with LLMs\nshows better context utilization compared to our strong RNN-\nT based baselines. In addition, our ablation studies show that\nthe system is robust to noise perturbations and shows abilities\nto perform a phonetic disambiguation. As part of the future\nwork, we plan to extend the methods towards long context\nand other modalities.\n6. REFERENCES\n[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.,\n\u201cLlama: Open and efficient foundation language mod-\nels,\u201d 2023.\n[2] OpenAI, \u201cGpt-4 technical report,\u201d 2023.\n[3] Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey\nZweig, Florian Metze, and Yatharth Saraf,\n\u201cContex-\ntual RNN-T for open domain ASR,\u201d in INTERSPEECH.\n2020, pp. 11\u201315, ISCA.\n[4] Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim, et al.,\n\u201cContextualized streaming end-to-end speech recogni-\ntion with trie-based deep biasing and shallow fusion,\u201d\nin INTERSPEECH, 2021, pp. 1772\u20131776.\n[5] Kanthashree Mysore Sathyendra, Thejaswi Muniyappa,\nFeng-Ju Chang, et al.,\n\u201cContextual adapters for per-\nsonalized speech recognition in neural transducers,\u201d in\nICASSP. 2022, pp. 8537\u20138541, IEEE.\n[6] Golan Pundak, Tara N. Sainath, et al., \u201cDeep context:\nEnd-to-end contextual speech recognition,\u201d\nin 2018\nIEEE Spoken Language Technology Workshop. 2018,\npp. 418\u2013425, IEEE.\n[7] Alex Xiao, Weiyi Zheng, Gil Keren, et al.,\n\u201cScaling\nasr improves zero and few shot learning,\u201d in INTER-\nSPEECH, 2021.\n[8] Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon,\net al., \u201cShallow-fusion end-to-end contextual biasing,\u201d\nin INTERSPEECH. 2019, pp. 1418\u20131422, ISCA.\n[9] Duc Le, Gil Keren, Julian Chan, et al., \u201cDeep shallow\nfusion for rnn-t personalization,\u201d in 2021 IEEE Spoken\nLanguage Technology Workshop (SLT), 2021, pp. 251\u2013\n257.\n[10] Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur\nGandhe, et al.,\n\u201cRobust acoustic and semantic con-\ntextual biasing in neural transducers for speech recog-\nnition,\u201d CoRR, vol. abs/2305.05271, 2023.\n[11] Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al.,\n\u201cAdaptive contextual biasing for transducer based\nstreaming speech recognition,\u201d 2023.\n[12] Jian Wu, Yashesh Gaur, et al.,\n\u201cOn decoder-only ar-\nchitecture for speech-to-text and large language model\nintegration,\u201d 2023.\n[13] Paul K. Rubenstein et al., \u201cAudiopalm: A large language\nmodel that can speak and listen,\u201d 2023.\n[14] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al.,\n\u201cPrompting large language models with speech recogni-\ntion abilities,\u201d 2023.\n[15] Alec Radford, Jong Wook Kim, et al., \u201cRobust speech\nrecognition via large-scale weak supervision,\u201d in ICML.\n23\u201329 Jul 2023, vol. 202 of Proceedings of Machine\nLearning Research, pp. 28492\u201328518, PMLR.\n[16] Anmol Gulati, James Qin, et al.,\n\u201cConformer:\nConvolution-augmented transformer for speech recog-\nnition,\u201d\nin INTERSPEECH. 2020, pp. 5036\u20135040,\nISCA.\n[17] Jianlin Su, Yu Lu, et al., \u201cRoformer: Enhanced trans-\nformer with rotary position embedding,\u201d\nCoRR, vol.\nabs/2104.09864, 2021.\n[18] Alex Graves et al., \u201cConnectionist temporal classifica-\ntion: labelling unsegmented sequence data with recur-\nrent neural networks,\u201d in ICML. 2006, vol. 148 of ACM\nInternational Conference Proceeding Series, pp. 369\u2013\n376, ACM.\n[19] Edward J. Hu, Yelong Shen, et al., \u201cLora: Low-rank\nadaptation of large language models,\u201d in ICLR. 2022,\nOpenReview.net.\n[20] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng\nChiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le,\n\u201cSpecaugment: A simple data augmentation method for\nautomatic speech recognition,\u201d\nINTERSPEECH, Sep\n2019.\n[21] Myle Ott, Sergey Edunov, Alexei Baevski, et al.,\n\u201cfairseq: A fast, extensible toolkit for sequence mod-\neling,\u201d\nin ACL (Demonstrations), Minneapolis, Min-\nnesota, June 2019, pp. 48\u201353, Association for Compu-\ntational Linguistics.\n[22] Tom Ko, Vijayaditya Peddinti, et al., \u201cAudio augmenta-\ntion for speech recognition,\u201d in INTERSPEECH. 2015,\npp. 3586\u20133589, ISCA.\n[23] Duc\nLe,\nThilo\nKoehler,\nChristian\nFuegen,\nand\nMichael L. Seltzer,\n\u201cG2g: Tts-driven pronunciation\nlearning for graphemic hybrid asr,\u201d in ICASSP, 2020,\npp. 6869\u20136873.\n[24] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol\nVinyals,\n\u201cListen, attend and spell,\u201d\nCoRR, vol.\nabs/1508.01211, 2015.\n[25] Tim Dettmers and Luke Zettlemoyer, \u201cThe case for 4-\nbit precision: k-bit inference scaling laws,\u201d in ICML.\n2023, vol. 202, pp. 7750\u20137774, PMLR.\n[26] Jiayu Ding, Shuming Ma, et al.,\n\u201cLongnet: Scaling\ntransformers to 1,000,000,000 tokens,\u201d 2023.\n"
  },
  {
    "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute",
    "link": "https://arxiv.org/pdf/2309.11197.pdf",
    "upvote": "4",
    "text": "The Languini Kitchen: Enabling Language Modelling\nResearch at Different Scales of Compute\nAleksandar Stani\u0107\u2020\u2217\nDylan Ashley\u2020\nOleg Serikov\u2021\nLouis Kirsch\u2020\nFrancesco Faccio\u2020\u2021\nJ\u00fcrgen Schmidhuber\u2020\u2021\nThomas Hofmann\u00a7\nImanol Schlag\u00a7\u2217\u00b6\n\u2020 The Swiss AI Lab IDSIA/USI/SUPSI\n\u2021 AI Initiative, King Abdullah University of Science and Technology\n\u00a7 ETH Z\u00fcrich\nAbstract\nThe Languini Kitchen serves as both a research collective and codebase designed to empower\nresearchers with limited computational resources to contribute meaningfully to the field of\nlanguage modelling1. We introduce an experimental protocol that enables model compar-\nisons based on equivalent compute, measured in accelerator hours. The number of tokens\non which a model is trained is defined by the model\u2019s throughput and the chosen compute\nclass. Notably, this approach avoids constraints on critical hyperparameters which affect\ntotal parameters or floating-point operations. For evaluation, we pre-process an existing\nlarge, diverse, and high-quality dataset of books that surpasses existing academic bench-\nmarks in quality, diversity, and document length. On it, we compare methods based on\ntheir empirical scaling trends which are estimated through experiments at various levels of\ncompute. This work also provides two baseline models: a feed-forward model derived from\nthe GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold\nthroughput. While the GPT baseline achieves better perplexity throughout all our levels of\ncompute, our LSTM baseline exhibits a predictable and more favourable scaling law. This\nis due to the improved throughput and the need for fewer training tokens to achieve the\nsame decrease in test perplexity. Extrapolating the scaling laws leads of both models results\nin an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the\nfoundation for meaningful and reproducible language modelling research.\nContents\n1\nIntroduction\n2\n2\nBackground: Language Modelling\n3\n2.1\nWhy Scalability Matters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nExisting Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nThe Languini Books Benchmark\n6\n3.1\nComparison based on Compute Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nThe Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2.1\nEvaluation and Test Sets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1See languini-kitchen.github.io\n*Shared first authorship. Contact: imanol.schlag@gmail.com\n\u00b6Work done partially at IDSIA.\n1\narXiv:2309.11197v1  [cs.LG]  20 Sep 2023\n4\nThe Baselines\n10\n4.1\nTokenisation Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.1.1\nAnalysing SentencePiece Vocabularies . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.1.2\nPerformance Comparison of Different Vocabulary Sizes . . . . . . . . . . . . . . . . . .\n13\n4.2\nThe Feed-Forward Baseline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.2.1\nEvaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.3\nThe Recurrent Baseline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.3.1\nThe Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.3.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n5\nThe Languini Codebase\n22\n6\nOpen Research Questions\n24\n7\nConclusion\n25\nA OOD Scale Plots\n32\n1\nIntroduction\nLanguage modelling, a critical aspect of natural language processing (NLP), involves predicting the prob-\nability distribution over a sequence of words in a language. Its importance underpins a variety of NLP\ntasks such as machine translation (Vaswani et al., 2017), text generation (Brown et al., 2020), and ques-\ntion answering (Devlin et al., 2019). Presently, language modelling research primarily emphasises finetuning\nlarge pre-trained models (Ding et al., 2023; Zhang et al., 2023) as well as techniques on prompting (Liu\net al., 2023) and programming with large language models (Schlag et al., 2023; Dohan et al., 2022) which\nhave greatly improved performance across a variety of NLP tasks. However, this focus has inadvertently\nhampered the development of novel language modelling methodologies that require the model to be trained\nfrom scratch. The prevailing sentiment of \u201cbigger equals better\u201d can overshadow the potential benefits of\nalternative architectures and innovative methodologies, which may offer unique advantages.\nTransformers, the backbone of this trend, have proven their efficacy by setting the standard across a broad\nspectrum of tasks (Vaswani et al., 2017). Interestingly, recent work shows how the Transformer can be\nderived from Fast Weight Programmers from the \u201990s (Schmidhuber, 1991; Katharopoulos et al., 2020; Schlag\net al., 2021). However, Transformers are not without limitations. They exhibit issues such as quadratic\ncomputational complexity with respect to the sequence length, difficulty in capturing relevant tokens from\nlarge contexts (Tworkowski et al., 2023), and limitations due to the finite nature of its context (Dong et al.,\n2023). Furthermore, transformers have a large inference cost, which can pose a significant challenge when\ndeploying models in resource-constrained environments (Chitty-Venkata et al., 2023; Bondarenko et al.,\n2021). These limitations underscore the need for continued refinement and innovation.\nAdditionally, recent work argues that published modifications to the vanilla Transformer architecture did\nnot meaningfully improve performance on a question-answering task (Narang et al., 2021). After extensive\nempirical evaluation, the authors of that study conjecture that various improvements do not transfer across\nimplementation and tasks \u2014 an issue also present in other machine learning areas such as e.g. recommen-\ndation systems (Ferrari Dacrema et al., 2019), optimisation (Sivaprasad et al., 2020; Choi et al., 2020), or\ngenerative adversarial networks (Lucic et al., 2018).\nTo address these challenges, we introduce the Languini Kitchen, a novel benchmark, codebase, and research\ncollective.\nLanguini Kitchen, or just Languini, aims to create an environment that enables researchers,\nin particular those with limited computational resources, to make meaningful contributions to language\nmodelling research. This is achieved through an experimental protocol that constrains experiments to various\nscales of compute and through a public code repository that enables reproducible experiments. Languini is\na blend of the words language and linguine where the latter is a type of pasta similar to spaghetti which\nironically stands for the research nature of the code in the Languini code repository.\n2\nRecent work showed that the scaling laws are not universal across various model architectures (Tay et al.,\n2023). Furthermore, their results indicate that the vanilla transformer still comes out on top in a direct\ncomparison with eleven other recently published models.\nTo enable progress, Languini focuses on fair\ncomparisons and reproducible results on complex and general benchmark. Different models are compared\nbased on their performance trend as compute increases (Kaplan et al., 2020; Hoffmann et al., 2022) and the\nresulting scale plots will hopefully serve as a platform for identifying promising models or techniques that\nwarrant further scale-up.\nFor evaluation, we use a filtered version of the books3 subset from The Pile (Gao et al., 2020) and the\nBigScience ROOTS corpus (Lauren\u00e7on et al., 2022) which has been used previously as training data for\nvarious large language models (e.g. see Scao et al. (2022); Dey et al. (2023); Biderman et al. (2023); Touvron\net al. (2023a;b)).\nAfter rigorous filtering, our version of the dataset consists of approximately 85GB of\nhigh-quality monolingual text from 158,577 published books which span a large variety of modern topics\nand stories that significantly surpass the complexity and size of previous academic benchmarks. Models\nwhich are trained on the Languini Books benchmark are compared at different compute scales based on\ntheir perplexity on held-out data. This includes out of distribution splits with books on certain topics (such\nas learning a new language) which are excluded from the training data in order to evaluate the model\u2019s\npredictive ability across several books as context.\nLanguini\u2019s open-source codebase provides a range of functions, from facilitating the model development\nprocess to logging mechanisms. The project is inspired by Scenic, a lightweight library that facilitates rapid\nprototyping of novel vision models (Dehghani et al., 2022). Similar to Scenic, the Languini codebase aims to\nkeep the core functionality simple and prevents any dependencies between projects. Furthermore, researchers\nare encouraged to incorporate their projects into the Languini codebase, hopefully fostering a continually\nincreasing collection of previous work to ease the comparison to new methods. In this work, we introduce\nthe two initial Languini models: a feed-forward, GPT-based, decoder-only Transformer model (Section 4.2)\nand a recurrent quasi-LSTM (Section 4.3). For each model and each compute class, we empirically find the\nbest hyperparameter configuration which results in scaling plots that allow the comparison of each model\u2019s\nscaling law.\nIn summary, our research contributions are the following:\n\u2022 An experimental protocol for the comparison of language modelling research under different scales\nof compute.\n\u2022 A high-quality filtering of the books3 datasets for language modelling research with out of distribu-\ntion splits for evaluating long-range dependencies.\n\u2022 A scaling law comparison between a GPT-based model and a quasi-LSTM model where the quasi-\nLSTM\u2019s scaling law is superior.\n\u2022 A codebase for researchers to simplify development and enable fair and meaningful comparison with\nscalability in mind.\n\u2022 An empirical analysis of byte-pair encoding tokenisation.\n2\nBackground: Language Modelling\nLanguage modelling is a central task in NLP where raw text is typically segmented into a sequence of words or\nsubwords using a tokeniser, which operates based on a predefined vocabulary (Mikolov et al., 2010; Al-Rfou\net al., 2019). These segmented units are commonly referred to as tokens. With this tokenised representation\nin place, the goal of a language model becomes the prediction of a subsequent token given its preceding\nsequence of tokens. This objective can be formally defined as maximising the probability of a sequence of\ntokens w1, w2, ..., wN:\np(w1, w2, ..., wN) =\nN\nY\nt=1\np(wt|w0, ..., wt\u22121)\n(1)\n3\nwhere p(wt|w0, ..., wt\u22121) is the probability of token wt given the sequence of previous tokens w0, ..., wt\u22121.\nThe performance of a language model can be evaluated using the total cross-entropy loss, which for a given\ndataset is defined as:\nL = \u2212\nN\nX\nt=1\nlog p(wt|w0, ..., wt\u22121)\n(2)\nThe cross-entropy measures the negative log-likelihood of the observed data under the model. Lower loss\nindicates a better model, but comparing raw loss values can be unintuitive. Therefore, the loss is often\ntransformed into perplexity, a more interpretable measure, defined as:\nPPL = exp\n \n\u2212 1\nN\nN\nX\nt=1\nlog p(wt|w0, w1, ..., wt\u22121)\n!\n(3)\n= exp\n\u0012 L\nN\n\u0013\n(4)\nwhere the cross entropy, or average loss, is equal to L\nN , and N is the number of tokens in the sequence.\nConsider a model predicting from vocabulary of M tokens that will predict p(wt|w0, w1, ..., wt\u22121) =\n1\nM for\nany t. Such a uniform model would have the following perplexity:\nPPL = exp\n \n\u2212 1\nN\nN\nX\nt=1\nlog p(wt|w0, w1, ..., wt\u22121)\n!\n(5)\n= exp\n \n\u2212 1\nN\nN\nX\nt=1\nlog 1\nM\n!\n(6)\n= exp\n\u0012\n\u2212 log 1\nM\n\u0013\n(7)\n= exp (log(M))\n(8)\n= M\n(9)\nThus, by exponentiating the average loss during training, perplexity can be interpreted as the effective\nvocabulary size of a uniform model.\nWhile perplexity is a standard measure in language modelling, it has limitations. The perplexity measures\nof two methods are only directly comparable when the same tokenisation is used. This is because its value\nis influenced by the granularity of tokenisation which depends on the tokenisation algorithm and vocabulary\nsize. A larger vocabulary increases the difficulty of each individual prediction (higher loss) but may reduce\nthe number of predictions in total (lower N). Previous work has shown that this is not an equal trade-off as\nincreasing the vocabulary has diminishing returns (see appendix in Hutchins et al. (2022)). Consequently,\nmodels trained with different tokenisers can produce perplexity values that are not directly comparable.\nTo alleviate this, we introduce the measure of normalised perplexity. This measure adjusts for differences\nin tokenisation granularity by dividing the cross-entropy with the total number of bytes of the decoded text\nB, rather than the number of tokens N:\nnormalised PPL = exp\n\u0012 L\nB\n\u0013\n(10)\nNormalised perplexity makes it possible to compare the same model trained on different tokenisers, as it\nprovides a standardised measure that adjusts for the variability introduced by the choice of tokenisation.\n4\nNaturally, if different models are compared using different tokenisation algorithms, it remains open if the\nrelative difference is due to the choice of model or choice of tokenisation. Nevertheless, this measure ensures\na more equitable comparison between methods and contributes to a more nuanced understanding of their\nrelative performance. Furthermore, normalised perplexity is dataset independent, allowing also for a relative\ncomparison of perplexity across different problems such as modelling natural language or modelling code.\n2.1\nWhy Scalability Matters\nThe scalability of a language model refers to its ability to improve performance as more computational\nresources are invested, usually by training larger models on more training data. Scalability is a critical\naspect to consider when evaluating the potential of an architecture because it indicates how well the model\ncan leverage additional resources.\nScaled-up language models, i.e. large language models (LLMs; see Zhao et al. (2023a) for a recent review),\nhave demonstrated to be broad few-shot learners (Brown et al., 2020; Schulman et al., 2022; Chowdhery\net al., 2022; OpenAI, 2023). LLMs excel on numerous tasks without or with little need for task-specific\nfinetuning. They achieve excellent results on question answering, text summarisation, translation, and other\nNLP tasks (OpenAI, 2023), but are also increasingly applied to other modalities such as images (Saharia\net al., 2022; Alayrac et al., 2022), audio Ghosal et al. (2023), and reinforcement learning settings Driess\net al. (2023). However, raw LLMs do not align well with human values and additional work is necessary to\ntransform a raw LLM into a robust, helpful, and harmless conversational agent (Bai et al., 2022a;b).\nWhile the ability of LLMs on various downstream tasks can provide valuable insights, relying on downstream\nperformance as the main measure for comparison presents several challenges. First, downstream performance\nkeeps improving due to the development of new finetuning and prompting strategies (Hu et al., 2021; Liu\net al., 2023). Thus, any fixed prompting strategy will quickly be outdated. Second, many evaluation datasets\nfor LLMs are too difficult for models that were trained at smaller scales. Third, evaluating such datasets\nadds a considerable amount of complexity to the evaluation process. Lastly, downstream performance has\nbeen found to correlate strongly with pretraining perplexity (Raffel et al., 2020). For these reasons, in this\nwork, we only focus on the perplexity on held-out data.\n2.2\nExisting Benchmarks\nLanguage modelling has a rich history with a variety of benchmarks for model evaluation. Some notable\nexamples include Penn Treebank (PTB, Mikolov et al. (2010)), WikiText-2 (WT2, Merity et al. (2017)),\nWikiText-103 (Merity et al., 2017), enwik8 and enwik9 Mahoney (2011), and Project Gutenberg (PG19, Rae\net al. (2020)).\nThese datasets represent a broad range of sizes and complexity. The PTB and WT2 are tiny corpora with a\nlimited vocabulary and little variety. The enwik8 and enwik9 datasets are used for evaluating the performance\nof compression algorithms. They consist of the first 108 and 109 bytes of an English Wikipedia XML dump\nfrom 2006. With just 1 GB of text, models often train multiple epochs on these datasets and are prone\nto overfit on the training data. WikiText-103 was created in 2016 and contains about 100M tokens from\na fixed vocabulary of 103k different words resulting in about 515 MBs of data. It consists of preprocessed\nWikipedia articles and has been often used as an academic language modelling benchmark since then. The\nissue with Wikipedia articles is their relatively short size. The average length of a Wikipedia article is about\n3,600 words (approximately 4,300 tokens), limiting the length of long-term dependencies. The most recent\nand largest dataset is PG19. PG19 consists of 28,752 books with an average length of 69k tokens resulting\nin about 10 GB of data or about 2M training tokens when using a subword vocabulary of 32k. The PG19\ndataset is large enough to train models with billions of parameters. However, all books were published over\n100 years ago and thus don\u2019t reflect today\u2019s English language or diversity of topics.\nBesides, on previous benchmarks models were often compared simply based on the average loss or perplexity\non held-out data. While such comparisons offer insights, the best models are often also the most compute-\nintensive ones Brown et al. (2020). With the rise of well-funded industry labs, it has become increasingly\ndifficult for academic labs to do research at that scale. E.g., all publications which advance the state of the\n5\nart on PG19 are from Google or Google Deepmind with model sizes of up to 1.3B parameters (Hutchins\net al., 2022). Training such models requires dedicated servers with multiple state of the art accelerators\ntraining for several days just to reproduce the results.\nRecent work presents the idea of cramming experiments into a single day and a single consumer GPU\n(Geiping & Goldstein, 2023).\nIn Section 3, we will also advocate for a shift away from unconstrained\nperplexity comparisons. While experiments offer valuable insights, they do not adequately account for the\nscalability factor, a key element in training large language models. The Languini benchmark, in an effort\nto demonstrate scalability, compares models based on different amounts of accelerator hours, resulting in a\nscaling plot or scaling law (Kaplan et al., 2020; Hoffmann et al., 2022). This approach seeks to provide a fair\nand meaningful comparison of language modelling research at varying compute scales, thereby promoting\ninclusivity for research groups with limited funding resources.\n3\nThe Languini Books Benchmark\nThe Languini Books benchmark represents a notable shift from previous language modelling benchmarks.\nIt emphasizes reproducibility, scalability, and a comparison based on accelerator hours.\nBy focusing on\nthese aspects, Languini fosters a direct and effective comparison of different language models based on their\nperformance at different scales of computational resources, aligning closely with the practical reality of\ntraining and evaluating such models.\n3.1\nComparison based on Compute Class\nA critical component of the Languini benchmark involves the concept of a compute class. This measure\nrepresents the number of accelerator hours (both parallel and sequential) spent during the training of the\nmodel. It diverges from the convention of comparing models based on their number of parameters or the\ntotal number of floating point operations (FLOPs).\nThe number of parameters or total FLOPs are hardware-agnostic metrics. However, these measures fall short\nof capturing the actual computational efficiency of the evaluated algorithms. Two models with an identical\nnumber of parameters or FLOPs can exhibit vastly different performances due to the model\u2019s underlying\ndesign and its ability to exploit the hardware\u2019s capabilities.\nIn particular, these hardware-agnostic metrics fail to account for the parallelisability of a model. As ad-\nvancements in semiconductor technology, particularly in parallel computing and high-performance microar-\nchitecture, continue to reshape the industry, models that scale well with an increased number of parallel\nprocessors can vastly outperform others given the same amount of total FLOPs.\nOn the Languini benchmark, the evaluation requires the measure of normalised perplexity (see Section 2)\nat different levels of accelerator hours spent.\nWith accelerator hours increasing exponentially, this data\nserves to estimate the scaling law, helping researchers understand and extrapolate the trajectory of model\nperformance as computational resources are scaled up further. In practice, the number of accelerator hours\nused in this paper is not the actual training time but is calculated before training based on a specific model\u2019s\nthroughput (tokens per second) w.r.t. specific hardware. This increases flexibility as it allows the model\nto be trained on any hardware as long as the throughput is apriori measured w.r.t. the same reference\nhardware. The Languini codebase provides a script to measure the throughput of any PyTorch language\nmodel. Currently, this reference hardware is the Nvidia RTX 3090, chosen for its prevalence and accessibility\nin academic organisations and the compute classes considered in this work are 6, 12, 24, 48, and 96 hours.\nWe use the following software versions PyTorch 2.0.0, Triton 2.0.0, Nvidia driver 535, and CUDA version\n12.2.\nConsider an example where we have a specific model architecture with a given hyperparameter configuration\n(or config in short). We first evaluate its training throughput v (number of tokens per second) on our reference\nhardware using an untrained instance with the throughput script provided by the Languini codebase. The\nthroughput script uses the profiler of the DeepSpeed library (Rasley et al., 2020) to measure the time it takes\nto perform a forward pass, a backward pass, and a weight update for any PyTorch model. For a specific\n6\ncompute class given by h accelerator hours, we can calculate the total number of tokens T that we can\nprocess in that time: T = 3600vh. Given the total number of tokens T, we calculate the number of steps by\ndividing it by the number of tokens per batch which is batch size \u00d7 sequence length \u00d7 number of gradient\naccumulation steps.\nNote that because we measured the throughput before training we do not actually need to train our model\non our reference hardware.\nWe can train on any other or even multiple accelerators as long as we use\nthe same model config used to measure throughput. As we train the model we log the loss or normalised\nperplexity at certain training step intervals. To transform the learning curves from loss-over-steps into loss-\nover-accelerator-time we simply multiply the current step number by the number of tokens per step and\ndivide by the throughput of that model configuration on the reference hardware. This can be done during\nor after training.\nFurthermore, it is also possible to approximately convert the compute class of n hours on accelerator A into\nk hours on accelerator B through the total number of tokens T. This is because given a model M we can\nmeasure the throughput on the accelerators A and B and calculate the respective accelerator hours needed\nto consume the same number of tokens. E.g. training a specific GPT model for T tokens takes 6h on an\nRTX 3090 but training the same config on an A100 takes 2.5h. We find that this factor is roughly constant\nthroughout various scales of GPT models. Hence, future work may eventually move on to better hardware\nwithout the need to retrain all previous models. In Table 6 we included the accelerator hours for other\ncommon deep learning hardware that was available to us at the time.\nA limitation of this method is that certain models might perform better on new hardware. As a result, the\nperformance ratio between model X and model Y on hardware A might differ when tested on newer hardware\nB. Given the common use of GPUs to train models this is effectively already the case Hooker (2021). The use\nof a reference accelerator is mainly to enable effective compute constraints. Future researchers may decide\nto use different hardware for their evaluation. But for a fair comparison, previous work would have to be\nalso evaluated on that reference hardware.\n3.2\nThe Dataset\nThe Languini codebase is designed to support various datasets. In this work, we introduce the first dataset\ndubbed Languini Books. Languini Books is a filtered version from the popular books3 dataset, a subset of\nThe Pile (Gao et al., 2020) which has been used as training data for various LLMs (e.g. Scao et al. (2022);\nDey et al. (2023); Biderman et al. (2023); Touvron et al. (2023a;b)). The books3 dataset comprises a large\ncollection of published books, encompassing approximately 101 GB of data.\nWe remove all books which are shorter than roughly 50 KB as they mostly consist of boilerplate text and little\nto no content. We also remove all non-English books as there are too few for any reasonable multilingual\nlanguage modelling. To do so, we repeatedly sampled 200 bytes of text from each book and classify the\nlanguage using langdetect (Joulin et al., 2016b;a) until we either sampled 50 times or one language has\nachieved above 90% presence. We then remove all books where English is not the most common language\nand with more than 5 non-English samples. The only exception here are books used for the language learning\ndata split which we elaborate further in Section 3.2.1.\nWe tokenise all remaining books using a 32k SentencePiece model using BPE that was trained on the data\nof WikiText-103 (Merity et al., 2017). Through manual inspection, we find that books with relatively low\naverage bytes per token are often undesired books with large amounts of numerical values (e.g. food calorie\ntables, price guides), non-latex mathematics, books with little natural text (e.g. a collection of artworks\nwith titles, dates, author names, and auction prices, but obviously without images), or books with otherwise\nextensive unusual formatting (e.g. large number of lines for the reader to write down their own business\nplan). Upon manual inspection, we decided to remove all books with less than 3.2 average bytes per token.\nLastly, we train a Gensim Doc2Vec model (Rehurek & Sojka, 2011) to encode each book as a vector rep-\nresentation. We then use the cosine similarity measure to find exact and near duplicates. Previous work\nshowed that even simple deduplication methods can speed up training significantly (Tirumala et al., 2023).\nAfter extensive manual inspection, we decided to remove any books that have a cosine similarity of 0.87 or\n7\nhigher. This captures various duplicates and near duplicates such as new editions or books which have been\npublished again with slight differences (such as differences due to catering to British and American markets).\nThis step resulted in the removal of 5,514 or 3.36% of books.\nThe final dataset consists of 84.5 GB of text data across 158,577 books with a total of 23.9B tokens given the\nWikiText-trained vocabulary. Each book has on average 559 KB of text or about 150k tokens, and a median\nof 476 KB of text or 128k tokens. We plot a T-SNE projection of the vector representations of the languini\nbooks in Figure 2 to visualise the diversity of the data. Furthermore, we distribute a list of filenames and a\nscript with which the processed data can be extracted from the official books3 dataset.\nFigure 1: Distribution of book lengths (in bytes) of the Languini Books dataset.\n3.2.1\nEvaluation and Test Sets\nFrom the Languini Books data, we remove various books for evaluation purposes. This includes a standard\ni.i.d. test set with 80 books sampled at random. Furthermore, we create several out of distribution test sets\nto measure a model\u2019s ability to capture long dependencies and learn during inference through e.g. in-context\nlearning (Dong et al., 2022; Kirsch et al., 2022), dynamic evaluation Krause et al. (2018), or meta-learning\n(Irie et al., 2022; Kirsch & Schmidhuber, 2021). We split these test sets into the following categories: French\nLanguage Learning, Discworld, Java, Statistics, and Woodworking. The size of each of these sets is shown\nin Table 1.\nSplit\nTopic\nBooks\nBytes\nTokens\nBytes per Token\nlanglearn\nFrench Language Learning\n34\n16,571,748\n6,582,737\n2.52\ndiscworld\nDiscworld Series\n45\n24,095,020\n6,944,831\n3.47\njava\nJava Programming\n109\n108,747,871\n30,818,604\n3.53\nstats\nStatistics\n43\n30,266,165\n8,283,405\n3.65\nwood\nWoodworking\n19\n7,846,725\n2,146,089\n3.67\nTable 1: Size and topics of the books from every out of distribution split. Tokens and bytes per token are\nmeasured using WikiText tokenisation from Section 4.1.2.\nFrench Language Learning\nThis dataset tests a model\u2019s ability to generalize to an unseen language\nunder a curriculum. If the model is able to generalize online well, it should perform increasingly well on each\nsample in this dataset as it progresses through them. This ordered dataset consists of 17 French learning\nbooks with English text followed by 17 pure French books. Each subset is roughly ordered according to the\nperceived difficulty it would pose to a language model trained only on English text. As most books with\n8\nFigure 2: T-SNE plot of the learned vector representation for each book in the Languini Books dataset.\nUnder a small Doc2Vec model, the books cluster into semantic concepts. The plot gives an impression of\nthe large variety of books due to the large number of small clusters.\nnon-English content were removed in the early preprocessing of the data, this dataset was constructed from\na subset of the removed books. The French learning books were further selected for having a good balance\nof French and English tokens as well as having the word \u201cFrench\u201d in their title. The pure French books were\narbitrarily taken from books that exclusively contained French tokens. Additional curation of the dataset\nwas done heuristically. The dataset was ordered heuristically using titles and token counts as guides.\nDiscworld\nThe Discworld dataset consists of the available novels of Terry Pratchett set in the Discworld\nfantasy universe.\nThere are 45 books in this dataset, with only 6 books missing from the main series.\nBooks are ordered chronologically with the 35 available books in the main series first, then the 4 Science of\nDiscworld books, and finally, the remaining 6 found books ordered arbitrarily. As the books have recurring\ncharacters, places, or themes, a language model able to generalize online well should perform increasingly\nwell on each sample of the dataset and should do markedly worse on this dataset than it would on many\nother books in the principle datasets. As early processing of the dataset filtered similar books out of the\ndataset already, this dataset was constructed by searching for relevant keywords in the filenames of the books\n(i.e., \u201cpratchett\u201d, \u201cdiscworld\u201d, \u201cdiskworld\u201d, \u201cjosh kidby\u201d, or \u201cpaul kidby\u201d).\nJava\nThis Java dataset focuses on the Java programming language. In total, there are 109 books in this\ndataset covering a wide variety of applications. There is no specific ordering for this dataset; however, a\nlanguage model able to generalize online well should perform increasingly well, in expectation, as it processes\nsubsequent samples of this dataset. This dataset was created by searching through all the books that contain\nboth the string \u201cjava\u201d and the string \u201cpublic static\u201d (the most common type declaration in java) anywhere\nin their full text. Through an analysis of the titles of the found books, books deemed to not be regarding\nJava or a library using Java were removed from all datasets.\n9\nStatistics\nThis dataset tests a model\u2019s ability to understand statistics while having little to no previous\nlearning on the subject. It consists of 44 books. As with the Java dataset, there is no specific ordering to\nthis dataset, but, in expectation, a model able to generalize online well should perform increasingly well as\nit processes subsequent samples of this dataset. This dataset was created by searching through the titles of\nbooks to find all that contained either the word \u201cprobability\u201d or \u201cstatistics\u201d. An introductory textbook on\nstatistics should most likely contain either of these terms. Subsequent filtering was done by hand.\nWoodworking\nThis is a simple dataset consisting of only books related to woodworking projects. There\nare a total of 19 books in this dataset.\nThere is no specific ordering to this dataset, but, as with the\nother datasets, a model able to generalize well online should, in expectation, perform increasingly well as\nit processes subsequent samples of this dataset. This dataset was created by searching for all book titles\ncontaining \u201cwoodworking\u201d or some variation of it. Project books with woodworking will mostly contain this\nword. Subsequent filtering was done by hand. Note that some of the books in this dataset use images to\nconvey some information which is not available in the dataset.\n4\nThe Baselines\nIn language modelling research, setting appropriate baselines is fundamental. Baselines offer a standard\npoint of reference, facilitating the comparative analysis of new models or methodologies. For this study, we\nhave selected two distinct architectures. The first is the widely-used GPT model, a highly parallelisable feed-\nforward architecture, for which we will conduct an in-depth performance analysis. The second is a recurrent\narchitecture derived from the LSTM (Hochreiter & Schmidhuber, 1997). This section aims to detail these\nbaseline models and their results. But before that, we will discuss tokenisation in Section 4.1.\n4.1\nTokenisation Analysis\nTokenisation is a fundamental step in language modelling, aiming to convert input text into a sequence of\ntokens that a model can process. A common standard for tokenisation is currently the use of SentencePiece\nmodels (Kudo & Richardson, 2018) with Byte-Pair Encoding (BPE, Gage (1994); Sennrich et al. (2015)).\nThese tokenisation models find the most frequent pairs of bytes in a text and repeatedly merge them to form\ntokens. Including all possible bytes as part of the initial vocabulary allows the tokeniser to encode any text\nand thus handle even words that have not been seen during training.\nExisting LLMs utilise vocabularies of varying sizes. These range from thousands of tokens to over 200,000\ntokens. Larger vocabularies are often used for large multi-lingual models such as GPT-3 (Brown et al., 2020),\nPaLM (Chowdhery et al., 2022), or Megatron-LM (Shoeybi et al., 2019). In this subsection, we delve into\nsome of the challenges and intricacies of using a BPE tokeniser and analyse the performance implications of\ndifferent vocabulary sizes.\n4.1.1\nAnalysing SentencePiece Vocabularies\nIn this subsection we present our analysis of various SentencePiece models which were trained on the training\nsplit of the Languini Books data. We analyse vocabularies from 2,048 to 131,072 unique tokens. The analysis\nreveals various shortcomings of the produced byte-pair encoding vocabularies which may be addressed in\nfuture work.\nSentencePiece constructs duplicate tokens that are never used.\nIn our experiments, we observed\nthat SentencePiece tokeniser generates a number of tokens that have identical string decoding. For example,\none such token is \u201cb\\xef\\xbf\\xbd\u201d which is the UTF-8 encoding for the special character U+FFFD also\nknown as the \u201creplacement character\u201d. There are 128 different tokens that all decode to this character.\nApart from this special character, there are 80 other tokens which we found to have duplicates mapping into\nthe same string representation. These tokens typically represent single characters, such as lower- and upper-\ncase letters, punctuation characters and special characters such as \u201c#\u201d, \u201c/\u201d, \u201c_\u201d etc. The two duplicate\ntoken groups are grouped together in terms of the order in the vocabulary. Interestingly, one group is at the\n10\n\u201cbeginning\u201d of the vocabulary, with token values below 200, whereas the other group is at the \u201cend\u201d of the\nvocabulary. Only one of these groups of tokens is actually used by the tokeniser to tokenise the text, ensuring\nthat the tokenisation encoding-decoding process is deterministic. However, this phenomenon is wasteful, as\nin total there are 207 duplicate tokens that would ideally be used to encode other (sub-)words. We observed\nthe same duplicate tokens across all vocabulary sizes we investigated and also across all data sets we used\nto extract these vocabularies.\nSentencePiece constructs near duplicates which make up 24.9% of the vocabulary.\nSentence-\nPiece by default encodes semantically identical words into different tokens, for example, we observed that\n\u201cthe\u201d, \u201cThe\u201d, \u201c the\u201d, \u201c The\u201d, \u201cTHE\u201d and \u201c THE\u201d all get assigned to different tokens. See Table 2 for further\nexamples. Making all token string representations lowercase and removing whitespace or punctuation marks\nwe found that there are 8,160 duplicate tokens in a vocabulary of 32,768. This does not include further\nsimilarities such as the genus, numerus, and kasus of different words. It is likely counterproductive to have\nseparate tokens for such semantically similar strings because improving the token representation of one token\ndoes not translate to improvements in other tokens.\nsubword\n+space\n+case\n+space, +case\n+caps\n+space, +caps\n\u201cof\u201d\n\u201c of\u201d\n\u201cOf\u201d\n\u201c Of\u201d\n\u201cOF\u201d\n-\n\u201cent\u201d\n\u201c ent\u201d\n\u201cEnt\u201d\n\u201c Ent\u201d\n\u201cENT\u201d\n-\n\u201cwas\u201d\n\u201c was\u201d\n\u201cWas\u201d\n\u201c Was\u201d\n-\n\u201c WAS\u201d\n\u201cnot\u201d\n\u201c not\u201d\n\u201cNot\u201d\n\u201c Not\u201d\n\u201cNOT\u201d,\n-\n\u201cfrom\u201d\n\u201c from\u201d\n\u201cFrom\u201d\n\u201c From\u201d\n\u201cFROM\u201d\n-\n\u201chouse\u201d\n\u201c house\u201d\n-\n\u201c House\u201d\n-\n-\n\u201clove\u201d\n\u201c love\u201d\n\u201cLove\u201d\n\u201c Love\u201d\n-\n-\n\u201cchapter\u201d\n\u201c chapter\u201d\n\u201cChapter\u201d\n\u201c Chapter\u201d\n\u201cCHAPTER\u201d\n\u201c CHAPTER\u201d\nTable 2: Further examples of vocabulary entries explained by simple transformations.\nA\nBPE\nvocabulary\nconstructed\nfrom\nrandomly\nsampled\nLanguini\nBooks\ncontains 63% tokens which are identical\nwith\na\nvocabulary\nconstructed\nfrom\nEnglish\nWikipedia.\nWhen creating the Languini Books dataset (by filtering the books3 dataset) we con-\nstructed a BPE vocabulary of size 32,768 from the WikiText-103 dataset (Merity et al., 2017). We then\nconstructed a BPE vocabulary of the same size by training on the Languini Books dataset. Due to the\ndifferent nature of text contained in books compared to Wikipedia articles, we expected these vocabularies\nto have large differences, and the one trained on the Languini Books dataset to offer advantages for language\nmodelling on this dataset. To our surprise, we found that the two vocabularies share 20,771 or 63% of all\ntokens.\nThe frequency of the tokens follows approximately a Zipfian distribution.\nNatural language has\nthe property that the frequency of a word is approximately proportional to one over its rank when ordered\n(Zipf, 2013). We find that BPE vocabs closely follow this distribution. In Figure 3, we compare the frequency\nof tokens over their rank in the training data for vocabulary sizes ranging from 2,048 to 131,072. In this\nlog-log plot, we normalised the rank of the token to the range [0, 100] which means that a point on the x-axis\nshould be interpreted as a percentage of all tokens since smaller vocabularies are rescaled to fit into the same\nplot with larger vocabularies. The sharp drop to the right is due to rare and unused tokens as discussed\nabove. While it is probably advantageous that the learned vocabulary follows the same distribution as natural\nlanguage text, this may also be a disadvantage because the number of training steps that are performed on\na specific token is directly proportional to its frequency. We further note that a larger vocabulary follows\nthe same trend and only reduces the frequency of each token. Thus, a large vocabulary may be undesirable\nbecause it \u201callocates\u201d significantly fewer training steps for a significant number of tokens.\nUnigram tokenisation results in an almost identical frequency distribution.\nIn Figure 4 we\ncompare vocabularies of size 32,768 encoded with either the BPE or unigram algorithm. The data suggests\nthat unigram tokenisation results in a similar distribution except for the last 10% of the tokens where it\n11\n10\n3\n10\n2\n10\n1\n100\n101\n102\nVocabulary percentage (tokens sorted and scaled)\n103\n104\n105\n106\n107\n108\n109\nFrequency of apperance\nFrequency of appearance of tokens from vocabularies with different sizes\nVocab. size\n2,048\n4,096\n8,192\n16,384\n32,768\n65,536\n131,072\nFigure 3: Token frequency sorted and scaled such that its rank (x-axis) lies within 0 and 100. The dashed\nline is the ideal Zipfian distribution scaled to fit the vocabulary with 16,384 unique tokens.\nappears to be more equally distributed compared to BPE. When trained, the language modelling performance\nbetween a unigram and BPE tokenisation with a vocabulary of 32,768 unique tokens resulted in no significant\nperformance difference.\n100\n101\n102\n103\n104\nToken index (sorted)\n104\n105\n106\n107\n108\n109\nFrequency of apperance\nFrequency of appearance of tokens\nTokenizer\nBPE\nUnigram\nFigure 4: Comparison of the sorted token frequency for a vocabulary due to byte-pair and unigram tokeni-\nsation.\nThe vocabulary distribution is uniform across the dataset.\nIn Figure 5, we compare the frequency\ndistribution of 5 randomly sampled books with the distribution across 200 books. We find that the order of\nthe tokens changes but the overall distribution is approximately identical.\n12\n100\n101\n102\n103\n104\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nSource\n200 books\nrandom book 1\nrandom book 2\nrandom book 3\nrandom book 4\nFigure 5: Comparison of the frequency distribution of 5 randomly sampled books against the distribution\nover 200 randomly chosen books.\n4.1.2\nPerformance Comparison of Different Vocabulary Sizes\nLanguage models face the trade-off between tokenisation granularity and model complexity.\nA smaller\nvocabulary typically implies a higher granularity of tokenisation which in turn increases the total number of\ntokens to be processed. Conversely, a larger vocabulary results in a shorter training sequence as each token\ntends to represent more bytes.\nA larger vocabulary is particularly useful for feed-forward models due to their finite context.\nA large\nvocabulary captures more bytes of raw text per tokens, thus the feed-forward model can condition on more\ntext while processing the same number of tokens.\nHowever, larger vocabularies increase the number of\nparameters and reduce the throughput. For the purpose of Languini, the ideal vocabulary size strikes a\nbalance between computational efficiency and model perplexity.\nIn this section, we empirically explore the effects of different vocabulary sizes on normalised perplexity.\nTo study the impact of vocabulary size, we kept the model architecture (up to the embedding) and optimi-\nsation fixed and only change the granularity of tokenisation. We trained a GPT tiny model with a batch\nsize of 160 for 138k steps across seven different vocabulary sizes from 2048 until 131,072. We present our\nresults in Figure 6.\nThe findings from our comparison highlight a clear trade-off between vocabulary size and computational\nefficiency. Increasing the size of the vocabulary improves performance but has diminishing gains and sig-\nnificantly slow down the model (see Table 3). Our results indicate that a 16k and 32k vocabulary strikes a\ngood balance. They offer improved performance without an excessive increase in computational costs. Since\nmost models in languini will be on the smaller side, we decided on using a 16k vocabulary for the remaining\nexperiments.\n4.2\nThe Feed-Forward Baseline\nWith the decoder-only Transformer, the feed-forward approach has emerged as the new paradigm in language\nmodelling. Its advantage is the processing and prediction of all sequence elements in a parallel manner.\nHowever, despite the theoretical ability to process longer sequences, the decoder-only Transformer model\nstruggles to generalise to sequences that are significantly longer than what it has been trained with (Press\net al., 2022). Although recent work has made progress in this regard (Luo et al., 2022).\n13\nvocabulary size\nbytes per token\ntokens per second\n2,048\n2.84\n183,065\n4,096\n3.21\n174,568\n8,192\n3.54\n163,448\n16,384\n3.81\n141,434\n32,768\n4.02\n112,855\n65,536\n4.17\n78,778\n131,072\n4.25\n45,742\nTable 3: Best throughput of a GPT tiny model with different vocabulary sizes.\n104\n105\nsteps\n2.30\n2.35\n2.40\n2.45\n2.50\n2.55\n2.60\nnormalised ppl\n(a) Normalised perplexity over steps\n104\n105\n106\naccelerator seconds\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\n2.55\n2.60\nnormalised ppl\n2048\n4096\n8192\n16384\n131072\n32768\n65536\n(b) Normalised perplexity over accelerator seconds\nFigure 6: Fast evaluation of normalised perplexity on held-out data for a GPT tiny model trained with a batch\nsize of 160 for 138k steps across seven different SentencePiece BPE vocabulary sizes. Larger vocabularies\nrequire more memory and FLOPs to run which leads to different amounts of accelerator time.\nThe feature of the autoregressive decoder-only Transformer model to scale well with parallel compute allowed\nthe scaling to hundreds of billions of parameters (Brown et al., 2020). Our implementation is based on the\nofficial TensorFlow implementation of GPT-2 (Radford et al., 2019; Karpathy, 2023).\nWithin our implementation, we provide configurations, i.e., hyperparameter settings, for different model sizes.\nOur configurations follow Radford et al. (2019) with ranges from 110M parameters to 1.5B parameters. To\nwiden the scale of models to the lower end, we have included two additional sizes, namely, mini and tiny.\nAll models are trained with a BPE vocabulary of 16,384 unique tokens as described in Section 4.1.\nGPT Model\ngigaFLOPs\nParams\ndmodel\nnlayers\nnheads\ndhead\nmini\n20.4\n27.6M\n512\n4\n8\n32\ntiny\n45.1\n53.9M\n768\n4\n12\n64\nsmall\n109.6\n110.6M\n768\n12\n12\n64\nmedium\n352.4\n336.4M\n1024\n12\n16\n64\nlarge\n760.5\n731.1M\n1536\n24\n16\n96\nXL\n1,555.2\n1,478.2M\n2048\n24\n24\n128\nTable 4: Overview of our GPT model sizes, flops, parameters, and differing hyperparameters. Sizes are\nchosen such that they are comparable in size and flops with the GPT models. FLOPs are the total number\nof floating point operations during the forward pass with a batch size of 1 as measured by the DeepSpeed\nflops profiler.\nThe decoder-only transformer architecture consists of multiple transformer layers. Each layer consists of two\nsublayers: The self-attention and position-wise multi-layer perception (MLP). The self-attention mechanism\nallows the model to weigh the significance of the proceeding tokens in the input sequence relative to the\ncurrent token. Using multi-head attention, it can capture various types of relationships in the data. The\n14\nmini\ntiny\nsmall\nmedium\nlarge\nXL\nmodel size\n0\n100000\n200000\n300000\n400000\n500000\ntokens per second\nNVIDIA A100-SXM4-80GB\nNVIDIA GeForce RTX 3090\nTesla V100-SXM2-32GB-LS\nTesla P100-SXM2-16GB\nFigure 7: Maximum throughput (tokens per second) achieved on various accelerators for all GPT model\nsizes.\nposition-wise MLP sublayer is applied next. It consists of 2 linear maps with the ReLU non-linearity applied\nin between. Typically, the middle activations of the MLP are four times the size of the hidden representation.\nBoth sublayers are connected residually (Srivastava et al., 2015; Hochreiter, 1991) and are preceded by layer\nnormalisation (Ba et al., 2016). We add a learned position embedding to each token before the first layer.\nOur implementation leverages PyTorch\u2019s support for mixed precision training (Huang et al., 2020). In this\nmode, suitable operations (e.g. matrix multiplication) are executed with float16, leveraging tensor cores,\nwhile parameters for which more precision are kept in float32 which is crucial. The conversions are done\nautomatically. This not only enhances training speed but also ensures stability. Additionally, we leverage the\ncompile functionality of PyTorch 2.0.0 and Triton 2.0.0 (Wu, 2023) to further improve a model\u2019s throughput.\nAll models are trained with the Adam optimiser and with a cosine learning rate decay schedule which decays\nthe starting learning rate of 0.0006 by a factor of 100 over the total number of training steps.\nIt\u2019s worth mentioning that our experiments did not incorporate the recently published flash attention mech-\nanism (Dao et al., 2022), despite its growing popularity. In our experiments, the use of flash attention results\nin significant speedups but coincides with unexpected and large gradient spikes when used on our reference\nhardware (Nvidia\u2019s RTX 3090), hindering the model\u2019s ability to recover and continue training. In contrast,\nour native PyTorch implementation displayed stability throughout the entire training process of all experi-\nments. The issue seemingly lies with certain hardware requirements of flash attention (Dao, 2023). We leave\nit for future work to provide an in-depth experimental evaluation of flash attention within the constraints of\nthe Languini Books benchmark.\nTo calculate the affordable number of training steps of every GPT model for various compute classes,\nwe measured their throughput w.r.t. our reference hardware. For future work, we also documented the\nthroughput of all our GPT models on other accelerators here and in the GPT project folder of the GitHub\nrepository.\n4.2.1\nEvaluation\nWe evaluate the GPT models using a fast and slow evaluation. To track performance (average loss, normalised\nperplexity, or others) during training, we evaluate the models on the held-out data for just 500 batches, using\na batch size of 16, the default sequence length of 512 tokens, and, most crucially, by averaging the loss across\nall predictions. This is a low estimate of the actual performance of the model because the context for every\nprediction varies from 1 to 512 tokens and predictions with less context perform significantly worse (Kaplan\net al., 2020). Ideally, we would measure performance with a batch size of 1 and such that each token has the\nlargest possible context. However, due to the nature of our implementation and the use of learned position\nembeddings, this would require us to recompute all previous tokens for every prediction.\n15\nGPT Model\nRTX 3090\nP100-16GB\nV100-32GB\nA100-80GB\nmini\n238,719 (97)\n40,292 (56)\n194,130 (320)\n562,594 (320)\ntiny\n141,864 (76)\n21,452 (40)\n108,221 (104)\n343,182 (272)\nsmall\n55,416 (34)\n8,310 (16)\n43,839 (50)\n135,107 (128)\nmedium\n16,618 (10)\n2,503 (3)\n14,387 (17)\n47,851 (53)\nlarge\n7,058 (4)\nOOM\n7,296 (9)\n27,713 (36)\nXL\nOOM\nOOM\nOOM\n14,559 (20)\nTable 5: Overview of the best tokens per second measures and their respective batch sizes in brackets for\ndifferent GPT model sizes on several accelerators. OOM stands for out of memory.\nWe choose a middle ground where we evaluate over the last 128 tokens using a batch size of 1. We refer\nto this evaluation as the slow evaluation because it requires the processing of four times as many batches.\nIn practice, We found that the normalised perplexity of our models does not significantly improve when\nevaluating over the last 64 or the last 32 tokens in the sequence but are exponentially more expensive. Thus,\nthe slow evaluation of all our GPT models is only evaluating the last 128 predictions of the sequence which\nresults in a context length between 384 and 512 tokens. Consequently, during evaluation, we slide our batches\nin the temporal direction not by the sequence length of 512 tokens but by 128 tokens which requires us to\nprocess four times as many batches.\n4.2.2\nResults\nGiven a specific model size, its ideal throughput, and compute class, we calculate the total number of tokens\nwe can process in that time. In Figure 8, we present the fast evaluation results (i.e. loss averaged over all\ntokens of a batch instead of only the over the last n predictions) for the trade-off between batch size and\nnumber of training steps for all model sizes. Note that larger models have slower throughputs which means\nthat they process fewer tokens given the same compute class.\nOur results indicate that the compute-optimal batch size for all GPT models lies roughly between 256 and\n512 elements. All models are trained with a sequence length of 512 which results in the optimal number of\ntokens per batch to be roughly between 131k and 262k. As seen in Figure 8, the compute-optimal batch size\nseems to increase slightly as the same size model is trained for longer. This may indicate that the compute-\noptimal batch size is not constant throughout training but increases as previous work suggests (Smith et al.,\n2017; McCandlish et al., 2018; Shallue et al., 2018; Zhang et al., 2019). This insight is also supported by\nthe common strategy of increasing the batch size towards the end of a long training run when training large\nlanguage models. We leave it for future work to propose strategies for adaptive batch sizes and how they\nrelate to the common practice of decaying the learning rate.\nIn Table 6 we summarise the normalised perplexity results for each compute class by evaluating the last\n128 tokens of each batch as elaborated in Section 4.2.1 (unlike the results in Figure 6 and 8 which plot the\nfast evaluation). Note, that the hours on our reference hardware can be converted into hours on any other\nhardware through the total number of tokens (see Section 3.1 for further details). Using the normalised\nperplexity at different scales, we create a scale plot over accelerator seconds and FLOPs in Figure 9. As\nexpected, perplexity over accelerator time closely follows a power law.\nWe also evaluate the best GPT models for each scale on the out of distribution splits. The results are listed\nin Table 11 below together with the evaluation of the qLSTM model of Section 4.3.\n4.3\nThe Recurrent Baseline\nRecurrent Neural Networks (RNNs), particularly the Long Short-Term Memory (LSTM; Hochreiter &\nSchmidhuber (1997)), have been a cornerstone in the development and evolution of deep learning for se-\nquential data. RNNs are universal function approximators, i.e. they can be seen as general computers with\nfinite state that can process arbitrary long inputs with constant memory and compute which is linearly\nproportional to the length of the sequence (Siegelmann & Sontag, 1991; Siegelmann, 1996; Schmidhuber,\n16\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\n(a) GPT mini\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\ncompute class\n6h\n12h\n24h\n48h\n96h\nbatch size\n32\n64\n128\n256\n512\n1024\n2048\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\n(b) GPT tiny\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\n(c) GPT small\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\n(d) GPT medium\n104\n105\n106\nstep\n2.10\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\nnormalised perplexity\n(e) GPT large\nFigure 8: Fast evaluation of normalised perplexity on held-out data for GPT models of different sizes trained\nin different compute classes with different trade-offs between batch size and number of training steps. All\nmodels seem to have a similar compute-optimal batch size despite large differences in model size.\n1990). In contrast, the transformer model from Section 4.2, while also Turing complete (P\u00e9rez et al., 2019),\nrequires a quadratic increase in compute and memory, and, in practice, cannot effectively process arbitrary\nlong sequences. Furthermore, recurrence can be an advantageous bias for sequence models which enables\nthem to generalise to out of distribution settings in more systematic ways (e.g. see Anil et al. (2022)).\nOn language modelling benchmarks, however, transformer models have outperformed RNNs. The inherent\nsequential computation in RNNs limits their parallel processing capabilities, something easily achieved by\nattention models. Unlike RNNs which compute a vector representation step by step, attention models do so\nby attending to the entire sequence at once. This makes it more resource-intensive but allows for time-based\n17\ncompute\nnormalised\nconfig\nbatch size\ntotal\ntotal\n\u2217theoretical\n\u2217theoretical\nclass\nperplexity\ntrain tokens\nexaFLOPs\nA100 hours\nV100 hours\n6h\n2.262\nsmall\n128\n1.2B\n0.769\n2.46\n7.58\n12h\n2.197\nsmall\n128\n2.4B\n1.538\n4.92\n15.17\n24h\n2.146\nsmall\n256\n4.8B\n3.075\n9.84\n30.34\n48h\n2.087\nmedium\n256\n2.9B\n5.930\n16.67\n55.44\n96h\n2.032\nmedium\n256\n5.7B\n11.859\n33.34\n110.89\nTable 6: Average normalised perplexity results evaluated with a context of 384 or more of the overall best\nGPT runs of each compute class (based on the slow eval). The hours in each compute class are w.r.t. the\nideal RTX 3090 throughput as measured by the Languini throughput script. From the hours and throughput,\nwe calculate the total number of tokens to be processed during training. The total number of floating point\noperations is calculated by (total train tokens/sequence length) \u00d7 forward flops of the model \u00d7 3. As in\nprevious work, we estimate the FLOP count of the backward pass to be double the forward pass (Kaplan\net al., 2020).\n\u2217Given the total number of tokens we can compute the ideal accelerator hours for other\nhardware based on the throughput of the same model config.\nparallel processing. Consequently, attention models can fully leverage the parallel processing capabilities of\ntoday\u2019s accelerators.\nThere has been previous work on enhancing the parallel processing abilities of RNNs. One intriguing direction\nis the quasi-RNN (qRNN, Bradbury et al. (2016)). Typically, a qRNN has a recurrent function that can\nalso be processed in parallel. To make this possible, the recurrent weight matrix is often removed. In this\nsection, we introduce the quasi-LSTM (qLSTM), a qRNN which achieves significantly higher throughputs\nwhile staying as true to the original LSTM as possible. While the presented qLSTM still lags behind our\nGPT baseline in throughput, we find that its compute-optimal batch size is significantly smaller, and it\nalso achieves a larger gain in perplexity while processing the same number of tokens. In comparison with\nthe GPT baseline, the data efficiency counterbalances its relatively reduced throughput, allowing for a\nmeasurable performance on the Languini Books benchmark.\n4.3.1\nThe Model\nThe qLSTM is a variation of the LSTM which is why we will first describe the LSTM model. Our LSTM\nmodel uses the same architecture as the Transformer baseline from Section 4.2 where the only difference is\nthe multi-head attention sublayer which we replace with a multi-head LSTM cell. Analogous to the multi-\nhead attention, the multi-head LSTM cell splits the LSTM cell into multiple heads which perform the same\noperation in a lower dimensional space. The following equations describe the classic LSTM cell adapted for\none head:\nft = \u03c3(Wfxt + Ufht\u22121 + bf)\n(11)\nit = \u03c3(Wixt + Uiht\u22121 + bi)\n(12)\nzt = \u03d5(Wzxt + Uzht\u22121 + bz)\n(13)\not = \u03c3(Woxt + Uoht\u22121 + bo)\n(14)\nct = ct\u22121 \u2299 ft + it \u2299 zt\n(15)\nht = ot \u2299 \u03d5(ct)\n(16)\nx\u2032\nt = Whht\n(17)\nwhere W \u2208 Rdhead\u00d7dmodel are the feed-forward weight matrices and U \u2208 Rdhead\u00d7dhead are the recurrent weight\nmatrices of this head, b \u2208 Rdhead are bias vectors, xt \u2208 Rdmodel is the hidden representation of step t of the\ncurrent layer, ht\u22121 \u2208 Rdhead is the state representation of this head for step t\u22121, \u03c3 is the sigmoid function, \u03d5\nis the tanh function, \u2299 is the Hadaramard product or element-wise multiplication of two tensors, c \u2208 Rdhead\n18\n100\n101\n102\n103\naccelerator hours\n1.90\n2.00\n2.10\n2.20\n2.30\n2.40\n2.50\n2.60\nnormalised ppl\n(a) Slope: \u22120.082 Intercept: 2.406\n1017\n1018\n1019\n1020\n1021\nflops\n1.70\n1.80\n1.90\n2.00\n2.10\n2.20\n2.30\n2.40\n2.50\nnormalised ppl\nlabel\nbest 6h config\nbest 12h config\nbest 24h config\nbest 48h config\nbest 96h config\neval\nfast eval\nslow eval\n(b) Slope: \u22120.084 Intercept: 5.737\nFigure 9: Scale plot of the best GPT configs evaluated on the test split. Fast eval considered predictions over\ntokens in the sequence. Slow eval evaluates only on the last 128 predictions which increases the minimum\ncontext per token from 1 to 384. Top: normalised perplexity over accelerator seconds. Vertical dashed lines\nare the different compute classes starting from 6h. Bottom: normalised perplexity over giga FLOPs. Like\nKaplan et al. (2020), we estimate the FLOPs of the backward pass to be two times the forward pass.\nis the cell state of this head, Wh \u2208 Rdmodel\u00d7dhead is the projection back to the embedding size dmodel, and x\u2032\nt\nis the output of the LSTM sublayer.\nDespite the increased parallel structure of the muli-head LSTM, each head performs multiple small matrix\nmultiplications for every step in the sequence. With large backpropagation spans, such as the 512 steps we\ndo in all our experiments, this results in significant sequential computation and drops in the utilisation of the\n19\naccelerator hardware. By dropping the recurrent weights U and the dependency of the gates on the previous\nstate ht\u22121 we further increase the parallelisability and arrive at our multi-head qLSTM formulation:\nft = \u03c3(Wfxt + bf)\n(18)\nit = \u03c3(Wixt + bi)\n(19)\nzt = \u03d5(Wzxt + bz)\n(20)\not = \u03c3(Woxt + bo)\n(21)\nct = ct\u22121 \u2299 ft + it \u2299 zt\n(22)\nht = ot \u2299 \u03d5(ct)\n(23)\nx\u2032\nt = Whht\n(24)\nNote that the only sequential operation that remains is an element-wise linear map:\nct = ct\u22121 \u2299 ft + ut\n(25)\nwhere we summarised it \u2299 zt into the update vector ut \u2208 Rdhead.\nA parallel implementation of recurrence.\nThe sequential part of the qLSTM in Eq.\n25 can be\nexpanded over 4 steps of the sequence as follows.\nct = ct\u22121 \u2299 ft + ut\n(26)\nct = (ct\u22122 \u2299 ft\u22121 + ut\u22121) \u2299 ft + ut\n(27)\nct = ((ct\u22123 \u2299 ft\u22122 + ut\u22122) \u2299 ft\u22121 + ut\u22121) \u2299 ft + ut\n(28)\nct = (((ct\u22124 \u2299 ft\u22123 + ut\u22123) \u2299 ft\u22121 + ut\u22121) \u2299 ft\u22121 + ut\u22121) \u2299 ft + ut\n(29)\nct =\nct\u22124 \u2299 ft\u22123 \u2299 ft\u22122 \u2299 ft\u22121 \u2299 ft\n+ ut\u22123 \u2299 ft\u22122 \u2299 ft\u22121 \u2299 ft\n+ ut\u22122 \u2299 ft\u22121 \u2299 ft\n+ ut\u22121 \u2299 ft\n+ ut\n(30)\nWe can rewrite Eq. 30:\n[ct]j =\nX\ni\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nct\u22124\nut\u22123\nut\u22122\nut\u22121\nut\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ni,j\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nft\u22123 \u2299 ft\u22122 \u2299 ft\u22121 \u2299 ft\nft\u22122 \u2299 ft\u22121 \u2299 ft\nft\u22121 \u2299 ft\nft\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ni,j\n(31)\n(32)\nOr more generally, we can describe a tensor F \u2208 R4\u00d75\u00d7dhead which consists of the following matrix of vectors\nF =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nft\u22123\n1\n0\n0\n0\nft\u22123 \u2299 ft\u22122\nft\u22122\n1\n0\n0\nft\u22123 \u2299 ft\u22122 \u2299 ft\u22121\nft\u22122 \u2299 ft\u22121\nft\u22121\n1\n0\nft\u22123 \u2299 ft\u22122 \u2299 ft\u22121 \u2299 ft\nft\u22122 \u2299 ft\u22121 \u2299 ft\nft\u22121 \u2299 ft\nft\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n(33)\nsuch that\n20\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nct\u22123\nct\u22122\nct\u22121\nct\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nk,j\n=\nX\ni\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nct\u22124\nut\u22123\nut\u22122\nut\u22121\nut\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ni,j\nFk,i,j\n(34)\nwhich allows for the parallel computation of all cell states. Given all cell states, we can then compute the\nsublayer outputs for each step simultaneously using Eq. 23 and 24.\nCentral to this approach is the tensor F. In theory, F, or parts of it, can be computed efficiently in linear\ntime w.r.t. the sequence length using a parallel prefix sum algorithm. Although such an algorithm exists in\nthe CUDA framework, it is currently not available in PyTorch.\nNevertheless, we observe that computing F in native PyTorch for a few tokens can significantly improve the\nmodel\u2019s hardware utilisation and throughput. We refer to the tokens that we process in parallel as blocks\nand find that certain block lengths dramatically increase the throughput of our qLSTM models.\nModel\nnheads\ndhead\nblock length\ntokens per second\nimplementation\nLSTM small\n1\n768\n-\n1,462\nminimal for-loop\nLSTM small\n12\n64\n-\n1,464\nminimal for-loop\nqLSTM small\n1\n768\n-\n4,499\nminimal for-loop\nqLSTM small\n12\n64\n-\n4,494\nminimal for-loop\nqLSTM small\n1\n768\n1\n2,235\nblock-parallel\nqLSTM small\n12\n64\n1\n2,352\nblock-parallel\nqLSTM small\n12\n64\n16\n11,143\nblock-parallel\nqLSTM small\n12\n64\n32\n8,638\nblock-parallel\nTable 7: The best throughputs achieved on the RTX 3090 for the LSTM and quasi-LSTM with different\nimplementations, block lengths and number of heads. The minimal for-loop implementation computes Eq.\n25 in sequence whereas the block-parallel implementation uses Eq. 34 to compute all tokens within a block\nin parallel. Increasing the block length results in higher throughput and higher hardware utilisation but is\nless memory efficient.\nRunning several experiments at different scales with models that achieve 100-fold less throughput is unfea-\nsible. For this reason, we limit our experimental evaluation to the fastest LSTM variant, the Multi-Head\nqLSTM model with a block length of 8 or 16. For easier comparison, we define the qLSTM model sizes in\nTable 8 to be within roughly 10% of the parameter and/or total FLOP count of the respective GPT model\nsizes from Table 4.\nqLSTM Model\ngigaFLOPs\nParams\ndmodel\nnlayers\nnheads\ndhead\nblock length\nmini\n19.9\n27.8M\n512\n4\n8\n32\n16\ntiny\n44.3\n55.9M\n768\n4\n12\n64\n8\nsmall\n107.3\n117.3M\n768\n12\n12\n64\n16\nmedium\n352.7\n361.1M\n1024\n12\n16\n64\n16\nlarge\n780.4\n787.0M\n1536\n24\n16\n96\n16\nXL\n1,633.6\n1,628.2M\n2048\n24\n24\n128\n16\nTable 8: Overview of our qLSTM model sizes, flops, parameters, and differing hyperparameters. FLOPs are\nthe total number of floating point operations during the forward pass with a batch size of 1 as measured by\nthe DeepSpeed flops profiler.\n21\nqLSTM Model\nRTX 3090\nV100-32GB\nA100-80GB\nmini\n94,781 (82)\n82,318 (112)\n186,145 (284)\ntiny\n36,930 (60)\n33,533 (84)\n62,031 (213)\nsmall\n11,143 (17)\n9,060 (22)\n23,433 (63)\nmedium\n2,509 (5)\n1,929 (7)\n6,731 (22)\nlarge\n1,006 (2)\n773 (3)\n4,064 (13)\nXL\nOOM\nOOM\n1,720 (5)\nTable 9: Overview of the best tokens per second measures and their respective batch sizes in brackets for\ndifferent qLSTM model sizes on several accelerators. OOM stands for out of memory.\n4.3.2\nResults\nWe present the best normalised perplexity scores for the compute classes from 6h to 96h in Table 10. In\nFigure 10b we compare the total FLOPs of the best qLSTM and GPT models. We find that the qLSTM\nmodels counter-balance their 5-6k times slower throughput with a faster convergence which requires fewer\ntokens than the GPT models to achieve a similar perplexity. As a result, our qLSTM model beats the GPT\nbaseline after roughly 2 exaFLOPs.\nAs indicated in the previous Section, our qLSTM implementation does not make ideal use of the accelerator\nhardware. In Figure 9a we compare our best qLSTM models with our best GPT models on the basis of\naccelerator hours and we find that the qLSTM lags behind on all scales up to 96h. However, we observe that\nthe qLSTM achieves a steeper scaling law than the GPT models, indicating a cross-over point at roughly\n50,000 accelerator hours.\ncompute\nnormalised\nconfig\nbatch size\ntotal\ntotal\nclass\nperplexity\ntrain tokens\nexaFLOPs\n6h\n2.518\nmini\n80\n2.05B\n0.239\n12h\n2.463\ntiny\n80\n1.60B\n0.414\n24h\n2.361\nsmall\n84\n0.96B\n0.605\n48h\n2.280\nsmall\n80\n1.93B\n1.211\n96h\n2.215\nsmall\n160\n3.85B\n2.421\nTable 10: Normalised perplexity values of the best qLSTM runs for each compute class. The number of\ntokens and exaFLOPs are computed as in Table 6.\nWe compare the qLSTM with the GPT model on our out of distribution splits in Table 11 (and Figure 11\nin the appendix). On the langlearn, discworld, and wood splits the models tend to have higher normalised\nperplexity. On the java and stats this is less the case. In fact, both 96h models have lower normalised\nperplexity on java and stats books than on the regular test split.\n5\nThe Languini Codebase\nThe Languini Kitchen codebase is fundamentally a research-focused codebase, created with the intent of being\neasy to use, comprehensible, and sufficiently performant for our benchmark. One of our primary objectives is\nto provide researchers with an environment that enables them to draw meaningful and equitable comparisons\nwith prior works. Furthermore, Languini also serves as a platform for identifying promising methods that\nhave the potential to be scaled up.\nThe codebase supports data parallel experiments but not model parallel. Model parallelism is necessary to\nscale up experiments to very large models, typically with billions of parameters or more, by distributing the\nmodel across multiple devices. However, few researchers will have access to such expansive computational\nresources and are thus outside the motivation of Languini. Models trained in Languini ought to fit within\nthe GPU memory of the chosen reference hardware.\n22\n100\n101\n102\n103\n104\n105\naccelerator hours\n1.60\n1.70\n1.80\n1.90\n2.00\n2.10\n2.20\n2.30\n2.40\n2.50\n2.60\n2.70\nnormalised ppl\n(a) qLSTM slope: \u22120.112 intercept: 2.722\nGPT slope:\u22120.082 intercept: 2.406\n1017\n1018\n1019\n1020\nflops\n1.70\n1.80\n1.90\n2.00\n2.10\n2.20\n2.30\n2.40\n2.50\nnormalised ppl\nmodel\nquasi-LSTM\nGPT\nlabel\nbest 6h config\nbest 12h config\nbest 24h config\nbest 48h config\nbest 96h config\n(b) qLSTM slope: \u22120.134 intercept: 7.883\nGPT slope:\u22120.084 intercept: 5.737\nFigure 10: Scale plot of the best qLSTM configs and the best results of the GPT models using the slow\nevaluation over the last 128 predictions per batch on the test split. Top: normalised perplexity over acceler-\nator seconds. Vertical dashed lines are the different compute classes starting from 6h. Bottom: normalised\nperplexity over total FLOPs. Like previous work, we estimate the FLOPs of the backward pass to be two\ntimes the forward pass (Kaplan et al., 2020).\nThe Languini codebase is inspired by Scenic, a lightweight library for the development of vision models\n(Dehghani et al., 2022). It similarly provides various model-agnostic features, ranging from logging and\ndata loading to training and evaluation functionalities. In order to maintain clarity and avoid complexity,\nexperiments and the code will be placed in distinct and isolated project folders. Every research endeavour will\n23\ncompute\nmodel\nnormalised perplexity on ood splits\nclass\nlanglearn\ndiscworld\njava\nstats\nwood\n6h\nGPT\n4.042\n2.393\n2.264\n2.131\n2.370\nqLSTM\n5.168\n2.670\n2.772\n2.531\n2.719\n12h\nGPT\n3.744\n2.326\n2.165\n2.062\n2.293\nqLSTM\n4.865\n2.619\n2.736\n2.466\n2.639\n24h\nGPT\n3.521\n2.273\n2.104\n2.011\n2.232\nqLSTM\n4.525\n2.526\n2.588\n2.354\n2.511\n48h\nGPT\n3.330\n2.217\n2.036\n1.948\n2.153\nqLSTM\n4.158\n2.440\n2.464\n2.252\n2.424\n96h\nGPT\n3.135\n2.158\n1.977\n1.898\n2.088\nqLSTM\n3.834\n2.343\n2.324\n2.176\n2.325\nTable 11: Evaluation of the best GPT and qLSTM models on all out of distribution splits.\nhave its exclusive project directory, complete with its own necessary library code. This preserves simplicity\nand ensures that each project remains independent of subsequent advancements. For this reason, Languini\nwill prevent interdependencies between projects. Once a project concludes, its respective folder ought to\nremain unchanged to guarantee reproducibility. Although this approach may lead to some code redundancy,\nwe believe this is a justifiable trade-off for a research-based codebase to prevent the core from deteriorating.\nTo be listed in the Languini leaderboard, researchers are expected to provide not just the model code, but\nalso configurations for all compute classes. Furthermore, every project has to provide scripts to download\ntraining logs and final model checkpoints from an archival hoster. We recommend utilising Zenodo.org, a\nreputable open repository developed by CERN under the European OpenAIRE program, for this purpose\n(European Organization For Nuclear Research & OpenAIRE, 2013).\nThe Languini Kitchen codebase is licensed under Apache 2.0, granting researchers the freedom to employ\nthe code as they deem fit. Nonetheless, we urge researchers to contribute their most noteworthy results as\na dedicated project folder, accompanied by instructions and a reference to their published work. This will\nfurther facilitate reproducibility and allow peers to draw comparisons with ease.\n6\nOpen Research Questions\nThe field of language modelling has never been more exciting. With a benchmark where virtually all models\nare underfitting, it shifts the focus away from ad-hoc regularisation techniques to innovations that will\nhopefully be directly applicable at scale. In this section, we highlight just a few of the interesting directions\nthat future work may explore.\nBetter tokenisation.\nOur exploration of common BPE tokenisation vocabularies, as detailed in Section\n4.1, has brought several intriguing findings to light. Notably, many tokens can be derived using elementary\nsymmetries. We also observed that the size of the vocabulary can substantially influence performance. These\ndiscoveries underscore the potential for innovative tokenisation methods. While recent studies underscore\nthe benefits of byte-level modes, they remain inferior to BPE tokenisers in compute-constrained experiments.\nImplementational efficiency.\nRecent work, such as flash attention, has highlighted the inefficiencies\ninherent in the native implementation of resource-intensive aspects of the model.\nEnhanced compilers,\nlibraries, or a more in-depth understanding of a low-level implementation could boost the throughput of a\nmodel without necessitating conceptual changes. An example of such is Rockmate (Zhao et al., 2023b) which\nis a tool to make models more memory efficient at the cost of re-computing certain activations.\nOptimisation improvements.\nWhile our experiments utilise simply Adam, there have been various\nadvancements in the optimisation of language models. However, a recent study indicates that some perceived\nadvantages diminish when experiments account for data or compute disparities (Kaddour et al., 2023).\n24\nThe Languini Books benchmark, being more expansive and akin to large-scale data than prior academic\nbenchmarks, coupled with the existing model implementation within the Languini codebase, can facilitate a\nbetter assessment of novel optimisation techniques.\nIntroduction of new models.\nLanguini provides a feed-forward and a recurrent baseline. Each approach\nhas its unique strengths and limitations. Over the past few years, several models have been published which\ndeclare their supremacy over the decoder-only transformer model in some way, but few have demonstrated\ntheir scalability. Examples of such are the following: a Linear Transformer (Schmidhuber, 1991; Katharopou-\nlos et al., 2020; Schlag et al., 2021) called TransNormer (Qin et al., 2023), a block-recurrent Transformer\n(Hutchins et al., 2022), novel parallelisable RNN called RWKV (Peng et al., 2023), or a state-space model\nfor language modelling called H3 (Fu et al., 2023). Unfortunately, each one of them has been trained and\nevaluated on different data and hardware making a direct comparison impossible.\nThe Languini Books\nbenchmark, however, could serve as a platform for such models to demonstrate their benefits in a fair and\nreproducible way with scalability in mind.\nAdvancement in theory.\nThe Languini Books benchmark boasts a significant enough scale to empirically\ndemonstrate model-specific scaling laws. Furthermore, our preliminary results indicate that the compute-\noptimal batch size is also model-specific and depends weakly on the size of the model but more work is\nrequired to establish a principled approach that scales.\nEnhanced generalisation.\nThe Languini Books dataset incorporates several out of distribution splits.\nThese splits mirror the divergence between the data on which the language model was trained and the\ncontext wherein it is deployed. The splits we introduced emphasize vast volumes of unique context that were\nremoved from the training corpus, necessitating models to adapt and learn on the fly. Given the limited\ncontext of current models, this may demand novel strategies, possibly via efficient online learning algorithms\nor novel and dynamic architectures equipped with the capacity to meta-learn.\n7\nConclusion\nIn this work, we introduced the Languini Kitchen, a research collective and codebase designed to democratize\nlanguage modelling research by facilitating meaningful contributions across varying scales of computational\nresources. We presented an experimental protocol that emphasizes the use of accelerator hours as a more\ninformative and equitable metric for comparison, addressing limitations inherent in the conventional measures\nof the number of parameters or FLOPs.\nUtilising a filtered version of the books3 dataset, we demonstrated the utility of our approach in offering\na fair and meaningful platform for comparing language models. We provided two baseline models, a feed-\nforward model based on the GPT-2 architecture and a recurrent model based on the new LSTM variation\ndesigned for larger throughput. Our empirical analysis revealed that while the GPT-2-based model performs\nstrongly in absolute terms, the quasi-LSTM exhibits superior scaling laws, converging more efficiently with\nfewer tokens.\nAs a future direction, the scalability of our quasi-LSTM model offers intriguing possibilities for optimization\nand performance improvement. Furthermore, the Languini Kitchen\u2019s codebase is open for community con-\ntributions, encouraging ongoing research and development aimed at improving the performance of language\nmodels and identifying new candidates to be scaled up.\nBy setting new standards for fair comparison and offering tools for practical implementation, we believe that\nthe Languini Kitchen lays the foundation for advancing the state of the art in language modelling research.\nBroader Impact Statement\nThe Languini Kitchen aims to democratize access to state-of-the-art language modelling research by creating\nan equitable framework for comparing performance across different scales of computational resources. In\ndoing so, it opens up opportunities for researchers and institutions with limited computational capabilities\n25\nto contribute meaningfully to the field. This democratization can lead to increased diversity in research\nperspectives, potentially yielding innovative solutions to existing problems and fostering greater inclusivity\nin the field of machine learning.\nLastly, it\u2019s worth considering that any advancements in language modelling, including those made more\naccessible by the Languini Kitchen, come with ethical implications related to data privacy, algorithmic bias,\nand the potential misuse of generated text. As the Languini Kitchen makes it easier to develop more capable\nlanguage models, it also magnifies the importance of ensuring that these technologies are developed and\ndeployed responsibly.\nAuthor Contributions\n\u2022 Aleksandar Stani\u0107: Ran experiments and contributed to the codebase, manuscript, and discus-\nsions.\n\u2022 Dylan Ashley: Contributed to the dataset and discussions.\n\u2022 Louis Kirsch: Contributed to the manuscript and discussions.\n\u2022 Oleg Serikov: Contributed in running experiments.\n\u2022 Francesco Faccio: Contributed to the presentation of the project and the manuscript.\n\u2022 J\u00fcrgen Schmidhuber: Advised the project.\n\u2022 Thomas Hofmann: Advised the project.\n\u2022 Imanol Schlag: Initiated and led the project. Ran experiments and contributed to the codebase,\ndataset, and manuscript.\nAcknowledgments\nWe extend our sincere thanks to Bobby He and Sotiris Anagnostidis for their valuable feedback on the initial\ndraft of this manuscript and to Vincent Herrmann for helping to setup the website. This work was partially\nfunded by ERC Advanced grant no: 742870 to J. Schmidhuber.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.\nCharacter-level language\nmodeling with deeper self-attention. In Proc. Conference on Artificial Intelligence (AAAI), pp. 3159\u2013\n3166, Honolulu, HI, USA, January 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose\nSlone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.\nExploring length generalization in large\nlanguage models. Advances in Neural Information Processing Systems, 35:38546\u201338556, 2022.\nLei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.\nURL http://arxiv.org/abs/1607.06450.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from\nai feedback. arXiv preprint arXiv:2212.08073, 2022b.\n26\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite\nfor analyzing large language models across training and scaling. In International Conference on Machine\nLearning, pp. 2397\u20132430. PMLR, 2023.\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges\nof efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021.\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks.\narXiv preprint arXiv:1611.01576, 2016.\nTom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural Information\nProcessing Systems (NeurIPS), Virtual only, December 2020.\nKrishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vishwanath, and Arun K Somani.\nA survey of techniques for optimizing transformer inference. arXiv preprint arXiv:2307.07982, 2023.\nDami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J. Maddison, and George E. Dahl.\nOn empirical comparisons of optimizers for deep learning, 2020. URL https://openreview.net/forum?\nid=HygrAR4tPS.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nTri Dao. Github flash attention issue 190, 2023. URL https://github.com/Dao-AILab/flash-attention/\nissues/190.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-\nefficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.\nMostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A jax library\nfor computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 21393\u201321398, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec-\ntional transformers for language understanding. In Proc. North American Chapter of the Association for\nComputational Linguistics on Human Language Technologies (NAACL-HLT), pp. 4171\u20134186, Minneapolis,\nMN, USA, June 2019.\nNolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hes-\ntness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale\ncluster. arXiv preprint arXiv:2304.03208, 2023.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al.\nParameter-efficient fine-tuning of large-scale pre-trained language\nmodels. Nature Machine Intelligence, 5(3):220\u2013235, 2023.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai\nWu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv\npreprint arXiv:2207.10342, 2022.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang\nSui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with transformers.\narXiv preprint arXiv:2302.14502, 2023.\n27\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\nEuropean Organization For Nuclear Research and OpenAIRE. Zenodo, 2013. URL https://www.zenodo.\norg/.\nMaurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are we really making much progress? a\nworrying analysis of recent neural recommendation approaches. In Proceedings of the 13th ACM Conference\non Recommender Systems, RecSys \u201919, pp. 101\u2013109, New York, NY, USA, 2019. Association for Computing\nMachinery.\nISBN 9781450362436.\ndoi: 10.1145/3298689.3347058.\nURL https://doi.org/10.1145/\n3298689.3347058.\nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry\nhungry hippos: Towards language modeling with state space models.\nIn The Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?id=COZDy0WYGg.\nPhilip Gage. A new algorithm for data compression. C Users Journal, 12(2):23\u201338, 1994.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nJonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. In\nInternational Conference on Machine Learning, pp. 11117\u201311143. PMLR, 2023.\nDeepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using\ninstruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023.\nS. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f\u00fcr Informatik,\nLehrstuhl Prof. Brauer, Technische Universit\u00e4t M\u00fcnchen, 1991. Advisor: J. Schmidhuber.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780,\n1997.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\nSara Hooker. The hardware lottery. Communications of the ACM, 64(12):58\u201365, 2021.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nMengdi\nHuang,\nChetan\nTekur,\nand\nMichael\nCarilli.\nPytorch\nauto-\nmatic\nmixed\nprecision\ntraining,\n2020.\nURL\nhttps://pytorch.org/blog/\naccelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/.\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.\nBlock-recurrent\ntransformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=uloenYmLCAo.\nKazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. A modern self-referential weight\nmatrix that learns to modify itself. In International Conference on Machine Learning, pp. 9660\u20139677.\nPMLR, 2022.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.\nFasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016a.\n28\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.\nBag of tricks for efficient text\nclassification. arXiv preprint arXiv:1607.01759, 2016b.\nJean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J Kusner. No train no gain: Revisit-\ning efficient training algorithms for transformer-based language models. arXiv preprint arXiv:2307.06440,\n2023.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\nAndrej Karpathy. nanogpt. https://github.com/karpathy/nanoGPT, 2023.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret.\nTransformers are RNNs:\nFast autoregressive transformers with linear attention. In Proc. Int. Conf. on Machine Learning (ICML),\nVirtual only, July 2020.\nLouis Kirsch and J\u00fcrgen Schmidhuber. Meta learning backpropagation and improving it. Advances in Neural\nInformation Processing Systems, 34:14122\u201314134, 2021.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning\nby meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence\nmodels. In International Conference on Machine Learning, pp. 2766\u20132775. PMLR, 2018.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. In EMNLP, 2018.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven\nLe Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. The big-\nscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing\nSystems, 35:31809\u201331826, 2022.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing. ACM\nComput. Surv., 55(9), jan 2023. ISSN 0360-0300. doi: 10.1145/3560815. URL https://doi.org/10.\n1145/3560815.\nMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal?\na large-scale study. Advances in neural information processing systems, 31, 2018.\nShengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer may not\nbe as powerful as you expect. Advances in Neural Information Processing Systems, 35:4301\u20134315, 2022.\nMatt Mahoney. Large text compression benchmark, 2011.\nSam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch\ntraining. arXiv preprint arXiv:1812.06162, 2018.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In\nInt. Conf. on Learning Representations (ICLR), Toulon, France, April 2017.\nTom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan \u010cernock`y, and Sanjeev Khudanpur. Recurrent neural\nnetwork based language model. In Eleventh annual conference of the international speech communication\nassociation, 2010.\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma\nMalkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across\nimplementations and applications? arXiv preprint arXiv:2102.11972, 2021.\n29\nOpenAI. Gpt-4 technical report, 2023.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\nChung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv\npreprint arXiv:2305.13048, 2023.\nJorge P\u00e9rez, Javier Marinkovi\u0107, and Pablo Barcel\u00f3. On the turing completeness of modern neural network\narchitectures. arXiv preprint arXiv:1901.03429, 2019.\nOfir Press, Noah Smith, and Mike Lewis.\nTrain short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=R8sQPpGCv0.\nZhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei\nYuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters. arXiv preprint arXiv:2307.14995,\n2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compres-\nsive transformers for long-range sequence modelling. In Int. Conf. on Learning Representations (ICLR),\nVirtual only, April 2020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.\nhtml.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\nenable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505\u20133506, 2020.\nRadim Rehurek and Petr Sojka. Gensim\u2013python framework for vector space modelling. NLP Centre, Faculty\nof Informatics, Masaryk University, Brno, Czech Republic, 3(2), 2011.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-\nto-image diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9,\nAlexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear Transformers are secretly fast weight program-\nmers. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July 2021.\nImanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, J\u00fcrgen Schmidhuber,\nand Xian Li. Large language model programs. KLR workshop, ICML 2023, 2023.\nJ. Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Technical\nReport Fki-147-91, Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen, March 1991.\nJ\u00fcrgen Schmidhuber. Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem. PhD\nthesis, Technische Universit\u00e4t M\u00fcnchen, 1990.\n30\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe\nCeron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun\nVijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Ra-\njeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch,\nVik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff,\nJoshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger\nJiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven\nBills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, and Nick Ryder,\nNov 2022. URL https://openai.com/blog/chatgpt/.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909, 2015.\nChristopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E\nDahl.\nMeasuring the effects of data parallelism on neural network training.\narXiv preprint\narXiv:1811.03600, 2018.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\nHava T Siegelmann. Recurrent neural networks and finite automata. Computational intelligence, 12(4):\n567\u2013574, 1996.\nHava T Siegelmann and Eduardo D Sontag. Turing computability with neural nets. Applied Mathematics\nLetters, 4(6):77\u201380, 1991.\nPrabhu Teja Sivaprasad, Florian Mai, Thijs Vogels, Martin Jaggi, and Fran\u00e7ois Fleuret. Optimizer bench-\nmarking needs to account for hyperparameter tuning. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings\nof the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-\ning Research, pp. 9036\u20139045. PMLR, 13\u201318 Jul 2020.\nURL https://proceedings.mlr.press/v119/\nsivaprasad20a.html.\nSamuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don\u2019t decay the learning rate, increase\nthe batch size. arXiv preprint arXiv:1711.00489, 2017.\nRupesh K Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks. In the Deep Learning\nworkshop at Int. Conf. on Machine Learning (ICML), Lille, France, July 2015.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q. Tran, Dani Yogatama, and Donald Metzler.\nScaling laws vs model architectures: How does\ninductive bias influence scaling?, 2023. URL https://openreview.net/forum?id=GGItImF9oG5.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving llm pretraining via\ndocument de-duplication and diversification. arXiv preprint arXiv:2308.12284, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr\nMi\u0142o\u015b. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170,\n2023.\n31\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nIn Proc. Advances in Neural Information Processing\nSystems (NIPS), pp. 5998\u20136008, Long Beach, CA, USA, December 2017.\nPeng Wu. Pytorch 2.0: The journey to bringing compiler technologies to the core of pytorch (keynote). In\nProceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization, CGO\n2023, pp. 1, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701016.\ndoi: 10.1145/3579990.3583093. URL https://doi.org/10.1145/3579990.3583093.\nGuodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue, and\nRoger B Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic\nmodel. Advances in neural information processing systems, 32, 2019.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu,\nTianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint\narXiv:2308.10792, 2023.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, et al.\nA survey of large language models.\narXiv preprint\narXiv:2303.18223, 2023a.\nXunyi Zhao, Th\u00e9otime Le Hellard, Lionel Eyraud-Dubois, Julia Gusak, and Olivier Beaumont. Rockmate:\nan efficient, fast, automatic and generic tool for re-materialization in pytorch. In International Conference\non Machine Learning, 2023b.\nGeorge Kingsley Zipf. The psycho-biology of language: An introduction to dynamic philology. Routledge,\n2013.\nA\nOOD Scale Plots\nThe following are scale plots for the best GPT and qLSTM models on the out of distribution splits of the\nlanguini books dataset.\n32\n101\n102\ncompute class (hours)\n3.5\n4.0\n4.5\n5.0\nnormalised perplexity\nlanglearn split\nModel\nGPT data\nGPT regression\nqLSTM data\nqLSTM regression\n(a) language learn\n101\n102\ncompute class (hours)\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\nnormalised perplexity\ndiscworld split\nModel\nGPT data\nGPT regression\nqLSTM data\nqLSTM regression\n(b) discworld\n101\n102\ncompute class (hours)\n2.0\n2.2\n2.4\n2.6\n2.8\nnormalised perplexity\njava split\nModel\nGPT data\nGPT regression\nqLSTM data\nqLSTM regression\n(c) java\n101\n102\ncompute class (hours)\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nnormalised perplexity\nstats split\nModel\nGPT data\nGPT regression\nqLSTM data\nqLSTM regression\n(d) statistics\n101\n102\ncompute class (hours)\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\nnormalised perplexity\nwood split\nModel\nGPT data\nGPT regression\nqLSTM data\nqLSTM regression\n(e) woodworking\nFigure 11: Scale plots on the ood data for the best GPT and qLSTM models.\n33\n"
  },
  {
    "title": "Controllable Dynamic Appearance for Neural 3D Portraits",
    "link": "https://arxiv.org/pdf/2309.11009.pdf",
    "upvote": "2",
    "text": "Controllable Dynamic Appearance for Neural 3D Portraits\nShahRukh Athar\nStony Brook University\nsathar@cs.stonybrook.edu\nZhixin Shu\nAdobe Research\nzshu@adobe.com\nZexiang Xu\nAdobe Research\nzexu@adobe.com\nFujun Luan\nAdobe Research\nfluan@adobe.com\nSai Bi\nAdobe Research\nsbi@adobe.com\nKalyan Sunkavalli\nAdobe Research\nsunkaval@adobe.com\nDimitris Samaras\nStony Brook University\nsamaras@cs.stonybrook.edu\nNovel View Synthesis\nRealistic Illumination e\ufb00ects during novel \nexpression and head-pose reanimation \nTraining Data with strong \nnon-ambient illumination\nDynamic Canonical \nappearance\nR\nvlmk\nn\nRGB\nGround \nTruth\nCoDyNeRF \nFaceMSE: 2.16e-3\nRigNeRF \nFaceMSE: 3.6e-3\nDeformed Space\nCanonical Space\n(a)\n(b)\n(c)\n(d)\nFigure 1. Our method, CoDyNeRF, enables the creation of reanimatable Neural 3D portraits from a video captured under challenging\nlighting conditions with a smartphone (column (a)). CoDyNeRF uses a novel dynamic canonical appearance model (column (b), discussed in\nSect 3.2) to predict illumination effects during reanimation and novel view synthesis (column (c)). Compared to prior work [3], CoDyNeRF\u2019s\nprediction of illumination effects is more accurate and its renders of articulated facial expression and head-pose are of higher quality (column\n(d)).\nAbstract\nRecent advances in Neural Radiance Fields (NeRFs) have\nmade it possible to reconstruct and reanimate dynamic\nportrait scenes with control over head-pose, facial ex-\npressions and viewing direction. However, training such\nmodels assumes photometric consistency over the deformed\nregion e.g. the face must be evenly lit as it deforms with\nchanging head-pose and facial expression. Such photometric\nconsistency across frames of a video is hard to maintain,\neven in studio environments, thus making the created\nreanimatable neural portraits prone to artifacts during\nreanimation. In this work, we propose CoDyNeRF, a system\nthat enables the creation of fully controllable 3D portraits\nin real-world capture conditions.\nCoDyNeRF learns to\napproximate illumination dependent effects via a dynamic\nappearance model in the canonical space that is conditioned\non predicted surface normals and the facial expressions and\nhead-pose deformations. The surface normals prediction\nis guided using 3DMM normals that act as a coarse prior\nfor the normals of the human head, where direct prediction\nof normals is hard due to rigid and non-rigid deformations\ninduced by head-pose and facial expression changes. Using\nonly a smartphone-captured short video of a subject for\ntraining, we demonstrate the effectiveness of our method\non free view synthesis of a portrait scene with explicit head\npose and expression controls, and realistic lighting effects.\narXiv:2309.11009v2  [cs.CV]  21 Sep 2023\nThe project page can be found here.\n1. Introduction\nThe creation of photo-realistic human portraits with explicit\ncontrol of head-pose and facial expressions remains a topic\nof active research in the computer graphics and computer vi-\nsion communities. Fully controllable Neural 3D portraits are\nnecessary in AR/VR applications where an immersive 3D\nexperience is important. Recent advances in neural render-\ning and novel view synthesis [4, 5, 10, 11, 26, 27, 29, 30, 32,\n35, 46, 49, 50] have demonstrated impressive image-based\nrendering of complex scenes and objects. Recently, these\nmethods have also been extended to model and reanimate the\nhuman head as shown in [3, 13, 51]. These works typically\nemploy a learnable deformation to map the deforming head\nto a canonical space, where the texture and geometry is pre-\ndicted and rendered. The canonical space represents a mostly\nstatic appearance of the face, akin to a UV texture map, with\nan optional dependence on expressions [3, 13] in order to\ncapture texture changes induced by them. The deformation,\ngeometry, and texture are learnt via back-propagation using\nthe photometric error with respect to the ground truth. In\norder for such a setup to successfully learn the appearance,\ndeformation, and geometry, the training data must be pho-\ntometrically consistent. More specifically, the color of a\nparticular position on the human head must remain constant\nonce mapped to the canonical space, regardless of the articu-\nlated head-pose and facial expression. However, in realistic\nlighting conditions, this is rarely the case.\nIn the real world, there is self-shadowing of the face, and\nthe head casts its shadow on other parts of the scene as it\nrotates and facial expressions change. Similarly, spatially\nvarying skin reflectance induces specularities that change\nwith viewing angles, head poses, and facial expressions. The\nassumption of a static appearance in the canonical space is\nno longer true. As a result, deformable NeRFs trained on\nsuch data with a static canonical appearance assumption\nsuffer from registration error that leads to blurriness in the\nrenderings and inaccurate reproduction of specularities, shad-\ning and shadows. Further, if these models use a canonical\nspace dependent on expression parameters, such as [3, 13],\nthe aforementioned illumination-dependent effects become\nentangled with them. This entanglement leads to artifacts\nin articulated expression and inaccurate reproduction of the\nillumination effects during reanimation.\nAccurately reproducing illumination-dependent effects\ndue to strongly non-ambient lighting requires a modification\nof the canonical space.\nIt cannot be static, it must be\ndynamic i.e. the canonical space must vary in appearance\nas illumination effects change in the deformed space.\nMore specifically, in a dynamic portrait scene that is\ncaptured under constant but unknown non-ambient lighting,\nthe following lighting effects must be reproduced in the\ncanonical space 1) Specularities, which depend on viewing\ndirections and surface normals 2) Shading, that is dependent\non head-pose and facial expression deformations as they\ndetermine the relative orientation of the surface normals\nto the lighting and 3) Cast shadows, which are dependent\non whether or not a strong light source is occluded w.r.t a\npoint. Capturing each of these effects, especially the cast\nshadows, using a physically based model is untenable in\na deforming NeRF framework due to the exponentially\nincreasing MLP evaluations that need to be performed.\nOur key insight is that, given enough training data, the\nimplicit and explicit dependencies of the aforementioned\nillumination effects on the surface geometry, the head-pose,\nand the facial expression deformations can be approximated\nby an appropriately conditioned MLP. Based on this, we\npresent CoDyNeRF, a method that uses a dynamic canonical\nappearance space, modelled by an MLP, to enable the\ncreation of reanimatable and photorealistic neural 3D\nportraits using data captured in realistic lighting conditions.\nCoDyNeRF predicts illumination dependent effects directly\nin the canonical space by conditioning an MLP on the\ndynamic surface normals and 3DMM keypoints as well as\nexpression and pose deformations. This conditioning makes\nit easier for the MLP to interpolate illumination dependent\neffects for novel facial expressions, head-poses and views\nwithout sacrificing the quality of facial expression and\nhead-pose articulation.\nHowever, a challenge remains. While 3DMM keypoints,\nfacial expression and head-pose deformations are given by\nthe 3DMM, the surface normals are not. Due to the lack\nof available ground truth geometry, it is hard to estimate\naccurate surface normals for each point in the scene, espe-\ncially on the head and face, which are dynamic and undergo\nstrong deformations with changing facial expression and\nhead-pose. One possible solution is to use the normals given\nby the density field of the NeRF. However, as shown in Fig 4\nand observed in prior work [43], these are often noisy and\ninaccurate. Instead, CoDyNeRF uses a carefully designed\nMLP, that is able to leverage both 3DMM and scene normal\npriors, to predict the surface normals. The MLP-predicted\nnormals capture more detail than 3DMM-based normals\nand, due to the 3DMM-prior, transform correctly as the head\nundergoes rigid and non-rigid deformations. These normals\nare then used to supervise the gradient density normals\nof the NeRF in order to ensure accurate reconstruction\nof the dynamic head geometry and, consequently, the\naccurate reproduction of illumination effects. Once trained,\nCoDyNeRF realistically reproduces shadowing, shading and\nspecular effects during reanimation with explicit control of\nhead pose, facial expression and camera viewpoint.\nIn summary, our contributions in this paper are as follows:\n1) Using a dynamic canonical appearance, we are capable of\ncreating a fully reanimatable 3D neural portrait from data\ncaptured in strong, non-ambient lighting conditions.\n2) We propose a method to predict accurate and detailed\nsurface normals of the deforming human head, in addition to\nthe static scene, which is critical for the dynamic appearance\nlearning.\n3) We enable a realistic re-animation of lighting and\nspecularity effects on the human face with changing\nhead-pose and facial expressions.\n2. Related works\nCoDyNeRF enables the realistic rendering of illumination\neffects for controllable neural portraits captured in challeng-\ning lighting conditions. It is closely related to recent work\non neural rendering, novel view synthesis, 3D face modeling,\nand controllable face generation.\nNeural Scene Representations and Novel View Synthe-\nsis. CoDyNeRF is related to recent work in neural ren-\ndering and novel view synthesis [3\u20135, 10, 11, 13, 23, 26\u2013\n33, 35, 36, 40, 43, 45\u201347, 49\u201351]. Neural Radiance Fields\n(NeRF) use a Multi-Layer Perceptron (MLP), F, to learn a\nvolumetric representation of a scene. For every 3D point on\na ray, NeRFs predict an associated color and density which\nis volume rendered to give the final color. While NeRFs are\nable to reproduce specular effects, they do so at the cost of\ngeometric fidelity of the scene [43]. RefNeRF [43] extends\nNeRFs to explicitly handle specularities of static scenes by\nimproving the learnt surface normals, and consequently the\nscene geometry. While NeRFs such as RefNeRF are able\nto generate photo-realistic images for novel view synthe-\nsis, it is only designed for a static scene and is unable to\nrepresent scene dynamics. Our approach ensures the accu-\nrate reproduction of illumination effects when reanimating\nneural portraits since, it is specifically designed to model\nillumination effects of a dynamic scene.\nDynamic Neural Scene Representations. There has been\nan active effort to extend NeRFs to dynamic scenes. Most\nworks do so by imposing temporal constraints either explic-\nitly using scene flow [25, 26, 35, 46] or implicitly by using\na canonical frame [32, 35]. Authors of [33] build upon [32]\nand model topologically changing deformations by lifting the\ncanonical space to high dimensions. The deformation fields\nin these approaches are conditioned on learnt latent codes\nwithout specific physical or semantic meaning, and therefore\nnot controllable in an intuitive manner. Further, such models\nonly work with limited illumination change across frames,\nsince learning a common registration becomes significantly\ndifficult without any photometric consistency.\nControllable Face Generation. Generative Adversarial\nNetworks(GANs)[12, 16\u201319, 53] have enabled high-quality\nimage generation and have inspired a large collection of\nwork [2, 6\u20138, 22, 34, 38, 39, 41, 42] capable of face manipu-\nlation. However, it is challenging to enable high-quality view\nsynthesis and 3D controls of the portraits as most of these\nworks lack any 3D understanding and are purely image-\nbased. Works such as [1, 9, 20, 21] fix this by using an\nintermediate 3D face representation, via a 3D Morphable\nModel, to reanimate face images/videos. While head poses\nand facial expressions are modelled with good detail in these\nmodels, thanks to the 3DMM, they are often unable to per-\nform novel view synthesis as they focus on face region but\nneglect the geometry or appearance of the scene.\nIn a similar vein, NerFACE[10], uses NeRF to model a\n4D face avatar and allows pose/expression control on the\nhead. However, no view synthesis can be performed on\nthe scene and the subject is assumed to be uniformly lit\nthroughout the capture process. Neural Head Avatars [13],\nIMAvatar [51] and RigNeRF [3] improve upon the results of\nNerFACE further by using a 3DMM prior more explicitly.\nNeural Head Avatars learns a per-vextex pose and expression\nconditioned deformation for the FLAME mesh along with\na detailed texture while IMAvatar [51] learns a per-point\nFLAME basis, used for registering different head-poses and\nfacial expressions. RigNeRF uses the 3DMM deformation\nas a prior on its deformation field that maps points from the\narticulated space to the canonical space. Unlike our method,\nall three aforementioned methods require an ambiently lit\nface throughout the capture process and are unable to render\nexpression and head-pose dependent illumination effects.\n3. CoDyNeRF\nIn this section, we describe our method, CoDyNeRF, that\nenables the creation of reanimatable 3D portrait scenes from\nvideos captured in the real-world with non-ambient lighting\nconditions. A deformable Neural Radiance Field (NeRF)\n[30], with a per-point 3DMM guided deformation field mod-\nels facial expression and head-pose. It maps points from\nthe deformed space to a dynamic canonical space of the\nmodel where the volume density and the appearance is pre-\ndicted. The dynamic canonical space is conditioned on the\nsurface normals, head-pose and facial expression deforma-\ntions along with other shading and shadowing based cues\n(Sec. 3.2). The surface normals, defined in the scene world\nco-ordinates, are dynamic and vary with head pose and fa-\ncial expression. CoDyNeRF predicts these normals using an\nMLP that is trained with 3DMM normals and scene normals\nas a prior (Sec. 3.2.1). An overview of the full architecture\nis shown in Fig 2. Once trained, CoDyNeRF is not only\nable to control facial expression and head pose of the subject\nbut is also able to faithfully capture the varying illumination\neffects, such as specularities and shadows.\nF\nxcan\nx\n\u03c3\n\u03d5\n(\u0394x, \u0394y, \u0394z)\nxM = x +\n3DMMDef(x, \u03b2exp, \u03b2pose)\nexp (DistToMesh(x))\nxM\n\u2a01\n\u03c9\nD\nCanonical Space\n3DMM Deformation Field \n\u03b2exp,\u00a0pose\nR\nvlmk\nDistToMesh(x)\nn\nRGB\nFigure 2. Overview of CoDyNeRF. CoDyNeRF is a deformable NeRF architecture that consists of three learnable MLPs: a deformation\nMLP D and density MLP F and a dynamic appearance MLP T . Given an image, we shoot rays through each of its pixels. For every ray, we\ndeform each point on it according to a 3DMM-guided deformation field similar to prior work [3]. Next, the deformed point is given as input\nto the color MLP, F, which predicts the density and neural features that are passed onto the dynamic appearance MLP T . T then takes as\ninput normals, the reflection vector about the normal, the pose and expression deformations along with spherical harmonics shading and\nhead landmark positions to predict dynamic RGB of the point. The final color of the pixel is calculated via volume rendering.\n3.1. A 3DMM-guided Deformable Neural Radiance\nField\nA\nneural\nradiance\nfield\n(NeRF)\nis\ndefined\nas\na\ncontinuous\nfunction\nF\n:\n(\u03b3\u03b3\u03b3m(x(ti)),\u03b3\u03b3\u03b3n(d))\n\u2192\n(c(x(ti), d), \u03c3(x(ti))), that, given the position of a point\nin the scene x(ti) = o + tid that lies on a ray orig-\ninating at o with direction d, outputs the color c =\n(r, g, b) and the density \u03c3. F is usually represented as a\nmulti-layer perceptron (MLP) and \u03b3\u03b3\u03b3m : R3 \u2192 R3+6m\nis the positional encoding [30] defined as \u03b3\u03b3\u03b3m(x)\n=\n(x, ..., sin(2kx(ti)), cos(2kx(ti)), ...) where m is the total\nnumber of frequency bands and k \u2208 {0, ..., m \u2212 1}. The ex-\npected color of the pixel through which a camera ray passes\nis calculated via volume rendering as follows:\nC =\nX\nt\n\u03c9tc(x(t));\nwhere wi = exp(9\nX\nj<i\n\u03c3j (tj+1 \u2212 tj)) (1 9 exp(9\u03c3i (ti+1 9 ti))) (1)\nThe parameters of F are trained to minimize the L2 distance\nbetween the expected color and the ground-truth.\nNeRFs can be extended to model dynamic scenes by us-\ning a deformation field to map each 3D point of the scene\nto a canonical space, where the volumetric rendering takes\nplace [3, 32, 33, 35]. The deformation field is also repre-\nsented by an MLP Di : x \u2192 xcan where Di is defined as\nD(x, \u03c9i) = xcan and \u03c9i is a per-frame latent deformation\ncode. Following prior work [3], we use a 3DMM prior on\nthe deformation field as follows:\n\u02c6D(x) = 3DMMDef(x, \u03b2i,exp, \u03b2i,pose)\n+ D(\u03b3\u03b3\u03b3a(x),\u03b3\u03b3\u03b3b(3DMMDef(x, \u03b2i,exp, \u03b2i,pose)), \u03c9i)\nxcan =x + \u02c6D(x)\n(2)\nwhere, 3DMMDef(x, \u03b2i,exp, \u03b2i,pose) is the deformation prior\ngiven by the 3DMM, \u03b2i,exp, \u03b2i,pose are the articulated facial\nexpression and head-pose of the frame i, and \u03b3\u03b3\u03b3a,\u03b3\u03b3\u03b3b are the\npositional encoding functions with frequencies a and b re-\nspectively. The deformation prior, can be written as follows:\n3DMMDef(x, \u03b2i,exp, \u03b2i,pose) = 3DMMDef(\u02c6x,\u03b2exp,\u03b2pose)\nexp(DistToMesh(x))\n(3)\nwhere,\n\u02c6x is the closest point on the mesh to x,\nDistToMesh(x) = ||x \u2212 \u02c6x|| is the distance between x and\n\u02c6x and 3DMMDef(\u02c6x, \u03b2i,exp, \u03b2i,pose) is the deformation of the\nvertex \u02c6x as follows:\n3DMMDef(\u02c6x, \u03b2exp, \u03b2pose) = \u02c6xFLAME(\u03b2exp, can,\u03b2pose, can) \u2212 \u02c6xFLAME(\u03b2exp,\u03b2pose)\n(4)\nwhere, \u02c6xFLAME(\u03b2exp, can,\u03b2pose, can) is the position of x in the canon-\nical space and \u02c6xFLAME(\u03b2exp,\u03b2pose) is its position with head pose\nand facial expression parameters {\u03b2exp, \u03b2pose}.\n3.2. An Illumination aware dynamic canonical ap-\npearance model\nIn the canonical space, CoDyNeRF predicts the density and\na dynamic RGB appearance. The dynamic RGB is condi-\ntioned on the surface normals, head-pose, and expression\ndeformations along with other shading and shadowing cues\nsuch, as the reflection vector and global location of the head.\nBelow, we describe each aspect of the appearance model.\nA spatially conditioned density prediction model. First,\nCoDyNeRF predicts the density at any point by conditioning\non its position in the canonical space and its distance to the\nmesh:\n\u03c3(x), \u03c4 = F(\u03b3\u03b3\u03b3c(xcan), DistToMesh(x))\n(5)\nwhere, F is an MLP, \u03c4 is a feature vector, DistToMesh(x) =\n||x \u2212 \u02c6x|| is the distance of x to the closest mesh vertex \u02c6x\nand \u03b3\u03b3\u03b3c is the positional encoding function with c frequencies.\nAdditional conditioning on DistToMesh(x) modestly boosts\nboth the training speed and quality of results by allowing F\nto distinguish between points in the canonical space that have\nnever been deformed and points that have been deformed to\nthe canonical space. We provide experiments supporting this\nin the supplementary section.\nAn illumination aware dynamic canonical appearance\nmodel. Next, CoDyNeRF predicts dynamic RGB condi-\ntioned on inputs that capture local geometry, surface proper-\nties and viewing direction. Since the captured neural portrait\nis a dynamic scene, the outgoing radiance at any point x,\nis implicitly dependent on facial expression and head-pose,\n{\u03b2exp, pose} (or {\u03b2e,p}) due surface properties and incoming\nradiance being dependent on them. More specifically, at\nany point x for a particular articulation of facial expression\nand head-pose, \u03b2e,p, the outgoing radiance is given by the\nrendering equation as follows:\nLr (x, \u03c9o, \u03b2e,p) =\nR\n\u03c9i \u03c1 (x, \u03c9i, \u03c9o, \u03b2e,p) (n \u00b7 \u03c9i) Li (x, \u03c9i, \u03b2e,p) d\u03c9i\n(6)\nwhere, \u03c1 is the articulation dependent BRDF, n is the normal\nat x and \u03c9i, \u03c9o are the incoming and outgoing ray directions\nrespectively. We approximate this integral in the canonical\nspace by an MLP, T , as follows:\n(R, G, B) = T (\u03c4, n, R, vlmk, 3DMMDefexp, 3DMMDefpose, \u03d5i)\n(7)\nwhere, \u03c4 are features from the density prediction net-\nwork from Eq.\n(5), n is the surface normal, vlmk are\nthe facial landmarks, R = 2(d.n)n \u2212 d is the reflec-\ntion vector, 3DMMDefexp := 3DMMDef(x, \u03b2exp, \u03b2pose, can)\nis the expression-only deformation given by the 3DMM,\n3DMMDefpose := 3DMMDef(x, \u03b2exp, can, \u03b2pose) is the head-\npose only deformation given by the 3DMM and \u03d5i is a\nper-frame latent vector that is learnt through optimization.\nEach input in Eq. (7) contains information that is essential\nto the prediction of accurate illumination effects. First, sur-\nface reflectance and absorption properties are captured by \u03c4\nwhich is predicted in the canonical space and thus is forced\nto only model deformation-independent properties of the sur-\nface. The surface normal n is used to model shading effects\nand, along with the reflection vector R, specular effects. The\nface landmarks, vlmk, along with expression and head-pose\ndeformations,3DMMDefexp and 3DMMDefpose are used to\nmodel cast shadows, inter-reflections and any other illumina-\ntion effects that depend on the global orientation of the head\nand deformations due to facial expressions and head-pose.\nThe latent-code captures any appearance changes due to the\ncamera.\n3.2.1\nPrediction of Dynamic Surface Normals\nPrediction of shading and specular effects (through reflection\nvector R) requires accurate surface normals. Within a NeRF,\nn\nMeshn(x)\nGradn(x)\nDistToMesh(x)\n\ud835\udca9\nFigure 3.\nNormals Prediction Architecture. The Normals\nprediction network takes as input the mesh normals of a given point\nx, its distance to the mesh and the normals given by gradient of the\nNeRF\u2019s density field. The details of the architecture are discussed\nin Section 3.2.1.\none straightforward way to calculate the normals at any point\nx is to define it as the negative of the derivative of the density\nfield, \u03c3(x), w.r.t x i.e Gradn(x) = \u2212 \u2207x\u03c3(x)\n|\u2207x\u03c3(x)|. However, as\nshown in Fig 4 and observed in prior work [43], unless these\nnormals are regularized [43], they are incredibly noisy and\nrather unusable. To get around this, we use an MLP, N, to\npredict the normals. We design N in a manner that allows it\nto exploit local priors from the 3D head mesh and the scene\nto predict the normals at each point of the dynamic neural\nportrait. More specifically, the normal at any point is given\nas follows:\nn = N(Meshn(x), Gradn(x), DistToMesh(x))\n(8)\nwhere, Meshn(x) is normal vector of the mesh vertex closest\nto x, Gradn(x) is the normal calculated by the negative gra-\ndient of the density w.r.t the input point and DistToMesh(x)\nis the distance of x to the mesh. With these three inputs, N,\nis able to rely on the 3DMM mesh normals for points close\nto the head, while relying on gradient normals Gradn(x) ev-\nerywhere else. In supplementary, be demonstrate the utility\nof each input to N by ablating each one of them.\nWe train N through a combination of weak supervision\non mesh and scene normals, and regularization losses. The\nprediction of N is forced to be weakly consistent with the\n3DMM on its vertices as follows:\nRMesh,n = \u03bbMesh,n\nP\nv ||N(v) \u2212 Meshn(v)||\n(9)\nwhere, v are the vertices of the mesh and \u03bbMesh,n is the\nregularization constant. The normals predicted by N are\nalso forced to be forward facing by using the following\nregularization [43]:\nRdir,n = P\ni wi(xi) max (0, N(xi) \u00b7 di)2\n(10)\nwhere, xi are points along the ray passing through pixel i\nwith direction di and wi(xi) is the weight of xi per Eq. (1).\nIn order to ensure the gradient density normals Gradn(x)\nare themselves accurate, we regularize both the normals pre-\ndicted by N and the gradient density normals to be consistent\nwith each other:\nRn = P\ni wi(xi)||N(xi) \u2212 Gradn(x)||\n(11)\nRender\nDynamic Surface \nNormals\nCoDyNeRF\nRigNeRF\nFigure 4. Dynamic Surface Normals. Surface normals directly\ninfluence the specularities and shading of the human head. Here we\nvisualise the surface normals given by the density field of CoDyN-\neRF. As can be seen, the normals are able to capture the dynamic\nforeground with fine detail without sacrificing accuracy. In compar-\nison, the normals of RigNeRF [3] are noisy and fail to capture the\nfacial geometry accurately.\nAs observed in [43], this regularization ensures that highly\nspecular surfaces of the scene are not explained away as\nsubsurface emmisive lobes. The full loss on N is:\nLN =\nRMesh,n\n|\n{z\n}\nEnsures consistency\nof normals on vertices of the head\n+\nRdir,n\n| {z }\nEnsures normals are\nforward facing\n+\nRn\n|{z}\nEnsures consistency between\nN and Gradn normals\n(12)\nSparsity regularization for fast calculation of Rdir,n and\nRn. Calculating both Rdir,n and Rn is very computationally\nexpensive as it requires a second derivative calculation at\neach point along the ray (usually \u223c 100 points for most\nstandard NeRF architectures) for each sampled ray in the\nbatch (typically around 1000 rays). A simple way to reduce\nthe computational burden is to evaluate the above sum only\non a subset of the points on a ray as follows\nRn =\nX\ni\nwi(x\u2032\ni)||N(x\u2032\ni) \u2212 Gradn(x\u2032\ni)||;\nwhere x\u2032\ni \u2208 Si,k\n(13)\nwhere, Si,k is the set of top k points, sorted by weight wi(x\u2032\ni),\nof the ray passing through pixel i. However, as the weights\npredicted by the NeRF are relatively broadly distributed,\nsuch regularization does not minimize Eq. (13) over the\nwhole scene consistently. To ensure the predicted weights\nare more tightly distributed around the surface, we enforce a\nCauchy regularization to enforce sparsity [15]\nRcauchy = \u03bbc\nP\ni log\n\u0010\n1 + \u03c3(xi)2\nc\n\u0011\n(14)\nsimilar to [15], we only apply this on the coarse MLP. Eq.\n(13) and Eq. (14) improve the underlying dynamic scene ge-\nometry and significantly improve the quality of the gradient\ndensity normals.\n4. Experimental Results\nIn this section, we show results of head-pose control, facial\nexpression control, and novel view synthesis using CoDyN-\neRF. For each scene, the model is trained on a short portrait\nvideo captured using a consumer smartphone.\nBaseline approaches. We compare CoDyNeRF quantita-\ntively and qualitatively to the following prior work for human\nhead animation 1) RigNeRF [3] is a method for reanimating\nneural portraits with full control of facial expressions, head\npose and viewing direction. Similar to us, the authors use\na volumetric representation to model the dynamic scene. 2)\nNeural Head Avatars [13] creates a neural head model by de-\nforming a mesh with pose and expression dependent offsets\nalong with an MLP dependent texture space. 3) PointAvatar\n[52] uses a point cloud and an SDF as the geometric repre-\nsentation of the avatar in the canonical space. Additionally,\nPointAvatar seperates the appearnace of the avatar into an\nalbedo and a normals-conditioned RGB shading. RigNeRF\n[3] and Neural Head Avatars [13] use an expression and pose\ndependent canonical space while PointAvatar uses a normals\nconditioned shading network to handle apperance variation\nduring the capture. Unlike in [3], we do not optimize the ap-\npearance or deformation latent code during testing. Since we\nwant to evaluate the fidelity of reproduction of illumination\ndependent effects for novel head-pose, facial expressions\nand views we cannot assume access to testing frames. We\nalways use the deformation and appearance code of the first\nframe during testing.\nTraining and evaluation Data. The training and validation\ndata was captured using an iPhone 13 Pro Max for all the\nexperiments in the paper. In the first half of the capture,\nwe ask the subject to enact a wide range of expressions and\nspeech while trying to keep their head still as the camera is\npanned around them. In the next half, the camera is fixed at\nhead-level and the subject is asked to rotate their head as they\nenact a wide range of expressions. Camera parameters are\ncalculated using COLMAP [37]. FLAME [24] parameters\nand spherical harmonics coefficients Llm are obtained via\nstandard photometric and landmark fitting obtained by [14].\nAll videos are between 50-80 seconds long; we use the first\n\u223c 1200-1500 frames for training and the remaining 120-150\nheld out frames, with novel expressions and head-poses, for\nvalidation. Please find full details of each experiment in the\nsupplementary document.\n4.1. Evaluation on Test Data\nWe evaluate CoDyNeRF, along with three state-of-the-art\nbaselines, Neural Head Avatars [13], RigNeRF [3], and\nPointAvatar [52] on held out testing frames. These frames\ncontain a variety of facial expressions and head-poses. In\nFig 5, we show a qualitative comparison between CoDyN-\neRF and the baselines. We observe that outdoor-captured\nvideos, where pose and expression deformations under sun-\nlight creates large appearance variations, pose a significant\nchallenge to existing methods[3, 13, 52].\nNeural Head\nPointAvatar\nGround Truth\nCoDyNeRF\nNeural Head \nAvatars\nRigNeRF\n(1)\n(2)\n(3)\n(4)\n(5)\nFigure 5. Qualitative comparison by reanimation with novel facial expression, head-pose, and camera view parameters. Here\nwe reanimate CoDyNeRF, RigNeRF [3] and Neural Head Avatars [13] with novel facial expressions and head-pose extracted from the\nground-truth frame in Row 1. As can be observed in the highlighted red boxes, while RigNeRF [3] is able to generate some shadowing\nand specular effects on the face, they are incorrect. Additionally, RigNeRF\u2019s results have significant artifacts around the mouth and face\nregions. Similarly, due to the use of an explicit mesh and the entanglement of illumination effects with expressions, Neural Head Avatars\n[13] is unable to recover accurate geometries or predict accurate illuminations effects. Since PointAvatar\u2019s design does not take into account\ncast shadows and specularities, it struggles to reproduce them accurately as can be seen in columns (1) - (5). In contrast, our approach,\nCoDyNeRF, is able to accurately reproduce cast shadows and specularities without sacrificing the quality of facial expressions and head-pose\narticulation.\nAvatars [13] and RigNeRF [3], entangle illumination depen-\ndent effects with expression parameters in their canonical\nspace and are not able to faithfully reconstruct the target\nappearance, creating artifacts in the overall appearance in-\ncluding in shadows, specularities, and in the mouth and eye\nregion. Neural Head Avatars [13] is able to generate some\nshadows and specularities but they often lack of details and\ndo not match the ground truth well. Further, due to their\nheavy reliance on an explicit face mesh, results from [13]\nare unable to accurately capture the head geometry in detail,\nresulting in an unnaturally deformed shape. RigNeRF [3],\nsimilar to NHA [13], uses an expression and pose (through\ndeformation features) conditioned canonical space and is\nable to generate some shadows and specularities. However,\nSubject 1\nSubject 2\nSubject 3\nSubject 4\nModels\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nCoDyNeRF (Ours)\n23.46\n0.40\n1.92e \u2212 3\n21.24\n0.33\n2.16e \u2212 3\n22.3\n0.53\n1.0e \u2212 3\n23.56\n0.40\n1.5e \u2212 3\nRigNeRF [3]\n21.0\n0.41\n3.64e-3\n20.86\n0.35\n3.60e-3\n21.67\n0.54\n1.7e-3\n22.26\n0.45\n2.1e-3\nNHA [13]\n-\n-\n0.058\n-\n-\n0.0382\n-\n-\n0.070\n-\n-\n0.068\nPointAvatar [52]\n-\n-\n6.3e-3\n-\n-\n8.38e-3\n-\n-\n1.05e-2\n-\n-\n7.74e-3\nTable 1. Quantitative results of Subject 1,2,3 and 4 on test data. Here we calculate PSNR and LPIPS over the full image while FaceMSE is\nonly restricted to the MSE calculated over the face region. Our results are better than RigNeRF [3], Neural Head Avatars [13] and IMAvatar\n[51] across all subjects.\n(a) Novel View Synthesis\n(b) 3DMM Driven Reanimation with accurate modelling of illumination e\ufb00ects\nFigure 6. Applications of CoDyNeRF. CoDyNeRF enables the creation of Neural 3D Portraits captured in challenging lighting conditions\nwith the accurate reproduction of illumination effects during novel expression and pose reanimation. In (a), we demonstrate that specularities,\nespecially on the forehead and the nose, change realistically with changing viewing directions. In (b) we demonstrate the realistic\nreproduction of shading, shadows and specularities when the neural portrait is reanimated with a driving 3DMM (shown as white insets in the\ntop left of each image). We see from results in the top row of (b) that the shading on the forehead and the specularities change realistically.\nSimilarly, in the bottom row of (b) we see the nose being unshaded as the head rotates and the shadow cast by the ear changes realistically.\nSubject 1\nSubject 2\nModels\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nPSNR \u2191\nLPIPS \u2193\nFaceMSE \u2193\nCoDyNeRF \u2212\u03b2exp,pose, \u2212normals,\u2212vlmk\n19.8\n0.47\n4.03e-3\n19.1\n0.51\n4.26e-3\nCoDyNeRF \u2212vlmk\n22.1\n0.45\n2.64e-3\n20.93\n0.33\n3.30e-3\nCoDyNeRF (Full Model)\n23.46\n0.40\n1.92e \u2212 3\n21.24\n0.37\n2.46e \u2212 3\nTable 2. Ablation of dynamic appearance conditioning.\nthey are often inaccurate and visually unnatural. For exam-\nple, the specularity columns 1 and 2 of Fig 5 are incorrect, as\nwell as the specularity around the eyes of columns 3 and 4.\nSimilarly, the shadow on the mouth of column 5 is incorrect.\nAdditionally, RigNeRF [3] results have significant artifacts,\nespecially around the mouth and the eyes. artifacts around\nthe mouth can be seen in columns (1), (2), (3) and (5) and\nthere are artifacts around the eyes in columns (2), (3) and\n(4) of Fig 5 respectively. PointAvatar [52] uses separate\nshading and texture networks to disentangle the albedo from\nthe shading. However, its design does not take into account\ncast shadows and specularities, thus it is unable to learn and\npredict them*. PointAvatar is unable to reproduce the shad-\nows and specularities on the forehead in columns (1) - (4) of\nFig 5. It is also unable to predict the shadow cast by the nose\non the mouth in column (5) of Fig 5. In contrast, CoDyNeRF\ncan faithfully reproduce the shadows and specularities on\nthe forehead in columns (1) - (4) and the shadow cast by the\nnose on the mouth in column (5) of Fig 5.\nIn contrast to prior work, CoDyNeRF is able to faithfully\nreproduce illumination effects and generate high quality ren-\nders. As can be seen in Fig 5, our method captures dynamic\nappearance details, such as shadow patterns and specular-\nities, under varying pose and expression more accurately\nwithout sacrificing the quality of expression and head-pose\narticulation. In Table 1, we provide quantitative evaluation of\nthese four methods. We report image similarity measurement\n(PSNR), perceptual quality (LPIPS) and quality of facial ap-\npearance reconstruction (FaceMSE i.e MSE measured over\n*We analyse this further in the appendix section.\nthe face region only) on novel expressions and head-poses.\nAs we can see, CoDyNeRF outperforms previous approaches\nacross all subjects.\n4.2. Ablation of the Dynamic Canonical apperance\nModel\nIn Table 2, we have ablated the conditioning of the canoni-\ncal appearance model on 3DMM-based inputs and normals.\nFirst, we measured the performance of model with a static\ncanonical space. This model does not use any surface nor-\nmals or 3DMM-based information as input, and as can be\nseen in Table 2, it performs quite poorly. When the sur-\nface normals, expression and pose parameters are added,\nthe results are much better, but the model still lacks global\nhead-position conditioning via the landmarks i.e vlmk, and\nthus is unable to reason about self-shadowing of the face.\nOur full model (third row), with conditioning from the sur-\nface normals, 3DMM expression, pose and the landmarks\nperforms the best.\n5. Limitations and Conclusion\nCoDyNeRF has certain limitations that we address in this\nsection. Similar to prior work, CoDyNeRF is subject specific\nand trains an individual model per scene. It also requires\ntraining sequences of atleast 40 seconds in order to sample\nthe expression and head-pose space sufficiently. CoDyNeRF\nalso does not support relighting with novel lighting. Being\na method that allows photorealistic facial reanimation, Co-\nDyNeRF may have potentially negative societal impact if\nmisused. We discuss this further in the supplementary.\nIn conclusion, we present CoDyNeRF, a method that\nenables the creation of reanimatable Neural 3D Portraits\ncaptured in challenging lighting conditions. Using a dynamic\ncanonical appearance representation, CoDyNeRF is able to\naccurately reproduce illumination effects with varying facial\nexpression and head-pose. Additionally, we also propose\na dynamic normals prediction module that utilizes 3DMM\npriors, along with a importance sampling based regularizer,\nto predict accurate dynamic surface normals. We believe this\nwork takes us a step closer to creating in-the-wild Neural 3D\nportraits that are captured with casual smartphone devices.\n6. Acknowledgements\nThis work was supported in part by the CDC/NIOSH through\ngrant U01 OH012476 and a gift from Adobe.\nReferences\n[1] ShahRukh Athar, Albert Pumarola, Francesc Moreno-Noguer,\nand Dimitris Samaras. Facedet3d: Facial expressions with 3d\ngeometric detail prediction. arXiv preprint arXiv:2012.07999,\n2020. 3\n[2] ShahRukh Athar, Zhixin Shu, and Dimitris Samaras. Self-\nsupervised deformation modeling for facial expression editing.\nIn Face and Gesture, 2020. 3\n[3] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shecht-\nman, and Zhixin Shu. Rignerf: Fully controllable neural 3d\nportraits. In CVPR, June 2022. 1, 2, 3, 4, 6, 7, 8\n[4] Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, and\nTobias Ritschel. X-fields: Implicit neural view-, light-and\ntime-image interpolation. ACM TOG, 2020. 2\n[5] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard\nPons-Moll. Stereo radiance fields (srf): Learning view syn-\nthesis for sparse views of novel scenes. In CVPR, 2021. 2,\n3\n[6] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,\nSunghun Kim, and Jaegul Choo. Stargan: Unified genera-\ntive adversarial networks for multi-domain image-to-image\ntranslation. In CVPR, 2018. 3\n[7] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains. In\nCVPR, 2020.\n[8] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin\nTong.\nDisentangled and controllable face image genera-\ntion via 3d imitative-contrastive learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5154\u20135163, 2020. 3\n[9] M. Doukas, Mohammad Rami Koujan, V. Sharmanska, A.\nRoussos, and S. Zafeiriou. Head2head++: Deep facial at-\ntributes re-targeting. IEEE Transactions on Biometrics, Be-\nhavior, and Identity Science, 3:31\u201343, 2021. 3\n[10] Guy Gafni, Justus Thies, Michael Zollh\u00a8ofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction, 2020. 2, 3\n[11] Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang,\nand Jia-Bin Huang. Portrait neural radiance fields from a\nsingle image. arXiv preprint arXiv:2012.05903, 2020. 2, 3\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 3\n[13] Philip-William Grassal, Malte Prinzler, Titus Leistner,\nCarsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neural\nhead avatars from monocular rgb videos. In CVPR, pages\n18653\u201318664, June 2022. 2, 3, 6, 7, 8, 15, 16\n[14] Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei,\nand Stan Z Li. Towards fast, accurate and stable 3d dense\nface alignment. In Proceedings of the European Conference\non Computer Vision (ECCV), 2020. 6\n[15] Peter Hedman,\nPratul P. Srinivasan,\nBen Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In ICCV, pages\n5875\u20135884, October 2021. 6\n[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\nImage-to-image translation with conditional adversarial net-\nworks. In CVPR, 2017. 3\n[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019.\n[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019.\n[19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. In CVPR, 2020. 3\n[20] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng\nXu, Justus Thies, Matthias Niessner, Patrick P\u00b4erez, Christian\nRichardt, Michael Zollh\u00a8ofer, and Christian Theobalt. Deep\nvideo portraits. ACM TOG, 2018. 3\n[21] M. Koujan, M. Doukas, A. Roussos, and S. Zafeiriou.\nHead2head: Video-based neural head synthesis. In 2020\n15th IEEE International Conference on Automatic Face and\nGesture Recognition (FG 2020) (FG), pages 319\u2013326, Los\nAlamitos, CA, USA, may 2020. IEEE Computer Society. 3\n[22] Marek Kowalski, Stephan J Garbin, Virginia Estellers, Tadas\nBaltru\u02c7saitis, Matthew Johnson, and Jamie Shotton. Config:\nControllable neural face image generation.\nIn Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XI 16, pages\n299\u2013315. Springer, 2020. 3\n[23] Christoph Lassner and Michael Zollh \u02dcA\u00b6fer. Pulsar: Efficient\nsphere-based neural rendering. In CVPR, 2021. 3\n[24] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and expres-\nsion from 4D scans. ACM Transactions on Graphics, (Proc.\nSIGGRAPH Asia), 36(6), 2017. 6\n[25] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green,\nChristoph Lassner, Changil Kim, Tanner Schmidt, Steven\nLovegrove, Michael Goesele, and Zhaoyang Lv. Neural 3d\nvideo synthesis, 2021. 3\n[26] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of\ndynamic scenes. In CVPR, 2021. 2, 3\n[27] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. Adv. Neural\nInform. Process. Syst., 2020. 2\n[28] Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael\nZollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of\nvolumetric primitives for efficient neural rendering, 2021.\n[29] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. arXiv:2008.02268, 2020. 2\n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. 2020. 2, 3, 4, 15\n[31] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf: Unifying neural implicit surfaces and radiance\nfields for multi-view reconstruction.\narXiv preprint\narXiv:2104.10078, 2021.\n[32] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nICCV, 2021. 2, 3, 4\n[33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021. 3, 4\n[34] Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto\nSanfeliu, and Francesc Moreno-Noguer. Ganimation: One-\nshot anatomically consistent facial animation. International\nJournal of Computer Vision, 128(3):698\u2013713, 2020. 3\n[35] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-NeRF: Neural Radiance Fields\nfor Dynamic Scenes. In CVPR, 2021. 2, 3, 4\n[36] Gernot Riegler and Vladlen Koltun. Stable view synthesis. In\nCVPR, 2021. 3\n[37] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, 2016. 6\n[38] Zhixin Shu, Mihir Sahasrabudhe, Riza Alp Guler, Dimitris\nSamaras, Nikos Paragios, and Iasonas Kokkinos. Deform-\ning autoencoders: Unsupervised disentangling of shape and\nappearance. In ECCV, 2018. 3\n[39] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman,\nand D. Samaras. Neural face editing with intrinsic image\ndisentangling. In CVPR, 2017. 3\n[40] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wetzstein.\nScene representation networks: Continuous 3d-structure-\naware neural scene representations. In Adv. Neural Inform.\nProcess. Syst., 2019. 3\n[41] Ayush Tewari, Mohamed Elgharib, Florian Bernard, Hans-\nPeter Seidel, Patrick P\u00b4erez, Michael Zollh\u00a8ofer, and Christian\nTheobalt. Pie: Portrait image embedding for semantic control.\nACM Transactions on Graphics (TOG), 39(6):1\u201314, 2020. 3\n[42] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian\nBernard, Hans-Peter Seidel, Patrick P\u00b4erez, Michael Z\u00a8ollhofer,\nand Christian Theobalt. Stylerig: Rigging stylegan for 3d\ncontrol over portrait images, cvpr 2020. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). IEEE,\njune 2020. 3\n[43] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan. Ref-nerf: Struc-\ntured view-dependent appearance for neural radiance fields.\nIn CVPR, pages 5491\u20135500, June 2022. 2, 3, 5, 6\n[44] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\nOwens, and Alexei A Efros. Cnn-generated images are sur-\nprisingly easy to spot...for now. In CVPR, 2020. 16\n[45] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In CVPR, 2021.\n3\n[46] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In CVPR, 2021. 2, 3\n[47] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neural\nsurface reconstruction by disentangling geometry and appear-\nance. NIPS, 33, 2020. 3\n[48] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario\nFritz. Artificial fingerprinting for generative models: Rooting\ndeepfake attribution in training data. In ICCV, 2021. 16\n[49] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv:2010.07492, 2020. 2, 3\n[50] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv:2010.07492, 2020. 2\n[51] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C.\nB\u00a8uhler, Xu Chen, Michael J. Black, and Otmar Hilliges. I\nm avatar: Implicit morphable head avatars from videos. In\nCVPR, pages 13545\u201313555, June 2022. 2, 3, 8, 12, 13, 16\n[52] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J\nBlack, and Otmar Hilliges.\nPointavatar:\nDeformable\npoint-based head avatars from videos.\narXiv preprint\narXiv:2212.08377, 2022. 6, 8, 13, 14\n[53] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\nUnpaired image-to-image translation using cycle-consistent\nadversarial networks. In ICCV, 2017. 3\nControllable Dynamic Appearance for Neural 3D\nPortraits\n-Appendix-\nContents\n1. Introduction\n2\n2. Related works\n3\n3. CoDyNeRF\n3\n3.1. A 3DMM-guided Deformable Neural Radi-\nance Field . . . . . . . . . . . . . . . . . .\n4\n3.2. An Illumination aware dynamic canonical\nappearance model . . . . . . . . . . . . . .\n4\n3.2.1\nPrediction of Dynamic Surface Nor-\nmals . . . . . . . . . . . . . . . . .\n5\n4. Experimental Results\n6\n4.1. Evaluation on Test Data\n. . . . . . . . . . .\n6\n4.2. Ablation of the Dynamic Canonical apper-\nance Model . . . . . . . . . . . . . . . . .\n9\n5. Limitations and Conclusion\n9\n6. Acknowledgements\n9\n7. Comparisons against IMAvatar\n12\n7.1. Ambient and Non-Ambient Synthetic Dataset\n12\n7.2. Comparisons against IMAvatar on Synthetic\nData . . . . . . . . . . . . . . . . . . . . .\n12\n8. Comparisons against PointAvatar\n13\n9. Ablating the design of the normals network\n13\n10. Details About the Testing Set\n14\n11. Ablation of Spatial conditioning of the density pre-\ndiction model\n15\n12. Ablation of Cauchy Regularization\n15\n13. Additional Metrics\n15\n14. Experimental Details\n15\n15. Societal Impact\n16\n7. Comparisons against IMAvatar\nIn this section we provide details about comparisons against\nIMAvatar\u2020. When IMavatar is trained on our dataset, which\nis captured under strong-non ambient lighting, we find that\nit is unable to converge to a reasonable result (see Fig 7). In\norder to isolate the strong non-ambient lighting as the reason\nfor failure we use synthetic data.\n7.1. Ambient and Non-Ambient Synthetic Dataset\nWe create two synthetic datasets using blender: 1) An \u2018Ambi-\nent Data\u2019 capture setting, which contains a synthetic human\nhead in ambient lighting and 2) A \u2018Non-Ambient Data\u2019 cap-\nture setting with a strong directional light source that causes\ncast shadows and specularities. Both datasets are captured\nusing a fixed camera with changing head-pose but a con-\nstant neutral facial expression. Sample images from both the\ndatasets are shown in Fig 8.\nCoDyNeRF\nRender\nGround Truth\nDepth\nNormals\nLBS Weights\nBlendshape \ufb01elds\nFigure 7. Results of IMAvatar [51] on our dataset. Due\nto the changing illumination effects induced by head-pose\nvariation in strong non-ambient lighting, IMavatar fails to\nconverge.\n7.2. Comparisons against IMAvatar on Synthetic\nData\nWe now compare the results of CoDyNeRF and IMAvatar\n[51] on synthetic data. Our first finding is that, while IMA-\nvatar [51] converges to a reasonable solution on the Am-\nbient Data, it diverges on Non-Ambient Data (see Fig 9).\nThe large appearance changes induced by cast-shadows and\nspecularites in the Non-Ambient data leads to instability in\nIMavatar\u2019s training and it\u2019s consequent divergence. We also\ncontacted the authors of IMAvatar [51] regarding this insta-\nbility; they confirmed that since IMavatar is not designed\n\u2020We\nuse\nthe\ncode\nfrom\ntheir\nofficial\nrepo\nat:\nhttps://github.com/zhengyuf/IMavatar\nAmbient Data\nNon-Ambient Data\nFigure 8. Samples from the Synthetic Datasets described in\nSect 7.1\nfor such strong non-ambient illumination conditions, train-\ning it would be challenging. Quantitative results on both\ndatasets are shown in Table 3. As can be seen, CoDyNeRF\noutperfoms IMavatar in both Ambient and Non-Ambient\nData settings by a large margin.\nCoDyNeRF\nRender\nGround Truth\nDepth\nNormals\nLBS Weights\nBlendshape \ufb01elds\nAmbient Data\nNon-Ambient Data\nFigure 9. Results of IMAvatar [51] on Synthetic Data. Due\nto the changing illumination effects induced by head-pose\nvariation in strong non-ambient lighting, IMavatar fails to\nconverge.\nAmbient Data\nNon-Ambient Data\nModels\nFaceMSE \u2193\nFaceMSE \u2193\nCoDyNeRF (Ours)\n1.03e-4\n1.32e-4\nIMAvatar [51]\n1.56e-2\n-\nTable 3. Quantitative results on synthetic data\n8. Comparisons against PointAvatar\nIn this section we demonstrate the inablity of Pointavatar\n[52] to model specularities and cast shadows using synthetic\ndata described in the Sect 7.1 \u2021. PointAvatar [52] uses a\npoint cloud and an SDF as the geometric representation of\nthe avatar in the canonical space and the deformation from\nthe canonical space to the deformed space is done by an\nMLP that predicts deformation blendshapes. The apperance\nof each point is modelled as a product of an albedo and\nshading. The per-point albedo is predicted by an MLP that\n\u2021We\nuse\nthe\ncode\nfrom\ntheir\nofficial\nrepo\nat:\nhttps://github.com/zhengyuf/PointAvatar\nis conditioned on the canonical position and the shading is\npredicted using an MLP conditioned on the normals. Since\nthe shading network is only condition on the normals, it does\nnot model cast shadows (which depend on global geome-\ntry and lighting) and specularities (which depend on both\nnormals and viewing direction), thus it is unable to learn\nand predict them. As can be seen in Fig 10, CoDyNeRF\nand PointAvatar perform comparably on the dataset with\nambient lighting conditions. However, when the lighting\nis non-ambient, CoDyNeRF performs considerably better.\nFor example, in the top row of Fig 10, we see that PointA-\nvatar [52] is unable to predict the shadow cast by the ear and\nthe specularity on the neck. Similarly, in the bottom row,\nPointAvatar [52] fails to capture the shadow cast by the nose\non the face (however, it does predict the shading correctly).\nIn contrast, CoDyNeRF is able to faithfully reproduce the\ncast shadow of the ear and the specularity on the neck in the\ntop row and the cast shadow of the nose on the face in the\nbottom row. This can also be seen in the quantitative results\nin Table 4, where CoDyNeRF outperforms PointAvatar on\nboth datasets, especially on the non-ambient data.\nAmbient Data\nNon-Ambient Data\nModels\nFaceMSE \u2193\nFaceMSE \u2193\nCoDyNeRF (Ours)\n1.03e-4\n1.32e-4\nPointAvatar [52]\n5.99e-4\n2.33e-3\nTable 4. Quantitative results of on Ambient and Non-Ambient\nSynthetic data described in Sect 7.1. We see that CoDyNeRF\u2019s\nresults are better than PointAvatar [52] on both datasets with the\ndifference being larger on the non-ambient dataset due to PointA-\nvatar\u2019s inability to predict cast shadows and specularities.\n9. Ablating the design of the normals network\nWe design the normals prediction network of CoDyNeRF,\nN, in a manner that allows it to exploit local priors from\nthe 3D head mesh and the scene to predict the normals at\neach point of the dynamic neural portrait. The normals at\nany point x is as follows:\nn = N(Meshn(x), Gradn(x), DistToMesh(x))\n(15)\nThe intuition behind this is that depending on the distance\nof the point to the mesh (DistToMesh(x)) N can rely on\neither the mesh normals from the mesh-vertex closest to x\n(Meshn(x)) or the scene normals (Gradn(x)) to predict the\nnormals. To test the efficacy of this design we ablate each\ninput on non-ambient synthetic data described in Sect 7.1\nby calculating the angular error between the predicted and\nground-truth normals on vertices from the head and neck\nregions. While we have sparse supervision on 5000 ver-\ntices of the head region region through RMesh,n (Eq. 9 of\nGround Truth\nCoDyNeRF\nPointAvatar\nPointAvatar\nNon-Ambient Data\nAmbient Data\nGround Truth\nCoDyNeRF\nFigure 10. Comparison of CoDyNeRF and PointAvatar [52] on Synthetic Data. While PointAvatar performs quite well on\nambient data (on the Left), it fails to accurate predict illumination effects that are present in non-ambient data (Right). In\nthe top row, we see that PointAvatar is unable to predict the cast shadow of the ear and the specularity on the neck. In the\nbottom row, we see that PointAvatar [52] is unable to generate the cast shadow of the nose. In constrast, CoDyNeRF is able to\nreproduce the cast shadow of the ear, the specularity on the neck (top row) and the cast shadow of nose (bottom row) with high\nfidelity.\nNo Meshn(x)\nNo Gradn(x)\nNo DistToMesh(x)\nFull Model\nModels\nAngular Error \u2193\nAngular Error \u2193\nAngular Error \u2193\nAngular Error \u2193\nHead Region\n12.72o\n3.39o\n4.16o\n3.43o\nNeck Region\n8.63o\n6.73o\n5.12o\n4.89o\nTable 5. Ablation of the inputs to the normals prediction network N using\nnon-ambient Synthetic data. Lowest Value , Second Lowest value .\nthe paper), we do not have any supervision on the neck re-\ngion. We measure the angular error on both the head and\nneck regions of an upsampled version of the mesh, sampling\n10k novel vertices from each region. Table 5 shows that\nthe model with all three inputs Meshn(x), Gradn(x) and\nDistToMesh(x) (called \u2018Full Model\u201d) has a low angular er-\nror on both head and neck regions. Not having Meshn(x) as\ninput hurts normals prediction accuracy of both regions, not\nhaving Gradn(x) hurts the the neck region accuracy only.\n10. Details About the Testing Set\nWe make sure that the test set is completely held out from\nthe training set by using the last 120-150 frames of the video\nas the test set. Similar to RigNeRF [3], the test set contains\nhead-poses that have relative angles to the closest training\nranging from 0o \u2212 15o. In Fig 11, we have recreated Fig 5\nfrom the paper with the closest training set frame to provide\ncontext to the reanimation results.\nGround Truth\nCoDyNeRF\n(1)\n(2)\n(3)\n(4)\nClosest  \nTraining Frame\n\u0394Pose Angle: 15o \n\u0394Pose Angle: 9o \n\u0394Pose Angle: 7o \nPose Angle: 14o \n\u0394\n(5)\nPose Angle: 8.5o \n\u0394\nFigure 11. Test set reanimation with closest training frames\nMethod\nPSNR (\u2191)\nLPIPS (\u2193)\nFaceMSE (\u2193)\nCoDyNeRF\n23.46\n0.40\n1.93e-3\nCoDyNeRF wo Cauchy Regularization\n22.5\n0.51\n2.34e-3\nTable 6. Ablation for the Cauchy Regularization.\n11. Ablation of Spatial conditioning of the den-\nsity prediction model\nCoDyNeRF predicts the density at each point, x, using\nthe canonical position of the point xcan and the distance\nof the point from the mesh in the deformed space i.e\nDistToMesh(x). Such a conditioning allows F to distin-\nguish between points in the canonical space that have never\nbeen deformed and points that have been deformed to the\ncanonical space, leading to faster convergence and better\nresults. In Table 7, we measure the test-set PSNR of CoDyN-\neRF with and without conditioning on DistToMesh(x) (Co-\nDyNeRF and CoDyNeRF (w/o spatial cond.) respectively)\non two subjects. As can be seen, CoDyNeRF converges to a\nhigher PSNR much faster when the density is conditioned\non the distance to the mesh (i.e DistToMesh(x)).\n12. Ablation of Cauchy Regularization\nWe ablate the use of the cauchy regularization (Eq. 14 of the\npaper) to improve underlying scene normals. We use cauchy\nregularization to ensure the density distribution along the ray\nis sparse, thus ensuring there are only a few points of high\ndensity along the ray. This allows the importance sampling\nbased regularization of the normals (Eq 13) to work more\neffectively, as it can act only on the top k = 30 points and\ncan ignore the remaining points.\nAs can be seen quantitatively in Table 6 and qualitatively\nin Fig 12, the cauchy regularization leads to significantly\nsharper renders and fidelity of normals. In the top row of\nFig 12, we observe that both the render of both the face\nregion and the background is sharper when using the Cauchy\nloss. In the bottom row, we see that the face and background\nnormals are both significantly more accurate. This improve-\nment in quality is also reflected in the quantitative metrics as\ndemonstrated in Table 6.\n13. Additional Metrics\n. We also evaluate the SSIM metric on the real data and\nreport it in Table 8. As can be seen, CoDyNeRF outperforms\nprior work across all subjects. In Fig 13, we show an illus-\ntrative example of the region over which the FaceMSE i.e\nMSE over the face region, is calculated.\nGT\nW/O Cauchy Loss\nWith Cauchy Loss\nRender\nNormals\nFigure 12. Qualitative comparison between model with\nand without a Cauchy Loss.\nFigure 13.\nAn example of the region over which the\nFaceMSE is calculated.\n14. Experimental Details\nAll our models were trained on 4 A100 GPUs (with 40GB\nVRAM each) using 128 samples per ray and a batch-size of\n1000 rays. For all methods, we use 10 frequencies to encode\nthe position of a point and 4 frequencies to encode the direc-\ntion of the ray as input to F. For CoDyNeRF and RigNeRF,\nwe use 10 frequencies to encode the position of a point as\ninput to D. We also use a coarse-to-fine regularization on\nthe positional encoding of the position input to D for 10k\nepochs. The architecture of F is identical to the original\nNeRF MLP [30] with 8 layers of 256 hidden units each. We\nuse the same F for all our methods. D consists of 8 layers\nwith 128 hidden units in each. We use the same architec-\nture for D for both CoDyNeRF and RigNeRF. The normals\nnetwork N consists of three layers with 128 hidden units.\nWe train CoDyNeRF and RigNeRF with an initial learning\nrate of 5e\u22124 which is decayed to 5e\u22125 by the end of training.\nFor Neural Head Avatars [13] we follow the training settings\ngiven in the config file of the publically available github\nrepo. Similarly, for PointAvatar we follow the training set-\nting given in the publically available github repo, with the\nSubject\nMethod\nEpochs Trained\nPSNR\nPSNR after 25k more training epochs\nSubject 1\nCoDyNeRF\n200000\n23.46\n23.51\nCoDyNeRF (w/o spatial cond.)\n200000\n22.31\n22.37\nSubject 2\nCoDyNeRF\n200000\n21.24\n21.31\nCoDyNeRF (w/o spatial cond.)\n200000\n20.97\n21.13\nTable 7. Training convergence of CoDyNeRF with and without spatial conditioning of the density using DistToMesh(x). As\ncan be seen, using spatial conditioning boosts both the rate of convergence and the final PSNR.\nSubject 1\nSubject 2\nSubject 3\nSubject 4\nModels\nSSIM \u2191\nSSIM \u2191\nSSIM \u2191\nSSIM \u2191\nCoDyNeRF (Ours)\n0.83\n0.76\n0.73\n0.81\nRigNeRF [3]\n0.71\n0.73\n0.68\n0.76\nNHA [13]\n0.58\n0.61\n0.55\n0.48\nPointAvatar [51]\n0.64\n0.65\n0.67\n0.73\nTable 8. SSIM on test data. Highest Value .\nonly difference being that we train with 220k points instead\nof 409000 points due to GPU memory constraints.\n15. Societal Impact\nSince CoDyNeRF is capable of reanimating face, it is prone\nto misuse by bad actors to generate deep-fakes. However,\nthe authors of [44] show that it is possible to train discrimi-\nnative classifiers to detect images and videos generated by\nsynthetic methods like CoDyNeRF. Another possible solu-\ntion to detect synthetically generated faces is to use works\nsuch as [48] to watermark training images in order to detect\nreal or fake images.\nSubject\nMethod\nEpochs Trained\nApp Code dim\nDef Code dim\nSubject 1\nCoDyNeRF\n200000\n8\n8\nRigNeRF\n200000\n8\n8\nSubject 2\nCoDyNeRF\n200000\n8\n8\nRigNeRF\n200000\n8\n8\nSubject 3\nCoDyNeRF\n200000\n8\n8\nRigNeRF\n200000\n8\n8\nSubject 4\nCoDyNeRF\n200000\n8\n8\nRigNeRF\n200000\n8\n8\nTable 9. Training configuration for all the experiments.\n"
  }
]