[
  {
    "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models",
    "link": "https://arxiv.org/pdf/2308.00675.pdf",
    "upvote": "34",
    "text": "Tool Documentation Enables Zero-Shot\nTool-Usage with Large Language Models\nCheng-Yu Hsieh1\u2020, Si-An Chen2\u2020, Chun-Liang Li3, Yasuhisa Fujii4,\nAlexander Ratner1, Chen-Yu Lee3, Ranjay Krishna1\u2217, Tomas Pfister3\u2217\n1University of Washington, 2National Taiwan University,\n3Google Cloud AI Research, 4Google Research\ncydhsieh@cs.washington.edu\nAbstract\nToday, large language models (LLMs) are taught to use new tools by providing a\nfew demonstrations of the tool\u2019s usage. Unfortunately, demonstrations are hard to\nacquire, and can result in undesirable biased usage if the wrong demonstration is\nchosen. Even in the rare scenario that demonstrations are readily available, there is\nno principled selection protocol to determine how many and which ones to provide.\nAs tasks grow more complex, the selection search grows combinatorially and in-\nvariably becomes intractable. Our work provides an alternative to demonstrations:\ntool documentation. We advocate the use of tool documentation\u2014descriptions for\nthe individual tool usage\u2014over demonstrations. We substantiate our claim through\nthree main empirical findings on 6 tasks across both vision and language modalities.\nFirst, on existing benchmarks, zero-shot prompts with only tool documentation\nare sufficient for eliciting proper tool usage, achieving performance on par with\nfew-shot prompts. Second, on a newly collected realistic tool-use dataset with\nhundreds of available tool APIs, we show that tool documentation is significantly\nmore valuable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight the\npossibility of using tool documentation to automatically enable new applications:\nby using nothing more than the documentation of GroundingDino, Stable Diffu-\nsion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released\nGrounded-SAM [23] and Track Anything [70] models.\n1\nIntroduction\nToday, large language models (LLMs) summon the imagery of a craftsman: when asked to solve a\ncomplex task, they decompose the task into simpler sub-tasks and assemble the best possible tools to\ntackle each sub-task [51, 72]. For example, consider the complex task of question answering given\nthe image in Figure 1. To answer \u201cwhether the two magnets will attract or repel each other\u201d, the LLM\nneeds the following: it needs to identify the positions of the magnets in the image, extract general\nknowledge explaining that \u201copposite (same) poles attract (repel)\u201d. Just like a competent craftsman\nwho knows what their tools are capable of, an LLM with such knowledge of its tools will be able\nto invoke one tool (e.g. its Text Detector) to identify the north and south poles and a second tool\n(e.g. Knowledge Retriever) to extract pertinent background knowledge about magnetic forces. But\nhow does an LLM know which tool is capable of what?\n\u2020Work done as student researchers at Google Cloud AI Research.\n*The authors contributed equally to this work.\nPreprint. Under review.\narXiv:2308.00675v1  [cs.CL]  1 Aug 2023\nFigure 1: Example workflow of tool-using with LLMs to solve a multi-modal question answering\ntask. Given the input question with an image, the LLM selects appropriate tools from the tool set\nand generates an execution plan to answer the question correctly. Here, the LLMs outlines a plan\nto first use Text Detector to understand the positioning of the magnets in the image, then leverage\nKnowledge Retriever to obtain relevant background knowledge about magnets, then finally generate\nthe solution based on the previous steps.\nCurrently, LLM tool-usage provides LLMs with few-shot demonstrations (demos) of what its tools can\ndo, hoping that these demos will help generalize the model\u2019s behavior to newer complex tasks. This\nprocess has been rather successful so far. These few-shot demos contain one or several exemplars\nof <input, output> mappings [68] on given instructions and their corresponding tool-use plans\n(illustrated in Figure 2). LLMs are expected to find patterns within these demos and generalize\nthem for new tasks. On textual tasks, LLMs have presented with demos of calculators [15, 47, 56],\nPython interpreters [13, 18] and search engines [62, 43, 50, 56, 40] can perform logical and arithmetic\noperations to obtain more accurate and factual knowledge. On visual tasks, LLMs with demos of\npretrained vision models can do complex visual reasoning [37, 40, 57, 16, 73], can generate and\neven edit images [19, 9]. On embodied robotic tasks, LLMs can similarly be used to reason and\nplan [75, 21, 1, 17].\nWe argue that this reliance on demos in tool using is unnecessary in some cases, and might be even\nlimiting. In fact, recent work finds that LLMs tend to be sensitive to demos [81], and carefully\nselecting demos is needed to avoid biasing or overfitting to a particular usage [12]. This leads to the\nfollow-up question: how do we choose which few-shot demos to use? There are no known principled\napproaches to select demos without human intervention or to even efficiently enable humans to choose\nor create them. To make the matter worse, when we scale up the number of tools that LLMs have\naccess to, this few-shot selection process becomes combinatorially intractable. Just as a craftsman\ndoesn\u2019t need to see a new tool being demonstrated and can instead discern their capabilities from\nreading a user manual for the tool, we seek to enable LLMs to learn how to use tools without seeing\nany demos.\nOur work provides an alternative to demonstrations: tool documentation (doc). Similar to the\nmetaphor of a manual indicating an physical tool\u2019s capabilities, a software tool\u2019s docs outline what\nthe tool can and cannot be used for and how to invoke it. Docs provide relatively neutral instruction\nabout the tools\u2019 functionalities and how individual tools should be used (illustrated in Figure 2), and\nthey are usually conveniently available through the creation of the tools organically. Intuitively, just\nas the craftman leans to use a new tool by reading the manual, we provide LLMs with README files\nwhen encountering a new tool/repository. With docs, an LLM may not necessarily need demos to use\na new tool.\nDistinct from existing work that rely mostly on few-shot demos for tool-learning, in this work,\nwe study whether LLMs can instead solely rely on docs to use tools. We study the tool-learning\nperformances of LLMs as we include or exclude docs, and vary the number of demos from few-shot\ndown to zero-shot. We conduct the experiments on 6 tasks across vision and text modalities. Our\nexperiments show that:\n\u2022 Surprisingly, when provided with tool docs, LLMs\u2019 zero-shot tool-using performance is on par\nor even better than their few-shot counterparts, showing that including docs is an effective way\nto sidestep the few-shot demos needed.\n2\nDemonstration: Below are examples mapping a problem to a tool-use plan.\n\u25cf\nQuestion: Which property do these three objects have in common?\nTool-use Plan:\nText Detector \u2192 Knowledge Retriever \u2192 Solution Generator\n\u25cf\nQuestion: Which material is this jar made of?\nTool-use Plan: \nImage Captioner \u2192 Solution Generator\nDEMO\nChips\nPretzel\nFries\nDemonstration \nDescription: examples of questions and the tool-use plan.\n\u25cf Question: \nWhich property do these objects have in common?\nTool-use Plan:\nText Detector \u2192 Knowledge Retriever \u2192 Solution Generator\n\u25cf Question: [\u2026]\nTool-use Plan: [\u2026]\n\u25cf Question: [\u2026]\nTool-use Plan: [\u2026]\nDocumentation \nDescription: available tools and their functionalities.\n\u25cf Text Detector:\nIt detects the text in an image [\u2026]\n\u25cf Knowledge Retriever: \nIt retrieves relevant knowledge [\u2026]\n\u25cf Search Engine: \nIt searches the web for relevant info [\u2026]\n\u25cf Image Captioner: \nIt generates a caption for an image [\u2026] \n\u25cf \u2026\nFigure 2: Two types of knowledge for prompting LLMs for tool-use: Demonstrations (demos) and\nDocumentations (docs). Demos consist of <input, output> pairs on input instructions and their\ncorresponding output tool-use plans. They require manual efforts for careful curation on every new\ntask, and the model performance can be sensitive to which demos are used [81, 12]. Many demos\nmay also be necessary for good coverage when the number of tools scales up. On the other hand,\ndocs provide descriptions for the tool functionality, and are usually organically available for tools.\n\u2022 Building on the above finding, we relax the few-shot demo constraint, and show that we can\nefficiently scale up to a significantly larger tool set, on a newly collected API usage dataset, by\nsimply providing the LLMs with docs.\n\u2022 We show how to seamlessly add new tools along with their docs to a tool set for LLMs to\nsolve unseen tasks on image editing and video tracking, all without any further demos in a\nplug-and-play manner.\n\u2022 Finally, with unseen tools developed recently as building blocks, we showcase LLMs are\ncapable of re-inventing popular yet even more recent works Grounded-SAM [23] and Track\nAnything [70], which suggests a potential from zero-shot tool usage to automatic knowledge\ndiscovery.\n2\nRelated work\nLLMs with retrieval augmentation and tools.\nIn spite of the remarkable achievements demon-\nstrated by LLMs, the performance can be further boosted with external tool usages to be more\naccurate, efficient or versatile for wider applications. The authors in [51] detailed the cognitive\norigins, the paradigm shift of foundation models, and the complementary roles of tools and models to\nLLMs. The example tool usage starts from knowledge retrieval [6, 20, 33, 74, 77] and expands to\nsearch engine [43, 31, 32, 62, 58, 46, 40], QA system [56], calculator [15, 47, 56], the Python inter-\npreter [18, 13, 65, 24, 46, 16], simulation engines [37], machine learning models [57, 73, 69, 40, 16],\nor even tools created by LLMs [11]. Pioneer works of LLMs with tools often rely on human su-\npervision [62, 31] or additional self-supervised learning techniques [56], which pose challenges for\npractical plug-and-play usage. Recent advancements eliminate additional training by using example\ndemos in the prompt [19, 75, 73, 57, 40, 46]. Our work further simplifies prompt design by only\nleveraging documentation for individual tools, while maintaining competitive performance.\nPlanning with LLMs.\nLanguage models are proven to have potential to conduct planning for\nsolving complex tasks or decompose the complex tasks into sub-problems when prompted properly.\n[21, 22] retrieve demos at test-time with large knowledge space coverage to generate admissible\nactions. [28] relies on pre-designed demos for task decomposition. Similarly, recent works of tool\nusing with LLMs leverage the example demonstrations of solving examples tasks with a planning\nof tools [13, 19, 75, 73, 57, 40, 46]. However, crafting demos of interactions between tools may be\nchallenging in practice when the number of tools surges. Concurrent work [48, 52, 71] tackles the\nchallenge by using strong LLMs such as GPT-4 [45] to create large instruction-following datasets\nthat cover diverse instructions and corresponding tool-use plans, typically through mechanisms like\n3\nself-instruct [66]. The resultant datasets can then be used to finetune and equip other LLMs (e.g.,\nLLaMA [63] and OPT [79]) the ability to use a large collection of tools for unseen instructions. On\nthe other hand, our work showcases the potential for LLMs to utilize any unseen new tools by reading\ntheir tool docs.\nDemonstration and Documentation.\nLearning from demonstration is popular in reinforcement\nlearning [49, 4, 44, 55]. [8] propose the in-context learning algorithm for efficient and effective\ndownstream task adaptations through showing example demonstrations. Inspired by the success,\nmost of existing LLM tool-using works rely on few-shot demonstration [13, 19, 75, 73, 57, 40, 46].\nHowever, [12] show that having more example demonstration might counter-intuitively degrade\nperformance, and a careful selection might be needed. [35] proposes a retrieval method for demo\nselection, which implicitly requires a larger set of examples to be selected. Using documentation\nto improve algorithms is relatively under-explored. [7, 82] propose document reading algorithms\nfor specific games. [83] introduced DocPrompting, which employs a trained retriever on the given\ntraining data to boost code generation by retrieving relevant documents. In this work, we take a step\ntowards exploring the zero-shot tool planning in LLMs solely with the aid of documentation, and\ninvestigate a wide range of diverse tasks from language to vision domains. While [64, 42] showcase\npure zero-shot planning capability of LLMs, they do not study either the tool usage or the unseen\nscenarios to the language models. ViperGPT [16] is a concurrent work, which focuses on visual\nprogramming in Python and uses function implementations and specifications as documentation.\nLastly, while AutoGPT [3] provides several demos that showcase the LLM\u2019s capability of tool\nusing through documentation reading, our study focuses on a systematic exploration ranging from\nreal-world use cases to academic benchmarks.\n3\nExperimental setup\n3.1\nGeneral workflow\nWe follow the general framework of tool-using with LLMs in [51], which encompasses many of\nthe recent works [75, 27, 19, 57, 73, 69, 40]. Specifically, given a natural language instruction, an\nLLM planner generates a program to be sequentially executed where each step of the program may\nrely on using tools selected from a tool set. After the program is generated, it is then executed\nby an environment which finally returns the execution results. Here, the program extends beyond\nconventional coding practice [76, 53, 25] and is more closely associated with automata theory [59]:\na set of instructions of automations (e.g. tools in our case). Therefore, the tool set can be libraries\nwith specific programming languages (e.g. Python), or general computation with properly defined\ninput-output, such as trained models, API calls, and beyond.\n3.2\nTool-use prompting methods\nAs discussed in Section 1, two main types of information are considered in prompting LLMs\nfor tool-using plans: demonstrations (demos) and documentations (docs). Demos showcase how\ntool interactions can accomplish specific tasks, while docs describe individual tool functionalities\nwithout task-specific ties as shown in Figure 2. In the experiment, we explore combinations of\nincluding/excluding docs and demos in prompts, as well as varying numbers of demos.\n3.3\nEvaluation tasks\nWe conduct our experiments on 6 tasks across multiple modalities with a variety of tool sets. We\ndescribe the setup and the tool sets for each task below. Except for specific cases where it is explicitly\nspecified, the LLM planner is ChatGPT (gpt-3.5-turbo).\nMulti-modal question answering on ScienceQA. ScienceQA [39] consists of multi-modal multiple-\nchoice science questions that requires language and visual understanding as well as domain-specific\nknowledge to answer correctly. On ScienceQA, we follow the setup used in Chameleon [40] and\nemploy the same tool set with 7 tools, such as the search engine and the image text detector.\nTabular math reasoning on TabMWP. TabMWP [41] is a math reasoning dataset with various\nforms of tables. It requires a model to understand structured or domain-specific tables, and utilize the\n4\nZero Shot\n\u25cf\nllmcloud firewall allow my_vm --port \n8000 --protocol tcp [Hallucination]\n\u25cf\ntouch my_file\n\u25cf\nscp -P 8000 /path/to/my_file \nuser@server_ip:~  [Wrong command]\n\u25cf\n[Missing topic creation]\n\u25cf\nllmcloud publish-message --project \nPROJ --message \"Hi\"  [Hallucination]\nDocumentation\n\u25cf\nllmcloud comp firewall-rules create \nNAME --allow tcp:8000\n\u25cf\ntouch my_file\n\u25cf\nllmcloud \ncomp \nscp \n--port \n8000 \nmy_file my_vm:./\n\u25cf\nllmcloud pubsub topics create TOPIC\n\u25cf\nllmcloud pubsub topics publish PROJ \n--message \"hi\"\nFew Shot\n\u25cf\nllmcloud comp firewall-rules \ncreate NAME --allow tcp:8000\n\u25cf\ntouch my_file\n\u25cf\nllmcloud comp scp --P 8000 \nmy_file my_vm:./  [Wrong flag]\n\u25cf\n[Missing topic creation]\n\u25cf\nllmcloud pubsub topics publish \nPROJ my-topic --message \"Hi\"\nQuestion: \n\u25cf\nHere is a new cloud service called LLMVM, which provides its \nown SDK CLI tool (llmcloud). \n\u25cf\nCreate a  firewall allowing port 8000. Please touch a file \nmy_file then copy it to server with port 8000. \n\u25cf\nLast, please publish a message \u201cHi\u201d to the project.\nAnswer (in GCP)\n\u25cf\ngcloud \ncompute \nfirewall-rules \ncreate \nNAME \n--allow tcp:8000\n\u25cf\ntouch my_file\n\u25cf\ngcloud compute scp --port 8000 my_file my_vm:./\n\u25cf\ngcloud pubsub topics create TOPIC\n\u25cf\ngcloud pubsub topics publish PROJ --message \"hi\"\nFigure 3: The new LLM Cloud Platform command-line toolkit, which is an unseen toolset to existing\nLLMs based on real-world Google Cloud command-line tools through renaming.\ninformation to answer corresponding math questions. On TabMWP, we also follow Chameleon [40]\nwith the same tool set with 9 tools, such as program generator and column lookup.\nMulti-modal reasoning on NLVRv2. NLVRv2 [60] requires the model to verify whether a statement\nis true on a pair of images, requiring compositional understanding of both texts and images. On\nNLVRv2, we follow the setup used in Visual Programming (VisProg) [19] with 20 vision modules\n(tools) for image understanding and manipulation. Since VisProg only relies on few-shot demonstra-\ntions and does not utilize documentations for the modules. We generate the documentation for each\nmodule by including descriptions on the functionality of the module and the function signature. We\nprovide the full documentations we use for each module in the appendix.\nUnseen API usage on a newly collected dataset. Existing benchmarks used in literature come\nwith a limited set of tools. To explore real-world use cases involving a large number of tools, we\ncollect a new benchmark called the LLM Cloud CLI that consists of 200 commands representing the\nfunctionalities of the Google Cloud Platform (GCP) command-line interface (CLI). Each command\nin our CLI is renamed from its corresponding GCP command, preserving the semantics and logic\nof the original tools, while being unseen to the language models. For instance, the command\ngcloud compute create NAME, responsible for creating a virtual machine, is renamed to be\nllmvm compute make NAME. The renaming conventions also allow us to utilize authentic GCP\nexamples as few-shot demos and leverage the corresponding GCP documentation. The benchmark\ncomprises 50 questions, each focused on creating and configuring specific cloud services using\ncommand-line tools. Each question requires at least two commands to complete the task. We show\nan example in Figure 3, and include more in appendix.\nDue to the length constraints of the LLM we use, we cannot fit documentation of 200 tools in a single\nprompt. Therefore, we employ a simple TF-IDF search using the questions as queries to retrieve the\nmost relevant documentations and truncate them to fit within the prompt length. More details can be\nfound in the appendix.\nImage editing with natural language. We consider image editing as a form of qualitative evaluation.\nThis process calls for the model to plan and use different vision modules to handle complex natural\nlanguage instructions. For instance, to execute an instruction like \"replace the red bus with a green\nbicycle\", the model must localize the red bus, generate its segmentation mask, and then inpaint\nthe masked area. We use the tool sets from VisProg. Unlike VisProg, which depends on few-shot\ndemonstrations, our model only looks at the module documentation. We further include the recently\nreleased image understanding works, Segment Anything (SAM) [30] and Grouding DINO [38] to\nexpand the tool set to test the zero-shot capability on the new and unseen tools in a plug-and-play\nfashion.\nVideo tracking. Video tracking is also utilized in this study as a qualitative evaluation. This task aims\nto acquire the masks of a tracked object in each frame of a video, necessitating the deployment of\nprocesses such as object localization, segmentation, and tracking. In addition to SAM and Groudning\nDINO, we incorporate the documentation of an unseen object tracking module, Xmen [14] into the\nVisProg framework with the aim to showcase the model\u2019s ability to adapt and employ new tools\nwithout the need for explicit demonstrations again on a different task.\n5\nFigure 4: Tool-using performance with gpt-3.5-turbo on different benchmarks, which covers\nfrom langauge to vision modalities. We report results with and without documentation (doc) and\ndemonstations (demo), and their combinations. Clearly, with documentation only (upper-left blue\ndot) shows competitive performance across all datasets.\n4\nEmpirical findings\nWe showcase the importance of tool documentation in three-fold: First, we show that tool documen-\ntations reduces the need of demonstrations (Section 4.1). Second, based on the finding, we further\nshow that relying on documentation rather than demonstrations provides a more scalable solution\nto equip LLMs with a large number of available tools (Section 4.2). Finally, we show that with\ntool documentations alone, LLMs are able to comprehend and utilize most recent vision models to\naccomplish impressive results on image editing and video tracking tasks, on which existing results\nare achieved either with human-crafted demos or predefined procedures (Section 4.3).\n4.1\nDocumentations sidestep the need for demonstrations\nIn this section, we show how tool documentations reduce the need of demonstrations. We present the\nfindings on three datasets: ScienceQA, TabMWP, and NLVRv2. We evaluate the model performance,\nwith and without tool documentations, across varying number of demonstrations (demo) on each\ndataset.\nIn Figure 4, we see that when provided with tool docs, the model is able to maintain stable performance\nas we strip away the number of demos used. In fact, without using any demos (i.e., 0-shot), the\nmodel is able to achieve on par performances to using 16-shot on TabMWP, and using 12-shot on\nNLVRv2. On ScienceQA, the model can even achieve better performance solely with docs compared\nto additionally using 10-shot demos. On the other hand, without tool docs, the model performance is\nvery sensitive to the number of demos used. As we decrease the number of demos, we see significant\nperformance drop on all three datasets. This highlights the importance of tool docs and shows that it\nprovides an effective way to reduce the reliance on demos. In Table 1, when compared to existing\nbaseline methods, we also see that with doc, even 0-shot can perform very competitively.\nBy sidestepping the need for demos, we are able to alleviate the efforts needed to carefully curate\nthese demos. For example, aligned with recent studies [81, 12], we observe in Figure 4 that the model\nperformance is sensitive to which demos are used, shown by the large performance variances under\n5-shot on ScienceQA and 2-shot on NLVRv2.\n4.2\nDocumentations enable efficient scaling on tool-using\nThe findings in Section 4.1 show that one can in fact reduce the reliance on few-shot demos with tool\ndocs. By relaxing this constraint, we study whether tool docs enables a more scalable way to equip\nLLMs with a large number of tools, wherein few-shot demos can specifically fall short on covering\nlimited tool-use cases. We present our findings in this section on the newly collected LLM Cloud\nCLI dataset with 200 available tools.\nQualitative walk-through result.\nFigure 3 serves as a qualitative example illustrating the limita-\ntions of the LLMs with different information. As expected, zero-shot LLM successfully identifies and\nresponds to the touch command, which is familiar and well-known. However, when faced with the\n6\nTable 1: Comparisons to existing baseline methods on different benchmarks. We follow [40, 19]\nto select the beasline methods for each benchmark task. We see that 0-shot with doc performs\ncompetitively, outperforming CoT and PoT on ScienceQA and TabMWP. On NLVRv2, ViLT-NLVR\nis finetuned on the dataset, while the LLM performs in a zero-shot fashion.\nBenchmark\nMethods\nCoT [67]\nwithout doc (0-shot)\nwith doc (0-shot)\nScienceQA\n78.54\n78.25\n79.91\nPoT [13]\nwithout doc (0-shot)\nwith doc (0-shot)\nTabMWP\n89.28\n84.13\n92.69\nViLT-NLVR [29]\nwithout doc (0-shot)\nwith doc (0-shot)\nNLVRv2\n76.30\n0.00\n63.40\nFigure 5: Command planning of LLM Cloud Platform CLI with and without documentation (doc)\nand demonstations (demo), and their combinations. Few-shot demonstration without documentation\nresults in unsatisfactory performance due to low coverage of large number of tools, while reading\ndocumentation significantly boosts the performance.\nunseen LLM-Cloud command lines, the zero-shot LLM fails to generate accurate responses involving\nthese unfamiliar tools due to its lack of knowledge regarding their syntax and usage.\nWhile few-shot demonstrations have the potential to enhance model performance, it is important\nto acknowledge that the coverage of these demonstrations is limited due to the vast number of\ncommand-line tools. Consequently, certain commands or flags may not be adequately covered. In\nFigure 3, although we observe data copying is commonly appeared the few-shot examples, however,\nthe model encounters difficulties in correctly configuring the less common flag --port, instead\nhallucinating the use of -P based on familiarity with the scp -P command in Linux.\nConversely, in the same example illustrated in Figure 3, by solely utilizing the provided documentation,\nthe language models not only successfully discern the steps required for utilizing tools (such as a\nhidden step of creating a topic before sending messages), but also possess the ability to accurately\nconfigure flags (e.g., --port) by leveraging information extracted from the documentation.\nQuantitative comparisons.\nWe calculate the command-line level F1 score of each example and\nreport the average F1 across 50 examples. Figure 5 showcases the performance of various LLMs in the\nzero-shot setting, where they have no prior exposure to the LLM-Cloud command-line tools we create.\nAs anticipated, all zero-shot LLMs demonstrate low F1 scores. Zero-shot text-davinci-002\nachieves an F1 score of 0.02, while the gpt-3.5-turbo model achieves a slightly higher score of\n0.13. The improved performance of the gpt-3.5-turbo model can be attributed to better handling\nof common Linux commands, such as touch. As mentioned in quantitative comparison, few-shot\ndemos improve upon zero-shot, but still fail on uncovered commands or flags in the demo. Therefore,\nthe best few-shot demo in text-davinci-002 and gpt-3.5-turbo are only with 0.05 and 0.19\nF1 scores respectively. On the other hand, LLM with documentation boosts the performance by a\nlarge margin to be 0.37 in text-davinci-002 and 0.45 in gpt-3.5-turbo .\n7\nFigure 6: Plug-and-play new vision tools without demonstration. We add GroundingDINO [38],\nSegment Anything (SAM) [30], XMem [14] as new tools for VisProg. Solely with the documentations\nof the new tools, the LLM is able to automatically \u201cre-invent\u201d recent Grounded-SAM [23] and\nTrack Anything [70] without knowing these derivatives, taking a further step toward automatic\nknowledge discovery.\nWe further compare the performance of the documentation reading with that of the documentation\nsupplemented with few-shot demonstrations. In the case of text-davinci-002 , with documen-\ntation only, we achieves an F1 score of 0.37. Conversely, the documentation augmented with\ndifferent shots yields an average F1 score of 0.35. Similarly, in the gpt-3.5-turbo experiment, the\nperformance with different shot demonstrations (0.44, 0.44, 0.42) are consistently lower than the\ndocumentation-only performance (0.45).\nThese results highlight two observations. First, the performance of the model is highly sensitive to the\nselection of few-shot demonstrations. The observation aligns the finding in [12] that more few-shot\ndemos might be redundant and even degrade performance due to spurious correlations. It emphasizes\nthe importance of careful selection and design, which may involve more human effort. Second, the\nzero-shot documentation reading baseline exhibits remarkable robustness and delivers competitive\nperformance across both examples. This highlights the potential value and reliability of relying solely\non the documentation, which is usually easy to get in many packages and tools.\n4.3\nPlug-and-play with new image and video tools\nIn this section, we validate that one can equip LLMs with unseen tools to solve novel tasks solely\nwith tool docs, and without any further demos. We present our results on image editing and video\ntracking tasks. We show that LLMs can effectively re-invent existing human-programmed image\nediting and video tracking pipelines, backed by state-of-the-art vision models to achieve impressive\nresults.\nRecent advancements in vision models, including GroundingDINO [38], an advanced open-set object\ndetector; Segment Anything (SAM) [30], a cutting-edge image segmentation tool; and XMem [14], a\n8\nstate-of-the-art video object segmentation tool, accompany the progress of language models. These\nbreakthroughs, emerging in the past year, serve as additional tools that are yet unfamiliar to our\nLLM (gpt-3.5-turbo). By expanding VisProg to include these new tools, we embark on the\nintriguing exploration of whether LLMs can effortlessly comprehend the documentation associated\nwith these new models, and combine these tools in a plug-and-play manner, enabling a wide range of\napplications.\nIn Figure 6, when performing an image editing request \u201creplace the bench with a blue sofa\u201d, the\nLLM generates a VisProg program that harnesses the power of GroundingDINO and SAM from the\nexpanded tool set to segment the bench, and apply the stable diffusion [54] for synthesizing the sofa.\nThis program re-invents the wheel by replicating the behavior of recent popular project, Grounded-\nSAM [23] without prior knowledge of this repository. Similarly, when tasked with video tracking\n\u201ctrack the cat in the video\u201d, the generated VisProg program by the LLM incorporates GroundingDINO\ntogether SAM for first frame segmentation as the initialization for XMem to do video tracking. It\nagain re-invents the results obtained in the contemporary work, Track Anything [70]. We note that\nTaskMatrix [69] also has an updated approach with Grounded-SAM. However, they pre-program the\nentire Grounded-SAM editing pipeline as an image editing function, allowing the LLM to control it\nrather than enabling the LLM to generate the editing program using the building tools alone as we\npresent here.\nBy successfully re-inventing the functionalities of Grounded-SAM and Track Anything without prior\nknowledge, solely relying on the available building blocks, the LLM demonstrates not only its capacity\nto effortlessly comprehend and combine new tools with documentation only but also highlights its\npotential for automatic knowledge discovery. It discovers new insights through leveraging its existing\nknowledge only without further demonstration.\n4.4\nPerformance v.s. documentation quality\nWe investigates the impact of documentation quality on performance. To assess LLM\u2019s capability\nto comprehend realistic documentation, we refrain from engineering or curating the content of the\ndocumentation. Instead, we vary the document length by truncating the documents and keeping the\nfirst n words, using it as a proxy for assessing thoroughness and quality. In this ablation, we consider\nthe LLM-Cloud benchmark, which has long documentation based on real-world GCP CLI manuals.\nWe illustrate the result in Figure 7.\n200\n300\n400\n500\n600\n700\n800\nDocumentation Length\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nF1 Score\ngpt-3.5-turbo (doc)\ntext-davinci-002 (doc)\ngpt-3.5-turbo (best 15 shots)\ntext-davinci-002 (best 15 shots)\nFigure 7: Performance of zero-shot documentation LLM when varying the input document length.\nIn both text-davinci-002 and gpt-3.5-turbo experiments, we consistently observe a trend\nwhere performance improves as the document length increases, up to a length of 600. This finding\naligns with our hypothesis that the models possess the ability to comprehend and leverage documen-\ntation effectively. Remarkably, this improvement in performance is achieved without any additional\ntraining, fine-tuning nor document curation . It highlights the tremendous value of providing compre-\nhensive documentation, as it empowers the models to leverage a wide range of command-line tools at\nscale, solely through the process of reading and understanding the documentation.\nWe note that a degradation in performance after the document length exceeds 600 words. We attribute\nthis decline to the inherent challenges associated with comprehending lengthy documents in language\nmodels [61]. However, we foresee the ongoing advancements in handling long inputs in language\nmodels will gradually address this limitation [10, 5, 2]. We leave exploring solutions for overcoming\nthis limitation for future research.\n9\n5\nConclusion\nIn this paper, we examined the effectiveness of tool docs in enabling zero-shot tool usage with LLMs.\nWe first showed that LLMs can achieve on par or better performance than their few-shot counterparts\nwhen provided with tool docs. We then scaled up to a significantly larger tool set on a newly collected\nAPI through docs only. By simply plugging in new tools along with their docs, LLMs are able to\ntackle unseen tasks in image editing and video tracking without further demos and replicate the\nfunctionalities of recent popular projects, suggesting a potential for automatic knowledge discovery.\nOverall, we shed light on a new perspective of tool usage with LLMs by focusing on their internal\nplanning and reasoning capabilities with docs, rather than explicitly guiding their behaviors with\ndemos.\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel\nHo, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nKuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic\naffordances. In arXiv preprint arXiv:2204.01691, 2022.\n[2] Anthropic.\n100k\ncontext\nwindows.\nhttps://www.anthropic.com/index/\n100k-context-windows, 2023. Accessed: 05/15/2023.\n[3] AutoGPT. Auto gpt. https://autogpt.net/category/chatgpt-tools/autogpt/, 2023.\nAccessed: 05/15/2023.\n[4] Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelli-\ngence 15, pages 103\u2013129, 1995.\n[5] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. Unlimiformer: Long-\nrange transformers with unlimited length input. arXiv preprint arXiv:2305.01625, 2023.\n[6] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\net al. Improving language models by retrieving from trillions of tokens. In International\nconference on machine learning, pages 2206\u20132240. PMLR, 2022.\n[7] SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a\nmonte-carlo framework. Journal of Artificial Intelligence Research, 43:661\u2013704, 2012.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[9] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[10] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and\nbeyond with rmt. arXiv preprint arXiv:2304.11062, 2023.\n[11] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models\nas tool makers. arXiv preprint arXiv:2305.17126, 2023.\n[12] Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you\nneed for in-context learning? 2023.\n10\n[13] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts\nprompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv\npreprint arXiv:2211.12588, 2022.\n[14] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In Computer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII, pages 640\u2013658. Springer,\n2022.\n[15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[16] Sur\u00eds D\u00eddac, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[17] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied\nmultimodal language model. In arXiv preprint arXiv:2303.03378, 2023.\n[18] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435,\n2022.\n[19] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559, 2022.\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\naugmented language model pre-training. In International conference on machine learning,\npages 3929\u20133938. PMLR, 2020.\n[21] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118\u20139147. PMLR, 2022.\n[22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied\nreasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.\n[23] IDEA-Research. Grounded-segment-anything. https://github.com/IDEA-Research/\nGrounded-Segment-Anything, 2023. Accessed: 05/15/2023.\n[24] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using\nlarge language models. arXiv preprint arXiv:2303.05398, 2023.\n[25] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to\ncode in programmatic context. arXiv preprint arXiv:1808.09588, 2018.\n[26] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark,\nand Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896\u20131907,\nOnline, November 2020. Association for Computational Linguistics.\n[27] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.\n[28] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\narXiv preprint arXiv:2210.02406, 2022.\n11\n[29] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\nconvolution or region supervision. In International Conference on Machine Learning, pages\n5583\u20135594. PMLR, 2021.\n[30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n[31] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation.\narXiv preprint arXiv:2107.07566, 2021.\n[32] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\naugmented language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\n[33] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Advances in Neural Information Processing\nSystems, 33:9459\u20139474, 2020.\n[34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557,\n2019.\n[35] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\n[36] Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.\nTapex: Table pre-training via learning a neural sql executor. arXiv preprint arXiv:2107.07653,\n2021.\n[37] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny\nZhou, and Andrew M Dai. Mind\u2019s eye: Grounded language model reasoning through simulation.\narXiv preprint arXiv:2210.05359, 2022.\n[38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[39] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering. In The 36th Conference on Neural Information Process-\ning Systems (NeurIPS), 2022.\n[40] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language\nmodels. arXiv preprint arXiv:2304.09842, 2023.\n[41] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. In International Conference on Learning Representations (ICLR),\n2023.\n[42] Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, and William Yang Wang.\nMultimodal procedural planning via dual text-image prompting. 2023.\n[43] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n[44] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\nvolume 1, page 2, 2000.\n[45] OpenAI. Gpt-4 technical report. 2023.\n12\n[46] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,\nand Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language\nmodels. arXiv preprint arXiv:2303.09014, 2023.\n[47] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv\npreprint arXiv:2205.12255, 2022.\n[48] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\n[49] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in\nneural information processing systems, 1, 1988.\n[50] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in language models.\narXiv preprint\narXiv:2210.03350, 2022.\n[51] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint\narXiv:2304.08354, 2023.\n[52] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to\nmaster 16000+ real-world apis, 2023.\n[53] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code genera-\ntion and semantic parsing. arXiv preprint arXiv:1704.07535, 2017.\n[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[55] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In Proceedings of the fourteenth interna-\ntional conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and\nConference Proceedings, 2011.\n[56] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[57] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023.\n[58] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan\nUng, Moya Chen, Kushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational\nagent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188, 2022.\n[59] Michael Sipser. Introduction to the theory of computation. ACM Sigact News, 27(1):27\u201329,\n1996.\n[60] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for\nreasoning about natural language grounded in photographs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 6418\u20136428, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\n[61] Simeng Sun, Katherine Thai, and Mohit Iyyer. Chapterbreak: A challenge dataset for long-range\nlanguage models. arXiv preprint arXiv:2204.10878, 2022.\n[62] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239, 2022.\n13\n[63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[64] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng\nLim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large\nlanguage models. arXiv preprint arXiv:2305.04091, 2023.\n[65] Xingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot structured\nprediction from natural language. arXiv preprint arXiv:2210.12810, 2022.\n[66] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560, 2022.\n[67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[68] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao\nLiu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently.\narXiv preprint arXiv:2303.03846, 2023.\n[69] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[70] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\nSegment anything meets videos, 2023.\n[71] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teach-\ning large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752,\n2023.\n[72] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-\ndation models for decision making: Problems, methods, and opportunities. arXiv preprint\narXiv:2303.04129, 2023.\n[73] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[74] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,\n2022.\n[75] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\n[76] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code\ngeneration. arXiv preprint arXiv:1704.01696, 2017.\n[77] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chen-\nguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language\nmodels are strong context generators. In The Eleventh International Conference on Learning\nRepresentations, 2023.\n[78] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023.\n[79] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n14\n[80] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul-\ntimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923,\n2023.\n[81] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use:\nImproving few-shot performance of language models. In International Conference on Machine\nLearning, pages 12697\u201312706. PMLR, 2021.\n[82] Victor Zhong, Tim Rockt\u00e4schel, and Edward Grefenstette.\nRtfm: Generalising to novel\nenvironment dynamics via reading. arXiv preprint arXiv:1910.08210, 2019.\n[83] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. Docprompting:\nGenerating code by retrieving the docs. In The Eleventh International Conference on Learning\nRepresentations, 2023.\n15\nA\nBroader impacts and limitations\nThis work studies the importance of tool documentations in equipping LLMs with the ability to\ncompose usages of a variety of tools to accomplish complex tasks. However, as discussed in [51], it\nis imperative to contemplate what tools should be made available to LLMs as well as how one should\ninterpret and rely on the results obtained from the models. We envision tool documentations as a\nchannel to guide LLMs in more safely using the tools, aligning with the original intended use of the\ntools.\nB\nImplementation details\nIn this section, we provide further implementation details on each task. We conduct all our experi-\nments on Debian GNU/Linux 10 machines with 40GB A100 GPUs.\nB.1\nScienceQA\nOn ScienceQA [39], we closely follow the original setup 1 used in Chameleon [40], including the\ntool docs and few-shot demos (when used). We however find that the \u201cImage Captioner\u201d module used\nin the original work often provides less accurate captions on given images. In the documentation, we\nthus add the description on this observation for the \u201cImage Captioner\u201d module as shown in Figure 8.\nFigure 8: Documentations used in ScienceQA datasets.\nWe used the original tool docs in\nChameleon [40] and added the description for \u201cImage Captioner\u201d that the generated captions may be\ninaccurate.\nB.2\nTabMWP\nOn TabMWP [41], we strictly follow the original setup used in Chameleon [40]. We refer the readers\nto [40] and their open-sourced implementations for further details.\nB.3\nNLVRv2\nOn NLVRv2, we follow the setup used in [19]. However, as tool docs are not used in [19], we create\nour own docs for the tools used. Figure 9 shows the tool docs we use for several available tools used\nin VisProg [19].\n1https://github.com/lupantech/chameleon-llm\n16\nFigure 9: Example documentations used for tools in VisProg [19].\n17\nFigure 10: The documentation examples from GCP CLI. We crawl the website, remove the HTML\ntags and apply the renaming procedure as the documentation of the created LLM-Cloud CLI.\nB.4\nLLM-Cloud CLI\nMore examples.\nIn Table 2, we show more examples of the created LLM-Cloud CLI dataset, based\non GCP CLI.\nCreating tool documentations.\nOn the LLM-Cloud CLI dataset, we create tool documentations\nusing the widely-used BeautifulSoup 2 library to scrape the GCP CLI documentation. We removed\nHTML tags and implemented the renaming procedures for LLM-Cloud CLI documentation. We\nnote that we purposely do not eliminate unrelated content such as terms and hyperlinks. An example\ndocumentation from GCP before our renaming procedure is shown in Figure 10. This is to prevent\nexcessive engineering of the documentation for better assessing the robustness of LLM documentation\nreading ability.\nDocumentation retrieval details.\nGiven the extensive number of command-line tools in our exper-\niments (200 in total), the complete documentation cannot fit within a single prompt. Consequently,\nfor each query, we employ a simple TF-IDF search to retrieve the top 10 relevant documentations.\nWe then truncate the length to a maximum of 600 words. We note that the actual token count depends\non the tokenizer used by each LLM and is typically more than 600.\n2https://pypi.org/project/beautifulsoup4/\n18\nTable 2: More examples of the created LLM-Cloud CLI dataset.\nQuestion\nCommands in GCP\nCommands after renaming (Final\nAnswer)\nShow me how to deploy\nocr-xer container and invoke\nit with a schedule every 2\nhours on a project \u201ctest_proj\u201d\nin sdk command lines. The\nocr-xer container is located\nat \u201cus-docker.pkg.dev/gcr-\ncleaner/ocr-xer/ocr-xer\u201d.\n\u2022 gcloud config set project test_proj\n\u2022 gcloud\nrun\ndeploy\nocr-xer\n--image=us-docker.pkg.dev/gcr-\ncleaner/ocr-xer/ocr-xer\n\u2022 gcloud scheduler jobs create http\nNAME --schedule --schedule=\"0\n*/2 * * *\"\n\u2022 llmcloud\nconfig\nset\nproject\ntest_proj\n\u2022 llmcloud\nrun\ndeploy\nocr-xer\n--image=us-docker.pkg.dev/gcr-\ncleaner/ocr-xer/ocr-xer\n\u2022 llmcloud scheduler jobs make http\nNAME --schedule --schedule=\"0\n*/2 * * *\"\nHow to deploy a machine\nlearning model model.pt\nsaved in my local to cloud\nvia sdk command line?\n\u2022 gsutil cp model.pt LOC/model.pt\n\u2022 gcloud ai-platform versions cre-\nate VERSION --model MODEL --\norigin gs://LOC/model.pt\n\u2022 llmutil cp model.pt LOC/model.pt\n\u2022 llmcloud ai-platform versions cre-\nate VERSION --model MODEL --\norigin gs://LOC/model.pt\nHow to get transcript of a\nvideo test.mp4 at local via\nthe cloud SDK?\n\u2022 ffmpeg -i test.mp4 -ac 2 -f wav out-\nput.wav\n\u2022 gsutil cp test.wav LOC/test.wav\n\u2022 gcloud ml speech recognize-long-\nrunning --uri LOC/test.wav\n\u2022 ffmpeg -i test.mp4 -ac 2 -f wav out-\nput.wav\n\u2022 llmutil cp test.wav LOC/test.wav\n\u2022 llmcloud ml speech recognize-long-\nrunning --uri LOC/test.wav\nHow to create a composer\nenviroment with a private ip\nnetwork?\n\u2022 gcloud composer environments cre-\nate my_env\n\u2022 gcloud compute networks subnets\nupdate default --enable-private-ip-\ngoogle-access\n\u2022 llmcloud composer environments\nmake my_env\n\u2022 llmcloud compute networks sub-\nnets update default\n--enable-\nprivate-ip-google-access\nHow to create a service\naccount test@service.com\nwith the name \u201cAutoML\u201d\n\u201cBigQuery\nData\nEditor\u201d\nand \u201c\"AutoML Recommen-\ndations Service Account\u201d\npermissions?\n\u2022 gcloud\niam\nservice-accounts\ntest@service.com --display-name\nAutoML\n\u2022 gcloud\nprojects\nadd-iam-\npolicy-binding\nPROJ_ID\n--\nmember=\"test@service.com\"\n--role \"roles/bigquery.dataEditor\"\n\u2022 gcloud projects add-iam-policy-\nbinding\nPROJ_ID\n--member\n\"test@service.com\"\n--role\n\"roles/automlrecommenda-\ntions.serviceAgent\"\n\u2022 llmcloud\niam\nservice-accounts\ntest@service.com --display-name\nAutoML\n\u2022 llmcloud\nprojects\nadd-iam-\npolicy-binding\nPROJ_ID\n--\nmember=\"test@service.com\"\n--role \"roles/bigquery.dataEditor\"\n\u2022 llmcloud projects add-iam-policy-\nbinding\nPROJ_ID\n--member\n\"test@service.com\"\n--role\n\"roles/automlrecommenda-\ntions.serviceAgent\"\n19\nB.5\nImage editing and video tracking\nAs discussed in Section 4.3, by providing tool documentations, we can easily add on new tools to\nenable LLMs in solving novel tasks such as image editing and video tracking. Here, we leverage\nthe recent advancements in vision models and expand the tool set used in VisProg [19] with three\nnew tools: GroundingDINO [38], Segment Anything (SAM) [30], and XMem [14]. We provide their\ncorresponding documentations in Figure 11.\nFigure 11: Documentation of new tools introduced in VisProg. BETTERLOC, BETTERSEG,\nTRACK calls GroundingDINO, Segment Anything, XMem, respectively.\n20\nC\nExperimental results\nIn this section, we show the experimental results on each task with comparisons to more baselines.\nScienceQA.\nIn Table 3, we compare zero-shot prompting with tool documentations to other baseline\nmethods. We include the following baseline methods that are finetuned on the ScienceQA training set\nfor performance reference: ViLT [29], VisualBERT [34], UnifiedQA CoT [39], MM-CoT [80], and\nLLaMA-Adapter [78]. We report the results obtained from [40] for the finetuned methods. For fair\ncomparison, we shall focus on zero/few-shot settings. Thus, we include Chain-of-Thought (CoT) [67]\nand Chameleon [40] as the few-shot baselines to compare to. We see that with tool docs, we can\nnot only achieve better performance than the few-shot methods without any demos, but we can also\nmatch (outperform) several models specifically finetuned on the dataset.\nTable 3: Comparing zero-shot prompting with tool docs to existing baseline methods on ScienceQA.\nWe see that zero-shot prompting with tool docs performs competitively, outperforming the two\nfew-shot baselines and several finetuned models.\nFinetuned methods\nFew-shot methods\nZero-shot methods\nBenchmark\nViLT\nVisualBERT\nUnifiedQA CoT\nMM-CoT\nLLaMA-Adapter\nCoT\nChameleon\n0-shot with docs\nScienceQA\n61.14\n61.87\n74.11\n84.91\n85.19\n78.54\n79.20\n79.91\nTabMWP.\nSimilarly, in Table 4, we compare zero-shot prompting with tool docs to various\nfinetuned models and few-shot baselines, inlcuding: UnifiedQA [26], TAPEX [36], Chain-of-Thought\n(CoT) [67], Program-of-Thought (PoT) [13], and Chameleon [40]. We report the results obtained\nfrom [40] for UnifiedQA, TAPEX, and CoT. We see that with tool docs, zero-shot prompting\nsignificantly outperforms finetuned models, and baseline few-shot methods, CoT and PoT. When\ncompared to Chameleon that utilizes 16 few-shot tool-usage demos, tool docs enable the model to\nperform comparably without relying on any demos.\nTable 4: Comparing zero-shot prompting with tool docs to existing baseline methods on TabMWP.\nWe see that with tool docs, even zero-shot prompting without any tool-usage demos achieves better\nperformance than finetuned models and few-shot CoT and PoT baseline. It also performs comparably\nto Chameleon that employs 16-shot tool-usage demos.\nFinetuned methods\nFew-shot methods\nZero-shot methods\nBenchmark\nUnifiedQA\nTAPEX\nCoT\nPoT\nChameleon\n0-shot with docs\nTabMWP\n57.35\n58.52\n82.03\n89.28\n93.88\n92.69\nNLVRv2.\nIn Table 5, we compare zero-shot prompting with tool docs to a finetuned model on\nNLVRv2 and various few-shot baselines. Specifically, we consider ViLT [29] as the finetuned\nbaseline and VisProg [19] with varying numbers of tool-usage demos as the few-shot baselines.\nWe report the result obtained from [19] for ViLT. Since VisProg does not utilize tool docs, we see\nthat its performance is very sensitive to the number of demos used. In addition, we also observe\nlarge performance variances when we randomly select different demos used for prompting, e.g.,\nthe standard deviation for 2-shot prompting reaches 16.1 percentage point. This indicates that the\nfew-shot demos may require careful curation for the model to achieve good performance. On the\nother hand, with tool docs, zero-shot prompting can already achieve decent performance compared to\nonly using few-shot demos.\nTable 5: Comparing zero-shot prompting with tool docs to existing baseline methods on NLVRv2.\nFinetuned methods\nFew-shot methods\nZero-shot methods\nBenchmark\nViLT\nVisProg (0-shot)\nVisProg (2-shot)\nVisProg (4-shot)\nVisProg (12-shot)\n0-shot with docs\nNLVRv2\n76.30\n0\n43.1 \u00b1 16.1\n66.5 \u00b1 1.4\n69.1 \u00b1 0.1\n63.4\n21\nLLM Cloud-CLI.\nIn Table 6, we present the results on LLM-Cloud CLI with different underlying\nLLM planners. On both text-davinci-002 and gpt-3.5-turbo, when there is a large number\nof tools, we see documentation is much more important than few-shot demonstrations, where zero-\nshot with docs achieves significantly better performances than few-shot without docs. Additionally,\nwhen provided with docs, the LLMs are able to figure out how to use the tools without the need of\ndemonstrations.\nTable 6: Results on the LLM-Cloud CLI.\nLLM\nNumber of Demos\nDocumentations\nF1\ntext-davinci-002\n0\nNo\n0.02\n5\nNo\n0.02 \u00b1 0.02(0.05)\n10\nNo\n0.05 \u00b1 0.02(0.11)\n15\nNo\n0.05 \u00b1 0.05(0.1)\n5\nYes\n0.35 \u00b1 0.02(0.37)\n10\nYes\n0.35 \u00b1 0.02(0.37)\n15\nYes\n0.35 \u00b1 0.01(0.37)\n0\nYes\n0.37\ngpt-3.5-turbo\n0\nNo\n0.13\n5\nNo\n0.18 \u00b1 0.06(0.21)\n10\nNo\n0.19 \u00b1 0.06(0.23)\n15\nNo\n0.18 \u00b1 0.06(0.22)\n5\nYes\n0.44 \u00b1 0.02(0.47)\n10\nYes\n0.44 \u00b1 0.02(0.48)\n15\nYes\n0.42 \u00b1 0.04(0.49)\n0\nYes\n0.45\nImage editing.\nWe provide more image editing examples achieved by zero-shot prompting with\ntool docs in Figure 12. In particular, we show that with tool docs, we are able to reproduce the image\nediting examples achieved by VisProg [19] without using any few-shot demos, wherein VisProg\nrelies on 10 task-specific few-shot demos.\n22\nFigure 12: Image editing examples by zero-shot prompting gpt-3.5-turbo with tool docs. Zero-\nshot prompting with docs is able to reproduce the results achieved by VisProg using few-shot\ndemos [19].\n23\n"
  },
  {
    "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
    "link": "https://arxiv.org/pdf/2308.00304.pdf",
    "upvote": "22",
    "text": "SKILLS-IN-CONTEXT PROMPTING: UNLOCKING\nCOMPOSITIONALITY IN LARGE LANGUAGE MODELS\nJiaao Chen\u2217, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu & Jianshu Chen\u2020\nTencent AI Lab, Bellevue, WA, 98004\nABSTRACT\nWe consider the problem of eliciting compositional generalization capabilities in large language\nmodels (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the\nLLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generaliza-\ntion), which is a critical reasoning capability of human-like intelligence. However, even the current\nstate-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-\nin-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more\ncomplex problems. We find that it is crucial to demonstrate both the skills and the compositional\nexamples within the same prompting context. With as few as two examplars, our SKiC prompting\ninitiates strong synergies between skills and their composition capabilities. Notably, it empowers\nLLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect\ngeneralization on a broad range of challenging compositionality tasks. Intriguingly, SKiC prompting\nunlocks the latent potential of LLMs, enabling them to leverage pre-existing internal skills acquired\nduring earlier pretraining stages, even when these skills are not explicitly presented in the prompting\ncontext. This results in the capability of LLMs to solve unseen complex problems by activating and\ncomposing internal competencies. With such prominent features, SKiC prompting is able to achieve\nstate-of-the-art performance on challenging mathematical reasoning benchmarks (e.g., MATH).\n1\nIntroduction\nPre-trained large language models (LLMs) have achieved great success in solving natural language processing (NLP)\ntasks (Brown et al., 2020; Radford et al., 2019; Smith et al., 2022; Chowdhery et al., 2022; Lewkowycz et al., 2022;\nSanh et al., 2021; Wei et al., 2021; Mishra et al., 2022; Chung et al., 2022; Ouyang et al., 2022; OpenAI, 2023;\nTouvron et al., 2023b). When the size of the model continuously scales up, LLMs exhibit strong zero-shot and few-\nshot performance on a wide range of NLP tasks (Brown et al., 2020; Wei et al., 2021; Chowdhery et al., 2022; Zhou\net al., 2022; Wang et al., 2022a; Li et al., 2022; Wang et al., 2022b; Kojima et al., 2022; Shi et al., 2022; Magister\net al., 2022; Ho et al., 2022; Nye et al., 2021; Dua et al., 2022; OpenAI, 2023) \u2014 a salient behavior characterized\nby the scaling law (Kaplan et al., 2020; Hoffmann et al., 2022) and emergent abilities (Wei et al., 2022a). However,\nLLMs still struggle with compositional generalization, i.e., the ability to use existing known skills to solve more\ncomplex problems than they have seen before (i.e., \u201ceasy-to-hard\u201d generalization) (Zhou et al., 2022; Dziri et al.,\n2023). In this paper, we will focus on developing a prompting strategy for LLMs that can effectively unlock their\ngeneric compositional generalization capabilities.\nIdeally, if an LLM has already learned a rich set of skills, it should be able to solve any problem whose solutions are\ncomposable from these skills. To unlock such great potential, the key is to teach the LLMs how to use these skills to\nconstruct a solution to any unseen, more difficult problem. Towards this goal, there have been a series of prompting\nstrategies being developed to improve the reasoning and compositionality capabilities. Notably, chain-of-thought\n(CoT) prompting (Wei et al., 2022b) significantly improves the reasoning performance of LLMs by demonstrating\nhow to approach a complex problem through a sequence of simple and basic steps. Follow-ups such as Least-to-Most\nprompting (Zhou et al., 2022) and decomposed prompting (Khot et al., 2022) propose a two-stage strategy, which first\n\u2217Affiliated with Georgia Institute of Technology. This work is done during internship at Tencent AI Lab.\n\u2020Correspondence to: Jiaao Chen <jiaaochen@gatech.edu>, Jianshu Chen <jianshuchen@global.tencent.com>.\narXiv:2308.00304v2  [cs.CL]  14 Aug 2023\nA PREPRINT\ndecomposes the original problem into a set of subproblems, and then solve and combine each of them sequentially.\nAlthough these methods significantly boost the performance over standard prompting in solving many challenging\ncompositional generalization tasks, they still cannot perform systematic generalization well enough over problems that\nare significantly harder than the ones they have seen. Moreover, least-to-most prompting and decomposed prompting\nare restricted to solving a certain class of tasks, where each problem can be decomposed as a sequence of subproblems.\nAnd for problems with general computation graphs (Dziri et al., 2023), it is generally less intuitive, if not possible, to\nconstruct the prompting exemplars.\nIn this paper, we develop an effective one-stage prompting strategy, named SKills-in-Context (SKiC) prompting, to\nunlock the general compositional generalization capability in LLMs. The key insight is to teach the LLM to explicitly\nground each of its reasoning steps on the (more elementary) skills. Specifically, the SKiC prompt is constructed from\nthree main blocks. The first block contains the skills that LLMs may need to use in order to solve a more complex\nproblem, which include both descriptions of the skills and the instructions (with a few examples) on how to use them.\nThese skills can be distilled either manually or automatically via prompting the LLMs. The second part consists of a\nfew (generally two in most of our cases) examplars that demonstrate how to explicitly compose skills into a solution\nto a more complex problem. The last part is the problem to be solved. Interestingly, with both the skills and their\nexplicit compositions presented in the context, the LLMs successfully learn how to ground each reasoning step on\nthe knowledge and skills that they have already mastered, yielding the desirable general compositional generalization\ncapabilities. Notably, unlike the Least-to-Most or decomposed prompting, our proposed approach is a one-stage\nprompting method, without the need to call LLMs multiple times. Therefore, it can be easily used in a plug-and-play\nmanner, as the CoT prompting and the standard prompting.\nWe evaluate our proposed SKiC prompting on a wide range of challenging compositional generalization tasks. Our\nexperiments show that SKiC prompting achieves state-of-the-art performance on all of these benchmarks, and it even\nachieves near-perfect generalization on unseen harder problems on some of the datasets. Moreover, the improvement\nmargins compared to the previous methods are significant. For example, SKiC outperforms previous state-of-the-art\nprompting strategies on unseen longer cases by 16.3% on last-letters (Zhou et al., 2022), 25.5% on addition, 45.0%\non multiplication (Dziri et al., 2023),9.93% on Commaqa-E (Khot et al., 2021), 36.0% on dynamic programming\n(Dziri et al., 2023), 2.7% on GSM8K(Cobbe et al., 2021), and 12.1% on MATH (Hendrycks et al., 2021). Notably,\nour results on GSM8K and MATH further reveal that SKiC prompting allows the LLMs to generalize beyond the\nskills provided in the context and solve problems by using the vast reservoir of the internal skills they acquired during\nthe prior pretraining stage. It clearly demonstrates that SKiC prompting unleashes strong synergies between skills\nand their composition capabilities, which teaches LLMs to generalize to harder problems than they have seen and to\nproblems that require innovative compositions of existing knowledge.\n2\nMethodology\nRecent progress in LLMs has demonstrated strong capabilities in solving various NLP tasks (Brown et al., 2020;\nRadford et al., 2019; Smith et al., 2022; Chowdhery et al., 2022; Lewkowycz et al., 2022; Sanh et al., 2021; Wei et al.,\n2021; Mishra et al., 2022; Chung et al., 2022; Ouyang et al., 2022). However, they usually suffer from generalizing\nto more complex problems that require LLMs to compose different capabilities (Zhou et al., 2022) from the examples\nthey have seen, i.e., compositional generalization or \u201ceasy-to-hard\u201d generalization. This discrepancy is mainly due\nto the lack of the ability to compose basic skills in innovative ways to solve more difficult problems (Dziri et al.,\n2023), which is natural for humans in problem solving. Empowering language models with the ability to compose\nthe skills that they have seen to solve more complex tasks is important to mirror human intelligence and to reach\nsuperintelligence. To this end, this work introduces a novel prompting strategy, Skills-in-Context (SKiC) prompting,\nto teach language models composing elementary skills to solve problems for better compositional generalization.\n2.1\nSkills-in-Context Prompting\nSkills-in-context prompting facilitates compositional generalization by explicitly instructing language models to utilize\nbasic skills to solve complex problems.3 A SKiC prompt consists of three major parts: (i) Characterization of the basic\nskills that are needed to solve complex problems, including the description of the skills and the instruction on how to\nuse them (with few-shot examplars). (ii) Examples of how to compose basic skills into solutions to complex problems.\n(iii) The problem to be solved. An example is shown in Figure 1. The language model is first provided with several\nbasic skills such as getting the last letter of one word followed by several examples introduced to illustrate the process\nof utilizing these basic skills to answer the complex problem. For example, to take the last letter of a series of words,\n3The term \u201cbasic skills\u201d within SKiC prompting are not necessarily atomic skills. Rather, they could be any skills (e.g., a\ncomposite skill by itself) that serve as the foundational blocks for tackling more complex problems.\n2\nA PREPRINT\nFigure 1: Skills-in-Context Prompting. The prompt consists of three building blocks: (i) the (basic) skills for solving\na complex task, (ii) (few-shot) examples of how to compose the skills to solve the complex problems, and (iii) the\nproblem to be solved. The few-shot demonstration examples for the skills are omitted above for brevity. The above\nprompt will be fed into an LLM to generate the output \u2014 see Figure 21 for an example of the output. Note that the\ncompositional examplars demonstrate how to ground the reasoning steps onto the basic skills (highlighted in colors).\nlanguage models need to use the \u201cwords to list\u201d skill to first add the asked words to a list and then use the \u201clast letter\u201d\nskill to iteratively obtain the last letter of each word.\nComparison to previous prompting strategies\nFigure 2 visualizes the differences between our proposed SKiC\nprompting and the previous related prompting methods. Different from Chain-of-Thoughts prompting, our SKiC\nprompting provides explicit grounding on the basic skills for reasoning steps towards final answers. Compared to\nrecent prompting methods for handling compositional problems such as Least-to-Most prompting (LtM) (Zhou et al.,\n2022) and Decomp (Khot et al., 2022), our SKiC is superior in several dimensions: (i) Our SKiC prompting is more\ngeneral to solve extended sets of problems. Previous decomposing-based approaches like LtM and Decomp usually\nsolve complex problems in a two-stage fashion by first decomposing the problem into a linear sequence of subproblems\nand then solving them sequentially. However, many of the tasks that have complex computation graphs such as\nmultiplication and dynamic programming problems (Dziri et al., 2023) cannot be easily and fully decomposed in one\nstage, which makes it hard to apply these decomposition-based approaches. (ii) The decomposition operation can also\nbe viewed as one basic skill in our SKiC prompt (for example, we view the decomposition operation as one of the\nskills in the question-answer task in Figure 13). (iii) SKiC solves the complex problems in a single stage, which could\nalleviate the error propagation compared to decomposition-based approaches that require multiple distinct stages.\nDue to the one-stage nature, our SKiC prompting can replace other one-stage strategies such as the CoT promptings in\na plug-and-play manner. And it can also be easily combined with other ensemble techniques such as self-consistency\n(Wang et al., 2022a) and Progressive-Hint Prompting (Zheng et al., 2023) to further boost the performance.\n2.2\nConstruction of the SKiC Prompts\nOne key step in constructing our SKiC prompts is to distill the (basic) skills that might be needed for solving problems\nassociated with a task. We now introduce two approaches (shown in Figure 3) to achieve this objective.\n3\nA PREPRINT\nFigure 2: The building blocks of different prompting strategies. Blue cells stand for different intermediate steps,\ngreen cells denote the answers to the asked question, and red cells refer to the provided skills in our Skills-in-Context\nprompting. A block of several cells represents one distinct stage in a two-stage prompting strategy (e.g., problem\ndecomposition stage in the Least-to-Most prompting). Standard prompting provides only labeled examplars in the\ncontext. Chain-of-Thoughts prompting further provides a step-by-step rationale preceding the answer. Decomposed\nprompting is a two-stage prompting method, which first breaks the questions into sub-problems, and then utilizes\nstandard or Chain-of-Thoughts prompting to solve each sub-problem sequentially to derive the final answer. Least-to-\nMost prompting adopts a two-stage strategy: it first generates multiple questions in an easy-to-hard manner, and then\nsequentially answers each of them until solving the original question. In contrast, our Skills-in-Context prompting is a\nsimple one-stage prompting, which places both the (basic) skills and the demonstrations of how to compose them into\nsolutions within the same prompt context. This teaches the LLM how to explicitly and adeptly ground each reasoning\nstep onto the skills (illustrated in dashed lines), which unleashes strong synergies between skills and composition\ncapabilities in LLMs, leading to strong compositionality over unseen harder problems.\nDistill Skills via Human\nThis is a fully manual approach, where the basic skills are manually summarized from a\nfew (less than 10) problems we want to solve. For example, given several samples from the last-letter-concatenation\ntask, we manually identify that \u201cwords to list\u201d and \u201clast letter\u201d are common basic skills to be used. Based on the\ndiscovered skills, we add a few (1 \u223c 2) simple examples to illustrate these basic skills alone. Once the in-context\nskills are constructed, we add the compositional examplars to demonstrate the composition of these skills to solve a\nproblem (Figure 1). This approach puts all the essential skills in the context and is generally applicable to narrow\ndomain problems that require the composition of limited basic skills for solving harder problems (e.g., with larger-\nsize). It is also beneficial for semi-parametric LLMs, which can dynamically access the most relevant skills from\nexternal memories based on each input instance and integrate them into the problem context (Pan et al., 2023).\nDistill Skills via Prompting LLMs\nThis is a semi-automatic approach, where we first prompt the LLMs to au-\ntomatically generate the necessary basic skills (i.e., the descriptions and examples) followed by human review and\nadjustment. For instance, when identifying the skills required to address the mathematical reasoning problems in the\nMATH task (Hendrycks et al., 2021), which encompasses a range of problems from Algebra, Counting and Probabil-\nity, Geometry, Intermediate Algebra, Number Theory, PreAlgebra, to PreCalculus, we prompt the LLM with phrases\nlike \u201cbasic skills in Algebra\u201d. This leads the model to generate foundational skills, such as \u201cFactoring\u201d (see Figure 19\nfor the full list of the skills). Next, we manually construct the compositional examplars by grounding the reasoning\nsteps on the skills. It is worth noting that an exemplar might require skills not explicitly presented in the prompt\ncontext. In these instances, we anchor the reasoning to inherent skills within the LLMs, confirming their presence\nthrough zero-shot prompting. For example, in the compositional exemplar showcased in Figure 20, aside from lever-\naging in-context skills such as \u201cCombination\u201d and \u201cSub\u201d, it also employs skills like \u201cPascal\u2019s Triangle\u201d \u2014 a capability\nnot present in the context but inherently known to the LLM. Such a construction of the examplars will encourage the\nmodel to generalize beyond the in-context skills and compose solutions from the internal capabilities as well \u2014 see\nFigure 4 for an example of the generated solution that activates the internal skills <Angle Bisector Theorem> and\n<Heron\u2019s Formula>. To be more specific, for every problem in the MATH task, around 24% of the skills, as shown\nin Table 5, applied in the reasoning steps stem from the LLM\u2019s internal pre-trained knowledge (see Table 6 for the\n4\nA PREPRINT\nFigure 3: Two approaches to creating our SKiC prompt for a given task, depending on how we distill the skills.\n(a) We manually summarize the basic skills from the sample problems associated with the task, and then construct\nthe compositional examplars on how to compose these skills to tackle the problem. (b) We prompt the LLMs to\nautomatically generate the necessary basic skills, followed by human review and adjustments. Then we manually\ncraft the compositionl examplars by grounding their reasoning steps onto either the provided in-context skills or the\ninherent skills within the LLMs, where the existence of the inherent skills in LLMs is verified by zero-shot prompting.\nFigure 4: An example of the generated solution on the MATH task using our SKiC prompting. The skills <Angle\nBisector Theorem> and <Heron\u2019s Formula> are neither provided in the SKiC prompting context (see Figure 19) nor\nused in any given examplars. LLMs harness the internal skills in their pre-trained knowledge to solve the problem.\nmost frequently used internal skills). The ability to harness both in-context skills and inherent capabilities is crucial\nfor addressing complex reasoning problems, which typically require varied compositions across a broad spectrum of\nfoundational skills. Manually enumerating every required skill within a prompt context is often impractical. Mean-\nwhile, LLMs have accumulated a vast reservoir of knowledge and skills during their pre-training. Leveraging these\ninternal competencies can unlock significant potential, allowing LLMs to tackle even more complex challenges.\n3\nExperiments\nIn this section, we show the superior compositional capabilities of our SKiC prompting by evaluating it in two settings:\n\u2022 Composition over in-context skills, where all the essential skills needed to solve the problems are provided in\nthe context. The tasks we evaluate in this setting include symbolic manipulation (Wei et al., 2022b; Zhou et al.,\n2022; Khot et al., 2022), arithmetic operation (Dziri et al., 2023), question answering (Khot et al., 2022), and\ndynamic programming (Dziri et al., 2023). In this setting, we mainly examine the ability to generalize from easy\ndemonstration examplars to more difficult testing problems (i.e., easy-to-hard generalization).\n5\nA PREPRINT\n\u2022 Generalization beyond in-context skills, where models also need to harness skills beyond what have been pro-\nvided in the context and tap into the internal skills for math reasoning like GSM8K (Wei et al., 2022b; Zhou et al.,\n2022) and MATH (Hendrycks et al., 2021) problems. In this context, the primary challenge lies in achieving\ndiverse compositions across a wide range of foundational skills to solve a complex reasoning problem.\n3.1\nComposition over In-Context Skills: Easy-to-Hard Generalization\nIn this section, we begin by evaluating our SKiC prompting strategy on tasks that require only a limited skill set,\nyet pose challenges in terms of easy-to-hard generalization capabilities. Under these circumstances, we construct our\nSKiC prompts manually, adhering to the first methodology outlined in Section 2.2. We mainly consider foundation\nmodels including LLAMA-65B (Touvron et al., 2023a), text-davinvi-003 (Brown et al., 2020), ChatGPT and GPT-\n4 (OpenAI, 2023). Additional experiments on LLAMA2 (Touvron et al., 2023b) can be found in Appendix A.\n3.1.1\nSymbolic Manipulation: Last Letters\nFollowing Zhou et al., we first assess the compositionality in LLMs through the last-letter-concatenation task. For a\ngiven list of words, the LLM needs to generate an output that is the concatenation of the last letter from each word\nin the list. We compare our SKiC with zero/few-shot standard prompting (4-shot) (Brown et al., 2020), CoT (Wei\net al., 2022b) and Least-to-Most prompting (LtM) (Zhou et al., 2022) on different large language models, including\nLLAMA-65B (Touvron et al., 2023a), text-davinvi-003 (Brown et al., 2020; Ouyang et al., 2022), and ChatGPT. And\nwe evaluate them on different subsets of testing problems that include 1, 2, 4, 6, 8, 10, 12 words4, respectively. The\nexamplars in all the prompts are constructed from the cases with 1 or 2 words. Therefore, the evaluations on the test\nsubsets with 1, 2 words are in-distribution, and the ones on 4, 6, 8, 10, 12 words are out-of-distribution.\nA SKiC prompt contains the skills and two examples of how to compose these skills as shown in Figure 7 and Figure 8.\nThe model is given the needed skills such as putting the given words to a list and getting the last letter of one word,\nand then two examples of how to compose these skills to take the last letters of two given words.\nTable 1: Accuracy of different prompting methods on different evaluation subsets of the last-letter-concatenation task.\nThe testing problems with 1 and 2 words are in-distribution evaluation, while the ones with 4 \u223c 12 words are (harder)\nout-of-distribution evaluations.\nModel\nPrompting\n#-shots\n1\n2\n4\n6\n8\n10\n12\nLLAMA-65B\nzero-shot\n0\n0\n0\n0\n0\n0\n0\n0\n4-shots\n4\n72.0\n66.0\n50.0\n26.0\n10.0\n6.0\n0\nCoT\n4\n76.0\n70.0\n58.0\n42.0\n30.0\n26.0\n20.0\nLtM\n4\n76.0\n72.0\n66.0\n50.0\n46.0\n36.0\n25.0\nSKiC\n2\n81.0\n97.0\n77.0\n59.0\n56.0\n48.0\n36.0\ntext-davinci-003\nzero-shot\n0\n0\n0\n0\n0\n0\n0\n0\n4-shots\n4\n99.0\n97.0\n89.0\n68.0\n45.0\n27.0\n10.0\nCoT\n4\n100.0\n99.0\n90.0\n75.0\n52.0\n39.0\n31.0\nLtM\n4\n100.0\n99.0\n94.0\n90.0\n87.0\n84.0\n80.0\nSKiC\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n99.0\n98.0\nChatGPT\nzero-shot\n0\n99.0\n98.0\n93.0\n88.0\n84.0\n80.0\n77.0\n4-shots\n4\n100.0\n100.0\n95.0\n92.0\n90.0\n86.0\n85.0\nCoT\n4\n100.0\n100.0\n97.0\n95.0\n92.0\n88.0\n85.0\nLtM\n4\n100.0\n100.0\n99.0\n95.0\n92.0\n92.0\n88.0\nSKiC\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\nThe results are reported in Table 1. We observe that standard zero/few-shot prompting generalizes poorly on the\ntesting problems that are harder than the examplars in the prompting context. For example, 4-shot standard prompting\nonly achieves 10% accuracy with text-davinci-003 when solving testing problems that involves 12 words. Chain-of-\nThoughts and Least-to-Most prompting improve the overall performance but still degrade quickly over longer inputs.\nOur Skills-in-Context prompting significantly boosts the accuracy in all the test cases especially when there are more\ninput words \u2014 it achieves nearly perfect generalization to harder problems with text-davinvi-003 and ChatGPT. This\nsuggests that by showing the basic skills and teaching the models how to use the skills (with just two examples), our\ndesigned Skills-in-Context prompting achieves better compositionality. An example of the generated answer when\nusing SKiC prompting to concatenate the last letters of 10 words can be found in Figure 21.\n4From https://github.com/first20hours/google-10000-english/tree/master.\n6\nA PREPRINT\nTable 2: Accuracy of different prompting methods on the task of adding two numbers with different digits (2,3,4,5,6,7)\nand multiplying two numbers with different digits (2,3,4,5). For the addition task, the prompting examplars are\nconstructed to demonstrate the addition between two numbers with 2 or 3 digits. Therefore, the results for adding\nnumbers with 4 \u223c 7 digits measure the desirable compositional generalization capabilities over harder problems. For\nthe multiplication task, the prompting examplars are constructed to demonstrate how to multiply two numbers with 2\nor 3 digits. Therefore, the results for multiplying numbers with 4 and 5 digits measure the compositional generalization\ncapability over harder problems. \u2020 denotes our method.\nModel\nPrompting\n#-shots\nAddition\nMultiplication\n2\n3\n4\n5\n6\n7\n2\n3\n4\n5\nLLAMA-65B\nzero-shot\n0\n58.0\n40.5\n22.5\n8.0\n0\n0\n28.0\n17.0\n0\n0\n4-shots\n4\n64.5\n46.5\n28.0\n10.0\n0\n0\n24.0\n18.0\n0\n0\nCoT\n4\n60.0\n52.5\n24.0\n12.0\n1.0\n0\n22.0\n21.0\n0\n0\nSKiC\u2020\n2\n82.5\n74.5\n66.5\n52.0\n38.0\n22.0\n50.0\n42.0\n12.0\n8.0\ntext-davinci-003\nzero-shot\n0\n100.0\n100.0\n98.0\n87.5\n74.5\n54.0\n76.0\n14.5\n0\n0\n4-shots\n4\n100.0\n100.0\n98.0\n92.0\n80.5\n58.5\n82.0\n18.0\n0\n0\nCoT\n4\n100.0\n100.0\n92.0\n68.5\n42.0\n38.0\n86.0\n20.5\n2.0\n0\nfinetuned\n0\n-\n-\n-\n-\n-\n-\n99.0\n55.0\n1.0\n0.0\nSKiC\u2020\n2\n100.0\n100.0\n99.0\n98.0\n99.0\n98.5\n100.0\n58.0\n42.5\n36.0\nChatGPT\nzero-shot\n0\n100.0\n100.0\n100.0\n92.0\n86.5\n78.0\n99.0\n55.0\n1.0\n0\n4-shots\n4\n100.0\n100.0\n100.0\n94.0\n90.5\n83.5\n99.0\n58.0\n1.0\n0\nCoT\n4\n100.0\n100.0\n98.5\n90.0\n87.5\n80.0\n99.0\n54.5\n13.0\n2.0\nSKiC\u2020\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n82.0\n72.0\n48.5\n3.1.2\nArithmetic Operation\nFollowing Dziri et al., we evaluate the compositional capabilities on two arithmetic operation tasks: addition and\nmultiplication. These two tasks involves complicated composition over skills such as one-digit addition or multipli-\ncation, carry over, concatenation and etc.(Dziri et al., 2023), making it difficult especially for long form addition or\nmultiplication. We compare our Skills-in-Context prompting (SKiC) with zero/few-shot standard prompting (Brown\net al., 2020) and Chain-of-Thoughts prompting (CoT) (Wei et al., 2022b) on different foundation models including\nLLAMA-65B, text-davinvi-003, and ChatGPT. We exclude the Least-to-Most prompting (Zhou et al., 2022) as it is\ndifficult to design linear problem decomposition for addition or multiplication task. We also include text-davinci-003\nfinetuned with scratchpad method (Nye et al., 2021; Dziri et al., 2023) on the multiplication task for comparison.\nAddition\nWe construct different subsets of testing problems, which ask to output the sum of two numbers with\n2,3,4,5,6,7 digits, respectively. The given in-context examplars are only constructed to demonstrate the addition of\ntwo numbers with 2-digits or 3-digits. Consequently, the results for 4,5,6,7-digits summation are out-of-distribution\nevaluation. Figures 9\u201310 show the basic skills and, for brevity, one compositional examplar, respectively. We first\npresent the basic skills like extracting digits from a number and then show the model how to use these skills to\nadd two numbers with two examples. The results are shown in Table 2. Even though large language models such\nas text-davinci-003 and ChatGPT can perform well in adding smaller numbers in zero-shot and few-shots settings,\nthey often fail to add larger numbers accurately such as adding two 7-digits numbers. Chain-of-Thoughts prompting\ndoes not improve the capability significantly. When utilizing our proposed Skills-in-Context prompting, there are\nconsistent performance improvements on all the models (achieving over 68.9% improvements on 7-digits addition\nwith text-davinci-003 compared to baselines) and even achieve 100% accuracy with ChatGPT. This underscores the\nsignificance of concurrently presenting both the skills and their compositional examplars within a unified prompt\ncontext to enhance compositional generalization. An example of the generated answer using SKiC prompting for\nsolving 6-digits addition can be found in Figure 22.\nMultiplication\nNext, we evaluate the compositional generalization performance on the multiplication task. Specif-\nically, we construct different subsets of evaluation problems that ask for the product of two numbers with 2,3,4,5\ndigits, respectively. The given in-context examplars in all the prompts are constructed to demonstrate 2-digit and 3-\ndigit multiplications. Therefore, the results for 4,5-digits multiplications measure the compositional generalization to\nunseen harder problems. The construction of our Skills-in-Context prompting is shown in Figure 11 and Figure 12,\nwhich illustrate the skills and the compositional examplar, respectively. The evaluation results are reported in Table 2.\nAll the models with standard prompts or Chain-of-Thoughts prompts can not handle the multiplication, especially for\nlarger number (e.g., 0% accuracy for ChatGPT when multiplying two 5-digits numbers). After explicit showing the\n7\nA PREPRINT\nTable 3: Performance of different prompting methods on Commaqa-E datasets (measured in Exact Match). The\nrows of \u201cCompositional Generalization\u201d reports the results on the new (unseen) compositional questions from the\ncompositional generalization split. \u2020 denotes our method.\nCommaqa-E\nPrompting\n#-shots\nLLAMA-65B\ntext-davinci-003\nChatGPT\nCommaqa-E\nTest\nzero-shot\n0\n12.0\n34.0\n42.0\n4-shots\n4\n15.0\n42.0\n47.0\nCoT\n4\n27.0\n44.0\n55.0\nDecomp\n12\n32.0\n58.0\n64.0\nSKiC\u2020\n2\n44.0\n66.0\n70.0\nCompositional\nGeneralization\nzero-shot\n0\n16.3\n26.8\n30.6\n4-shots\n4\n24.6\n33.5\n40.3\nCoT\n4\n30.8\n38.2\n46.4\nDecomp\n12\n40.4\n66.6\n73.5\nSKiC\u2020\n2\n52.0\n74.8\n80.8\nmodels with the necessary skills to compute the product of two numbers as well as the detailed process of composing\nthese skills, our SKiC prompting significantly improve the multiplication accuracy. It highlights the superiority of our\nprompting approach to tackle compositional and out-of-distribution testing cases. Our error analysis reveals that most\nof the errors in SKiC prompting are caused by missing the multi-digit addition capability, which can be incorporated as\na basic skill (Figure 9 and Figure 10) in the prompting context. When equipped with such extra skills, ChatGPT with\nSKiC can achieves 100% accuracy for both 2-digit and 3-digit multiplications.5 We show an example of the generated\nanswer for solving 4-digits multiplication in Figure 23.\n3.1.3\nLong-Context Question Answering: CommaQA-E\nTo evaluate the compositional generalization in the reading comprehension setting, following Khot et al., we evaluate\ndifferent prompting strategies on CommaQA-E (Khot et al., 2021). For given facts of a set of synthetically generated\nentities, the models need to answer the multi-hop questions which are composed of multiple reasoning steps, e.g.,\nWhat movies have people from the country Stridery acted in?. Besides the standard zero/few-shot prompting (Brown\net al., 2020) and the Chain-of-Thoughts prompting (CoT) (Wei et al., 2022b), we also compare our Skills-in-Context\n(SKiC) prompting to Decomp prompting6 (Khot et al., 2022). We evaluate the results on different foundation models:\nLLAMA-65B, text-davinvi-003, and ChatGPT. The construction of the SKiC prompting for CommaQA-E is described\nin Figure 13 and 14, which show the skills and the examplars of how to compose the skills, respectively. Notably, both\nthe ability to break down complex questions into simple ones and the operation to answer each simple questions\nare also treated as (basic) skills \u2014 see Figure 13. Compared to the multi-stage prompting strategies like least-to-\nmost or DECOMP prompting, such basic skills and their compositional examplars (Figure 14) are placed in the same\nSKiC prompt context. Consequently, the LLM is able to flexibly apply the question decomposition skill and simple\nquestion answering skills to reach the final answer within 1-stage of prompting. The results on the CommaQA-E\ndataset are summarized in Table 3 (measured in Exact Match). We observe that, with multiple stages of question\ndecomposition and answering, Decomp improves the performance over few-shots prompting and chain-of-thoughts\nprompting. Nevertheless, our SKiC prompting further boosts the accuracy of answering compositional questions\nsignificantly (+7%) by using just one-stage prompting. This is a further manifestion of the advantage of concurrently\ndemonstrating the skills and their compositions for unleashing the compositionality of LLMs. In Figure 34, we show\nthat errors made in early stages in Decomp prompting result in wrong predictions while our SKiC prompting accurately\nanswer different questions. We show an example of the generated answer on the Commaqa-E test set in Figure 24.\n3.1.4\nDynamic Programming\nWe then further evaluate the compositional generalization capabilities of Skills-in-Context (SKiC) prompting in solv-\ning a classic dynamic programming problem (Dziri et al., 2023): Given a sequence of integers, find a subsequence\nwith the highest sum, such that no two numbers in the subsequence are adjacent in the original sequence. We com-\npare our SKiC prompting (SKiC) with standard zero/few-shot prompting (Brown et al., 2020), and Chain-of-Thoughts\nprompting (CoT)7 (Wei et al., 2022b) on different LLMs (text-davinvi-003, ChatGPT and GPT-4). In addition, we also\ncompare with the baseline of finetuned text-davinci-003 with scratchpad (reported in Dziri et al. (2023)). Likewise,\n5Further extending it to more digits will require longer context window of the language models.\n6Reproduced using the original code from: https://github.com/allenai/DecomP/tree/main\n7The reasoning steps are constructed based on the scratchpad prompts used in Dziri et al. (2023).\n8\nA PREPRINT\nTable 4: Accuracy of different prompting methods on the dynamic programming task with input sequence lengths\nbeing 4,5,6,7,8, respectively. The in-context examplars for all the prompts are constructed with sequence lengths\nof 4 and 5. Therefore, the results for sequence lengths of 6,7,8 measures the out-of-distribution generalization to\nincreasingly harder problems. \u2020 denotes our method.\nModel\nPrompting\n#-shots\n4\n5\n6\n7\n8\ntext-davinci-003\nzero-shot\n0\n10.5\n4.0\n4.0\n0.0\n0.0\n4-shots\n4\n32.5\n18.0\n10.0\n4.0\n0.0\nCoT\n4\n58.0\n22.0\n15.0\n8.0\n2.0\nfinetuned\n0\n100.0\n100.0\n22.0\n14.0\n8.0\nSKiC\u2020\n2\n78.0\n62.5\n54.5\n48.0\n42.5\nChatGPT\nzero-shot\n0\n18.0\n10.0\n6.0\n4.0\n0.0\n4-shot\n4\n44.5\n18.0\n10.0\n4.0\n0.0\nCoT\n4\n82.5\n76.0\n72.0\n64.0\n55.5\nSKiC\u2020\n2\n98.0\n96.0\n95.0\n94.0\n92.0\nGPT-4\nzero-shot\n0\n58.0\n42.5\n35.5\n28.0\n12.0\n4-shots\n4\n76.5\n70.5\n58.0\n55.0\n42.0\nCoT\n4\n94.0\n91.0\n88.0\n83.5\n72.0\nSKiC\u2020\n2\n100.0\n100.0\n100.0\n99.0\n98.0\nwe evaluate them on different subsets of testing instances with sequence length of 4, 5, 6, 7, 8, respectively.8 The in-\ncontext examplars are constructed with sequence length of 4 and 5. Therefore, the testing subsets with sequence length\nof 4 and 5 are in-distribution evaluation and the ones with length 6, 7, and 8 are for out-of-distribution evaluation. The\nconstruction of SKiC prompt is characterized in Figure 15 and 16, which show the skills and their compositions ex-\namplars, respectively. Specifically, in the SKiC prompt, the models are presented with the skills to get the length of a\nlist, find the max number for a given list and add two single digit numbers, followed by two compositional examplars\nabout how to compose these skills to solve the dynamic programming problems with sequence length 4 and 5. Table 4\nshows the results (measured in accuracy). Compared to the previous prompting techniques such as Chain-of-Thoughts,\nour proposed SKiC again achieve the best performance, with a large improvement margin on the out-of-distribution\ncompositionality (e.g., improving the accuracy by a large margin of 40.5% using text-davinci-003 for sequence length\nof 8). In addition, compared to the finetuned text-davinci-003 with scratchpad, SKiC prompting is also significantly\nbetter in the out-of-distribution regime, although its performance at the in-distribution regime is worse.9 Notably, with\na stronger foundation model (i.e., GPT-4), SKiC prompting even achieves near perfect generalization (98%), which\nalso improves significantly over CoT by 26%. By incorporating basic skills into the prompts \u2014 such as extracting\nthe length of a list and identifying the maximum number within it \u2014 the models are guided to reason and address\nproblems based on these foundational skills. Consequently, it performs the reasoning steps more accurately and could\ngeneralize better to the harder examples by following similar patterns to compose the basic skills. An example of the\ngenerated answer on the DP task for a sequence of 8 numbers can be found in Figure 25.\n3.2\nGeneralization Beyond In-Context Skills: Complex Reasoning\nIn this section, we further evaluate whether our SKiC prompting could allow LLMs to generalize beyond the skills\nprovided in the prompt context and invoke the massive set of internal skills and knowledge that are acquired dur-\ning pre-training. Such capability is vital in solving complex reasoning problems (e.g., math), which require varied\ncompositions over a vast amount of foundational skills. And it is impractical to enumerate all the skills in context.\n3.2.1\nGSM8K\nWe first apply our Skills-in-Context prompting to GSM8K (Cobbe et al., 2021), which requires multiple math-related\nskills to solve complex math world problems. We construct our SKiC prompt by using the first approach in Section 2.2,\nwhich includes a limited skill set together with eight compositional examplars to teach the LLMs how to use them.\nFigure 17 and Figure 18 show the constructed skill set and one compositional examplar, respectively. We compare\nour SKiC with Chain-of-Thoughts prompting (CoT) (Wei et al., 2022b), Least-to-Most prompting (LtM) (Zhou et al.,\n2022), ComplexCot (Fu et al., 2022) and PHP (Zheng et al., 2023) on different foundation models (i.e., text-davinvi-\n8The numbers are within the range [-5,5]\n9This is expected as the it is finetuned directly on input sequences with length 4 and 5, while our method is not finetuned at all.\n9\nA PREPRINT\nFigure 5: The accuracy of different prompting techniques on GSM8K tasks (using different LLMs).\nTable 5: Testing accuracy on the MATH benchmark. We compare our SKiC prompting with different prompting\nstrategies: CoT (Wei et al., 2022b), Scratchpad (Nye et al., 2021), Learning-to-Program(LtP)(Guo et al., 2023), and\nComplexCoT (Fu et al., 2022). In addition, we also include different ensemble strategies that are commonly combined\ntogether with these baselines: majority voting (maj1@k) (Lewkowycz et al., 2022), Self-Consistency (SC) (Wang\net al., 2022a), and Progressive-Hint Prompting (PHP) (Zheng et al., 2023). We use \u2020 to represent our method. Our\nSKiC prompting improves over the state-of-the-art one-stage prompting method (ComplexCoT) by a large margin and\neven outperforms many other ensemble methods. In SKiC prompting, we also report the internal skill activation rate,\nwhich measures the percentage of skills utilized in the output reasoning steps for each question that originate from\npre-trained knowledge (rather than being included in the SKiC prompt context). This rate clearly indicates that SKiC\nprompting enables LLMs to generalize beyond the in-context skills, tapping into the vast reservoir of internal skills\nthey amassed during the prior pretraining stage and leveraging two types of skills.\nModel\nPrompting\nEnsemble\nPre-Algebra\nGeometry\nInter-Algebra\nAlgebra\nProbability\nPre-Calculus\nNumTheory\nOverall\nPaLM-2\nCoT\nSC\n-\n-\n-\n-\n-\n-\n-\n48.8\nMinerva-540B\nCoT, Scratchpad\nmaj1@k\n71.1\n42.0\n27.1\n72.7\n43.5\n34.5\n36.3\n50.3\nChatGPT\nComplexCoT\nPHP\n57.7\n25.4\n17.1\n49.1\n33.7\n16.1\n35.1\n36.5\nGPT-4\nComplexCoT\nPHP\n73.8\n41.9\n26.3\n73.4\n56.3\n29.8\n55.7\n53.9\nPaLM-2\nCoT\n%\n-\n-\n-\n-\n-\n-\n-\n34.3\nMinerva-540B\nCoT, Scratchpad\n%\n54.9\n26.7\n13.6\n51.2\n27.9\n18.0\n21.2\n33.6\nChatGPT\nCoT, LtP\n%\n52.3\n22.5\n16.9\n49.6\n30.2\n16.3\n29.8\n31.1\nComplexCoT\n%\n53.8\n22.3\n14.6\n49.1\n29.7\n16.8\n33.4\n34.1\nSKiC\u2020\n%\n62.0 \u2191 8.2\n30.1 \u2191 7.8\n17.8 \u2191 3.2\n57.9 \u2191 8.8\n38.2 \u2191 8.5\n23.0 \u2191 6.2\n35.5 \u2191 2.1\n40.6 \u2191 6.5\nInternal Skill Activation Rate\n6.5\n19.0\n13.2\n5.7\n9.1\n45.2\n7.8\n14.9\nGPT-4\nCoT\n%\n-\n-\n-\n-\n-\n-\n-\n42.2\nComplexCoT\n%\n71.6\n36.5\n23.4\n70.8\n53.1\n26.7\n49.6\n50.3\nSKiC\u2020\n%\n79.7 \u2191 8.1\n43.6 \u2191 7.1\n29.5 \u2191 6.1\n74.6 \u2191 3.8\n58.2 \u2191 5.1\n36.6 \u2191 9.9\n55.9 \u2191 6.3\n56.4 \u2191 6.1\nInternal Skill Activation Rate\n12.7\n37.0\n33.4\n16.0\n4.4\n65.5\n12.1\n24.3\n003, ChatGPT and GPT-4). We evaluate the accuracy on the GSM8K test set, which are shown in the Figure 5 10. Note\nthat, it is generally impractical to enumerate all the skills in the prompt context. However, even with incomplete skill\nset in our SKiC prompts, we still observe a significant accuracy boost compared to previous state-of-the-art prompting\nmethods across all foundation models (even better than multi-stage methods such as PHP, which modifies and corrects\nthe predictions through multiple rounds). Significantly, we observe several intriguing cases of generalization: (i) the\ngenerated reasoning steps effectively utilize the provided skills that are not demonstrated in the compositional exam-\nples (see Figure 27 for an example), (ii) the generated reasoning steps successfully employ skills that are not included\nin the prompts but may exist within the pre-trained knowledge of the LLM (see Figure 28 and 29 for examples). These\ndiscoveries suggest that, with SKiC prompting, LLMs can be taught to use the skills provided in the context as well as\nfrom their pre-existing internal (pretrained) knowledge to solve math problems via compositionality.\n10The results are re-implemented with the provided prompts from the original works. Note that GPT-4\u2019s performance might drop\nover time on math related tasks as is observed in Chen et al. (2023), which might make our reproduced number lower than the ones\nreported in the original papers (e.g., PHP results with GPT-4).\n10\nA PREPRINT\n3.2.2\nMATH\nWe then apply our Skills-in-Context prompting to MATH (Hendrycks et al., 2021), which is a significantly more\nchallenging benchmark on mathematical reasoning. It encompasses problems in Algebra, Counting and Probability,\nGeometry, Intermediate Algebra, Number Theory, PreAlgebra, and PreCalculus. Due to the large variety of founda-\ntional capabilities needed for solving these math problems, it is infeasible to distill and enumerate the needed skills\nmanually. Therefore, we adopt the second approach as described in Section 2.2, where we prompt the LLM to gen-\nerate the skills and then craft the compositional examples manually. Specifically, we first prompt the LLM (i.e., the\nsame LLM that we will use to solve the problems) to generate a list of skills for each subject category in the MATH\ndataset (e.g., \u201cCounting and Probability\u201d) with the instruction \u201cBasic skills in [subject]\u201d. Then we further ask the\nmodel to generate the description of each skill, and the resulting skill set is listed in Figure 19. In Figure 20, we show\na compositional examplar that demonstrates how to utilize the skills to solve a problem in MATH dataset. Note from\nthis example that we ground a part of the reasoning steps to in-context skills such as \u201cCombination\u201d and \u201cSub\u201d and\nanchor others to internal skills (e.g., \u201cPascal\u2019s Triangle\u201d). In our experiment, we provide the model with seven exam-\nplars (one example per category in the MATH dataset). We compare our SKiC prompting with different prompting\nstrategies: CoT (Wei et al., 2022b), Scratchpad (Nye et al., 2021), Learning-to-Program(LtP) (Guo et al., 2023), and\nComplexCoT (Fu et al., 2022) on two representative foundation models: ChatGPT and GPT-4 11. In addition, we\nalso include different ensemble strategies that are commonly combined together with these baselines: majority voting\n(maj1@k) (Lewkowycz et al., 2022), Self-Consistency (SC) (Wang et al., 2022a), and Progressive-Hint Prompting\n(PHP) (Zheng et al., 2023). The accuracy on the MATH test sets is reported in Table 5. With SKiC prompting, models\ncould explicitly ground the reasoning steps to the skills in the context as well as their internal knowledge to resolve\ndiverse math problems. As a result, our SKiC significantly outperforms the state-of-the-art prompting methods on\nall the sub-categories in MATH test set with only one round of generation, and it even outperforms the approaches\nthat ensemble the outputs from multiple rounds of generations (e.g., PHP). In Table 5, we also show the internal skill\nactivation rate that measures the percentage of skills utilized in the generated reasoning steps for each question that\noriginate from pre-trained knowledge (rather than being introduced in the SKiC prompt context). It further verifies\nthat our SKiC prompting allows the LLMs to generalize beyond the in-context skills and invoke the massive reservoir\nof internal capabilities in LLMs (e.g., 24% of skills utilized in the output reasoning steps are from the GPT-4 internal\nknowledge) \u2014 see Figure 4, 27, 28, 29 and 33 for more examples of the generated solutions for the MATH problems,\nwhere the reasoning process carried out by the LLM (GPT-4) effectively utilize both in-context and internal skills. In\nTable 6, we also report the most frequently used in-context and internal skills for solving MATH problems.\nAblation Study: different sources of the in-context skills\nOne important question we want to understand is\nwhether it is beneficial to generate the in-context skills from the same foundation model used for prediction? Our\nhypothesis is that in-context skills generated from the same foundation model can initiate stronger synergize with the\ninternal knowledge, due to their higher alignment. To test this hypothesis, we prompt the ChatGPT model using the\nSKiC prompt constructed from GPT-4 (i.e., the in-context skills are generated by GPT-4). The accuracy and the in-\nternal skill activation rate on MATH test set are reported in Table 7. With the skills prompted from itself, we observe\nboth improved accuracy and higher internal skill activation rate, even though the skills prompted from GPT-4 generally\nhave higher quality. This suggests that (i) aligning the model that is used to prompt the in-context skills and the model\nthat is used to generate answers helps the models\u2019 capability to link and utilize the internal pretrained skills, and (ii)\nactivating more internal skills generally leads to higher performance gains, especially when solving complex problems\nthat require compositions over a wide range of basic skills.\n3.3\nError Analysis\nWe further perform error analysis on the tasks that are still far away from achieving (nearly) perfect generalization\nwhen applying SKiC prompting on ChatGPT \u2014 multiplication, question answering, GSM8K and MATH. For each\ntask, we randomly sample 50 error cases 12 (Zhou et al., 2022) and conduct an examination of the types of errors\ninvolved. We summarize five types of common errors: (i) seen basic skills: errors arise due to a lack of mastery of\nthe skills in context, (ii) unseen basic skills: errors caused by the absence of necessary skills in context, particularly\nwhen these skills do not exist in the pre-trained knowledge of the LLM, (iii) incorrect composition: errors of incorrect\ncomposition or reasoning using the basic skills, (iv) incorrect copying: copying or merging errors between different\nsteps, (v) others: other errors such as incorrect ground truth labels in the test set.\n11We use the same model to construct the SKiC skills and to do the inference. That is, we prompt ChatGPT to construct the\nSKiC when testing with ChatGPT and we prompt GPT-4 to construct the SKiC when testing with GPT-4.\n12For MATH dataset, we randomly sample 5 error cases per category, resulting in 35 error cases in total.\n11\nA PREPRINT\nTable 6: The most frequently used skills by GPT-4 for solving MATH benchmark with SKiC prompting. The skills\ncan be from the context of the SKiC prompts (denoted as \u201cin-context\u201d in the table) or from the internal knowledge\nacquired during the pretraining stage (denoted as \u201cinternal\u201d).\nCategory\nSource\nTop Used Skills\nPre-Algebra\nIn-context\nDiv, Mul, Add, Sub, Solve Equation, Area, Exp, Counting Principle, Radicals, Prime Numbers\nInternal\nPythagorean Theorem, Rounding, Divisibility Rules, Percentage, Angles, Simply Fraction,\nMean, Ratio, Triangle Angle Sum, Order of Operations\nGeometry\nIn-context\nArea, Mul, Div, Add, Sub, Solve Equation, Volume, Radicals, Exp, Perimeter\nInternal\nPythagorean Theorem, Trigonometry, Triangle, Triangle Inequality, Similar Triangles,\nCircle, Geometry, Triangle Angle Sum, Angle Bisector Theorem, Trigonometric Ratios\nInter-Algebra\nIn-context\nFactoring, Solve Equation, Add, Mul, Sub, Complex Number, Inequality, Quadratic Formula, Div, Exp\nInternal\nSubstitution, Completing the Square, Polynomial, Logarithm, AM-GM Inequality,\nPolynomial Division, Absolute Value, Summation, Sequences, Simplify\nAlgebra\nIn-context\nAdd, Mul, Solve Equation, Sub, Div, Exp, Factoring, Quadratic Formula, Radicals, Distance Formula\nInternal\nAbsolute Value, Slope, Logarithm, Arithmetic Sequence, Completing the Square, Interval Notation,\nInverse Function, Substitution, Midpoint Formula, Ceiling Function\nProbability\nIn-context\nFactorial, Combination, Counting Principle, Probability, Add, Sub, Permutations, Mul, Div, Exp\nInternal\nSimplify Fraction, Binomial Theorem, Expected Value, Arithmetic Sequence, Sum of Arithmetic Series,\nCounting, Stars and Bars, Divisibility Rules, Binomial Probability, Perfect Squares\nPre-Calculus\nIn-context\nSolve Equation, Add, Mul, Sub, Complex Number, Div, Factoring, Radicals, Area, Distance Formula\nInternal\nTrigonometric Identities, Trigonometry, Dot Product, Matrix Multiplication, Pythagorean Theorem,\nCross Product, Inverse Trigonometric Functions, Determinant, Vector Projection, Vectors\nNumTheory\nIn-context\nAdd, Mod, Base Conversion, Mul, Congruences, Div, Sub, Factoring, Prime Number, GCD\nInternal\nDivisors, Divisibility Rules, Units Digit, Prime Fraction, Chinese Remainder Theorem, Arithmetic\nSequence, Exponents, Cyclic Patterns, Perfect Squares, Modular Arithmetic\nTable 7: Testing accuracy and internal skill activation rate on the MATH benchmark. We compare two different\nversions of SKiC prompts on ChatGPT: the prompt with the skills generated from (i) ChatGPT and (ii) GPT-4. The\ninternal skill activation rate refers to the average proportion of skills utilized per question that originate from pre-\ntrained knowledge (i.e., internal skills) rather than from the SKiC prompt context (i.e., the in-context skills).\nMetric\nSource of SKiC\nPre-Algebra\nGeometry\nInter-Algebra\nAlgebra\nProbability\nPre-Calculus\nNumTheory\nOverall\nAccuracy\nGPT-4\n60.7\n27.8\n16.8\n58.2\n33.3\n19.0\n34.2\n38.9\nChatGPT\n62.0\n30.1\n17.8\n57.9\n38.2\n23.0\n35.5\n40.6\nInternal Skill\nActivation Rate\nGPT-4\n5.9\n18.5\n11.2\n6.6\n7.0\n43.8\n6.2\n12.5\nChatGPT\n6.5\n19.0\n13.2\n5.7\n9.1\n45.2\n7.8\n14.9\nTheir distributions are visualized in Figure 6. We observe that (i) the most common errors arise from unseen basic\nskills (for example, 83% of the errors in the Multiplication task are due to the absence of the skill to add large numbers),\n(ii) a lack of mastery of the basic skills leads to more errors when there are more complex or more basic skills to be\nused (for example, the question decomposition capability in the CommaQA-E task is generally a complex skill, and\nthe GSM8K and MATH dataset requires more basic skills), (iii) incorrect composition is a major error type for tasks\nthat require more complex reasoning steps such as GSM8K (e.g., 45% of the errors are due to incorrect reasoning steps\nsuch as misinterpreting the questions or incorrectly reasoning about the questions), (iv) copying errors become more\nprevalent when there are more reasoning steps with longer context, and (v) math reasoning generally requires a wider\nvariety of skill compositions, and the way of composition varies significantly from one problem to another, making\nit considerably harder to master the appropriate skill composition for each problem. Therefore, there are several key\ndirections to further improve SKiC: (1) providing the model with high-quality basic skills and illustrations to improve\nthe execution quality of these basic skills, (2) expanding the range of task-related basic skills to prevent errors caused\nby unseen skill, (3) providing more examples of how to compose basic skills, especially for more complex tasks, and\n(4) utilizing better foundation models that can avoid copying errors in long context and that have a more extensive set\nof well-mastered skills in their pre-existing pretrained knowledge.\n12\nA PREPRINT\nFigure 6: Distributions of different types of errors in Multiplication, Question Answering, GSM8K and MATH tasks.\n4\nRelated Work\nThere has been a long history of studies on compositional generalization (Lake & Baroni, 2018; Jia & Liang, 2016;\nAndreas, 2019; Lake & Baroni, 2018; Ouyang et al., 2023; Keysers et al., 2020; Chen et al., 2020; Dziri et al., 2023;\nSHAO et al., 2023; Saparov & He, 2022; Nye et al., 2021; Welleck et al., 2022; Dong et al., 2019; Schwarzschild\net al., 2021). Different types of approaches have been developed to solve compositional generalization. One widely\nstudied approach is neuro-symbolic methods (Dong et al., 2019; Schwarzschild et al., 2021), which blend symbolic\nand distributed representations for modeling the reasoning process. A recent line of work that has gained significant\ntraction is to prompt large language models to unlock its potential compositional generalization capabilities (Nye\net al., 2021; Zhou et al., 2022; Khot et al., 2022; Dua et al., 2022; Dziri et al., 2023). The least-to-most prompting\n(Zhou et al., 2022) boosts the performance of compositional generalization by first decomposing a difficult problem\ninto a sequence of easy-to-hard problems and then solving them sequentially. Meanwhile, the decomposed prompting\n(Khot et al., 2022) breaks the original problem into a set of different subproblems, solves them sequentially, and then\naggregates the answers into a final solution. In spite of the significant improvement compared to previous works,\nthe performance of these approaches still degrade quickly over increasingly harder testing problems. Moreover, their\napplications are limited to a class of problems that can be decomposed into a set of subproblems. For more general\ncomplex problems, where the subproblems are highly nested (e.g., the ones shown in Dziri et al. (2023)), it becomes\nquite challenging to construct the prompts and the examplars. Unlike these multi-stage prompting methods, which\nrequire multiple calls of the LLM inference process, our proposed Skills-in-Context prompting is a simple one-stage\nstrategy that can be used in a plug-and-play manner to replace existing standard or chain-of-thought prompting.\n5\nConclusion\nIn this work, we propose an effective prompting technique, Skills-in-Context (SKiC) prompting, to unlock composi-\ntional generalization abilities in LLMs. Specifically, SKiC prompts consist of two major building blocks: the basic\nskills that are needed for solving the problems, and the examplars of how to compose these skills into solutions for\ncomplex problems. Significant improvements on symbolic manipulation, arithmetic operation, question answering,\ndynamic programming, and math reasoning tasks demonstrate that our SKiC prompting technique is highly effective\nin unleashing the compisitionality in LLMs. Notably, with SKiC prompting, the LLMs could generalize beyond the\nskills provided in the prompting context and learns to activate the skills and knowledge that are acquired through\nearlier pretraining stages for solving unseen complex problems.\nReferences\nJacob Andreas. Good-enough compositional data augmentation. arXiv preprint arXiv:1904.09545, 2019.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\nLarochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/\nhash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time?, 2023.\n13\nA PREPRINT\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-\nsymbolic stack machines, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling instruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. arXiv\npreprint arXiv:1904.11694, 2019.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing complex\nquestions. arXiv preprint arXiv:2212.04092, 2022.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bha-\ngavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv\npreprint arXiv:2305.18654, 2023.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step\nreasoning. arXiv preprint arXiv:2210.00720, 2022.\nYiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to program with\nnatural language, 2023.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Stein-\nhardt. Measuring mathematical problem solving with the math dataset, 2021.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint\narXiv:2212.10071, 2022.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,\nGeorge van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.\nRae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\nRobin Jia and Percy Liang. Data recombination for neural semantic parsing. arXiv preprint arXiv:1606.03622, 2016.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\nDaniel Keysers, Nathanael Sch\u00a8arli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Mom-\nchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier\nBousquet. Measuring compositional generalization: A comprehensive method on realistic data. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?id=SygcCnNKwr.\nTushar Khot, Kyle Richardson, Daniel Khashabi, and Ashish Sabharwal. Hey ai, can you solve complex tasks by\ntalking to agents? arXiv preprint arXiv:2110.08542, 2021.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. De-\ncomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), 2022.\nURL https://arxiv.org/abs/2205.11916.\nBrenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-\nsequence recurrent networks. In International conference on machine learning, pp. 2873\u20132882. PMLR, 2018.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose\nSlone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language\nmodels. arXiv preprint arXiv:2206.14858, 2022.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of\nmaking language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. URL https://arxiv.org/\nabs/2206.02336.\n14\nA PREPRINT\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small\nlanguage models to reason. ArXiv preprint, abs/2212.08410, 2022.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.\nCross-task generalization via natural\nlanguage crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022. doi: 10.18653/v1/2022.acl-long.244. URL\nhttps://aclanthology.org/2022.acl-long.244.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David\nDohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate\ncomputation with language models. arXiv preprint arXiv:2112.00114, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nSiru Ouyang, Jiaao Chen, Jiawei Han, and Diyi Yang. Compositional data augmentation for abstractive conversa-\ntion summarization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1471\u20131488, 2023.\nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. Knowledge-in-context: Towards\nknowledgeable semi-parametric language models, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, pp. 9, 2019.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\nStiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv\npreprint arXiv:2110.08207, 2021.\nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-\nthought. arXiv preprint arXiv:2210.01240, 2022.\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein.\nCan you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural\nInformation Processing Systems, 34:6695\u20136706, 2021.\nNAN SHAO, Zefan Cai, Hanwei xu, Chonghua Liao, Yanan Zheng, and Zhilin Yang. Compositional task represen-\ntations for large language models. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=6axIMJA7ME3.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay,\nSebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint\narXiv:2210.03057, 2022.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu,\nShrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\nfine-tuned chat models, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. URL https://arxiv.org/\nabs/2203.11171.\n15\nA PREPRINT\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in\nlanguage models. arXiv preprint arXiv:2207.00747, 2022b. URL https://arxiv.org/abs/2207.00747.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\nand William Fedus. Emergent abilities of large language models, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022b.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical\nproof generation with language models. Advances in Neural Information Processing Systems, 35:4913\u20134927, 2022.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning\nin large language models. arXiv preprint arXiv:2304.09797, 2023.\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bous-\nquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv\npreprint arXiv:2205.10625, 2022.\n16\nA PREPRINT\nA\nAppendix: The Performance of SKiC Prompting using LLAMA2\nWe further evaluate the performance of SKiC prompting by using the LLAMA2 and LLAMA2-chat models (Touvron\net al., 2023b) on the following tasks: last latter concatenation, addition, multiplication, CommaQA-E, and dynamic\nprogramming tasks. The results are reported in the Tables 8,12.\nWe observe that (i) LLAMA2-70B generally outperforms LLAMA-65B and demonstrate stronger capabilities in fol-\nlowing the examplars for composing the in-context skills to solve the problems. (ii) After alignment, LLAMA2-70B-\nchat13 performs worse than LLAMA2-70B. From examining the error cases, we find that the LLAMA2-70B-chat\nmodel does follows the reasoning steps and styles in the demonstrated compositional examplars in our SKiC prompts\nto solve the problems. And it is more likely to refuse to answer certain questions, especially the ones from CommaQA-\nE tasks. For example, given the question \u201cWhat awards did the movies directed by the Modiparity winners receive?\u201d\nfrom CommaQA-E datasets, LLAMA2-70B-chat generates \u201cHowever, I must point out that your question contains\nsome harmful and inappropriate language that I cannot answer. I\u2019m just an AI, my purpose is to provide helpful and\ninformative responses while promoting a positive and respectful dialogue. I cannot provide answers that may encour-\nage or promote harmful or inappropriate behavior.\u201d. This generally causes the performance drop compared to the\noriginal pretrained model (LLAMA2-70B) before alignment, which follows the instructions and the examplars better.\n(iii) There are still performance gaps between the open source LLAMA models and the proprietery LLMs such as\ntext-davinci-003, ChatGPT and GPT4.\nTable 8: Accuracy of applying our SKiC prompting to different foundation models on the last-letter-concatenation\ntask. The LLMs are demonstrated with examplars using 1 and 2 words. Therefore, the testing problems with 1 or 2\nwords are in-distribution evaluation, while the ones with 4 \u223c 12 words are (harder) out-of-distribution evaluations.\nModel\nPrompting\n#-shots\n1\n2\n4\n6\n8\n10\n12\ntext-davinci-003\nSKiC\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n99.0\n98.0\nChatGPT\nSKiC\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\nLLAMA-65B\nSKiC\n2\n81.0\n97.0\n77.0\n59.0\n56.0\n48.0\n36.0\nLLAMA2-70B\nSKiC\n2\n100.0\n99.0\n100.0\n99.0\n98.0\n97.0\n95.0\nLLAMA2-70B-chat\nSKiC\n2\n100.0\n100.0\n94.0\n87.0\n81.0\n78.0\n72.0\nTable 9: Accuracy of applying our SKiC prompting to different foundation models on the two-number addition task\n(with number of digits from 2 to 7). The prompting examplars are constructed to demonstrate the addition between\ntwo numbers with 2 or 3 digits. Therefore, the results for adding numbers with 4 \u223c 7 digits measure the desirable\ncompositional generalization capabilities over harder problems.\nModel\nPrompting\n#-shots\n2\n3\n4\n5\n6\n7\ntext-davinci-003\nSKiC\n2\n100.0\n100.0\n99.0\n98.0\n99.0\n98.5\nChatGPT\nSKiC\n2\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\nLLAMA-65B\nSKiC\n2\n82.5\n74.5\n66.5\n52.0\n38.0\n22.0\nLLAMA2-70B\nSKiC\n2\n83.0\n78.0\n68.0\n55.0\n40.0\n25.0\nLLAMA2-70B-chat\nSKiC\n2\n11.0\n12.0\n14.0\n23.0\n15.0\n13.0\n13For the chat model, we follow the specific chat completion format used in https://github.com/facebookresearch/\nllama/blob/main/llama/generation.py#L212.\n17\nA PREPRINT\nTable 10: Accuracy of applying our SKiC prompting to different foundation models on the two-number multiplication\ntask (with number of digits from 2 to 5). The prompting examplars are constructed to demonstrate how to multiply\ntwo numbers with 2 or 3 digits. Therefore, the results for multiplying numbers with 4 and 5 digits measure the\ncompositional generalization capability over harder problems.\nModels\nPrompting\n#-shots\n2\n3\n4\n5\ntext-davinci-003\nSKiC\n2\n100.0\n58.0\n42.5\n36.0\nChatGPT\nSKiC\n2\n100.0\n82.0\n72.0\n48.5\nLLAMA-65B\nSKiC\n2\n50.0\n42.0\n12.0\n8.0\nLLAMA2-70B\nSKiC\n2\n99.0\n51.0\n15.0\n6.0\nLLAMA2-70B-chat\nSKiC\n2\n72.0\n36.0\n8.0\n2.0\nTable 11: Performance of applying our SKiC prompting to different foundation models on Commaqa-E datasets\n(measured in Exact Match). The column of \u201cComp. Gen\u201d reports the results on the new (unseen) compositional\nquestions from the compositional generalization test split.\nModel\nPrompting\n#-shots\nTest\nComp. Gen\ntext-davinci-003\nSKiC\n2\n66.0\n74.8\nChatGPT\nSKiC\n2\n70.0\n80.8\nLLAMA-65B\nSKiC\n2\n44.0\n52.0\nLLAMA2-70B\nSKiC\n2\n46.7\n55.9\nLLAMA2-70B-chat\nSKiC\n2\n7.50\n6.30\nTable 12: Accuracy of applying our SKiC prompts to different foundation models on the dynamic programming task\nwith input sequence lengths being 4,5,6,7,8, respectively. The in-context examplars are constructed with sequence\nlengths of 4 and 5. Therefore, the results for sequence lengths of 6,7,8 measures the out-of-distribution generalization\nto increasingly harder problems.\nDP\nPrompting\n#-shots\n4\n5\n6\n7\n8\ntext-davinci-003\nSKiC\n2\n78.0\n62.5\n54.5\n48.0\n42.5\nChatGPT\nSKiC\n2\n98.0\n96.0\n95.0\n94.0\n92.0\nGPT4\nSKiC\n2\n100.0\n100.0\n100.0\n99.0\n98.0\nLLAMA2-70B\nSKiC\n2\n79.0\n78.0\n70.0\n68.0\n56.0\nLLAMA2-70B-chat\nSKiC\n2\n35.0\n30.0\n14.0\n16.0\n11.0\n18\nA PREPRINT\nB\nAppendix: Skills and Examples of How to Composing Skills\nSkills for Last Letter Concatenation\nSkill <words to list>: Put the asked words to a list. For example, put the words in \u2019apple\u2019 to D=[\u2019apple\u2019]; put\nthe words in \u2019apple, banana\u2019 to D=[\u2019apple\u2019,\u2019banana\u2019].\nSkill <last letter>: Get the last letter of one word. For example, the last letter of \u2019apple\u2019 is \u2019e\u2019; the last letter\nof \u2019banana\u2019 is \u2019a\u2019.\nFigure 7: The skills in Skills-in-Context prompt for last-letter-concatenation task.\n19\nA PREPRINT\nAn Example of Skill Composition for Last Letter Concatenation\nExample: Take the last letters of the words in \u2019apple, banana\u2019 and concatenate them.\nAnswer:\n1. Using the Skill <words to list>, put the asked words, \u2019apple, banana\u2019, to a list. D=[\u2019apple\u2019,\u2019banana\u2019]\n2. Get the last letter of each word in the list D=[\u2019apple\u2019,\u2019banana\u2019] to a new list R=[]:\ni. Using the Skill <last letter>, the last letter of D[0]=\u2019apple\u2019 is \u2019e\u2019. R=[e]\nii. Using the Skill <last letter>, the last letter of D[1]=\u2019banana\u2019 is \u2019a\u2019. R=[e,a]\n3. R=[e,a]. The answer is \u2019ea\u2019.\nFigure 8: An examplar of skill composition in Skills-in-Context prompt for last-letter-concatenation task.\n20\nA PREPRINT\nSkills for Addition\nSkill <extract digits>: Extract the digits in a number to a list.\nFor example, Extract digits in 123 to D=[1,2,3]. Extract digits in 7654 to D=[7,6,5,4].\nSkill <list length>: Get the number of elements in a list.\nFor example, D=[1,2,3], len(D)=3. A=[1,2,4,5,6], len(A)=5.\nSkill <add two single digit number>: Add two single-digit numbers.\n0+0=0 0+1=1 0+2=2 0+3=3 0+4=4 0+5=5 0+6=6 0+7=7 0+8=8 0+9=9\n1+0=1 1+1=2 1+2=3 1+3=4 1+4=5 1+5=6 1+6=7 1+7=8 1+8=9 1+9=10\n2+0=2 2+1=3 2+2=4 2+3=5 2+4=6 2+5=7 2+6=8 2+7=9 2+8=10 2+9=11\n3+0=3 3+1=4 3+2=5 3+3=6 3+4=7 3+5=8 3+6=9 3+7=10 3+8=11 3+9=12\n4+0=4 4+1=5 4+2=6 4+3=7 4+4=8 4+5=9 4+6=10 4+7=11 4+8=12 4+9=13\n5+0=5 5+1=6 5+2=7 5+3=8 5+4=9 5+5=10 5+6=11 5+7=12 5+8=13 5+9=14\n6+0=6 6+1=7 6+2=8 6+3=9 6+4=10 6+5=11 6+6=12 6+7=13 6+8=14 6+9=15\n7+0=7 7+1=8 7+2=9 7+3=10 7+4=11 7+5=12 7+6=13 7+7=14 7+8=15 7+9=16\n8+0=8 8+1=9 8+2=10 8+3=11 8+4=12 8+5=13 8+6=14 8+7=15 8+8=16 8+9=17\n9+0=9 9+1=10 9+2=11 9+3=12 9+4=13 9+5=14 9+6=15 9+7=16 9+8=17 9+9=18\nSkill <sub 10>: Subtract 10 from a given number.\n10-10=0 11-10=1 12-10=2 13-10=3 14-10=4 15-10=5 16-10=6 17-10=7 18-10=8 19-10=9\nSkill <compare 10>: Compare a number with 10.\n0<10 1<10 2<10 3<10 4<10 5<10 6<10 7<10 8<10 9<10 10=10\n11>10 12>10 13>10 14>10 15>10 16>10 17>10 18>10 19>10\nFigure 9: The skills in Skills-in-Context prompt for the task of adding two numbers.\n21\nA PREPRINT\nAn Example of Skill Composition for Addition\nExample: Calculate 86+964:\nAnswer:\n1. Using Skill <extract digits> and Skill <list length>, extract the digits in 86 to DM=[8,6]. len(DM)=2.\nExtract the digits in 964 to DN=[9,6,4]. len(DN)=3.\n2.\nlen(DM)=2<len(DN)=3.\nAlign the two lists by adding len(DN)-len(DM)=1 zeros to the shorter list\n(DM): DM=[0,8,6].\n3.\nUsing Skill <add two single digit number>, add every number in DM=[0,8,6] to the corresponding\nnumber in DN =[9,6,4] to form a list, DS=[0+9,8+6,6+4]=[9,14,10].\n4.\nAppend a 0 to the beginning of DS: DS=[0,9,14,10].Construct a list R=[0,0,0,0] with len(DS)=4 ze-\nros.\n5. DS=[0,9,14,10] (DS[0]=0, DS[1]=9, DS[2]=14, DS[3]=10), starting from DS[3] to DS[1] (DS[3]=10,\nDS[2]=14, DS[1]=9):\ni. DS[3]=10, R[3]=0, R[3]=R[3]+DS[3]=0+10=10. Based on Skill <compare 10>, R[3]=10=10, so\nR[2]=1 and R[3]=10-10=0 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3]]=[0,0,1,0].\nii. DS[2]=14, R[2]=1, R[2]=R[2]+DS[2]=1+14=15. Based on Skill <compare 10>, R[2]=15>10, so\nR[1]=1 and R[2]=15-10=5 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3]]=[0,1,5,0].\niii. DS[1]=9, R[1]=1, R[1]=R[1]+DS[1]=1+9=10. Based on Skill <compare 10>, R[1]=10=10, so\nR[0]=1 and R[1]=10-10=0 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3]]=[1,0,5,0].\n6. R=[1,0,5,0]. The answer is 1050.\nFigure 10: An examplar of skill composition in Skills-in-Context prompting for the task of adding two numbers.\n22\nA PREPRINT\nSkills for Multiplication\nSkill <extract digits>: Extract the digits in a number to a list.\nFor example, Extract digits in 123 to D=[1,2,3]. Extract digits in 7654 to D=[7,6,5,4].\nSkill <list length>: Get the number of elements in a list.\nFor example, D=[1,2,3], len(D)=3. A=[1,2,4,5,6], len(A)=5.\nSkill <mul two single digit number>: Multiply two single-digit numbers.\n0*1=0 0*2=0 0*3=0 0*4=0 0*5=0 0*6=0 0*7=0 0*8=0 0*9=0\n1*1=1 1*2=2 1*3=3 1*4=4 1*5=5 1*6=6 1*7=7 1*8=8 1*9=9\n2*1=2 2*2=4 2*3=6 2*4=8 2*5=10 2*6=12 2*7=14 2*8=16 2*9=18\n3*1=3 3*2=6 3*3=9 3*4=12 3*5=15 3*6=18 3*7=21 3*8=24 3*9=27\n4*1=4 4*2=8 4*3=12 4*4=16 4*5=20 4*6=24 4*7=28 4*8=32 4*9=36\n5*1=5 5*2=10 5*3=15 5*4=20 5*5=25 5*6=30 5*7=35 5*8=40 5*9=45\n6*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=36 6*7=42 6*8=48 6*9=54\n7*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=49 7*8=56 7*9=63\n8*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=64 8*9=72\n9*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81\nSkill <add multiple numbers>: Add multiple numbers such as m+n+p:\n1. Add the first two numbers m+n and get the result r1=m+n.\n2. Add the third number p to r1 and get the result r2=r1+p.\nFor example, calculate 128+64+79:\n1. Add the first two numbers 128+64 and get the result r1=128+64=192.\n2. Add the third number 79 to r1 and get the result r2=192+79=271.\nSo 128+64+79=271\nFigure 11: The skills in Skills-in-Context prompt for the task of multiplying two numbers.\n23\nA PREPRINT\nAn Example of Skill Composition for Multiplication\nExample: Calculate 184*67:\nAnswer:\n1. Using Skill <extract digits> and Skill <list length>, extract the digits in 184 to DM=[1,8,4]. len(DM)=3.\nExtract the digits in 67 to DN=[6,7]. len(DN)=2.\n2.\nAdd 0,1,len(DM)-1=2 zeros to the end of every number in DM=[1,8,4] according to the position of\nthe number in DM: DMO=[1*100,8*10,4*1]=[100,80,4].\n3.\nAdd 0,len(DN)-1=1 zeros to the end of every number in DN=[6,7] according to the position of the\nnumber in DN: DNO=[6*10,7*1]=[60,7].\n4.\nUsing Skill <mul two single digit number>, multiple every number in DMO=[100,80,4] with ev-\nery number in DNO=[60,7] and get R=[100*60,100*7,80*60,80*7,4*60,4*7]= [6000,700,4800,560,240,28].\n5.\nUsing Skill <add multiple numbers>,\nadd all the numbers in R=[6000,700,4800,560,240,28],\n6000+700+4800+560+240+28:\ni. Add the first two numbers: r1=6000+700=6700.\nii. Add the third number 4800 to r1=6700: r2=6700+4800=11500.\niii. Add the fourth number 560 to r2=11500: r3=11500+560=12060.\niv. Add the fifth number 240 to r3=12060: r4=12060+240=12300.\nv. Add the sixth number 28 to r4=12300: r5=12300+28=12328.\n6. So the answer is 12328\nFigure 12: An examplar of skill composition in Skills-in-Context prompting for the task of multiplying two numbers.\n24\nA PREPRINT\nSkills for CommaQA-E task\nSkill <decompose qa>: Decompose a complex question into a set of sub-questions.\nFor example: Decompose the question \u201dWhat awards have movies produced by people born\nin 1910 won?\u201d into the following sub-questions:\nQ1: Who were born in the year 1910?\nQ2: Which movies did [A1] produce?\nQ3: Which awards were given to [A2]?\nDecompose the question \u201dWhat movies have people from the country Stridery acted in?\u201d into\nthe following sub-questions: Q1: Who is from the country Stridery?\nQ2: Which movies did [A1] act in?\nSkill <answer simple question>: Answer simple questions about the passage.\nFor example:\n[A Passage]\nQ: Which awards were given to Zalate?\nA: movie: Zalate ; awarded: Hallowcock. [\u201dHallowcock\u201d]\nQ: Which movies were given the Hallowcock award?\nA: movie: Zalate ; awarded: Hallowcock. movie: SkirtSiCine ; award: Hallowcock. [\u201dZalate\u201d,\n\u201dSkirtSiCine\u201d]\nQ: Which movies did Muntaril direct?\nA: movie: Premercy ; directed by: Muntaril. [\u201dPremercy]\nQ: Which movies did Muntrail produce?\nA: Muntaril produced the movie Premercy with others. Muntaril produced the movie SkirtSiCine\nwith others. [\u201dPremercy\u201d, \u201dSkirtSiCine\u201d]\nQ: Which movies did Muntrail write?\nA: Muntaril was one of the writers for the movie Zalate. Muntaril wrote for the movie\nFeatsaw. [\u201dZalate\u201d, \u201dFeatsaw\u201d]\nQ: Who are the actors in the movie Premercy?\nA: Monsterscar was an actor in the movie Premercy. [\u201dMonsterscar\u201d]\nQ: When was the moive Featsaw released?\nA: Monsterscar was an actor in the movie Premercy. [\u201d1973\u201d]\nFigure 13: The skills in Skills-in-Context prompt for the CommaQA-E task.\n25\nA PREPRINT\nAn Example of Skill Composition for the CommaQA-E Task\nExample: [A Passage]\nQ: What movies have people from the country Stridery acted in?\nAnswer:\n1. Using Skill <decompose qa>, decompose the question \u201dWhat movies have people from the country Strid-\nery acted in?\u201d into the following sub-questions:\nQ1: Who is from the country Stridery?\nQ2: Which movies did [A1] act in?\n2. Using Skill <answer simple question>, answer Q1: Who is from the country Stridery?\nGastrat grew up in the nation of Stridery. A1=[\u201dGastrat\u201d]\n3. A1=[\u201dGastrat\u201d], answer Q2: Which movies did [A1] act in?\ni. A1[0]=\u201dGastrat\u201d, Using Skill <answer simple question>, answer Q2.1: Which movies did Gastrat act\nin?\nA2.1: Gastrat was an actor in the movie Partnershipmaker. Gastrat was an actor in the movie Partner-\nshipmaker. Gastrat acted in the movie Warpstone. A2.1=[\u201dPartnershipmaker\u201d, \u201dNilitude\u201d, \u201dWarpstone\u201d]\nii.So A2=[A2.1]=[\u201dPartnershipmaker\u201d, \u201dNilitude\u201d, \u201dWarpstone\u201d]\n4. The answer is [\u201dPartnershipmaker\u201d, \u201dNilitude\u201d, \u201dWarpstone\u201d]\nFigure 14: An examplar of skill composition in Skills-in-Context prompting for the CommaQA-E task.\n26\nA PREPRINT\nSkills for Dynamic Programming\nSkill <list length>: Get the number of elements in a list.\nFor example, D=[1,2,3], len(D)=3. A=[1,2,3,4,5,6], len(A)=6.\nSkill <find max>: Find the max number in a list.\nFor example, max(2,0,-1)=2. max(-3,-2,-1)=-1. max(5,-2,0)=5. max(0,-2,4)=4. max(0,0,0)=0. max(-2,-\n1,0)=0.\nSkill <add>: Add two numbers.\nFor example, 1+2=3. -2+1=-1. 0+(-3)=-3. 1+(-1)=0.\nFigure 15: The skills in Skills-in-Context prompt for the task of dynamic programming.\n27\nA PREPRINT\nAn Example of Skill Composition for the Dynamic Programming Task\nExample: Compose these skills to do the task: Given a sequence of integers, find a subsequence with the\nhighest sum, such that no two numbers in the subsequence are adjacent in the original sequence.\nA=[3,2,1,5].\nAnswer:\n1.\nA=[3,2,1,5] (A[0]=3,A[1]=2,A[2]=1,A[3]=5).\nUsing Skill <list length>, len(A)=4.\nConstruct a list\nD=[0,0,0,0] with the len(A)=4 zeros.\n2. Update the last element in D. A[3]=5. Using Skill <find max>, max(5,0)=5. D[3]=5 and D=[0,0,0,5].\n3.\nUpdate the second last element in D. A[3] = 5, A[2] = 1.\nUsing Skill <find max>, max(5,1,0) =\n5. D[2] = 5 and D=[0,0,5,5].\n4. Starting from the third last (len(A)-2=2) element in D, update each element in D based on A=[3,2,1,5]\n(A[0]=3,A[1]=2,A[2]=1,A[3]=5):\ni.\nD[3]=5, D[2]=5, A[1]=2.\nUsing Skill <add>, A[1]+D[3]=2+5=7.\nUsing Skill <find max>,\nmax(D[2],A[1]+D[3],0)=max(5,7,0)=7. D[1]=7 and D=[0,7,5,5].\nii.\nD[2]=5, D[1]=7, A[0]=3.\nUsing Skill <add>, A[0]+D[2]=3+5=8.\nUsing Skill <find max>,\nmax(D[1],A[0]+D[2],0)=max(7,8,0)=8. D[0]=8 and D=[8,7,5,5].\n5. D=[8,7,5,5]. The highest sum is D[0]=8.\nFigure 16: An examplar of skill composition in Skills-in-Context prompting for the dynamic programming task to find\nthe highest sum of the subsequence.\n28\nA PREPRINT\nSkills for GSM8K\nSkill <extract digits>: Extract the digits in a number to a list. For example, extract digits in 123 to D=[1,2,3]. Extract\ndigits in 7654 to D=[7,6,5,4]\nSkill <list length>: Get the number of elements in a list. For example, D=[1,2,3], len(D)=3. A=[1,2,4,5,6], len(A)=5.\nSkill <add two single digit number>: Add two single-digit numbers. For example, 0+0=0 0+1=1 0+2=2 0+3=3 0+4=4\n0+5=5 0+6=6 0+7=7 0+8=8 0+9=9\nSkill <sub two single digit number>: Subtract two single-digit numbers. For example, 0-0=0 0-1=-1 0-2=-2 0-3=-3\n0-4=-4 0-5=-5 0-6=-6 0-7=-7 0-8=-8 0-9=-9\nSkill <sub 10>: Subtract 10 from a given number. 10-10=0 11-10=1 12-10=2 13-10=3 14-10=4 15-10=5 16-10=6\n17-10=7 18-10=8 19-10=9\nSkill <add 10>: Add 10 to a given number. -10+10=0 -9+10=1 -8+10=2 -7+10=3 -6+10=4 -5+10=5 -4+10=6 -3+10=7\n-2+10=8 -1+10=9\nSkill <compare 0>: Compare a number with 0. 10>0 9>0 8>0 7>0 6>0 5>0 4>0 3>0 2>0 1>0 0=0 -1>0 -2>0\n-3>0 -4>0 -5>0 -6>0 -7>0 -8>0 -9>0\nSkill <compare 10>: Compare a number with 10. 0<10 1<10 2<10 3<10 4<10 5<10 6<10 7<10 8<10 9<10 10=10\n11>10 12>10 13>10 14>10 15>10 16>10 17>10 18>10 19>10\nSkill <mul two single digit number>: Multiply two single-digit numbers. For example, 4*1=4 4*2=8 4*3=12 4*4=16\n4*5=20 4*6=24 4*7=28 4*8=32 4*9=36\nSkill <add multiple numbers>: Add multiple numbers such as m+n+p:\n1. Add the first two numbers m+n and get the result r1=m+n.\n2. Add the third number p to r1 and get the result r2=r1+p.\nSkill <add>: Use the skills to add two numbers. For example, calculate 86+964 [The steps to perform the add]\nSkill <mul>: Use the skills to multiply two numbers.\nFor example, calculate 86*964 [The steps to perform the\nmultiplication]\nSkill <sub>: Use the skills to subtract a number from another number. For example, calculate 964-86 [The steps to\nperform the subtractraction]\nSkill <age>: Describe the age of a person. If a person is P years old, Q years ago, the person was P-Q years old. If a\nperson is P years old, in Q years, the person will be P+Q years old. If person A is P years old, person B is Q years old, and\nperson A is R years older than person B, then P=Q+R. If person A is P years old, person B is Q years old, and person A is\nR years younger than person B, then P=Q-R.\nSkill <solve equation>: Solve an equation. When subtracting or adding the same number from both sides of the equation,\nthe equation is still true. When moving a number from one side of the equation to the other side, the sign of the number\nchanges. When moving a multiplication from one side of the equation to the other side, the sign of the multiplication\nchanges. When moving a division from one side of the equation to the other side, the sign of the division changes.\nFor example, if the equation is 3x+5=7, move +5 to the other side, then 3x=7-5. Using the Skill <sub>, 3x=7-5=2. Move\n3 to the other side, then x=2/3.\nFigure 17: The skills in Skills-in-Context prompt for GSM8K.\n29\nA PREPRINT\nAn Example of Skill Composition for GSM8K\nExample: Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years\nold, how old is Kody?\nAnswer:\n1. Mohamed is currently twice 30 years old. Using the Skill <mul>, Mohamed is currently 30*2 = 60 years\nold.\n2.\nUsing Skill <age>, four years ago, Mohamed was 4 years younger than now.\nUsing the Skill\n<sub>, Mohamed was 60-4 = 56 years old.\n3. Four years ago, Kody was only half as old as Mohamed. Using the Skill <div>, Kody was 56/2 =\n28 years old.\n4.\nUsing Skill <age>, currently, Kody is 4 years older than four years ago.\nUsing the Skill <add>,\nKody is currently 28+4 = 32 years old.\n5. The answer is 32.\nFigure 18: An examplar of skill composition in Skills-in-Context prompting for GSM8K math problems.\n30\nA PREPRINT\nSkills for MATH\nYou have the knowledge of many skills, the following are some examples:\nSkill <Add>: Add two numbers. For example, 128+987=1115.\nSkill <Sub>: Subtract a number from another number. For example, 128-67=61.\nSkill <Mul>: Multiply two numbers. For example, 128*76=9728.\nSkill <Div>: Divide a number from another number. For example 12/3=4.\nSkill <Mod>: Modulus or modulo, it finds the remainder of a division operation. For example, 10 mod 3 = 1, because 10 divided by 3 leaves a\nremainder of 1.\nSkill <Exp>: An exponent refers to the number of times a number is multiplied by itself. [More Details]\nSkill <Base Conversion>: Base conversion is a way to change numbers from one base to another. [More Details]\nSkill <Radicals>: A radical represents the root of a number. The square root (represented by sqrt) is the most common radical. [More Details]\nSkill <Factoring>: In the context of integers, factorization involves expressing a number as the product of prime numbers. [More Details]\nSkill <Solve Equation>: Solve an equation. [More Details]\nSkill <Quadratic Formula>: The quadratic formula is used to solve quadratic equations. [More Details]\nSkill <Complex Number>: The quadratic formula is used to solve quadratic equations. [More Details]\nSkill <Piecewise Function: Continuous>: A piecewise function is continuous if it is continuous at every point in its domain. [More Details]\nSkill <Factorial>: Factorial is a function that multiplies a given number by every number below it until 1. [More Details]\nSkill <Probability>: Probability is the measure of the likelihood that an event will occur. [More Details]\nSkill <Conditional Probability>: The probability of an event occurring given that another event has already occurred. [More Details]\nSkill <Probability Addition Rule>: The Addition Rule in probability is used to calculate the probability of either of two events happening. [More\nDetails]\nSkill <Probability Multiplication Rule>: A way to determine the probability of two events occurring at the same time (conjointly). [More Details]\nSkill <Counting Principle>: If there are m ways to do one thing, and n ways to do another, then there are m*n ways of doing both. [More Details]\nSkill <Permutations>: Permutations refer to the arrangement of items in a specific order. [More Details]\nSkill <Combination>: Combinations refer to the selection of items without regard to the order. [More Details]\nSkill <Perimeter>: The perimeter of a shape is the distance around its boundary. [More Details]\nSkill <Area>: The area of a shape is the amount of space that it covers. [More Details]\nSkill <Volume>: Volume is the measure of the amount of space that a three-dimensional object occupies. [More Details]\nSkill <Prime Numbers>: A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. [More Details]\nSkill <Composite Numbers>: Composite numbers are positive integers that have at least one positive divisor other than one or the number itself.\n[More Details]\nSkill <GCD>:The Greatest Common Divisor (GCD), also known as the Greatest Common Factor (GCF), [More Details]\nSkill <LCM>:The Least Common Multiple (LCM) of two integers is the smallest positive integer that is divisible by both numbers without\nleaving a remainder. [More Details]\nSkill <Congruences>: Two integers a and b are said to be congruent modulo n if they have the same remainder when divided by n. [More Details]\nFigure 19: The skills in Skills-in-Context prompt for MATH.\n31\nA PREPRINT\nAn Example of Skill Composition for MATH\nExample: Shown below are rows 1, 2, and 3 of Pascal\u2019s triangle.[Pascal\u2019s triangle]. Let (ai), (bi), (ci) be\nthe sequence, from left to right, of elements in the 2005th, 2006th, and 2007th rows, respectively, with the\nleftmost element occurring at i = 0. Compute [P2006\ni=0\nbi\nci \u2212 P2005\ni=0\nai\nbi .]\nAnswer:\n1. Using the Skill <Pascal\u2019s Triangle>, the number in the n-th row and k-th column of the Pascal\u2019s triangle is\nC(n,k).\n2.\nSuppose (ai), (bi), (ci) represent the entries in rows n - 1, n, n + 1 of Pascal\u2019s triangle.\nai = C(n \u2212 1, i), bi = C(n, i), ci = C(n + 1, i)\n3. Using the Skill <Combination>, P2006\ni=0\nbi\nci = P2006\ni=0\n(\n2006\ni )\n(\n2007\ni ) = P2006\ni=0\n2007\u2212i\n2007\n= 2007 \u2212 2006\u00b72007\n2\u00b72007\n=\n2007 \u2212 (2006 \u2217 2007)/(2 \u2217 2007) = 2007 \u2212 1003 = 1004\n4. Using the Skill <Combination>, P2005\ni=0\nai\nbi = P2005\ni=0\n(\n2005\ni )\n(\n2006\ni ) = P2005\ni=0\n2006\u2212i\n2006\n= 2006 \u2212 2005\u00b72006\n2\u00b72006\n=\n2006 \u2212 (2005 \u2217 2006)/(2 \u2217 2006) = 2006 \u2212 1002.5 = 1003.5\n5. Using the Skill <Sub>, [P2006\ni=0\nbi\nci \u2212 P2005\ni=0\nai\nbi ] = 1004 \u2212 1003.5 = 0.5\n6. The answer is 0.5.\nFigure 20: An examplar of skill composition in Skills-in-Context prompting for MATH problems.\n32\nA PREPRINT\nC\nAppendix: Generation Examples\nWe further share some example generation from ChatGPT with our Skills-in-Context prompts on all the tasks in\nFigure 21,22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 4.\nLast Letter Concatenation\nQuestion: Take the last letters of the words in \u2019than, appropriate, sees, daisy, disturbed, espn, stable, double,\nluther, shut\u2019 and concatenate them.\nModel Output:\n1. Using the Skill <words to list>, put the asked words, \u2019than, appropriate, sees, daisy, disturbed, espn,\nstable, double, luther, shut\u2019, to a list. D=[\u2019than\u2019, \u2019appropriate\u2019, \u2019sees\u2019, \u2019daisy\u2019, \u2019disturbed\u2019, \u2019espn\u2019, \u2019stable\u2019,\n\u2019double\u2019, \u2019luther\u2019, \u2019shut\u2019]\n2.\nGet the last letter of each word in the list D=[\u2019than\u2019, \u2019appropriate\u2019, \u2019sees\u2019, \u2019daisy\u2019, \u2019disturbed\u2019,\n\u2019espn\u2019, \u2019stable\u2019, \u2019double\u2019, \u2019luther\u2019, \u2019shut\u2019] to a new list R=[]:\ni. Using the Skill <last letter>, the last letter of D[0]=\u2019than\u2019 is \u2019n\u2019. R=[n]\nii. Using the Skill <last letter>, the last letter of D[1]=\u2019appropriate\u2019 is \u2019e\u2019. R=[n, e]\niii. Using the Skill <last letter>, the last letter of D[2]=\u2019sees\u2019 is \u2019s\u2019. R=[n, e, s]\niv. Using the Skill <last letter>, the last letter of D[3]=\u2019daisy\u2019 is \u2019y\u2019. R=[n, e, s, y]\nv. Using the Skill <last letter>, the last letter of D[4]=\u2019disturbed\u2019 is \u2019d\u2019. R=[n, e, s, y, d]\nvi. Using the Skill <last letter>, the last letter of D[5]=\u2019espn\u2019 is \u2019n\u2019. R=[n, e, s, y, d, n]\nvii. Using the Skill <last letter>, the last letter of D[6]=\u2019stable\u2019 is \u2019e\u2019. R=[n, e, s, y, d, n, e]\nviii. Using the Skill <last letter>, the last letter of D[7]=\u2019double\u2019 is \u2019e\u2019. R=[n, e, s, y, d, n, e, e]\nix. Using the Skill <last letter>, the last letter of D[8]=\u2019luther\u2019 is \u2019r\u2019. R=[n, e, s, y, d, n, e, e, r]\nx. Using the Skill <last letter>, the last letter of D[9]=\u2019shut\u2019 is \u2019t\u2019. R=[n, e, s, y, d, n, e, e, r, t]\n3. R=[n, e, s, y, d, n, e, e, r, t]. The answer is \u2019n e s y d n e e r t\u2019.\nFigure 21: An example of the generated answer on last-letter-concatenation task using ChatGPT with our Skills-in-\nContext prompting.\n33\nA PREPRINT\nAddition\nQuestion: Calculate 165789+876534\nModel Output:\n1. Using Skill <extract digits> and Skill <list length>, extract the digits in 165789 to DM=[1,6,5,7,8,9]. len(DM)=6.\nExtract the digits in 876534 to DN=[8,7,6,5,3,4]. len(DN)=6.\n2. len(DM)=6=len(DN)=6. Align the two lists: DM=[1,6,5,7,8,9], DN=[8,7,6,5,3,4].\n3.\nUsing Skill <add two single digit number>, add every number in DM=[1,6,5,7,8,9] to the corresponding num-\nber in DN =[8,7,6,5,3,4] to form a list, DS=[1+8,6+7,5+6,7+5,8+3,9+4]=[9,13,11,12,11,13].\n4. Append a 0 to the beginning of DS: DS=[0,9,13,11,12,11,13]. Construct a list R=[0,0,0,0,0,0,0] with len(DS)=7 zeros.\n5.\nDS=[0,9,13,11,12,11,13] (DS[0]=0, DS[1]=9, DS[2]=13, DS[3]=11, DS[4]=12, DS[5]=11, DS[6]=13), starting\nfrom DS[6] to DS[1] (DS[6]=13, DS[5]=11, DS[4]=12, DS[3]=11, DS[2]=13, DS[1]=9):\ni. DS[6]=13, R[6]=0, R[6]=R[6]+DS[6]=0+13=13. Based on Skill <compare 10>, 13>10, R[6]=13>10, so R[5]=1\nand R[6]=13-10=3 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[0,0,0,0,0,1,3].\nii. DS[5]=11, R[5]=1, R[5]=R[5]+DS[5]=1+11=12. Based on Skill <compare 10>, 12>10, R[5]=12>10, so R[4]=1\nand R[5]=12-10=2 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[0,0,0,0,1,2,3].\niii.\nDS[4]=12, R[4]=1, R[4]=R[4]+DS[4]=1+12=13.\nBased on Skill <compare 10>, 13>10, R[4]=13>10, so\nR[3]=1 and R[4]=13-10=3 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[0,0,0,1,3,2,3].\niv. DS[3]=11, R[3]=1, R[3]=R[3]+DS[3]=1+11=12. Based on Skill <compare 10>, 12>10, R[3]=12>10, so R[2]=1\nand R[3]=12-10=2 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[0,0,1,2,3,2,3].\nv. DS[2]=13, R[2]=1, R[2]=R[2]+DS[2]=1+13=14. Based on Skill <compare 10>, 14>10, R[2]=14>10, so R[1]=1\nand R[2]=14-10=4 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[0,1,4,2,3,2,3].\nvi. DS[1]=9, R[1]=1, R[1]=R[1]+DS[1]=1+9=10. Based on Skill <compare 10>, 10=10, R[1]=10=10, so R[0]=1\nand R[1]=10-10=0 by Skill <sub 10>. R=[R[0],R[1],R[2],R[3],R[4],R[5],R[6]]=[1,0,4,2,3,2,3].\n6. R=[1,0,4,2,3,2,3]. The answer is 1042323.\nFigure 22: An example of the generated answer on the addition task using ChatGPT with Skills-in-Context prompting.\n34\nA PREPRINT\nMultiplication\nQuestion: Calculate 1467*7651\nModel Output:\n1.\nUsing Skill <extract digits> and Skill <list length>, extract the digits in 1467 to DM=[1,4,6,7].\nlen(DM)=4. Extract the digits in 7651 to DN=[7,6,5,1]. len(DN)=4.\n2. Add 0,1,len(DM)-1=3 zeros to the end of every number in DM=[1,4,6,7] according to the position of the\nnumber in DM: DMO=[1*1000,4*100,6*10,7*1]=[1000,400,60,7].\n3. Add 0,1,len(DN)-1=3 zeros to the end of every number in DN=[7,6,5,1] according to the position of the\nnumber in DN: DNO=[7*1000,6*100,5*10,1*1]=[7000,600,50,1].\n4.\nUsing Skill <mul two single digit number>,\nmultiple every number in DMO=[1000,400,60,7]\nwith\nevery\nnumber\nin\nDNO=[7000,600,50,1]\nand\nget\nR=[1000*7000,1000*600,1000*50,1000*1,\n400*7000,400*600,400*50,400*1,60*7000,60*600,60*50,60*1,7*7000,7*600,7*50,7*1]=\n[7000000,600000,50000,1000,2800000,240000,20000,400,420000,36000,3000,60,49000,4200,350,7].\n5. Using Skill <add multiple numbers>, add all the numbers in\nR=[7000000,600000,50000,1000,2800000,240000,20000,400,420000,36000,3000,60,49000,4200,350,7],\n7000000+600000+50000+1000+2800000+240000+20000+400+420000+36000+3000+60+49000+4200\n+350+7:\ni. Add the first two numbers: r1=7000000+600000=7600000.\nii. Add the third number 50000 to r1=7600000: r2=7600000+50000=7650000.\niii. Add the fourth number 1000 to r2=7650000: r3=7650000+1000=7651000.\niv. Add the fifth number 2800000 to r3=7651000: r4=7651000+2800000=10451000.\nv. Add the sixth number 240000 to r4=10451000: r5=10451000+240000=10691000.\nvi. Add the seventh number 20000 to r5=10691000: r6=10691000+20000=10711000.\nvii. Add the eighth number 400 to r6=10711000: r7=10711000+400=10711400.\nviii. Add the ninth number 420000 to r7=10711400: r8=10711400+420000=11131400.\nix. Add the tenth number 36000 to r8=11131400: r9=11131400+36000=11167400.\nx. Add the eleventh number 3000 to r9=11167400: r10=11167400+3000=11170400.\nxi. Add the twelfth number 60 to r10=11170400: r11=11170400+60=11170460.\nxii. Add the thirteenth number 49000 to r11=11170460: r12=11170460+49000=11219460.\nxiii. Add the fourteenth number 4200 to r12=11219460: r13=11219460+4200=11223660.\nxiv. Add the fifteenth number 350 to r13=11223660: r14=11223660+350=11224010.\nxv. Add the sixteenth number 7 to r14=11224010: r15=11224010+7=11224017.\n6. So the answer is 11224017\nFigure 23: An example of the generated answer on the multiplication task using ChatGPT with Skills-in-Context\nprompting.\n35\nA PREPRINT\nCommaQA-E\nQuestion:\nmovie: Skob ; directed by: Haldron. movie: Riften ; directed by: Conanopeia. movie: Tetroxidine ; directed\nby: Conanopeia. movie: Sheepcrest ; directed by: Flumph. movie: Sheepcrest ; award: Mariskenna. movie:\nSkob ; award: Jubeus. movie: Tetroxidine ; award: Handt. movie: Riften ; awarded: Handt. movie:\nTetroxidine ; written by: Conanopeia. movie: Riften ; written by: Conanopeia. movie: Riften ; writer:\nFlumph. movie: Tetroxidine ; written by: Haldron. movie: Skob ; written by: Haldron. movie: Sheepcrest\n; writer: Flumph. movie: Skob ; year: 1962. movie: Tetroxidine ; release year: 1997. movie: Riften ;\nrelease year: 1972. movie: Sheepcrest ; release year: 1997. Flumph was an actor in the movie Riften.\nFlumph acted in the movie Sheepcrest. Conanopeia was an actor in the movie Riften. Conanopeia acted in\nthe movie Tetroxidine. Conanopeia acted in the movie Sheepcrest. Haldron acted in the movie Tetroxidine.\nHaldron acted in the movie Skob. Haldron was born in the year 1939. Flumph was born in the year 1938.\nConanopeia was born in 1938. Haldron is from the country of Poquet. Conanopeia grew up in the nation of\nPoquet. Flumph grew up in the nation of Stridery. Diaqum was awarded to Flumph. Lidus was awarded to\nConanopeia. Haldron won the Lidus award. Conanopeia was one of the producers of the movie Tetroxidine.\nFlumph was one of the producers of the movie Sheepcrest. Flumph produced the movie Skob with others.\nHaldron was one of the producers of the movie Sheepcrest. Haldron was one of the producers of the movie\nRiften. Haldron produced the movie Skob with others. Conanopeia was one of the producers of the movie\nRiften.\nQ: What movies have the people from Poquet written?\nModel Output:\n1. Using Skill <decompose qa>, decompose the question \u201dWhat movies have the people from Poquet writ-\nten?\u201d into the following sub-questions:\nQ1: Who is from the country Poquet?\nQ2: Which movies did [A1] write?\n2. Using Skill <answer simple question>, answer Q1: Who is from the country Poquet?\nConanopeia grew up in the nation of Poquet. Haldron is from the country of Poquet. A1=[\u201dConanopeia\u201d,\n\u201dHaldron\u201d]\n3. A1=[\u201dConanopeia\u201d, \u201dHaldron\u201d]. Using Skill <answer simple question>, answer Q2: Which movies did\n[A1] write?\ni. A1[0]=\u201dConanopeia\u201d, answer Q2.1: Which movies did Conanopeia write?\nA2.1: movie: Tetroxidine ; written by: Conanopeia. movie: Riften ; written by: Conanopeia.\nA2.1=[\u201dTetroxidine\u201d, \u201dRiften\u201d]\nii. A1[1]=\u201dHaldron\u201d, answer Q2.2: Which movies did Haldron write?\nA2.2: movie: Skob ; written by: Haldron. A2.2=[\u201dSkob\u201d]\niii. So A2=A2.1+A2.2=[\u201dTetroxidine\u201d, \u201dRiften\u201d, \u201dSkob\u201d]\n4. The answer is [\u201dTetroxidine\u201d, \u201dRiften\u201d, \u201dSkob\u201d]\nFigure 24: An example of the generated answer on the CommaQA-E task using ChatGPT with our Skills-in-Context\nprompting.\n36\nA PREPRINT\nDP\nQuestion: Given a sequence of integers, find a subsequence with the highest sum, such that no two numbers\nin the subsequence are adjacent in the original sequence.\nA=[1, -5, 3, -4, -1, -1, 2, 2].\nModel Output:\n1. A=[1, -5, 3, -4, -1, -1, 2, 2] (A[0]=1,A[1]=-5,A[2]=3,A[3]=-4,A[4]=-1,A[5]=-1,A[6]=2,A[7]=2). Using\nSkill <list length>, len(A)=8. Construct a list D=[0,0,0,0,0,0,0,0] with the len(A)=8 zeros.\n2.\nUpdate the last element in D. A[7]=2.\nUsing Skill <find max>, max(2,0)=2.\nD[7]=2 and\nD=[0,0,0,0,0,0,0,2].\n3.\nUpdate the second last element in D. A[7]=2, A[6]=2.\nUsing Skill <find max>, max(2,2,0)=2.\nD[6]=2 and D=[0,0,0,0,0,0,2,2].\n4.\nStarting from the third last (len(A)-2=6) element in D, update each element in D based on A=[1,\n-5, 3, -4, -1, -1, 2, 2] (A[0]=1,A[1]=-5,A[2]=3,A[3]=-4, A[4]=-1,A[5]=-1,A[6]=2,A[7]=2):\ni.\nD[7]=2, D[6]=2, A[5]=-1.\nUsing Skill <add>, A[5]+D[7]=-1+2=1.\nUsing Skill <find max>,\nmax(D[6],A[5]+D[7],0)=max(2,1,0)=2. D[5]=2 and D=[0,0,0,0,0,2,2,2].\nii.\nD[6]=2, D[5]=2, A[4]=-1.\nUsing Skill <add>, A[4]+D[6]=-1+2=1.\nUsing Skill <find max>,\nmax(D[5],A[4]+D[6],0)=max(2,1,0)=2. D[4]=2 and D=[0,0,0,0,2,2,2,2].\niii. D[5]=2, D[4]=2, A[3]=-4. Using Skill <add>, A[3]+D[5]=-4+2=-2. Using Skill <find max>,\nmax(D[4],A[3]+D[5],0)=max(2,-2,0)=2. D[3]=2 and D=[0,0,0,2,2,2,2,2].\niv.\nD[4]=2, D[3]=2, A[2]=3.\nUsing Skill <add>, A[2]+D[4]=3+2=5.\nUsing Skill <find max>,\nmax(D[3],A[2]+D[4],0)=max(2,5,0)=5. D[2]=5 and D=[0,0,5,2,2,2,2,2].\nv.\nD[3]=2, D[2]=5, A[1]=-5.\nUsing Skill <add>, A[1]+D[3]=-5+2=-3.\nUsing Skill <find max>,\nmax(D[2],A[1]+D[3],0)=max(5,-3,0)=5. D[1]=5 and D=[0,5,5,2,2,2,2,2].\nvi.\nD[2]=5, D[1]=5, A[0]=1.\nUsing Skill <add>, A[0]+D[2]=1+5=6.\nUsing Skill <find max>,\nmax(D[1],A[0]+D[2],0)=max(5,6,0)=6. D[0]=6 and D=[6,5,5,2,2,2,2,2].\n5. D=[6,5,5,2,2,2,2,2]. The highest sum is D[0]=6.\nFigure 25: An example of the generated answer on the dynamic programming task using ChatGPT with our Skills-in-\nContext prompting.\n37\nA PREPRINT\nGSM8K\nQuestion: The girls are trying to raise money for a carnival. Kim raises $320 more than Alexandra, who raises\n$430, and Maryam raises $400 more than Sarah, who raises $300. How much money, in dollars, did they all\nraise in total?\nModel Output:\n1. Alexandra raises $430.\n2. Kim raises $320 more than Alexandra. Using the Skill <add>, Kim raises $430+$320=$750.\n3. Sarah raises $300.\n4. Maryam raises $400 more than Sarah. Using the Skill <add>, Maryam raises $300+$400=$700.\n5. Using the Skill <add>, they all raise $430+$750+$300+$700=$2180 in total.\n6. The answer is 2180.\nFigure 26: An example of the generated answer on the GSM8K task using ChatGPT with Skills-in-Context prompting.\n38\nA PREPRINT\nGSM8K\nQuestion: It\u2019s strawberry-picking time on Grandma Concetta\u2019s farm. Tony can pick 6 quarts of strawberries\nper hour, while Bobby picks one less quart of strawberries per hour than Tony. Kathy can pick twice as many\nstrawberries per hour as Bobby, and Ricky picks two fewer quarts of strawberries per hour than does Kathy.\nIn total, how many quarts of strawberries can Tony, Bobby, Ricky, and Kathy pick per hour on Grandma\nConcetta\u2019s farm?\nModel Output:\n1. Tony can pick 6 quarts of strawberries per hour.\n2.\nBobby picks one less quart of strawberries per hour than Tony.\nUsing the Skill <sub>, Bobby\npicks 6-1=5 quarts of strawberries per hour.\n3.\nKathy can pick twice as many strawberries per hour as Bobby.\nUsing the Skill <mul>, Kathy\npicks 5*2=10 quarts of strawberries per hour.\n4.\nRicky picks two fewer quarts of strawberries per hour than does Kathy.\nUsing the Skill <sub>,\nRicky picks 10-2=8 quarts of strawberries per hour.\n5.\nIn total, Tony, Bobby, Ricky, and Kathy can pick 6+5+10+8 quarts of strawberries per hour.\nUsing\nthe Skill <add multiple numbers>:\ni. Add the first two numbers using Skill <add>: r1=6+5=11.\nii. Add the third number 10 to r1=11 using Skill <add>: r2=11+10=21.\niii. Add the fourth number 8 to r2=21 using Skill <add>: r3=21+8=29.\n6. So the answer is 29.\nFigure 27: An example of the generated answer on the GSM8K task with our Skills-in-Context prompting, where\n<add multiple numbers> is included as a basic skill in the SKiC prompting context (see Table 17) but is not demon-\nstrated in any given examplar to show how to use it. LLMs automatically figure out how to use such skills in an\ninnovative composition to solve an unseen complex problem.\n39\nA PREPRINT\nGSM8K\nQuestion: A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or\nelectronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while\nthe electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize\nprofit at the end of this month by making a choice, how much profit would this be?\nModel Output:\n1. If the merchant buys jewelry worth $5,000 and the jewelry market goes up 2.5%, using the Skill <mul>,\nthe value of the jewelry will increase by $5,000*2.5%= $125. Using Skill <add>, the value of the jewelry\nwill be $5,000+$125=$5125.\n2.\nIf the merchant buys electronic gadgets worth $8,000 and the electronic gadgets market goes up\n1.2%, using the Skill <mul>, the value of the electronic gadgets will increase by $8,000*1.2%= $96. Using\nSkill <add>, the value of the electronic gadgets will be $8,000+$96=$8096.\n3.\nThe merchant wants to maximize profit.\nUsing the Skill <sub>, the profit from buying jewelry\nwill be $5125-$5000=$125. The profit from buying electronic gadgets will be $8096-$8000=$96.\n4.\nUsing the Skill <compare>, $125>$96, so the merchant should buy jewelry to maximize profit.\nThe profit will be $125.\n5. The answer is 125.\nFigure 28: An example of the generated answer on the GSM8K task with our Skills-in-Context prompting, where the\nskill <compare> are neither included in the SKiC prompting context (see Table 17) nor used in any given examplars.\nLLMs utilize the skills pre-existing in their pre-trained knowledge to solve the problem.\n40\nA PREPRINT\nGSM8K\nQuestion: Mr. Jackson\u2019s fourth-grade class has 27 students. He wants to give each student 2 glue sticks. The\nglue sticks come in packs of 8. How many packs will Mr. Jackson need to buy so every student can have 2\nglue sticks, assuming he can only buy whole packs and he expects to have some extra glue sticks left over?\nModel Output:\n1. Mr. Jackson\u2019s fourth-grade class has 27 students and he wants to give each student 2 glue sticks. Using the\nSkill <mul>, Mr. Jackson needs 27*2=54 glue sticks.\n2. The glue sticks come in packs of 8. Using the Skill <div>, Mr. Jackson needs 54/8=6.75 packs of\nglue sticks.\n3.\nMr.\nJackson can only buy whole packs.\nUsing the Skill <round>, Mr.\nJackson needs to buy 7\npacks of glue sticks.\n4. The answer is 7.\nFigure 29: An example of the generated answer on the GSM8K task with our Skills-in-Context prompting, where\nthe skill <round> are neither included in the original SKiC prompting context (see Table 17) nor used in any given\nexamplars. LLMs utilize the skills pre-existing in their pre-trained knowledge to solve the problem.\n41\nA PREPRINT\nMATH\nQuestion: What is the smallest possible average of four distinct positive even integers?\nModel Output:\n1. The smallest possible average of four distinct positive even integers is achieved when the four integers are\nthe smallest possible.\n2. The smallest four distinct positive even integers are 2, 4, 6, and 8.\n3. Using the Skill <Average>, the average of these four numbers is (2+4+6+8)/4=20/4=5.\n4. The answer is 5.\nFigure 30: An example of the generated answer on the MATH task with our Skills-in-Context prompting, where the\nskill <Average> are neither included in the original SKiC prompting context (see Table 19) nor used in any given\nexamplars. LLMs(GPT4) utilize the skills pre-existing in their pre-trained knowledge to solve the problem.\n42\nA PREPRINT\nMATH\nQuestion: How many numbers are in the list 2008, 2003, 1998, . . . , 8, 3?\nModel Output:\n1. The list starts at 2008 and ends at 3, and the difference between consecutive terms is 5.\n2.\nUsing the Skill <Arithmetic Sequence>, the n-th term of an arithmetic sequence can be found us-\ning the formula an = a1 + (n \u2212 1)d, where an is the nth term, a1 is the first term, n is the term number, and\nd is the common difference.\n3. Substitute an with 3, a1 with 2008, and d with \u22125 into the formula, 3 = 2008 + (n \u2212 1)(\u22125).\n4. Using the Skill <Solve Equation>, move 2008 to the other side, (n \u2212 1)(\u22125) = 3 \u2212 2008. Using\nthe Skill <Sub>, (n \u2212 1)(\u22125) = \u22122005.\n5.\nUsing the Skill <Solve Equation>, move \u2217(\u22125) to the other side, n \u2212 1 = \u22122005/ \u2212 5.\nUsing\nthe Skill <Div>, n \u2212 1 = 401.\n6.\nUsing the Skill <Solve Equation>, move \u22121 to the other side, n = 401 + 1.\nUsing the Skill\n<Add>, n = 402.\n7. The answer is 402.\nFigure 31: An example of the generated answer on the MATH task with our Skills-in-Context prompting, where the\nskill <Arithmetic Sequence> are neither included in the original SKiC prompting context (see Table 19) nor used in\nany given examplars. LLMs(GPT4) utilize the skills pre-existing in their pre-trained knowledge to solve the problem.\n43\nA PREPRINT\nMATH\nQuestion: Suppose P is the point (5, 3) and Q is the point (\u22123, 6). What is the midpoint of PQ?\nModel Output:\n1.\nUsing the Skill <Midpoint Formula>, the midpoint of two points (x1, y1) and (x2, y2) is given by\n\u0000 x1+x2\n2\n, y1+y2\n2\n\u0001\n.\n2. For points P(5, 3) and Q(\u22123, 6), the midpoint is\n\u0010\n5+(\u22123)\n2\n, 3+6\n2\n\u0011\n.\n3. Using the Skill <Add> and <Div>, the midpoint is\n\u0000 2\n2, 9\n2\n\u0001\n= (1, 4.5).\n4. The answer is (1, 4.5).\nFigure 32: An example of the generated answer on the MATH task with our Skills-in-Context prompting, where the\nskill <Midpoint Formula> are neither included in the original SKiC prompting context (see Table 19) nor used in any\ngiven examplars. LLMs(GPT4) utilize the skills pre-existing in their pre-trained knowledge to solve the problem.\n44\nA PREPRINT\nMATH\nQuestion: If a and b are vectors such that \u2225a\u2225 = 2, \u2225b\u2225 = 7, and a \u00d7 b =\n 3\n2\n6\n!\n, then find the smallest\npossible angle between a and b.\nModel Output:\n1. Using the Skill <Cross Product>, the magnitude of the cross product of two vectors is equal to the product\nof the magnitudes of the vectors and the sine of the angle between them. So, we have \u2225a\u00d7b\u2225 = \u2225a\u2225\u2225b\u2225 sin \u03b8,\nwhere \u03b8 is the angle between a and b.\n2.\nUsing the Skill\n<Vector Magnitude>,\nthe magnitude of the cross\nproduct \u2225a \u00d7 b\u2225 is\n\u221a\n32 + 22 + 62 =\n\u221a\n49 = 7.\n3.\nSubstituting the given values, we get 7 = 2 \u2217 7 \u2217 sin \u03b8.\nUsing the Skill <Div> and <Mul>, we\nget sin \u03b8 = 7/(2 \u2217 7) = 1/2.\n4.\nUsing the Skill <Inverse Trigonometric Functions>, the angle whose sine is 1/2 is 30 degrees or\n150 degrees. But we want the smallest possible angle, so \u03b8 = 30 degrees.\n5. The answer is 30.\nFigure 33: An example of the generated answer on the MATH task with our Skills-in-Context prompting, where the\nskill <Cross Product>, <Vector Magnitude>,<Inverse Trigonometric Functions> are neither included in the original\nSKiC prompting context (see Table 19) nor used in any given examplars. LLMs(GPT4) utilize the skills pre-existing\nin their pre-trained knowledge to solve the problem.\n45\nA PREPRINT\nFigure 34: Examples of the generated answers from Decomp prompting and our SKiC prompting, respectively. The\ncorrect answer should be \u201cMyristorrhoid, Chimpwurst, Geissant, Riften\u201d. Errors (highlighted in red) in early stages in\nDecomp propagate to final incorrect answers while our SKiC avoids such errors (highlighted in green).\n46\n"
  },
  {
    "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
    "link": "https://arxiv.org/pdf/2308.00436.pdf",
    "upvote": "20",
    "text": "SELFCHECK:\nUSING LLMS TO ZERO-SHOT CHECK\nTHEIR OWN STEP-BY-STEP REASONING\nNing Miao1*\nYee Whye Teh1\nTom Rainforth1\nABSTRACT\nThe recent progress in large language models (LLMs), especially the invention of\nchain-of-thought prompting, has made it possible to automatically answer questions\nby stepwise reasoning. However, when faced with more complicated problems that\nrequire non-linear thinking, even the strongest LLMs make mistakes. To address\nthis, we explore whether LLMs are able to recognize errors in their own step-by-\nstep reasoning, without resorting to external resources. To this end, we propose\nSelfCheck, a general-purpose zero-shot verification schema for recognizing such\nerrors. We then use the results of these checks to improve question-answering\nperformance by conducting weighted voting on multiple solutions to the question.\nWe test SelfCheck on three datasets\u2014GSM8K, MathQA, and MATH\u2014and find\nthat it successfully recognizes errors and, in turn, increases final answer accuracies.\n1\nINTRODUCTION\nRecent years have witnessed dramatic changes in the areas of NLP and AI brought on by significant\nadvances in LLMs. From GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), Llama (Tou-\nvron et al., 2023) and Falcon (Almazrouei et al., 2023) to GPT-4 (OpenAI, 2023) and PaLM-2 (Google,\n2023), the increasing model sizes and exploding amount of training data have empowered LLMs to\nachieve human-level performance on a large range of tasks, including summarization, translation,\nand question answering. The invention of Chain-of-Thought prompting (CoT, Wei et al. (2022)) has\nfurther enhanced LLMs\u2019 ability to solve complex problems by generating step-by-step solutions.\nHowever, the performance of even the largest LLMs is still unsatisfactory on more difficult reasoning\nproblems. For example, GPT-4 with CoT prompting only correctly answers 42.5% of problems in the\nMATH dataset (Bubeck et al., 2023; Hendrycks et al., 2021), which is far below human level. Such\nproblems require careful and extensive multi-step reasoning to solve, and LLMs are consequently\nprone to make mistakes: even though their error rate on individual steps may be low, the probability\nof generating at least one erroneous step can still be quite high, undermining the final answer.\nRecent works have tried to overcome this limitation by checking for errors in these step-by-step\nsolutions (Cobbe et al., 2021; Li et al., 2022; Ling et al., 2023). Such checks can then be used to\nprovide confidence scores in answers and select between different possible alternatives. This checking\nhas typically been performed either by using an external verification model (Cobbe et al., 2021; Lyu\net al., 2023; Peng et al., 2023), or through few-shot in-context learning (Brown et al., 2020) of an\nLLM (Weng et al., 2022; Ling et al., 2023).\nUnfortunately, existing methods generally require extra training data and/or domain-specific exem-\nplars, which often makes them inconvenient to use in practice and restricts them to specific domains\nor data formats. The aim of our work is thus to instead provide a general-purpose, zero-shot, approach\nto checking that relies only on the original LLM, without the need for additional external resources.\nTo this end, we introduce SelfCheck, a zero-shot step-by-step checker for self-identifying errors in\nLLM reasoning chains. SelfCheck uses the LLM to individually check the conditional correctness of\neach step in the chain based on the preceding steps, in a manner similar to a human going back to\ncheck their working. The results of these individual checks are then integrated to form an overall\ncorrectness estimation for the whole reasoning chain.\nKey to SelfCheck\u2019s success is a novel mechanism for performing the checking of individual steps. As\nwe will show, the naive approach of directly asking the LLM to check a step is typically ineffective.\nInstead, we introduce a multi-stage approach that breaks the problem down into a series of simpler\n1Department of Statistics, University of Oxford. *Email: <ning.miao@stats.ox.ac.uk>.\n1\narXiv:2308.00436v3  [cs.AI]  5 Oct 2023\ntasks, leverages the generative strengths of the LLM, and decorrelates errors between the original\ngeneration and checking. Specifically, using separate calls to the LLM we first extract the target and\nrelevant context for the step, then regenerate an independent alternative step from these, and finally\ncompare the two. The original step is then deemed to pass the check if it matches the regeneration.\nBesides providing an estimation of correctness for each solution, SelfCheck can also boost final\nanswer accuracies for the original questions by weighted voting. Namely, given multiple solutions to\na question, it uses confidence scores as weights to vote among the answers, which provides a soft\nway to focus on more accurate solutions.\nWe evaluate SelfCheck on three math tasks, namely GSM8K (Cobbe et al., 2021), MathQA (Amini\net al., 2019), and MATH (Hendrycks et al., 2021). For all datasets, we find that using SelfCheck\nachieves a significant increase in final answer accuracies compared with simple majority voting and\nother baselines. We also see that SelfCheck provides an accurate confidence estimation for LLM\u2019s\nsolutions, which decreases the proportion of incorrect solutions by 9%, 22.8%, and 16.2% on the three\ndatasets respectively when filtering out solutions with low confidence scores. We further perform a\nnumber of ablations to justify some of our key design choices in the SelfCheck approach.\nTo summarize, we introduce SelfCheck as a novel and effective zero-shot schema for self-checking\nstep-by-step reasoning in LLMs. Unlike previous methods, SelfCheck does not need any finetuning or\nexample crafting, so can be directly applied to reasoning tasks in different domains. Our experiments\nconfirm that it can, in turn, be used to improve final predictive performance of LLMs. Our code is\navailable at https://github.com/NingMiao/SelfCheck.\n2\nRELATED WORK\nHow to automatically check the correctness of a sequence of reasoning steps is a long-standing\nquestion. We now discuss how previous methods have tried to tackle this in an LLM context. We note\nthat none of these works are able to work in the zero-shot setting covered by SelfCheck, requiring\neither problem-specific examples, an external model, and/or finetuning.\nFew-shot verification\nThough our focus will be on zero-shot checking, for some problems one\nmay have hand-crafted exemplars available that are specifically designed to that particular question-\nanswering task. Previous methods have been designed to perform checking of LLMs\u2019 generated\nsolutions in this few-shot checking scenario.\nFor example, the Self-Verification (SV) approach of Weng et al. (2022) verifies the whole solution by\nbackward prediction. That is, it uses the conclusion from CoT reasoning to predict a masked condition\nin the question. However, it only supports single-step checking and is based on the assumption that\nevery piece of information in the question can be recovered using a correct solution of it, which is\noften not the case. Consequently, it is only applicable to simpler tasks, such as GSM8K.\nThe Deductive Verification (DV) approach of Ling et al. (2023) instead looks to verify independent\nsub-tasks, as per SelfCheck. However, its verifier only supports checking reasoning chains in a special\nformat called Natural Programs. As a result, it can only work with a specific specialised generator,\nwithout serving as a general verifier for multi-step reasoning.\nVerification with external resources\nIn some cases, there might be external resources available to\nverify the logical correctness or faithfulness of LLM outputs. Lyu et al. (2023) translate a question\ninto a symbolic reasoning chain using an LLM and solve the problem by a symbolic logic solver.\nPeng et al. (2023) introduced an external database to check for incorrect knowledge in LLM outputs.\nThese methods are limited by the availability of external resources and are typically restricted to\nchecking for certain types of errors.\nTraining/finetuning a verifier\nA few other methods train or finetune a separate verifier model\nto check reasoning chains. Cobbe et al. (2021) finetuned a GPT-3 model on GSM8K to predict the\ncorrectness of a solution as a whole. Li et al. (2022) trained a binary deberta-v3-large (He et al., 2020)\nclassifier on each domain to predict step correctness. More recently, Lightman et al. (2023) built a\nlarge dataset, which contains step-wise correctness labels from human labelers, and finetuned a GPT-4\nmodel on it. Unlike SelfCheck, all of these methods require extra data and external computational\nresources, restricting their applicability and ease of use.\n2\nFigure 1: Example of using SelfCheck, focusing on the checking of a particular step (Step 5). To\ncheck the correctness of the step, SelfCheck goes through 4 stages. First, in the target extraction stage,\nit figures out that the main purpose of Step 5 is to complete the square. In the information collection\nstage, it then establishes that Step 5 only directly relies on Step 4. Next, the step regeneration\nstage instructs the LLM to complete the square independently, only using Step 4 as context. The\nregeneration result shows that the center and radius of the circle are (3, 0) and 3, which is different\nfrom what is implied by the original Step 5. Consequently, the result comparison stage concludes that\nStep 5 is likely to be wrong. After checking all the steps, SelfCheck integrates the results to form an\noverall confidence score, w. See Appendix A for a complete version of the example.\n3\nSELFCHECK: USING LLMS TO CHECK THEIR OWN REASONING\nRather than relying on external resources or problem-specific data like the aforementioned approaches,\nit would be highly beneficial if we could develop self-contained checking schemes that require only\nthe original LLM itself. In other words, we would like to use the LLM to identify errors in its own\nstep-by-step reasoning, analogously to how a human might go back to check their working.\nUnfortunately, directly asking the LLM to check its own reasoning is largely ineffective: it almost\ninvariably declares that the original answer is correct, with Ling et al. (2023) finding answers checked\nin this way are deemed correct more than 90% of the time regardless of whether they actually are. As\nwe will show in Section 5, individually prompting the LLM to check each step in the CoT reasoning\nfares slightly better, but is still only able to offer marginal gains compared to not checking at all.\nA more nuanced method to perform this checking is thus required. To this end, we introduce\nSelfCheck, a general-purpose, zero-shot, checking schema for self-identifying errors in LLM CoT\nreasoning. Given a question, q, and its step-by-step solution, s, produced by some generator (which\nwill generally be an LLM with appropriate CoT prompting), SelfCheck considers each step of s in\nturn and tries to establish its individual correctness based on the preceding steps. This checking is\ndone by leveraging an LLM (which can either be the same LLM used to generate s or a separate\none), but rather than directly asking the LLM to perform the check, we instead introduce a novel step\nchecking method (see Section 3.1) that exploits their generative modeling strengths. The results of\nthe checks on individual steps are then combined into a single confidence score, w \u2208 [0, 1], for the\nwhole solution. These confidence scores, in turn, allow us to improve predictive performance, by\nusing them to perform weighted voting on multiple solutions to the same question.\n3\n3.1\nSTEP CHECKING\nTo check individual steps of the reasoning process, the first thing we should note is that the correctness\nof each step is highly dependent on its context, namely the question and previous steps in the solution.\nFor example, we usually need to refer to previous steps for the definition of variables and the meaning\nof specific numbers. If each step is conditionally correct based on the provided context and the last\nstep provides an answer in the required format, then the overall reasoning will itself be correct. The\ntarget of the step checking is thus simply to check the conditional correctness of each step based on\nthe provided context. That is, we only care about catching errors at the current step, and can assume\nall information from its context to be correct.\nA simple idea to try and achieve this would be to feed the current step as well as all its context to an\nLLM and directly ask it to \u2018check the correctness of the step\u2019. However, in practice, we find that this\ntask is too difficult for the LLM to do effectively, even with careful prompting that exemplifies how\nto do the checking in detail (see Section 5). This difficulty comes first from the fact that there are\nmultiple aspects to the checking problem that the checker must deal with simultaneously: it needs to\nunderstand the key content in the step and then collect all related information from the context, before\nactually checking for its correctness. Second, \u2018checking\u2019 is a less common task in the training corpus\nof most LLMs, such that it is a problem that does not necessarily play to their strengths. Finally, there\nare likely to be strong correlations between the errors such a checker will make with the errors made\nin the original generation, undermining its usefulness.\nTo address these difficulties, SelfCheck instead decomposes the checking task for each step into four\nstages: target extraction, information collection, step regeneration, and result comparison. The LLM\nis used to execute each stage successively, with the outcome of the result comparison providing the\ncorrectness prediction.\nThe idea behind this decomposition is to make the LLM focus on an easier task at each stage and\nensure the individual tasks carried out are more closely aligned to the LLM\u2019s strengths. Moreover, by\nfocusing on regenerating and then comparing, we hope to reduce the correlations between the errors\nof the checking and the original generation.\nAt a high level, the stages work by first prompting the LLM to figure out the target of the current\nstep and what information it uses to achieve the target; we find that the LLM is usually able to\nperform these tasks extremely accurately. Then we ask the LLM to re-achieve the target using only\nthe collected information, providing an alternative to the original step that maintains the same purpose\nin the overall reasoning process. Here the clear description of the target and the simplified context we\nprovide make the regeneration stage less challenging. As a result, we hope its output will be more\nreliable and thus serve as a useful reference. Even if this is not the case, it will still hopefully provide\na viable alternative, with a distinct generation, that can be used for comparison. The last stage then\nuses the LLM to compare the original step with the regenerated output. If their main conclusions\nmatch/mismatch, this provides evidence that the original step was correct/incorrect.\nA worked example of this step-checking process is provided in Figure 1. In the following, we describe\neach of the subtasks in detail and provide our specific instructions to the LLM. We note here that\nthe different LLM queries are made independently, rather than keeping the queries and answers\nfrom previous stages in context. Thus, for example, when the LLM is called to carry out the step\nregeneration, it does not have access to the original generation. The same prompts are used across\nLLMs and datasets, thereby providing a general-purpose approach.\nTarget extraction\nTo check a step (for example, Step 5 in Figure 1), we first need to figure out\nwhat the step is trying to achieve. Without a specific target, the regeneration stage would proceed in\na random direction, making it impossible to serve as a reference to the original step. We thus use\nthe LLM itself to extract the target of a step using the question and all previous steps (Steps 0-4 in\nFigure 1) with the following prompt (we omit some line breaks due to space limitations):\nThe following is a part of the solution to the problem [Question]: [Step 0,..., Step i]. What specific action\ndoes the step [Step i] take? Please give a brief answer using a single sentence and do not copy the steps.\nDuring execution, we copy the question and steps into [Question] and [Step 0, ..., Step i] to form\nthe actual input to the LLM. The reason for requesting a brief answer is to try and keep the amount\nof information retained to the minimum needed, thereby avoiding unnecessary influence on the\nregeneration and hopefully reducing correlations in errors in turn.\n4\nInformation collection\nTo reduce the difficulty of the regeneration stage and avoid unrelated\ninformation from affecting the result, we filter out information that is not directly related to the\ncurrent step. Specifically, we ask the LLM to select useful items from the question and all previous\nitems with the following prompt, where [Information j] is simply the j-th sentence in the question:\nThis is a math question: [Question]. The following is information extracted from the question:\nInformation 0: [Information 0]\nInformation 1: [Information 1]\n...\nThe following are the first a few steps in a solution to the problem:\nStep 0: [Step 0]\nStep 1: [Step 1]\n...\nStep i-1: [Step i-1]\nWhich previous steps or information does the next step [Step i] directly follow from?\nAfter retrieving the free-text response from the LLM, we extract step or information ids by regular\nexpression. For example in Figure 1, the current step requires Step 4 and no information from the\nquestion as context. The selected steps and information are then fed into the regeneration stage.\nStep regeneration\nGiven the target and necessary information of the step, we can now ask the\nLLM to achieve the target independently with only the collected information, without seeing the\noriginal step. Because the step is usually a small jump from previous conclusions, and the information\ncollection stage has already filtered out irrelevant information, we can usually trust regeneration\nresults. The prompt for this stage is:\nWe are in the process of solving a math problem. We have some information from the problem:\nInformation 0: [Information I0]\nInformation 1: [Information I1]\n...\nThe following are some previous steps:\nStep 0: [Step S0]\nStep 1: [Step S1]\n...\nThe target for the next step is:\n[Target]\nPlease try to achieve the target with the information from the problem or previous steps.\nHere [Target] is the output from the target extraction stage. [Information Ii] and [Step Si] correspond\nto the specific items selected by the information collection stage. In Figure 1, only Step 4 and no\ninformation from the question is directly related to the current step, so SelfCheck simply copies the\ncontent of Step 4 into [Step S0] and removes the block containing [Information Ii].\nResult comparison\nThe last step is to compare results from the regeneration stage and the original\nstep with the following prompt:\nThe following are 2 solutions to a math problem.\nSolution 1: [Regeneration output]\nSolution 2: [Step i]\nCompare the key points from both solutions step by step and then check whether Solution 1 \u2018supports\u2019,\n\u2018contradicts\u2019 or \u2018is not directly related to\u2019 the conclusion in Solution 2. Pay special attention to the difference\nin numbers.\nIf the regeneration output \u2018supports\u2019/\u2018contradicts\u2019 the original step, we can conclude that the original\nstep is likely correct/incorrect respectively. Sometimes, the correctness of the original step cannot be\ndirectly inferred from the regeneration output. For example, when the target is to simplify an equation,\nthen there may be multiple valid solutions. In such cases, we are not sure about the correctness of the\noriginal step, which makes \u2018is not directly related to\u2019 the third possible outcome of the check.\n3.2\nRESULTS INTEGRATION\nAfter running step-checking and getting a checking result for each step, we need an integration\nfunction \u03d5 to give a confidence score, w \u2208 [0, 1], for the overall correctness of the solution. The input\nof \u03d5 should be a vector in the form of [r0, r1, ..., rn], where each item ri represents the step checking\nresult for Step i. We will use ri = \u22121, 0, and 1 to represent the step-checking results \u2018contradict\u2019,\n\u2018is not directly related to\u2019 and \u2018support\u2019 respectively. We find that the following simple integration\nfunction works well in practice\nw = \u03d5([r0, r1, ..., rn]) = 2 \u2217 Sigmoid\n \n\u2212\u03bb\u22121\nn\nX\ni=0\n1ri=\u22121 \u2212 \u03bb0\nn\nX\ni=0\n1ri=0\n!\n,\n(1)\nwhere \u03bb\u22121 and \u03bb0 are two non-negative hyperparameters with \u03bb\u22121 > \u03bb0; we fix \u03bb\u22121 = 1 and\n\u03bb0 = 0.3 in our experiments. The rationale of this setup is that the more failed checks we see,\nthe more likely the overall reasoning process, and thus final solution, are wrong. Note here that,\nbecause the checks are themselves imperfect, we do not necessarily want to immediately reject the\nwhole solution from a single step-check failure, especially for ri = 0 cases. This is why we take\na \u2018soft\u2019 approach to the verification with a confidence score. The number of successful checks,\ni.e. Pn\ni=0 1ri=1, is deliberately not included in our integration function as an increased number of\n5\nsuccessful checks does not actually increase our confidence in the overall solution: shorter reasoning\nchains are generally preferable to longer ones for a given question and LLM.\nOnce calculated, the resulting confidence score can be directly used as a weight for voting between\ndifferent possible solutions. We can thus use SelfCheck to increase the accuracy of an LLM\u2019s answers\nby generating multiple possible solutions, calculating confidence scores for each, and then choosing\nour final answer through weighted voting.\n4\nEXPERIMENTS\nWe now run experiments on three math-reasoning datasets to evaluate SelfCheck\u2019s effectiveness in\nchecking multi-step reasoning and improving final answer accuracies. Note here that our focus on\nmath-reasoning problems is due to ease of performance evaluation and dataset availability; SelfCheck\nis directly applicable to other question-answering problems with nominal changes to our prompts.\nDatasets\nGSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and MATH (Hendrycks et al.,\n2021) consist of math problems on primary school, middle school, and competition levels, containing\n1319, 2985, and 5000 test samples, respectively. For GSM8K and MathQA, we evaluate SelfCheck\non the whole test sets. Due to limited resources, we use a subset of MATH test set taken from Ling\net al. (2023).1 Besides the levels of difficulty, the three datasets differ from each other in the following\naspects. Firstly, MathQA provides 5 options to choose from for each problem, while GSM8K and\nMATH have no options. Secondly, GSM8K only has arithmetic problems, while MathQA and MATH\ncontain more diverse problems in geometry, physics, probability, and algebra.\nLLMs\nWe use GPT-3.5 (gpt-3.5-0301) and GPT-4 (gpt-4-0613) as our LLMs, focusing in particular\non the former due to budget restrictions. Note that the same prompts are used for all datasets with\nboth LLMs during evaluation; no dataset-specific customization or tuning has been performed. When\ndevising the prompts, a small number of training samples from MathQA dataset were utilized.\nBaselines\nWe use majority voting (also known as Self-Consistency Decoding (Wang et al., 2022)\nin the context of CoT reasoning) as our main baseline following Ling et al. (2023) and Lightman\net al. (2023). Despite its simplicity, this is still quite a strong baseline in the current literature. In\nparticular, most existing few-shot methods report similar results compared with it (Weng et al., 2022;\nLing et al., 2023). We also compare with previously quoted results from Self Verification (SV, Ling\net al. (2023)) and Deductive Verification (DV, Weng et al. (2022)) when possible. We note though\nthat these approaches are not directly comparable to SelfCheck in general, as they require additional\nexemplars which will often not be available in practice. Despite this, we will find that SelfCheck\noutperforms them when comparisons are possible.\nWe omit results from Faithful-CoT (Lyu et al., 2023), because it has already been shown to decrease\nthe accuracies on GSM8K and MATH by 11.8% and 4.2%, respectively compared to majority\nvoting (Ling et al., 2023). It is also impossible for us to compare with training/finetuning based\nmethods such as Lightman et al. (2023), because we have neither access to their finetuned models nor\ncomputation resources to repeat their training/finetuning. The significant extra data and resources\nthey require also means their contributions are somewhat tangential to SelfCheck regardless.\n4.1\nFINAL ANSWER CORRECTNESS\nFigure 2 shows the performance gains using the confidence scores from SelfCheck to do weighted\nvoting compared with baseline methods. The upper plots show that accuracies of both SelfCheck and\nmajority voting have the same increasing tendency as the number of generated solutions per question\nincreases, which is a result of the variance reduction provided by averaging over more solutions.\nThe bottom plots show the difference in accuracy between the two including the standard error in\nthe estimate. We can see that by allocating higher weights to correct solutions, SelfCheck achieves\nsignificantly higher accuracies than majority voting for all solution numbers per question. We also\nfind the improvements of SelfCheck (compared with majority voting) to be higher than Deductive\nVerification and Self-Verification in their reported settings, despite the use of in-context learning\n1https://github.com/lz1oceani/verify_cot/tree/main/results/chatgpt3.5/\nnatural_program/MATH_np.json\n6\n0.66\n0.69\n0.72\n0.75\n0.78\n0.81\n0.84\nAccuracy\nMajority Voting\nSelfCheck\n0.60\n0.63\n0.66\n0.69\n0.72\n0.75\n0.78\nAccuracy\nMajority Voting\nSelfCheck\n0.33\n0.36\n0.39\n0.42\n0.45\n0.48\n0.51\nAccuracy\nMajority Voting\nSelfCheck\n1 2 3 4 5 6 7 8 9 10\n#Solutions per question\n0.00\n0.02\n0.04\nAccuracy\nDV\nSV\n(a) GSM8K\n1 2 3 4 5 6 7 8 9 10\n#Solutions per question\n0.00\n0.02\n0.04\nAccuracy\n(b) MathQA\n1 2 3 4 5 6 7 8 9 10\n#Solutions per question\n0.00\n0.02\n0.04\nAccuracy\nDV\n(c) MATH\u2217\nFigure 2: The upper plots show the accuracies of SelfCheck and majority voting for different numbers\nof generated solutions per question with GPT-3.5. The lower plots show the accuracy gaps between\neach method and majority voting, where DV and SV stand for Deductive Verification (Weng et al.,\n2022) and Self-Verification (Ling et al., 2023), respectively. It is difficult to compare with DV and SV\nwith respect to absolute accuracies because they are using different generator models. However, we\ncan see that SelfCheck achieves higher relative performance gains than both in their reported settings.\nTable 1: SelfCheck significantly increases final answer accuracies with both GPT-3.5 and GPT-\n4, even we only have 2 candidate solutions for each question. \u2206Acc is the performance gain of\nSelfCheck compared with majority voting (MV), with the \u00b1 indicating the standard error. \u2717\u2717, \u2717\u2713and\n\u2713\u2713represent the proportions of questions with 0, 1 or 2 correct solutions. We see that the gains from\nSelfCheck are typically larger in cases where it is common for only one of the solutions to be correct,\nas these are the cases using weighted voting can influence the final answer.\nDataset\nGenerator\nChecker\n\u2717\u2717 (%)\n\u2717\u2713 (%)\n\u2713\u2713 (%)\nAcc(MV, %)\nAcc(SelfCheck, %)\n\u2206Acc (%)\nGPT-3.5\nGPT-3.5\n16.8\n23.0\n60.2\n71.7\n74.3\n2.8\u00b10.9\nGSM8K\nGPT-4\nGPT-4\n8.8\n8.2\n83.0\n87.1\n86.9\n-0.2\u00b10.2\nGPT-4\nGPT-3.5\n8.8\n8.2\n83.0\n87.1\n88.1\n1.0\u00b10.3\nGPT-3.5\nGPT-3.5\n27.6\n26.4\n46.0\n59.2\n64.6\n5.4\u00b11.1\nMathQA\nGPT-4\nGPT-4\n16.2\n11.0\n72.8\n78.3\n80.9\n2.6\u00b10.4\nGPT-4\nGPT-3.5\n16.2\n11.0\n72.8\n78.3\n81.2\n3.0\u00b10.4\nGPT-3.5\nGPT-3.5\n52.6\n23.2\n24.2\n35.8\n38.0\n2.2\u00b10.7\nMATH\u2217\nGPT-4\nGPT-4\n42.0\n20.2\n37.8\n47.9\n51.3\n3.4\u00b10.6\nGPT-4\nGPT-3.5\n42.0\n20.2\n37.8\n47.9\n48.9\n1.0\u00b10.8\nfrom additional examples. We will perform additional ablations on how performance changes when\nensembling over a larger number of solutions in Section 5.1.\nTo investigate the effect of using more powerful LLMs, and of using a different LLM for the\ngeneration and checking, we further conducted experiments with GPT-4 and a mix of GPT-4 and\nGPT-3.5. Because of the high cost of calling the GPT-4 API, we randomly sample 500 questions from\neach dataset to form the test sets and generate 2 (instead of 10) answers to each question. In Table 1,\nwe see that SelfCheck significantly outperforms majority voting with both GPT-3.5 and GPT-4. We\nalso notice that using GPT-3.5 to check GPT-4 generated answers yields surprisingly good results,\nactually outperforming checking with GPT-4 on the simpler GSM8K and MathQA tasks. This is\nlikely because using different LLMs helps to further decorrelate the errors of the generator and the\nchecker, and shows that using a cheaper LLM can still often be sufficient for the checking. For the\nmore difficult problems in MATH, using GPT-4 as checker always produces better results, but even\nhere the checking from GPT-3.5 is beneficial compared to doing no checking at all.\n4.2\nVERIFICATION PERFORMANCE\nBesides serving as a confidence score calculator to improve the performance of voting, SelfCheck\ncan also predict the correctness of a single solution. To do so, we simply set a threshold t to the\nconfidence score, where solutions with confidence scores w \u2265 t are classified as correct.\n7\n0.3\n0.6\n0.9\nThreshold t\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nProportion\nReal + in Pred +\nReal \n in Pred \n(a) GSM8K\n0.3\n0.6\n0.9\nThreshold t\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nProportion\nReal + in Pred +\nReal \n in Pred \n(b) MathQA\n0.3\n0.6\n0.9\nThreshold t\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nProportion\nReal + in Pred +\nReal \n in Pred \n(c) MATH\u2217\nFigure 3: When raising the classification thresholds t, the proportions of real correct solu-\ntions in predicted correct solutions (Real + in Pred +) increase for GSM8K (67.5%\u219276.5%),\nMathQA (59.4%\u219282.2%) and MATH (34.6%\u219250.8%).\n0.0\n0.5\n1.0\nFP rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTP rate\nGSM8K\nMathQA\nMATH\nFigure 4: True positive rates (TP)\nvs. false positive rates (FP) as clas-\nsification threshold, t, is varied.\nFigure 4 shows the ROC curves for each dataset. As a com-\nparison, directly prompting GPT-3.5 to verify whole reasoning\nchains leads to no meaningful control on the false and true pos-\nitive rates (FP and TP): they are always both 100% on MATH\nand 98% on GSM8K, as observed by Ling et al. (2023). In\nother words, the checker always predicts the answer as correct,\nproviding no useful information.\nAs well as verification accuracies, we may also care about the\nsolution quality after filtering out solutions with low confidence\nscores w. Figure 3 shows that by increasing the threshold t,\nSelfCheck can filter out more incorrect solutions, such that\na higher proportion of the solutions that pass the check are\nindeed correct (Real + in Pred +). Though this is at the cost of\nmisclassifying more of the real correct solutions as incorrect, this can be a useful feature in cases\nwhere the risk of choosing an incorrect solution is higher than rejecting a correct one.\n5\nANALYSIS\nWe now perform some ablations to justify some of the key design choices made by SelfCheck and\nprovide insights on its behavior. Limited by budget and time, all experiments in this section are\nperformed on a subset of the MathQA test set with 100 randomly selected questions.\n5.1\nMORE SOLUTIONS PER QUESTION?\n1\n10\n20\n30\n40\n50\n#Solutions per question\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\nMajority Voting\nSelfCheck\nFigure 5: SelfCheck achieves significantly\nhigher final answer accuracies than majority\nvoting for large ensembles of solutions.\nServing as a method to reduce variance, majority\nvoting increased final answer accuracies on different\ndatasets when we increased from 2 to 10 solutions\nin Figure 2. In cases where we only care about fi-\nnal predictive performance, one might thus question\nwhether it is better to simply use our computational\nresources to keep increasing the size of this ensemble,\nrather than relying on a checking scheme.\nHowever, as shown in Figure 5, this effect saturates\nfor larger solution ensembles, with the accuracy of majority voting never going above that achieved\nwhen n = 9, thereby never reaching the performance we already achieved by SelfCheck for the\nsmaller ensemble. Moreover, the performance of SelfCheck continues to increase as the ensemble\ngrows. By lowering the weights (confidence) of incorrect solutions, SelfCheck increases the chance\nof selecting the correct answers, even when their generation probabilities in the generator LLM are\nlow. Therefore, with SelfCheck, LLMs can effectively rectify their own biased beliefs by themselves.\n8\n5.2\nALBATION STUDIES\nIn order to pick apart the effect of several critical design choices for SelfCheck, we compare SelfCheck\nwith some of its variants with respect to final answer and verification accuracies on MathQA.\nGlobal v.s. step-by-step checking\nThe first question is can we simply ask an LLM to check the\nwhole solution without taking steps into consideration. To answer it, we prompt the LLM to perform\nglobal checking with the following instruction:\nThe following is a question and a solution to it from a student. Carefully check whether the solution is correct\nstep by step. End your response with your conclusion that starts with \"Correct\", \"Wrong\" or \"Not Sure\".\nQuestion: [Question]\nSolution: [Step 0, Step 1,..., Step n]\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n#Solutions per question\n0.6\n0.7\n0.8\nAccuracy\nSelfCheck\nGlobal Check\nSingle Stage Check\nError Check (0-shot)\nError Check (1-shot)\nMajority Voting\nFigure 6: Generation accuracies for variants\nof SelfCheck on MathQA with GPT-3.5.\nSimilar to the findings of Ling et al. (2023), we find that\nthe global checker outputs \"correct\" most of the time\nand rarely recognizes an error. Consequently, its final\nanswer accuracies are very close to majority voting (in\nFigure 6) and its verification accuracy (55.0%) is only\nmarginally above random guess (50.0%). This lack of\nability to deal with the difficulty of global checking is\nwhat makes step checking necessary.\nSingle-stage v.s. multiple-stage step checking\nNext,\nwe ask whether we really need to decompose the step\nchecking into several stages? To answer this, we design\nthe following prompt to use the LLM directly.\nThe following is a question and the first a few steps in its solution.\nQuestion: [Question]\nSolution: [Step 0, Step 1,..., Step i-1]\nCheck the correctness of the next step: [Step i]\nPlease consider the information it relies on and check step by step. Please end your response with your\nconclusion that starts with \"Correct\", \"Wrong\" or \"Not Sure\".\nTable 2: Verification accuracies for vari-\nants of SelfCheck on MathQA with GPT-\n3.5. The reported verification accuracy is\nthe average of true positive and true nega-\ntive rates.\nMethod\nAccuracy (%)\nSelfCheck\n66.7%\nGlobal Check\n55.0%\nSingle stage Check\n57.2%\nError Check (0-shot)\n63.1%\nError Check (1-shot)\n64.2%\nFigure 6 and Table 2 show that although this is better\nthan global checking, it is still significantly worse than\nSelfCheck with its multi-stage checking. This indicates\nthat checking a step in a single stage is still too challeng-\ning for the LLM, so it is necessary to further decompose\nstep checking into a pipeline of easier sub-tasks.\nError check v.s. regenerate and compare\nWe now\njustify the choice to perform step regeneration and com-\nparison instead of direct error checking for each step.\nTo do so, we replace our regeneration stage and com-\nparison stage with a single error-checking stage. We\nfirst compare with a zero-shot version of the variant\nwith the following prompt:\nGiven the following information:\nInformation 0: [Information I0]\nInformation 1: [Information I1]\n...\nStep 0: [Step S0]\nStep 1: [Step S1]\n...\nCheck the correctness of the next step [Step i]\nPlease check for grounding errors, reasoning errors and calculation errors step by step. Please end your\nresponse with your conclusion that starts with \"Correct\", \"Wrong\" or \"Not Sure\".\nWe then add an examplar from Ling et al. (2023) (see Appendix B) to make a more powerful one-shot\nerror checker. However, results in Figure 6 and Table 2 show that even with a very detailed and\ninstructive example, direct error checking still performs worse than our regenerate and compare\napproach, which supports our previous argument that LLMs are better at generation than checking.\n6\nCONCLUSIONS\nIn this paper, we have introduced SelfCheck, a general-purpose, zero-shot, step-by-step checking\nscheme for LLMs. Unlike previous approaches, SelfCheck does not require any additional data\nor external resources: it uses the LLM to identify errors in its own reasoning, leveraging a novel\nregenerate-and-compare approach. By using the results of this checking to perform weighted voting\nover different solutions, we find that SelfCheck is able to, in turn, increase final predictive accuracy.\n9\nREFERENCES\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language\nmodel with state-of-the-art performance. 2023.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\nHajishirzi. MathQA: Towards interpretable math word problem solving with operation-based\nformalisms. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pp. 2357\u20132367, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nGoogle. Palm 2 technical report. arXiv preprint arXiv:2303.08774, 2023.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. In International Conference on Learning Representations, 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track\n(Round 2), 2021.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the\nadvance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.\nDeductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,\nand Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379,\n2023.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars\nLiden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language\nmodels with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n10\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners\nwith self-verification. arXiv preprint arXiv:2212.09561, 2022.\n11\nAPPENDIX A\nA COMPLETE EXAMPLE OF SELFCHECK\nTo help readers get a better idea of how SelfCheck works, we show a complete version of the example\nin Figure 1, including the exact inputs and outputs of different stages. Please note that we have\ncompiled all latex codes for readability, though the inputs and outputs to LLMs are in their original\nform. The question is\nLet S be the set of complex numbers z such that the real part of 1\nz is equal to 1\n6. This set\nforms a curve. Find the area of the region inside the curve.\nWe feed the following instruction to the LLM that serves as the generator:\nSolve the following problem step by step. Please start each step with \"Step :\" and split\nsentences in each step with \"\\n\\n\". Please finish you response with \"So the answer is ...\".\nProblem: Let S be the set of complex numbers z such that the real part of 1\nz is equal to 1\n6.\nThis set forms a curve. Find the area of the region inside the curve.\nAfter getting the following solution, we start checking Step 5 as an example:\nSolution:\n(Step 0) Let z = x + yi be a complex number, where x and y are real numbers.\n(Step 1) We are given that the real part of 1\nz is equal to 1\n6.\n(Step 2) Using the fact that the real part of a complex number a + bi is a, we have\nRe\n\u0000 1\nz\n\u0001\n= Re\n\u0010\n1\nx+yi\n\u0011\n=\nx\nx2+y2 .\n(Step 3) Setting Re\n\u0000 1\nz\n\u0001\n= 1\n6, we get\nx\nx2+y2 = 1\n6.\n(Step 4) Cross-multiplying and rearranging, we get the equation 6x = x2 + y2.\n(Step 5) Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\n(Step 6) This is the equation of a circle with center\n\u0000 3\n2, 0\n\u0001\nand radius 3\n2.\n(Step 7) The area of the region inside the circle is 9\u03c0\n4 . So the answer is 9\u03c0\n4 .\nVariable definition extraction\nThis optional stage extracts the definitions of variables that appear\nin the reasoning chain. SelfCheck feeds the definitions to the regeneration stage to avoid LLM not\nunderstanding the meaning of a variable. The input to the LLM is:\nThe following is the solution to a math question: Question: Let S be the set of complex\nnumbers z such that the real part of 1\nz is equal to 1\n6. This set forms a curve. Find the area of\nthe region inside the curve.\nSolution:\n(Step 0) Let z = x + yi be a complex number, where x and y are real numbers.\n...\n(Step 7) The area of the region inside the circle is 9\u03c0\n4 . So the answer is 9\u03c0\n4 .\nAre there variables in the solution?\nIf so, please list the definition of variable in\nthe form of \"1. variable is defined as...\".\nThe underlined parts are the type of question, the question itself and the steps in its solution,\nrespectively.\nThe output from the LLM is:\nYes. There are variables in the solution.\n1. zzz is defined as a complex number of the form x + yi\nx + yi\nx + yi, where xxx and yyy are real numbers.\nThe bold part is then saved to form a part of the input in the regeneration stage.\nTarget extraction\nTo get a brief and clear target of the current step, the input to the LLM is:\nThe following is a part of the solution to the problem: Let S be the set of complex numbers\nz such that the real part of 1\nz is equal to 1\n6. This set forms a curve. Find the area of the\n12\nregion inside the curve.\n(Step 0) Let z = x + yi be a complex number, where x and y are real numbers.\n...\n(Step 5) Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\nWhat specific action does the step \"Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\"\ntake? Please give a brief answer using a single sentence and do not copy the steps.\nThe underlined parts are the question and reasoning steps before the current one, including the current\none.\nThe output of the LLM is:\nThe step completes the square to rewrite the equation in standard form of a circle.\nThe whole sentence is saved and forms the most important part of the input in the regeneration stage.\nInformation Collection\nTo get sentences in the question and previous steps in the solution that are\ndirectly related to the current step, the input to the LLM is:\nThis is a math question: Question: Let S be the set of complex numbers z such that the\nreal part of 1\nz is equal to 1\n6. This set forms a curve. Find the area of the region inside the curve.\nThe following is information extracted from the question:\nInformation 0: Let S be the set of complex numbers z such that the real part of 1\nz is equal to\n1\n6.\nInformation 1: This set forms a curve.\nInformation 2: Find the area of the region inside the curve.\nThe following is the first a few steps in a solution to the problem:\nStep 0: Let z = x + yi be a complex number, where x and y are real numbers.\n...\nStep 4: Cross-multiplying and rearranging, we get the equation 6x = x2 + y2.\nWhich previous steps or information does the next step \"Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\" directly follow from?\nThe underlined information (contents after Information X:) is simply a list of sentences in the question.\nThe underlined steps are reasoning steps before (not including) the current one in the solution.\nThe output from the LLM is:\nThe next step \"Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\" directly follows\nfrom Step 4.\nThen we use regular expression to extract the information and step id from LLM\u2019s output. Some steps\nmay only rely on information from the question while others may only use results from previous\nsteps.\nStep regeneration\nIn this stage, we instruct the LLM to achieve the extracted target with only\ncollected information with the following prompt:\nWe are in a process of solving a math problem.\nVariables are defined as: 1. z is defined as a complex number of the form x + yi, where x\nand y are real numbers.\nThe following are some previous steps: Step 0: Cross-multiplying and rearranging, we get\nthe equation 6x = x2 + y2.\n13\nThe target for next step is: The step completes the square to rewrite the equation in standard\nform of a circle.\nPlease try to achieve the target with the information from previous steps.\nThe underlined parts are the type of question, variable definition (if any), collected information\nfrom the question (if any), collected previous steps (if any) and the extracted target. Please notice\nthat the current step does not directly rely on any information from the question, we removed the\ncorresponding block from the input.\nThe output from the LLM is:\nTo rewrite the equation in standard form of a circle, we need to complete the square\nfor both xxx and yyy.\nStarting from the given equation: 6x = x2 + y2\n6x = x2 + y2\n6x = x2 + y2.\nWe add and subtract (6/2)2 = 9\n(6/2)2 = 9\n(6/2)2 = 9 on the right-hand side:\n6x + 9 \u2212 9 = x2 + y2 + 9 \u2212 9\n6x + 9 \u2212 9 = x2 + y2 + 9 \u2212 9\n6x + 9 \u2212 9 = x2 + y2 + 9 \u2212 9.\nSimplifying: (x \u2212 3)2 + y2 = 9\n(x \u2212 3)2 + y2 = 9\n(x \u2212 3)2 + y2 = 9. This is now in the standard form of a circle with\ncenter (3, 0)\n(3, 0)\n(3, 0) and radius 333.\nThen we can directly compare the regenerated step with the original one in the next stage.\nResult comparison\nThe input to the LLM is:\nThe following are 2 solutions to a math problem:\nSolution 1: To rewrite the equation in standard form of a circle, we need to complete the\nsquare for both x and y.\nStarting from the given equation: 6x = x2 + y2.\nWe add and subtract (6/2)2 = 9 on the right-hand side: 6x + 9 \u2212 9 = x2 + y2 + 9 \u2212 9.\nSimplifying: (x \u2212 3)2 + y2 = 9. This is now in the standard form of a circle with center\n(3, 0) and radius 3.\nSolution 2: Completing the square, we obtain\n\u0000x \u2212 3\n2\n\u00012 + y2 = 9\n4.\nCompare the key points from both solutions step by step and then check whether Solution\n1 \"supports\", \"contradicts\" or \"is not directly related to\" the conclusion in Solution 2. Pay\nspecial attention to difference in numbers.\nThe underlined parts are simply the problem type, the regenerated step and the original step.\nThe output is:\nIn both solutions, the goal is to rewrite the given equation in standard form of a circle.\nHowever, in Solution 1, the completion of the square results in the center of the circle being\n(3, 0) and a radius of 3. In Solution 2, the completion of the square results in the center of\nthe circle being (3/2, 0) and a radius of 3/2.\nTherefore, Solution 1 contradicts the conclusion in Solution 2 as the centers and radii of\nthe circles are different.\nIf the word \u2018support\u2019 or \u2018contradict\u2019 appears in the last line of the output, SelfCheck outputs a step\nscore 1 or -1. Otherwise, SelfCheck outputs 0 meaning that two steps are not related.\nAPPENDIX B\nEXAMPLAR FOR ONE-SHOT ERROR CHECKING\nThe following example from Ling et al. (2023) is used to perform one-shot error checking.\nHere is some information:\n\u201c1. Alice gets 2 apples.\n2. Alice gets twice as many oranges as bananas.\n3. Original price per apple: $2\n14\nDiscount: 50%\nPrice per apple after discount: $2 * (1 - 0.50) = $2 * 0.50 = $1\"\nBased on the given information, here is a reasoning process:\n\"Calculate Alice\u2019s cost of the 5 apples after a 50% discount.\nPrice per apple after discount: $1\nApples be eaten: 3\nOranges be eaten: 6\nTotal apples: 5\nTotal oranges: x\nTotal bananas: 2x\nAlice\u2019s total number of fruits: 5 + x + 2x = 5 + 4x\nAlice\u2019s total cost of 5 apples: $1 * (5 - 3) = $2\u201d\nDouble-check the reasoning process, let\u2019s analyze its correctness, and end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s think step by step.\nGrounding check\nPurpose: Find references for numbers in the reasoning process sequentially\nRule: Any ungrounded number makes the check fails\n\"x\" is a variable, we do not care about variables.\n\"y\" is a variable, we do not care about variables.\n\"5\" is a quantity from the given information that represents the number of apples.\n\"50%\" is a quantity from the given information that represents the discount.\n\"$1\" is a quantity from the given information that represents the price per apple after the\ndiscount.\n\"3\" is a quantity not from the given information, it is ungrounded.\n\"6\" is a quantity not from the given information, it is ungrounded.\n\"$2\" is a quantity from the given information that represents the original price per apple.\nResult: Based on the check, there are ungrounded numbers, so the grounding check fails.\nReasoning check:\nPurpose: Check whether the reasoning logic correctly answers the question\nRule: Any logic error makes the check fails\nTo calculate the total cost of apples after a 50% discount, we should multiply the number of\napples by the discounted price. But to answer the question, we do not need to calculate the\ntotal number of fruit Alice gets.\nResult: Based on the check, there are some logic mistakes, so the reasoning check fails.\nCalculation check:\nPurpose: Check whether the calculation process is consistent\nRule: Any inconsistent calculation makes the check fails\ncalculation1:\nequation: $1 * (5 - 3), answer: $2\n(5 - 3) = 2\n$1 * 2 = $2 is consistent with the answer, so the calculation is correct.\ncalculation2:\nequation: 5 + x + 2x, answer: 5 + 4x\nx + 2x = 3x\n5 + 3x is inconsistent with the answer, so the calculation is incorrect.\nResult: Based on the check, the calculation process is inconsistent, so the calculation check\nfails.\nCheck results: Ground check fails, Reasoning check fails, Calculation check fails.\nRule: Any failed check makes the reasoning incorrect.\nSo the answer is \"no\".\n15\n"
  },
  {
    "title": "Predicting masked tokens in stochastic locations improves masked image modeling",
    "link": "https://arxiv.org/pdf/2308.00566.pdf",
    "upvote": "14",
    "text": "Stochastic positional embeddings improve masked image modeling\nAmir Bar 1 2 3 Florian Bordes 3 Assaf Shocher 2 Mahmoud Assran 3 Pascal Vincent 3 Nicolas Ballas 3\nTrevor Darrell 2 Amir Globerson 1 Yann LeCun 3 4\nAbstract\nMasked Image Modeling (MIM) is a promis-\ning self-supervised learning approach that en-\nables learning from unlabeled images. Despite\nits recent success, learning good representations\nthrough MIM remains challenging because it re-\nquires predicting the right semantic content in\naccurate locations. For example, given an incom-\nplete picture of a dog, we can guess that there is\na tail, but we cannot determine its exact location.\nIn this work, we propose to incorporate location\nuncertainty into MIM by using stochastic posi-\ntional embeddings (StoP). Specifically, we condi-\ntion the model on stochastic masked token posi-\ntions drawn from a Gaussian distribution. StoP\nreduces overfitting to location features and guides\nthe model toward learning features that are more\nrobust to location uncertainties. Quantitatively,\nStoP improves downstream MIM performance on\na variety of downstream tasks, including +1.7%\non ImageNet linear probing using ViT-B, and\n+2.5% for ViT-H using 1% of the data.1\n1. Introduction\nMasked Image Modeling (MIM) enables learning from un-\nlabeled images by reconstructing masked parts of the image\ngiven the rest of the image as context. In recently years, new\nMIM methods have emerged (Xie et al., 2021; Bao et al.,\n2021; He et al., 2021; Assran et al., 2023). Masked Auto-\nEncoders (MAE) (He et al., 2021) are trained to minimize\na reconstruction error in pixel space, and I-JEPA (Assran\net al., 2023) reconstructs image features. MIM is appeal-\ning compared to invariance-based self-supervised learning\nmethods like DINO (Caron et al., 2021) and iBOT (Zhou\net al., 2021) as MIM do not suffer from the same limita-\ntions, namely, it does not require heavy use of hand-crafted\n1Tel Aviv University\n2UC Berkeley\n3Meta AI (FAIR)\n4New York University.\nCorrespondence to:\nAmir Bar\n<amir.bar@cs.tau.ac.il>.\n1See https://github.com/amirbar/StoP for code.\nFigure 1. Given a partial image of a dog, can you precisely deter-\nmine the location of its tail? Existing Masked Image Modeling\n(MIM) models like MAE (He et al., 2021) and I-JEPA (Assran\net al., 2023) predict tokens deterministically and do not model\nlocation uncertainties (a), we propose to predict the target (masked\ntokens) in stochastic positions (StoP) which prevents overfitting to\nlocations features. StoP leads to improved MIM performance on\ndownstream tasks, including linear probing on ImageNet (b).\naugmentations (Xiao et al.; He et al., 2021), mini-batch\nstatistics, or a uniform cluster prior (Assran et al., 2022).\nDespite the recent success of MIM, we argue that learning\ngood representations using MIM remains challenging due to\nlocation uncertainties because it requires predicting the right\nsemantic content in accurate locations. For example, given\nan incomplete picture of a dog (see Figure 1a), we might\nguess there\u2019s a tail, but we can\u2019t be sure exactly where it is,\nas it could realistically be in several different places. With-\nout explicitly modeling this location uncertainty, existing\nMIM models like MAE and I-JEPA might overfit on seman-\ntic content in arbitrary locations (e.g, the tail location).\nIn this work, we propose to address location uncertainty\nin MIM by turning existing MIM models into stochastic\nones. Instead of training the model to make predictions in\nexact locations, we use Stochastic Positional embeddings\n(StoP) to introduce noise to the masked token\u2019s positions,\nimplicitly forcing the model to make stochastic predictions.\nStoP guides the model towards learning features that are\nmore resilient to location uncertainties, such as the fact that\na tail exists in a general area rather than a specific point,\nwhich improves downstream performance (Figure 1b).\nSpecifically, we model the position of every masked token\n1\narXiv:2308.00566v2  [cs.CV]  27 Feb 2024\nStochastic positional embeddings improve masked image modeling\nas a random variable with a Gaussian distribution where its\nmean is the position of the patch, and the covariance matrix\nis learned. We find it crucial to design StoP carefully so that\nthe model does not collapse back to deterministic positional\nembeddings by scaling down the covariance matrix weights\nto overcome the noise.\nTo prevent collapse, we propose to tie between the scales\nof the noise and input context. With this constraint, scaling\ndown the noise also scales down the input context, which\nmakes the reconstruction task too hard to achieve. On the\nother hand, increasing the scale of the noise leads to very\nstochastic masked token positions, which makes the recon-\nstruction task difficult as well. We provide a theoretical\nproof, showing that our solution indeed prevents collapse.\nOur contributions are as follows. First, we propose the idea\nof Stochastic Positional embeddings (StoP) and apply it to\nMIM to address the location uncertainty in MIM, namely\nthat the location of semantic features is stochastic. Second,\nwe demonstrate that adding StoP to I-JEPA, a recent MIM\napproach, leads to improved performance on a variety of\ndownstream tasks, highlighting its effectiveness. Lastly,\nimplementing StoP for MIM requires only three extra lines\nof code, without adding any runtime or memory overhead.\n2. Preliminaries - Masked Image Modeling\nThe idea in MIM is to train a model to reconstruct masked\nparts in an image given the rest of the image as context. In\nthis process, a neural network f\u03b8 learns the context represen-\ntations, and a network g\u03d5 is used to reconstruct the masked\nregions. In this section we describe the MIM algorithm,\nthen discuss how to apply StoP to MIM in Section 3.\nPatchification. Given an image, the first stage is to tokenize\nthe image. For the case of Vision Transformers (Dosovit-\nskiy et al., 2020), an input image Ix \u2208 RH\u00d7W \u00d73 is first\npatchified into a sequence of non-overlapping image patches\n\u02c6p = (\u02c6p1, ..., \u02c6pk) where \u02c6pi \u2208 RH\u2032\u00d7W \u2032\u00d73 and K =\nHW\nH\u2032W \u2032\nis the number of patches. Then, each patch \u02c6pi is projected\nto Rde through a linear fully connected layer and its cor-\nresponding positional embedding features are added to it,\nresulting in the patchified set p = {p1, ...pK}.\nMasking. Let x = {pi|i \u2208 Bx} be the set of context\npatches where Bx denotes the set of context indices (i.e.,,\nthe visible tokens in Figure 2). We denote by By the indices\nof the target patches y. The context and target patches are\nchosen via random masking as in He et al. (2021) or by\nsampling target continuous blocks as in Assran et al. (2023).\nContext encoding.\nThe context tokens are processed\nvia an encoder model f\u03b8 to obtain deep representations:\nsx = f\u03b8(x), where sxi \u2208 Rde is the ith context token repre-\nsentation. Each token sxi is then projected from the output\ndimension of the encoder de to the input dimension of the\npredictor dp via a matrix B \u2208 Rdp\u00d7de, and it is enriched\nwith deterministic positional embedding \u03c8i \u2208 Rdp:\nci = \u03c8i + Bsxi\n(1)\nMasked tokens. We define the set of masked tokens, where\nevery masked token mj for j \u2208 By is composed of the\npositional embeddings of the jth patch \u03c8j and a bias term\n\u02dcm that is shared across all masked tokens, namely:\nmj = \u03c8j + \u02dcm\n(2)\nPrediction and loss. Finally, the predictor function g\u03d5 is ap-\nplied to predict the target features \u02c6sy = g\u03d5(c, m). To super-\nvise the prediction, the ground truth sy = {syi}i\u2208By is ob-\ntained either by using the raw RGB pixels or via a latent rep-\nresentation of the pixels. The loss\n1\n|By|\nP\ni\u2208By L(syi, \u02c6syi)\nis then applied to minimize the prediction error.\n3. Masked Image Modeling with StoP\nThis section presents the StoP formulation, and how to uti-\nlize it in MIM while avoiding collapsing back to determin-\nistic positional embeddings. A high-level schematic view\nof the model is included in Figure 2, and a pseudo-code\nimplementation is included in Algorithm 1.\nStochastic Positional Embeddings (StoP). Instead of train-\ning the model to make predictions in exact locations, we\npropose to use stochastic positional embeddings which im-\nplicitly force the model to make stochastic predictions. This\nis meant to teach the model that locations cannot be pre-\ndicted precisely, resulting in improved robustness.\nFormulating StoP requires defining the distribution of the\nstochastic positions, parameterizing it appropriately, and\nimplementing measures to prevent the model from scaling\ndown the noise to the point where it becomes negligible.\nGiven a position j, we denote by \u02c6\u03c8j the random variable\nproviding the position embedding. We assume that \u02c6\u03c8j is\ndistributed as Gaussian whose mean is the fixed embedding\n\u03c8j, and whose covariance matrix is \u03a3 \u2208 Rdp\u00d7dp:\n\u02c6\u03c8j \u223c N(\u03c8j, \u03a3)\n(3)\nNaturally, we want to learn an optimal \u03a3. To parameterize\n\u03a3, we use a general formulation of a low-rank covariance\nmatrix:\n\u03a3 = \u03c3AAT\n(4)\nWhere A \u2208 Rdp\u00d7de is a learned matrix and \u03c3 \u2208 R+ is a\npositive scalar hyperparameter used to control the Noise\nto Signal Ratio (NSR).2 By learning the matrix A, this for-\n2At this point, it may seem unnecessary to have an additional \u03c3\nparameter. However, later we will tie A to other model parameters,\nand thus \u03c3 will not be redundant and determine the scale of the\nnoise.\n2\nStochastic positional embeddings improve masked image modeling\nFigure 2. Masked image modeling using stochastic positional embeddings (StoP). g\u03d5 predicts target tokens given masked tokens with\nstochastic positions mj and context tokens ci obtained via f\u03b8. StoP is applied to masked tokens only, leading to features that are more\nrobust to location uncertainties.\nmulation allows assigning different noise levels to different\nlocation components (e.g., high and low resolution), as well\nas capturing correlations between location features.\nUsing this formulation is challenging for two reasons. First,\nthe sampling process of \u02c6\u03c8 is non-differential w.r.t A, and\ntherefore we cannot derive gradients to directly optimize it\nwith SGD. Second, learning might result in the optimiza-\ntion process setting the values of \u03a3 to zero, leading to no\nrandomness. Next, we move to solve these issues.\nReparametrization Trick. Since \u02c6\u03c8j is sampled from a pa-\nrameterized distribution, it is non-differentiable in A. How-\never, a standard trick in these cases is to reparameterize\nthe distribution so that the sampling is from a fixed dis-\ntribution that does not depend on A (e.g., see Kingma &\nWelling (2013)). Specifically, we generate samples from\n\u02c6\u03c8j by first sampling a vector nj \u2208 Rde from a standard\nGaussian distribution: nj \u223c N(0, \u03c3I). Then, \u02c6\u03c8j is set to:\n\u02c6\u03c8j = Anj + \u03c8j\n(5)\nThe resulting distribution of \u02c6\u03c8j is equal to that in Equation 3,\nhowever, we can now differentiate directly through A.\nCollapse to deterministic positions (A=0). Intuitively,\nadding noise to an objective hurts the training loss, and thus\nif A appears only in (5), training should set it to zero. We\nindeed observe this empirically, suggesting that A cannot\nonly appear in a single place in the model. In what follows\nwe propose an approach to overcoming this issue.\nAvoiding collapse by weight tying A=B. To avoid the\ncollapse to deterministic positions, we propose to tie the\nweights of A and B (originally defined in Eq. 1), such that\nthe same matrix A projects both the context tokens sxi and\nAlgorithm 1 MIM w/ StoP pseudo-code. requires only a\nminor implementation change, highlighted in light gray.\n1: Input: num iterations K, image dist S, hyperparam \u03c3, posi-\ntional embeddings \u03c8\n2: Params: A, \u02dcm, encoder f\u03b8, predictor g\u03d5\n3: for itr = 1, 2, ..., K do\n4:\nIx \u223c S\n5:\np \u2190 patchify(Ix)\n6:\n(x, Bx), (y, By) \u2190 mask(p)\n7:\nsx \u2190 f\u03b8(x)\n8:\n# apply StoP on a sequence of tokens\n9:\nnj \u223c N(0, \u03c3I)\n10:\n# \u03c8Bx, \u03c8By - masked/context positional embeddings\n11:\nm = An +\u03c8By + \u02dcm\n12:\nc = Asx + \u03c8Bx\n13:\n# predict targets\n14:\n\u02c6sy \u2190 g\u03d5(c, m)\n15:\nsy \u2190 get target(y)\n16:\nloss \u2190 L(\u02c6sy, sy)\n17:\nsgd step(loss; {\u03b8, \u03d5, A, \u02dcm})\n18: end for\nthe noise tokens nj:\nci = Asxi + \u03c8i\nmj = Anj + \u03c8j + \u02dcm\n(6)\nThis tying means that the scale of the noise and the input\nare both determined by A, and thus the noise cannot be set\nto zero, without affecting other parts of the model. This can\nbe understood by considering two extreme cases:\n\u2022 If A = 0, there is complete certainty about the posi-\ntional embeddings but all context is lost (Asxi = 0).\n\u2022 If A has large magnitude, the context information is\npreserved but the noise is amplified and camouflages\nmasked tokens positional embeddings (Anj \u226b \u03c8j).\n3\nStochastic positional embeddings improve masked image modeling\nThis dual role of A forces the model to trade-off between\nthe positions of the masked tokens and the context tokens.3\nIn the following proposition, we formally show that if the\nweights A and B are tied then A cannot collapse. More\nspecifically, A = 0 occurs only if in the original determin-\nistic setting B goes to zero and doesn\u2019t utilize the context\nanyway. Formally, consider a regression task where F pre-\ndicts some target yj given a stochastic position Anj+\u03c8j+ \u02dcm\nwhere nj \u223c N(0, \u03c3I) and projected context token Bxi. De-\nnote Jtied, Jdet the loss functions when tying the weights A\nand B, and when using deterministic positional embeddings\nrespectively:\nJtied(A) =\nX\ni,j\nEnj[(F(Anj + \u03c8j + \u02dcm, Axi) \u2212 yj)2]\nJdet(B) =\nX\ni,j\n[(F(\u03c8j + \u02dcm, Bxi) \u2212 yj)2]\nProposition 3.1. If the weights of A and B are tied (namely\nA = B) then dJtied\ndA\n\f\f\nA=0 = 0 iff dJdet\ndB\n\f\f\nB=0 = 0\nProof is included in Appendix A.\nOptimal Predictor. Our approach relies on using stochastic\npositional embeddings. Here we provide further analysis,\nshowing that the optimal predictor performs spatial smooth-\ning. Consider a random variable X (corresponding to the\ncontext in our case. For simplicity assume X is just the\npositional embedding of the context) that is used to predict\na variable Y (corresponding to the target in our case). But\nnow instead of predicting from X, we use a noise variable Z\nthat is independent of both X, Y , and provide the predictor\nwith only the noisy result R = g(X, Z). Here g is some\nmixing function (in our case g(x, z) = x + z). We next\nderive the optimal predictor f(R) in this case. Formally we\nwant to minimize:\nER,Y [(f(R) \u2212 Y )2]\n(7)\nProposition 3.2. If Z is a Gaussian with zero mean and unit\nvariance, the optimal predictor that minimizes Equation 7\nis:\nf(r) =\nZ\nx\nE[Y |X = x]\n1\n\u221a\n2\u03c0 e\u22120.5(x\u2212r)2dx\nThus, the optimal predictor amounts to a convolution of the\nclean expected values with a Gaussian. See Appendix B for\nthe proof.\n3Note that an implicit assumption here is that \u03c8 and sx have\nfixed magnitude. This is true for sine-cosine features and for sx\nwhich are layer normalized by the transformer last layer.\n4. Experiments and Results\nNext, we turn to discuss the main experiments presented in\nthe paper. In Section 4.1, we describe the application of StoP\nto various downstream tasks including image recognition,\ndense prediction, and low-level vision tasks. In Section 4.2\nwe discuss the ablation study and design choices. The full\nimplementation details are included in Appendix C.\n4.1. Downstream Tasks\nWe conducted pre-training of StoP on top of I-JEPA, which\nis a state-of-the-art MIM model. We train on IN-1k for a\nperiod of 600 epochs using ViT-B/16 and ViT-L/16 architec-\ntures for the encoder and predictor or for 300 epochs when\nusing ViT-H/14. Subsequently, we proceeded to evaluate\nthe model\u2019s performance on a variety of downstream tasks.\nAdditional results and comparison to invariance-based ap-\nproaches are included Appendix C.2.\nImage recognition. For image classification, we perform\na linear probing evaluation of StoP on multiple datasets,\nincluding ImageNet (IN-1k) (Russakovsky et al., 2015),\nPlaces 205 (Zhou et al., 2014a), iNaturalist 2018 (Van Horn\net al., 2018), and CIFAR 100 (Krizhevsky, 2009). These\ndatasets vary in their size, their purpose, and the geographi-\ncal environments from which the images were captured. For\nexample, IN-1k contains over 1.2 million images compared\nto CIFAR-100 which contains only 60, 000 images, and\nwhile IN-1k is focused on object recognition, iNaturalist\nand Places are focused on scene and species recognition.\nIn Table 1, we present the linear probing image classification\nresults conducted on IN-1k under different linear evaluation\nprotocols using different amounts of data, and by aggregat-\ning features from different layers. E.g, \u201c100%, last 4 layers\u201d\napplies linear probing on the entire IN-1k data and the repre-\nsentation of each image is comprised of a concatenation of\nfour feature vectors, each one summarizes information from\nits corresponding layer via average pooling. In Table 2 we\ncompare linear probing results of common MIM methods on\nIN-1k, reporting past published performance. In Table 2 all\nperform linear probing over the output from the last layer.\nStoP improves the baseline performance using all archi-\ntectures examined. For example, +2.5% linear probing\nperformance gains with ViT-H using 1% of the labeled data\nand 1.6% when using features from the last 4 layers using\nViT-B on the full IN-1k data. Furthermore, using StoP leads\nto improvements in downstream linear probing tasks (see\nTable 4). For example, StoP leads to 3.3% improvement on\niNAT using ViT-H and 1.3% on counting. This confirms that\nthe learned representations lead to improvements in a large\nvariety of image recognition tasks. On full finetuning using\n1% of the labeled data, we observe similar performance\nimprovements (see Table 5), e.g, +2.3% improvements on\n4\nStochastic positional embeddings improve masked image modeling\nArch\nMethod\n1%, last layer\n100%, last layer\n100%, last 4 layers\nViT-B/16\nI-JEPA\n57.1\n70.9\n72.9\n+StoP\n60.3 (+3.2%)\n72.6 (+1.7%)\n74.5 (+1.6%)\nViT-L/16\nI-JEPA\n64.2\n76.1\n77.5\n+StoP\n65.1 (+0.9%)\n77.1 (+1.0%)\n78.5 (+1.0%)\nViT-H/14\nI-JEPA\n62.9\n78.2\n79.3\n+StoP\n65.4 (+2.5%)\n79.0 (+0.8%)\n79.6 (+0.3%)\nTable 1. StoP compared to deterministic sinusoidal positional embeddings on IN-1k. StoP leads to consistent linear probing\nimprovement in all settings. When applying linear probing on a trained ViT-H model with StoP, using only 1% of the labeled data and\nusing averaged pooled features from the last layer, StoP results in an +2.5% improvement. The baseline I-JEPA uses sinusoidal positional\nembeddings.\nMethod\nArch.\nEpochs\nTop-1\ndata2vec\nViT-L/16\n1600\n77.3\nMAE\nViT-B/16\n1600\n68.0\nViT-L/16\n1600\n75.8\nViT-H/14\n1600\n76.6\nI-JEPA\nViT-B/16\n600\n70.9\nViT-L/16\n600\n76.1\nViT-H/14\n300\n78.2\n+StoP (ours)\nViT-B/16\n600\n72.6\nViT-L/16\n600\n77.1\nViT-H/14\n300\n79.0\nTable 2. Linear-evaluation on IN-1k. Replacing sinusoidal po-\nsitional embeddings with StoP in I-JEPA significantly improves\nlinear probing results.\nTop-1 accuracy using ViT-L model. We provide the full\nfinetuning results in Table 16, Appendix C.2.\nCounting and depth ordering. We assess the downstream\nperformance on tasks that require fine-grained objects rep-\nresentations like counting and depth ordering using the\nCLEVR (Johnson et al., 2017) dataset. Table 4 provides\nevidence that using StoP significantly improve counting\n(+1.3%) and slightly improve depth ordering (+0.1%).\nDense prediction. To evaluate how well StoP performs on\ndense prediction tasks, e.g, tasks that require fine-grained\nspatial representations, we utilized the learned models for\nsemi-supervised video object segmentation on the DAVIS\n2017 (Pont-Tuset et al., 2017) dataset. We follow previ-\nous works (e.g Jabri et al. (2020); Caron et al. (2021)) and\nuse the pretrained model to extract frames features and use\npatch-level affinities between frames to track the first seg-\nmentation mask. We include video semi-supervised video-\nobject segmentation by tracking results in Table 3. We find\nthat StoP significantly improves over I-JEPA with determin-\nistic sinusoidal location features. For example, we observe\nan improvement of +2.5% in J&F using ViT-L.\nMethod\nArch.\nJ-Mean\nF-Mean\nJ&F Mean\nMAE\nViT-B/16\n49.4\n52.6\n50.9\nViT-L/16\n52.5\n54.3\n53.4\nViT-H/14\n54.0\n57.0\n55.5\nI-JEPA\nViT-B/16\n56.1\n56.2\n56.1\nViT-L/16\n56.1\n55.7\n55.9\nViT-H/14\n58.5\n60.9\n59.7\n+StoP\nViT-B/16\n56.6\n57.3\n57.0\nViT-L/16\n58.1\n58.7\n58.4\nViT-H/14\n58.9\n61.2\n60.1\nTable 3. Video objects semi-supervised segmentation. MIM with\nStoP learns features with a finer level of granularity. Results are\nreported on DAVIS 2017 dataset.\n4.2. Ablation Study\nOur primary focus is to evaluate the effectiveness of StoP.\nTo demonstrate this, we assess various design options us-\ning ViT-B architecture for the encoder and predictor. We\npre-train for 300 epochs on IN-1k based on the I-JEPA (As-\nsran et al., 2023) MIM model. We then assessed the linear\nprobing performance on IN-1k using only 1% of the labels.\nStoP compared to deterministic positional embeddings.\nThe most common choices for positional embeddings for\nVision Transformers are sine-cosine location features (also\nused in MAE, I-JEPA) and learned positional embedding.\nWe evaluate the MIM downstream performance using each\nof these options and using StoP (see Table 6). The results in-\ndicate that using StoP improves the performance by +3.2%\ncompared to sinusoidal and learned positional embeddings.\nLearned vs. predefined covariance matrix. To confirm\nthat learning the covariance matrix \u03a3 = \u03c3AAT (and specif-\nically A) is beneficial compared to using a predefined co-\nvariance matrix, we compare to stochastic positional em-\nbeddings with a predefined covariance matrix \u03a3 = \u03c3I,\nwithout any learning. We compare both options using dif-\nferent \u03c3 hyperparameter values. Figure 3 indicates that it\nis advantageous to learn \u03a3 rather than use fixed parameters.\n5\nStochastic positional embeddings improve masked image modeling\nMethod\nArch.\nCIFAR100\nPlaces205\niNat18\nCLEVR/Count\nCLEVR/Dist\ndata2vec\nViT-L/16\n81.6\n54.6\n28.1\n85.3\n71.3\nMAE\nViT-B/16\n68.1\n49.2\n26.8\n86.6\n70.8\nViT-L/16\n77.4\n54.4\n33.0\n92.1\n73.0\nViT-H/14\n77.3\n55.0\n32.9\n90.5\n72.4\nI-JEPA\nViT-B/16\n69.2\n53.4\n43.4\n82.2\n70.7\nViT-L/16\n83.6\n56.5\n48.4\n85.6\n71.2\nViT-H/14\n87.5\n58.4\n47.6\n86.7\n72.4\n+StoP\nViT-B/16\n81.2\n54.3\n44.7\n83.7\n71.3\nViT-L/16\n84.7\n57.2\n49.2\n85.7\n70.2\nViT-H/14\n87.7\n58.4\n50.9\n88.0\n72.5\nTable 4. Linear-probe transfer for various downstream tasks. Linear-evaluation on downstream image classification, object counting,\nand depth ordering tasks. Using StoP instead of sinusoidal deterministic positions leads to improvements on all tasks. E.g, +3.3% on\niNAT18 and +1.3% on Counting.\nMethod\nEpochs\nTop-1\nSine Cosine\n600\n69.4\nStoP (ours)\n600\n71.7\nTable 5. Finetuning results over IN-1k with 1% labels. StoP\nsignificantly improves finetuning performance compared to using\nsine-cosine positional embeddings. Using ViT-L/16 architecture.\nFigure 3. Learned vs. predefined stochastic positions. Using\nthe learned covariance matrix as in StoP, e.g, \u03a3 = \u03c3AAT leads\nto +3.5% improvement compared to smaller gains with a fixed\ncovariance matrix \u03a3 = \u03c3I. Accuracy is reported based on linear\nprobing evaluation using 1% of the data from IN-1k.\nOur findings show that setting the hyperparameter value to\n\u03c3 = 0.25 leads to an improvement of 3.5% points compared\nto deterministic positional embeddings (\u03c3 = 0).\nApplication of StoP to different tokens. We apply StoP to\ncontext and/or masked tokens. The results in Table 7 con-\nfirm our design choice, showing that StoP is most beneficial\nwhen it is applied solely to masked tokens, compared to\ncontext tokens, or both masked and context tokens.\n4.3. Analysis\nTo explain how StoP affects MIM, we analyze the learned\nmodel weights, visualize the stochastic positional embed-\ndings, and visualize the predicted features.\nMethod\nTop-1\nSine Cosine\n54.3\nLearned Pos. Embedding\n54.4\nStochastic Positions (StoP)\n57.8\nTable 6. Different positional embeddings. Linear probing on\nIN-1K using only 1% of the labels. Stochastic Positions (StoP)\noutperforms other common deterministic variants by 3.3%.\n0.0\n0.2\n0.4\n0.6\nnoise standard deviation \n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized L1\n A (weight matrix)\n m (masked token)\nFigure 4. Increasing \u03c3 induces regularization. Changing the\nprior \u03c3 (where \u03a3 = \u03c3AAT ) induces regularization over A and in-\ncreases the norm of the masked token, which preserves the masked\ntoken information in comparison to the added noise.\nStoP induces regularization. The matrix A is used to\nproject both noise tokens and context embedding tokens.\nWe hypothesize that StoP implicitly regularizes A. To test\nthis hypothesis we train models using StoP changing only\nthe hyperparam \u03c3 (see Figure 4). We find that increasing the\nvalue of \u03c3 leads to a decrease in the norm of A, which can\nbe viewed as regularization. On the other hand, increasing\n\u03c3 leads to an increase in the norm of the masked token bias\n\u02dcm. We speculate that the masked token bias increases in\nscale to prevent losing its information relative to the noise.\nTo further analyze this phenomenon, we train additional\nmodels while applying l1 or l2 regularization on A while\nkeeping the positional embeddings of masked tokens de-\nterministic. We find that StoP leads to +2% improvement\n6\nStochastic positional embeddings improve masked image modeling\nMethod\nTop-1\nNo Noise (Sine Cosine)\n54.3\nContext tokens only\n55.1\nMasked + context tokens\n56.8\nMasked tokens only\n57.8\nTable 7. Applying noise to different tokens. Applying learned\nnoise to context and/or masked tokens positional embeddings (sine-\ncosine). Reporting linear evaluation accuracy (using 1% of IN-1k).\nMethod\nTop-1\nSine Cosine\n54.3\nx2 Low res (bilinear resize)\n52.1\nx2 Low res (max pooling)\n54.1\nStochastic Positions (StoP)\n57.8\nTable 8. Low resolution prediction. Performance of StoP com-\npared to models that predict features on lower scales via max\npooling or bilinear resizing. Reporting linear evaluation accuracy\n(using 1% of IN-1k). StoP performs better than low res prediction.\nover l1 and +2.1% over l2 regualrization. Therefore, we\nconclude that StoP is superior to simple regularization.\nStochastic positional embedding visualization. To vi-\nsualize how StoP affects the similarity between different\npositions, we plot the similarity matrix between a stochastic\nposition embedding query and the predefined sine-cosine\ndeterministic positions (Figure 5). With StoP, we find that\nquery locations are more similar to a wider range of neigh-\nboring locations. Building on this observation, we train\nmodels to investigate if directly predicting lower-scale fea-\ntures is beneficial. We trained models to predict features in\nboth the original scale and a downscaled version by a factor\nof 2, using bilinear resizing and max pooling for downscal-\ning. However, we found that predicting lower scale features\ndoes not improve performance (see Table 8).\nPrediction visualization. We include heatmap visualiza-\ntion to visualize the similarity of a predicted token to all\nother tokens within the same image (see Figure 6). For a\ngiven image, mask, and a masked patch of interest, we apply\ncosine similarity between the predicted patch and all other\ntoken representations within the same image, followed by\na softmax. For I-JEPA with sine-cosine positional embed-\ndings, the visualization indicates that adjacent tokens tend\nto share similar features, implying a correlation between the\nfeatures and spatial location. In contrast, StoP produces pre-\ndictions correlated with non-neighboring small areas. We\nspeculate that using StoP leads to learning features that are\nmore semantic and prevents overfitting to location features.\nFigure 5. Similarity matrices of deterministic and stochastic\npositional embedding (StoP) to a query position. Each row\nrepresents the similarity given a different query position. StoP\nleads to a spatially smooth similarity matrix, thereby making it\nhard to distinguish the exact location of a given patch.\n5. Related Work\nMasked image modeling (MIM). There is a significant\nbody of research exploring visual representation learning\nby predicting corrupted sensory inputs. Denoising autoen-\ncoders (Vincent et al., 2010), for example, use random noise\nas input corruption, while context encoders (Pathak et al.,\n2016) regress an entire image region based on its surround-\ning. The idea behind masked image modeling (He et al.,\n2021; Xie et al., 2021; Bao et al., 2021) has emerged as\na way to address image denoising. In this approach, a\nVision Transformer (Dosovitskiy et al., 2020) is used to\nreconstruct missing input patches. The Masked Autoen-\ncoders (MAE) architecture (He et al., 2021), for example,\nefficiently reconstructs missing patches in pixel space and\nachieves strong performance on large labeled datasets. Other\napproaches, such as BEiT (Bao et al., 2021), predict a la-\ntent code obtained using a pretrained tokenizer. However,\npixel-level pre-training has been shown to outperform BEiT\nin fine-tuning. SimMiM (Xie et al., 2021) explores sim-\nple reconstruction targets like color clusters but shows no\nsignificant advantages over pixel space reconstruction. Re-\ncently, Image-JEPA (I-JEPA) (Assran et al., 2023; LeCun,\n2022) was proposed as a non-generative approach for self-\nsupervised learning of semantic image representations. I-\nJEPA predicts the representations of various target blocks\nin an image from a single context block to guide it toward\n7\nStochastic positional embeddings improve masked image modeling\nFigure 6. Feature visualization. We plot the similarity between\nthe predicted features of a given patch (marked in white within the\nmasked black area) and other features in the same image. Using\nStoP produces features that are less location based compared to I-\nJEPA baseline that have strong correlation with the target location.\nproducing semantic representations. Our approach builds\non this line of work and we propose to deal with location\nuncertainty using stochastic positional embeddings which\nwas not explored before.\nPositional Embeddings in Transformers. One of the core\ncomponents of the Transformer architecture (Vaswani et al.,\n2017) is the Self-Attention block, which is a permutation in-\nvariant function, e.g, changing the order of the input tokens\ndoes not change the function output. Consequently, it is\nnecessary to feed input tokens together with their positional\nembedding to describe their location. Absolute positional\nembeddings like fixed 2D sinusoidal features (Bello et al.,\n2019) or learned location features are the prevalent type of\npositional embeddings for the Vision Transformer (Doso-\nvitskiy et al., 2020). Relative positional embeddings have\nrecently gained popularity in NLP due to their ability to\naddress the gap between the training and testing sequence\nlength (Su et al., 2021; Chu et al., 2021; Press et al., 2021).\nFor example, (Press et al., 2021) proposed ALiBi to bias\nself-attention to assign higher confidence to neighboring lo-\ncations, and SPE (Liutkus et al., 2021) proposed a stochastic\napproximation for relative positional embedding in linear\ntransformers. Differently, we propose StoP to tackle loca-\ntion uncertainties in MIM, and it can be easily applied on\ntop of any existing deterministic variant.\nInvariance-based methods. These methods incorporate a\nloss that encourages similarity between augmented views of\nthe the same image while avoiding a trivial solution. For ex-\nample, contrastive learning prevents collapse by introducing\nnegative examples (Hadsell et al., 2006; Dosovitskiy et al.,\n2014; Chen et al., 2020a; He et al., 2019; Chen et al., 2020b;\nDwibedi et al., 2021). This can be achieved using a memory\nbank of previous instances (Wu et al., 2018; Oord et al.,\n2018; Tian et al., 2019; Misra & van der Maaten, 2020).\nHowever, there are also non-contrastive solutions that have\nbeen proposed. Of particular interest, a momentum encoder\nhas been shown to prevent collapse even without negative\npairs (Grill et al., 2020; Caron et al., 2021; Salakhutdi-\nnov & Hinton, 2007). Other methods include stopping the\ngradient to one branch (Chen & He, 2021) or applying\nregularization using batch statistics (Zbontar et al., 2021;\nBardes et al., 2021; 2022; Ermolov et al., 2020; Hua et al.,\n2021). MoCo v3 (Chen et al., 2021), then DINO (Caron\net al., 2021) extended these approaches for Vision Trans-\nformer, and iBOT (Zhou et al., 2021) proposed to add a MIM\nloss to DINO. These approaches perform extremely well\non ImageNet linear-probing, yet they rely on batch statis-\ntics, struggle under non-uniform distributions (Assran et al.,\n2022), and require hand-crafted image augmentations (Xiao\net al.). Our approach is based on MIM that requires less\nassumptions on batch statistics or handcrafted invariances.\n6. Limitations\nWe applied StoP to I-JEPA which performs image recon-\nstruction in the feature space. However, our attempts to ap-\nply StoP to MIM that use pixel based reconstruction, mainly\nMAE, were not successful. We speculate that adding StoP\nto MAE might make pixel reconstruction too difficult to\nachieve. Additionally, StoP tackles location uncertainty but\nnot appearance uncertainty, which we believe is implicitly\nmodeled by reconstructing tokens in feature space. Also,\nwhen modeling stochastic positions it may might be possi-\nble to condition the noise on the input image, namely the\ncontext tokens. We leave this extension for future work.\nLastly, while combining StoP with MIM shows significant\nimprovements, invariance-based approaches still perform\nslightly better (e.g, iBOT, DINO) than MIM approaches.\n7. Conclusion\nIn this work, we proposed to use stochastic positional em-\nbedding (StoP) to tackle location uncertainty in MIM. By\nconditioning on stochastic masked token positions, our\nmodel learns features that are more robust to location uncer-\ntainty. The effectiveness of this approach is demonstrated\non various datasets and downstream tasks, outperforming\nexisting MIM methods and highlighting its potential for\nself-supervised learning. Based on our experiments and\nvisualizations, modeling location uncertainties with StoP\nreduces overfitting to location features.\n8\nStochastic positional embeddings improve masked image modeling\nReferences\nAssran, M., Balestriero, R., Duval, Q., Bordes, F., Misra,\nI., Bojanowski, P., Vincent, P., Rabbat, M., and Ballas,\nN. The hidden uniform cluster prior in self-supervised\nlearning. arXiv preprint arXiv:2210.07277, 2022.\nAssran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent,\nP., Rabbat, M., LeCun, Y., and Ballas, N. Self-supervised\nlearning from images with a joint-embedding predictive\narchitecture. arXiv preprint arXiv:2301.08243, 2023.\nBao, H., Dong, L., and Wei, F. Beit: Bert pre-training of\nimage transformers. arXiv preprint arXiv:2106.08254,\n2021.\nBardes, A., Ponce, J., and LeCun, Y. Vicreg: Variance-\ninvariance-covariance regularization for self-supervised\nlearning. arXiv preprint arXiv:2105.04906, 2021.\nBardes, A., Ponce, J., and LeCun, Y.\nVicregl: Self-\nsupervised learning of local visual features.\narXiv\npreprint arXiv:2210.01571, 2022.\nBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V.\nAttention augmented convolutional networks. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision, pp. 3286\u20133295, 2019.\nCaron, M., Touvron, H., Misra, I., J\u00b4egou, H., Mairal, J.,\nBojanowski, P., and Joulin, A.\nEmerging properties\nin self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. preprint arXiv:2002.05709, 2020a.\nChen, X. and He, K. Exploring simple siamese represen-\ntation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n15750\u201315758, 2021.\nChen, X., Fan, H., Girshick, R., and He, K. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020b.\nChen, X., Xie, S., and He, K. An empirical study of train-\ning self-supervised vision transformers. arXiv preprint\narXiv:2104.02057, 2021.\nChu, X., Tian, Z., Zhang, B., Wang, X., Wei, X., Xia, H.,\nand Shen, C. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882, 2021.\nDosovitskiy, A., Springenberg, J. T., Riedmiller, M. A., and\nBrox, T. Discriminative unsupervised feature learning\nwith convolutional neural networks. In NIPS, 2014.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\nDwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., and\nZisserman, A. With a little help from my friends: Nearest-\nneighbor contrastive learning of visual representations. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 9588\u20139597, 2021.\nErmolov, A., Siarohin, A., Sangineto, E., and Sebe, N.\nWhitening for self-supervised representation learning. In\nInternational Conference on Machine Learning, 2020.\nGoyal, P., Duval, Q., Reizenstein, J., Leavitt, M., Xu, M.,\nLefaudeux, B., Singh, M., Reis, V., Caron, M., Bo-\njanowski, P., Joulin, A., and Misra, I. Vissl. https://\ngithub.com/facebookresearch/vissl, 2021.\nGrill, J.-B., Strub, F., Altch\u00b4e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020.\nHadsell, R., Chopra, S., and LeCun, Y. Dimensionality\nreduction by learning an invariant mapping. 2006 IEEE\nComputer Society Conference on Computer Vision and\nPattern Recognition (CVPR\u201906), 2:1735\u20131742, 2006.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. arXiv preprint arXiv:1911.05722, 2019.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00b4ar, P., and Girshick, R.\nMasked autoencoders are scalable vision learners. arXiv\npreprint arXiv:2111.06377, 2021.\nHua, T., Wang, W., Xue, Z., Ren, S., Wang, Y., and Zhao, H.\nOn feature decorrelation in self-supervised learning. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pp. 9598\u20139608, October\n2021.\nJabri, A., Owens, A., and Efros, A. Space-time correspon-\ndence as a contrastive random walk. Advances in neural\ninformation processing systems, 33:19545\u201319560, 2020.\nJohnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei,\nL., Lawrence Zitnick, C., and Girshick, R. Clevr: A\ndiagnostic dataset for compositional language and ele-\nmentary visual reasoning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2901\u20132910, 2017.\n9\nStochastic positional embeddings improve masked image modeling\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. Technical report, 2009.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nLeCun, Y. A path towards autonomous machine intelligence\nversion 0.9. 2, 2022-06-27. 2022.\nLiutkus, A., C\u00b4\u0131fka, O., Wu, S.-L., Simsekli, U., Yang, Y.-H.,\nand Richard, G. Relative positional encoding for trans-\nformers with linear complexity. In Meila, M. and Zhang,\nT. (eds.), Proceedings of the 38th International Confer-\nence on Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pp. 7067\u20137079. PMLR,\n18\u201324 Jul 2021. URL https://proceedings.mlr.\npress/v139/liutkus21a.html.\nMisra, I. and van der Maaten, L. Self-supervised learning\nof pretext-invariant representations. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 6707\u20136717, 2020.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nPathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and\nEfros, A. A.\nContext encoders: Feature learning by\ninpainting. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2536\u20132544,\n2016.\nPont-Tuset, J., Perazzi, F., Caelles, S., Arbel\u00b4aez, P., Sorkine-\nHornung, A., and Van Gool, L. The 2017 davis chal-\nlenge on video object segmentation.\narXiv preprint\narXiv:1704.00675, 2017.\nPress, O., Smith, N. A., and Lewis, M. Train short, test\nlong: Attention with linear biases enables input length\nextrapolation. arXiv preprint arXiv:2108.12409, 2021.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. Imagenet large scale\nvisual recognition challenge. International Journal of\nComputer Vision, 115(3):211\u2013252, 2015.\nSalakhutdinov, R. and Hinton, G. Learning a nonlinear em-\nbedding by preserving class neighbourhood structure. In\nArtificial Intelligence and Statistics, pp. 412\u2013419. PMLR,\n2007.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu,\nY. Roformer: Enhanced transformer with rotary position\nembedding. arXiv preprint arXiv:2104.09864, 2021.\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. In European Conference on Computer Vision,\n2019.\nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C.,\nShepard, A., Adam, H., Perona, P., and Belongie, S. The\ninaturalist species classification and detection dataset. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 8769\u20138778, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998\u20136008, 2017.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol,\nP.-A., and Bottou, L. Stacked denoising autoencoders:\nLearning useful representations in a deep network with\na local denoising criterion. Journal of machine learning\nresearch, 11(12), 2010.\nWu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised\nfeature learning via non-parametric instance discrimina-\ntion. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 3733\u20133742, 2018.\nXiao, T., Wang, X., Efros, A. A., and Darrell, T. What\nshould not be contrastive in contrastive learning. In Inter-\nnational Conference on Learning Representations.\nXie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q.,\nand Hu, H. Simmim: A simple framework for masked\nimage modeling. arXiv preprint arXiv:2111.09886, 2021.\nZbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.\nBarlow twins: Self-supervised learning via redundancy\nreduction. arXiv preprint arXiv:2103.03230, 2021.\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S.,\nNeumann, M., Dosovitskiy, A., Beyer, L., Bachem, O.,\nTschannen, M., Michalski, M., Bousquet, O., Gelly, S.,\nand Houlsby, N. A large-scale study of representation\nlearning with the visual task adaptation benchmark, 2019.\nURL https://arxiv.org/abs/1910.04867.\nZhou, B., Lapedriza, A., Xiao, J., Torralba, A., and\nOliva, A.\nLearning deep features for scene recog-\nnition using places database.\nIn Ghahramani, Z.,\nWelling, M., Cortes, C., Lawrence, N., and Wein-\nberger, K. (eds.), Advances in Neural Information\nProcessing Systems, volume 27. Curran Associates,\nInc.,\n2014a.\nURL\nhttps://proceedings.\nneurips.cc/paper/2014/file/\n3fe94a002317b5f9259f82690aeea4cd-Paper.\npdf.\n10\nStochastic positional embeddings improve masked image modeling\nZhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva, A.\nLearning deep features for scene recognition using places\ndatabase. Advances in neural information processing\nsystems, 27, 2014b.\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.,\nand Kong, T. Ibot: Image bert pre-training with online\ntokenizer. arXiv preprint arXiv:2111.07832, 2021.\n11\nStochastic positional embeddings improve masked image modeling\nAppendix\nA. Noise collapse and weight tying\nConsider the following loss function where nj \u223c N(0, \u03c3I).:\nJ = \u03a3i,jEnj[(F(Anj + \u03c8j + \u02dcm, Bxi) \u2212 yj)2]\n(8)\nProposition A.1. If A, B are different set of parameters then dJ\ndA\n\f\f\nA=0 = 0\nProof.\n\u2202J\n\u2202A =\nX\ni,j\nEnj[ \u2202\n\u2202A\u2225F(Anj + \u03c8j + \u02dcm, Bxi) \u2212 yj\u22252]\n=\nX\ni,j\nEnj[2(F(Anj + \u03c8j + \u02dcm, Bxi) \u2212 yj)\u2202F(Anj + \u03c8j + \u02dcm, Bxi)\n\u2202(Anj + \u03c8j + \u02dcm)\nnT\nj ]\nSet A = 0, then derivative becomes:\n\u2202J\n\u2202A\n\f\f\f\nA=0 = 2\nX\ni,j\n(F(\u03c8j + \u02dcm, Bxi) \u2212 yj)\u2202F(\u03c8j + \u02dcm, Bxi)\n\u2202(\u03c8j + \u02dcm)\nEnj[nT\nj ] = 0\nDefine the following the loss with weight tying and the deterministic loss without noise:\nJtied(A) = J(A, A) =\nX\ni,j\nEnj[(F(Anj + \u03c8j + \u02dcm, Axi) \u2212 yj)2]\n(9)\nJdet(B) = J(A = 0, B) =\nX\ni,j\n[(F(\u03c8j + \u02dcm, Bxi) \u2212 yj)2]\n(10)\nProposition A.2. If dJtied\ndA\n\f\f\nA=0 = 0 iff dJdet(B)\ndB\n\f\f\f\nB=0 = 0\nProof. Next, we show that A = 0 is a critical point of Jtied iff B = 0 is a critical point of Jdet:\n\u2202Jtied\n\u2202A\n\f\f\f\nA=0 =\nX\ni,j\n(F(\u03c8j + \u02dcm, 0) \u2212 yj)\u2207F(\u03c8i, 0)xT\ni\n(11)\n\u2202Jdet\n\u2202B\n\f\f\f\nB=0 =\nX\ni,j\n(F(\u03c8j + \u02dcm, 0) \u2212 yj)\u2207F(\u03c8j, 0)xT\ni\n(12)\nTherefore \u2202Jtie\n\u2202A\n\f\f\f\nA=0 = 0 iff \u2202Jdet\n\u2202B\n\f\f\f\nB=0\nB. Optimal Predictor\nConsider a random variable X (corresponding to the context in our case. For simplicity assume X is just the positional\nembedding of the context) that is used to predict a variable Y (corresponding to the target in our case). But now instead of\npredicting from X, we use a noise variable Z that is independent of both X, Y , and provide the predictor with only the noisy\n12\nStochastic positional embeddings improve masked image modeling\nresult R = g(X, Z). Here g is some mixing function (in our case g(x, z) = x + z). We next derive the optimal predictor\nf(R) in this case. Formally we want to minimize:\nER,Y [(f(R) \u2212 Y )2]\n(13)\nA classic result in estimation is that this is optimized by the conditional expectation f(r) = E[Y |R = r].\nWe simplify this as follows:\nE[Y |R = r]\n=\nX\nx,y\nyp(Y = y, X = x|R = r)\n=\nX\nx,y\nyp(y|X = x)p(X = x|R = r)\n=\nX\nx\nE[Y |X = x]p(X = x|R = r)\nwhere in the second line we used the fact that:\np(y, x|r) = p(y|x, r)p(x|r) = p(y|x)p(x|r)\n(14)\nTo further illustrate, consider the case where z is Gaussian with zero mean and unit variance. Then p(x|r) is also Gaussian\nwith expectation r, and the expression above amounts to convolution of the clean expected values with a Gaussian:\nE[Y |R = r] =\nZ\nx\nE[Y |X = x]\n1\n\u221a\n2\u03c0 e\u22120.5(x\u2212r)2dx\n(15)\nC. Experiments and Results\nWe include the full implementation details, pretraining configs and evaluation protocols for the Ablations (see Appendix C.1),\nDownstream Tasks (Appendix C.2), as well as full results and comparisons to invariance-based methods.\nC.1. Ablations\nHere we pretrain all models for 300 epochs using 4 V100 nodes, on a total batch size of 2048. In all the ablation study\nexperiments, we follow the exact recipe of (Assran et al., 2023). We include the full config in Table 9 for completeness.\nTo evaluate the pretrained models, we use linear probing evaluation using 1% of IN-1k (Russakovsky et al., 2015). To\nobtain the features of an image, we apply the target encoder over the image to obtain a sequence of tokens corresponding\nto the image. We then average the tokens to obtain a single representative vector. The linear classifier is trained over this\nrepresentation, maintaining the rest of the target encoder layers fixed.\nC.2. Downstream Tasks\nHere we pretrain I-JEPA with StoP for 600 epochs using 4 V100 nodes, on a total batch size of 2048 using ViT-B (see\nconfig in Table 10) and ViT-L (see config in Table 11). For ViT-H we use float16 and train for 300 epochs and follow the\nconfig in Table 12. We follow similar configs compared to (Assran et al., 2023) except we usually use a lower learning rate.\nIntuitively, since StoP is stochastic it is more sensitive to high learning rates.\nFor evaluation on downstream tasks, we use the features learned by the target-encoder and follow the protocol of\nVISSL (Goyal et al., 2021) that was utilized by I-JEPA (Assran et al., 2023). Specifically, we report the best linear\nevaluation number among the average-pooled patch representation of the last layer and the concatenation of the last 4 layers\nof the average-pooled patch representations. We report full results including comparisons to invariance-based methods for\nIN-1k linear evaluation Table 14, 1% IN-1k finetuning results in Table 16, and other downstream tasks in Table 13.\nFor baselines that use Vision Transformers (Dosovitskiy et al., 2020) with a [cls] token (e.g, iBOT (Zhou et al., 2021),\nDINO (Caron et al., 2021) or MAE (He et al., 2021)), we use the default configurations of VISSL (Goyal et al., 2021) to\nevaluate the publicly available checkpoints on iNaturalist18 (Van Horn et al., 2018), CIFAR100 (Krizhevsky et al., 2009),\nClevr/Count (Johnson et al., 2017; Zhai et al., 2019), Clevr/Dist (Johnson et al., 2017; Zhai et al., 2019), and Places205 (Zhou\n13\nStochastic positional embeddings improve masked image modeling\net al., 2014b). Following the evaluation protocol of VISSL (Goyal et al., 2021), we freeze the encoder and return the best\nnumber among the [cls] token representation of the last layer and the concatenation of the last 4 layers of the [cls]\ntoken.\nFor semi-supervised video object segmentation, we propagate the first labeled frame in a video using the similarity between\nadjacent frames features. To label the video using the frozen features, we follow the code and hyperparams of (Caron et al.,\n2021). To evaluate the segmented videos, we use the evaluation code of DAVIS 2017 (Pont-Tuset et al., 2017) and include\nfull results in Table 15.\nconfig\nvalue\noptimizer\nAdamW\nepochs\n300\nlearning rate\n1e\u22123\nweight decay\n(0.04, 0.4)\nbatch size\n2048\nlearning rate schedule\ncosine decay\nwarmup epochs\n15\nencoder arch.\nViT-B\npredicted targets\n4\npredictor depth\n6\npredictor attention heads\n12\npredictor embedding dim.\n384\n\u03c3 (noise hyperparam)\n0.25\nTable 9. Pretraining setting for ablations. Using ViT-B en-\ncoder, trained for 300 epochs, config strictly follows (Assran\net al., 2023).\nconfig\nvalue\noptimizer\nAdamW\nepochs\n600\nlearning rate\n8e\u22124\nweight decay\n(0.04, 0.4)\nbatch size\n2048\nlearning rate schedule\ncosine decay\nwarmup epochs\n15\nencoder arch.\nViT-B\npredicted targets\n4\npredictor depth\n6\npredictor attention heads\n12\npredictor embedding dim.\n384\n\u03c3 (noise hyperparam)\n0.25\nTable 10. Pretraining setting for downstream tasks (ViT-B).\nAll models trained for 600 epochs.\nconfig\nvalue\noptimizer\nAdamW\nepochs\n600\nlearning rate\n8e\u22124\nweight decay\n(0.04, 0.4)\nbatch size\n2048\nlearning rate schedule\ncosine decay\nwarmup epochs\n15\nencoder arch.\nViT-L\npredicted targets\n4\npredictor depth\n12\npredictor attention heads\n16\npredictor embedding dim.\n384\n\u03c3 (noise hyperparam)\n0.25\nTable 11. Pretraining setting for downstream tasks (ViT-L).\nAll models trained for 600 epochs.\nconfig\nvalue\noptimizer\nAdamW\nepochs\n600\nlearning rate\n1e\u22123\nweight decay\n(0.04, 0.4)\nbatch size\n2048\nlearning rate schedule\ncosine decay\nwarmup epochs\n40\nencoder arch.\nViT-H\npredicted targets\n4\npredictor depth\n12\npredictor attention heads\n16\npredictor embedding dim.\n384\n\u03c3 (noise hyperparam)\n0.2\nTable 12. Pretraining setting for downstream tasks (ViT-H).\nTrained for 300 epochs.\n14\nStochastic positional embeddings improve masked image modeling\nMethod\nArch.\nCIFAR100\nPlaces205\niNat18\nCLEVR/Count\nCLEVR/Dist\nInvariance-based methods (use extra image augmentations)\nDINO\nViT-B/16\n84.8\n55.2\n50.1\n83.2\n53.4\niBOT\nViT-B/16\n85.5\n56.7\n50.0\n62.1\n64.6\nViT-L/16\n88.3\n60.4\n57.3\n85.7\n62.8\nMasked Image Modeling Methods\ndata2vec\nViT-L/16\n81.6\n54.6\n28.1\n85.3\n71.3\nMAE\nViT-B/16\n68.1\n49.2\n26.8\n86.6\n70.8\nViT-L/16\n77.4\n54.4\n33.0\n92.1\n73.0\nViT-H/14\n77.3\n55.0\n32.9\n90.5\n72.4\nI-JEPA\nViT-B/16\n69.2\n53.4\n43.4\n82.2\n70.7\nViT-L/16\n83.6\n56.5\n48.4\n85.6\n71.2\nViT-H/14\n87.5\n58.4\n47.6\n86.7\n72.4\n+StoP\nViT-B/16\n81.2\n54.3\n44.7\n83.7\n71.3\nViT-L/16\n84.7\n57.2\n49.2\n85.7\n70.2\nViT-H/14\n87.7\n58.4\n50.9\n88.0\n72.5\nTable 13. Linear-probe transfer for various downstream tasks. Linear-evaluation on downstream image classification, object counting,\nand tracking tasks. StoP significantly outperforms previous MIM methods that don\u2019t utilize image augmentations like I-JEPA and MAE,\nand decreases the gap with the best invariance-based methods that utilize data augmentations during pretraining.\nMethod\nArch.\nEpochs\nTop-1\nInvariance-based methods (use extra image augmentations)\nSimCLR v2\nRN152 (2\u00d7)\n800\n79.1\nBYOL\nRN200 (2\u00d7)\n800\n79.6\nDINO\nViT-B/16\n400\n78.1\nViT-B/8\n300\n80.1\nMoCo v3\nViT-B/16\n300\n76.7\nViT-BN-L/7\n300\n81.0\nMSN\nViT-L/7\n200\n80.7\niBOT\nViT-B/16\n250\n79.8\nViT-L/16\n250\n81.0\nMasked Image Modeling methods\ndata2vec\nViT-L/16\n1600\n77.3\nMAE\nViT-B/16\n1600\n68.0\nViT-L/16\n1600\n75.8\nViT-H/14\n1600\n77.2\nI-JEPA\nViT-B/16\n600\n72.9\nViT-L/16\n600\n77.5\nViT-H/14\n300\n79.3\n+StoP (ours)\nViT-B/16\n600\n74.5\nViT-L/16\n600\n78.5\nViT-H/14\n300\n79.6\nTable 14. Linear-evaluation on IN-1k. Performance of invariance\nbased and MIM approaches.\nMethod\nArch.\nJ-Mean\nF-Mean\nJ&F Mean\nInvariance-based methods (use extra image augmentations)\nDINO\nViT-B/16\n60.7\n63.9\n62.3\niBOT\nViT-B/16\n60.9\n63.3\n62.1\nViT-L/16\n61.7\n63.9\n62.8\nMasked Image Modeling Methods\nMAE\nViT-B/16\n49.4\n52.6\n50.9\nViT-L/16\n52.5\n54.3\n53.4\nViT-H/14\n54.0\n57.0\n55.5\nI-JEPA\nViT-B/16\n56.1\n56.2\n56.1\nViT-L/16\n56.1\n55.7\n55.9\nViT-H/14\n58.5\n60.9\n59.7\n+StoP\nViT-B/16\n56.6\n57.3\n57.0\nViT-L/16\n58.1\n58.7\n58.4\nViT-H/14\n58.9\n61.2\n60.1\nTable 15. Video objects semi-supervised segmentation.\nMIM and Invarianced-based methods. Results reported on\nDAVIS 2017 dataset.\nMethod\nArch.\nEpochs\nTop-1\nInvariance-based methods (use extra image augmentations)\nDINO\nViT-B/8\n300\n70.0\niBOT\nViT-B/16\n400\n69.7\nMasked Image Modeling methods\nMAE\nViT-L/16\n1600\n67.0\nI-JEPA\nViT-L/16\n600\n69.4\n+StoP (ours)\nViT-L/16\n600\n71.7\nTable 16. Finetuning results over ImageNet with 1% labels.\nComparison of MIM and invariance-based methods.\n15\n"
  },
  {
    "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
    "link": "https://arxiv.org/pdf/2308.00113.pdf",
    "upvote": "13",
    "text": "Three Bricks to Consolidate\nWatermarks for Large Language Models\nPierre Fernandez1,3, Antoine Chaffin1,2, Karim Tit1, Vivien Chappelier2, Teddy Furon1\n1Centre Inria de l\u2019Universit\u00b4e de Rennes\n2Imatag\n3FAIR, Meta\nAbstract\u2014Discerning between generated and natural texts is\nincreasingly challenging. In this context, watermarking emerges\nas a promising technique for ascribing text to a specific generative\nmodel. It alters the sampling generation process to leave an invis-\nible trace in the output, facilitating later detection. This research\nconsolidates watermarks for large language models based on\nthree theoretical and empirical considerations. First, we introduce\nnew statistical tests that offer robust theoretical guarantees\nwhich remain valid even at low false-positive rates (less than\n10-6). Second, we compare the effectiveness of watermarks using\nclassical benchmarks in the field of natural language processing,\ngaining insights into their real-world applicability. Third, we\ndevelop advanced detection schemes for scenarios where access\nto the LLM is available, as well as multi-bit watermarking.\nIndex Terms\u2014Watermarking, Large Language Model\nI. INTRODUCTION\nThe misuse of Large Language Models (LLMs) like Chat-\nGPT [1], Claude [2], or the open-sourced LLaMA [3] may be-\ncome a threat as their availability and capabilities expand [4]\u2013\n[6]. LLMs might help generate fake news by reducing costs to\nspread disinformation at scale [7], [8], with a potential impact\non public opinion and democratic outcomes [9]. They could\nhelp impersonate people, facilitate scams [10], or make student\nassessments impossible. Enforcing fair and responsible usage\nthrough regulations and technical means would be useful.\nMonitoring the usage of LLMs with passive forensics is\ndifficult because generated texts are hardly distinguishable\nfrom real ones, be it for humans or algorithms [11], [12].\nWatermarking is a promising technique explored for generative\nimage models [13]\u2013[15] and generative text LLMs [16]\u2013[19].\nIn this case, watermarking either alters the sample generation\nprocess [16], [19] or changes the probability distribution of\nthe generated tokens [17], [20], to leave an imperceptible\ntrace in the generated text. This literature then describes a\ndetection mechanism analyzing the generated tokens to see if\ntheir distribution follows the one induced by the watermark.\nWe introduce three contributions to consolidate the current\nliterature, one for each of the following paragraphs and sec-\ntions. Each part can be read independently.\nFirst, false positives can have serious consequences in con-\ntexts where the integrity and accuracy of results are essential,\nsuch as falsely accusing a user of producing fake news or a stu-\ndent of cheating in an exam. However, current approaches [17],\n[18] focus their study on sensitivity (True Positive Rate: TPR)\nMail: pierre.fernandez@inria.fr - Code: facebookresearch/three bricks/\nx(0) x(1)\u2026 x(h) x(h+1)... x(T) \nx(-C)... x(-h)... x(-2) x(-1)\nLLM\nlogits l = ( l1\u2026lV  )\nHashing\nSeed RNG\nSample\nLLM context\nWatermark context\ns1\nScore\nHashing\nSeed RNG\nWatermark context\nCurrent token\nx(0)\nGeneration\nDetection\nFig. 1: General illustration of watermarking for LLM (top: generation,\nbottom: detection). Details and notations in Sect. II-B.\nrather than on specificity (linked to False Positive Rate: FPR).\nThe FPR has never been empirically checked at interesting\nscales (with more than 1k negative examples). Our large-scale\nexperiments reveal that hypotheses of previous works do not\nhold and that their detection thresholds largely underestimate\nthe false positives at low FPR. This work provides grounded\nstatistical tests that theoretically guarantee false positive-rates\nand accurate p-values in real-world regimes. We validate them\nempirically and show that they provide a close-to-perfect\ncontrol of the FPR, even at low values (< 10\u22126).\nSecond, we compare the watermarking methods, analyzing\npractical implications of watermarks on traditional Natural\nLanguage Processing (NLP) benchmarks. Indeed, current wa-\ntermark evaluation mainly considers the deviation from the\noriginal LLM distribution, for example using perplexity. This\nis in contrast with the LLM litterature, where models are\nrather evaluated on their effective usefulness, e.g. free-form\ncompletion tasks such as question answering. Such evaluations\nare much more informative on the actual abilities of the model\nwhen used on downstream tasks.\nThird, we expand these algorithms to advanced detection\nschemes. When access to the LLM is possible at detection\ntime, we provide optimal statistical tests. We also investigate\nmulti-bit watermarking (hiding binary messages as water-\nmarks) when current approaches only tackle zero-bit water-\nmarking. This allows not only to determine whether the text\nwas generated by the watermarked LLM, but also to identify\nwhich version of the model generated it.\narXiv:2308.00113v2  [cs.CL]  8 Nov 2023\nII. TECHNICAL BACKGROUND\nA. Large Language Models (LLMs)\nLLMs are neural networks that generate text by computing\nthe likelihood of generating a sequence of tokens given a\ncontext [21]. This paper focuses on decoder-only models,\na.k.a. auto-regressive LLMs. The tokens are pieces of words\nfrom a vocabulary V. From a context x(\u2212C), ..., x(\u22121), the\nmodel estimates the probability of each token of V being the\nnext. It computes a vector \u2113 \u2208 R|V| of logits, transformed into\n\u0010\nP\n\u0010\nX(0) = x\n\f\fx(\u2212C), . . . , x(\u22121)\u0011\u0011\nx\u2208V = softmax(\u2113; \u03b8) (1)\nwhere \u03b8 is a temperature. The generation of a sequence\nfrom the context samples a token from this distribution, then\nappends it to the context and iterates the process. Various\nsampling schemes exist: greedy search, beam search, top-k\nsampling [22], [23], nucleus-sampling (top-p) [24], etc.\nB. Watermarking Text Generation\n1) Modification of the Distribution [17], [18], [20]: The\noriginal distribution (1), denoted p for short, is replaced by a\nsimilar distribution q = F(p, k) where k is a secret key and F\nan arbitrary function. In the work of Kirchenbauer et al. [17],\nthe secret key determines a partitioning of V = Gk \u222a \u00afGk. The\ngreenlist Gk contains \u03b3|V| tokens, where \u03b3 \u2208 [0, 1]. The logit\nof every token in the greenlist is incremented by \u03b4 > 0, and\nthe softmax operator outputs q. The sampling then proceeds\nas usual. Intuitively, this increases the probabilty of generating\ngreenlist tokens. On the other hand, E[F(p, K)] = p so on\nexpectation over the set of cryptographic keys, watermarking\ndoes not bias the global distribution of words (K being the\nrandom variable representing the key).\nThe detection tokenizes the text and counts how many\ntokens are in their greenlist. More formally, for a text of T\ntokens, the score ST is the number of greenlist tokens (x(t)\nand k(t) respectively indicate the tth token and key):\nST =\nT\nX\nt=1\n1\n\u0010\nx(t) \u2208 Gk(t)\n\u0011\n.\n(2)\n2) Modification of the Sampling [16], [19]: The watermark\nembedding replaces the traditional sampling schemes by a de-\nterministic process. For instance, Aaronson et al. [16] choose\nthe next token by computing x(0) = arg maxv\u2208V r1/pv\nv\n, where\np1 is the distribution (1) and r \u2208 [0, 1]|V| a secret vector\ngenerated from the secret key k. Intuitively, this encourages\nthe generation of tokens that have both high rv and pv\nvalues. It also presents the interesting property that \u2200v \u2208 V,\nP(X(0) = v) = pv over the randomness of the secret vector,\nwhen distributed uniformly over [0, 1]|V| (demonstration in\nApp. A). In other words, this watermarking does not bias the\ndistribution on expectation over the secret vector.\nThe detection computes the following score for T tokens:\nST = \u2212\nT\nX\nt=1\nln\n\u0010\n1 \u2212 r(t)\nx(t)\n\u0011\n.\n(3)\n1(Nucleus sampling can be applied before generating p)\nC. Quality-Robustness Trade-off\nFor both methods we can trade off generation quality against\nrobustness by varying the watermarking strength. In [17],\nincreasing the \u03b4 parameter increases the generation of green\ntokens at the risk of including unlikely tokens. In [16],\nincreasing the temperature \u03b8 has the same effect, since it\nflattens the probability vector (1), thus diminishing the relative\nimportance of pv over rv.\nD. Key Management\nThe secret key k giving birth to the greenlist Gk in [17] or to\nthe sampling of r in [16] must have a wide diversity. A fixed\nkey causes security issues and biases the text generation. One\npossibility is to make it dependent of the time t as proposed\nin [19]. The secret key is then different from one token to\nanother. Yet, this brings synchronization issue at the detection\nstage (e.g. when a sentence is deleted). A common practice\nensuring self-synchronization - illustrated in Fig. 1 - makes\nthe key dependent of the window of h previous tokens: k(t) =\nH(x(t\u22121), . . . , x(t\u2212h), k), where H is a cryptographic hash\nfunction and k the master key. This secret is the seed that\ninitializes a random number generator (RNG) at time t. In turn,\nthe RNG is used to generate the greenlist Gk(t) or to sample\nr(t). The width of this window defines a trade-off between\ndiversity of the key and robustness of the watermarking. In\nthe specific case where h = 0, the key is the same for all\ntokens (k(t) = k), which makes the watermarking particularly\nrobust to text editing [25].\nE. Z-Test\nThe detection tests the hypothesis H0: \u201cthe text is natural\u201d\n(human written or written without watermark), against H1:\n\u201cthe text has been generated with watermark\u201d.\nCurrent approaches [16], [17] approximate the underlying\ndistribution of the score ST by using a Z-test. This statistical\nhypothesis test determines whether a sample mean differs\nsignificantly from its expectation when the standard deviation\nis known. It computes the so-called Z statistics:\nZ = ST /T \u2212 \u00b50\n\u03c30/\n\u221a\nT\n,\n(4)\nwhere \u00b50 and \u03c30 are the expectation and standard deviation\nper token under the null hypothesis H0, i.e. when the analyzed\ntext is not watermarked. The Z-test is typically used for large\nsample sizes assuming a normal distribution under the null\nhypothesis thanks to the central limit theorem. This assumption\nis key for computing the p-value, i.e. the probability of\nobserving a value of Z at least as extreme as the one observed\nz, under the null hypothesis:\np-value(z) = P(Z > z|H0) = 1 \u2212 \u03a6(z),\n(5)\nwhere \u03a6 is the cumulative distribution function of the normal\ndistribution. At detection time, we fix a false positive rate\n(FPR) and flag the text as watermarked if p-value(z) < FPR.\n10\n8\n10\n6\n10\n4\n10\n2\n100\nTheoretical FPR\n10\n8\n10\n6\n10\n4\n10\n2\n100\nEmpirical FPR\nAaronson et al.\nKirchenbauer et al.\n(a) Z-tests\n10\n8\n10\n6\n10\n4\n10\n2\n100\nTheoretical FPR\n10\n8\n10\n6\n10\n4\n10\n2\n100\nEmpirical FPR\nAaronson et al.\nKirchenbauer et al.\n(b) Tests of III-B\n10\n6\n10\n4\n10\n2\n100\nTheoretical FPR\n10\n6\n10\n4\n10\n2\n100\nEmpirical FPR\nAaronson et al.\nKirchenbauer et al.\n(c) Tests of III-B, rectified with III-C\n0.0\n0.5\n1.0\np-value\n0\n1\n2\n3\n4\n5\nDensity\nKirchenbauer et al.\n0.0\n0.5\n1.0\np-value\nAaronson et al.\nContext width\nwhen seeding:\n \n0\n1\n2\n4\n6\n8\nFig. 2: Empirical checks of false positive rates for different watermarks and values of the context width h. Results are computed over 10\nmaster keys \u00d7 100k sequences of 256 tokens sampled from Wikipedia. We compare three detection tests: (Left) using Z-tests; (Middle)\nusing new statistical tests presented in III-B; (Right) using the new statistical tests with the rectified scoring strategy of III-C. Theoretical\nvalues do not hold in practice for Z-tests, even for high values of h, and empirical FPRs do not match theoretical ones. This is solved by\nbasing detection on grounded statistical tests and analytic p-values, as well as by revising the scoring strategy.\nIII. RELIABILITY OF THE DETECTION\nIn this section, large-scale evaluations of the FPR show\na gap between theory and practice. It is closed with new\nstatistical tests and by rectifying the scoring method.\nA. Empirical Validation of FPR with Z-Scores\nSo far, the FPR has been checked on only around 500\nnegative samples [17], [18], [20]. We scale this further and\nselect 100k texts from multilingual Wikipedia to cover the\ndistribution of natural text. We tokenize with LLaMA\u2019s tok-\nenizer, and take T = 256 tokens/text. We run detection tests\nwith varying window length h when seeding the RNG. We\nrepeat this with 10 different master keys, which makes 1M\ndetection results under H0 for each method and h value. For\nthe detection of the greenlist watermark, we use \u03b3 = 0.25.\nFig. 2a compares empirical and theoretical FPRs. Theoret-\nical guarantees do not hold in practice: the empirical FPRs\nare much higher than the theoretical ones. We also observed\nthat distributions of p-values were not uniform (which should\nbe the case under H0). Besides, the larger the watermarking\ncontext window h, the closer we get to theoretical guarantees.\nIn pratice, one would need h >> 8 to get reliable p-values,\nbut this makes the watermarking method less robust to attacks\non generated text because it hurts synchronization.\nB. New Non-Asymptotical Statistical Tests\nThe Gaussian assumption of Z-tests breaks down for short\nor repetitive texts. Here are non-asymptotical tests for both\nmethods that reduce the gap between empirical and theoretical\nFPR, especially at low FPR values as shown in Fig. 2.\n1) Kirchenbauer et al. [17]: Under H0, we assume that the\nevent x(t) \u2208 Gk(t) occurs with probability \u03b3, and that these\nevents are i.i.d. Therefore, ST (2) is distributed as a binomial\nof parameters T and \u03b3. Consider a text under scrutiny whose\nscore equals s. The p-value is defined as the probability of\nobtaining a score higher than s under H0:\np-value(s) = P(ST > s|H0) = I\u03b3(s, T \u2212 s + 1),\n(6)\nbecause S \u223c B(T, \u03b3) whose c.d.f. is expressed by Ix(a, b) the\nregularized incomplete Beta function.\n2) Aaronson et al. [16]: Under H0, we assume that the\ntext under scrutiny and the secret vector are independent, so\nthat rx(t)\ni.i.d.\n\u223c\nU(0, 1). Therefore, ST (3) follows a \u0393(T, 1)\ndistribution. The p-value associated to a score s reads:\np-value(s) = P(ST > s|H0) = \u0393(T, s)\n\u0393(T) ,\n(7)\nwhere \u0393 is the upper incomplete gamma function. Under H1,\nthe score is expected to be higher as proven in App. A, so the\np-value is likely to be small.\nC. Rectifying the Detection Scores\nEven with grounded statistical tests, empirical FPRs are still\nhigher than theoretical ones. In fact, Kirchenbauer et al. [17]\nmention that random variables are only pseudo-random since\nrepeated windows generate the same secret. This can happen\neven in a short text and especially in formatted data. For\ninstance in a bullet list, the sequence of tokens \\n\\n*_ repeats\na lot as shown in Fig. 3. Repetition pulls down the assumption\nof independence necessary for computing the p-values.\nWe experimented with two simple heuristics mitigating this\nissue at the detection stage. The first one takes into account a\ntoken only if the watermark context window has not already\nbeen seen during the detection. The second scores the tokens\nfor which the h + 1-tuple formed by {watermark context +\ncurrent token} has not already been seen. Note, that the latter\nis present in [17], although without ablation and without being\noth ri ps \\n\n\\n\nN es oth ri ps is a genus of th ri ps in the family Ph la e\noth rip idae . \\n\n\\n\n## Species \\n\n\\n\n* N es oth ri ps a lex andra e \\n\n* N es oth ri ps a or ist us \\n\n* N es oth ri ps ar to car pi \\n\n* N es oth ri ps bad ius \\n\n* N es oth ri ps bar row i \\n\n* N es oth ri ps bre vic oll is \\n\n* N es oth ri ps brig al owi \\n\n* N es oth ri ps cap ric orn is \\n\n* N es oth ri ps car ver i \\n\n* N es oth ri ps co or ong i \\n\n* N es oth ri ps dou l li \\n\n* N es oth ri ps east op i \\n\n* N es oth ri ps f od inae \\n\n* N es oth ri ps hem id is cus \\n\n* N es oth ri ps l ativ ent ris \\n\n* N es oth ri ps lever i \\n\n* N es oth ri ps major \\n\n* N es oth ri ps mal ac ca e \\n\n* N es oth ri ps minor \\n\n* N es oth ri ps n iger \\n\n* N es oth ri ps n ig ris et is \\n\n* N es oth ri ps\nFig. 3: Typical example of a vanilla text with low p-value because\nof repeated tokens. It is 10\u221221, using the greenlist watermark with\n\u03b3 = 1/4 and h = 2 on 256 tokens (we only show half of the text).\nTABLE I: Performances on classical free-form generation benchmarks when completion is done with watermarking. h is the watermark\ncontext width. We report results for methods: Aaronson et al. [16] / Kirchenbauer et al. [17]. \u201c-\u201d means no watermarking.\nGSM8K\nHuman Eval\nMathQA\nMBPP\nNQ\nTQA\nAverage\nModel\nh\n7B\n-\n10.3\n12.8\n3.0\n18.0\n21.7\n56.9\n20.5\n1\n10.3 / 11.1\n12.8 / 9.8\n2.9 / 2.8\n18.2 / 16.0\n21.8 / 19.5\n56.9 / 55.3\n20.5 / 19.1\n4\n10.4 / 10.8\n12.8 / 9.2\n3.0 / 2.8\n17.8 / 16.4\n21.8 / 20.2\n56.9 / 55.1\n20.4 / 19.1\n13B\n-\n17.2\n15.2\n4.3\n23.0\n28.2\n63.6\n25.3\n1\n17.2 / 17.3\n15.2 / 14.6\n4.3 / 3.6\n22.8 / 21.2\n28.2 / 25.1\n63.6 / 62.2\n25.2 / 24.0\n4\n17.2 / 16.8\n15.2 / 15.9\n4.2 / 4.1\n22.6 / 21.2\n28.2 / 24.5\n63.6 / 62.2\n25.2 / 24.1\n30B\n-\n35.1\n20.1\n6.8\n29.8\n33.5\n70.0\n32.6\n1\n35.3 / 35.6\n20.7 / 20.7\n6.9 / 7.5\n29.6 / 28.8\n33.5 / 31.6\n70.0 / 69.0\n32.7 / 32.2\n4\n35.1 / 34.1\n20.1 / 22.6\n6.9 / 7.0\n29.8 / 28.8\n33.5 / 31.6\n70.0 / 68.7\n32.6 / 32.1\nused in further experiments. Of the two, the second one is\nbetter since it counts more ngrams, and thus has better TPR.\nIt can also deal with the specific case of h = 0.\nFigure 2c reports empirical and theoretical FPRs when\nchoosing not to score already seen h + 1-tuples. They now\nmatch perfectly, except for h = 0 where the FPR is still\nslightly underestimated. In short, we guarantee FPR thanks\nto new statistical tests and by scoring only tokens for which\n{watermark context + current token} has not been scored.\nIV. WATERMARK EVALUATION\nThis section introduces evaluation with the revised statistical\ntests, and investigate the impact of LLM watermarking on\nclassical NLP benchmarks.\nA. Robustness Analysis\nWe now compare watermarking methods by analyzing the\nTPR when detecting watermarked texts. For detection, we\nemploy the previous statistical tests and scoring strategy.\nWe flag a text as watermarked if its p-value is lower than\n10\u22125 ensuring an FPR=10\u22125. For these experiments, we stay\nclose to a chatbot scenario. We prompt Guanaco-7b [26], an\ninstruction fine-tuned version of LLaMA, with the first 1k\nprompts from the Alpaca dataset [27]. For generation, we\nuse top-p sampling with p = 0.95, and in the case of [17]\na temperature \u03b8 = 0.8 and \u03b3 = 1/4. We simulate synonym\nattacks by randomly replacing tokens with probability 0.3\n(other attacks are studied in related work [18]).\nTab. II reports the TPR for different strength of the water-\nmark (see Sect. II-C), and the S-BERT [28] similarity score\nbetween the generated texts with and without watermarking\nto measure the semantic distortion induced by the watermark.\nResults in Tab. II reveals different behaviors. For instance,\n[17] has a finer control over the trade-off between watermark\nstrength and quality. Its TPR values ranges from 0.0 to 0.9,\nwhile [16] is more consistent but fails to achieve TPR higher\nthan 0.8 even when the S-BERT score is degraded a lot.\nThe watermark context width also has a big influence.\nWhen h is low, we observed that repetitions happen more\noften because the generation is easily biased towards certain\nrepetitions of tokens. It leads to average S-BERT scores below\n0.5 and unusable completions. On the other hand, low h also\nmakes the watermark more robust, especially for [17]. It is\nalso important to note that h has an influence on the number\nTABLE II: Robustness analysis of the watermarks, with rectified\nstatistical tests. We report the TPR@FPR=10\u22125 and the S-BERT\nscores over 10 \u00d7 1k completions, for different hyperparameters\ncontroling the strength of the watermark (\u03b4 in [17] and \u03b8 in [16]\n- see Sect. II-C). The \u2018TPR aug.\u2019 is the TPR when texts are attacked\nbefore detection by randomly replacing tokens with probability 0.3.\nAaronson et al. [16]\nKirchenbauer et al. [17]\nh\nMetric\n\u03b8 : 0.8\n0.9\n1.0\n1.1\n\u03b4 : 1.0\n2.0\n3.0\n4.0\n0\nS-BERT\n0.60\n0.56\n0.52\n0.44\n0.63\n0.61\n0.57\n0.50\nTPR\n0.20\n0.31\n0.42\n0.51\n0.00\n0.16\n0.58\n0.70\nTPR aug.\n0.04\n0.06\n0.09\n0.10\n0.00\n0.02\n0.20\n0.39\n1\nS-BERT\n0.62\n0.61\n0.59\n0.55\n0.63\n0.62\n0.60\n0.56\nTPR\n0.35\n0.51\n0.66\n0.77\n0.02\n0.41\n0.77\n0.88\nTPR aug.\n0.04\n0.10\n0.20\n0.36\n0.00\n0.05\n0.30\n0.58\n4\nS-BERT\n0.62\n0.62\n0.61\n0.59\n0.62\n0.62\n0.60\n0.57\nTPR\n0.43\n0.59\n0.71\n0.80\n0.02\n0.44\n0.76\n0.88\nTPR aug.\n0.01\n0.02\n0.06\n0.18\n0.00\n0.00\n0.03\n0.14\nof analyzed tokens since we only score tokens for which the\nh + 1-tuple has not been seen before (see Sect. III-C). If h is\nhigh, almost all these tuples are new, while if h is low, the\nchance of repeated tuples increases. For instance in our case,\nthe average number of scored tokens is around 100 for h = 0,\nand 150 for h = 1 and h = 4.\nB. Impact of Watermarks on Free-Form Generation Tasks\nPrevious studies measure the impact on quality using distor-\ntion metrics such as perplexity or similarity score as done in\nTab. II. However, such metrics are not informative of the utility\nof the model for downstream tasks [24], where the real interest\nof LLMs lies. Indeed, watermarking LLMs could be harmful\nfor tasks that require very precise answers. This section rather\nquantifies the impact on typical NLP benchmarks, in order to\nassess the practicality of watermarking.\nLLMs are typically evaluated either by comparing samples\nof plain generation to a set of target references (free-form\ngeneration) or by comparing the likelihood of a predefined set\nof options in a multiple choice question fashion. The latter\nmakes little sense in the case of watermarking, which only\naffects sampling. We therefore limit our evaluations to free-\nform generation tasks. We use the evaluation setup of LLaMA:\n1) Closed-book Question Answering (Natural Questions [29],\nTriviaQA [30]): we report the 5-shot exact match performance;\n2) Mathematical reasoning (MathQA [31], GSM8k [32]), we\nreport exact match performance without majority voting; 3)\nCode generation (HumanEval [33], MBPP [34]), we report\nthe pass@1 scores. For [17], we shift logits with \u03b4 = 1.0\nbefore greedy decoding. For [16], we apply top-p at 0.95 to\nthe probability vector, then apply the watermarked sampling.\nTab. I reports the performance of LLaMA models on the\naforementioned benchmarks, with and without the watermark\nand for different window size h. The performance of the LLM\nis not significantly affected by watermarking. The approach of\nKirchenbauer et al. (II-B1) is slightly more harmful than the\none of Aaronson et al. (II-B2), but the difference w.r.t. the\nvanilla model is small. Interestingly, this difference decreases\nas the size of the model increases: models with higher genera-\ntion capabilities are less affected by watermarking. A possible\nexplanation is that the global distribution of the larger models\nis better and thus more robust to small perturbations. Overall,\nevaluating on downstream tasks points out that watermarking\nmay introduce factual errors that are not well captured by\nperplexity or similarity scores.\nV. ADVANCED DETECTION SCHEMES\nThis section introduces improvements to the detection\nschemes of Sect. III. Namely, it develops a statistical test when\naccess to the LLM is granted, as well as multi-bit decoding.\nA. Neyman-Pearson and Simplified Score Function\nThe following is specific for the scheme of Aaronson et al.\n[16] (a similar work may be conducted with [18]). Under H0,\nwe have rv \u223c U[0,1], whereas rv \u223c Beta(1/pv, 1) under H1\n(see Corollary (14) in App. A). The optimal Neyman-Pearson\nscore function is thus:\nST =\nT\nX\nt=1\nln fH1(rx(t))\nfH0(rx(t)) =\nT\nX\nt=1\n\u0012\n1\npx(t) \u2212 1\n\u0013\nln(rx(t)) + A\nwhere A doesn\u2019t depend on r and can thus be discarded. There\nare two drawbacks: (1) detection needs the LLM to compute\npx(t), (2) there is no close-form formula for the p-value.\nThis last point may be fixed by resorting to a Chernoff\nbound, yet without guarantee on its tightness: p-value(s) \u2264\ne\nP\nt ln\n\u03bbt\n\u03bbt+c \u2212cs, with c solution of P\nt(c + \u03bbt)\u22121 = \u2212s and\n\u03bbt = px(t)/(1 \u2212 px(t)). Experiments show that this detection\nyields extremely low p-values for watermarked text, but they\nare fragile: any attack increases them to the level of the original\ndetection scheme (3), or even higher because generated logits\nare sensitive to the overall LLM context. An alternative is to\nremove weighting:\nST =\nT\nX\nt=1\nln (rx(t)) ,\n(8)\nwhose p-value is given by: p-value(s) =\n\u03b3(T,\u2212s)\n\u0393(T ) . In our\nexperiments, this score function does not match the original\ndetection presented in [16].\nB. Multi-bit Watermarking\n1) Theory: It is rather easy to turn a zero-bit watermarking\nscheme into multi-bit watermarking, by associating a secret\nkey per message. The decoding runs detection with every key\nand the decoded message is the one associated to the key\ngiving the lowest p-value p. The global p-value becomes 1 \u2212\n(1 \u2212 p)M, where M is the number of possible messages.\nAlgorithm 1 Multi-bit watermarking for LLMs\nRequires: model LLM, secret\u2019s dimension d = max(M, |V|),\nwatermark context width h, message m \u2208 {0, . . . , M \u2212 1}\nGeneration (one step):\nlogits \u2113 \u2190 LLM\n\u0010\nx(\u2212C), . . . , x(\u22121)\u0011\nseed \u2190 Hash(x(\u2212h), . . . , x(\u22121))\nr \u2190 RNGseed(d)\nr(m) \u2190 CyclicShift(r, m) = (rm, .., rd, r0, .., rm\u22121)\nx(0) \u2190 Sample(\u2113, r(m)1,...,|V|)\nIdentification:\nS \u2190 0d\nfor t \u2208 {h, . . . , T}:\nseed \u2190 Hash(x(t\u2212h), . . . , x(t\u22121))\nr(t) \u2190 RNGseed(d)\nS \u2190 S + CyclicShift(f(r(t)), x(t))\np \u2190 p-value(S1,...,M)\nm \u2190 argmin(p)\np \u2190 1 \u2212 (1 \u2212 pm)M\nRunning detection for M keys is costly, since it requires M\ngenerations of the secret vector. This is solved by imposing\nthat the secret vectors of the messages m \u2208 {0, . . . , M \u2212 1}\nare crafted as circular shifts of m indices of r = r(0):\nr(m) = CyclicShift(r, m)\n= (rm, rm+1, .., rd, r0, .., rm\u22121) .\nGenerating r as a d-dimensional vector, with d \u2265 |V|, we\nare able to embed M \u2264 d different messages, by keeping\nonly the first |V| dimensions of each circularly-shifted vector.\nThus, the number of messages may exceed the size of the\ntoken vocabulary |V|.\nBesides, the scoring functions (2) (3) may be rewritten as:\nST (m) =\nT\nX\nt=1\nf\n\u0010\nr(t)(m)\n\u0011\nx(t) ,\n(9)\nwhere f : Rd 7\u2192 Rd is a component-wise function, and\nx(t) is the selected token during detection. This represents the\nselection of f\n\u0000r(t)(m)\n\u0001\nat position x(t). From another point\nof view, if we shift f\n\u0000r(t)\u0001\nby x(t), the score for m = 0\nwould be its first component, m = 1 its second one, etc. We\nmay also write:\nST =\nT\nX\nt=1\nCyclicShift\n\u0010\nf\n\u0010\nr(t)\u0011\n, x(t)\u0011\n,\n(10)\nand the first M components of ST are the scores for each m.\nAs a side note, this is a particular case of the parallel\ncomputations introduced by Kalker et al. [35].\n2) Experiments: In a tracing scenario the message is the\nidentifier of a user or a version of the model. The goal is to\ndecide if any user or model generated a given text (detection)\nand if so, which one (identification). There are 3 types of\nerror: false positive: flag a vanilla text; false negative: miss\na watermarked text; false accusation: flag a watermarked text\nbut select the wrong identifier.\nWe simulate M \u2032=1000 users that generate 100 watermarked\ntexts each, using the Guanaco-7b model. Accuracy can then\nbe extrapolated beyond the M \u2032 identifiers by adding identifiers\nTABLE III: Identification accuracy for tracing users by watermarking.\nSequences are between 4 and 252 tokens long, and 149 on average.\nNumber of users M\n10\n102\n103\n104\nFPR= 10\u22123\nAaronson et al. [16]\n0.80\n0.72\n0.67\n0.62\nKirchenbauer et al. [17]\n0.84\n0.77\n0.73\n0.68\nFPR= 10\u22126\nAaronson et al. [16]\n0.61\n0.56\n0.51\n0.46\nKirchenbauer et al. [17]\n0.69\n0.64\n0.59\n0.55\nwith no associated text, for a total of M > M \u2032 users. Text\ngeneration uses nucleus sampling with top-p at 0.95. For [17],\nwe use \u03b4 = 3.0, \u03b3 = 1/4 with temperature \u03b8 at 0.8. For [16],\nwe use \u03b8 = 1.0. For both, the context width is h = 4. A text\nis deemed watermarked if the score is above a threshold set\nfor a given global FPR (see III). Then, the source is identified\nas the user with the lowest p-value.\nTab. III shows that watermarking enables identification\nbecause its performance is dissuasive enough. For example,\namong 105 users, we successfully identify the source of a\nwatermarked text 50% of the time while maintaining an FPR\nof 10\u22126 (as long as the text is not attacked). At this scale,\nthe false accusation rate is zero (no wrong identification\nonce we flag a generated text) because the threshold is set\nhigh to avoid FPs, making false accusations unlikely. The\nidentification accuracy decreases when M increases, because\nthe threshold required to avoid FPs gets higher. In a nutshell,\nby giving the possibility to encode several messages, we trade\nsome accuracy of detection against the ability to identify users.\nVI. CONCLUSION\nThis research offers theoretical and empirical insights that\nwere kept aside from the literature on watermarks for LLMs.\nNamely, existing methods resort to statistical tests which are\nbiased, delivering incorrect false positive rates. This is fixed\nwith grounded statistical tests and a revised scoring strategy.\nWe additionally introduce evaluation setups, and detection\nschemes to consolidate watermarks for LLMs. Further work\nmay investigate how to adapt watermarks for more complex\nsampling schemes (e.g. beam search as in [17]), since gener-\nation yield significantly better quality with these methods.\nOverall, we believe that watermarking is both reliable and\npractical. It already holds many promises as a technique for\nidentifying and tracing LLM outputs, while being relatively\nnew in the context of generative models.\nACKNOWLEDGMENTS\nWork supported by ANR / AID under Chaire SAIDA ANR-\n20-CHIA-0011. We also thank Thomas Scialom, Herv\u00b4e J\u00b4egou\nand Matthijs Douze for their insights throughout this work.\nREFERENCES\n[1] OpenAI, \u201cChatGPT: Optimizing language models for dialogue.,\u201d 2022.\n[2] AnthropicAI, \u201cIntroducing Claude,\u201d 2023.\n[3] H. Touvron et al., \u201cLlama: Open and efficient foundation language\nmodels,\u201d arXiv, 2023.\n[4] L. Weidinger et al., \u201cTaxonomy of risks posed by language models,\u201d in\nACM Conference on Fairness, Accountability, and Transparency, 2022.\n[5] E. Crothers, N. Japkowicz, and H. Viktor, \u201cMachine generated text: A\ncomprehensive survey of threat models and detection methods,\u201d arXiv,\n2022.\n[6] J. P. Cardenuto, J. Yang, R. Padilha, R. Wan, D. Moreira, H. Li, S. Wang,\nF. Andal\u00b4o, S. Marcel, and A. Rocha, \u201cThe age of synthetic realities:\nChallenges and opportunities,\u201d arXiv, 2023.\n[7] K. Kertysova, \u201cArtificial intelligence and disinformation: How AI\nchanges the way disinformation is produced, disseminated, and can be\ncountered,\u201d Security and Human Rights, vol. 29, no. 1-4, 2018.\n[8] S. Kreps, R. M. McCain, and M. Brundage, \u201cAll the news that\u2019s fit to\nfabricate: Ai-generated text as a tool of media misinformation,\u201d Journal\nof experimental political science, 2022.\n[9] E. Ku\u02c7sen and M. Strembeck, \u201cPolitics, sentiments, and misinformation:\nAn analysis of the twitter discussion on the 2016 austrian presidential\nelections,\u201d Online Social Networks and Media, 2018.\n[10] M. T. review, \u201cJunk websites filled with ai-generated text are pulling in\nmoney from programmatic ads,\u201d 2023.\n[11] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, \u201cAutomatic\ndetection of generated text is easiest when humans are fooled,\u201d arXiv,\n2019.\n[12] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn,\n\u201cDetectgpt: Zero-shot machine-generated text detection using probability\ncurvature,\u201d arXiv, 2023.\n[13] N. Yu, V. Skripniuk, D. Chen, L. Davis, and M. Fritz, \u201cResponsible\ndisclosure of generative models using scalable fingerprinting,\u201d in ICLR,\n2022.\n[14] P. Fernandez, G. Couairon, H. J\u00b4egou, M. Douze, and T. Furon, \u201cThe\nstable signature: Rooting watermarks in latent diffusion models,\u201d ICCV,\n2023.\n[15] Y. Wen, J. Kirchenbauer, J. Geiping, and T. Goldstein, \u201cTree-ring\nwatermarks: Fingerprints for diffusion images that are invisible and\nrobust,\u201d arXiv, 2023.\n[16] S. Aaronson and H. Kirchner, \u201cWatermarking GPT Outputs,\u201d 2023.\n[17] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein,\n\u201cA watermark for large language models,\u201d ICML, 2023.\n[18] J. Kirchenbauer, J. Geiping, Y. Wen, M. Shu, K. Saifullah, K. Kong,\nK. Fernando, A. Saha, M. Goldblum, and T. Goldstein, \u201cOn the\nreliability of watermarks for large language models,\u201d 2023.\n[19] M. Christ, S. Gunn, and O. Zamir, \u201cUndetectable watermarks for\nlanguage models,\u201d Cryptology ePrint Archive, 2023.\n[20] X. Zhao, P. Ananth, L. Li, and Y.-X. Wang, \u201cProvable robust water-\nmarking for ai-generated text,\u201d arXiv, 2023.\n[21] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA neural probabilistic lan-\nguage model,\u201d NeurIPS, vol. 13, 2000.\n[22] A. Fan, M. Lewis, and Y. Dauphin, \u201cHierarchical neural story genera-\ntion,\u201d arXiv, 2018.\n[23] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\n\u201cLanguage models are unsupervised multitask learners,\u201d OpenAI, 2019.\n[24] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, \u201cThe curious\ncase of neural text degeneration,\u201d arXiv, 2019.\n[25] X. Zhao, Y.-X. Wang, and L. Li, \u201cProtecting language generation models\nvia invisible watermarking,\u201d arXiv, 2023.\n[26] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, \u201cQlora:\nEfficient finetuning of quantized llms,\u201d arXiv, 2023.\n[27] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,\nP. Liang, and T. B. Hashimoto, \u201cStanford Alpaca: An instruction-\nfollowing LLaMA model,\u201d 2023.\n[28] N. Reimers and I. Gurevych, \u201cSentence-bert: Sentence embeddings using\nsiamese bert-networks,\u201d arXiv, 2019.\n[29] T. Kwiatkowski et al., \u201cNatural questions: a benchmark for question\nanswering research,\u201d Trans. of the ACL, vol. 7, 2019.\n[30] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, \u201cTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension,\u201d\narXiv, 2017.\n[31] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\nD. Song, and J. Steinhardt, \u201cMeasuring mathematical problem solving\nwith the math dataset,\u201d arXiv, 2021.\n[32] K. Cobbe et al., \u201cTraining verifiers to solve math word problems,\u201d arXiv,\n2021.\n[33] M. Chen et al., \u201cEvaluating large language models trained on code,\u201d\narXiv, 2021.\n[34] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\nE. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton, \u201cProgram synthesis\nwith large language models,\u201d 2021.\n[35] T. Kalker, G. Depovere, J. Haitsma, and M. Maes, \u201cVideo watermarking\nsystem for broadcast monitoring,\u201d in Proc. of SPIE, Security and\nWatermarking of Multimedia Contents, vol. 3657, 1999.\nAPPENDIX\nA. Demonstrations for [16]\n1) Sampling probability\nProposition. Consider a discrete distribution p = (p1, . . . , pV ) and V random variables R = (R1, . . . , RV ) s.t. Rv\niid\n\u223c U[0,1].\nLet V \u22c6 = arg maxv R1/pv\nv\n. Then P(V \u22c6 = v) = pv.\nProof. For any v \u2208 V, Rv\niid\n\u223c U[0,1] so, \u2212 ln(Rv) follows an exponential distribution E(1). Let Zv := \u2212 1\npv ln(Rv). By\nconstruction, Zv \u223c E(pv), with density fZv(z) = pve\u2212pv.z. We now have:\nV \u22c6 = arg max\nv\nR\n1\npv\nv\n= arg min\nv\nZv.\n(11)\nA well known result about exponential laws is that (see the-gumbel-trick for following lines):\nZ\n=\nmin\nv\nZv \u223c E\n X\nv\npv\n!\n= E (1) ,\n(12)\nP(V \u22c6 = v)\n=\npv\nP\nj pj\n= pv.\n(13)\nThis shows that for a given secret vector r, the watermarking chooses a word which may be unlikely (low probability\npV \u22c6). Yet, on expectation over the secret keys, ie. over r.v. R = (R1, . . . , RV ), the distribution of the chosen token follows\nthe distribution given by the LLM.\nCorollary. RV \u22c6 \u223c Beta(1/pV \u22c6, 1).\nProof.\nZ = ZV \u22c6 = \u2212 1\npV \u22c6 ln(RV \u22c6) \u223c E(1),\n(14)\nwhich translates to RV \u22c6 = e\u2212pV \u22c6E with E \u223c E(1), with p.d.f. fRV \u22c6 (r) = r\n1\npV \u22c6 \u22121\npV \u22c6\n. Therefore, RV \u22c6 \u223c Beta(1/pV \u22c6, 1).\n2) Detection\nWe denote by x(1), . . . , x(T ) the sequence of tokens in the text, by p(t) the probability vector output by the LLM and by\nR(t) \u2208 [0, 1]|V| the key random vector at time-step t. We define Rt := R(t)\nx(t) and pt := p(t)\nx(t) at time-step t. The score is\nST = \u2212 PT\nt=1 ln(1 \u2212 Rt).\nProposition (p-value under H0). The p-value associated to a score s is defined as:\np-value(s) = P(ST > s|H0) = \u0393(T, s)\n\u0393(T) ,\n(15)\nwhere \u0393(T, s) is the upper incomplete gamma function.\nProof. Under H0, the assumption is s.t. Rt\niid\n\u223c U[0,1]. Then, \u2212 ln(1 \u2212 Rt) follows an exponential distribution E(1). Therefore\nS \u223c \u0393(T, 1) (see sum of Gamma distributions). Therefore the p-value associated to a score s is\np-value(s) = 1 \u2212 \u03b3(T, s)\n\u0393(T)\n= \u0393(T, s)\n\u0393(T) ,\n(16)\nwhere \u0393(T, s) is the upper incomplete gamma function, \u03b3(T, s) is the lower incomplete gamma function.\nCorollary. Per token,\n\u00b50 = E(ST /T|H0) = 1,\n\u03c32\n0 = V(ST /T|H0) = 1/T.\n(17)\nProposition (Bound on expected score under H1). Under H1, E(ST ) \u2265 T +\n\u0012\u03c02\n6 \u2212 1\n\u0013\nHT , where HT = \u2212 PT\nt=1 pt ln(pt)\nis the entropy of the completion.\nProof. From (14), Rt = exp(\u2212ptE) with E \u223c E(1), so:\nE(S) = \u2212E\n\" T\nX\nt=1\nln(1 \u2212 exp(\u2212ptE))\n#\n= \u2212\nT\nX\nt=1\nZ \u221e\n0\nln(1 \u2212 e\u2212ptx)e\u2212xdx\n= \u2212\nT\nX\nt=1\nZ 1\n0\n1\npt\nr1/pt\u22121(\u2212 ln(1 \u2212 r))dr\n(by change of variable x = \u22121/pt ln(r) )\nThen, using integration by parts with u = 1 \u2212 r1/pt and v = ln(1 \u2212 r), the integral becomes:\n\u2212\nZ 1\n0\n1\npt\nr1/pt\u22121 ln(1 \u2212 r)dr =\nZ 1\n0\n1 \u2212 r1/pt\n1 \u2212 r\ndr = H1/pt\nwhere Hz is the z-th harmonic number also defined as Hz = P\u221e\nn=1\n1\nn \u2212\n1\nn+z. Therefore, we have:\n\u2212\nZ 1\n0\n1\npt\nr1/pt\u22121 ln(1 \u2212 r)dr =\n\u221e\nX\nn=1\n1\nn \u2212\n1\nn + 1/pt\n= 1 +\n\u221e\nX\nn=1\n1\nn + 1 \u2212\n1\nn + 1/pt\n.\nNow, \u2200n \u2208 N\u22c6, we have:\n(n + 1)2\n\u0012\n1\nn + 1 \u2212\n1\nn + 1/pt\n\u0013\n= (n + 1)(n + 1/pt) \u2212 (n + 1)2\nn + 1/pt\n=\n1 + n\n1/pt + n (1/pt \u2212 1)\n\u2265 \u2212 1 + n\n1/pt + n ln(pt)\n\u2265 \u2212 pt ln(pt).\nTherefore, by summing over all t \u2208 [1, T],\nE(S) \u2265 T +\n \u221e\nX\nn=1\n1\n(n + 1)2\n!  T\nX\nt=1\n\u2212pt ln(pt)\n!\n= T +\n\u0012\u03c02\n6 \u2212 1\n\u0013\nHT .\nProposition (Variance of score under H1). V(ST ) \u2264 T \u03c02\n6 .\nProof. For Rt \u223c Beta(1/pt, 1):\nV(ln(1 \u2212 Rt)) = \u03c81(1) \u2212 \u03c81(1 + 1/pt)\n(18)\nwhere \u03c81 is the trigamma function, which can be expressed as the following serie \u03c81(z) = P\u221e\nn=0 1/(n + z)2. Then\n\u03c81(1) = \u03c02/6 and \u03c81(1 + 1/pt) > 0, so that V(ln(1 \u2212 Rt)) \u2264 \u03c02/6. The results comes because the sampled tokens are\nindependent.\nB. Free-form evaluations\nWe provide in Table IV the full results of the free-form evaluations of the different models. This extends the results of\nTable I in the main paper. The models are evaluated with the same evaluation protocol as in LLaMA.\nTABLE IV\nGSM8K\nHuman Eval\nMathQA\nMBPP\nNQ\nTQA\nAverage\nModel\nWM Method\nh\n7B\nNone\n-\n10.31\n12.80\n2.96\n18.00\n21.72\n56.89\n20.45\nAaronson et al.\n0\n10.54\n12.80\n3.00\n18.00\n21.77\n56.88\n20.50\n1\n10.31\n12.80\n2.88\n18.20\n21.75\n56.87\n20.47\n2\n10.31\n12.80\n2.94\n18.00\n21.75\n56.86\n20.44\n3\n10.39\n12.80\n2.96\n18.20\n21.69\n56.85\n20.48\n4\n10.39\n12.80\n2.98\n17.80\n21.80\n56.88\n20.44\n6\n10.61\n12.80\n2.96\n18.00\n21.75\n56.86\n20.50\n8\n10.46\n12.80\n2.90\n18.20\n21.75\n56.85\n20.49\nKirchenbauer et al.\n0\n9.63\n12.80\n2.20\n16.20\n20.06\n55.09\n19.33\n1\n11.14\n9.76\n2.82\n16.00\n19.50\n55.30\n19.09\n2\n11.07\n6.71\n2.62\n16.00\n20.44\n55.07\n18.65\n3\n10.16\n10.98\n2.38\n14.40\n20.08\n55.65\n18.94\n4\n10.77\n9.15\n2.76\n16.40\n20.17\n55.14\n19.06\n6\n10.01\n9.76\n3.16\n17.00\n20.78\n54.90\n19.27\n8\n11.37\n11.59\n2.90\n16.40\n20.66\n55.36\n19.71\n13B\nNone\n-\n17.21\n15.24\n4.30\n23.00\n28.17\n63.60\n25.25\nAaronson et al.\n0\n17.29\n15.24\n4.24\n22.80\n28.17\n63.60\n25.22\n1\n17.21\n15.24\n4.30\n22.80\n28.20\n63.61\n25.23\n2\n17.51\n15.24\n4.20\n22.80\n28.20\n63.59\n25.26\n3\n17.44\n15.24\n4.22\n22.60\n28.23\n63.57\n25.22\n4\n17.21\n15.24\n4.20\n22.60\n28.20\n63.63\n25.18\n6\n16.98\n15.24\n4.28\n23.20\n28.23\n63.61\n25.26\n8\n17.21\n15.24\n4.22\n22.80\n28.20\n63.62\n25.22\nKirchenbauer et al.\n0\n14.33\n14.02\n3.04\n20.80\n24.32\n62.13\n23.11\n1\n17.29\n14.63\n3.62\n21.20\n25.12\n62.23\n24.02\n2\n16.45\n11.59\n3.54\n20.60\n25.54\n62.44\n23.36\n3\n17.06\n16.46\n3.58\n19.80\n25.90\n62.37\n24.20\n4\n16.76\n15.85\n4.08\n21.20\n24.49\n62.24\n24.10\n6\n15.85\n14.63\n4.00\n18.20\n26.32\n62.19\n23.53\n8\n17.29\n14.63\n3.68\n21.00\n25.46\n62.17\n24.04\n30B\nNone\n-\n35.10\n20.12\n6.80\n29.80\n33.55\n70.00\n32.56\nAaronson et al.\n0\n35.48\n20.12\n6.88\n29.80\n33.52\n69.98\n32.63\n1\n35.33\n20.73\n6.88\n29.60\n33.52\n70.03\n32.68\n2\n35.33\n20.73\n6.94\n30.00\n33.49\n70.00\n32.75\n3\n35.71\n20.73\n6.92\n30.00\n33.52\n70.02\n32.82\n4\n35.10\n20.12\n6.90\n29.80\n33.49\n70.01\n32.57\n6\n35.33\n20.73\n6.86\n29.80\n33.49\n69.98\n32.70\n8\n35.33\n20.73\n6.94\n30.00\n33.52\n70.01\n32.75\nKirchenbauer et al.\n0\n31.84\n21.95\n6.88\n28.40\n31.66\n69.03\n31.63\n1\n35.56\n20.73\n7.54\n28.80\n31.58\n68.98\n32.20\n2\n33.21\n17.07\n6.48\n27.40\n31.83\n69.44\n30.91\n3\n33.89\n24.39\n6.54\n27.80\n32.49\n69.22\n32.39\n4\n34.12\n22.56\n6.96\n28.80\n31.55\n68.74\n32.12\n6\n34.34\n24.39\n7.32\n29.80\n31.63\n69.08\n32.76\n8\n34.95\n20.12\n7.42\n27.20\n32.08\n69.31\n31.85\n"
  }
]