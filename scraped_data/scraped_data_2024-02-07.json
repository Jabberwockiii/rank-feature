[
  {
    "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
    "link": "https://arxiv.org/pdf/2402.03620.pdf",
    "upvote": "94",
    "text": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nPei Zhou 1 Jay Pujara 1 Xiang Ren 1 Xinyun Chen 2 Heng-Tze Cheng 2\nQuoc V. Le 2 Ed H. Chi 2 Denny Zhou 2 Swaroop Mishra 2 Huaixiu Steven Zheng 2\nAbstract\nWe introduce SELF-DISCOVER, a general frame-\nwork for LLMs to self-discover the task-intrinsic\nreasoning structures to tackle complex reasoning\nproblems that are challenging for typical prompt-\ning methods. Core to the framework is a self-\ndiscovery process where LLMs select multiple\natomic reasoning modules such as critical think-\ning and step-by-step thinking, and compose them\ninto an explicit reasoning structure for LLMs to\nfollow during decoding. SELF-DISCOVER sub-\nstantially improves GPT-4 and PaLM 2\u2019s per-\nformance on challenging reasoning benchmarks\nsuch as BigBench-Hard, grounded agent reason-\ning, and MATH, by as much as 32% compared\nto Chain of Thought (CoT). Furthermore, SELF-\nDISCOVER outperforms inference-intensive meth-\nods such as CoT-Self-Consistency by more than\n20%, while requiring 10-40x fewer inference com-\npute. Finally, we show that the self-discovered\nreasoning structures are universally applicable\nacross model families: from PaLM 2-L to GPT-4,\nand from GPT-4 to Llama2, and share commonal-\nities with human reasoning patterns.\n1. Introduction\nLarge Language Models (LLM) (Brown et al., 2020; Chowd-\nhery et al., 2022; OpenAI, 2023b; Anil et al., 2023) pow-\nered by transformers (Vaswani et al., 2017) have produced\nimpressive breakthroughs in generating coherent texts (Ope-\nnAI, 2022), and following instructions (Zhong et al., 2021;\nMishra et al., 2022c; Wei et al., 2021; Chung et al., 2022;\nOuyang et al., 2022). In pursuit of the goal to enhance\nLLMs\u2019 capability to reason and solve complex problems,\nvarious prompting methods have been proposed, drawing\ninspirations from cognitive theories of how humans rea-\n1University of Southern California 2Google DeepMind. Cor-\nrespondence to: Pei Zhou <peiz@usc.edu>, Swaroop Mishra\n<swaroopmishra@google.com>, Huaixiu Steven Zheng < steven-\nzheng@google.com>.\nPreprint.\nson. For example, few-shot and zero-shot chain-of-thought\n(CoT) (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022;\nYasunaga et al., 2023) resembles how humans solve prob-\nlems step-by-step, decomposition-based prompting (Zhou\net al., 2022a; Drozdov et al., 2022; Patel et al., 2022; Hao\net al., 2023; Khot et al., 2022) is inspired by how humans\nbreakdown a complex problem into a series of smaller\nsubproblems, and then solve those subproblems one by\none (Polya, 2004), and step-back prompting (Zheng et al.,\n2023) is motivated by how humans reflect on task nature\nto derive general principles. However, a fundamental limi-\ntation is that each technique itself serves as an atomic rea-\nsoning module making an implicit prior assumption of the\nprocess on how to tackle a given task. Instead, we argue\nthat each task has a unique intrinsic structure underlying\nthe reasoning process involved in solving it efficiently. For\ninstance, least-to-most prompting (Zhou et al., 2022a; Droz-\ndov et al., 2022) has shown to be much more effective than\nCoT (Wei et al., 2022) at solving tasks such as symbolic\nmanipulation and compositional generalization, due to the\ndecomposition structure of the tasks.\nThis paper aims at self-discovering the underlying reasoning\nstructure unique to each task, while being highly efficient in\nterms of computation. Our approach, SELF-DISCOVER, is\ninspired by how humans internally devise a reasoning pro-\ngram for problem-solving (Newell et al., 1958; Rasmussen,\n1983), as illustrated in Figure 2 . From a set of atomic\nreasoning modules described in natural language such as\n\u201cbreakdown into sub tasks\u201d and \u201ccritical thinking\u201d, an LLM,\nand task examples without labels, SELF-DISCOVER com-\nposes a coherent reasoning structure intrinsic to the task\n(Stage 1) and then solves instances of the task using the\ndiscovered structure (Stage 2). Stage 1 operates at the task-\nlevel and uses three actions to guide the LLM to generate\na reasoning structure for the task. At Stage 2, during the\nfinal decoding, the LLM simply follows the self-discovered\nstructure to arrive at the final answer.\nSolving problems using SELF-DISCOVER brings several\nbenefits compared to other methods for LLM reasoning.\nFirst, the discovered reasoning structure is grounded in\natomic reasoning modules benefiting from the strengths of\nmultiple reasoning modules in contrast to applying a priori\nmodule such as CoT. Second, SELF-DISCOVER is efficient\n1\narXiv:2402.03620v1  [cs.AI]  6 Feb 2024\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTask\nChain-of-Thought \n(CoT)\nDirect Answer\nAnswer\nSelf-Discover \nReasoning \nStructures (Ours)\nAnswer\nTask\nTask\nTask-Speci\ufb01c \nReasoning Structure\nRationale\nAnswer\nStructured \nReasoning\nAvg. BBH: +11%\nT4D: + 39%\nMATH: +5.5%\nAvg. BBH: +7%\nT4D: + 29%\nMATH: +8.5%\nFigure 1. SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve\nchallenging tasks. Through testing on challenging reasoning benchmarks incuding Big Bench-Hard (BBH), agent reasoning (T4D), and\nMATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM\n2-L. Full BBH results are in Appendix C Table 3.\nin computation as it only requires 3 more inference steps on\nthe task-level, while being more performant than inference-\nheavy ensemble approaches such as self-consistency (Wang\net al., 2022). Lastly, the discovered reasoning structure\nis intrinsic to the task, and conveys LLMs\u2019 insights about\nthe task in a more interpretable way than the optimized\nprompts (Zhou et al., 2022b; Yang et al., 2023).\nWe test SELF-DISCOVER on 25 challenging reasoning\ntasks including Big Bench-Hard (BBH) (Suzgun et al.,\n2022), Thinking for Doing (T4D) (Zhou et al., 2023) and\nMATH (Hendrycks et al., 2021). SELF-DISCOVER outper-\nforms CoT on 21/25 task with performance gains up to 42%\n(Figure 1), highlighting the advantage of the self-discovered\nreasoning structure composed from the atomic reasoning\nmodules against a single a priori CoT module. Furthermore,\nwe demonstrate that SELF-DISCOVER achieves superior\nperformance against inference-heavy methods such as CoT\n+ Self-Consistency and majority voting of every module\nwhile requiring 10-40x fewer inference compute (Figure 5).\nFinally, we compare SELF-DISCOVER with prompts op-\ntimized (OPRO) using a training set (Yang et al., 2023)\n(Figure 9). We find that SELF-DISCOVER still performs on\npar or better than OPRO while the self-discovered reasoning\nstructure are much more interpretable.\nWe conduct a set of analysis to understand the effectiveness\nof SELF-DISCOVER. By breaking down BBH tasks into 4\ndifferent categories, we find that SELF-DISCOVER performs\nbest on tasks requiring world knowledge and has a mod-\nerate performance boost on algorithmic tasks compared to\nCoT (Figure 4). This is further confirmed by the error anal-\nysis on MATH, where 74.7% model failures comes from\ncomputation errors (e.g. math). We also take a closer look\nat the self-discovered reasoning structures, and show the\nuniversality of them by transferability study from PaLM\n2-L to GPT-4, and from GPT-4 to Llama-2-70B. We hope\nto encourage more future work on structured reasoning for\nsolving challenging problems using LLMs.\n2. Self-Discovering Reasoning Structures for\nProblem-Solving\nWe take inspiration from how humans use prior knowledge\nand skills to devise a reasoning program to solve prob-\nlems (Newell et al., 1958; Rasmussen, 1983). When we\nface a new problem, we often first search internally what\nknowledge and skills from our prior experience might be\nhelpful to solve it. Then we will attempt to apply relevant\nknowledge and skills to this task. And finally we will con-\nnect multiple individual skills and knowledge to solve the\nproblem. We design SELF-DISCOVER to enact these steps\ninto two stages as illustrated in Figure 2.\nGiven a task and a set of reasoning module descriptions\nrepresenting high-level problem-solving heuristics such as\n\u201cUse critical thinking\u201d and \u201cLet\u2019s think step by step\u201d, Stage 1\nof SELF-DISCOVER aims to uncover the intrinsic reasoning\nstructure for solving this task via meta-reasoning. Specifi-\ncally, we uses three meta-prompts to guide LLMs to select,\nadapt, and implement an actionable reasoning structure with\nno labels or training required. We format the structure in\nkey-value pairs similar to JSON due to interpretability and\nfindings on following JSON boosts reasoning and generation\nquality (Zhou et al., 2023; OpenAI, 2023a). The structure of\n2\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nLanguage \nModel\nAtomic Reasoning Modules\nReasoning Structure\n{\n    \"Type and color of each item\": \"\"\n    \"Number of items of each color\": \"\"\n    \"Number of items of each type\": \"\"\n    \"Number of items of each color and type\": \n    \"Final answer\":\n}\nTask: Reasoning \ncolored objects\nSelf-Discover\nLanguage \nModel\nAnswer\nStage 1: Discover Reasoning Structure on Task-Level\nStage 2: Solve Problems Using Discovered Structure on Instance-Level\nReasoning Structure\nTask Instance\nKey-Value pairs\nFill in the Values based on \nKeys during decoding\nFigure 2. Illustration of using SELF-DISCOVER for problem-solving. Given a generative LM, task, and seed reasoning module\ndescriptions, we guide LMs to generate a reasoning structure in key-value format to solve the task. Finally, models can follow the\nself-discovered structures to solve the every instance from the task by filling in the values in JSON step-by-step.\nthe meta-prompts and full prompts are shown in Appendix.\nStage 1 operates on task-level, meaning we only need to run\nSELF-DISCOVER once for each task. Then, in Stage 2, we\ncan simply use the discovered reasoning structure to solve\nevery instance of the given task by instructing models to\nfollow the provided structure by filling each key and arrive\nat a final answer.\n2.1. Stage 1: Self-Discover Task-Specific Structures\nThe first stage consists of three actions: 1) SELECT, where\nrelevant reasoning modules for task-solving are chosen from\nthe set of reasoning module descriptions; 2) ADAPT, where\ndescriptions of selected reasoning modules are rephrased to\nbe more specific to the task at hand; and 3) IMPLEMENT,\nwhere the adapted reasoning descriptions are implemented\ninto a structured actionable plan so that the task can be\nsolved by following the structure.\nSELECT\nFirst, not every reasoning module is helpful for\nevery task, so the first stage of SELF-DISCOVER guides\nmodel to select modules that are useful based on task exam-\nples. For example, \u201creflective thinking\u201d might help search\nfor first-principle theories on science problems, while \u201ccre-\native thinking\u201d helps on generating a novel continuation to\na story. Given raw set of reasoning module descriptions\nD such as \u201ccritical thinking\u201d, and \u201cbreak the problem into\nsub-problems\u201d (full set in Appendix A), and a few task ex-\namples without labels ti \u2208 T, SELF-DISCOVER first selects\na subset of reasoning modules DS that are useful for solving\nthe tasks by using a model M and a meta-prompt pS:\nDS = M(pS \u2225 D \u2225 ti).\n(1)\nADAPT\nSince each reasoning module provides a general\ndescription of how to solve problems, the next step of SELF-\nDISCOVER aims at tailoring each selected module to the\ntask at hand. For example, from \u201cbreak the problem into sub-\nproblems\u201d to \u201ccalculate each arithmetic operation in order\u201d\nfor arithmetic problems. Given selected reasoning module\nsubset DS from the previous step, ADAPT rephrases each\nof the selected module to be more specific to the task. Sim-\nilarly to SELECT, this stage uses a meta-prompt pA and\na generative model M to generate the adapted reasoning\nmodule descriptions DA:\nDA = M(pA \u2225 DS \u2225 ti).\n(2)\nIMPLEMENT\nFinally, given the adapted reasoning mod-\nule descriptions DA, SELF-DISCOVER operationalizes the\nreasoning modules into an implemented reasoning struc-\nture DI with specified instruction on what to generate for\neach step. In addition to a meta prompt pI, IMPLEMENT\nalso provides a demonstration of a human-written reason-\ning structure Shuman on another task to better convert the\nnatural language descriptions into a reasoning structure:\nDI = M(pA \u2225 Shuman \u2225 DA \u2225 ti).\n(3)\n2.2. Stage 2: Tackle Tasks Using Discovered Structures\nAfter the three stages, we have an implemented reasoning\nstructure DI uniquely adapted for the task we need to solve\nT. Then we can simply append the reasoning structure to\nall instances of the task and prompt models to follow the\nreasoning structure to generate an answer A:\nA = M(DS \u2225 t), \u2200t \u2208 T.\n(4)\nMore details of prompts are included in Appendix A.\n3. Experiment Setup\n3.1. Tasks\nWe focus on diverse reasoning benchmarks that are still\nchallenging for LLMs: BIG-Bench Hard (BBH) (Suzgun\n3\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nSelf-Discover\nSELECT\nADAPT\nIMPLEMENT\n\u2756\nStep-by-step\n\u2756\nBreak down \n\u2756\nPropose-verify\nAll Seed Modules\n\u2756\nStep-by-step\n\u2756\nBreak down \n\u2756\n\u2026\nSelected \nModules\n\u2756\nStep-by-step analyze each item\n\u2756\nBreak down to type and color \u2026\nAdapted  Modules\n{   \"Type and color of each item\":\n    \"Number of items of each color\":\n    \"Number of items of each type\": \n...}\nReasoning Structure\nLanguage \nModel\nSelected \nModules\nLanguage \nModel\nAdapted  \nModules\nLanguage \nModel\nFigure 3. Illustration of three actions of SELF-DISCOVER. We use LMs to compose a coherent reasoning structure by selecting relevant\nmodules, adapting to task-specific descriptions, and implement a reasoning structure in JSON.\net al., 2022) contains 23 carefully-selected challenging tasks\nfrom BIG-Bench (Srivastava et al., 2023). BBH tasks cover\na diverse range of reasoning problems spanning the follow-\ning 4 categories according to their authors: 1) Algorithmic\nand Multi-Step Arithmetic Reasoning, 2) Natural Language\nUnderstanding, 3) Use of World Knowledge, and 4) Mul-\ntilingual Knowledge and Reasoning. We also test on a\ngrounded social agent reasoning task called Thinking for\nDoing (T4D) where models must leverage mental state rea-\nsoning to determine actions to perform (Zhou et al., 2023),\nwhere GPT-4 with CoT only reaches around 50%. Finally,\nwe subsample 200 examples from the MATH (Hendrycks\net al., 2021) test set, and generate instance-level reasoning\nstructures via a one-shot demonstration to adapt to the com-\nplexity of MATH tasks. For evaluations, we use accuracy to\nmeasure the model performance on BBH, T4D and MATH\n(details can be found in Appendix B).\n3.2. Models\nWe use several state-of-the-art LLMs: GPT-4 (gpt-4-turbo-\npreview) (OpenAI, 2023b), GPT-3.5-turbo (ChatGPT) (Ope-\nnAI, 2022)1, instruction-tuned PaLM 2-L (Anil et al.,\n2023)2, and an open-source LLM Llama2-70B (Touvron\net al., 2023).\n3.3. Baselines\nWe compare SELF-DISCOVER with other zero-shot prompt-\ning methods for LLM reasoning:\n1accessed in October-December 2023\n2For MATH, we use a PaLM 2-L model with a stronger instruc-\ntion tuning to enable better instruction following of more complex\nreasoning structures.\n\u2022 Direct Prompting, where model directly generates the\nanswer without intermediate reasoning steps.\n\u2022 CoT (Wei et al., 2022; Kojima et al., 2022), where\nmodels are prompted to generate a reasoning process\nleading to the final answer.\n\u2022 Plan-and-Solve (Wang et al., 2023), where models are\nprompted to first generate a plan and then solve the\nproblem. SELF-DISCOVER differs by grounding the\nreasoning structure in atomic reasoning modules, and\nprompting the decoding to follow the explicit key-value\nreasoning structure.\nNext, we also consider other baselines that make use of\nthe raw seed reasoning modules (RM) we pass to SELF-\nDISCOVER. We compare with the following methods\u2019 per-\nformance and the inference call efficiency on a subset of\ntasks.\n\u2022 CoT-Self-Consistency (Wang et al., 2022), we sample\nmultiple outputs from LLM with CoT and aggregate an-\nswers to get the final answer. We compare this method\non a subset of tasks due to the cost of repetitive queries.\n\u2022 Majority voting of each RM: we prompt models to\nsolve the tasks by appending each RM and use majority\nvoting of all answers to get the final answer. We exam-\nine whether integrating multiple RMs into a coherent\nreasoning structure is advantageous to applying each\nRM to solve the task and use majority voting to ensem-\nble them post-hoc, which costs much more inference\ncomputation.\n\u2022 Best of each RM: this method assumes that we have ac-\ncess to oracle labels and uses the highest accuracy from\n4\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTable 1. Self-Discover significantly improves LLM reasoning\nacross a diverse set of 25 complex tasks: BBH, T4D and MATH.\nCoT: zero-shot Chain of Thought (Kojima et al., 2022). PS: plan-\nand-solve prompting (Wang et al., 2023).\nMethod\nBBH\nT4D\nMATH\nPaLM 2-L\n56%\n30%\n45%\nPaLM 2-L + CoT\n60%\n40%\n42%\nPaLM 2-L + PS\n61%\n42%\n49%\nPaLM 2-L + Self-Discover\n67%\n69%\n50.5%\nGPT-4\n58%\n51%\n70.5%\nGPT-4 + CoT\n75%\n52%\n71%\nGPT-4 + PS\n73%\n53%\n70%\nGPT-4 + Self-Discover\n81%\n85%\n73%\napplying each RM. We compare with this to examine\nwhether SELF-DISCOVER competes with methods that\ndepend on perfect prior knowledge of which RM to\nuse on a new task.\nFurthermore, for analysis on universality of reasoning struc-\ntures, we compare with a prompt-optimization method that\nrequire a training set to improve prompts: LLMs as optimiz-\ners (OPRO) (Yang et al., 2023). We aim to show that when\nwe apply structures or prompts optimized from one model,\nthe reasoning structures can retain more performance gains\nthan the wordings of prompts.\n4. Results\nWe answer the following questions through experimental\nresults: 1) Does discovering reasoning structures improve\nLLM reasoning capabilities? (4.1) 2) Which categories\nof problems do SELF-DISCOVER perform the best? (4.2)\nand 3) Can SELF-DISCOVER boost LLM performance ef-\nficiently? (4.3) Finally, we will show qualitative examples\nof self-discovered structures, LLM output following the\nstructures, and compare with LLM output following other\nprompting methods for reasoning (4.4).\n4.1. Does SELF-DISCOVER Improve LLM Reasoning?\nOverall, SELF-DISCOVER improves PaLM 2-L and GPT-\n4\u2019s reasoning across diverse set of reasoning tasks. Ta-\nble 1 shows the overall results on complex reasoning tasks\nof BBH, T4D and MATH using PaLM 2-L and GPT-4.\nWe compare Self-Discover with baselines including direct\nprompting, CoT, and Plan-and-Solve (PS).\nOn aggregated 23 tasks of BBH, SELF-DISCOVER achieves\n7% and 6% absolute improvement on PaLM 2-L over Chain-\nof-Thought and Plan-and-Solve, respectively. Similar gains\n(6% and 8%) are observed when SELF-DISCOVER is applied\nto GPT-4. Breakdown results of each task\u2019s improvement\nMultilingual\nAlgorithmic\nNLU\nWorld Knowledge\nProblem Categories\n0\n5\n10\n15\n20\n25\nAvg. Accuracy Delta (%)\n4.0\n14.2\n14.3\n19.8\n12.0\n5.2\n8.0\n19.7\nSelf-Discover Performance Improvement Across 4 Categories\nSelf-Discover Over Direct\nSelf-Discover Over CoT\nFigure 4. Breakdown of SELF-DISCOVER performance im-\nprovement on 4 categories on PaLM 2-L. SELF-DISCOVER per-\nforms the best on tasks requiring world knowledge.\nover direct answering and CoT of PaLM 2-L are shown\nin Figure 1, where we find SELF-DISCOVER outperforms\nthem on over 20/24 tasks. For a per-task performance for\nall 23 BBH tasks, please refer to Appendix C.\nOn\nthe\ngrounded\nsocial\nagent\ntask\nT4D,\nSELF-\nDISCOVER\nreaches\nover\n\u2265\n27%\n(32%)\nabsolute\nimprovement over all baselines on PaLM 2-L (GPT-4).\nSELF-DISCOVER achieves 69% and 85% accuracy on\nPaLM 2-L and GPT-4, significantly outperforming previous\nSoTA prompting method such as Foresee and Reflect (FaR)\nwhich employs an expert-designed reasoning structure.\nIn contrast, SELF-DISCOVER generates the reasoning\nstructure automatically from a set of atomic reasoning\nmodules without human interventions.\nFor MATH, we observe a moderate gain of 1%-7% (2%-3%)\non PaLM 2-L (GPT-4) from SELF-DISCOVER compared\nto the baselines. Upon error analysis (see Appendix D for\ndetails), we find that the reasoning structures generated by\nPaLM 2-L from SELF-DISCOVER are correct 87.5% of the\ntime: human experts can follow the reasoning structures\nto solve the tasks perfectly. The majority of the failures\n(74.7%) comes from errors in executing the computations,\nconsistent with prior findings (Zheng et al., 2023).\n4.2. Which Types of Problems Do\nSELF-DISCOVER Help the Most?\nSELF-DISCOVER performs best on tasks that require\ndiverse world knowledge.\nFigure 4 presents the aver-\nage improvement in terms of delta in accuracy of SELF-\nDISCOVER over direct answer and CoT on 4 categories\nof reasoning tasks we test.\nWe adopt the categoriza-\ntion from Suzgun et al. (2022).\nWe find that SELF-\nDISCOVER improves over these two baselines on all cate-\ngories, but especially on tasks that require world knowledge\nsuch as sports understanding, movie recommendation, and\nruin names.\nThese tasks demand models to reason using fact and general\n5\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\n0\n10\n20\n30\n40\n# of Inference Calls Per Instance\n65\n70\n75\n80\n85\nAverage Accuracy\nBBH-Movie Recommendation\n0\n10\n20\n30\n40\n35\n40\n45\n50\n55\n60\nBBH-Geometric Shapes\nSelf-Discover\nDirect\nCoT\nCoT+Self-Consistency\nPlan-and-Solve\nMajority voting each RM\nBest of each RM*\nFigure 5. Comparison of accuracy with number of inference calls required per instance. For CoT-Self-Consistency, we sample 10\ntimes. Best of each RM method requires gold labels (*). SELF-DISCOVER requires only 1 inference call per instance (plus 3 more\nmeta-prompts on the task-level), same as Direct and CoT while reaching better performance compared with 40x more call required\nmethods (majority voting of each RM) on GPT-4. We acknowledge that SELF-DISCOVER input and output are longer than CoT and\nDirect prompting, increasing cost. However, as the number of instances increases, the efficiency of SELF-DISCOVER in terms of inference\nper instance is highly desirable.\ncommonsense knowledge. We interpret SELF-DISCOVER\u2019s\nadvantages on these tasks as strength from integrating mul-\ntiple reasoning modules from various perspectives as only\napplying CoT might miss key knowledge in the reasoning\nprocess. We observe that the gain on the Algorithmic cate-\ngory is moderate, consistent with the findings from Sec. 4.1\non MATH.\n4.3. How Efficient is SELF-DISCOVER?\nSELF-DISCOVER achieves better performance while re-\nquiring 10-40x fewer inference computer compared to\nself-consistency or majority voting. Here we examine\na subset of 2 tasks from BBH and present a more thor-\nough comparison of methods including those requiring\nmany inference calls that are too costly to run on all 24\ntasks. Figure 5 shows average accuracy and number of\ninference calls required per instance for each method us-\ning GPT-4. Accuracy wise (y-axis), we find that SELF-\nDISCOVER outperforms other baselines even those that re-\nquire repeated inference calls such as CoT-self-consistency\nand majority voting of applying each RM. Efficiency wise\n(x-axis), SELF-DISCOVER only requires one call per in-\nstance and three more inference calls on the task-level, CoT-\nself-consistency requires 10 times more since we have to\nsample 10 times for each instance, and methods using each\nRM requires 40 times more as we use 40 RMs. In summary,\nSELF-DISCOVER presents itself a strong reasoning boosting\nmethod that is efficient to deploy on large-scale.\n4.4. Qualitative Examples\nWe show examples of model-discovered structures for differ-\nent reasoning tasks in Figure 6 from PaLM 2-L. We observe\nthat each structure is uniquely adapted to the task, integrates\nreasoning_about_colored_objects\n{\n    \"Type and color of each item\":\n    \"Number of items of each color\":\n    \"Number of items of each type\":\n    \"Number of items of each color \nand type\":\n    \"Final answer\":\n}\ncausal_judgement\n{\n    \"Identify the chain of events in \nthe story\":\n    \"Identify the consequences of \neach event\":\n    \"Identify the cause-and-effect \nrelationships between events\":\n    \"Choose a \ufb01nal answer based \non the reasoning\":\n}\ndyck_languages\n{\n\"Parentheses that are not closed properly\":\n\"Stack to store the closing parentheses\":\n\"If the next symbol is a closing parenthesis, pop the stack and \ncheck if the popped symbol matches the next symbol\":\n\"If the stack is empty, add the next symbol to the stack\":\n}\nDevise an algorithm\nRe\ufb02ect on \ntask nature\nBreak down \nto sub-tasks\nFigure 6. Examples of self-discovered structures on BBH tasks\nusing PaLM 2-L. We observe traits of atomic reasoning modules\nsuch as \u201cstep-by-step thinking\u201d, \u201creflect on task nature\u201d, and an in-\nteresting creative thinking case where models devise an algorithm\nusing stack to solve parenthesis parsing task.\nmultiple reasoning modules, and provides insights on how\nto solve the tasks. Furthermore, example of comparing\nreasoning processes from CoT, Plan-and-Solve, and SELF-\nDISCOVER is shown in Figure 7. We find that CoT and\nPlan-and-Solve makes incorrect assertions early and arrives\nat a wrong answer while following structure from SELF-\nDISCOVER leads the model to generate logical conclusions\n(\u201cpath is closed as the beginning and ending coordinates\nare the same\u201d) and arrive at the correct answer.\n5. Deep Diving Into Self-Discovered Reasoning\nStructures\nAfter experimental results showing the effectiveness and\nefficiency of SELF-DISCOVER on a range of reasoning\n6\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nThis SVG path element <path d=\"M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L \n45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69\"/> draws a:\n(A) circle (B) heptagon (C) hexagon (D) kite (E) line (F) octagon (G) pentagon(H) rectangle (I) sector (J) triangle\nThis SVG path consists of \nmultiple lines drawn \nbetween di\ufb00erent points. \nThese points do not form \na regular geometric shape \nsuch as a circle, \npentagon, heptagon, \nhexagon, or octagon. \u2026 \nThey do, however, seem \nto form a shape with four \nsides, which could be \nconsidered a form of a \nkite\u2026\nThus the \ufb01nal answer is \n(D) kite.\nLet\u2019s think \nstep-by-step (CoT)\nLet\u2019s devise a plan and solve \nthe problem (Plan-and-Solve)\nThis SVG Path element \nconsists of many \"L\" \ncommands which draw a \nstraight line from the \ncurrent point to the new \npoint. But it does not \nclose the path, meaning \nthe shape is not a regular \npolygon like a hexagon, \nheptagon, octagon or \npentagon. The \ninstructions make 4 lines \nin total but they are not \nstraight and do not form a \nparallelogram or a \nrectangle. \u2026\nThus the \ufb01nal answer is \n(D) kite.\n\u274c\n\u274c\nSelf-Discover Reasoning Structure (Ours)\n{   \"Simplify SVG Path\": ...\n    \"Breakdown of Path Commands\": {\n       \"Move to Command (M)\": \"Sets the starting point for the next \ncommand without drawing anything.\",\n        \"Line to Command (L) steps\": \n            {\"Start and end coordinates of each line segment\": \n\"M 55.57,80.69 L 57.38,65.80: From point (55.57, 80.69) to (57.38, \n65.80)\"}, \u2026 and \ufb01nally closing the shape at (55.57, 80.69)\"}\n    \"Critical Thinking Analysis\": {\n        \"Logical Reasoning\": {\n            \"Analysis of path continuity\": \"The path includes \nmultiple line segments that connect distinct points. The path ends by \nconnecting back to the starting point, indicating a closed shape.\",\n            \"Identification of closed shapes\": \"The \ufb01nal line \nsegment connects the last point back to the \ufb01rst point, which is \ncharacteristic of a closed shape.\"},\n      \u2026\n    \"Final Reasoning and Decision\": \"With 7 distinct points all \nconnected in a closed path, the shape formed is a heptagon.\",\n    \"Final Answer\": \"B) heptagon\"} \u2705\nFigure 7. Comparison of generated reasoning process from CoT, Plan-and-Solve, and SELF-DISCOVER on BBH-geometric shape\ntask. Both CoT and Plan-and-Solve incorrectly asserts that the path does not form a regular shape as it is not a closed path (highlighted in\nred) and arrive at a wrong answer. The reasoning structure (in blue Courier font) from SELF-DISCOVER first breaks down each line\nsegment and analyze the coordinates carefully, then leverages logical reasoning to conclude that it forms a closed shape as the path ends at\nthe same coordinate (highlighted in purple and orange), and selects the correct answer through final reasoning.\ntasks, this section further analyzes are all actions of SELF-\nDISCOVER needed and what other benefits can self-\ndiscovered structures bring? In Sec. 5.1, we show that it\nis critical to the model\u2019s performance to use the reasoning\nstructures discovered through the three steps of SELECT,\nADAPT and IMPLEMENT. In Sec. 5.2, we demonstrate\nthe universality of the self-discovered reasoning structures\nby (1) applying the structures discovered by PaLM 2-L to\nGPT-4, (2) applying the structures discovered by GPT-4 to\nLlama-2-70B. We further show the commonalities between\nthe reasoning structures and human reasoning patterns in\nAppendix E.\n5.1. Importance of SELF-DISCOVER Actions\nWe conduct ablation study on the three actions: SELECT,\nADAPT, and IMPLEMENT to analyze the effects of SELF-\nDISCOVER actions. Figure 8 show results using GPT-4 on 4\nreasoning tasks when we apply SELECT (-S) or apply SE-\nLECT and ADAPT (-SA) or apply all three actions. We find\nthat with each stage, model\u2019s zero-shot reasoning capability\nimprove consistently across tasks, indicating that all three\nactions are beneficial. In particular, after all three actions\nSAI, the reasoning structures are adapted to be task specific,\nand bring the most gain to solving the reasoning tasks.\nSnarks\nMovie\nT4D\nGeometry\nTasks\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n89\n73\n50\n43\n90\n73\n65\n50\n93\n79\n80\n45\n94\n86\n85\n61\nAblaton Studies on 3 Self-Discover Actions: SELECT, ADAPT, IMPLEMENT (SAI)\nCoT\nOurs-S\nOurs-SA\nOurs-SAI\nFigure 8. Ablation study on three SELF-DISCOVER actions on\n4 reasoning tasks: all three actions are beneficial for task-solving.\n5.2. Towards Universality of Discovered Reasoning\nStructures\nApplying PaLM 2-L Discovered Structures to GPT-4\nWe first use a PaLM 2-L model to discover the reasoning\nstructures of 4 reasoning tasks. Then, we apply the resulting\nreasoning structures to the decoding of GPT-4 as grounding.\nWe compare our approach to OPRO (Yang et al., 2023)\nwhich discovered zero-shot-prompts through optimizations.\nWe apply OPRO prompts optimized using PaLM 2-L on\neach task to GPT-4 on the same reasoning tasks. Figure 9\nshows that SELF-DISCOVER outperforms OPRO on 3 out\nof 4 tasks despite that OPRO used 20% data to optimize the\n7\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nSnarks\nMovie\nT4D\nGeometry\nTasks\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n94\n74\n52\n43\n94\n79\n79\n57\nTransferrability of PaLM 2-L Optimized Prompts/Structures on GPT-4\nOPRO*\nSelf-Discover\nFigure 9. Transferrability tests of optimized prompts (OPRO)\nand composed structures (SELF-DISCOVER). The results shown\nare from GPT-4 using the prompts and structures optimized or\ncomposed using PaLM 2-L. We find that self-discovered reasoning\nstructure transfers more robustly than optimized prompts.\nprompt. In contrast, SELF-DISCOVER is done in a zero-shot\nmanner, demonstrating the efficiency of our method and\nuniversality of the discovered reasoning structures.\nApplying GPT-4 Discovered Structures to Llama2 and\nChatGPT\nMotivated by transferrability performance\nacross LLMs, we further investigate can self-discovered rea-\nsoning structures from LLMs boost reasoning for smaller\nLMs that are challenging to come up with structures them-\nselves3. We use GPT-4 to discover the task-intrinsic rea-\nsoning structures, and then apply those structures to the\ndecoding of open-sourced Llama2-70B as well as GPT-3.5-\nturbo (ChatGPT) on two subsets of tasks from BBH. We\nfind that using self-discovered structures on Llama2 (52%)\noutperforms CoT (42%) on disambiguation QA zero-shot\nand on GPT-3.5-turbo (56%) outperforms CoT (51%) on ge-\nometry with 3-shot demonstration from structured reasoning\nprocess.\n6. Related Work\n6.1. Prompting Methods\nRecent advancements in the area of LLMs have given rise\nto a plethora of few-shot (Brown et al., 2020) and instruc-\ntion (Mishra et al., 2022c; Wei et al., 2021; Ouyang et al.,\n2022) prompting techniques, including Chain-of-Thought\nprompting (CoT) (Nye et al., 2021; Wei et al., 2022), Least-\nto-most prompting (Zhou et al., 2022a; Drozdov et al.,\n2022), Decomposed prompting (Khot et al., 2022), Re-\nframing (Mishra et al., 2022b), Help Me Think Prompt-\ning (Mishra & Nouri, 2023), Stepback Prompting (Zheng\net al., 2023) and search-based approaches like Tree-of-\nThought (ToT) (Yao et al., 2023a), Graph-of-Thought (Besta\net al., 2023; Yao et al., 2023b), Branch-solve-merge (Saha\net al., 2023) and RAP (Hao et al., 2023). Each of the\n3We tried zero-shot meta prompting Llama2 but observed low-\nquality structure outputs.\nprompting methods has some strengths and weaknesses\nin terms of their successful application domain. Our work\nSELF-DISCOVER presents the missing piece in the prompt-\ning literature, as SELF-DISCOVER provides a way to self-\ncompose over various prompting methods via the proposed\nself-discovery mechanism.\nComposing over prompting\nmethods in SELF-DISCOVER is analogous to the program-\nming literature where a program is written using various\nbasic building blocks such as for loop, if/else condition etc.\n6.2. Reasoning and Planning\nWith the development of various reasoning and plan-\nning benchmarks such as GSM8K (Cobbe et al., 2021),\nMath (Hendrycks et al.), BigBench (Srivastava et al., 2023)\netc., various methods have been proposed to improve model\nperformance. Often these methods induce specific reason-\ning structures mimicking the reasoning structure of the un-\nderlying task associated with the dataset. For example,\nchain of thought (Wei et al., 2022) and scratchpad (Nye\net al., 2021) induce generation of explanations associated\nwith a reasoning question. Similarly other methods induces\nspecific reasoning structures such as question summariza-\ntion (Kuznia et al., 2022), question decomposition (Patel\net al., 2022), program generation (Mishra et al., 2022a;\nChen et al., 2022; Gao et al., 2023b), etc. However, in a real\nworld user traffic, queries can be diverse covering various\nreasoning structures. Our work SELF-DISCOVER allows\nmodels to combine multiple reasoning approaches by self-\ncomposing into a structure without the need to access task\nlabels. There have been some related work that explores\nLLM combining skills in-context such as SkiC (Chen et al.,\n2023), devising a strategy (Gao et al., 2023a), and planning\nwith iterative quering (Liu et al., 2023). However, they\nrequire human annotating skills and reasoning plans while\nSELF-DISCOVER leverages a scalable solution with the help\nof LLM\u2019s meta-task reasoning capabilities.\n7. Conclusion\nWe introduce SELF-DISCOVER, an efficient and performant\nframework for models to self-discover a reasoning structure\nfor any task from a seed set of general problem-solving\nskills. We observe drastic improvements on challenging\nreasoning benchmarks from multiple LLMs up to 30%. Ab-\nlations study of SELF-DISCOVER demonstrates that the\ncomposed reasoning structures are universally transferable\nbetween LLMs. Forward looking, we are excited to explore\nmore on LLM structured reasoning to push the boundary\nof problem-solving and discover potentials for Human-AI\ncollaboration.\n8\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nAcknowledgement\nWe thank Andrew Dai and Adams Yu of Google DeepMind\nfor their insightful feedback on this paper.\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\nSolving elaborate problems with large language models.\narXiv preprint arXiv:2308.09687, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, J., Pan, X., Yu, D., Song, K., Wang, X., Yu, D.,\nand Chen, J. Skills-in-context prompting: Unlocking\ncompositionality in large language models. arXiv preprint\narXiv:2308.00304, 2023.\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program\nof thoughts prompting: Disentangling computation from\nreasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nDrozdov, A., Sch\u00e4rli, N., Aky\u00fcrek, E., Scales, N., Song,\nX., Chen, X., Bousquet, O., and Zhou, D. Composi-\ntional semantic parsing with large language models. arXiv\npreprint arXiv:2209.15003, 2022.\nFernando, C., Banarse, D., Michalewski, H., Osindero,\nS., and Rockt\u00e4schel, T. Promptbreeder: Self-referential\nself-improvement via prompt evolution. arXiv preprint\narXiv:2309.16797, 2023.\nGao, C., Jiang, H., Cai, D., Shi, S., and Lam, W. Strategyllm:\nLarge language models as strategy generators, executors,\noptimizers, and evaluators for problem solving. arXiv\npreprint arXiv:2311.08803, 2023a.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\nY., Callan, J., and Neubig, G. Pal: Program-aided lan-\nguage models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023b.\nHao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,\nand Hu, Z. Reasoning with language model is planning\nwith world model.\narXiv preprint arXiv:2305.14992,\n2023.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring\nmathematical problem solving with the math dataset. Sort,\n2(4):0\u20136.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset, 2021.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,\nClark, P., and Sabharwal, A. Decomposed prompting:\nA modular approach for solving complex tasks. In The\nEleventh International Conference on Learning Repre-\nsentations, 2022.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. Ad-\nvances in neural information processing systems, 35:\n22199\u201322213, 2022.\nKuznia, K., Mishra, S., Parmar, M., and Baral, C. Less is\nmore: Summary of long instructions is better for program\nsynthesis. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp.\n4532\u20134552, 2022.\nLiu, T., Guo, Q., Yang, Y., Hu, X., Zhang, Y., Qiu, X.,\nand Zhang, Z.\nPlan, verify and switch: Integrated\nreasoning with diverse x-of-thoughts.\narXiv preprint\narXiv:2310.14628, 2023.\nMishra, S. and Nouri, E. HELP ME THINK: A simple\nprompting strategy for non-experts to create customized\ncontent with models. In Rogers, A., Boyd-Graber, J.,\nand Okazaki, N. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2023, pp. 11834\u201311890,\nToronto, Canada, July 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.\n751. URL https://aclanthology.org/2023.\nfindings-acl.751.\n9\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nMishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S.,\nBaral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A.,\nClark, P., et al. Lila: A unified benchmark for mathemati-\ncal reasoning. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp.\n5807\u20135832, 2022a.\nMishra, S., Khashabi, D., Baral, C., Choi, Y., and Hajishirzi,\nH. Reframing instructional prompts to gptk\u2019s language.\nIn Findings of the Association for Computational Linguis-\ntics: ACL 2022, pp. 589\u2013612, 2022b.\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-\ntask generalization via natural language crowdsourcing\ninstructions. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 3470\u20133487, 2022c.\nNewell, A., Shaw, J. C., and Simon, H. A. Elements of a\ntheory of human problem solving. Psychological review,\n65(3):151, 1958.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\nM., Luan, D., et al. Show your work: Scratchpads for\nintermediate computation with language models. arXiv\npreprint arXiv:2112.00114, 2021.\nOpenAI. Chatgpt: Optimizing language models for dia-\nlogue, 2022. URL https://openai.com/blog/\nchatgpt/.\nOpenAI.\nJson generation mode,\n2023a.\nURL\nhttps://platform.openai.com/docs/\nguides/text-generation/json-mode.\nOpenAI, R. Gpt-4 technical report. arXiv, pp. 2303\u201308774,\n2023b.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nPatel, P., Mishra, S., Parmar, M., and Baral, C. Is a ques-\ntion decomposition unit all we need? In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 4553\u20134569, 2022.\nPolya, G. How to solve it: A new aspect of mathematical\nmethod, volume 85. Princeton university press, 2004.\nRasmussen, J. Skills, rules, and knowledge; signals, signs,\nand symbols, and other distinctions in human perfor-\nmance models. IEEE transactions on systems, man, and\ncybernetics, (3):257\u2013266, 1983.\nSaha, S., Levy, O., Celikyilmaz, A., Bansal, M., Weston,\nJ., and Li, X. Branch-solve-merge improves large lan-\nguage model evaluation and generation. arXiv preprint\narXiv:2310.15123, 2023.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language\nmodels. Transactions on Machine Learning Research,\n2023.\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\npdf.\nWang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W.,\nand Lim, E.-P. Plan-and-solve prompting: Improving\nzero-shot chain-of-thought reasoning by large language\nmodels. arXiv preprint arXiv:2305.04091, 2023.\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi,\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\nconsistency improves chain of thought reasoning in lan-\nguage models. In The Eleventh International Conference\non Learning Representations, 2022.\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners. In International\nConference on Learning Representations, 2021.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824\u201324837, 2022.\nYang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and\nChen, X. Large language models as optimizers. arXiv\npreprint arXiv:2309.03409, 2023.\n10\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,\nand Narasimhan, K. Tree of thoughts: Deliberate prob-\nlem solving with large language models. arXiv preprint\narXiv:2305.10601, 2023a.\nYao, Y., Li, Z., and Zhao, H. Beyond chain-of-thought,\neffective graph-of-thought reasoning in large language\nmodels. arXiv preprint arXiv:2305.16582, 2023b.\nYasunaga, M., Chen, X., Li, Y., Pasupat, P., Leskovec, J.,\nLiang, P., Chi, E. H., and Zhou, D. Large language models\nas analogical reasoners. arXiv preprint arXiv:2310.01714,\n2023.\nZheng, H. S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E. H.,\nLe, Q. V., and Zhou, D. Take a step back: Evoking\nreasoning via abstraction in large language models. arXiv\npreprint arXiv:2310.06117, 2023.\nZhong, R., Lee, K., Zhang, Z., and Klein, D.\nAdapt-\ning language models for zero-shot learning by meta-\ntuning on dataset and prompt collections. arXiv preprint\narXiv:2104.04670, 2021.\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X.,\nSchuurmans, D., Cui, C., Bousquet, O., Le, Q. V., et al.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. In The Eleventh International\nConference on Learning Representations, 2022a.\nZhou, P., Madaan, A., Potharaju, S. P., Gupta, A., McKee,\nK. R., Holtzman, A., Pujara, J., Ren, X., Mishra, S.,\nNematzadeh, A., et al. How far are large language mod-\nels from agents with theory-of-mind?\narXiv preprint\narXiv:2310.03051, 2023.\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,\nChan, H., and Ba, J. Large language models are human-\nlevel prompt engineers. In The Eleventh International\nConference on Learning Representations, 2022b.\n11\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nA. Self-Discover Prompt Details\nTable 2 shows all 39 reasoning modules we use for SELF-DISCOVER, adopted from Fernando et al. (2023), that contain\ncognitive heuristics of problem-solving.\nFigure 10 contains the structure of the three actions of SELF-DISCOVER during Stage 1, where it discovers an intrinsic\nreasoning structure on the task-level.\nFor Stage 2, where we use the self-discovered structure to solve the task instances, we start with the prompt: \u201cFollow the\nstep-by-step reasoning plan in JSON to correctly solve the task. Fill in the values following the keys by reasoning specifically\nabout the task given. Do not simply rephrase the keys.\u201d, followed by the reasoning structure, and finally the task instance.\nRm\u0094d1m1p\u00a6\n\u0017\u0143\u0138\u0145\u0134\u0147\u013c\u0142\u0141\u0134\u013f\u013c\u014d\u0138\u0003\u0147\u013b\u0138\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\n\u0140\u0142\u0137\u0148\u013f\u0138\u0146\u0003\u013c\u0141\u0147\u0142\u0003\u0134\u0003\u0146\u0147\u0138\u0143\u02ed\u0135\u014c\u02ed\u0146\u0147\u0138\u0143\u0003\n\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0143\u013f\u0134\u0141\u0003\u013c\u0141\u0003\u0012\u001b\u0017\u0016\u0003\u0139\u0142\u0145\u0140\u0134\u0147\u02ea\n\u0017\u0143\u0138\u0145\u0134\u0147\u013c\u0142\u0141\u0134\u013f\u013c\u014d\u0138\u0003\u0147\u013b\u013c\u0141\u013e\u013c\u0141\u013a\u0003\n\u0011\u0140\u0143\u013f\u0138\u0140\u0138\u0141\u0147\u0003\u0134\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0146\u0147\u0145\u0148\u0136\u0147\u0148\u0145\u0138\u0003\u0139\u0142\u0145\u0003\n\u0146\u0142\u013f\u0149\u0138\u0145\u0146\u0003\u0147\u0142\u0003\u0139\u0142\u013f\u013f\u0142\u014a\u0003\u0146\u0147\u0138\u0143\u02ed\u0135\u014c\u02ed\u0146\u0147\u0138\u0143\u0003\u0134\u0141\u0137\u0003\n\u0134\u0145\u0145\u013c\u0149\u0138\u0003\u0134\u0147\u0003\u0136\u0142\u0145\u0145\u0138\u0136\u0147\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u0146\u02ea\n\u009c1d1!\u00a6\n\u001b\u0138\u013f\u0138\u0136\u0147\u0003\u0146\u0138\u0149\u0138\u0145\u0134\u013f\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0146\u0003\n\u0147\u013b\u0134\u0147\u0003\u0134\u0145\u0138\u0003\u0136\u0145\u0148\u0136\u013c\u0134\u013f\u0003\u0147\u0142\u0003\u0148\u0147\u013c\u013f\u013c\u014d\u0138\u0003\u013c\u0141\u0003\u0142\u0145\u0137\u0138\u0145\u0003\n\u0146\u0142\u013f\u0149\u0138\u0003\u0147\u013b\u0138\u0003\u013a\u013c\u0149\u0138\u0141\u0003\u0147\u0134\u0146\u013e\u02ea\n\u001b\u0138\u013f\u0138\u0136\u0147\u0003\u0146\u0138\u0149\u0138\u0145\u0134\u013f\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0146\u0003\u0147\u013b\u0134\u0147\u0003\u0134\u0145\u0138\u0003\n\u0136\u0145\u0148\u0136\u013c\u0134\u013f\u0003\u0139\u0142\u0145\u0003\u0146\u0142\u013f\u0149\u013c\u0141\u013a\u0003\u0147\u013b\u0138\u0003\u0147\u0134\u0146\u013e\u0146\u0003\u0134\u0135\u0142\u0149\u0138\u02ea\n\t\u013f\u013f\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0003\u0137\u0138\u0146\u0136\u0145\u013c\u0143\u0147\u013c\u0142\u0141\u0146\n\u0194\n\u000b\u0145\u013c\u0147\u013c\u0136\u0134\u013f\u0003\u0147\u013b\u013c\u0141\u013e\u013c\u0141\u013a\u02ea\u0003\u02ec\n\u0194\n\u001b\u0147\u0138\u0143\u02ed\u0135\u014c\u02ed\u001b\u0147\u0138\u0143\u02ea\u0003\u02ec\n\u0194\n\u0018\u0145\u0142\u0143\u0142\u0146\u0138\u0003\u0134\u0141\u0137\u0003\u0149\u0138\u0145\u013c\u0139\u014c\u02ea\u0003\u02ec\n\u02ec\n\u001c\u0134\u0146\u013e\u0003\u0138\u014b\u0134\u0140\u0143\u013f\u0138\u0146\u0003\u014a\u0302\u0142\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u02ea\n\r\u014b\u0134\u0140\u0143\u013f\u0138\u0003\u033b\u02ea\u0003\u02ec\n\r\u014b\u0134\u0140\u0143\u013f\u0138\u0003\u033c\u02ea\u0003\u02ec\n\u0004'\u0004\u0094\u00a6\n\u001a\u0138\u0143\u013b\u0145\u0134\u0146\u0138\u0003\u0134\u0141\u0137\u0003\u0146\u0143\u0138\u0136\u013c\u0139\u014c\u0003\u0138\u0134\u0136\u013b\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\n\u0140\u0142\u0137\u0148\u013f\u0138\u0003\u0146\u0142\u0003\u0147\u013b\u0134\u0147\u0003\u013c\u0147\u0003\u0135\u0138\u0147\u0147\u0138\u0145\u0003\u013b\u0138\u013f\u0143\u0146\u0003\u0146\u0142\u013f\u0149\u013c\u0141\u013a\u0003\n\u0147\u013b\u0138\u0003\u0147\u0134\u0146\u013e\u02ea\n\t\u0137\u0134\u0143\u0147\u0003\u0138\u0134\u0136\u013b\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0003\n\u0137\u0138\u0146\u0136\u0145\u013c\u0143\u0147\u013c\u0142\u0141\u0003\u0147\u0142\u0003\u0135\u0138\u0147\u0147\u0138\u0145\u0003\u0146\u0142\u013f\u0149\u0138\u0003\u0147\u013b\u0138\u0003\u0147\u0134\u0146\u013e\u0146\u02ea\n\u001b\r\u0014\r\u000b\u001c\r\f\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0003\u0137\u0138\u0146\u0136\u0145\u013c\u0143\u0147\u013c\u0142\u0141\u0146\u02ea\n\u0194\n\u000b\u0145\u013c\u0147\u013c\u0136\u0134\u013f\u0003\u0147\u013b\u013c\u0141\u013e\u013c\u0141\u013a\u02ea\u0003\u02ec\n\u0194\n\u001b\u0147\u0138\u0143\u02ed\u0135\u014c\u02ed\u001b\u0147\u0138\u0143\u02ea\u0003\u02ec\n\u02ec\n\t\f\t\u0018\u001c\r\f\u0003\u0140\u0142\u0137\u0148\u013f\u0138\u0003\u0137\u0138\u0146\u0136\u0145\u013c\u0143\u0147\u013c\u0142\u0141\u02ea\n\u0018\u0134\u013c\u0145\u0138\u0137\u0003\u0011\u0015\u0018\u0014\r\u0015\r\u0016\u001c\u0003\u001b\u0147\u0138\u0143\u0003\n\f\u0138\u0140\u0142\u0141\u0146\u0147\u0145\u0134\u0147\u013c\u0142\u0141\n\u001a\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0137\u0138\u0146\u0136\u0145\u013c\u0143\u0147\u013c\u0142\u0141\u0003\r\u014b\u0134\u0140\u0143\u013f\u0138\n\u001a\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0018\u013f\u0134\u0141\u0003\r\u014b\u0134\u0140\u0143\u013f\u0138\n\u001c\u0134\u0146\u013e\u0003\u0138\u014b\u0134\u0140\u0143\u013f\u0138\u0146\u0003\u014a\u0302\u0142\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u02ea\n\r\u014b\u0134\u0140\u0143\u013f\u0138\u0003\u033b\u02ea\u0003\u02ec\n\r\u014b\u0134\u0140\u0143\u013f\u0138\u0003\u033c\u02ea\u0003\u02ec\n\u001c\u0134\u0146\u013e\u0003\u0138\u014b\u0134\u0140\u0143\u013f\u0138\u0146\u0003\u014a\u0302\u0142\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u02ea\u0003\u02eb\u02eb\u02eb\nFigure 10. Meta-Prompts for the three actions of SELF-DISCOVER. Each meta-prompt consists of an instruction in the beginning and\nthe end, reasoning module descriptions, and task examples without labels. For IMPLEMENT, to show model an example of a reasoning\nstructure (plan), we present a human-written structure in JSON for another task.\nB. Evaluation Details\nWe use accuracy and exact matching as with other methods tested on BBH, T4D and MATH. To properly evaluate the\ngenerated answers from LLMs, we prompt the models to end the answer with \u201cThus, the final answer is [X]\u201d, where X\nis either one answer option such as \u201cA\u201d or a string such as \u201cvalid\u201d. During evaluation, we manually examine each task\u2019s\noutputs from LLMs and design heuristics to extract the final answers. For MATH dataset, we find that it is challenging to\nextract the answers accurately. As a result, we subsample 200 test examples from MATH, and manually sanity check and\nannotate the extracted answers for all methods tested in our paper.\nC. BBH Per Task Performance\nPer-task performance on BBH (23 tasks in total) are shown in Table 3.\nD. Error Analysis\nWe perform an error analysis of SELF-DISCOVER on the MATH dataset of 200 samples to understand the failure modes.\nWe manually annotate whether the generated reasoning structure is correct or not together with whether the correctness of\nmodel prediction using SELF-DISCOVER. A reasoning structure is defined as correct if a human expert can solve the task by\nsimply following the reasoning structure.\n12\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTable 2. All 39 reasoning modules consisting of high-level cognitive heuristics for problem-solving. We adopt them from Fernando et al.\n(2023).\nReasoning Modules\n1 How could I devise an experiment to help solve that problem?\n2 Make a list of ideas for solving this problem, and apply them one by one to the problem to see if any progress can be made.\n3 How could I measure progress on this problem?\n4 How can I simplify the problem so that it is easier to solve?\n5 What are the key assumptions underlying this problem?\n6 What are the potential risks and drawbacks of each solution?\n7 What are the alternative perspectives or viewpoints on this problem?\n8 What are the long-term implications of this problem and its solutions?\n9 How can I break down this problem into smaller, more manageable parts?\n10 Critical Thinking: This style involves analyzing the problem from different perspectives, questioning assumptions, and evaluating\nthe evidence or information available. It focuses on logical reasoning, evidence-based decision-making, and identifying\npotential biases or flaws in thinking.\n11 Try creative thinking, generate innovative and out-of-the-box ideas to solve the problem. Explore unconventional solutions,\nthinking beyond traditional boundaries, and encouraging imagination and originality.\n12 Seek input and collaboration from others to solve the problem. Emphasize teamwork, open communication, and leveraging the\ndiverse perspectives and expertise of a group to come up with effective solutions.\n13 Use systems thinking: Consider the problem as part of a larger system and understanding the interconnectedness of various elements.\nFocuses on identifying the underlying causes, feedback loops, and interdependencies that influence the problem, and developing holistic\nsolutions that address the system as a whole.\n14 Use Risk Analysis: Evaluate potential risks, uncertainties, and tradeoffs associated with different solutions or approaches to a\nproblem. Emphasize assessing the potential consequences and likelihood of success or failure, and making informed decisions based\non a balanced analysis of risks and benefits.\n15 Use Reflective Thinking: Step back from the problem, take the time for introspection and self-reflection. Examine personal biases,\nassumptions, and mental models that may influence problem-solving, and being open to learning from past experiences to improve\nfuture approaches.\n16 What is the core issue or problem that needs to be addressed?\n17 What are the underlying causes or factors contributing to the problem?\n18 Are there any potential solutions or strategies that have been tried before? If yes, what were the outcomes and lessons learned?\n19 What are the potential obstacles or challenges that might arise in solving this problem?\n20 Are there any relevant data or information that can provide insights into the problem? If yes, what data sources are available,\nand how can they be analyzed?\n21 Are there any stakeholders or individuals who are directly affected by the problem? What are their perspectives and needs?\n22 What resources (financial, human, technological, etc.) are needed to tackle the problem effectively?\n23 How can progress or success in solving the problem be measured or evaluated?\n24 What indicators or metrics can be used?\n25 Is the problem a technical or practical one that requires a specific expertise or skill set? Or is it more of a conceptual or\ntheoretical problem?\n26 Does the problem involve a physical constraint, such as limited resources, infrastructure, or space?\n27 Is the problem related to human behavior, such as a social, cultural, or psychological issue?\n28 Does the problem involve decision-making or planning, where choices need to be made under uncertainty or with competing\nobjectives?\n29 Is the problem an analytical one that requires data analysis, modeling, or optimization techniques?\n30 Is the problem a design challenge that requires creative solutions and innovation?\n31 Does the problem require addressing systemic or structural issues rather than just individual instances?\n32 Is the problem time-sensitive or urgent, requiring immediate attention and action?\n33 What kinds of solution typically are produced for this kind of problem specification?\n34 Given the problem specification and the current best solution, have a guess about other possible solutions.\n35 Let\u2019s imagine the current best solution is totally wrong, what other ways are there to think about the problem specification?\n36 What is the best way to modify this current best solution, given what you know about these kinds of problem specification?\n37 Ignoring the current best solution, create an entirely new solution to the problem.\n38 Let\u2019s think step by step.\n39 Let\u2019s make a step by step plan and implement it with good notion and explanation.\n13\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTable 3. Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.\nBig Bench-Hard Task\nHuman (Avg.)\nHuman (Max)\nGPT-4\nDirect\nGPT-4\n+ CoT\nGPT-4\n+ Self-Discover\nPaLM 2-L\nDirect\nPaLM 2-L\n+ CoT\nPaLM 2-L\n+ Self-Discover\nboolean_expressions\n79\n100\n73\n83\n85\n71\n84\n84\ncausal_judgement\n70\n100\n67\n75\n80\n46\n59\n61\ndate_understanding\n77\n100\n74\n80\n81\n73\n78\n78\ndisambiguation_qa\n67\n93\n60\n70\n80\n54\n50\n57\ndyck_languages\n48\n100\n69\n73\n77\n94\n95\n98\nformal_fallacies\n91\n100\n60\n60\n80\n60\n63\n69\ngeometric_shapes\n54\n100\n30\n56\n60\n33\n34\n39\nhyperbaton\n75\n100\n68\n69\n76\n80\n75\n82\nlogical_deduction_seven_objects\n40\n89\n60\n70\n70\n45\n39\n50\nmovie_recommendation\n61\n90\n70\n70\n86\n83\n54\n66\nmultistep_arithmetic_two\n10\n25\n10\n92\n70\n4\n50\n47\nnavigate\n82\n100\n70\n90\n90\n38\n63\n67\nobject_counting\n86\n100\n90\n100\n100\n27\n44\n70\npenguins_in_a_table\n78\n100\n80\n100\n90\n70\n67\n75\nreasoning_about_colored_objects\n75\n100\n77\n80\n79\n36\n79\n75\nruin_names\n78\n100\n90\n80\n97\n79\n58\n90\nsalient_translation_error_detection\n37\n80\n40\n50\n70\n56\n48\n60\nsnarks\n77\n100\n73\n89\n97\n58\n62\n86\nsports_understanding\n71\n100\n54\n61\n90\n44\n47\n89\ntemporal_sequences\n91\n100\n96\n99\n100\n99\n97\n99\ntracking_shuffled_objects_seven_objects\n65\n100\n24\n80\n68\n22\n58\n36\nweb_of_lies\n81\n100\n15\n80\n71\n54\n42\n67\nword_sorting\n63\n100\n65\n90\n85\n12\n4\n15\nOut of 200 examples, we find that 87.5% (175) examples have correct reasoning structures. 12.5% (25) examples have\nincorrect reasoning structures leading to prediction errors. Table 4 shows 4 such examples where the LLM misunderstands\nthe task, or makes an error in one of the steps or adds unnecessary steps in the reasoning structure.\nNext, we analyze the errors made by the model in SELF-DISCOVER: out of 99 examples where the model prediction is\nwrong, wrong reasoning structures account for only 25.3% of the errors. The remaining 74.7% errors are due to errors in\nthe intermediate calculations such as math computations. Table 5 shows 3 examples of such errors. This insight indicates\nthat future improvements should aim at improving the step-wise calculation accuracy of LLMs, such as using tools or code\ngeneration.\nE. Further Anaysis\nModel-Discovered Reasoning Structures vs. Human Reasoning Patterns\nWe investigate whether LLM-discovered\nreasoning structures share some commonalities with human reasoning patterns. We give humans 3 task instances without\nlabels and an example reasoning structure (same as SELF-DISCOVER meta-reasoning stage) and ask them to write a\nreasoning structure for a task before solving it. Figure 11 shows comparison of human and LLM-composed reasoning\nstructures on the BBH-navigation task. We observe similar structures such as mental-noting after each movement. From\npromising findings of LLM self-discovered structures boost and share traits of human meta-reasoning, we hope to encourage\nmore future work to study humna-AI collaboration for complex problem-solving.\n14\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTable 4. Examples of wrong reasoning structures for MATH. The first error in the reasoning structure is highlighted in red.\nPrompt\nReasoning Structure\nError\nHow many numbers between\n1 and 2005 are integer multi-\nples of 3 or 4 but not 12?\n1. Find the number of multiples of 3 between 1 and\n2005.\n2. Find the number of multiples of 4 between 1\nand 2005.\n3. Find the number of multiples of 12 between 1\nand 2005.\n4. Add the number of multiples of 3 and 4.\n5. Subtract the number of multiples of 12 (to avoid\ndouble counting).\n6. The result is the number of integers between 1\nand 2005 that are multiples of 3 or 4 but not 12.\nNeed to sub-\ntract the num-\nber\nof\nmul-\ntiples of 12\ntwice instead\nof once.\nHow many numbers are in\nthe list 6,7,10,11,14,15,...,\n94,95,98?\n1. Find the number of pairs in the list.\n2. Find the number of numbers in each pair.\n3. Multiply the number of pairs by the number of\nnumbers in each pair to find the total number of\nnumbers in the list.\nLLM\nmisun-\nderstands the\ntask: the rea-\nsoning\nstruc-\nture is irrele-\nvant.\nA restaurant offers three\ndesserts, and exactly twice\nas many appetizers as main\ncourses. A dinner consists of\nan appetizer, a main course,\nand a dessert. What is the\nleast number of main courses\nthat the restaurant should of-\nfer so that a customer could\nhave a different dinner each\nnight in the year 2003?\n1. Define the variables:\n- M: number of main courses\n- A: number of appetizers (twice the number of\nmain courses)\n- D: number of desserts (fixed at 3)\n- T: total number of possible dinner combinations\n2. Calculate the total number of possible dinner\ncombinations:\n- T = M * A * D\n3. Set the total number of possible dinner combi-\nnations equal to the number of nights in the year\n2003:\n- T = 2003\n4. Substitute the expression for A in terms of M:\n- T = M * (2 * M) * 3\n5. Solve the equation for M:\n- 6M 2 = 2003\n6. Find the smallest integer value of M that satis-\nfies the equation.\nThere are 365\nnights in the\nyear 2003.\nDetermine the number of\nways to arrange the letters of\nthe word NINE.\n1. Count the total number of letters in the word\nNINE.\n2. Identify the number of times each letter appears\nin the word.\n3. Calculate the factorial of the total number of\nletters.\n4. Divide the factorial by the factorial of the num-\nber of times each letter appears.\n5. Multiply the results from step 4 for each unique\nletter. }\nStep 5 is un-\nnecessary and\nleads to error\nin the final re-\nsult.\n15\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nTable 5. Examples of wrong calculations for MATH. The first error in the intermediate computations is highlighted in red.\nPrompt\nSelf-Discover Prediction\nError\nHow many ways are there\nto arrange 6 people around\na circular table with 7 seats?\n(Two seatings are considered\nthe same if one is a rotation\nof the other.)\n\"1. Calculate the total number of ways to arrange\n6 people in 7 seats without considering rotations\nas the same.\": 7 * 6 * 5 * 4 * 3 * 2 = 5040\n- There are 7 choices for the first person.\n- There are 6 choices for the second person.\n- There are 5 choices for the third person.\n- There are 4 choices for the fourth person.\n- There are 3 choices for the fifth person.\n- There are 2 choices for the sixth person.\n\"2. Divide the result from step 1 by the number of\nrotations that are considered the same.\": 5040 / 6\n= 840\n- There are 6 rotations that are considered the same.\nThere are 7 ro-\ntations instead\nof 6 that are\nconsidered the\nsame.\nA\ncustomer\nordered\n15\npieces of gourmet chocolate.\nThe order can be packaged\nin small boxes that contain\n1, 2 or 4 pieces of chocolate.\nAny box that is used must\nbe full. How many different\ncombinations of boxes can\nbe used for the customer\u2019s 15\nchocolate pieces? One such\ncombination to be included\nis to use seven 2-piece boxes\nand one 1-piece box.\n\"1. Find the number of ways to package the choco-\nlates using only boxes of 1 piece.\": 1,\n\"2. Find the number of ways to package the choco-\nlates using only boxes of 2 pieces.\": 1,\n\"3. Find the number of ways to package the choco-\nlates using only boxes of 4 pieces.\": 0,\n\"4. Find the number of ways to package the choco-\nlates using a combination of boxes of 1 and 2\npieces.\": 7,\n\"5. Find the number of ways to package the choco-\nlates using a combination of boxes of 1 and 4\npieces.\": 3,\n\"6. Find the number of ways to package the choco-\nlates using a combination of boxes of 2 and 4\npieces.\": 2,\n\"7. Find the number of ways to package the choco-\nlates using a combination of boxes of 1, 2, and 4\npieces.\": 4,\n\"8. Sum up the total number of ways from the\nseven cases above.\": 1 + 1 + 0 + 7 + 3 + 2 + 4 = 18\nThere is 0 way\nto package 15\nchocolates us-\ning only boxes\nof 2 pieces.\nHow many primes are in the\nrow of Pascal\u2019s Triangle that\nstarts with a 1 followed by a\n6?\n\"1. Identify the row of Pascal\u2019s Triangle that starts\nwith a 1 followed by a 6.\": \"1, 6, 15, 20, 15, 6, 1\",\n\"2. List all the numbers in that row.\": [1, 6, 15, 20,\n15, 6, 1],\n\"3. Check each number in the list for primality.\":\n[False, True, False, False, False, True, False],\n\"4. Count the number of primes found in the list.\":\n2\n6\nis\nnot\na\nprime.\n16\nSELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures\nHuman-Written Structure:\n{\n\u201cPosition after instruction 1\u201d:\n\u201cPosition after instruction 2\u201d:\n\u2026\n\u201cPosition after instruction n\u201d:\n\u201cIs \ufb01nal position the same as starting \nposition?\u201d:\n}\nModel-Discovered Structure:\n{\n    \"Break down instructions into individual movements\": \n{\n        \"Instruction 1\": \"\",\n        \"Effect on position after Instruction 1\": \"\",\n        \"Instruction 2\": \"\",\n        \"Effect on position after Instruction 2\": \"\",\n        \u2026\n        \"Additional instructions if present\": \"\"\n    },\n    \"Simplify the sequence of movements\": {\n        \"Simpli\ufb01ed representation of series\": \"\"\n    } \u2026 \n}        \nShared \nStructure: \nStep-wise \nmental notes\nTask-Navigation:  If you follow these instructions, do you return to the starting point? Always \nface forward. Take 1 step backward. Take 9 steps left. Take 2 steps backward. Take 6 steps forward. \nTake 4 steps forward. Take 4 steps backward. Take 3 steps right.\nFigure 11. Case study of human-written structure shares commonalities with LLM-discovered reasoning structure. We observe\nsimilar reasoning patterns\u2013both structures contain step-wise analysis of each instruction.\n17\n"
  },
  {
    "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
    "link": "https://arxiv.org/pdf/2402.04248.pdf",
    "upvote": "22",
    "text": "Can Mamba Learn How to Learn?\nA Comparative Study on In-Context Learning Tasks\nJongho Park1, Jaeseung Park2\u2217, Zheyang Xiong3, Nayoung Lee3, Jaewoong Cho1,\nSamet Oymak4, Kangwook Lee1,3, Dimitris Papailiopoulos1,3\n1 KRAFTON, 2 Seoul National University,\n3 University of Wisconsin-Madison, 4 University of Michigan, Ann Arbor\nAbstract\nState-space models (SSMs), such as Mamba Gu & Dao (2023), have been proposed\nas alternatives to Transformer networks in language modeling, by incorporating\ngating, convolutions, and input-dependent token selection to mitigate the quadratic\ncost of multi-head attention. Although SSMs exhibit competitive performance,\ntheir in-context learning (ICL) capabilities, a remarkable emergent property of\nmodern language models that enables task execution without parameter optimiza-\ntion, remain underexplored compared to Transformers. In this study, we evaluate\nthe ICL performance of SSMs, focusing on Mamba, against Transformer models\nacross various tasks. Our results show that SSMs perform comparably to Trans-\nformers in standard regression ICL tasks, while outperforming them in tasks like\nsparse parity learning. However, SSMs fall short in tasks involving non-standard\nretrieval functionality. To address these limitations, we introduce a hybrid model,\nMambaFormer, that combines Mamba with attention blocks, surpassing individual\nmodels in tasks where they struggle independently. Our findings suggest that hybrid\narchitectures offer promising avenues for enhancing ICL in language models.\n1\nIntroduction\nModern large language models (LLMs) exhibit remarkable in-context learning (ICL) capabilities,\nenabling them to learn new tasks with a few demonstrations and without further weight fine-tuning.\nAlthough the exact emergence mechanism of these capabilities warrants further theoretical and\nempirical investigation (Chan et al., 2022; Wei et al., 2022; Min et al., 2022b; Schaeffer et al., 2023),\nexperiments on larger Transformer-based models consistently demonstrate that their ICL capabilities\nimprove as training loss reduces (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023).\nMeta-learning, or \u201clearning to learn,\u201d has been extensively studied (Schmidhuber et al., 1997; Ravi\n& Larochelle, 2016) and recently regained interest in the context of ICL, particularly concerning\nTransformer models (Vaswani et al., 2017). Garg et al. (2022), for example, proposed various ICL\ntasks, such as learning linear regression, and evaluated the ability of transformers to perform them\nwhen specifically trained to do so. On the other hand, Min et al. (2022a) studied fine-tuning language\nmodels to explicitly learn and perform ICL. Following these footsteps, numerous research studies\nhave been dedicated to understanding the mechanics of Attention that enable such meta-learning\ncapabilities, either through constructive arguments or extensive experimental investigation (Aky\u00fcrek\net al., 2022; Li et al., 2023b; von Oswald et al., 2023b; Bai et al., 2023; Yang et al., 2023a; Li et al.,\n2023a; von Oswald et al., 2023a).\n\u2217This work was done during an internship at KRAFTON.\nEmail: <jongho.park@krafton.com>. Correspondence: <dimitris@papail.io>\nPreprint. Under review.\narXiv:2402.04248v1  [cs.LG]  6 Feb 2024\nTransformer\nMamba\nMambaFormer\nLinear regression\n\u2713\n\u2713\n\u2713\nSparse linear regression\n\u2713\n\u2713\n\u2713\n2NN regression\n\u2713\n\u2713\n\u2713\nDecision Tree\n\u2713\n\u25b2\n\u2713\nOrthogonal-outlier regression\n\u2713\n\u25b2\n\u2713\nMany-outlier regression\n\u25b2\n\u2713\n\u2713\nSparse parity\n\u2717\n\u2713\n\u2713\nChain-of-Thought I/O\n\u2713\n\u2713\n\u2713\nVector-valued MQAR\n\u2713\n\u2717\n\u2713\nTable 1: Model performances on various ICL tasks. We label the model\u2019s performance with \u2713 if the\nmodel performs on par with other baseline models, \u2717 if the model fails to learn the task, and \u25b2 if\nthere exists performance gap compared to other models. Transformer fails in learning sparse parity\nMamba fails in vector-valued MQAR. Our proposed MambaFormer succeeds in all tasks.\nAs Transformer language models are currently the only large models that have been reported to be\ncapable of ICL in practice, this raises the question:\nCan attention-free models perform ICL?\nThis question holds merit, especially considering that several recent studies have attempted to move\nbeyond attention-based networks due to their quadratic cost (Gu et al., 2022b; Dao et al., 2022; Gu &\nDao, 2023; Poli et al., 2023; Peng et al., 2023; Sun et al., 2023; Yang et al., 2023b). In this work, we\nfocus specifically on state-space models (SSMs), and particularly Mamba (Gu & Dao, 2023). Mamba\nwas recently demonstrated to be highly efficient while achieving near state-of-the-art performance in\nstandard pretraining language data sets, such as the Pile (Gao et al., 2020), but at smaller model scales\n(e.g., up to 3 billion parameters), surpassing transformers and other attention-free architectures across\nvarious language and non-language tasks. However, ICL capabilities usually emerge at scales beyond\n3 billion parameters. As a result, the potential of these attention-free models to perform ICL remains\nunderexplored, as testing such hypotheses usually requires scaling beyond the 7 billion parameter\nlevel. Nonetheless, we can still investigate small-scale ICL capabilities by specifically training a\nmodel to perform in-context learning, following the approach of Garg et al. (2022).\nContributions.\nIn this study, we introduce a diverse set of ICL tasks to evaluate the performance\nof Transformer and various SSMs, including state-of-the-art models like Mamba and S4 (Gu et al.,\n2022b). Our findings reveal that most of these SSMs can effectively perform ICL, matching the\nperformance of Transformers across multiple tasks. However, Mamba demonstrates some limitations\nin learning decision trees and retrieval tasks (as also noted by (Arora et al., 2023)), but can outperform\nTransformers in other complex ICL tasks, such as sparse parity, where Transformer models struggle.\nPerformance of different models on each task is summarized in Table 1.\nSince there seem to be tasks where either family of models is better, we explore the impact of\ninterleaving SSM blocks with multi-head attention blocks, similar to (Gu & Dao, 2023). We\nintroduce MambaFormer, a novel hybrid architecture that integrates Mamba and Attention layers,\nwhile eliminating the need for positional encodings, as shown in Figure 1. MambaFormer seems\nto leverage the strengths of both Mamba and Transformers, exhibiting good performance across all\nevaluated ICL tasks and simultaneously learning sparse parity and retrieval.\nWe believe that our findings underscore the importance of broadening the understanding of ICL\nbeyond Transformers, as significant progress has been made in the context of attention-free architec-\ntures.\nWe acknowledge that a limitation of our study lies in the focus on non-language ICL tasks and smaller\nmodels. It is possible that an architectural comparison between SSMs and transformers for more\ngeneral ICL tasks in actual language settings at higher parameter counts might not be yield the same\nobservations as we offer here. Nevertheless, our results indicate that, apart from its difficulty in\nsome retrieval tasks, similar to those noted by (Arora et al., 2023), there seems to be no fundamental\nobstacle for Mamba to perform in-context learning.\n2\nMamba\nMulti-Head\nAttention\nInputs\nOutputs\nMamba\nTraining data\nprompts: \nFigure 1: MambaFormer is a hybrid architecture that replaces MLP blocks within the transformer\nwith Mamba blocks. Importantly, the architecture also starts with a Mamba block and does not use\npositional encoding. In our ICL evaluations, we find that MambaFormer consistently achieves a\nbest-of-both-worlds performance compared to Transformer and Mamba.\n2\nRelated Work\nTransformer-based in-context Learning.\nThe role of attention in ICL has been the focus of\nboth theoretical and empirical research. Studies have primarily focused on meta-learning (Ravi &\nLarochelle, 2016; Min et al., 2022a), where one explicitly trains for ICL. Notably, Garg et al. (2022)\nhave examined transformers in in-context regression tasks, from learning linear regression to learning\ndecision trees. Subsequent works have suggested that attention may mimic various optimization\nalgorithms (Aky\u00fcrek et al., 2022; von Oswald et al., 2023b; Dai et al., 2023). In fact, Ahn et al. (2023);\nMahankali et al. (2023) have provably shown that gradient descent is optimal in linear regression ICL\nfor linear attention.\nWhile these settings might appear simplistic and detached from language models, Bhattamishra\net al. (2023) showed that a frozen GPT-2 can implement the nearest neighbor algorithm, drawing\nconnections between the ICL in existing language models and the stylized setting of training for\nICL from random initialization. Furthermore, Olsson et al. (2022) also empirically demonstrate that\n\u201cinduction heads\u201d, which are attention heads that solve a simple retrieval problem, correlate with ICL\nbehavior, providing a strong connection between retrieval and ICL.\nSub-quadratic architectures.\nThe number of effective floating point operations in an attention\nlayer scales quadratically with respect to the input sequence length. Numerous approximations or\nalternative model architectures have been proposed to overcome the quadratic dependence. These\nrange from approximating attention mechanisms (Beltagy et al., 2020; Wang et al., 2020) to the\ndevelopment of novel recurrent convolutional models such as structured state-space models (Gu et al.,\n2022b).\nS4 (Gu et al., 2022a) is a family of sequence models characterized by a discretized state-space model\nht = Aht\u22121 + Bxt, yt = Cht,\n(1)\nwhere ht represents the hidden state and (A, B, C) are input-independent (transformed) parameters.\nThe recurrence is expressible as a convolution, enabling near-linear complexity using Fast Fourier\nTransform. Viewed in this framework, Linear Transformers (Katharopoulos et al., 2020), which\nemploy linear attention without softmax, can be seen as a variant of linear SSM.\nBuilding upon this concept, H3 (Dao et al., 2022), which integrates an S4 with dual gated connections.\nThe recent Mamba (Gu & Dao, 2023) departs from the standard SSM by introducing a selection\n3\nmechanism that makes (A, B, C) in Equation (1) dependent on xt, which allows for input-dependent\nsequence mixing.\nThere are other notable attention-free models such as Hyena (Poli et al., 2023), RWKV (Peng\net al., 2023), RetNet (Sun et al., 2023), and GLA (Yang et al., 2023b). Despite of state-of-the-art\nperformance for models like Mamba, Arora et al. (2023) have demonstrated that subquadratic models\nstill lag behind attention on multi-query recall tasks, which is a generalization of the induction head\ntask (Olsson et al., 2022).\nIn Xie et al. (2021), the authors proposed a synthetic language-based in-context learning dataset and\nshow that transformers and LSTMs are capable of ICL. Moreover, Aky\u00fcrek et al. (2024) suggested\na langauge based ICL benchmark, by training on regular languages generated by random finite\nautomata, and also underscored the gap between Transformers and subquadratic complexity models.\n3\nExperimental Setup\nWe evaluate the ICL capabilities of SSMs and Transformers by training each model from scratch on\neach specific task, detailed in Section 3.1. Section 3.2 outlines the ICL and related tasks investigated\nin our study. We provide a brief summary of our tasks in the following Table 2.\nTask\ndim (d)\npoints (N)\nExample/Function Sampling\nTask-specific\nLinear regression\n20\n41\nx, w \u223c N(0, Id)\n\u2013\nSparse Linear regression\n20\n101\nx, w \u223c N(0, Id), sparsity(w) \u2190 k\nk = 3\n2NN regression\n20\n101\nW(1)\nij , W(2)\nij \u223c N(0, 1)\n\u2013\nDecision Tree\n20\n101\nx, Leaf \u223c N(0, 1), non_leaf \u223c {1, ..., d}\ndepth = 4\nOrthogonal-outlier regression\n20\n101\nx, w \u223c N(0, Id), u, v \u223c w\u22a5\np = 0.5\nMany-outlier regression\n20\n512\nx \u223c N(0, I) w.p. 1 \u2212 p, else (x, y) = (1, 1)\np = 0.9\nSparse Parity\n10\n140\nx \u223c {\u22121, 1}d, y = Q\nj\u2208I x[j]\nk = 2\nChain-of-Thought I/O\n10\n101\nx \u223c N(0, Id), Wij \u223c N(0, 2/k), v \u223c N(0, Ik)\nh = 8\nVector-valued MQAR\n20\n128\nk, v \u223c Unif(Sd\u22121)\n32 k-v pairs\nTable 2: Summary of Tasks. All models are trained for 500,000 iterations (except for the vector-valued\nMQAR; see Appendix B.2).\n3.1\nModel Training for In-context Learning\nWe train models to learn specific function classes F in-context. Training begins by generating random\nprompts: selecting a function f \u2208 F from distribution DF and sampling a sequence of random inputs\nx1, . . . , xN \u2208 Rd i.i.d. from DX . Here, N and d represent the number of in-context examples and\nthe dimension of xi, respectively. These inputs create the prompt P = (x1, f(x1), . . . , xN, f(xN)).\nWe train the model f\u03b8, parameterized by \u03b8, by minimizing the expected loss over all prompts:\nmin\n\u03b8\nEP\n\"\n1\nN\nN\u22121\nX\ni=1\n\u2113(f\u03b8(P i), f(xi))\n#\n,\n(2)\nwhere P i := (x1, f(x1), . . . , xi, f(xi), xi+1) and \u2113(\u00b7, \u00b7) is a loss function. For f : Rd \u2192 R, we\nappend d \u2212 1 zeros to f(x). We use appropriate loss functions for each ICL task.\nModel architecture.\nWe primarily focus on SSMs, including (1) Mamba (Gu & Dao, 2023), a state-\nof-the-art SSM model with selection mechanism; (2) S4 (Gu et al., 2022a), a linear time-invariant\ncounterpart to Mamba; and (3) S4-Mamba, a variant where Mamba\u2019s input-dependent S6 is replaced\nwith input-independent S4. The primary differences between the S4 models lie in the application of\nmultiplicative gating and the module order.2\nTraining.\nWe train each model by sampling a batch of random prompts at each training step and\nupdating the model parameters using Adam optimizer (Kingma & Ba, 2014). We use a batch size of\n64 and trained for 500,000 iterations (except for the vector-valued MQAR task; see Appendix B.2).\n2https://github.com/state-spaces/s4/blob/main/models/s4\n4\nEvaluation.\nWe evaluate model performance on in-context learning using task and data distributions\nDF and DX consistent with training. A function and a sequence of N inputs are sampled from\nDF and DX , respectively, to generate a test prompt Ptest = (x1, f(x1), . . . , xN, f(xN)). We\ncreate 1,280 prompts and measure the empirical mean of Eq. (2) across them for in-context learning\nperformance.\nTo plot performance as model capacity grows, we calculate the total floating point operations (FLOPs)\nused for training. The calculation for Transformer and Mamba can be found in Appendix C, which are\nbased on (Kaplan et al., 2020; Gu & Dao, 2023). Model configurations and training implementation\ndetails are provided in Appendix A.\n3.2\nIn-context learning tasks\nWe provide an overview of the ICL and related tasks investigated in this study. Some tasks are\nadapted from (Garg et al., 2022), and we follow the settings outlined in their work. The tasks are\nsummarized in Table 2.\n3.2.1\nLearning regression\nFor all regression tasks, in-context examples xi are sampled from the Gaussian distribution N(0, Id),\nwhere Id is the d \u00d7 d identity matrix. We use the squared error loss for model training.\nLinear regression.\nWe examine the class of linear functions F = {f|f(x) = w\u22a4x, w \u2208 Rd}\nwhere w is sampled from the Gaussian distribution N(0, Id). We set d = 20.\nSparse linear regression.\nThe setting is identical to linear regression, except that w is sampled\nfrom N(0, Id), after which k coordinates are randomly retained in w, and the rest are set to zero. We\nset k = 3.\nTwo-layer neural network.\nWe consider the class of two-layer ReLU neural networks F =\n{f|f(x) = W(2)\u03c3\n\u0000W(1)x\n\u0001\n}, where W(2) \u2208 R1\u00d7h, W(1) \u2208 Rh\u00d7d, and \u03c3(\u00b7) = max(0, \u00b7) is the\nReLU function. Each element of the weight matrices is independently drawn from N(0, 1). We use\nd = 20 and h = 100.\nDecision Tree\nWe consider a full binary tree with a fixed depth and input x \u2208 Rd. Leaf node values\nare sampled from N(0, 1), and the rest are sampled uniformly from {1, ..., d}, functioning as indices\nof x. At a given non-leaf node, we move to the right if x[i] > 0, where i is the sampled index, and\notherwise move to the left. y is the leaf node value when the traversal terminates.\n3.2.2\nLearning with outliers\nThe problems that belong to this family adopt the basic setting of the standard linear regression task.\nWith a fixed probability p, each pair of (xi, f(xi)) in the prompt is replaced with \u201cdummy\u201d vectors\nwhich are either out of the training distribution, or confounders designed to increase the complexity\nof the task. We test p \u2208 {0.5, 0.9} as replacement probabilities for tasks described below. During\ntraining, we do not compute the loss for the replaced outliers.\nOrthogonal-outlier regression.\nEach pair of (xi, f(xi)) is randomly replaced with ((axu +\nbxv)/(a2\nx + b2\nx), (ayu + byv)/(a2\nx + b2\nx)), where u, v \u2208 w\u22a5. (u, v) := (w1 \u2212 projw(w1), w2 \u2212\nprojw(w2)) and w1 and w2 are sampled from N(0, Id) and the coefficients ax, bx, ay, by are inde-\npendently sampled from N(0, 1).\nMany-outlier regression.\nIn this setting, xi and f(xi) are randomly replaced with a d-dimensional\nvector of ones {1}d and an one-hot vector [1, 0, . . . , 0], respectively, with probability 90%. Here, we\ntest longer sequences of N = 512.\n3.2.3\nLearning discrete functions\nSparse parity.\nFollowing the setting from Bhattamishra et al. (2023), we consider the class of\nfunctions F = {f|f(x) = Q\nj\u2208S xi[j]}, where xi[j] denotes the j-th element of the vector xi and S\nis a subset of {1, . . . , d} with the size k. Each xi is sampled uniformly at random from {\u22121, 1}d,\n5\nand S of size k is randomly sampled from the set {1, . . . , d}. For this task, we train a model using\nthe cross-entropy loss and evaluate the model using a binary indicator for accuracy, which assigns 1\nto correct predictions and 0 to incorrect ones.\n3.2.4\nLearning Chain-of-Thought\nChain-of-Thought-I/O.\nFollowing the setting from Li et al. (2023b), we consider the class of\ntwo-layer ReLU neural networks F = {f|f(x) = W(2)\u03c3\n\u0000W(1)x\n\u0001\n}, where W(2) \u2208 R1\u00d7h, W(1) \u2208\nRh\u00d7d, and \u03c3(\u00b7) is the ReLU function. We set d = 10, and h = 8. We additionally interleave the\nintermediate hidden feature si = \u03c3\n\u0000W(1)xi\n\u0001\nin our input training sequence in a Chain-of-Thought\nstyle. Given the input sequence (x1, s1, f(x1), \u00b7 \u00b7 \u00b7 , xN, sN, f(xN), xtest), the model is evaluated on\nthe final output prediction \u02c6y based on the input sequence and the intermediate layer prediction \u02c6stest.\n3.2.5\nLearning retrieval\nVector-valued multi-query associative recall\nWe test the model\u2019s ability to do multi-query associative recall (MQAR) (Arora et al., 2023). While\nMQAR is not an ICL task, model\u2019s ability to do associative recall (AR) is highly related to model\u2019s\nability to learn in-context (Olsson et al., 2022). To better measure the model\u2019s ability to retrieve\ninformation from context, we consider a variant of MQAR such that keys and values are vector-valued\nso each vector can be seen as a \u201cunique token\u201d and the retrieval accuracy can be measured by the\nmean squared error between retrieved vectors and target vectors. Specifically, in this task, the model\nis given a sequence of key-value pairs of vectors {k1, v1, ..., kn, vn}, where ki, vi \u2208 Sd\u22121 are\nsampled uniformly from the unit d-sphere. The query consists of sequence of vectors {q1, ..., qm}.\nFor each query qj, there exists some 1 \u2264 l \u2264 n such that qj = kl. The model must learn to output\nvl associated with the query qj for each of the queries, producing m outputs total. We train a model\nusing the squared error error.\n4\nExperiment results\nIn this section, we demonstrate that Mamba can be trained from scratch to perform various ICL tasks.\nFurthermore, we identify specific tasks in which one model performs better than the other and vice\nversa.\n4.1\nMamba can in-context learn!\nAs shown in Figure 2, Mamba consistently outperforms its more simple counterparts S4-Mamba and\nS4. In simple tasks such as linear regression, the gap between Mamba and S4-Mamba is much smaller\nthan that of S4-Mamba and S4. Given that the main difference between Mamba and S4-Mamba is the\ninput-dependent selection mechanism, appropriate gating and stacking of MLPs (i.e., the difference\nbetween S4-Mamba and S4) seem to be more significant for such tasks. However, in comparison,\ninput-dependent selection makes meaningful progress for more complex tasks such as 2NN regression\nand learning decision trees.\nMamba can also perform on par with Transformer even as the total FLOPs scale up. This is surprising\ngiven that Transformer and attention have been the focus of many previous works for its unique ICL\ncapability. Moreover, Mamba tends to perform better in smaller parameter settings when controlling\nfor equal depth, i.e., keeping the number of attention, MLP, and Mamba blocks equivalent.\n4.2\nPerformance gaps in more complex ICL tasks\nWe also consider a family of more complex ICL tasks, namely learning decision tree, sparse parity,\nand Chain-of-Thought (Figures 2 and 4). The figure shows that Transformers can solve Decision\nTree and Vector-valued MQAR, while Mamba cannot. In Sparse Parity task of Figure 5, however,\nTransformer is unable to learn the function family while Mamba can.\nFiltering outliers in regression.\nOrthogonal-outlier regression and many-outlier regression, like\nother outlier tasks, focus on the model\u2019s ability to learn to ignore dummy vectors, either by the\n6\n1015\n1016\n1017\n1018\nFLOPs\n100\nSquare Loss\nSparse Linear Regression: eval at 10 examples\nS4\nS4-Mamba\nMambaFormer\nStandard\nMamba\nTransformer\n1015\n1016\n1017\n1018\nFLOPs\n10\n2\n10\n1\n100\n101\nSquare Loss\nLinear Regression ICL: eval at 30 examples\n1017\n1018\nFLOPs\n100\n101\nSquare Loss\nMany-outlier Linear ICL: eval at 20th clean example\n1015\n1016\n1017\n1018\nFLOPs\n10\n2\n10\n1\n100\nSquare Loss\nSparse Linear Regression: eval at 20 examples\n1015\n1016\n1017\nFLOPs\n10\n2\n10\n1\n100\nSquare Loss\nChain-of-Thought-I/O: eval at 100 examples\n1015\n1016\n1017\n1018\nFLOPs\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\nSquare Loss\n2NN Regression ICL: eval at 100 examples\n1016\n1017\n1018\nFLOPs\n10\n2\n10\n1\n100\nSquare Loss\nDecision Tree ICL: eval at 100 examples\nFigure 2: A suite of ICL tasks ran for Transformer, Mamba, S4, and hybrid architectures where each\ncolor represents a different architecture. More transparent points indicate earlier stages of training;\nplotted models are trained in between [100k, 500k] iterations. We represent each model in terms of\nits number of floating point operations (FLOPs) used for training.\nfact that the xi \u2208 w\u22a5, or by the fact that yi is a vector instead of a zero-padded scalar value. This\nexplicitly requires the models to look at the previous input sequences, and discover the properties\nthat distinguish the dummy vectors from training examples while learning the class of functions the\ntraining prompt represents.\nFor orthogonal-outlier regression task with a relatively short sequence length of 101 (see Table 2 for\ntask descriptions), Mamba performs on par with Transformer, as seen in Figure 3. Interestingly, for\nmany-outlier regression where we test on a sequence length of 512 and 90% all-ones replacement,\nMamba significantly outperforms Transformers. This is also in line with what Gu & Dao (2023)\n7\n1016\n1017\n1018\nFLOPs\n10\n1\n100\n101\nSquare Loss @ 30-shot\nScaling Filter Orthogonal Regression\nMamba\nTransformer\nStandard Hybrid\nMambaFormer\n1016\n1017\n1018\nFLOPs\n10\n2\n10\n1\n100\n101\nSquare Loss @ 50-shot\nScaling Filter Orthogonal Regression\nMamba\nTransformer\nStandard Hybrid\nMambaFormer\nFigure 3: Scaling curves with respect to the total number of FLOPs on Orthogonal-outlier Regression\nTask.\nreport, in which Mamba fares better for the induction task for long sequence lengths. These two\nresults indicate that Mamba has no significant issue with filtering out unnecessary information, while\nretaining the ability to learn linear regression in-context.\n0\n20\n40\n60\n80\n100\nin-context examples\n10\n2\n10\n1\n100\n101\nsquared error\nChain-of-Thought-I/O\ntf_standard\nmamba_standard\ntf_small\nmamba_small\ntf_tiny\nmamba_tiny\n0\n20\n40\n60\n80\n100\nin-context examples\n10\n2\n10\n1\n100\n101\nsquared error\nChain-of-Thought-I/O\ntf, k=4\nmamba, k=4\ntf, k=8\nmamba, k=8\ntf, k=16\nmamba, k=16\nFigure 4: Performance of Transformer and Mamba models on the Chain-of-Thought-I/O task.\nExperiments on varying the model size (left) and varying the hidden dimension (right).\nChain-of-Thought I/O.\nFigure 4 shows that Mamba models are capable of in-context learning\nin a chain-of-thought manner, performing comparably to Transformer models across the tested\nconfigurations. In smaller model configurations, Mamba models exhibit superior performance\ncompared to Transformer models. However, as model size increases, Transformer models begin to\nsurpass Mamba models. The performance of Transformer models remains relatively stable across\ndifferent problem sizes, while Mamba models\u2019 performance is significantly influenced by the size of\nthe hidden layer. Specifically, Mamba models excel over Transformer models at smaller problem\nsizes (i.e., smaller hidden dimensions), but their advantage diminishes as the problem size expands.\n4.3\nChallenges in parity and retrieval\nWe run vector-valued MQAR on two settings: (1) 32 key-value pairs with 16 queries and (2) 32\nkey-value pairs with 4 queries. From Table 3, we can see that Mamba fails to retrieve vectors\naccurately as the mean squared error for retrieving normed vectors are greater than 0.1 in all cases.\nAs a sidenote, all models trained with 16 queries have lower test loss than models trained with 4\nqueries. A possible explanation is that, for a single sequence of data that represents an MQAR task,\nwe can think of each (q, v) pair as a \u201ctraining sample\u201d, so a sequence with 16 queries contains more\n\u201ctraining samples\u201d than that of a sequence with 4 queries. This also shows that having more queries\ndoes not necessarily make the task harder.\n8\nEmbedding dimension (d)\n64\n128\nMamba\n7.23e-1\n1.50e-1\n6 MambaBlocks + 1 Standard Hybrid\n1.54e-3\n5.86e-5\nTransformer w/o PE\n7.61e-5\n5.55e-5\nTransformer w/ PE\n3.99e-5\n2.46e-7\nMambaFormer\n1.03e-5\n3.79e-7\n(a) 32 key-value pairs with 16 queries.\nEmbedding dimension (d)\n64\n128\nMamba\n8.64e-1\n1.64e-1\n6 MambaBlocks + 1 Standard Hybrid\n1.99e-2\n1.37e-2\nTransformer w/o PE\n1.14e-3\n8.66e-5\nTransformer w/ PE\n5.17e-6\n8.76e-7\nMambaFormer\n7.30e-6\n3.37e-6\n(b) 32 key-value pairs with 4 queries.\nTable 3: Test loss (mean squared error) on vector-valued MQAR and respective model configurations.\nWe test for both Transformer with Positional Encoding (PE) and without. All models have 4 layers\nwith roughly the same number of parameters. An exception is \u201c6 MambaBlocks + 1 Standard Hybrid\u201d\nmodel, but we still consider it as (equivalently) having 4 layers since 6 MambaBlocks are equivalent\nto 3 Mamba layers as described in Figure 6.\n0\n50\n100\nin-context examples\n0.4\n0.6\n0.8\n1.0\nAccuracy\nSparse Parity ICL Evaluation\nTransformer\nS4-Mamba\nMamba\nS4\nFigure 5: Although Transformer fails to converge, Mamba and S4-Mamba can learn sparse parity\nof d = 10 and k = 2. Each model is trained with an embedding dimension of 256 and depth of 12\nlayers (approximately 10 million parameters) up to 500k iterations. Transformer failed to learn even\nup to an embedding dimension of 768 and 24 layers.\nWhile Mamba fails on simple retrieval tasks such as MQAR, the tables turn for the task of learning\nsparse parity (Figure 5). Transformer fails to do better than random guessing, in line with the\nempirical evidence of Bhattamishra et al. (2023). We confirm this is the case for Transformer sizes of\nembedding dimensions up to 768 and up to 24 layers when trained for at most 1 million iterations.\nHowever, Mamba succeeds in this task with ease, solving sparse parity for (d, k) = (10, 2) with a\nnetwork as small as 2 layers. Even more surprisingly, S4-Mamba is able to solve parity as well; this\nmay mean that proper convolution or gating may be more important than input-dependent selection.\nOur result hints at that the initial (causal) convolution that Mamba provides before the attention\nlayer may be crucial to solving parities, a similar phenomenon observed for Vision Transformers in\ncomputer vision tasks (Yu et al., 2022).\nIt is known that any algorithm for learning parities requires either a super-linear memory of \u03c9(d) or a\nsuper-polynomial number of samples in d (Raz, 2016; Kol et al., 2017). While Transformer is known\nto have better memory due to its quadratic attention mechanism, our results on learning sparse parities\nbrings forth the question on how different architectures may utilize its memory differently in terms\n9\nof function approximation. We leave the theoretical and empirical question of which architectural\ncomponent allows for learning parities as an avenue for further study.\n5\nThe Advantage of Hybrid Architectures for In-context Learning\nFeed\nForward\nMulti-Head\nAttention\nPositional \nEmbedding\nInputs\nOutputs\n(a) Transformer\n(b) Mamba\n(c) Standard Hybrid\n(d) MambaFormer\nMamba\nMamba\nInputs\nOutputs\nMamba\nMulti-Head\nAttention\nPositional \nEmbedding\nInputs\nOutputs\nMamba\nMulti-Head\nAttention\nInputs\nOutputs\nMamba\nFigure 6: Model Architectures. (a) and (b) denote the standard Transformer and Mamba architectures.\n(c) denotes the hybrid architecture of Mamba and Attention blocks, following the design proposed\nin Gu & Dao (2023). (d) demonstrates the proposed architecture, namely MambaFormer, which\nreplaces the Positional Encoding with a Mamba block. For convenience, we denote 2 blocks of either\nMamba, Multi-head Attention, or a Feed Forward Network as 1 layer.\nIn the previous section, we have observed that Transformers perform better than SSMs in some\ntasks, such as learning decision trees or retrieval, while SSMs excel in others, such as learning sparse\nparities or learning heavy-outlier linear regression, possibly due to its recurrent nature. However, can\nwe achieve the best of both worlds without sacrificing performance in our suite of ICL tasks?\nWe answer this in the affirmative; that we can indeed reach competitive performance in our suite of ICL\ntasks, achieving performance comparable to that of Transformers and Mamba, while simultaneously\nexcelling in specific tasks that either fail in. We can achieve strong performance by interleaving\nAttention and Mamba, where a key ingredient is having Mamba as the first layer.\nIn this section, we investigate two hybrid architectures that combine Transformer and Mamba, namely\nStandard Hybrid and MambaFormer as illustrated in Figure 6. Standard Hybrid is the architecture\nof interleaving MHA and Mamba by replacing the MLP block with Mamba. MambaFormer is nearly\nidentical to Standard Hybrid but with an additional Mamba block as its initial layer and no particular\npositional encoding. Although many works have found that interleaving multi-head attention and\nLTI SSMs beneficial (Zuo et al., 2022; Mehta et al., 2022; Pilault et al., 2023), interestingly Gu &\nDao (2023) have not found significant benefits of interleaving. In the following results, we show that\ninterleaving with Mamba as its initial layer can help solve both sparse parity and retrieval, each task\nunsolvable by Mamba and Transformer.\n5.1\nSimultaneously learning parities and retrieval\nAs highlighted in Bhattamishra et al. (2023); Barak et al. (2022), learning sparse parity in-context\nseems to be difficult for Transformer and some SSMs like Hyena. Yet interestingly, as seen in Figure 7,\nMambaFormer successfully learns parity as quickly as Mamba in terms of sample complexity. While\nthe Standard Hybrid model is also capable, it exhibits much worse sample efficiency.\nWe perform an ablation study by equipping Transformer with an initial Mamba block without any\npositional encoding. Although this variant Transformer only has fewer Mamba blocks than Standard\nHybrid, it solves parity almost as efficiently as Mamba. Not only does this show us that order of\nlayers in interleaving matter, as shown in Press et al. (2022), but also that Mamba can complement\nTransformer without hurting performance in ICL. This result brings up intriguing difference between\nthe function learning capabilities of Attention and Mamba; we leave this question up for further study.\n10\n105\n106\nTotal Parameters (non-embedding)\n0\n100000\n200000\n300000\n400000\n500000\nMedian Convergence Time (iterations)\nSparse Parity ICL Convergence\nTransformer\nMamba\nStandard Hybrid\nMambaFormer\nTF w/ initial Mamba\nFigure 7: Median convergence time of learning parity over 5 random seeds for max. 500k iterations,\nwhere 500k convergence time signifies failed learning. Having the initial layer as Mamba is essential\nfor efficiently learning parities. Tested model configurations are specified in Appendix A.\nClosing the gap in retrieval.\nThe gap between Mamba and Transformer in vector-valued MQAR\ntask is largely due to the fact that Mamba (as an SSM) compresses context into smaller states when\ngenerating output, while the Attention mechanism in Transformer does not compress the context. The\namount of information about the context Mamba has at each state depends on the dimension of hidden\nstate (as the hidden states capture the important information in the context) and it is challenging if the\ntask is to accurately retrieve a specific part of the context by a query that is placed after the context.\nTo close the gap in the vector-valued MQAR task between Mamba and Transformer, and without\nsacrificing too much of the efficiency, we add one attention layer within layers of Mamba blocks. In\nparticular, in a Mamba model of 4 layers (8 Mamba blocks stacked homogeneously), we replace the\nmiddle two blocks with Standard Hybrid (w/o positional embedding). As shown in Table 3, Mamba\nmodel gains a significant improvement in vector-valued MQAR by having one Standard Hybrid. We\nfurther test MambaFormer on the same task and find that MambaFormer almost entirely closes the\ngap to transformer in vector-valued MQAR task.\n5.2\nAll-in-one ICL performance\nWhile MambaFormer succeeds in two tasks that were either deemed difficult for Mamba for Trans-\nformer, it also performs equally as well as Transformer and Mamba in the rest of our suite of ICL\ntasks. In Figure 2, we see that MambaFormer and Standard Hybrid both learn decision trees as well\nas Transformer, even at larger parameter sizes. Even more surprisingly, MambaFormer efficiently\nlearns linear regression better than both models even in the presence of 90% noisy data in Many-\noutlier regression, as a MambaFormer trained on 100k iterations (< 1017 FLOPs) performs as well\nas models trained with 10 times the number of FLOPs.\nIn conclusion, we find the best of both worlds within our diverse array of ICL tasks; a hybrid\narchitecture that can solve as difficult problems as retrieval and parity, while performing on par with\nTransformer and Mamba in other ICL tasks. Given our results, it will be interesting to see how\nhybrid architectures perform in other kinds of ICL tasks, such as those discussed in (Xie et al., 2021;\nAky\u00fcrek et al., 2024).\n11\n6\nDiscussion\nIn this work, we have provided a comprehensive investigation of in-context learning with state-space\nmodels (SSMs) and contrasted them with the transformer architecture. Our study has revealed that\nSSMs, especially Mamba, are capable in-context learners. On the other hand, our evaluations revealed\nthat neither SSMs nor transformers are great at all tasks, specifically, SSMs struggle with decision\ntree learning and retrieval tasks whereas transformers struggle with sparse parity. This has led us\nto the hybrid architecture MambaFormer which achieves a best-of-both-worlds performance on our\nICL suite.\nFuture research directions include exploring (1) how performance on our ICL suite correlates with\ngeneral language modeling capabilities, such as perplexity on standard NLP benchmarks, (2) the\npotential for developing more effective architectures by integrating elements from transformers,\nSSMs, and gating mechanisms, (3) identifying architectural features that contribute to effective\nin-context learning, and (4) assessing the impact of MambaFormer and other innovative architectures\non language modeling performance.\n12\nReferences\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned\ngradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. 3\nAky\u00fcrek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context\nlearning? investigations with linear models. In The Eleventh International Conference on Learning\nRepresentations, 2022. 1, 3\nAky\u00fcrek, E., Wang, B., Kim, Y., and Andreas, J. In-context language learning: Architectures and\nalgorithms. arXiv preprint arXiv:2401.12973, 2024. URL https://arxiv.org/abs/2401.\n12973. 4, 11\nArora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and R\u00e9, C. Zoology:\nMeasuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927,\n2023. URL https://arxiv.org/abs/2312.04927. 2, 4, 6\nBai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context\nlearning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. 1\nBarak, B., Edelman, B., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden progress in\ndeep learning: Sgd learns parities near the computational limit. Advances in Neural Information\nProcessing Systems, 35:21750\u201321764, 2022. 10\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020. 3\nBhattamishra, S., Patel, A., Blunsom, P., and Kanade, V. Understanding in-context learning in\ntransformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016,\n2023. URL https://arxiv.org/abs/2310.03016. 3, 5, 9, 10\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020. 1\nChan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill,\nF. Data distributional properties drive emergent in-context learning in transformers. Advances in\nNeural Information Processing Systems, 35:18878\u201318891, 2022. 1\nDai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn in-context? lan-\nguage models secretly perform gradient descent as meta-optimizers. In Findings of the Association\nfor Computational Linguistics: ACL 2023, pp. 4005\u20134019, 2023. 3\nDao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and R\u00e9, C. Hungry hungry hippos:\nTowards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022.\nURL https://arxiv.org/abs/2212.14052. 2, 3\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A.,\nNabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language\nmodeling, 2020. 2\nGarg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a\ncase study of simple function classes. Advances in Neural Information Processing Systems, 35:\n30583\u201330598, 2022. 1, 2, 3, 5, 16\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752, 2023. URL https://arxiv.org/abs/2312.00752. 1, 2, 3, 4, 5, 7,\n10, 16\nGu, A., Goel, K., Gupta, A., and R\u00e9, C. On the parameterization and initialization of diagonal\nstate space models. In Advances in Neural Information Processing Systems, volume 35, pp.\n35971\u201335983, 2022a. 3, 4\n13\nGu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces.\nIn The Tenth International Conference on Learning Representations, ICLR 2022, 2022b. URL\nhttps://openreview.net/forum?id=uYLFoz1vlAC. 2, 3\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\nWu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,\n2020. 1, 5\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In Proceedings of the 37th International Conference on Machine\nLearning, ICML 2020, volume 119 of Proceedings of Machine Learning Research, pp. 5156\u20135165,\n2020. URL http://proceedings.mlr.press/v119/katharopoulos20a.html. 3\nKingma, D. P. and Ba, J.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 4, 16\nKol, G., Raz, R., and Tal, A. Time-space hardness of learning sparse parities. In Proceedings of the\n49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1067\u20131080, 2017. 9\nLi, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. Transformers as algorithms: Generalization\nand stability in in-context learning, 2023a. 1\nLi, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. Dissecting chain-of-thought:\nCompositionality through in-context filtering and learning. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023b. 1, 6, 16\nMahankali, A., Hashimoto, T. B., and Ma, T. One step of gradient descent is provably the optimal\nin-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. 3\nMehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated\nstate spaces. arXiv preprint arXiv:2206.13947, 2022. 10\nMin, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context.\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 2791\u20132809, 2022a. 1, 3\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Re-\nthinking the role of demonstrations: What makes in-context learning work?\narXiv preprint\narXiv:2202.12837, 2022b. URL https://arxiv.org/abs/2202.12837. 1\nMuennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf,\nT., and Raffel, C. Scaling data-constrained language models. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. URL https://openreview.net/forum?id=\nj5BuTrEj35. 1\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A.,\nBai, Y., and et al., A. C. In-context learning and induction heads. arXiv preprint arXiv:2209.11895,\n2022. URL https://arxiv.org/abs/2209.11895. 3, 4, 6\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M.,\nGrella, M., Kiran, K. G., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint\narXiv:2305.13048, 2023. URL https://arxiv.org/abs/2305.13048. 2, 4\nPilault, J., Fathi, M., Firat, O., Pal, C., Bacon, P.-L., and Goroshin, R. Block-state transformers. In\nThirty-seventh Conference on Neural Information Processing Systems, 2023. 10\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C.\nHyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866,\n2023. URL https://arxiv.org/abs/2302.10866. 2, 4\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing\nthe compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. 10\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 16\n14\nRavi, S. and Larochelle, H. Optimization as a model for few-shot learning. In International conference\non learning representations, 2016. 1, 3\nRaz, R. Fast learning requires good memory: A time-space lower bound for parity learning. In 2016\nIEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 266\u2013275. IEEE,\n2016. 9\nSchaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models a\nmirage? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=ITw9edRDlD. 1\nSchmidhuber, J., Zhao, J., and Wiering, M. Shifting inductive bias with success-story algorithm,\nadaptive levin search, and incremental self-improvement. Machine Learning, 28:105\u2013130, 1997. 1\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A\nsuccessor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. URL\nhttps://arxiv.org/abs/2307.08621. 2, 4\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\nL., and Polosukhin, I.\nAttention is all you need.\nIn Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, pp. 5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. 1\nvon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and\nVladymyrov, M. Transformers learn in-context by gradient descent. In International Conference\non Machine Learning, pp. 35151\u201335174. PMLR, 2023a. 1\nvon Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N.,\nSandler, M., Vladymyrov, M., Pascanu, R., et al. Uncovering mesa-optimization algorithms in\ntransformers. arXiv preprint arXiv:2309.05858, 2023b. URL https://arxiv.org/abs/2309.\n05858. 1, 3\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768, 2020. 3\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M.,\nZhou, D., Metzler, D., et al.\nEmergent abilities of large language models.\narXiv preprint\narXiv:2206.07682, 2022. 1\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit\nbayesian inference. In International Conference on Learning Representations, 2021. 4, 11\nYang, L., Lee, K., Nowak, R., and Papailiopoulos, D. Looped transformers are better at learning\nlearning algorithms, 2023a. 1\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with\nhardware-efficient training. arXiv preprint arXiv:2312.06635, 2023b. 2, 4\nYu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually\nwhat you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 10819\u201310829, 2022. 9\nZuo, S., Liu, X., Jiao, J., Charles, D., Manavoglu, E., Zhao, T., and Gao, J. Efficient long sequence\nmodeling via state space augmented transformer. arXiv preprint arXiv:2212.08136, 2022. 10\n15\nA\nExperimental Setup\nA.1\nModel Architectures\nWe focus on decoder-only Transformer models, particularly those from the GPT-2 family (Rad-\nford et al., 2019), Mamba (Gu & Dao, 2023), and their Hybrid variants, including Standard and\nMambaFormer configurations. These models are evaluated across a range of sizes, as detailed in\nTable 4. Transformer layers consist of a Multi-Head Attention (MHA) block followed by a Multilayer\nPerceptron (MLP) block. Mamba models consist of two Mamba blocks per layer. The Hybrid variants\nmerge these approaches, combining a single MHA block with a Mamba block. For MHA blocks, we\nuse 8 number of heads. Refer to Figure 6 for a visualization of the architectures considered.\nA.2\nModel Training\nWe train all of our models on A100-SXM4-40GB GPUs for 500,000 training steps on all tasks. We\nuse Adam optimizer Kingma & Ba (2014) with a fixed learning rate. The default value is set to\n0.0001, following the default learning rate in Garg et al. (2022), and search various learning rates\nin {5e \u2212 5, 1e \u2212 4, 2e \u2212 4, 4e \u2212 4}. We observe that the training procedure is the most sensitive to\nchoosing the right learning rate. In particular, as the number of parameters of the models increases,\nthe training procedure is prone to gradient explosions, especially in Mamba and hybrid architecutres.\nHence, we clip the gradient norm, with values in {5.0, 10.0, 50.0}.\nAs for the train and test data, we fix the dimension of x to be 20, and fix the batch size to be 64. As\nsuggested in Garg et al. (2022), we also observe that curriculum is crucial in certain ICL tasks. We\nadopt a curriculum of 15 steps every 2000 steps both on the dimension of x and the number of points\n(half the length of the training prompt).\nTable 4: Different configurations of Models.\n# layers\nembed dim\nStandard\n12\n768\nSmall\n8\n512\nX-small\n4\n256\nXX-small\n2\n128\nB\nImplementation Details\nThis section further elaborates on the task descriptions from Section 3.\nB.1\nChain-of-Thought-I/O results\nTable 5 presents the configurations for the Chain-of-Thought-I/O task using a 2-layer ReLU neural\nnetwork, following the setup described by Li et al. (2023b). In the model scale experiment, the input\ndimension d = 10 and hidden layer dimension k = 8 are held constant while varying the model scale.\nAdditionally, the hidden dimension k is varied among 4, 8, 16 while fixing the model scale to small\nto identify the effect of problem scale.\nTable 5: Model configurations for Chain-of-Thought-I/O experiments.\nModel\n# layers\nembed dim\n# heads (MHA)\nStandard\n12\n256\n8\nSmall\n6\n128\n4\nTiny\n3\n64\n2\n16\nB.2\nVector-valued MQAR\nThe training set consists of 300, 000 training samples. We train for 64 epochs with batch size of\n64 and evaluate on a test set of 3, 000 samples. For each setting, we sweep with learning rates in\nnp.logspace(-4, -2, 4) and report the best result among all learning rates.\nB.3\nOrthogonal-outlier Regression\nWe run Orthogonal-outlier regression on all four model architectures, with varying number of\nparameters. The curves above generally capture the trend that Mamba and Transformer perform on\npar with each other, and Standard Hybrid and MambaFormer show better curves compared to the\nvanilla models.\nOne curve that stands out would be Mamba\u2019s loss curve in the smaller regime. As a future direction,\nwe have tested trading off the depth and width while fixing the total number of parameters with the\nsmallest Mamba model configuration in Table 6.\nTable 6: Various Mamba model configurations with fixed number of parameters.\nModel\n# layers\nembed dim\nStandard\n4\n128\n2\n192\n8\n96\n16\n64\nThe best performing model configuration was with 16 layers, and embedding dimension of 64. This\nsuggests that with fixed number of parameters, increasing the depth may boost the downstream task\nperformance on ICL tasks.\nC\nFLOPs computation\nWe count the number of multiplications in a Mamba block and a Transformer block in Table 7\nand Table 8. We assume batch size B = 1. To calculate FLOPs, we multiply the number of\nmultiplications by 6 to account for the multiply-accumulate cost in both forward and backward pass.\nNote that a Standard Hybrid block is an attention block stacked with a Mamba block, so the number\nof multiplications in a Standard Hybrid block is 10LD2 + 2L2D, ignoring the linear terms.\nTable 7: Number of multiplications in a Transformer block.\nNumber of multiplications\nQKV projection\n3LD2\nOuter product and multiply V\n2L2D\nOuter projection\nLD2\nFFN with ffw_width=4\n8LD2\nTable 8: Number of multiplications in a Mamba block. Unless specified otherwise, we assume E = 2\nand R = 2.\nNumber of multiplications\nInput projection\n2LED2\nSSM\n7LEDN + 2RLED\nOutput projection\nLED2\n17\n"
  },
  {
    "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
    "link": "https://arxiv.org/pdf/2402.04252.pdf",
    "upvote": "19",
    "text": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters\nQuan Sun1*\nJinsheng Wang1\u2217\nQiying Yu1,2\u2217\nYufeng Cui1\nFan Zhang1\nXiaosong Zhang1\nXinlong Wang1\n1 Beijing Academy of Artificial Intelligence\n2 Tsinghua University\n\u2217equal contribution\ncode & models: baaivision/EVA/EVA-CLIP-18B\nAbstract\nScaling up contrastive language-image pretraining\n(CLIP) is critical for empowering both vision and multi-\nmodal models.\nWe present EVA-CLIP-18B, the largest\nand most powerful open-source CLIP model to date, with\n18-billion parameters. With only 6-billion training sam-\nples seen, EVA-CLIP-18B achieves an exceptional 80.7%\nzero-shot top-1 accuracy averaged across 27 widely rec-\nognized image classification benchmarks, outperforming\nits forerunner EVA-CLIP (5-billion parameters) and other\nopen-source CLIP models by a large margin. Remarkably,\nwe observe a consistent performance improvement with the\nmodel size scaling of EVA-CLIP, despite maintaining a\nconstant training dataset of 2-billion image-text pairs from\nLAION-2B and COYO-700M. This dataset is openly avail-\nable and much smaller than the in-house datasets (e.g., DFN-\n5B, WebLI-10B) employed in other state-of-the-art CLIP\nmodels. EVA-CLIP-18B demonstrates the potential of EVA-\nstyle [30, 29, 63] weak-to-strong visual model scaling. With\nour model weights made publicly available, we hope to fa-\ncilitate future research in vision and multimodal foundation\nmodels.\n1. Introduction\nRecent years witnessed the rapid growth of Large Mul-\ntimodal Models (LMMs) [3, 64, 62, 69, 5, 46], with CLIP\nmodels [53, 19, 63, 43, 75, 28, 17] serving as a foundational\nvision encoder to deliver robust and transferable visual rep-\nresentations, and Large Language Models (LLMs) [65, 54]\nserving as a general interface for reasoning across different\nmodalities. However, as LLMs have scaled up to around\n100B parameters or higher [11, 20, 65], the adopted vision\nfoundation models continue to operate at a much smaller\nscale, lagging far behind the LLMs.\n*Correspondence to wangxinlong@baai.ac.cn\nFigure 1: Scaling behavior of EVA-CLIP with zero-shot classi-\nfication performance averaged across 27 image classification\nbenchmarks, compared with the current state-of-the-art and largest\nCLIP models (224px). The diameter of each circle demonstrates\nthe forward GFLOPs \u00d7 the number of training samples seen. The\nperformance of EVA-CLIP consistently improves as scaling up.\nThis paper introduces EVA-CLIP-18B, the largest open-\nsource CLIP model with 18-billion parameters to narrow\nthis gap. EVA-CLIP [63] open-sources a series of effective\nand efficient CLIP models, which have been leveraged as the\nvision foundation by numerous impactful works across 2D /\n3D vision and multimodal modeling [42, 78, 77, 50, 69, 64].\nWe further scale up EVA-CLIP to this significant parameter\nsize building upon the scaling philosophy of EVA [30, 29]\nand EVA-CLIP [63]. With merely 6-billion training samples\nseen and trained on publicly available datasets, EVA-CLIP-\n18B achieves the exceptional 80.7% average zero-shot top-\n1 accuracy on 27 widely recognized image classification\nbenchmarks, significantly surpassing its forerunner EVA-02-\nCLIP-E/14+ (5-billion parameters) and other open-source\nCLIP models. Besides, the models have not exhibited any\n1\narXiv:2402.04252v1  [cs.CV]  6 Feb 2024\ntotal\nimage\ntext\nsamples image batch\nimage cls. video cls. retrieval\nmodel+\n#param. #param. #param.\ndata\nseen\nsize\nsize gpus for training avg. acc. avg. acc.\nMR\nEVA-01-CLIP-g/14+ [63]\n1.1B\n1.0B\n124M\nLAION-400M [59]\n11B\n2242\n41k\n256\u00d7A100 (40GB)\n72.2\n66.2\n80.9\nEVA-01-CLIP-g/14+ [63]\n1.3B\n1.0B\n354M\nMerged-2B [63]\n11B\n2242\n114k 112\u00d7A100 (40GB)\n75.1\n68.8\n85.3\nOpenCLIP-G/14+ [2]\n2.5B\n1.8B\n695M\nLAION-2B [58]\n39B\n2242\n160k 512\u00d7A100 (80GB)\n76.2\n68.7\n85.7\nInternVL-C+ [17]\n14.0B\n6.0B\n8.0B\ncustom [17]\n29B\n2242\n164k 640\u00d7A100 (80GB)\n78.0\n73.7\n86.6\nDFN5B-CLIP-H/14+ [28]\n1.0B\n632M\n354M\nDFN-5B [28]\n39B\n2242\n79k\nTPUv4\n78.3\n67.0\n86.6\nEVA-02-CLIP-E/14+ [63]\n5.0B\n4.4B\n695M\nLAION-2B [58]\n9B\n2242\n144k 144\u00d7A100 (80GB)\n78.7\n72.1\n85.7\nDFN5B-CLIP-H/14+ [28]\n1.0B\n632M\n354M\nDFN-5B [28]\n5B\n3782\n79k\nTPUv4\n79.2\n68.4\n87.2\nEVA-CLIP-8B +\n8.1B\n7.5B\n695M\nMerged-2B [63]\n9B\n2242\n178k 384\u00d7A100 (40GB)\n79.4\n73.6\n86.2\nEVA-CLIP-18B +\n18.1B\n17.5B\n695M\nMerged-2B+\n6B\n2242\n108k 360\u00d7A100 (40GB)\n80.7\n75.0\n87.8\nTable 1: CLIP model configurations and zero-shot performance on 33 benchmarks including 27 image classification, 4 video\nclassification and 2 image-text retrieval datasets. DFN-5B [28] are 5B images filtered from a pool of 43B uncurated image-text pairs\nconsisting of 12.8B image-text pairs from CommonPool-12.8B [32] and 30B additional public image-text pairs. The dataset used for training\nInternVL-C [17] is custom mixtures, see detail in [17].\nmethod\nImageNet-1K [26]\nImageNet-V2 [57]\nImageNet-Adv. [36]\nImageNet-Ren. [35]\nImageNet-Ske. [68]\nObjectNet [8]\nCIFAR-10 [40]\nCIFAR-100 [40]\nMNIST [41]\nCaltech101 [31]\nSUN397 [72]\nFGVC Aircraft [47]\nCountry-211 [53]\nStanford Cars [39]\nBirdsnap [9]\nDTD [21]\nEuroSAT [34]\nFER2013 [33]\nFlowers-102 [49]\nFood-101 [10]\nGTSRB [61]\nPCam [67]\nPets [51]\nRendered SST2 [53]\nRESISC45 [18]\nSTL-10 [23]\nVOC2007 [27]\n.avg. top-1 acc.\nEVA-01-CLIP-g/14+ 78.5 71.5 73.6 92.5 67.6 72.3 98.3 88.7 62.6 87.7 74.2 32.4 28.9 91.7 65.8 61.7 73.8 52.2 74.5 93.5 49.3 49.9 94.2 58.4 70.3 98.9 85.7 72.2\nEVA-01-CLIP-g/14+ 79.3 72.5 74.1 92.7 68.4 75.3 99.1 90.1 72.0 89.5 74.7 39.9 31.8 90.7 70.2 67.8 73.2 56.0 79.7 93.7 66.5 62.4 94.9 58.6 71.4 99.5 84.7 75.1\nOpenCLIP-G/14+\n80.4 73.6 69.3 92.8 69.9 73.0 98.3 87.5 71.6 89.4 75.0 53.6 34.9 94.9 73.0 69.1 71.1 59.6 81.5 93.1 62.7 63.6 95.3 65.3 72.6 98.5 87.4 76.2\nInternVL-C +\n83.2 77.3 83.8 95.7 74.3 80.6 99.4 93.1 80.6 89.5 76.3 53.3 35.1 94.4 69.2 70.8 79.4 56.2 85.8 95.3 65.5 48.7 96.3 68.4 74.4 99.4 80.0 78.0\nDFN5B-CLIP-H/14 + 83.5 77.4 71.7 92.9 72.8 76.7 98.8 90.5 85.8 89.5 77.0 71.4 34.4 95.8 77.4 70.7 65.2 54.7 92.5 95.8 67.7 65.2 96.5 54.8 76.1 98.9 81.5 78.3\nEVA-02-CLIP-E/14+ 82.1 75.7 82.1 94.7 72.2 79.6 99.3 93.2 74.7 90.5 75.3 58.7 37.0 94.7 77.6 68.2 75.9 59.0 84.5 94.9 67.7 64.4 96.0 62.6 75.7 99.3 87.9 78.7\nDFN5B-CLIP-H/14+ 84.3 78.3 79.6 93.6 73.3 79.6 98.8 90.5 83.6 88.9 77.4 72.5 37.9 96.0 80.5 70.9 61.1 56.1 91.6 96.2 67.9 69.6 96.8 55.5 75.9 99.1 81.9 79.2\nEVA-CLIP-8B +\n83.5 77.7 85.2 95.3 74.3 81.2 99.3 92.3 84.8 89.6 76.2 60.5 41.7 94.8 79.0 71.0 68.9 56.1 86.4 95.5 70.9 58.1 96.4 66.2 75.3 99.3 85.1 79.4\nEVA-CLIP-18B +\n83.8 77.9 87.3 95.7 74.7 82.2 99.4 93.8 83.0 89.8 77.7 59.7 43.1 94.9 79.9 72.1 79.8 59.3 86.0 95.8 72.4 65.2 96.1 67.5 76.9 99.6 85.8 80.7\nTable 2: EVA-CLIP zero-shot image classification performance on 27 datasets. We report top-1 accuracy on all datasets. The best results\nare in bold and the second best are underlined.\nsignal of performance saturation, shedding light on further\nscaling of vision models. An intuitive demonstration is\nshown in Figure 1.\nThe successful training of EVA-CLIP-18B exemplifies\nthe potential of the EVA-style visual model scaling philoso-\nphy. We keep open-sourcing the training code and weights of\nour models to encourage further research and empower the\ndevelopment of vision and multimodal foundation models.\n2. Weak-to-Strong Vision Scaling\nOur scaling-up procedure is guided by the principles of\nEVA [30] and EVA-CLIP [63]. The EVA philosophy for\nscaling visual models follows a weak-to-strong paradigm,\ndesigned to improve visual models through a strategic pro-\ngression. This process begins with a large EVA vision model\ndistilling knowledge from a small EVA-CLIP model, which\nin turn serves as the vision encoder initialization to stabilize\nand accelerate the training of a larger EVA-CLIP. After\nthat, the closed-loop scaling-up cycle continues and a larger\nEVA is distilled out. Throughout our model scaling cycle,\nthe training dataset remains largely fixed to demonstrate the\neffectiveness of our model-scale specific scaling philosophy,\nalthough scaling up datasets can further unleash the scaling\npower of our method.\nSpecifically, in this work, we pre-train a large EVA model\nnamed EVA-18B using a small EVA-CLIP (EVA-02-CLIP-\nE/14+) [63] as the teacher, which is trained to reconstruct\nmasked image-text aligned vision features from EVA-02-\nCLIP-E/14+. EVA-18B omits bias terms of QKV projec-\ntions and uses RMSNorm [76] instead of LayerNorm [4]\nfollowing LLaMA [65]. Subsequently, we leverage the EVA\nmodel as the vision encoder initialization for EVA-CLIP\npre-training with the image-text contrastive learning objec-\ntive. Besides, we also introduce a smaller counterpart, EVA-\nCLIP-8B, which undergoes similar pre-training methodolo-\ngies. Notably, our experiments demonstrate sustained perfor-\nmance improvement with the progressive weak-teach-strong\nscaling up of EVA-CLIP.\n3. Experiments\nSettings. Following EVA-CLIP [63], we initialized the\nmodel with pre-trained vision and text encoders. Specifi-\n2\nzero-shot text retrieval\nzero-shot image retrieval\nFlickr30K\nCOCO\nFlickr30K\nCOCO\nmethod+\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nMR\nEVA-01-CLIP-g/14 +\n87.9\n98.0\n99.5\n61.7\n83.2\n89.9\n72.5\n91.5\n95.4\n44.5\n69.1\n77.7\n80.9\nEVA-01-CLIP-g/14+\n93.3\n99.5\n99.9\n69.4\n88.3\n93.2\n79.2\n95.2\n97.3\n51.1\n74.7\n82.5\n85.3\nOpenCLIP-G/14+\n93.5\n99.3\n99.7\n69.0\n87.8\n93.1\n80.9\n95.1\n97.2\n52.6\n76.1\n83.6\n85.7\nEVA-02-CLIP-E/14+\n94.3\n99.6\n99.8\n69.4\n88.6\n93.3\n79.7\n94.9\n97.3\n52.5\n75.9\n83.4\n85.7\nEVA-CLIP-8B +\n95.6\n99.6\n99.9\n70.3\n89.3\n93.9\n80.8\n95.5\n97.6\n53.0\n76.0\n83.4\n86.2\nDFN5B-CLIP-H/14 +\n92.9\n99.3\n99.9\n72.3\n90.2\n94.6\n80.1\n95.2\n97.3\n53.9\n78.0\n85.6\n86.6\nInternVL-C +\n93.8\n99.7\n100.0\n70.3\n89.2\n93.8\n82.1\n96.0\n98.1\n54.1\n77.1\n84.8\n86.6\nDFN5B-CLIP-H/14 +\n93.6\n99.3\n99.6\n71.8\n90.4\n94.9\n82.1\n96.0\n97.9\n55.6\n79.2\n86.3\n87.2\nEVA-CLIP-18B +\n96.7\n99.7\n100.0\n73.6\n90.9\n95.0\n83.3\n96.3\n98.3\n56.2\n78.5\n85.6\n87.8\nTable 3: Zero-shot retrieval performance on Flickr30K [74] and COCO [45].\ncally, we employ a pre-trained EVA-18B [30, 29] as the\nvision encoder and EVA-02-CLIP-E/14+ [63] for the text\nencoder. We adopt the LAMB optimizer [73] with \u03b21 = 0.9,\n\u03b22=0.95, and a weight decay of 0. We apply different learn-\ning rates and layer decay rates to the vision encoder and text\nencoder to ensure optimal training. We set the peak learning\nrate as 4e-4 and 4e-5 for the vision encoder and the text\nencoder respectively, with 2000 warm-up steps. Afterwards,\nthe learning rates decay to 0 with a cosine schedule. The\nlearning rate layer decay rates are configured as 0.9 and 0.75\nfor the vision and text encoders. The temperature parameter\nremains constant at 0.01. Further, we use the DeepSpeed\noptimization library [56] with ZeRO stage-3 partition [55],\ngradient checkpointing [16] and flash attention [24] to opti-\nmize the training cost.\nDataset. Our Merged-2B dataset consists of 1.6 billion sam-\nples from LAION-2B [58] and 0.4 billion samples from\nCOYO-700M [12].\nNote that the use of a subset from\nLAION-2B is not the result of deliberate filtering, but rather\ndue to image downloading failures. The use of 0.4 billion\nCOYO-700M samples aims to complement the number of\ntraining samples to nearly the same as LAION-2B. Merged-\n2B+ consists of all samples from Merged-2B, along with ad-\nditional 20 million samples from LAION-COCO [1] and 23\nmillion samples from Merged-video including VideoCC [48],\nInternVid [70] and WebVid-10M [6]. Merged-video is in-\ncluded at the end of the training process.\nEVA-CLIP-18B pre-trains with 5.4 billion samples from\nMerged-2B seen with 50% of patch dropout ratio [44], 0.6\nbillion samples from Merged-2B and 20 million samples\nfrom LAION-COCO without patch dropout, and 24 million\nsamples from Merged-video with 50% of patch dropout ratio.\nEvaluation. We evaluate on 33 widely used datasets across\nimage, video classification and image-text retrieval. All\ndatasets used to evaluate EVA-CLIP-18B are reported in\nTable 11. We utilize the specified prompt templates follow-\ning [53, 38].\nimage encoder\ntext encoder\n# params\nmethod\nlayers width heads layers width heads image text\ntotal\nEVA-CLIP-8B +\n32\n4096\n32\n32\n1280\n20\n7.5B 695M 8.1B\nEVA-CLIP-18B +\n48\n5120\n40\n32\n1280\n20\n17.5B 695M 18.1B\nTable 4: Architecture configurations.\nZero-Shot Image Classification. We show the exceptional\nperformance of EVA-CLIP on all 27 zero-shot image classi-\nfication benchmarks in Table 2. EVA-CLIP-18B achieves\n80.7% top-1 accuracy averaged across all 27 benchmarks.\nThese results significantly outperform the previous best open-\nsourced DFN5B-CLIP-H/14+ [28] by +1.5%, and the largest\nexisting CLIP model, InternVL-C [17], by +2.7%. For Bird-\nsnap dataset, the download was limited to 2195 test images\ndue to broken links.\nmethod+\n#Frames UCF-101 K-400 K-600 K-700 avg.\nEVA-01-CLIP-g/14+\n1\n76.0\n65.4\n64.5\n58.8\n66.2\nDFN5B-CLIP-H/14+\n1\n78.2\n65.2\n65.5\n59.2\n67.0\nDFN5B-CLIP-H/14+\n1\n79.2\n66.7\n67.0\n60.7\n68.4\nOpenCLIP-G/14+\n1\n80.5\n67.1\n66.9\n60.3\n68.7\nEVA-01-CLIP-g/14+\n1\n78.9\n67.3\n67.3\n61.5\n68.8\nEVA-02-CLIP-E/14+\n1\n83.1\n70.7\n70.0\n64.4\n72.1\nEVA-CLIP-8B +\n1\n85.7\n71.3\n71.2\n66.1\n73.6\nInternVL-C +\n1\n85.2\n71.8\n71.7\n66.4\n73.7\nEVA-CLIP-18B +\n1\n86.0\n72.9\n72.9\n68.2\n75.0\nEVA-CLIP-18B +\n8\n88.2\n79.3\n79.2\n72.1\n79.7\nEVA-CLIP-18B +\n16\n88.4\n79.4\n79.4\n72.2\n79.8\nTable 5: EVA-CLIP zero-shot video classification performance.\nWe report top1 accuracy for UCF-101 [60], average of top1\nand top5 accuracy for Kinetics-400 [15], Kinetics-600 [13] and\nKinetics-700 [14].\nZero-Shot Video Classification. We report the top-1 accu-\nracy for UCF-101 [60] and the mean of top-1 and top-5 accu-\nracy for Kinetics-400 [15], Kinetics-600 [13] and Kinetics-\n700 [14]. In Table 5 we demonstrate that EVA-CLIP-18B\nalso outperforms other CLIP models on zero-shot video clas-\n3\nmethod+\nIN-1K\nIN-A\nIN-R\nIN-V2\nIN-Sketch\nObjectNet\n\u2206\u2193\navg. acc.\nDFN5B-CLIP-H/14+\n83.5\n71.7\n92.9\n77.4\n72.8\n76.7\n4.4\n79.2\nOpenCLIP-G/14+\n80.4\n69.3\n92.8\n73.6\n69.9\n73.0\n3.9\n76.5\nSigLIP-SO [75] (reported)\n82.0\n71.9\n95.1\n76.1\n74.0\n70.6\n3.7\n78.3\nDFN5B-CLIP-H/14+\n84.3\n79.6\n93.6\n78.3\n73.3\n79.6\n2.8\n81.5\nEVA-01-CLIP-g/14 +\n78.5\n73.6\n92.5\n71.5\n67.6\n72.3\n2.5\n76.0\nEVA-01-CLIP-g/14+\n79.3\n74.1\n92.7\n72.5\n68.4\n75.3\n2.2\n77.1\nBASIC-L [52] (reported)\n85.7\n85.6\n95.7\n80.6\n76.1\n82.3\n1.4\n84.3\nSigLIP-SO+ [75] (reported)\n83.0\n82.5\n95.8\n77.2\n74.5\n77.0\n1.3\n81.7\nEVA-02-CLIP-E/14+\n82.1\n82.1\n94.7\n75.7\n72.2\n79.6\n1.0\n81.1\nInternVL-C +\n83.2\n83.8\n95.7\n77.3\n74.3\n80.6\n0.7\n82.5\nEVA-CLIP-8B +\n83.5\n85.2\n95.3\n77.7\n74.3\n81.2\n0.6\n82.9\nEVA-CLIP-18B +\n83.8\n87.3\n95.7\n77.9\n74.7\n82.2\n0.2\n83.6\n(a) Zero-shot performance on ImageNet variants and ObjectNet. \u201cavg. acc.\u201d: the averaged top-1 accuracy on different ImageNet variants (i.e., IN-{1K, V2,\nReaL, Adv., Ren., Ske.}), and ObjectNet. \u201c\u2206\u2193\u201d: The gap between the averaged top-1 accuracy and the ImageNet-1K top-1 accuracy (the lower the better).\nEVA-CLIP suffers from the smallest performance drop (only 0.2% top-1 accuracy gap for EVA-CLIP-18B) while EVA-CLIP-18B achieves 83.6% top-1\naccuracy averaged on all 6 benchmarks.\nmethod\nImageNet-1K [26]\nImageNet-V2 [57]\nImageNet-Adv. [36]\nImageNet-Ren. [35]\nImageNet-Ske. [68]\nObjectNet [8]\nCIFAR-10 [40]\nCIFAR-100 [40]\nMNIST [41]\nSUN397 [72]\nBirdsnap [9]\nDTD [21]\nEuroSAT [34]\nFood-101 [10]\nPCam [67]\nRESISC45 [18]\nSTL-10 [23]\n.avg. top-1 acc.\nBASIC-L [52] (reported)\n85.7\n80.6\n85.6\n95.7\n76.1\n82.3\n97.5\n82.3\n40.3\n76.2\n59.2\n64.6\n51.0\n95.1\n59.6\n72.7\n99.6\n76.7 (77.8)\nEVA-CLIP-18B +\n83.8\n77.9\n87.3\n95.7\n74.7\n82.2\n99.4\n93.8\n83.0\n77.7\n79.9\n72.1\n79.8\n95.8\n65.2\n76.9\n99.6\n84.9 (84.1)\n-1.9\n-2.7\n+1.7\n+0.0\n-1.4\n-0.1\n+1.9\n+11.5\n+42.7\n+1.5\n+20.7\n+7.5\n+28.8\n+0.7\n+5.6\n+4.2\n+0.0\n+8.2 (+6.3)\n(b) Comparison EVA-CLIP-18B\u2019s zero-shot image classification performance with BASIC-L [52] on 17 datasets. Our report includes the top-1 accuracy\nfor all datasets, considering that BASIC-L only provided top-1 accuracy for these specific 17 datasets. ( ) is the average top-1 accuracy removing Birdsnap due\nto the different test size between EVA-CLIP-18B and BASIC-L. EVA-CLIP-18B outperforms BASIC-L with a notable margin of +8.2 (+6.3) in average top-1\naccuracy, despite exhibiting lower performance on ImageNet variants.\nTable 6: Robustness evaluation of CLIP models and comparison with BASIC-L [52] on 17 Benchmarks.\nsification benchmarks by a large margin. When sampling\na single center frame per video, EVA-CLIP-18B achieves\naccuracies of 86.0%, 72.9%, 72.9%, and 68.2% across the\nfour evaluated benchmarks. Further, when uniformly sample\n8 or 16 frames per video, we observe an improvement of\n+4.7% / +4.8% averaged across four benchmarks compared\nto the single-frame setting.\nZero-Shot Image-Text Retrieval. In Table 3, we report the\nzero-shot image and text retrieval results on Flickr30K [74]\nand COCO [45]. EVA-CLIP-18B achieves an average re-\ncall of 87.8% across all retrieval benchmarks, significantly\noutperforming competitors.\nRobustness. In Table 6, we demonstrate that scaling up\nEVA-CLIP significantly enhances the robustness of visual\nrepresentations. EVA-CLIP suffers from the smallest per-\nformance drop (\u2206\u2193) between ImageNet-1K and ImageNet\nvariants including adversarial ones, with merely 0.2% top-1\naccuracy gap for EVA-CLIP-18B.\nFor a more robust and comprehensive evaluation of ro-\nbustness and zero-shot performance, it is advisable to include\nmore benchmarks covering more image distributions. How-\never, we want to note that higher ImageNet top-1 accuracy\ndoes not necessarily lead to better overall performance, as\nevidenced in Table 6b, where BASIC-L [52] exhibits higher\nImageNet-related top-1 accuracy but considerably lower\noverall average top-1 accuracy compared to EVA-CLIP-\n18B across a broader range of datasets and distributions,\nshowing a difference of -8.2%.\nLinear Probing on ImageNet-1K. In Table 7, we present\nthe results of linear probing on ImageNet-1K [26]. EVA-\nCLIP-18B achieves an average top-1 accuracy of 88.9%,\nsurpassing InternVL-C [17] by 0.7%.\nmethod\n#param top1 acc.\nOpenCLIP-G/14 (reported)\n1.8B\n86.2\nEVA-01-CLIP-g/14 +\n1.0B\n86.5\nEVA-02-CLIP-E/14+\n4.4B\n88.1\nInternVL-C (reported)\n5.9B\n88.2\nEVA-CLIP-8B +\n7.5B\n88.5\nEVA-CLIP-18B +\n17.5B\n88.9\nTable 7: Linear Probing on ImageNet-1K [26]. The top-1 ac-\ncuracy shows a continuous improvement with the scaling up of\nEVA-CLIP.\n4\n3D Representation. We adopt the Uni3D [77] setting to\nexplore the effectiveness of scaling up teachers. With the\nscaling up of EVA-CLIP in Table 8, we observe consis-\ntent improvements in 3D representation learning capabili-\nties. Further, Uni3D-base equipped with EVA-CLIP-18B\nsets new records on ModelNet [71] and ScanObjectNN [66]\nbenchmarks.\nteacher\ndata\nO-LVIS\nMNet40\nScanObjNN\nOpenCLIP-G/14 +\nw/o LVIS\n44.5\n85.8\n58.9\nEVA-02-CLIP-E/14+ w/o LVIS\n45.8\n86.1\n61.7\nEVA-CLIP-8B +\nw/o LVIS\n46.2\n87.3\n62.7\nEVA-CLIP-18B +\nw/o LVIS\n47.0\n87.6\n65.3\nEVA-02-CLIP-E/14+ Ensembled\n51.7\n86.3\n63.8\nEVA-CLIP-18B +\nEnsembled 53.2(+1.5) 88.6(+2.3) 67.8(+4.0)\nTable 8: EVA-CLIP-18B enhances zero-shot 3d classification\nperformance. We use Uni3D-base [77] as the baseline and scale\nthe teacher from 5B to 18B. We report top-1 accuracy on Objaverse-\nLVIS [25], ModelNet40 [71] and ScanObjectNN [66].\n4. Ablation Studies\nVideo Data. In Table 9, we conduct ablations on EVA-\nCLIP-18B\u2019s zero-shot performance, comparing results when\ntrained with and without Merged-Video. The training ob-\njective for the video data aligns with that of images, encom-\npassing the extraction of features from video where 8 frames\nare uniformly sampled. The mean of all [CLS] embeddings\nserves as a representation for the video. The outcomes reveal\nsubstantial performance improvements associated with train-\ning using Merged-Video. The zero-shot performance, aver-\naged across UCF-101 [60] and Kinetics-400 [15] / 600 [13]\n/ 700 [14], indicates a gain of +0.7 for evaluation with one\nmiddle frame and +0.8 for evaluation with 8 frames.\nclassification\nretrieval\nimage video (#F 1) video (#F 8) avg. recall\nw/o video data\n80.7\n74.3\n78.9\n87.9\nw/ video data\n80.7\n75.0 (+0.7)\n79.7 (+0.8)\n87.8 (-0.1)\nTable 9: Video data enhances zero-shot video classification per-\nformance. We respectively report performances averaged on 27\nimage classification benchmarks, 4 video benchmarks and 2 image-\ntext retrieval benchmarks.\nImage Resolution. In Table 10, we investigate the impact of\nlarger image resolutions on zero-shot performance. Notably,\nthere is an average top-1 accuracy gain of +0.9 when the\nresolution increases from 2242 to 4482 for EVA-CLIP-8B.\nSimilarly, an increase from 2242 to 3362 results in a gain of\n+0.5, even when trained with low global batch sizes of 24k\nfor EVA-CLIP-8B + and 23k for EVA-CLIP-18B+.\nmethod+\nresolution IN-1K IN-A IN-R IN-V2 IN-Ske. ObjectNet avg.\nEVA-CLIP-8B +\n224\u00d7224\n83.5\n85.2 95.3\n77.7\n74.3\n81.2\n82.9\nEVA-CLIP-8B +\n448\u00d7448\n83.8\n88.7 95.4\n77.7\n74.1\n82.9\n83.8\n+0.3\n+3.5 +0.1\n+0.0\n-0.2\n+1.7\n+0.9\nEVA-CLIP-18B + 224\u00d7224\n83.8\n87.3 95.7\n77.9\n74.7\n82.2\n83.6\nEVA-CLIP-18B+ 336\u00d7336\n83.9\n88.9 95.6\n78.2\n74.3\n83.6\n84.1\n+0.1\n+1.6\n-0.1\n+0.3\n-0.4\n+1.4\n+0.5\nTable 10: Increasing resolution. We report zero-shot performance\non ImageNet variants and ObjectNet.\n5. Conclusion\nWe present EVA-CLIP-18B, the currently largest and\nmost performant open-sourced CLIP model with 18-billion\nparameters. We show that following EVA\u2019s weak-to-strong\nvision scaling principle, we can further scale up CLIP mod-\nels to a new record and advance SOTA on multiple prevalent\nbenchmarks across image, video and 3D domains. Impor-\ntantly, we demonstrate that scaling up the size of EVA-CLIP\nmodels consistently boosts performance with no sign of sat-\nuration, shedding light on future vision model scaling.\nReferences\n[1] Laion coco: 600m synthetic captions from laion2b-en. https:\n//laion.ai/blog/laion-coco/. 3\n[2] Reaching 80 zero-shot accuracy with openclip: Vit-g/14 trained on\nlaion-2b. https://laion.ai/blog/giant-openclip/.\n2\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech,\nIain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican,\nMalcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198, 2022. 1\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer\nnormalization. arXiv preprint arXiv:1607.06450, 2016. 2\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng\nWang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A\nversatile vision-language model for understanding, localization, text\nreading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1\n[6] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder for end-to-end\nretrieval. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 1728\u20131738, 2021. 3\n[7] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of\nimage transformers. arXiv preprint arXiv:2106.08254, 2021. 9\n[8] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christo-\npher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Ob-\njectnet: A large-scale bias-controlled dataset for pushing the limits\nof object recognition models. In NeurIPS, 2019. 2, 4, 9, 10\n[9] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander,\nDavid W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale\nfine-grained visual categorization of birds. In CVPR, 2014. 2, 4, 9,\n10\n[10] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013\nmining discriminative components with random forests. In ECCV,\n2014. 2, 4, 9, 10\n5\n[11] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165,\n2020. 1\n[12] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee,\nWoonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-\ntext pair dataset.\nhttps://github.com/kakaobrain/\ncoyo-dataset, 2022. 3\n[13] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier,\nand Andrew Zisserman. A short note about kinetics-600. arXiv\npreprint arXiv:1808.01340, 2018. 3, 5, 9\n[14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A\nshort note on the kinetics-700 human action dataset. arXiv preprint\narXiv:1907.06987, 2019. 3, 5, 9\n[15] Joao Carreira and Andrew Zisserman. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset. In CVPR, 2017. 3, 5,\n9\n[16] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Train-\ning deep nets with sublinear memory cost, 2016. 3\n[17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen\nXing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin\nLi, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up\nvision foundation models and aligning for generic visual-linguistic\ntasks. arXiv preprint arXiv:2312.14238, 2023. 1, 2, 3, 4\n[18] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image\nscene classification: Benchmark and state of the art. Proceedings of\nthe IEEE, 2017. 2, 4, 9, 10\n[19] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Worts-\nman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig\nSchmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning, 2022. 1\n[20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten\nBosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen\nShi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily\nReif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Ja-\ncob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeon-\ntaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan\nFirat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling lan-\nguage modeling with pathways. arXiv preprint arXiv:2204.02311,\n2022. 1\n[21] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi.\nDescribing textures in the wild. In CVPR, 2014. 2, 4, 9, 10\n[22] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D\nManning. ELECTRA: Pre-training text encoders as discriminators\nrather than generators. arXiv preprint arXiv:2003.10555, 2020. 9\n[23] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-\nlayer networks in unsupervised feature learning. In AISTAT, 2011.\n2, 4, 9, 10\n[24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher\nR\u00b4e. Flashattention: Fast and memory-efficient exact attention with\nio-awareness, 2022. 3\n[25] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar\nMichel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha\nKembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d\nobjects. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13142\u201313153, 2023. 5\n[26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database. In\nCVPR, 2009. 2, 4, 9, 10\n[27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman.\n\u201dthe PASCAL Visual Object Classes\nChallenge 2007 (VOC2007) Results.\n\u201dhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2007/workshop/index.html\u201d, 2007.\n2, 9, 10\n[28] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt,\nAlexander Toshev, and Vaishaal Shankar. Data filtering networks.\narXiv preprint arXiv:2309.17425, 2023. 1, 2, 3\n[29] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong\nWang, and Yue Cao. Eva-02: A visual representation for neon\ngenesis. arXiv preprint arXiv:2303.11331, 2023. 1, 3\n[30] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xing-\ngang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva:\nExploring the limits of masked visual representation learning at\nscale. arXiv preprint arXiv:2211.07636, 2022. 1, 2, 3\n[31] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative\nvisual models from few training examples: An incremental bayesian\napproach tested on 101 object categories. In CVPRW, 2004. 2, 9, 10\n[32] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase,\nGeorgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman,\nDhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Gian-\nnis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani\nMarathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ran-\njay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shu-\nran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal\nShankar, and Ludwig Schmidt. Datacomp: In search of the next gen-\neration of multimodal datasets. arXiv preprint arXiv:2304.14108,\n2023. 2\n[33] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron\nCourville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan\nTang, David Thaler, Dong-Hyun Lee, et al. Challenges in repre-\nsentation learning: A report on three machine learning contests. In\nICONIP, 2013. 2, 9, 10\n[34] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian\nBorth. Eurosat: A novel dataset and deep learning benchmark for\nland use and land cover classification. IEEE J. Sel. Top. Appl. Earth\nObs. Remote Sens., 2019. 2, 4, 9, 10\n[35] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath,\nFrank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Para-\njuli, Mike Guo, et al. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. In CVPR, 2021. 2, 4,\n9, 10\n[36] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and\nDawn Song. Natural adversarial examples. In CVPR, 2021. 2, 4, 9,\n10\n6\n[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV, 2016.\n9\n[38] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gor-\ndon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar,\nHongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\nFarhadi, and Ludwig Schmidt. Openclip. https://github.\ncom/mlfoundations/open_clip, 2021. 3\n[39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object\nrepresentations for fine-grained categorization. In ICCVW, 2013. 2,\n9, 10\n[40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers\nof features from tiny images. 2009. 2, 4, 9, 10\n[41] Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning applied to document recognition. Proceed-\nings of the IEEE, 1998. 2, 4, 9, 10\n[42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:\nBootstrapping language-image pre-training with frozen image en-\ncoders and large language models. arXiv preprint arXiv:2301.12597,\n2023. 1\n[43] Xianhang Li, Zeyu Wang, and Cihang Xie. Clipa-v2: Scaling clip\ntraining with 81.1arXiv preprint arXiv:2306.15658, 2023. 1\n[44] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer,\nand Kaiming He. Scaling language-image pre-training via masking,\n2022. 3, 9\n[45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Mi-\ncrosoft coco: Common objects in context. In European conference\non computer vision, pages 740\u2013755. Springer, 2014. 3, 4, 9\n[46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual\ninstruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1\n[47] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and\nAndrea Vedaldi. Fine-grained visual classification of aircraft. arXiv\npreprint arXiv:1306.5151, 2013. 2, 9, 10\n[48] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth,\nSantiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-\nvideo modalities from image captions, 2022. 3\n[49] Maria-Elena Nilsback and Andrew Zisserman. Automated flower\nclassification over a large number of classes. In ICVGIP, 2008. 2, 9,\n10\n[50] Ting Pan, Lulu Tang, Xinlong Wang, and Shiguang Shan. Tokenize\nanything via prompting. arXiv preprint arXiv:2312.09128, 2023. 1\n[51] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V.\nJawahar. Cats and dogs. In CVPR, 2012. 2, 9, 10\n[52] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei\nYu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Com-\nbined scaling for zero-shot transfer learning.\narXiv preprint\narXiv:2111.10050, 2021. 4\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 1, 2, 3,\n9, 10\n[54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al.\nExploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 2020. 1\n[55] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong\nHe. Zero: Memory optimizations toward training trillion parameter\nmodels. In SC20, 2020. 3, 9\n[56] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong\nHe. Deepspeed: System optimizations enable training deep learning\nmodels with over 100 billion parameters. In KDD, 2020. 3, 9\n[57] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal\nShankar. Do imagenet classifiers generalize to imagenet?, 2019. 2,\n4, 9, 10\n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade\nGordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush\nKatta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open\nlarge-scale dataset for training next generation image-text models.\narXiv preprint arXiv:2210.08402, 2022. 2, 3\n[59] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert\nKaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Je-\nnia Jitsev, and Aran Komatsuzaki.\nLaion-400m: Open dataset\nof clip-filtered 400 million image-text pairs.\narXiv preprint\narXiv:2111.02114, 2021. 2\n[60] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101:\nA dataset of 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402, 2012. 3, 5, 9\n[61] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian\nIgel. Man vs. computer: Benchmarking machine learning algorithms\nfor traffic sign recognition. Neural networks, 2012. 2, 9, 10\n[62] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu,\nZhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative multimodal models are\nin-context learners. arXiv preprint arXiv:2312.13286, 2023. 1\n[63] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.\nEva-clip: Improved training techniques for clip at scale. arXiv\npreprint arXiv:2303.15389, 2023. 1, 2, 3\n[64] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang,\nYueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and\nXinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023. 1\n[65] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Na-\nman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample.\nLlama:\nOpen and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023. 1, 2\n[66] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh\nNguyen, and Sai-Kit Yeung. Revisiting point cloud classification:\nA new benchmark dataset and classification model on real-world\ndata. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 1588\u20131597, 2019. 5\n[67] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and\nMax Welling. Rotation equivariant cnns for digital pathology. In\nMICCAI, 2018. 2, 4, 9, 10\n[68] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.\nLearning robust global representations by penalizing local predictive\npower. NeurIPS, 2019. 2, 4, 9, 10\n[69] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi,\nYan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.\nCogvlm: Visual expert for pretrained language models.\narXiv\npreprint arXiv:2311.03079, 2023. 1\n[70] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma,\nXinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He,\nPing Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Intern-\nvid: A large-scale video-text dataset for multimodal understanding\nand generation, 2024. 3\n7\n[71] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang\nZhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep\nrepresentation for volumetric shapes. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1912\u2013\n1920, 2015. 5\n[72] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and\nAntonio Torralba. Sun database: Large-scale scene recognition from\nabbey to zoo. In CVPR, 2010. 2, 4, 9, 10\n[73] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar,\nSrinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer,\nand Cho-Jui Hsieh. Large batch optimization for deep learning:\nTraining bert in 76 minutes, 2019. 3, 9\n[74] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.\nFrom image descriptions to visual denotations: New similarity met-\nrics for semantic inference over event descriptions. TACL, 2014. 3,\n4, 9\n[75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas\nBeyer. Sigmoid loss for language image pre-training. arXiv preprint\narXiv:2303.15343, 2023. 1, 4\n[76] Biao Zhang and Rico Sennrich. Root mean square layer normaliza-\ntion. arXiv preprint arXiv:1910.07467, 2019. 2\n[77] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun\nHuang, and Xinlong Wang. Uni3d: Exploring unified 3d representa-\ntion at scale. arXiv preprint arXiv:2310.06773, 2023. 1, 5\n[78] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed El-\nhoseiny. Minigpt-4: Enhancing vision-language understanding with\nadvanced large language models. arXiv preprint arXiv:2304.10592,\n2023. 1\n8\nEVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters\nSupplementary Material\nDataset\nClasses Test size Evaluation Metric\nImageNet-1K [26]\n1000\n50,000\naccuracy\nImageNet-V2 [57]\n1000\n10,000\naccuracy\nImageNet-Adversarial [36]\n1000\n7,500\naccuracy\nImageNet-R(endition) [35]\n1000\n30,000\naccuracy\nImageNet-Sketch [68]\n1000\n50,899\naccuracy\nObjectNet [8]\n1000\n50,273\naccuracy\nCIFAR-10 [40]\n10\n10,000\naccuracy\nCIFAR-100 [40]\n100\n10,000\naccuracy\nMNIST [41]\n10\n10,000\naccuracy\nCaltech101 [31]\n101\n9144\naccuracy\nSUN397 [72]\n397\n108,754\naccuracy\nFGVC Aircraft [47]\n100\n3,333\naccuracy\nCountry-211 [53]\n211\n21,100\naccuracy\nStanford Cars [39]\n196\n8,041\naccuracy\nBirdsnap [9]\n500\n2,195\naccuracy\nDescribable Textures [21]\n47\n1,880\naccuracy\nEuroSAT[34]\n10\n27,000\naccuracy\nFacial Emotion Recognition 2013 [33] 8\n3,574\naccuracy\nOxford Flowers 102 [49]\n102\n6,149\naccuracy\nFood-101 [10]\n102\n25,250\naccuracy\nGTSRB [61]\n43\n12,630\naccuracy\nPatchCamelyon [67]\n2\n32,768\naccuracy\nOxford-IIIT Pets [51]\n37\n3,669\naccuracy\nRendered SST2 [53]\n2\n1,821\naccuracy\nRESISC45 [18]\n45\n31,500\naccuracy\nSTL-10 [23]\n10\n8000\naccuracy\nPascal VOC 2007 Classification [27]\n20\n4,952\naccuracy\nUC-F101 [60]\n101\n11,213\naccuracy\nKinetics-400 [15]\n400\n19,240\nmean(top1, top5)\nKinetics-600 [13]\n600\n29,788\nmean(top1, top5)\nKinetics-700 [14]\n700\n33,966\nmean(top1, top5)\nFlickr30K [74]\n-\n1000\nrecall\nCOCO [45]\n-\n5000\nrecall\nTable 11: Datasets used to evaluate EVA-CLIP models.\n6. Training Settings\nWe present detailed training settings of EVA-CLIP-8B\nand EVA-CLIP-18B in Tabs. 12 and 13.\n7. Image Transformations for Evaluation\nTwo prevalent image transformations utilized in zero-shot\nevaluation are: 1) direct resizing of images to a fixed size,\nsuch as 224\u00d7224, and 2) resizing images based on the short-\nest side, followed by center cropping to achieve a fixed size.\nIn Table 14, our study systematically investigates the impact\nconfig\nEVA-CLIP-{8B, 8B+}\nimage enc. weight init.\nEVA-8B / EVA-CLIP-8B\ntext enc. weight init.\nEVA-02-CLIP-E/14+ / EVA-CLIP-8B\nimage-text data\nMerged-2B\nimage enc. peak learning rate\n4e-4 / 2e-4\nimage enc. layer-wise lr decay [22, 7]\n0.9 / 0.85\ntext enc. peak learning rate\n4e-5 / 2e-5\ntext enc. layer-wise lr decay [22, 7]\n0.75\nlearning rate schedule\ncosine decay\noptimizer\nLAMB [73]\noptimizer hyper-parameters\n\u03b21, \u03b22, \u03f5 = 0.9, 0.95, 1e-6\nweight decay\n0\ninput resolution\n2242 / 4482\npatch size\n142\nbatch size\n178k / 24k\nsamples seen\n9B / 800M\ndrop image patch [44]\n0.5 / 0.0\ndrop path [37]\n0.0\nrandom resized crop\n(0.9, 1)\nnumerical precision\nDeepSpeed bf16 [56]\nZeRO optimizer [55]\nstage 3\nTable 12: EVA-CLIP-8B and EVA-CLIP-8B+ training settings.\nconfig\nEVA-CLIP-{18B,18B+}\nimage enc. weight init.\nEVA-18B / EVA-CLIP-18B\ntext enc. weight init.\nEVA-02-CLIP-E/14+ / EVA-CLIP-18B\nimage-text data\nMerged-2B+\nimage enc. peak learning rate\n4e-4 / 2e-4\nimage enc. layer-wise lr decay [22, 7]\n0.9 / 0.85\ntext enc. peak learning rate\n4e-5 / 2e-5\ntext enc. layer-wise lr decay [22, 7]\n0.75\nlearning rate schedule\ncosine decay\noptimizer\nLAMB [73]\noptimizer hyper-parameters\n\u03b21, \u03b22, \u03f5 = 0.9, 0.95, 1e-6\nweight decay\n0\ninput resolution\n2242 / 3362\npatch size\n142\nbatch size\n108k / 23k\nsamples seen\n6B / 400M\ndrop image patch [44]\n0.5 / 0.0\ndrop path [37]\n0.0\nrandom resized crop\n(0.9, 1)\nnumerical precision\nDeepSpeed bf16 [56]\nZeRO optimizer [55]\nstage 3\nTable 13: EVA-CLIP-18B and EVA-CLIP-18B+ training settings.\nof these two image transformations in zero-shot evaluations.\nNotably, there exists a significant performance gap between\nthe two transformations, observed particularly in zero-shot\nimage classification on ObjectNet [8] and VOC2007 [27],\nand zero-shot retrieval on Flickr30K [74] and COCO [45].\nEVA-CLIP-18B shows robustness with almost the same\naverage accuracy across different image transformations in\nzero-shot image/video classification.\nFor zero-shot image classification and video classification,\n9\nmethod\nImageNet-1K [26]\nImageNet-V2 [57]\nImageNet-Adv. [36]\nImageNet-Ren. [35]\nImageNet-Ske. [68]\nObjectNet [8]\nCIFAR-10 [40]\nCIFAR-100 [40]\nMNIST [41]\nCaltech101 [31]\nSUN397 [72]\nFGVC Aircraft [47]\nCountry-211 [53]\nStanford Cars [39]\nBirdsnap [9]\nDTD [21]\nEuroSAT [34]\nFER2013 [33]\nFlowers-102 [49]\nFood-101 [10]\nGTSRB [61]\nPCam [67]\nPets [51]\nRendered SST2 [53]\nRESISC45 [18]\nSTL-10 [23]\nVOC2007 [27]\n.avg. top-1 acc.\nDFN5B-CLIP-H/14+* 84.0 77.8 79.6 92.9 72.4 79.6 98.8 90.5 83.6 88.7 77.0 64.9 36.1 95.7 80.5 70.9 61.1 56.1 91.6 96.1 67.8 69.6 96.7 55.5 75.9 99.1 78.2 78.5\nDFN5B-CLIP-H/14+\u2020 84.3 78.3 79.3 93.6 73.3 73.5 98.8 90.5 83.6 88.9 77.4 72.5 37.9 96.0 80.3 70.9 61.1 56.1 91.4 96.2 67.9 69.6 96.8 55.5 75.9 99.1 81.9 78.9\nEVA-CLIP-18B *\n83.7 77.9 87.3 95.6 74.4 82.2 99.4 93.8 83.0 89.4 77.5 58.4 41.8 94.9 79.9 71.9 79.8 59.3 85.9 95.8 72.4 65.2 96.0 67.5 76.8 99.6 82.4 80.4\nEVA-CLIP-18B \u2020\n83.8 77.7 86.2 95.7 74.7 76.2 99.4 93.8 83.0 89.8 77.7 59.7 43.1 94.9 78.4 72.1 79.8 59.3 86.0 95.7 72.3 65.2 96.1 67.5 76.9 99.6 85.8 80.4\n(a) Impact of image transformations on zero-shot image classification performance. Different transformations can significantly influence zero-shot image\nclassification performance, particularly for ObjectNet [8]. EVA-CLIP-18B shows robustness with the same average top-1 accuracy across different image\ntransformations.\nmethod+\n#Frames UCF-101 K-400 K-600 K-700 avg. acc.\nDFN5B-CLIP-H/14+*\n1\n78.5\n65.2\n66.0\n59.2\n67.2\nDFN5B-CLIP-H/14+\u2020\n1\n79.2\n66.7\n67.0\n60.7\n68.4\nEVA-CLIP-18B *\n1\n86.0\n72.2\n72.6\n67.4\n74.6\nEVA-CLIP-18B \u2020\n1\n85.6\n72.9\n72.9\n68.2\n74.9\nEVA-CLIP-18B *\n8\n88.2\n79.3\n79.2\n72.0\n79.7\nEVA-CLIP-18B \u2020\n8\n87.9\n79.2\n79.1\n72.1\n79.6\n(b) Impact of image transforms on zero-shot video classification performance.\nzero-shot text retrieval\nzero-shot image retrieval\nFlickr30K\nCOCO\nFlickr30K\nCOCO\nmethod+\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 MR\nDFN5B-CLIP-H/14 +* 92.3 99.1\n99.7\n70.6 89.6\n94.4\n80.7 95.5\n97.7\n54.1 78.0\n85.4\n86.4\nDFN5B-CLIP-H/14 +\u2020 93.6 99.3\n99.6\n71.8 90.4\n94.9\n82.1 96.0\n97.9\n55.6 79.2\n86.3\n87.2\nEVA-CLIP-18B *\n95.4 99.5\n99.8\n72.8 89.7\n94.3\n83.2 95.9\n97.8\n55.6 77.9\n85.3\n86.7\nEVA-CLIP-18B \u2020\n96.7 99.7 100.0 73.6 90.9\n95.0\n83.3 96.3\n98.3\n56.2 78.5\n85.6\n87.8\n(c) Impact of image transforms on zero-shot retrieval performance.\nTable 14: Impact of image transformations on zero-shot evaluation. \u2020 denotes the direct resizing of images to a fixed size, while *\nindicates resizing images based on the shortest side and subsequently center cropping to achieve a fixed size.\nwe present results obtained by selecting the best-performing\ntransformation between the two. In the case of zero-shot\nretrieval tasks, we specifically choose the transformation that\ninvolves direct resizing of images to a fixed size.\n10\n"
  },
  {
    "title": "MusicRL: Aligning Music Generation to Human Preferences",
    "link": "https://arxiv.org/pdf/2402.04229.pdf",
    "upvote": "16",
    "text": "2024-2-7\nMusicRL: Aligning Music Generation to Human\nPreferences\nGeoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zal\u00e1n Borsos, Brian\nMcWilliams, Victor Ungureanu, Olivier Bachem, Olivier Pietquin1, Matthieu Geist1, L\u00e9onard Hussenot,\nNeil Zeghidour2 and Andrea Agostinelli\nGoogle DeepMind\nWe propose MusicRL, the first music generation system finetuned from human feedback. Appreciation\nof text-to-music models is particularly subjective since the concept of musicality as well as the specific\nintention behind a caption are user-dependent (e.g. a caption such as \u201cupbeat workout music\u201d can\nmap to a retro guitar solo or a technopop beat). Not only this makes supervised training of such\nmodels challenging, but it also calls for integrating continuous human feedback in their post-deployment\nfinetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete\naudio tokens finetuned with reinforcement learning to maximize sequence-level rewards. We design\nreward functions related specifically to text-adherence and audio quality with the help from selected\nraters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect\na substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from\nHuman Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human\nfeedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the\nbaseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according\nto human raters. Ablation studies shed light on the musical attributes influencing human preferences,\nindicating that text adherence and quality only account for a part of it. This underscores the prevalence of\nsubjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning\nof music generation models. Samples can be found at google-research.github.io/seanet/musiclm/rlhf/.\n1. Introduction\nGenerative modelling of music has experienced a leap forward: while it was until recently either limited\nto the fine modelling of individual instruments (D\u00e9fossez et al., 2018; Engel et al., 2017, 2020) or the\ncoarse generation of polyphonic music (Dhariwal et al., 2020), models can now handle open-ended,\nhigh-fidelity text-controlled music generation (Agostinelli et al., 2023; Copet et al., 2023; Forsgren\nand Martiros, 2022; Liu et al., 2023). In particular, text-to-music systems such as MusicLM (Agostinelli\net al., 2023) and MusicGen (Copet et al., 2023) build on audio language models, as they cast the\ngenerative process as an autoregressive prediction task in the discrete representation space of a neural\naudio codec (D\u00e9fossez et al., 2022; Zeghidour et al., 2022). While this approach has demonstrated its\nability to generate realistic speech (Borsos et al., 2023a,b; Wang et al., 2023), sound events (Kreuk\net al., 2022) and music, it suffers from a few shortcomings. First, the next-token prediction task used\nto train these systems \u2014 while generic enough to model arbitrary audio signals \u2014 lacks any prior\nknowledge about musicality that could bias those towards generating music that is more appealing\nto listeners. Second, while the temperature sampling used at inference allows for generating diverse\naudio from a single text caption, this diversity is only desirable along certain axes such as melody\nor performance, while musicality and adherence to the prompt should remain consistently high.\n1Now at Cohere\n2Now at Kyutai\nCorresponding author(s): gcideron@google.com, agostinelli@google.com\narXiv:2402.04229v1  [cs.LG]  6 Feb 2024\nMusicRL: Aligning Music Generation to Human Preferences\nMusicRL-R  vs. MusicLM   \nMusicRL-U  vs. MusicLM   \nMusicRL-U  vs. MusicRL-R \nMusicRL-RU vs. MusicLM   \nMusicRL-RU vs. MusicRL-U \nMusicRL-RU vs. MusicRL-R \n65.0\n58.6\n36.0\n66.7\n43.9\n42.6\n22.1\n21.2\n33.0\n22.9\n29.7\n35.6\n12.9\n20.2\n31.0\n10.4\n26.4\n21.8\nWin\nTie\nLoss\nFigure 1 | Results of the qualitative side-by-side evaluation for the RLHF finetuned models. In each X\nvs. Y comparison, the green bar corresponds to the percentage of times model X was preferred, the\nyellow bar to the percentage of ties and the red bar to the percentage of times model Y was preferred.\nMusicRL-R is the MusicLM model finetuned on quality and text adherence reward. MusicRL-U is\nfinetuned on a reward model of user preferences. MusicRL-RU is finetuned sequentially on quality and\nadherence to text and then on a reward model of user preferences. While every RLHF finetuned version\nof MusicLM significantly outperforms MusicLM, MusicRL-R and MusicRL-U achieve comparable\nperformance, while MusicRL-RU is overall the preferred model.\nThese fundamental issues of autoregressive generative models have been extensively observed and\naddressed in the context of language modelling. For example, several works have explored finetuning\nmachine translation models to maximise the BLEU score (Ranzato et al., 2016; Wu et al., 2016)\nor summarization models to improve the relevant ROUGE metric (Ranzato et al., 2016; Roit et al.,\n2023; Wu and Hu, 2018). Such metrics are typically sequence-level, and evaluate the output of a\nnon-differentiable sampling process (e.g., greedy decoding, temperature sampling). This is typically\ncircumvented by using a reinforcement learning method which models the metric of interest of a\nreward function and the generative model as a policy. The underlying algorithmic similarity between\nsuch text generation systems and autoregressive music models suggests that \u2014 given the proper\nreward functions\u2014 one could use reinforcement learning to improve music generation.\nMusic generated given a prompt should exhibit three properties: adherence to the input text, high\nacoustic quality (absence of artifacts), and \u201cmusicality\u201d or general pleasantness. Automatic metrics\nhave been proposed to quantify the text adherence like Classifier KLD (Yang et al., 2022) or MuLan\nCycle Consistency (Agostinelli et al., 2023) as well as acoustic quality with Fr\u00e9chet Audio Distance\n(Kilgour et al., 2019). Such metrics could be used as reward functions. Yet, designing automatic\nproxies to measure musicality is challenging. Most of the previous approaches (Guimaraes et al., 2017;\nJaques et al., 2017; Kotecha, 2018; Latif et al., 2023) rely on complex music theory rules, are restricted\nto specific musical domains (e.g., classical piano) and only partially align with human preferences. This\ngap between automatic metrics and human preferences has again been extensively studied in language\nmodelling, with RLHF (Reinforcement Learning from Human Preferences) becoming the de facto way\nof aligning conversational models (Achiam et al., 2023; Team et al., 2023) with human feedback.\nHuman preferences as referred in previous work (Ouyang et al., 2022; Stiennon et al., 2020) mainly\nrefers to the preferences of raters. Raters may not be representative of the population interacting with\nthe model (e.g. rating services such as Amazon Mechanical Turk3 uses a global workforce). Especially\nin the context of music, this population gap can have a significant impact on the preferences (Trehub\net al., 2015). Collecting large scale user preferences data could help bridge the population gap by\n3https://www.mturk.com/\n2\nMusicRL: Aligning Music Generation to Human Preferences\ncollecting considerably more interactions in contrast with raters.\nIn this work, we introduce MusicRL, a text-to-music generative model finetuned with reinforcement\nlearning. Starting from a MusicLM baseline, we use an automatic measure of text adherence as well as a\nnew acoustic fidelity metric as reward functions to perform RL finetuning. Human evaluations indicate\nthat generations from the resulting MusicRL-R are preferred over those from MusicLM 83% of the\ntime, as measured by \ud835\udc64\ud835\udc56\ud835\udc5b/(\ud835\udc64\ud835\udc56\ud835\udc5b + \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60). Then, to explicitly align the model with human judgment, we\ncollect a dataset of pairwise preferences from users interacting with MusicLM to fit a reward model. Ab-\nlation studies on the reward model trained on user interaction data demonstrate that user preferences\nstrongly correlate with musicality. Extensive human evaluations reveal that the music generations com-\ning from the resulting MusicRL-U are preferred over the base model 74% of the time. Finally, we com-\nbine automatic rewards and human feedback to finetune MusicRL-R into MusicRL-RU and show that\nthis model outperforms all alternatives more than 62% of the time. To the best of our knowledge, this\nwork is the first attempt at leveraging human feedback at scale to improve an audio generative model.\n2. Related Work\nMusic generation. While earlier approches to musical audio generation were limited in terms\nof producing high quality outputs (Dhariwal et al., 2020) or semantically consistent long audios\n(Hawthorne et al., 2022), recent research has achieved a level of quality that allows for an enjoyable\nlistening experience. A first line of work casts the task of music generation as categorical prediction in\nthe discrete token space provided by a neural audio codec (D\u00e9fossez et al., 2022; Zeghidour et al.,\n2022), and trains a Transformer-based (Vaswani et al., 2017) model for next token prediction (Borsos\net al., 2023a) or parallel token decoding (Borsos et al., 2023b; Garcia et al., 2023; Parker et al., 2024).\nCombining this generative backbone with text-conditioning either through a text encoder or text-audio\nembeddings (Elizalde et al., 2022; Huang et al., 2022) provides high-quality text-to-music models\n(Agostinelli et al., 2023; Copet et al., 2023). A parallel line of work relies on diffusion models and casts\nthe task of music generation as denoising of audio waveforms and spectrograms (Huang et al., 2023)\nor learned latent representations (Liu et al., 2023; Schneider et al., 2023). In both cases, the models\nare trained offline on a collection of existing musical recordings and inference is run in a stochastic\nfashion (e.g. diffusion or temperature sampling), which provides diversity but also uncertainty on\nthe outputs (e.g. in terms of text-adherence or quality). Previous work (Kharitonov et al., 2023) has\ncircumvented this issue by sampling many sequences, ranking them with a score function (e.g. a\nreference-free audio quality estimator) and returning the best candidate. This considerably increases\ninference cost and requires well-defined score functions.\nMusicRL addresses these limitations by finetuning a MusicLM (Agostinelli et al., 2023) model with\nreinforcement learning, using reward functions derived from automatic metrics, small scale high-\nquality human ratings, and large scale user feedback. To the best of our knowledge, MusicRL is the\nfirst music generation system that shows the benefits from integrating feedback from hundreds of\nthousands of users.\nRL-finetuning of music generation models. Most previous works in RL-finetuning music generation\nmodels involve designing handmade reward signals based on principles of music theory (Guimaraes\net al., 2017; Jaques et al., 2017; Kotecha, 2018; Latif et al., 2023) or simple patterns like repetitions\n(Karbasi et al., 2021). Jaques et al. (2017) uses a set of rules inspired by a melodic composition theory\n(Gauldin, 1988) (e.g., stay in key, play motifs and repeat them, avoid excessively repeating notes)\nin combination with a KL regularization term. These approaches have several limitations: the rule\nsets can be incomplete or contradictory, practitioners must find the correct balance between different\nrewards, and the rules themselves derive from music theory, which is an imperfect approximation of\n3\nMusicRL: Aligning Music Generation to Human Preferences\nFigure 2 | Given a dataset of music captions, MusicLM generates audio samples that are scored with a\nreward function. The RL algorithm finetune the model to maximise the received reward.\nhuman musical preferences. Jiang et al. (2020) finetune an online music accompaniment generation\nmodel with four reward models learned from data and rule-based reward that assign -1 when a note is\nexcessively repeated. Each reward model corresponds to the probability of a chunk of the generation\ngiven a context (the context and the chunk to predict is different for each reward). These rewards\nare learned with a masked language model (Devlin et al., 2019) loss on a music dataset. Yet, such\nmethods only apply to restricted musical domains (e.g. monophonic piano) or symbolic generation. In\ncontrast with previous work, MusicRL learns human preferences from its own raw audio generations.\nThis allows for improving music generation across the whole spectrum of musical genres and styles,\nfrom lo-fi hip-hop to orchestral symphonies and modal jazz.\nRL from human feedback. RLHF recently became a critical step in the training of conversational\nmodels used in applications such as Bard (Gemini Team, 2023) or GPT-4 (OpenAI, 2023). RLHF has\nfirst been applied to solve Atari games (Christiano et al., 2017) before being used widely, for example\nin natural language tasks (Bai et al., 2022; Jaques et al., 2019; Ouyang et al., 2022; Stiennon et al.,\n2020; Ziegler et al., 2019) or in image generation (Lee et al., 2023; Wallace et al., 2023). Wallace\net al. (2023) uses Direct Optimisation Algorithm (DPO) (Rafailov et al., 2023) to finetune a diffusion\nmodel on human preference data. To the best of our knowledge, we are the first to apply RLHF to\nmusic generation models.\n3. Method\n3.1. MusicLM\nMusicLM (Agostinelli et al., 2023) is an autoregressive model for generating music from text de-\nscriptions. Following the design of AudioLM (Borsos et al., 2023a), MusicLM relies on two different\ntypes of audio representations for generation: semantic tokens, which are quantized representations\nof masked audio language models such as w2v-BERT (Chung et al., 2021) and acoustic tokens, the\ndiscrete representations produced by neural audio codecs such as SoundStream (Zeghidour et al.,\n2022). While the semantic tokens ensure the long-term structural coherence of the generation process,\nthe acoustic tokens allow for high-quality synthesis. To ensure high-bitrate reconstructions, Sound-\nStream uses residual vector quantization (RVQ) \u2014 a stack of vector quantizers where each quantizer\noperates on the residual produced by the previous quantizers \u2014 to discretize the continuous audio\nrepresentations, imposing a hierarchical structure on the acoustic tokens. Additionally, MusicLM\nrelies on MuLan (Huang et al., 2022), a joint music-text contrastive model, for conditioning the audio\ngeneration task on descriptive text.\nMusicLM was initially introduced as a 3-stage Transformer-based autoregressive model. The first\nstage learns the mapping between MuLan and semantic tokens. The second stage predicts the first\n4\nMusicRL: Aligning Music Generation to Human Preferences\nlevels from the output of the SoundStream RVQ (coarse acoustic tokens) from MuLan and semantic\ntokens. The last stage predicts the remaining SoundStream RVQ levels (fine acoustic tokens) from\ncoarse acoustic tokens.\nFor the purpose of RL finetuning, we choose to optimise the semantic and coarse acoustic modelling\nstages, which are the most important contributors to acoustic quality, adherence to the text and\noverall appeal of the generated music. We address the challenges of jointly optimizing semantic and\ncoarse acoustic modelling by using a single autoregressive stage that operates on frame-interleaved\nsemantic and acoustic tokens. While simplifying the RL setup and problem formulation, this approach\nincreases modeled token sequence length. We address this with a hierarchical transformer, similarly to\nLee et al. (2022); Yang et al. (2023); Yu et al. (2023). Finally, instead of the original autoregressive\nfine acoustic modelling stage of MusicLM, we use Soundstorm (Borsos et al., 2023b) for achieving\nefficient parallel generation.\nFor simplicity, by referring to MusicLM in this work, we refer only to the autoregressive modelling\nstage of interleaved semantic and coarse acoustic tokens, which is the text conditioned modelling\nstage that can be finetuned with RL.\n3.2. RL finetuning procedure\nWe use the standard formulation of RL in the context of finetuning large language models as done in\nprevious work (Ziegler et al., 2019). Figure 2 illustrates the RL training loop. The agent acts according\nto its policy \ud835\udf0b\ud835\udf03 with \ud835\udf03 the weights that parameterize the policy. The policy is an autoregressive model\ntaking as input \ud835\udc4e0, . . . , \ud835\udc4e\ud835\udc61\u22121, the sequence of previously generated tokens and outputs a probability\ndistribution over the next action, i.e., the next token to pick : \ud835\udc4e\ud835\udc61 \u223c \ud835\udf0b\ud835\udf03(.|\ud835\udc4e0 . . . \ud835\udc4e\ud835\udc61\u22121). The RL finetuning\nphase aims at maximizing \ud835\udd3c\ud835\udf0b\ud835\udf03 [\u00cd\n\ud835\udc61 \ud835\udc5f(\ud835\udc4e0 . . . \ud835\udc4e\ud835\udc61)] with \ud835\udc5f a given reward function. We use a KL regularized\nversion of the REINFORCE algorithm (Jaques et al., 2017; Williams, 1992) to update the policy\nweights. Given a trajectory (\ud835\udc4e\ud835\udc61)\ud835\udc47\n\ud835\udc61=0 and denoting \ud835\udc60\ud835\udc61 = (\ud835\udc4e0...\ud835\udc4e\ud835\udc61\u22121), the corresponding policy gradient\nobjective to maximise is\n\ud835\udd41(\ud835\udf03) = (1 \u2212 \ud835\udefc)[\n\ud835\udc47\n\u2211\ufe01\n\ud835\udc61=0\nlog \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc61|\ud835\udc60\ud835\udc61)(\n\ud835\udc47\n\u2211\ufe01\n\ud835\udc56=\ud835\udc61\n\ud835\udc5f(\ud835\udc60\ud835\udc56) \u2212 \ud835\udc49\ud835\udf19(\ud835\udc60\ud835\udc61))]\n\u2212\ud835\udefc\n\ud835\udc47\n\u2211\ufe01\n\ud835\udc61=0\n\u2211\ufe01\n\ud835\udc4e\u2208\ud835\udc34\n[log(\ud835\udf0b\ud835\udf03(\ud835\udc4e|\ud835\udc60\ud835\udc61)/\ud835\udf0b\ud835\udf030(\ud835\udc4e|\ud835\udc60\ud835\udc61))],\nwith \ud835\udc34 the action space which here corresponds to the codebook, \ud835\udefc the KL regularization strength,\nand \ud835\udc49\ud835\udf19 the baseline. The baseline value function \ud835\udc49\ud835\udf19 is used to decrease the variance in the policy\ngradient objective (Sutton and Barto, 2018) and it is trained to estimate the mean return of the\ncurrent policy. The baseline is learned as follows:\nmin\n\ud835\udf19 \ud835\udd3c\ud835\udf0b\ud835\udf03\n\u2211\ufe01\n\ud835\udc61\n(\n\ud835\udc47\n\u2211\ufe01\n\ud835\udc58=\ud835\udc61\n\ud835\udc5f(\ud835\udc60\ud835\udc58) \u2212 \ud835\udc49\ud835\udf19(\ud835\udc60\ud835\udc61))2.\nBoth the policy and the value function are initialized from the initial MusicLM checkpoint with weight\n\ud835\udf030.\n3.3. Reward Signals\nText adherence. We derive a reward model for text adherence from pretrained MuLan (Huang\net al., 2022) embeddings. MuLan is a contrastive audio-text embedding model trained on music clips\n5\nMusicRL: Aligning Music Generation to Human Preferences\nFigure 3 | The AI Test Kitchen MusicLM interface. The user can write a prompt or choose from\nsuggestions. Each prompt generates two 20s clips, and the user can label their favorite clip among\nthe two with a trophy.\nand weakly-associated, free-form text annotations. We compute the cosine similarity between the\ntext embedding of the input prompt and the audio embedding of the generated music, resulting in\na reward value in [\u22121; 1]. We refer to this metric as MuLan score. Because our models generate\n30-second audios, while MuLan is trained on 10-second audio clips, we divide each audio into three\nsegments, we calculate MuLan scores for each segment, and we average the results.\nAcoustic quality. Another main attribute of musical generation is acoustic quality, e.g. whether a clip\nsounds like a professional recording or is contaminated with artifacts. We rely on a reference-free\nquality estimator trained to predict the human Mean Opinion Score (MOS - between 1 and 5) of a\n20 second music clip. We train the model on a mix of human-created and MusicLM-generated music\nclips, where each clip was rated by 3 raters. The raters were tasked to judge only the acoustic quality,\nto avoid confounding factors such as musicality. We refer to this metric as the quality score. Because\nour models generate 30-second clips, we compute quality scores on the first 20 seconds and on the\nlast 20 seconds, and average the two scores.\nUser preferences. We deploy the pretrained text-to-music MusicLM model through the AITK web-\nbased interface 4 to a large scale userbase. We choose to collect feedback through pairwise compar-\nisons (Christiano et al., 2017): when a user seizes a prompt, we generate two 20s candidate clips and\nlet the user optionally assign a trophy to one of them. An important design choice implied by this pro-\ncess is the absence of specific instructions, which is intended not to bias users towards precise musical at-\ntributes and rather communicate their overall subjective taste. We only consider preferences from users\nthat listen to both generations. After filtering, we obtain a dataset of pairwise user data of size 300,000.\nThis dataset minimizes the biases that often arise from human raters (as detailed in Appendix C).\nOur reward model takes as input the caption\u2019s text and corresponding audio tokens and outputs\na scalar score. This model is trained with a Bradley-Terry Model (Bradley and Terry, 1952) as in\n4https://aitestkitchen.withgoogle.com/\n6\nMusicRL: Aligning Music Generation to Human Preferences\nChristiano et al. (2017), which enables learning a pointwise ELO score from pairwise preferences.\nIt is initialized with the MusicLM checkpoint, as first results demonstrated that, starting from scratch,\nthe reward model was not able to do better than chance at predicting human preferences. We split\nthe user preference dataset into a train split of size 285,000 and an evaluation split of size 15,000.\nAfter training for 10,000 steps on batches of 32 pairs, the reward model achieves 60% of accuracy\non the evaluation set (see Figure 6).\nTo pre-assess the performance of the reward model, we conduct an internal small-scale human\nevaluation on 156 audio comparisons from the user preference dataset. In 60% of cases, our team\u2019s\npreferences aligned with the established preferences in the dataset. This result is comparable to\nthe performance of the reward model. Furthermore, this low agreement rate highlights the inher-\nent subjectivity in judging music preferences, compared to domains such as summarization where\nStiennon et al. (2020) estimated at 73-77% the agreement rate for the OpenAI human preference\ndataset. When finetuning MusicLM on the user preference reward model, since our models generate\n30-second audios, we average the scores computed from the first and last 20 seconds of audio.\n4. Experimental Setup\n4.1. Datasets\nGiven the pretrained reward signals as described in Section 3.3, the RL finetuning step uses a dataset\nexclusively composed of captions, used for prompting all MusicLM-based models. Consequently, no\nground-truth audio is involved in the finetuning process. We follow the same procedure as Huang et al.\n(2023) for synthetically generating captions from three sources. We use the LaMDA model (Thoppilan\net al., 2022) to generate descriptions of 150,000 popular songs. After providing song titles and artists,\nLaMDA\u2019s responses are processed into 4 million descriptive sentences about the music. We split 10,028\ncaptions from MusicCaps (Agostinelli et al., 2023) into 35,333 single sentences describing music.\nFurthermore, we collect 23,906 short-form music tags from MusicCaps. Additionally, we extend the\nprevious captions with the 300,000 prompts collected from users, as described in Section 3.3. We\nrandomly split the data, using 90% for training and 10% for evaluation.\n4.2. Training procedure\nIn the following experiments, we RL-finetune the MusicLM model with the same RL algorithm and the\nsame hyperparameters. The common decoding scheme is temperature sampling with temperature\n\ud835\udc47 = 0.99. The temperature was chosen with subjective inspection to have a good quality-diversity\ntradeoff for the generations of MusicLM. The RL-finetuned models differs only with the reward\nfunction employed during their training process.\nMusicRL-R. We RL-finetune MusicLM for 20,000 training steps (1) with the MuLan reward, (2)\nwith the quality reward, and (3) with a linear combination of the MuLan and the quality reward:\nthe resulting models are respectively called MusicRL-MuLan, MusicRL-Quality, and MusicRL-R.\nThroughout our experiments, we normalize the quality reward from [1; 5] to [0; 1] as preliminary\nexperiments have shown that the combination of the MuLan and the quality reward gives the best\nresults when both rewards are on the same scale. We still display in figures the un-normalized scores.\nMusicRL-U. We RL-finetune MusicLM for 5000 training steps with the user preference reward model\nto obtain a model that we call MusicRL-U.\nMusicRL-RU. To combine all the reward signals, we RL-finetune MusicRL-R for 1000 training steps\non the user preference reward model. For this experiment, the KL regularization is computed between\n7\nMusicRL: Aligning Music Generation to Human Preferences\nthe model being finetuned and MusicRL-R. The resulting model is called MusicRL-RU. We find that\nthe sequential approach of first finetuning on MuLan and quality and then finetuning on the user\npreference reward outperforms learning from the three rewards at the same time. We hypothesize\nthis comes from the fact that it takes a small number of gradient steps (under 2000) before over\noptimizing on the user preference reward while it takes around 10,000 steps to optimise the other\nrewards. Moreover, using the user preference reward model in a final stage in this matter may allow\nthe model to align better on the human preferences.\n4.3. Evaluation\nThe main metrics we report in our experiments are the quality reward, the MuLan reward, and the\nuser preference reward model. We report the metrics either against the training step to show progress\nalong the training, or against the KL divergence to the base model. This is typically used as a proxy\nto measure the distance to the base checkpoint and thus the retention of the original capabilities of\nthe model (Christiano et al., 2017; Roit et al., 2023).\nFor the qualitative evaluation, we use 101 diverse, internally-collected prompts, representing a\nbalanced range of musical genres (see Appendix A for the full list). We use these prompts to generate\naudio samples from each evaluated model. We select raters for their experience listening to varied\nmusical styles (>6 years) and fluency in written English. During the qualitative evaluation, raters\nare presented with two audio clips generated by different models using the same text prompt. We\nask raters to rate each clip on a scale of 1 to 5, considering adherence to the text prompt, acoustic\nquality and overall appeal to the audio clip. Each comparison is performed by three different raters,\ntotaling 303 ratings per model comparison. From these ratings, we compute a win rate metric which\nis defined as \ud835\udc64\ud835\udc56\ud835\udc5b/(\ud835\udc64\ud835\udc56\ud835\udc5b + \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60).\n4.4. Checkpoint selection\nFor all RL-finetuned models, we select the best checkpoint by inspecting the quantitative results and\nlistening to the music generations. For MusicRL-R, MusicRL-U, and MusicRL-RU we respectively\nchoose the checkpoint after 10,000 training steps, 2000 training steps, and 1000 training steps.\n5. Results\nWe aim to answer the following questions: (1) Can RL-finetuning on MuLan and quality rewards\nimprove the generation quality of text-to-music models such as MusicLM? (2) Can RLHF improve the\nalignment of the generated music to generic preferences from users? (3) Is it possible to combine all\nreward signals to further improve performance?\n5.1. Quantitative Results\nIn all quantitative evaluations, we analyse model progress during RL finetuning by tracking scores\nof rewards against the KL divergence from the initial model. Regardless of whether we train with a\nsingle reward model or a combination of both as in MusicRL-R, we evaluate model performance on\nall reward signals.\nFigure 4 shows that RL-finetuning successfully optimises both quality and MuLan scores. Specifically,\nfinetuning on the quality reward alone leads to the greatest increase in quality score (from 3.5 MOS\nto 4.6 MOS), and a smaller increase in the MuLan score (from 0.58 to 0.61). Conversely, finetuning\non only the MuLan reward maximises the MuLan score (from 0.58 to 0.71), with a less pronounced\n8\nMusicRL: Aligning Music Generation to Human Preferences\n0\n10\n20\n30\n40\n50\n60\nKL divergence\n3.6\n3.8\n4.0\n4.2\n4.4\n4.6\nQuality Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\n0\n10\n20\n30\n40\n50\n60\nKL divergence\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\nMuLan Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\nFigure 4 | Quality (left) or MuLan score (right) vs KL divergence for the RL-finetuned models. The KL\ndivergence is computed between the RL-finetuned models and MusicLM except for MusicRL-RU where\nthe KL divergence is computed against MusicRL-R. The black cross corresponds to the checkpoint\nused to start the training of MusicRL-RU. RL-finetuning successfully optimises the quality and the\nMuLan scores (MusicRL-R). Additionally, optimizing the user preference reward (MusicRL-RU,\nMusicRL-RU) improves the quality score while marginally decreasing the MuLan score.\n0\n10\n20\n30\n40\n50\n60\nKL divergence\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nUser Preference Reward Model Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\nFigure 5 | User Preference Reward Model Score for the different RL-finetuned models. The KL\ndivergence is computed between the RL-finetuned models and MusicLM except for MusicRL-RU where\nthe KL divergence is computed against MusicRL-R. The black cross corresponds to the checkpoint\nused to start the training of MusicRL-RU. RL-finetuning successfully improves the user preference\nreward model score of the generations (see MusicRL-U and MusicRL-RU curves). When trained on\nother rewards (MuLan and/or quality) the user preference reward model score slightly improves.\nquality score improvement (from 3.5 MOS to 4.1 MOS). Leveraging both quality and MuLan rewards\nsignificantly improves both scores (quality: 3.5 MOS to 4.4 MOS; MuLan: 0.58 to 0.71), while\nmarginally increasing KL divergence. Given the promising and stable performance in simultaneously\noptimizing MuLan and quality scores, we perform qualitative evaluations only on MusicRL-R.\nFigure 8 (in Appendix B) shows that after 10,000 finetuning steps on the quality reward, the reward\nmodel trained on user preference begins assigning lower scores to music samples. This suggests that\nfinetuning solely on the quality reward is prone to reward over-optimisation (Coste et al., 2023; Jiang\net al., 2020; Ram\u00e9 et al., 2024).\n9\nMusicRL: Aligning Music Generation to Human Preferences\nModel\nMOS\n# wins\nMusicLM\n3.07\n133\nMusicRL-R\n3.54\n362\nMusicRL-U\n3.54\n372\nMusicRL-RU\n3.82\n460\nTable 1 | Average mean opinion score (MOS) and number of wins across all rating tasks, for each\nmodel. The music generated from the RL-finetuned models are significantly scored higher in average\nthan the ones from MusicLM. The best performing model both in term of MOS and number of wins is\nMusicRL-RU.\nFigure 5 demonstrates that finetuning with the user preference reward model significantly improves\ngeneration scores, increasing them from -1.5 to over 1.5. Figure 4 shows that despite not training on\nthe quality reward, the quality score increases from 3.5 MOS to 4 MOS. The MuLan score slightly\ndecreases from 0.58 to 0.55. Yet, Figure 7 highlights that over-optimizing the user preference reward\nmodel can drastically reduce the MuLan score. Overall, this suggests that user preference feedback\nparticularly enhances audio quality while having minimal impact on text adherence.\nFigure 5 shows that optimizing the user preference reward model on a model finetuned for 10,000\nsteps on quality and MuLan improves the user preference reward model score significantly. Figure 4\nshows that the quality score slightly increases while the MuLan score slightly decreases, which\nconfirms the impact of the user preference reward model observed in the previous paragraph.\n5.2. Qualitative Results\nFigure 1 presents human rater evaluations of pairwise comparisons between all possible model com-\nbinations across MusicLM, MusicRL-R, MusicRL-U and MusicRL-RU. When compared to MusicLM,\nMusicRL-R wins 65% of the time, ties 22.1% and loses 12.9%. This translates into a 83% win rate\nin favor of MusicRL-R. MusicRL-U is also strongly preferred over MusicLM as it achieves a 74%\nwin rate against MusicLM. The best performing model overall is MusicRL-RU. When compared to\nMusicLM, MusicRL-RU is strongly preferred by the raters with a 87% win rate. When compared\nto the other RL-finetuned models, MusicRL-RU achieves a win rate of 66% against MusicRL-R,\nand 62% against MusicRL-U. All results described above are statistically significant according to a\npost-hoc analysis using the Wilcoxon signed-rank test (Rey and Neuh\u00e4user, 2011).\nTable 1 summarises results from all qualitative evaluations by showing average mean opinion score\n(MOS) and number of wins across all rating tasks, for each model. On both metrics, all RL-finetuned\nmodels outperform MusicLM, with MusicRL-RU being the best performing model.\nLastly, MusicRL-R and MusicRL-U perform comparably according to raters, as shown in Figure 1\nand Table 1.\n5.3. Takeaway\nOur results demonstrate several key findings: (1) MusicRL-R shows that RL-finetuning on text\nadherence and quality rewards improves the generation quality of MusicLM; (2) MusicRL-U con-\nfirms the ability to leverage generic user preferences data to improve MusicLM; (3) MusicRL-RU\noutperforms all other models, demonstrating that the above reward signals are complementary and\ncan be combined for the highest performance.\n10\nMusicRL: Aligning Music Generation to Human Preferences\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nEpochs over the training dataset\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\nAccuracy\n3s\n5s\n10s\n20s\nNo Text\nQuality Predictor\nMuLan Predictor\nFigure 6 | Ablations on the user preference reward model. The reward model is learned either with no\ntext tokens (No Text) or with a cropped version of the input audio (i.e. 10s, 5s, 3s). While dropping\nthe text tokens does not significantly impact the accuracy of the reward model, cropping the audio\nsubstantially degrades performance. This suggests that text adherence and audio quality are not the\nprimary factors influencing user audio preferences, as additionally shown by the low accuracy when\nusing text adherence based (MuLan) or audio quality based predictors for user preference.\n6. Understanding Human Feedback Through the Lens of the Reward Model\nIn this section, we analyse reward model accuracy to uncover the specific music elements that influence\nuser preferences. This analysis directly addresses our research question: What is the user paying\nattention to when rating the audio?\nWe categorise generated music into three components which might drive the users\u2019 choice on their\naudio preferences: (1) text adherence, (2) audio quality, and (3) musicality. In particular, defining\nand modelling musicality is a complex task, which underscores our focus on human feedback as a\nsolution, moving beyond rule-based limitations.\n6.1. Importance of the text input\nTo isolate the impact of text on pairwise preference evaluation, we drop text tokens while training the\nreward model. Accuracy remains stable as shown by Figure 6. Additionally, we measure how often\nthe music clip with the highest MuLan score corresponds to the preferred one. On the evaluation set,\nthese indicators only match 51.6% of the time, which is very close to random accuracy. Overall, these\nfindings indicate that adherence to the text prompt was not a primary driver of human preference in\nour experiment. This aligns with our quantitative results in Section 5.1, which show no significant\nimprovement in text adherence as measured by MuLan, when training MusicRL-U.\n6.2. Importance of the audio quality\nSince audio quality remains relatively consistent within a generated sequence, a few seconds of audio\nshould provide sufficient information to evaluate this aspect. We train reward models on different\ninput audio tokens length corresponding to 10, 5, and 3 seconds. As shown in Figure 6 the evaluation\naccuracy on pairwise preference decreases as we reduce the length of the input tokens, dropping from\n60 to 56% when using 3-5 seconds of input audio. The significant accuracy decrease suggests that\nother musical components play a complementary role in user preference. Additionally, we replicate the\n11\nMusicRL: Aligning Music Generation to Human Preferences\nanalysis done in 6.1 and measure how often the music clip with the highest quality score is preferred.\nAs shown in Figure 6 the quality predictor achieves 53.3% accuracy on the evaluation dataset. These\nfindings indicate that audio quality is not the only driver of human preference, while being a better\nsignal than text adherence. This is consistent with our quantitative results in Section 5.1, where\ntraining MusicRL-U improves marginally on the quality score. Overall, this analysis shows that user\npreference is influenced by music elements which go beyond text adherence and audio quality.\n7. Limitations and Future Work\nAligning feedback and evaluation. When training on user preference data, a limitation of our current\nsetup is the population gap between those who provide feedback to improve the model (general users)\nand those who assess the results (selected raters). A direction for future work is to directly measure\nthe perceived improvements from the user\u2019s perspective.\nUsing on-policy data. For the reasons explained in Section 3.1, in this work we collected user\npreferences on a different version of MusicLM compared to the one used for RL finetuning. A clear\npath for improvement is to iteratively collect on-policy data (data generated by the model that is\nbeing finetuned) and use it to update the model. Eventually, this would allow for real integrated\nfeedback where finetuned models are continuously deployed to collect new feedback while improving\nthe user experience.\nRefining the user preference dataset. Several interesting research directions involve refining\nthe large user interaction dataset. For instance, identifying and retaining examples where users\nexpress a confident and clear preference could reduce noise and improve the overall dataset quality.\nFurthermore, focusing on techniques to train robust reward models on smaller, but highly relevant\ndatasets could facilitate research directions such as model personalization for specific users.\n8. Conclusion\nIn this work, we introduce MusicRL, the first text-to-music generative model aligned with human\npreferences. In a first set of experiments, we derive sequence-level reward functions that inform on\nthe adherence to the caption as well as the acoustic quality. When finetuning a pretrained MusicLM\nmodel to optimise these rewards with RL, the quantitative and qualitative results show consistent\nimprovements over the pretrained baseline. We then show for the first time that we can align music\ngeneration with generic preferences from users. To do so, we collect 300,000 user generated captions\nand audio pairs along with pairwise preferences through a web interface. We leverage this large-scale\nfeedback to train a reward model and improve our model through RLHF, again consistently outper-\nforming the baseline. Lastly, we combine all reward signals to produce the highest performing model.\nAdditional analysis indicates that the signal extracted from user preferences contains information\nbeyond text adherence and audio quality. This highlights the subjective and complex nature of musical\nappeal, emphasizing the value of integrating user feedback when improving music generation models.\nAcknowledgments and Contributions\nAcknowledgments. We thank the AI Test Kitchen team of Google for their contributions in designing\nand deploying the MusicLM experience to users at scale: Sai Kiran Gorthi, Kristin Yim, Phillip Maier,\nSejal Khatri, Shirley Leung, Yilun Liu, Yi Yao and Elias Roman. We thank other members and contrib-\nutors of MusicLM: Timo Denk, Mauricio Zuluaga, Marco Tagliasacchi, Matt Sharifi, Michael Dooley,\nChristian Frank and Hema Manickavasagam. We also thank Robert Dadashi, Nando de Freitas and\n12\nMusicRL: Aligning Music Generation to Human Preferences\nDoug Eck for their valuable feedback on earlier drafts of the paper. Finally, we thank the individuals\nwho designed and built the RL training infrastructure used in this paper: Johan Ferret, Nino Vieillard,\nAlexis Jacq, Sabela Ramos, Piotr Stanczyk, Danila Sinopalnikov, Am\u00e9lie H\u00e9liou, and Nikola Momchev.\nWe are also grateful to all the users of MusicLM for their valuable feedback and their contribution\nin creating MusicRL.\nContributions. Geoffrey Cideron and Andrea Agostinelli (main investigators for the project), Neil\nZeghidour (core contributions on initial design, supervision, quality reward modelling, paper writing),\nSertan Girgin (core infrastructure), Mauro Verzetti (infrastructure, core quality reward modelling),\nL\u00e9onard Hussenot (supervision, core paper writing), Victor Ungureanu (core quality reward mod-\nelling), Matej Kastelic (user data infrastructure), Zal\u00e1n Borsos, Brian McWilliams and Damien Vincent\n(core MusicLM modelling), Olivier Bachem and Olivier Pietquin (supervision, initial design), and\nMatthieu Geist (supervision).\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\nS. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nA. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts,\nM. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank. Musiclm: Generating music from text,\n2023.\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,\net al. Training a helpful and harmless assistant with reinforcement learning from human feedback.\narXiv preprint arXiv:2204.05862, 2022.\nZ. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul,\nD. Grangier, M. Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023a.\nZ. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi. Soundstorm:\nEfficient parallel audio generation, 2023b.\nR. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired\ncomparisons. Biometrika, 39, 1952.\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning\nfrom human preferences. Advances in neural information processing systems, 30, 2017.\nY. Chung, Y. Zhang, W. Han, C. Chiu, J. Qin, R. Pang, and Y. Wu. W2v-bert: Combining contrastive\nlearning and masked language modeling for self-supervised speech pre-training. arXiv:2108.06209,\n2021.\nJ. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D\u00e9fossez. Simple and\ncontrollable music generation, 2023.\nT. Coste, U. Anwar, R. Kirk, and D. Krueger. Reward model ensembles help mitigate overoptimization.\narXiv preprint, 2023.\nA. D\u00e9fossez, N. Zeghidour, N. Usunier, L. Bottou, and F. R. Bach. SING: symbol-to-instrument\nneural generator. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference\n13\nMusicRL: Aligning Music Generation to Human Preferences\non Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al,\nCanada, pages 9055\u20139065, 2018. URL https://proceedings.neurips.cc/paper/2018/\nhash/56dc0997d871e9177069bb472574eb29-Abstract.html.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\nfor language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/N19-1423.\nP. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: A generative model\nfor music. arXiv:2005.00341, 2020.\nA. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi.\nHigh fidelity neural audio compression.\narXiv:2210.13438, 2022.\nB. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang. Clap: Learning audio concepts from natural\nlanguage supervision, 2022.\nJ. H. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, and K. Simonyan. Neural\naudio synthesis of musical notes with wavenet autoencoders. In D. Precup and Y. W. Teh, editors,\nProceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,\nAustralia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1068\u2013\n1077. PMLR, 2017. URL http://proceedings.mlr.press/v70/engel17a.html.\nJ. H. Engel, L. Hantrakul, C. Gu, and A. Roberts. DDSP: differentiable digital signal processing. In\nInternational Conference on Learning Representations (ICLR), 2020.\nS. Forsgren and H. Martiros. Riffusion - Stable diffusion for real-time music generation, 2022. URL\nhttps://riffusion.com/about.\nH. F. Garcia, P. Seetharaman, R. Kumar, and B. Pardo. Vampnet: Music generation via masked acoustic\ntoken modeling, 2023.\nR. Gauldin. A practical approach to eighteenth-century counterpoint. Prentice-Hall, 1988.\nG. Gemini Team. Gemini: A family of highly capable multimodal models. 2023.\nG. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objective-\nreinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint\narXiv:1705.10843, 2017.\nC. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash, M. Malinowski, S. Dieleman, O. Vinyals,\nM. M. Botvinick, I. Simon, H. Sheahan, N. Zeghidour, J. Alayrac, J. Carreira, and J. H. Engel. General-\npurpose, long-context autoregressive modeling with perceiver AR. In K. Chaudhuri, S. Jegelka,\nL. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning\n(ICML), 2022.\nQ. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis. Mulan: A joint embedding of music\naudio and natural language. In International Society for Music Information Retrieval Conference\n(ISMIR), 2022.\n14\nMusicRL: Aligning Music Generation to Human Preferences\nQ. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank,\net al. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint\narXiv:2302.03917, 2023.\nN. Jaques, S. Gu, D. Bahdanau, J. M. Hern\u00e1ndez-Lobato, R. E. Turner, and D. Eck. Sequence tutor:\nConservative fine-tuning of sequence generation models with kl-control. In International Conference\non Machine Learning, pages 1645\u20131654. PMLR, 2017.\nN. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Picard.\nWay off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv\npreprint arXiv:1907.00456, 2019.\nN. Jiang, S. Jin, Z. Duan, and C. Zhang. Rl-duet: Online music accompaniment generation using deep\nreinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34,\npages 710\u2013718, 2020.\nS. M. Karbasi, H. S. Haug, M.-K. Kvalsund, M. J. Krzyzaniak, and J. T\u00f8rresen. A generative model for\ncreating musical rhythms with deep reinforcement learning. 2nd Conference on AI Music Creativity,\n2021.\nE. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi,\nand N. Zeghidour. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision.\nTransactions of the Association for Computational Linguistics, 11:1703\u20131718, 2023. URL https:\n//api.semanticscholar.org/CorpusID:256627687.\nK. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi. Fr\u00e9chet audio distance: A reference-free metric for\nevaluating music enhancement algorithms. In INTERSPEECH, 2019.\nN. Kotecha. Bach2bach: generating music using a deep reinforcement learning approach. arXiv\npreprint arXiv:1812.01060, 2018.\nF. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00e9fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi.\nAudiogen: Textually guided audio generation, 2022.\nS. Latif, H. Cuay\u00e1huitl, F. Pervez, F. Shamshad, H. S. Ali, and E. Cambria. A survey on deep rein-\nforcement learning for audio-based applications. Artificial Intelligence Review, 56(3):2193\u20132240,\n2023.\nD. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual\nquantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 11523\u201311532, 2022.\nK. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu.\nAligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.\nD. J. Lewkowicz. The concept of ecological validity: What are its limitations and is it bad to be invalid?\nInfancy, 2(4):437\u2013450, 2001.\nH. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. P. Mandic, W. Wang, and M. D. Plumbley. Audioldm: Text-\nto-audio generation with latent diffusion models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt,\nS. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages\n21450\u201321474. PMLR, 2023. URL https://proceedings.mlr.press/v202/liu23f.html.\nOpenAI. Gpt-4 technical report. 2023.\n15\nMusicRL: Aligning Music Generation to Human Preferences\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. Advances in\nNeural Information Processing Systems, 35:27730\u201327744, 2022.\nJ. D. Parker, J. Spijkervet, K. Kosta, F. Yesiler, B. Kuznetsov, J.-C. Wang, M. Avent, J. Chen, and D. Le.\nStemgen: A music generation model that listens, 2024.\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn.\nDirect preference\noptimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290,\n2023.\nA. Ram\u00e9, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. Warm: On the\nbenefits of weight averaged reward models, 2024.\nM. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representa-\ntions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL\nhttp://arxiv.org/abs/1511.06732.\nD. Rey and M. Neuh\u00e4user. Wilcoxon-signed-rank test. In M. Lovric, editor, International Encyclopedia\nof Statistical Science, pages 1658\u20131659. Springer, 2011. doi: 10.1007/978-3-642-04898-2\\_616.\nURL https://doi.org/10.1007/978-3-642-04898-2_616.\nP. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi, M. Geist, S. Girgin, L. Hussenot,\nO. Keller, N. Momchev, S. R. Garea, P. Stanczyk, N. Vieillard, O. Bachem, G. Elidan, A. Hassidim,\nO. Pietquin, and I. Szpektor. Factually consistent summarization via reinforcement learning with\ntextual entailment feedback. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, editors, Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages 6252\u20136272. Association for Computational\nLinguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.344. URL https://doi.org/10.18653/\nv1/2023.acl-long.344.\nF. Schneider, O. Kamal, Z. Jin, and B. Sch\u00f6lkopf. Mo\u00fbsai: Text-to-music generation with long-context\nlatent diffusion, 2023.\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.\nLearning to summarize with human feedback. Advances in Neural Information Processing Systems,\n33:3008\u20133021, 2020.\nR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,\n2023.\nM. Tervaniemi. The neuroscience of music\u2013towards ecological validity. Trends in Neurosciences, 2023.\nJ. C. Thomas and W. A. Kellogg. Minimizing ecological gaps in interface design. IEEE Software, 6(1):\n78\u201386, 1989.\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239,\n2022.\n16\nMusicRL: Aligning Music Generation to Human Preferences\nS. E. Trehub, J. Becker, and I. Morley. Cross-cultural perspectives on music and musicality. Philosophical\nTransactions of the Royal Society B: Biological Sciences, 370(1664):20140096, 2015.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in neural information processing systems (NeurIPS), 2017.\nB. Wallace, M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty,\nand N. Naik. Diffusion model alignment using direct preference optimization. arXiv preprint\narXiv:2311.12908, 2023.\nC. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao,\nand F. Wei. Neural codec language models are zero-shot text to speech synthesizers. CoRR,\nabs/2301.02111, 2023. doi: 10.48550/ARXIV.2301.02111. URL https://doi.org/10.48550/\narXiv.2301.02111.\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229\u2013256, 1992.\nY. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In S. A.\nMcIlraith and K. Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial\nIntelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th\nAAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana,\nUSA, February 2-7, 2018, pages 5602\u20135609. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11987.\nURL https://doi.org/10.1609/aaai.v32i1.11987.\nY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/\n1609.08144.\nD. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu. Diffsound: Discrete diffusion model\nfor text-to-sound generation. arXiv:2207.09983, 2022.\nD. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, Z. Zhao, S. Watanabe,\nand H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2023.\nL. Yu, D. Simig, C. Flaherty, A. Aghajanyan, L. Zettlemoyer, and M. Lewis. Megabyte: Predicting\nmillion-byte sequences with multiscale transformers. arXiv preprint arXiv:2305.07185, 2023.\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end\nneural audio codec. IEEE ACM Trans. Audio Speech Lang. Process., 30, 2022.\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n17\nMusicRL: Aligning Music Generation to Human Preferences\nA. Qualitative Evaluation.\nFor the qualitative evaluation, the list of the 101 diverse prompts is the following:\n\u2019A mid-tempo country chorus with a choir counter melody \u2019,\n\u2019grunge with a drum n bass beat\u2019,\n\u2019An eerie, powerful slow metal guitar riff with drums backing that builds tension and anticipation.\u2019,\n\u2019A wistful, nostalgic indie folk-pop song with a strong bass and a deep male voice\u2019,\n\u2019Reggeaton with deep bass and a rapping voice\u2019,\n\u2019A modern, romantic slow waltz played by a jazz trio\u2019,\n\u2019A rock-steady intro with trumpets providing the backing to a gentle guitar\u2019,\n\u2019a funky disco song with a bass player\u2019,\n\u2019A slow rumba composition with a female voice supported by a piano a percussions\u2019,\n\u2019A sad pop melody with piano and strings accompaniment\u2019,\n\u2019Chinese music instruments in futuristic theme, fast pace\u2019,\n\u2019A frantic drum machine beat and pew-pew laser noises fill the cavernous warehouse rave\u2019,\n\u2019fast, classical music with a church organ with an eerie feeling, for a dark thriller soundtrack\u2019,\n\u2019A fast, energetic tango played by an accordion and a violin\u2019,\n\u2019A mellow british-rock acoustic chorus\u2019,\n\u2019Repetitive house music with strong percussive line\u2019,\n\"The sitar\u2019s slow, meandering melody was accompanied by the tabla\u2019s steady beat, creating a sound\nthat was both calming and enchanting.\",\n\u2019Energetic punk rock with a female voice singing\u2019,\n\u2019a cheerful children song with a simple xylophone backing\u2019,\n\u2019An energetic gospel choir performance\u2019,\n\u2019slow, mellow, and instrumental new age music for meditation.\u2019,\n\u2019Flamenco performance full of energy\u2019,\n\u2019Melodic danceable brazilian music with percussions.\u2019,\n\u2019An indie-rock chorus is played by a male singer with a small band backing.\u2019,\n\u2019epic movie soundtrack\u2019,\n\"The K-pop group\u2019s powerful vocals were accompanied by a lush string arrangement, creating a\ntruly epic soundscape.\",\n\u2019A funk bass intro with a guitar playing short chords and a drums backing\u2019,\n\u2019Salsa music played by an orchestra\u2019,\n\u2019A small band plays a latin danceable song\u2019,\n\u2019A whistling tune for a western duel soundtrack\u2019,\n\u2019A samba beat and a lively chorus combine to create a festive atmosphere.\u2019,\n\u2019A jazzy pop song played by a big band\u2019,\n\u2019a ska-punk trumpet riff supported by an up-beat guitar\u2019,\n\u2019male bass low grave voice male-singing a medieval song with a mandolin\u2019,\n\u2019a fast symphonic metal guitar solo with a choir backing\u2019,\n\u2019chorus of a sweet acoustic rock ballad\u2019,\n\u2019A bluesy piano riff drives the band as they belt out a soulful tune.\u2019,\n\u2019A slow, swing pop song with piano and drums backing\u2019,\n\u2019A fusion of reggaeton and electronic dance music, with a spacey, otherworldly sound.\u2019,\n\u2019A marching band plays a catchy tune\u2019,\n\u2019A classical orchestral waltz for a costume dance\u2019,\n\u2019Irish folk chorus with a mandolin and team whistle\u2019,\n\u2019A male voice sings a pop anthem accompanied by his piano\u2019,\n\u2019A catchy pop tune is sung on top a dance drumbeat\u2019,\n\"The soprano\u2019s voice soared over the delicate accompaniment of the piano, filling the opera house\nwith beauty and emotion.\",\n\u2019Rap song with a female melodic line\u2019,\n\u2019a reggae song with guitar and singing\u2019,\n\u2019A corny pop chorus sung by a female voice with a lot of autotune\u2019,\n18\nMusicRL: Aligning Music Generation to Human Preferences\n\"The marimba\u2019s soulful melody was accompanied by the steady beat of the drums, creating a bluesy\nsound that was both melancholy and uplifting.\",\n\u2019A gospel choir sings on top a metal guitar backing\u2019,\n\u2019A powerful female voice sings with soul and energy over a driving drum beat.\u2019,\n\u2019A repetitive lullaby sung by a female voice with a carillon backing\u2019,\n\u2019Traditional fast song played by a male voice with an accordion backing\u2019,\n\u2019An up-beat reggae with a deep male voice and a piano striking the chords\u2019,\n\u2019Slow, melodic music backed by a sitar and strings.\u2019,\n\u2019Funky piece with a strong, danceable beat, a prominent bassline and a keyboard melody.\u2019,\n\"A danceable, fast and cheerful swing tune from the 50\u2019s\",\n\u2019a professional solo cellist playing a sad melody for solo cello on the cello, high quality recording\u2019,\n\u2019A rock guitar riff, a slide guitar solo and a flute melody create a lively, upbeat sound.\u2019,\n\u2019an a cappella chorus singing a christmas song\u2019,\n\u2019nice ragtime guitar chord progression\u2019,\n\"A cheerful R\u2019n\u2019B song is played by two singers with a trumpet melody\",\n\u2019A dance song with a fast melody taken from sampled voice, giving the impression of percussions\u2019,\n\u2019a gospel song with a female lead singer\u2019,\n\u2019a nostalgic tune played by accordion band\u2019,\n\u2019A mariachi song with an epic twist and symphonic orchestra backing\u2019,\n\u2019A middle-easter tune with percussions and flutes\u2019,\n\u2019Jazz composition for piano and trumpet\u2019,\n\u2019A slow blues intro with a harmonica and minimal backing.\u2019,\n\u2019The experimental modular synthesizer created a unique soundscape by combining the sounds of\nwater with electronic music.\u2019,\n\u2019a cheerful ragtime with guitar\u2019,\n\u2019Industrial techno sounds, with hypnotic rhythms. Strings playing a repetitive melody creates an\nunsettling atmosphere.\u2019,\n\u2019The microphone picked up the soulful, funky scream of the lead singer as he reached the climax of\nthe song.\u2019,\n\u2019The snare drum and lute played a lively duet, with the snare drum providing a steady beat and the\nlute playing a melody on top.\u2019,\n\u2019The two rappers traded verses over a pulsating synth beat, creating a sound that was both energetic\nand infectious.\u2019,\n\u2019A bagpipe is playing an aggressive tune with a punk backing\u2019,\n\u2019A string quartet plays a lively tune.\u2019,\n\u2019A very fast piano cadenza that is hard to play.\u2019,\n\u2019A lone harmonica plays a haunting melody over the sound of the wind blowing through the desert.\u2019,\n\u2019An aggressive, but sad punk verse, with a prominent slow guitar melody and dark bass line.\u2019,\n\u2019a band playing cumbia in a boat along the magdalena river in colombia\u2019,\n\u2019A slow jamaican ska song with an organ backing\u2019,\n\u2019The gramophone needle crackled and hissed as it spun across the vinyl record, filling the room with\na warm, nostalgic sound.\u2019,\n\u2019fast piano toccata\u2019,\n\"Romantic R\u2019n\u2019B song with a warm female voice\",\n\u2019A cheerful bollywood-style group dance\u2019,\n\u2019Dance music with a melodic synth line and arpeggiation\u2019,\n\u2019The wooden bongo drums beat a deep, resonating bass as the dancers move their bodies to the\nmusic.\u2019,\n\u2019a tenor singing with a backing guitar\u2019,\n\u2019Slow trap song with a lot of reverb and autotune\u2019,\n\u2019A syncopated progressive rock tune with a saxophone \u2019,\n\u2019A syncopated drum beat backs a hard rock guitar riff\u2019,\n\u2019a gregorian chant\u2019,\n\u2019A danceable folk waltz is played by an accordion\u2019,\n\u2019A bagpipe is playing a fast catchy tune in a dance-pop song\u2019,\n19\nMusicRL: Aligning Music Generation to Human Preferences\n\u2019A full orchestra playing a violin concerto from the 1800s\u2019,\n\u2019The trap beat was layered with acoustic string sounds creating a catchy chorus.\u2019,\n\u2019A church choir sings a high-pitched, soothing melody.\u2019,\n\u2019An energetic dance-pop song, sang by a powerful female voice\u2019,\n\u2019An harmonica plays a melancholic solo over an acoustic guitar\u2019,\n\u2019A fast rap chorus is sung on top of a simple catchy tune\u2019\nB. Additional Quantitative Evaluation Plots\nFigure 7 and Figure 8 show the progress of the RL-finetuned models along training as measured by\nthe three reward signals.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nstep\n3.6\n3.8\n4.0\n4.2\n4.4\n4.6\nQuality Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nstep\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\nMuLan Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\nFigure 7 | Quality (left) or MuLan score (right) vs step for the RL-finetuned models. The black cross\ncorresponds to the checkpoint used to start the training of MusicRL-RU. RL-finetuning successfully\noptimises the quality and the MuLan scores (MusicRL-R). Additionally, optimizing the user preference\nreward (MusicRL-RU, MusicRL-RU) improves the quality score while the MuLan score starts to be\nsignificantly impacted when the model over optimise the user preference reward.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nstep\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nUser Preference Reward Model Score\nMusicRL-MuLan\nMusicRL-Quality\nMusicRL-R\nMusicRL-U\nMusicRL-RU\nstart RU\nFigure 8 | User Preference Reward Model Score for the different RL-finetuned models. The black\ncross corresponds to the checkpoint used to start the training of MusicRL-RU. RL-finetuning suc-\ncessfully improves the user preference reward model score of the generations (see MusicRL-U and\nMusicRL-RU curves). When trained on other rewards (MuLan and/or quality) the user preference\nreward model score slightly improves.\n20\nMusicRL: Aligning Music Generation to Human Preferences\nC. Advantages of User Data\nIn Section 5, we show that we could leverage a model that was trained with human rater data to\nimprove a music generation model with RL. However, rater data have some limitations and biases.\nIn behavioural sciences, the ecological validity5 of a lab study refers its potential of generalization to\nthe real world (Lewkowicz, 2001). In the context of music, it is crucial to experiment on real-world\nsettings (Tervaniemi, 2023). Thomas and Kellogg (1989) explore the ecological validity concept in the\ncontext of interface design and say that \"User-related ecological gaps are caused by characteristics\nof users - such as what motivates them, their cognitive abilities, preferences, and habits - that may\nvary between the lab and the target environment.\" The concept of user-related ecological gaps is\nparticularly relevant for the finetuning and the evaluation of large language models as the raters and\nusers are often dissimilar.\nPopulation Gap. Raters are often not representative of the user population especially as the rating\ntask is often outsourced to crowdsourcing services which employ people in different countries than\nthe ones the model is deployed in e.g. Amazon Mechanical Turk6 proposes a global workforce for\nrating tasks. This population difference creates biases such as cultural biases which can impact the\nmusic preferences (Trehub et al., 2015).\nMotivation Gap. As mentioned in Thomas and Kellogg (1989), the motivation gap which corresponds\nto the difference of motivations between the different users can have a significant effect on the results.\nIn our context, while the users of music generation models have a genuine interest in playing with the\nmodel, the incentive of the raters are very different. Hence, for rating tasks, it is crucial to give specific\nset of instructions to make sure the raters make their decisions aligned with what the creator of the\nrating task would expect which also can be a source of biases. Whereas for users, we are interested in\ngeneral interactions where no instructions are given.\nDataset Size. Due to the cost of rater data, the number of collected human preference is often below\n100,000 (Lee et al., 2023; Stiennon et al., 2020; Ziegler et al., 2019). On the other hand, the number\nof user interactions can be orders of magnitude higher once a model is deployed.\n5https://en.wikipedia.org/wiki/Ecological_validity\n6https://www.mturk.com/\n21\n"
  },
  {
    "title": "Scaling Laws for Downstream Task Performance of Large Language Models",
    "link": "https://arxiv.org/pdf/2402.04177.pdf",
    "upvote": "15",
    "text": "Scaling Laws for Downstream Task Performance of\nLarge Language Models\nBerivan Isik\u00b6\u2217\nNatalia Ponomareva\u00a7\nHussein Hazimeh\u00a7\nDimitris Paparas\u00a7\nSergei Vassilvitskii\u00a7\nSanmi Koyejo\u00b6\u00a7\nStanford University\u00b6, Google Research\u00a7\nAbstract\nScaling laws provide important insights that can guide the design of large language models (LLMs).\nExisting work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in\ntransfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned\non a downstream task, we often also care about the downstream performance. In this work, we study\nthe scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation\ntasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream\nperformance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score.\nOur experiments indicate that the size of the finetuning dataset and the distribution alignment between the\npretraining and downstream data significantly influence the scaling behavior. With sufficient alignment,\nboth downstream cross-entropy and BLEU score improve monotonically with more pretraining data. In\nsuch cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a\nlog-law. However, there are also cases where moderate misalignment causes the BLEU score to fluctuate\nor get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By\nanalyzing these observations, we provide new practical insights for choosing appropriate pretraining data.\n1\nIntroduction\nScaling laws quantify the relationship between a model\u2019s performance and key design factors such as the size\nof the training data or the model\u2019s architecture. In the context of LLMs, these laws offer valuable guidance\nfor model development, resource allocation, and selection of appropriate training data. Extensive research\nhas focused on scaling laws for upstream perplexity or cross-entropy loss (i.e., evaluated on pretraining\ndata), demonstrating that these quantities can be well predicted using power laws (Kaplan et al., 2020;\nHoffmann et al., 2022; Gordon et al., 2021; Hernandez et al., 2022; Fernandes et al., 2023; Bansal et al., 2022;\nHenighan et al., 2020; Johnson et al., 2018). However, in practical applications, LLMs often undergo transfer\nlearning\u2013they are first pretrained on unsupervised data and then finetuned for specific downstream1 tasks\nsuch as coding or translation. The question of whether scaling laws can be used to predict downstream task\nperformance is critical, yet remains largely unanswered (Hernandez et al., 2021; Tay et al., 2021). Here, the\nterm task performance refers to metrics that measure task-related quantities such as accuracy and BLEU\nscore, which are different from next-token prediction metrics such as cross-entropy.\nIn this work, we study scaling laws for transfer learning and focus on machine translation tasks. Specifically,\nwe look into the relation between the pretraining dataset size and the downstream task performance after\nfinetuning on the task. We find that, in addition to the finetuning data size and the choice of the performance\nmetric, this relation fundamentally depends on the alignment between the pretraining data and the downstream\ntask. While similar observations have been made in different contexts in the transfer learning literature\n\u2217Work done while interning at Google Research. Contact: berivan.isik@stanford.edu\n1We use the term downstream to refer to the finetuning task or metrics computed on it, and the term upstream to refer to\nthe metrics computed on the pretraining dataset.\n1\narXiv:2402.04177v1  [cs.CL]  6 Feb 2024\n(Tamkin et al., 2020; Agostinelli et al., 2022), our work provides new insights and concrete scaling laws for\nthe downstream performance of LLMs.\nWe carry out systematic experiments in which we pretrain LLMs on multilingual unsupervised datasets\nand then finetune them on several machine translation tasks. Across the experiments, we vary the type\nof pretraining data (to control the degree of distribution alignment with the downstream task) and the\nfinetuning data size. We study two metrics: downstream BLEU (Papineni et al., 2002) score2 and downstream\ncross-entropy. We find that in settings where the distributions are well-aligned, both BLEU and downstream\ncross-entropy improve monotonically with more pretraining. In these settings, we demonstrate that the\nBLEU score can be well predicted using the following log-law: f(Dp) = (log(A \u00b7 D\u03b1\np ))\u03b2, where Dp denotes\nthe size of the pretraining data, and A, \u03b1, \u03b2 are the coefficients to be fit. We further propose a power-law\nL(Dp) = E +\nA\nD\u03b1\np for the downstream cross-entropy as the pretraining data scales \u2013 echoing similar laws\ndeveloped for the upstream cross-entropy as a function of the pretraining dataset size (Kaplan et al., 2020;\nHoffmann et al., 2022) and downstream cross-entropy as a function of the finetuning dataset size (Hernandez\net al., 2021).\nHowever, when distributions are not sufficiently aligned and the finetuning data size is relatively small,\nwe find that there are cases where the BLEU score exhibits an unclear, non-monotonic behavior, whereas the\ndownstream cross-entropy still improves monotonically following a power-law. This observation suggests that\nusing cross-entropy as a proxy for task-related metrics like BLEU score may lead to critical misjudgments in\npractice if used to make decisions about the \u201crelevance\u201d of the pretraining data for the downstream task or\nthe required size of the pretraining data for the target downstream performance.\nFinally, our empirical studies suggest that pretraining brings little to no improvement on the BLEU score\nwhen the finetuning (translation) dataset is already large enough, complementing the findings of Hernandez\net al. (2021).\nOur contributions and main findings can be summarized as:\n\u2022 We carry out systematic experiments on 770-million and 3-billion encoder-decoder T5 (Raffel et al.,\n2020) models to study how downstream performance, measured by downstream cross-entropy and\nBLEU score, scales with the pretraining dataset size. For pretraining, we experiment with different\nsubsets of the Multilingual C4 (MC4) dataset (Raffel et al., 2020), including English (en), German (de),\nFrench (fr), and Romanian (ro). For finetuning, we study the following translation tasks: WMT-17\nen-de (Bojar et al., 2017), WMT-15 en-fr (Bojar et al., 2014), and WMT-16 en-ro (Bojar et al., 2016).\n\u2022 We observe that, when the distributions of the pretraining and downstream tasks are well-aligned, the\nBLEU score and downstream cross-entropy improve monotonically with more pretraining. For BLEU\nscore, we propose a new log scaling law and show that it has good predictive accuracy.\n\u2022 When the distributions are not sufficiently aligned and the finetuning data size is relatively small, the\nBLEU score fluctuates or even gets worse with more pretraining\u2013losing the monotonic scaling behavior.\nIn these same settings, we find that the downstream cross-entropy still scales monotonically according\nto a power-law.\n\u2022 We argue that the value of pretraining data should be evaluated using downstream task-related metrics\nlike BLEU score and propose a practical guide for such an assessment by leveraging the proposed scaling\nlaw for BLEU score.\n2\nRelated Work\nScaling laws for transformers.\nScaling laws for LLMs have attracted significant attention as they can\ninform the decisions about key design choices such as model size and the type and size of the pretraining\ndata (Kaplan et al., 2020; Hoffmann et al., 2022; Hernandez et al., 2021). Most of the pioneering work has\nfocused on how upstream cross-entropy loss or perplexity scales with more pretraining data, larger models, or\nlonger training (Kaplan et al., 2020; Hoffmann et al., 2022). Follow-up works have analyzed scaling behavior\nof translation models (Ghorbani et al., 2021; Zhuocheng et al., 2023; Gordon et al., 2021; Fernandes et al.,\n2In the rest of the paper, we will drop \u201cdownstream\u201d when we refer to the downstream BLEU score.\n2\n2023; Bansal et al., 2022; Zhang et al., 2022), studied theoretical foundation behind scaling laws (Sharma and\nKaplan, 2020; Hutter, 2021; Bahri et al., 2021), or extended the laws to the vision models (Zhai et al., 2022;\nJain et al., 2023). Closest to our work, Hernandez et al. (2021) have analyzed transfer learning but with\na focus on how the cross-entropy loss behaves as the finetuning data scales. Unlike our work, their scaling\nlaw describes the relation between the size of a (finetuning) dataset and the cross-entropy loss on the same\ndataset \u2013 making this closer to the standard scaling laws in the literature since the finetuning loss and the\nfinetuning dataset are computed over samples from the same distribution. On the other hand, we propose\nscaling laws for the downstream metrics on the finetuning dataset as the pretraining data scales \u2013 switching\nthe focus to an \u201cout-of-distribution\u201d analysis. The only work we are aware of that has proposed scaling laws\nfor the downstream task performance as a function of pretraining dataset size is by Sun et al. (2017) who\nhave focused on classification tasks in the vision domain and used small models relative to LLMs.\nTransferability metrics and value of pretraining.\nWhile it may be commonly suggested that pretraining\ndata improves both upstream and downstream performance, this rule has been challenged in the vision domain.\nZoph et al. (2020); He et al. (2019); Shen et al. (2019); Ghiasi et al. (2018); Mikami et al. (2022) have\ndemonstrated that pretraining can sometimes have no effect on the downstream task performance and\nsometimes it can even hurt the performance. We make similar observations in the language domain with\nextensive experiments on LLMs and identify cases where (a) adding more pretraining data hurts the downstream\ntask performance when pretraining data is not aligned enough with the task and (b) pretraining does not\nimprove the downstream task performance noticeably when the finetuning dataset is large enough. Another\nrelated line of work is on transferability metrics (Tamkin et al., 2020; Chiang and Lee, 2022; Ibrahim et al.,\n2022; Tran et al., 2019; Agostinelli et al., 2022; Tran et al., 2019; Nguyen et al., 2020; You et al., 2021;\nDai et al., 2019; Huang et al., 2022; Ibrahim et al., 2022; Tran et al., 2019; Bao et al., 2019; Van Asch\nand Daelemans, 2010; Plank and Van Noord, 2011), which are efficient heuristics used to select the most\nappropriate source models or pretraining data for a given target task. We note that transferability metrics are\ndesigned to solve ranking problems, different from scaling laws. For example, these metrics answer questions\nsuch as given a pool of source models (or pretraining datasets), which source model (or pretraining dataset)\nis the best to finetune on for a given target task. These metrics are not designed to predict the performance\nof the model when key quantities (e.g., pretraining data size) are scaled.\n3\nScaling Laws for Transfer Learning\nIn this section, we present our proposed scaling laws for BLEU score and downstream cross-entropy. We\nalso discuss when these laws apply and provide practical guidance for assessing the value of a pretraining\ndataset for a given target downstream task. The details of the experimental results will be later discussed in\nSection 5.\n3.1\nA Scaling Law for the BLEU Score\nDifferent from cross-entropy and perplexity, which follow a power-law scaling behavior Kaplan et al. (2020);\nHoffmann et al. (2022), we find out that BLEU score scales closer to a log-law, as evident from Figures 1, 2,\nand 3. Therefore, we propose the following scaling law for BLEU score as a function of the pretraining dataset\nsize Dp:\nf(Dp) = (log(A \u00b7 D\u03b1\np ))\u03b2,\n(1)\nwhere A, \u03b1, and \u03b2 are coefficients to be fit. We notice that these coefficients depend on how aligned the\npretraining dataset with the target downstream task (translation from language 1 to language 2) and how\nlarge the finetuning (translation) dataset is. With extensive experiments across several translation tasks and\nmultilingual pretrained models, we demonstrate that the law in (1) indeed well describes BLEU score scaling,\nwith a small prediction error which we quantify in Appendix B.2.\n3\n3.2\nIs Cross-Entropy Loss Always a Good Metric?\nWe also compare the downstream cross-entropy loss and the BLEU score empirically as prior work has made\nthe assumption that upstream or downstream cross-entropy loss is a good indicator for a model\u2019s downstream\ntask performance. Following the well-understood scaling behavior of the upstream cross-entropy loss as a\nfunction of the pretraining dataset size Kaplan et al. (2020); Hoffmann et al. (2022), we demonstrate that the\nsame scaling law can also describe the downstream cross-entropy loss as\nL(Dp) = E + A\nD\u03b1p\n,\n(2)\nwhere E, A, and \u03b1 are the coefficients to be optimized. Throughout the paper, we report BLEU score\nand cross-entropy together for a direct comparison and discover several cases where the two metrics do not\ncorrelate well. This supports some of the findings of Ghorbani et al. (2021) suggesting inconsistency between\nthe BLEU score and the cross-entropy, but also shows that the exponential relationship (between the two\nmetrics) advocated by Gordon et al. (2021) does not always hold. More specifically, our empirical results\nshow that while cross-entropy loss always monotonically decreases (with appropriate learning rate) as the\npretraining dataset size increases, BLEU score may show a non-monotonic trend when the pretraining data is\nnot sufficiently aligned with the task. For instance, in Figure 3-(top, right), increasing the en-MC4, de-MC4,\nor ro-MC4 pretraining datasets\u2019 size sometimes decreases the BLEU score on WMT-15 English-to-French\n(en-fr) translation task. Even though they may initially follow the law in (1) for smaller pretraining dataset\nsizes, the scaling law breaks for larger data for these datasets and task. Overall, the BLEU score never reaches\na good value compared to other pretraining datasets that include some amount of French \u2013 indicating that\npretraining datasets that do not include French are not aligned enough with this particular translation task.\nHowever, if we were to look at only the cross-entropy loss in Figure 3-(bottom, right), we would conclude that\nall the pretraining datasets bring noticeable improvements to the model and they all are worth adding into\nthe pretraining data \u2013 which would be a poor decision.\nA remotely related observation on the mismatch between the task-related metrics and the cross-entropy\nby McKenzie et al. (2023), who looked at how the downstream task performance changes as the model grows,\nsuggests that LLMs may show worse task performance with increased model size but, similar to our findings,\nthis is not captured by the monotonically decreasing cross-entropy loss.\n3.3\nWhen Do Scaling Laws Fall Short in Transfer Learning?\nWhile the cross-entropy loss always follows a monotonically decreasing trend which can be captured by\nthe scaling law in (2), we do not always see a monotonic increase in the BLEU score when increasing the\npretraining dataset size (see Figure 2-(top, center) and Figure 3-(top, right)). We observe that this only\nhappens when the pretraining dataset is not sufficiently aligned with the translation task \u2013 which results\nin low BLEU scores overall compared to models that were pretrained in other datasets. For the pretrained\nmodels that lead to high BLEU scores after finetuning, we consistently see that the BLEU score increases\nmonotonically and can be well described with the scaling law in (1). Therefore, whether the scaling law could\nfit the empirical BLEU scores or not could be a good first-check in assessing the value of pretraining data for\nthe downstream (translation) task. We elaborate more on this in the next section.\n3.4\nA Guide for Pretraining Data Valuation\nFinally, combining our findings on the scaling behavior of BLEU score, we propose the following guide for\nassessing the value of pretraining dataset for a target downstream task:\n1. Given a pretraining dataset, pretrain as long as possible under the given computational and time\nconstraints3. Periodically choose pretraining checkpoints, finetune on them, and record the downstream\nperformance metric (we recommend the BLEU score over cross-entropy due to the discussion in\nSection 3.3).\n3We avoid repeating sequences as repetitions may complicate the scaling behavior (Hernandez et al., 2022; Muennighoff et al.,\n2023; Tirumala et al., 2023). This means as pretraining goes on, we effectively pretrain each checkpoint on a \u201clarger dataset\u201d.\n4\n2. Since the law in (1) has three coefficients to be fit, once we have 3 pairs of (number of pretraining tokens\nseen, BLEU score), we try to find the optimal coefficients. If the BLEU scores have a non-monotonic\nbehavior, we cannot fit the scaling law. Since the non-monotonic behavior could be an indication of\nmisalignment (following the discussion in Section 3.3), we recommend checking the BLEU score of the\nbest available finetuned checkpoint and comparing it to the performance of the non-pretrained model\ntrained on the downstream task directly. If the scaling law fits well, then we make the initial prediction\nfor the BLEU score as we increase the pretraining dataset size (or pretrain for more steps). If we are\nnot satisfied with the predicted BLEU score, then we conclude that it is not worth pretraining on this\ndataset. If the predicted BLEU score is high enough, then we keep pretraining until we reach the target\nBLEU score. If the scaling law breaks at any point, we conclude that the pretraining dataset is not\nsufficiently aligned with the downstream task and pretraining further may not be beneficial.\n4\nExperimental Setup\nIn the experiments, we first pretrain a model without doing more than one pass over any of the examples.\nThen, we finetune selected checkpoints of the pretrained model. Naturally, there is a one-to-one mapping\nbetween the checkpoint number and the number of pretraining tokens seen. This way, we collect pairs of\n(number of pretraining tokens, BLEU score) and (number of pretraining tokens, downstream cross-entropy\nloss) to analyze them with the proposed scaling laws in (1) and (2). All the plots are on a log-log scale.\nModel.\nWe use the 3-billion encoder-decoder T5 model with 24 encoder layers, 24 decoder layers, embedding\ndimension 1024, and 32 heads with dimension 128. We note that this is the same model as the T5-3B model\nin Abnar et al. (2022). In Appendix B, we also provide results with a smaller 770-million encoder-decoder\nT5 model. This model corresponds to T5-Large in Raffel et al. (2020). We share more details about the\narchitectures in Appendix A. For encoding the text as WordPiece tokens (Sennrich et al., 2016; Kudo, 2018),\nwe use SentencePiece (Kudo and Richardson, 2018) trained with a vocabulary of size 250, 112 that covers all\nthe languages in the MC4 dataset (Raffel et al., 2020).\nDatasets.\nWe use the English (en), German (de), French (fr), and Romanian (ro) portions of the MC4\ndataset. We experiment with both pretraining on these languages individually as well as mixing pairs of\nlanguages. In Figure 1, we present results for the models pretrained on (left) a mixture of 50% en-MC4 +\n50% de-MC4, (center) a mixture of 50% en-MC4 + 50% fr-MC4, and (right) a mixture of 50% en-MC4 +\n50% ro-MC4 \u2013 meaning that 50% of one pretraining batch is sampled from en-MC4 and the other 50% is\nsampled from the other language. In Figure 2, we show results for the models pretrained only on en-MC4. In\nFigure 3, in addition to these, we also present results for the models pretrained on a mixture of 30% en-MC4\n+ 70%-fr and a mixture of 70% en-MC4 + 30%-fr as well as models pretrained only on de-MC4, only on\nfr-MC4, and only on ro-MC4. We finetune the pretrained models on WMT-17 en-de (Bojar et al., 2017),\nWMT-15 en-fr (Bojar et al., 2014), and WMT-16 en-ro (Bojar et al., 2016), separately. To understand the\neffect of the finetuning dataset size on the scaling laws, we sometimes use a smaller randomly sampled portion\nfrom these translation datasets and indicate the number of tokens used.\nHyperparameters.\nDuring pretraining, we use a batch size of 256 and a sequence length of 512 for\n1, 000, 000 steps except for the ro-MC4 pretraining. For ro-MC4, we pretrain for 510, 000 steps since otherwise,\nwe would need to do repetitions over the sequences. Following Raffel et al. (2020), we use an \u201cinverse square\nroot\u201d learning rate schedule,\n1\n\u221a\nmax(n,k), where n is the current pretraining step and k is set to 104. We\ndo a grid search for the base learning rate from {0.05, 0.1, 0.5, 1.0, 2.0, 5.0} and pick the best one for each\npretrained model based on upstream cross entropy. During finetuning, again following Raffel et al. (2020),\nwe use a batch size of 128 and a sequence length of 512 for 300 steps. We use a constant learning rate by\nselecting the best from {0.001, 0.005, 0.01, 0.05, 0.1}. In both stages, we use the AdaFactor optimizer (Shazeer\nand Stern, 2018).\n5\nFigure 1: (top) BLEU score vs pretraining dataset size: f(Dp) = (log(A \u00b7 D\u03b1\np))\u03b2. (left) WMT-17\nen-to-de translation task. Pretraining dataset has 50% en-MC4 + 50% de-MC4. Dotted, dashed, and solid\nblue curves correspond to the fitted scaling laws for different finetuning dataset sizes, Df = 6M, Df = 31M,\nDf = 3B tokens, respectively. (center) WMT-15 en-to-fr translation task. Pretraining dataset has 50%\nen-MC4 and 50% fr-MC4. Dotted, dashed, and solid orange curves correspond to the fitted scaling laws for\ndifferent finetuning dataset sizes, Df = 42M, Df = 210M, Df = 21B tokens, respectively. (right) WMT-16\nen-to-ro translation task. Pretraining dataset has 50% en-MC4 + 50% ro-MC4. Dotted, dashed, and solid\ngreen curves correspond to the fitted scaling laws for different finetuning dataset sizes, Df = 625K, Df = 3M,\nDf = 312M tokens, respectively. (bottom) Cross-entropy (CE) validation loss vs pretraining\ndataset size: L(Dp) = E +\nA\nD\u03b1\np . Same models as the top row. For all the plots, the markers are the actual\nexperimental results and the black horizontal curves correspond to the non-pretrained model directly trained\non the task dataset. The finetuning dataset size increases in the order of dotted-dashed-solid for\nall the curves including the black horizontal lines.\nOptimizing the scaling law coefficients.\nTo fit the coefficients in the scaling laws in (1) and (2), similar\nto Hoffmann et al. (2022), we use the Huber loss (Huber, 1992) and the L-BFGS algorithm (Nocedal, 1980)\nto estimate the scaling law robustly in the presence of outliers. For the Huber loss, we use \u03b4 = 0.1 for the\nBLEU score and \u03b4 = 1e \u2212 3 for the downstream cross-entropy loss. We select the best fit among a grid of\ninitializations and report the prediction error computed via the Huber loss in Appendix B.2. To optimize the\ncoefficients, we use the first four data points that require the smallest amount of pretraining data and leave\nthe remaining data points as held-out data to evaluate the accuracy of the laws. We note that, ideally, three\npoints should be enough since both laws have three coefficients to be optimized for. However, adding more\npoints improves the fit by making the optimization more robust to outliers. We provide more details about\nhow to optimize the scaling law coefficients in Appendix A.2. We refer the reader to Appendix B.2 for the\nlist of optimized coefficients and the prediction errors for each law we present in the next section.\n5\nResults and Analysis\nIn Figure 1, we analyze the models that are pretrained on different portions of (left) a mixture of 50% en-MC4\n+ 50% de-MC4, (center) a mixture of 50% en-MC4 + 50% fr-MC4, and (right) a mixture of 50% en-MC4 +\n50% ro-MC4. These models are then finetuned on different portions of (left) en-de, (center) en-fr, and (right)\nen-ro translation datasets. In the top row, we report the BLEU score and, in the bottom row, we report\nthe downstream cross-entropy loss. The dotted, dashed, and solid lines correspond to the scaling laws in\n6\nFigure 2: (top) BLEU score vs pretraining dataset size: f(Dp) = (log(A \u00b7 D\u03b1\np))\u03b2. (left) WMT-17\nen-to-de translation task. Dotted, dashed, and solid red curves correspond to the fitted scaling laws for\ndifferent finetuning dataset sizes, Df = 6M, Df = 31M, Df = 3B tokens, respectively. (center) WMT-15\nen-to-fr translation task. Dotted, dashed, and solid red curves correspond to the fitted scaling laws for\ndifferent finetuning dataset sizes, Df = 42M, Df = 210M, Df = 21B tokens, respectively. (right) WMT-16\nen-to-ro translation task. Dotted, dashed, and solid red curves correspond to the fitted scaling laws for\ndifferent finetuning dataset sizes, Df = 625K, Df = 3M, Df = 312M tokens, respectively. (bottom)\nCross-entropy (CE) validation loss vs pretraining dataset size: L(Dp) = E +\nA\nD\u03b1\np . Same models as\nthe top row. For all the plots, the markers are the actual experimental results and the black horizontal curves\ncorrespond to the non-pretrained model directly trained on the task dataset. The finetuning dataset size\nincreases in the order of dotted-dashed-solid for all the curves including the black horizontal\nlines.\n(1) and (2) for different finetuning dataset sizes Df. The black lines correspond to \u201cnon-pretrained\u201d models\n(randomly initialized) that are directly trained on different portions of the finetuning dataset. In all cases, the\nscaling laws fit well to the empirical results (the markers) with prediction error at most 0.061 for the BLEU\nscore (\u03b4 = 0.1) and 5.95e \u2212 12 for the downstream cross-entropy (\u03b4 = 1e \u2212 3) (see Appendix B.2 for more\ndetails). As expected, as the finetuning dataset size increases (e.g., going in the order of dotted-dashed-solid\nlines), the BLEU score increases and the cross-entropy loss decreases smoothly and monotonically. Similarly,\nas the pretraining dataset size Dp increases (along the x-axis), we see improvements in both metrics. Notice\nthat the improvements by an increase in the pretraining dataset size is more effective for smaller finetuning\ndatasets. When the finetuning dataset is large enough (e.g., solid lines), BLEU score is more or less constant\nregardless of the pretraining dataset size. In fact, we see little to no improvement of pretraining compared to\nthe non-pretrained models (black lines) when the finetuning dataset is large. This implies that, for these\ntasks, there is no need to pretrain the models when the finetuning dataset is large enough.\nLuckily, we can correctly predict whether this is going to be the case (i.e., whether the available\nfinetuning data is enough to eliminate pretraining altogether) with the use of scaling laws.\nAll we need to do is to pretrain the model on a small portion of the pretraining dataset with\nreasonable compute cost to optimize the coefficients of the scaling laws, and then follow the\nguideline provided in Section 3.4.\nIn Figure 2, we change the pretraining dataset to 100% en-MC4 in all plots. Intuitively, we expect this\ndataset to be less aligned with the translation tasks than the multilingual pairs in Figure 1 since it does\nnot include one of the languages in the translation tasks. Indeed, we see smaller BLEU score and higher\ncross-entropy loss in general for the same finetuning dataset size. Most of the conclusions from Figure 1 carry\n7\nFigure 3: Comparison of scaling behavior for different pretraining datasets.\n(top) BLEU score vs\npretraining dataset size: f(Dp) = (log(A \u00b7 D\u03b1\np))\u03b2. (left) WMT-17 en-de translation task. (right) WMT-\n15 en-fr translation task. (bottom) Cross-entropy (CE) validation loss vs pretraining dataset size:\nL(Dp) = E +\nA\nD\u03b1\np . Same as the top row but for CE loss instead of BLEU score. For all the plots, the markers\nare the actual experimental results and the black horizontal curves correspond to the non-pretrained model\ndirectly trained on the task dataset.\nover to the results in Figure 2. For instance, the pretraining data matters less when the finetuning dataset is\nlarge enough. One noticeable difference is in the BLEU scores for the en-fr translation task (center). We see\nthat, for Df = 42M and Df = 210M, the scaling law for BLEU score actually breaks once the pretraining\ndataset size passes a threshold while the cross-entropy loss scales as expected. This is counter-intuitive\nbecause the BLEU score sometimes decreases for larger pretraining dataset. Notice that this break in scaling\nlaw does not happen in en-de or en-ro translation tasks as the scaling law fits well to the pretraining data\nwith prediction error at most 0.025 for these tasks (\u03b4 = 0.1). To better investigate this, in Figure 3, we take\na closer look at some less aligned pretraining datasets due to the choice of language.\nIn Figure 3-(left), we provide the scaling laws for en-de translation task where the pretraining datasets\nare 100% en-MC4 (same as Figure 2-(left)), 50% en-MC4 and 50% de-MC4 (same as Figure 1-(left)), 100%\nde-MC4, 100% fr-MC4 (less aligned), and 100% ro-MC4 (less aligned). Notice that the last two pretraining\ndatasets are expected to be the least aligned with the translation task since the translation pair does not\ninclude these languages. We see that, despite this, the scaling laws consistently fit well for both the BLEU\nscore and the cross-entropy loss. However, this is not always the case for the en-fr translation task. In\nFigure 3-(right), we provide the scaling laws for the en-fr translation task where the pretraining datasets are\ndifferent mixtures of en-MC4 and fr-MC4 datasets. We also include the \u201cless aligned\u201d pretraining datasets such\nas 100% de-MC4 and 100% ro-MC4. Surprisingly, we see that the scaling law for the BLEU score breaks after\nsome point for the only-English (100% en-MC4), only-German (100% de-MC4), and only-Romanian (100%\nro-MC4) pretraining datasets while the cross-entropy loss always follows the scaling law in (2). Interestingly,\nwe do not observe such a break in the BLEU score scaling for the only-French (100% fr-MC4) pretraining\n8\nFigure 4: BLEU score vs. downstream cross-entropy loss. (left) For en-de translation task, we see a\nconsistent correlation between the two metrics for all the pretraining datasets. This supports the findings of\nGordon et al. (2021). (right) For en-fr translation task, the two metrics usually show an arbitrary relation.\nSometimes, the BLEU score increases while the cross-entropy also increases. Unlike the en-de results in (left),\nthe exponential relation in (Gordon et al., 2021) is not observed here.\ndataset \u2013 hinting that not including French data in pretraining leads to poor scaling in the en-fr translation\ntask but not including English does not have such an effect. We also notice that the BLEU score is the lowest\nfor these three pretraining datasets where scaling breaks. This suggests that the scaling law in (1)\nworks well for the BLEU score as long as the pretraining dataset has the promise to give rise\nto a good performance. However, when the scaling law does not fit well, we may suspect the\nBLEU score to be low overall. Therefore, whether we can fit the scaling law for the BLEU\nscore seems to give a good indication about the degree of alignment between the pretraining\ndata and the particular translation task.\nRemark 5.1. We observe another interesting phenomenon in Figure 3. For both en-de and en-fr tasks, 100%\nen-MC4 leads to significantly worse BLEU score and downstream cross-entropy than the more aligned 50%\nen-MC4 + 50% de/fr-MC4 balanced datasets, respectively. However, de-MC4 and fr-MC4 perform almost as\nwell as the balanced datasets in en-de and en-fr tasks. We leave the investigation of why pretraining on only\nGerman/French helps more than pretraining on only English for the given en-de and en-fr tasks to future\nwork.\nWe also highlight that we cannot make any strong conclusion about the degree of alignment of the\npretraining dataset with the task by only looking at the downstream cross-entropy loss because of the\ninconsistency with the BLEU score, a task-related metric, observed in the en-fr plots in Figures 2 and 3. This\nis a counter-example for the claim by Gordon et al. (2021) that the two metrics have an exponential relation.\nTo better demonstrate this, in Figure 4, we provide a BLEU score vs. downstream cross-entropy log-log plot for\nen-de and en-fr translation tasks, respectively. While the two metrics indeed seem correlated in Figure 4-(left)\non the en-de task, we observe a somewhat arbitrary relation for the en-fr task in Figure 4-(right) in some\ncases \u2013 which clearly cannot be explained with an exponential relation. This suggest that downstream\ncross-entropy is not always a good indicator for BLEU score. This raises the question whether\nthe scaling laws that have been developed for the upstream cross-entropy loss are actually\nuseful predictors for models\u2019 downstream behavior.\nRemark 5.2. We also revisit the definition of the BLEU score to better understand the root cause of the\nnon-smooth behavior and check if we could see a smooth monotonic scale in at least some elements of the\nBLEU score calculation. Recall that the common form of BLEU score is defined as\nBLEU = brevity-penalty \u00b7\n 4\nY\ni=1\nprecisioni\n!1/4\n,\n(3)\nwhere precisionn refers to the precision of n-grams, and the second term is the geometric mean of the precision\nwhen n is varied from 1 to 4. In all the experiments, we observe brevity-penalty = 1, i.e., the non-smooth\n9\nbehavior can be attributed to the precision terms. Hence, our findings, including the scaling law in (1), would\nalso apply for precision\u2013another downstream task metric.\n6\nDiscussion and Conclusion\nWe study the scaling behavior of the downstream performance of LLMs as the pretraining data grows and\npropose scaling laws for both downstream cross-entropy and the BLEU score. We demonstrate through\nextensive experiments that the scaling behavior is significantly influenced by (1) the degree of alignment\nbetween the pretraining and the downstream data and (2) the finetuning dataset size. In favorable cases\nwhere the distributions are sufficiently aligned, we show that BLEU score can be accurately predicted using a\nlog scaling law. However, with less alignment, there are cases where BLEU score fluctuates unpredictably\nwhereas downstream cross-entropy improves monotonically. We also observe that when the finetuning dataset\nsize is sufficiently large, pretraining has little to no value.\nOur findings highlight the importance of studying downstream performance metrics and not making\ndecisions solely based on cross-entropy (whether upstream or downstream). This echoes the findings of\nSchaeffer et al. (2023) about the discrepancy in behavior between smooth and non-smooth metrics when\nmodels are scaled.\nReferences\nAbnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H. (2022). Exploring the limits of large scale pre-training.\nIn International Conference on Learning Representations.\nAgostinelli, A., Uijlings, J., Mensink, T., and Ferrari, V. (2022).\nTransferability metrics for selecting\nsource model ensembles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7936\u20137946.\nBahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). Explaining neural scaling laws. arXiv\npreprint arXiv:2102.06701.\nBansal, Y., Ghorbani, B., Garg, A., Zhang, B., Cherry, C., Neyshabur, B., and Firat, O. (2022). Data scaling\nlaws in nmt: The effect of noise and architecture. In International Conference on Machine Learning, pages\n1466\u20131482. PMLR.\nBao, Y., Li, Y., Huang, S.-L., Zhang, L., Zheng, L., Zamir, A., and Guibas, L. (2019). An information-theoretic\napproach to transferability in task transfer learning. In 2019 IEEE international conference on image\nprocessing (ICIP), pages 2309\u20132313. IEEE.\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M.,\nSaint-Amand, H., et al. (2014). Findings of the 2014 workshop on statistical machine translation. In\nProceedings of the ninth workshop on statistical machine translation, pages 12\u201358.\nBojar, O. r., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu,\nQ., Logacheva, V., Monz, C., Negri, M., Post, M., Rubino, R., Specia, L., and Turchi, M. (2017). Findings\nof the 2017 conference on machine translation (wmt17). In Proceedings of the Second Conference on\nMachine Translation, Volume 2: Shared Task Papers, pages 169\u2013214, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nBojar, O. r., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., Jimeno Yepes, A., Koehn,\nP., Logacheva, V., Monz, C., Negri, M., Neveol, A., Neves, M., Popel, M., Post, M., Rubino, R., Scarton,\nC., Specia, L., Turchi, M., Verspoor, K., and Zampieri, M. (2016). Findings of the 2016 conference on\nmachine translation. In Proceedings of the First Conference on Machine Translation, pages 131\u2013198, Berlin,\nGermany. Association for Computational Linguistics.\n10\nChiang, C.-H. and Lee, H.-y. (2022). On the transferability of pre-trained language models: A study from\nartificial datasets. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages\n10518\u201310525.\nDai, X., Karimi, S., Hachey, B., and Paris, C. (2019). Using similarity measures to select pretraining data\nfor ner. In Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n1460\u20131470.\nFernandes, P., Ghorbani, B., Garcia, X., Freitag, M., and Firat, O. (2023). Scaling laws for multilingual\nneural machine translation. arXiv preprint arXiv:2302.09650.\nGhiasi, G., Lin, T.-Y., and Le, Q. V. (2018). Dropblock: A regularization method for convolutional networks.\nAdvances in neural information processing systems, 31.\nGhorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., and Cherry, C. (2021).\nScaling laws for neural machine translation. In International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Kaplan, J. (2021). Data and parameter scaling laws for neural machine\ntranslation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t., editors, Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing, pages 5915\u20135922, Online and Punta\nCana, Dominican Republic. Association for Computational Linguistics.\nHe, K., Girshick, R., and Doll\u00e1r, P. (2019). Rethinking imagenet pre-training. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4918\u20134927.\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P.,\nGray, S., et al. (2020). Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701.\nHernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds,\nZ., Henighan, T., Hume, T., et al. (2022). Scaling laws and interpretability of learning from repeated data.\narXiv preprint arXiv:2205.10487.\nHernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. (2021). Scaling laws for transfer. arXiv preprint\narXiv:2102.01293.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks,\nL. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556.\nHuang, L.-K., Huang, J., Rong, Y., Yang, Q., and Wei, Y. (2022). Frustratingly easy transferability estimation.\nIn International Conference on Machine Learning, pages 9201\u20139225. PMLR.\nHuber, P. J. (1992). Robust estimation of a location parameter. In Breakthroughs in statistics: Methodology\nand distribution, pages 492\u2013518. Springer.\nHutter, M. (2021). Learning curve theory. arXiv preprint arXiv:2102.04074.\nIbrahim, S., Ponomareva, N., and Mazumder, R. (2022). Newer is not always better: Rethinking transferability\nmetrics, their peculiarities, stability and performance. In Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, pages 693\u2013709. Springer.\nJain, A., Swaminathan, G., Favaro, P., Yang, H., Ravichandran, A., Harutyunyan, H., Achille, A., Dabeer, O.,\nSchiele, B., Swaminathan, A., et al. (2023). A meta-learning approach to predicting performance and data\nrequirements. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3623\u20133632.\nJohnson, M., Anderson, P., Dras, M., and Steedman, M. (2018). Predicting accuracy on large datasets\nfrom smaller pilot data. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 450\u2013455.\n11\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J.,\nand Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\nKudo, T. (2018). Subword regularization: Improving neural network translation models with multiple subword\ncandidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 66\u201375.\nKudo, T. and Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. EMNLP 2018, page 66.\nMcKenzie, I. R., Lyzhov, A., Pieler, M., Parrish, A., Mueller, A., Prabhu, A., McLean, E., Kirtland, A., Ross,\nA., Liu, A., et al. (2023). Inverse scaling: When bigger isn\u2019t better. arXiv preprint arXiv:2306.09479.\nMikami, H., Fukumizu, K., Murai, S., Suzuki, S., Kikuchi, Y., Suzuki, T., Maeda, S.-i., and Hayashi, K.\n(2022). A scaling law for syn2real transfer: How much is your pre-training effective? In Joint European\nConference on Machine Learning and Knowledge Discovery in Databases, pages 477\u2013492. Springer.\nMuennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., and Raffel,\nC. (2023). Scaling data-constrained language models. In Thirty-seventh Conference on Neural Information\nProcessing Systems.\nNguyen, C., Hassner, T., Seeger, M., and Archambeau, C. (2020). Leep: A new measure to evaluate\ntransferability of learned representations. In International Conference on Machine Learning, pages 7294\u2013\n7305. PMLR.\nNocedal, J. (1980). Updating quasi-newton matrices with limited storage. Mathematics of computation,\n35(151):773\u2013782.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311\u2013318.\nPlank, B. and Van Noord, G. (2011). Effective measures of domain similarity for parsing. In Proceedings of\nthe 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,\npages 1566\u20131576.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020).\nExploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551.\nSchaeffer, R., Miranda, B., and Koyejo, S. (2023). Are emergent abilities of large language models a mirage?\nIn Thirty-seventh Conference on Neural Information Processing Systems.\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 1715\u20131725.\nSharma, U. and Kaplan, J. (2020). A neural scaling law from the dimension of the data manifold. arXiv\npreprint arXiv:2004.10802.\nShazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. In\nInternational Conference on Machine Learning, pages 4596\u20134604. PMLR.\nShen, Z., Liu, Z., Li, J., Jiang, Y.-G., Chen, Y., and Xue, X. (2019). Object detection from scratch with deep\nsupervision. IEEE transactions on pattern analysis and machine intelligence, 42(2):398\u2013412.\nSun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843\u2013852.\n12\nTamkin, A., Singh, T., Giovanardi, D., and Goodman, N. (2020). Investigating transferability in pretrained\nlanguage models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages\n1393\u20131401.\nTay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani,\nA., and Metzler, D. (2021). Scale efficiently: Insights from pretraining and finetuning transformers. In\nInternational Conference on Learning Representations.\nTirumala, K., Simig, D., Aghajanyan, A., and Morcos, A. S. (2023). D4: Improving llm pretraining via\ndocument de-duplication and diversification. In Thirty-seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track.\nTran, A. T., Nguyen, C. V., and Hassner, T. (2019). Transferability and hardness of supervised classification\ntasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1395\u20131405.\nVan Asch, V. and Daelemans, W. (2010). Using domain similarity for performance estimation. In Proceedings\nof the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 31\u201336.\nYou, K., Liu, Y., Wang, J., and Long, M. (2021). Logme: Practical assessment of pre-trained models for\ntransfer learning. In International Conference on Machine Learning, pages 12133\u201312143. PMLR.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. (2022). Scaling vision transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113.\nZhang, B., Ghorbani, B., Bapna, A., Cheng, Y., Garcia, X., Shen, J., and Firat, O. (2022). Examining\nscaling and transfer of language model architectures for machine translation. In International Conference\non Machine Learning, pages 26176\u201326192. PMLR.\nZhuocheng, Z., Gu, S., Zhang, M., and Feng, Y. (2023). Scaling law for document neural machine translation.\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8290\u20138303.\nZoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk, E. D., and Le, Q. (2020). Rethinking pre-training\nand self-training. Advances in neural information processing systems, 33:3833\u20133845.\n13\nA\nAdditional Experimental Details\nA.1\nModel Architectures\nWe provide the architecture details of the T5-3B and T5-770M models in Tables 1 and 2. These models were\ninitially introduced by Raffel et al. (2020).\nTable 1: T5-3B Raffel et al. (2020) architecture details.\nEmbedding Dimension\n1024\nNumber of Heads\n32\nNumber of Encoder Layers\n24\nNumber of Decoder Layers\n24\nHead Dimension\n128\nMLP Dimension\n16384\nTable 2: T5-770M Raffel et al. (2020) architecture details.\nEmbedding Dimension\n1024\nNumber of Heads\n16\nNumber of Encoder Layers\n24\nNumber of Decoder Layers\n24\nHead Dimension\n64\nMLP Dimension\n2816\nA.2\nOptimizing the Scaling Law Coefficients\nIn this section, we provide more details on how we optimize the coefficients of the scaling laws. Following\nHoffmann et al. (2022), we use the Huber loss (Huber, 1992) to minimize overfitting to the outliers. Huber\nloss is particularly useful to suppress the effect of the outlier data points in the optimization problem. More\nspecifically, if the data point with value r is predicted by the law as \u02c6r, the loss for that data point would be\n\u2113\u03b4(r, \u02c6r) =\n(\n1\n2(r \u2212 \u02c6r)2\nfor |r \u2212 \u02c6r| \u2264 \u03b4,\n\u03b4 \u00b7 (|r \u2212 \u02c6r| \u2212 1\n2\u03b4)\notherwise.\n(4)\nDue to the numerical range difference between the BLEU score (between 0 and 100) and the downstream\ncross-entropy typically taking much smaller values, we use \u03b4 = 0.1 for the BLEU score law in (1) and \u03b4 = 1e\u22123\nfor the downstream cross-entropy law in (2).\nFor optimization, we use the L-BFGS algorithm (Nocedal, 1980). Specifically, for the BLEU score law in\n(1), we solve\nmin\nE,A,\u03b1,\u03b2\nX\nData point i\n\u2113\u03b4(log fi, log \u02c6f(Dpi)),\n(5)\nwhere Dpi is the pretraining dataset size and fi is the BLEU score for the data point i, and \u02c6f(\u00b7) is the\napproximation for the optimal law f(\u00b7). Similarly, for the downstream cross-entropy loss law in (2), we solve\nmin\nE,A,\u03b1\nX\nData point i\n\u2113\u03b4(log Li, log \u02c6L(Dpi)),\n(6)\nwhere Dpi is the pretraining dataset size and Li is the downstream cross-entropy loss for the data point i,\nand \u02c6L(\u00b7) is the approximation for the optimal law L(\u00b7).\n14\nB\nAdditional Experimental Results\nIn this section, we provide additional experimental results that we had to skip in the main body due to the\npage limit.\nB.1\nResults on T5-770M\nIn Figures 5 and 6, we present results similar to Figures 1 and 2 in Section 5, but for T5-770M instead of\nT5-3B. In general, we observe a similar trend. The proposed scaling laws describe the downstream behavior\nwell when the pretraining and downstream data are aligned. Similar to the results in T5-3B in the main body\nof the paper, in Figure 6-(top, right), we observe a break in the scaling law when the pretraining dataset is\n100% en-MC4 and the task is en-fr translation \u2013 suggesting the same misalignment for this pretraining data\nand task that was also observed in Section 5 on the larger T5-3B model.\nFigure 5: (top) BLEU score vs pretraining dataset size: f(Dp) = (log(A \u00b7 D\u03b1\np))\u03b2. (left) WMT-17\nen-to-de translation task. Pretraining dataset has 50% en-MC4 + 50% de-MC4. Dotted and dashed blue\ncurves correspond to the fitted scaling laws for different finetuning dataset sizes, Df = 6M and Df = 31M\ntokens, respectively. (right) WMT-15 en-to-fr translation task. Pretraining dataset has 50% en-MC4 and\n50% fr-MC4. Dotted and dashed orange curves correspond to the fitted scaling laws for different finetuning\ndataset sizes, Df = 42M and Df = 210M tokens, respectively. (bottom) Cross-entropy (CE) validation\nloss vs pretraining dataset size: L(Dp) = E +\nA\nD\u03b1\np . Same models as the top row. For all the plots, the\nmarkers are the actual experimental results and the black horizontal curves correspond to the non-pretrained\nmodel directly trained on the task dataset. The finetuning dataset size increases in the order of\ndotted-dashed for all the curves including the black horizontal lines.\n15\nFigure 6: (top) BLEU score vs pretraining dataset size: f(Dp) = (log(A \u00b7 D\u03b1\np))\u03b2. (left) WMT-17\nen-to-de translation task. Dotted and dashed red curves correspond to the fitted scaling laws for different\nfinetuning dataset sizes, Df = 6M and Df = 31M tokens, respectively. (right) WMT-15 en-to-fr translation\ntask. Dotted and dashed red curves correspond to the fitted scaling laws for different finetuning dataset\nsizes, Df = 42M and Df = 210M tokens, respectively. (bottom) Cross-entropy (CE) validation\nloss vs pretraining dataset size: L(Dp) = E +\nA\nD\u03b1\np . Same models as the top row. For all the plots, the\nmarkers are the actual experimental results and the black horizontal curves correspond to the non-pretrained\nmodel directly trained on the task dataset. The finetuning dataset size increases in the order of\ndotted-dashed for all the curves including the black horizontal lines.\nB.2\nOptimized Coefficients and Prediction Errors of the Scaling Laws\nIn Tables 3, 4, 5, and 6, we provide the optimized coefficients for the scaling laws plotted in Figures 1 and 2\ntogether with the prediction error.\n16\nTable 3: The coefficients for the BLEU score law f(Dp) = (log(A \u00b7 D\u03b1\np ))\u03b2 for the results in Figure 1-(top).\nFor the BLEU score laws, we use \u03b4 = 0.1 for the Huber Loss. We report log A instead of A since A typically\ntakes very small and very large values.\nPretraining Dataset\nFinetuning Dataset\nFinetuning Dataset Size\nlog A\n\u03b1\n\u03b2\nPrediction Error\n50% en + 50% de-MC4\nWMT-17 en-de\n6M\n\u2212180.75\n9.00\n0.75\n0.034\n50% en + 50% de-MC4\nWMT-17 en-de\n31M\n\u22121.68 \u00d7 103\n84.04\n0.49\n0.050\n50% en + 50% de-MC4\nWMT-17 en-de\n3B\n\u22121.64 \u00d7 108\n9.91 \u00d7 106\n0.19\n0.048\n50% en + 50% fr-MC4\nWMT-15 en-fr\n42M\n\u22121.82 \u00d7 104\n8.98 \u00d7 102\n0.42\n0.061\n50% en + 50% fr-MC4\nWMT-15 en-fr\n210M\n\u22122.33 \u00d7 104\n1.21 \u00d7 103\n0.40\n0.013\n50% en + 50% fr-MC4\nWMT-15 en-fr\n21B\n5.08 \u00d7 103\n4.61 \u00d7 108\n0.16\n0.005\n50% en + 50% ro-MC4\nWMT-16 en-ro\n625K\n\u221236.02\n1.77\n1.28\n0.042\n50% en + 50% ro-MC4\nWMT-16 en-ro\n3M\n\u22120.115.03\n5.69\n0.89\n0.015\n50% en + 50% ro-MC4\nWMT-16 en-ro\n312M\n\u22121.82 \u00d7 104\n9.04 \u00d7 102\n0.40\n0.015\nTable 4: The coefficients for the downstream cross-entropy law L(Dp) = E +\nA\nD\u03b1\np for the results in Figure 1-\n(bottom). For the downstream cross-entropy laws, we use \u03b4 = 10\u22125 for the Huber Loss.\nPretraining Dataset\nFinetuning Dataset\nFinetuning Dataset Size\nE\nA\n\u03b1\nPrediction Error\n50% en + 50% de-MC4\nWMT-17 en-de\n6M\n3.21 \u00d7 10\u22125\n35.45\n0.64\n1.36 \u00d7 10\u221212\n50% en + 50% de-MC4\nWMT-17 en-de\n31M\n3.28 \u00d7 10\u22125\n4.70 \u00d7 102\n0.78\n3.17 \u00d7 10\u221212\n50% en + 50% de-MC4\nWMT-17 en-de\n3B\n2.24 \u00d7 10\u22125\n2.56 \u00d7 10\u22122\n0.36\n5.76 \u00d7 10\u221214\n50% en + 50% fr-MC4\nWMT-15 en-fr\n42M\n2.72 \u00d7 10\u22125\n2.01 \u00d7 106\n1.18\n7.52 \u00d7 10\u221213\n50% en + 50% fr-MC4\nWMT-15 en-fr\n210M\n2.57 \u00d7 10\u22125\n1.75 \u00d7 107\n1.30\n2.24 \u00d7 10\u221213\n50% en + 50% fr-MC4\nWMT-15 en-fr\n21B\n1.11 \u00d7 10\u22127\n3.41 \u00d7 10\u22125\n1.82 \u00d7 10\u22122\n5.20 \u00d7 10\u221214\n50% en + 50% ro-MC4\nWMT-16 en-ro\n625K\n2.45 \u00d7 10\u22125\n0.49\n0.41\n3.61 \u00d7 10\u221212\n50% en + 50% ro-MC4\nWMT-16 en-ro\n3M\n2.62 \u00d7 10\u22125\n2.40\n0.49\n2.19 \u00d7 10\u221212\n50% en + 50% ro-MC4\nWMT-16 en-ro\n312M\n2.08 \u00d7 10\u22125\n3.94\n0.53\n5.95 \u00d7 10\u221212\nTable 5: The coefficients for the BLEU score law f(Dp) = (log(A \u00b7 D\u03b1\np ))\u03b2 for the results in Figure 2-(top).\nFor the BLEU score laws, we use \u03b4 = 0.1 for the Huber Loss. We report log A instead of A since A typically\ntakes very small and very large values.\nPretraining Dataset\nFinetuning Dataset\nFinetuning Dataset Size\nlog A\n\u03b1\n\u03b2\nPrediction Error\n100% en-MC4\nWMT-17 en-de\n6M\n\u22121.88\n0.15\n3.30\n0.014\n100% en-MC4\nWMT-17 en-de\n31M\n\u22121.81 \u00d7 104\n896.12\n0.28\n0.006\n100% en-MC4\nWMT-17 en-de\n3B\n1.02 \u00d7 10\u22127\n104.92\n0.42\n0.015\n100% en-MC4\nWMT-15 en-fr\n42M\n1.00\n2.57 \u00d7 10\u22125\n1.11 \u00d7 104\n0.042\n100% en-MC4\nWMT-15 en-fr\n210M\n\u22126.38 \u00d7 107\n3.43 \u00d7 106\n0.20\n0.034\n100% en-MC4\nWMT-15 en-fr\n21B\n204.81\n3.80 \u00d7 1014\n9.97 \u00d7 10\u22123\n0.004\n100% en-MC4\nWMT-16 en-ro\n625K\n\u221210.54\n0.55\n1.12\n0.008\n100% en-MC4\nWMT-16 en-ro\n3M\n\u221240.41\n2.11\n0.79\n0.025\n100% en-MC4\nWMT-16 en-ro\n312M\n3.61\n8.17 \u00d7 105\n0.19\n0.018\nTable 6: The coefficients for the downstream cross-entropy law L(Dp) = E +\nA\nD\u03b1\np for the results in Figure 2-\n(bottom). For the downstream cross-entropy laws, we use \u03b4 = 10\u22125 for the Huber Loss.\nPretraining Dataset\nFinetuning Dataset\nFinetuning Dataset Size\nE\nA\n\u03b1\nPrediction Error\n100% en-MC4\nWMT-17 en-de\n6M\n3.22 \u00d7 10\u221213\n3.18 \u00d7 10\u22123\n0.15\n5.79 \u00d7 10\u221212\n100% en-MC4\nWMT-17 en-de\n31M\n3.24 \u00d7 10\u22125\n5.20 \u00d7 10\u22123\n0.20\n9.25 \u00d7 10\u221213\n100% en-MC4\nWMT-17 en-de\n3B\n2.24 \u00d7 10\u22125\n2.56 \u00d7 10\u22122\n0.36\n5.76 \u00d7 10\u221214\n100% en-MC4\nWMT-15 en-fr\n42M\n3.49 \u00d7 10\u22125\n1.05 \u00d7 10\u22122\n0.25\n3.63 \u00d7 10\u221213\n100% en-MC4\nWMT-15 en-fr\n210M\n4.24 \u00d7 10\u22125\n19.39\n0.66\n5.40 \u00d7 10\u221213\n100% en-MC4\nWMT-15 en-fr\n21B\n1.26 \u00d7 10\u22127\n2.59 \u00d7 10\u22125\n4.81 \u00d7 10\u22123\n3.63 \u00d7 10\u221214\n100% en-MC4\nWMT-16 en-ro\n625K\n5.79 \u00d7 10\u221212\n1.03 \u00d7 10\u22123\n7.76 \u00d7 10\u22122\n5.56 \u00d7 10\u221212\n100% en-MC4\nWMT-16 en-ro\n3M\n1.78 \u00d7 10\u221212\n9.98 \u00d7 10\u22124\n8.33 \u00d7 10\u22122\n8.23 \u00d7 10\u221212\n100% en-MC4\nWMT-16 en-ro\n312M\n5.85 \u00d7 10\u22125\n1.37 \u00d7 103\n0.88\n3.05 \u00d7 10\u221213\n17\n"
  },
  {
    "title": "Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models",
    "link": "https://arxiv.org/pdf/2402.03749.pdf",
    "upvote": "9",
    "text": "Vision Superalignment: Weak-to-Strong Generalization\nfor Vision Foundation Models\nJianyuan Guo * 1 2 Hanting Chen * 1 Chengcheng Wang 1 Kai Han 1 Chang Xu 2 Yunhe Wang 1\njianyuan guo@outlook.com; {chenhanting,wangchengcheng11,kai.han}@huawei.com; c.xu@sydney.edu.cn;\nAbstract\nRecent advancements in large language models\nhave sparked interest in their extraordinary and\nnear-superhuman capabilities, leading researchers\nto explore methods for evaluating and optimiz-\ning these abilities, which is called superalignment.\nIn this context, our paper delves into the realm\nof vision foundation models, focusing on the\nconcept of weak-to-strong generalization, which\ninvolves using a weaker model to supervise a\nstronger one, aiming to enhance the latter\u2019s capa-\nbilities beyond the former\u2019s limits. We introduce\na novel and adaptively adjustable loss function\nfor weak-to-strong supervision. Our comprehen-\nsive experiments span various scenarios, includ-\ning few-shot learning, transfer learning, noisy la-\nbel learning, and common knowledge distillation\nsettings. The results are striking: our approach\nnot only exceeds the performance benchmarks set\nby strong-to-strong generalization but also sur-\npasses the outcomes of fine-tuning strong models\nwith whole datasets. This compelling evidence\nunderscores the significant potential of weak-to-\nstrong generalization, showcasing its capability\nto substantially elevate the performance of vi-\nsion foundation models. The code is available\nat https://github.com/ggjy/vision weak to strong.\n1. Introduction\nThe evolution and maturation of artificial intelligence are\nprofoundly reliant on human evaluation, guidance, and ex-\nperience. In the realm of computer vision, convolutional\nnetworks acquire semantic knowledge of images through\nextensive labeling provided by experts, such as object bound-\n*Equal contribution 1Huawei Noah\u2019s Ark Lab, Beijing, China\n2School of Computer Science, Faculty of Engineering, University\nof Sydney, Sydney, Australia. Correspondence to: Yunhe Wang\n<yunhe.wang@huawei.com>.\nPreprint.\n2024/2/5 12:33\nAwesome-pyecharts\nfile:///C:/Users/g00592856/Downloads/render (3).html\n1/1\nCLS-CIFAR-S\nCLS-CIFAR-D\nCLS-ImageNet-S\nCLS-ImageNet-D\nTL-ImageNet\nTL-iNat\nFSL-miniImageNe\nLNL-CIFAR\n77.03\n78.51\n73.68\n76.52\n83.62\n75.6\n70.36\n91.2\n77.33\n79.31\n73.8\n76.64\n83.69\n75.89\n70.46\n91.04\n77.58\n79.63\n74.16\n76.94\n83.86\n76.03\n70.82\n91.45\nKD\nFitNet\nRKD\nDKD\nTrain from scratch\nAugConf\nAdaptConf (Ours)\nFigure 1. Our proposed AdaptConf achieves the best performance\non a broad range of tasks compared with other knowledge distilla-\ntion based methods. The corresponding values are calculated by av-\neraging results on eack task. CLS-CIFAR-S: Table 1, CLS-CIFAR-\nD: Table 2a, CLS-ImageNet-S: Table 4, CLS-ImageNet-D: Table 4,\nTL-ImageNet: Table 3a, TL-iNat: Table 3b, FSL-miniImageNet:\nTable 5, LNL-CIFAR: Table 7.\naries in the COCO dataset (Lin et al., 2014) or image cate-\ngories in ImageNet (Deng et al., 2009). Similarly, in fields\nlike robotics, reinforcement learning (Kaelbling et al., 1996)\noften depends on human-defined reward functions to steer\nmachines towards optimal performance. In the domain of\nNatural Language Processing (NLP), recurrent neural net-\nworks (Hochreiter & Schmidhuber, 1997) and Transform-\ners (Vaswani et al., 2017) are capable of learning the distri-\nbution of language from vast amounts of unsupervised text\ngenerated by humans. This synergy suggests that AI models\nare essentially advancing while standing on the shoulders\nof human intelligence, leveraging the depth and breadth\nof human expertise to reach new heights of capability and\nunderstanding.\nThe landscape of deep learning has undergone a transforma-\ntive shift, with neural networks increasingly demonstrating\ncapabilities that surpass human performance across various\ndomains. For instance, AlphaGO (Silver et al., 2016) show-\n1\narXiv:2402.03749v1  [cs.CV]  6 Feb 2024\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\ncased an ability in the game of Go that far exceeded the\nprowess of the strongest human players. Similarly, GPT-like\nmodels (Brown et al., 2020) are capable of achieving results\nbeyond the average human level in a variety of exams. No-\ntably, this phenomenon emerged even earlier in the field of\ncomputer vision. As early as 2015, meticulously designed\nconvolutional neural networks (He et al., 2015) were already\nachieving results on large-scale visual classification tasks\nlike ImageNet that surpassed human performance. This\ntrend of superhuman achievements has driven the research\ncommunity to focus on how to control, evaluate, and opti-\nmize these exceptionally capable models, recognizing the\nimmense potential they hold for advancing our understand-\ning and application of artificial intelligence.\nTo address the intricate challenge of leveraging human\nexpertise in supervising superhuman AI models, the con-\ncept of \u201csuperalignment\u201d has been introduced. This ap-\nproach aims to align superhuman models in a way that\nmaximizes their learning from human input. A seminal\nwork in this area is the concept of Weak-to-Strong General-\nization (WSG) (Burns et al., 2023). This research presents\nan intriguing analogy to explore the feasibility of using\nweaker models to supervise stronger ones. The results from\nthis line of inquiry are stimulating: strong models, already\nendowed with robust generalization and representational\ncapabilities, can achieve performances that surpass their\nweaker counterparts through simple supervision provided\nby these weaker models. This supervision often involves\nincomplete or flawed labels, yet the stronger models can\neffectively transcend these limitations. Such findings have\nnot only affirmed the viability of Weak-to-Strong General-\nization but have also demonstrated its efficacy in fields like\nnatural language processing and reinforcement learning.\nIn this paper, we delve into the topic of \u201cvision superalign-\nment,\u201d specifically investigating the applicability of Weak-\nto-Strong Generalization (WSG) within the context of vision\nfoundation models. Our study meticulously designs and ex-\namines multiple scenarios in computer vision, including\nfew-shot learning, transfer learning, noisy label learning,\nand traditional knowledge distillation settings. In these sce-\nnarios, stronger models are trained to learn from weaker\nmodels. Through detailed validation and comparative ex-\nperiments, we demonstrate the feasibility of WSG in the\nvisual domain. Furthermore, we introduce an improved and\nadaptive confidence scheme to enhance the efficacy of WSG.\nOur study not only validates the concept of WSG in vision\nbut also contributes significantly to the broader pursuit of\nsuperalignment across various AI modalities. Our work\nrepresents a substantial step forward in understanding and\noptimizing the interaction between human-level expertise\nand superhuman AI capabilities, potentially paving the way\nfor groundbreaking advancements in artificial intelligence.\n2. Related Works\nThe pursuit of enhancing the performance of deep neural\nnetworks in computer vision has led to the development of\nthe teacher-student learning paradigm (Hinton et al., 2015).\nThis approach typically involves a stronger model (teacher)\nimproving the performance of a weaker model (student),\nwith extensive research focusing on optimizing the capa-\nbilities of the weaker model. Various strategies have been\nproposed to achieve this. For instance, (Romero et al., 2014)\nsuggests that in addition to the output logits, incorporating\nintermediate layer features for supervision can significantly\nboost the student\u2019s learning. (Park et al., 2019) posits that\nthe relationships between samples can serve as valuable\nsupervisory information.\nIn a further refinement of this approach, (Zhao et al., 2022)\nredefines classical knowledge distillation (KD) loss, seg-\nmenting it into target-class and non-target-class distillation\nto balance the transfer of these two types of information\nmore effectively.\n(Heo et al., 2019) delves into the de-\ntails and components of feature distillation, arriving at an\nimproved method for the transfer of feature knowledge.\nMeanwhile, (Chen et al., 2021a) explores cross-stage fea-\nture transfer as an alternative to the conventional same-stage\nfeature transfer. These methods have proven effective for\nstrong-to-weak generalization scenarios.\nHowever, with the gradual increase in the size and complex-\nity of vision foundation models, the focus has shifted to-\nwards weak-to-strong generalization, i.e., how a weak model\ncan improve a strong model. In this context, (Furlanello\net al., 2018) investigates knowledge distillation between\nteachers and students of equal size, demonstrating the feasi-\nbility of distilling models of the same size. Building upon\nthis, (Xie et al., 2020) introduces the use of additional un-\nlabeled data for knowledge distillation among models of\nequal size, further validating the effectiveness of strong-to-\nstrong generalization, especially in scenarios with abundant\ndata availability. This body of research sets the stage for\nour exploration into weak-to-strong generalization, a rela-\ntively uncharted yet promising domain in the field of vision\nfoundation models.\n3. Vision Superalignment\nIn order to investigate how to supervise and optimize su-\nperhuman computer vision models, our focus centers on\nthe study of weak-to-strong generalization for vision foun-\ndation models. In this section, we initially delve into ex-\namining and defining what constitutes vision foundation\nmodels. These models, characterized by their extensive ca-\npacity and versatility, form the backbone of our research.\nSubsequently, we address a critical challenge inherent in\nthe weak-to-strong generalization approach: the inaccuracy\n2\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nof output labels from weak models. In response to this,\nwe introduce an innovative solution \u2013 adaptive confidence\ndistillation. This method is designed to enhance the learn-\ning process of strong models by effectively utilizing the\nguidance provided by weak models. Adaptive confidence\ndistillation operates on the principle that even imperfect or\npartially accurate guidance from a weak model can be a\nvaluable learning tool for a stronger model, provided that\nthe information is processed and adapted correctly.\n3.1. Vision Foundation Models\nIn our exploration of weak-to-strong generalization for vi-\nsion foundation models, it is crucial to first define what con-\nstitutes these models. There are several potential categories\nof candidates that represent vision foundation models, each\ncharacterized by their unique capabilities and approaches in\nthe realm of computer vision.\nText-Visual Fusion Models: The first category includes\nmodels that integrate visual and linguistic tasks. A notable\nexample is the work of Radford et al. (Radford et al., 2021),\nwhich constructs a foundational model by aligning com-\nputer vision tasks with language tasks through image-text\npre-training pairs. This approach bridges the gap between\ntextual and visual information, providing a comprehensive\nunderstanding of both domains.\nImage Generation Models: The second category focuses\non models that are capable of generating images, which can\nbe considered as a form of modeling the image space. Rom-\nbach et al.(Rombach et al., 2022) demonstrate this through\ntheir ability to generate a plethora of images from textual de-\nscriptions, establishing a basis for image generation models.\nSimilarly, Chen et al.(Chen et al., 2020) employ a GPT-like\npre-training method for images, resulting in a generative\nTransformer model with significant generalization capabili-\nties in image creation.\nArchitecture for General or Zero-Shot Visual Tasks: The\nthird category seeks to develop architectures capable of\nsolving a range of visual tasks, either generally or in a zero-\nshot manner. Bai et al.(Bai et al., 2023) approach this by\nmodeling a series of image tasks as sequential challenges,\ncreating a large vision model that addresses a spectrum of\nvisual problems. Additionally, Kirillov et al.(Kirillov et al.,\n2023) propose the \u201cSegment Anything\u201d model, achieving\nimpressive zero-shot segmentation results.\nIn our quest to identify the most suitable candidates for\nvision foundation models to be used in weak-to-strong gen-\neration tasks, we propose a definition focused on versatility\nand effectiveness in the visual domain. We posit that vision\nfoundation models should be applicable to a broad range of\nvisual tasks while delivering high-quality performance.\nBased on this criterion, we suggest that backbones pre-\ntrained on ImageNet represent strong contenders as vision\nfoundation models. The rationale for this choice is twofold.\nFirstly, these backbones have proven to be highly adapt-\nable and effective for key tasks in computer vision, such as\nclassification, detection, and segmentation. By fine-tuning\nthese backbones, state-of-the-art (SOTA) accuracies can be\nachieved in these tasks, demonstrating their robustness and\nversatility. Secondly, there is an extensive array of pretrain-\ning algorithms developed specifically for these models (He\net al., 2022a), which further qualifies them as universal pre-\ntraining models for vision tasks. Additionally, these types\nof models are often used as one of the branches in vision-\nlanguage multimodal models (Du et al., 2022), highlighting\ntheir applicability in cross-modal tasks.\nTherefore, for our experimental analysis, we choose to fo-\ncus on these backbone models as representatives of vision\nfoundation models. We aim to conduct our weak-to-strong\ngeneralization analysis using the fundamental task of im-\nage classification as a baseline. This approach allows us to\nthoroughly assess the capabilities and potential of weak-to-\nstrong generation in a controlled yet comprehensive manner,\noffering insights that could be extrapolated to other, more\ncomplex vision tasks.\n3.2. Adaptive Confidence Distillation\nIn this subsection, we explore the methodology for imple-\nmenting weak-to-strong generalization in vision foundation\nmodels. The central question we address is how a weak vi-\nsion foundation model can supervise a stronger counterpart\neffectively.\n(Burns et al., 2023) proposes an augmented\nconfidence loss approach, which is formulated as:\nLconf(f) = (1 \u2212 \u03b1)CE(f(x), fw(x)) + \u03b1CE(f(x), \u02c6f(x)),\n(1)\nwhere f represent the strong model that needs to be opti-\nmized, and fw denote the weak model, \u02c6f(x) refers to the\nhard label predicted by the strong model for an input image\nx. The loss function incorporates the cross-entropy loss\n(CE) and is balanced by a hyperparameter \u03b1. In this for-\nmulation, the first term of the loss function resembles the\ntraditional knowledge distillation loss, signifying the learn-\ning process of the strong model from the weak model. Given\nthat the labels provided by the weak model may not always\nbe accurate, the second term of the loss function encour-\nages the strong model to leverage its superior generalization\nabilities and prior knowledge to refine its predictions.\nThe strength of this approach lies in its ability to balance\ndirect learning from the weak model with the strong model\u2019s\nintrinsic capacity for understanding and interpreting the vi-\nsual data. This method paves the way for the strong model\nto surpass the limitations of the weak model, utilizing the\nlatter\u2019s guidance while simultaneously enhancing its predic-\ntions through its advanced capabilities.\n3\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nTeacher\nResNet20\nResNet32\nResNet8\u00d74\nWRN-16-2\nWRN-40-1\nVGG8\n68.93\n71.72\n72.41\n72.71\n72.30\n71.99\nStudent\nResNet56\nResNet110\nResNet32\u00d74\nWRN-40-2\nWRN-40-2\nVGG13\n72.94\n74.80\n79.90\n77.20\n77.20\n75.26\nKD (Hinton et al., 2015)\n73.81\n76.45\n79.32\n78.25\n77.97\n76.41\nFitNet (Romero et al., 2014)\n70.51\n73.15\n77.65\n76.71\n76.12\n76.39\nRKD (Park et al., 2019)\n72.98\n75.62\n80.10\n77.27\n77.76\n76.20\nReviewKD (Chen et al., 2021a)\n70.15\n72.30\n77.22\n75.86\n75.78\n74.22\nDKD (Zhao et al., 2022)\n73.90\n76.57\n79.52\n78.18\n77.95\n76.62\nAugConf (Burns et al., 2023)\n73.86\n76.72\n80.34\n78.34\n78.15\n76.55\nAdaptConf (Ours)\n74.17\n76.86\n80.64\n78.58\n78.40\n76.84\n\u2206\n+1.23\n+2.06\n+0.74\n+1.38\n+1.20\n+1.58\nTable 1. Results on the CIFAR-100 validation set. Teachers and students are in the same architectures. And \u2206 represents the\nperformance improvement over the student model trained from scratch. All results are the average over 3 trials.\nAddressing the limitations inherent in the supervision pro-\nvided by weak models and the inaccuracies of strong models\u2019\nself-generated hard labels, a more sophisticated approach is\nrequired beyond a simple weighted combination of these la-\nbels. Given the challenge in directly discerning the accuracy\nof each label, leveraging confidence as a metric for selecting\nthe most probable correct label emerges as a viable solution.\nWe propose to use the discrepancy between the soft label and\nthe hard label as an indicator of the model\u2019s confidence. The\nunderlying rationale is that when a model\u2019s soft label closely\naligns with its hard label, it suggests a higher confidence in\nits own judgment. To capitalize on this insight, we introduce\nan adaptive confidence loss that dynamically adjusts based\non the model\u2019s confidence level. The specific formulation\nof this loss is as follows:\nLAC(f) = (1 \u2212 \u03b2(x))CE(f(x), fw(x)) + \u03b2(x)CE(f(x), \u02c6f(x)),\n\u03b2(x) =\nexp(CE(f(x), \u02c6f(x)))\nexp(CE(f(x), \u02c6f(x))) + exp(CE(f(x), \u02c6\nfw(x)))\n.\n(2)\nIn this formula, \u03b2(x) is a function of the input image x\nthat calculates the confidence weight and \u02c6\nfw(x) is the hard\nlabel of x in the weak model. This weight determines the\nbalance between learning from the weak model and relying\non the strong model\u2019s own predictions. The cross-entropy\nloss (CE) is used for both components, with the first term\nfocusing on learning from the weak model and the second\nterm emphasizing the strong model\u2019s self-supervision.\nThis adaptive confidence loss enables a more nuanced ap-\nproach to weak-to-strong generalization. By adjusting the\nweight based on confidence levels, it allows the strong model\nto discern when to prioritize its own predictions over the\nguidance of the weak model and vice versa. This adaptabil-\nity is key to overcoming the inaccuracies and limitations\nof both models, leading to more effective learning and en-\nhanced performance in vision foundation models.\n4. Experiment\nIn this section, we report our main empirical results on\nvarious tasks, including baselines and promising methods.\nAll implementation details are attached in supplementary\nmaterials.\n4.1. Tasks\nImage Classification. Our experiments are primarily fo-\ncused on two benchmark datasets. CIFAR-100 (Krizhevsky\net al., 2009) is a widely recognized dataset for image\nclassification, comprising 32\u00d732 pixel images across 100\ncategories, with training and validation sets containing\n50,000 and 10,000 images, respectively. Conversely, Ima-\ngeNet (Deng et al., 2009) is a large-scale dataset for classifi-\ncation tasks, encompassing 1.28 million training images and\n50,000 validation images across 1,000 classes. Additionally,\nwe explore scenarios where only soft labels generated by a\nweak teacher model are available for training.\nFew-shot learning. We explore few-shot learning across the\nminiImageNet (Vinyals et al., 2016) dataset which contains\n100 classes sampled from ILSVRC-2012 (Russakovsky\net al., 2015). We randomly split the dataset into 64, 16,\nand 20 classes as training, validation, and testing sets, re-\nspectively. And ensure that each class has 600 images of\n84\u00d784 image size. We utilize the ResNet36 to explore the\nweak-to-strong generalization performance in few-shot task.\nTo demonstrate weak-to-strong generalization performance,\nwe follow Meta-Baseline and conduct related experiments\non classifier stage and meta stage.\nTransfer learning. We explore transfer learning across two\nbenchmark datasets: ImageNet (Deng et al., 2009), and iNat-\nuralist 2018 (Van Horn et al., 2018), the latter comprising\n437,513 training images and 24,426 test images distributed\nacross 8,142 species. We utilize the ViT-B (Dosovitskiy\net al., 2020) that has been pretrained on the ImageNet train-\ning set using the self-supervised MAE (He et al., 2022b)\n4\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nTeacher\nShuffleNet-V1\nShuffleNet-V1\nMobileNet-V2\nMobileNet-V2\nShuffleNet-V2\n72.40\n72.40\n66.85\n66.85\n74.44\nStudent\nResNet32\u00d74\nWRN-40-2\nVGG13\nResNet50\nResNet32\u00d74\n79.90\n77.20\n75.26\n80.43\n79.90\nKD (Hinton et al., 2015)\n80.19\n78.02\n75.39\n78.64\n80.31\nFitNet (Romero et al., 2014)\n77.61\n75.15\n72.36\n75.92\n78.05\nRKD (Park et al., 2019)\n80.30\n77.23\n76.21\n79.89\n80.39\nReviewKD (Chen et al., 2021a)\n78.43\n75.98\n73.69\n77.05\n77.84\nDKD (Zhao et al., 2022)\n80.55\n78.10\n75.81\n79.65\n80.67\nAugConf (Burns et al., 2023)\n80.62\n77.92\n76.43\n80.75\n80.84\nAdaptConf (Ours)\n80.99\n78.55\n76.58\n80.98\n81.06\n\u2206\n+1.09\n+1.35\n+1.32\n+0.55\n+1.16\n(a) Trained with teacher\u2019s prediction and ground truth label. \u2206 represents the improvement over the student trained from scratch.\nTeacher\nShuffleNet-V1\nShuffleNet-V1\nMobileNet-V2\nMobileNet-V2\nShuffleNet-V2\n72.40\n72.40\n66.85\n66.85\n74.44\nStudent\nResNet32\u00d74\nWRN-40-2\nVGG13\nResNet50\nResNet32\u00d74\nKD (Hinton et al., 2015)\n77.92\n76.45\n72.13\n73.32\n78.27\nFitNet (Romero et al., 2014)\n75.74\n74.03\n70.57\n71.45\n76.42\nRKD (Park et al., 2019)\n76.59\n75.70\n70.28\n72.06\n77.84\nAugConf (Burns et al., 2023)\n78.25\n76.37\n72.51\n74.48\n78.81\nAdaptConf (Ours)\n78.48\n76.66\n72.93\n74.67\n79.04\n\u2206\n+6.08\n+4.26\n+6.08\n+7.82\n+4.37\n(b) Trained with teacher\u2019s prediction only. \u2206 represents the performance improvement over the teacher model.\nTable 2. Results on the CIFAR-100 validation set. Teachers and students are in the different architectures. All results are the average\nover 3 trials.\napproach, leveraging only image data without labels. Our\nresults are reported for the fine-tuning phase, which is con-\nducted under the guidance of a weak teacher model on each\nbenchmark. Furthermore, we investigate scenarios in which\nonly soft labels produced by the weak teacher model are\nused for training.\nLearning with noisy labels. We evaluate our approach\nusing two datasets with simulated label noise, specifi-\ncally CIFAR-10 (Krizhevsky et al., 2009) and CIFAR-\n100 (Krizhevsky et al., 2009). Consistent with prior re-\nsearch (Li et al., 2020; Tanaka et al., 2018), we introduce\ntwo distinct types of simulated noisy labels: symmetric and\nasymmetric. Symmetric noise is introduced by randomly\nsubstituting the labels of a certain proportion of the train-\ning data with other possible labels uniformly. In contrast,\nasymmetric noise involves systematic mislabeling to mimic\nreal-world errors, such as flipping the labels to closely re-\nlated classes. For example, in CIFAR-10, truck is mislabeled\nas automobile, bird as airplane, and cat is interchanged with\ndog. For CIFAR-100, similar mislabeling is applied within\neach of the super-classes in a circular fashion.\nBaseline methods. The predominant framework for imple-\nmenting teacher-student training paradigms is knowledge\ndistillation (Hinton et al., 2015). This approach outlines\na method where a larger, more complex teacher network\nguides the training of a more compact student network.\nNonetheless, inspired by the findings of Burns et al. (Burns\net al., 2023), our work pivots towards a scenario where the\nstudent network surpasses the teacher in visual capabilities.\nDespite this inversion of roles, there remains valuable dark\nknowledge in the teacher network that can be transferred\nto the student, either through logits or via intermediate rep-\nresentational features. To benchmark our experiments, we\nemploy a range of established (Hinton et al., 2015; Romero\net al., 2014; Park et al., 2019; Heo et al., 2019; Chen et al.,\n2021a; Hao et al., 2023a) and recently proposed (Zhao et al.,\n2022; Burns et al., 2023) distillation techniques as baseline\nmethods.\n4.2. Main Results\n4.2.1. IMAGE CLASSIFICATION.\nCIFAR-100 image classification. We commence our in-\nvestigation with an exploration of weak-to-strong general-\nization (WSG) on the CIFAR-100 dataset. The outcomes\nof this investigation are delineated in Tables 1 and\n2.\nSpecifically, Table 1 presents the scenarios in which both\nteacher and student models share the same network architec-\ntures. We examine a range of prevalent vision architectures\nsuch as ResNet (He et al., 2016), WRN (Zagoruyko & Ko-\nmodakis, 2016), and VGG (Simonyan & Zisserman, 2014).\n5\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nTeacher: ResNet50 (80.36)\nTeacher + GT\nTeacher\nStudent: ViT-B (MAE pretrain)\n83.53\n-\nKD (Hinton et al., 2015)\n83.62\n82.32\nFitNet (Romero et al., 2014)\n82.48\n81.02\nRKD (Park et al., 2019)\n82.19\n80.98\nDKD (Zhao et al., 2022)\n83.68\n-\nAugConf (Burns et al., 2023)\n83.70\n82.38\nAdaptConf (Ours)\n83.86\n82.51\n\u2206\n+0.33\n+2.15\n(a) Top-1 results on the ImageNet validation set.\nTeacher: ResNet101 (67.42)\nTeacher + GT\nTeacher\nStudent: ViT-B (MAE pretrain)\n75.28\n-\nKD (Hinton et al., 2015)\n75.60\n71.57\nFitNet (Romero et al., 2014)\n73.68\n70.11\nDKD (Zhao et al., 2022)\n75.82\n-\nAugConf (Burns et al., 2023)\n75.90\n71.73\nAdaptConf (Ours)\n76.03\n71.99\n\u2206\n+0.75\n+4.57\n(b) Top-1 results on the iNaturalist 2019 test set.\nTable 3. Transfer learning results. The student model is a ViT-\nB (Dosovitskiy et al., 2020) pretrained by the self-supervised MAE\nframework (He et al., 2022b). \u2206 denotes the performance improve-\nment over student/teacher in second/third columns.\nTeacher\nResNet18\nMobileNet-V1\n69.75\n71.57\nStudent\nResNet34\nResNet50\n73.47\n76.22\nKD (Hinton et al., 2015)\n73.68\n76.52\nFitNet (Romero et al., 2014)\n70.93\n73.61\nRKD (Park et al., 2019)\n73.65\n76.45\nReviewKD (Chen et al., 2021a)\n72.99\n75.28\nDKD (Zhao et al., 2022)\n73.74\n76.72\nAugConf (Burns et al., 2023)\n73.80\n76.64\nAdaptConf (Ours)\n74.16\n76.94\n\u2206\n+0.69\n+0.72\nTable 4. Top-1 results on the ImageNet validation set. \u2206 repre-\nsents the performance improvement over the student model trained\nfrom scratch.\nWe employ various KD methods to assess the potential of\nlarger-capacity students guided by limited-capacity teachers.\nRemarkably, in nearly all cases employing KD-based ap-\nproaches, the student models outperform those trained from\nscratch. Furthermore, both AugConf (Burns et al., 2023)\nand our proposed AdaptConf method surpasses all previous\ndistillation techniques across all teacher-student pairs. This\nhighlights that simply emulating a weak teacher does not\nyield the most favorable outcomes. Notably, AdaptConf\nconsistently achieves superior performance compared to\nAugConf (Burns et al., 2023), underscoring the advantage\nof our dynamic adaptive confidence weighting. This ap-\nproach provides a more refined mechanism for facilitating\nweak-to-strong knowledge transfer.\nTable 2 presents the results for teacher-student pairs from\ndifferent series, such as ShuffleNet (Zhang et al., 2018) and\nMobileNet (Sandler et al., 2018). Additionally, take the\nMobileNetV2-ResNet50 pair as an example, the experimen-\ntal results reveal that when the teacher model is significantly\nweaker, i.e.a substantial performance gap exists between\nthe weak teacher model and the strong student model, none\nof the KD-based methods were able to effectively enhance\nthe strong student\u2019s performance, except for AugConf and\nAdaptConf. The possible reason is that these methods in-\nclude the predictions of the strong student in the loss func-\ntion. This proves that self-training methods, akin to those\ndescribed in (Lee et al., 2013), can mitigate the bias from a\nsuboptimal teacher model. It is important to note that Fit-\nNet (Romero et al., 2014) consistently underperforms when\ncompared to training from scratch. This could be attributed\nto its sole focus on intermediate features, which may be\nmore misleading for the strong student to learn from than\nsoft predictions, as suggested by (Hao et al., 2023b). Over-\nall, our AdaptConf achieves an improvement of 0.5%-2%\non all evaluated teacher-student pairings, whether they are\nfrom the same or different series.\nFurthermore, we investigate a scenario where only the\nteacher\u2019s output is available, as shown in Table 2b. In this\ncontext, it becomes evident that AugConf and AdaptConf\nyields more significant improvements compared to other\nKD-based methods when ground truth is absent. This ob-\nservation underscores the suitability of our confidence dis-\ntillation approach for more extreme WSG scenarios where\nground truth is not available.\nImageNet image classification. Table 4 presents the top-1\naccuracy for image classification on the ImageNet dataset.\nOur AdaptConf method achieves significant improvements\nacross both WSG scenarios, whether employing the same\nor different architectures.\nTeacher\nResNet12\nResNet18\n59.65\n77.80\n60.83\n78.96\nStudent\nResNet36\nResNet36\n60.91\n79.01\n60.91\n79.01\n1-shot\n5-shot\n1-shot\n5-shot\nKD\n60.94\n79.14\n61.57\n79.79\nRKD (Park et al., 2019)\n59.74\n78.30\n60.80\n78.82\nAugConf (Burns et al., 2023)\n61.38\n79.33\n61.66\n79.46\nAdaptConf (Ours)\n61.50\n79.52\n62.29\n79.96\n\u2206\n+2.59\n+2.67\n+3.38\n+3.11\nTable 5. Average 5-way accuracy (%) with 95% confidence in-\nterval on the miniImageNet validation set in Classification\nTraining stage. \u2206 represents the performance improvement over\nthe student model trained from scratch. All results are the average\nover 3 trials.\n6\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nTeacher\nResNet12 (Class-stage)\nResNet18 (Class-stage)\nResNet12 (Meta-stage)\nResNet18 (Meta-stage)\n59.20\n60.63\n65.26\n66.51\nStudent\nResNet36\nResNet36\nResNet36\nResNet36\n65.08\n65.08\n65.08\n65.08\nKD (Hinton et al., 2015)\n63.43\n65.04\n66.08\n65.93\nRKD (Park et al., 2019)\n64.79\n65.42\n65.96\n65.46\nAugConf (Burns et al., 2023)\n65.15\n65.59\n65.9\n65.78\nAdaptConf (Ours)\n65.38\n65.74\n66.08\n65.95\n\u2206\n+0.30\n+0.66\n+1.00\n+0.87\nTable 6. Average 5-way accuracy on the miniImageNet validation set at Meta-Learning stage. \u2206 represents the performance\nimprovement over the student model trained from scratch. All results are the average over 3 trials.\ndataset\nCIFAR-10\nCIFAR-100\nnoise type\nasymmetric\nsymmetric\nasymmetric\nsymmetric\nTeacher\nPR18\nPR18\nPR18\nPR18\n92.98\n99.56\n95.80\n99.80\n73.20\n92.67\n76.16\n92.90\nStudent\nPR34\nPR34\nPR34\nPR34\n93.69\n99.61\n96.13\n99.77\n74.80\n92.94\n78.20\n93.77\nTop-1\nTop-5\nTop-1\nTop-5\nTop-1\nTop-5\nTop-1\nTop-5\nKD (Hinton et al., 2015)\n93.54\n99.84\n95.90\n99.84\n75.49\n93.67\n77.61\n93.74\nRKD (Park et al., 2019)\n92.42\n99.75\n95.99\n99.85\n74.20\n93.54\n76.92\n93.09\nAugConf (Burns et al., 2023)\n92.60\n99.75\n95.10\n99.83\n74.99\n93.72\n78.34\n94.02\nAdaptConf (Ours)\n93.69\n99.84\n96.13\n99.87\n75.61\n93.78\n78.64\n94.03\n\u2206\n+0.00\n+0.23\n+0.00\n+0.10\n+0.81\n+0.84\n+0.44\n+0.26\nTable 7. Top-1 and top-5 results on the CIFAR-10/CIFAR-100 noise label validation set. \u2206 represents the performance improvement\nover the student model trained from scratch. All results are the average over 3 trials.\n4.2.2. FEW-SHOT LEARNING\nFor the few-shot learning task, we conduct distillation ex-\nperiments separately in the classification (Table 5) and meta-\nlearning (Table 6) stages. We compare and evaluate the\nperformances of student when trained with teachers of dif-\nferent sizes. In the classification experiments, only RKD\nresults in a performance degradation of the student model,\nwhile the usage of other methods led to varying degrees of\nimprovement. Notably, our confidence-based method out-\nperforms previous knowledge distillation based ones. In the\nmeta-learning stage, we employ weights from different train-\ning stages of the same model as the teacher. Experimental\nresults demonstrate significant advantages of our proposed\nmethod. Even when using the Class-stage weight as the\nteacher, our approach achieves a +0.66% improvement over\nthe baseline set by a weaker ResNet18(Class-stage) teacher\nmodel. Furthermore, when using the same stage weight as\nthe teacher, our confidence-based method surpasses previ-\nous knowledge distillation results to a greater extent.\n4.2.3. TRANSFER LEARNING\nTable 3 examines the efficacy of transfer learning using the\niNaturalist (Van Horn et al., 2018) and ImageNet (Deng\net al., 2009) datasets. When our method is trained with\nground truth labels on ImageNet, it demonstrates a notable\nenhancement, achieving an increase of +0.33% in top-1 ac-\ncuracy on a model with a high precision of 83.5%. Even\nwithout ground truth labels, our approach still secures a\n+2.15% improvement over the baseline set by a weaker\nResNet50 teacher model. On the iNaturalist dataset, our\nconfidence-based method also surpasses previous knowl-\nedge distillation results by a considerable margin.\n4.2.4. LEARNING WITH NOISY LABELS\nIn Table 7, we analyze the effectiveness of weak-to-strong\nusing the CIFAR-10 and CIFAR-100 datasets under two sim-\nulated noisy label settings. When training the model on the\nsample dataset (CIFAR-10), all methods except ours, nega-\ntively impact the model given its already high accuracy. This\nunderscores the robustness of our method, irrespective of\nthe performance gap between the teacher and student mod-\nels. On the CIFAR-100 dataset, our method demonstrates a\nperformance improvement of 0.81% in top-1 accuracy under\nthe asymmetric noise type setting.\n4.3. Ablation Study\nRobustness of confidence distillation. In this study, we\ninvestigate the necessity of devising a method that goes be-\nyond a mere weighted combination of labels. As depicted\nin Eq. 1, despite its straightforward approach of integrating\ndirect learning from a weaker model with the intrinsic capac-\nity of a stronger model, AugConf (Burns et al., 2023) still\n7\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nKD\nAugConf\nAdaptConf\n75.4\n75.6\n75.8\n76.0\n76.2\nAccuracy\nTeacher: ResNet20 (68.93%)   Student: ResNet110 (74.80%)\nKD\nAugConf\nAdaptConf\n=0.1\nT=0.1\n=0.2\nT=0.3\n=0.3\nT=0.5\n=0.4\nT=0.7\n=0.5\nT=0.9\n=0.6\nT=2\n=0.7\nT=4\n=0.8\nT=6\n=0.9\nT=8\nKD\nAugConf\nAdaptConf\nKD\nAugConf\nAdaptConf\n74.5\n75.0\n75.5\n76.0\n76.5\n77.0\nAccuracy\nTeacher: ResNet56 (73.42%)   Student: ResNet110 (74.80%)\nKD\nAugConf\nAdaptConf\n=0.1\nT=0.1\n=0.2\nT=0.3\n=0.3\nT=0.5\n=0.4\nT=0.7\n=0.5\nT=0.9\n=0.6\nT=2\n=0.7\nT=4\n=0.8\nT=6\n=0.9\nT=8\nKD\nAugConf\nAdaptConf\nKD\nAugConf\nAdaptConf\n79.0\n79.5\n80.0\n80.5\n81.0\nAccuracy\nTeacher: ShuffleNet-V1 (72.40%)   Student: ResNet32x4 (79.90%)\nKD\nAugConf\nAdaptConf\n=0.1\nT=0.1\n=0.2\nT=0.3\n=0.3\nT=0.5\n=0.4\nT=0.7\n=0.5\nT=0.9\n=0.6\nT=2\n=0.7\nT=4\n=0.8\nT=6\n=0.9\nT=8\nKD\nAugConf\nAdaptConf\nFigure 2. Ablation study examining the impact of hyper-parameter variation on confidence distillation results. The parameter \u03b1 for\nAugConf is adjusted across a range from 0.1 to 0.9, while the temperature T for AdaptConf is scaled from 0.1 to 8.\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nProportion\nEpoch=30, T=0.5\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=120, T=0.5\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=180, T=0.5\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=240, T=0.5\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nProportion\nEpoch=30, T=4\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=120, T=4\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=180, T=4\n0-0.05\n0.05-0.1\n0.1-0.15\n0.15-0.2\n0.2-0.25\n0.25-0.3\n0.3-0.35\n0.35-0.4\n0.4-0.45\n0.45-0.5\nValue Range of \n0.0\n0.2\n0.4\n0.6\n0.8\nEpoch=240, T=4\nFigure 3. Quantitative analysis about the value of \u03b2(x) in Eq. 2 on the CIFAR-100 dataset. The evaluation is based on the ShuffleNetV1-\nResNet32x4 teacher-student architecture pair.\nrequires manual tuning of a hyper-parameter \u03b1 to balance\nthe ratio of two different objectives. The setting of different\n\u03b1 values can have varying impacts across different contexts.\nSimilarly, although our proposed AdaptConf does not re-\nquire manual adjustment of \u03b1 to balance the proportions of\nobjectives, we can manipulate the temperature T to control\nthe degree of probability distribution in soft labels during the\ncomputation of the cross-entropy CE(\u00b7), following a conven-\ntional distillation method (Hinton et al., 2015). Therefore,\nwe explore the effects of these two methods under different\nhyper-parameter settings on the final outcome. Overall, the\nperformance of KD, AugConf, and AdaptConf improves\nsequentially across various architectural settings. Moreover,\nit can be observed that AugConf exhibits a larger fluctuation\nin results compared to AdaptConf, indicating that the influ-\nence of \u03b1 on AugConf is more significant than the effect of\nT on AdaptConf, which suggests that our AdaptConf has\nsuperior robustness. Additionally, the average outcomes\nachieved by AdaptConf are consistently higher than those\nof AugConf under different hyper-parameter settings.\nRobustness of confidence distillation. In this section, we\nperform a quantitative analysis of the confidence weight\ndetermined by our dynamic function \u03b2(x) as delineated in\nEq. 2, with the findings illustrated in Figure 3. We selected\ncheckpoints from four distinct training phases and calcu-\nlated their specific \u03b2(x) values on the validation set. It can\nbe observed that as training progresses, the proportion of\nsamples with \u03b2 = 0.5 increases, indicating that the student\nmodel\u2019s performance is improving and being aligned with\nthe weak teacher\u2019s correct classifications. A higher tempera-\nture setting T reduces the cross-entropy (CE) discrepancy\nbetween the teacher and student, promoting a more uniform\nbalance between the weak teacher\u2019s guidance and the strong\n8\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nstudent\u2019s own predictions. Consequently, the number of\nsamples where \u03b2 = 0.5 also increases with training. These\nphenomena collectively validate that our proposed Adapt-\nConf can dynamically adjust the learning ratio between the\ntwo components.\n5. Conclusion\nIn this paper, we investigate weak-to-strong generalization\nfor vision foundation models and unveil a promising avenue\nfor enhancing the capabilities of artificial intelligence in the\nvisual domain. By leveraging an innovative adaptive confi-\ndence loss mechanism, we demonstrate the feasibility and\neffectiveness of using weaker models to supervise and im-\nprove stronger counterparts. Our findings not only validate\nthe potential of weak-to-strong generalization but also set\nthe stage for future research endeavors aimed at unlocking\nfurther advancements in AI model performance. This work\ncontributes a significant step forward in the pursuit of more\nsophisticated, efficient, and capable AI systems, emphasiz-\ning the importance of nuanced supervision mechanisms in\nachieving superhuman performance in vision tasks.\nReferences\nBai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell,\nT., Malik, J., and Efros, A. A.\nSequential modeling\nenables scalable learning for large vision models. arXiv\npreprint arXiv:2312.00785, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 2020.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L.,\nAschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M.,\nLeike, J., et al. Weak-to-strong generalization: Eliciting\nstrong capabilities with weak supervision. arXiv preprint\narXiv:2312.09390, 2023.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D.,\nand Sutskever, I. Generative pretraining from pixels. In\nInternational conference on machine learning, 2020.\nChen, P., Liu, S., Zhao, H., and Jia, J. Distilling knowledge\nvia knowledge review. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2021a.\nChen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta-\nbaseline: Exploring simple meta-learning for few-shot\nlearning. In Proceedings of the IEEE/CVF international\nconference on computer vision, pp. 9062\u20139071, 2021b.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, 2009.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\nDu, Y., Liu, Z., Li, J., and Zhao, W. X.\nA survey\nof vision-language pre-trained models. arXiv preprint\narXiv:2202.10936, 2022.\nFurlanello, T., Lipton, Z., Tschannen, M., Itti, L., and\nAnandkumar, A. Born again neural networks. In Interna-\ntional Conference on Machine Learning, pp. 1607\u20131616.\nPMLR, 2018.\nHao, Z., Guo, J., Han, K., Hu, H., Xu, C., and Wang, Y.\nVanillakd: Revisit the power of vanilla knowledge dis-\ntillation from small scale to large scale. arXiv preprint\narXiv:2305.15781, 2023a.\nHao, Z., Guo, J., Han, K., Tang, Y., Hu, H., Wang, Y.,\nand Xu, C. One-for-all: Bridge the gap between hetero-\ngeneous architectures in knowledge distillation. arXiv\npreprint arXiv:2310.19444, 2023b.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectifiers: Surpassing human-level performance on\nimagenet classification. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 1026\u20131034,\n2015.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\n2016.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00b4ar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000\u201316009, 2022a.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00b4ar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2022b.\nHeo, B., Kim, J., Yun, S., Park, H., Kwak, N., and Choi,\nJ. Y. A comprehensive overhaul of feature distillation. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision, 2019.\nHinton, G., Vinyals, O., and Dean, J.\nDistilling\nthe knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\n9\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 1997.\nKaelbling, L. P., Littman, M. L., and Moore, A. W. Re-\ninforcement learning: A survey. Journal of artificial\nintelligence research, 1996.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C.,\nLo, W.-Y., et al.\nSegment anything.\narXiv preprint\narXiv:2304.02643, 2023.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nLee, D.-H. et al. Pseudo-label: The simple and efficient\nsemi-supervised learning method for deep neural net-\nworks.\nIn Workshop on challenges in representation\nlearning, ICML, 2013.\nLi, J., Socher, R., and Hoi, S. C. Dividemix: Learning with\nnoisy labels as semi-supervised learning. arXiv preprint\narXiv:2002.07394, 2020.\nLi, S., Xia, X., Ge, S., and Liu, T. Selective-supervised\ncontrastive learning with noisy labels. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 316\u2013325, 2022.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, 2014.\nPark, W., Kim, D., Lu, Y., and Cho, M. Relational knowl-\nedge distillation. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2019.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n10684\u201310695, 2022.\nRomero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta,\nC., and Bengio, Y. Fitnets: Hints for thin deep nets. arXiv\npreprint arXiv:1412.6550, 2014.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,\nS., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bern-\nstein, M., et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 115:\n211\u2013252, 2015.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\nbottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. Mastering the\ngame of go with deep neural networks and tree search.\nnature, 2016.\nSimonyan, K. and Zisserman, A.\nVery deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nTanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. Joint\noptimization framework for learning with noisy labels. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2018.\nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C.,\nShepard, A., Adam, H., Perona, P., and Belongie, S. The\ninaturalist species classification and detection dataset. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 2017.\nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.\nMatching networks for one shot learning. Advances in\nneural information processing systems, 29, 2016.\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\nwith noisy student improves imagenet classification. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 10687\u201310698, 2020.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.\nZhang, X., Zhou, X., Lin, M., and Sun, J. Shufflenet: An\nextremely efficient convolutional neural network for mo-\nbile devices. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018.\n10\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nZhao, B., Cui, Q., Song, R., Qiu, Y., and Liang, J. De-\ncoupled knowledge distillation. In Proceedings of the\nIEEE/CVF Conference on computer vision and pattern\nrecognition, 2022.\n11\nVision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models\nA. Implementation details.\nA.1. ImageNet Classification\nCIFAR-100. We adopt the vision architectures of the teacher and student models as outlined in the traditional distillation\npapers (Hao et al., 2023b; Zhao et al., 2022). It should be noted that the previous codebase (Zhao et al., 2022) conducted\nexperiments on CIFAR-100 using only 1 GPU. To expedite our experiments, we leverage the distributed Pytorch frame-\nwork (Paszke et al., 2019) to train and do inference on 8 GPUs. Consequently, some hyperparameter settings and results\nmay not align exactly with the previous paper. Specifically, we employ the SGD optimizer with a momentum of 0.9. The\nlearning rate starts at 0.2 and decays with a minimum learning rate of 2e-3 using a cosine annealing schedule. We train for\n240 epochs with a batch size of 512 spread across 8 GPUs, and apply a weight decay of 0.0005. Standard data augmentation\ntechniques, including random resized crop and horizontal flip are utilized.\nImageNet. we employ the SGD optimizer with a momentum of 0.9. The learning rate starts at 0.1 and decays with a rate of\n0.1 every 30 epochs. We train for 100 epochs with a batch size of 512 spread across 8 GPUs, and apply a weight decay of\n0.0001. Standard data augmentation techniques, including random resized crop, horizontal flip and label smoothing are\nutilized.\nA.2. Transfer learning.\nTo fine-tune the self-supervised pretrained ViT-B on ImageNet and iNaturalist, we adopt the hyperparameter settings from\nMAE (He et al., 2022b). The adamw optimizer is employed for this purpose. The learning rate begins at 2e-3 and gradually\ndecays with a minimum learning rate of 1e-6, utilizing a cosine annealing schedule. We conduct training for 100 epochs,\nutilizing a batch size of 4096 across 8 GPUs. A weight decay of 0.05 is applied to mitigate overfitting. The fine-tuning\nprocess incorporates robust data augmentation techniques, including auto-augment, mixup, cutmix, and stochastic drop path.\nA.3. Few-shot Leaerning\nWe use ResNet12 and follow the setting of (Chen et al., 2021b) on miniImageNet dataset, and created ResNet18 and\nResNet36 by increasing the number of layers in original ResNet12. For the classification training stage, we use the SGD\noptimizer with momentum 0.9. The learning rate starts from 0.1 and the decay factor is set to 0.1. On miniImageNet, we\ntrain 100 epochs with the batch size of 128 on 4 GPUs, the learning rate decays at 90 epoch, and the weight decay is 0.0005.\nStandard data augmentation strategies including random resized crop and horizontal flip are applied. For meta-learning\nstage, we use the SGD optimizer with momentum 0.9. The learning rate is fixed as 0.001. The batch size is set to 4, i.e.,\neach training batch contains 4 few-shot tasks to compute the average loss. The cosine scaling parameter \u03c4 is initialized as 10.\nFor knowledge distillation, the kd loss weight is set to 1, the temperature is set to 10. We use the threshold with 8 and 0.25\nfor classifier stage and meta stage, respectively.\nA.4. Learning with noisy labels\nFor CIFAR-10/100 datasets, we follow (Li et al., 2022) use a PreAct ResNet18 network, and created PreAct ResNet34 by\nincreasing the number of layers in PreAct ResNet12. We train our models using SGD with a momentum of 0.9, a weight\ndecay of 1e-4, and a batch size of 128. The network is trained for 250 epochs and the warm-up epoch is set to 1 dufring\ntraining stage. We set the initial learning rate as 0.1, and reduce it by a factor of 10 after 125 and 200 epochs. The fine-tuning\nstage of Sel-CL+ has 70 epochs, where the learning rate is 0.001. We always set the Mixup hyperparameter to 1, scalar\ntemperature to 0.1, and loss weights to 1. We try two settings of simulated noisy labels: symmetric and asymmetric. And\nthe noise ratio is set to 0.2 and 0.4, respectively. For knowledge distillation, we set the threshold to 0.5 and assign a weight\nof 1 to the knowledge distillation loss.\n12\n"
  },
  {
    "title": "Multi-line AI-assisted Code Authoring",
    "link": "https://arxiv.org/pdf/2402.04141.pdf",
    "upvote": "8",
    "text": "Multi-line AI-assisted Code Authoring\nOmer Dunay\nomerdu@meta.com\nMeta Platforms, Inc.\nMenlo Park, CA, USA\nDaniel Cheng\ndanielcheng@meta.com\nMeta Platforms, Inc.\nBellevue, WA, USA\nAdam Tait\nadamtait@meta.com\nMeta Platforms Inc.\nUSA\nParth Thakkar\nparthdt@meta.com\nMeta Platforms, Inc.\nMenlo Park, CA, USA\nPeter C. Rigby\u2217\npcr@meta.com\nMeta Platforms, Inc.\nUSA\nAndy Chiu\nachiu@meta.com\nMeta Platforms, Inc.\nUSA\nImad Ahmad\nimadahmad@meta.com\nMeta Platforms, Inc.\nMenlo Park, CA, USA\nArun Ganesan\narunganesan@meta.com\nMeta Platforms, Inc.\nUSA\nChandra Maddila\ncmaddila@meta.com\nMeta Platforms, Inc.\nBellevue, WA, USA\nVijayaraghavan Murali\nvijaymurali@meta.com\nMeta Platforms, Inc.\nMenlo Park, CA, USA\nAli Tayyebi\nalitayyebi@meta.com\nMeta Platforms, Inc.\nNew York, NY, USA\nNachi Nagappan\nnnachi@meta.com\nMeta Platforms, Inc.\nUSA\nABSTRACT\nCodeCompose is an AI-assisted code authoring tool powered by\nlarge language models (LLMs) that provides inline suggestions to\n10\u2019s of thousands of developers at Meta. In this paper, we present\nhow we scaled the product from displaying single-line suggestions\nto multi-line suggestions. This evolution required us to overcome\nseveral unique challenges in improving the usability of these sug-\ngestions for developers.\nFirst, we discuss how multi-line suggestions can have a \"jarring\"\neffect, as the LLM\u2019s suggestions constantly move around the devel-\noper\u2019s existing code, which would otherwise result in decreased\nproductivity and satisfaction.\nSecond, multi-line suggestions take significantly longer to gen-\nerate; hence we present several innovative investments we made\nto reduce the perceived latency for users. These model-hosting\noptimizations sped up multi-line suggestion latency by 2.5x.\nFinally, we conduct experiments on 10\u2019s of thousands of engi-\nneers to understand how multi-line suggestions impact the user\nexperience and contrast this with single-line suggestions. Our ex-\nperiments reveal that (i) multi-line suggestions account for 42% of\ntotal characters accepted (despite only accounting for 16% for dis-\nplayed suggestions) (ii) multi-line suggestions almost doubled the\npercentage of keystrokes saved for users from 9% to 17%. Multi-line\nCodeCompose has been rolled out to all engineers at Meta, and\nless than 1% of engineers have opted out of multi-line suggestions.\n1\nINTRODUCTION\nCodeCompose [15] provides inline suggestions as a software engi-\nneer types code, but it was originally only designed to predict tokens\nthat would complete the current line. Such single-line suggestions\nshould be quick, highly accurate, and help with the immediate con-\ntext. In contrast, multi-line suggestions need to be deeply informed\n\u2217Rigby is a professor at Concordia University in Montreal, QC, Canada.\nand intelligent. These code blocks can help users discover APIs, best\npractices, and implementation details. In this work, we describe\nhow we added multi-line suggestions to CodeCompose.\nCodeCompose suggests code as the user types, impacting the\nmain authoring workflow. Multi-line suggestions can have a \u201cjar-\nring\u201d effect as the displayed suggestions move around code that\nthe user has already written. This mingling of suggested code with\nhuman written code has a high cognitive load on the user, as they\nhave to review and rework code that they trust and wrote, with\ncode that they need to verify. We adhered to strict design principles\nof not being intrusive, and of fitting seamlessly into the existing\nuser experience.\nThere were three main challenges in developing multi-line sug-\ngestions at scale: (1) eliminating the jarring effect (2) providing\nresponsive suggestions with low latency for large blocks of code to\nbe generated (3) rolling out and evaluating the impact of multi-line\nto 10\u2019s of thousands of developers.\nChallenge 1. User interface experience - Eliminating the\n\u201cjarring\u201d effect\nAvoiding the \u201cjarring\u201d effect is simple with single-line sugges-\ntions: we only show suggestions when the cursor is at the end of a\nline (with the exception of special characters at the end like brack-\nets). In contrast, multi-line is complex as it can disrupt code that\nis already written. Creating an algorithm for this solution poses a\nnon-trivial technical problem, including figuring out how to deter-\nmine the context of the cursor reliably. Moreover responses from\nthe LLM can be not well formatted, and aligning them into the\ncursor position may cause further distraction for the user.\nCodeCompose handles these challenges with a combination\nof pre-processing and post-processing algorithms that utilize a\nsemantic context understanding of the programming language and\ncursor position scope. Section 3 discusses this multi-line algorithm\nbased on the semantic scope to minimize noise to the user.\narXiv:2402.04141v1  [cs.SE]  6 Feb 2024\nOmer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C. Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachi\nNagappan\nChallenge 2. Responsive User Experience\nCodeCompose\u2019s inline suggestions show up automatically as the\nuser types. Each suggestion aligns with a given state of the active\nfile, and is being invalidated as soon as the user types an additional\nkeystroke or moves their cursor. Multi-line suggestions are long by\nnature; therefore it may take a few seconds for the LLM to generate\nthem, during which time the user may hit another keystroke and\ndismiss the response before they have even seen the suggestion.\nSince latency is a key factor in determining the \"display rate\",\ni.e. the number of suggestions that users are actually able to view,\nwe invested in reducing the latency of long multi-line suggestions\nthrough both the client extension and the model-hosting service,\nas outlined in Section 4.\nChallenge 3. Production release and effectiveness measure-\nment\nThe final challenge we faced was in monitoring the effectiveness\nof the experimental features enumerated in the prior sections. We\ntracked a host of metrics online to evaluate whether each feature\nwas improving the user experience, and adjusted the product ac-\ncordingly. In particular, we needed to evaluate whether users found\nmulti-line suggestions more useful compared to only single-line\nsuggestions, due to the higher latency and reduced display rate of\nmulti-line suggestions.\nThese metrics included acceptance rate, display rate, latency, %\nkeystrokes saved, and # of characters accepted per user. These were\nbroken down by single-line vs multi-line suggestions to determine\nthe net benefit of each feature for users.\nResult Summary. In monitoring our online A/B tests, we found\nthat the investment in multi-line suggestions disproportionately\nincreased throughput, as:\n\u2022 Multi-line suggestions accounted for 42% of total charac-\nters accepted day (despite only accounting for at 16% of\ndisplayed suggestions)\n\u2022 Multi-line suggestions drove a significant net increase in %\nkeystrokes saved from 9% to 17%, as shown in Table 1\nThis paper is structured as follows. In Section 2, we give back-\nground on Meta and introduce our experimental methodology. In\nSections 3 to 5, we address each challenge. In Section 6, we discuss\nthreats to validity. In Sections 7 and 8, we discuss related work and\nconclude the paper.\n2\nBACKGROUND AND METHODOLOGY\n2.1\nMeta\nMeta is a major industrial software organization with a complex\ncode base that covers a wide range of applications ranging from\nsocial networking and virtual reality to software engineering in-\nfrastructure, such as continuous integration tooling and workplace\ncoordination. Thousands of developers work on billions of lines of\ncode (LOCs) in a monolithic repository that contains source code\nwritten in tens of programming languages. At Meta, code authoring\nis one of the prominent activities in the Software Development Life\nCycle (SDLC). A significant amount of contextual knowledge on the\ninternal software development processes, and libraries, is contained\nwithin in the source code or is confined to a small set of developers.\nFor example, a question like \u201cHow can I query a table from Hive in\nHack?\u201d can be answered using that knowledge. The state of the\nsource code and the developers associated with the source code\nare in a constant state of evolution - internal frameworks and tools\nget added and deprecated, while developers move across teams\nand change roles. At Meta\u2019s scale, keeping up with the knowledge\nrequired to accomplish coding tasks is a major challenge. Addition-\nally, the dynamic environment at a large software company like\nMeta poses several challenges concerning knowledge discovery\nand improving developer productivity.\n2.2\nCodeCompose at Meta\nAt Meta, we built an AI-assisted code authoring system named\nCodeCompose to explore the application of LLM technology for\ncode authoring [15]. Our team conducted R&D on the underlying\nLLM architecture and converged on using InCoder-6.7B [13] as the\nbase model for the initial version of CodeCompose. Recently, with\nthe release of CodeLlama [19], we switched to using a fine-tuned\nversion of CodeLlama-7B as the foundation model.\nCodeCompose has several desired characteristics for powering\nan inline suggestion experience: (i) multi-linguality, due to the\nCodeLlama\u2019s training on a multitude of programming languages,\n(ii) customized for Meta, as a result of our internal fine-tuning\nof CodeLlama on organization-specific data, (iii) natural language\nproficiency, which gives it the capability to generate and understand\ninline code comments, and (iv) bi-directionality, due to its fill-in-\nthe-middle (FIM) training that allows it look at both the code before\nand after the cursor when generating a suggestion.\n2.3\nMeasures for Evaluating CodeCompose\nTo evaluate CodeCompose we used the following measures:\n\u2022 # of suggestions displayed per user per day\n\u2022 E2E latency of generating and displaying suggestions\n\u2022 Acceptance rate (# accepted / # displayed for suggestions\nshown to user for > 750ms)\n\u2022 # chars accepted per user per day\n\u2022 % keystrokes saved saved (# chars accepted / # of chars\ntyped by user)\nWe think of these metrics as a funnel, with suggestions displayed\nat the top, leading to increased throughput in suggestions accepted,\nand increasing the total % of keystrokes saved. Latency serves as a\nguardrail metric, but also feeds into this funnel since lower latency\nincreases the # of suggestions displayed.\nAt Meta, changes are rolled out in randomized double blind trials,\ni.e. A/B tests [20]. We describe the setup for each of our rollouts to\nevaluate the effectiveness of each new multi-line feature against\nthe holistic user experience in Section 5.\n3\nADDRESSING CHALLENGE 1\nUser interface experience - Eliminating the \u201cjarring\u201d effect\nIn this section, we explore the unique challenges and consid-\nerations associated with deploying a coding assistant capable of\nmulti-line suggestions within a large-scale industrial organization\nlike Meta. These insights are derived from feedback received from\nhundreds of users that were using CodeCompose as early adopters.\nContrary to initial intuition, the multi-line use case presents a\nhigher level of complexity compared to the single-line use case,\nMulti-line AI-assisted Code Authoring\nFigure 1: Single-line \"jarring\" effect example: The user cur-\nsor positioned between \"def\" keyword and the \"quicksort\"\nfunction, inline suggestion appears and moves the existing\nuser code to the right.\nfrom both a product perspective and technical implementation\nstandpoint. The primary objectives for each use case are as follows:\n\u2022 Single-line: The aim is to provide quick and highly accu-\nrate assistance, facilitating task completion and reducing\nthe burden of keystrokes for straightforward and repetitive\ntasks.\n\u2022 Multi-line: The goal is to offer deeply informed, intelligent\nassistance that aids users in discovering APIs, best practices,\nand implementation details.\nWe provide examples of the single-line completion in Figure 1,\nthe difficulties in multi-line completion in Figure 2, and our multi-\nline strategy in Figure 3. The captions for the figures are extensive\nto allow the reader to walkthrough the examples.\n3.1\nDefinition of the \u201cjarring effect\"\nCodeCompose suggests code as the user types. This means sugges-\ntions appear frequently while the user is engaged in their primary\nauthoring workflow. One of our strictest design principles is that\nsuggestions should not be intrusive and should seamlessly inte-\ngrate into the user flow. Based on user feedback, we identified\nsuggestions that significantly disrupt the user, resulting in a jarring\nexperience. The two primary instances of this are (1) suggestions\nthat shift existing code that the user is currently reading or typing,\neither upwards, downwards, or to the right as shown in Figure 1 (2)\nsuggestions that do not align with the structure of the code, such\nas poorly formatted suggestions that overlap the existing scope of\nthe cursor as shown in Figure 2.\n3.2\nApproach to address the \u201cJarring Effect\"\nAddressing the \u201cjarring effect\" for single-line suggestions is rela-\ntively uncomplicated. To prevent the displacement of existing code,\nwe simply refrain from displaying suggestions if there is any code\nto the right of the cursor, with certain exceptions such as }, ) and ].\nSince no characters exist on the right-hand side of the cursor when\na suggestion appears, no user code can be shifted to the right. Fur-\nthermore, as we only suggest until the end of the line, we eliminate\nthe risk of introducing unformatted code that overlaps the existing\ncursor scope. However, for multi-line suggestions, the \"jarring ef-\nfect\" is more likely to occur, as existing code below the cursor is\nconstantly being pushed up and down.\nTo circumvent the jarring effect for multi-line suggestions, we\nadhere to the following rules:\n\u2022 Multi-line suggestions are triggered only when the cursor\nis positioned at the end of the scope that owns the cursor.\n\u2022 Multi-line suggestions should be shown until the end of the\ncurrent scope.\nFigure 2: Example showing multi-line \"jarring\" effect: the\nuser cursor was between a function name and the next line\ncontaining the statement \"test1 = 1\". When the suggestion\noccurs, the existing line is pushed down disrupting the de-\nveloper\u2019s flow and forcing them review the suggested \"quick-\nsort\" function while also determining the correct location of\ntheir existing code.\nWhen users write code, their flow and mindset is in the most\ninner scope. Therefore, satisfying rule (1) ensures that when we\nsuggest multi-line to the user, the lines below that are pushed down\nare in the outer encapsulating scope, thereby not disrupting the\nuser flow and not causing the jarring effect. Furthermore, satisfying\nrule (2) ensures the suggestion structure is completing until the\nend of the current scope with no overlap.\nWe specifically trigger multi-line in the following cases:\n(1) The cursor is positioned at the end of the most inner scope\nthat contains it.\n(2) The cursor is at the end of a line that defines a new symbol\nthat creates a new scope.\n(3) The cursor is at the end of a notebook cell (for Bento [1]\nnotebook use case).\n(4) The user explicitly requested using a shortcut key.\n3.3\nTechnical Implementation of the Strategy\nThe requirement to trigger multi-line only in certain cases, based\non the cursor position, necessitated the integration of semantic\ncontext understanding into the CodeCompose system. The existing\nCodeCompose system employs a typical client-server architecture\n(shown in Figure 4) in which the server is an inference tier that\nruns the model and a client is an editor that surfaces code sugges-\ntions. We encode the bulk of the client-side logic in a Language\nServer Protocol (LSP) conformant language server that is reused\nacross multiple editor integrations. To mediate requests between\nthe client and server, we implemented a language server in Rust\nthat we reuse across our various editor integration. While most\nlanguage servers are designed to support a wide array of tradi-\ntional IDE functionality (autocomplete, jump-to-definition, etc.),\nthe CodeCompose language server supports only one meaningful\nrequest type: \"textDocument /inlineCompletions\".\nOmer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C. Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachi\nNagappan\n(a) Multi-line suggested since the cursor is positioned at the end of\nan empty function which is the inner scope.\n(b) Multi-line suggested since the cursor is positioned at the end of\nan non-empty function which is the inner scope.\n(c) Multi-line suggested since the cursor is at the end of \"if statement\"\nwhich is the inner scope.\n(d) Multi-line suggested since the cursor is at the end of a newly\ndefined function.\n(e) Multi-line suggested since the cursor is at the end of a newly\ndefined \"if statement\".\n(f) Multi-line is not suggested since the inner scope if function and\nthe cursor is not at the end of it.\n(g) Both multi-line and single-line not suggested since there are\ncharacters after the cursor in the same line.\nFigure 3: Examples showing pre-processing stage: Deciding\nbased on the cursor position which type of suggestion should\nbe displayed.\nAvoiding the jarring effect for single-line was trivial from a tech-\nnical point of view as it required a simple conditional statement.\nFor multi-line on the other hand, the strategy described above is\nmuch more complex and requires detailed semantic context under-\nstanding of the programming language.\nRequest workflow - Pre-processing: A completion request is sent\nfrom the client to the language server, the CodeCompose language\nserver parses the current state of the file and understands the cursor\nposition. If the cursor\u2019s position aligns with one of the cases outlined\nabove, this request is marked as a multi-line request. The request is\nthen validated in the local cache; if it is a cache miss, it is sent to\nthe CodeCompose model flagged as a multi-line request (examples\nshown in Figure 3).\nRequest workflow - Post-processing: The model LLM generates\nsuggestions and sends back the response to the language server.\nThe model response does not have a strict guarantee for how well it\nwill be structured; it may contain overlap with code in the current\nscope. Therefore, in the language server, the response goes through\nan additional step of post-processing that truncates the response in\ncase it overlaps the current scope, as shown in Figure 5.\nFinally, we built an extensive unit test suite (473 tests) to validate\nthat there was no end-to-end jarring effect or incorrect truncation\nwith our implementation.\nIn summary: (1) Multi-line suggestions are triggered only\nwhen the cursor is at the end of scope (2) Suggestions\nare shown until the end of the current block (3) After the\nsuggestion is accepted, the cursor needs to be moved to\nthe end of the suggested block.\n4\nADDRESSING CHALLENGE 2\nResponsive User Experience\nCodeCompose\u2019s inline suggestions show up as the user types,\neach suggestion aligns with the given state of the file and invalidated\n(dismissed in the editor\u2019s User Interface) as soon as the user types an\nadditional keystroke or moves their cursor. This implies that latency\nis a key factor in determining the \"display rate\" - the percentage of\nsuggestions shown to users.\nFor single-line we limit the generation of a suggestion to stop at\neither newline or 25 max tokens. The average latency is low, there-\nfore display rate is not a major concern. Multi-line generations are\nmuch longer (p50 325 chars, p90 450 chars), and max tokens is set to\n120. This can take over 2 seconds for large suggestions; therefore, re-\nducing latency for multi-line is significantly more likely to increase\nthe display rate. In this section we describe multiple approaches\nand projects that were built focused on reducing the latency of\ngenerating long multi-line generations and increasing its display\nrate. The improvements were done across all main components of\nthe CodeCompose System - the client-side editor extension, the\nlanguage server and the service hosting the model.\n4.1\nImprovements in the editor client extension\n(i.e. VSCode / Bento [1] Notebooks)\nDuring initial testing, CodeCompose\u2019s multi-line suggestions were\ngenerated automatically as the user typed. However, from the user\u2019s\nMulti-line AI-assisted Code Authoring\nFigure 4: System architecture of CodeCompose: Client editor that surface the suggestions, a language server to mediate requests\nwith CodeCompose model service host. In the request \"multi-line\" flag is passed to the model service.\nFigure 5: Example showing the post-processing stage: The\ncursor is in the scope of the \u201cfoo\u201d function. Although, the\nmodel returns a multi-line suggestion of both the \u201cfoo\" and\n\"foo2\" functions, postprocessing will remove the code in the\nred box, and will only display suggestions for the in-scope\n\u201cfoo\u201d function to the user.\nperspective, there was not a deterministic way to know whether\na CodeCompose suggestion was still being processed and would\nappear momentarily, or whether CodeCompose had decided not\nto display a suggestion. This unpredictability caused frustration to\nthe user and made the system appear unreliable.\nTo solve this, we introduced an inline spinner indicator, as seen\nin Figure 6, which pops up next to the user cursor as long as there\nis an active completion that still needs to be responded to. As\nsoon as the language server\u2019s pre-processing (described in 3) de-\ntermines that the current request should be multi-line, it sends a\n\"CodeCompose/fetchingMultiline\" notification to the editor client,\nwhich then renders the indicator. The editor dismisses the indicator\nonce the request is responded to or canceled.\nThe user\u2019s mental model is rooted in knowing whether a sug-\ngestion is expected to appear or not. The inline indicator lets users\nknow if a multi-line suggestion is being processed, and therefore\nallows them to make a decision on whether to type the next char-\nacter \u2013 or to wait a little longer to see the upcoming suggestion.\nThis feedback improves the overall display rate by nudging users\nFigure 6: Inline indicator (marked with orange underline)\nwith a spinner shown to the left of the cursor alerting the\nuser CodeCompose may show soon an AI suggestion\n.\nto wait a little longer, especially for longer multi-line suggestions.\nThis results in a transparent, predictable experience for users that\nhelps build their trust in the CodeCompose system.\nBecause multi-line suggestions take significantly longer to\ngenerate, we added a spinning UI indicator so that users\nwere aware that a multi-line suggestion was being gen-\nerated. This reduced the perceived latency and increased\nusage of multi-line suggestions over single-line sugges-\ntions.\n4.2\nOptimizations to the model-hosting service\nWe implemented numerous improvements to the model-hosting\nservice with the primary goal of reducing latency. Our hypothesis\nwas that latency reduction would increase all of our quantitative\noutcome metrics, particularly for multiline suggestions where gen-\neration time was significantly longer. Suggestions being available\nsooner would mean that more suggestions were shown to users,\nbefore they would have taken another action (e.g. typing the next\ncharacter) that would cause the suggestion not be shown.\nOmer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C. Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachi\nNagappan\nFrom mid-June through September, we ran a series of A/B ex-\nperiments [20] on CodeCompose, which is rolled out to 10\u2019s of\nthousands of Meta\u2019s engineers, that incrementally improve latency:\nWe initially rolled out to 10% of the population as a smoke test,\nbefore comparing metrics at 50% rollout for a one week duration\nto validate the latency impact across all our outcome metrics. Be-\nlow, we highlight each of the latency experiments we ran, then\nsummarize the net metric gains in Section 5.\nFlash Attention. When we implemented Flash Attention [12]\non our 6.7B InCoder-based FIM-trained model on multi-line sug-\ngestions, we saw a median 2.53x reduction in first token latency\nand median 1.15x reduction in latency to the final token. Flash\nAttention is an improvement primarily in the model\u2019s attention\nstack, so a greater improvement in first token latency was expected.\nIn our experiment, we found a 13% increase in the number of sug-\ngestions displayed, an 8% increase in acceptance rate, a 3% increase\nin the average length of suggestions accepted. Overall, we saw a\n74% reduction in median latency because Flash Attention improved\nsingle-line suggestion latency even more than multi-line and single-\nline suggestions are the majority of suggestions that our models\ngenerate.\nCUDA graphs and fused kernels. Using CUDA graphs and\nfused kernels[8] together on our 6.7B InCoder-based FIM-trained\nmodel, we observed that a reduction in our median generation la-\ntency by 29%, and at the 90th percentile a reduction in latency of\n40%. The latency reductions led to increases in our absolute number\nof displayed suggestions by 26% and total number of suggestions\naccepted by 17%. However, we also observed the acceptance rate\ndecline of 1%. From this experiment, we learned that acceptance\nrate is not strictly correlated with latency. We hypothesize that the\nimprovement in latency during this experiment resulted in sugges-\ntions being shown in environments of increasingly low acceptance\nrates. For example, if the user rejected a suggestion then typed only\none more character, a newly generated suggestion was unlikely to\nbe accepted even if displayed immediately with low latency.\nQueue Priority. We studied the effect of the longer multi-line\nsuggestions on our GPU capacity utilization. We discovered that\nlonger generations have an outsized effect on capacity load. Addi-\ntionally, requests for longer multi-line suggestions were receiving\ntime-out responses at nearly 2x the rate of single-line requests. We\nsuspect the reason for the large difference between time-out rates\nis created by the triggering strategies we employ (as described in\nSection 3). To alleviate the queue contention, we implemented QoS\nto advantage our longer multi-line requests. We increased the queue\ngestation time relative to shorter single-line requests to boost the\nmulti-line success rate. This effort succeeded in reducing the magni-\ntude of multi-line time-out responses. We succeeded in increasing\nthe number of displayed multi-line suggestions and observed a 1%\nabsolute increase in keystrokes saved.\nStreaming and early cancellation. Building on our work to\ntruncate generated suggestions at the end of scope (see Section 3.3),\nour analysis discovered that 54% of all characters generated were\nbeing truncated before being shown. Further, 47% of all suggestions\ngenerated by the model were never displayed. To reduce this wasted\neffort by our GPU capacity, we introduced a streaming response\nmechanism that allowed early cancellation when the client closes\nthe stream. With streaming, the client is able to communicate back\nto the model service that either the generated suggestion is no\nlonger needed at all or that the model has generated characters\nthat will be truncated. We observed a 45% improvement to overall\nmodel service request round trip latency as a result of implementing\nstreaming with cancellation.\nBatching. We used 2-way tensor parallelism to serve responses\n1.5x faster. Since this uses two GPUs to serve a request, but does not\nimprove the latency by a factor of two, it effectively requires more\ncapacity to handle the same workload. However, we found that\nthis can be more than compensated by using continuous batching,\ngiving us significantly faster responses while also gaining improved\neffective capacity due to the efficacy of batching multiple requests.\nImproving latency has a compounding effect. Reducing latency\ndistributions increases the effective capacity of our machines to\nserve more requests. For instance, the above optimizations we can\nserve the same request with a 2.5x reduction in the number of\nGPUs. The main reason is that there is less wasted computation on\ncancelled requests and request can be batched. Reducing capacity\nutilization further reduces queuing times and the distribution of\nround trip latency.\nBy combining streaming alongside tensor parallelism and contin-\nuous batching, we sped up the median singleline suggestion from\n440ms to 280ms (1.5x faster/35% latency reduction), and the me-\ndian multi-line suggestion from 2000ms to 750ms (2.5x faster/60%\nlatency reduction). This in turn led to a 16% relative improvement\nin characters accepted by users. This confirms that latency has a\nsignificant impact on the effectiveness of code completion tools.\nWe invested in five techniques to optimize the model-\nhosting service, which reduced median latency of multi-\nline suggestions down to 750ms. This in turn increased the\n# of suggestions shown to users and the overall keystrokes\nsaved by users, as validated through A/B experiments in\nSection 5. Having a responsive UX was crucial to the adop-\ntion of multi-line suggestions.\n5\nADDRESSING CHALLENGE 3\nProduction release and effectiveness measurement\nThe final challenge faced was in monitoring the effectiveness\nof the experimental features enumerated in the prior sections. We\ntracked a host of metrics in real-time to evaluate whether each\nfeature was improving the user experience, and adjusted the fea-\ntures accordingly. These metrics included acceptance rate, display\nrate, latency, % keystrokes saved, and # of characters accepted per\nuser. The metrics were broken down by single-line vs multi-line\nsuggestions to determine the net benefit of each feature for users.\nWe ran A/B experiments with a 50/50 split to control and test for\neach feature release.\n5.1\nExperiments for Release of Multi-line\nSuggestions\nFirst, when rolling out each of the multi-line trigger points (as\nexplained in Figure 3), we closely monitored both acceptance rate\nand throughput for any negative impact on the user experience:\nMulti-line AI-assisted Code Authoring\nFigure 7: The # of displayed suggestions per user per day,\nat p50 and p75. This display metric dropped in mid-June as\nhigher latency multi-line suggestions rolled out, but recov-\nered by end of August due to our investments in perceived\nlatency.\n(1) Multi-line suggestions require more mental effort for users,\nas they must review multiple lines rather than reading a\nsingle-line suggestion\n(2) Multi-line suggestions cause a substitution effect \u2013 some\nof the trigger points (e.g. upon hitting newline in a new\nfunction block) overlap with single-line. Due to the longer\nlatency for generating multi-line suggestions, users may\nend up typing past these trigger points, causing a missed\ndisplay opportunity for CodeCompose to save the users on\ntyped keystrokes (whereas a single-line suggestion would\nhave displayed, given its lower latency)\n(3) Users may sometimes prefer seeing consecutive single-line\nsuggestions, instead of one or two longer multi-line sug-\ngestions, depending on their workflows\nAs seen in Figure 7, when initially rolling out multi-line sugges-\ntions in mid-June, the number of displayed suggestions per user\ndropped significantly, due to the longer generation time which led\nto users typing past the trigger point.\nHowever, due to our investments in (1) reducing median multi-\nline latency from 2000ms to 750ms (2) showing the inline indicator\n(both of which were described in Section 4), this display metric\ngradually restored close to original levels by end of August. These\nlatency investments decreased the gap in perceived latency between\nsingle-line and multi-line suggestions, which in turn contributed to\nthe continued increase in throughput as users found multi-line sug-\ngestions more and more valuable with the increased responsiveness\nof the suggestion UI.\nThrough our production monitoring, we found that acceptance\nrate also remained similar at 29% between the two suggestion types,\nreinforcing that despite the longer suggestions (which are harder to\npredict accurately), multi-line suggestions were still favored at sim-\nilar rates as single-line. Hence overall throughput increases since\nmore lines of codes were accepted at a given display opportunity.\nFigure 8: Total characters accepted, broken down by sugges-\ntion type. Multi-line suggestions trigger with higher latency\nand lower frequency (16% of total volume of displayed sug-\ngestions). Yet multi-line suggestions account for 42% of total\ncharacters accepted for users.\nTable 1: The % keystrokes saved at each step of the release\ncycle. (a) The combined effect from the UX Product Release\n(mid-June to August) was a 5% absolute increase from multi-\nline, while taking away only 1% absolute from single-line,\ndemonstrating a net improvement to the user of 4% absolute.\n(b) A new model release (CodeLlama-7B) further showed\nthe value of multi-line as the improvement surface going\nforward, as the new model increased single-line throughput\nfrom 8% to 10% (+25% relative), but a much larger increase of\n5% to 7% (+40% relative) for multi-line.\nFurthermore, since we triggered multi-line suggestions less fre-\nquently to reduce the jarring effect, multi-line suggestions only\naccounted for roughly 16% of total volume of suggestions displayed.\nYet as shown in Figure 8, multi-line suggestions accounted for\n42% of total characters accepted by users. These throughput met-\nrics demonstrated the success of our investments in reducing user\nperceived latency, which in turn increased usage of multi-line sug-\ngestions relative to single-line suggestions.\nFinally, our online monitoring metrics demonstrate that multi-\nline suggestions are the correct surface for further improvement\nto the product over time. As shown in Table 1, when releasing\nthe CodeLlama-7B model, we observed that the impact on multi-\nline was much greater: +40% relative improvement in multi-line\nthroughput, compared to only +25% relative improvement in single-\nline throughput. This opens the door to further investments in\ngenerating longer and longer suggestions \u2013 for example, by increas-\ning the max token length for generation, or by triggering multi-line\nsuggestions at more locations.\nOmer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C. Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachi\nNagappan\n5.2\nUser Feedback and Opt-out Rate\nWe monitored qualitative user feedback to triangulate the metric\nimprovements we observed in the prior section. Before CodeCom-\npose handled the \"jarring\" effect, and before new CodeLlama-7B\nwas deployed, several users complained about the \"jarring effect\"\nand quality of multi-line suggestions:\n\u2022 \"[Negative] Lately, I\u2019ve been finding Python multi-line sugges-\ntions distracting. This DevX feels substantially different than\nsingle-line completions: It shifts my code around, causing me\nto shift my gaze from the line I\u2019m writing to figure out what\nhappened. Sometimes the code is useful, but it\u2019s largely been\nnot useful compared to single-line completions (I imagine we\nsee this in suggestion accept rate data). I wonder how we can\nmake this DevX less interruptive, while we work on improving\nsuggestion quality.\"\nAfter developing CodeCompose\u2019s multi-line algorithm, the feed-\nback was much more positive:\n\u2022 \"[Positive] I had so many moments of surprise and delight to\nthe point of taking screenshots of suggestions. CodeCompose\nbeginning to automate so much of the boring/painful parts of\ncoding to make it super fun & fast. I haven\u2019t felt a magical\nproductivity leap like this since <infra feature change from\nseveral years ago>.\"\n\u2022 \"[Positive] I had no idea <language> had multi-line strings;\nthis popped up when in a situation where I was better off\nusing one and was grateful for the discovery\"\n\u2022 \"[Positive] Regarding CodeCompose: Have I become that pre-\ndictable as a human being! My intention was to write a Python\nfunction that generates a SQL case-when statement based on\na dictionary I previously defined. I typed the name of the\nfunction and it autogenerated the 5-line return statement. It\ncorrectly guessed I wanted to create a SQL case-when state-\nment and wrote the function just like I was thinking in my\nhead!!! I always thought the way an individual writes code is\nunique to that individual! So have I become that predictable\nas a human being! What even is real, anymore!\"\n\u2022 \"[Positive] Dealing with annoying boilerplate, ex. propagating\n5 variables through state -> query -> response. CodeCompose\nmakes faster work of these than my usual copy+paste+select+change\nmotion [...] Both writing the generic function & replacing the\ncode was 50%+ accepted suggestions\"\n\u2022 \"[Positive] CodeCompose is just amazing when writing generic\nfunctions like this. Of course, this isn\u2019t perfect since it did not\nget the else part of the function correct but still helped me\nreduce my keystrokes by 90%.\"\n\u2022 \"[Mixed] I\u2019ve been noticing several cases where the first lines\nof the CodeCompose suggestion are quite good, but the sug-\ngestion gets a bit too ambitious and goes on several lines\nbeyond that with code that is subjectively not what I want\nand objectively less related to the intent I expressed with what\nI had typed so far.\"\n\u2022 \"[Negative] I find the multi-line suggestions are rarely useful,\nand cause issues if I accidentally accept them.\"\nUser feedback underscores the intricate equilibrium required\nwhen launching a feature like multi-line suggestions. This balance\nis between recall \u2013 as manifested in the display rate and the number\nof lines in each suggestion \u2013 and the quality of the suggestion itself.\nAs we enhanced the quality of multi-line suggestions by mitigating\nany jarring effects and improving the model\u2019s quality, we observed\na corresponding improvement in user feedback. While we leave a\nfull thematic or ground theory analysis of the qualitative feedback\nas future work, the initial user feedback has been overwhelmingly\npositive, as shown in the above testimonials.\nWe also note that very few engineers disabled the multi-line\nfeature. The overall opt-out rate of CodeCompose stayed constant\nover time at < 0.8% of all users. We allowed users to specifically\ntoggle off multi-line suggestions, while keeping single-line sugges-\ntions on. The multi-line opt-out rate here is even lower at < 0.1% of\nall users, indicating widespread adoption and favorability toward\nmulti-line suggestions.\nDuring the roll-out of multi-line suggestions, we moni-\ntored metrics like acceptance rate, display rate, latency, and\nthroughput to evaluate the net benefit of multi-line sug-\ngestions vs. single-line suggestions. These metrics demon-\nstrated that the investment in multi-line suggestions dis-\nproportionately increased throughput, as multi-line sug-\ngestions accounted for 42% of total characters accepted\nper user per day (despite only accounting for at 16% of dis-\nplayed suggestions), and drove the significant net increase\nin percentage of keystrokes saved nearly doubling from\n9% to 17% (as shown in Table 1). Less than 1% of engineers\nopted out and disabled multi-line suggestions.\n6\nTHREATS TO VALIDITY\n6.1\nGeneralizability\nThis study is conducted at Meta. Although this is a single company\nit includes codebases many times larger than those typically studied\nin open source projects. It also contains code that covers a wide\nrange of software systems from social network to infrastructure\nprojects, such as databases and continuous integration systems.\nThere is also software that runs directly on VR hardware. We are\nable to release CodeCompose to 10\u2019s of thousands of engineers.\nOur results may not generalize to smaller companies and projects\nthat do not have a large codebase to train their models on.\n6.2\nConstruct and Internal Validity\nIn Section 2.3, we describe the metrics we track to determine if multi-\nline CodeCompose is effective. There have been few large scale\ndeployments of AI-assisted, so we used the existing infrastructure\nat Meta to track: display rate, latency, % keystrokes saved, # of\ncharacters accepted per user, and opt-out rate. In our A/B tests\nacross 10\u2019s of thousands of engineers, the results were positive for\neach metric and convincing enough to allow us to roll out multi-\nline to the entire company. We created dashboards to continue\nto monitor these metrics. In future work, we would like to track\nmetrics that other researchers and companies have found useful.\n7\nDISCUSSION AND RELATED WORK\nWhile there is a large body of software engineering research work\non code completions [11, 14, 17, 18, 22], there has been limited\nMulti-line AI-assisted Code Authoring\ndeployment of these in large industrial environments [2\u20135]. With\nthe advent of Generative AI, advances in the code assistance area\nare now in weeks rather than months. Several of these results are\ncirculated as blog posts and not traditional papers, with limited\ndiscussion of evaluating the trade-offs between single-line and\nmulti-line suggestions.\nHence, while multi-line support is common for AI code assis-\ntants, to the best of our knowledge, we are the first to describe\ncontributions in:\n(1) Using scope-based multi-line algorithm for AI-assisted sug-\ngestions\n(2) Applying general LLM optimizations to improve the re-\nsponsiveness of AI-assisted suggestions in an enterprise\nproduct\n(3) Deploying A/B industrial-scale rollouts to quantify the ef-\nfectiveness of multi-line vs single-line suggestions\n7.1\nScope-Based Multi-Line Algorithm\nWhile many tools provide multi-line suggestions, we did not find\nany discussion of the algorithm that they use to avoid the \u201cjarring\u201d\neffect or to thoughtfully limit the amount of code shown. For ex-\nample, Amazon\u2019s CodeWhisperer [5] can be configured to produce\nmulti-line suggestions, but the suggestions appear to run unlimited\nbeyond the current semantic scope, causing a constant distraction\nfor the user. They also seem to be preferentially triggered in highly\nspecific contexts like a docstring, rather than through an automatic\nsemantic-based algorithm as described in Section 3.\nGoogle\u2019s ML code completion[4] does automatically constrain\ntheir multi-line suggestions based on a semantic filter, but this\nresults in a far more restrictive algorithm compared to our approach.\nThey show both the next token and the full line suggestion in a\ndropdown menu, with the length of the completion dependent on\nthe current token. While the authors show a multi-line completion,\nit is unclear how many additional lines or what other restrictions\nare placed upon the completions, and no multi-line algorithm is\ndescribed.\nIn contract, CodeCompose\u2019s multi-line algorithm automatically\ntriggers as the user types \u2013 while remaining judicious in which\ntrigger points are used, and selectively limiting the generated sug-\ngestion to the user\u2019s current scope. Multi-line suggestions are more\ndifficult to accurately generate, but the scope-based algorithm en-\nables us to display suggestions that fit within the user\u2019s current\nthinking context, thus supporting their train of thought without\ncreating additional noise.\n7.2\nResponsive UX\nWith regards to latency reductions to make the product more re-\nsponsive, many of the techniques we describe are standard in soft-\nware engineering (e.g. early cancellation / streaming), or published\ntechniques for optimizing transformer performance (e.g. Flash\nAttention[12], xFormers[8]). However there is little published re-\nsearch on the impact of deploying these in an industrial setting.\nWhile various external product releases, such as Copilot, Codeium,\nCodeWhisperer [6, 7], mention latency reductions as important\ndrivers for improving the quality of the user experience, they do\nnot offer a rigorous evaluation of latency impact across a suite of\nproduct metrics.\nHence, to the best of our knowledge, we are the first to contribute\nboth an enumeration of the specific algorithms that drive these\nlatency reductions, and a measurement of the impact of these when\nreleased in a production environment.\n7.3\nEvaluating Effectiveness of Multi-Line vs\nSingle-Line Suggestions\nAs described in Section 5, it is not straightforward to determine\nwhether multi-line suggestions are a net improvement to the prod-\nuct experience, as a series of rapid single-line suggestions could\nbe received better by users. To the best of our knowledge, we are\nthe first to show industry metrics from A/B rollouts that demon-\nstrate the trade-off between single-line and multi-line suggestions.\nAs Section 5.1 showed, our experiment metrics provide evidence\nfor the hypothesis that despite latency tradeoffs, longer multi-line\nsuggestions are more impactful to users than a series of shorter\nsingle-line suggestions.\nQuantitative Comparisons. Although a direct comparison\nof quantitative results is impossible between products (given the\ndifferent contexts, users, and tooling), when similar measures are re-\nported, we can compare the overall trends. At Google, engineers [4]\ndeployed a hybrid semantic ML code completion to 10k+ Google em-\nployees (over three months across eight programming languages).\nThey observed a 6% reduction in coding iteration time (time be-\ntween builds and tests) for users exposed to single-line ML comple-\ntion compared to a control group. The authors measured a relatively\nlow keystrokes savings: 0.6% saved from multi-line, compared to\n7% of multi-line keystroke savings in CodeCompose. This differ-\nences indicates either lower display rate or lower number of lines\nin each suggestion generated by Google. The authors mention that\na semantic filtering is applied during post-processing phase \u2013 this\ntype of filtering is not applied in CodeCompose, and may explain a\nlower display rate in Google\u2019s product.\nIn contrast, at Meta, rather than using a semantic filter and\ndropdown menu constraint, CodeCompose completions are shown\ndirectly in the file, and are dependent on the position of the cursor.\nHence, our suggestions automatically trigger in more cursor loca-\ntions, likely leading to a higher reported display rate and overall\nthroughput.\nAmazon\u2019s CodeWhisperer [5] is a fully functional code comple-\ntion tool integrated into the IDE. Amazon\u2019s analysis found that\nlanguage models can generate code with correct syntax and pass\nunit tests in programming languages they are not intentionally\ntrained on. The tool can be configured to produce multi-line sug-\ngestions, but these suggestions appear to trigger in more limited\ncontexts and with significant latency for the user, making it un-\nclear how effective multi-line is. We were unable to find any details\nfrom Amazon on the quantitative metric impact that single-line vs\nmulti-line suggestions have on the overall product.\nThere have been several empirical evaluations of GitHub\u2019s Copi-\nlot [2] in actual use for automatic code completion. Nguyen et al.\n[16] used 33 LeetCode questions to create queries for Copilot in\nfour different programming languages. They found that Copilot\u2019s\nJava suggestions have the highest correctness score (57%) while\nOmer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C. Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachi\nNagappan\nJavaScript is the lowest (27%). Overall, Copilot\u2019s suggestions had low\ncomplexity with no notable differences between the programming\nlanguages. They also observed some potential Copilot shortcom-\nings: such as generating code that can be further simplified and\ncode that relies on undefined helper methods [16].\nQualitative Comparisons. Not all industry usage of code com-\npletions has been positive. A user study [21] with 24 participants to\nunderstand how programmers use and perceive Copilot found that\nwhile it did not necessarily improve the task completion time or\nsuccess rate, most participants preferred to use it in daily program-\nming tasks, since Copilot often provided a useful starting point\nand saved the effort of searching online. However, participants\ndid face difficulties in understanding, editing, and debugging code\nsnippets generated by Copilot, which significantly hindered their\ntask-solving effectiveness [21].\nThese mixed results were also echoed by Bird et al. [10], that\nfound while developer perceived an acceleration in coding, they\noften had to spend much more time reviewing the generated code.\nOn the other hand, some other studies found that generated sug-\ngestions helped in discovering new APIs [9].\nOur prior work on CodeCompose found an overwhelming 91.5%\npositive feedback for single-line completions accelerating the cod-\ning and allowing developers to discover new APIs across the monorepo [15].\nLikewise, the user feedback described in Section 5.2 shows that users\nfound a noticeable improvement in their experience as multi-line\nsuggestions rolled out, indicating success with generating longer\nblocks of boilerplate code and more complex suggestions that help\nwith discovery. Thus our work lays a foundation for the broader\ncommunity to continue investing in this direction with generating\nmulti-line suggestions while maintaining a positive user experience.\n8\nCONCLUSION AND CONTRIBUTION\nIn this paper, we make the following contributions related to the\nchallenges we addressed:\n\u2022 Challenge 1. The Jarring Effect: We developed the fol-\nlowing scope-based algorithm (1) Multi-line suggestions\nare triggered only when the cursor is at the end of scope (2)\nSuggestions are shown until the end of the current block\n(3) After the suggestion is accepted, the cursor needs to be\nmoved to the end of the suggested block.\n\u2022 Challenge 2. Responsive UX: Multi-line suggestions take\nsignificantly longer to generate; hence we had to reduce the\nperceived user latency to ensure user adoption compared\nto single-line suggestions. This was done by (i) adding a UI\nindicator so users were aware of that a multi-line suggestion\nas being generated (ii) running optimizations to the model\nhosting service (e.g. Flash Attention, persistent K-V cache)\n\u2022 Challenge 3. Production Release Effectiveness: During\nthe roll-out of multi-line suggestions, we monitored met-\nrics like acceptance rate, display rate, latency, and through-\nput to evaluate the net benefit of multi-line suggestions\nvs. single-line suggestions. These metrics demonstrated\nthat the investment in multi-line suggestions dispropor-\ntionately increased throughput, as multi-line suggestions\naccounted for 42% of total characters accepted per user\nper day (despite only accounting for at 16% of displayed\nsuggestions), and drove the significant net increase in per-\ncentage of keystrokes saved nearly doubling from 9% to 17%.\nLess than 1% of engineers opted out and disabled multi-line\nsuggestions.\n9\nACKNOWLEDGEMENTS\nWe want to acknowledge and thank the following people for their\nwork, help, and support in building the multi-line CodeCompose\nexperience at Meta: Dian Belanger, Michael Bolin, Renuka Fernan-\ndez, Negar Ghorbani, Kelly Hirano, Diana Hsu, Kristian Kristensen,\nKillian Murphy, Chris Nixon, Zach Rait, Marshall Roch, Shahin\nSefati, and Yiru Zhu.\nREFERENCES\n[1] Meta Accessed 2021.\nELI5: Bento - Interactive Notebook that Em-\npowers Development Collaboration and Best Practices.\nMeta.\nhttps:\n//developers.facebook.com/blog/post/2021/09/20/eli5-bento-interactive-\nnotebook-empowers-development-collaboration-best-practices/\n[2] GitHub Accessed 2021. GitHub Copilot. GitHub. https://github.com/features/\ncopilot\n[3] Microsoft Accessed 2021. Microsoft Intellicode. Microsoft. https://visualstudio.\nmicrosoft.com/services/intellicode\n[4] Google Accessed 2021. ML Enhanced Code Completion. Google.\nhttps://ai.\ngoogleblog.com/2022/07/ml-enhanced-code-completion-improves.html\n[5] Amazon Accessed 2023. Amazon CodeWhisperer. Amazon. https://aws.amazon.\ncom/codewhisperer\n[6] Accessed 2023. Codeium vs Amazon CodeWhisperer.\nhttps://codeium.com/\ncompare/comparison-codewhisperer-codeium\n[7] Accessed 2023. Smarter, more efficient coding: GitHub Copilot goes beyond\nCodex with improved AI model.\nhttps://github.blog/2023-07-28-smarter-\nmore-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-\nmodel/\n[8] GitHub Accessed 2023. xformers. GitHub. https://github.com/facebookresearch/\nxformers/tree/main/xformers\n[9] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. 7, OOPSLA1,\nArticle 78 (apr 2023), 27 pages. https://doi.org/10.1145/3586030\n[10] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini\nKalliamvakou, Travis Lowdermilk, and Idan Gazit. 2023. Taking Flight with\nCopilot: Early insights and opportunities of AI-powered pair-programming tools.\nQueue 20, 6 (jan 2023), 35\u201357. https://doi.org/10.1145/3582083\n[11] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples\nto improve code completion systems. In Proceedings of the 7th joint meeting of\nthe European software engineering conference and the ACM SIGSOFT symposium\non The foundations of software engineering (ESEC/FSE \u201909).\n[12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\narXiv:2205.14135 [cs.LG]\n[13] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A\nGenerative Model for Code Infilling and Synthesis. arXiv:2204.05999 [cs.SE]\n[14] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Pre-\ndiction by Feeding Trees to Transformers. In 2021 IEEE/ACM 43rd International\nConference on Software Engineering (ICSE). 150\u2013162.\nhttps://doi.org/10.1109/\nICSE43902.2021.00026\n[15] Vijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel\nCheng, Negar Ghorbani, Renuka Fernandez, Nachiappan Nagappan, and Pe-\nter C Rigby. 2024. AI-assisted Code Authoring at Scale: Fine-tuning, deploying,\nand mixed methods evaluations. In Proceedings of the Foundations of Software\nEngineering (FSE \u201924).\n[16] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub copilot\u2019s\ncode suggestions. In Proceedings of the 19th International Conference on Mining\nSoftware Repositories (MSR \u201922).\n[17] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2015. Intelligent Code\nCompletion with Bayesian Networks. ACM Transactions on Software Engineering\nand Methodology (TOSEM) 25, 1, Article 3 (12 2015).\n[18] R. Robles and M. Lanza. 2008. How Program History Can Improve Code Com-\npletion. In 2008 23rd IEEE/ACM International Conference on Automated Software\nEngineering. 317\u2013326. https://doi.org/10.1109/ASE.2008.42\n[19] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xi-\naoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom\nMulti-line AI-assisted Code Authoring\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Fer-\nrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\nGabriel Synnaeve. 2023.\nCode Llama: Open Foundation Models for Code.\narXiv:2308.12950 [cs.CL]\n[20] Qianhua Shan, David Sukhdeo, Qianying Huang, Seth Rogers, Lawrence Chen,\nElise Paradis, Peter C. Rigby, and Nachiappan Nagappan. 2022. Using nudges to\naccelerate code reviews at scale (ESEC/FSE 2022). Association for Computing Ma-\nchinery, New York, NY, USA, 472\u2013482. https://doi.org/10.1145/3540250.3549104\n[21] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (CHI EA \u201922).\n[22] Wen Zhou, Seohyun Kim, Vijayaraghavan Murali, and Gareth Ari Aye. 2022.\nImproving Code Autocompletion with Transfer Learning. In 2022 IEEE/ACM\n44th International Conference on Software Engineering: Software Engineering in\nPractice (ICSE-SEIP). 161\u2013162. https://doi.org/10.1145/3510457.3513061\n"
  },
  {
    "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
    "link": "https://arxiv.org/pdf/2402.03766.pdf",
    "upvote": "8",
    "text": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\nXiangxiang Chu1*, Limeng Qiao1*, Xinyu Zhang1*, Shuang Xu1, Fei Wei1, Yang Yang1,3,\nXiaofei Sun1, Yiming Hu,1 Xinyang Lin1, Bo Zhang1, Chunhua Shen2\n1Meituan Inc.\n2Zhejiang University, China\n3Dalian University of Technology, China\nAbstract\nWe introduce MobileVLM V2, a family of significantly\nimproved vision language models upon MobileVLM, which\nproves that a delicate orchestration of novel architectural\ndesign, an improved training scheme tailored for mobile\nVLMs, and rich high-quality dataset curation can substan-\ntially benefit VLMs\u2019 performance. Specifically, MobileVLM\nV2 1.7B achieves better or on-par performance on stan-\ndard VLM benchmarks compared with much larger VLMs\nat the 3B scale.\nNotably, our 3B model outperforms a\nlarge variety of VLMs at the 7B+ scale. Our models will\nbe released at https://github.com/M eituan-\nAutoML/MobileVLM.\n1. Introduction\nTo date, vision language models (VLMs) [4, 46, 58, 75]\nhave been a crucial research focus of the community of ar-\ntificial intelligence [71]. Integrating large language mod-\nels (LLM) with multi-modality features is verified to have\nunprecedented versatile capability, demonstrated by propri-\netary models like Gemini [26], and GPT-4V [53] which\nhave exhibited stunning performance on various down-\nstream tasks [1,26]. However, challenges remain to enable\nan vision language model as a capable intelligence assis-\ntant to be deployed to real scenarios such as mobile devices,\nself-driving cars, and embodied AI systems, etc.\nMost recently, MobileVLM [15] comes first in explor-\ning the capacity of VLMs at the mobile scale with inno-\nvative hardware-oriented architectures. MoE-LLaVA [41]\nmanages to adapt the mixture-of-experts method [22, 30]\nfor VLMs which outrageously pushed the limits of smaller\nmodels to yield surpassing performance compared with\nmuch larger ones. As the latest survey [71] indicates, the\nprogress of VLMs exhibits a trend of extended modali-\nties, refined training pipelines, efficient architectures, and\nhigher-quality training datasets.\nIn this paper, we establish faster and stronger baselines\n*Equal contribution, sorted solely by the alphabetical order of surnames\n15\n20\n25\n30\n35\n40\n45\n50\nSpeed (tokens/s)\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nAvg. Accuracy (%)\nShareGPT4V 7B\nLLaVA-v1.5 7B\nLLaVA-v1.5 3.3B\nLLaVA-v1.5 1.4B\nMobileVLM 3B \nMobileVLM 1.7B\nMobileVLM V2 7B\nMobileVLM V2 3B\nMobileVLM V2 1.7B\nFigure 1. Comparison of SOTA VLMs in terms of average per-\nformance across several standard benchmarks and speed (tested\non an NVIDIA Jeston Orin with llama.cpp). MobileVLM V2\nachieves new state-of-the-art results with much faster inference\nspeed.\nbuilt upon MobileVLM [15]. Our key improvements are\nmade on mainly three aspects, namely, exploiting con-\ntributive training data on small VLMs, exploring effec-\ntive training strategies, and renovating a high-performance\nlightweight projector. Specifically, we utilize 1.2 million\nhigh-quality image-text pairs conducted by ShareGPT4V\n[10] to effectively align vision-language features, and in-\ncorporate more academic tasks to increase data diver-\nsity and instruction-following capacity, such as ScienceQA\n[49], TextVQA [60], SBU [54], etc.. As for the training\nparadigm, we conduct thorough training of all parameters\nof projector and language model during both the pretrain-\ning and instruction tuning stages, which proves advanta-\ngeous in harnessing the full potential of superior-quality\ndata. Additionally, we introduce a more streamlined yet po-\ntent projection mechanism that bridges vision and language\nmodels. By improving the representation of image tokens\nwith enhanced positional information, we can significantly\ncompress the number of image tokens without much perfor-\nmance degradation.\nOur main contributions are summarized as follows:\n1. We explore and evaluate the performance of increasing\n1\narXiv:2402.03766v1  [cs.CV]  6 Feb 2024\ntraining data for small vision language models, which\nsignificantly bridges the gap between small VLMs\nsuch as MobileVLM [15] and large ones.\n2. We dive into better training strategies for mobile sce-\nnarios and design a novel training scheme on how to\nfully exploit the potential of more high-quality multi-\nmodal data. We propose a very lightweight projector\nto significantly reduce the visual tokens with slight per-\nformance drops.\n3. Our method achieves a new state-of-the-art tradeoff\nbetween performance and inference speed across sev-\neral vision language benchmarks.\nBy scaling our\nmodel to 7B parameters, our method outperforms pre-\nvious SOTA models with clear margins.\n2. Related Work\nLarge Language Models.\nOver the last few years, the\nemergence of LLMs has led to significant advancement in\nnatural language processing, including GPT-3 [6], PaLM\n[13], OPT [72], and BLOOM [59].\nA representative\nwork of democratic LLMs is LLaMA, which further en-\nhances the enthusiasm for research on improving LLM ef-\nfects. Following the representative works InstructGPT [55]\nand ChatGPT [51], Alpaca and Vicuna fintuned LLaMA\nwith human-guided annotations to enhance the language\ninteraction abilities.\nRecently, many organizations have\nproposed their large language models [3, 21, 52, 63, 69],\nwhich have been evaluated on multiple benchmarks and\nachieved excellent performance.\nDue to limited compu-\ntational resources of edge devices such as mobile phones,\nautonomous driving cars, and embodied AI systems, sev-\neral lite-LLMs [5,15,39,56,62] have attracted considerable\nattention.\nIt is worth noting that TinyLLaMA [56] con-\nducts great open-source work on their 1.1B base and chat\nmodels. MobileLLaMA [15] scales down the architecture\nof LLaMA and releases 1B and 3B models trained from\nscratch with public datasets. Meantime, model compression\ntechniques including quantization and pruning on large lan-\nguage models is actively studied too [23, 24, 68]. Lately,\nthe precision of LLMs is reduced to W4A8 [38] or even to\nbinary [37] for faster inference with minimal accuracy drop.\nMultimodal Large Language Models.\nSeveral works\n[36, 58, 61] have proposed a series of multimodal model\narchitectures, which are primarily composed of the vision\nencoder and language model. [12, 66] explore visual lan-\nguage reasoning tasks under a unified transformer architec-\nture. The paradigm of the vision-language model demon-\nstrates better generalizability than traditional vision models\ndesigned for specific tasks. With the rapid development of\nLLMs, many works [4,10,46,75] have concentrated on in-\nfusing visual knowledge into LLMs. FROMAGe [32] and\nLLaVA [46] directly feed visual tokens to LLMs, thereby\nenabling the LLM to comprehend semantic information of\nimages and correctly respond to queries for visual con-\ntents. InternLM-XComposer2 [20] proposes a Partial LoRA\n(PLoRA) approach to preserve the integrity of pre-trained\nlanguage knowledge, which strikes a balance between pre-\ncise vision understanding and text composition with liter-\nary talent. Its highly competitive performance shows re-\nmarkable proficiency in the realm of multimodal under-\nstanding. To address the urgent demand for deployment of\nMLLMs on edge devices, several works [15, 26, 41, 67, 76]\nhave been proposed. Gemini [26], a leader in this field,\nhas released lightweight vision-language models Gemini\nNano with 1.8B/3.25B parameters, which are specifically\ndesigned for smartphones. However, their models and data\nare not open-sourced. MobileVLM [15] may be the first\napproach to offer open-source 1B/3B vision-language mod-\nels under resource-constrained scenarios. MoE-LLaVA [41]\nproposes an MoE-based sparse model architecture to en-\nhance visual understanding capabilities. LLaVA-Phi [76]\nutilizes Phi\u20132.7B as a language foundation model to achieve\nsuperior performance in vision language reasoning tasks.\nVary [67] introduces an improved vision vocabulary and\nenhance the image features, leading to better generality.\nNotably, Vary [67] demonstrates superior potential in fine-\ngrained perceptual tasks.\nMultimodal Instruction-Tuning. To address the modality\ngap between vision and language, works such as [58] em-\nploy contrastive learning to align visual and textual feature\nrepresentations.\nFurthermore, methods in [33, 57] lever-\naged the capabilities of large language models to achieve\nuniversal detection or segmentation with strong zero-shot\ncapabilities. Inspired by the success in natural language\nprocessing, Liu et al. [46] introduced visual instruction\ntuning aiming at creating a general-purpose multimodal\nmodel with language as task instructions. SVIT [73] scales\nup visual instruction tuning by constructing a dataset of\n4.2 million visual instruction tuning data, which is fea-\ntured by the high quality and rich diversity generated by\nprompting GPT-4 with the abundant manual annotations of\nimages. ShareGPT4V [10] constructs 100K high-quality\ncaptions from GPT4-Vision for instruction tuning, which\nhave been expanded to 1.2 million detailed and informa-\ntive captions for pre-training, covering world knowledge,\nobject attributes, spatial relationships, and aesthetic assess-\nments. LVIS-instruct4v [65] builds 220K visually aligned\nand context-aware instructions produced by prompting the\npowerful GPT-4V with images from LVIS.\n3. Method\nOur method follows a similar framework as MobileVLM\n[15].\nAs shown in Figure 2, the overall architecture of\nMobileVLM V2 consists of a pre-trained vision encoder to\nextract image features, a pre-trained large language model\n2\nLDPv2\nMobileLLaMA\nTokenizer\nX\ud835\udc97\nX\ud835\udc92\nH\ud835\udc97\nH\ud835\udc92\nY\ud835\udc82\nPoint-wise\nConv \u00d7 2\nAvgPool\nVision \nEncoder\nH\ud835\udc97\nDepth-wise\nConv\n\u2026\n\u2026\n,\n\u2026\n\ud835\udc87\ud835\udc97\n\u2744\n\ud83d\udd25\n\ud83d\udd25\nMultimodal Inputs\nPEG\n\ud835\udc87\ud835\udc97\nFigure 2. MobileVLM V2\u2019s architecture. Xv and Xq indicate\nimage and language instruction, respectively, and Ya refers to the\ntext response from the language model MobileLLaMA. The dia-\ngram in the lower right corner is a detailed description of LDPv2,\ni.e., the lightweight downsample projector v2.\nMobileLLaMA to process multi-modal tokens and gen-\nerate final answers, and a mobile-friendly projector, i.e.,\nlightweight downsample projector (denoted as LDPv2), to\nalign image features with the language model. We introduce\nthe details of each component in the following.\n3.1. Vision Encoder\nFollowing MobileVLM, we use CLIP ViT-L/14 [58] as\nthe vision encoder Fenc, which is contrastively pre-trained\non millions of image-language pairs and proven to be effec-\ntive for VLMs [15]. Particularly, images Xv \u2208 RH\u00d7W \u00d7C\nare first resized to a resolution of 336 \u00d7 336, and flattened\ninto patches with a stride P of 14. High-level visual em-\nbeddings fv \u2208 RNv\u00d7Dv are then extracted to represent the\nsemantic information of images, where Nv = HW/P 2 and\nDv denote the sequence length and the hidden size of visual\nembeddings. Formally we have,\nfv = Fenc(Xv).\n(1)\n3.2. Language Model\nWe employ the MobileLLaMA [15] series as the foun-\ndational large language model (LLM). This choice has three\nadvantages. Firstly, MobileLLaMA is designed to facilitate\noff-the-shelf deployment and has demonstrated real-time\nspeed on resource-limited devices with impressive perfor-\nmance. Keeping the language model unchanged helps us\nconduct controlled experiments to explore many other as-\npects such as scaling up the data corpus, improving training\nstrategy, renovating new designs for projectors, and so on.\nSecondly, MobileLLaMA shares the same tokenizer with\nLLaMA2, which helps to perform distillation without any\npain. Last but not least, it is trained on open datasets and\nhas no risk of evaluation pollution from data leakage. This\nalso helps to confirm whether and how this model can out-\nperform other proprietary counterparts.\nSpecifically, we adopt MobileLLaMA-1.4B-Chat and\nMobileLLaMA-2.7B-Chat. The text input Xq is first to-\nkenized and processed to the text tokens Hq \u2208 RNt\u00d7Dt,\nwhere Nt denotes the sequence length of text tokens and\nDt is the hidden size of the word embedding space. The\ntext tokens Hq and the visual tokens Hv are transformed by\nthe projector, to be concatenated as the input of the language\nmodel. The final response Ya with a length L is generated\nin an autoregressive manner as follows,\np(Ya|Hv, Hq) =\nL\nY\ni=1\np(yi|Hv, Hq, y<i).\n(2)\n3.3. Lightweight Downsample Projector\nInspired by the LDP design of MobileVLM [15], we in-\ntroduce a new projector to perform better vision-language\nfeature alignment with fewer parameters. It contains three\ncomponents, i.e., feature transformation, token reduction,\nand positional information enhancement. First, we employ\ntwo point-wise convolution layers on image tokens to match\nthe feature dimension of LLM. Then, we introduce an av-\nerage pooling layer to extremely compress the number of\nimage tokens. Finally, a very simple but effective module\nPEG [16] with skip connection is applied to enhance po-\nsitional information. Compared with LDP [15], this posi-\ntional part is more efficient and reduces 99.8% number of\nparameters, and is slightly faster in running speed.\nIn formulation, the lightweight downsample projector\nLDPv2 (connoted as P) transforms the visual embedding\nfv \u2208 RNv\u00d7Dv to the modality-aligned visual tokens Hv\nwith positional enhancement. It is worth noting that this\ndesign is also deployment-friendly since it is composed of\nwell-supported operators by mainstream inference frame-\nworks. Given an average kernel k, the number of remaining\ntokens is only 1/k2 of the input features. Concretely, we\nformulate the proposed LDPv2 as follows,\nHv = P(fv) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nf0\n= PW(GELU(PW(fv)))\nf1\n= AvgPool2\u00d72(f0)\nHv\n= DW(f1) + f1.\n(3)\nwhere PW and DW are pointwise and depthwise convolu-\ntions respectively, GELU is a GELU [28] activation layer,\nand AvgPool2\u00d72 is a 2 \u00d7 2 average pooling layer.\n3.4. Training Strategy\nOur training process is split into two stages: pre-training\nand multi-task training. As illustrated in Table 1, unlike\n3\nMethod\nPre-training\nMulti-task training\nV\nP\nL\nV\nP\nL\nLLaVA-1.5-7B [44]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nShareGPT4V-7B [10] \u2713\u2717\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\nMobileVLM [15]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nMobileVLM V2\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\nTable 1. Comparision of training strategy. V, P and L respec-\ntively represent the vision encoder, the projector, and the language\nmodel. \u2713indicates that the corresponding model parameters are\noptimized in training, \u2717 signifies that the parameters are frozen,\nand \u2713\u2717 represents that a subset of model parameters is frozen.\nprevious training paradigm of LLaVA-1.5 [44] and Mo-\nbileVLM [15], MobileVLM V2 opens the projector and\nlarge language model consistently in both stages, with the\nvisual encoder frozen.\n3.4.1\nPre-training\nMost Vision-Language Models (VLMs) [7, 44] commonly\nfreeze the visual encoder and the language model during\npre-training to avoid optimization difficulties. ShareGPT-\n4V [10] partially freezes the visual encoder and makes the\nlanguage model trainable.\nIn our case, we initialize the\nweights of the visual encoder and language model from\nCLIP ViT-L/14 [58] and MobileLLaMA [15], respectively.\nThis initialization serves as a robust foundation for the sub-\nsequent unified training process. We allow for full train-\ning of the projector and the LLM while fixing the vision\nencoder. Note that freezing ViT also reduces the training\ncost. The model\u2019s training objective is then concentrated\non the prediction of the next token, utilizing an autoregres-\nsive loss function. By honing in on this specific task, the\nmodel is better equipped to learn the intricacies of language\ngeneration in the context of visual information, leading to\nimproved performance on multimodal tasks.\nAs reported in Table 2, during the pretraining stage, our\nmodel was trained using the ShareGPT4V-PT dataset [10],\ncomprising 1.2 million image-text pairs. This dataset is in-\nstrumental in enhancing the model\u2019s image-text alignment\ncapabilities, which is a critical aspect of multimodal repre-\nsentation learning.\n3.4.2\nMulti-task Training\nAfter the pre-training phase of image-text alignment\nlearning, MobileVLM V2 has acquired a rudimentary capa-\nbility to comprehend image content. However, it lacks pro-\nficiency in utilizing visual information for analysis and dia-\nlogue in a suite of downstream tasks. Consequently, during\nDatasets\nType\nSamples\nPretraining\nShareGPT4V-PT [10]\nCaption\n1.2M\nMulti-task training\nVisual Dialog [19]\nConversation\n123K\nText-VQA [60]\nVQA(Open)\n35K\nVSR [43]\nVQA(Open)\n13K\nVIGC [64]\nVQA(Open)\n37K\nIConQA [50]\nVQA(MC)\n107K\nSQA [49]\nVQA(MC)\n13K\nCOCO [11]\nCaption\n592K\nSBU [54]\nCaption\n844K\nShareGPT4V [10]\nMixed\n665K\nTotal\n3.6M\nTable 2. Datasets used for MobileVLM V2 training. Column\n\u201cType\u201d is the task of the dataset, where \u201cMixed\u201d indicates that\nthe dataset contains samples belonging to various tasks. Column\n\u201cSamples\u201d is the number of image-text pairs in each dataset.\nthe multi-task training phase, we introduce multiple vision-\nlanguage tasks, engaging parameters in the training process\nto endow the model with the capacity for multi-task analysis\nand image-text conversing.\nIn the multi-task training phase, we employ a plethora\nof datasets featuring a variety of tasks to further develop\nthe model\u2019s array of skills. As outlined in Table 2, these\ndatasets are meticulously chosen to improve conversational\nabilities using the Visual Dialog dataset [19], OCR skills\nthrough the TextVQA dataset [60], scene understanding ca-\npabilities via COCO Caption [11] and SBU [54] datasets,\nand location understanding by the VSR dataset [43] etc.\nNote that we have cleaned and refined the SBU dataset,\nwhere the data volume might not match exactly with the\nofficially released one. In total, the aggregated data for this\nphase consists of 2.4 million samples, ensuring a compre-\nhensive learning experience across different modalities and\ntasks. Examples regarding the dialogue formats across the\nvarious datasets are provided in the Appendix A.\n4. Experiment\n4.1. Training\nPre-training As listed in Table 3, LDPv2 is randomly\ninitialized in this stage, and the language model and the\nvision encoder are initialized with the pre-trained weights\nfrom MobileLLaMA and CLIP ViT-L/14, respectively. For\noptimization, we utilize the AdamW optimizer [48] with\nno weight decay. Maximum learning rates for LDPv2 and\nother components are configured as 1e\u22123 and 2e\u22125, respec-\ntively, following a cosine learning rate schedule. The pre-\ntraining phase involves a global batch size of 256 across 8\n4\nNVIDIA A100 GPUs for about 5 hours.\nMulti-task Training During this training stage, the\nweights of MobileVLM V2 are initialized from the first\nstage. Most training hyper-parameters are similar to the pre-\ntraining phase. Additionally, the learning rate is set to 4e\u22125.\nTraining in this stage requires 8 NVIDIA A100 GPUs for\naround 9 hours with a global batch size of 128. Detailed\ntraining settings are shown in Table 3.\nConfiguration\nPre-training\nMulti-task training\nVision encoder init\nCLIP ViT-L\nCLIP ViT-L\nLLM init\nMobileLLaMA MobileVLM V2 PT\nProjector init\nRandom\nMobileVLM V2 PT\nImage resolution\n3362\n3362\nImage token num\n144\n144\nGlobal batch size\n256\n128\nTraining steps\n5K\n19K\nOptimizer\nAdamW\nAdamW\nLR schedule\nCosine decay\nCosine decay\nProjector LR\n1e\u22123\n4e\u22125\nBase LR\n2e\u22125\n4e\u22125\nWeight decay\n0\n0\nWarm-up ratio\n0.03\n0.03\nDeepSpeed Stage\n2\n3\nTable 3. Training hyperparameters of MobileVLM V2. Note\nthat MobileVLM V2 PT indicates the parameters saved after the\npre-training phase. LR denotes learning rate.\n4.2. Comparisons with State-of-the-art Methods\nFollowing MobileVLM, we adopt a list of benchmarks\nincluding image question answering series GQA [29], SQA\n[49], TextVQA [60], comprehensive benchmarks MME\n[25], MMBench [47], and object hallucination benchmark\nPOPE [40].\nComparison with SOTA methods. We evaluate the per-\nformance of MobileVLM V2 and show the accuracy re-\nsult in Table 4. Note that previous models mainly focus\non accuracy improvement, regardless of the run time la-\ntency. Although our models are targeted for real applica-\ntions regarding two aspects, they outperform most of the\nprevious models with clear margins. With an advantage of\n75% faster speed, MobileVLM V2 3B still outperforms a\nvery recent work MoE-LLaVA-2.7B\u00d74 [41] by 1.4 points\non the average score. Noting MoE-LLaVA-2.7B\u00d74 [41] al-\nready achieves comparable or better accuracy over many\n7B+ VLMs.\nMany of them introduce large amounts of\ntraining costs. In contrast, our method achieves new state-\nof-the-art results while our training cost is comparable to\ncomputation-friendly LLaVA-1.5 [45].\nLatency Comparison. Since many models haven\u2019t been\nsupported by advanced mobile inference frameworks, we\ncompare the latency of a larger spectrum of recent models\nusing the PyTorch framework on an NVIDIA A100 GPU,\nas shown in Figure 3. MobileVLM V2 models are gener-\nally faster and stronger in terms of token generation and av-\nerage scores on tested benchmarks. Especially, MoileVLM\nV2 1B/3B is 37.37 tokens/s and 28.97 tokens/s respectively,\nwhich are both 1.65\u00d7 faster than their counterparts of MoE-\nLLaVA, yet with higher average performance.\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\nSpeed (tokens/s)\n60\n62\n64\n66\n68\nAvg. Accuracy (%)\nMobileVLM 1.7B\nMobileVLM 3B\nMobileVLM V2 1.7B\nMobileVLM V2 3B\nMoE-LLaVA-1.6Bx4\nMoE-LLaVA-2.7Bx4\nFigure 3. Speed comparison of SOTA VLMs on an NVIDIA A100\nGPU. The accuracy is averaged on six VLM benchmarks (see Ta-\nble 4). (tested with a batch size of 1, generating 256 tokens).\nComparison with MoE-LLaVA. MoE-LLaVA utilizes\nseveral experts to achieve good performance, and each ex-\npert is a small model to improve the inference speed. Al-\nthough only a proportion of parameters is activated, it still\nrequires storing the whole parameters, which inevitably in-\ncurs IO overhead in mobile scenarios. Moreover, it\u2019s non-\ntrivial to apply model compressing tricks [14, 24, 37, 74]\nto these models.\nIn contrast, our method can be well-\nsupported and optimized for deployment. In a word, our in-\nference speed advantage evaluated on the Tesla A100 GPU\nwill be further enlarged if tested on resource-limited envi-\nronments. In principle, MobileVLM V2 can also be com-\nbined with its MoE design. But how to combine it with-\nout sacrificing the memory and latency advantage of Mo-\nbileVLM V2 remains as our future work.\nComparison with MobileVLM. Table 4 shows that Mo-\nbileVLM V2 significantly improves the accuracy perfor-\nmance of MobileVLM. The average accuracy is boosted\nby 5.3 points. Since these two methods share the same en-\ncoders both for vision and language modalities, we attribute\nthe improvement to the enhanced data, training strategies,\nand the new design of the projector.\nThis good perfor-\nmance of MobileVLM V2 indicates that MobileLLaMA\nfrom [15] is also a good baseline for small language mod-\nels because it\u2019s built on the open resource dataset Redpa-\njama [17], which is reproducible and has a low risk of eval-\nuation data leakage. Exploring open and more powerful\nsmall language models remains as our future work.\n5\nMethod\nLLM\nRes. GQA SQAI VQAT POPE\nMMEP\nMMBdev Avg.\nIDEFICS-80B [34]\nLLaMA-65B\n224\n45.2\n\u2013\n30.9\n\u2013\n\u2013\n54.5\n\u2013\nBLIP-2 [35]\nVicuna-13B\n224\n41.0\n61.0\n42.5\n85.3\n1293.8\n\u2013\n-\nInstructBLIP [18]\nVicuna-13B\n224\n49.5\n63.1\n50.7\n78.9\n1212.8\n\u2013\n-\nShikra [9]\nVicuna-13B\n224\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n58.8\n\u2013\nOpenflamingo [2]\nMPT-7B\n336\n\u2013\n\u2013\n33.6\n\u2013\n\u2013\n4.6\n\u2013\nQwen-VL [4]\nQwen-7B\n448\n59.3\n67.1\n63.8\n\u2013\n1487.6\n38.2\n-\nmPLUG-Owl [70]\nLLaMA-7B\n224\n\u2013\n\u2013\n\u2013\n\u2013\n967.3\n49.4\n\u2013\nIDEFICS-9B [34]\nLLaMA-7B\n224\n38.4\n\u2013\n25.9\n\u2013\n\u2013\n48.2\n\u2013\nMiniGPT-v2 [8]\nLLaMA-7B\n448\n60.3\n\u2013\n\u2013\n\u2013\n\u2013\n12.2\n\u2013\nMiniGPT-4 [75]\nVicuna-7B\n224\n32.2\n\u2013\n\u2013\n\u2013\n581.7\n23.0\n\u2013\nInstructBLIP [18]\nVicuna-7B\n224\n49.2\n60.5\n50.1\n\u2013\n\u2013\n36.0\n\u2013\nLLaVA-1.5 [44]\nVicuna-7B\n336\n62.0\n66.8\n58.2\n85.9\n1510.7\n64.3\n68.8\nShareGPT4V [10]\nVicuna-7B\n336\n63.3\n68.4\n60.4\n85.7\n1567.4\n68.8\n70.8\nMoE-LLaVA-1.6B\u00d74 [41]\nStableLM-1.6B\n336\n60.4\n62.6\n47.8\n84.3\n1300.8*\n59.4\n63.3\nMoE-LLaVA-2.7B\u00d74 [41]\nPhi-2.7B\n336\n61.1\n68.7\n50.2\n85.0\n1396.4*\n65.5\n66.7\nMobileVLM 1.7B [15]\nMobileLLaMA 1.4B 336\n56.1\n57.3\n41.5\n84.5\n1196.2\n53.2\n58.7\nMobileVLM V2 1.7B\nMobileLLaMA 1.4B 336\n59.3\n66.7\n52.1\n84.3\n1302.8\n57.7\n64.2\nMobileVLM 3B [15]\nMobileLLaMA 2.7B 336\n59.0\n61.2\n47.5\n84.9\n1288.9\n59.6\n62.8\nMobileVLM V2 3B\nMobileLLaMA 2.7B 336\n61.1\n70.0\n57.5\n84.7\n1440.5\n63.2\n68.1\nMobileVLM V2 7B\nVicuna-7B\n336\n62.6\n74.8\n62.3\n85.3\n1560.7\n69.2\n72.1\nMobileVLM V2 7B w/o AvgPool Vicuna-7B\n336\n64.6\n74.8\n66.8\n86.1\n1558.7\n70.8\n73.5\nTable 4. Comparison with SOTA methods on six VLM benchmarks. GQA [29]; SQAI: ScienceQA-IMG [49]; VQAT: TextVQA [60];\nPOPE [40]; MMEP: MME Perception [25]; MMBdev: MMBench-dev [47]; Column Res. is the image resolution of vision model. Column\nAvg. indicates the average accuracy on six evaluation benchmarks. The values in the MMEP column should be divided by 2000 before\nbeing included in the average accuracy calculation. * represents that results not provided in the paper [41] were evaluated using their latest\nrepo (commit 5ba14e8). The value of MobileVLM on SQAI is erroneous in [15] and is corrected in this version.\n4.3. Model Scaling Analysis\n2\n3\n4\n5\n6\n7\nParams (B)\n60\n62\n64\n66\n68\n70\n72\nAvg. Accuracy (%)\nMobileVLM 3B\nMobileVLM 1.7B\nMobileVLM V2 7B\nMobileVLM V2 3B\nMobileVLM V2 1.7B\n+4.00%\n+3.90%\nFigure 4. Average performance improvement on six VLM bench-\nmarks when scaling MobileVLM V2\u2019s models across several stan-\ndard tasks (see also Table 4).\nWhile our target is to design powerful multimodal mod-\nels for resource-constrained scenarios, we further scale up\nour model to verify the upper bound of performance. This\nalso forms an apple-to-apple comparison with many exis-\ntent VLMs. Specifically, we utilize Vicuna-7B as the LLM\nGQA\n ScienceQA\nTextVQA\nPOPE\nMME\n MMBench-dev\n62.0\n63.3\n62.6\n66.8\n68.4\n74.8\n58.2\n60.4\n62.3\n85.9\n85.7\n85.3\n1510.7\n1567.4\n1560.7\n64.3\n68.8\n69.2\n62.0\n63.3\n62.6\n66.8\n68.4\n74.8\n58.2\n60.4\n62.3\n85.9\n85.7\n85.3\n1510.7\n1567.4\n1560.7\n64.3\n68.8\n69.2\n62.0\n63.3\n62.6\n66.8\n68.4\n74.8\n58.2\n60.4\n62.3\n85.9\n85.7\n85.3\n1510.7\n1567.4\n1560.7\n64.3\n68.8\n69.2\n LLAVA-1.5 7B\nShareGPT4V 7B\nMobileVLM V2 7B\nFigure 5. Radar plot of MobileVLM V2\u2019s performance compared\nwith its peers on 6 standard benchmarks.\nmodel and show the result in Figure 4, where a comprehen-\nsive multimodal performance improvement emerges. We\ncompare our MobileVLM V2 7B with mainstream large-\nscale VLMs like LLaVA-1.5 7B [44] and ShareGPT4V 7B\n6\n[10] in both accuracy and the inference speed, which is\nshown in Figure 5 and Table 5. It can be seen that our\nMobileVLM V2 7B not only secures a significant perfor-\nmance edge across multiple benchmarks but also establishes\na clear lead in inference speed over large-scale representa-\ntives. While being nearly 20% faster, MobileVLM V2 7B\noutperforms ShareGPT4V by 1.3 points regarding the aver-\nage performance. This further illustrates the effectiveness\nof our data scaling strategy, training strategy, and the novel\nprojector design.\nGiven the observation that the latency gap is narrowed\nupon the 7B model, we remove the token reduction com-\nponent, i.e., average pooling with a kernel of 2.\nUnder\nthis setting, our 7B model has the same latency speed as\nShareGPT4V. We show the result in the last row of Ta-\nble 4. MobileVLM V2 7B (w/o AvgPool) achieves an aver-\nage score of 73.5, which significantly outperforms LLaVA-\n1.5 by 4.7 points. Compared with MobileVLM V2 7B (with\nAvgPool), the performance gain mainly comes from the in-\ncreased score on the TextVQA task, which is an OCR task.\nThis task contains many small objects where token reduc-\ntion could be harmful. We retain how to effectively make\nuse of high-resolution input in our future work.\n4.4. Latency Measurement on Mobile Devices\nTo maintain consistency, we measure the inference la-\ntency of MobileVLM V2 on the NVIDIA AGX Jetson Orin\nplatform with identical configurations as MobileVLM [15].\nThe llama.cpp [27] framework is employed as the in-\nference framework. Specifically, for the Jetson Orin plat-\nform, we develop more efficient CUDA implementations\nfor LDPv2 (Section 3.3) to fully leverage the hardware to\nhave the best performance. Table 5 reports the comparison\nresults of inference latency. To more objectively evaluate\nthe inference performance, we introduce Evalavg, which\ndenotes the actual generation speed of output tokens, calcu-\nlated by dividing the total time by the number of 256 output\ntokens.\nInference Latency on NVIDIA Jetson Orin. On the\nJetson Orin platform, we conclude that MobileVLM V2\ndemonstrates a lower inference latency than its counterparts\nat the same parameter scale. As shown in Table 5, Mo-\nbileVLM V2 achieves the best performance in the inference\nspeed, which can be attributed to a more lightweight de-\nsign of the projector: we optimize the original 576 tokens\nof visual prompts to 144 ones, while the performance on the\naverage accuracy remains the same or becomes even better.\n5. Ablation Study\n5.1. Effect of Data Scaling\nIn this section, we explore the effect of the data enhance-\nment strategy. As shown in Table 6, the first row represents\nModel\nAvg.\naccuracy\nEvalavg\n(tokens/s) Total (s)\nShareGPT4V-7B\n70.8\n13.00\n19.69\nLLaVA-1.5 7B\n68.8\n12.96\n19.75\nMobileVLM V2 7B\n72.1\n15.49\n16.53\nLLaVA-1.5 3.3B\n62.7\n20.45\n12.52\nMobileVLM 3B\n62.8\n30.80\n8.31\nMobileVLM V2 3B\n68.1\n30.80\n8.38\nLLaVA-1.5 1.4B\n55.7\n43.39\n5.90\nMobileVLM 1.7B\n58.7\n49.80\n5.14\nMobileVLM V2 1.7B\n64.2\n51.63\n4.96\nTable 5. Latency comparison of mobile-scale VLMs on NVIDIA\nJetson Orin. The language model of VLMs is quantized to 4-bit\nwith llama.cpp. The average accuracy is evaluated on the same\nsix benchmarks as Table 4.\nD T GQA SQAI VQAT POPE MMEP MMBdev Avg.\n(a) \u2717 \u2717\n56.1\n57.3\n41.5\n84.5 1196.2\n53.2\n58.7\n(b) \u2713 \u2717\n57.5\n63.9\n49.8\n83.9 1157.5\n51.6\n60.8\n(c) \u2713 \u2713 58.5\n65.4\n50.8\n83.4 1262.6\n55.4\n62.8\nTable 6. Ablation results of Data Strategy and Training Strategy\non MobileVLM 1.7B. \u2018D\u2019 and \u2018T\u2019 represent our Data Strategy and\nTraining strategy, respectively.\nthe baseline of MobileVLM 1.7B. We replace the pretrain-\ning dataset with the ShareGPT4V dataset [10] and expand\nthe instruction tuning dataset to 2.4M, as described in Sec-\ntion 3.4.2. Note that the training strategy and model ar-\nchitecture remain the same as the baseline. Performance\ngains on GQA, SQA, and TextVQA demonstrate that the\nmodel\u2019s ability in cognition and dialogue has been improved\nafter data enhancement. However, we also observe that in\nthe configuration of more high-quality data, maintaining the\noriginal training strategy cannot fully exploit the data ben-\nefits and leads to performance degradation on MME and\nMMBench.\n5.2. Effect of Training Strategy\nBased on the observations from Section 5.1, we further\nexplore a more reasonable training strategy. Under the set-\nting of enhanced data and small-scale VLMs, unlocking the\nlanguage model in the whole training stage enables more\neffective learning. Therefore, we opt for simultaneous fine-\ntuning of the projector as well as the language model. Com-\npared results are shown in the second and third rows in Ta-\nble 6. We can see that this training setup enables a com-\nprehensive improvement in model performance across most\nVLM benchmarks, with a 2-point gain on average accuracy.\n7\nVL Projector Architecture Design\nw/o D&T Tokens\nParams.\nGQA SQAI\nVQAT\nPOPE\nMME\nMMBdev\nAvg.\n[PW]\u00d72\n\u2717\n576\n6.30M\n56.9\n57.1\n43.7\n85.7\n1137.7\n52.8\n58.8\n[PW]\u00d72[DW \u03ba=1PW]\u00d71[DW \u03ba=2PW]\u00d71\n\u2717\n144\n18.94M\n56.1\n57.3\n41.5\n84.5\n1196.2\n53.2\n58.7\n[PW]\u00d72\n\u2713\n576\n6.30M\n59.9\n63.7\n53.9\n85.0\n1271.3\n56.0\n63.7\n[PW]\u00d72[DW \u03ba=1PW]\u00d71[DW \u03ba=2PW]\u00d71\n\u2713\n144\n18.94M\n58.5\n65.4\n50.8\n83.4\n1262.6\n55.4\n62.8\n[PW]\u00d72[AvgPool\u03c1=2]\u00d71\n\u2713\n144\n6.30M\n59.3\n65.0\n53.1\n84.0\n1292.2\n54.5\n63.2\n[PW]\u00d72[AvgPool\u03c1=2]\u00d71[LearnablePE]\u00d71\n\u2713\n144\n6.59M\n59.1\n67.1\n52.3\n84.3\n1286.7\n55.5\n63.7\n[PW]\u00d72[AvgPool\u03c1=2]\u00d71[DW \u03ba=1]\u00d71\n\u2713\n144\n6.32M\n59.3\n66.7\n52.1\n84.3\n1302.8\n57.7\n64.2\nTable 7. The exploration of projector design based on MobileLLaMA 1.4B. The PW represents pointwise-conv and DW is depthwise-\nconv. The subscript \u00d7 indicates the number of times the corresponding module is stacked repeatedly. The superscript \u03ba indicates the\n3 \u00d7 3 conv stride and \u03c1 indicates the pooling kernel size. Note that D represents our proposed Data Strategy and T for Training Strategy\nin MobileVLM V2. The green row is the proposed LDPv2 in our MobileVLM V2.\n5.3. Effect of Projector\nLooking back at MobileVLM, the proposed LDPv1 re-\nduces the number of tokens by 75% (576 \u2192 144) while\nmaintaining almost equivalent performance, see the first and\nsecond rows in Table 7. Based on the newly proposed data\nconfiguration and training strategy, the above corresponding\narchitectures can achieve 5.4 and 4.5 average improvements\nrespectively (the third and fourth rows). However, when\nwe try to continue to explore better alignment methods of\nvisual and language features, we observe that increasing\nthe number of learnable parameters can easily lead to op-\ntimization dilemmas, resulting in a decline in the overall\nalignment effect. Based on this observation, we firstly re-\nplace the [DW \u03baPW] block in LDPv1 with an embarrass-\ningly simple parameter-free operation, i.e., 2 \u00d7 2 average\npooling, and find that we could obtain an average perfor-\nmance improvement of 0.4 points (see row 5 in Table 7).\nSecondly, to make the aligned visual features with stronger\npositional information, the simplest idea is to employ learn-\nable positional encoding for enhancement, which brings 0.5\nperformance gains in the 6th row. Further, inspired by the\nclaims of PEG [16], whose position encoding is dynami-\ncally generated and conditioned on the local neighborhood\nof the input tokens, we then replace the learnable PE with\na PEG layer to obtain better-enhanced features. The sev-\nenth row in Table 7 indicates an improvement of 0.5 points.\nIt is worth mentioning that, a PEG layer applied in Mo-\nbileVLM V2 1.7B only contains 0.02M (2048 \u00d7 3 \u00d7 3)\nparameters. Compared with the [DW \u03baPW] block in Mo-\nbileVLM, the number of learnable parameters has dropped\nby nearly 630\u00d7 (12.64M \u2192 0.02M), but the overall per-\nformance is improved by 1.4 points, which demonstrates the\neffectiveness of our design.\n6. Conclusion\nIn a nutshell, we propose a series of efficient vision\nlanguage models called MobileVLM V2 based on Mo-\nbileVLM [15]. We explore data scaling schemes, improved\ntraining strategies, and efficient modality alignment design\nto improve the overall performance under the setting of\nsmall VLM models. With a comparable training cost as\n[44], our method achieves a new state-of-the-art Pareto front\nin terms of accuracy and latency, targeted for real product\nenvironments. Our model outperforms many larger mod-\nels with substantial inference advantages, which paves a\npromising way to enjoy advanced AI on resource-limited\nscenarios.\nAcknowledgements:\nThis work was in part supported by\nNational Key R&D Program of China (No. 2022ZD0118-\n700).\nReferences\n[1] Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue\nOu, Alex B\u00a8auerle, \u00b4Angel Alexander Cabrera, Krish Dho-\nlakia, Chenyan Xiong, and Graham Neubig.\nAn in-\ndepth look at gemini\u2019s language abilities.\narXiv preprint\narXiv:2312.11444, 2023. 1\n[2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bit-\nton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei\nKoh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig\nSchmidt. Openflamingo, Mar. 2023. 6\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.\nQwen technical report.\narXiv preprint\narXiv:2309.16609, 2023. 2\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 2023. 1, 2, 6\n[5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory An-\nthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. Pythia: A suite for analyz-\n8\ning large language models across training and scaling. In In-\nternational Conference on Machine Learning, pages 2397\u2013\n2430. PMLR, 2023. 2\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 2\n[7] Junbum Cha,\nWooyoung Kang,\nJonghwan Mun,\nand\nByungseok Roh. Honeybee: Locality-enhanced projector for\nmultimodal llm. arXiv preprint arXiv:2312.06742, 2023. 4\n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan\nZhang,\nRaghuraman\nKrishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning.\narXiv preprint\narXiv:2310.09478, 2023. 6\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 6\n[10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 1, 2, 4, 6, 7\n[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zit-\nnick. Microsoft coco captions: Data collection and evalu-\nation server, 2015. 4\n[12] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying\nvision-and-language tasks via text generation. In Interna-\ntional Conference on Machine Learning, pages 1931\u20131942.\nPMLR, 2021. 2\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2\n[14] Xiangxiang Chu, Liang Li, and Bo Zhang.\nMake repvgg\ngreater again: A quantization-aware approach.\nIn AAAI,\n2024. 5\n[15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\nXiaolin Wei, et al.\nMobilevlm: A fast, reproducible and\nstrong vision language assistant for mobile devices. arXiv\npreprint arXiv:2312.16886, 2023. 1, 2, 3, 4, 5, 6, 7, 8\n[16] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and\nChunhua Shen. Conditional positional encodings for vision\ntransformers. In The Eleventh International Conference on\nLearning Representations, 2023. 3, 8\n[17] Together Computer. Redpajama: An open source recipe to\nreproduce llama training dataset, 2023. 5\n[18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 6\n[19] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos\u00b4e MF Moura, Devi Parikh, and Dhruv Ba-\ntra. Visual dialog. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 326\u2013335,\n2017. 4\n[20] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin\nWang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\nDuan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan,\nYang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen,\nConghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Ji-\naqi Wang. Internlm-xcomposer2: Mastering free-form text-\nimage composition and comprehension in vision-language\nlarge model. arXiv preprint arXiv:2401.16420, 2024. 2\n[21] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang.\nGlm: General language\nmodel pretraining with autoregressive blank infilling. In Pro-\nceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n320\u2013335, 2022. 2\n[22] David Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever.\nLearning factored representations in a deep mixture of ex-\nperts. 2013. 1\n[23] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language\nmodels can be accurately pruned in one-shot, 2023. 2\n[24] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh.\nGptq:\nAccurate post-training quantization\nfor generative pre-trained transformers.\narXiv preprint\narXiv:2210.17323, 2022. 2, 5\n[25] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 5, 6\n[26] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen,\nZhang Mengdan, Peixian Chen, Sirui Zhao, Shaohui Lin,\nDeqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li,\nand Xing Sun. A challenger to gpt-4v? early explorations of\ngemini in visual expertise. arXiv preprint arXiv:2312.12436,\n2023. 1, 2\n[27] Georgi Gerganov. llama.cpp. https://github.com/g\ngerganov/llama.cpp. [Accessed: 2023-11-07]. 7\n[28] Dan Hendrycks and Kevin Gimpel.\nGaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 3\n[29] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700\u20136709, 2019. 5, 6\n[30] Robert A Jacobs, Michael I Jordan, Stuart J Nowlan, and Ge-\noffrey E Hinton. Adaptive mixtures of local experts. Neural\nComputation, 3(1):79\u201387, 1991. 1\n[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. Accessed:\n2023-03-01. 12\n9\n[32] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal gen-\neration. arXiv preprint arXiv:2301.13823, 2023. 2\n[33] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 2\n[34] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelisc: An open web-scale filtered dataset of interleaved\nimage-text documents.\narXiv preprint arXiv:2306.16527,\n2023. 6\n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 6\n[36] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in neural infor-\nmation processing systems, 34:9694\u20139705, 2021. 2\n[37] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu.\nNorm tweaking: High-performance low-bit quantization of\nlarge language models. In AAAI, 2024. 2, 5\n[38] Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yi-\nfan Lu, Xiangxiang Chu, Yerui Sun, and Yuchen Xie.\nA\nspeed odyssey for deployable quantization of llms.\narXiv\npreprint arXiv:2311.09550, 2023. 2\n[39] Yuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del\nGiorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are\nall you need ii: phi-1.5 technical report, 2023. 2\n[40] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 5, 6\n[41] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng\nJin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava:\nMixture of experts for large vision-language models, 2024.\n1, 2, 5, 6\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEur. Conf. Comput. Vis., pages 740\u2013755. Springer, 2014. 12\n[43] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier.\nVisual spatial reasoning. Transactions of the Association for\nComputational Linguistics, 2023. 4\n[44] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 4, 6, 8, 12\n[45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 5\n[46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 2\n[47] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n5, 6\n[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 4\n[49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering. In Advances\nin Neural Information Processing Systems, pages 27730\u2013\n27744, 2022. 1, 4, 5, 6\n[50] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.\nIconqa:\nA new benchmark for abstract diagram under-\nstanding and visual language reasoning.\narXiv preprint\narXiv:2110.13214, 2021. 4\n[51] OpenAI.\nChatGPT.\nhttps://openai.com/blog/ChatGPT/,\n2023. Online; accessed 2023-01-01. 2\n[52] OpenAI. Gpt-4 technical report. 2023. Technical Report. 2\n[53] OpenAI. Gpt-4v(ision) system card. 2023. 1\n[54] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In Neural Information Processing Systems (NIPS),\n2011. 1, 4\n[55] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al.\nTraining\nlanguage models to follow instructions with human feed-\nback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022. 2\n[56] Tianduo Wang Peiyuan Zhang, Guangtao Zeng and Wei Lu.\nTinyllama, Sep 2023. 2\n[57] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,\nJipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Ling-\npeng Kong Tong Zhang. Detgpt: Detect what you need via\nreasoning. arXiv preprint arXiv:2305.14167, 2023. 2\n[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2, 3, 4\n[59] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00b4e,\nAlexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022. 2\n[60] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 1, 4, 5, 6\n[61] Hao Tan and Mohit Bansal.\nLxmert:\nLearning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 2\n10\n[62] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-\nton, Viktor Kerkez, and Robert Stojnic. Galactica: A large\nlanguage model for science. 2022. 2\n[63] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://gith\nub.com/InternLM/InternLM, 2023. 2\n[64] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,\nPan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,\net al.\nVigc: Visual instruction generation and correction.\narXiv preprint arXiv:2308.12714, 2023. 4\n[65] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning.\narXiv preprint\narXiv:2311.07574, 2023. 2\n[66] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022. 2\n[67] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng\nGe, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu\nZhang.\nVary: Scaling up the vision vocabulary for large\nvision-language models. arXiv preprint arXiv:2312.06109,\n2023. 2\n[68] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien\nDemouth, and Song Han. Smoothquant: Accurate and effi-\ncient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages\n38087\u201338099. PMLR, 2023. 2\n[69] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce\nBian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan,\net al. Baichuan 2: Open large-scale language models. arXiv\npreprint arXiv:2309.10305, 2023. 2\n[70] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 6\n[71] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan\nSu, Chenhui Chu, and Dong Yu.\nMm-llms: Recent ad-\nvances in multimodal large language models. arXiv preprint\narXiv:2401.13601, 2024. 1\n[72] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Christopher Dewan, Mona Diab, Xian Li,\nXi Victoria Lin, et al. OPT: Open pre-trained transformer\nlanguage models. 2022. 2\n[73] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up\nvisual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023. 2\n[74] Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng\nBai, Miao Sun, Ziyu Zhao, Xiaobo Lu, and Xiangxiang Chu.\nLidar-ptq:post-training quantization for point cloud 3d object\ndetection. International Conference on Learning Represen-\ntations (ICLR 2024), 2024. 5\n[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2, 6\n[76] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng\nMou, and Jian Tang.\nLlava-\u03d5:\nEfficient multi-modal\nassistant with small language model.\narXiv preprint\narXiv:2401.02330, 2024. 2\n11\nA. Dialogue formats of various datasets.\nDuring the pre-training phase, we utilized the 1.2\nmillion image-text pairs from the pre-training phase\nof ShareGPT4V, which primarily includes COCO [42],\nSAM [31], and LLaVA-1.5 pre-training data [44]. Subse-\nquently, in the multi-task training phase, we collected 2.4\nmillion high-quality instruction data. A detailed format of\nthese data is provided in Table 8 and Table 9.\nB. Examples of MobileVLM V2.\nFigure 6 shows qualitative results of MobileVLM V2\n1.7B in multimodal conversations in various scenarios in-\ncluding of fine-grained attribute understanding, image un-\nderstanding and relation reasoning, attributes and spatial re-\nlationships understanding, OCR and world knowledge, etc.\n12\nDataset\nImage\nDialogue Content\nVIGC\nUser: <image>\\nWhat is the person in the image doing?\nAssistant: The person in the image is skiing down a snow-\ncovered slope.\nIConQA\nUser: <image>\\nSelect the picture that shows equal parts.A.\nFirst image.\\nB. Second image.\\nC. Third image.\\nAnswer\nwith the option\u2019s letter from the given choices directly.\nAssistant: C.\nTextVQA\nUser:<image>\\nWhat brand name is on the box?\\n Refer-\nence OCR token: Cergo, AUDREY, is, all, you, need, 3com,\nThis \\nAnswer the question using a single word or phrase.\nAssistant: 3com.\nCOCO Caption\nUser: <image>\\nWhat are the key elements in this picture?\nAssistant: A bicycle replica with a clock as the front wheel.\nUser: <image>\\nWhat is this photo about?\nAssistant: A room with blue walls and a white sink and door.\nTable 8. Examples from the multi-task training data for MobileVLM V2 (Part 1).\n13\nDataset\nImage\nDialogue Content\nVisual Dialog\nUser: <image>\\nHow many bikes there?\nAssistant: 3.\nUser: What color are bikes?\nAssistant: I see green red and white.\nUser: Are they parked on stock parking?\nAssistant: NO.\nUser: What color of their helmets?\nAssistant: green and white and other is black.\nSBU\nUser: <image>\\nCan you elaborate on the elements of the pic-\nture provided?\nAssistant: A wooden chair in the living room.\nSQA\nUser:<image>\\nWhich of these states is farthest east?\n\\nContext:N/A\\nOptions:(A) Montana (B) New Mexico (C)\nArkansas (D) Nebraska\\n\nAssistant: Maps have four cardinal directions, or main direc-\ntions. Those directions are north, south, east, and west.\\nA com-\npass rose is a set of arrows that point to the cardinal directions.\nA compass rose usually shows only the first letter of each car-\ndinal direction.\\nThe north arrow points to the North Pole. On\nmost maps, the north is at the top of the map. To find the answer,\nlook at the compass rose. Look at which way the east arrow is\npointing. Arkansas is the farthest east. The answer is C.\nVSR\nUser: <image>\\nIs the pizza above the couch?\\nAnswer the\nquestion using a single word or phrase.\nAssistant: No.\nTable 9. Examples from the multi-task training data for MobileVLM V2 (Part 2).\n14\n<image> What is this photo about?\nA man and a boy fishing on a lake.\nHow many person there?\nThere are 2 people in the image.\nUser\nMobileVLM V2\nUser\nMobileVLM V2\nWhat might be the relationship between these people?\nBased on the image, it is likely that the man and the boy are father and son. They are \nsitting together on a log, fishing in a body of water, and the father is holding a fishing \nrod. This suggests that they share a close bond and enjoy spending time together \noutdoors, engaging in a recreational activity like fishing. The father's presence and the \nfather-son relationship are further supported by the fact that they are fishing together, \nwhich is a common bonding activity for parents and their children.\nUser\nMobileVLM V2\n<image> What's her mood like?\nThe young woman in the image is smiling, which suggests that she is in a positive and \ncheerful mood.\nWhat color sweater is she wearing?\nShe is wearing a red sweater.\nUser\nMobileVLM V2\nUser\nMobileVLM V2\n<image> What are they doing?\nA baby is playing with a teddy bear.\nWhat is on the right of this image?\nThere is a teddy bear on the right side of the image.\nUser\nMobileVLM V2\nUser\nMobileVLM V2\n<image> Describe this image in detail.\nThe image captures a charming storefront, nestled in a quaint street corner. The store's \nname, \"The Kitchen Store\", is prominently displayed in white letters above the door, \ninviting passersby to explore its offerings. Above the door, a wreath of white and green \nberries adds a touch of nature to the scene. The store is adorned with a variety of items, \nincluding a sign that reads \"The Root Kitchen Store\", suggesting a range of culinary \ndelights within. The window display showcases an array of items, including a vase and \na bowl, hinting at the store's diverse range of products. The overall scene paints a \npicture of a cozy and inviting store, ready to welcome customers into its warm and \nwelcoming interior.\nUser\nMobileVLM V2\nImage understanding and relation reasoning\nFine-grained attribute understanding\nAttributes and spatial relationships understanding\nOCR and world knowledge\nFigure 6. Examples of MobileVLM V2 1.7B in various scenes.\n15\n"
  },
  {
    "title": "Diffusion World Model",
    "link": "https://arxiv.org/pdf/2402.03570.pdf",
    "upvote": "7",
    "text": "Diffusion World Model\nZihan Ding 1 2 Amy Zhang 1 3 Yuandong Tian 1 Qinqing Zheng 1\nAbstract\nWe introduce Diffusion World Model (DWM), a\nconditional diffusion model capable of predicting\nmultistep future states and rewards concurrently.\nAs opposed to traditional one-step dynamics mod-\nels, DWM offers long-horizon predictions in a\nsingle forward pass, eliminating the need for re-\ncursive queries. We integrate DWM into model-\nbased value estimation (Feinberg et al., 2018),\nwhere the short-term return is simulated by fu-\nture trajectories sampled from DWM. In the con-\ntext of offline reinforcement learning, DWM can\nbe viewed as a conservative value regularization\nthrough generative modeling. Alternatively, it can\nbe seen as a data source that enables offline Q-\nlearning with synthetic data. Our experiments on\nthe D4RL (Fu et al., 2020) dataset confirm the\nrobustness of DWM to long-horizon simulation.\nIn terms of absolute performance, DWM signifi-\ncantly surpasses one-step dynamics models with\na 44% performance gain, and achieves state-of-\nthe-art performance.\n1\nIntroduction\nReinforcement learning (RL) algorithms can be broadly\nclassified into two classes: model-based (MB) and model-\nfree (MF) algorithms. The foundation of MB algorithms is a\npredictive model of environment feedback, often referred to\nas a world model (Ha & Schmidhuber, 2018). World models\nserve as simulators of real environments, and policies can be\nderived from them through action searching (Schrittwieser\net al., 2020; Ye et al., 2021), policy optimization within\nsuch simulators (Sutton, 1991; Dean et al., 2020; Feinberg\net al., 2018; Hafner et al., 2019a), or a combination of\nboth (Hansen et al., 2022b; 2023; Chitnis et al., 2023).\nThe prediction accuracy of world models is critical to the per-\nformance of MB approaches. In practice, due to inevitable\nmodeling error, MB methods typically exhibit worse perfor-\n1FAIR,\nMeta\n2Princeton\nUniversity\n3University\nof\nTexas,\nAustin.\nCorrespondence to:\nZihan Ding\n<zi-\nhand@princeton.edu>.\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nSimulation Horizon\n0.2\n0.4\n0.6\n0.8\nReturn (Mean \u00b1 Std)\nReturn v.s. Simulation Horizon (walker2d-medium-v2)\nOne-step Dynamics Model\nDiffusion World Model\nFigure 1.1: The return of TD3+BC trained using diffusion\nworld model and one-step dynamics model.\nmance compared with their MF counterparts, which directly\nlearn a policy from interacting with the true environment.\nNonetheless, MB methods come with an advantage of sam-\nple efficiency (Deisenroth et al., 2013; Dean et al., 2020).\nThey usually achieve decent performance with much fewer\nenvironment interactions, thus more suitable for dealing\nwith practical real-world problems. This highlights one of\nthe most fundamental problems in the area of model-based\nRL: how can we effectively reduce the world modeling er-\nror?\nTraditional world models are one-step dynamics models\npredicting reward and next state based on the current state\nand action (Kaiser et al., 2019; Janner et al., 2019; Yu et al.,\n2020; Kidambi et al., 2020; Hansen et al., 2022a; Hafner\net al., 2019a; 2020; 2023). When planning for multiple\nsteps into the future, these models are recursively invoked,\nleading to a rapid accumulation of errors and unreliable\npredictions for long-horizon rollouts. Figure 1.1 plots the\nperformance of an MB approach with one-step dynamics\nmodel. The return quickly collapses as the rollout length\nincreases, highlighting the issue of compounding errors for\nsuch models (Asadi et al., 2019; Lambert et al., 2022; Xiao\net al., 2019). Recently, there has been growing interest of\nutilizing sequence modeling techniques to solve decision\nmaking problems, as seen in various studies (Chen et al.,\n2021; Janner et al., 2021; 2022; Zheng et al., 2022; Ajay\net al., 2022; Micheli et al., 2022; Robine et al., 2023; Zheng\net al., 2023b). This raises an intriguing question that our\npaper seeks to answer:\nCan sequence modeling tools effectively reduce the error\nin long-horizon prediction and improve the performance of\n1\narXiv:2402.03570v2  [cs.LG]  11 Feb 2024\nDiffusion World Model\nMBRL algorithms?\nWe introduce Diffusion World Model (DWM), a diffusion\nprobabilistic model designed for predicting long-horizon\noutcomes.\nConditioning on current state, action, and\nexpected return, diffusion world model simultaneously\npredicts multistep future states and rewards, eliminating\nthe source of error accumulation as there is no recursive\nquerying of the would model. As shown in Figure 1.1,\ndiffusion world model is robust to long-horizon simulation,\nwhere the performance does not deteriorate even with\nsimulation horizon 31. See Section 4 for more experiments.\nIn this paper, we particularly consider the offline RL setup,\nwhere the objective is to learn a policy from a static dataset\nwithout online interactions. The detachment from online\ntraining circumvents the side effects of exploration and al-\nlows us to investigate the quality of world models thor-\noughly. We propose a generic Dyna-type (Sutton, 1991)\nmodel-based framework. In brief, we first train a diffusion\nworld model using the offline dataset, then train a policy\nusing imagined data generated by the diffusion world model,\nin an actor-critic manner. Particularly, to generate the target\nvalue for training the critic, we introduce Diffusion Model\nValue Expansion (Diffusion-MVE) that uses diffusion world\nmodel generated future trajectories to simulate the return up\nto a chosen horizon. As we will elaborate later, Diffusion-\nMVE can be interpreted as a value regularization for offline\nRL through generative modeling, or alternatively, a way to\nconduct offline Q-learning with synthetic data.\nOur framework is flexible to carry any MF actor-critic RL\nmethod of choice, and the output policy is efficient at in-\nference time, as the world model does not intervene with\naction generation.\nWe benchmark diffusion-based and traditional one-step\nworld models on 9 locomotion tasks from the D4RL bench-\nmark (Fu et al., 2020). We further consider a variant of our\napproach where the diffusion model is substituted with a\nTransformer architecture (Vaswani et al., 2017). All these\ntasks are in continuous action and observation spaces. Our\nresults confirm that both sequence-level world models out-\nperform one-step models, where diffusion world model\nachieves a 44% performance gain over one-step models.\nMoreover, recent advances in offline RL methods have\nbeen concentrated on MF algorithms (Kumar et al., 2020;\nKostrikov et al., 2021; Wang et al., 2022; Garg et al., 2023;\nDing & Jin, 2023), where a few works have blurred the\nboundary between MB and MF methods (Chen et al., 2021;\nJanner et al., 2021; 2022; Ajay et al., 2022; Zheng et al.,\n2023b). Our method achieves state-of-the-art (SOTA) per-\nformance, eliminating the gap between MB and MF algo-\nrithms.\n2\nPreliminaries\nOffline RL. We consider an infinite-horizon Markov de-\ncision process (MDP) defined by pS, A, R, P, p0, \u03b3q, where\nS is the state space, A is the action space. Let \u2206pSq be the\nprobability simplex of the state space. R : S \u02c6 A \u00de\u00d1 R is a\ndeterministic reward function, P : S \u02c6 A \u00de\u00d1 \u2206pSq defines\nthe probability distribution of transition, p0 : S \u00de\u00d1 \u2206pSq\ndefines the distribution of initial state s0, and \u03b3 P p0, 1q\nis the discount function.\nThe task of RL is to learn a\npolicy \u03c0 : S \u00de\u00d1 A that maximizes its return Jp\u03c0q \u201c\nEs0\u201ep0psq,at\u201e\u03c0p\u00a8|stq,st`1\u201eP p\u00a8|st,atq\n\u201c\u01598\nt\u201c0 \u03b3tRpst, atq\n\u2030\n.\nGiven a trajectory \u03c4\n\u201c\n\u2423\ns0, a0, r0, . . . , s|\u03c4|, a|\u03c4|, r|\u03c4|\n(\n,\nwhere |\u03c4| is the total number of timesteps, the return-to-go\n(RTG) at timestep t is gt \u201c \u0159|\u03c4|\nt1\u201ct \u03b3t1\u00b4trt1. In offline RL,\nwe are constrained to learn a policy solely from a static\ndataset generated by certain unknown policies. Throughout\nthis paper, we use Doffline to denote the offline data\ndistribution and use Doffline to denote the offline dataset.\nDiffusion Model. Diffusion probabilistic models (Sohl-\nDickstein et al., 2015; Ho et al., 2020; Song et al., 2020)\nare generative models that create samples from noises by an\niterative denoising process. It defines a fixed Markov chain,\ncalled the forward or diffusion process, that iteratively adds\nGaussian noise to xpkq starting from a data point xp0q:\nqpxpk`1q|xpkqq \u201c N\n`?1 \u00b4 \u03b2kxpkq, \u03b2kI\n\u02d8\n, 0 \u010f k \u010f K \u00b4 1. (1)\nAs the number of diffusion steps K \u00d1 8, xpKq essen-\ntially becomes a random noise. We learn the corresponding\nreverse process that transforms random noise to data point:\np\u03b8pxpk\u00b41q|xpkqq \u201c N\n`\n\u00b5\u03b8pxpkqq, \u03a3\u03b8pxpkqq\n\u02d8\n, 1 \u010f k \u010f K. (2)\nSampling from a diffusion model amounts to first sampling\na random noise xpKq \u201e Np0, Iq then running the reverse\nprocess. To learn the reverse process, we optimize the vari-\national lower bound of the marginal likelihood p\u03b8pxp0q:pKqq.\nThere are multiple equivalent ways to optimize the lower\nbound (Kingma et al., 2021), and we take the noise predic-\ntion route as follows. One can rewrite xpkq \u201c ?s\u03b1kxp0q `\n?1 \u00b4 s\u03b1k\u03b5, where s\u03b1k \u201c \u015bK\nk1\u201c1p1 \u00b4 \u03b2k1q, and \u03b5 \u201e Np0, Iq\nis the noise injected for xpkq (before scaling). We then pa-\nrameterize a neural network \u03b5\u03b8pxpkq, kq to predict \u03b5 injected\nfor xpkq. Moreover, a conditional variable y can be easily\nadded into both processes via formulating qpxpk`1q|xpkq, yq\nand p\u03b8pxpk\u00b41q|xpkq, yq, respectively. We further deploy\nclassifier-free guidance (Ho & Salimans, 2022) to promote\nthe conditional information, which essentially learns both\nconditioned and unconditioned noise predictors. More pre-\ncisely, we optimize the following loss function:\nEpxp0q,yq,k,\u03b5,b\n\u203a\u203a\u203a\u03b5\u03b8\n\u00b4\nxpkqpxp0q, \u03b5q, k, p1 \u00b4 bq \u00a8 y ` b \u00a8 \u2205\n\u00af\n\u00b4 \u03b5\n\u203a\u203a\u203a\n2\n2, (3)\nwhere xp0q and y are the true data point and conditional\ninformation sampled from data distribution, \u03b5 \u201e Np0, Iq is\n2\nDiffusion World Model\nthe injected noise, k is the diffusion step sampled uniformly\nbetween 1 and K, b \u201e Bernoullippuncondq is used to indi-\ncate whether we will use null condition, and finally, xpkq \u201c\n?s\u03b1kx0 ` ?1 \u00b4 s\u03b1k\u03b5. Algorithm A.1 details how to sample\nfrom a guided diffusion model. In section 3, we shall intro-\nduce the form of xp0q and y in the context of offline RL, and\ndiscuss how we utilize diffusion models to ease planning.\n3\nDiffusion World Model\nIn this section, we introduce a general recipe for model-\nbased offline RL with diffusion world model.\nOur\nframework consists of two training stages, which we will\ndetail in Section 3.1 and 3.2, respectively. In the first stage,\nwe train a diffusion model to predict a sequence of future\nstates and rewards, conditioning on the current state, action\nand target return. Next, we train an offline policy using\nan actor-critic method, where we utilize the pretrained\ndiffusion model for model-based value estimation. Algo-\nrithm 3.1 presents this framework with a simple actor-critic\nalgorithm with delayed updates, where we assume a\ndeterministic offline policy. Our framework can be easily\nextended in a variety of ways. First, we can generalize it\nto account for stochastic policies. Moreover, the actor-critic\nalgorithm we present is of the simplest form. It can be\nextended to combine with various existing offline learning\nalgorithms. In Section 4, we discuss three instantiations of\nAlgorithm 3.1, which embeds TD3+BC (Fujimoto & Gu,\n2021), IQL (Kostrikov et al., 2021), and Q-learning with\npessimistic reward (Yu et al., 2020), respectively.\n3.1\nConditional Diffusion Model\nWe train a return-conditioned diffusion model p\u03b8 on length-\nT subtrajectories, where the conditioning variable is the\nRTG of a subtrajectory.\nThat is, y \u201c gt and xp0q \u201c\npst, at, rt, st`1, rt`1, . . . , st`T \u00b41, rt`T \u00b41q. As introduced\nin Section 2, we employ classifier-free guidance to promote\nthe role of RTG. Stage 1 of Algorithm 3.1 describes the train-\ning procedure in detail. For the actual usage of the trained\ndiffusion model in the second stage of our pipeline, we pre-\ndict future T \u00b4 1 states and rewards based on a target RTG\ngeval and also current state st and action at. These predicted\nstates and rewards are used to facilitate the value estimation\nin policy training, see Section 3.2. As the future actions are\nnot needed, we do not model them in our world model.\nTo enable the conditioning of st and at, we slightly adjust\nthe standard sampling procedure (Algorithm A.1), where we\nfix st and at for every denoising step in the reverse process,\nsee Algorithm A.2.\n3.2\nModel-Based RL with Diffusion World Model\nSection 2 introduces a range of usages of the world model.\nWe are particularly interested in the data augmentation strat-\nAlgorithm 3.1: A General Actor-Critic Framework for\nOffline Model-Based RL with Diffusion World Model\n// Stage 1:\nWorld Model Training\n1 Hyperparameters: number of diffusion steps K, null\nconditioning probability puncond, noise parameters s\u03b1k\n2 while not converged do\n3\nSample length-T subtrajectory\nxp0q \u201c pst, at, rt, st`1, rt`1, . . . , st`T \u00b41, rt`T \u00b41q\nfrom Doffline\n4\nCompute RTG gt \u00d0 \u0159T \u00b41\nh\u201c0 \u03b3hrt`h\n// optimize \u03b8 via Equation (3)\n5\nSample \u03b5 \u201e Np0, Iq and k P rKs uniformly\n6\nCompute xpkq \u00d0 ?s\u03b1kxp0q ` ?1 \u00b4 s\u03b1k\n7\ny \u00d0 \u2205 with probability puncond, otherwise y \u00d0 gt\n8\nTake gradient step on \u2207\u03b8\n\u203a\u203a\u203a\u03b5\u03b8pxpkq, k, yq \u00b4 \u03b5\n\u203a\u203a\u203a\n2\n2\n// Stage 2:\nOffline Policy Training\n9 Hyperparameters: rollout length H, conditioning RTG geval,\nguidance parameter \u03c9, target network update frequency n\n10 Initialize the actor and critic networks \u03c0\u03c8, Q\u03d5\n11 Initialize the weights of target networks s\u03c8 \u00d0 \u03c8, s\u03d5 \u00d0 \u03d5\n12 for i \u201c 1, 2, . . . until convergence do\n13\nSample state-action pair pst, atq from Doffline\n// diffuion model value expansion\n14\nSample prt, pst`1, prt`1, . . . , pst`T \u00b41, prt`T \u00b41 \u201e\np\u03b8p\u00a8|st, at, gevalq with guidance parameter \u03c9\n15\nCompute the target Q value\ny \u201c \u0159H\u00b41\nh\u201c0 \u03b3hprt`h ` \u03b3HQ s\u03d5ppst`H, \u03c0 s\n\u03c8ppst`Hqq\n// update the critic\n16\n\u03d5 \u00d0 \u03d5 \u00b4 \u03b7\u2207\u03d5 }Q\u03d5pst, atq \u00b4 y}2\n2\n// update the actor\n17\nUpdate the actor network:\n\u03c8 \u00d0 \u03c8 ` \u03b7\u2207\u03c8Q\u03d5pst, \u03c0\u03c8pstqq\n// update the target networks\n18\nif i mod n then\n19\ns\u03d5 \u00d0 s\u03d5 ` wp\u03d5 \u00b4 s\u03d5q\n20\ns\u03c8 \u00d0 s\u03c8 ` wp\u03c8 \u00b4 s\u03c8q\negy, as the resulting policy is model-free and thus can act\nfast at inference time. We propose an actor-critic algorithm,\nwhere the critic is trained on synthetic data generated by the\ndiffusion model. In a nutshell, we estimate the Q-value by\nthe sum of short-term return, simulated by the DWM, and\nlong-return value, estimated by a proxy Q function learned\nthrough temporal difference (TD) learning.\nDefinition 3.1 (H-step Diffusion Model Value Ex-\npansion). Let pst, atq be a state-action pair.\nSam-\nple prt, pst`1, prt`1, . . . ,pst`T \u00b41, prt`T \u00b41 from the diffusion\nmodel p\u03b8p\u00a8|st, at, gevalq. Let H be the simulation horizon,\nwhere H \u0103 T. The H-step diffusion model value expansion\nestimate of the value of pst, atq is given by\npQH\ndiffpst, atq \u201c \u0159H\u00b41\nh\u201c0 \u03b3hprt`h ` \u03b3H pQppst`H, pat`Hq, (4)\nwhere pat`H \u201c \u03c0ppst`Hq and pQ is the proxy value function.\nWe employ this expansion to compute the target value in\nTD learning, see Algorithm 3.1. This mechanism is key\n3\nDiffusion World Model\nto the success of our algorithm and has several appealing\nproperties.\n1. In deploying the standard model-based value expan-\nsion (MVE, Feinberg et al. (2018)), the imagined tra-\njectory is derived by recursively querying the one-step dy-\nnamics model f\u03b8pst`1, rt|st, atq, which is the root cause\nof error accumulation. As an advantage over MVE, our\nDWM generates the imagined trajectory (without actions)\nas a whole.\n2. More interestingly, MVE uses the policy predicted ac-\ntion pat \u201c \u03c0ppstq when querying f\u03b8. This can be viewed\nas an on-policy value estimation of \u03c0 in a simulated en-\nvironment. In contrast, Diffusion-MVE operates in an\noff-policy manner, as \u03c0 does not influence the sampling\nprocess. As we will explore in Section 4, the off-policy\ndiffusion-MVE excels in offline RL, significantly sur-\npassing the performance of one-step-MVE. We will now\ndelve into two interpretations of this, each from a unique\nperspective.\n(a) Our approach can be viewed as a policy iteration algo-\nrithm, alternating between policy evaluation (line 13-16)\nand policy improvement (line 17) steps. Here, pQ is the\nestimator of the policy value function Q\u03c0. In the context\nof offline RL, TD learning often lead to overestimation of\nQ\u03c0 (Thrun & Schwartz, 2014; Kumar et al., 2020). This is\nbecause \u03c0 might produce out-of-distribution actions, lead-\ning to erroneous values for pQ, and the policy is defined\nto maximize pQ. Such overestimation negatively impacts\nthe generalization capability of the resulting policy when it\nis deployed online. To mitigate this, a broad spectrum of\noffline RL methods apply various forms of regularization\nto the value function (Kumar et al., 2020; Kostrikov et al.,\n2021; Garg et al., 2023), to ensure the resulting policy re-\nmains close to the data. As the DWM is trained exclusively\non offline data, it can be seen as a synthesis of the behavior\npolicy that generates the offline dataset. In other words,\ndiffusion-MVE introduces a type of value regularization\nfor offline RL through generative modeling.\nMoreover, our approach significantly differs from existing\nvalue pessimism notions. One challenge of offline RL is that\nthe behavior policy that generates the offline dataset is of-\nten of low-to-moderate quality, so that the resulting dataset\nmight only contain trajectories with low-to-moderate returns.\nAs a result, many regularization techniques introduced for\noffline RL are often overly pessimistic (Ghasemipour et al.,\n2022; Nakamoto et al., 2023). To address this issue, we typ-\nically condition on large out-of-distribution (OOD) values\nof geval when sampling from the DWM. Putting differently,\nwe ask the DWM to output an imagined trajectory under an\noptimistic goal.\n(b) Alternatively, we can also view the approach as an of-\nfline Q-learning algorithm (Watkins & Dayan, 1992), where\npQ is estimating the optimal value function Q\u02da using off-\npolicy data. Again, the off-policy data is generated by the\ndiffusion model, conditioning on OOD RTG values. In\nessence, our approach can be characterized as offline Q-\nlearning on synthetic data.\nComparison with Transformer-based World Models.\nCurious readers may wonder about the key distinctions\nbetween DMW and existing Transformer-based world\nmodels (Chen et al., 2022; Micheli et al., 2022; Robine et al.,\n2023). These models, given the current state st and action\nat, leverage the autoregressive structure of Transformer to\nincorporate past information to predict st`1. To forecast\nmultiple steps into the future, they must make iterated\npredictions. In contrast, DWM makes long-horizon predic-\ntions in a single query. It is worth noting that it is entirely\npossible to substitute the diffusion model in our work with a\nTransformer, and we justify our design choice in Section 4.2.\n4\nExperiments\nOur experiments are design with two objectives. (1) First,\nwe want to investigate the effectiveness of DWM in reducing\nthe compounding error for MBRL. (2) Second, we want to\nevaluate the performance of our proposed Algorithm 3.1 and\ncompare it with the counterparts using one-step dynamics\nmodel, as well as other SOTA MF approaches.\nTo achieve those goals, we consider 3 instantiations of Al-\ngorithm 3.1, where we integrate TD3+BC (Fujimoto & Gu,\n2021), IQL (Kostrikov et al., 2021), Q-learning with pes-\nsimistic reward (which we refer to as PQL) into the frame-\nwork, as the offline RL algorithm in the second stage. These\nalgorithms come with different conservatism notions defined\non the action (TD3+BC), the value function (IQL), and the\nreward (PQL), respectively. Specifically, the PQL algorithm\nis inspired by the MOPO algorithm Yu et al. (2020), where\nwe penalize the world model predicted reward by the un-\ncertainty of its prediction. Nonetheless, it is distinct from\nMOPO in the critic learning. MOPO uses standard TD learn-\ning on model-generated transitions, whereas we use MVE\nor Diff-MVE for value estimation. In the sequel, we refer to\nour algorithms as DWM-TD3BC, DWM-IQL, and DWM-\nPQL, respectively. For DWM-IQL, we have observed per-\nformance enhancement using a variant of Diff-MVE based\non the \u03bb-return technique (Schulman et al., 2015), therefore\nwe incorporate it as a default feature. Detailed descriptions\nof these algorithms are deferred to Appendix C.\nBaselines. We consider 3 variants of our algorithms where\nwe substitute DWM by one-step dynamics models and use\nstandard MVE, named O-TD3BC, O-IQL and O-PQL, cor-\nrespondingly. For the MF baselines, we consider 3 popular\napproaches with SOTA performance: TD3+BC, IQL, and\n4\nDiffusion World Model\nDecision Diffuser (DD, Ajay et al. (2022)).\nBenchmark. We conduct experiments on 9 datasets of lo-\ncomotion tasks from the D4RL (Fu et al., 2020) benchmark,\nand report the obtained normalized return (0-1 with 1 as\nexpert performance). We train each algorithm for 5 in-\nstances with different random seeds, and evaluate them for\n10 episodes. All reported values are means and standard\ndeviations aggregated over 5 random seeds.\nHyperparameters. We set the sequence length of DWM\nto be T \u201c 8 (discussed in Sec. 4.1).\nThe number of\ndiffusion steps is K \u201c 5 for training. For DWM inference,\nan accelerated inference technique is applied with a\nreduced number of diffusion steps N \u201c 3, as detailed in\nSecition 4.2. The training and sampling details of DWM\nrefer to Appendix A, and the training details of each offline\nalgorithm refer to Appendix D.\n4.1\nOffline RL\nMain Results. For MB algorithms, we sweep over the sim-\nulation horizon H P t1, 3, 5, 7u and a set of evaluation RTG\nvalues. The RTG values we search vary across environ-\nments, and we specify it in Table D.2. We report the best\nresult for each algorithm in Table 4.1. The predominant\ntrends we found are:\nThe proposed DWM significantly outperforms the one-\nstep counterparts, with a notable 44% performance gain.\nInterestingly, when applied in MB settings, one-step dynam-\nics models typically degrade the performances of corre-\nsponding MF algorithms, as shown by comparison between\nTD3+BC and O-TD3BC, IQL and O-IQL. DWMs, however,\nare able to maintain even surpass the original MF algo-\nrithms when applied in MB setting, as shown in comparison\nof IQL and DWM-IQL. Overall, DWM algorithms achieve\nperformance on par with SOTA MF algorithms.\nThis is attributed to the strong expressivity of diffusion\nmodels and the prediction of entire sequences all at once,\nwhich circumvents the compounding error issue in multistep\nrollout of traditional one-step dynamics models. The point\nwill be further discussed in the studies of simulation length\nas next paragraph.\nLong Horizon Planning with DWM. To explore the re-\nsponse of different world models to long simulation hori-\nzons, we compare the performance DWM methods (DWM-\nTD3BC and DWM-IQL) with their one-step counterparts (O-\nTD3BC and O-IQL) when the simulation horizon H used\nin policy training changes. To explore the limit of DWM\nmodels, we train another set of DWMs with longer sequence\nlength T \u201c 32 and investigate the performance of down-\nstream RL algorithms for H P t1, 3, 7, 15, 31u. The algo-\nrithms with one-step dynamics models have simulation hori-\nzon from 1 to 5. Figure 4.1 plots the results across 9 tasks.\n100\n101\n0.4\n0.6\n0.8\nReturn (Mean \u00b1 Std)\nhopper-m\n100\n101\n0.2\n0.4\n0.6\n0.8\nhopper-mr\n100\n101\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nhopper-me\n100\n101\n0.2\n0.4\n0.6\n0.8\nReturn (Mean \u00b1 Std)\nwalker2d-m\n100\n101\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nwalker2d-mr\n100\n101\n0.00\n0.25\n0.50\n0.75\n1.00\nwalker2d-me\n100\n101\nSimulation Horizon\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nReturn (Mean \u00b1 Std)\nhalfcheetah-m\n100\n101\nSimulation Horizon\n0.0\n0.1\n0.2\n0.3\n0.4\nhalfcheetah-mr\n100\n101\nSimulation Horizon\n0.2\n0.4\n0.6\n0.8\nhalfcheetah-me\nO-TD3BC\nO-IQL\nDWM-IQL\nDWM-TD3BC\nFigure 4.1: Performances of MBRL methods with different\nsimulation horizons used in policy training. The x-axis has\nrange r1, 31s in a logarithm scale.\nO-IQL and O-TD3BC exhibit a clearly performance drop as\nthe simulation horizon increases. For most tasks, their per-\nformances peak with relatively short simulation horizons,\nlike one or two. This suggests that longer model-based\nrollout with one-step dynamics models suffer from severe\ncompounding errors. On the contrary, DWM-TD3BC and\nDWM-IQL maintain relatively high returns without signifi-\ncant performance degradation, even using horizon length 31.\nWe further compare the performances of algorithms with\nDWM trained with sequence length T \u201c 8 and T \u201c 32. Ta-\nble 4.2 presents average best return across 9 tasks (searched\nover RTG values and simulation horizon H). Even though\nDWM is robust to long-horizon simulation and in certain\ncases we have found the optimal H is larger than 8, a se-\nquence length exceeding T \u201c 8 does not further improve\nthe performance. Therefore we choose T \u201c 8 for our main\nexperiments. Detailed results are provided in Table E.1.\n4.2\nAblation Studies\nIn this section, we discuss and ablate the design choices\nmade for our proposed algorithms with DWM.\nTransformer v.s. Diffusion Model. Algorithm 3.1 is ca-\npable of accommodating various types of sequence models,\nincluding Transformer (Vaswani et al., 2017), one of the\n5\nDiffusion World Model\nModel-Free\nModel-Based\nEnv.\nTD3+BC\nIQL\nDD\nO-TD3BC\nO-IQL\nO-PQL\nDWM-TD3BC\nDWM-IQL\nDWM-PQL\nhopper-m\n0.58 \u02d8 0.11\n0.48 \u02d8 0.08\n0.49 \u02d8 0.07\n0.39 \u02d8 0.04\n0.45 \u02d8 0.05\n0.63 \u02d8 0.12\n0.65 \u02d8 0.10\n0.54 \u02d8 0.11\n0.50 \u02d8 0.09\nwalker2d-m\n0.77 \u02d8 0.09\n0.75 \u02d8 0.15\n0.67 \u02d8 0.16\n0.39 \u02d8 0.15\n0.52 \u02d8 0.24\n0.74 \u02d8 0.14\n0.70 \u02d8 0.15\n0.76 \u02d8 0.05\n0.79 \u02d8 0.08\nhalfcheetah-m\n0.47 \u02d8 0.01\n0.46 \u02d8 0.07\n0.49 \u02d8 0.01\n0.44 \u02d8 0.05\n0.44 \u02d8 0.03\n0.45 \u02d8 0.01\n0.46 \u02d8 0.01\n0.44 \u02d8 0.01\n0.44 \u02d8 0.01\nhopper-mr\n0.53 \u02d8 0.19\n0.25 \u02d8 0.02\n0.66 \u02d8 0.15\n0.26 \u02d8 0.05\n0.25 \u02d8 0.03\n0.32 \u02d8 0.03\n0.53 \u02d8 0.09\n0.61 \u02d8 0.13\n0.39 \u02d8 0.03\nwalker2d-mr\n0.75 \u02d8 0.19\n0.48 \u02d8 0.23\n0.44 \u02d8 0.26\n0.23 \u02d8 0.13\n0.24 \u02d8 0.07\n0.62 \u02d8 0.22\n0.46 \u02d8 0.19\n0.35 \u02d8 0.14\n0.35 \u02d8 0.13\nhalfcheetah-mr\n0.43 \u02d8 0.01\n0.44 \u02d8 0.01\n0.38 \u02d8 0.06\n0.43 \u02d8 0.01\n0.42 \u02d8 0.02\n0.42 \u02d8 0.01\n0.43 \u02d8 0.01\n0.41 \u02d8 0.01\n0.43 \u02d8 0.01\nhopper-me\n0.90 \u02d8 0.28\n0.86 \u02d8 0.22\n1.06 \u02d8 0.11\n0.31 \u02d8 0.18\n0.39 \u02d8 0.19\n0.43 \u02d8 0.18\n1.03\u02d8 0.14\n0.90 \u02d8 0.25\n0.80 \u02d8 0.18\nwalker2d-me\n1.08 \u02d8 0.01\n1.09 \u02d8 0.00\n0.99 \u02d8 0.15\n0.60 \u02d8 0.25\n0.57 \u02d8 0.18\n0.61 \u02d8 0.22\n1.10 \u02d8 0.00\n1.04 \u02d8 0.10\n1.10 \u02d8 0.01\nhalfcheetah-me\n0.73 \u02d8 0.16\n0.60 \u02d8 0.23\n0.91 \u02d8 0.01\n0.27 \u02d8 0.12\n0.61 \u02d8 0.22\n0.61 \u02d8 0.22\n0.75 \u02d8 0.16\n0.71 \u02d8 0.14\n0.69 \u02d8 0.13\nAverage\n0.69\n0.61\n0.68\n0.37\n0.43\n0.54\n0.68\n0.64\n0.61\n0.660\n0.447\n0.643\nTable 4.1: Comparison of different MF and MB methods on the D4RL dataset: normalized return (mean \u02d8 std).\nDWM-TD3BC\nDWM-IQL (w/o \u03bb)\nT=8\nT=32\nT=8\nT=32\n0.68 \u02d8 0.10\n0.60 \u02d8 0.12\n0.57 \u02d8 0.09\n0.61\u02d8 0.10\nTable 4.2: The average performance of DWM algorithms\nacross 9 tasks, using DWM with different sequence lengths.\nmost successful sequence models. However, analogous to\nthe compounding error issue for one-step dynamics model,\nTransformer is subject to inherent error accumulation due\nto its autoregressive structure. Therefore, we hypothesize\nTransformer will underperform and choose diffusion model.\nTo verify this hypothesis, we replace the diffusion model\nwith Transformer in our proposed algorithms, and compare\nthe resulting performance with DWM methods. We par-\nticularly consider the combination with TD3+BC and IQL,\nwhere we call the obtained algorithms T-TD3BC and T-IQL.\nWe test T-TD3BC and T-IQL with parameter sweeping over\nsimulation horizon H P t1, 3, 5, 7u, same as DWM methods.\nFor the evaluation RTG, we take the value used in Decision\nTransformer (Chen et al., 2021) and normalize it as the train-\ning data. The rest experimental setup follows the same con-\nfigurations as Section 4.1. From the comparison results in\nTable 4.3, we see that DWM consistently outperforms Trans-\nformer across offline RL algorithm instantiations and envi-\nronments. The experiment details refer to Appendix E.2.\nWe note that while T-IQL matches O-IQL in performance,\nT-TD3BC exceeds the performance of O-TD3BC.\nDiffusion Steps and Inference Step Ratio. The number of\ntraining diffusion steps K can heavily influence the model-\ning quality, where a larger value of K generally leads to bet-\nter performance. At the same time, sampling from the diffu-\nsion models is recognized as a slow procedure, as it involves\nK internal denoising steps. We apply the stride sampling\ntechnique (Nichol & Dhariwal, 2021) to accelerate the sam-\npling process with reduced internal steps N, see Appendix A\nfor more details. However, the sampling speed comes at the\ncost of quality. It is important to strike a balance between in-\nference speed and prediction accuracy. We investigate how\nto choose the number of K and N to significantly accelerate\nEnv.\nT-TD3BC\nT-IQL\nDWM-TD3BC\nDWM-IQL\nhopper-m\n0.58 \u02d8 0.08\n0.55 \u02d8 0.08\n0.65 \u02d8 0.10\n0.54 \u02d8 0.11\nwalker2d-m\n0.60 \u02d8 0.16\n0.72 \u02d8 0.12\n0.70 \u02d8 0.15\n0.76 \u02d8 0.05\nhalfcheetah-m\n0.42 \u02d8 0.03\n0.43 \u02d8 0.01\n0.46 \u02d8 0.01\n0.44 \u02d8 0.01\nhopper-mr\n0.25 \u02d8 0.06\n0.26 \u02d8 0.09\n0.53 \u02d8 0.09\n0.61 \u02d8 0.13\nwalker2d-mr\n0.13 \u02d8 0.06\n0.23 \u02d8 0.12\n0.46 \u02d8 0.19\n0.35 \u02d8 0.14\nhalfcheetah-mr\n0.40 \u02d8 0.01\n0.39 \u02d8 0.01\n0.43 \u02d8 0.01\n0.41 \u02d8 0.01\nhopper-me\n0.66 \u02d8 0.25\n0.62 \u02d8 0.16\n1.03\u02d8 0.14\n0.90 \u02d8 0.25\nwalker2d-me\n0.58 \u02d8 0.15\n1.03 \u02d8 0.09\n1.10 \u02d8 0.00\n1.04 \u02d8 0.10\nhalfcheetah-me\n0.36 \u02d8 0.17\n0.44 \u02d8 0.08\n0.75 \u02d8 0.16\n0.71 \u02d8 0.14\nAvg.\n0.44\n0.52\n0.68\n0.64\nTable 4.3: The performance of different instantiations of\nAlgorithm 3.1 using DWM and Transformer world models.\nsampling without sacrificing model performance.\nWe train DWM with different numbers of diffusion steps\nK P t5, 10, 20, 30, 50, 100u, where the sequence length\nis T\n\u201c 8.\nWe set four inference step ratios rinfer P\nt0.2, 0.3, 0.5, 1.0u and use N \u201c rrinfer \u00a8 Ks internal steps in\nstride sampling. Figure 4.2 reports the prediction errors of\nDMW for both observation and reward sequences, defined\nin Equation (17). We note that the prediction error depends\non the evaluation RTG, and we report the best results across\nmultiple values of it, see Table D.2. An important obser-\nvation is that rinfer \u201c 0.5 is a critical ridge for distinguish-\ning the performances with different inference steps, where\nN \u0103 K{2 hurts the prediction accuracy significantly. More-\nover, within the regime rinfer \u011b 0.5, a small diffusion steps\nK \u201c 5 performs roughly the same as larger values. There-\nfore, we choose K \u201c 5 and rinfer \u201c 0.5 for our main experi-\nments, which leads to the number of sampling steps N \u201c 3.\nWe have also repeated the above experiments for DWM with\nlonger sequence length T \u201c 32. The results also support the\nchoice rinfer \u201c 0.5 but favors K \u201c 10, see Appendix E.3.\nOOD Evaluation RTG Values. We found that the evalu-\nation RTG values play a critical role in determining the\nperformance of our algorithm. Our preliminary experi-\nments on trajectory preidction have suggested that in dis-\ntribution evaluation RTGs underperforms OOD RTGs, see\nAppendix E.4.2. Figure 4.3 reports the return of DWM-IQL\nand DWM-TD3BC across 3 tasks, with different values of\n6\nDiffusion World Model\nFigure 4.2: The average observation and reward prediction\nerrors (across 9 tasks and simulation horizon H P r7s)\nfor DWM trained with different diffusion steps K, as the\ninference step ratio rratio changes.\ngeval1. We report the results averaged over different simu-\nlation horizons 1, 3, 5 and 7. The compared RTG values\nare different for each task, but are all OOD. Appendix E.4.1\nshows the distributions of training RTGs for each task. The\nresults show that the actual return does not always match\nwith the specified geval. This is a well-known issue of return-\nconditioned RL methods (Emmons et al., 2021; Zheng et al.,\n2022; Nguyen et al., 2022). Nonetheless, OOD evalua-\ntion RTGs generally performs well. Figure 4.3 shows both\nDWM-TD3BC and DWM-IQL are robust to OOD evalua-\ntion RTGs. We emphasize the reported return is averaged\nover training instances with different simulation horizons,\nwhere the peak performance, reported in Table 4.1 is higher.\nOur intuition is to encourage the diffusion model to take\nan optimistic view of the future return for the current state.\nOn the other hand, the evaluation RTG cannot be overly\nhigh. As shown in task halfcheetah-mr, increasing RTG\ngeval \u0105 0.4 will further decrease the actual performances for\nboth methods. The optimal RTG values vary from task to\ntask, and the complete experiment results are provided in\nAppendix E.4.\nFigure 4.3: Comparison of DWM methods using different\nevaluation RTG values (displayed in parenthesis).\n\u03bb-Return Value Estimation. The Dreamer series of\nwork (Hafner et al., 2019a; 2020; 2023) applies the \u03bb-return\ntechnique (Schulman et al., 2015) for value estimation, used\nthe imagined trajectory. This technique can be seamlessly\n1We note that the return and RTG are normalized in different\nways: the return computed by the D4RL benchmark is undis-\ncounted and normalized by the performance of one SAC policy,\nwhereas the RTG we use in training is discounted and normalized\nby hand-selected constants.\nembedded into our framework as a modification of the stan-\ndard Diff-MVE. More precisely, given a state-action pair\npst, atq sampled from the offline dataset, we recursively\ncompute the \u03bb-target value for h \u201c H, . . . , 0:\npQ\u03bb\nt`h \u201c prt`h ` \u03b3\n#\np1 \u00b4 \u03bbqQ s\u03d5ppst`h`1, \u03c0 s\u03c8ppst`h`1qq ` \u03bb pQ\u03bb\nt`h`1\nif h \u0103 H\nQ s\u03d5ppst`H, \u03c0 s\u03c8ppst`Hqq\nif h \u201c H (5)\nusing DWM predicted states tpst`huH\nh\u201c0 and rewards\ntprtuH\nh\u201c0. We can use pQ\u03bb\nt as the target Q value for TD\nlearning, as a modification of line 15 of Algorithm 3.1. For\nalgorithms that also learn the state-only value function, like\nIQL, the Q s\u03d5 function can be replaced by the V s\u03c8 function.\nWorth noting, Equation (5) reduces to the vanilla Diff-MVE\nwhen \u03bb \u201c 1.\nWe conduct experiments to compare the vanilla diff-MVE\nand the \u03bb-return variant for DWM-TD3BC and DWM-IQL,\nusing \u03bb \u201c 0.95. We search over RTG values (specified in\nAppendix Table D.2) and simulation horizons 1, 3, 5, 7. The\nresults are summarized in Table 4.4. The \u03bb-return technique\nis beneficial for DWM-IQL, but harmful for DWM-TD3BC.\nWe speculate that since Equation (5) iteratively invokes the\nQ s\u03d5 or the V s\u03c8 function, it favors approaches with more ac-\ncurate value estimations. While IQL regularizes the value\nfunctions, TD3+BC only has policy regularization and is\nshown to be more prone to the value over-estimation issue\nin our experiments. Based on these results, we incorpo-\nrated the \u03bb-return technique into DWM-IQL, but let DWM-\nTD3BC use the vanilla Diff-MVE. We let DWM-PQL uses\nthe vanilla Diff-MVE for the sake of algorithmic simplicity.\nDWM-TD3BC\nDWM-IQL\nEnv.\nw/o \u03bb\nw/ \u03bb\nw/o \u03bb\nw/ \u03bb\nhopper-m\n0.65 \u02d8 0.10\n0.68 \u02d8 0.13\n0.50 \u02d8 0.08\n0.54 \u02d8 0.11\nwalker2d-m\n0.70 \u02d8 0.15\n0.74 \u02d8 0.08\n0.62 \u02d8 0.19\n0.76 \u02d8 0.05\nhalfcheetah-m\n0.46 \u02d8 0.01\n0.40 \u02d8 0.01\n0.46 \u02d8 0.01\n0.44 \u02d8 0.01\nhopper-mr\n0.53 \u02d8 0.09\n0.50 \u02d8 0.23\n0.29 \u02d8 0.04\n0.61 \u02d8 0.13\nwalker2d-mr\n0.46 \u02d8 0.19\n0.23 \u02d8 0.10\n0.27 \u02d8 0.09\n0.35 \u02d8 0.14\nhalfcheetah-mr\n0.43 \u02d8 0.01\n0.39 \u02d8 0.02\n0.43 \u02d8 0.01\n0.41 \u02d8 0.01\nhopper-me\n1.03 \u02d8 0.14\n1.05 \u02d8 0.16\n0.78 \u02d8 0.24\n0.90 \u02d8 0.25\nwalker2d-me\n1.10 \u02d8 0.00\n0.89 \u02d8 0.13\n1.08 \u02d8 0.03\n1.04 \u02d8 0.10\nhalfcheetah-me\n0.75 \u02d8 0.16\n0.71 \u02d8 0.22\n0.73 \u02d8 0.14\n0.74 \u02d8 0.16\nAvg.\n0.68\n0.62\n0.57\n0.64\nTable 4.4: Comparison of the performance of DWM meth-\nods using vanilla Diff-MVE and the \u03bb-return variant.\nAdditional Experiments. We also investigate the effects\nof fine-tuning DWM with relabelled RTGs. We have found\nthis technique is of limited utility and hence exclude it in\nthe final design, see detailed results in Appendix E.5.\n5\nRelated Work\nModel-Based RL One popular MB technique is action\nsearching. Using the world model, one simulates the out-\ncomes of candidate actions, which are sampled from pro-\nposal distributions or policy priors (Nagabandi et al., 2018;\n7\nDiffusion World Model\nWilliams et al., 2015), and search for the optimal one. This\ntype of approaches has been successfully applied to games\nlike Atari and Go (Schrittwieser et al., 2020; Ye et al.,\n2021) and continuous control problems with pixel obser-\nvations (Hafner et al., 2019b). Alternatively, we can opti-\nmize the policy through interactions with the world model.\nThis idea originally comes from the Dyna algorithm (Sut-\nton, 1991). The primary differences between works in this\nregime lie in their usages of the model-generated data. For\nexample, Dyna-Q (Sutton, 1990) and MBPO (Janner et al.,\n2019) augment the true environment data by world model\ngenerated transitions, and then conduct MF algorithms on ei-\nther augmented or generated dataset. Feinberg et al. (2018)\nproposes to improve the value estimation by unrolling the\npolicy within the world model up to a certain horizon. The\nDreamer series of work Hafner et al. (2019a; 2020; 2023)\nuse the rollout data for both value estimation and policy\nlearning. More recently, Hansen et al. (2022b; 2023); Chit-\nnis et al. (2023) combine both techniques to solve contin-\nuous control problems. As we cannot go over all the MB\napproaches, we refer readers to Wang et al. (2019); Amos\net al. (2021) for more comprehensive review and bench-\nmarks of them.\nMost of the aforementioned approaches rely on simple\none-step world models fprt, st`1|st, atq. The Dreamer\nseries of work (Hafner et al., 2019a; 2020; 2023) use recur-\nrent neural networks (RNN) to engage in past information\nfor predicting the next state. Lately, Robine et al. (2023);\nMicheli et al. (2022); Chen et al. (2022) have independently\nproposed Transformer-based world models as a replacement\nof RNN. Janner et al. (2020) uses a generative model to\nlearn the occupancy measure over future states, which can\nperform long-horizon rollout with a single forward pass.\nOffline RL Directly applying online RL methods to of-\nfline RL usually lead to poor performances. The failures\nare typically attributed to the extrapolation error (Fujimoto\net al., 2019). To address this issue, a number of conser-\nvatism notions has been introduced to encourage the policy\nto stay close with the offline data. For model-free meth-\nods, these notions are applied to the value functions (Kumar\net al., 2020; Kostrikov et al., 2021; Garg et al., 2023) or to\nthe policies (Wu et al., 2019; Jaques et al., 2019; Kumar\net al., 2019; Fujimoto & Gu, 2021). Conservatism has also\nbeen incorporated into MB techniques through modified\nMDPs. For instance, MOPO (Yu et al., 2020) builds upon\nMBPO and relabels the predicted reward when generating\ntransitions. It subtracts the uncertainty of the world model\u2019s\nprediction from the predicted reward, thereby softly promot-\ning state-action pairs with low-uncertainty outcome. In a\nsimilar vein, MOReL (Kidambi et al., 2020) trains policies\nusing a constructed pessimistic MDP with terminal state.\nThe agent will be moved to the terminal state if the predic-\ntion uncertainty of the world model is high, and will receive\na negative reward as a penalty.\nSequence Modeling for RL There is a surge of recent\nresearch interest in applying sequence modeling tools to\nRL problems. Chen et al. (2021); Janner et al. (2021) first\nconsider the offline trajectories as autoregressive sequences\nand model them using Transformer architectures (Vaswani\net al., 2017). This has inspired a line of follow-up research,\nincluding Meng et al. (2021); Lee et al. (2022). Normalizing\nflows like diffusion model (Sohl-Dickstein et al., 2015; Ho\net al., 2020; Song et al., 2020), flow matching (Lipman\net al., 2022) and consistency model (Song et al., 2023) have\nalso been incorporated into various RL algorithms, see e.g.,\nWang et al. (2022); Chi et al. (2023); Hansen-Estruch et al.\n(2023); Jia et al. (2023); Ding & Jin (2023); Du et al. (2023);\nXu et al. (2023); Mishra & Chen (2023). Among these\nworks, Diffuser (Janner et al., 2022) and Decision Diffuser\n(DD) (Ajay et al., 2022) are most close to our work, as\nthey also predict future trajectories. However, the usage of\ngenerated trajectories significantly differs. Diffuser serves\na dual role of both world model and policy. It predicts\nfuture actions and states simultaneously and executes the\npredicted subsequent action. Decision Diffuser separates\nthe prediction of states and actions. The diffusion model\nsolely models the future state sequence conditioning on a\ntarget return, whereas the action is predicted by an inverse\ndynamics model, given current state and predicted next state.\nConcurrent Works Several concurrent works have utilized\nthe diffusion model (DM) for world modeling in a variety\nof ways. Here we highlight the key differences between\nour approach and theirs. Anonymous (2024) trains a DM\nto predict the next single state st`1, conditioning on past\nstates st\u00b4H, . . . , st and actions at\u00b4H, . . . , at. This concept\nis similarly applied in UniSim (Yang et al., 2023). However,\nthese models still plan step by step, whereas our model plans\nmultiple future steps at once. SynthER (Lu et al., 2023) is\nin the same spirit as MBPO (Janner et al., 2019). It trains\nan unconditioned DM to generate transitions and augments\nthe training dataset. PolyGRAD (Rigter et al., 2023) learns\na DM to predict a sequence of future states st`1, . . . , st`H\nand rewards rt`1, . . . , rt`H, conditioning on the initial state\nst and corresponding actions at, . . . , at`H. Given that the\nactions are also unknown, PolyGRAD alternates between\npredicting the actions (via stochastic Langevin dynamics\nusing policy score) and denoising the states and rewards\nduring the DM\u2019s sampling process. This approach results in\ngenerating on-policy trajectories. In contrast, our approach\nis off-policy, since it does not interact with the policy during\nthe sampling process. Zhang et al. (2023) trains a discretized\nDM with masked and noisy input. Despite still predicting\nstep by step at inference time, this work mainly focuses on\nprediction tasks and does not conduct RL experiments.\n8\nDiffusion World Model\n6\nConclusion and Future Work\nWe present a general framework of leveraging diffusion\nmodels as world models, in the context of offline RL. This\nframework can be easily extended to accommodate online\ntraining. Specifically, we utilize DWM generated trajec-\ntories for model-based value estimation. Our experiments\nshow that this approach effectively reduces the compound-\ning error in MBRL. We benchmarked DWM against the\ntraditional one-step dynamics model, by training 3 different\ntypes of offline RL algorithms using imagined trajectories\ngenerated by each of them. DWM demonstrates a notable\nperformance gain and achieves SOTA performance, on par\nwith the most advanced MF approaches. However, there\nare also limitations of our work. Currently, DWM is trained\nfor each individual environment and is task-agnostic. An\nintriguing avenue for future research would be extending\nDWM to multi-environment and multi-task settings. Addi-\ntionally, to circumvent the side effects of exploration, we\nonly investigate DWM in the offline RL setting. This raises\nan interesting question regarding the performance of DWM\nin online settings. Lastly but most importantly, although\nwe adopt the stride sampling technique to accelerate the in-\nference, the computational demand of DWM remains high.\nFurther enhancements to speed up the sampling process\ncould be crucial for future usages of DWM to tackle larger\nscale problems.\nReferences\nAjay, A., Du, Y., Gupta, A., Tenenbaum, J., Jaakkola,\nT., and Agrawal, P.\nIs conditional generative model-\ning all you need for decision-making? arXiv preprint\narXiv:2211.15657, 2022.\nAmos, B., Stanton, S., Yarats, D., and Wilson, A. G. On\nthe model-based stochastic value gradient for continuous\nreinforcement learning. In Learning for Dynamics and\nControl, pp. 6\u201320. PMLR, 2021.\nAnonymous. Diffusion world models, 2024. URL https:\n//openreview.net/forum?id=bAXmvOLtjA.\nAsadi, K., Misra, D., Kim, S., and Littman, M. L. Com-\nbating the compounding-error problem with a multi-step\nmodel. arXiv preprint arXiv:1905.13320, 2019.\nChen, C., Wu, Y.-F., Yoon, J., and Ahn, S. Transdreamer:\nReinforcement learning with transformer world models.\narXiv preprint arXiv:2202.09481, 2022.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.\nDecision transformer: Reinforcement learning via se-\nquence modeling. In Thirty-Fifth Conference on Neural\nInformation Processing Systems, 2021. URL https:\n//openreview.net/forum?id=a7APmM4B9d.\nChi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burch-\nfiel, B., and Song, S.\nDiffusion policy: Visuomotor\npolicy learning via action diffusion.\narXiv preprint\narXiv:2303.04137, 2023.\nChitnis, R., Xu, Y., Hashemi, B., Lehnert, L., Dogan, U.,\nZhu, Z., and Delalleau, O. Iql-td-mpc: Implicit q-learning\nfor hierarchical model predictive control. arXiv preprint\narXiv:2306.00867, 2023.\nDean, S., Mania, H., Matni, N., Recht, B., and Tu, S. On\nthe sample complexity of the linear quadratic regulator.\nFoundations of Computational Mathematics, 20(4):633\u2013\n679, 2020.\nDeisenroth, M. P., Neumann, G., Peters, J., et al. A survey\non policy search for robotics. Foundations and Trends\u00ae\nin Robotics, 2(1\u20132):1\u2013142, 2013.\nDing, Z. and Jin, C. Consistency models as a rich and\nefficient policy class for reinforcement learning. arXiv\npreprint arXiv:2309.16984, 2023.\nDu, Y., Yang, M., Dai, B., Dai, H., Nachum, O., Tenenbaum,\nJ., Schuurmans, D., and Abbeel, P. Learning universal\npolicies via text-guided video generation. arXiv preprint\narXiv:2302.00111, 2023.\nEmmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.\nRvs: What is essential for offline rl via supervised learn-\ning? arXiv preprint arXiv:2112.10751, 2021.\nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez,\nJ. E., and Levine, S. Model-based value estimation for ef-\nficient model-free reinforcement learning. arXiv preprint\narXiv:1803.00101, 2018.\nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine,\nS. D4rl: Datasets for deep data-driven reinforcement\nlearning. arXiv preprint arXiv:2004.07219, 2020.\nFujimoto, S. and Gu, S. A minimalist approach to offline re-\ninforcement learning. In Thirty-Fifth Conference on Neu-\nral Information Processing Systems, 2021. URL https:\n//openreview.net/forum?id=Q32U7dzWXpc.\nFujimoto, S., Hoof, H., and Meger, D. Addressing function\napproximation error in actor-critic methods. In Interna-\ntional conference on machine learning, pp. 1587\u20131596.\nPMLR, 2018.\nFujimoto, S., Meger, D., and Precup, D. Off-policy deep\nreinforcement learning without exploration. In Interna-\ntional Conference on Machine Learning, pp. 2052\u20132062.\nPMLR, 2019.\nGarg, D., Hejna, J., Geist, M., and Ermon, S. Extreme\nq-learning: Maxent rl without entropy. arXiv preprint\narXiv:2301.02328, 2023.\n9\nDiffusion World Model\nGhasemipour, K., Gu, S. S., and Nachum, O. Why so pes-\nsimistic? estimating uncertainties for offline rl through\nensembles, and why their independence matters. Ad-\nvances in Neural Information Processing Systems, 35:\n18267\u201318281, 2022.\nHa, D. and Schmidhuber, J. World models. arXiv preprint\narXiv:1803.10122, 2018.\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to\ncontrol: Learning behaviors by latent imagination. arXiv\npreprint arXiv:1912.01603, 2019a.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,\nLee, H., and Davidson, J. Learning latent dynamics for\nplanning from pixels. In International conference on\nmachine learning, pp. 2555\u20132565. PMLR, 2019b.\nHafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas-\ntering atari with discrete world models. arXiv preprint\narXiv:2010.02193, 2020.\nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering\ndiverse domains through world models. arXiv preprint\narXiv:2301.04104, 2023.\nHansen, N., Lin, Y., Su, H., Wang, X., Kumar, V., and\nRajeswaran, A.\nModem: Accelerating visual model-\nbased reinforcement learning with demonstrations. arXiv\npreprint arXiv:2212.05698, 2022a.\nHansen, N., Wang, X., and Su, H. Temporal difference\nlearning for model predictive control. arXiv preprint\narXiv:2203.04955, 2022b.\nHansen, N., Su, H., and Wang, X. Td-mpc2: Scalable, ro-\nbust world models for continuous control. arXiv preprint\narXiv:2310.16828, 2023.\nHansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J. G.,\nand Levine, S.\nIdql: Implicit q-learning as an actor-\ncritic method with diffusion policies.\narXiv preprint\narXiv:2304.10573, 2023.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\nJanner, M., Fu, J., Zhang, M., and Levine, S. When to trust\nyour model: Model-based policy optimization. Advances\nin neural information processing systems, 32, 2019.\nJanner, M., Mordatch, I., and Levine, S. gamma-models:\nGenerative temporal difference learning for infinite-\nhorizon prediction. Advances in Neural Information Pro-\ncessing Systems, 33:1724\u20131735, 2020.\nJanner, M., Li, Q., and Levine, S. Offline reinforcement\nlearning as one big sequence modeling problem.\nIn\nThirty-Fifth Conference on Neural Information Process-\ning Systems, 2021.\nURL https://openreview.\nnet/forum?id=wgeK563QgSw.\nJanner, M., Du, Y., Tenenbaum, J. B., and Levine, S. Plan-\nning with diffusion for flexible behavior synthesis. arXiv\npreprint arXiv:2205.09991, 2022.\nJaques, N., Ghandeharioun, A., Shen, J. H., Ferguson,\nC., Lapedriza, A., Jones, N., Gu, S., and Picard, R.\nWay off-policy batch deep reinforcement learning of\nimplicit human preferences in dialog. arXiv preprint\narXiv:1907.00456, 2019.\nJia, Z., Liu, F., Thumuluri, V., Chen, L., Huang, Z., and Su,\nH. Chain-of-thought predictive control. arXiv preprint\narXiv:2304.00776, 2023.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Camp-\nbell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza-\nkowski, P., Levine, S., et al.\nModel-based reinforce-\nment learning for atari. arXiv preprint arXiv:1903.00374,\n2019.\nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims,\nT. Morel: Model-based offline reinforcement learning.\nAdvances in neural information processing systems, 33:\n21810\u201321823, 2020.\nKingma, D., Salimans, T., Poole, B., and Ho, J. Varia-\ntional diffusion models. Advances in neural information\nprocessing systems, 34:21696\u201321707, 2021.\nKostrikov, I., Nair, A., and Levine, S. Offline reinforcement\nlearning with implicit q-learning, 2021.\nKumar, A., Fu, J., Tucker, G., and Levine, S. Stabilizing\noff-policy q-learning via bootstrapping error reduction.\narXiv preprint arXiv:1906.00949, 2019.\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conserva-\ntive q-learning for offline reinforcement learning. arXiv\npreprint arXiv:2006.04779, 2020.\nLambert, N., Pister, K., and Calandra, R. Investigating com-\npounding prediction errors in learned dynamics models.\narXiv preprint arXiv:2203.09637, 2022.\nLee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman,\nD., Xu, W., Guadarrama, S., Fischer, I., Jang, E.,\nMichalewski, H., et al. Multi-game decision transformers.\narXiv preprint arXiv:2205.15241, 2022.\nLipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and\nLe, M. Flow matching for generative modeling. arXiv\npreprint arXiv:2210.02747, 2022.\n10\nDiffusion World Model\nLu, C., Ball, P. J., and Parker-Holder, J. Synthetic experience\nreplay. arXiv preprint arXiv:2303.06614, 2023.\nMeng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W.,\nWen, Y., Zhang, H., Wang, J., and Xu, B. Offline pre-\ntrained multi-agent decision transformer: One big se-\nquence model conquers all starcraftii tasks. arXiv preprint\narXiv:2112.02845, 2021.\nMicheli, V., Alonso, E., and Fleuret, F.\nTransform-\ners are sample efficient world models. arXiv preprint\narXiv:2209.00588, 2022.\nMish, M. D. A self regularized non-monotonic activation\nfunction [j]. arXiv preprint arXiv:1908.08681, 2019.\nMishra, U. A. and Chen, Y.\nReorientdiff: Diffusion\nmodel based reorientation for object manipulation. arXiv\npreprint arXiv:2303.12700, 2023.\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S.\nNeural network dynamics for model-based deep rein-\nforcement learning with model-free fine-tuning. In 2018\nIEEE international conference on robotics and automa-\ntion (ICRA), pp. 7559\u20137566. IEEE, 2018.\nNakamoto, M., Zhai, Y., Singh, A., Mark, M. S., Ma, Y.,\nFinn, C., Kumar, A., and Levine, S. Cal-ql: Calibrated of-\nfline rl pre-training for efficient online fine-tuning. arXiv\npreprint arXiv:2303.05479, 2023.\nNguyen, T., Zheng, Q., and Grover, A. Conserweightive\nbehavioral cloning for reliable offline reinforcement learn-\ning. arXiv preprint arXiv:2210.05158, 2022.\nNichol, A. Q. and Dhariwal, P. Improved denoising diffusion\nprobabilistic models. In International Conference on\nMachine Learning, pp. 8162\u20138171. PMLR, 2021.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-weighted regression: Simple and scalable\noff-policy reinforcement learning.\narXiv preprint\narXiv:1910.00177, 2019.\nRigter, M., Yamada, J., and Posner, I.\nWorld models\nvia policy-guided trajectory diffusion. arXiv preprint\narXiv:2312.08533, 2023.\nRobine, J., H\u00a8oftmann, M., Uelwer, T., and Harmeling, S.\nTransformer-based world models are happy with 100k\ninteractions. arXiv preprint arXiv:2303.07109, 2023.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Con-\nvolutional networks for biomedical image segmentation.\nIn Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Confer-\nence, Munich, Germany, October 5-9, 2015, Proceedings,\nPart III 18, pp. 234\u2013241. Springer, 2015.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. Nature, 588(7839):\n604\u2013609, 2020.\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,\nP. High-dimensional continuous control using generalized\nadvantage estimation. arXiv preprint arXiv:1506.02438,\n2015.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. Deterministic policy gradient algorithms.\nIn International conference on machine learning, pp. 387\u2013\n395. Pmlr, 2014.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nSong, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consis-\ntency models. arXiv preprint arXiv:2303.01469, 2023.\nSutton, R. S. Integrated architectures for learning, plan-\nning, and reacting based on approximating dynamic pro-\ngramming. In Machine learning proceedings 1990, pp.\n216\u2013224. Elsevier, 1990.\nSutton, R. S. Dyna, an integrated architecture for learning,\nplanning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013\n163, 1991.\nThrun, S. and Schwartz, A. Issues in using function approx-\nimation for reinforcement learning. In Proceedings of the\n1993 connectionist models summer school, pp. 255\u2013263.\nPsychology Press, 2014.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Lan-\nglois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba,\nJ. Benchmarking model-based reinforcement learning.\narXiv preprint arXiv:1907.02057, 2019.\nWang, Z., Hunt, J. J., and Zhou, M. Diffusion policies as an\nexpressive policy class for offline reinforcement learning.\narXiv preprint arXiv:2208.06193, 2022.\nWatkins, C. J. and Dayan, P. Q-learning. Machine learning,\n8:279\u2013292, 1992.\n11\nDiffusion World Model\nWilliams, G., Aldrich, A., and Theodorou, E. Model pre-\ndictive path integral control using covariance variable\nimportance sampling. arXiv preprint arXiv:1509.01149,\n2015.\nWu, Y. and He, K. Group normalization. In Proceedings of\nthe European conference on computer vision (ECCV), pp.\n3\u201319, 2018.\nWu, Y., Tucker, G., and Nachum, O.\nBehavior regu-\nlarized offline reinforcement learning. arXiv preprint\narXiv:1911.11361, 2019.\nXiao, C., Wu, Y., Ma, C., Schuurmans, D., and M\u00a8uller, M.\nLearning to combat compounding-error in model-based\nreinforcement learning. arXiv preprint arXiv:1912.11206,\n2019.\nXu, Y., Li, N., Goel, A., Guo, Z., Yao, Z., Kasaei, H., Kasaei,\nM., and Li, Z. Controllable video generation by learning\nthe underlying dynamical system with neural ode. arXiv\npreprint arXiv:2303.05323, 2023.\nYamagata, T., Khalil, A., and Santos-Rodriguez, R. Q-\nlearning decision transformer: Leveraging dynamic pro-\ngramming for conditional sequence modelling in offline\nrl. In International Conference on Machine Learning, pp.\n38989\u201339007. PMLR, 2023.\nYang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuur-\nmans, D., and Abbeel, P. Learning interactive real-world\nsimulators. arXiv preprint arXiv:2310.06114, 2023.\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. Mas-\ntering atari games with limited data. Advances in Neural\nInformation Processing Systems, 34:25476\u201325488, 2021.\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S.,\nFinn, C., and Ma, T. Mopo: Model-based offline policy\noptimization. Advances in Neural Information Processing\nSystems, 33:14129\u201314142, 2020.\nZhang, L., Xiong, Y., Yang, Z., Casas, S., Hu, R., and\nUrtasun, R. Learning unsupervised world models for\nautonomous driving via discrete diffusion. arXiv preprint\narXiv:2311.01017, 2023.\nZheng, Q., Zhang, A., and Grover, A. Online decision\ntransformer. arXiv preprint arXiv:2202.05607, 2022.\nZheng, Q., Henaff, M., Amos, B., and Grover, A. Semi-\nsupervised offline reinforcement learning with action-free\ntrajectories. In International Conference on Machine\nLearning, pp. 42339\u201342362. PMLR, 2023a.\nZheng, Q., Le, M., Shaul, N., Lipman, Y., Grover, A., and\nChen, R. T. Guided flows for generative modeling and de-\ncision making. arXiv preprint arXiv:2311.13443, 2023b.\n12\nDiffusion World Model\nA\nImplementation Details of Diffusion World Model\nWe summarize the architecture and hyperparameters used for our experiments. For all the experiments, we use our own\nPyTorch implementation that is heavily influenced by the following codebases:\nDecision Diffuser (Ajay et al., 2022) https://github.com/anuragajay/decision-diffuser\nDiffuser (Janner et al., 2022) https://github.com/jannerm/diffuser/\nSSORL (Zheng et al., 2023a) https://github.com/facebookresearch/ssorl/\nArchitecture. As introduced in Section 3.1, the diffusion world model p\u03b8 used in this paper is chosen to model a length-T\nsubtrajecotires pst, at, rt, st`1, rt`1, . . . , st`T \u00b41, rt`T \u00b41q. At inference time, it predicts the subsequent subtrajecotry of\nT \u00b4 1 steps, conditioning on initial state-action pair pst, atq and target RTG y \u201c gt:\nprt, pst`1, prt`1, . . . , pst`T \u00b41, prt`T \u00b41 \u201e p\u03b8p\u00a8|st, at, y \u201c gtq.\n(6)\nThere are two reasons we choose not to model future actions in the sequence. First, our proposed diffusion model value\nexpansion (Definition 3.1) does not require the action information for future steps. Second, previous work have found that\nmodeling continuous action through diffusion is less accurate (Ajay et al., 2022).\nThroughout the paper, we train guided diffusion models for state-reward sequences of length T \u201c 8. The number of diffusion\nsteps is K \u201c 5. The probability of null conditioning puncond is set to 0.25, and the batch size is 64. We use the cosine noise\nschedule proposed by Nichol & Dhariwal (2021). The discount factor is \u03b3 \u201c 0.99, and we normalize the discounted RTG by\na task-specific reward scale, which is 400 for Hopper, 550 for Walker, and 1200 for Halfcheetah tasks.\nFollowing Ajay et al. (2022), our noise predictor \u03b5\u03b8 is a temporal U-net (Janner et al., 2022; Ronneberger et al., 2015)\nthat consists of 6 repeated residual blocks, where each block consists of 2 temporal convolutions followed by the group\nnorm (Wu & He, 2018) and a final Mish nonlinearity activation (Mish, 2019). The diffusion step k is first transformed to\nits sinusoidal position encoding and projected to a latent space via a 2-layer MLP, and the RTG value is transformed into\nits latent embedding via a 3-layer MLP. In our diffusion world model, the initial action at as additional condition is also\ntransformed into latent embedding via a 3-layer MLP, and further concatenated with the embeddings of the diffusion step\nand RTG.\nOptimization. We optimize our model by the Adam optimizer with a learning rate 1 \u02c6 10\u00b44 for all the datasets. The final\nmodel parameter s\u03b8 we consider is an exponential moving average (EMA) of the obtained parameters over the course of\ntraining. For every 10 iteration, we update s\u03b8 \u201c \u03b2s\u03b8 ` p1 \u00b4 \u03b2q\u03b8, where the exponential decay parameter \u03b2 \u201c 0.995. We train\nthe diffusion model for 2 \u02c6 106 iterations.\nSampling with Guidance. To sample from the diffusion model, we need to first sample a random noise xpKq \u201e Np0, Iq\nand then run the reverse process. Algorithm A.1 presents the general process of sampling from a diffusion model trained\nunder classifier-free guidance.\nIn the context of offline RL, the diffusion world model generates future states and rewards based on the current state st,\nthe current action at and the target return geval, see Section 3. Therefore, the sampling process is slightly different from\nAlgorithm A.1, as we need to constrain the initial state and initial action to be st and at, respectively. The adapted algorithm\nis summarized in Algorithm A.2.\nFollowing Ajay et al. (2022), we apply the low temperature sampling technique for diffusion models. The temperature is set\nto be \u03b1 \u201c 0.5 for sampling at each diffusion step from Gaussian Npp\u00b5\u03b8, \u03b12p\u03a3\u03b8q, with p\u00b5\u03b8 and p\u03a3\u03b8 being the predicted mean\nand covariance.\nAccelerated Inference. Algorithm A.1 and A.2 run the full reverse process, Building on top of them, we further apply\nthe stride sampling technique as in Nichol & Dhariwal (2021) to speed up sampling process. Formally, in the full reverse\nprocess, we generates xpk\u00b41q by xpkq one by one, from k \u201c K till k \u201c 1:\nxpk\u00b41q \u201c\n?s\u03b1k\u00b41\u03b2k\n1 \u00b4 s\u03b1k\npxp0q `\n?\u03b1kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nxpkq ` \u03c3k\u03b5, \u03b5 \u201e Np0, Iq,\n(7)\nwhere pxp0q is the prediction of the true data point (line 5 of Algorithm A.1), \u03c3k \u201c\nc\n\u03b2kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nis the standard deviation\nof noise at step k (line 8 of in Algorithm A.1). We note that s\u03b1k \u201c \u015bK\nk1\u201c1p1 \u00b4 \u03b2k1q where the noise schedule t\u03b2kuK\nk\u201c1 is\npredefined, see Section 2 and Appendix A.\n13\nDiffusion World Model\nRunning a full reverse process amounts to evaluating Equation (7) for K times, which is time consuming. To speed up\nsampling, we choose N diffusion steps equally spaced between 1 and K, namely, \u03c41, . . . , \u03c4N, where \u03c4N \u201c K. We then\nevaluate Equation (7) for the chosen steps \u03c41, . . . , \u03c4N. This effectively reduces the inference time to approximately N{K of\nthe original. In our experiments, we train the diffusion model with K \u201c 5 diffusion steps and sample with N \u201c 3 inference\nsteps, see Section 4.2 for a justification of this number.\nAlgorithm A.1: Sampling from Guided Diffusion Models\n1 Input: trained noise prediction model \u03b5\u03b8, conditioning parameter y, guidance parameter \u03c9, number of diffusion steps K\n2 xpKq \u201e Np0, Iq\n3 for k \u201c K, . . . , 1 do\n4\np\u03b5 \u00d0 \u03c9 \u00a8 \u03b5\u03b8pxpkq, k, yq ` p1 \u00b4 \u03c9q \u00a8 \u03b5\u03b8pxpkq, k, \u2205q\n// estimate true data point xp0q\n5\npxp0q \u00d0\n1\n?s\u03b1k\n`\nxpkq \u00b4 ?1 \u00b4 s\u03b1tp\u03b5\n\u02d8\n// Sample from the posterior distribution qpxpk\u00b41q|xpkq, xp0qq\n// See Equation (6) and (7) of Ho et al. (2020)\n6\np\u00b5 \u00d0\n?s\u03b1k\u00b41\u03b2k\n1 \u00b4 s\u03b1k\npxp0q `\n?\u03b1kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nxpkq\n7\np\u03a3 \u00d0 \u03b2kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nI\n8\nxpk\u00b41q \u201e Npp\u00b5, p\u03a3q\n9 Output: xp0q\nAlgorithm A.2: Diffusion World Model Sampling\n1 Input: trained noise prediction model \u03b5\u03b8, initial state st, initial action at, target return geval, guidance parameter \u03c9,\nnumber of diffusion steps K\n2 xpKq \u201e Np0, Iq\n// apply conditioning of st and at\n3 xpKqr0 : dimpstq ` dimpatqs \u00d0 concatenatepst, atq\n4 for k \u201c K, . . . , 1 do\n5\np\u03b5 \u00d0 \u03c9 \u00a8 \u03b5\u03b8pxpkq, k, gevalq ` p1 \u00b4 \u03c9q \u00a8 \u03b5\u03b8pxpkq, k, \u2205q\n// estimate true data point xp0q\n6\npxp0q \u00d0\n1\n?s\u03b1k\n`\nxpkq \u00b4 ?1 \u00b4 s\u03b1tp\u03b5\n\u02d8\n// Sample from the posterior distribution qpxpk\u00b41q|xpkq, xp0qq\n// See Equation (6) and (7) of Ho et al. (2020)\n7\np\u00b5 \u00d0\n?s\u03b1k\u00b41\u03b2k\n1 \u00b4 s\u03b1k\npxp0q `\n?\u03b1kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nxpkq\n8\np\u03a3 \u00d0 \u03b2kp1 \u00b4 s\u03b1k\u00b41q\n1 \u00b4 s\u03b1k\nI\n9\nxpk\u00b41q \u201e Npp\u00b5, p\u03a3q\n// apply conditioning of st and at\n10\nxpk\u00b41qr0 : dimpstq ` dimpatqs \u00d0 concatenatepst, atq\n11 Output: xp0q\nB\nImplementation Details of One-step Dynamics Model\nThe traditional one-step dynamics model f\u03b8pst`1, rt|st, atq is typically represented by a parameterized probability distribu-\ntion over the state and reward spaces, and optimized through log-likelihood maximization of the single-step transitions:\nmax\n\u03b8\nEpst,at,rt,st`1q\u201eDoffline rlog f\u03b8pst`1, rt|st, atqs ,\n(8)\n14\nDiffusion World Model\nwhere pst, at, rt, st`1q is sampled from the offline data distribution Doffline. As in Kidambi et al. (2020), we model f\u03b8 as\na Gaussian distribution Np\u00b5\u03b8, \u03a3\u03b8q, where the mean \u00b5\u03b8 and the diagonal covariance matrix \u03a3\u03b8 are parameterized by two\n4-layer MLP neural networks with 256 hidden units per layer. We use the ReLU activation function for hidden layers. The\nfinal layer of \u03a3\u03b8 is activated by a SoftPlus function to ensure validity. We train the dynamics models for 1 \u02c6 106 iterations,\nusing the Adam optimizer with learning rate 1 \u02c6 10\u00b44.\nC\nDiffusion World Model Based Offline RL Methods\nIn Section 4, we consider 3 instantiations of Algorithm 3.1 where we integrate TD3+BC, IQL, Q-learning with pessimistic\nreward (PQL) into our framework. These algorithms are specifically designed for offline RL, with conservatism notions\ndefined on actions (TD3+BC), value function (IQL), and rewards (PQL) respectively. In the sequel, we refer to our\ninstantiations as DWM-TD3BC, DWM-IQL and DWM-PQL. The detailed implementation of them will be introduced\nbelow.\nC.1\nDWM-TD3BC: TD3+BC with Diffusion World Model\nBuilding on top of the TD3 algorithm (Fujimoto et al., 2018), TD3+BC (Fujimoto & Gu, 2021) employs explicit behavior\ncloning regularization to learn a deterministic policy. The algorithm works as follows.\nThe critic training follows the TD3 algorithm exactly. We learn two critic networks Q\u03d51 and Q\u03d52 as double Q-learning (Fu-\njimoto et al., 2018) through TD learning. The target value in TD learning for a transition ps, a, r, s1q is given by:\ny \u201c r ` \u03b3 min\niPt1,2u Q\u03d5i\n`\ns1, a1 \u201c Clipp\u03c0 s\u03c8ps1q ` \u03b5, \u00b4C, Cq\n\u02d8\n,\n(9)\nwhere \u03b5 \u201e Np0, \u03c32Iq is a random noise, \u03c0 s\u03c8 is the target policy and Clip(\u00a8) is an operator that bounds the value of the action\nvector to be within r\u00b4C, Cs for each dimension. Both Q\u03d51 and Q\u03d52 will regress into the target value y by minimizing the\nmean squared error (MSE), which amounts to solving the following problem:\nmin\n\u03d5i Eps,a,r,s1q\u201eDoffline\n\u201d`\nypr, s1q \u00b4 Q\u03d5ips, aq\n\u02d82\u0131\n, i P t1, 2u.\n(10)\nFor training the policy, TD3+BC optimizes the following regularized problem:\nmax\n\u03c8\nEps,aq\u201eDoffline\n\u201d\n\u03bbQ\u03d51ps, \u03c0\u03c8psqq \u00b4 }a \u00b4 \u03c0\u03c8psq}2\u0131\n,\n(11)\nwhich Doffline is the offline data distribution. Without the behavior cloning regularization term }a \u00b4 \u03c0\u03c8psq}2, the above\nproblem reduces to the objective corresponding to the deterministic policy gradient theorem (Silver et al., 2014). Note that \u03c0\nis always trying to maximize one fixed proxy value function.\nBoth the updates of target policy \u03c0 s\u03c8 and the target critic networks Q s\u03d5i are delayed in TD3+BC. The whole algorithm is\nsummarized in Algorithm C.1.\n15\nDiffusion World Model\nAlgorithm C.1: DWM-TD3BC\n1 Inputs: offline dataset Doffline, pretrained diffusion world model p\u03b8, simulation horizon H, conditioning RTG geval,\ntarget network update frequency n, coefficient \u03bb, parameters for action perturbation and clipping: \u03c3, C\n2 Initialize the actor and critic networks \u03c0\u03c8, Q\u03d51, Q\u03d52\n3 Initialize the weights of target networks s\u03c8 \u00d0 \u03c8, s\u03d51 \u00d0 \u03d51, s\u03d52 \u00d0 \u03d52\n4 for i \u201c 1, 2, . . . until convergence do\n5\nSample state-action pair pst, atq from Doffline\n// diffuion model value expansion\n6\nSample prt, pst`1, prt`1, . . . , pst`T \u00b41, prt`T \u00b41 \u201e p\u03b8p\u00a8|st, at, gevalq\n7\nSample \u03b5 \u201e Np0, \u03c32Iq\n8\npa\u03b5\nt`H \u00d0 Clipp\u03c0 s\u03c8ppst`Hq ` \u03b5, \u00b4C, Cq\n9\nCompute the target Q value y \u201c \u0159H\u00b41\nh\u201c0 \u03b3hprt`h ` \u03b3H miniPt1,2u Q s\u03d5ippst`H, pa\u03b5\nt`Hq\n// update the critic\n10\n\u03d51 \u00d0 \u03d51 \u00b4 \u03b7\u2207\u03d51 }Q\u03d51pst, atq \u00b4 y}2\n2\n11\n\u03d52 \u00d0 \u03d52 \u00b4 \u03b7\u2207\u03d52 }Q\u03d52pst, atq \u00b4 y}2\n2\n// update the actor\n12\nUpdate the actor network: \u03c8 \u00d0 \u03c8 ` \u03b7\u2207\u03c8\n\u00b4\n\u03bbQ\u03d51\n\u00b4\nst, \u03c0\u03c8pstq\n\u00af\n\u00b4 }at \u00b4 \u03c0\u03c8pstq}2\u00af\n// update the target networks\n13\nif i mod n then\n14\ns\u03d51 \u00d0 s\u03d51 ` wp\u03d5 \u00b4 s\u03d51q\n15\ns\u03d52 \u00d0 s\u03d52 ` wp\u03d5 \u00b4 s\u03d52q\n16\ns\u03c8 \u00d0 s\u03c8 ` wp\u03c8 \u00b4 s\u03c8q\n17 Output: \u03c0\u03c8\nC.2\nDWM-IQL: IQL with Diffusion World Model\nIQL (Kostrikov et al., 2021) applies pessimistic value estimation on offline dataset. In addition to the double Q functions\nused in TD3+BC, IQL leverages an additional state-value function V\u03bepsq, which is estimated through expectile regression:\nmin\n\u03be\nEps,aq\u201eDoffline\n\u201e\nL\u03c4\n\u02c6\nmin\niPt1,2u Q s\u03d5ips, aq \u00b4 V\u03bepsq\n\u02d9\u0237\n,\n(12)\nwhere L\u03c4puq \u201c |\u03c4 \u00b4 1u\u01030|u2 with hyperparameter \u03c4 P p0.5, 1q, As \u03c4 \u00d1 1, V\u03bepsq is essentially estimating the maximum\nvalue of Qps, aq. This can be viewed as implicitly performing the policy improvement step, without an explicit policy. Using\na hyperparameter \u03c4 \u0103 1 regularizes the value estimation (of an implicit policy) and thus mitigates the overestimation issue\nof Q function. The Q function is updated also using Eq. (10) but with target y \u201c r ` \u03b3V\u03beps1q. Finally, given the Q and the\nV functions, the policy is extracted by Advantage Weighted Regression (Peng et al., 2019), i.e., solving\nmax\n\u03c8\nEps,aq\u201eDoffline rexp p\u03b2pQ\u03d5ps, aq \u00b4 V\u03bepsqqq log \u03c0\u03c8pa|sqs .\n(13)\nThe update of the target critic networks Q s\u03d5i are delayed in IQL. The whole algorithm is summarzied in Algorithm C.2.\nC.3\nDWM-PQL: Pessimistic Q-learning with Diffusion World Model\nPrevious offline RL algorithms like MOPO (Yu et al., 2020) have applied the conservatism notion directly to the reward\nfunction, which we referred to as pessimistic Q-learning (PQL) in this paper. Specifically, the original algorithm proposed\nby Yu et al. (2020) learns an ensemble of m one-step dynamics models tp\u03b8iuiPrms, and use a modified reward\n\u02dcrps, aq \u201c prps, aq \u00b4 \u03baups, a|p\u03b81, . . . , p\u03b8mq\n(14)\nfor learning the Q functions, where pr is the mean prediction from the ensemble, ups, a|p\u03b81, . . . , p\u03b8mq is a measurement of\nprediction uncertainty using the ensemble.\nYu et al. (2020) parameterize each dynamics model by a Gaussian distribution, and measure the prediction uncertainty\nusing the maximum of the Frobenious norm of each covariance matrix. Since the diffusion model does not have such\n16\nDiffusion World Model\nAlgorithm C.2: DWM-IQL\n1 Inputs: offline dataset Doffline, pretrained diffusion world model p\u03b8, simulation horizon H, conditioning RTG geval,\ntarget network update frequency n, expectile loss parameter \u03c4\n2 Initialize the actor, critic and value networks \u03c0\u03c8, Q\u03d51, Q\u03d52, V\u03be\n3 Initialize the weights of target networks s\u03d51 \u00d0 \u03d51, s\u03d52 \u00d0 \u03d52\n4 for i \u201c 1, 2, . . . until convergence do\n5\nSample state-action pair pst, atq from Doffline\n// diffuion model value expansion\n6\nSample prt, pst`1, prt`1, . . . , pst`T \u00b41, prt`T \u00b41 \u201e p\u03b8p\u00a8|st, at, gevalq\n7\nCompute the target Q value y \u201c \u0159H\u00b41\nh\u201c0 \u03b3hprt`h ` \u03b3HV\u03beppst`Hq\n// update the V -value network\n8\n\u03be \u00d0 \u03be \u00b4 \u03b7\u2207\u03beL\u03c4pminiPt1,2u Q s\u03d5ips, aq \u00b4 V\u03bepsqq\n// update the critic (Q-value networks)\n9\n\u03d51 \u00d0 \u03d51 \u00b4 \u03b7\u2207\u03d51 }Q\u03d51pst, atq \u00b4 y}2\n2\n10\n\u03d52 \u00d0 \u03d52 \u00b4 \u03b7\u2207\u03d52 }Q\u03d52pst, atq \u00b4 y}2\n2\n// update the actor\n11\nUpdate the actor network: \u03c8 \u00d0 \u03c8 ` \u03b7\u2207\u03c8 exp\n`\n\u03b2pminiPt1,2u Q\u03d5ips, aq \u00b4 V\u03bepsqq\n\u02d8\nlog \u03c0\u03c8pa|sq\n// update the target networks\n12\nif i mod n then\n13\ns\u03d51 \u00d0 s\u03d51 ` wp\u03d5 \u00b4 s\u03d51q\n14\ns\u03d52 \u00d0 s\u03d52 ` wp\u03d5 \u00b4 s\u03d52q\n15 Output: \u03c0\u03c8\nparameterization, and it is computationally daunting to train an ensemble of diffusion models, we propose an alternative\nuncertainty measurement similar to the one used in MoRel (Kidambi et al., 2020).\nGiven pst, atq and geval, we randomly sample m sequences from the DWM, namely,\npri\nt, psi\nt`1, pri\nt`1, . . . , psi\nt`T \u00b41, pri\nt`T \u00b41, i P rms.\n(15)\nThen, we take the 1st sample as the DWM output with modified reward:\n\u02dcrt1 \u201c\nm\n\u00ff\ni\u201c1\n1\nmpri\nt1 \u00b4 \u03ba\nmax\niPrms,jPrms\n\u02c6\u203a\u203a\u203apri\nt1 \u00b4 prj\nt1\n\u203a\u203a\u203a\n2\n`\n\u203a\u203a\u203apsi\nt1`1 \u00b4 psj\nt1`1\n\u203a\u203a\u203a\n2\n2\n\u02d9\n, t1 \u201c t, . . . , t ` T \u00b4 2.\n(16)\nThis provides an efficient way to construct uncertainty-penalized rewards for each timestep along the diffusion predicted\ntrajectories. Note that this does not apply to the reward predicted for the last timestep. The rest of the algorithm follows IQL\nbut using MSE loss instead of expectile loss for updating the value network.\nThe DWM-PQL algorithm is summarized in Algorithm C.3\nFor baseline methods with one-step dynamics model, the imagined trajectories starting from sample pst, atq \u201e Doffline\nare derived by recursively sample from the one-step dynamics model f\u03b8p\u00a8|s, aq and policy \u03c0\u03c8p\u00a8|sq: p\u03c4pst, atq \u201e pf\u03b8 \u02dd\n\u03c0\u03c8qH\u00b41pst, atq. By keeping the rest same as above, it produces MBRL methods with one-step dynamics, namely O-IQL,\nO-TD3BC and O-PQL.\nD\nTraining and Evaluation Details of Offline RL Algorithms\nD.1\nCommon Settings\nWe conduct primary tests on TD3+BC and IQL for selecting the best practices for data normalization. Based on the results,\nTD3+BC, O-TD3BC and DWM-TD3BC applies observation normalization, while other algorithms (O-PQL, DWM-PQL,\nIQL, O-IQL and DWM-IQL) applies both observation and reward normalization.\nAll algorithms are trained with a batch size of 128 using a fixed set of pretrained dynamics models (one-step and diffusion).\nThe discount factor is set as \u03b3 \u201c 0.99 for all data.\n17\nDiffusion World Model\nAlgorithm C.3: DWM-PQL\n1 Inputs: offline dataset Doffline, pretrained diffusion world model p\u03b8, simulation horizon H, conditioning RTG geval,\ntarget network update frequency n, pessimism coefficient \u03bb, number of samples for uncertainty estimation m\n2 Initialize the actor, critic and value networks \u03c0\u03c8, Q\u03d51, Q\u03d52, V\u03be\n3 Initialize the weights of target networks s\u03d51 \u00d0 \u03d51, s\u03d52 \u00d0 \u03d52\n4 for i \u201c 1, 2, . . . until convergence do\n5\nSample state-action pair pst, atq from Doffline\n// diffuion model value expansion\n6\nSample m subtrajectories tprj\nt, psj\nt`1, prj\nt`1, . . . , psj\nt`T \u00b41, prj\nt`T \u00b41um\nj\u201c1 \u201e p\u03b8p\u00a8|st, at, gevalq\n7\nModify the rewards of the first subtrajectory as in Eq. (16): \u02dcrt, pst`1, \u02dcrt`1, . . . , pst`T \u00b41, prt`T \u00b41\n8\nCompute the target Q value y \u201c \u0159H\u00b41\nh\u201c0 \u03b3h\u02dcrt`h ` \u03b3HV\u03bepps1\nt`Hq\n// update the V -value network\n9\n\u03be \u00d0 \u03be \u00b4 \u03b7\u2207\u03be|| miniPt1,2u Q s\u03d5ips, aq \u00b4 V\u03bepsqq||2\n2\n// update the critic (Q-value networks)\n10\n\u03d51 \u00d0 \u03d51 \u00b4 \u03b7\u2207\u03d51 }Q\u03d51pst, atq \u00b4 y}2\n2\n11\n\u03d52 \u00d0 \u03d52 \u00b4 \u03b7\u2207\u03d52 }Q\u03d52pst, atq \u00b4 y}2\n2\n// update the actor\n12\nUpdate the actor network: \u03c8 \u00d0 \u03c8 ` \u03b7\u2207\u03c8 exp\n`\n\u03b2pminiPt1,2u Q\u03d5ips, aq \u00b4 V\u03bepsqq\n\u02d8\nlog \u03c0\u03c8pa|sq\n// update the target networks\n13\nif i mod n then\n14\ns\u03d51 \u00d0 s\u03d51 ` wp\u03d5 \u00b4 s\u03d51q\n15\ns\u03d52 \u00d0 s\u03d52 ` wp\u03d5 \u00b4 s\u03d52q\n16 Output: \u03c0\u03c8\nD.2\nMF Algorithms\nTD3+BC and IQL are trained for 1\u02c6106 iterations, with learning rate 3\u02c610\u00b44 for actor, critic and value networks. The actor,\ncritic, and value networks are all parameterized by 3-layer MLPs with 256 hidden units per layer. We use the ReLU activation\nfunction for each hidden layer. IQL learns a stochastic policy which outputs a Tanh-Normal distribution, while TD3+BC has\na deterministic policy with Tanh output activation. The hyperparameters for TD3+BC and IQL are provided in Table D.1.\nTD3+BC\nIQL\npolicy noise\n0.2\nexpectile\n0.7\nnoise clip\n0.5\n\u03b2\n3.0\npolicy update frequency\n2\nmax weight\n100.0\ntarget update frequence\n2\npolicy update frequence\n1\n\u03b1\n2.5\nadvantage normalization\nFalse\nEMA w\n0.005\nEMA w\n0.005\nTable D.1: Hyperparameters for training TD3+BC and IQL.\nThe baseline DD (Ajay et al., 2022) algorithm uses diffusion models trained with sequence length T \u201c 32 and number of\ndiffusion steps K \u201c 5. It requires additionally training an inverse dynamics model (IDM) for action prediction, which is\nparameterized by a 3-layer MLP with 1024 hidden units for each hidden layer and ReLU activation function. The dropout\nrate for the MLP is 0.1. The IDMs are trained for 2 \u02c6 106 iterations for each environment. For a fair comparison with the\nother DWM methods, DD uses N \u201c 3 internal sampling steps as DWM. We search over the same range of evaluation RTG\ngeval for DD and the other DWM methods.\n18\nDiffusion World Model\nD.3\nMB algorithms\nDWM-TD3BC, DWM-IQL and DWM-PQL are trained for 5 \u02c6 105 iterations. Table D.2 summarizes the hyperparameters\nwe search for each experiment. The other hyperparameters and network architectures are the same as original TD3+BC and\nIQL in above sections. DWM-IQL with \u03bb-return takes \u03bb \u201c 0.95, following Hafner et al. (2023).\nThe counterparts with one-step dynamics models are trained for 2 \u02c6 105 iterations due to a relatively fast convergence from\nour empirical observation. Most of the hyperparameters also follow TD3+BC and IQL. The PQL-type algorithms (O-PQL\nand DWM-PQL) further search the pessimism coefficient \u03ba (defined in Eq. (16)) among t0.01, 0.1, 1.0u.\nEnv\nEvaluation RTG\nH\nhopper-medium-v2\n[0.6, 0.7, 0.8]\n[1,3,5,7]\nwalker2d-medium-v2\n[0.6, 0.7, 0.8]\n[1,3,5,7]\nhalfcheetah-medium-v2\n[0.4, 0.5, 0.6]\n[1,3,5,7]\nhopper-medium-replay-v2\n[0.6, 0.7, 0.8]\n[1,3,5,7]\nwalker2d-medium-replay-v2\n[0.6, 0.7, 0.8]\n[1,3,5,7]\nhalfcheetah-medium-replay-v2\n[0.4, 0.5, 0.6]\n[1,3,5,7]\nhopper-medium-expert-v2\n[0.7, 0.8, 0.9]\n[1,3,5,7]\nwalker2d-medium-expert-v2\n[0.8, 0.9, 1.0]\n[1,3,5,7]\nhalfcheetah-medium-expert-v2\n[0.6, 0.7, 0.8]\n[1,3,5,7]\nTable D.2: List of the hyperparameters we search for DWM methods.\nE\nMore Results\nE.1\nDetailed Results of Long Horizon Planning with DWM\nThe section provides the detailed results of the experiments for long horizon planning with DWM in Section 4.1. Table E.1\nsummarizes the normalized returns (with means and standard deviations) of DWM-IQL and DWM-TD3BC for different\nsimulation horizons t1, 3, 7, 15, 31u.\n19\nDiffusion World Model\nReturn (mean\u02d8std)\nEnv.\nSimulation Horizon\nDWM-IQL\nDWM-TD3BC\nhopper-medium-v2\n1\n0.54 \u00b1 0.11\n0.68 \u00b1 0.12\n3\n0.55 \u00b1 0.10\n0.63 \u00b1 0.11\n7\n0.56 \u00b1 0.09\n0.66 \u00b1 0.13\n15\n0.58 \u00b1 0.12\n0.77 \u00b1 0.15\n31\n0.61 \u00b1 0.11\n0.79 \u00b1 0.15\nwalker2d-medium-v2\n1\n0.65 \u00b1 0.23\n0.56 \u00b1 0.13\n3\n0.74 \u00b1 0.11\n0.74 \u00b1 0.13\n7\n0.71 \u00b1 0.13\n0.74 \u00b1 0.11\n15\n0.66 \u00b1 0.15\n0.73 \u00b1 0.13\n31\n0.67 \u00b1 0.20\n0.75 \u00b1 0.12\nhalfcheetah-medium-v2\n1\n0.44 \u00b1 0.01\n0.35 \u00b1 0.03\n3\n0.44 \u00b1 0.01\n0.39 \u00b1 0.01\n7\n0.44 \u00b1 0.01\n0.40 \u00b1 0.01\n15\n0.44 \u00b1 0.02\n0.40 \u00b1 0.01\n31\n0.44 \u00b1 0.01\n0.40 \u00b1 0.01\nhopper-medium-replay-v2\n1\n0.18 \u00b1 0.06\n0.52 \u00b1 0.21\n3\n0.37 \u00b1 0.18\n0.44 \u00b1 0.23\n7\n0.39 \u00b1 0.14\n0.52 \u00b1 0.28\n15\n0.37 \u00b1 0.18\n0.67 \u00b1 0.25\n31\n0.37 \u00b1 0.15\n0.59 \u00b1 0.22\nwalker2d-medium-replay-v2\n1\n0.32 \u00b1 0.15\n0.13 \u00b1 0.02\n3\n0.27 \u00b1 0.24\n0.19 \u00b1 0.10\n7\n0.25 \u00b1 0.20\n0.22 \u00b1 0.14\n15\n0.26 \u00b1 0.19\n0.22 \u00b1 0.10\n31\n0.27 \u00b1 0.19\n0.17 \u00b1 0.12\nhalfcheetah-medium-replay-v2\n1\n0.38 \u00b1 0.05\n0.02 \u00b1 0.00\n3\n0.39 \u00b1 0.02\n0.17 \u00b1 0.05\n7\n0.39 \u00b1 0.02\n0.22 \u00b1 0.03\n15\n0.38 \u00b1 0.03\n0.26 \u00b1 0.03\n31\n0.37 \u00b1 0.03\n0.26 \u00b1 0.05\nhopper-medium-expert-v2\n1\n0.86 \u00b1 0.25\n0.88 \u00b1 0.17\n3\n0.90 \u00b1 0.19\n0.94 \u00b1 0.22\n7\n0.88 \u00b1 0.28\n0.93 \u00b1 0.24\n15\n0.85 \u00b1 0.20\n0.91 \u00b1 0.19\n31\n0.84 \u00b1 0.23\n0.93 \u00b1 0.23\nwalker2d-medium-expert-v2\n1\n0.80 \u00b1 0.22\n0.74 \u00b1 0.21\n3\n1.02 \u00b1 0.09\n0.89 \u00b1 0.13\n7\n0.98 \u00b1 0.2\n0.82 \u00b1 0.19\n15\n1.06 \u00b1 0.05\n0.84 \u00b1 0.14\n31\n1.05 \u00b1 0.06\n0.87 \u00b1 0.03\nhalfcheetah-medium-expert-v2\n1\n0.60 \u00b1 0.18\n0.39 \u00b1 0.01\n3\n0.52 \u00b1 0.14\n0.43 \u00b1 0.07\n7\n0.63 \u00b1 0.13\n0.44 \u00b1 0.03\n15\n0.66 \u00b1 0.14\n0.50 \u00b1 0.08\n31\n0.65 \u00b1 0.17\n0.49 \u00b1 0.09\nTable E.1: Comparison of the normalized returns with different simulation horizons for DWM-TD3BC and DWM-IQL. The\nreported values are the best performances across different RTG values (listed in Table D.2).\nE.2\nAlgorithm 3.1 with Transformer-based World Model\nFollowing the same protocol as DWM, the Transformer model is trained to predict future state-reward sequences, conditioning\non the initial state-action pair. We use a 4-layer transformer architecture with 4 attention heads, similar to the one in\nZheng et al. (2022). Specifically, all the actions except for the first one are masked out as zeros in the state-action-\nreward sequences. Distinct from the original DT (Chen et al., 2021) where the loss function only contains the action\nprediction error, here the Transformer is trained with state and reward prediction loss. The Transformers are trained with\noptimizers and hyperparameters following ODT (Zheng et al., 2022). The evaluation RTG for Transformers takes values\n3600{400 \u201c 9.0, 5000{550 \u00ab 9.1, 6000{1200 \u201c 5.0 for hopper, walker2d and halfcheetah environments, respectively. The\n20\nDiffusion World Model\ncomplete results of T-TD3BC and T-IQL are provided in Table E.2 and Table E.3 respectively.\nSimulation Horizon\nEnv\n1\n3\n5\n7\nhopper-m\n0.50\u02d80.05\n0.57\u02d80.08\n0.58\u02d80.08\n0.57\u02d80.08\nwalker2d-m\n0.36\u02d80.15\n0.40\u02d80.20\n0.60\u02d80.16\n0.53\u02d80.17\nhalfcheetah-m\n0.18\u02d80.07\n0.41 \u02d80.03\n0.38\u02d80.08\n0.42\u02d80.03\nhopper-mr\n0.24\u02d80.01\n0.23\u02d80.05\n0.25\u02d80.06\n0.22\u02d80.08\nwalker2d-mr\n0.12 \u02d80.04\n0.09\u02d80.05\n0.13\u02d80.06\n0.12\u02d80.05\nhalfcheetah-mr\n0.40\u02d80.01\n0.39\u02d80.02\n0.39\u02d80.03\n0.39\u02d80.02\nhopper-me\n0.41\u02d80.13\n0.57\u02d80.19\n0.66\u02d80.25\n0.52\u02d80.15\nwalker2d-me\n0.34\u02d80.22\n0.58\u02d80.15\n0.58\u02d80.26\n0.46\u02d80.37\nhalfcheetah-me\n0.14\u02d80.06\n0.31\u02d80.09\n0.36\u02d80.17\n0.29\u02d80.12\nTable E.2: The normalized returns of T-TD3BC.\nSimulation Horizon\nEnv\n1\n3\n5\n7\nhopper-m\n0.48\u02d80.08\n0.54\u02d80.10\n0.55\u02d80.08\n0.51\u02d80.09\nwalker2d-m\n0.54\u02d80.18\n0.62\u02d80.19\n0.72\u02d80.12\n0.72\u02d80.14\nhalfcheetah-m\n0.42\u02d80.03\n0.42\u02d80.02\n0.43\u02d80.01\n0.43\u02d80.01\nhopper-mr\n0.17\u02d80.05\n0.24\u02d80.09\n0.26\u02d80.09\n0.20\u02d80.07\nwalker2d-mr\n0.17\u02d80.12\n0.17\u02d80.14\n0.23\u02d80.12\n0.16\u02d80.11\nhalfcheetah-mr\n0.38\u02d80.04\n0.39\u02d80.01\n0.38\u02d80.04\n0.39\u02d8 0.03\nhopper-me\n0.62\u02d80.16\n0.59\u02d80.21\n0.47\u02d80.21\n0.47\u02d80.21\nwalker2d-me\n0.67\u02d80.23\n0.87\u02d80.21\n1.03\u02d80.09\n0.71\u02d80.22\nhalfcheetah-me\n0.39\u02d80.19\n0.43\u02d80.13\n0.44\u02d80.08\n0.43\u02d80.09\nTable E.3: The normalized returns of T-IQL.\nE.3\nDetails Results for Diffusion Steps and Inference Step Ratio\nThe section provides more details about the ablated experiments for diffusion steps and inference step ratio in Section 4.2.\nLet \u03c4 denote a length-T subtrajectory pst, at, rt, st`1, . . . , st`T \u00b41, rt`T \u00b41q. The average prediction errors of a DWM p\u03b8\nfor states and rewards along sequences are defined as:\ns\u03f5s \u201c E\u03c4\u201eDoffline,pst\u201ep\u03b8p\u00a8|s1,a1,gevalq\n\u00ab\n1\nT\nt`T \u00b41\n\u00ff\nt1\u201ct\n}pst1 \u00b4 st1}2\nff\n, and\ns\u03f5r \u201c E\u03c4\u201eDoffline,pst\u201ep\u03b8p\u00a8|s1,a1,gevalq\n\u00ab\n1\nT\nt`T \u00b41\n\u00ff\nt1\u201ct\n}prt1 \u00b4 rt1}2\nff\n.\n(17)\nWe first note that all the prediction errors depend on the evaluation RTG geval. For the ease of clean presentation, we search\nover multiple values of geval, listed in Table D.2, and report the best results.\nIn addition to Figure 4.2, the average prediction errors for diffusion models with T \u201c 32 (longer sequence) and diffusion\nsteps K P t5, 10, 20u are shown in Figure E.1. Based on the results, K \u201c 10 and rinfer \u201c 0.5 are selected to strike a good\nbalance between prediction accuracy and inference speed. The corresponding numerical results are listed in Table E.4 and\nE.5.\n21\nDiffusion World Model\nFigure E.1: Average observation and reward prediction errors (across 9 tasks and simulation horizon H P r31s) for DWM\ntrained with T \u201c 32 and different diffusion steps K, as the inference step ratio rratio changes.\nDiffusion Step K\nInfer Step Ratio rratio\nInfer Step N\nObs. Error\nReward Error\n5\n0.2\n1\n2.815\n0.009\n0.3\n2\n1.028\n0.001\n0.5\n3\n0.873\n0.001\n1.0\n5\n0.851\n0.001\n10\n0.2\n1\n3.114\n0.011\n0.3\n2\n1.601\n0.002\n0.5\n3\n1.028\n0.001\n1.0\n5\n0.943\n0.001\n20\n0.2\n1\n3.052\n0.014\n0.3\n2\n1.595\n0.002\n0.5\n3\n0.963\n0.001\n1.0\n5\n0.890\n0.001\n30\n0.2\n1\n3.112\n0.018\n0.3\n2\n1.623\n0.003\n0.5\n3\n0.993\n0.001\n1.0\n5\n0.896\n0.001\n50\n0.2\n1\n3.275\n0.022\n0.3\n2\n1.726\n0.003\n0.5\n3\n1.031\n0.001\n1.0\n5\n0.944\n0.001\n100\n0.2\n1\n3.239\n0.023\n0.3\n2\n1.732\n0.003\n0.5\n3\n1.021\n0.001\n1.0\n5\n0.923\n0.001\nTable E.4: The average (across tasks and simulation horizon H P r7s) observation and reward prediction errors for DWM\nwith T \u201c 8 and different inference steps N \u201c rrinfer \u00a8 Ks.\n22\nDiffusion World Model\nDiffusion Step K\nInfer Step Ratio rinfer\nInfer Step N\nObs. Error\nReward Error\n5\n0.2\n1\n7.390\n0.030\n0.3\n2\n6.003\n0.015\n0.5\n3\n5.065\n0.010\n1.0\n5\n4.853\n0.010\n10\n0.2\n1\n6.650\n0.029\n0.3\n2\n5.799\n0.016\n0.5\n3\n4.811\n0.010\n1.0\n5\n5.157\n0.011\n20\n0.2\n1\n6.273\n0.031\n0.3\n2\n5.254\n0.015\n0.5\n3\n4.794\n0.011\n1.0\n5\n5.088\n0.012\nTable E.5: The average (across tasks and simulation horizon H P r31s) observation and reward prediction errors for DWM\nwith T \u201c 32 and different inference steps N \u201c rrinfer \u00a8 Ks.\nE.4\nDetailed Results of OOD Evaluation RTG Value\nThe section provides more details about ablated experiments for evaluation RTG values in Section 4.2.\nE.4.1\nDATA DISTRIBUTION\nThe normalized discounted return (as RTG labels in training) for the entire D4RL dataset over the nine tasks are analyzed\nin Fig. E.2. Compared with RTG values in our experiments as Table D.2, the data maximum is usually smaller than the\nevaluation RTG values that leads to higher performances, as observed in our empirical experiments.\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nnormalized discounted return\n0\n5\n10\n15\ndensity\nhopper-medium-v2\nmax return=0.61\n0.0\n0.2\n0.4\n0.6\n0.8\nnormalized discounted return\n0\n1\n2\n3\n4\ndensity\nhopper-medium-replay-v2\nmax return=0.64\n0.4\n0.5\n0.6\n0.7\nnormalized discounted return\n0\n10\n20\n30\ndensity\nhopper-medium-expert-v2\nmax return=0.68\n0.0\n0.2\n0.4\n0.6\nnormalized discounted return\n0\n2\n4\n6\n8\ndensity\nwalker2d-medium-v2\nmax return=0.54\n0.0\n0.2\n0.4\n0.6\nnormalized discounted return\n0\n2\n4\ndensity\nwalker2d-medium-replay-v2\nmax return=0.54\n0.0\n0.2\n0.4\n0.6\nnormalized discounted return\n0\n5\n10\ndensity\nwalker2d-medium-expert-v2\nmax return=0.65\n0.0\n0.1\n0.2\n0.3\n0.4\nnormalized discounted return\n0\n5\n10\n15\ndensity\nhalfcheetah-medium-v2\nmax return=0.37\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nnormalized discounted return\n0\n2\n4\n6\n8\ndensity\nhalfcheetah-medium-replay-v2\nmax return=0.35\n0.0\n0.2\n0.4\n0.6\nnormalized discounted return\n0.0\n2.5\n5.0\n7.5\n10.0\ndensity\nhalfcheetah-medium-expert-v2\nmax return=0.63\nFigure E.2: Normalized discounted returns for each environment.\n23\nDiffusion World Model\nE.4.2\nIN-DISTRIBUTION V.S. OUT-OF-DISTRIBUTION RTG\nAnalogous to Equation (17), we can define the breakdown prediction errors for each timestep t1, t \u010f t1 \u010f t ` T \u00b4 1.\nFigure E.3 and Figure E.4 plot the results, using different values of geval. The OOD RTGs generally perform better.\nIt is worth noting that the prediction error naturally grows as the horizon increases. Intuitively, given a fixed environment,\nthe initial state of the D4RL datasets are very similar, whereas the subsequent states after multiple timesteps become quite\ndifferent.\nFigure E.3: The breakdown prediction errors of DWM at each prediction timestep with different RTGs. The DWM is trained\nwith T \u201c 8 and K \u201c 5.\n24\nDiffusion World Model\nFigure E.4: The breakdown prediction errors of DWM at each prediction timestep with different RTG. The DWM is trained\nwith T \u201c 32 and K \u201c 10.\nE.5\nAdditional Experiments: RTG Relabeling and Model Fine-tuning\nUnlike dynamic programming in traditional RL, sequential modeling methods like diffusion models and DT are suspected\nto fail to stitch suboptimal trajectories. RTG relabeling is proposed to alleviate this problem for DT (Yamagata et al., 2023),\nthrough iteratively relabeling RTG g from training dataset to be:\n\u02dcgt \u201c rt ` \u03b3 maxpgt`1, pV pst`1qq \u201c maxpgt, rt ` pV pst`1q,\n(18)\n25\nDiffusion World Model\nwhere the pV function is separately learned from the offline dataset using CQL (Kumar et al., 2020), and the max operator is\nused to prevent underestimation due to the pessimism of pV . The original formulation in Yamagata et al. (2023) does not\ninclude the \u03b3 term as DT uses undiscounted RTG, i.e., \u03b3 \u201c 1.\nWe apply the RTG relabeling for DWM fine-tuning in the policy learning phase of vanilla DWM-IQL algorithm, without\nthe \u03bb-return technique. The value function pV comes from the IQL algorithm. We take the first 10% steps of the entire\npolicy learning as warm up steps, where we do not apply RTG relabeling. This is because pV can be inaccurate at the\nbeginning of training. The modified algorithm DWM-IQL(R) achieves an average score of 0.61, improved over score 0.57\nfor DWM-IQL(w/o \u03bb), under exactly the same training and test settings. Results are provided in Table E.6 with details in\nTable E.7. Nonetheless, the improvement is of limited unity compared with the \u03bb-return, thus we do not include it in the\nfinal design.\nEnv\nDWM-IQL(w/o \u03bb)\nDWM-IQL(w/ RTG Relabel)\nhopper-m\n0.50 \u02d8 0.08\n0.59 \u00b1 0.13\nwalker2d-m\n0.62 \u02d8 0.19\n0.65 \u00b1 0.17\nhalfcheetah-m\n0.46 \u02d8 0.01\n0.47 \u00b1 0.01\nhopper-mr\n0.29 \u02d8 0.04\n0.27 \u00b1 0.02\nwalker2d-mr\n0.27 \u02d8 0.09\n0.32 \u00b1 0.15\nhalfcheetah-mr\n0.43 \u02d8 0.01\n0.43 \u00b1 0.02\nhopper-me\n0.78 \u02d8 0.24\n0.88 \u00b1 0.26\nwalker2d-me\n1.08 \u02d8 0.03\n1.1 \u00b1 0.0\nhalfcheetah-me\n0.73 \u02d8 0.14\n0.79 \u00b1 0.10\nAvg.\n0.57\n0.61\nTable E.6: The results of finetuning DMW via RTG relabeling in the policy training phase: normalized return (mean \u02d8 std)\nhopper-medium-v2\nhopper-medium-replay-v2\nhopper-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.54 \u02d8 0.06\n1\n0.6\n0.21 \u02d8 0.02\n1\n0.7\n0.71 \u02d8 0.30\n3\n0.6\n0.51 \u02d8 0.09\n3\n0.6\n0.22 \u02d8 0.02\n3\n0.7\n0.79 \u02d8 0.24\n5\n0.6\n0.51 \u02d8 0.12\n5\n0.6\n0.23 \u02d8 0.01\n5\n0.7\n0.71 \u02d8 0.18\n7\n0.6\n0.59 \u02d8 0.13\n7\n0.6\n0.23 \u02d8 0.01\n7\n0.7\n0.79 \u02d8 0.24\n1\n0.7\n0.51 \u02d8 0.04\n1\n0.7\n0.21 \u02d8 0.01\n1\n0.8\n0.59 \u02d8 0.25\n3\n0.7\n0.49 \u02d8 0.07\n3\n0.7\n0.23 \u02d8 0.01\n3\n0.7\n0.79 \u02d8 0.24\n5\n0.7\n0.49 \u02d8 0.08\n5\n0.7\n0.25 \u02d8 0.02\n5\n0.7\n0.71 \u02d8 0.18\n7\n0.7\n0.48 \u02d8 0.07\n7\n0.7\n0.25 \u02d8 0.03\n7\n0.7\n0.79 \u02d8 0.24\n1\n0.8\n0.52 \u02d8 0.04\n1\n0.8\n0.2 \u02d8 0.03\n1\n0.9\n0.60 \u02d8 0.28\n3\n0.8\n0.48 \u02d8 0.06\n3\n0.8\n0.23 \u02d8 0.03\n3\n0.9\n0.88 \u02d8 0.26\n5\n0.8\n0.48 \u02d8 0.08\n5\n0.8\n0.26 \u02d8 0.05\n5\n0.9\n0.83 \u02d8 0.23\n7\n0.8\n0.47 \u02d8 0.07\n7\n0.8\n0.27 \u02d8 0.02\n7\n0.9\n0.75 \u02d8 0.28\nwalker2d-medium-v2\nwalker2d-medium-replay-v2\nwalker2d-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.59 \u02d8 0.19\n1\n0.6\n0.25 \u02d8 0.12\n1\n0.8\n1.08 \u02d8 0.00\n3\n0.6\n0.57 \u02d8 0.18\n3\n0.6\n0.21 \u02d8 0.11\n3\n0.8\n1.07 \u02d8 0.06\n5\n0.6\n0.58 \u02d8 0.21\n5\n0.6\n0.18 \u02d8 0.05\n5\n0.8\n1.09 \u02d8 0.00\n7\n0.6\n0.50 \u02d8 0.2\n7\n0.6\n0.15 \u02d8 0.09\n7\n0.8\n1.09 \u02d8 0.02\n1\n0.7\n0.65 \u02d8 0.17\n1\n0.7\n0.32 \u02d8 0.15\n1\n0.9\n1.08 \u02d8 0.01\n3\n0.7\n0.55 \u02d8 0.19\n3\n0.7\n0.16 \u02d8 0.08\n3\n0.9\n1.07 \u02d8 0.05\n5\n0.7\n0.54 \u02d8 0.23\n5\n0.7\n0.20 \u02d8 0.03\n5\n0.9\n1.10 \u02d8 0.00\n7\n0.7\n0.52 \u02d8 0.18\n7\n0.7\n0.15 \u02d8 0.10\n7\n0.9\n1.10 \u02d8 0.01\n1\n0.8\n0.60 \u02d8 0.22\n1\n0.8\n0.25 \u02d8 0.11\n1\n1.0\n1.08 \u02d8 0.0\n3\n0.8\n0.57 \u02d8 0.21\n3\n0.8\n0.18 \u02d8 0.08\n3\n1.0\n1.08 \u02d8 0.03\n5\n0.8\n0.56 \u02d8 0.21\n5\n0.8\n0.21 \u02d8 0.02\n5\n1.0\n1.08 \u02d8 0.04\n7\n0.8\n0.54 \u02d8 0.23\n7\n0.8\n0.18 \u02d8 0.12\n7\n1.0\n1.08 \u02d8 0.04\nhalfcheetah-medium-v2\nhalfcheetah-medium-replay-v2\nhalfcheetah-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.4\n0.46 \u02d8 0.01\n1\n0.4\n0.43 \u02d8 0.02\n1\n0.6\n0.69 \u02d8 0.12\n3\n0.4\n0.47 \u02d8 0.01\n3\n0.4\n0.43 \u02d8 0.03\n3\n0.6\n0.76 \u02d8 0.11\n5\n0.4\n0.47 \u02d8 0.01\n5\n0.4\n0.42 \u02d8 0.01\n5\n0.6\n0.76 \u02d8 0.16\n7\n0.4\n0.47 \u02d8 0.01\n7\n0.4\n0.42 \u02d8 0.02\n7\n0.6\n0.79 \u02d8 0.10\n1\n0.5\n0.45 \u02d8 0.01\n1\n0.5\n0.41 \u02d8 0.03\n1\n0.7\n0.70 \u02d8 0.16\n3\n0.5\n0.45 \u02d8 0.01\n3\n0.5\n0.41 \u02d8 0.01\n3\n0.7\n0.74 \u02d8 0.14\n5\n0.5\n0.44 \u02d8 0.02\n5\n0.5\n0.4 \u02d8 0.01\n5\n0.7\n0.75 \u02d8 0.12\n7\n0.5\n0.45 \u02d8 0.01\n7\n0.5\n0.4 \u02d8 0.03\n7\n0.7\n0.75 \u02d8 0.11\n1\n0.6\n0.43 \u02d8 0.02\n1\n0.6\n0.38 \u02d8 0.03\n1\n0.8\n0.68 \u02d8 0.20\n3\n0.6\n0.43 \u02d8 0.01\n3\n0.6\n0.38 \u02d8 0.03\n3\n0.8\n0.73 \u02d8 0.19\n5\n0.6\n0.43 \u02d8 0.01\n5\n0.6\n0.39 \u02d8 0.03\n5\n0.8\n0.72 \u02d8 0.13\n7\n0.6\n0.43 \u02d8 0.02\n7\n0.6\n0.38 \u02d8 0.03\n7\n0.8\n0.75 \u02d8 0.13\nTable E.7: The normalized returns of DWM-IQL (w/ RTG relabel) with diffusion step K \u201c 5, sampling steps N \u201c 3, and\nsimulation horizon H P t1, 3, 5, 7u.\n26\nDiffusion World Model\nE.6\nExample Raw Results\nWhile we cannot enumerate all the experiment results, we provide some raw results of DWM methods. Table E.8 and E.9\nreport the performance of DWM-IQL (without \u03bb-return) and DQM-TD3BC with parameters swept over both simulation\nhorizon and RTG values. Table E.10 report the performance of DWM-PQL with parameters swept over pessimism coefficient\n\u03bb and RTG values, for a fixed horizon H \u201c 5.\nhopper-medium-v2\nhopper-medium-replay-v2\nhopper-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.47 \u02d8 0.05\n1\n0.6\n0.26 \u02d8 0.06\n1\n0.7\n0.57 \u02d8 0.26\n3\n0.6\n0.47 \u02d8 0.09\n3\n0.6\n0.23 \u02d8 0.01\n3\n0.7\n0.66 \u02d8 0.21\n5\n0.6\n0.46 \u02d8 0.06\n5\n0.6\n0.25 \u02d8 0.04\n5\n0.7\n0.78 \u02d8 0.24\n7\n0.6\n0.49 \u02d8 0.09\n7\n0.6\n0.23 \u02d8 0.02\n7\n0.7\n0.73 \u02d8 0.27\n1\n0.7\n0.5 \u02d8 0.08\n1\n0.7\n0.29 \u02d8 0.04\n1\n0.8\n0.43 \u02d8 0.19\n3\n0.7\n0.46 \u02d8 0.07\n3\n0.7\n0.25 \u02d8 0.02\n3\n0.8\n0.7 \u02d8 0.19\n5\n0.7\n0.48 \u02d8 0.06\n5\n0.7\n0.23 \u02d8 0.01\n5\n0.8\n0.72 \u02d8 0.21\n7\n0.7\n0.48 \u02d8 0.07\n7\n0.7\n0.26 \u02d8 0.01\n7\n0.8\n0.75 \u02d8 0.25\n1\n0.8\n0.49 \u02d8 0.1\n1\n0.8\n0.2 \u02d8 0.05\n1\n0.9\n0.47 \u02d8 0.14\n3\n0.8\n0.47 \u02d8 0.06\n3\n0.8\n0.19 \u02d8 0.03\n3\n0.9\n0.52 \u02d8 0.26\n5\n0.8\n0.47 \u02d8 0.07\n5\n0.8\n0.3 \u02d8 0.06\n5\n0.9\n0.59 \u02d8 0.16\n7\n0.8\n0.49 \u02d8 0.08\n7\n0.8\n0.25 \u02d8 0.05\n7\n0.9\n0.61 \u02d8 0.22\nwalker2d-medium-v2\nwalker2d-medium-replay-v2\nwalker2d-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.46 \u02d8 0.22\n1\n0.6\n0.27 \u02d8 0.09\n1\n0.8\n0.97 \u02d8 0.22\n3\n0.6\n0.55 \u02d8 0.23\n3\n0.6\n0.15 \u02d8 0.02\n3\n0.8\n1.03 \u02d8 0.12\n5\n0.6\n0.54 \u02d8 0.25\n5\n0.6\n0.13 \u02d8 0.01\n5\n0.8\n1.05 \u02d8 0.09\n7\n0.6\n0.5 \u02d8 0.2\n7\n0.6\n0.11 \u02d8 0.0\n7\n0.8\n1.08 \u02d8 0.03\n1\n0.7\n0.62 \u02d8 0.19\n1\n0.7\n0.26 \u02d8 0.14\n1\n0.9\n1.01 \u02d8 0.13\n3\n0.7\n0.59 \u02d8 0.2\n3\n0.7\n0.13 \u02d8 0.12\n3\n0.9\n1.05 \u02d8 0.1\n5\n0.7\n0.56 \u02d8 0.2\n5\n0.7\n0.11 \u02d8 0.01\n5\n0.9\n1.05 \u02d8 0.11\n7\n0.7\n0.47 \u02d8 0.23\n7\n0.7\n0.11 \u02d8 0.0\n7\n0.9\n0.96 \u02d8 0.25\n1\n0.8\n0.59 \u02d8 0.21\n1\n0.8\n0.26 \u02d8 0.17\n1\n1.0\n0.97 \u02d8 0.14\n3\n0.8\n0.6 \u02d8 0.2\n3\n0.8\n0.16 \u02d8 0.12\n3\n1.0\n1.0 \u02d8 0.11\n5\n0.8\n0.56 \u02d8 0.21\n5\n0.8\n0.13 \u02d8 0.11\n5\n1.0\n1.03 \u02d8 0.16\n7\n0.8\n0.59 \u02d8 0.19\n7\n0.8\n0.14 \u02d8 0.1\n7\n1.0\n0.97 \u02d8 0.21\nhalfcheetah-medium-v2\nhalfcheetah-medium-replay-v2\nhalfcheetah-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.4\n0.45 \u02d8 0.01\n1\n0.4\n0.42 \u02d8 0.01\n1\n0.6\n0.61 \u02d8 0.16\n3\n0.4\n0.46 \u02d8 0.01\n3\n0.4\n0.43 \u02d8 0.01\n3\n0.6\n0.7 \u02d8 0.13\n5\n0.4\n0.46 \u02d8 0.01\n5\n0.4\n0.42 \u02d8 0.02\n5\n0.6\n0.73 \u02d8 0.15\n7\n0.4\n0.46 \u02d8 0.01\n7\n0.4\n0.43 \u02d8 0.01\n7\n0.6\n0.73 \u02d8 0.14\n1\n0.5\n0.43 \u02d8 0.02\n1\n0.5\n0.39 \u02d8 0.01\n1\n0.7\n0.43 \u02d8 0.08\n3\n0.5\n0.44 \u02d8 0.01\n3\n0.5\n0.41 \u02d8 0.01\n3\n0.7\n0.66 \u02d8 0.11\n5\n0.5\n0.44 \u02d8 0.02\n5\n0.5\n0.39 \u02d8 0.03\n5\n0.7\n0.67 \u02d8 0.13\n7\n0.5\n0.44 \u02d8 0.02\n7\n0.5\n0.39 \u02d8 0.03\n7\n0.7\n0.68 \u02d8 0.17\n1\n0.6\n0.42 \u02d8 0.02\n1\n0.6\n0.38 \u02d8 0.03\n1\n0.8\n0.59 \u02d8 0.17\n3\n0.6\n0.43 \u02d8 0.01\n3\n0.6\n0.39 \u02d8 0.02\n3\n0.8\n0.71 \u02d8 0.13\n5\n0.6\n0.43 \u02d8 0.01\n5\n0.6\n0.39 \u02d8 0.03\n5\n0.8\n0.69 \u02d8 0.14\n7\n0.6\n0.43 \u02d8 0.01\n7\n0.6\n0.39 \u02d8 0.03\n7\n0.8\n0.67 \u02d8 0.14\nTable E.8: The normalized return of DWM-IQL (without \u03bb-return) with diffusion step K \u201c 5, N \u201c 3, and simulation\nhorizon H P t1, 3, 5, 7u.\n27\nDiffusion World Model\nhopper-medium-v2\nhopper-medium-replay-v2\nhopper-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.62 \u02d8 0.1\n1\n0.6\n0.4 \u02d8 0.12\n1\n0.7\n1.02 \u02d8 0.18\n3\n0.6\n0.65 \u02d8 0.11\n3\n0.6\n0.3 \u02d8 0.06\n3\n0.7\n1.0 \u02d8 0.21\n5\n0.6\n0.63 \u02d8 0.12\n5\n0.6\n0.29 \u02d8 0.05\n5\n0.7\n0.98 \u02d8 0.17\n7\n0.6\n0.63 \u02d8 0.11\n7\n0.6\n0.29 \u02d8 0.05\n7\n0.7\n1.03 \u02d8 0.14\n1\n0.7\n0.6 \u02d8 0.11\n1\n0.7\n0.27 \u02d8 0.11\n1\n0.8\n0.94 \u02d8 0.24\n3\n0.7\n0.65 \u02d8 0.1\n3\n0.7\n0.38 \u02d8 0.06\n3\n0.8\n0.94 \u02d8 0.24\n5\n0.7\n0.62 \u02d8 0.14\n5\n0.7\n0.36 \u02d8 0.14\n5\n0.8\n0.92 \u02d8 0.15\n7\n0.7\n0.65 \u02d8 0.11\n7\n0.7\n0.35 \u02d8 0.08\n7\n0.8\n0.9 \u02d8 0.28\n1\n0.8\n0.58 \u02d8 0.07\n1\n0.8\n0.25 \u02d8 0.06\n1\n0.9\n0.95 \u02d8 0.21\n3\n0.8\n0.63 \u02d8 0.13\n3\n0.8\n0.53 \u02d8 0.09\n3\n0.9\n0.99 \u02d8 0.17\n5\n0.8\n0.63 \u02d8 0.11\n5\n0.8\n0.43 \u02d8 0.18\n5\n0.9\n0.97 \u02d8 0.2\n7\n0.8\n0.62 \u02d8 0.09\n7\n0.8\n0.4 \u02d8 0.11\n7\n0.9\n0.97 \u02d8 0.17\nwalker2d-medium-v2\nwalker2d-medium-replay-v2\nwalker2d-medium-expert-v2\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\nSimulation Horizon\nRTG\nReturn (Mean \u02d8 Std)\n1\n0.6\n0.7 \u02d8 0.15\n1\n0.6\n0.46 \u02d8 0.19\n1\n0.8\n1.08 \u02d8 0.01\n3\n0.6\n0.67 \u02d8 0.15\n3\n0.6\n0.29 \u02d8 0.16\n3\n0.8\n1.09 \u02d8 0.0\n5\n0.6\n0.61 \u02d8 0.2\n5\n0.6\n0.3 \u02d8 0.18\n5\n0.8\n1.09 \u02d8 0.0\n7\n0.6\n0.6 \u02d8 0.17\n7\n0.6\n0.23 \u02d8 0.17\n7\n0.8\n1.1 \u02d8 0.0\n1\n0.7\n0.65 \u02d8 0.17\n1\n0.7\n0.37 \u02d8 0.14\n1\n0.9\n1.08 \u02d8 0.03\n3\n0.7\n0.68 \u02d8 0.17\n3\n0.7\n0.31 \u02d8 0.19\n3\n0.9\n1.09 \u02d8 0.0\n5\n0.7\n0.63 \u02d8 0.12\n5\n0.7\n0.27 \u02d8 0.18\n5\n0.9\n1.09 \u02d8 0.02\n7\n0.7\n0.63 \u02d8 0.2\n7\n0.7\n0.28 \u02d8 0.28\n7\n0.9\n1.09 \u02d8 0.04\n1\n0.8\n0.64 \u02d8 0.13\n1\n0.8\n0.43 \u02d8 0.2\n1\n1.0\n1.08 \u02d8 0.0\n3\n0.8\n0.7 \u02d8 0.16\n3\n0.8\n0.31 \u02d8 0.14\n3\n1.0\n1.09 \u02d8 0.01\n5\n0.8\n0.68 \u02d8 0.15\n5\n0.8\n0.37 \u02d8 0.22\n5\n1.0\n1.09 \u02d8 0.0\n7\n0.8\n0.61 \u02d8 0.19\n7\n0.8\n0.29 \u02d8 0.23\n7\n1.0\n1.1 \u02d8 0.0\nhalfcheetah-medium-v2\nhalfcheetah-medium-replay-v2\nhalfcheetah-medium-expert-v2\n1\n0.4\n0.44 \u02d8 0.01\n1\n0.4\n0.43 \u02d8 0.02\n1\n0.6\n0.44 \u02d8 0.14\n3\n0.4\n0.46 \u02d8 0.01\n3\n0.4\n0.43 \u02d8 0.01\n3\n0.6\n0.58 \u02d8 0.17\n5\n0.4\n0.45 \u02d8 0.01\n5\n0.4\n0.43 \u02d8 0.02\n5\n0.6\n0.71 \u02d8 0.16\n7\n0.4\n0.45 \u02d8 0.01\n7\n0.4\n0.43 \u02d8 0.02\n7\n0.6\n0.75 \u02d8 0.16\n1\n0.5\n0.43 \u02d8 0.02\n1\n0.5\n0.41 \u02d8 0.03\n1\n0.7\n0.43 \u02d8 0.08\n3\n0.5\n0.44 \u02d8 0.01\n3\n0.5\n0.41 \u02d8 0.02\n3\n0.7\n0.66 \u02d8 0.11\n5\n0.5\n0.44 \u02d8 0.01\n5\n0.5\n0.41 \u02d8 0.01\n5\n0.7\n0.69 \u02d8 0.13\n7\n0.5\n0.44 \u02d8 0.02\n7\n0.5\n0.41 \u02d8 0.02\n7\n0.7\n0.49 \u02d8 0.13\n1\n0.6\n0.42 \u02d8 0.02\n1\n0.6\n0.39 \u02d8 0.03\n1\n0.8\n0.66 \u02d8 0.17\n3\n0.6\n0.43 \u02d8 0.01\n3\n0.6\n0.39 \u02d8 0.02\n3\n0.8\n0.67 \u02d8 0.17\n5\n0.6\n0.43 \u02d8 0.01\n5\n0.6\n0.39 \u02d8 0.03\n5\n0.8\n0.67 \u02d8 0.2\n7\n0.6\n0.43 \u02d8 0.01\n7\n0.6\n0.39 \u02d8 0.03\n7\n0.8\n0.65 \u02d8 0.14\nTable E.9: The normalized return of DWM-TD3BC with diffusion step K \u201c 5, N \u201c 0.5, and simulation horizon\nH P t1, 3, 5, 7u.\n28\nDiffusion World Model\nhopper-medium-v2\nhopper-medium-replay-v2\nhopper-medium-expert-v2\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\n0.01\n0.6\n0.49 \u02d8 0.11\n0.01\n0.6\n0.39 \u02d8 0.03\n0.01\n0.7\n0.78 \u02d8 0.18\n0.01\n0.7\n0.48 \u02d8 0.08\n0.01\n0.7\n0.26 \u02d8 0.02\n0.01\n0.8\n0.80 \u02d8 0.18\n0.01\n0.8\n0.48 \u02d8 0.10\n0.01\n0.8\n0.32 \u02d8 0.07\n0.01\n0.9\n0.65 \u02d8 0.16\n0.1\n0.6\n0.47 \u02d8 0.04\n0.1\n0.6\n0.28 \u02d8 0.06\n0.1\n0.7\n0.74 \u02d8 0.27\n0.1\n0.7\n0.49 \u02d8 0.07\n0.1\n0.7\n0.33 \u02d8 0.02\n0.1\n0.8\n0.75 \u02d8 0.21\n0.1\n0.8\n0.48 \u02d8 0.08\n0.1\n0.8\n0.28 \u02d8 0.04\n0.1\n0.9\n0.71 \u02d8 0.19\n1.0\n0.6\n0.48 \u02d8 0.07\n1.0\n0.6\n0.24 \u02d8 0.02\n1.0\n0.7\n0.63 \u02d8 0.13\n1.0\n0.7\n0.50 \u02d8 0.09\n1.0\n0.7\n0.25 \u02d8 0.03\n1.0\n0.8\n0.52 \u02d8 0.13\n1.0\n0.8\n0.47 \u02d8 0.07\n1.0\n0.8\n0.27 \u02d8 0.05\n1.0\n0.9\n0.48 \u02d8 0.21\nwalker2d-medium-v2\nwalker2d-medium-replay-v2\nwalker2d-medium-expert-v2\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\n0.01\n0.6\n0.61 \u02d8 0.19\n0.01\n0.6\n0.12 \u02d8 0.10\n0.01\n0.8\n1.09 \u02d8 0.04\n0.01\n0.7\n0.62 \u02d8 0.21\n0.01\n0.7\n0.18 \u02d8 0.11\n0.01\n0.9\n1.07 \u02d8 0.05\n0.01\n0.8\n0.61 \u02d8 0.18\n0.01\n0.8\n0.19 \u02d8 0.12\n0.01\n1.0\n1.06 \u02d8 0.07\n0.1\n0.6\n0.62 \u02d8 0.19\n0.1\n0.6\n0.14 \u02d8 0.09\n0.1\n0.8\n1.08 \u02d8 0.06\n0.1\n0.7\n0.62 \u02d8 0.21\n0.1\n0.7\n0.16 \u02d8 0.09\n0.1\n0.9\n1.06 \u02d8 0.09\n0.1\n0.8\n0.65 \u02d8 0.18\n0.1\n0.8\n0.19 \u02d8 0.11\n0.1\n1.0\n1.04 \u02d8 0.12\n1.0\n0.6\n0.76 \u02d8 0.13\n1.0\n0.6\n0.24 \u02d8 0.11\n1.0\n0.8\n1.09 \u02d8 0.01\n1.0\n0.7\n0.76 \u02d8 0.14\n1.0\n0.7\n0.35 \u02d8 0.13\n1.0\n0.9\n1.09 \u02d8 0.01\n1.0\n0.8\n0.79 \u02d8 0.08\n1.0\n0.8\n0.34 \u02d8 0.18\n1.0\n1.0\n1.10 \u02d8 0.01\nhalfcheetah-medium-v2\nhalfcheetah-medium-replay-v2\nhalfcheetah-medium-expert-v2\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\nPessimism \u03ba\nRTG\nReturn (Mean \u02d8 Std)\n0.01\n0.4\n0.43 \u02d8 0.01\n0.01\n0.4\n0.42 \u02d8 0.01\n0.01\n0.6\n0.55 \u02d8 0.09\n0.01\n0.5\n0.44 \u02d8 0.01\n0.01\n0.5\n0.39 \u02d8 0.04\n0.01\n0.7\n0.66 \u02d8 0.17\n0.01\n0.6\n0.42 \u02d8 0.02\n0.01\n0.6\n0.39 \u02d8 0.03\n0.01\n0.8\n0.64 \u02d8 0.11\n0.1\n0.4\n0.43 \u02d8 0.01\n0.1\n0.4\n0.42 \u02d8 0.01\n0.1\n0.6\n0.54 \u02d8 0.13\n0.1\n0.5\n0.44 \u02d8 0.01\n0.1\n0.5\n0.4 \u02d8 0.03\n0.1\n0.7\n0.62 \u02d8 0.09\n0.1\n0.6\n0.42 \u02d8 0.02\n0.1\n0.6\n0.38 \u02d8 0.04\n0.1\n0.8\n0.67 \u02d8 0.13\n1.0\n0.4\n0.43 \u02d8 0.01\n1.0\n0.4\n0.43 \u02d8 0.01\n1.0\n0.6\n0.61 \u02d8 0.18\n1.0\n0.5\n0.44 \u02d8 0.01\n1.0\n0.5\n0.4 \u02d8 0.04\n1.0\n0.7\n0.69 \u02d8 0.13\n1.0\n0.6\n0.44 \u02d8 0.02\n1.0\n0.6\n0.38 \u02d8 0.04\n1.0\n0.8\n0.64 \u02d8 0.16\nTable E.10: The normalized return of DWM-PQL with diffusion step K \u201c 5, N \u201c 3, and simulation horizon H \u201c 5.\n29\n"
  },
  {
    "title": "IMUSIC: IMU-based Facial Expression Capture",
    "link": "https://arxiv.org/pdf/2402.03944.pdf",
    "upvote": "5",
    "text": "IMUSIC: IMU-based Facial Expression Capture\nYoujia Wang1,2\u2217\nYiwen Wu1,2\u2217\nRuiqian Li1\nHengan Zhou1,2\nHongyang Lin1,3\nYingwenqi Jiang1\nYingsheng Zhu1\nGuanpeng Long1,4\nJingya Wang1\nLan Xu1\nJingyi Yu1\n1ShanghaiTech University\n2LumiAni Technology\n3Deemos Technology\n4ElanTech Co., Ltd.\n{wangyj2, wuyw2023, lirq1, zhouha24, linhy, jiangywq,\nzhuysh, longgp2022, wangjingya, xulan1, yujingyi}@shanghaitech.edu.cn\nhttps://sites.google.com/view/projectpage-imusic\nFigure 1. We introduce IMUSIC, an innovative facial capture scheme based on IMUs. IMUSIC utilizes IMUs specially designed for facial\napplications, emphasizing miniaturization. IMUSIC demonstrates its ability to accurately capture facial information without visual input.\nAbstract\nFor facial motion capture and analysis, the dominated\nsolutions are generally based on visual cues, which cannot\nprotect privacy and are vulnerable to occlusions. Inertial\nmeasurement units (IMUs) serve as potential rescues yet\nare mainly adopted for full-body motion capture. In this\npaper, we propose IMUSIC to fill the gap, a novel path for\nfacial expression capture using purely IMU signals, signifi-\ncantly distant from previous visual solutions. The key design\nin our IMUSIC is a trilogy. We first design micro-IMUs to\nsuit facial capture, companion with an anatomy-driven IMU\nplacement scheme. Then, we contribute a novel IMU-ARKit\ndataset, which provides rich paired IMU/visual signals for\ndiverse facial expressions and performances. Such unique\nmulti-modality brings huge potential for future directions\nlike IMU-based facial behavior analysis. Moreover, utiliz-\ning IMU-ARKit, we introduce a strong baseline approach\nto accurately predict facial blendshape parameters from\npurely IMU signals. Specifically, we tailor a Transformer\ndiffusion model with a two-stage training strategy for this\nnovel tracking task. The IMUSIC framework empowers us\nto perform accurate facial capture in scenarios where vi-\nsual methods falter and simultaneously safeguard user pri-\narXiv:2402.03944v1  [cs.CV]  3 Feb 2024\nvacy.\nWe conduct extensive experiments about both the\nIMU configuration and technical components to validate\nthe effectiveness of our IMUSIC approach. Notably, IMU-\nSIC enables various potential and novel applications, i.e.,\nprivacy-protecting facial capture, hybrid capture against\nocclusions, or detecting minute facial movements that are\noften invisible through visual cues.\nWe will release our\ndataset and implementations to enrich more possibilities of\nfacial capture and analysis in our community.\n1. Introduction\nFacial expressions, as subtle as minute muscle move-\nments, play a key role in effectively conveying a wide range\nof emotions.\nThese nonverbal cues have particular im-\nportance in human interaction, as they provide emotional\ndepth beyond spoken words. Anatomically, expressive fa-\ncial movements emerge from the intricate coordination of\nfacial muscles, activated by the brain through the facial\nand trigeminal nerves. For a considerable time, capturing\nand analyzing facial expressions have predominantly de-\npended on visual cues. Computer vision techniques such\nas 3DDFA [31, 101] offer a rapid means of acquiring fa-\ncial geometry and expression from a single image, whereas\nDECA [27] manages to recover even finer facial details.\nApple\u2019s ARKit[5] represents a significant leap in real-time\nfacial geometry acquisition, enabling users to create ani-\nmated avatars that mimic their expressions. However, ap-\nproaches based on visual signals also pose challenges: they\ndo not protect privacy and are vulnerable to occlusions or\nside facial poses. Further, they fail to capture nuanced fa-\ncial motions that are difficult to track visually. In this paper,\nwe explore a novel approach for facial movement capture\nby leveraging the latest advances in inertial measurement\nunits (IMUs) that can work independently or in conjunction\nwith visual capture.\nIMUs, initially used in mobile devices, have been pro-\ngressively embraced in whole-body motion capture.\nIn\ncomparison to conventional vision-based systems, IMUs\nboast portability and minimal spatial prerequisites. Typ-\nically attached to various body joints, IMUs can capture\nessential acceleration and axis angle data that can further\ntranslate to body motion. For example, pioneering work\nby Loper et al. [48] adapted this axis angle data for in-\ntegration with the SMPL human body model, thereby en-\nabling full-body motion capture. Most recently, Yi et al.\n[90] achieved comprehensive body motion capture using as\nfew as six IMUs, leveraging the stability and generative ca-\npabilities of Transformer Diffusion [43]. In contrast, there\nis almost no use of IMU for facial motion capture. The rea-\nson is that the challenges are multi-fold. Hardware-wise,\nintegrating IMUs with additional sensors (like inertial and\ngeomagnetic detectors) and communication modules (such\nas Bluetooth and Wi-Fi) results in a size prohibitive large\non faces.\nAlgorithm-wise, IMUs generally offer sparser\ndata with lower signal-to-noise ratios compared to visual\nsystems. Furthermore, unlike body motion capture where\nspatial positions correlate to bone rotations, facial expres-\nsions are predominantly muscle-driven, posing a unique\nchallenge in translating IMU data.\nIn this paper, we present a novel IMU-based facial move-\nment capture solution, or IMUSIC (see Fig. 1 for detail).\nIMUSIC utilizes IMUs specially designed for facial appli-\ncations, emphasizing miniaturization. Specifically, by sepa-\nrating the detection and data modules of the IMU, we en-\nsure that the device attached to the face is compact and\nlightweight. This design avoids interfering with natural fa-\ncial movements and allows reliable data transmission and\nsynchronization. IMUSIC additionally adopts an anatomy-\ndriven strategy for placing IMUs in correspondence with\nspecific muscles that control facial expressions. In addition,\nIMUSIC attaches an auxiliary IMU to the temporal bone\nbehind the ears, a location immune to expression-induced\nmovement and unaffected by neck rotation, to cancel out\nglobal head motion.\nWe contribute the first facial IMU\ndataset that consists of both IMU and visual signals, bene-\nfiting the CG community. By covering the IMUs with skin-\ntoned tapes, we managed to capture corresponding vision\nsignals, e.g., by using ARKit. The resulting IMU-ARKit\ndataset records both signals from participants engaged in\nvarious activities, including speaking different languages,\nmaking facial expressions while interacting with another\nparticipant, speaking with emotional intonation, etc.\nNext, we use the IMU-ARKit dataset to train a neural\nnetwork to infer facial expressions solely from IMU sig-\nnals. Previous visual-based techniques require datasets of\nan ultra-large number of participants to account for discrep-\nancies in age, gender, ethnicity, etc. Limited by the dif-\nficulty of collection, our dataset still lacks diversity. We\nhence augment it by synthesizing virtual IMU signals from\nthese visual datasets. We discuss in depth the discrepancies\nbetween the actual captured vs. synthesized IMU signals\nand set out to train a Transformer Diffusion based neural\nnetwork [43] to bridge the gap between the two as well as to\nreliably infer Blendshape parameters directly from the IMU\nsignals.\nComprehensive experiments show that IMUSIC\nperforms comparable to ARKit on front-facing participants\nunder well-lit conditions.\nMore importantly, it is much\nmore robust in handling challenging scenarios like side-\nfacing participants, faces under occlusions, or poor lighting\nconditions, which are commonly problematic to ARKit and\nother visual-based techniques.\nIn addition to serving as an alternative facial motion cap-\nture system, IMUSIC enables various novel applications. In\nan era where digital privacy is a paramount concern, IMU-\nSIC offers a novel method to capturing facial expressions\nFigure 2. Evolution of body and facial motion capture. This sequence illustrates the progression from marker-based, to marker-free\ncapturing, culminating in IMU-based capture. IMUs have already been applied to full-body motion capture, and we propose IMUSIC to\nfill the void in IMU-based facial capture (bottom right).\nwithout visual input, thereby safeguarding personal visual\ndata. This functionality is particularly crucial in the domain\nof Virtual YouTubers (VTubers), where content creators\nseek to animate avatars while concealing their true identi-\nties. Utilizing IMUSIC, VTubers are empowered to anony-\nmously manipulate the expressions of their digital personas.\nSecondly, we introduce a hybrid capture system to supple-\nment traditional facial capture methods in scenarios where\nvisual signals are compromised. An exemplary case is dur-\ning song recordings, where the performer\u2019s facial expres-\nsions might be concealed by microphones or similar stu-\ndio apparatus. In such instances, the mouth and cheeks of\nartists are obscured, resulting in distorted expressions cap-\ntured by visual tools like ARKit. In contrast, IMUSIC can\nseamlessly capture these occluded facial movements and\nensure accurate syncing of audio recordings with facial ani-\nmations for invaluable virtual music production. Moreover,\nIMUSIC demonstrates exceptional capability in detecting\nsubtle facial movements and capturing expressions in ar-\neas less amenable to vision feature analysis. For instance,\nsubtle cheek movements caused by puffing, which often oc-\ncur invariant in vision signals, can be precisely captured by\nIMUSIC through the orientation data of the IMU. Other\nmicro-expressions like the slight twitching of mouth cor-\nners, which may go unnoticed visually, can also be reliably\ncaptured by IMUSIC, providing insights for detecting an\nindividual\u2019s emotion changes.\n2. Related Works\nOur attempt to deploy IMUs for facial motion capture\nbenefits from the evolutionary trajectory of human motion\ncapture technology as Fig. 2. Tracing this evolution, be-\nginning in the 1990s, marker-based optical motion cap-\nture technology emerged as the mainstream methodology\nin the field [10, 28, 32]. In this approach, actors are re-\nquired to wear reflective markers on their bodies for mo-\ntion tracking. Since the 2000s, groundbreaking develop-\nments [18, 22, 55, 81] have revolutionized motion capture\ntechnology. This advancement has made it possible to cap-\nture human motion without the need for actors to wear any\nspecial markers [44, 48, 68]. However, for 3D skeletal re-\nconstruction, technologies typically require fixed-position\ndevices like Zhang [97] or multi-camera setups as in [55].\nThis substantially constrains the range of motion and space\navailable to the actors during capture. To address this limi-\ntation, Vlasic et al. [80] pioneered the use of IMUs in human\nmotion capture, offering a more flexible and less restrictive\nmethod. Subsequently, this field witnessed rapid advance-\nments, leading to the development of more stable systems\n[21, 38, 84], and the introduction of sparser configurations\n[83, 93]. In the commercial arena, companies like Sony [73]\nand Xsens [88] have launched consumer-grade motion cap-\nture sensors, offering solutions with reduced spatial require-\nments and expanded operational ranges to accommodate a\nwider consumer base.\nFacial Mocap\nFace shape representation traditionally re-\nlies on a sparse collection of 2D facial landmark points.\nSome pioneering works [17, 19, 20, 100] have adeptly uti-\nlized 2D facial landmarks for facial capture. With the grow-\ning demand for 3D visual effects, there has been an increas-\ning focus on 3D face capture and reconstruction.\nIn the realm of facial expression capture, a pivotal ad-\nvancement was introduced by Blanz and Vetter [11] who\nproposed a novel approach for general face representation\nusing linear combinations. This methodology marked a sig-\nnificant turning point in the field, offering a structured and\nnuanced way to capture facial expressions. In the film and\nFigure 3. Overview of IMUSIC. We first introduce the hardware design and the data acquisition pipeline. Subsequently, we delve into the\ndata calibration process and the methodology for facial motion recovery utilizing IMU signals. Following the deployment of IMUSIC, we\ndemonstrate its effectiveness through various applications, underlining its precision and portability.\nvisual effects industry, several studies [9, 12, 53, 54, 56, 65,\n94] have utilized facial markers, visually tracking and com-\nputing their movement to predict specific facial expressions\nand motions.\nThe captured motions are then manually refined in post-\nproduction to achieve realistic facial animations in visual ef-\nfects. However, for consumer-level users, these methods are\nprohibitively expensive, heavily reliant on post-production\nmanual adjustments, inconvenient, and limited by location\nconstraints.\nSeveral works [7, 14, 26, 72, 86] have ad-\ndressed this issue through a marker-free approach that pre-\ndicts facial motions from a single RGB image, effectively\nreducing costs. Subsequently, numerous challenges in this\nfield have been resolved following this methodology. Re-\nsearch by Cao et al. [13], Paysan et al. [60] has enhanced the\nprocess of expression motion capture by employing various\nstrategies to eliminate video jitter and improve robustness.\nWith the enhancement of image processing capabilities\nby deep learning algorithms, Batista et al. [8], Laine et al.\n[41], Olszewski et al. [59] have improved facial motion\ncapture from various dimensions.Regression based meth-\nods with CNN network [31, 33, 75] achieve robust results\nfrom a single image, while video-based facial motion cap-\nture techniques [34, 45, 49, 92] have become increasingly\nsophisticated. Lombardi et al. [47] proposes a deep appear-\nance model to handle complex geometry and texture in fa-\ncial animation. Yoon et al. [91] uses a united model to pro-\ncess images in the wild and Cao et al. [16] uses a universal\navatar prior (UAP) to train a high resolution avatar with a\nmoblie phone. In another way, Zhang et al. [95] proposes\nhigh-fidelity performer-specific facial capture method from\n4D scan.\nHowever, visually based methods [15, 86] are prone to\ninterference from occlusions.\nReferring to the works of\nQammaz and Argyros [63], they have partially alleviated\nthe influence of occlusion by ensuring that a significant por-\ntion of the area predicts information from obscured regions.\nIn recent years, facial capture based on wearable devices has\nshown promising results. These methods are less suscepti-\nble to interference and are notably portable. However, all\nthe visually based methods mentioned above face the risk\nof compromising user privacy due to prolonged exposure to\ncameras.\nSensor-based Mocap\nAs manufacturing technology pro-\ngresses and research in the field deepens, IMUs are benefit-\ning from consistent and precise improvements. Research fo-\ncused on the fundamental performance of IMUs calibration\n[6, 23, 29, 67] and correction [2, 46, 79, 80] have enabled\nstable data detection and effectively addressed their signif-\nicant drift issues. Solving these problems improves the sta-\nbility and precision of IMUs, making them a popular choice\nfor many research teams in body mocap applications. Most\nrelated tasks predominantly utilize IMU signals as part of a\nmultimodal system for body mocap. IMU capture in collab-\noration with video is one of the most extensively researched\nmultimodal methods. The initial work of Malleson et al.\n[51], Pons-Moll et al. [61, 62], Von Marcard et al. [82] laid\nthe groundwork for the development of video-collaborated\nIMU systems, and ultimately, in recent years, this technol-\nogy has achieved a level of perfection[30, 37, 52, 98]. Depth\ncamera[36, 99], Optical markers [4] and physics constraints\n[3, 25, 40, 87, 89] are also common-used collaboration\nfor IMU mocap. There are also efforts focused on recon-\nstructing body motion using only 17 distinct IMU signals\n[58, 69]. As research on IMU signals deepens, the Sparse\nIMU [39, 66, 71, 74] concept emerged. Subsequently, the\nSIP [83] initiative successfully reduced the number of IMUs\nto six, while the DIP [39] project accomplished real-time\nmocap, culminating in the TransPose [90] work which re-\nfined these advancements. Additionally, employing IMUs\nfor gait analysis [35] is also one of the more popular ap-\nplications. Compared to vision-based methods, these ap-\nproaches offer significant advantages, including a wider\nrange of motion, freedom from obstructions and lighting\nconstraints, benefits that are equally applicable to facial mo-\ntion capture.\nHowever, adapting this method for facial application is\nnot straightforward. Initially, while commercial IMUs like\nnoiton [58], Xsens [88], and mocopi [73] have achieved\nsignificant integration, they are predominantly designed for\nbody mocap and are disproportionately large and heavy for\nfacial capture. With the advancement of hardware, there\nare now attempts to use IMUs for capturing movements of\nsmaller limbs. Makaussov et al. [50], Mummadi et al. [57]\ncustomized IMU gloves for hand posture estimation. This\nbreakthrough shed light on the potential of IMUs in recon-\nstructing smaller body parts. While capturing hand posture\nprimarily utilizes IMUs to detect skeletal movements, re-\nlying chiefly on rotational angles for effective hand mo-\ntion detection, facial capture does not share this charac-\nteristic. Furthermore, unlike the relatively fixed joints in\nhand movements, most facial regions are capable of multi-\ndegree movements informed by anatomical studies [76]. In\nour study, we referenced anatomical structures to maximize\nthe use of IMUs in regions with extensive muscle motion.\nHowever, even with this strategy, directly modeling these\nmovements through mere rotational data proves to be com-\nplex. Aha [1] employing lazy learning techniques to con-\nduct human mocap with just accelerometer signals, has sig-\nnificantly influenced our approach. Despite the limited per-\nformance of this method, it revealed the importance of ac-\nceleration signals in predicting movements, providing valu-\nable information that rotation alone cannot offer, such as the\neffects of force. This revelation has guided us to give equal\nimportance to both pose and acceleration in the processing\nof IMU data.\n3. Facial IMU\nFig. 3 provides an overview of IMUSIC. The left sec-\ntion outlines our data collection process. Participants are\nrequested to wear multiple IMUs and perform a range of fa-\ncial expressions and motions to capture corresponding sig-\nnals. We designed these compact IMUs to attach to the face\nsimultaneously without impeding facial movements. In the\nmiddle section, we illustrate our network\u2019s data processing\nworkflow. The collected IMU data is first aligned to a uni-\nfied coordinate system using calibration. Then we establish\nan IMU2Face model that reconstructs facial actions based\non the aligned IMU signals. The right section highlights\nthe potential applications of IMUSIC, offering solutions to\naddress some limitations of visual-based facial capture.\nWithin the field of motion capture, IMU plays a critical\nrole in reflecting the spatial movements of an object by mea-\nsuring its orientation and acceleration. IMUs designed for\nfull-body motion capture, such as Xsens, Sony Mocopi and\nothers, have been widely applied commercially. These units\nusually consist of various parts, including detecting sensors\nand data transmission modules, making them too hulking\nto be used for facial motion capture. Furthermore, employ-\ning multiple units of this model for facial capture can lead\nto severe occlusion, preventing observation of the partici-\npant\u2019s facial expressions. This necessitates the development\nof a custom-designed IMU, specifically tailored to meet the\nunique requirements and scale of facial motion capture.\nOur design preserves the function of standard IMU while\nminimizing weight and size to cater to the requirements for\nfacial capture. Fig. 4 (top) compares the size of our IMU.\nWe achieved significant miniaturization by separating the\nsensor module from the data transmission module. This de-\nsign allowed our sensor module to be exceptionally com-\npact, measuring only 12*10 mm and weighing merely 0.4\ngrams, a stark reduction to a quarter of the Xsens module\u2019s\narea and only 15% of its weight.\n3.1. Facial IMU Sensor\nFig. 4(down) provides a detailed overview of the spe-\ncific hardware components utilized in our study. The sensor\nmodule incorporates a total of nine-axis sensing sub-units,\nwhich include the QMC5883P [64] from Silicon Power, a\nthree-axis magnetic field sensor with a measurement range\nof \u00b130 gauss, and the QMI8658 [64] integrated chip, which\ncombines a three-axis gyroscope and accelerometer. These\nsensors are capable of accurately recording spatial positions\nand accelerations at a rate of 60fps. The data transmission\nmodule is primarily based on the ESP32 controller. It em-\nploys the UDP protocol to collect and correct data detected\nby the sensor module. Additionally, we use a Wi-Fi module\nto transmit the computed data to the host computer. This\ndata includes time stamps, quaternion representations, and\nacceleration values at each recorded instance.\nThe data transmission module of our Face IMU sensor\nsystem requires only a 5V battery supply. This setup pro-\nvides the essential conditions for the portability and wear-\nability of the Face IMU sensor system. Furthermore, as\nthe connection to the host computer is via Wi-Fi, users can\nFigure 4. Size and architecture design of our IMU. Top: size com-\nparison. Our IMU is small enough to interfere with natural facial\nexpressions. Bottom: architecture design. Our IMU has two main\ncomponents: the face unit and the primary unit. The face unit\ncontains an Accelerometer, Gyroscope and Magnetometer. It\u2019s at-\ntached to the face to measure the acceleration and orientation sig-\nnal. Data collected by the face unit is transmitted to the primary\nunit by wire. The primary unit powers the face unit, corrects data,\nand forwards signals to the host computer via Wi-Fi.\nmove freely within the Wi-Fi signal range while wearing\nthe Face IMU, enabling high degrees of mobility.\nFig. 5 exemplifies the remarkable sensitivity of our IMU\nin capturing even the most minute motion signals. In this\ndemonstration, a participant wearing an IMU detector on\nthe eyebrow performs subtle blink movements. In the wave-\nform diagram shown in (d), we can clearly see a complete\nmotion sequence: starting with the initial acceleration as\nthe eyebrow begins its ascent, followed by a deceleration\nphase as the movement slows, and culminating in a station-\nary phase when the eyebrow ceases to move. The graph dis-\ntinctly demarcates these stages, underscoring the precision\nwith which our IMU detects even the most delicate move-\nments. Our supplementary video provides a more dynamic\nvisualization of the motion.\n3.2. Synchronization and Calibration\nWe delved deeply into the essential technology for cap-\nturing facial information in synchrony using multiple IMUs.\nTo achieve this, it is imperative to address two fundamen-\ntal challenges: synchronization and calibration. We des-\nignated one ESP32 as the auxiliary ESP32, employing it\nas a benchmark for synchronizing and calibrating the oth-\ners.\nTo ensure coherent and coordinated data collection\nfrom these diverse sensors, we integrated a calibration pro-\ngram into this ESP32 within the data transfer module dur-\ning hardware design. Before data acquisition by the IMUs,\nwe used the data module of the auxiliary ESP32\u2019s clock\nas a reference point. We transmitted pulse signals through\nthe DuPont line to each IMU\u2018s ESP32 for calibration pur-\nposes. Upon receiving this pulse signal, each ESP32 aligns\nits internal clock with the external reference, synchronizing\nthe timestamps across all IMUs. With these synchronized\nsignals, we obtain the raw signals \u2020S = {\u2020R, \u2020A} from\nall the n IMUs, where \u2020R = {\u2020R0, \u2020R1, \u00b7 \u00b7 \u00b7 , \u2020Rn} repre-\nsent the rotation matrix sequence of the number i IMU, and\n\u2020A = {\u2020A0, \u2020A1, \u00b7 \u00b7 \u00b7 , \u2020An} represent the acceleration se-\nquence of the number i IMU. The rotation matrix represents\nthe rotation from the IMU\u2019s local coordinate system to the\nworld coordinate system. Notably, we denote the index of\nthe auxiliary IMU as 0. For ease of subsequent analyses, we\ninitially convert the acceleration from the local coordinate\nsystem to the world coordinate system. For the IMU i\u2019s data\non the frame j, we denote the raw acceleration and orienta-\ntions as \u2020ai\nj and \u2020Ri\nj. The world coordinate acceleration can\nbe represented as:\nai\nj = (\u2020Ri\nj)\u22121\u2020ai\nj.\n(1)\nNext, acknowledging the variability in facial structures\nand the potential for slight discrepancies in IMU placement\neach time, we adopted the concept of a Neutral facial per-\nformance, similar to the approach used by [26, 90] in body\nmocap. After wearing the IMUs for the participants, we\nhad each participant relax the facial muscles, presenting\na Neutral state, and recorded the orientation of each IMU\n{Rneutral}. In subsequent calculations, we used the orienta-\ntion relative to this pose as a baseline:\n\u2217Ri\nj = (Ri\nneutral)\u22121\u2020Ri\nj.\n(2)\nConsidering that two types of movements influence an\nIMU\u2019s signal when placed on the face \u2014 the overall move-\nment of the head and the movements caused by facial ex-\npressions. Our focus is on deducing expressions from IMU\nsignals. As illustrated in Fig. 5 (b), not only do we de-\nploy IMUs across the facial region, but we also strategically\nplace an auxiliary IMU behind the ear, as shown in (c). This\nplacement is specifically designed to mitigate the impact of\ngeneral head movements on the orientations and accelera-\ntions detected by the other IMUs. For convenience, we de-\nfine the index of the auxiliary IMU to be 0. The calibrated\nIMU rotation can be expressed as\nRi\nj =\n\u001a (\u2217Ri\n0)\u22121\u2217Ri\nj\nif j \u0338= 0,\n\u2217Ri\n0\notherwise\n(3)\nWe denote the calibrated IMU signal as S = {R, A}.\nFigure 5. Data acquisition pipeline of IMUSIC. Initially, we position IMUs at specific facial locations to capture IMU signals corresponding\nto various fundamental facial movements. Simultaneously, ARKit parameters are recorded. We design a simulator for data augmentation\nmodule in parallel.\n4. IMU-based Facial Motion Capture\n4.1. IMU Placement and Anatomic Guidance\nTo accurately capture facial movements, it is imperative\nto attach IMUs to distinct regions on the surface of the face.\nIn the left panel of Fig. 5, we present our strategic arrange-\nment of IMUs. This layout is informed by a detailed anal-\nysis of the distribution of facial muscles[76]. We demar-\ncated distinct facial zones, referencing key anatomical land-\nmarks including: the zygomaticus area, crucial for express-\ning emotions like joy or sorrow; the buccinator and mentalis\narea, fundamental for movements pertinent to speech; the\norbicularis oculi area, important for a spectrum of expres-\nsions such as smiling and frowning; and the frontalis area,\nintegral for conveying sentiments such as dissatisfaction or\nmelancholy. In every designated region, we meticulously\nplaced at least one IMU to ensure comprehensive monitor-\ning of the key muscle groups and facial zones. Additionally,\nin regions characterized by dense muscle presence or com-\nplex movements, we opted for dual IMU placement. This\napproach is exceptionally effective in capturing a complete\nrange of muscle movements in these specific areas.\nFurthermore, our design strategy was acutely focused\non minimizing the IMUs\u2019 impact on both the natural facial\nmovements and the comfort of our participants. Acknowl-\nedging the sensitivity of certain facial regions, we inten-\ntionally avoided placing IMUs on the eyelids and the cor-\nners of the mouth, as these areas are not only crucial for\na wide range of expressions but are also prone to discom-\nfort if constrained. To ensure the IMUs were discreet, we\nutilized flesh-colored tape for attachment, allowing them to\nblend seamlessly with the skin while minimizing visual dis-\ntraction. The wiring was also thoughtfully routed along the\nperiphery of the face to avoid interfering with the partici-\npant\u2019s expressions and to preserve the clarity of any accom-\npanying visual data. These measures enabled us to maintain\nthe fidelity of the IMU data and any visual records while re-\nspecting the comfort and expressiveness of the participants.\n4.2. Capturing IMU-ARKit Dataset\nBlendshape technology is widely used in the realm of\nfacial animation and motion capture due to its ability to\ngenerate highly realistic and nuanced expressions.\nThis\ntechnology operates on the principle of parametric mod-\neling.\nSpecifically, a blendshape model is defined by\na collection of blendshape weights, denoted as W\n=\n{w1, w2, \u00b7 \u00b7 \u00b7 , wm}, a blendshape model can be represented\nas:\nM(W) = B0 +\nm\nX\nk\nwkBk.\n(4)\nwhere B0 represents the neutral face, Bk is the blendshape\nbasis vector, and m is the number of blendshapes.\nBy\nlinearly interpolating between different blend shapes, this\napproach allows for the creation of multiple facial expres-\nsions.\nOur challenge is to derive these blend shape parameters\nW from the IMU data S. Given that the IMU is capable\nof capturing motion and orientation, we propose a method\nfor mapping these physical measurements to blend shape\ncoefficients. This requires the development of an algorithm\nthat converts IMU readings into meaningful hybrid shape\nparameters.\nIn order to realize a data-driven solution for predicting\nfacial blendshape weights using IMU, we set out to create a\nfacial IMU with the ARKit dataset, as demonstrated in the\ncentral panel of Fig. 5 This dataset was carefully compiled\nto contain paired data from IMU and ARKit to ensure a\ncomprehensive base for model training.\nOur dataset contains records from 20 different partici-\npants.\nThese individuals are all in the 18-40 age range,\nproficient in English, and have some background in acting,\nthus providing richly varied and vivid facial expressions.\nThe left panel of Fig. 5(a) shows an example of the data\ncollection setup. We installed a set of 11 IMU detectors\nfor each participant and sat in the acquisition seat, with the\nteleprompter screen placed directly in front of the partici-\npant, next to an iPad that captured the visual information.\nthe iPad used LiveLinkface [78] to capture the visual infor-\nmation, and the captured visual signals were divided into\ntwo parts: the RGB video sequence and the ARKit Parame-\nters.\nBefore the formal data collection process began, partic-\nipants were given time to acclimatize to the sensation of\nwearing the IMU to ensure natural and unrestricted facial\nmovements. Participants then gently triple-clicked the IMU\nlocated on mentalis, which was used as a later stage to syn-\nchronize the IMU signals with the visual signals. Data col-\nlection for each participant was divided into three different\nsections, each preceded by a sample video for the partici-\npant to mimic. In the first section, participants read aloud\nthe provided content in a calm tone, with a split between\nnative language and English. This was done to capture the\nnatural facial movements associated with the language. In\nthe second session, participants were asked to read aloud the\nsame content, but with a specific emotion consistent with\nthe context of the script, thus adding emotional layers to\nthe facial expressions. Finally, the third segment asked par-\nticipants to sequentially make a series of facial expressions\nthat were based on specific medical classifications, ensuring\na full range of emotions and movements.\n4.3. Facial IMU Simulator Augmentation\nDespite the thoroughness of our IMU data collection\nprogram, generating a sufficiently large dataset for train-\ning models remains a significant challenge. This is primar-\nily due to the logistical constraints associated with amass-\ning a vast quantity of IMU-ARKit IMU data. To address\nthis issue, we drew inspiration from the work of\n[90],\nwho adeptly used simulate human IMU signals to trans-\nform other types of motion capture (Mocap) data into IMU-\nMocap data. Motivated by this innovative approach, we\ndeveloped the Face-IMU Simulator, an advanced tool de-\nsigned to animate facial meshes using ARKit data and ac-\ncurately simulate the corresponding orientation and accel-\neration signals on the facial surface, as demonstrated in the\nright panel of Fig. 5.\nOur methodology for ARKit data collection incorporated\ntwo primary strategies.\nFirstly, we utilized the MEAD\ndataset [85], a rich source of facial expression data that\nprovided a solid foundation for our simulations. However,\nrecognizing that MEAD predominantly contains short sen-\ntences and lacks some clips focused on specific expressions,\nwe adopted a second strategy.\nThis involved recording\nadditional expression sequences using LiveLinkFace [78],\nthereby enriching our dataset with approximately two hours\nof specialized clips. These additional sequences, compris-\ning mainly short text readings and expressive movements,\nsignificantly broadened the scope and depth of our dataset.\nLeveraging the ARKit sequences, we computed a topo-\nlogically consistent face model for each frame, adhering\nto Equation. 4 and utilizing the basis provided by the ICT\nModel [42].\nFor each selected vertex i, corresponding to the positions\nof the IMU-ARKit data\u2019s IMU i, to simulate the IMU vi\njs\u2019\nacceleration for each frame j, we employed the following\nformula:\nai\nj = 1\nc\u03c4 (vi\nj\u2212c + vi\nj+c \u2212 2vi\nj),\n(5)\nwhere \u03c4 is the time interval of two frames, and c is the con-\nstant that controls the degree of smoothing.\nTo determine orientation, we focused on the facial region\ncontaining the selected vertex. We select a face that con-\ntains the vertex, denoted as {vi\nj, vi,sup[1]\nj\n, vi,sup[2]\nj\n}. We iden-\ntified two adjacent edges to this vertex, labeled as ei,1\nj\n=\n(vi\nj \u2212 vi,sup[1]\nj\n)norm and ei,2\nj\n= (vi\nj \u2212 vi,sup[2]\nj\n)norm. The first\nvector from these edges was utilized as the x-axis for the\nIMU coordinate system. We calculated the rotation matrix\nof the IMU relative to the world coordinate system as\n\u2020Ri\nj =\n\u0014\nei,1\nj ,\nei,1\nj\n\u00d7ei,2\nj\n\u2225ei,1\nj\n\u00d7ei,2\nj\n\u2225,\n(ei,1\nj\n\u00d7ei,2\nj\n)\u00d7ei,2\nj\n\u2225(ei,1\nj\n\u00d7ei,2\nj\n)\u00d7ei,2\nj\n\u2225\n\u0015\n.\n(6)\nAs with the IMU-ARKit data, we calibrate \u2020Ri\nj by Eq. 2\nand Eq. 3 to obtain the data used for network training Ri\nj.\nThis comprehensive approach to simulating both the accel-\neration and orientation of IMUs ensures that our dataset is\nnot only extensive but also rich in the detailed, accurate rep-\nresentation of facial movements.\n4.4. IMU Based Facial Tracker\nIn this study, a novel data flow was established, as de-\npicted in Fig. 6. Initially, a calibration process was applied\nto both the simulate and the raw IMU-ARKit data. This\nprocedure was instrumental in generating paired data sets of\nIMU-ARKit parameters, which were subsequently utilized\nfor the training of the network.\nThe network ingests calibrated IMU data as its input. To\nsimplify the inputs to the network, we first express the rota-\nFigure 6. Training pipeline. We use a two-stage approach for training, dividing into a pre-training phase using simulate data, and a finetune\nphase using adaptively combined data from real and simulate data. We use a Transformer Diffusion Module to predict expressions from\nIMU signals and supervise the output using ARKit parameters.\ntions as quaternions:\nqi\nj =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nq\nmax(0,1+Rj\ni,11+Rj\ni,22+Rj\ni,33)\n2\n,\nq\nmax(0,1+Rj\ni,11\u2212Rj\ni,22\u2212Rj\ni,33)\n2\nsgn(Rj\ni,32 \u2212 Rj\ni,23),\nq\nmax(0,1\u2212Rj\ni,11+Rj\ni,22\u2212Rj\ni,33)\n2\nsgn(Rj\ni,13 \u2212 Rj\ni,31),\nq\nmax(0,1\u2212Rj\ni,11\u2212Rj\ni,22+Rj\ni,33)\n2\nsgn(Rj\ni,21 \u2212 Rj\ni,12)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\n(7)\nwhere sgn(\u00b7) represents a symbolic function. We concate-\nnate qi\nj with ai\nj, denoted as ci\nj \u2208 R7, and then concatenate\nall 11 IMUs\u2019 signal as Cj =\n\u0002c1,\nc2,\n\u00b7 \u00b7 \u00b7\ncn\n\u0003\n\u2208 R77.\nThese inputs are meticulously calibrated, ensuring both the\naccuracy and reliability of the IMU signals. In details, for\neach frame j of imu i, we convert the rotation matrix Ri\nj into\nquaternion, and then put it together with the acceleration to\nform a 7-dimensional vector. We combine the signals from\n11 IMUs in 120 consecutive frames to Cj \u2208 R77\u00d7120 that\nserves as the input to the network.\nOur network \u03a8(\u00b7) comprises three critical components:\nan initial Fully Connected (FC) layer, a transformer-based\ndiffusion core, a concluding FC layer, and an MLP em-\nbedding network.\nWe denote the two FC layer and the\ntransformer-based diffusion core as \u03c8(\u00b7).\nOur network is intricately designed around a diffusion\narchitecture. Central to its operation is the concept of iter-\native denoising, a process repeated n times to achieve the\nfinal predicted 120 frames\u2019 blendshape weights W\u03a8,j \u2208\nR55\u00d7120.\nAt the outset of this process, we set xn\nj as a random noise\nwhich has the same dimension with W\u03a8,j. For the t itera-\ntion, we feed the integer t into the MLP Embedding module.\nThen, the IMU signal Cj for the condition is concatenated\nwith xt\nj and fed into the \u03c8] network along with the embed-\nding output, to calculate xt\u22121\nj\n:\nxt\u22121\nj\n= \u03c8(MLP(t), xt\nj, Cj).\n(8)\nWe iteratively compute until we get x0\nj as the predicted\nblendshape weights W\u03a8,j.\nThroughout the training process, ground truth ARKit pa-\nrameters Wj serve for supervision. We assess the network\u2019s\nperformance using an L1 loss function,\nL = |W\u03a8 \u2212 W| .\n(9)\nThe equation evaluates the accuracy of the network\u2019s out-\nputs against these ground truth parameters.\nOur training regimen is systematically divided into two\nstages, the pretrain stage and the finetune stage, each de-\nsigned to maximize network performance.\nThe pretrain\nstage involves training with simulate data sourced from our\nFace-IMU simulator. This simulator provides a diverse ar-\nray of facial expressions and movements, enabling the net-\nwork to learn from a broad spectrum of data and ensuring\nits generalizability.\nRecognizing the inherent differences between simulate\nand IMU-ARKit data, such as variations in skin elasticity\nand magnetic field-induced offsets, we implemented a fine-\ntune stage. This stage is dedicated to finetuning the network\nwith actual IMU-ARKit data. To circumvent the risk of\noverfitting due to the limited quantity of IMU-ARKit data,\nwe adopted a blended training approach, mixing virtual and\nIMU-ARKit data in a 4:1 ratio. This strategy ensures that\nwhile the network benefits from the nuances of IMU-ARKit\ndata, it remains rooted in the extensive learning provided\nby the virtual data. Such a balanced approach enhances the\nnetwork\u2019s adaptability and efficacy in practical scenarios.\nFigure 7. Gallery of IMUSIC. Each row corresponds to five different expressions of one participant. For each subfigure, the upper right\nsection presents the facial motion reconstructed from IMU signals. The lower left section showcases the image reference.\n5. Experiment\nIn this section, we present the experimental results of\nIMUSIC in facial motion capture and reconstruction. We\nbegin by detailing the implementation aspects and display-\ning a gallery of accurately reconstructed assets by IMU-\nSIC in Fig. 7, highlighting the precision achievable with\nour method. For each participant, five different expressions\nwere selected for presentation. Displayed in the bottom left\ncorner are photographs of our participants, captured along-\nside the IMU signal acquisition. Through IMUSIC, we de-\nrived blendshape parameters for each frame from these IMU\nsignals. For the generation and rendering of facial assets,\nwe leveraged Dreamface [96], ensuring a high-fidelity vi-\nsual output of our facial capture technique.\nWe then conduct a comprehensive evaluation of our\npipeline\u2019s modules, including the IMU placement and\nnumber, assessing both qualitatively and quantitatively in\nSec. 5.1. Then, we compare our results with state-of-the-art\ntechniques in the field in Sec. 5.2. Finally, we show specific\napplication scenarios for IMUSIC in Sec. 5.3.\nFigure 8. Experiment on IMU placement on the face. This figure presents our anatomically-based facial partitioning, highlighting the\nselected points and the corresponding experiments conducted for each facial region. The left image shows our chosen points on the face,\nwhile the other images elaborate on the individual experiments conducted for each specific area. The upper section presents a distribution\nmap of the test points allocated to each region, the middle section identifies the primary expressions and movements associated with that\narea, and the lower section exhibits the acceleration curves of the IMUs situated at each designated point.\nTable 1. IMU measurement in different regions. The data in the table is expressed as variance times 10\u22122.\nIMU Index\n#1\n#2\n#3\n#4\n#5\nFrontalis Area\nRaising Eyebrows\n0.12\n0.06\n0.05\n0.06\n-\nOne-sided Frown\n0.46\n0.40\n0.37\n0.36\nFurrowing\n0.21\n0.08\n0.05\n0.10\nZygomaticus area\nOne-sided Pouting\n0.91\n0.72\n0.34\n0.30\n-\nSquinting\n0.30\n0.23\n0.19\n0.18\nSmile\n0.15\n0.12\n0.14\n0.10\nBuccinator and Mentalis Area\nReading #1\n0.39\n1.25\n1.25\n1.84\n4.04\nReading #2\n0.42\n2.58\n1.75\n4.19\n8.08\nReading #3\n0.15\n1.59\n1.55\n4.60\n8.62\nReading #4\n1.01\n1.80\n1.30\n2.58\n5.34\nOur network architecture is inspired by the work of Li\net al. [43]. We use noised blendshape weights as input and\nIMU data as transformer conditions. The network output is\nidentical to the inputs. We use Adam as the optimizer with\nlearning-rate 2 \u00d7 10\u22124, \u03b1 = 0.9, \u03b2 = 0.999. We train and\nevaluate our network on a single NVIDIA RTX3090 GPU.\nThe pre-training process takes \u2248 4 hours on the simulated\ndataset and \u2248 0.5 hours for the finetune process on a single\nidentity with paired data.\n5.1. Evaluation of IMU Configuration\nEvaluation of IMU location\nWe comprehensively detail\nour facial partitioning design based on anatomical consider-\nations [76]. A key factor after establishing these partitions\nis determining the optimal placement of IMUs on the face\nto attain a better signal-to-noise ratio. Given the high sen-\nsitivity of IMUs, strategically positioning them promises to\ncapture more extensive data. Furthermore, since these flex-\nible positions exhibit larger motion amplitudes, they inher-\nently capture signals with a higher signal-to-noise ratio un-\nder constant noise levels. Therefore, IMU locations with\ngreater signal variance, particularly during specific regional\nexpressions and movements, are identified as more adapt-\nable and preferable placement points.\nWe qualitatively showcase our empirical approach to\nIMU placement across different facial regions in Fig. 8.\nThe image on the far left illustrates the schematic for select-\ning IMU positions, with white dots on the face representing\n4\n6\n8\n10\nIMU Number\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nPVE [mm]\n4\n6\n8\n10\nIMU Number\n0.0170\n0.0175\n0.0180\n0.0185\n0.0190\n0.0195\n0.0200\n0.0205\nMSE\nFigure 9. Numbers of IMU with PVE and MSE. We conduct ex-\nperiments with various numbers of IMU, and calculate the 3D per\nvertex error (PVE) on deformed mesh and MSE on blendshape\nweights for quantitative evaluation.\nthe actual IMU locations we chose. The images, arranged\nfrom left to right, depict the experimental positioning of test\npoints in the Frontalis Area, Zygomaticus Area, and Bucci-\nnator and Mentalis Area. The topmost images show the test\nIMU positions selected in each region, with red and purple\nindicating the final positions we utilized. These points were\nstrategically chosen to avoid interference and align with the\nmuscle distribution of each respective area. The middle im-\nages demonstrate the specific facial movements performed\nby participants wearing IMUs. The acceleration data col-\nlected from each IMU during these movements, shown in\nthe lower part of the images, was qualitatively compared.\nOur final IMU position selections all exhibited strong sig-\nnal strength, indicating significant movement amplitude.\nWe quantitatively present the variance comparison for\nvarious actions across different facial regions in Table 1.\nNotably, in the Frontalis Area, IMU1 shows significantly\nhigher variance, guiding our decision for its placement. In\nthe Buccinator and Mentalis Area, IMUs 1 to 3 are dedi-\ncated to Buccinator detection, with IMUs 4 and 5 focusing\non the Mentalis. Among these, IMUs 2 and 5 display the\nmost notable variance and are thus chosen for placement in\nthis region. For the Zygomaticus Area, the distinct vari-\nance of IMU1 makes it the preferred placement point. We\nalso illustrate the selected facial expressions and the corre-\nsponding IMU data in Fig. 8. Although this may reduce\nthe stability of blink data, it guarantees efficient capture of\nother eye movements, ensuring the comfort of the eyes.\nEvaluation of IMU numbers\nBased on the IMU place-\nment experiment, we further tested the effect of the num-\nber of IMUs on expression prediction.\nAmong the 11\nIMU placements mentioned above, we sequentially and uni-\nformly removed the data from the IMUs. For each exper-\niment, we controlled for other variables, and both used a\n2-stage training pipeline, including pre-training using sim-\nulated data, and finetuning using real data. The curves in\nFig. 9 show that at each reduction in the number of IMUs,\nthe network\u2019s prediction of expressions by IMU signal-\ning accuracy is uniformly reduced. This indicates that our\nIMUs have low overlap between them and maximize the\nacquisition of signals from various regions of the face. As\nthe device is still in its prototype phase, the time investment\nfor deployment significantly increases with each additional\nIMU. Therefore, we have chosen to use 11 IMUs for our\nempirical research, balancing efficiency and effectiveness.\n5.2. Evaluation of IMU-based Facial Capture\nAs the first IMU-based facial capture scheme, we com-\npared it with the state-of-the-art image based expression\nprediction method DECA [27] and 3DDFA V2 [31]. We\ndo the experiment on the testset of IMU-ARKit dataset, and\ntake the iPad captured image as the input of DECA and\n3DDFA V2. The results are shown in Fig. 10. Columns 2,\n5, and 6 respectively demonstrate the results of our IMU-\nbased reconstruction compared to the image reconstruc-\ntion results of DECA and 3DDFA V2. Judging from the\noutcomes illustrated in the figures, our reconstruction re-\nsults are comparable to those of state-of-the-art methods.\nCompared to DECA, our method demonstrates a slight im-\nprovement in eye reconstruction; and in comparison with\n3DDFA V2, our approach yields a relatively better repre-\nsentation of eyebrow expressions. While our method still\nexhibits some misalignment issues, our approach consis-\ntently achieves a comparable level of quality overall.\nFurthermore, we conduct a comprehensive evaluation of\ntwo key steps: data augmentation and finetuning. For each\ninput sample, we implemented three distinct experimental\nsettings. Initially, we present results using the full pipeline,\nincorporating both data augmentation and finetuning. Sub-\nsequently, we show results of training the network entirely\nwithout simulate data, denoted as w/o simulate. Finally, we\ndisplay results when deploying the network prediction with-\nout the finetuning, denoted as w/o finetune. These varia-\ntions are illustrated in columns 2, 3, and 4 of Fig. 10 sequen-\ntially. It is noticeable that both w/o simulate and w/o fine-\ntune have greater discrepancies in mouth reconstruction.\nAdditionally, the reconstruction results w/o finetune show\na greater number of errors in the eye region. Overall, these\nobservations underscore that utilizing the complete pipeline\nresults in more accurate estimations of expressions.\nIn supplementary to the qualitative results, we introduce\na quantitative evaluation, depicted in Fig. 11. Inspired by\n[27, 31, 70], we calculate the 3D per vertex error (PVE)[70]\non the deformed mesh as an indicator of the similarity be-\ntween ARKit capture and our prediction, along with the 3D\nlandmark vertex error (PVE LMK) to further illustrate the\nsimilarity on visually significant areas. We also calculate\nMSE on predicted blendshape weights with ARKit. The\nred line represents the metrics for each frame using our\nFigure 10. Qualitative comparison and ablation study. The first column displays the reference image. The second column illustrates\nthe reconstruction results of our pipeline. The third and fourth columns show the results of our pipeline without simulation and without\nfinetuning. Columns 5 to 7 illustrate results of DECA [27], 3DDFA V2 [31] and ARKit [5].\nTable 2. Quantitative ablation study of our method.\nMethod\nPVE[mm]\u2193\nPVE LMK[mm]\u2193\nMSE\u2193\nmedian\nmean\nstd\nmedian\nmean\nstd\nOurs\n0.67\n0.72\n0.31\n1.07\n1.15\n0.51\n0.0057\nw/o simulate\n1.18\n1.28\n0.74\n1.93\n2.18\n1.32\n0.0322\nw/o finetune\n1.54\n1.65\n0.60\n2.52\n2.74\n1.07\n0.0187\ncomprehensive two-step method, while the purple and blue\nlines represent the metrics for each frame using the single-\nstep approach. It is evident that the metrics of the complete\npipeline demonstrate clear advantages.\n0\n200\n400\n600\n800\n1000\n1200\n1\n2\n3\n4\nPVE [mm]\n0\n200\n400\n600\n800\n1000\n1200\n2\n4\n6\nLMKVE [mm]\n0\n200\n400\n600\n800\n1000\n1200\nFrame\n0.000\n0.025\n0.050\n0.075\n0.100\nMSE\nOurs\nOurs w/o simulate\nOurs w/o fnetune\nFigure 11. Quantitative result of our method on a data sequence.\nWe plot the PVE, PVE LMK and MSE calculated per frame with\nARKit as ground truth on a sequence in our testset. The result\nshows the reliability of our two stage training strategy.\nWe also present specific numerical values in Table 2.\nThis quantitative analysis reveals that our full pipeline, inte-\ngrating both simulate dataset and finetuning, yields the most\nfavorable outcomes. It indicates that there is a tendency to-\nwards underfitting in the results without the finetunning step\nand overfitting without the simulate dataset. These findings\nunderscore the importance of our approach in network train-\ning.\n5.3. Application\nPrivacy Protected Facial Animation\nIn today\u2019s digital\nlandscape, privacy concerns are increasingly at the fore-\nfront, particularly in the field of facial animation and digital\navatars. The surge in popularity of virtual platforms un-\nderscores the necessity of safeguarding user anonymity and\nprivacy, especially for those who choose to express them-\nselves through digital avatars. This issue is especially crit-\nical in contexts like virtual streaming or VTubing, where\ncreators often seek to preserve their privacy while engaging\nwith a broad audience. Traditional facial animation meth-\nods predominantly hinge on visual data capture, typically\nnecessitating model localization via camera-based systems.\nThese methods, reliant on visual inputs to track facial move-\nments, can inadvertently breach privacy. Moreover, they are\nconstrained by environmental factors, such as the need for\nadequate lighting and a direct visual path to the user\u2019s face,\nleading to challenges in poorly lit settings or when the user\u2019s\nFigure 12. The application of Vtubing. Our method utilizes IMUs\nfor facial animation, eliminating the requirement for direct expo-\nsure under camera.\nface isn\u2019t consistently oriented towards the camera. Our\ninnovative approach leverages Inertial Measurement Units\n(IMUs) for facial animation, thus obviating the need for\ndirect visual capture of the face. This technique not only\nupholds user privacy by avoiding the capture and storage of\nvisual facial data but also surmounts the limitations inherent\nin traditional camera-based systems. With our IMU-based\ntechnology, factors like lighting conditions or the orienta-\ntion of the face are no longer impediments. In the realm of\nVTubing, our method offers significant benefits. It ensures\nprivacy for content creators who prefer anonymity, while\nadeptly addressing the challenges of operating in environ-\nments unsuitable for conventional visual capture methods,\nsuch as in low light conditions or when the user is not di-\nrectly facing the camera. This application is exemplified in\nFig. 12, where a user wear our IMU device and employs\nIMUSIC to deduce expression blendshape parameters. We\nfacilitate avatar animation through a sequence of parame-\nters streamed into Unity [77]. As depicted in the figure, our\nmethod accurately predicts expressions even under complex\nscenarios, such as when the presenter turns their head or ex-\nperiences occlusion.\nHybrid Capture\nWe now explore the concept of hybrid\ncapture, a technique that synergizes the capabilities of two\ndistinct technologies: IMU-based facial capture and Ap-\nple\u2019s ARKit. Hybrid capture entails integrating blendshape\nparameters predicted by our IMU system with those de-\ntected by ARKit. This method is particularly advantageous\nin scenarios where ARKit\u2019s visual signals are obscured, al-\nlowing the IMU-derived blendshape signals to supplement\nFigure 13. The application of virtual album production and song\nrecording. In this setting, IMUSIC effectively compensates for\nobscured mouth movements, ensuring a comprehensive capture of\nfacial expressions.\nand substitute the visual data. Our implementation of hy-\nbrid capture is showcased on virtual album production. In\nstudio settings, a common issue is the occlusion of actors\u2019\nmouths by microphones, hindering vision-based systems\nlike ARKit from capturing complete facial expressions.\nHere, IMUs adeptly compensate for the obscured mouth\nmovements, ensuring a comprehensive facial capture. This\nresults in virtual performances that are both more realistic\nand expressive. In Fig. 13, we illustrate this application, us-\ning parameter sequences from the IMUSIC pipeline to re-\nplace the corresponding ARKit-recorded sequences, focus-\ning on parameters such as Jawopen, Mouthsmile, etc. The\noutcomes confirm our method\u2019s efficacy in supplementing\nactors\u2019 occluded mouth movements.\nCapture Minute Motion\nLeveraging the IMU\u2019s high sen-\nsitivity, our study was able to detect minute facial mo-\ntion with precision. In Fig. 14, the IMU\u2019s effectiveness is\nshowcased in capturing a participant\u2019s subtle cheek puffing,\nmarked by the dark gray segment on the second row. We\nuse the rotation measured by the IMU at the cheek position\nto calculate the angle with the neutral pose, which is then\nFigure 14. Application for capturing minute motion. For the first\nrow, we show an image of the participant puffing slightly, the re-\nsults detected by ARKit, and the results we measured using the\nIMU. In the second row of the graph, the dark grey area represents\nthe time the participant puffs his cheek, and the red and purple\nlines represent the Checkpuff parameters obtained by our method\nand ARKit respectively.\nmapped to the \u2019Cheekpuff\u2019 blendshape parameter.\nWe also demonstrate the comparison of our mapping (red\nline) to the weight detected by ARKit (purple line), reveal-\ning a greater alignment with the actual puffing action in the\nIMU data. Notably, the ARKit signal barely changes with\nthe bulge because the action is so subtle. For visual clar-\nity, we show the participant\u2019s puff of the check at a given\nmoment (indicated by the red triangle) in the first row, illus-\ntrating the IMU\u2019s heightened sensitivity relative to ARKit\u2019s\nsignal capture capabilities.\n5.4. Limitation and Discussion\nWe have demonstrated the compelling capability of\nIMUSIC for its non-visual facial motion capture in a vari-\nety of applications. Nevertheless, as a novel trial for purely\nIMU-based facial performance capture, our approach is\nsubject to some limitations.\nFirst, in our prototyped hardware, the signal detec-\ntion module still needs to be attached to the facial skins\nof the performance with double-sided and adhesive tapes,\nwhile the data transmission also necessitates tedious wiring.\nGiven the expected continuation of the technological trend\nof material science and integrated circuits, IMU-like proto-\ntypes are becoming cheaper, smaller, safer, and more perva-\nsive. Inspired by the motion gloves using flexible electronic\nmaterials [57], it\u2019s promising to explore more user-friendly\nand wearable solutions with electronic materials, as concep-\ntually depicted in Fig. 15. We believe that a motion capture\nsystem like IMUSIC will become a viable alternative to tra-\nditional facial capture technologies.\nBesides, the training scheme in IMUSIC still relies on\na two-stage pretrain-then-finetuning paradigm, which lim-\nits its generalizability compared to vision-based approaches\nlike DECA and ARKit.\nThis is mainly due to the data\nscarcity of our IMU-ARKit dataset. Besides our data aug-\nmentation, it is promising to utilize the unsupervised and\nself-supervised strategy [24] to reduce data dependency fur-\nther. Yet, it requires future exploration to bridge the dif-\nference between simulated and actual IMU signals and en-\nhance our simulation module to more closely mimic real\ndata. Moreover, the current purely IMU-based facial cap-\nture scheme still falls short of capturing extremely chal-\nlenging facial motions, especially for those regions near the\neyelids, where IMU sensors cannot be easily attached. It\u2019s\ninteresting to combine our scheme with other non-visual\nmodalities, like audio and optoelectronic signals for more\nfaithful and privacy-protected facial capture.\nTo stimulate the above novel and exciting paths of fa-\ncial capture in our community, we will make both our IMU-\nARKit dataset and the companion codes of our IMUSIC ap-\nproach publicly available. Our approach is the first of its\nkind to open up the research direction for data-driven IMU-\nbased facial tracking and analysis. Thus, we believe that\nthe accessible IMUSIC implementation and IMU-ARKit\ndataset will lay a solid cornerstone and bring huge poten-\ntial for future exploration. Note that despite the inherent\nprivacy-preserving capability of IMUSIC, the collection of\nextensive facial data remains a challenge. We have ensured\ninformed consent from all participants regarding data usage\nand collection processes and have secured IRB permission\nto uphold ethical standards in our research. In summary,\nwhile IMUSIC presents a novel path for facial capture, its\nevolution is intertwined with advancements in hardware de-\nsign, training methodologies, and ethical data collection\npractices.\n6. Conclusion\nWe have introduced IMUSIC, a novel paradigm for fa-\ncial motion capture from purely IMU signals, distinct from\ntraditional visual methods. In IMUSIC, our tailored micro-\nIMUs are strategically attached to face regions aligned with\nfacial anatomy, to capture a wide spectrum of nuanced facial\nmovements. We then provide IMU-ARKit, the first-of-its-\nkind dataset with synchronous IMU and visual signals of di-\nverse facial expressions from various performers. Based on\nIMU-ARKit, we propose a strong baseline method for the\nnovel task: IMU-driven facial motion capture. Specifically,\nFigure 15. The conceptual visualization showcases the next gener-\nation of IMU-based facial capture devices. The image is generated\nby DALL-E.\nwe tailor a transformer-based diffusion model to predict\nthe expression parameters from purely IMU signals with a\nnovel two-stage training strategy. We conduct extensive ex-\nperiments to demonstrate the effectiveness of our approach.\nWe then showcase a series of novel potential applications\nusing IMUSIC, ranging from privacy-protecting facial cap-\nture for anonymous Virtual YouTubers, to hybrid capture to\novercome challenges like occlusions, or to detect minute fa-\ncial movements that are often invisible through visual cam-\neras. With the novel task and our companion dataset and\nbaseline solution, we believe IMUSIC renews the research\nboundary of facial motion capture and analysis.\nReferences\n[1] David W Aha. Lazy learning. In Lazy learning, pages 7\u201310.\nSpringer, 1997. 5\n[2] Norhafizan Ahmad, Raja Ariffin Raja Ghazilla, Nazirah M\nKhairi, and Vijayabaskar Kasi. Reviews on various inertial\nmeasurement unit (imu) sensor applications. International\nJournal of Signal Processing Systems, 1(2):256\u2013262, 2013.\n4\n[3] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo,\nAndrew Fitzgibbon, and Thomas J Cashman. Flag: Flow-\nbased 3d avatar generation from sparse observations.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13253\u201313262, 2022.\n5\n[4] Sheldon Andrews, Ivan Huerta, Taku Komura, Leonid Si-\ngal, and Kenny Mitchell. Real-time physics-based motion\ncapture with sparse sensors.\nIn Proceedings of the 13th\nEuropean conference on visual media production (CVMP\n2016), pages 1\u201310, 2016. 5\n[5] Apple. Arkit. https://developer.apple.com/\narkit/, 2023. 2, 13\n[6] Eric R Bachmann, Robert B McGhee, Xiaoping Yun, and\nMichael J Zyda. Inertial and magnetic posture tracking for\ninserting humans into networked virtual environments. In\nProceedings of the ACM symposium on Virtual reality soft-\nware and technology, pages 9\u201316, 2001. 4\n[7] Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang,\nSheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei\nJiang, Jue Wang, et al. High-fidelity 3d digital human head\ncreation from rgb-d selfies. ACM Transactions on Graphics\n(TOG), 41(1):1\u201321, 2021. 4\n[8] J\u00b4ulio C\u00b4esar Batista, V\u00b4\u0131tor Albiero, Olga RP Bellon, and Lu-\nciano Silva. Aumpnet: simultaneous action units detection\nand intensity estimation on multipose facial images using a\nsingle convolutional neural network. In 2017 12th IEEE in-\nternational conference on automatic face & gesture recog-\nnition (FG 2017), pages 866\u2013871. IEEE, 2017. 4\n[9] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel,\nPaul A Beardsley, Craig Gotsman, Robert W Sumner, and\nMarkus H Gross. High-quality passive facial performance\ncapture using anchor frames. ACM Trans. Graph., 30(4):\n75, 2011. 4\n[10] L Bianchi, D Angelini, GP Orani, and F Lacquaniti. Kine-\nmatic coordination in human gait: relation to mechanical\nenergy cost. Journal of neurophysiology, 79(4):2155\u20132170,\n1998. 3\n[11] V Blanz and T Vetter. A morphable model for the synthe-\nsis of 3d faces. In 26th Annual Conference on Computer\nGraphics and Interactive Techniques (SIGGRAPH 1999),\npages 187\u2013194. ACM Press, 1999. 3\n[12] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla\nSheffer. High resolution passive facial performance cap-\nture. In ACM SIGGRAPH 2010 papers, pages 1\u201310. 2010.\n4\n[13] Chen Cao, Yanlin Weng, Stephen Lin, and Kun Zhou.\n3d shape regression for real-time facial animation. ACM\nTransactions on Graphics (TOG), 32(4):1\u201310, 2013. 4\n[14] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun\nZhou. Facewarehouse: A 3d facial expression database for\nvisual computing. IEEE Transactions on Visualization and\nComputer Graphics, 20(3):413\u2013425, 2013. 4\n[15] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler.\nReal-time high-fidelity facial performance capture. ACM\nTransactions on Graphics (ToG), 34(4):1\u20139, 2015. 4\n[16] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authentic\nvolumetric avatars from a phone scan. ACM Transactions\non Graphics (TOG), 41(4):1\u201319, 2022. 4\n[17] Xudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face\nalignment by explicit shape regression. International jour-\nnal of computer vision, 107:177\u2013190, 2014. 3\n[18] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affin-\nity fields. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 7291\u20137299,\n2017. 3\n[19] Timothy F Cootes, Christopher J Taylor, David H Cooper,\nand Jim Graham. Active shape models-their training and\napplication. Computer vision and image understanding, 61\n(1):38\u201359, 1995. 3\n[20] Timothy F. Cootes, Gareth J. Edwards, and Christopher J\nTaylor. Active appearance models. IEEE Transactions on\npattern analysis and machine intelligence, 23(6):681\u2013685,\n2001. 3\n[21] Juan Antonio Corrales, Francisco A Candelas, and Fer-\nnando Torres. Hybrid tracking of human operators using\nimu/uwb data fusion by a kalman filter.\nIn Proceedings\nof the 3rd ACM/IEEE international conference on Human\nrobot interaction, pages 193\u2013200, 2008. 3\n[22] Edilson de Aguiar, Christian Theobalt, Marcus Magnor,\nHolger Theisel, and H-P Seidel.\nM/sup 3: marker-free\nmodel reconstruction and motion tracking from 3d voxel\ndata. In 12th Pacific Conference on Computer Graphics and\nApplications, 2004. PG 2004. Proceedings., pages 101\u2013\n110. IEEE, 2004. 3\n[23] Michael B Del Rosario, Heba Khamis, Phillip Ngo, Nigel H\nLovell, and Stephen J Redmond.\nComputationally effi-\ncient adaptive error-state kalman filter for attitude estima-\ntion. IEEE Sensors Journal, 18(22):9332\u20139342, 2018. 4\n[24] Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philippe-\nHenri Gosselin, Christian Theobalt, and Louis Chevallier.\nTowards high fidelity monocular face reconstruction with\nrich reflectance using self-supervised learning and ray trac-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 12819\u201312829, 2021. 16\n[25] Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben\nLundell, Thomas J Cashman, and Jamie Shotton.\nFull-\nbody motion from a single head-mounted device: Generat-\ning smpl poses from partial observations. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 11687\u201311697, 2021. 5\n[26] Bernhard Egger, William AP Smith, Ayush Tewari, Ste-\nfanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian\nBernard, Timo Bolkart, Adam Kortylewski, Sami Romd-\nhani, et al. 3d morphable face models\u2014past, present, and\nfuture. ACM Transactions on Graphics (ToG), 39(5):1\u201338,\n2020. 4, 6\n[27] Yao Feng, Haiwen Feng, Michael J Black, and Timo\nBolkart. Learning an animatable detailed 3d face model\nfrom in-the-wild images. ACM Transactions on Graphics\n(ToG), 40(4):1\u201313, 2021. 2, 12, 13\n[28] Giancarlo Ferrigno, NA Borghese, and Antonio Pedotti.\nPattern recognition in 3d automatic human motion analysis.\nISPRS Journal of Photogrammetry and Remote Sensing, 45\n(4):227\u2013246, 1990. 3\n[29] Eric Foxlin. Inertial head-tracker sensor fusion by a com-\nplementary separate-bias kalman filter. In Proceedings of\nthe IEEE 1996 Virtual Reality Annual International Sym-\nposium, pages 185\u2013194. IEEE, 1996. 4\n[30] Andrew Gilbert, Matthew Trumble, Charles Malleson,\nAdrian Hilton, and John Collomosse.\nFusing visual and\ninertial sensors with semantics for 3d human pose estima-\ntion. International Journal of Computer Vision, 127:381\u2013\n397, 2019. 5\n[31] Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen\nLei, and Stan Z Li. Towards fast, accurate and stable 3d\ndense face alignment. In European Conference on Com-\nputer Vision, pages 152\u2013168. Springer, 2020. 2, 4, 12, 13\n[32] Yan Guo, Gang Xu, and Saburo Tsuji. Understanding hu-\nman motion patterns.\nIn Proceedings of the 12th IAPR\nInternational Conference on Pattern Recognition, Vol. 3-\nConference C: Signal Processing (Cat. No. 94CH3440-5),\npages 325\u2013329. IEEE, 1994. 3\n[33] Yudong Guo, Jianfei Cai, Boyi Jiang, Jianmin Zheng, et al.\nCnn-based real-time dense face reconstruction with inverse-\nrendered photo-realistic face images.\nIEEE transactions\non pattern analysis and machine intelligence, 41(6):1294\u2013\n1307, 2018. 4\n[34] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-\nard Pons-Moll, and Christian Theobalt. Livecap: Real-time\nhuman performance capture from monocular video. ACM\nTransactions On Graphics (TOG), 38(2):1\u201317, 2019. 4\n[35] Julius Hannink, Thomas Kautz, Cristian F Pasluosta, Karl-\nG\u00a8unter Ga\u00dfmann, Jochen Klucken, and Bjoern M Eskofier.\nSensor-based gait parameter extraction with deep convolu-\ntional neural networks. IEEE journal of biomedical and\nhealth informatics, 21(1):85\u201393, 2016. 5\n[36] Thomas Helten, Meinard Muller, Hans-Peter Seidel, and\nChristian Theobalt. Real-time body tracking with one depth\ncamera and inertial sensors. In Proceedings of the IEEE\ninternational conference on computer vision, pages 1105\u2013\n1112, 2013. 5\n[37] Roberto Henschel, Timo Von Marcard, and Bodo Rosen-\nhahn. Accurate long-term multiple people tracking using\nvideo and body-worn imus. IEEE Transactions on Image\nProcessing, 29:8476\u20138489, 2020. 5\n[38] Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, and\nQiang Xu. Deepfuse: An imu-aware network for real-time\n3d human pose estimation from multi-view image. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 429\u2013438, 2020. 3\n[39] Yinghao\nHuang,\nManuel\nKaufmann,\nEmre\nAksan,\nMichael J Black, Otmar Hilliges, and Gerard Pons-Moll.\nDeep inertial poser: Learning to reconstruct human pose\nfrom sparse inertial measurements in real time. ACM Trans-\nactions on Graphics (TOG), 37(6):1\u201315, 2018. 5\n[40] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender,\nLarissa Laich, Patrick Snape, and Christian Holz. Avatar-\nposer: Articulated full-body pose tracking from sparse mo-\ntion sensing. In European Conference on Computer Vision,\npages 443\u2013460. Springer, 2022. 5\n[41] Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shun-\nsuke Saito, Ronald Yu, Hao Li, and Jaakko Lehtinen.\nProduction-level facial performance capture using deep\nconvolutional neural networks. In Proceedings of the ACM\nSIGGRAPH/Eurographics symposium on computer anima-\ntion, pages 1\u201310, 2017. 4\n[42] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He,\nKarl Bladin, and Hao Li. Dynamic facial asset and rig gen-\neration from a single scan. ACM Trans. Graph., 39(6):215\u2013\n1, 2020. 8\n[43] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose es-\ntimation via ego-head pose estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17142\u201317151, 2023. 2, 11\n[44] Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, and Jingyi Yu.\nPiano: A parametric hand bone model from magnetic res-\nonance imaging. arXiv preprint arXiv:2106.10893, 2021.\n3\n[45] Daizong Liu, Hongting Zhang, and Pan Zhou. Video-based\nfacial expression recognition using graph convolutional net-\nworks. In 2020 25th International Conference on Pattern\nRecognition (ICPR), pages 607\u2013614. IEEE, 2021. 4\n[46] Huajun Liu, Xiaolin Wei, Jinxiang Chai, Inwoo Ha, and\nTaehyun Rhee.\nRealtime human motion control with a\nsmall number of inertial sensors. In Symposium on inter-\nactive 3D graphics and games, pages 133\u2013140, 2011. 4\n[47] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep appearance models for face rendering. ACM\nTransactions on Graphics (ToG), 37(4):1\u201313, 2018. 4\n[48] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J Black.\nSmpl: A skinned\nmulti-person linear model. ACM Transactions on Graph-\nics, 34(6), 2015. 2, 3\n[49] Luming Ma and Zhigang Deng.\nReal-time hierarchical\nfacial performance capture.\nIn Proceedings of the ACM\nSIGGRAPH Symposium on Interactive 3D Graphics and\nGames, pages 1\u201310, 2019. 4\n[50] Oleg Makaussov, Mikhail Krassavin, Maxim Zhabinets,\nand Siamac Fazli.\nA low-cost, imu-based real-time on\ndevice gesture recognition glove.\nIn 2020 IEEE Inter-\nnational Conference on Systems, Man, and Cybernetics\n(SMC), pages 3346\u20133351. IEEE, 2020. 5\n[51] Charles Malleson, Andrew Gilbert, Matthew Trumble, John\nCollomosse, Adrian Hilton, and Marco Volino. Real-time\nfull-body motion capture from video and imus. In 2017\nInternational Conference on 3D Vision (3DV), pages 449\u2013\n457. IEEE, 2017. 4\n[52] Charles Malleson, John Collomosse, and Adrian Hilton.\nReal-time multi-person motion capture from multi-view\nvideo and imus. International Journal of Computer Vision,\n128:1594\u20131611, 2020. 5\n[53] Jennifer L McGinley, Richard Baker, Rory Wolfe, and\nMeg E Morris. The reliability of three-dimensional kine-\nmatic gait measurements: a systematic review. Gait & pos-\nture, 29(3):360\u2013369, 2009. 4\n[54] Vladimir Medved. Measurement and Analysis of Human\nLocomotion. Springer, 2021. 4\n[55] Brice Michoud, Erwan Guillou, Hector Briceno, and Sa\u00a8\u0131da\nBouakaz. Real-time marker-free motion capture from mul-\ntiple cameras. In 2007 IEEE 11th International Conference\non Computer Vision, pages 1\u20137. IEEE, 2007. 3\n[56] Emily Miller, Kenton Kaufman, Trevor Kingsbury, Erik\nWolf, Jason Wilken, and Marilynn Wyatt. Mechanical test-\ning for three-dimensional motion analysis reliability. Gait\n& posture, 50:116\u2013119, 2016. 4\n[57] Chaithanya Kumar Mummadi, Frederic Philips Peter Leo,\nKeshav Deep Verma, Shivaji Kasireddy, Philipp M Scholl,\nJochen Kempfle, and Kristof Van Laerhoven. Real-time and\nembedded detection of hand gestures with an imu-based\nglove. In Informatics, page 28. MDPI, 2018. 5, 16\n[58] Noitom. Noitom Motion Capture Systems. https://\nwww.noitom.com/, 2015. 5\n[59] Kyle Olszewski, Joseph J Lim, Shunsuke Saito, and Hao\nLi. High-fidelity facial and speech animation for vr hmds.\nACM Transactions on Graphics (TOG), 35(6):1\u201314, 2016.\n4\n[60] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami\nRomdhani, and Thomas Vetter. A 3d face model for pose\nand illumination invariant face recognition. In 2009 sixth\nIEEE international conference on advanced video and sig-\nnal based surveillance, pages 296\u2013301. Ieee, 2009. 4\n[61] Gerard\nPons-Moll,\nAndreas\nBaak,\nThomas\nHelten,\nMeinard M\u00a8uller, Hans-Peter Seidel, and Bodo Rosenhahn.\nMultisensor-fusion for 3d full-body human motion capture.\nIn 2010 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pages 663\u2013670. IEEE,\n2010. 4\n[62] Gerard Pons-Moll, Andreas Baak, Juergen Gall, Laura\nLeal-Taixe, Meinard Mueller, Hans-Peter Seidel, and Bodo\nRosenhahn. Outdoor human motion capture using inverse\nkinematics and von mises-fisher sampling. In 2011 Interna-\ntional Conference on Computer Vision, pages 1243\u20131250.\nIEEE, 2011. 4\n[63] Ammar Qammaz and Antonis A Argyros. A unified ap-\nproach for occlusion tolerant 3d facial pose capture and\ngaze estimation using mocapnets.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 3178\u20133188, 2023. 4\n[64] QST Inc. QST Corporation Limited. https://www.\nqstcorp.com/, 2012. 5\n[65] Cl\u00b4ement Reverdy, Sylvie Gibet, and Caroline Larboulette.\nOptimal marker set for motion capture of dynamical facial\nexpressions. In Proceedings of the 8th ACM SIGGRAPH\nConference on Motion in Games, pages 31\u201336, 2015. 4\n[66] Qaiser Riaz, Guanhong Tao, Bj\u00a8orn Kr\u00a8uger, and Andreas\nWeber. Motion reconstruction using very few accelerom-\neters and ground contacts. Graphical Models, 79:23\u201338,\n2015. 5\n[67] Daniel Roetenberg, Henk J Luinge, Chris TM Baten, and\nPeter H Veltink. Compensation of magnetic disturbances\nimproves inertial and magnetic sensing of human body seg-\nment orientation. IEEE Transactions on neural systems and\nrehabilitation engineering, 13(3):395\u2013405, 2005. 4\n[68] Javier Romero, Dimitrios Tzionas, and Michael J Black.\nEmbodied hands. ACM Transactions on Graphics, 36(6):\n1\u201317, 2017. 3\n[69] Martin Schepers, Matteo Giuberti, Giovanni Bellusci, et al.\nXsens mvn: Consistent tracking of human motion using in-\nertial sensing. Xsens Technol, 1(8):1\u20138, 2018. 5\n[70] Soshi Shimada, Vladislav Golyanik, Patrick P\u00b4erez, and\nChristian Theobalt. Decaf: Monocular deformation capture\nfor face and hand interactions, 2023. 12\n[71] Ronit Slyper and Jessica K Hodgins. Action capture with\naccelerometers.\nIn Proceedings of the 2008 ACM SIG-\nGRAPH/Eurographics symposium on computer animation,\npages 193\u2013199, 2008. 5\n[72] William AP Smith, Alassane Seck, Hannah Dee, Bernard\nTiddeman, Joshua B Tenenbaum, and Bernhard Egger.\nA morphable face albedo model.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5011\u20135020, 2020. 4\n[73] SONY.\nMobile Motion Capture \u201dmocopi\u201d.\nhttps:\n//www.sony.net/Products/mocopi-dev/en/,\n2023. 3, 5\n[74] Jochen Tautges, Arno Zinke, Bj\u00a8orn Kr\u00a8uger, Jan Baumann,\nAndreas Weber, Thomas Helten, Meinard M\u00a8uller, Hans-\nPeter Seidel, and Bernd Eberhardt.\nMotion reconstruc-\ntion using sparse accelerometer data. ACM Transactions\non Graphics (ToG), 30(3):1\u201312, 2011. 5\n[75] Anh Tuan Tran, Tal Hassner, Iacopo Masi, and G\u00b4erard\nMedioni.\nRegressing robust and discriminative 3d mor-\nphable models with a very deep neural network. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 5163\u20135172, 2017. 4\n[76] Zarins Uldis. Anatomy of Facial Expressions. Anatomy\nNext, Inc., 2017. 5, 7, 11\n[77] Unity Technologies. Unity. https://unity.com/,\n2023. Version 2023.1. 14\n[78] UnrealEngine. Live link face. https://apps.apple.\ncom/us/app/live-link-face/id1495370836,\n2023. 8\n[79] Rachel V Vitali, Ryan S McGinnis, and Noel C Perkins.\nRobust error-state kalman filter for estimating imu orienta-\ntion. IEEE Sensors Journal, 21(3):3561\u20133569, 2020. 4\n[80] Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John\nBarnwell, Markus Gross, Wojciech Matusik, and Jovan\nPopovi\u00b4c. Practical motion capture in everyday surround-\nings. ACM transactions on graphics (TOG), 26(3):35\u2013es,\n2007. 3, 4\n[81] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan\nPopovi\u00b4c. Articulated mesh animation from multi-view sil-\nhouettes. In Acm Siggraph 2008 papers, pages 1\u20139. 2008.\n3\n[82] Timo Von Marcard, Gerard Pons-Moll, and Bodo Rosen-\nhahn. Human pose estimation from video and imus. IEEE\ntransactions on pattern analysis and machine intelligence,\n38(8):1533\u20131547, 2016. 4\n[83] Timo Von Marcard, Bodo Rosenhahn, Michael J Black, and\nGerard Pons-Moll. Sparse inertial poser: Automatic 3d hu-\nman pose estimation from sparse imus. In Computer graph-\nics forum, pages 349\u2013360. Wiley Online Library, 2017. 3,\n5\n[84] Timo Von Marcard, Roberto Henschel, Michael J Black,\nBodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-\ncurate 3d human pose in the wild using imus and a mov-\ning camera. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 601\u2013617, 2018. 3\n[85] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,\nWayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change\nLoy. Mead: A large-scale audio-visual dataset for emo-\ntional talking-face generation. In European Conference on\nComputer Vision, pages 700\u2013717. Springer, 2020. 8\n[86] Thibaut Weise, Sofien Bouaziz, Hao Li, and Mark Pauly.\nRealtime performance-based facial animation. ACM trans-\nactions on graphics (TOG), 30(4):1\u201310, 2011. 4\n[87] Alexander Winkler, Jungdam Won, and Yuting Ye. Quest-\nsim: Human motion tracking from sparse sensors with sim-\nulated avatars. In SIGGRAPH Asia 2022 Conference Pa-\npers, pages 1\u20138, 2022. 5\n[88] XSENS.\nXsens Technologies B.V.\nhttps://www.\nxsens.com/, 2011. 3, 5\n[89] Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lobstr:\nReal-time lower-body pose prediction from sparse upper-\nbody tracking signals. In Computer Graphics Forum, pages\n265\u2013275. Wiley Online Library, 2021. 5\n[90] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-\ntime 3d human translation and pose estimation with six in-\nertial sensors. ACM Transactions on Graphics (TOG), 40\n(4):1\u201313, 2021. 2, 5, 6, 8\n[91] Jae Shin Yoon,\nTakaaki Shiratori,\nShoou-I Yu,\nand\nHyun Soo Park. Self-supervised adaptation of high-fidelity\nface models for monocular performance tracking. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4601\u20134609, 2019. 4\n[92] Jun Yu and Zengfu Wang.\nA video-based facial motion\ntracking and expression recognition system.\nMultimedia\nTools and Applications, 76:14653\u201314672, 2017. 4\n[93] Qilong Yuan and I-Ming Chen. Localization and velocity\ntracking of human via 3 imu sensors. Sensors and Actuators\nA: Physical, 212:25\u201333, 2014. 3\n[94] Li Zhang, Noah Snavely, Brian Curless, and Steven M\nSeitz. Spacetime faces: high resolution capture for mod-\neling and animation.\nIn ACM SIGGRAPH 2004 Papers,\npages 548\u2013558. 2004. 4\n[95] Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang\nLin, Ruixiang Cao, Wei Yang, Lan Xu, and Jingyi Yu.\nVideo-driven neural physically-based facial asset for pro-\nduction, 2022. 4\n[96] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan\nZhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu,\nand Jingyi Yu. Dreamface: Progressive generation of ani-\nmatable 3d faces under text guidance, 2023. 10\n[97] Zhengyou Zhang. Microsoft kinect sensor and its effect.\nIEEE multimedia, 19(2):4\u201310, 2012. 3\n[98] Zhe Zhang, Chunyu Wang, Wenhu Qin, and Wenjun Zeng.\nFusing wearable imus with multi-view images for human\npose estimation: A geometric approach. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2200\u20132209, 2020. 5\n[99] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai,\nLu Fang, and Yebin Liu. Hybridfusion: Real-time perfor-\nmance capture using a single depth sensor and sparse imus.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), pages 384\u2013400, 2018. 5\n[100] Yi Zhou, Wei Zhang, Xiaoou Tang, and Harry Shum. A\nbayesian mixture model for multi-view face alignment. In\n2005 IEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition (CVPR\u201905), pages 741\u2013746.\nIEEE, 2005. 3\n[101] Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. Face\nalignment in full pose range: A 3d total solution. IEEE\ntransactions on pattern analysis and machine intelligence,\n41(1):78\u201392, 2017. 2\n"
  },
  {
    "title": "EscherNet: A Generative Model for Scalable View Synthesis",
    "link": "https://arxiv.org/pdf/2402.03908.pdf",
    "upvote": "5",
    "text": "EscherNet: A Generative Model for Scalable View Synthesis\nXin Kong1*\nShikun Liu1* Xiaoyang Lyu2\nMarwan Taher1\nXiaojuan Qi2\nAndrew J. Davison1\n1Dyson Robotics Lab, Imperial College London\n2The University of Hong Kong\n\u2217Corresponding Authors: {x.kong21,shikun.liu17}@imperial.ac.uk\nFigure 1. We introduce EscherNet, a diffusion model that can generate a flexible number of consistent target views (highlighted in blue)\nwith arbitrary camera poses, based on a flexible number of reference views (highlighted in purple). EscherNet demonstrates remarkable\nprecision in camera control and robust generalisation across synthetic and real-world images featuring multiple objects and rich textures.\nAbstract\nWe introduce EscherNet, a multi-view conditioned dif-\nfusion model for view synthesis.\nEscherNet learns im-\nplicit and generative 3D representations coupled with a\nspecialised camera positional encoding, allowing precise\nand continuous relative control of the camera transforma-\ntion between an arbitrary number of reference and target\nviews. EscherNet offers exceptional generality, flexibility,\nand scalability in view synthesis \u2014 it can generate more\nthan 100 consistent target views simultaneously on a sin-\ngle consumer-grade GPU, despite being trained with a fixed\nnumber of 3 reference views to 3 target views. As a result,\nEscherNet not only addresses zero-shot novel view synthe-\nsis, but also naturally unifies single- and multi-image 3D\nreconstruction, combining these diverse tasks into a single,\ncohesive framework.\nOur extensive experiments demon-\nstrate that EscherNet achieves state-of-the-art performance\nin multiple benchmarks, even when compared to methods\nspecifically tailored for each individual problem. This re-\nmarkable versatility opens up new directions for designing\nscalable neural architectures for 3D vision. Project page:\nhttps://kxhit.github.io/EscherNet.\n1. Introduction\nView synthesis stands as a fundamental task in computer vi-\nsion and computer graphics. By allowing the re-rendering\nof a scene from arbitrary viewpoints based on a set of ref-\nerence viewpoints, this mimics the adaptability observed in\n1\narXiv:2402.03908v1  [cs.CV]  6 Feb 2024\nhuman vision. This ability is not only crucial for practi-\ncal everyday tasks like object manipulation and navigation,\nbut also plays a pivotal role in fostering human creativity,\nenabling us to envision and craft objects with depth, per-\nspective, and a sense of immersion.\nIn this paper, we revisit the problem of view synthesis\nand ask: How can we learn a general 3D representation to\nfacilitate scalable view synthesis? We attempt to investigate\nthis question from the following two observations:\ni) Up until now, recent advances in view synthesis have\npredominantly focused on training speed and/or rendering\nefficiency [12, 18, 30, 47]. Notably, these advancements\nall share a common reliance on volumetric rendering for\nscene optimisation. Thus, all these view synthesis methods\nare inherently scene-specific, coupled with global 3D spatial\ncoordinates. In contrast, we advocate for a paradigm shift\nwhere a 3D representation relies solely on scene colours\nand geometries, learning implicit representations without\nthe need for ground-truth 3D geometry, while also main-\ntaining independence from any specific coordinate system.\nThis distinction is crucial for achieving scalability to over-\ncome the constraints imposed by scene-specific encoding.\nii) View synthesis, by nature, is more suitable to be cast\nas a conditional generative modelling problem, similar to\ngenerative image in-painting [25, 59]. When given only a\nsparse set of reference views, a desired model should pro-\nvide multiple plausible predictions, leveraging the inherent\nstochasticity within the generative formulation and drawing\ninsights from natural image statistics and semantic priors\nlearned from other images and objects. As the available\ninformation increases, the generated scene becomes more\nconstrained, gradually converging closer to the ground-truth\nrepresentation. Notably, existing 3D generative models cur-\nrently only support a single reference view [20\u201323, 43]. We\nargue that a more desirable generative formulation should\nflexibly accommodate varying levels of input information.\nBuilding upon these insights, we introduce EscherNet,\nan image-to-image conditional diffusion model for view\nsynthesis.\nEscherNet leverages a transformer architec-\nture [50], employing dot-product self-attention to capture\nthe intricate relation between both reference-to-target and\ntarget-to-target views consistencies.\nA key innovation\nwithin EscherNet is the design of camera positional encod-\ning (CaPE), dedicated to representing both 4 DoF (object-\ncentric) and 6 DoF camera poses. This encoding incorpo-\nrates spatial structures into the tokens, enabling the model to\ncompute self-attention between query and key solely based\non their relative camera transformation. In summary, Esch-\nerNet exhibits these remarkable characteristics:\n\u2022 Consistency: EscherNet inherently integrates view con-\nsistency thanks to the design of camera positional encod-\ning, encouraging both reference-to-target and target-to-\ntarget view consistencies.\n\u2022 Scalability: Unlike many existing neural rendering meth-\nods that are constrained by scene-specific optimisation,\nEscherNet decouples itself from any specific coordinate\nsystem and the need for ground-truth 3D geometry, with-\nout any expensive 3D operations (e.g. 3D convolutions\nor volumetric rendering), making it easier to scale with\neveryday posed 2D image data.\n\u2022 Generalisation: Despite being trained on only a fixed\nnumber of 3 reference to 3 target views, EscherNet ex-\nhibits the capability to generate any number of target\nviews, with any camera poses, based on any number of\nreference views. Notably, EscherNet exhibits improved\ngeneration quality with an increased number of reference\nviews, aligning seamlessly with our original design goal.\nWe conduct a comprehensive evaluation across both\nnovel view synthesis and single/multi-image 3D reconstruc-\ntion benchmarks. Our findings demonstrate that EscherNet\nnot only outperforms all 3D diffusion models in terms of\ngeneration quality but also can generate plausible view syn-\nthesis given very limited views. This stands in contrast to\nthese scene-specific neural rendering methods such as In-\nstantNGP [30] and Gaussian Splatting [18], which often\nstruggle to generate meaningful content under such con-\nstraints. This underscores the effectiveness of our method\u2019s\nsimple yet scalable design, offering a promising avenue for\nadvancing view synthesis and 3D vision as a whole.\n2. Related Work\nNeural 3D Representations\nEarly works in neural 3D\nrepresentation learning focused on directly optimising on\n3D data, using representations such as voxels [26] and point\nclouds [39, 40], for explicit 3D representation learning. Al-\nternatively, another line of works focused on training neural\nnetworks to map 3D spatial coordinates to signed distance\nfunctions [34] or occupancies [28, 36], for implicit 3D rep-\nresentation learning. However, all these methods heavily\nrely on ground-truth 3D geometry, limiting their applicabil-\nity to small-scale synthetic 3D data [2, 54].\nTo accommodate a broader range of data sources, dif-\nferentiable rendering functions [32, 45] have been intro-\nduced to optimise neural implicit shape representations with\nmulti-view posed images. More recently, NeRF [29] paved\nthe way to a significant enhancement in rendering qual-\nity compared to these methods by optimising MLPs to en-\ncode 5D radiance fields. In contrast to tightly coupling 3D\nscenes with spatial coordinates, we introduce EscherNet as\nan alternative for 3D representation learning by optimising a\nneural network to learn the interaction between multi-view\nposed images, independent of any coordinate system.\nNovel View Synthesis\nThe success of NeRF has sparked\na wave of follow-up methods that address faster training\nand/or rendering efficiency, by incorporating different vari-\nants of space discretisation [3, 12, 14], codebooks [48], and\n2\nPoints\nVoxels\n<latexit sha1_base64=\"ydC2pm3VlnYPfSrK8+OGgiflbaY=\">ADEnicfVLThsxFHWmL5i+AizZILKpKhTNVH3sKlQ6gY1iAaQmCi6du4Mbjy2ZXtK0Ch/0T3b8gvsqm7A/2C/gaeNF04kXpXx+c+fO6DasGtS5Lfreje/QcPH62sxo+fPH32vL2fmJVZRj2mRLKnFGwKLjEvuNO4Jk2CUVeErHe43/9Csay5X87K40DkoJM85A+epYXsjm9WoLy+4w+nx/sH7JB62O0k3mdnWMkjnoEPm1hutf5kI8WqEqVjAqw9TxPtBjUYx5nAaZxVFjWwMR4XoK7EPnOlhV8hHZQH3LplfSMCsJq6zxAWfi+QocDWgkwk5AtYwMhQhZyRnmBljI6iLXQjkb6sKJVqYRPfpSWUfVwge2ogx0M7SlYspIaFoPHZcGdM6LkKRUiVFI5ULBQurEOsPlQqrfk1juMNfhG51SYiY+jrN9NsweOgn/kmjAafMyzoDU5QwmfrtFNlOg/4XyOW/QI/i5jTSxUNYBievunb7puj153dD/MjWSGbZJu8ICl5R3bJR9IjfcLIFbkm38lN9C26jX5EP/+GRq15zgYJLPp1BwrFBFM=</latexit>SDF > \uffff\n<latexit sha1_base64=\"P/hWhUEb5ApMHet2+hcJif4LVbY=\">ADEXicfVLbhMxFHWGVzu80nbZTU2CFXRDCqwYVFBhdhUTQVpK3Wi6Nq5MzXx2JbtoYNG+Qr23cIvsENs+wV8Ab9RTwgLJxJ3dXzuw+c+qBbcuiT53Ylu3b5z97aenz/wcNHj7sbmydWVYbhkCmhzBkFi4JLHDruBJ5pg1BSgad0+rb1n35GY7mSH90XjaMSCslzsB5atzdzOY1msL7nD24eDd62Tc7SX9ZG47qyBdgB5Z2GC80fmTRSrSpSOCbD2PE20GzVgHGcCZ3FWdTAplDgeQnuQuS7O1bwCdpRc8ilFzIwKghrPMAZeHbCh0OaCXA1CFbwhQZChGykjPMDbCQ1UWuhXI21IW1VqYVPflUWUfV0ge2ogx0O7OVYspIaFsPHZcGdM6LkKRUiUlI5ULBUmptneFyKdWvSax2mOvwjU4pMRcfx9kB+m0YPQTP9JowCnzrMnAFCXUM7+dIt0f8CufwX6FEc+9NIlw9hFZw876cv+y+O93r7bxZHska2yRPylKTkFdkn78mADAkjNbki38j36Gv0I/oZ/fobGnUWOVsksOj6Br5iBD0=</latexit>SDF < \uffff\nSDF\n<latexit sha1_base64=\"Wy5n8WXswBg1Z5+UQHJt2/8OBpg=\">ADI3icfVJNbxMxEHWr7J8peXIJSIcClpFuxUfJ6SqRYJLRUCkrZRdRbPO7MbEa1u2t2wb9Q/wP7hzhb/ADXHhwJm/gbeEgxOJOT2/eWO/8UyuODM2jn92gkuXr1y9tnE9vHz1u073c2tQyNrTXFEJZf6OAeDnAkcWY5HiuNUOUcj/L5fps/OkFtmBTv7KnCrIJSsIJRsI6adB8U2+MmOo3OotTO0EKUqhnLHvae98ZvX+6lhpUVZJNuPx7EF9FbB8kS9MkyhpPNzu90KmldobCUgzHjJFY2W4C2jHI8D9PaoAI6hxLHFdgZL6Ke4WyKJlscMOGsDbX0ZAtjHUBRukb9hIW85qAbn61gjhQ591nBKBYaqM+qslBcWuP7wkZJ3Zqevq+NzeXKA6bOKaj2F9cuk1pA27qf+KBFaz0yTyXfOpTBZewUtoYq5lYKXD5+sdFso/o5WSX5gPw/QFumloPHA/lqhBiv1o0UK2g25OXfTKdOoRf8TMvFP6FAYutVIVhdhHRzuDJKngydvHvd395ZLskHukftkmyTkGdklr8iQjAglH8ln8oV8DT4F34LvwY+/0qCzrLlLvAh+/QGZlgoq</latexit>\nf ([x, y, z, \u03b8, \uffff]) = [RGB\u03c3]\nNeRF\n<latexit sha1_base64=\"O8KnzplzFCLA50Z8gcYtTnjXBHU=\">ADJnicfVI9jxMxEHWjzuWrxyUNBEpOFAU7SIOKqQTUNCcCOhyFykborEzu2fitS3bC0Gr/AT+Bz0t/AU6hOjo+Bt4Q65wIjHV85s39ozfUC24dUnyqxVduHjp8s7ulfjqtes3brb3bp1YVRmGQ6aEMiMKFgWXOHTcCRxpg1BSgad0/rzJn75HY7mSx+6jxkJheQ5Z+A8NW3fy/fHdUbzmj59k2vk71A4WBFDJaT+0/PU8fTdjfpJ6vobIN0DbpkHYPpXutPNlOsKlE6JsDacZpoN6nBOM4ELuOsqiBzaHAcQnuTOS9jhV8hnZSH3HpuxsYFchq6zxAWfhZw4QDWgkwi5AtY4MhQhZyRnmBljI6iLXQjkb9oULrUzT9OxdZR1VGw/YijLQzUduXaMhGb0MPHBgM5EZKUKjELqVwo2ChdWGe43Cj1/ovtCXMdntEpJVbNx7E32Lth8Mj/+CuNBpwyD+oMTFHCYundKbJeg/4n5PJc6FEc+9VINxdhG5w87KeP+wevH3UPn62XZJfcIXfJPknJE3JIXpIBGRJGPpEv5Cv5Fn2Ovkc/op/pFrXObBH9/gtxmwuG</latexit>\nf ([XR, \u2206P]) = XT\nZero-1-to-3\n<latexit sha1_base64=\"DY4NUgjfkSeEgampKQe6xXr4S4=\">ADRXicfZJLj9MwEMfd8FrCqwtHLhW9LKiqkhUPCQlpBQhxKRS03a3UdKuxOwmjm3ZDhRF+UZ8D+7cENw5cUNcISlFwi1iTn/ZsaeGQ/VglsXRZ9awanTZ86e2zkfXrh46fKV9u7VI6sKw3DElFBmTMGi4BJHjuBY20QcirwmC4eNf7jN2gsV/LQvdM4zSGTPOUMXI1m7Sfp3qRMaNoZV7Myv+sOnZ6zA8N9gUJ0cTm8+CunIbN2N+pHK+tsi3gtumRtw9lu62syV6zIUTomwNpJHGk3LcE4zgRWYVJY1MAWkOEkB/dKpL2OFXyOdloOuKyLHxrlhZXW1QJlVo/CdzighQCz9GkOC2QohE8lZ5gaYD7VWaqFctavC5damabo+evCOqo2HrAFZaCbOW9dpoyEpnXf8daATnmQ0qVmPsoFQo2UpfWGS43Uuv1ENsdpto/o1NKrIoPw+Qx1r9hcFBP/LlGA06ZW2UCJsthWdW/kyW9Rv0vkMs/gbUKw3o14s1F2BZH+/34bv/Oi9vdg4frJdkh18kNskdico8ckKdkSEaEkfkI/lMvgQfgm/B9+DH79Cgtc65RjwLfv4C0Y8W7g=</latexit>\nf ([XR\n\uffff\u2236N, PR\n\uffff\u2236N, PT\n\uffff\u2236M]) = XT\n\uffff\u2236M\nEscherNet\nFigure 2. 3D representations overview. EscherNet generates a\nset of M target views XT\n1:M based on their camera poses PT\n1:M,\nleveraging information gained from a set of N reference views\nXR\n1:N and their camera poses PR\n1:N. EscherNet presents a new way\nof learning implicit 3D representations by only considering the\nrelative camera transformation between the camera poses of PR\nand PT , making it easier to scale with multi-view posed images,\nindependent of any specific coordinate systems.\nencodings using hash tables [30] or Gaussians [18].\nTo enhance NeRF\u2019s generalisation ability across diverse\nscenes and in a few-shot setting, PixelNeRF [58] attempts\nto learn a scene prior by jointly optimising multiple scenes,\nbut it is constrained by the high computational demands re-\nquired by volumetric rendering. Various other approaches\nhave addressed this issue by introducing regularisation tech-\nniques, such as incorporating low-level priors from local\npatches [33], ensuring semantic consistency [16], consid-\nering adjacent ray frequency [56], and incorporating depth\nsignals [9]. In contrast, EscherNet encodes scenes directly\nthrough the image space, enabling the learning of more gen-\neralised scene priors through large-scale datasets.\n3D Diffusion Models\nThe emergence of 2D generative\ndiffusion models has shown impressive capabilities in gen-\nerating realistic objects and scenes [15, 42]. This progress\nhas inspired the early design of text-to-3D diffusion mod-\nels, such as DreamFusion [38] and Magic3D [19], by op-\ntimising a radiance field guided by score distillation sam-\npling (SDS) from these pre-trained 2D diffusion models.\nHowever, SDS necessitates computationally intensive itera-\ntive optimisation, often requiring up to an hour for conver-\ngence. Additionally, these methods, including recently pro-\nposed image-to-3D generation approaches [8, 27, 55], fre-\nquently yield unrealistic 3D generation results due to their\nlimited 3D understanding, giving rise to challenges such as\nthe multi-face Janus problem.\nTo integrate 3D priors more efficiently, an alterna-\ntive approach involves training 3D generative models di-\nrectly on 3D datasets, employing representations like point\nclouds [31] or neural fields [4, 11, 17]. However, this de-\nsign depends on 3D operations, such as 3D convolution and\nvolumetric rendering, which are computationally expensive\nand challenging to scale.\nTo address this issue, diffusion models trained on multi-\nview posed data have emerged as a promising direction,\ndesigned with no 3D operations. Zero-1-to-3 [21] stands\nout as a pioneering work, learning view synthesis from\npaired 2D posed images rendered from large-scale 3D ob-\nject datasets [6, 7]. However, its capability is limited to\ngenerating a single target view conditioned on a single ref-\nerence view. Recent advancements in multi-view diffusion\nmodels [20, 22, 23, 43, 44, 57] focused on 3D generation\nand can only generate a fixed number of target views with\nfixed camera poses. In contrast, EscherNet can generate an\nunrestricted number of target views with arbitrary camera\nposes, offering superior flexibility in view synthesis.\n3. EscherNet\nProblem Formulation and Notation\nIn EscherNet, we\nrecast the view synthesis as a conditional generative mod-\nelling problem, formulated as:\nX T \u223c p(X T |X R, PR, PT ).\n(1)\nHere, X T = {XT\n1:M} and PT = {PT\n1:M} represent a set of\nM target views XT\n1:M with their global camera poses PT\n1:M.\nSimilarly, X R = {XR\n1:N} and PR = {PR\n1:N} represent\na set of N reference views XR\n1:N with their global camera\nposes PR\n1:N. Both N and M can take on arbitrary values\nduring both model training and inference.\nWe propose a neural architecture design, such that the\ngeneration of each target view XT\ni\n\u2208 X T solely depends\non its relative camera transformation to the reference views\n(PR\nj )\u22121PT\ni , \u2200PR\nj \u2208 PR, introduced next.\n3.1. Architecture Design\nWe design EscherNet following two key principles: i) It\nbuilds upon an existing 2D diffusion model, inheriting its\nstrong web-scale prior through large-scale training, and ii)\nIt encodes camera poses for each view/image, similar to\nhow language models encode token positions for each to-\nken. So our model can naturally handle an arbitrary number\nof views for any-to-any view synthesis.\nMulti-View Generation\nEscherNet can be seamlessly in-\ntegrated with any 2D diffusion model with a transformer\narchitecture, with no additional learnable parameters. In\nthis work, we design EscherNet by adopting a latent\ndiffusion architecture, specifically StableDiffusion\nv1.5 [42]. This choice enables straightforward compar-\nisons with numerous 3D diffusion models that also leverage\nthe same backbone (more details in the experiment section).\nTo tailor the Stable Diffusion model, originally designed\nfor text-to-image generation, to multi-view generation as\n3\nLightweight\nVision Encoder\nProjection\nReshape\n [(B, N), H, W, C] -> [B, (N, HW), C]\nTarget View\nReference View \nN views\nB Objects\nN views\nN views\nM views\nB Objects\nM views\nM views\nCross Attention \nBlock with CaPE\nFeed-Forward  \nBlock\nResidual \nBlock \u00d7 2\nSelf-Attention\nBlock with CaPE\nReference-to-Target\nConsistency\nTarget-to-Target\nConsistency\nReshape\n[(B, M), H, W, C] -> [B, (M, HW), C]\nDot-Product\nAttention with CaPE\nLinear\nLinear\nLinear\nLinear\nQuery\nKey\nValue\nReshape\n[B, (M, HW), C -> (B, M), H, W, C]\nKey\nQuery\nValue\nMatMul\nScale\nSoftMax\nMatMul\nCaPE for\nRef. Poses\nCaPE for\nTar. Poses\nStable Diffusion\nU-Net Layer\nConvNeXt-V2\nTiny\nFigure 3. EscherNet architecture details. EscherNet adopts the Stable Diffusion architectural design with minimal but important mod-\nifications. The lightweight vision encoder captures both high-level and low-level signals from N reference views. In U-Net, we apply\nself-attention within M target views to encourage target-to-target consistency, and cross-attention within M target and N reference views\n(encoded by the image encoder) to encourage reference-to-target consistency. In each attention block, CaPE is employed for the key and\nquery, allowing the attention map to learn with relative camera poses, independent of specific coordinate systems.\napplied in EscherNet, several key modifications are imple-\nmented. In the original Stable Diffusion\u2019s denoiser U-Net,\nthe self-attention block was employed to learn interactions\nwithin different patches within the same image. In Esch-\nerNet, we re-purpose this self-attention block to facilitate\nlearning interactions within distinct patches across M dif-\nferent target views, thereby ensuring target-to-target consis-\ntency. Likewise, the cross-attention block, originally used\nto integrate textual information into image patches, is repur-\nposed in EscherNet to learn interactions within N reference\nto M target views, ensuring reference-to-target consistency.\nConditioning Reference Views\nIn view synthesis, it is\ncrucial that the conditioning signals accurately capture\nboth the high-level semantics and low-level texture details\npresent in the reference views. Previous works in 3D dif-\nfusion models [21, 22] have employed the strategy of en-\ncoding high-level signals through a frozen CLIP pre-trained\nViT [41] and encoding low-level signals by concatenating\nthe reference image into the input of the U-Net of Sta-\nble Diffusion. However, this design choice inherently con-\nstrains the model to handle only one single view.\nIn EscherNet, we choose to incorporate both high-level\nand low-level signals in the conditioning image encoder,\nrepresenting reference views as sets of tokens. This de-\nsign choice allows our model to maintain flexibility in han-\ndling a variable number of reference views.\nEarly ex-\nperiments have confirmed that using a frozen CLIP-ViT\nalone may fail to capture low-level textures, preventing the\nmodel from accurately reproducing the original reference\nviews given the same reference view poses as target poses.\nWhile fine-tuning the CLIP-ViT could address this issue,\nit poses challenges in terms of training efficiency. Instead,\nwe opt to fine-tune a lightweight vision encoder, specifi-\ncally ConvNeXtv2-Tiny [53], which is a highly efficient\nCNN architecture. This architecture is employed to com-\npress our reference views to smaller resolution image fea-\ntures. We treat these image features as conditioning tokens,\neffectively representing each reference view. This configu-\nration has proven to be sufficient in our experiments, deliv-\nering superior results in generation quality while simultane-\nously maintaining high training efficiency.\n3.2. Camera Positional Encoding (CaPE)\nTo encode camera poses efficiently and accurately into ref-\nerence and target view tokens within a transformer archi-\ntecture, we introduce Camera Positional Encoding (CaPE),\ndrawing inspiration from recent advancements in the lan-\nguage domain. We first briefly examine the distinctions be-\ntween these two domains.\n\u2013 In language, token positions (associated with each\nword) follow a linear and discrete structure, and their length\ncan be infinite. Language models are typically trained with\nfixed maximum token counts (known as context length),\nand it remains an ongoing research challenge to construct\na positional encoding that enables the model to behave rea-\nsonably beyond this fixed context length [13, 35].\n\u2013 In 3D vision, token positions (associated with each\ncamera) follow a cyclic, continuous, and bounded struc-\nture for rotations and a linear, continuous, and unbounded\nstructure for translations. Importantly, unlike the language\ndomain where the token position always starts from zero,\nthere are no standardised absolute global camera poses in\n4\na 3D space. The relationship between two views depends\nsolely on their relative camera transformation.\nWe now present two distinct designs for spatial posi-\ntion encoding, representing camera poses using 4 DoF for\nobject-centric rendering and 6 DoF for the generic case, re-\nspectively. Our design strategy involves directly applying\na transformation on global camera poses embedded in the\ntoken feature, which allows the dot-product attention to di-\nrectly encode the relative camera transformation, indepen-\ndent of any coordinate system.\n4 DoF CaPE\nIn the case of 4 DoF camera poses, we adopt\na spherical coordinate system, similar to [21, 22], denoted\nas P = {\u03b1, \u03b2, \u03b3, r} including azimuth, elevation, camera\norientation along the look-at direction, and camera distance\n(radius), each position component is disentangled.\nMathematically, the position encoding function \u03c0(v, P),\ncharacterised by its d-dimensional token feature v \u2208 Rd\nand pose P, should satisfy the following conditions:\n\u27e8\u03c0(v1, \u03b81), \u03c0(v2, \u03b82)\u27e9 = \u27e8\u03c0(v1, \u03b81 \u2212 \u03b82), \u03c0(v2, 0)\u27e9, (2)\n\u27e8\u03c0(v1, r1), \u03c0(v2, r2)\u27e9 = \u27e8\u03c0(v1, r1/r2), \u03c0(v2, 1)\u27e9.\n(3)\nHere \u27e8\u00b7, \u00b7\u27e9 represents the dot product operation, \u03b81,2 \u2208\n{\u03b1, \u03b2, \u03b3}, within \u03b1, \u03b3 \u2208 [0, 2\u03c0), \u03b2 \u2208 [0, \u03c0), and r1,2 >\n0. Essentially, the relative 4 DoF camera transformation is\ndecomposed to the relative angle difference in rotation and\nthe relative scale difference in view radius.\nNotably, Eq. 2 aligns with the formula of rotary posi-\ntion encoding (RoPE) [46] derived in the language domain.\nGiven that log(r1) \u2212 log(r2) = log(s \u00b7 r1) \u2212 log(s \u00b7 r2) (for\nany scalar s > 0), we may elegantly combine both Eq. 2\nand Eq. 3 in a unified formulation using the design strat-\negy in RoPE by transforming feature vector v with a block\ndiagonal rotation matrix \u03d5(P) encoding P.\n\u2013 4 DoF CaPE: \u03c0(v, P) = \u03d5(P)v,\n\u03d5(P) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03a8\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03a8\n0\n...\n...\n0\n...\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n\u03a8\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03a8 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03a8\u03b1\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03a8\u03b2\n0\n...\n...\n0\n\u03a8\u03b3\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n\u03a8r\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(4)\nRotation: \u03a8\u03b8 =\n\u0014cos \u03b8\n\u2212 sin \u03b8\nsin \u03b8\ncos \u03b8\n\u0015\n,\n(5)\nView Radius: \u03a8r =\n\u0014cos(f(r))\n\u2212 sin(f(r))\nsin(f(r))\ncos(f(r))\n\u0015\n,\n(6)\nwhere f(r) = \u03c0\nlog r \u2212 log rmin\nlog rmax \u2212 log rmin\n\u2208 [0, \u03c0].\n(7)\nHere, dim(v) = d should be divisible by 2|P| = 8.\nNote, it\u2019s crucial to apply Eq. 7 to constrain log r within\nthe range of rotation [0, \u03c0], so we ensure the dot product\nmonotonically corresponds to its scale difference.\n6 DoF CaPE\nIn the case of 6 DoF camera poses, denoted\nas P = [ R t\n0 1 ] \u2208 SE(3), each position component is en-\ntangled, implying that we are not able to reformulate as a\nmulti-dimensional position as in 4 DoF camera poses.\nMathematically, the position encoding function \u03c0(v, P)\nshould now satisfy the following condition:\n\u27e8\u03c0(v1, P1), \u03c0(v2, P2)\u27e9 =\n\n\u03c0(v1, P\u22121\n2 P1), \u03c0(v2, I)\n\u000b\n.\n(8)\nLet\u2019s apply a similar strategy as used in 4 DoF CaPE,\nwhich increases the dimensionality of P \u2208 R4\u00d74 to \u03d5(P) \u2208\nRd\u00d7d by reconstructing it as a block diagonal matrix, with\neach diagonal element being P. Since \u03d5(P) also forms a\nreal Lie group, we may construct \u03c0(\u00b7, \u00b7) for a key and query\nusing the following equivalence:\n\u0000\u03d5(P\u22121\n2 P1) v1\n\u0001\u22ba (\u03d5(I)v2) =\n\u0000v\u22ba\n1\u03d5(P\u22ba\n1P\u2212\u22ba\n2 )\n\u0001\nv2\n(9)\n= (v\u22ba\n1 \u03d5(P\u22ba\n1))(\u03d5(P\u2212\u22ba\n2 )v2) = (\u03d5(P1)v1)\u22ba(\u03d5(P\u2212\u22ba\n2 )v2) (10)\n=\n\n\u03c0(v1, \u03d5(P1)), \u03c0(v2, \u03d5(P\u2212\u22ba\n2 ))\n\u000b\n.\n(11)\n\u2013 6 DoF CaPE: \u03c0(v, P) = \u03d5(P)v,\n\u03d5(P) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03a8\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03a8\n0\n...\n...\n0\n...\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n\u03a8\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03a8 =\n(\nP\nif key\nP\u2212\u22ba\nif query .\n(12)\nHere, dim(v) = d should be divisible by dim(P) = 4.\nSimilarly, we need to re-scale the translation t for each\nscene within a unit range for efficient model training.\nIn both 4 and 6 DoF CaPE implementation, we can effi-\nciently perform matrix multiplication by simply reshaping\nthe vector v to match the dimensions of \u03a8 (8 for 4 DoF,\n4 for 6 DoF), ensuring faster computation. The PyTorch\nimplementation is attached in Appendix A.\n4. Experiments\nTraining Datasets\nIn this work, we focus on object-\ncentric view synthesis, training our model on Objaverse-1.0\nwhich consists of 800K objects [7]. This setting allows us to\nfairly compare with all other 3D diffusion model baselines\ntrained on the same dataset. We adopt the same training data\nused in Zero-1-to-3 [21], which contains 12 randomly ren-\ndered views per object with randomised environment light-\ning. To ensure the data quality, we filter out empty rendered\nimages, which make up roughly 1% of the training data.\nWe trained and reported results using EscherNet with\nboth 4 DoF and 6 DoF CaPE. Our observations revealed\nthat 6 DoF CaPE exhibits a slightly improved performance,\nwhich we attribute to its more compressed representation\nspace. However, empirically, we found that 4 DoF CaPE\nyields visually more consistent results when applied to real-\nworld images. Considering that the training data is confined\nwithin a 4 DoF object-centric setting, we present EscherNet\nwith 4 DoF CaPE in the main paper. The results obtained\nwith 6 DoF CaPE are provided in Appendix C.\nIn all experiments, we re-evaluate the baseline models\nby using their officially open-sourced checkpoints on the\nsame set of reference views for a fair comparison. Further\ndetails on our experiment setting and training strategies are\nprovided in Appendix B.\n5\n4.1. Results on Novel View Synthesis\nWe evaluate EscherNet in novel view synthesis on the\nGoogle Scanned Objects dataset (GSO) [10] and the RTMV\ndataset [49], comparing with 3D diffusion models for view\nsynthesis, such as Zero-1-to-3 [21] and RealFusion [27]\n(primarily for generation quality with minimal reference\nviews). Additionally, we also evaluate on NeRF Synthetic\nDataset [29], comparing with state-of-the-art scene-specific\nneural rendering methods, such as InstantNGP [30] and 3D\nGaussian Splatting [18] (primarily for rendering accuracy\nwith multiple reference views).\nNotably, many other 3D diffusion models [20, 22, 23,\n43, 57] prioritise 3D generation rather than view synthesis.\nThis limitation confines them to predicting target views with\nfixed target poses, making them not directly comparable.\nCompared to 3D Diffusion Models\nIn Tab. 1 and Fig. 5,\nwe show that EscherNet significantly outperforms 3D dif-\nfusion baselines, by a large margin, both quantitatively and\nqualitatively. Particularly, we outperform Zero-1-to-3-XL\ndespite it being trained on \u00d710 more training data, and Re-\nalFusion despite it requiring expensive score distillation for\niterative scene optimisation [38]. It\u2019s worth highlighting\nthat Zero-1-to-3 by design is inherently limited to generat-\ning a single target view and cannot ensure self-consistency\nacross multiple target views, while EscherNet can generate\nmultiple consistent target views jointly and provides more\nprecise camera control.\nTraining\nData\n# Ref.\nViews\nGSO-30\nRTMV\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nRealFusion\n-\n1\n12.76\n0.758\n0.382\n-\n-\n-\nZero123\n800K\n1\n18.51\n0.856\n0.127\n10.16\n0.505\n0.418\nZero123-XL\n10M\n1\n18.93\n0.856\n0.124\n10.59\n0.520\n0.401\nEscherNet\n800k\n1\n20.24\n0.884\n0.095\n10.56\n0.518\n0.410\nEscherNet\n800k\n2\n22.91\n0.908\n0.064\n12.66\n0.585\n0.301\nEscherNet\n800k\n3\n24.09\n0.918\n0.052\n13.59\n0.611\n0.258\nEscherNet\n800k\n5\n25.09\n0.927\n0.043\n14.52\n0.633\n0.222\nEscherNet\n800k\n10\n25.90\n0.935\n0.036\n15.55\n0.657\n0.185\nTable 1. Novel view synthesis performance on GSO and RTMV\ndatasets.\nEscherNet outperforms Zero-1-to-3-XL with signifi-\ncantly less training data and RealFusion without extra SDS opti-\nmisation. Additionally, EscherNet\u2019s performance exhibits further\nimprovement with the inclusion of more reference views.\nCompared to Neural Rendering Methods\nIn Tab. 2 and\nFig. 4, we show that EscherNet again offers plausible view\nsynthesis in a zero-shot manner, without scene-specific op-\ntimisation required by both InstantNGP and 3D Gaussian\nSplatting. Notably, EscherNet leverages a generalised un-\nderstanding of objects acquired through large-scale train-\ning, allowing it to interpret given views both semantically\nand spatially, even when conditioned on a limited number\nof reference views. However, with an increase in the num-\nber of reference views, both InstantNGP and 3D Gaussian\n# Reference Views (Less \u2192 More)\n1\n2\n3\n5\n10\n20\n50\n100\nInstantNGP (Scene Specific Training)\nPSNR\u2191\n10.92\n12.42\n14.27\n18.17\n22.96\n24.99\n26.86\n27.30\nSSIM\u2191\n0.449\n0.521\n0.618\n0.761\n0.881\n0.917\n0.946\n0.953\nLPIPS\u2193\n0.627\n0.499\n0.391\n0.228\n0.091\n0.058\n0.034\n0.031\nGaussianSplatting (Scene Specific Training)\nPSNR\u2191\n9.44\n10.78\n12.87\n17.09\n23.04\n25.34\n26.98\n27.11\nSSIM\u2191\n0.391\n0.432\n0.546\n0.732\n0.876\n0.919\n0.942\n0.944\nLPIPS\u2193\n0.610\n0.541\n0.441\n0.243\n0.085\n0.054\n0.041\n0.041\nEscherNet (Zero Shot Inference)\nPSNR\u2191\n13.36\n14.95\n16.19\n17.16\n17.74\n17.91\n18.05\n18.15\nSSIM\u2191\n0.659\n0.700\n0.729\n0.748\n0.761\n0.765\n0.769\n0.771\nLPIPS\u2193\n0.291\n0.208\n0.161\n0.127\n0.114\n0.106\n0.099\n0.097\nTable 2. Novel view synthesis performance on NeRF Synthetic\ndataset. EscherNet outperforms both InstantNGP and Gaussian\nSplatting when provided with fewer than five reference views\nwhile requiring no scene-specific optimisation. However, as the\nnumber of reference views increases, both methods show a more\nsignificant improvement in rendering quality.\nSplatting exhibit a significant improvement in the render-\ning quality. To achieve a photo-realistic neural rendering\nwhile retaining the advantages of a generative formulation\nremains an important research challenge.\n# Reference Views (Less \u2192 More)\n1\n2\n3\n5\n10\n20\nInstantNGP (Scene Specific Training)\nPSNR 10.37 PSNR 11.72 PSNR 12.82 PSNR 15.58 PSNR 19.71 PSNR 21.28\n3D Gaussian Splatting (Scene Specific Training)\nPSNR 9.14\nPSNR 10.63 PSNR 11.43 PSNR 14.81 PSNR 20.15 PSNR 22.88\nEscherNet (Zero Shot Inference)\nPSNR 10.10 PSNR 13.25 PSNR 13.43 PSNR 14.33 PSNR 14.97 PSNR 15.65\nFigure 4. Generated views visualisation on the NeRF Synthetic\ndrum scene. EscherNet generates plausible view synthesis even\nwhen provided with very limited reference views, while neural\nrendering methods fail to generate any meaningful content. How-\never, when we have more than 10 reference views, scene-specific\nmethods exhibit a substantial improvement in rendering quality.\nWe report the mean PSNR averaged across all test views from the\ndrum scene. Results for other scenes and/or with more reference\nviews are shown in Appendix D.\n6\nReference\nViews\n1 View 2 Views\n5 Views\n1 View 2 Views\n5 Views\n1 View 2 Views\n5 Views\n1 View 2 Views\n5 Views\nZero-1-to-3-XL\n[1 View]\nEscherNet\n[1 View]\nEscherNet\n[2 Views]\nEscherNet\n[5 Views]\nGround\nTruth\nFigure 5. Novel view synthesis visualisation on GSO and RTMV datasets. EscherNet outperforms Zero-1-to-3-XL, delivering superior\ngeneration quality and finer camera control. Notably, when conditioned with additional views, EscherNet exhibits an enhanced resemblance\nof the generated views to ground-truth textures, revealing more refined texture details such as in the backpack straps and turtle shell.\n4.2. Results on 3D Generation\nIn this section, we perform single/few-image 3D generation\non the GSO dataset. We compare with SoTA 3D gener-\nation baselines: Point-E [31] for direct point cloud genera-\ntion, Shape-E [17] for direct NeRF generation, DreamGaus-\nsian [17] for optimising 3D Gaussian [18] with SDS guid-\nance, One-2-3-45 [20] for decoding an SDF using multiple\nviews predicted from Zero-1-to-3, and SyncDreamer [22]\nfor fitting an SDF using NeuS [51] from 16 consistent fixed\ngenerated views. We additionally include NeuS trained on\nreference views for few-image 3D reconstruction baselines.\nGiven any reference views, EscherNet can generate mul-\ntiple 3D consistent views, allowing for the straightforward\nadoption with NeuS [51] for 3D reconstruction. We gen-\nerate 36 fixed views, varying the azimuth from 0\u25e6 to 360\u25e6\nwith a rendering every 30\u25e6 at a set of elevations (-30\u25e6, 0\u25e6,\n30\u25e6), which serve as inputs for our NeuS reconstruction.\nResults\nIn Tab. 3 and Fig. 6, we show that EscherNet\nstands out by achieving significantly superior 3D recon-\nstruction quality compared to other image-to-3D generative\nmodels. Specifically, EscherNet demonstrates an approx-\nimate 25% improvement in Chamfer distance over Sync-\nDreamer, considered as the current best model, when con-\nditioned on a single reference view, and a 60% improvement\nwhen conditioned on 10 reference views. This impressive\nperformance is attributed to EscherNet\u2019s ability to flexibly\n# Ref. Views\nChamfer Dist. \u2193\nVolume IoU \u2191\nPoint-E\n1\n0.0447\n0.2503\nShape-E\n1\n0.0448\n0.3762\nOne2345\n1\n0.0632\n0.4209\nOne2345-XL\n1\n0.0667\n0.4016\nDreamGaussian\n1\n0.0605\n0.3757\nDreamGaussian-XL\n1\n0.0459\n0.4531\nSyncDreamer\n1\n0.0400\n0.5220\nNeuS\n3\n0.0366\n0.5352\nNeuS\n5\n0.0245\n0.6742\nNeuS\n10\n0.0195\n0.7264\nEscherNet\n1\n0.0314\n0.5974\nEscherNet\n2\n0.0215\n0.6868\nEscherNet\n3\n0.0190\n0.7189\nEscherNet\n5\n0.0175\n0.7423\nEscherNet\n10\n0.0167\n0.7478\nTable 3. 3D reconstruction performance on GSO. EscherNet\noutperforms all other image-to-3D baselines in generating more\nvisually appealing with accurate 3D geometry, particularly when\nconditioned on multiple reference views.\nhandle any number of reference and target views, providing\ncomprehensive and accurate constraints for 3D geometry.\nIn contrast, SyncDreamer faces challenges due to sensitivity\nto elevation angles and constraints imposed by a fixed 30\u25e6\nelevation angle by design, thus hindering learning a holistic\nrepresentation of complex objects. This limitation results in\ndegraded reconstruction, particularly evident in the lower\nregions of the generated geometry.\n4.3. Results on Text-to-3D Generation\nEscherNet\u2019s flexibility in accommodating any number of\nreference views enables a straightforward approach to the\n7\nReference\nOne-2-3-45-XL\nDreamGaussian-XL\nSyncDreamer\nEscherNet\nGround-Truth\nFigure 6. Single view 3D reconstruction visualisation on GSO. EscherNet\u2019s ability to generate dense and consistent novel views signif-\nicantly improves the reconstruction of complete and well-constrained 3D geometry. In contrast, One-2-3-45-XL and DreamGaussian-XL,\ndespite leveraging a significantly larger pre-trained model, tend to produce over-smoothed and noisy reconstructions; SyncDreamer, con-\nstrained by sparse fixed-view synthesis, struggles to tightly constrain geometry, particularly in areas in sofa and the bottom part of the bell.\nA bald eagle carved\nout of wood. \u21d2\nA robot made of vegetables, 4K. \u21d2\nFigure 7. Text-to-3D visualisation with MVDream (up) and\nSDXL (bottom). EscherNet offers compelling and realistic view\nsynthesis for synthetic images generated with user-provided text\nprompts. Additional results are shown in Appendix E.\ntext-to-3D generation problem by breaking it down into two\nstages: text-to-image, relying on any off-the-shelf text-to-\nimage generative model, and then image-to-3D, relying on\nEscherNet. In Fig. 7, we present visual results of dense\nnovel view generation using a text-to-4view model with\nMVDream [44] and a text-to-image model with SDXL [37].\nRemarkably, even when dealing with out-of-distribution\nand counterfactual content, EscherNet generates consistent\n3D novel views with appealing textures.\n5. Conclusions\nIn this paper, we have introduced EscherNet, a multi-view\nconditioned diffusion model designed for scalable view\nsynthesis. Leveraging Stable Diffusion\u2019s 2D architecture\nempowered by the innovative Camera Positional Embed-\nding (CaPE), EscherNet adeptly learns implicit 3D repre-\nsentations from varying number of reference views, achiev-\ning consistent 3D novel view synthesis. We provide detailed\ndiscussions and additional ablative analysis in Appendix F.\nLimitations and Discussions\nEscherNet\u2019s flexibility in\nhandling any number of reference views allows for au-\ntoregressive generation, similar to autoregressive language\nmodels [1, 5]. While this approach significantly reduces in-\nference time, it leads to a degraded generation quality. Ad-\nditionally, EscherNet\u2019s current capability operates within a\n3 DoF setting constrained by its training dataset, which may\nnot align with real-world scenarios, where views typically\nspan in SE(3) space. Future work will explore scaling Es-\ncherNet with 6 DoF training data with real-world scenes,\nstriving for a more general 3D representation.\n8\nAcknowledgement\nThe research presented here was supported by Dyson Tech-\nnology Ltd. Xin Kong holds a China Scholarship Council-\nImperial Scholarship. We would like to thank Sayak Paul\nand HuggingFace for contributing the training compute that\nfacilitated early project exploration. We would also like to\nacknowledge Yifei Ren for his valuable discussions on for-\nmulating the 6DoF CaPE.\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in Neural In-\nformation Processing Systems (NeurIPS), 2020. 8\n[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 2\n[3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n2022. 2\n[4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A\nunified approach to 3d generation and reconstruction. arXiv\npreprint arXiv:2304.06714, 2023. 3\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 8\n[6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 3, 17\n[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3, 5, 17\n[8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 3\n[9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised nerf: Fewer views and faster train-\ning for free. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 3\n[10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In Proceed-\nings of the IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2022. 6, 13\n[11] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 3\n[12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neu-\nral rendering at 200fps. In Proceedings of the International\nConference on Computer Vision (ICCV), 2021. 2\n[13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji,\nand Sinong Wang.\nLm-infinite: Simple on-the-fly length\ngeneralization for large language models.\narXiv preprint\narXiv:2308.16137, 2023. 4\n[14] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings\nof the International Conference on Computer Vision (ICCV),\n2021. 2\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems (NeurIPS), 2020. 3\n[16] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nIn Proceedings of the International Conference on Computer\nVision (ICCV), 2021. 3\n[17] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 3, 7, 13\n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(TOG), 2023. 2, 3, 6, 7, 13\n[19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 3\n[20] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 2, 3, 6, 7\n[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Proceedings of the In-\nternational Conference on Computer Vision (ICCV), 2023.\n3, 4, 5, 6, 13\n[22] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 3, 4, 5, 6, 7,\n13, 17\n[23] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\n9\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 2, 3, 6, 17\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In Proceedings of the International Confer-\nence on Learning Representations (ICLR), 2019. 13\n[25] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2\n[26] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn Proceedings of the IEEE/RSJ Conference on Intelligent\nRobots and Systems (IROS), 2015. 2\n[27] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 3, 6\n[28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 2\n[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020. 2, 6, 13\n[30] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 2022. 2, 3, 6, 13\n[31] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3, 7, 13\n[32] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020. 2\n[33] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n3\n[34] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove.\nDeepSDF: Learning\ncontinuous signed distance functions for shape representa-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019. 2\n[35] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico\nShippole. Yarn: Efficient context window extension of large\nlanguage models. arXiv preprint arXiv:2309.00071, 2023. 4\n[36] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger.\nConvolutional occupancy\nnetworks. In Proceedings of the European Conference on\nComputer Vision (ECCV), 2020. 2\n[37] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 8\n[38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 3, 6\n[39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2017.\n2\n[40] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in Neural Information\nProcessing Systems (NeurIPS), 2017. 2\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In Proceedings of the International Conference on\nMachine Learning (ICML), 2021. 4\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2022. 3\n[43] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n2, 3, 6\n[44] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 3, 8\n[45] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. Advances in\nNeural Information Processing Systems (NeurIPS), 2019. 2\n[46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu.\nRoformer: Enhanced trans-\nformer with rotary position embedding.\narXiv preprint\narXiv:2104.09864, 2021. 5\n[47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[48] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00a8uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields. In Proceedings of SIGGRAPH,\n2022. 2\n[49] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan\nKautz, Alexander Keller, Sameh Khamis, Thomas M\u00a8uller,\n10\nCharles Loop, Nathan Morrical, Koki Nagano, et al. Rtmv:\nA ray-traced multi-view synthetic dataset for novel view syn-\nthesis. arXiv preprint arXiv:2205.07058, 2022. 6, 13\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. 2\n[51] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 7\n[52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Image Process-\ning, 2004. 13\n[53] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 4\n[54] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 2\n[55] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 3\n[56] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-\nproving few-shot neural rendering with free frequency regu-\nlarization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3\n[57] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent image to 3d view syn-\nthesis via geometry-aware diffusion models. arXiv preprint\narXiv:2310.03020, 2023. 3, 6\n[58] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021. 3\n[59] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505\u20135514,\n2018. 2\n[60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 13\n11\nA. Python Implementation of CaPE\ndef compute_4dof_cape(v, P, s):\n\"\"\"\n:param v: input feature vector with its dimension must be divisible by 8\n:param P: list = [alpha, beta, gamma, r]\n:param s: a small scalar for radius\n:return: rotated v with its corresponding camera pose P\n\"\"\"\nv = v.reshape([-1, 8])\npsi = np.zeros([8, 8])\nfor i in range(4):\nif i < 3:\npsi[2 * i:2 * (i + 1), 2 * i:2 * (i + 1)] = \\\nnp.array([[np.cos(P[i]), -np.sin(P[i])], [np.sin(P[i]), np.cos(P[i])]])\nelse:\npsi[2 * i:2 * (i + 1), 2 * i:2 * (i + 1)] = \\\nnp.array([[np.cos(s * np.log(P[i])), -np.sin(s * np.log(P[i]))],\n[np.sin(s * np.log(P[i])), np.cos(s * np.log(P[i]))]])\nreturn v.dot(psi).reshape(-1)\nListing 1. Python implementation for 4 DoF CaPE.\ndef compute_6dof_cape(v, P, s=0.001, key=True):\n\"\"\"\n:param v: input feature vector with its dimension must be divisible by 4\n:param P: 4 x 4 SE3 matrix\n:param s: a small scalar for translation\n:return: rotated v with its corresponding camera pose P\n\"\"\"\nv = v.reshape([-1, 4])\nP[:3, 3] *= s\npsi = P if key else np.linalg.inv(P).T\nreturn v.dot(psi).reshape(-1)\nListing 2. Python implementation for 6 DoF CaPE.\n12\nB. Additional Training Details and Experimental Settings\nOptimisation and Implementation\nEscherNet is trained using the AdamW optimiser [24] with a learning rate of 1 \u00b7 10\u22124\nand weight decay of 0.01 for [256 \u00d7 256] resolution images. We incorporate cosine annealing, reducing the learning rate to\n1 \u00b7 10\u22125 over a total of 100,000 training steps, while linearly warming up for the initial 1000 steps. To speed up training, we\nimplement automatic mixed precision with a precision of bf16 and employ gradient checkpointing. Our training batches\nconsist of 3 reference views and 3 target views randomly sampled with replacement from 12 views for each object, with a\ntotal batch size of 672 (112 batches per GPU). The entire model training process takes 1 week on 6 NVIDIA A100 GPUs.\nMetrics\nFor 2D metrics used in view synthesis, we employ PSNR, SSIM [52], LPIPS [60]. For 3D metrics used in 3D\ngeneration, we employ Chamfer Distance and Volume IoU. To ensure a fair and efficient evaluation process, each baseline\nmethod and our approach are executed only once per scene per viewpoint. This practice has proven to provide stable averaged\nresults across multiple scenes and viewpoints.\nB.1. Evaluation Details\nIn NeRF Synthetic Dataset [29],\nwe consider and evaluate all 8 scenes provided in the original dataset. To assess per-\nformance with varying numbers of reference views, we train all baseline methods and our approach using the same set of\nviews randomly sampled from the training set. The evaluation is conducted on all target views defined in the test sets across\nall 8 scenes (with 200 views per scene). For InstantNGP [30], we run 10k steps (\u2248 1min) for each scene. For 3D Gaussian\nSplatting [18], we run 5k steps (\u2248 2min) for each scene.\nIn Google Scanned Dataset (GSO) [10],\nwe evaluate the same 30 objects chosen by SyncDreamer [22]. For each object,\nwe render 25 views with randomly generated camera poses and a randomly generated environment lighting condition to\nconstruct our test set. For each object, we choose the first 10 images as our reference views and the subsequent 15 images\nas our target views for evaluation. It\u2019s crucial to note that all reference and target views are rendered with random camera\nposes, establishing a more realistic and challenging evaluation setting compared to the evaluation setups employed in other\nbaselines: e.g. SyncDreamer uses an evenly distributed environment lighting to render all GSO data, and the reference view\nfor each object is manually selected based on human preference.1 Additionally, the evaluated target view is also manually\nselected based on human preference chosen among four independent generations.2\nIn evaluating 3D generation, we randomly sample 4096 points evenly distributed from the generated 3D mesh or point\ncloud across all methods. Each method\u2019s generated mesh is aligned to the ground-truth mesh using the camera pose of the\nreference views. Specifically in Point-E [31] and Shape-E [17], we rotate 90/180 degrees along each x/y/z axis to determine\nthe optimal alignment for the final mesh pose. Our evaluation approach again differs from SyncDreamer, which initially\nprojects the 3D mesh into their fixed 16 generated views to obtain depth maps. Then, points are sampled from these depth\nmaps for the final evaluation.3\nIn RTMV Dataset [49],\nwe follow the evaluation setting used in Zero-1-to-3 [21], which consists of 10 complex scenes\nfeaturing a pile of multiple objects from the GSO dataset. Similar to the construction of our GSO test set, we then randomly\nselect a fixed subset of the first 10 images as our reference views and the subsequent 10 views as our target views for\nevaluation.\n1https://github.com/liuyuan-pal/SyncDreamer/issues/21\n2https://github.com/liuyuan-pal/SyncDreamer/issues/21#issuecomment-1770345260\n3https://github.com/liuyuan-pal/SyncDreamer/issues/44\n13\nC. Additional Results on 6 DoF CaPE\nTo validate the effectiveness of the 6 DoF CaPE design, we demonstrate its performance in novel view synthesis on GSO and\nRTMV datasets in Tab. 4a and on the NeRF Synthetic dataset in Tab. 4c. We also provide 3D reconstruction results on GSO\ndataset in Tab. 4b. It is evident that EscherNet with 6 DoF CaPE achieves comparable, and often, slightly improved results\nwhen compared to our 4 DoF CaPE design.\nTraining\nData\n# Ref.\nViews\nGSO-30\nRTMV\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nRealFusion\n-\n1\n12.76\n0.758\n0.382\n-\n-\n-\nZero123\n800K\n1\n18.51\n0.856\n0.127\n10.16\n0.505\n0.418\nZero123-XL\n10M\n1\n18.93\n0.856\n0.124\n10.59\n0.520\n0.401\nEscherNet - 4 DoF\n800k\n1\n20.24\n0.884\n0.095\n10.56\n0.518\n0.410\nEscherNet - 4 DoF\n800k\n2\n22.91\n0.908\n0.064\n12.66\n0.585\n0.301\nEscherNet - 4 DoF\n800k\n3\n24.09\n0.918\n0.052\n13.59\n0.611\n0.258\nEscherNet - 4 DoF\n800k\n5\n25.09\n0.927\n0.043\n14.52\n0.633\n0.222\nEscherNet - 4 DoF\n800k\n10\n25.90\n0.935\n0.036\n15.55\n0.657\n0.185\nEscherNet - 6 DoF\n800k\n1\n20.89\n0.886\n0.093\n12.30\n0.569\n0.332\nEscherNet - 6 DoF\n800k\n2\n23.92\n0.917\n0.057\n14.18\n0.618\n0.252\nEscherNet - 6 DoF\n800k\n3\n25.21\n0.927\n0.045\n15.06\n0.643\n0.217\nEscherNet - 6 DoF\n800k\n5\n26.59\n0.937\n0.036\n15.71\n0.663\n0.190\nEscherNet - 6 DoF\n800k\n10\n27.75\n0.947\n0.030\n16.58\n0.688\n0.160\n(a) Novel view synthesis performance on GSO and RTMV datasets.\n# Ref. Views\nChamfer Dist. \u2193\nVolume IoU \u2191\nPoint-E\n1\n0.0447\n0.2503\nShape-E\n1\n0.0448\n0.3762\nOne2345\n1\n0.0632\n0.4209\nOne2345-XL\n1\n0.0667\n0.4016\nDreamGaussian\n1\n0.0605\n0.3757\nDreamGaussian-XL\n1\n0.0459\n0.4531\nSyncDreamer\n1\n0.0400\n0.5220\nNeuS\n3\n0.0366\n0.5352\nNeuS\n5\n0.0245\n0.6742\nNeuS\n10\n0.0195\n0.7264\nEscherNet - 4 DoF\n1\n0.0314\n0.5974\nEscherNet - 4 DoF\n2\n0.0215\n0.6868\nEscherNet - 4 DoF\n3\n0.0190\n0.7189\nEscherNet - 4 DoF\n5\n0.0175\n0.7423\nEscherNet - 4 DoF\n10\n0.0167\n0.7478\nEscherNet - 6 DoF\n1\n0.0274\n0.6382\nEscherNet - 6 DoF\n2\n0.0196\n0.7100\nEscherNet - 6 DoF\n3\n0.0180\n0.7348\nEscherNet - 6 DoF\n5\n0.0176\n0.7392\nEscherNet - 6 DoF\n10\n0.0160\n0.7628\n(b) 3D reconstruction performance on GSO.\n# Reference Views (Less \u2192 More)\n1\n2\n3\n5\n10\n20\n50\n100\nInstantNGP (Scene Specific Training)\nPSNR\u2191\n10.92\n12.42\n14.27\n18.17\n22.96\n24.99\n26.86\n27.30\nSSIM\u2191\n0.449\n0.521\n0.618\n0.761\n0.881\n0.917\n0.946\n0.953\nLPIPS\u2193\n0.627\n0.499\n0.391\n0.228\n0.091\n0.058\n0.034\n0.031\nGaussianSplatting (Scene Specific Training)\nPSNR\u2191\n9.44\n10.78\n12.87\n17.09\n23.04\n25.34\n26.98\n27.11\nSSIM\u2191\n0.391\n0.432\n0.546\n0.732\n0.876\n0.919\n0.942\n0.944\nLPIPS\u2193\n0.610\n0.541\n0.441\n0.243\n0.085\n0.054\n0.041\n0.041\nEscherNet - 4 DoF (Zero Shot Inference)\nPSNR\u2191\n13.36\n14.95\n16.19\n17.16\n17.74\n17.91\n18.05\n18.15\nSSIM\u2191\n0.659\n0.700\n0.729\n0.748\n0.761\n0.765\n0.769\n0.771\nLPIPS\u2193\n0.291\n0.208\n0.161\n0.127\n0.114\n0.106\n0.099\n0.097\nEscherNet - 6 DoF (Zero Shot Inference)\nPSNR\u2191\n13.73\n15.66\n16.91\n17.72\n18.47\n18.77\n19.24\n19.28\nSSIM\u2191\n0.664\n0.712\n0.745\n0.762\n0.779\n0.786\n0.795\n0.796\nLPIPS\u2193\n0.294\n0.197\n0.149\n0.120\n0.103\n0.095\n0.085\n0.084\n(c) Novel view synthesis performance on NeRF Synthetic dataset.\nTable 4. EscherNet 6 DoF presents a similar and sometimes improved performance than EscherNet 4 DoF.\n14\nD. Additional Results on NeRF Synthetic Dataset\nWe present additional visualisation on the NeRF Synthetic Dataset using EscherNet trained with 4 DoF CaPE.\n# Reference Views (Less \u2192 More)\n1\n2\n3\n5\n10\n20\n50\n100\nInstantNGP (Scene Specific Training)\nPSNR 9.45\nPSNR 11.41\nPSNR 13.64\nPSNR 19.30\nPSNR 23.14\nPSNR 26.18\nPSNR 28.54\nPSNR 28.87\nPSNR 10.37\nPSNR 11.72\nPSNR 12.82\nPSNR 15.58\nPSNR 19.71\nPSNR 21.28\nPSNR 23.09\nPSNR 23.78\n3D Gaussian Splatting (Scene Specific Training)\nPSNR 8.07\nPSNR 9.16\nPSNR 11.72\nPSNR 17.32\nPSNR 24.19\nPSNR 25.34\nPSNR 26.98\nPSNR 29.01\nPSNR 9.14\nPSNR 10.63\nPSNR 11.43\nPSNR 14.81\nPSNR 20.15\nPSNR 22.88\nPSNR 23.49\nPSNR 23.51\nEscherNet (Zero Shot Inference)\nPSNR 10.86\nPSNR 10.80\nPSNR 15.51\nPSNR 17.07\nPSNR 17.40\nPSNR 17.38\nPSNR 17.77\nPSNR 17.85\nPSNR 10.10\nPSNR 13.25\nPSNR 13.43\nPSNR 14.33\nPSNR 14.97\nPSNR 15.65\nPSNR 15.70\nPSNR 15.90\nTable 5. Novel View Synthesis on NeRF Synthetic Dataset. We report the average PSNR per scene, conditioned on the respective number\nof reference views.\n15\nE. Additional Results on Text-to-3D\nWe present additional visualisation on text-to-image-to-3D using EscherNet trained with 4 DoF CaPE.\nA robot made of vegetables.\nA nurse corgi.\nA cute steampunk elephant.\nA bull dog wearing a black pirate hat.\nAn astronaut riding a horse.\nMedieval House, grass, medieval,\nmedieval-decor, 3d asset.\nTable 6. Text-to-3D generation with SDXL (top 3) and MVDream (bottom 3).\n16\nF. Additional Discussions, Limitations and Future Work\nDirect v.s. Autoregressive Generation\nEscherNet\u2019s flexibility in handling arbitrary numbers of reference and target views\noffers multiple choices for view synthesis. In our experiments, we employ the straightforward direct generation to jointly\ngenerate all target views. Additionally, an alternative approach is autoregressive generation, where target views are generated\nsequentially, similar to text generation with autoregressive language models.\nFor generating a large number of target views, autoregressive generation can be significantly faster than direct generation\n(e.g. more than 20\u00d7 faster for generating 200 views). This efficiency gain arises from converting a quadratic inference cost\ninto a linear inference cost in each self-attention block. However, it\u2019s important to note that autoregressive generation may\nencounter a content drifting problem in our current design, where the generated quality gradually decreases as each newly\ngenerated view depends on previously non-perfect generated views. Autoregressive generation boasts many advantages in\nterms of inference efficiency and is well-suited for specific scenarios like SLAM (Simultaneous Localization and Mapping).\nAs such, enhancing rendering quality in such a setting represents an essential avenue for future research.\nStochasticity and Consistency in Multi-View Generation\nWe also observe that to enhance the target view synthesis\nquality, especially when conditioning on a limited number of reference views, introducing additional target views can be\nhighly beneficial. These supplementary target views can either be randomly defined or duplicates with the identical target\ncamera poses. Simultaneously generating multiple target views serves to implicitly reduce the inherent stochasticity in the\ndiffusion process, resulting in improved generation quality and consistency. Through empirical investigations, we determine\nthat the optimal configuration ensures a minimum of 15 target views, as highlighted in orange in Fig. 8. Beyond this threshold,\nany additional views yield marginal performance improvements.\n1 3 5\n10\n15\n20\n25\n30\n13\n15\n17\n19\n21\n23\n25\n17.35\n# Target Views\nPSNR\n(a) 1 Reference View\n1 3 5\n10\n15\n20\n25\n30\n13\n15\n17\n19\n21\n23\n25\n23.04\n# Target Views\nPSNR\n(b) 5 Reference Views\n1 3 5\n10\n15\n20\n25\n30\n13\n15\n17\n19\n21\n23\n25\n23.53\n# Target Views\nPSNR\n(c) 10 Reference Views\n1 3 5\n10\n15\n20\n25\n30\n13\n15\n17\n19\n21\n23\n25\n23.87\n# Target Views\nPSNR\n(d) 20 Reference Views\nFigure 8. Novel view synthesis with a different number of reference and target views. We present the averaged performance of\nEscherNet on one pre-selected target view across objects in the GSO dataset. We observe a clear improvement in view synthesis quality as\nthe number of both reference and target views increases. In this scenario, the multiple target views are essentially multiple duplicates of\nthe initially chosen single pre-selected view, a strategy we find effective in enhancing view synthesis quality.\nTraining Data Sampling Strategy\nWe have explored various combinations of N \u2208 {1, 2, 3, 4, 5} reference views and\nM \u2208 {1, 2, 3, 4, 5} target views during EscherNet training. Empirically, a larger number of views demand more GPU\nmemory and slow down training speed, while a smaller number of views may restrict the model\u2019s ability to learn multi-view\ncorrespondences. To balance training efficiency and performance, we set our training views to N = 3 reference views and\nM = 3 target views for each object, a configuration that has proven effective in practice. Additionally, we adopt a random\nsampling approach with replacement for these 6 views, introducing the possibility of repeated images in the training views.\nThis sampling strategy has demonstrated a slight improvement in performance compared to sampling without replacement.\nScaling with Multi-view Video\nEscherNet\u2019s flexibility sets it apart from other multi-view diffusion models [22, 23] that\nrequire a set of fixed-view rendered images from 3D datasets for training. EscherNet can efficiently construct training samples\nusing just a pair of posed images. While it can benefit from large-scale 3D datasets like [6, 7], EscherNet\u2019s adaptability\nextends to a broader range of posed image sources, including those directly derived from videos. Scaling EscherNet to\naccommodate multiple data sources is an important direction for future research.\n17\n"
  },
  {
    "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
    "link": "https://arxiv.org/pdf/2402.04236.pdf",
    "upvote": "5",
    "text": "CogCoM: Train Large Vision-Language Models Diving into Details through\nChain of Manipulations\nJi Qi 1 \u2021 Ming Ding 2 Weihan Wang 1 \u2021 Yushi Bai 1 \u2021 Qingsong Lv 2 Wenyi Hong 1 \u2021\nBin Xu 1 Lei Hou 1 Juanzi Li 1 Yuxiao Dong 1 Jie Tang 1\nqj20@mails.tsinghua.edu.cn, ming.ding@zhipuai.cn\nAbstract\nVision-Language Models (VLMs) have demon-\nstrated their widespread viability thanks to exten-\nsive training in aligning visual instructions to an-\nswers. However, this conclusive alignment leads\nmodels to ignore critical visual reasoning, and fur-\nther result in failures on meticulous visual prob-\nlems and unfaithful responses. In this paper, we\npropose Chain of Manipulations, a mechanism\nthat enables VLMs to solve problems with a se-\nries of manipulations, where each manipulation\nrefers to an operation on the visual input, either\nfrom intrinsic abilities (e.g., grounding) acquired\nthrough prior training or from imitating human-\nlike behaviors (e.g., zoom in). This mechanism\nencourages VLMs to generate faithful responses\nwith evidential visual reasoning, and permits users\nto trace error causes in the interpretable paths. We\nthus train CogCoM, a general 17B VLM with a\nmemory-based compatible architecture endowed\nthis reasoning mechanism. Experiments show\nthat our model achieves the state-of-the-art per-\nformance across 8 benchmarks from 3 categories,\nand a limited number of training steps with the\ndata swiftly gains a competitive performance. The\ncode and data are publicly available at this url.\n1. Introduction\nBenefiting from the advantage of Large Language Models\n(LLMs) in broad world knowledge, large Vision Language\nModels (VLMs) (Alayrac et al., 2022; Wang et al., 2023b)\nthat are further trained to understand vision have demon-\nstrated viabilities on broad scenarios, such as visual question\nanswering (Liu et al., 2023b), visual grounding (Peng et al.,\n2023), optical character recognition (Zhang et al., 2023b).\n1Tsinghua University 2Zhipu AI \u2021Done as intern at Zhipu AI.\nCorrespondence to: Bin Xu <xubin@tsinghua.edu.cn>, Jie Tang\n<jietang@tsinghua.edu.cn>.\nCogCOM\nVLM\nWhat is written on the pillar in front of the man \nin black top?\nGrounding(the man in black top) \nGrounding(pillar near the man at    )\nCropZoomIn(    , 4 times)\nFocus on    to answer the question\nFocus on     to answer the question\nThe letters written on the pillar are \nQUICK DEPOSIT.\nNO \nSMOKING\nQ:\nA:\n<latexit sha1_base64=\"k/+edjw63BcBkak5pVpf4fe8LfI=\">ACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE10OgQ4c+49Z+be68a\nBn3Jdfy8oK6tr6xvFzdLW9s7uXn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzj0R7ncKu3YExKFd0zazalmrumZVdOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVx\nc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit>b1\n<latexit sha1_base64=\"k/+edjw63BcBkak5pVpf4fe8LfI=\">ACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE10OgQ4c+49Z+be68a\nBn3Jdfy8oK6tr6xvFzdLW9s7uXn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzj0R7ncKu3YExKFd0zazalmrumZVdOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVx\nc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit>b1\n<latexit sha1_base64=\"k/+edjw63BcBkak5pVpf4fe8LfI=\">ACxnicjVHLToNAFD3FV62vqks3xMbEFQFLkO4a3XRZo30ktWmATispBQKDpmlM/AG3+mnGP9C/8M5IE10OgQ4c+49Z+be68a\nBn3Jdfy8oK6tr6xvFzdLW9s7uXn/oJ1GWeKxlhcFUdJ1nZQFfsha3OcB68YJc6ZuwDru5FLEO/csSf0ovOGzmPWnzj0R7ncKu3YExKFd0zazalmrumZVdOqEbCNmgCGpstVQb6aUfkNtxgigocMUzCE4IQDOEjp6cGAjpi4PubEJYR8GWd4RIm0GWUxynCIndB3TLtezoa0F56pVHt0SkBvQkoVJ6SJKC8hLE5TZTyTzoJd5j2XnuJuM/q7udeUWI47Yv/SLTL/qxO1cIxgyxp8qimWjKjOy10y2RVx\nc/VHVZwcYuIEHlI8IexJ5aLPqtSksnbRW0fGP2SmYMXey3MzfIpb0oAXU1SXg/aZliacWVW6hf5qIs4wjFOaZ7nqKOBJlrkPcYzXvCqNJRQyZSH71SlkGsO8WspT1+9YJBz</latexit>b1\n<latexit sha1_base64=\"YsAbQHKXhd0WG03Y0D8n2fbcl8=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHdJ+G+F/CW8ITPu1HMnanr847uZTxzj2PEy8MbsQs4v2pMw68kcQdS1OygPiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQ\njCkmIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgU2RYmR1LHNJVfkzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit>b2\n<latexit sha1_base64=\"YsAbQHKXhd0WG03Y0D8n2fbcl8=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a\n+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHdJ+G+F/CW8ITPu1HMnanr847uZTxzj2PEy8MbsQs4v2pMw68kcQdS1OygPiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQjCkmIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgU2RYmR1LHNJVfk\nzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit>b2\n<latexit sha1_base64=\"YsAbQHKXhd0WG03Y0D8n2fbcl8=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4alokFXZENywxyiNBQtphwIbSNu1UQ4iJP+BWP834B/oX3hlLogui07Q9c+49Z+be60a\n+lwjTfM9pK6tr6xv5zcLW9s7uXnH/oJ2Eacx4i4V+GHdJ+G+F/CW8ITPu1HMnanr847uZTxzj2PEy8MbsQs4v2pMw68kcQdS1OygPiXTKFds27J1ArZpViSonpm12pluGaZaJWSrGRbfcIshQjCkmIjgCDsw0FCTw8WTETE9TEnLibkqTjHIwqkTSmLU4ZD7IS+Y9r1MjagvfRMlJrRKT69MSl1nJAmpLyYsDxNV/FUOUt2mfdcecq7zejvZl5TYgXuiP1Lt8j8r07WIjBCVdXgU2RYmR1LHNJVfk\nzfUfVQlyiIiTeEjxmDBTykWfdaVJVO2yt46Kf6hMyco9y3JTfMpb0oAXU9SXg3bZsGzDuqU6hfZqPM4wjFOaZ7nqKOBJlrkPcYzXvCqNbRAS7WH71Qtl2kO8WtpT1+KqpBd</latexit>b2\n<latexit sha1_base64=\"C08P0sL86c4e7PRYQrhFA2bf210=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphATf8CtfprxD/QvDOWRBdEp2l759xzs\ny91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNgahWQrkaQf8MtBgjgIMEHD4ExR4sxPR0YIhJKyHGWERa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVg0\ns1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mdaWJVu+ytpfIfilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit>I1\n<latexit sha1_base64=\"C08P0sL86c4e7PRYQrhFA2bf210=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphATf8CtfprxD/QvDOWRBdEp2l759xzs\ny91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNgahWQrkaQf8MtBgjgIMEHD4ExR4sxPR0YIhJKyHGWERa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVg0\ns1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mdaWJVu+ytpfIfilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit>I1\n<latexit sha1_base64=\"C08P0sL86c4e7PRYQrhFA2bf210=\">ACxnicjVHLTsJAFD3UF+ILdemkZi4aqaKPHZEN7jDKI8ECWnLgA2lbdqphATf8CtfprxD/QvDOWRBdEp2l759xzs\ny91w49NxaMvWe0peWV1bXsem5jc2t7J7+714qDJHJ40wm8IOrYVsw91+dN4QqPd8KIWxPb4217fCHz7XsexW7g34hpyHsTa+S7Q9exBEHXl32zny8wo1Q0WfFUZ0axUqmeyaDKWLnCdNgahWQrkaQf8MtBgjgIMEHD4ExR4sxPR0YIhJKyHGWERa7KczwiR9qEWJwYFqFj+o5o101Rn/bSM1Zqh07x6I1IqeOINAHxIorlabrKJ8pZou8Z8pT3m1Kfzv1mhAqcEfoX7o58786WYvAEBVg0\ns1hQqR1TmpS6K6Im+u/6hKkENImIwHlI8odpRy3mdaWJVu+ytpfIfilRuXdSboJPeUsa8HyK+uKgdWKYJcO8KhZq5+moszjAIY5pnmXUEcDTfIe4RkveNXqmq8l2sM3Vcukmn38WtrTF1YekEc=</latexit>I1\n<latexit sha1_base64=\"Dmpt8GE6+k1qUxrNT+OzN28VIU=\">ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzN\nx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmUJYzXWREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXKOHCAwZRuAIQkH8JDS04YLBzFxHUyISwgJHe4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/85oT3W3Mf194zUiVuKG2L908z/6lQtEn2c6BoE1RrRlXHjEumu6Jubn+pSp\nJDTJzCPYonhJlWTvtsa02qa1e9XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit>I0\n<latexit sha1_base64=\"Dmpt8GE6+k1qUxrNT+OzN28VIU=\">ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzN\nx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmUJYzXWREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXKOHCAwZRuAIQkH8JDS04YLBzFxHUyISwgJHe4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/85oT3W3Mf194zUiVuKG2L908z/6lQtEn2c6BoE1RrRlXHjEumu6Jubn+pSp\nJDTJzCPYonhJlWTvtsa02qa1e9XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit>I0\n<latexit sha1_base64=\"Dmpt8GE6+k1qUxrNT+OzN28VIU=\">ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzN\nx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmUJYzXWREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJReyXHMOyNvEIq+YJ4k6vK863SLJafs6GXPAteAEsyqRcUXKOHCAwZRuAIQkH8JDS04YLBzFxHUyISwgJHe4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/85oT3W3Mf194zUiVuKG2L908z/6lQtEn2c6BoE1RrRlXHjEumu6Jubn+pSp\nJDTJzCPYonhJlWTvtsa02qa1e9XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Lashw+dlo/3</latexit>I0\nFigure 1. In comparison with existing vision-language models,\nCogCoM performs the multiple steps of evidential reasoning with\nchain of manipulations (CoM) to achieve the final answer.\nThe research employing VLMs as foundation models (Bai\net al., 2023; Sun et al., 2023b; Wang et al., 2023b) usually\ninvolves two main stages of training, where the first stage\ncultivates intrinsic visual understanding through exposure to\nmassive image-caption pairs, and the second stage endows\nthe models with problem-solving capabilities through an\ninstruction tuning. Some other studies (Dai et al., 2023;\nChen et al., 2023b; Zhang et al., 2023b) directly perform\nthe second stage for the applicable scenes.\nHowever, existing tuning methods train models to respond\nto instructions with conclusive linguistic answers upon vi-\nsual inputs, which leads models to ignore the essential visual\nreasoning and further results in failures in meticulous vi-\nsual problems, unfaithful responses, and even hallucinations.\nFor example in Figure 1, we test the top performing model\nCogVLM (Wang et al., 2023b) about the details in the image\n(i.e., texts written on pillar), and it directly gives an incor-\nrect answer (i.e., NO SMOKING), most likely from bias to\nvisual or linguistic priors (i.e., typical scenes with pillar in\noffice). The absence of this evidential reasoning with visual\nevidence leads to a rash response (Hwang et al., 2023).\nHumans solve the meticulous visual problems by marking\nor processing the given images for convenience and rigor,\n1\narXiv:2402.04236v1  [cs.CV]  6 Feb 2024\nwhich we refer to as manipulations. For example, we may\nfind targets by sequentially locating references, and concen-\ntrate on subtle details by zooming in a corresponding region.\nMost of the VLMs have developed numerous intrinsic multi-\nmodal capabilities (e.g., grounding boxes, recognizing texts)\nduring the first stage of training. By further imitating the\nfoundational human-like behaviours (e.g., cropping, zoom\nin), models have the potential to perform evidential visual\nreasoning for solving problems.\nThere are two major challenges that prevent us from training\nmodels with this mechanism. First, it is crucial to effectively\nproduce abundant training data with the evidential visual\nreasoning paths from existing linguistic instruction-answer\npairs. Second, it is difficult to build a general mechanism\nwith various manipulations, to train VLMs of dedicated\narchitectures while preserving their preset capabilities.\nIn this paper, we propose Chain of Manipulations (CoM),\na general mechanism that enables VLMs to perform ev-\nidential visual reasoning with a series of manipulations,\nwhere each manipulation is applied to the visual input to\nacquire different types of visual contents (e.g., boxes, texts,\nimages). We first construct an automatic data production\nframework based on existing image-question-answer cor-\npus. A linguistic annotator1 who is granted to utilize a set\nof manipulations is engaged to provide reasoning steps for\na given question, and fundamental visual tools are further\nemployed to acquire the corresponding returns requested by\nthe manipulations. A traversal on the tree branched by possi-\nble returns of the manipulations is then performed, to obtain\nthe final feasible paths that lead to the correct answer by the\nlast manipulation returns. Based on the produced data, we\npresent CogCoM, a 17B VLM trained with a memory-based\ncompatible architecture and a fusion of four categories of\ndata, to develop the general and reasoning multimodal capa-\nbilities. The model performs reasoning by actively adopting\nmultiple manipulations to acquire visual contents (e.g., refer-\nential regions bbx1, bbx2, and new image img1), and finally\nachieves the conclusive answer. In addition, due to the lack\nof evaluation resources, we further introduce a testbed with\nmeticulous visual problems involving reasoning paths, and\na keypoints-aware metric to investigate the correctness of\nthe final answer as well as the solving process.\nWe conduct extensive experiments on 8 benchmarks from 3\ncategories of capabilities, including detailed visual ques-\ntion answering with TextVQA (Singh et al., 2019), ST-\nVQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019),\nand GQA (Hudson & Manning, 2019), visual grounding\nwith RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al.,\n2016), and RefCOCOg (Mao et al., 2016), and the halluci-\nnation validation with POPE (Li et al., 2023d), and also a\n1A LLM prepended with task requirements is used as the anno-\ntator, who provides solving steps without knowing answers.\nproposed reasoning examination benchmark AutoCoM-test.\nThe results show that our model achieves the superior or\ncompetitive performance across the board. And the investi-\ngation on the proposed testbed shows that CogCoM swiftly\nachieves competitive performance with a few training steps\nby incorporating the produced reasoning chains.\n2. Methodology\nIn this section, we introduce the methodology of CogCoM.\nWe first introduce the terminology definition related to CoM\nin Section 2.1. In Section 2.2, we illustrate the data pro-\nduction process (Figure 2 left), which involves a linguistic\nannotator to provide manipulations-assisted solving steps,\nthe visual annotators to supply the manipulations returns,\nand the final traversal to gather viable paths. In Section 2.3,\nwe illustrate model training (Figure 2 right), where we con-\nduct a compatible devising to train a general multi-turns\nVLM with chain of manipulations.\n2.1. Terminology\nWe first introduce the formal definition of the terminolo-\ngies and data structure for an unambiguous understanding.\nWe define the manipulations as a flexible set that includes\nfunctions from a foundational predefined set, as well as\nfunctions devised by models themselves during inference\nto accommodate in-context learning. We thus predefine\nthe a set of manipulations functions that VLMs can de-\nvelop, either from prior training or by imitating human\nbehaviors: M \u2286{Grounding(tgt) \u2192 bbx, OCR(tgt) \u2192\ntxt, Calculate(tgt) \u2192 num, Counting(tgt) \u2192 num,\nCropZoomIn(bbx, x) \u2192 img}, where the parameters or\nreturns tgt, bbx, txt, num, x, img refer to the bounding\nboxes, zoom ratio, image, target description, number, and\ntext, respectively.\nGiven a language question Q upon an initial input image\nI0, a general vision-language model equipped with Chain\nof Manipulations (CoM) \u03c2 solves the problem to achieve\nthe corresponding answer as VLM(\u03c2|I0, Q) \u21d2 A, where \u03c2\nrefers to a chain of evidential reasoning steps2,\n\u03c2 = (step1, step2, ...)\nstepi = (fi, desci),\nfi \u2208 M\n(1)\nwhere fi refers to a instantiated manipulation function from\na set of manipulations definition M, and desci refers to\na language description including the execution of the ma-\nnipulation. This definition explicitly declares the symbolic\nexecution of the manipulations fi, and also compatible with\nthe existing free-form instruction-answer data structure with\nthe linguistic descriptions desci.\n2We use the symbol \u03c2 (i.e., sigma) to denote the CoM chain\nthroughout this paper for convenience.\n2\nLinguistic Annotator (LLM)\nGenerate solving steps for a given \nquestion, by optionally using chain of \nmanipulations from       or a newly \ncreated ones.\nStep1:                                             ,\nUsing Grounding(the pillar in front of the man \nat         ) to \ufb01nd .., the resultant region is         .\nUsing CropZoomIn(         , 4) to crop it on        , \nand zoom in it by 4 times to get new image     .\nUsing Grounding( the man in black shirt ) to \nlocate .., the position is at         .\nStep5:                                    ,\nStep2:                                             ,\nUsing OCR(     )  recognize the text written on \nthe image     , the result is  \u201cQUICK DEPOSIT\u201d.\nStep4:                                             ,\n<latexit sha1_base64=\"RWYa5PFtCXkZD\ntvP4QDF3c3RAQ=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3Uw\nTUIyUtx4w+41S8T/0D/wjtjCroOiHJnXPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8Xd\nvXYSprHWl7oh3HXdRLm84C1Bc+60Yxcyauzru7ZnMd+5YnPAwuBLTiPUnzjgI+45gqC\nW6z4MrEGxZBpWza6Uq7p1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEBQ7MNBQk8\nPFkxEhPUxIymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf\n5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBZnZe5pKor8ub6j6oEOU\nSEyXhI+ZhiTynfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4w\nCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bbx1\n<latexit sha1_base64=\"RWYa5PFtCXkZDtvP4QDF3c3RAQ=\">ACyHicjVHLSsNAFD2N\nr1pfVZdugkVwFZI+YpdFN+Kqgn1ALSVJp3UwTUIyUtx4w+41S8T/0D/wjtjCroOiHJnXPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHWl7oh3HXdRLm84C1Bc+60Yxcyau\nzru7ZnMd+5YnPAwuBLTiPUnzjgI+45gqCW6z4MrEGxZBpWza6Uq7p1GyrbFYoqFcrlbKtW4apVgnZaobFN1xjiBAeUkzAEBQ7MNBQk8PFkxEhPUxIymiKs8wyMKpE2JxYjhEHpL3zH\ntehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTynfdaVJlG1y946Kv+hmBKVey\n/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bbx1\n<latexit sha1_base64=\"HfMht1uhZfNRE3AH4G7DyqFOHtw=\">ACyHicjVHLSsNAFD3G\nV31XboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOuecmXuvlwQik4y9TxnTM7Nz86WFxaXldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2A\nX3i3hyp/cfTMTRmRwmvBO6g0j0he9Kgtqe9CtdsVZtnNusOYyaxq06ntORQ0mozV6ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6\nqQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9\nwcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ=</latexit>bbx2\n<latexit sha1_base64=\"3BVUemwIkNR4\nisyNX94YODCAX+0=\">ACzHicjVHLTsMwEBzCq5RXgSOXiAqJU0ha+uCG4AIXBJt\nQVBVSWrAIi85DqKquPIDXOG7EH8Af8HapBIcKnCUZD07M/bueknAU2nb7xPG5NT0zG\nxhrji/sLi0XFpZbadxJnzW8uMgFuem7KAR6wluQzYeSKYG3oB63h3ByrfuWci5XF0\nJgcJ64buTcSvue9Kgi4876FX3T7qOb1S2bZ267VmtWLaltOoOpUmBdWG3ajtmI5l61\nVGvk7i0hu0EcMHxlCMESQFAdwkdJzCQc2EsK6GBImKOI6z/CImkzYjFiuITe0feG\ndpc5GtFeaZa7dMpAb2ClCY2SRMT1CsTjN1PtPOCh3nPdSe6m4D+nu5V0ioxC2hf+\nlGzP/qVC0S12jqGjVlGhEVefnLpnuirq5+aMqSQ4JYSruU15Q7GvlqM+m1qS6dtVbV\n+c/NFOhau/n3Ayf6pY04NEUzfFBu2I5dcs53Snv7ejLmAdG9ieTawh0OcoEXeIZ7\nxglfj2JDG0Hj8phoTuWYNv5bx9AUC1ZKC</latexit>bbx3/I1\nUsing Grounding(the text written on the pillar \nin         ) to \ufb01nd .., the resultant region is         .\nStep3:                                             ,\n<latexit sha1_base64=\"HfMht1uhZfNRE3AH4G7DyqFOHtw=\">ACyHicjVHLSsNAFD3G\nV31XboJFsFVmNQ2tbuiG3GlYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOuecmXuvlwQik4y9TxnTM7Nz86WFxaXldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2A\nX3i3hyp/cfTMTRmRwmvBO6g0j0he9Kgtqe9CtdsVZtnNusOYyaxq06ntORQ0mozV6ZtMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6\nqQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSHhDAV9yifUuxr5bjPptZkunbVW1fnPzRToWrvF9\nwcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ=</latexit>bbx2\n<latexit sha1_b\nase64=\"luFztvg4pkmUxMB01yKJs5\nvirE=\">ACznicjVHLSsNAFD2Nr/\nqunQTLIKrkoioy6Ibly3YB9Qik+m\n0DebFZFIopbj1B9zqZ4l/oH/hnTEF\ntYhOSHLm3HPuzL3XSwI/VY7zWrAWF\npeWV4qra+sbm1vbpZ3dZhpnkosGj4N\nYtj2WisCPREP5KhDtRAoWeoFoeXeX\nOt4aCZn6cXStxonohmwQ+X2fM0VU5\nyZkashZMKlPb0tlp+KYZc8DNwdl5K\nsWl15wgx5icGQIRBEQ7AkNLTgQs\nHCXFdTIiThHwTF5hijbwZqQpGLF39\nB3QrpOzEe1ztS4OZ0S0CvJaeOQPD\nHpJGF9m3imcms2d9yT0xOfbcx/b0\n8V0iswpDYv3wz5X9uhaFPs5NDT7V\nlBhGV8fzLJnpir65/aUqRkS4jTuUV\nwS5sY567NtPKmpXfeWmfibUWpW73m\nuzfCub0kDdn+Ocx40jyvuacWtn5Sr\nF/moi9jHAY5onmeo4go1NEzH/GEZ\n6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4\n=</latexit>Q\n<latexit sha1_base64=\"Lz9T70LZH4j8t/WotJtOLW80Cn4=\">\nACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvDOmoBbRCUnOnHvOnbn3ekngp8px\nXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxP\nJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3s\nkScmnSsT7NPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGhDiNexSXhLlxTvtsG09q\nate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe\n</latexit>A\n<latexit sha1_base64=\"Lz9T70LZH4j8t/WotJtOLW80Cn4=\">\nACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl147KCrYW2yGQ6rcG8mEwKpRS3/oBb/SzxD/QvDOmoBbRCUnOnHvOnbn3ekngp8px\nXgvW3PzC4lJxubSyura+Ud7caqZxJrlo8DiIZctjqQj8SDSUrwLRSqRgoReIa+/uXMevh0KmfhxdqVEiuiEbRH7f50wR1e6ETN1yFoxP\nJzflilN1zLJngZuDCvJVj8sv6KCHGBwZQghEUIQDMKT0tOHCQUJcF2PiJCHfxAUmKJE3I5UgBSP2jr4D2rVzNqK9zpkaN6dTAnolOW3s\nkScmnSsT7NPDOZNftb7rHJqe82or+X5wqJVbgl9i/fVPlfn65FoY8TU4NPNSWG0dXxPEtmuqJvbn+pSlGhDiNexSXhLlxTvtsG09q\nate9ZSb+ZpSa1XueazO861vSgN2f45wFzYOqe1R1Lw8rtbN81EXsYBf7NM9j1HCBOhqm494wrNVt4bWxLr/lFqF3LONb8t6+ABoMpPe\n</latexit>A\n<latexit sha1_base64=\"luFztvg4pkmUxMB01yKJs5virE=\">ACznicjVHLSsNAFD2Nr/qunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B\n9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpeWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIRBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQpGLF39B3QrpOzEe1ztS\n4OZ0S0CvJaeOQPDHpJGF9m3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X9uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q\n<latexit sha1_base64=\"MGZVLUPfO1JBj5ou6+m0nbxL/Ig=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTaR2aJmEyUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVt\nfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHfj0RPMF8R5QXBXcftlCtO1THLngZuDirIVz0uv+AKXcRgyDAERwRFOISPlJ5LuHCQENfGmDhJSJg4xz1KpM0oi1OGT+yAvn3aXeZsRHvtmRo1o1NCeiUpbeyQJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVron9SzfJ/K9O16LQw5GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeWkz7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xToPGXtU9qLrn+5XacT7qIrawjV2a5yFqOEUdHn\nkLPOIJz9aZlVi31ugz1Srkmk18W9bDBy/ZkP8=</latexit>bbx1\n<latexit sha1_base64=\"hZG6VHgd7RXWpU+4TsdWqZIsxY=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIVd\nVl0I64qmLZQS0m0zo0L5KJWobf8Ctfpn4B/oX3hlTUIvohCRnzr3nzNx7vdgXqbSs14IxMzs3v1BcLC0tr6yuldc3GmUJYw7LPKjpOW5KfdFyB0pM9bcLdwPN50xueqnjzhiepiMJLOYp5J3AHoegL5kqiHM+76+53yxWraulT\ngM7BxXkqx6VX3CFHiIwZAjAEUIS9uEipacNGxZi4joYE5cQEjrOcY8SaTPK4pThEjuk74B27ZwNa8U61mdIpPb0JKEzukiSgvIaxOM3U8086K/c17rD3V3Ub093KvgFiJa2L/0k0y/6tTtUj0caxrEFRTrBlVHctdMt0VdXPzS1WSHG\nLiFO5RPCHMtHLSZ1NrUl276q2r4286U7Fqz/LcDO/qljRg+c4p0Fjr2ofVu2Lg0rtJB91EVvYxi7N8wg1nKEOh7wFHvGEZ+PciI1bY/SZahRyzSa+LePhAzSZkQE=</latexit>bbx3\n<latexit sha1_base64=\"QRAYKeCs4EYLlqr7Zz96\n9VaQGEc=\">ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/\n0L74xTUIvohCRnzr3nzNx7/TgQqXSc15w1N7+wuJRfLqysrq1vFDe3GmUJYzXWREScv3Uh6IkNelkAFvxQ\nn3Rn7Am/7wTMWbtzxJReyXHMOyNvEIq+YJ4k6vK863aLJafs6GXPAteAEsyqRcUXKOHCAwZRuAIQkH8\nJDS04YLBzFxHUyISwgJHe4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkNLGHmkiyksIq9NsHc+0s2J/8\n5oT3W3Mf194zUiVuKG2L908z/6lQtEn2c6BoE1RrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvtsa02q\na1e9XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7\nz1QrZzTb+Lashw+f9o/4</latexit>I1\n<latexit sha1_base64=\"3uQ8QX2fue98Q2WsirfBsNLdvpE=\">ACyHicjVHLSsNAFD2Nr1pfVZdu\ngkVwVRIRdVl0I64qmLagpSTaR2aJmEyUtx4w+41S8T/0D/wjtjCmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgRMebN1ymIo4u1Cjh7aHf\nj0RPMF8R5ak71XE75YpTdcyp4GbgwryVY/L7hCFzEYMgzBEUERDuEjpecSLhwkxLUxJk4SEibOcY8SaTPK4pThEzugb592lzkb0V57pkbN6JSQXklKGzukiSlPEtan2SaeGWfN/uY9Np76biP6B7nXkFiFa2L/\n0k0y/6vTtSj0cGRqEFRTYhdHctdMtMVfXP7S1WKHBLiNO5SXBJmRjnps20qald9Y38TeTqVm9Z3luhnd9Sxqw+3Oc06CxV3UPqu75fqV2nI+6iC1sY5fmeYgaTlGHR94Cj3jCs3VmJdatNfpMtQq5ZhPflvXw\nAYXbkSM=</latexit>txt1\n<latexit sha1_base64=\"wf9aHC2+VJkHDfvYU/0ZxLHsvQ4=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6\nrLoRlxVMG2hlpJMp3VoXiQTtRQ3/oBb/TLxD/QvDOmoBbRCUnOnHvPmbn3erEvUmlZrwVjbn5hcam4XFpZXVvfKG9uNdMoSxh3WORHSdtzU+6LkDtSJ+34S7gefzljc6VfHWDU9SEYWXchzbuAOQzEQzJVEOZ5316v1yhWraulz\ngI7BxXkqxGVX3CFPiIwZAjAEUIS9uEipacDGxZi4rqYEJcQEjrOcY8SaTPK4pThEjui75B2nZwNa8U61mdIpPb0JKE3ukiSgvIaxOM3U8086K/c17oj3V3cb093KvgFiJa2L/0k0z/6tTtUgMcKxrEFRTrBlVHctdMt0VdXPzS1WSHG\nLiFO5TPCHMtHLaZ1NrUl276q2r4286U7Fqz/LcDO/qljRg+c4Z0GzVrUPq/bFQaV+ko+6iB3sYp/meYQ6ztCAQ94Cj3jCs3FuxMatMf5MNQq5ZhvflvHwATI5kQA=</latexit>bbx2\nViT Encoder\n<latexit sha1_base64=\"BjPDv/IoisN4jPySDXrxkBnEZ\nqQ=\">ACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYGuhLTKZTmswLyaTQinFrT/gVj9L/AP9C+MKahFdEKSM\n+ec2fuvV4S+KlynNeCNTe/sLhUXC6trK6tb5Q3t5pnEkuGjwOYtnyWCoCPxIN5atAtBIpWOgF4tq7O9Px6GQqR9HV2\nqUiG7IBpHf9zlTRLU7IVO3nAXji8lNueJUHbPsWeDmoIJ81ePyCzroIQZHhACERThAwpPW24cJAQ18WYOEnIN3GBCUrk\nzUglSMGIvaPvgHbtnI1or3Omxs3plIBeSU4be+SJScJ69NsE89MZs3+lntscuq7jejv5blCYhVuif3LN1X+16drUejxN\nTgU02JYXR1PM+Sma7om9tfqlKUISFO4x7FJWFunNM+28aTmtp1b5mJvxmlZvWe59oM7/qWNGD35zhnQfOg6h5V3cvDSu0\n0H3URO9jFPs3zGDWco46G6fgjnvBs1a2hNbHuP6VWIfds49uyHj4AhL6T6g=</latexit>M\n+\n<latexit sha1_base64=\"mJeJclI8TdpbSWcC7W9vjITcNvc=\">AC0HicjVHLSsNAFD2Nr/qunQTLIKrkoioy6Ib3VWxD6ilTKbTNpiXyUQspYhbf8CtfpX4B\n/oX3hlTUIvohCRnzr3nzNx7nchzE2lZrzljanpmdi4/v7C4tLyWlhbryVhGnNR5aEXxg2HJcJzA1GVrvREI4oF8x1P1J2rYxWv34g4cPgQg4i0fJZL3C7LmeSqNalz2SfM294OmrLdqFolSy9zElgZ6CIbFXCwgsu0UEIjhQ+BAJIwh4YEnqasGEhIq6FIXExIVfHBUZYIG1KWYIyGLFX9O3RrpmxAe2VZ6LVnE7x6I1JaWKbNCHlxY\nTVaOp9pZsb95D7WnutuA/k7m5RMr0Sf2L9048786VYtEF4e6BpdqijSjquOZS6q7om5ufqlKkNEnMIdiseEuVaO+2xqTaJrV71lOv6mMxWr9jzLTfGubkDtn+OcxLUdkv2fsk+2yuWj7JR57GJLezQPA9QxgkqJL3NR7xhGfj3Lg17oz7z1Qjl2k28G0ZDx/Fb5TN</latexit>It\n<latexit sha1_base64=\"veN4EvEHuo6iohugj2nSrwf7lEM=\">AC0HicjVHLSsNAFD2Nr/qunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcR\nFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeiFcNhifDcQFSlKz3RiGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kqnXpM9nzBsejdqyXShaJUsvcxLYGSgiW5Ww8IJLdBCI4UPgQCSsAeGhJ4mbFiIiGthSFxMyNVxgREWSJtSlqAMRuwVfXu0a2ZsQHvlmWg1p1M8emNSmtgmTUh5MWF1mqnjqXZW7G/eQ+2p7jagv5N5+cRK9In9SzfO/K9O1SLRxaGuwaWaIs2o6njmkuquqJubX6qS5BARp3CH4j\nFhrpXjPptak+jaVW+Zjr/pTMWqPc9yU7yrW9KA7Z/jnAS13ZK9X7LP9orl42zUeWxiCzs0zwOUcYoKquR9jUc84dk4N26NO+P+M9XIZoNfFvGweyV5TF</latexit>At\n<latexit sha1_base64=\"0k0+D4mHvZwQZuyk7AOwscazgU=\">AC1nicjVHLSsNAFD3Gd32lunQTLIglEREXRbd6E7B1kItZTKONjQvkolSQt2JW3/ArX6S+Af6F94ZU1CL6IQkZ86958zce93\nY91Jp269jxvjE5NT0zGxpbn5hcksLzfSKEu4qPIj5Kmy1Lhe6GoS0/6ohkngWuL87c3oGKn12LJPWi8FT2Y9EO2FXoXqcSaI6Zvk8YLmZ8fDTq53HQGHbNiV29rFHgFKCYh1H5gvOcYEIHBkCISQhH0wpPS04MBGTFwbOXEJIU/HBQYokTajLEZjNgefa9o1yrYkPbKM9VqTqf49CaktLBOmojyEsLqNEvHM+2s2N+8c+2p7tanv1t4BcRKdIn9SzfM/K9O1SJxiT1dg0c1xZpR1fHCJdNdUTe\n3vlQlySEmTuELieEuVYO+2xpTaprV71lOv6mMxWr9rzIzfCubkDdn6OcxQ0tqrOTtU52a7U9otRz2AVa9ige6ihkMco07eN3jE56NpnFr3Bn3n6nGWKFZwbdlPHwALmSWeg=</latexit>It+1\n,\n<latexit sha1_base64=\"luFztvg4pkmUxMB01yKJs5virE=\">ACznicjVHLSsNAFD2Nr/qunQTLIKrkoioy6Ibly3YB9Qik+m0DebFZFIopbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XSwI/VY7zWrAWFpe\nWV4qra+sbm1vbpZ3dZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeXeXOt4aCZn6cXStxonohmwQ+X2fM0VU5yZkashZMKlPb0tlp+KYZc8DNwdl5KsWl15wgx5icGQIRBEQ7AkNLTgQsHCXFdTIiThHwTF5hijbwZqQpGLF39B3QrpOzEe1ztS4OZ0S0CvJaeOQPDHpJGF9m3imcms2d9yT0xOfbcx/b08V0iswpDYv3wz5X9uhaFPs5NDT7VlBhGV8fzLJnpir65/aUqRkS4jTuUVwS5sY567NtPKmpXfeWmfibUWpW73muzfCub\n0kDdn+Ocx40jyvuacWtn5SrF/moi9jHAY5onmeo4go1NEzH/GEZ6tmjaypdf8ptQq5Zw/flvXwAY5Ck+4=</latexit>Q\n<latexit sha1_base64=\"/T/YLaO9AY6UEpfHnTU/ygqt5M8=\">AC0HicjVHLSsNAFD2Nr\n/qunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeiFcNhifDcQFSlKz3RiGLBfMcTdef\nqRMXrNyJO3DC4kINItHzWC9yuy5kqnXpM9nzBsejdpWu1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEZjNgr+vZo18zYgPbKM9FqTqd49MakN\nLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWi0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe57lpnhXt6QB2z/HOQlquyV7v2\nSf7RXLx9mo89jEFnZongco4xQVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8Q15SB</latexit>A0\n<latexit sha1_base64=\"zM4q6wQW3lqPnlAeNIlhWc5UK4=\">AC0HicjVHLSsNAFD\n2Nr/qunQTLIKrkoioy6obl1XsA2opk+m0DeZlMhFLKeLWH3CrXyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ymp6ZncvPLywuLa+sFtbWa0mYxlxUeiFcNhifDcQFSlKz3Ri\nGLBfMcTdefqRMXrNyJO3DC4kINItHzWC9yuy5kqnXpM9nzBsejdp2u1C0SpZe5iSwM1BEtiph4QWX6CAERwofAgEkYQ8MCT1N2LAQEdfCkLiYkKvjAiMskDalLEZjNgr+vZo18z\nYgPbKM9FqTqd49MakNLFNmpDyYsLqNFPHU+2s2N+8h9pT3W1Afyfz8omV6BP7l26c+V+dqkWi0Ndg0s1RZpR1fHMJdVdUTc3v1QlySEiTuEOxWPCXCvHfTa1JtG1q94yHX/TmYpVe5\n7lpnhXt6QB2z/HOQlquyV7v2Sf7RXLx9mo89jEFnZongco4xQVMn7Go94wrNxbtwad8b9Z6qRyzQb+LaMhw8TN5SC</latexit>A1\n<latexit sha1_base64=\"KN0S1bKjKmsgCuZK2vWtgtEr8eA=\">ACznicjVHLSsNAFD3GV62vqks3wSK4KonYpu6KbtwIFawW2iKTcWyDeZFMCqUt/6A\nW/0s8Q/0L7wzTUEXRSckOXPuOXfm3uvGvpdKy/pYMBaXldWC2vF9Y3Nre3Szu5NGmUJFy0e+VHSdlkqfC8ULelJX7TjRLDA9cWt+3iu4rdDkaReF7LUSx6AeuH3oPHmSq0w2YHDmjy8nd6WyVak5du24bmrgVE+moHrqmHbF0quMfDWj0ju6uEcEjgwBEJIwj4YUno6sGEhJq6HMXEJIU/HBSYokjcjlSAFI/aRvn3\nadXI2pL3KmWo3p1N8ehNymjgkT0S6hLA6zdTxTGdW7LzcY51T3W1EfzfPFRArMSD2L9M+V+fqkXiAXVdg0c1xZpR1fE8S6a7om5u/qhKUoaYOIXvKZ4Q5to567OpPamuXfW6finVipW7XmuzfClbkDnk3RnA9ujit2rWJfnZQbZ/moC9jHAY5ong4auEATLd3xF7zizWgaQ2NiPE2lxkLu2cOvZTx/A1+/lEk=</late\nxit>M : Prede\ufb01ned Manipulations\n<latexit sha1_base64=\"BjPDv/IoisN4jPySDXrxkBnEZqQ=\">ACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYGuhLTKZTmswLyaTQinFrT/gVj9L/AP9C+MKahFdEKSM+ec2fuvV4S+KlynNeCN\nTe/sLhUXC6trK6tb5Q3t5pnEkuGjwOYtnyWCoCPxIN5atAtBIpWOgF4tq7O9Px6GQqR9HV2qUiG7IBpHf9zlTRLU7IVO3nAXji8lNueJUHbPsWeDmoIJ81ePyCzroIQZHhACERThAwpPW24cJAQ18WYOEnIN3GBCUrkzUglSMGIvaPvgHbtnI1or3Omxs3plIBeSU4be+SJScJ69NsE89MZs3+lntscuq7jejv5blCYhVuif3LN1X+16drUejxNTgU02JYXR1PM+Sma7om9tfqlKUISFO4x7FJWFunNM+28aTmtp1b5mJvxmlZvW\ne59oM7/qWNGD35zhnQfOg6h5V3cvDSu0H3URO9jFPs3zGDWco46G6fgjnvBs1a2hNbHuP6VWIfds49uyHj4AhL6T6g=</latexit>M\n<latexit sha1_b\nase64=\"/KcGymOf6Y2mVdHFOKgNt1v\nAXiY=\">AC0HicjVHLSsNAFD2Nr/\nqunQTLIKrkoioy6Ib3VWxD6ilTKbT\nNpiXyUQspYhbf8CtfpX4B/oX3hlTU\nIvohCRnzr3nzNx7nchzE2lZrzljanp\nmdi4/v7C4tLyWlhbryVhGnNR5aEX\nxg2HJcJzA1GVrvREI4oF8x1P1J2rYx\nWv34g4cPgQg4i0fJZL3C7LmeSqNa\nlz2SfM294Ompb7ULRKl6mZPAzkAR2\naqEhRdcoMQHCl8CASQhD0wJPQ0Yc\nNCRFwLQ+JiQq6OC4ywQNqUsgRlMGKv\n6NujXTNjA9orz0SrOZ3i0RuT0sQ2a\nULKiwmr0wdT7WzYn/zHmpPdbcB/Z3\nMydWok/sX7px5n91qhaJLg51DS7V\nFGlGVczl1R3Rd3c/FKVJIeIOIU7FI\n8Jc60c9nUmkTXrnrLdPxNZypW7Xm\nWm+Jd3ZIGbP8c5ySo7Zbs/ZJ9tlcsH\n2WjzmMTW9iheR6gjBNUCXvaziCc\n/GuXFr3Bn3n6lGLtNs4NsyHj4AI+\nUiQ=</latexit>I0\nn-th LM Block\nModel Training\nData Production\n<latexit sha1_base64=\"5GIlD1frWH3idT/Eewp1XutvIGM=\">AC7HicjVHLTtAFD0YaHm1DbBkMyKqoJvIdiLaJaILWIJEIFISRbYzOKM4Hms8bhNFfAI7dhVbfqDb8h2IP4C/4M7gSHSB2rFsnzn3njNz7\nw2zROTadR/mnPmFxXfvl5ZXVtc+fPxUWd84y2WhIt6MZCJVKwxynoiUN7XQCW9ligejMOHn4fC7iZ/4CoXMj3Vk4x3R0GcigsRBZqoXmWno/mYdNDJYu0L9L4clfH+gvrKBEPdKCU/MnCcNyrVN1ao+H79TqzoO7tvQDf85lXc+2qolzHsnKPDvqQiFBgBI4UmnCADk9bXhwkRHXxZQ4RUjYOMclVkhbUBanjIDYIX1j2rVLNqW98cytOqJTEnoVKRk+k0ZSniJsTmM2Xlhnw7lPbWe5m4T+oel14hYjQGx/9LNMv9XZ2rRuMA3W\n4OgmjLmOqi0qWwXTE3Z6+q0uSQEWdwn+KcGSVsz4zq8lt7a3gY0/2kzDmn1U5hZ4MrekAc+myN4GZ37N26t5J43q/kE56iVsYRu7NM+v2McRjtEk7yv8xh/cOalz7fxybl5SnblSs4m/lnP7DLpmoFE=</latexit>Grounding(tgt) ! bbx\n<latexit sha1_base64=\"8CKFL+3gEm0ubfp4YzCj6LvM/OE=\">AC5nicjVHLTsJAFD3UN75Ql26qxAQ3pBACLo1u3PmICAkQ0pYBJpS2mU59hLh258649Qfc6qcY/0D/wjtjSXRBdJq2Z86958zce53Q45G0rPeUMTU9Mz\ns3v5BeXFpeWc2srV9EQSxcVnUDLxB1x46Yx31WlVx6rB4KZg8dj9WcwaGK1y6ZiHjgn8ubkLWGds/nXe7akqh2Zqsp2TXpRseHZ7c52ZO7ZlPwXl/aQgRXpryW7UzWypeKpXKxZGpQsRJQJlDIW3plkayTIPOGJjoI4CLGEAw+JGEPNiJ6GijAQkhcCyPiBCGu4wy3SJM2pixGTaxA/r2aNdIWJ/2yjPSapdO8egVpDSxQ5qA8gRhdZqp47F2Vuwk75H2VHe7ob+TeA2JlegT+5dunPlfnapFos9XQOnmkLNqOrcxCXWXVE3N39UJckhJE7hDsUFYVcrx302t\nSbStave2jr+oTMVq/ZukhvjU92SBjyeojkZXBTzhXK+cFrK7h8ko57HJraRo3lWsI8jnKBK3nd4xgtejb5xbzwYj9+pRirRbODXMp6+AETlnXo=</latexit>OCR(tgt) ! txt\n<latexit sha1_base64=\"mgp/0Zlam0vN92e1OB21ULMLbRA=\">AC8HicjVHLTtAFD1xXzSUNm2X3YwaIVEVRTZ1mnSHYFN2IBGCShCynYkziu2xuM2KOIjuYtsf6LZ8BeIP4C+4MzhSWUTtWLbPnHvPmbn3hnkiCu26Vz\nXnwcNHj58sPa0vP1t5/qLx8tVBIUsV8V4kE6kOw6Dgich4Twud8MNc8SANE94PJ9sm3v/KVSFktq9Pc36cBnEmRiIKNFEnjfcDzaekm20rmX+RMt3JztbCcLrOpu/YQIl4rAOl5Dcm0vik0XRbvtf1P/nMgnbnwx3wO23mtVy7mqjWrmxcYoAhJCKUSMGRQRNOEKCg5wgeXOTEHWNGnCIkbJzjDHXSlpTFKSMgdkLfmHZHFZvR3ngWVh3RKQm9ipQMq6SRlKcIm9OYjZfW2bCLvGfW09ztlP5h5ZUSqzEm9l+6eb/6kwtGiN0bQ2CasotY6qLKpfSdsXcnP1VlSaHnDiD\nhxRXhCOrnPeZWU1haze9DWz82mYa1uyjKrfEjbklDXg+RbYHGy0vI8tb89vbm5Vo17CG7zFGs2zg018xi565P0dv/EHF45yfjg/nfO7VKdWaV7j3nJ+3QIb16GQ</latexit>CropZoomIn(bbx, x) ! img\n<latexit sha1_base64=\"glNiHPts8Mly1mjsEsAIuQV0Gis=\">AC63icjVHLTsMwEBzCq7wLHLlYVC4VClqC9wquHAEiQISRSgJlgkceQ4PFTxB9y4Ia78AFf4D8QfwF+wNqkEBwSOkqxnZ8beXT+NRKZd9\n63P6R8YHBoujYyOjU9MTpWnZ/YymauAtwMZSXgexmPRMLbWuiIH6SKe7Ef8X3/fNPk9y+4yoRMdvV1yo9iL0zEqQg8TdBxebGj+RXpupsyT7RIwpslHepl1lEiPNOeUvKSJXl8XK641Xqzsd5cYyZouo0G+0Lq6xWde2qoFjbsvyKDk4gESBHDI4EmuIHjJ6DlGDi5SwI3QJUxQJm+e4wShpc2JxYniEntM3pN1hgSa0N56ZVQd0SkSvIiXDAmk8RTF5jRm87l1Nuhv3l3rae52TX+/8IoJ1Tgj9C9dj/lfnalF4xRrtgZBNaUWMd\nUFhUtu2Juzr5VpckhJczEJ5RXFAdW2eszs5rM1m569n8u2Ua1OyDgpvjw9ySBtybIvs92Fup1prV2k690toRl3CHOaxRPNcRQtb2EabvG/xhGe8OLFz59w7D19Up6/QzOLHch4/ASwYoCM=</latexit>Counting(tgt) ! num\n<latexit sha1_base64=\"3MvpiuL8JcXNT4OrPGF91CDSxfU=\">ACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28IL\nnADQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzf\nomYxu7zG7aijXUVp9xSXaSMFRoA+BJriGCFyelrwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e9TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyAg\nzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDKbq/B2eB5295/nG9trtXjnoK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit>I1\n<latexit sha1_base64=\"3MvpiuL8JcXNT4OrPGF91CDSxfU=\">ACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28IL\nnADQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzf\nomYxu7zG7aijXUVp9xSXaSMFRoA+BJriGCFyelrwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e9TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyAg\nzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDKbq/B2eB5295/nG9trtXjnoK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit>I1\n<latexit sha1_base64=\"3MvpiuL8JcXNT4OrPGF91CDSxfU=\">ACxnicjVHLTsMwEBzCu7wKHLlEVEicIieUR28IL\nnADQGpIJS4brGaJpHjgCqExA9whU9D/AH8BWuTSnBA4CjJenZm7N2NsljmrG3EWd0bHxicmq6MjM7N79QXVw6y9NCcdHkaZyqiyjMRSwT0dRSx+IiUyLsR7E4j3r7Jn9+K1Qu0+RUDzJx1Q+7iexIHmqCTg6v/etqjXkBC+qNwGXeRiNgbIuCzf\nomYxu7zG7aijXUVp9xSXaSMFRoA+BJriGCFyelrwZARdoV7whRF0uYFHlAhbUEsQYyQ0B59u7RrlWhCe+OZWzWnU2J6FSldrJEmJZ6i2Jzm2nxhnQ36m/e9TR3G9A/Kr36hGrcEPqXbsj8r87UotHBjq1BUk2ZRUx1vHQpbFfMzd1vVWlyAg\nzcZvyimJulcM+u1aT29pNb0Obf7dMg5o9L7kFPswtacDKbq/B2eB5295/nG9trtXjnoK1jFOs1zG7s4wBGa5N3FE57x4hw4iVM4d19UZ6TULOPHch4/ASFJkDA=</latexit>I1\nVisual Annotators (Tools)\n<latexit sha1_base64=\"RWYa5PFtCXkZDtvP4QDF3c3RAQ=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kq\ngn1ALSVJp3UwTUIyUtx4w+41S8T/0D/wjtjCroOiHJnXPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHWl7oh3HXdRLm84C1Bc+60Yxcyauzru7ZnMd+5YnPAwuBLTiPUnzjgI+45gqCW6z4MrEGxZBpWza6Uq7p1GyrbFYoqFcrlbK\ntW4apVgnZaobFN1xjiBAeUkzAEBQ7MNBQk8PFkxEhPUxIymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTy\nnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bbx1\n<latexit sha1_base64=\"RWYa5PFtCXkZDtvP4QDF3c3RAQ=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZI+YpdFN+Kq\ngn1ALSVJp3UwTUIyUtx4w+41S8T/0D/wjtjCroOiHJnXPOTP3XjfyeSJM8z2nLS2vrK7l1wsbm1vbO8XdvXYSprHWl7oh3HXdRLm84C1Bc+60Yxcyauzru7ZnMd+5YnPAwuBLTiPUnzjgI+45gqCW6z4MrEGxZBpWza6Uq7p1GyrbFYoqFcrlbK\ntW4apVgnZaobFN1xjiBAeUkzAEBQ7MNBQk8PFkxEhPUxIymiKs8wyMKpE2JxYjhEHpL3zHtehka0F56Jkrt0Sk+vTEpdRyRJiReTLE8TVf5VDlLdJH3THnKu03p72ZeE0IFbgj9Szdn/lcnaxEYoa5q4FRTpBZnZe5pKor8ub6j6oEOUSEyXhI+ZhiTy\nnfdaVJlG1y946Kv+hmBKVey/jpviUt6QBz6eoLw7aZcOyDeuyWmqcZqPO4wCHOKZ5nqCBczTRIm+OZ7zgVbvQIu1em35TtVym2cevpT19AcNgkT8=</latexit>bbx1\n<latexit sha1_base64=\"HfMht1uhZfNRE3AH4G7DyqFOHtw=\">ACyHicjVHLSsNAFD3GV31XboJFsFVmNQ2tbuiG3Gl\nYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOuecmXuvlwQik4y9TxnTM7Nz86WFxaXldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/cfTMTRmRwmvBO6g0j0he9Kgtqe9CtdsVZtnNusOYyaxq06ntORQ0mozV6Z\ntMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSHhDAV9yifUuxr5b\njPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ=</latexit>bbx2\n<latexit sha1_base64=\"/s4AI4yOoSfQlmcVZM3q9QZdNDI=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK\n9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/Y5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVL\nWLcNUq4B0NYP8G64xRAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5\nTzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1oXavTb+pWibV7OPX0p6+AO+3kVI=</latexit>bbx3\n<latexit sha1_base64=\"HfMht1uhZfNRE3AH4G7DyqFOHtw=\">ACyHicjVHLSsNAFD3GV31XboJFsFVmNQ2tbuiG3Gl\nYFXQUpJ0WgfzIpmopbjxB9zql4l/oH/hnTEFXRSdkOTOuecmXuvlwQik4y9TxnTM7Nz86WFxaXldW18vrGeRbnqc/bfhzE6aXnZjwQEW9LIQN+maTcDb2AX3i3hyp/cfTMTRmRwmvBO6g0j0he9Kgtqe9CtdsVZtnNusOYyaxq06ntORQ0mozV6Z\ntMb0qKNZJXH7DNXqI4SNHCI4IkuIALjJ6rmCDISGsgxFhKUVC5zkesUjanFicGC6ht/Qd0O6qQCPaK89Mq306JaA3JaWJHdLExEspVqeZOp9rZ4VO8h5pT3W3If29wiskVOKG0L90Y+Z/daoWiT72dQ2Cako0oqrzC5dcd0Xd3PxRlSHhDAV9yifUuxr5b\njPptZkunbVW1fnPzRToWrvF9wcn+qWNODxFM3JwXnVsh3LPq1VWgfFqEvYwjZ2aZ4NtHCE7TJW+AZL3g1jo3EuDeG31RjqtBs4tcynr4A48ORTQ=</latexit>bbx2\n<latexit sha1_base64=\"/s4AI4yOoSfQlmcVZM3q9QZdNDI=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK\n9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/Y5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVL\nWLcNUq4B0NYP8G64xRAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5\nTzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1oXavTb+pWibV7OPX0p6+AO+3kVI=</latexit>bbx3\n<latexit sha1_base64=\"/s4AI4yOoSfQlmcVZM3q9QZdNDI=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwFZK+3RXdiKsK\n9gG1lCSd1sE0CclELcWNP+BWv0z8A/0L74wp6KLohCR3zj3nzNx7ndDjsTDN94y2tLyupZdz21sbm3v5Hf32nGQRC5ruYEXRF3HjpnHfdYSXHisG0bMnjge6zi3ZzLfuWNRzAP/SkxD1p/Y5+PuGsLglqO8zAoDfIF0zipVuqlom4aVq1kFesUlGpmrVL\nWLcNUq4B0NYP8G64xRAXCSZg8CEo9mAjpqcHCyZCwvqYERZRxFWe4RE50ibEYsSwCb2l75h2vRT1aS89Y6V26RSP3oiUOo5IExAvoliepqt8opwlush7pjzl3ab0d1KvCaECN4T+pZsz/6uTtQiMUFc1cKopVIiszk1dEtUVeXP9R1WCHELCZDykfESxq5\nTzPutKE6vaZW9tlf9QTInKvZtyE3zKW9KA51PUFwftomFVDeuyXGicpqPO4gCHOKZ51tDAOZpokTfHM17wql1oXavTb+pWibV7OPX0p6+AO+3kVI=</latexit>bbx3\n<latexit sha1_base64=\"2LswZlM15UTGqp2V+A6N+wNFqA=\">\nACznicjVHLTsJAFD3UF+ILdemkZi4Iq0x6pLoxiUmAiZATFsGmFA6zXSKIYS49Qfc6mcZ/0D/wjtjSVRidJq2Z849587ce/045I\nlynNectbC4tLySXy2srW9sbhW3d+qJSGXAaoEIhbzxvYSFPGI1xVXIbmLJvKEfsoY/uNDxojJhIvoWo1j1h56vYh3eApopotyXt95\nUkp7m6LJafsmGXPAzcDJWSrKovaKEDgQAphmCIoAiH8JDQ04QLBzFxbUyIk4S4iTNMUSBvSipGCo/YAX17tGtmbER7nTMx7oBOCem\nV5LRxQB5BOklYn2abeGoya/a3BOTU9tTH8/yzUkVqFP7F+mfK/Pl2LQhdnpgZONcWG0dUFWZbUdEXf3P5SlaIMXEadyguCQfGOe\nuzbTyJqV31jPxN6PUrN4HmTbFu74lDdj9Oc5UD8quydl9+q4VDnPRp3HvZxSPM8RQWXqKJmOv6IJzxbVWtkTa37T6mVyzy7+Las\nhw/pB5QU</latexit>!\n<latexit sha1_base64=\"QRAYKeCs4EYLlqr7Zz969VaQGEc=\"\n>ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0U3cV7QNqKcl0WoemSUgmSimCP+BWP038A/0L74xTUIvohCRnzr3nzNx7/TgQ\nqXSc15w1N7+wuJRfLqysrq1vFDe3GmUJYzXWREScv3Uh6IkNelkAFvxQn3Rn7Am/7wTMWbtzxJReyXHMOyNvEIq+YJ4k6vK86\n3aLJafs6GXPAteAEsyqRcUXKOHCAwZRuAIQkH8JDS04YLBzFxHUyISwgJHe4R4G0GWVxyvCIHdJ3QLu2YUPaK89UqxmdEtCbkN\nLGHmkiyksIq9NsHc+0s2J/85oT3W3Mf194zUiVuKG2L908z/6lQtEn2c6BoE1RrRlXHjEumu6Jubn+pSpJDTJzCPYonhJlWTvt\nsa02qa1e9XT8TWcqVu2Zyc3wrm5JA3Z/jnMWNA7K7lHZvTgsVU7NqPYwS72aZ7HqKCKGurkPcAjnvBsVa3Qyqy7z1QrZzTb+Las\nhw+f9o/4</latexit>I1\n:  What is written on the pillar in front of the man in black top?\n:  QUICK DEPOSIT\n<latexit sha1_base64=\"qIYQDeRMW7OoRDRjSVocIZPcYk=\">ACyHicjVHLTsJAFD3UF+ILdemkZi4IlMkIjuiG+MKExESJKQtA04obdNOVULc+ANu9cuMf6B/4Z2xGA0xOk3bM+fec2buvU7oiVgy9pox5uYXFpey7mV1bX1j\nfzm1mUcJHLG27gBVHLsWPuCZ83pJAeb4URt0eOx5vO8ETFmzc8ikXgX8hxyDsje+CLvnBtSVTDce6VjdfYMVqlbESMwlUyqx6YFpFptcXKCBd9SD/giv0EMBFghE4fEjCHmzE9LRhgSEkroMJcREhoeMc98iRNqEsThk2sUP6DmjXTlmf9soz1mqXTvHojUhpYo80AeVFhNVpo4n2lmxv3lPtKe625j+Tuo1Ilbimti/dNPM/+pULRJ9HOkaBNUakZV56Yuie6Kurn5rSpJDiFxCvcoHhF2tXLaZ1NrYl276q2t4286U7Fq76a5Cd7VLWnAM+OcBZelonVYtM7LhdpxOuosdr\nCLfZpnBTWco4GeQs84gnPxpkRGrfG+DPVyKSabfxYxsMHqviRNA=</latexit>bbx1\n+\n<latexit sha1_base64=\"SxfSvzv42IdpDU8dACsAC6r4gQ4=\">ACyHicjVHLTsJAFD3UF+ILdemkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5\nDe3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Ich73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AelqBPkXKGPA4SeODwIQm7sBDT0EJDCFxXUyIiwgJHe4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7\nvYp3lWUcpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ=</latexit>bbx2\n<latexit sha1_base64=\"SxfSvzv42IdpDU8dACsAC6r4gQ4=\">ACyHicjVHLTsJAFD3UF+ILdemkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5\nDe3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Ich73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AelqBPkXKGPA4SeODwIQm7sBDT0EJDCFxXUyIiwgJHe4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7\nvYp3lWUcpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ=</latexit>bbx2\n<latexit sha1_base64=\"SxfSvzv42IdpDU8dACsAC6r4gQ4=\">ACyHicjVHLTsJAFD3UF+ILdemkZi4IgMSkR3RjXGFiQgJEtKWASf0lXaqEuLGH3CrX2b8A/0L74zFaIjRadqeOfeM3PvtUNXxJKx14wxN7+wuJRdzq2srq1v5\nDe3LuMgiRzedAI3iNq2FXNX+LwphXR5O4y45dkub9mjExVv3fAoFoF/Ich73rW0BcD4ViSqKZt3/XKvXyBFWs1xsrMJFCtsNqBWSoyvb5AelqBPkXKGPA4SeODwIQm7sBDT0EJDCFxXUyIiwgJHe4R460CWVxyrCIHdF3SLtOyvq0V56xVjt0iktvREoTe6QJKC8irE4zdTzRzor9zXuiPdXdxvS3Uy+PWIlrYv/STP/q1O1SAxwpGsQVFOoGVWdk7okuivq5ua3qiQ5hMQp3Kd4RNjRymfTa2Jde2qt5aOv+lMxaq9k+YmeFe3pAHPjHMWXJaLpcNi6bxSqB+no85iB7\nvYp3lWUcpGmiSt8AjnvBsnBmhcWuMP1ONTKrZxo9lPHwArViRNQ=</latexit>bbx2\n<latexit sha1_base64=\"Xzwl2eLe8WruyuQkQHioGKvH10=\">AC3HicjVHLSsNAFD3GV31XbhwEyCIJRE1La7ohvdKVgt2FqSdFoH0yROJoVSunMnbv0Bt/o94h/oX3hnmoIuik5Icu\nbce87cO9eNfB5Ly/qYMCanpmdmM3PzC4tLyvZ1bXLOEyExype6Iei6jox83nAKpJLn1UjwZyO67Mr9+5Yxa+6TMQ8DC5kL2L1jtMOeIt7jiSqkd04bfTlrj2oBezerHUdEYTylgftRjZn5Q/sUtHaM4egVBqCQqFo2nlLrxzSdRZm31FDEyE8JOiAIYAk7MNBTM81bFiIiKujT5wgxHWcYB50iaUxSjDIfaOvm3aXadsQHvlGWu1R6f49ApSmtgmTUh5grA6zdTxRDsrdpx3X3uq2nr0d1OvDr\nESt8T+pRtl/lenepFoah74NRTpBnVnZe6JPpWVOXmj64kOUTEKdykuCDsaeXonk2tiXv6m4dHf/UmYpVey/NTfClqQBj6ZojgeXe3n7MG+f7+fKR+moM9jEFnZongWUcYIzVHT9L3jFm3FjPBiPxtMw1ZhINev4tYznb1M4mUs=</latexit>It+1 6= ?\n<latexit sha1_base64=\"3uQ8QX2fue98Q2WsirfBsNL\ndvpE=\">ACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I64qmLagpSTaR2aJmEyUtx4w+41S8T/0D/wjtjC\nmoRnZDkzLn3nJl7b5CEIlWO81qwZmbn5heKi6Wl5ZXVtfL6RiONM8m4x+Iwlq3AT3koIu4poULeSiT3h0HIm8HgR\nMebN1ymIo4u1Cjh7aHfj0RPMF8R5ak71XE75YpTdcyp4GbgwryVY/L7hCFzEYMgzBEUERDuEjpecSLhwkxLUxJ\nk4SEibOcY8SaTPK4pThEzugb592lzkb0V57pkbN6JSQXklKGzukiSlPEtan2SaeGWfN/uY9Np76biP6B7nXkFiFa\n2L/0k0y/6vTtSj0cGRqEFRTYhdHctdMtMVfXP7S1WKHBLiNO5SXBJmRjnps20qald9Y38TeTqVm9Z3luhnd9S\nxqw+3Oc06CxV3UPqu75fqV2nI+6iC1sY5fmeYgaTlGHR94Cj3jCs3VmJdatNfpMtQq5ZhPflvXwAYXbkSM=</lat\nexit>txt1\n\u2744\nEmbedding\nAdapter\nLanguage Weights\nVisual Weights\n<latexit sha1_base64=\"H9sLI9wGKyvcWKrO7gvtvNa/ao=\">AC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl047JF+4C2liSd1tC8mEzEUAri1h9wqz8l/oH+hXfGFNQiOiHJmXPvOTP3Xjvy3FgYxmtOm5tfWFzKLxdWV\ntfWN4qbW404TLjD6k7ohbxlWzHz3IDVhSs81o4s3zbY017dCbjzRvGYzcMLkUasa5vDQN34DqWIOqI9itGNcSxtNJT/SKJaNsqKXPAjMDJWSrGhZf0EfIRwk8MEQBD2YCGmpw0TBiLiuhgTxwm5Ks4wQYG0CWUxyrCIHdF3SLt2xga0l56xUjt0ikcvJ6WOPdKElMcJy9N0FU+Us2R/8x4rT3m3lP525uUTK3BN7F+6aeZ/dbIWgQFOVA0u1RQpRlbnZC6J6oq8uf6lKkEOEXES9ynOCTtKOe2zrjSxql321lLxN5UpWbl3stwE7/KWNGDz5zhnQeOgbB6VzdphqX\nKajTqPHexin+Z5jArOUWdvDke8YRn7UJLtTvt/jNVy2WabXxb2sMHp5qVjw=</latexit>Queryt\n<latexit sha1_base64=\"L60kNDzMRwZEVn/lSNyuTsFdZ9k=\">AC4XicjVHLSsNAFD2Nr1pfUZe6CBbBhZRERF0W3QhuKtgH1FKS6bSGpklIJmIp3bhzJ279Abf6M+If6F94Z5qCWkQnJDn3HvOzJ3rhJ4bC9N8y2hT0zOzc\n9n53MLi0vKvrpWiYMkYrzMAi+Iao4dc8/1eVm4wuO1MOJ2z/F41emeyHz1mkexG/gXoh/yRs/u+G7bZbYgqlvXgp+IwZnvD9sil1jFVsL+EUN/W8WTDVMiaBlYI80lUK9FdcoUADAl64PAhCHuwEdNThwUTIXENDIiLCLkqzFEjrQJVXGqsInt0rdDUT1lfYqlZ6zUjHbx6I1IaWCbNAHVRYTlbobKJ8pZsr95D5SnPFuf/k7q1SNW4IrYv3Tjyv/qZC8CbRypHlzqKVSM7I6lLom6FXly40tXghxC4iRuUT4izJRyfM+G0sSqd3m3tsq/q0rJypiltQk+5ClpwNb\nPcU6Cyl7BOihY5/v54nE6iw2sIUdmuchijhFCWXyvsUTnvGiMe1Ou9ceRqVaJtWs49vSHj8BSwCblA=</latexit>Keyt, Valuet\n<latexit sha1_base64=\"1AVZvMXUJX/idhDdIAMQnEbopY=\">AC5HicjVHLSsNAFD2Nr1pfVZcuDBbBRQmJiLosuhHcVLAPaEtJ0mkNzYtkIpbSpTt34tYfcKvfIv6B/oV3xhSqRXRCknPvefM3LlW6Dox1/W3jDIzOze/kF3\nMLS2vrK7l1zeqcZBENqvYgRtEdcuMmev4rMId7rJ6GDHTs1xWs/qnIl+7ZlHsBP4lH4Ss5Zk93+k6tsmJaue3m5zd8OE5G4zaelGdiIyiqmlaO1/QNV0udRoYKSgXeUg/4omOghgI4EHBh+csAsTMT0NGNAREtfCkLiIkCPzDCPkSJtQFaMKk9g+fXsUNVLWp1h4xlJt0y4uvREpVeySJqC6iLDYTZX5RDoL9jfvofQUZxvQ30q9PGI5roj9Szeu/K9O9MLRxbHswaGeQsmI7uzUJZG3Ik6uTnTFySEkTuAO5SPCtlSO71mVmlj2Lu7WlPl3WSlYEdtpbYIPcUoasPFznNO\nguq8Zh5pxcVAonaSjzmILO9ijeR6hDOUSHvWzhGS9KV7lT7pWHr1Ilk2o28W0pj5+boJst</latexit>Key0, Key1, ...\n<latexit sha1_base64=\"95TKWukBzyYQwOsTS9wREZuKnM=\">AC6HicjVHLSsNAFD2Nr1pfVZdugkVwUIioi6LblxWsA9oiyTptMZOk5BMxFL6Ae7ciVt/wK1+ifgH+hfeGVOwFtEJSe495wzc+c6IfdiYZpvGW1md\nm5+IbuYW1peWV3Lr29U4yCJXFZxAx5EdceOGfd8VhGe4KweRszuO5zVnN6JrNeuWR7gX8uBiFr9e2u73U81xYEXeQLTcFuxLBq84SNLsyiPpFbRd0wDGKZhqmWPh1YaVBAuspB/hVNtBHARYI+GHwIijlsxPQ0YMFESFgLQ8IijxVZxghR9qEWIwYNqE9+nYpa6SoT7n0jJXapV04vREpdeyQJiBeRLHcTVf1RDlL9DfvofKUZxvQ30m9+oQKXBL6l27M/K9O9iLQwZHqwaOeQoXI7tzUJVG3Ik+uf+tKkENImIzbVI8odpVyfM+60sSqd3m3tq/K6ZEZe6\nm3AQf8pQ0YOvnOKeD6p5hHRjW2X6hdJyOostbGOX5nmIEk5RoW8b/GEZ7xoV9qdq89fFG1TKrZxMTSHj8BWCec/Q=</latexit>Value0, Value1, ...\nSELF-ATTN\nMLP\nLanguage Weights\nVisual Weights\nLayer Norm\nn-th Memory Block\n\u2744\n\u2744\n\u2744\n\u2744\n<latexit sha1_base64=\"b2bQACR8spoFXUfIDRHakvG4W4\n4=\">AC73icjVHLSsNAFD3GV31XboJFqG6KImIuhRd6LKCVcGWkqTdDNxMnEB8V/cOdO3PoDbvUvxD/Qv/DOGMEHohO\nSnDn3njNz7/WTiKfKcZ7rP6BwaHhwsjo2PjE5FRxemY/FZkMWC0QkZCHvpeyiMespriK2GEimdf1I3bgH2/p+MEpkykX8\nZ6SFij64Uxb/PAU0Q1i0t1xc5J19uWIotbPA4vypUTXexLnYUZ6U4sz2/fOm2yWnIpjlv0TuDkoIV9VUXxCHS0IBMjQ\nBUMRTiCh5SeI7hwkBDXQI84SYibOMlRkmbURajDI/Y/qGtDvK2Zj2jM16oBOieiVpLSxQBpBeZKwPs028cw4a/Y3757\nx1He7oL+fe3WJVegQ+5fuI/O/Ol2LQhvrpgZONSWG0dUFuUtmuqJvbn+qSpFDQpzGLYpLwoFRfvTZNprU1K5765n4i8nUr\nN4HeW6GV31LGrD7fZw/wf5yxV2tuLsrpY3NfNQFzGEeZrnGjawgypq5H2Fezg0Tqxrq0b6/Y91erLNbP4sqy7N0MpoTo=\n</latexit>Grounding(tgt1) ! bbx1\n<latexit sha1_base64=\"H0Ooc4Z/ZMj6HIyBcoV6b4A+CU\nA=\">AC73icjVHLSsNAFD2N7/qunQTLIK6KEkRdVl0oUsFq4KVkqRjHEwzcTLRluI/uHMnbv0Bt/oX4h/oX3hnjOAD0Ql\nJzpx7z5m59/pJxFPlOM8Fq69/YHBoeKQ4OjY+MVmamt5LRSYDVg9EJOSB76Us4jGrK64idpBI5rX9iO37pxs6vn/OZMpFv\nKu6CTtqe2HMj3ngKaKapaWGYh3S9TalyOIWj8PLBRWqZnWxIXl4ojwpxYXt+51mtVkqOxXHLPsncHNQRr62RekJDbQgECBD\nGwxFOEIHlJ6DuHCQULcEXrESULcxBkuUSRtRlmMjxiT+kb0u4wZ2Pa8/UqAM6JaJXktLGPGkE5UnC+jTbxDPjrNnfvHv\nGU9+tS38/92oTq3BC7F+6j8z/6nQtCsdYMzVwqikxjK4uyF0y0xV9c/tTVYocEuI0blFcEg6M8qPtGkpnbdW8/EX0ymZ\nvU+yHMzvOpb0oDd7+P8CfaqFXel4u4sl2vr+aiHMYs5LNA8V1HDFrZRJ+8r3OMBj9aZdW3dWLfvqVYh18zgy7Lu3gBH+6E8\n</latexit>Grounding(tgt2) ! bbx2\n<latexit sha1_base64=\"61ZzdnoXO7upFU9SmdB0Ja42I2\no=\">AC73icjVHLSsNAFD2N73fVpZtgEdRFSayoS9GFLhVsK1gpSTrGwTQTJxMfFP/BnTtx6w+41b8Q/0D/wjtjCj4QnZD\nkzLn3nJl7r59EPFWO81Kwenr7+gcGh4ZHRsfGJ4qTU7VUZDJg1UBEQu7XsoiHrOq4ipi+4lkXtuPWN0/2dTx+hmTKRfxn\nrpM2GHbC2N+xANPEdUsLjYUuyBdZ0uKLG7xOLyaV6FqVhYakofHypNSnNu+f9GsNIslp+yYZf8Ebg5KyNeOKD6jgRYEAmRo\ngyGIhzBQ0rPAVw4SIg7RIc4SYibOMVhkmbURajDI/YE/qGtDvI2Zj2jM16oBOieiVpLQxRxpBeZKwPs028cw4a/Y3747\nx1He7pL+fe7WJVTgm9i9dN/O/Ol2LwhHWTA2cakoMo6sLcpfMdEXf3P5UlSKHhDiNWxSXhAOj7PbZNprU1K5765n4q8nUr\nN4HeW6GN31LGrD7fZw/QW2p7K6U3d3l0vpGPupBzGAW8zTPVaxjGzuokvc1HvCIJ+vUurFurbuPVKuQa6bxZVn370zNoT4=\n</latexit>Grounding(tgt3) ! bbx3\n<latexit sha1_base64=\"QA+RhXzVhkMBAd/4AoV0OYVQEa\ng=\">AC4nicjVHLSsNAFD2Nr/qOuhQhWAQFKYkWdSm60Z2CbcUHJUlHDU0yYTIRS+nKnTtx6w+41Y8R/0D/wjtjCj4QnZD\nkzLn3nJl7r5eEQSpt+6Vg9PUPDA4Vh0dGx8YnJs2p6VrKM+Gzqs9DLg49N2VhELOqDGTIDhPB3MgLWd1rbat4/ZKJNODxg\nWwn7DRyz+PgLPBdSVTDnDuR7Ip0nW3BkyPOo924u+h5V43VZauy1DBLdtnWy/oJnByUkK89bj7jBE1w+MgQgSGJBzCRUrP\nMRzYSIg7RYc4QSjQcYuRkibURajDJfYFn3PaXecszHtlWeq1T6dEtIrSGlhgTSc8gRhdZql45l2Vuxv3h3tqe7Wpr+Xe0X\nESlwQ+5eul/lfnapF4gwbuoaAako0o6rzc5dMd0Xd3PpUlSHhDiFmxQXhH2t7PXZ0pU1656+r4q85UrNr7eW6GN3VLG\nrDzfZw/QW2l7KyVnf1KaXMrH3URs5jHIs1zHZvYwR6q5H2NBziyWgaN8atcfeRahRyzQy+LOP+HamTmtk=</latexit>CropZoomIn(bbx3, 4)\n<latexit sha1_base64=\"76HXZux32y8Z7IeCXjMCYR2Nws\nc=\">AC6XicjVHLSsNAFD3GV31XboJVqFuSiKiLovduPOB1UIrJUmndWheTCZaKf6AO3fi1h9wqz8i/oH+hXfGCGoRnZD\nkzLn3nJl7rxv7PJGW9TJkDI+Mjo3nJianpmdm5/LzC8dJlAqPVb3Ij0TNdRLm85BVJZc+q8WCOYHrsxO3W1Hxk3MmEh6FR\n/IyZqeB0wl5m3uOJKqZX2lI1iNdf69yeFXkQadprzUE75xJR4jowpQ92bSb+YJVsvQyB4GdgQKytR/ln9FACxE8pAjAEIS\n9uEgoacOGxZi4k7RJ04Q4jrOcIVJ0qaUxSjDIbZL3w7t6hkb0l5Jlrt0Sk+vYKUJlZJE1GeIKxOM3U81c6K/c27rz3V3S7\np72ZeAbESZ8T+pfvM/K9O1SLRxraugVNsWZUdV7mkuquqJubX6qS5BATp3CL4oKwp5WfTa1JtG1q946Ov6qMxWr9l6Wm\n+JN3ZIGbP8c5yA4Xi/ZmyX7YKNQ3slGncMSlGkeW6hjF3so0re13jAI56MrnFj3Bp3H6nGUKZxLdl3L8DgiyeRw=</la\ntexit>OCR(img1) ! txt1\nFigure 2. The automatic data synthesis framework (left): a linguistic annotator (LLM) taught with task requirements and usage of\nmanipulations (prompt) is first asked to provide solving steps for a question Q, and the visual annotators (Tools) are then engaged to\nreplace the manipulations returns, followed by a final traversal on the tree branched by the possible returns is performed to find feasible\npaths terminating answer A. A compatible VLM architecture (right): a historic memory representation is maintained throughout around\nof multi-turns training, where t-th turn is evoked by an production of new image It.\n2.2. Data Production\nBased on the definition of data structure, we introduce a\ndata production framework, that could efficiently synthesize\nCoM data upon image-oriented question-answer pairs, and\nis also capable of producing high-quality annotations by\nreplacing the linguistic and visual annotators with human\nlabour. We describe the detailed data production approach\nwith the implementation for automatic data synthesis in this\npaper.\n2.2.1. DATA GENERATION\nGiven a general corpus D = {(I, Q, A)} consisting of\ntriplet samples of images and corresponding visual question-\nanswer pairs, our automatic data synthesis framework con-\nsists of a linguistic annotator and several visual annotators\naccording to the manipulations. For a question Q in each\nsample, we first engage the linguistic annotator to generate\nmanipulations-assisted solving steps with the CoM format\n(fi, desci), where the corresponding returns of the instanti-\nated manipulations executions are set with variables as place-\nholders. In this paper, we adopt GPT4 (OpenAI, 2023a),\na large language model with reliable language understand-\ning and generation abilities as the linguistic annotator. We\ndesign a comprehensive prompt including the task require-\nments, usage of manipulations, and output data format, and\nfurther manually annotate 5 demonstrations for a stable\ngeneration. The detailed implementations are available at\nAppendix B.3.\nWe then employ essential visual annotators to supply the re-\nturns of manipulations requested in the solving steps through\nexactly performing the corresponding manipulations. By\nempirically analyzing the manipulations from both prede-\nfined set and newly created ones (refers to Appendix B.2 for\na detailed statistics), we reveal the grounding and OCR are\ntwo fundamental manipulations, and most of the others can\nbe consequently derived (e.g., CropZoomIn along a region\nof box, Counting upon recognized boxes, and Calculate for\nthe identified formula). Therefore, we employ two reliable\nvisual tools, GroundingDINO (Liu et al., 2023c) and Pad-\ndleOCR (Du et al., 2020), and develop the implementations\nof these manipulations.\nWe execute the manipulations along with the reasoning steps,\nwhich turns the reasoning steps into a tree T , as the input of\ncurrent manipulation f1(x1) may rely on one of the multiple\nreturns of previous manipulation f2 \u2192 x2, i.e., x1 rely on\nx2 (e.g., step 2 for finding pillars in Figure 2). The designed\nprompt, and a corresponding generation with linguistic and\nvisual results are available at Appendix figure 6.\n3\n2.2.2. DATA PROCESSING\nThe tree T , that is rooted from the question Q and branched\nby the manipulations returns may contain negative paths\nthat can not lead to the correct answer A, in which the nodes\nmay come from error annotations or irrelevant references.\nWe then perform a traversal on each produced tree with the\nDepth First Search (DFS) to find all positive paths {Pi|Pi \u2208\nT , i = 1, 2, ...}, where each path Pi is terminated with the\nfinal answer A as the return of the last manipulation.\nThe operation of zoom in refers to an intellective human\nbehaviour, which could not be generated from the auto-\nmatic annotators appropriately and requires a cogitative\ndecision according to the visual scene. We leverage a sim-\nple but effective strategy to compensate the CropZoomIn\nmanipulation. During the traversal on tree T , at the node\nof manipulation f(x(bbx)) that involves a box bbx in the\ninput x, we first calculate the ratio r of the current im-\nage\u2019s area to the box\u2019s area, and then insert the manipu-\nlation CropZoomIn(bbx, n) in current step with the sat-\nisfiability condition (n is determined according to r, e.g.,\nn = 2 if r \u2265 36). Therefore, the final resultant chains\n{\u03c2i|\u03c2i := Pi, i = 1, 2, ...} are obtained by assigning the\ncorresponding values of reasoning steps with the positive\npaths. The overall data-producing algorithm is presented in\nAppendix B.1.\nTo adapt to the training of images-based multi-turns VLMs,\nwe can easily convert a steps-based chain to an images-\nbased chain by partitioning steps according to the manip-\nulations returns of images and merge the corresponding\ndescriptions (e.g., based on the images of I0, I1, the chain\nis converted into \u03c2 \u2192 (A0, A1) in Figure 2),\n\u03c2 \u2192 [(I0, Q), (I1, A1), (I2, A2), ...]\n(2)\nwhere It is the manipulation return from the t \u2212 1 step,\nand At is the sequential concatenation of elements in\n(desci(t\u22121), .., desci(t)] with the index mapping from t to i.\nWe implement this proposed data production framework\non 3 existing datasets that require meticulous recognition\nand object counting, TextVQA (Singh et al., 2019), ST-\nVQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019),\nto build 70K CoM chains. The detailed statistics of the data\ngeneration are available at Appendix B.2.\n2.3. Model Training\n2.3.1. ARCHITECTURE\nWe use the same model architecture as CogVLM (Wang\net al., 2023b), a general VLM approach that involves four\nfundamental components: (1) a Vision Encoder, (2) an MLP\nAdapter, (3) an LLM Backbone, and (4) a Visual Expert\nModule, for a reliable multimodal understanding. Con-\ncretely, the pre-trained EVA2-CLIP-E (Sun et al., 2023a)\nwith 4B parameters and Vicuna-7B-v1.5 (Chiang et al.,\n2023) are adopted as the vision encoder and LLM back-\nbone, respectively. A two-layer MLP (SwiGLU (Shazeer,\n2020)) is further engaged to map the output of the vision\nencoder into the linguistic space of the LLM backbone. The\nvisual expert module adds the vision-specific weights into\nthe attention layer and feed-forward layer of each block in\nthe LLM backbone, resulting in a total of 6.5B additional\nparameters for the deep fusion of modalities.\nBased on this general architecture, we develop a memory-\nbased multi-turns multi-images VLM approach. Specif-\nically, for a round of images-based multi-turns sample\n[(It, Qt, At)|t = 1, 2, ...], we keep the accumulated KV\nmemories of each layer of the LLM backbone through this\naround. And at each turn t for training and inference, we\ncalculate the attention function att as:\natt(X) = softmax(QtK\u2032T\nt\n\u221a\nd\n)V \u2032\nt\nK\u2032\nt = trunc(concat(K0, K1, ..., Kt))\nV \u2032\nt = trunc(concat(V 0, V 1, ..., V t))\n(3)\nwhere Qt \u2208 Rs\u00d7d is query representation of current layer,\nand the K\u2032\nt, V \u2032\nt \u2208 R(s\u00d7t)\u00d7d refer to the concatenation of\naccumulated representations and will be further truncated if\nthe sequence length s\u00d7t greater than a predefined threshold.\nAt the turn t > 0, the question Qt is sampled from a set\nof hard prompts (asking model to focus on history), and\nthe image It is cropped from It\u22121 and is amplified with the\nBicubic Interpolation (Keys, 1981).\n2.3.2. TRAINING\nThe proposed CogCoM-17B relies on two main stages of\ntraining, to develop the capabilities of general multimodal\ntask-solving as well as the evidential visual reasoning.\nFirst Stage of Pre-Training\nThis stage consists of two\nordinal sub-phases of training for foundational visual un-\nderstanding and grounded generation. Following the pre-\ntraining of CogVLM (Wang et al., 2023b), we first train\nmodel on 1.5B image-text pairs cleaned from the LAION-\n2B (Schuhmann et al., 2022) and COYO-700M (Byeon et al.,\n2022) with 120,000 iterations and batch size of 8,192. We\nthen train model on 40M grounded image-question-answer\ntriples cleaned from LAION-115M (Li et al., 2023c) with\n60,000 iterations and batch size of 1,024, where each noun\nphrase in the answer is followed by a list of coordinates\n[[x0, y0, x1, y1], ...]3 referring the phrase to the grounded\nobjects in the image. Both phases adopt the next token pre-\ndiction objective, and train the 6.5B parameters of visual\nexperts.\n3xi, yi \u2208 [000, 999] refer to the normalized pixel coordinates.\n4\nSecond Stage of Alignment\nThis stage further trains the\nmodel to align with human preferences on solving practical\nvisual problems. We fuse the produced CoM data with 3\ntypes of corpus, including MultiInstruct (Xu et al., 2022),\nLLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen\net al., 2023c), referring the abilities of instruction-following,\ntexts-recognizing, and detailed-captioning. This fusion re-\nsults in a total of 570K (I, Q, A) samples, where the answer\nA in CoM data consists of multiple turns. For the training\ndata of CoM, we randomly prepend the questions with a\nlunching prompt4 P M to questions Q = P M + Q asking\nmodels to optionally use manipulations for the adaption of\nexplicit eliciting. We empirically show that the model can\neffectively learn the evidential visual reasoning by ingesting\nthis portion of CoM data. We train 14,000 iterations with\na batch size of 160, where the learning rate reaches 10\u22125\nafter 280 steps of warm-up and then decays linearly. The\nparameters of 6.5B visual experts are trained with the objec-\ntive of next token prediction. These two stages of training\nresult in our standard version of CogCoM involving both\nchat and reasoning capabilities. More training details are\navailable at Appendix C.2.\n3. Experiment\nTo quantitatively validate the suitability and efficiency of the\nproposed method, we conduct experiments on 9 benchmarks\ncorresponding to 3 categories of multimodal capabilities,\nand a newly proposed testbed incorporating the evidential\nreasoning paths with a keypoints-aware metric. Following\nprevious works, we train two generalist versions of CogCoM\nfor adapting to the different scenarios of Visual Question\nAnswering and Visual Grounding, and evaluate the standard\nversion with a qualitative analysis (Hwang et al., 2023).\n\u2022 Detailed Visual Question Answering.\nThis task\ninvolves models to perform detailed reasoning or\nrecognition on images. We use 4 prominent bench-\nmarks including, GQA (Hudson & Manning, 2019),\nTextVQA (Singh et al., 2019), ST-VQA (Biten et al.,\n2019), and TallyVQA (Acharya et al., 2019).\n\u2022 Visual Grounding. Visual grounding evaluates the\ncrucial abilities of VLMs on meticulous position under-\nstanding. We evaluate our model on 3 standard bench-\nmarks, RefCOCO (Yu et al., 2016), RefCOCO+ (Yu\net al., 2016), and RefCOCOg (Mao et al., 2016).\n\u2022 Hallucination Examination. We also evaluate our\nmodel on a specific benchmark POPE (Li et al., 2023d)\nthat explores the crucial issue of visual objects halluci-\nnation, to investigate the helpfulness of incorporating\nevidential visual reasoning with model training.\n4See Appendix C.1 for examples.\n3.1. Experiments on Detailed VQA\nVLMs have demonstrated the well-known superiority in\nvisual scenes with salient content understanding. We evalu-\nate the effectiveness of CogCoM on VQAs on meticulous\nunderstanding, which typically require models to perform\nmultiple actions (find, read) or multiple reasoning steps\n(recognizing and then calculating). Following previous\nstudies (Wang et al., 2023b), we train our model obtained\nfrom the first-phase of stage-1 on a fusion of data, including\nan instruction corpus of MultiInstruct, 13 publicly available\nVQA datasets (only using training set), a newly created\nVQA dataset built through promoting GPT4-V (OpenAI,\n2023b) for image-oriented question-answer generation, and\nthe CoM corpus. This training results in a generalist VQA\nmodel incorporating CoM reasoning. For all existing VQA\ntasks, we directly prompt CogCoM with given questions\nand examine the correctness of outputted answers.\n3.1.1. GQA, TEXTVQA, ST-VQA, TALLYVQA\nSettings\nThe task of visual question answering aims to\nevaluate the correctness of answer generated by a model by\ngiven a question towards an image. GQA is a compositional\nVQA benchmark with diverse reasoning questions coming\nfrom semantic functional programs. TallyVQA is an objects\ncounting benchmark with human-annotated complex count-\ning questions involving challenging non-zero counterparts.\nTextVQA and ST-VQA are two texts understanding bench-\nmarks requiring models to answer questions through textual\ncues on images. We use the official evaluation scripts for\nGQA and TallyVQA, which calculate the accuracy score by\nthe Exact Matching (EM) between model predictions and\nanswers. For TextVQA and ST-VQA, we submit our model\npredictions to the official online websites for calculating the\naccuracy with VQA Score metric (Antol et al., 2015).\nResults\nThe experimental results are shown in Table 2.\nCogCoM achieves the state-of-the-art performance in com-\nparison with all generalist models across the board, and\nsignificantly surpass the baseline of the previous best model\non multiple benchmarks. On the datasets requiring complex\nreasoning and detailed texts recognition, CogCoM obtains\nthe results that are comparable to the specialist SOTAs.\nSpecifically, our model improves by 5.97 accuracy score\ncompared to the baseline model CogVLM on the GQA\nbenchmark, suggesting that the training incorporated with\nthe evidential visual reasoning chains effectively enhance\nthe general reasoning ability of models. On the counting\nbenchmark of TallyVQA, our model outperforms the base-\nline by 4.2 and 2.1 in accuracy score with simple and com-\nplex questions, respectively. This result demonstrates the\neffectiveness of the rigorous grounded learning on the sce-\nnarios requiring specific objects count calculations. On\n5\nType\nModel\nGQA\nTallyVQA\nTextVQA\nST-VQA\ntest-balanced\nsimple\ncomplex\ntest\ntest\nGeneralist\nFlamingo (Alayrac et al., 2022)\n-\n-\n-\n54.1\n-\nGIT (Wang et al., 2022a)\n-\n-\n-\n59.8\n-\nGI2 (Wang et al., 2022a)\n-\n-\n-\n67.3\n-\nBLIP-2 (Li et al., 2023c)\n44.7\u2020\n-\n-\n-\n21.7\nInstructBLIP (Dai et al., 2023)\n49.5\u2020\n-\n-\n-\n50.7\u2020\nQwen-VL (Bai et al., 2023)\n59.3\n-\n-\n63.8\n-\nCogVLM (Wang et al., 2023b)\n65.2\n79.8\n68.0\n69.7\n61.0\nCogCoM\n71.7\n84.0\n70.1\n71.1\n70.0\nSpecialist\nSOTAs\n72.1\n(CFR)\n86.0\n( PaLI-X)\n75.6\n(PaLI-X)\n71.4\n(PaLI-X)\n86.0\n(SMoLA)\nTable 1. Performance on Visual Question Answering benchmarks, where the results labeled with \u2020 refer to the few-shot setting. CogCoM\nachieves SOTA across the board, and demonstrates the effectiveness on the visual reasoning and scene texts recognition benchmarks.\n0\n2k\n4k\n6k\n8k\nTraining Steps\n2.15\n7.27\n20.00\n30.00\n40.00\n55.59\n60.00\nAccuracy Score\nZero\nshot / answer\nCogCoM / answer\nCogCoM / explanation\nFigure 3. Results shown training CogCoM only using the 70K\nCOM data without original supervised data. CogCoM swiftly\nachieves competitive performance with a few training steps, and\nthe explanation scores increase proportionally.\nthe detailed texts recognition benchmarks TextVQA and\nST-VQA, CogCoM surpassed the baseline by 1.4 and 9\npoints in accuracy, respectively. This result demonstrates\nthe effectiveness of the proposed approach in solving details\nrecognition problem.\n3.1.2. AUTOCOM-TEST\nDue to the lack of resource to explore the correctness of the\nevidential reasoning, we run the proposed data production\napproach on the validation set of TextVQA, a corpus that\ncontains sufficient questions and golden answers to achieve\n885 positive CoM chains. It is noteworthy that eliciting\nCoM chains do not raise the difficulty of answering visual\nquestions, but offers an opportunity to examine the correct-\nness of intermediate reasoning process. The distribution\nof chains and manipulations of the generated testbed are\nprovided in Appendix B.2\nSettings\nTo measure the correctness of CoM chains, we\nintroduce a keypoints-aware metric. The intuition is that\nwe care about the key elements including actions (i.e., ma-\nnipulation name), targets (i.e., manipulation input), and vi-\nsual contents (i.e., manipulation returns) of each step in the\npath, as well as the logical execution order of manipulations.\nGiven a pair of chain-answer annotation (c, a) and corre-\nsponding model prediction (c\u2032, a\u2032), we first sequentially ex-\ntract the key elements from c and c\u2032 to construct two ordered\nlists, and then replace the elements in the lists with their\nfixed indices in a Bag-of-Elements E = c \u222a c\u2032 to result in\nlists of k and k\u2032. We thus calculate the score as the normal-\nized Levenshtein Distance sc = Levenshtein(k, k\u2032)/N\nbetween the two lists, where N is the maximum length\nbetween k and k\u2032. We adopt this simple discretization strat-\negy with low time complexity to concentrate on the key\npoints as well as the solving order. We further consider\nthe linguistic matching of paragraphs by calculating the\nBLEU (Papineni et al., 2002) score between two chains\nsp = BLEU(c, c\u2032), and the final sore is a weighted combi-\nnation as acc = (0.6 \u00d7 sc + 0.4 \u00d7 sp)/2.\nResults\nWe train our first-stage model only using the pro-\nposed 70K CoM data without any original supervision for\nqualitatively evaluate the effectiveness of the chains. The\nresults are shown in Figure3. The 2.15 accuracy score refers\nto the zero-shot performance by prompting the first-stage\nmodel directly. We can see that by training incorporating\nwith the COM chains, our model can swiftly achieve the\ncompetitive performance with 48.41 accuracy score with 2k\ntraining steps, and obtain the optimal result of 55.59 with 8K\nsteps. The explanation scores gradually improve along with\nthe model performance, indicating that successful reason-\ning steps contribute to the achieving of final answer. Since\nthis study calculates the explanation score as a sequentially\nmatching of the discretized elements, which over-penalizes\nthe approximate results (e.g., similar boxes), further result-\ning in small discrepancies in scores and indistinct curves.\n6\nType\nModel\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntest-A\ntest-B\nval\ntest-A\ntest-B\nval\ntest\nGeneralist\nOFA-L* (Wang et al., 2022b)\n79.96\n83.67\n76.39\n68.29\n76.00\n61.75\n67.57\n67.58\nShikra-7B (Chen et al., 2023b)\n87.01\n90.61\n80.24\n81.60\n87.36\n72.12\n82.27\n82.19\nShikra-13B (Chen et al., 2023b)\n87.83\n91.11\n81.81\n82.89\n87.79\n74.41\n82.64\n83.16\nQwen-VL (Bai et al., 2023)\n89.36\n92.26\n85.34\n83.12\n88.25\n77.21\n85.58\n85.48\nCogVLM (Wang et al., 2023b)\n92.51\n93.95\n88.73\n87.52\n91.81\n81.43\n89.46\n90.09\nCogCoM\n92.34\n94.57\n89.15\n88.19\n92.80\n82.08\n89.32\n90.45\nSpecialist\nSOTAs\n92.64\n(UNINEXT)\n94.33\n(UNINEXT)\n91.46\n(UNINEXT)\n88.77\n(ONE-PEACE)\n92.21\n(ONE-PEACE)\n83.23\n(ONE-PEACE)\n89.22\n(ONE-PEACE)\n89.37\n(UNINEXT-H)\nTable 2. Results on Visual Grounding benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023).\nMethod\nPOPE\nBLIP-2 (Li et al., 2023c)\n-\nOtter (Li et al., 2023a)\n-\nMiniGPT4 (Zhu et al., 2023)\n70.4\nInstructBLIP (Dai et al., 2023)\n77.3\nLLaVA (Liu et al., 2023b)\n66.3\nLLaMA-Adapter v2 (Gao et al., 2023)\n-\nDreamLLM (Dong et al., 2023)\n76.5\nLLaVA-1.5 (Liu et al., 2023a)\n84.5\nEmu (Sun et al., 2023b)\n-\nCogVLM\n87.2\nCogCoM\n87.8\nTable 3. Evaluation on integrated and hallucination assessment\n(adversarial subset). We use the original questions as the input\nprompts for evaluation.\n3.2. Experiments on Visual Grounding\nThe task of visual grounding requires models to precisely\nprovide the corresponding coordinates of regions in an im-\nage based on the given target expression, which is a cru-\ncial aspect of detailed position understanding of VLMs\nand is attracted extensive attention. Following the exist-\ning work (Wang et al., 2023b), we train our model obtained\nby the first stage on a fusion of datasets, including an instruc-\ntion corpus MultiInstruct, a high-quality grounding VQA\ncorpus introduce in CogVLM, and the proposed CoM data.\nThis training results in a generalist grounding model that\nis excelling at visual grounding while capable of reasoning.\nFor all benchmarks, we prompt CogOM in a chat manner\nto ask the model to provide grounded coordinates, such as\n\u201cWhere is \u27e8expr\u27e9 answer in [x0,y0,x1,y1] format.\u201d, where\nthe \u27e8expr\u27e9 refers to the target expression.\nSettings\nWe evaluate CogCoM on three well-established\nvisual grounding benchmarks. We use the standard evalua-\ntion metric, that considers a prediction as correct when the\nintersection-over-union (IoU) between the predicted bound-\ning box and ground truth is greater than 0.5.\nResults\nThe results are shown in Figure 2. CogCoM\nachieves the best performance in 6 out of all 8 sub-sets.\nGiven that our model is trained on a fusion of the instruction\nfollowing, visual grounding, and CoM corpus to gain broad\napplicability, this result indicates that our model exhibits a\nsuperior grounding abilities while offers potential to solve\na variety of tasks. In addition, CogCoM achieves perfor-\nmance on par with the specialist SOTAs, demonstrating that\nincorporating evidential reasoning with grounding manipu-\nlations into the general training can enhance the multimodal\ncapabilities while preserving the mastery of grounding.\n3.3. Experiments on Hallucination Examination\nTo further investigate the helpfulness of incorporating the\nevidential visual reasoning on alleviating multimodal hal-\nlucinations, we further evaluate CogCoM on POPE, a typi-\ncal benchmark that evaluate the objects hallucination issue\nfaced by VLMs. We use our generalist VQA model for this\nevaluation, and obtain model predictions by directly asking\nthe corresponding questions in the original dataset.\nSettings\nPOPE aims to investigate the objects hallucina-\ntion of VLMs by asking the existence of objects in an image\nwith binary questions (i.e., yes/no). We use the challenging\nadversarial version of the dataset for standard evaluation.\nFor the calculation of scores, we use the official evaluation\nscripts to calculate the F1 score of predictions.\nResults\nThe experimental results on the POPE benchmark\nare shown in Table 3. We can see that CogCoM achieves the\nsuperior performance in comparison with previous VLMs\non the benchmark, demonstrating the improvements. Specif-\nically, CogCoM improves the performance and achieves the\nsuperior result on POP in comparison with baseline and the\nprevious VLMs. As we train CogCOM with a fusion of\ndiversified capabilities, this results suggest that our model\nperforms well on general multimodal tasks with a lower\nsusceptibility to hallucination, while preserving VQA, in-\nstruction following, and reasoning abilities.\n7\nUsing grounding(the plane) to \ufb01nd the plane in current \nimage, and there are two regions [[255,217,312,245], \n[193,113,481,208]] referring planes that can be found.\nAs the plane in [[193,113,481,208]] is the most notable \none, using grounding(the logo in region \n[[193,113,481,208]]) to locate the logo of the plane in \ngiven region, resulting in [[246,142,307,163]].\nUsing crop_and_zoomin( [[246,142,307,16\n3]],3)to crop the current image and magnify\nit by three times over to create a new image.\nRecognize the brand name of the logo in the new image, \nresulting in AirFrance.\nQ: What is the brand of the plane?\nA: AirFrance\nApplying grounding(the clock) to \ufb01nd the clock in this \nimage, resulting in the region of [[814, 531, 940,1062]].\nUse crop_and_zoomin([[814, 531, 940,10\n62]], 2) to crop this image along region [[81\n4, 531, 940,1062]] and then zoom in it by \ntwo times of its original size to yield a new \nimage, and re-input this new image.\nUsing read_time(the clock in the new image) to \nrecognize the time on the clock, and the result is 1:45.\nQ: What time does this photo show?\nA\u001d\u00031:45\nQ: what is the smallest part of the pie chart?\nA: pa hospital\nBased on grounding(the pie chart) to \ufb01nd the pie chart \nin current picture, and the region that I found is \n[[534,203,847,406 ]].\nUsing grounding(the smallest part of the pie chart in \n[[534,203,847,406]]) to \ufb01nd the region of the smallest \npart, and the result is [[554,369,693, 737]].\nUsing crop_and_zoomin([[534,203,847,406\n]], 3) to crop this picture and then resize the\ncropped by 3 times to get a new image.\nRecognize the content in the new image, and the \ufb01nal \nresult is \"pa hospital\".\nQ: How many wheels of the truck can be seen in this picture?\nA: ten\nUsing grounding(all individual wheels of the truck) to \nlocate all individual wheels in this picture, resulting in the \nboxes of [[56, 256, 96, 513], [105, 240, 144, 480], [223, \n238, 249, 475], [253, 238, 281, 476], [301, 229, 331, 458], \n[338, 226, 364, 452], [367, 227, 379, 454], [382, 228, 397, \n457], [395, 228, 411, 457], [416, 222, 443, 444]].\nBased on the identi\ufb01ed boxes of wheels, the number of \nvisible wheels of the truck in this picture is ten.\nQ: What might be the name of one team in this match?\nA: Dbacks\nUsing grounding(the name of teams) to \ufb01nd regions \ncorresponding to the name of teams in current image, \nresulting in [[0, 199, 91, 397], [368, 83, 438, 166], [445, \n81, 497, 161]].\nIn comparison with other two regions, the [0,99,91,397] \nshould include the name of one of the teams.\nUsing crop_and_zoomin([[0,99,91,397]], 2)\nto crop the current image and resize it by\ntwo times to get new image, and re-input it.\nIdentify the text referring to the team name in the new \nimage, resulting in Dbacks.\nQ: what is the name of the song on the sheet of music?\nA: across the distance\nUsing grounding(the sheet of music) to localize the \nsheet of music in this picture, and the resulting region is \n[[ 0,348,788,696]].\nUsing grounding(the title of the sheet in region \n[[0,348,788,696]]) to focus on the title of the sheet in the \ncorresponding region, and the region that should be \nfocused on is [[641,498,682,995 ]].\nAdopt crop_and_zoomin([[641,498,682,995\n]], 2) to crop this region and zoom in it two\ntimes over, resulting in a new image.\nRecognize the name of the song by looking at the new \nimage, and the answer is across the distance.\nFigure 4. CogCoM performs reasoning with details recognition, reading time, understanding charts, counting objects, and reading texts.\n3.4. Qualitative Analysis\nWe investigate the evidential reasoning capability of Cog-\nCoM on scenarios that requires different types of meticulous\nreasoning, including recognizing textual details, reading\ntime, understanding charts and counting objects. The results\nare shown in Figure 4. The first case demonstrates that\nCogCoM finds the region corresponding to the plane logo\nthrough two steps of grounding and then achieves the answer\nbased on zooming in the cropped region. The second case\nillustrates the ability of CogCoM in reading time, by locat-\ning the device that displays time and then transforming the\ntime into words based on the read timne manipulation. In\nthe forth example, CogCoM first identifies all visible truck\nwheels through grounding, and then leverages the counting\nmanipulation to consolidate the total number.\n4. Conclusion\nThis paper study the problems presented by the conclusive-\nalignment training of Vision-Language Models (VLMs),\nand propose a general mechanism, Chain of Manipulations\n(CoM), that enables VLMs to solve problems by actively\nmanipulating visual inputs to acquire essential contents.\nWe realize this methodology by building an efficient data\nproduction framework that engages linguistic and visual\nannotators to synthesis CoM chains, and a memory-based\narchitecture which is compatible with existing models. A\n17B VLM, CogCoM with the memory-based architecture\nis trained on the data fusion incorporating the implemented\nCoM chains. Experiments with quantitative results on 8\nbenchmarks and qualitative analysis demonstrate the effec-\ntiveness of the method in solving detailed visual problems.\n8\n5. Limitations\nThough we try to develop an accurate and robust frame-\nwork that engages remarkable LLM to provide basic solving\nsteps, adopts reliable visual tools to obtain visual contents,\nand then acquires feasible paths based on traversal, there\nare still limitations in our methodology that we hope to\nimprove in the future. First, We find that the diversity of\nlinguistic solving steps is insufficient, and the inaccuracy\nof visual tools (e.g., the rough granularity of grounding\nboxes, OCR failures on slant letters) will lead to a large\namount of negative paths (effectively utilizing these paths\nwould beneficial). We suggest to promote these limitations\nwith dedicate prompts and improved visual tools. Second,\nour current model re-input the manipulated images with a\nset of hard prompts, which may bring speed losses. This\nis expected to be improved by implementing the physical\nmanipuations into the calculations in vector space.\n6. Impacts\nThis work presents a general visual reasoning mechanism\nthat alleviate the problems caused by existing conclusion-\nalignment training for VLMs, introduces a data production\nframework involving LLMs and visual tools as reliable an-\nnotators, and devises a memory-based compatible VLM\narchitecture. We expect this work to bring three benefits to\nthe community. First, the proposed visual reasoning mecha-\nnism may push the progress of VLMs in solving complex\nvisual problems. Second, the introduced data production\nframework may be applied to widespread training scenarios\nto promote the development of current data-driven machine\nlearning. Third, we hope that the memory-based architec-\nture will be helpful for VLMs in multi-turn long contexts.\nReferences\nAcharya, M., Kafle, K., and Kanan, C. Tallyqa: Answering\ncomplex counting questions. In Proceedings of the AAAI\nconference on artificial intelligence, 2019.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 2022.\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,\nZitnick, C. L., and Parikh, D. Vqa: Visual question\nanswering. In Proceedings of the IEEE international\nconference on computer vision, 2015.\nAwadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y.,\nZhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa,\nS., et al. Openflamingo: An open-source framework\nfor training large autoregressive vision-language models.\narXiv preprint, 2023.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,\nZhou, C., and Zhou, J. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv preprint,\n2023.\nBiten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M.,\nValveny, E., Jawahar, C., and Karatzas, D. Scene text vi-\nsual question answering. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2019.\nByeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim,\nS. Coyo-700m: Image-text pair dataset, 2022.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12m: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021.\nChen, D., Liu, J., Dai, W., and Wang, B.\nVisual in-\nstruction tuning with polite flamingo. arXiv preprint\narXiv:2307.01003, 2023a.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic. arXiv preprint, 2023b.\nChen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J.,\nZhao, F., and Lin, D.\nSharegpt4v: Improving large\nmulti-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023c.\nChen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,\nPadlewski, P., Salz, D., Goodman, S., Grycner, A.,\nMustafa, B., Beyer, L., et al.\nPali: A jointly-scaled\nmultilingual language-image model. In The Eleventh\nInternational Conference on Learning Representations,\n2022.\nChen, X., Djolonga, J., Padlewski, P., Mustafa, B., Chang-\npinyo, S., Wu, J., Ruiz, C. R., Goodman, S., Wang, X.,\nTay, Y., et al. Pali-x: On scaling up a multilingual vision\nand language model. arXiv preprint, 2023d.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nDai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li,\nB., Fung, P., and Hoi, S. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\narxiv 2023. arXiv preprint arXiv:2305.06500.\n9\nDai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li,\nB., Fung, P., and Hoi, S. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\narxiv 2023. arXiv preprint, 2023.\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\nL., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Syn-\nergistic multimodal comprehension and creation. arXiv\npreprint arXiv:2309.11499, 2023.\nDu, Y., Li, C., Guo, R., Yin, X., Liu, W., Zhou, J., Bai, Y.,\nYu, Z., Yang, Y., Dang, Q., et al. Pp-ocr: A practical ultra\nlightweight ocr system. arXiv preprint arXiv:2009.09941,\n2020.\nGao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A.,\nZhang, W., Lu, P., He, C., Yue, X., et al. Llama-adapter\nv2: Parameter-efficient visual instruction model. arXiv\npreprint, 2023.\nHong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang,\nY., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: A\nvisual language model for gui agents. arXiv preprint\narXiv:2312.08914, 2023.\nHuang, K.-H., Zhou, M., Chan, H. P., Fung, Y. R., Wang, Z.,\nZhang, L., Chang, S.-F., and Ji, H. Do lvlms understand\ncharts? analyzing and correcting factual errors in chart\ncaptioning. arXiv preprint arXiv:2312.10160, 2023a.\nHuang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma,\nS., Lv, T., Cui, L., Mohammed, O. K., Liu, Q., et al.\nLanguage is not all you need: Aligning perception with\nlanguage models. arXiv preprint, 2023b.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2019.\nHwang, A., Head, A., and Callison-Burch, C. Grounded\nintuition of gpt-vision\u2019s abilities with scientific images.\narXiv preprint, 2023.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H.,\nLe, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision. In International conference on\nmachine learning, 2021.\nKeys, R. Cubic convolution interpolation for digital image\nprocessing. IEEE transactions on acoustics, speech, and\nsignal processing, 1981.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 2017.\nLi, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C.,\nand Liu, Z. Mimic-it: Multi-modal in-context instruction\ntuning. arXiv preprint arXiv:2306.05425, 2023a.\nLi, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z.\nOtter: A multi-modal model with in-context instruction\ntuning. arXiv preprint arXiv:2305.03726, 2023b.\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders\nand large language models. arXiv preprint, 2023c.\nLi, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen,\nJ.-R.\nEvaluating object hallucination in large vision-\nlanguage models.\narXiv preprint arXiv:2305.10355,\n2023d.\nLi, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen,\nC., Chen, L., and Wei, Y. Stablellava: Enhanced visual\ninstruction tuning with synthesized image-dialogue data.\narXiv preprint arXiv:2308.10253, 2023e.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, 2014.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. arXiv preprint, 2023b.\nLiu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C.,\nYang, J., Su, H., Zhu, J., et al. Grounding dino: Marry-\ning dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023c.\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\nand Murphy, K. Generation and comprehension of unam-\nbiguous object descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\n2016.\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View\nin Article, 2023a.\nOpenAI, R. Gpt-4v(ision) system card. Citekey: gptvision.,\n2023b.\nOrdonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\nimages using 1 million captioned photographs. Advances\nin neural information processing systems, 2011.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 2002.\n10\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F. Kosmos-2: Grounding multimodal large\nlanguage models to the world. arXiv preprint, 2023.\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs.\narXiv preprint\narXiv:2111.02114, 2021.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al.\nLaion-5b: An open large-\nscale dataset for training next generation image-text mod-\nels. Advances in Neural Information Processing Systems,\n2022.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 2018.\nShazeer, N.\nGlu variants improve transformer.\narXiv\npreprint arXiv:2002.05202, 2020.\nShrestha, R., Kafle, K., and Kanan, C. Answer them all!\ntoward universal visual question answering models. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2019.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\n2019.\nSun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-\nclip: Improved training techniques for clip at scale. arXiv\npreprint arXiv:2303.15389, 2023a.\nSun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,\nGao, H., Liu, J., Huang, T., and Wang, X. Generative\npretraining in multimodality. arXiv preprint, 2023b.\nWang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu,\nZ., Liu, C., and Wang, L. Git: A generative image-to-\ntext transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022a.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J.,\nZhou, C., Zhou, J., and Yang, H. Ofa: Unifying architec-\ntures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. 2022b.\nWang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G.,\nLuo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionllm: Large\nlanguage model is also an open-ended decoder for vision-\ncentric tasks. arXiv preprint, 2023a.\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\nexpert for pretrained language models. arXiv preprint,\n2023b.\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\nKhashabi, D., and Hajishirzi, H. Self-instruct: Aligning\nlanguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560, 2022c.\nWu, P. and Xie, S.\nV*:\nGuided visual search as a\ncore mechanism in multimodal llms.\narXiv preprint\narXiv:2312.14135, 2023.\nXu, Z., Shen, Y., and Huang, L. Multiinstruct: Improv-\ning multi-modal zero-shot learning via instruction tuning.\narXiv preprint arXiv:2212.10773, 2022.\nYin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y.,\nLi, K., Sun, X., and Chen, E. Woodpecker: Hallucination\ncorrection for multimodal large language models. arXiv\npreprint arXiv:2310.16045, 2023.\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\nModeling context in referring expressions. In Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part II 14, 2016.\nYu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang,\nS., Tian, Q., and Zhuang, Y. Hallucidoctor: Mitigating\nhallucinatory toxicity in visual instruction data. arXiv\npreprint arXiv:2311.13614, 2023.\nZeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y.,\nZhang, Y., and Kong, T. What matters in training a gpt4-\nstyle language model with multimodal inputs?\narXiv\npreprint arXiv:2307.02469, 2023.\nZhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W.,\nChen, K., and Luo, P. Gpt4roi: Instruction tuning large\nlanguage model on region-of-interest. arXiv preprint\narXiv:2307.03601, 2023a.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D.,\nand Sun, T. Llavar: Enhanced visual instruction tuning\nfor text-rich image understanding. arXiv preprint, 2023b.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint,\n2023.\n11\nA. Related Works\nA.1. Large Vision-Langauge Models as Foundations\nMost of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia et al., 2021),\nMSCOCO (Lin et al., 2014), VG (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021),\nSBU (Ordonez et al., 2011), LAION2B (Schuhmann et al., 2022), LAION400M (Schuhmann et al., 2021). Starting from\nFlamingo (Alayrac et al., 2022), a series of LVLMs have focused on training the adaptation layers to align the visual\nrepresentation to the frozen LLMs on a mixture of image-text pairs with the above corpus, including BLIP2 (Li et al., 2023c),\nKOSMOS (Huang et al., 2023b), and OpenFlamingo (Awadalla et al., 2023). Inspired by success of instruction tuning in\nLLMs (Wang et al., 2022c), a line of works have devoted efforts to build vision-oriented instruction-answer pairs through\nGPT4 and train models for imitation, such as LLAVA (Liu et al., 2023b), Otter (Li et al., 2023b), VisionLLM (Wang et al.,\n2023a), MultiInstruct (Xu et al., 2022), Lynx (Zeng et al., 2023), InstructBLIP (Dai et al.), CleverFlamingo (Chen et al.,\n2023a) and StableLLaVA (Li et al., 2023e). Recently, researchers have proven the efficiency of developing LVLMs with\ntwo stages of training, the first stage of abundant pretraining on image-caption pairs and the second stage of alignment on\nimage-question-answer triples, such as PALI (Chen et al., 2022), PaLI-X (Chen et al., 2023d), Qwen-VL (Bai et al., 2023),\nand CogVLM (Wang et al., 2023b).\nA.2. Large Vision-Language Models with Reasoning\nTo further enhance the ability of LVLMs in solving high-level visual problems, research focusing on various aspects of\nreasoning is attracting broad attention. We simply divide existing studies into tree broad categories. The first line of research\nfocus on enhance train models with a mastery of cross-modal grounded reasoning, where grounded instruction-following\nsupervision is build through public visual grounding dataset or GPT4-V for training, including KOSMOS-2 (Peng et al.,\n2023), Shikra (Chen et al., 2023b), and GPT4ROI (Zhang et al., 2023a). The second aspect of efforts have been devoted\ninto promoting models to understand artificial visual scenes, such as figures, charts, and receipts. These studies includes\nCogAgent (Hong et al., 2023) and CHARTVE (Huang et al., 2023a). Some other studies address the crucial problem of\nhallucination in LVLMs with counterfactual or interpretable reasoning (Yu et al., 2023; Yin et al., 2023). V* (Wu & Xie,\n2023) also contributes efforts to enhance the details recognition of VLMs based the LLM-guided searching process.\n12\nB. Details of Data Production\nIn this section, we further introduce the details of CoM data production, with the overall algorithm of a pseudo code,\nan example of the solving steps generation with LLM and corresponding guideline, an example of the reasoning chains\ncompletion with visual tools. We also list the details of data statistics for the synthesised training data as well as the\nevaluation data of AutoCoM-test, followed by a limitation analysis for the current data production method.\nB.1. The Algorithm for A Reproducible Data Production\nWe provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data generation, thereby\nfacilitating understanding and reproduction 1.\nAlgorithm 1 Synthesising Chain of Manipulations\n1: Define:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nManipulations : {fi : x \u2192 y | fi \u2208 M}\nLinguistic Annotator : \u03a8L\n//We use GPT4 in this work\nV isual Annotator : \u03a8V\n//We use PaddleOCR and GroundingDINO in this work\n2: Input: Image I, Question Q, Answer A\n3: // Linguistic Annotation\n4: Prompt \u03a8L with guideline P L to generate reasoning steps:\n\u03c2 = \u03a8L(Q|P L),\nwhere\n(\n\u03c2 = (steps1, steps2, ...)\nstepsi = (fi, desci)\n(4)\n5: Define tree T\n6: for i = 1 to |\u03c2| do\n7:\nExtract xi, yi instantiated with fi in stepi\n8:\nExtract referential boxes B from xi\n9:\nfor b in B do\n10:\nLeverage \u03a8V to acquire corresponding visual content y\u2032\ni = \u03a8(xi|I, b), and apply yi to tree\nT .level[i].append(yi)\n(5)\n11:\nend for\n12: end for\n13: Traverse T to obtain positive chains that leads to given answer with terminal return\n[\u03c21, \u03c22, ...] = DFS(T |A)\n(6)\n14: Return [\u03c21, \u03c22, ...]\n13\nB.2. Data Statistics\nWe develop a strategy to extract predicate phrases based constituency parsing with StandordCoreNLP, in which we extract\nverb, conjunction-connected verb phrase, preposition-connected verb phrase.\nBesides the standard CoM data incorporating manipulations with explicit visual evidences, the proposed data synthesising\nframework is compatible of producing implicit visual reasoning steps step\u2032\ni = (desci) without involving the manipulations.\nWe thereby also build this partial CoM data on the corpus consisting of absurd visual questions (i.e., asking unanswerable\nquestions based on the given image) to further resist the toxic hallucinations. Specifically, given an image I with a question\nQ,we prompt GPT-4V (OpenAI, 2023b) to solve the question step-by-step to acquire the reasoning chains.\nData Source\n#QAs\n#Chains\n#Steps/Chain\n#Manipulations Types/Chain\nTextVQA (Biten et al., 2019)\n10782\n13766\n2.93\n2.41\nST-VQA (Singh et al., 2019)\n4814\n3959\n2.88\n2.43\nTDIUC-count (Shrestha et al., 2019)\n53547\n54523\n2.35\n0.74\nTDIUC-absurd (Shrestha et al., 2019)\n11677\n11677\n4.09\n-\nAutoCoM-test\n4609\n8612\n3.26\n2.18\nTable 4. Detailed statistics the the training data and evaluation data synthesised with CoM production.\nlocate\nread\nidentify\ndetermine\nlook for\nobserve\nanalyze\nexamine\ncount\nzoom in\ncompare\nfind\nfocus on\nuse\nzoom\nextract\ninterpret\nrecognize\nsearch for\nzoom on\nnote\nlook at\nresearch\nunderstand\nprovide\nmake\nlist\ncheck\nenhance\nconduct\nmeasure\ncheck for\nmatch\nanswer\nconsult\nconvert\nconfirm\ndescribe\ninspect\ncalculate\nlook\nopen\nstate\nrefer\nnote down\nconsider\ngather\nestimate\ntranslate\nrecord\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nFigure 5. Distribution of the top-50 generated manipulations out of total 465 based on 5-shot prompting, where the first three bars are\nscaled with 20% for a smooth visualization of all data.\n14\nB.3. Details of Linguistic/Visual Annotations\nIn this work, we adopt the GPT4-turbo as the linguistic annotator for generating problems solving steps, and the API call was\nconducted during the period of 2023.9 - 2023.12. For the visual annotators, we leverage the the currently best-performing\ntools, GroundingDINO and PaddleOCR, to acquire all visual contents requested by the manipulations. For a clear description\nto the production setting and results, we illustrate the guiding prompt, and an example-based linguistic annotation results as\nwell as the visual annotation results in Figure 6.\nB.4. Limitation Analysis\nFor the implemented data framework, we engage the remarkable LLM to provide basic solving steps, adopt two reliable\nvisual tools (i.e., GroundingDINO and PaddleOCR) to acquire corresponding visual contents, and then perform the traversal\nto achieve feasible reasoning paths, which ensures the correctness and robustness of data synthesizing. However, we also\nfind that there are three major limitations caused by the employed models and could be improved in future:\n\u2022 The lack of diversity in linguistic reasoning steps. The 5-shot prompting to the GPT-4 gains a stable solving steps, but\nit also results in the descriptions for executing manipulations or general thinking are similar. We suggest that this can\nbe addressed by employing diversified prompts or requirements.\n\u2022 The inaccuracy of visual tools. We find that there are a considerable amount of negative paths caused by the failures\nof visual tools, such as the rough granularity of bounding boxes and the error recognition of slated letters or long\nsentences. This issue can be relieved by improving the semantic understanding capabilities of visual tools.\nC. Details of Training\nC.1. Launching Prompts\n\u2022 Please solve the problem gradually via a chain of manipulations, where in each step you can selectively\nadopt one of the following manipulations GROUNDING(a phrase)\u2192boxes, OCR(an image or a region)\u2192texts,\nCROP AND ZOOMIN(a region on given image)\u2192new image, CALCULATE(a computable target)\u2192numbers, or\ninvent a new manipulation, if that seems helpful. {QUESTION}\n\u2022 Please tackle a given question in a stepbystep manner. For each step one of the following manipulations (depicted as\nName(Input)\u2192Retrun) can be optionally used: GROUNDING(a phrase)\u2192boxes, OCR(an image or a region)\u2192texts,\nCROP AND ZOOMIN(a region on given image)\u2192new image, CALCULATE(a computable target)\u2192numbers, or\ndevelop a new manipulation yourself (if it is indeed required). {QUESTION}\n\u2022 Please go through the question incrementally with chain of manipulations (optionally use manipulation when needed)\nsuch as GROUNDING(a phrase)\u2192boxes, OCR(an image or a region)\u2192texts, CROP AND ZOOMIN(a region on\ngiven image)\u2192new image, CALCULATE(a computable target)\u2192numbers, and create a new manipulation if necessary.\n{QUESTION}\nC.2. Training settings\nParameters\nStage1-1\nState1-2\nStage-2\nObjective\nnext token prediction\nnext token prediction\nnext token prediction\nImages\n1.5B\n40M\n570K\nBatch size\n8192\n1024\n160\nIterations\n120,000\n60000\n14000\nOptimizer\nAdamW\nAdamW\nAdamW\nLearning rate\n1e-4\n1e-5\n1e-5\nWarm up steps\n7200\n1200\n280\nTrainable weights\n6.5B visual expert\n6.5B visual expert\n6.5B visual expert\nTable 5. Training details of all stages.\nD. Details of Representative Cases\n15\nYou are a visual assistant capable of generating solving steps for image-oriented visual questions. In each step, you can optionally use a \nmanipulation to operate the image, which can be used to acquire speci\ufb01c information from the image or to acquire the processed new image \n(please be aware that these manipulations will not actually be performed when you generate the solving steps). The manipulation can be one from \nthe prede\ufb01ned ones, or can be a new one you create yourself (should there indeed be a need), where the prede\ufb01ned manipulations with their \ndescriptions are listed below:\\n\\n{MANIPULATIONS}.\\n\\nGiven a question Q abount an image, please generate a series of essential solving \nsteps, where the output of each step is a tuple consisting of a Manipulation (leave it to None if the current step doesn't involve any manipulation) \nand a Description: (1) Manipulation f(x)->y, that is the manipulation `f` targeting `x` to obtain speci\ufb01c information or image `y`; (2) Description, \nwhich is a sentence describing the current solving step.\\n Please adhere the following format: given an input of 'Q: xxx. The essential solving \nsteps are: ', the output should like 'Step 1: (Manipulation, Description); Step 2: (Manipulation, Description); ...'.\\n\\n There are several examples:\\n \n{DEMONSTRATIONS}\\n\\n Q: {QUESTION} The essential solving steps are:\nREQUIREMENTS (PROMPT)\nMANIPULATIONS\ni-th calculate manipulation, that calculate the formula speci\ufb01ed by the target `tgt` in current image, and return \nthe calculation result `res_i`.\ngrounding_i(tgt)->bbx_i: \ni-th crop_and_zoomin manipulation which is useful to identify small and subtle objects in image, that \ufb01rst crops \nthe current image using the box `bbx` de\ufb01ned by the top-left and bottom-right coordinates, and then zoom in \nthe cropped image by two times and \ufb01nally return the resulting image `img_i`.\ncrop_and_zoomin_i(bbx)->img_i: \ni-th grounding manipulation, that locates the object(s) speci\ufb01ed by the target noun phrase `tgt` in current \nimage, and return the resulting bounding box(es) as `bbx_i` where each box is represented by the top-left \nand bottom-right coordinates.\nOCR_i(tgt)->txt_i:\ni-th OCR manipulation, that recognize the natural texts written on the target `tgt`, and return the recognized \ntexts `txt_i`.\ncalculate(tgt)->res_i:\nQuestion: What number police station is on the building?\nStep 1: grounding_1(the building)->bbx_1,\nIdentify the number of the police station on the building in box `bbx_1` and return the \nbounding box of the number as `bbx_2`.\nRecognize the number in the region `bbx_2` and return the recognized number as `txt_1`.\nQuestion: What number police station is on the building?\nAnswer: 43\nLocate the building in the image and return \nthe bounding box of the building as `bbx_1`.\nIdentify the number of the police station on \nthe building in box `bbx_1` and return the \nbounding box of the number as `bbx_2`.\nRecognize the number in the region `bbx_2` \nand return the recognized number as `txt_1`.\n<latexit sha1_base64=\"Eeipg0SaSzO/bMIo2f\nhu3xEhWXY=\">ACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3\n+mPiH+hfeGdMQS2iE5KcOfeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrj\nnBtwSrvB4M4q57Tsebzj9Axlv3PA4cOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyoZY+DswMlJCtal\nh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1\nLq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlV2RN\n9e/VCXISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1Hns\nYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>INPUTS\n<latexit sha1_base64=\"z5pJ6KZLmsSkixjimeA0/1F\n+qY=\">ACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M4glpEJ\nyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeozx4+DOG15bsYCHjFHcBGwVpIyN/QC1vQGRzLevGN\npxuOoIYJa4duP+I97ruCqKszp1F3Ghedk065YlUtcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUk\nJcxRnGKJE2pyxGS6xA/r2aXet2Yj20jNTap9OCehNSWlizQx5aWE5WmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ\n+Z/dbIWgR4OVA2cakoUI6vztUuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV5UpWbn3dW6ON3lLGrD9c5yT\n4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QxsgYf6QaBa1Zx7dl3L8Dgf2Sg=</latexit>OUTPUTSL\n<latexit sha1_base64=\"z5pJ6KZLmsSkixjimeA0/1F+qY=\">ACzHicjVHLSsNAF\nD2Nr1pfVZdugkVwVRIRdVl040K0YmMrWkqSTuvQvEgmQind+gNu9bvEP9C/8M4glpEJyQ5c+49Z+be6yUBz4RlvRSMqemZ2bnifGlhcWl5pby6dpnFeozx4+DOG15bsYCHjFHc\nBGwVpIyN/QC1vQGRzLevGNpxuOoIYJa4duP+I97ruCqKszp1F3Ghedk065YlUtcxJYGtQgV71uPyMG3QRw0eOEAwRBOEALjJ6rmHDQkJcGyPiUkJcxRnGKJE2pyxGS6xA/r2a\nXet2Yj20jNTap9OCehNSWlizQx5aWE5WmiufKWbK/eY+Up7zbkP6e9gqJFbgl9i/dZ+Z/dbIWgR4OVA2cakoUI6vztUuiJvbn6pSpBDQpzEXYqnhH2l/OyzqTSZql321lXxV\n5UpWbn3dW6ON3lLGrD9c5yT4HKnau9V7fPdSu1Qj7qIDWxim+a5jxqOUYdD3iEe8Ign49QxsgYf6QaBa1Zx7dl3L8Dgf2Sg=</latexit>OUTPUTSL\n<latexit sha1_base64=\"gNwd0WkqYb/+16EBVpkoNrWm\nwYE=\">ACz3icjVHLTsJAFD3UF+ILdemkZi4Iq0x6pLoxoWJECmQACHtMGBDaZt2qiE49YfcKt/ZfwD/QvjCVRi\ndFp2p45954zc+91Qs+NhWG8ZrS5+YXFpexybmV1bX0jv7lVi4MkYtxigRdEDceOuef63BKu8HgjLg9dDxedwZnMl6\n/4VHsBn5VjELeHtp93+25zBZEtS6tatmqXnXGtYtJ18wioZa+iwU1BAuspB/gUtdBGAIcEQHD4EYQ82YnqaMGEgJK\n6NMXERIVfFOSbIkTahLE4ZNrED+vZp10xZn/bSM1ZqRqd49Eak1LFHmoDyIsLyNF3FE+Us2d+8x8pT3m1Efyf1GhIrc\nE3sX7p5n91shaBHk5UDS7VFCpGVsdSl0R1Rd5c/1KVIeQOIm7FI8IM6Wc9lXmljVLntrq/ibypSs3LM0N8G7vCUN\n2Pw5zlQOyiaR0WzclgonajzmIHu9ineR6jhHOUYZF3iEc84VmraLfanXb/maplUs02vi3t4QMXQZO2</latexit>OUTPUTSV L\n<latexit sha1_base64=\"Eeipg0SaSzO/bMIo2f\nhu3xEhWXY=\">ACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0owhSsWkLtUgyndbYvEwmYi2u/AG3\n+mPiH+hfeGdMQS2iE5KcOfeM3PvdSLPTYRhvOa0icmp6Zn8bGFufmFxqbi8Uk/CNGbcYqEXxk3HTrj\nnBtwSrvB4M4q57Tsebzj9Axlv3PA4cOgJgYRb/t2L3C7LrMFUfWjk6pVO7soloyoZY+DswMlJCtal\nh8wTk6CMGQwgdHAEHYg42EnhZMGIiIa2NIXEzIVXGOexRIm1IWpwyb2D59e7RrZWxAe+mZKDWjUzx6Y1\nLq2CBNSHkxYXmaruKpcpbsb95D5SnvNqC/k3n5xApcEvuXbpT5X52sRaCLPVWDSzVFipHVscwlV2RN\n9e/VCXISJO4g7FY8JMKUd91pUmUbXL3toq/qYyJSv3LMtN8S5vSQM2f45zHNS3yuZO2TzdLlX2s1Hns\nYZ1bNI8d1HBIaqwyPsKj3jCs3asXWu32t1nqpbLNKv4trSHD4zXkSA=</latexit>INPUTS\nLocate the building in the image and return the bounding box of the building as `bbx_1`.\nStep 2: (grounding_2(number police station on\n               the building in box `bbx_1`)->bbx_2,\nStep 3: (OCR_1(number in region `bbx_2`)\n               ->txt_1,\nLinguistic\nAnnotation\nbbx_1\nbbx_1\nbbx_2\ntxt_1\nbbx_1\nbbx_2\nVisual\nAnnotation\nManipulations De\ufb01nition and Linguistic Annotation Guideline\nAn Example to show the linguistic annotation results and Visual annotation results\nFigure 6. An example shows the configuration, inputs, outputs of the linguistic annotation and visual annotation.\n16\nQuestion: What is written on the traf\ufb01c sign near the bus?\nModel:     Using GROUNDING(the traf\ufb01c sign near the bus) to locate the traf\ufb01c sign near the bus \nin current image, which is [[084,157,926,703;676,146,740,273]]. Utilizing OCR(texts in region \n[[084,157,926,703]]) to read the texts written on the traf\ufb01c sign within region [[084,157,926,703]], \nresulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi.\nCoM:\nVisual Grounding:\n     Question: Find the region in image that \"a traf\ufb01c sign\" describes.\n     Model:     [[678,145,739,270]\nGrounded Captioning:\n    Question: The goal of this task is to generate description for one part of the image. The part is speci\ufb01ed by [[678,145,739,270]]. \n    Model:     A no tax sign\nImage Captioning:\n    Question: In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.\n    Answer:   Taxi bus with rainbow colors on side.\nMulti Choice:\n    Question: Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer.\n    Answer:   black\nDetailed Captioning:\n    Question: Explain the visual content of the image in great detail.\n    Answer:   The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text \"Trafalgar\" in bold red \nletters across its side, indicating it's likely part of a tourist or travel company. The bus is positioned next to a traf\ufb01c sign that reads \"TAXI\", suggesting it might \nbe waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall \nimpression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life.\nGiven a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optionally use one of the following manipulations \n(de\ufb01ned as Name(Input)->Return) on the image: GROUNDING(a phrase)->boxes, OCR(an image or a region)->texts, CROP_AND_ZOOMIN(a region on given \nimage)->new_image, CALCULATE(a computable target)->numbers, or a new manipulation created by yourself (if it is indeed needed). \nFigure 7. CogCoM demonstrates the flexible capabilities for adapting to different multimodal scenarios, including evidential visual\nreasoning, Visual Grounding, Grounded Captioning, Image Captioning, Multi Choice, and Detailed Captioning.\n17\n"
  }
]