[
  {
    "title": "FLAP: Fast Language-Audio Pre-training",
    "link": "https://arxiv.org/pdf/2311.01615.pdf",
    "upvote": "16",
    "text": "FLAP: FAST LANGUAGE-AUDIO PRE-TRAINING\nChing-Feng Yeh, Po-Yao Huang, Vasu Sharma, Shang-Wen Li and Gargi Gosh\n{cfyeh,berniehuang,vasusharma,shangwel,gghosh}@meta.com\nFAIR, Meta\nABSTRACT\nWe propose Fast Language-Audio Pre-training (FLAP), a\nself-supervised approach that efficiently and effectively learns\naligned audio and language representations through masking,\ncontrastive learning and reconstruction. For efficiency, FLAP\nrandomly drops audio spectrogram tokens, focusing solely on\nthe remaining ones for self-supervision. Through inter-modal\ncontrastive learning, FLAP learns to align paired audio and\ntext representations in a shared latent space. Notably, FLAP\nleverages multiple augmented views via masking for inter-\nmodal contrast and learns to reconstruct the masked portion\nof audio tokens. Moreover, FLAP leverages large language\nmodels (LLMs) to augment the text inputs, contributing to\nimproved performance. These approaches lead to more robust\nand informative audio-text representations, enabling FLAP\nto achieve state-of-the-art (SoTA) performance on audio-text\nretrieval tasks on AudioCaps (achieving 53.0% R@1) and\nClotho (achieving 25.5% R@1).\nIndex Terms\u2014 Contrastive learning, audio-text retrieval\n1. INTRODUCTION\nRepresentation learning [1] has garnered significant mo-\nmentum on creating information-rich embeddings for down-\nstream tasks. Recently, self-supervised representation learn-\ning (SSL) [2, 3] has emerged as a prominent research area\nin hope of reducing human annotations. Traditionally, SSL\napproaches have been developed under the single-modality\nsetup for image [4, 5], text [6, 7], or audio/speech [8, 9, 10]\nindependently. However, there is a growing interest in repre-\nsentation learning across multiple modalities [11, 12, 13, 14],\nwhich brings both challenges and exciting new possibili-\nties. One breakthrough is Contrastive Language-Image Pre-\ntraining (CLIP) [11] which projects text and image embed-\ndings into a shared latent space, enabling applications like\ncross-modality retrieval and automatic captioning.\nMore\nrecently, Contrastive Language-Audio Pre-training (CLAP)\n[15, 16] learns representations for both text and audio and\ndelivered strong performance on audio-text retrieval tasks.\nThe key ingredients in CLIP and CLAP are their SSL ob-\njectives and model architectures. On objective, both CLIP\nand CLAP utilize contrastive learning, which aims to mini-\nmize the distance between embeddings in different modalities\nof the same instance, while differentiating the embeddings\nfrom different instances [17, 18, 19].\nOn model architec-\nture, both CLIP and CLAP adopted Transformer-like models\n[20], which have proven to be effective. Previous studies sug-\ngest this transformer+contrastive learning combination pro-\nduces high-quality embeddings for both uni-modal [4, 18, 21]\nand multi-modal [14, 22, 23] tasks. One major limitation of\nTransformer-like models is their quadratic complexity with\nrespect to sequence lengths, which becomes a computational\nbottleneck and restricts overall efficiency.\nTo improve computational efficiency, techniques with\nMasked AutoEncoders (MAE) such as image MAE [5],\nVideoMAE [24, 25] and AudioMAE [8] were recently pro-\nposed and achieved significant efficiency wins with minor\nperformance trade-off. Recently, Fast Language-Image Pre-\ntraining (FLIP) [26] applied similar techniques to image-text\nSSL. Recognizing that audio signals in nature are continuous\nand with variable in lengths, we explored the masking strate-\ngies for self-supervised language-audio representation learn-\ning. We term our model Fast Language-Audio Pre-training\n(FLAP). FLAP endeavors to establish aligned audio and lan-\nguage representations by incorporating masking, contrastive\nlearning and reconstruction techniques. For language-audio\ndatasets, very often the audio signals contain much richer\ninformation than the text counterparts. For example, an audio\nsegment of dog barking may reveal additional information\nsuch as volume and frequency, which are often missing in\nthe text. Also, text descriptions can vary in writing styles\nand generate inconsistent embeddings for the same seman-\ntics. Given such imbalanced information richness between\naudio and text, we utilize large language models (LLMs)\n[27, 28, 29] to enrich and unify the writing style for texts in\nthe language-audio task.\nPrevious works [16, 30, 31, 32] on language-audio pre-\ntraining received wide research interests.\nRecently, large-\nscale CLAP (LS-CLAP) [16] demonstrated strong results on\naudio-text retrieval on AudioCaps and Clotho benchmarks. In\nthis study, we further improve the LS-CLAP results by 1) us-\ning Masked Audio-Video Learners (MAViL) as pre-trained\naudio encoder 2) efficient masking for efficiency and robust-\nness 3) adding audio reconstruction for better embedding 4)\nutilizing LLMs for text augmentation. We observed signifi-\ncant performance boosts from FLAP, which outperformed the\nrecently proposed state-of-the-art systems [16].\narXiv:2311.01615v1  [cs.SD]  2 Nov 2023\nFig. 1. The architecture of FLAP, including audio/text encoders, efficient masking and audio reconstruction.\n2. CONTRASTIVE LEARNING\nThe fundamental framework of contrastive learning involves\nselecting an \u201canchor\u201d data sample, a data point from the same\ndistribution referred to as the \u201cpositive\u201d sample, and a data\npoint from a different distribution known as the \u201cnegative\u201d\nsample. Contrastive learning aims to reduce the distance be-\ntween the anchor and positive samples, which are part of the\nsame distribution, in the latent space. Simultaneously, it seeks\nto maximize the distance between the anchor and the negative\nsamples. For learning aligned audio and text representations,\nthe \u201cpositive\u201d examples refers to the representations of paired\naudio and text samples (i.e., an audio and its corresponding\ncaptions), while the negative examples are all the combina-\ntions of the unpaired audios and captions sampled in a batch.\nIn this work we employ the InfoNCE [33] loss for inter-modal\ncontrastive learning over audio and text pairs sampled from\na dataset (a, t) \u2208 D. Let a and t respectively denote the\ninstance-level audio and text representations. The InfoNCE\nloss Lc(a, t) is defined as:\nLc(a, t) = \u2212 1\nB\nB\nX\ni=1\nlog\nexp(S(ai, ti)/\u03c4)\nPB\nj=1 exp(S(ai, tj)/\u03c4))\n,\n(1)\nwhere S(ai, tj) =\naT\ni tj\n\u2225ai\u2225\u2225tj\u2225 is the cosine similarity between\nai, tj and \u03c4 is the softmax temperature. In Eq 1, the loss func-\ntion encourages the distance between the embeddings from\naudio and text from the same sample to be minimized and\nto be maximized from different samples, therefore achieving\nthe desired \u201dcontrasting\u201d effects. It is worth noting that the\nperformance of contrastive learning depends highly on the\nnumber of samples (B) being contrasted against each other\nwithin the same batch. Larger batch sizes (B) offers more\ninter-sample connections to stabilize the aggregated gradients\nfor updating model parameters, with increased need of com-\nputation and memory consumption.\n3. FLAP: EFFICIENT MASKING\nInspired by the recent success of FLIP[26] which attempts to\nemploy the masking technique for learning image-text rep-\nresentations, we propose Fast Language-Audio Pre-training\n(FLAP) for learning self-supervised audio-language represen-\ntations by employing masking for both contrastive learning\nand reconstruction. As depicted in Fig. 1, FLAP consists of\nan audio encoder, a text encoder, and audio decoder.\nFor\nFLAP\u2019s audio encoder, we adopt the audio backbone from\nMAViL [22], the SoTA audio model pre-trained on the audio\nand video clips of AudioSet [34]. MAViL is a self-supervised\nframework for learning audio-video representations that com-\nprises two stages. In the first stage, MAViL simultaneously\nlearns to reconstruct spectrogram and pixels, leveraging the\ncomplementary information from both modalities. In the sec-\nond stage, MAViL performs self-distillation where a student\nmodel predicts the contextualized features generated by the\nfirst-stage teacher model.\nFLAP performs instance-wise inter-modal contrastive\nlearning using Eq. 1 over the non-masked (visible) portion\nof audio spectrogram tokens. The masking strategy in FLAP\nsignificantly enhances the computation efficiency and pro-\nmotes more robust representation as masking can also be\nviewed as a data augmentation approach over the audio to-\nkens. Specifically, given a input tensor of shape (B, N, D),\nwhere B is the batch size, N is the sequence length, D is\nthe embedding dimension, masking reduces the shape to\n(B, N \u2032, D), where N \u2032 is smaller than N. This enables sig-\nnificant computation reduction for Transformer-like models\nas the model complexity grows quadratically with sequence\nlength (i.e. O(N 2)).\nWe investigated two masking strategies, namely 1-D and\n2-D masking, as illustrated in Fig. 2. Before masking, the in-\nput (in the form of mel-spectrogram) is transformed into patch\nembeddings.\nFor 1-D masking, the input tensor of shape\n(B, N, D) is first augmented with positional embeddings and\nthen randomly sampled on the T-axis to become (B, N \u2032, D).\nThe random sampling is performed on a shuffled and per-\nframe basis to the desired length N \u2032. 1-D masking is simple\nand effective in boosting robustness by random frame drop-\nping and reducing computation along with sequence lengths\nN. On the other hand, 2-D masking aims to build a more\nstructured sampling strategy on top of 1-D masking. Instead\nof directly sampling on the N-axis, 2-D masking first splits\nFig. 2. Frame dropping by 1-D and 2-D Masking.\nthe N-axis into M groups, each having K = N/M consec-\nutive frames. Next, both the M groups and the K frames in\neach group are sampled individually in the same fashion as in\n1-D masking and reduced to M \u2032 and K\u2032 respectively. Finally,\nboth M \u2032 and K\u2032 are merged back together and becomes the\nnew N \u2032 = M \u2032\u2217K\u2032. 2-D masking essentially splits the overall\nsequence (N) into numerous (M) fine-grained segments (K),\ntherefore enables more structured sampling through both ho-\nmogeneous sampling and dropping in each fine-grained seg-\nments. Both 2-D and 1-D maskings can achieve similar ef-\nficiency improvement with different masking ratios. For ex-\nample, a 75% masking ratio on N leads to 25% (= 100% -\n75%) computation cost for 1-D masking, while 50% on M\nand 50% on K for 2-D masking also leads to 25% (= 50%\n* 50%). The masked tensors are then directly sent to the au-\ndio encoder for computing the output embeddings for each\nframe and then averaged across the N-axis for per-instance\nembeddings. These masking strategies are particularly use-\nful for contrastive learning tasks as the per-example outputs\nare more robust to frame dropping. In addition, reduced se-\nquence length by masking also enables larger batch sizes to\nfit in GPUs, which benefits contrastive learning as more pairs\nare involved in the loss function for a single batch. Further-\nmore, the masking strategy can be view as a type of audio\naugmentation (e.g. SpecAug [35]) that promotes robustness\nof the learned representations. The masking is applied during\nthe training stage and is disabled during evaluation.\n4. AUDIO RECONSTRUCTION\nTo bolster the robustness of the learned audio embeddings,\nwe further propose an additional objective that promotes the\nincorporation of audio information into the embeddings. This\ncan be achieved by tasking the model with reconstructing\nthe original audio spectrogram tokens using the per-sample\nembeddings.\nAs depicted in Fig.\n1, before being aggre-\ngated across sequence length to produce the per-sample\naudio embeddings, the per-frame embeddings (of shape\n(B, T \u2032, D)) is sent to an audio decoder for reconstructing\nthe mel-spectrogram.\nEmpirically, we observe that recon-\nstructing only the spectrogram but not the text tokens yields\nbetter performance. We employ vanilla Transformer blocks\nas the audio f \u22121\na\n(.) decoders. The encoder\u2019s outputs (amm are\nfirstly projected and padded with trainable [MASK] tokens.\nAfter restoring the original order (time-frequency for audio\nand space-time for video tokens), we add the decoders\u2019 (fixed\n2-D sinusoidal) positional embeddings and input the restored\nsequences into the decoders. At the top of the decoders, we\nincorporate linear heads to reconstruct the raw inputs. Specif-\nically, the decoder outputs for spectrogram reconstruction\nare denoted as \u02c6a = f \u22121\na\n(gav(fa(a\u2032))). For notation clarity,\nwe omit the [MASK] tokens and linear projection head. Let\n\u02c6ai, araw\ni\n\u2208 RHa\nraw; i = 1 . . . n denote the audio decoder\u2019s\noutput and the ground truth reference of the i-th masked\nspectrogram patch. In masked audio reconstruction, FLAP is\nself-supervised by minimizing the mean squared error (MSE)\nloss Lraw\nr\ndefined as:\nLraw\nr\n= 1\nn\nn\nX\ni=1\n(\u02c6ai \u2212 araw\ni\n)2\n(2)\nThe MSE loss from reconstruction is then weighted and\nadded to the final loss along with the contrastive loss. With\nreconstruction, the model is encouraged to preserve con-\ndensed information into per-sample embeddings, as these\nembeddings not only have to be close to their text domain\ncounterparts, but also useful in producing original inputs. It\nis worth noting that reconstruction does come with a trade-off\non efficiency and batch size, as the audio decoder requires\nnon-trivial computation and memory usage.\n5. ENRICHED AUGMENTATION BY LLM\nLearning audio-text representations faces an additional chal-\nlenge stemming from the scarcity of audio-text pairs in exist-\ning audio-text corpora. Collecting human annotations for au-\ndio is both expensive and non-scalable. To address this issue,\nwe present a novel approach that harnesses the power of large\nlanguage models (LLMs) and audio event detection models\n(AEDs) to augment the limited number of text descriptions\navailable for audio. Table 1 shows examples of the original\ntext descriptions from training data and the class list to cap-\ntion transformation. From the original text descriptions, it is\nclear that both the richness of information is behind the corre-\nsponding audio signals and the writing styles are inconsistent\nacross samples. To reinterpret and enrich the same semantic\nfor natural language, we leverage the power of LLMs [27, 36,\n29] to enhance the descriptiveness of the audio captions on\naudio-text datasets such as AudioCaps and Clotho, which only\ncontains weak and limited descriptive captions. We first em-\nploy off-the-shelf AED model (i.e., MAViL [22]) to detect the\naudio events within a sample. And then we exploits a LLM\n(i.e., Vicuna [36]) along with engineered prompts to combine\nOriginal\nLLM-only\nAED+LLM\nWind blows as waves crash\nagainst a shoreline.\nThe wind gusts while the waves\ncrash against the shoreline\nThe waves crash against the shoreline, with the sound of wind blowing and creating\nwind noise. The ocean is in motion, with the wind blowing strongly\nA man is speaking.\nA person is delivering a speech.\nA man\u2019s speech can be heard rustling the leaves in the wind.\nA loud siren whizzes past.\nA piercing siren blares by.\nAn ambulance (siren) with a loud siren sound whizzes pasts by.\nAn engine revving.\nA car\u2019s engine revving.\nAn accelerating car engine revving with a vroom sound.\nWater sound.\nThe sound of water.\nAn intermittent sound of water flowing from a tap or faucet.\nThe sound of a boat.\nA boat is making sound.\nA water vehicle, specifically a motorboat or speedboat, is moving at a rapid pace\nwith winds blowing.\nTable 1. Comparison between original captions and augmented captions generated by LLM and AED.\nthe classification outputs and the original caption to generate\nricher captions for samples in AudioCaps and Clotho. Vicuna\nis an open-source instruction-following model fine-tuned on\nthe Llama-7b model [37]. From the examples in Table 1, uti-\nlizing this model generates more grammatical captions that\nremain faithful to the audio events.\nTo enrich text captions with LLM and detected audio\nevents, we used the following prompt: \u201cDescribe a situation\nwith AED results sounds and combine it with the origi-\nnal caption together.\u201d A limitation of the Vicuna model is\nits tendency to add unnecessary details or ignore relevant la-\nbels when generating captions. By adding AED outputs and\noriginal captions into the prompt, we leveraged the in-context\nlearning ability of Vicuna to enrich captions. During training,\nthe same set of audio signals with text descriptions replaced\nwith generated captions are augmented to the datasets.\n6. EXPERIMENTS\n6.1. Datasets and Setup\nAcross all experiments, similar to LS-CLAP [16], we use Au-\ndioCaps, Clotho, and 5 other datasets (Freesound, Epidemic\nSound, BBC Sound Effects, Free To Use Sounds, Sonniss\nGame effects) for training, while AudioCaps [38] and Clotho\n[39] are used for evaluation. It is worth noting that compared\nto LS-CLAP, we drop AudioStock due to its unavailability\nand therefore the size of the dataset for training is smaller\nthan LS-CLAP. The evaluation sets are identical for fair com-\nparisons. We built experiments on top of the LS-CLAP [16]\ntoolkit and adopted fvcore [40] for efficiency analysis. Cross-\nmodality retrieval between audio and text is used for evalu-\nation of the quality of the embeddings. For text-audio (T-A)\nretrieval, given the text as query, the audio recordings in the\nevaluation set are ranked based on the cosine similarities be-\ntween text and audio embeddings. The same procedure ap-\nplies to audio-text (A-T) retrieval. Recalls at top 1, 5 and 10\n(R@1, R@5 and R@10) are reported as metrics for both tasks\non AudioCaps and Clotho datasets.\nFor experiments without feature fusion, depending on the\naudio length, we either randomly chunk 10 seconds from\nlonger audios or pad to 10 seconds for shorter ones to form\ninput data of uniform lengths. For feature extraction, 25ms\nwindow size and 10ms window shift were used to extract\nmel-spectrogram features with 128 mel-bins.\nFor experi-\nments with feature fusion enabled, we followed the same\nprocedure as LS-CLAP [16], where audios are either padded\nor strided to create global and local versions followed by\n2-D convolutions for merging.\nFor SpecAug [35], up to\n192 audio frames (e.g.\n1.92 seconds) and up to 48 mel-\nbins are randomly replaced with zeros for each sample. For\ntext embedding generation, the texts paired with audio data\nare tokenized with a capped length of 77. RoBERTa [7] is\nused as text encoder for all experiments to be consistent with\nLS-CLAP [16].\nThe Adam [41] optimizer with \u03b21 = 0.99, \u03b22 = 0.9 was\nused during model training. The learning rate starts with a\nwarm-up stage, peaks at 10\u22124 and was decayed on a cosine\nschedule until the target number of epochs (45) is reached.\nSince both masking or reconstruction affects GPU mem-\nory usage which translates to largest batch size allowed per\nGPU, we report results with similar batch sizes to the base-\nline (2304) and also results with larger batch sizes enabled\nby efficient masking but using the equivalent computational\nresources (i.e. the same number of GPUs).\n6.2. Results on Efficient Masking and Reconstruction\nTo evaluate the performance of efficient masking and recon-\nstruction, the experimental results are summarized in Table\n2, in which all results are without feature fusion. The re-\nsults from LS-CLAP [16] are listed in row 1 serving as the\nbaseline. In row 2, the audio encoder is replaced with the re-\ncent MAViL [22] model with audio-and-video self-supervised\npre-training that achieves state-of-the-art performance on au-\ndio classification tasks. Note that we simply train MAViL\nwith the contrastive loss Eq. 1 on audio-text datasets without\nmasking or reconstruction applied. The results validate that\nstronger audio-modal yields improved audio-text representa-\ntions in audio-text retrieval tasks. In rows 3 and 5, 1-D and\n2-D masking are applied with masking ratio selected from\nadditional ablation studies, 0.4 for 1-D and 0.2/0.2 for 2-D\nrespectively. For 2-D masking, we split the sequence into 64\n(e.g. N = 64) groups of 8 (e.g. K = 8) frames from patch\nembeddings of length 256.\nFrom the comparison, we ob-\nserved similar sequence length reduction from 1-D (1 - 0.4\n= 60%) and 2-D ((1 - 0.2) \u00d7 (1 - 0.2) = 64%). But 2-D\nmasking delivers better improvement due to more structured\nModel\nGlobal Batch Size\nMasking\nAudioCaps Eval.\nClotho Eval.\nT-A Retrieval\nA-T Retrieval\nT-A Retrieval\nA-T Retrieval\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\n(1) LS-CLAP[16]\n2304\n\u2013\n32.7\n68.0\n81.2\n43.9\n77.7\n87.6\n15.6\n38.6\n52.3\n23.7\n48.9\n59.9\n(2) FLAP\n2304 (36 x 64 GPUs)\n\u2013\n34.8\n70.0\n82.7\n49.0\n79.5\n88.7\n16.3\n41.4\n53.9\n23.0\n49.2\n61.4\n(3) FLAP\n2304 (36 x 64 GPUs)\n1-D: 0.4\n36.0\n70.5\n83.0\n49.0\n78.9\n89.2\n16.8\n40.7\n53.4\n23.9\n48.9\n61.2\n(4) FLAP (+recon)\n2304 (36 x 64 GPUs)\n1-D: 0.4\n36.7\n71.2\n83.3\n47.2\n81.9\n90.0\n15.6\n39.5\n51.9\n21.7\n50.6\n61.3\n(5) FLAP\n2304 (36 x 64 GPUs)\n2-D: 0.2/0.2\n37.5\n73.5\n84.6\n49.6\n82.3\n89.4\n17.2\n41.1\n52.8\n23.7\n48.7\n62.3\n(6) FLAP (+recon)\n2304 (36 x 64 GPUs)\n2-D: 0.2/0.2\n37.2\n73.0\n84.9\n50.3\n81.4\n90.0\n17.0\n41.2\n53.5\n22.4\n49.0\n62.7\n(7) FLAP\n4608 (72 x 64 GPUs)\n2-D: 0.2/0.2\n38.3\n73.6\n85.1\n50.6\n83.1\n91.2\n16.7\n41.5\n54.2\n23.0\n48.6\n62.9\nTable 2. Experimental results on masking type, masking ratio and audio reconstruction (without feature fusion).\nmasking strategy. Both 1-D and 2-D masking reduce mem-\nory usage and preserve room for additional operations. On\ntop of masking, audio reconstruction is applied with 4 layers\nof Transformer decoding layers with 4 heads and 512 em-\nbedding dimensions for each layer. The results with audio\nreconstruction are listed in rows 4 and 6. The reconstruction\nobjective encourages FLAP to capture more abstract concepts\nfrom audio context to represent and predict raw audio spec-\ntrograms, without replying on additional class labels. This\nresults in stronger audio-text retrieval performance on Audio-\nCaps. Alternatively, the memory saving from masking can be\nalso utilized to process more samples in a single batch instead\nof audio reconstruction. Doubling the batch size produces the\nresults in row 7. Compared with rows 5 and 6, increasing\nthe batch size improves the robustness of the contrastive ob-\njective. In Eq.1, the positive pairs are encouraged to contrast\nagainst a larger collection of negative samples in the denom-\ninator, resulting in more well-aligned audio-text latent space\nwhere semantically correlated audio-text pairs are closer to\neach other and uncorrelated ones are distant. For contrastive\nlearning, sufficiently large batch size is crucial to the model\nperformance. It is worth noting that the number of GPUs are\nkept the same across the comparisons and larger batch size is\nachieved through efficient masking, which not only improves\nthe robustness of the model but also reduces computation and\nmemory footprints.\n6.3. Efficiency Analysis of 1-D/2-D Masking\nMasking provides benefits including bringing down the se-\nquence length for efficiency and improvement on model ro-\nbustness. However, similar to many efficiency-focused ap-\nproaches, the typical efficiency/performance tradeoff also ap-\nplies here. To analyze the correlation between masking ra-\ntios and the impact on model performance, models with dif-\nferent masking strategies and incremental masking ratios are\ntrained and compared in operational curves in Fig. 3, with\nAudioCaps results on top and Clotho results at bottom. In the\noperational curves, the computation complexity (in terms of\nGFLOPs) serves as the horizontal axis while the top 1 recall in\nretrieval (in terms of R@1) serves as the vertical axis. We also\nannotate each data point with (masking ratio, R@1) for eas-\nier numerical comparison. The GFLOPs are calculated using\nFig. 3. Text-Audio R@1 vs. GFLOPs on AudioCaps and\nClotho with Different Ratios for 1-D and 2-D Masking.\nthe fvcore [40] tool for the audio encoder only with a batch\nof 8 samples of 10-second lengths. The batch size was kept\nthe same for all masking ratios for fair comparison. The base-\nline results with no masking are also included at the rightmost\npositions in Fig. 3.\nFor each dataset, 1-D and 2-D masking are compared with\nincremental masking ratios. The masking ratios are on a per-\ndimension basis, meaning for the same masking ratio, 2-D\nmasking presents more aggressive frame dropping. For exam-\nple, when masking ratio is 0.3, 1-D masking preserves 70% of\nthe sequence while 2-D masking only preserves 49% (= 0.7\n* 0.7). From the curves in Fig. 3, efficient masking started\nimproving model robustness until too many frames in the se-\nquence are dropped (around 0.5). This is expected as infor-\nmation loss is increased along with the masking ratio. In addi-\ntion, similar to the observation in Table 2, 2-D masking pro-\nvides better recalls around similar GFLOPs therefore offers\nbetter trade-off than 1-D masking for being more structured.\nTaking masking ration = 0.2 for example, 2-D masking ap-\nproximately saves 25% of the computation and delivers better\nrecalls than result without masking. This shows that the ef-\nficient masking is effective in both improving efficiency and\nmodel robustness.\nModel\nFeature\nFusion\nBatch\nSize\nAudioCaps Eval.\nClotho Eval.\nT-A Retrieval\nA-T Retrieval\nT-A Retrieval\nA-T Retrieval\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\n(1) LS-CLAP[16]\nNo\n2304\n32.7\n68.0\n81.2\n43.9\n77.7\n87.6\n15.6\n38.6\n52.3\n23.7\n48.9\n59.9\n(2) FLAP\n2304\n37.5\n73.5\n84.6\n49.6\n82.3\n89.4\n17.2\n41.1\n52.8\n23.7\n48.7\n62.3\n(3) FLAP (+recon)\n2304\n37.2\n73.0\n84.9\n50.3\n81.4\n90.0\n17.0\n41.2\n53.5\n22.4\n49.0\n62.7\n(4) FLAP\n4608\n38.3\n73.6\n85.1\n50.6\n83.1\n91.2\n16.7\n41.5\n54.2\n23.0\n48.6\n62.9\n(5) FLAP (+LLM-aug)\n4608\n40.4\n74.7\n85.0\n51.5\n82.5\n92.5\n17.4\n41.3\n53.7\n21.6\n51.2\n63.1\n(6) LS-CLAP[16]\nYes\n2304\n36.2\n70.3\n82.5\n45.0\n76.7\n88.0\n17.2\n42.9\n55.4\n24.2\n51.1\n66.9\n(7) FLAP\n2304\n38.6\n74.2\n85.6\n49.6\n83.8\n91.1\n17.3\n43.1\n55.7\n24.4\n53.2\n66.4\n(8) FLAP (+recon)\n2304\n40.1\n74.8\n86.0\n50.8\n81.9\n91.0\n17.8\n44.0\n56.3\n24.6\n53.0\n66.7\n(9) FLAP\n4608\n39.9\n75.4\n86.6\n50.6\n81.7\n91.9\n17.5\n43.4\n56.0\n24.4\n52.1\n67.1\n(10) FLAP (+LLM-aug)\n4608\n41.5\n75.5\n86.0\n53.0\n84.1\n92.6\n20.3\n46.5\n58.8\n25.5\n53.4\n67.9\nTable 3. Experimental Results on Feature Fusion and Text Augmentation with Large Language Models (LLM). *NOTE: FLAP\nuses the same dataset as LS-CLAP [16], excluding AudioStock due to its unavailability.\n6.4. Results on Feature Fusion and LLM Augmentation\nThe setup without feature fusion in LS-CLAP adds padding\nfor shorter audio signals and applies random cropping for\nlonger ones to generate inputs to the model of uniform lengths\nof 10 seconds. It works well for feeding long audio signals to\nthe audio encoder without increasing the computational com-\nplexity. However, random cropping also implies information\nloss. Therefore, feature fusion [16] was introduced to further\nenhance the final retrieval performance and achieved signifi-\ncant improvements. To evaluate FLAP on the same setup, we\nadopted the same feature fusion and the corresponding results\nare listed in Table 3. In Table 3, results without feature fusion\nare listed in rows 1 to 5 and results with feature fusion are in\nrows 6 to 10. Rows 1 to 5 share same setups in Table 2, where\nrow 1 is the same CLAP baseline, row 2 is the 2-D masked\nMAViL with ratio 0.2, row 3 incorporates reconstruction loss\non top of row 2, row 4 doubles the batch size compared with\nrow 2 and row 5 augments LLM-generated text descriptions\non top of row 4. Rows 6 to 10 repeats the same setups as rows\n1 to 5 except inputs with feature fusion were used.\nCompared with rows 1 to 5, rows 6 to 10 are effectively\nimproved with feature fusion, as feature fusion combines\nglobal and cropped segments as inputs to the model. This\nbenefits more for long audio signals, as observed from the\nlarger improvements on Clotho, which contains more audio\nsegments longer than 10 seconds. Comparing rows 7-10 to\nrow 6, FLAP delivers similar performance improvement for\nfeature fusion setups similarly to rows 2-5. This demonstrates\nthat FLAP is highly versatile and adds complementary gains\non top of the already competitive feature fusion results. In\nrows 5 and 10, LLM augmentation mentioned in section 5\nis also applied on top of the best models to demonstrate the\nimpact from enriched and more consistent text descriptions.\nCompared with rows 4 and 9, results with augmentation from\nLLM-generated text descriptions show either similar or better\nperformance. Particularly, the results on Clotho with feature\nfusion showed larger improvement. Since the enriched text\ndescription tends to be longer as observed from examples in\nTable 1, feature fusion setups potentially benefit more for bet-\nter audio-text match and alignment. Rows 5 and 10 also serve\nas the best results with and without feature fusion for the pro-\nposed FLAP framework. Compared with CLAP, combining\nefficient masking which leads to increased batch sizes along\nwith enriched text description by LLMs yields significant\nimprovements across both text-audio and audio-text retrieval\ntasks on both datasets. For top 1 recall (R@1), FLAP in row\n5 without feature fusion performs better on majority of tasks\nthan the previous best results with feature fusion in row 6\n(36.2 to 40.4 for text-audio and 45.0 to 51.5 for audio-text on\nAudioCaps, 17.2 to 17.4 for text-audio on Clotho, with excep-\ntion on 24.2 to 21.6 for audio-text on Clotho). On the same\nfeature fusion setup, FLAP in row 10 further outperforms\nthe previous best results in row 6 on all tasks (36.2 to 41.5\nfor text-audio and 45.0 to 53.0 for audio-text on AudioCaps,\n17.2 to 20.3 for text-audio and 24.2 to 25.5 for audio-text on\nClotho). To the best of our knowledge, these results also serve\nas the current best performances on audio-text and text-audio\nretrieval tasks for AudioCaps and Clotho.\n7. CONCLUSION\nIn this paper, we introduce Fast Language-Audio Pre-training\n(FLAP) where contrastive learning meet masking.\nFLAP\nleads to better audio understanding, task performance, and\nenables efficient and effective learning on sequence modali-\nties such as audio and video. In addition, audio reconstruction\nand enriched text description augmentation by large language\nmodels (LLMs) are also investigated. Efficient masking re-\nduces both computation and memory footprint for training\nsamples, therefore enables larger batch sizes for contrastive\nlearning. Text augmentation from LLMs further enriches the\ntext descriptions for audio signals and produces more consis-\ntent writing styles. Combining both, FLAP delivers strong\nperformance on audio-text retrieval tasks with evaluation\non AudioCaps and Clotho benchmarks. The techniques in\nFLAP are versatile and applicable to representation learning\nin sequence modalities such as text, audio and video.\n8. REFERENCES\n[1] Yoshua Bengio, Aaron C. Courville, and Pascal Vincent,\n\u201cUnsupervised feature learning and deep learning: A re-\nview and new perspectives,\u201d CoRR, vol. abs/1206.5538,\n2012.\n[2] Linus Ericsson, Henry Gouk, Chen Change Loy, and\nTimothy M. Hospedales, \u201cSelf-supervised representa-\ntion learning: Introduction, advances and challenges,\u201d\nCoRR, vol. abs/2110.09327, 2021.\n[3] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin\nRiedmiller, and Thomas Brox, \u201cDiscriminative unsu-\npervised feature learning with convolutional neural net-\nworks,\u201d\nin Advances in Neural Information Process-\ning Systems, Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27,\nCurran Associates, Inc.\n[4] Xinlei Chen, Saining Xie, and Kaiming He, \u201cAn empir-\nical study of training self-supervised vision transform-\ners,\u201d in ICCV, 2021.\n[5] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll\u00b4ar, and Ross Girshick, \u201cMasked autoencoders\nare scalable vision learners,\u201d in CVPR, 2022.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, \u201cBERT: pre-training of deep bidi-\nrectional transformers for language understanding,\u201d in\nProc. NAACL-HLT, 2019.\n[7] Yinhan Liu, Myle Ott, and Naman Goyal et al.,\n\u201cRoBERTa: A robustly optimized BERT pretraining ap-\nproach,\u201d CoRR, vol. abs/1907.11692, 2019.\n[8] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,\nMichael Auli, Wojciech Galuba, Florian Metze, and\nChristoph Feichtenhofer,\n\u201cMasked autoencoders that\nlisten,\u201d in Advances in Neural Information Processing\nSystems, 2022.\n[9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-\nman Mohamed, \u201cHuBERT: self-supervised speech rep-\nresentation learning by masked prediction of hidden\nunits,\u201d IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 29, pp. 3451\u20133460, 2021.\n[10] Ashish Jaiswal,\nAshwin Ramesh Babu,\nMoham-\nmad Zaki Zadeh, Debapriya Banerjee, and Fillia Make-\ndon, \u201cA survey on contrastive self-supervised learning,\u201d\nTechnologies, vol. 9, no. 1, pp. 2, 2020.\n[11] Alec Radford, Jong Wook Kim, and Chris Hallacy et al.,\n\u201cLearning transferable visual models from natural lan-\nguage supervision,\u201d CoRR, vol. abs/2103.00020, 2021.\n[12] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettlemoyer,\nand Christoph Feichtenhofer, \u201cVideoCLIP: Contrastive\npre-training for zero-shot video-text understanding,\u201d in\nProceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, 2021, pp. 6787\u2013\n6800.\n[13] Po-Yao Huang, Mandela Patrick, Junjie Hu, Gra-\nham Neubig, Florian Metze, and Alexander Haupt-\nmann, \u201cMultilingual multimodal pre-training for zero-\nshot cross-lingual transfer of vision-language models,\u201d\nin Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021, pp.\n2443\u20132459.\n[14] Mandela Patrick, Po-Yao Huang, Ishan Misra, Florian\nMetze, Andrea Vedaldi, Yuki M. Asano, and Jo\u02dcao F.\nHenriques,\n\u201cSpace-time crop & attend:\nImproving\ncross-modal video representation learning,\u201d\nin 2021\nIEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2021, Montreal, QC, Canada, October 10-\n17, 2021. 2021, pp. 10540\u201310552, IEEE.\n[15] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-\nmail, and Huaming Wang, \u201cCLAP: learning audio con-\ncepts from natural language supervision,\u201d in ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE,\n2023, pp. 1\u20135.\n[16] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov, \u201cLarge-scale\ncontrastive language-audio pretraining with feature fu-\nsion and keyword-to-caption augmentation,\u201d\nin IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing, ICASSP, 2023.\n[17] R. Hadsell, S. Chopra, and Y. LeCun, \u201cDimensionality\nreduction by learning an invariant mapping,\u201d in 2006\nIEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR\u201906), 2006, vol. 2, pp.\n1735\u20131742.\n[18] Ting Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton,\n\u201cA simple framework for con-\ntrastive learning of visual representations,\u201d CoRR, vol.\nabs/2002.05709, 2020.\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick, \u201cMomentum contrast for unsupervised\nvisual representation learning,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 9729\u20139738.\n[20] Ashish Vaswani, Noam Shazeer, and Niki Parmar et al.,\n\u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017.\n[21] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli, \u201cwav2vec 2.0: A framework for self-\nsupervised learning of speech representations,\u201d in Ad-\nvances in Neural Information Processing Systems, 2020.\n[22] Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali,\nHaoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh,\nJitendra Malik, and Christoph Feichtenhofer, \u201cMAViL:\nmasked audio-video learners,\u201d in Advances in Neural\nInformation Processing Systems, 2023.\n[23] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and\nJuan Pablo Bello, \u201cWav2CLIP: learning robust audio\nrepresentations from CLIP,\u201d in Proc. ICASSP, 2022.\n[24] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang,\n\u201cVideoMAE: masked autoencoders are data-efficient\nlearners for self-supervised video pre-training,\u201d\nAd-\nvances in neural information processing systems, vol.\n35, pp. 10078\u201310093, 2022.\n[25] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and\nKaiming He, \u201cMasked autoencoders as spatiotemporal\nlearners,\u201d in NeurIPS, 2022.\n[26] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Fe-\nichtenhofer, and Kaiming He,\n\u201cScaling language-\nimage pre-training via masking,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 23390\u201323400.\n[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al., \u201cLLaMA: open and efficient foundation\nlanguage models,\u201d\narXiv preprint arXiv:2302.13971,\n2023.\n[28] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Sto-\nica, and Eric P. Xing, \u201cVicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality,\u201d https:\n//lmsys.org/blog/2023-03-30-vicuna/,\n2023.\n[29] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto,\n\u201cStanford alpaca: An\ninstruction-following\nllama\nmodel,\u201d\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca,\n2023.\n[30] Soham Deshmukh, Benjamin Elizalde, and Huaming\nWang, \u201cAudio retrieval with wavtext5k and clap train-\ning,\u201d arXiv preprint arXiv:2209.14275, 2022.\n[31] A.S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata,\nand S. Albanie, \u201cAudio retrieval with natural language\nqueries: A benchmark study,\u201d in IEEE Transactions on\nMultimedia, 2022.\n[32] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D.\nPlumbley, and Wenwu Wang,\n\u201cOn metric learning\nfor audio-text cross-modal retrieval,\u201d\narXiv preprint\narXiv:2203.15537, 2022.\n[33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, \u201cRep-\nresentation learning with contrastive predictive coding,\u201d\narXiv preprint arXiv:1807.03748, 2018.\n[34] Jort F. Gemmeke, Daniel P. W. Ellis, and Dylan Freed-\nman et al., \u201cAudioSet: an ontology and human-labeled\ndataset for audio events,\u201d in Proc. ICASSP, 2017.\n[35] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng\nChiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le,\n\u201cSpecAugment: a simple data augmentation method\nfor automatic speech recognition,\u201d\narXiv preprint\narXiv:1904.08779, 2019.\n[36] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica,\n\u201cJudging LLM-\nas-a-judge with MT-bench and chatbot arena,\u201d CoRR,\nvol. abs/2306.05685, 2023.\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample,\n\u201cLLaMA: open and\nefficient foundation language models,\u201d arXiv preprint\narXiv:2302.13971, 2023.\n[38] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,\nand Gunhee Kim, \u201cAudioCaps: generating captions for\naudios in the wild,\u201d in Proc. NAACL-HLT, 2019.\n[39] Konstantinos Drossos, Samuel Lipping, and Tuomas\nVirtanen,\n\u201cClotho: an audio captioning dataset,\u201d\nin\nProc. ICASSP, 2020.\n[40] Meta AI,\n\u201cfvcore:\nCollection of common code\nthat\u2019s shared among different research projects in fair\ncomputer vision team.,\u201d https://github.com/\nfacebookresearch/fvcore.\n[41] Diederik P Kingma and Jimmy Ba, \u201cAdam: A method\nfor stochastic optimization,\u201d in Proc. ICLR, 2014.\n"
  },
  {
    "title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
    "link": "https://arxiv.org/pdf/2311.01767.pdf",
    "upvote": "16",
    "text": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint\nTask Completion\nYiduo Guo1\u2217, Zekai Zhang1\u2217, Yaobo Liang2, Dongyan Zhao1, Nan Duan2\n1Peking University\n2Microsoft Research Asia\nyiduo@stu.pku.edu.cn,justinzzk@stu.pku.edu.cn,yaobo.liang@microsoft.com\nzhaody@pku.edu.cn,nanduan@microsoft.com\nAbstract\nRecent evaluations of Large Language Models\n(LLMs) have centered around testing their zero-\nshot/few-shot capabilities for basic natural lan-\nguage tasks and their ability to translate instruc-\ntions into tool APIs. However, the evaluation\nof LLMs utilizing complex tools to finish multi-\nturn, multi-modal instructions in a complex\nmulti-modal environment has not been inves-\ntigated. To address this gap, we introduce the\nPowerPoint Task Completion (PPTC) bench-\nmark to assess LLMs\u2019 ability to create and edit\nPPT files based on user instructions. It con-\ntains 279 multi-turn sessions covering diverse\ntopics and hundreds of instructions involving\nmulti-modal operations. We also propose the\nPPTX-Match Evaluation System that evaluates\nif LLMs finish the instruction based on the pre-\ndiction file rather than the label API sequence,\nthus it supports various LLM-generated API\nsequences. We measure 3 closed LLMs and\n6 open-source LLMs. The results show that\nGPT-4 outperforms other LLMs with 75.1%\naccuracy in single-turn dialogue testing but\nfaces challenges in completing entire sessions,\nachieving just 6% session accuracy. We find\nthree main error causes in our benchmark: er-\nror accumulation in the multi-turn session, long\nPPT template processing, and multi-modality\nperception. These pose great challenges for fu-\nture LLM and agent systems. We release the\ndata, code, and evaluation system of PPTC at\nhttps://github.com/gydpku/PPTC.\n1\nIntroduction\nRecent evaluation works for Large Language Mod-\nels (e.g. ChatGPT and GPT-4 (OpenAI, 2023)) fo-\ncus on their zero-shot/few-shot abilities on basic\nnatural language tasks (Jiao et al., 2023; Zhong\net al., 2023; Wang et al., 2023b; Qin et al., 2023a)\nand their tool-use ability to generate APIs for solv-\ning user instructions, such as basic APIs like a\ncalculator in tool transformer (Schick et al., 2023),\n*Equal contribution\nFigure 1: Within our benchmark, we simulate this multi-\nturn dialogue scenario between humans and LLMs to\nevaluate LLMs\u2019 PPT task completion performance.\nRapidAPIs in ToolLLM (Qin et al., 2023c), and\nhugggingface APIs in Gorilla (Patil et al., 2023).\nHowever, these tool-use works emphasize the trans-\nlation of natural language instructions into APIs\nand ignore the challenge of using APIs in the ob-\nservation of complex multi-modal environments\nto finish user instructions. Also, their evaluation\napproach focuses on comparing the generated APIs\nwith the label API sequence, assuming there\u2019s only\none unique solution. This approach becomes im-\npracticable in situations with multiple/unlimited\ncorrect solutions. To address these challenges, we\nintroduce Power-Point Task Completion (PPTC),\na benchmark that measures LLMs\u2019 performance in\ncreating and editing PPT file tasks based on user\ninstructions. We choose PowerPoint as it includes\nvarious elements like textbox, table, and image and\nsupports a wider range of APIs than Word and Ex-\ncel.\nOur benchmark has three distinctive features\narXiv:2311.01767v2  [cs.CL]  7 Nov 2023\nFigure 2: We illustrate how LLMs complete one turn in a session. (A) To prompt the LLM, we provide it with\nthe current instruction, previous instructions (dialogue history), PPT file content, and the API reference file. \u2019PPT\nreader\u2019 is a function that transforms the PPT file into the text-based format as the PPT file content. (B) The LLM\nthen generates the API sequence and executes it to obtain the prediction PPT file. (C) We evaluate attributes and\nposition relations in the prediction file.\nfrom other task completion benchmarks: (1) Multi-\nturn dialogue with varying difficulty. Our PPTC\nbenchmark simulates the multi-turn dialogue ses-\nsion between the user and the LLM (see Figure 1)\nand contains 279 multi-turn sessions. Each multi-\nturn session in our benchmark includes 2 to 17\nturns. Each turn consists of a user instruction that\ndescribes the user\u2019s needs, a feasible solution that\nprovides the correct solution, and the resulting label\noutput file. Some turns can be easily addressed us-\ning a single API, while over half of the instructions\nrequire multiple APIs for completion. We provide\nthe LLM with a reference API file that contains all\nfeasible APIs for selection. (2) Multi-modality.\nFinishing the instruction of our benchmark requires\nunderstanding the multi-modal PPT file content\nand using multi-modal API operations (e.g., PPTC\nhas 268 image operation-related instructions). (3)\nEvaluation based on the final status: We propose\nthe PPTX-Match Evaluation system to evaluate the\nLLM\u2019s outcome. To identify if the LLM completes\nthe instruction, it checks the PPT file produced by\nexecuting the LLM-generated APIs rather than the\nLLM-generated APIs, thus all API sequences that\nlead to the correct final status are acceptable.\nTo finish the instruction, we use the current in-\nstruction, past turns\u2019 instructions (dialogue history),\nthe PPT file content (specific environment informa-\ntion), and the reference API file as the input to\nprompt the LLM to generate an API sequence as\nthe solution (See Figure 2 (A)). Then we use the\nAPI executor to execute the API sequence and re-\nturn the user the resulting PPT file (See Figure 2\n(B)). We name the resulting PPT file as the predic-\ntion file. In the evaluation step (See Figure 2 (C)),\nthe PPTX-Match Evaluation system first uses the\nPython-PPTX library to extract all attributes from\nthe prediction PPT file and the label output file.\nThen it uses the position relation checker to check\nif objects\u2019 positions conform to the label relation\nand the attribute content checker to check if the at-\ntribute\u2019s content is matched with the corresponding\nlabel attribute\u2019s content. The LLM correctly com-\npletes the current turn\u2019s instruction if all attributes\nof the file pass these tests. Evaluation metrics in-\nclude turn-based accuracy which is the ratio of\ncorrectly completed turns to the total number of\nturns and session-based accuracy which is the ra-\ntio of correctly completed sessions to the overall\nsession count.\nWe measure the performance of three closed-\nsource LLMs (GPT-4, ChatGPT, and Davince-003)\nand six open-source LLMs (e.g., LLaMa-2) in\nour benchmark. We further test planning (e.g.,\nCoT (Wei et al., 2022)) and content selection al-\ngorithms\u2019 performance based on GPT-4. Experi-\nment results show that GPT-4 is the strongest LLM\namong all LLMs but still encounters challenges\nwhen completing entire multi-turn sessions. For ex-\nample, although GPT-4 achieves 75.1% turn-based\naccuracy in the creating new PPT file task, it only\nachieves 22.7% session-based accuracy as errors\nmade in previous turns. GPT-4 and other LLMs\nalso struggle to process long PPT templates (com-\nplex file environment). For example, GPT-4 only\nachieves 38.1% turn-based accuracy in the edit-\ning task. We further find that GPT-4 struggles to\nfinish instructions involving non-text modality op-\nerations, especially for position-related operations,\nsuch as \u2019Put object A on the top of the slide\u2019. It\nonly achieves 24% accuracy in these instructions.\nIn summary, this paper has the following contri-\nbutions:\n(1) We propose the PowerPoint Task Completion\nbenchmark to measure LLM\u2019s task completion per-\nformance within the PowerPoint official software.\nThis benchmark contains 279 multi-turn sessions\nwith hundreds of multi-modal instructions in the\ncomplex multi-modal environment.\n(2) We propose the PPTX-evaluation system to\nautomatically measure LLMs\u2019 performance in our\nbenchmark. We test 3 closed-source LLMs and\n6 open-source LLMs and find that GPT-4 is the\nstrongest LLM among all LLMs.\n(3) We further analyze LLMs in our benchmarks\nand find three key error factors: error accumulation\nin the session, long PPT template processing, and\nmulti-modality perception. These findings pose\nsignificant challenges for future LLMs and LLM-\nbased systems.\n2\nPPTC Benchmark\nIn this section, we introduce our Power-Point\nTask Completion (PPTC) benchmark, including\nthe overview of our benchmark, its collection and\nvalidation process, and the PPTX-Match Evalua-\ntion System for evaluation. We further analyze the\nstatistics information of our benchmark.\n2.1\nBenchmark Overview\nBenchmark components Our benchmark focuses\non two basic tasks within PowerPoint: creating the\nnew PPT file and editing the existing long PPT\ntemplate for measuring long PPT Content under-\nstanding. We have gathered 229 multi-turn dia-\nlogue sessions for creating the new PPT file and\n50 sessions for editing existing templates. Each\nmulti-turn session includes 2 to 17 turns. Each turn\ncomprises three parts: (1) the user instruction (2)\nthe label output file as the ground truth (3) one feasi-\nble API sequence for finishing the instruction. Our\nbenchmark also contains an API reference file that\nincludes 49 feasible APIs for various operations\nand can complete all instructions in our benchmark.\nFor each API, we describe its functionality and\narguments and provide usage guidelines. For com-\nplex APIs, we also offer example cases. We list the\ndetails of all APIs in Appendix A.\nTask description To complete the instruction in\none turn, in general, the AI assistant must compre-\nhend the user\u2019s current and prior instructions for\ncontext. It should also analyze the content of the\nPPT file to identify relevant objects mentioned in\nthe instruction. Additionally, it needs to select ap-\npropriate APIs from a reference API file to achieve\nthe user\u2019s goals. So we use these as the input of the\nAI assistant and it should output an API sequence\nas the solution. Then, it executes this API sequence\nand provides the user with the resulting PPT file as\nits response (See the whole process in Figure 2).\nAddressing LLM limitations in our bench-\nmark Compared to the general AI assistant, LLMs\nstill have two limitations for completing the task\nin our benchmarks: (1) LLMs can not directly pro-\ncess the PPT file. So we provide a PPT reader\nfunction that extracts all shapes and their informa-\ntion from the PPT file and transforms them into the\ntext format as the PPT file content. Then LLMs can\nunderstand and process the PPT file content. (2)\nLLMs cannot directly use PPT software through a\nkeyboard and mouse. Therefore, we have defined\nPPT APIs based on the operational logic within\nthe PPT software. and provide an implementation\nfor these APIs in Python that can swiftly gener-\nate PPT files. In future work, it may be possible\nto explore the use of large multimodal models to\nunderstand on-screen content and implement APIs\nusing a keyboard and mouse.\n2.2\nBenchmark Collection\nDesign Principles We follow these principles to\ndesign our benchmark: (1) Multi-turn instructions:\nOne session in our benchmark should contain multi-\nturn instructions to finish the user\u2019s complex need.\n(2) Instructions of varying difficulty: Some instruc-\ntions can be achieved with a single API, while\nothers necessitate a sequence of APIs for success-\nful completion. (3) Diverse multimodal operations:\nUser instructions should cover a wide range of op-\nerations on PPT, such as text-related, image-related,\nand position-related APIs. (4) Topic Consistency:\nThe dialogue in a session should center around the\nsession topic. Each user instruction in a session\naligns closely with the previous instructions (the\ncontext), ensuring a coherent and contextually rel-\nevant dialogue flow. (5) Practicability First: The\nsession topic and specific instructions should simu-\nlate the user\u2019s need in real world\nBenchmark Collection and Validation To col-\nlect user instructions, we engage 6 skilled crowd\nworkers who craft instructions in accordance with\nthe principles we\u2019ve outlined. Our crowd workers\ncomprise professional data science engineers well-\nversed in PowerPoint. To achieve practicability\nfirst, we request crowd workers to write instruc-\ntions based on their actual PowerPoint experience.\nOn each session, the workers are asked to first find\nand list a practicable session topic. For the edit-\ning PPT template task, the topic must based on the\ntemplate file background and is practicable to the\ntemplate*. To achieve multi-instructions and topic\nconsistency, the workers write instructions step by\nstep and make them consistent with the topic. To\nachieve diverse multi-operations, we ask them not\nto write session that only involves a single modality\noperation. Each worker takes on a specific role in\nthe instructional writing work and is encouraged to\nwrite instructions in his/her own words. Workers\nwere asked to spend at least 20 minutes on every\nsession. We delete repeated sessions and short ses-\nsions that have no more than 50 tokens.\nThen we ask the seventh worker to write the\n*We collect 50 PPT templates from the SlidesCarni-\nval website (https://www.slidescarnival.com/).\nSlidesCarnival is a free and open-source PPT template web-\nsite. Each session in the editing task has a unique template.\nWe encourage topic diversity in our templates. We remove\ntemplates that are too short (2\u223c5 slides) and have repeated\ntopics.\nfeasible API sequence with minimal API usage\nfor each instruction. Next, the workers create the\nPPT label file by using the provided API sequence.\nDuring the whole process, the principal engineer\nreviews and refines the instructions and API se-\nquences written by the above 7 workers for initial\nquality assurance.\nTo ensure the data quality of this benchmark, the\nthree authors of this paper further undertake the fol-\nlowing validation steps: (1) Assessing Instruction\nClarity and Relevance: They examine whether the\ninstructions are clear, contextually related to the\nsession topic, and align with the ongoing conver-\nsation. (2) API Sequence Execution: The authors\nexecute the provided API sequences to identify and\nrectify coding errors. (3) Goal Achievement Check:\nThey verify if the instructions\u2019 intended goals are\nsuccessfully completed in the label files.\nIn the event that errors are identified during this\nvalidation process, the authors promptly report\nthem to the respective workers for revision. The\nthree authors are computer science senior students\nand researchers.\n2.3\nPPTX-Match Evaluation System\nWe design the PPTX-Match Evaluation System to\nevaluate LLMs\u2019 performance on the PPTC bench-\nmark. Specifically, our PPTX-Match Evaluation\nSystem first uses a Python-PPTX Content Reader\nModule to iterate over all shapes in the prediction\nPPT file produced with the LLM and the label out-\nput file. A shape in the PPTX library typically\nrefers to an individual object, such as a text box\nor table. Then our system extracts attributes like\ntext, style, and position of the shapes using the\nPPTX library. Next, we check all attributes from\nthe prediction PPT file. For non-position attributes\n(e.g., text content), we first convert it and the cor-\nresponding attribute in the label PPT file into two\nstrings, and then we use the Exact Match method\nto examine if the two strings are the same. If they\nare different or we do not find the corresponding\nattribute in the label file, then we find an incorrect\nmatch. For the position attribute (e.g., location in-\nformation), we focus on checking if the objects in\nthe prediction PPT file satisfy the correct position\nrelation <A, B, REL>, where A and B are objects\nthat should satisfy the relation REL. In the bench-\nmark collection process, we ask crowd workers to\nlabel the position relation that objects should sat-\nisfy to finish the instruction. During the evaluation\nFigure 3: Statistics for PPTC. a) Session turn number distribution. b) Instruction API number distribution (tokens).\nc) Distribution of instructions involving Chart, Table, Picture, and Position. Instructions involving \u2019Position\u2019 need\nthe system to conduct position-related operations based on the understanding of spatial information. Note that one\ninstruction may involve multiple different modalities.\nphase, we extract the position attributes of objects\nA and B and use predefined functions to verify if\nthe objects\u2019 position attributes satisfy the position\nrelation.\nIf there are no incorrect matches for all non-\nposition attributes and no rule violations for all\nposition-related attributes, we consider the LLM\nhas successfully completed the user instruction.\n2.4\nBenchmark Statistics Analysis\nTo understand the properties of PPTC, we analyze\nthe instructions and APIs in the benchmark. Specif-\nically, we explore (i) the number of turns in a ses-\nsion, (ii) the difficulty of the instruction in terms of\nthe number of APIs required to finish it, and (iii) the\nnumber of multi-modality instructions. We report\nstatistics about the PPTC benchmark in Figure 3.\nThe number of turns in a session The session\nturn number distribution (Figure 3 (a)), measured\nas the number of turns in a session, shows that all\nsessions in our benchmark have at least two turns\nand almost all sessions have at least 3 turns (be-\ntween 3 and 13 turns for the 5th to 95th percentile,\nrespectively). The longest session has 17 turns,\nwhich is very challenging as the errors made in\nprevious turns can influence the completion of the\ncurrent instruction.\nDiffculty varies in APIs number The number\nof APIs in a sequence falls between 1 and 5 for\nthe 5th to 95th percentile (Figure 3 (b)), respec-\ntively, shows that our instructions\u2019 difficulty varies\nfrom a simple sentence that can be finished by one\nAPI to a complex instruction that requires the LLM\nto generate multiple APIs. The longest API se-\nquence consists of 29 APIs. Generating long API\nsequences is very challenging as the LLM needs\nto understand sub-goals in the complex instruction,\nselect appropriate APIs from the file, and generate\nAPIs in a reliable order.\nRich multi-modal instructions Our benchmark\nhas hundreds of instructions that involve multi-\nmodalities content (Figure 3 (c)).\nThe \"chart\"\nmodality has the fewest instructions, with 120,\nwhile the \"position\" modality has the most, with\n292 instructions.\nTo finish these instructions,\nLLMs need to employ related-modal APIs based\non the understanding of multi-modal file content.\n3\nAlgorithms\nIn this section, we introduce the algorithms we con-\nsidered to enhance the LLM\u2019s performance in our\nbenchmark. These algorithms can be categorized\ninto two approaches: planning algorithms that help\nthe LLM in decomposing the user instruction and\nsolving it step by step and selection algorithms that\nassist the LLM in choosing important environmen-\ntal information or APIs.\n3.1\nPlanning Algorithms\nComplex user instructions often require multiple\nintermediate steps to complete. We mainly consider\ntwo planning algorithms:\nZero-shot-CoT (Kojima et al., 2022) enables\nLLMs to autonomously generate intermediate\nreasoning processes for complex instruction by\nprompting LLMs to \"Let\u2019s think step by step\".\nTree of Thoughts (ToT) (Yao et al., 2023) en-\nables LLMs to follow tree-like reasoning paths,\nwhere each tree node represents a thinking state. It\nleverages LLMs to generate evaluations or votes\non different thoughts.\n3.2\nSelection Algorithms\nCombining the whole PPT file and the whole API\nfile into the LLM\u2019s input can result in an over-\nwhelming amount of redundant information, such\nas irrelevant file content and unhelpful APIs. Filter-\ning the redundant information would improve the\nefficiency of the LLM. In this context, we primarily\nfocus on two algorithms for selecting the PPT file\ncontent and APIs, respectively:\nContent Selection algorithm Firstly, we extract\nall shapes of the PPT file by Python-PPTX. Next,\nwe prompt the LLM to select the shapes for com-\npleting the user\u2019s instruction. We show the prompt\nin Figure 8, in which we add three demonstration\nexamples to guide the LLM to do selection. In\nthis algorithm, we replace the whole PPT file with\nthe selected shapes when prompting the LLM to\ngenerate the API sequence.\nAPI Selection algorithm The API selection al-\ngorithm is based on the embedding similarity to\nselect the most relevant APIs for user instructions.\nSpecifically, we use the text embedding API to get\nthe embeddings of all API descriptions and the cur-\nrent user instruction. Next, we compute the cosine\nsimilarity between the instruction embedding and\neach API description\u2019s embedding and rank them\nbased on the similarity score. In this algorithm, we\nreplace the whole reference API file with the top\nk APIs when prompting the LLM to generate the\nAPI sequence.\n4\nExperiments\n4.1\nLarge Language Models Selected for\nEvaluation\nHere, we assess different cutting-edge large lan-\nguage models using our benchmark. These chosen\nmodels showcase a wide array of capabilities and\nare highly regarded in the field. The evaluated large\nlanguage models include 3 closed-source LLMs\nand 6 open-source LLMs:\n\u2022 GPT-4 (OpenAI, 2023): The latest LLM in\nthe GPT series.\nGPT-4 is a cutting-edge,\nlarge-scale generative pre-trained transformer\nmodel. It offers improved performance and a\nwider knowledge base compared to its prede-\ncessors. It showcases human-level proficiency\nin several scenarios.\n\u2022 ChatGPT: ChatGPT is a conversational AI\nmodel crafted for dynamic interactions. It\u2019s\nlearned from extensive instruction data and\nfine-tuned through reinforcement learning\nwith human feedback (RLHF). This empow-\ners it to deliver responses that align with hu-\nman expectations, maintaining context and\ncoherence in conversations.\n\u2022 Text-Davinci-003 (Brown et al., 2020): GPT-\n3.5 sits between GPT-3 and GPT-4, enhancing\nperformance via additional instruction tuning.\nIt acts as a link between these models, facil-\nitating comparison. We\u2019ve chosen the Text-\nDavinci-003 variant from the GPT-3.5 series\nfor our evaluation.\n\u2022 LLaMa-2-Chat\n(Touvron\net\nal.,\n2023):\nLLaMa 2, an auto-regressive open-source lan-\nguage model, employs an optimized trans-\nformer design. Chat versions utilize super-\nvised fine-tuning (SFT) and reinforcement\nlearning with human feedback (RLHF) to\nmatch human preferences for helpfulness and\nsafety.\n\u2022 Baichuan-Chat: It is a transformer model\ntrained on approximately 1.2 trillion tokens.\nIt supports both Chinese and English, with a\ncontext window length of 4096.\n\u2022 Baichuan-2-Chat (Yang et al., 2023): It is\na large-scale multilingual language model\ntrained from scratch, on 2.6 trillion tokens.\nThe chat version uses Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Hu-\nman Feedback (RLHF) to align with humans.\n\u2022 WizardLM v1.2 (Xu et al., 2023a): WizardLM\nv1.2 is finetuned from LLaMa 2 using super-\nvised instruction fine-tuning, where instruc-\ntions are created by rewriting the initial in-\nstructions step by step.\n\u2022 Vicuna v1.5 (16k) (Chiang et al., 2023): Vi-\ncuna v1.5 (16k) is finetuned from LLaMa 2 us-\ning supervised instruction fine-tuning and lin-\near RoPE scaling. It\u2019s trained on about 125K\nconversations sourced from ShareGPT.com.\n\u2022 Code-LLaMa-instruct (Chiang et al., 2023):\nCode-LLaMa is a LLaMa-based LLM de-\nsigned for general code completion and un-\nderstanding. Its instruction version further\nsupports the chat function with users. Code\nLLaMa models feature a multitask training ob-\njective consisting of both autoregressive and\ncausal infilling prediction (predicting the miss-\ning part of a program given a surrounding con-\ntext).\n4.2\nExperimental Setup\nIn this section, we provide an overview of the ex-\nperimental setup utilized to assess the performance\nof LLMs on our PPTC benchmark.\n4.2.1\nTurn-Based and Session-Based\nEvaluations\nWe consider two performance evaluation ap-\nproaches in our benchmark: turn-based and session-\nbased evaluations. For the turn-based evaluation,\nwe measure the LLM\u2019s ability to finish a single turn.\nSpecifically, in this evaluation, we assume that the\nprevious turns have been correctly finished, and\nwe prompt the LLM to generate the API sequence\nto finish the current turn\u2019s user instruction. The\nprompt consists of the task instruction for finishing\nthe current user instruction, the API file contain-\ning feasible APIs, the parsed PPT file content from\nthe PPT file, and dialogue history consisting of\ninstructions of previous turns with their feasible\nAPI sequences (see the left of Figure 4). For the\nsession-based evaluation, we measure the LLM\u2019s\nability to finish a session containing multiple turns.\nFor all turns in a session, we prompt the LLM to\nfinish them sequentially. The prompt in this eval-\nuation has two differences: the API solutions for\nprevious turns in dialogue history are the outputs\nof the LLM instead of the correct API sequences.\n(2) The PPT content is parsed from the PPT file\nobtained by executing the previous outputs of the\nLLM (see the right of Figure 4). That means the\nerror made by LLMs in previous turns would influ-\nence subsequent turns.\nMetrics For turn-based evaluation, we report\nthe turn-based accuracy as the ratio of the number\nof successfully finished turns to the total number\nof turns. We also report the average token num-\nber of the input of one turn and the average API\nnumber for finishing one turn as the cost measure-\nment. For session-based evaluation, we report the\nsession-based accuracy as the ratio of the number\nof successfully finished sessions to the total num-\nber of sessions. We also report the average value of\nthe token number of all inputs in one session and\nthe average API number required to complete one\nsession as the cost measurement.\nInference prompt in PPTC\n(Task instruction) You are an AI assistant to help\nthe user to operate PowerPoint and edit the contents.\nGive you the user instruction:<Current user instruc-\ntion>, you can complete it based on the following\nAPIs and PPT file content. Current you are at page\n<Page id>. Please finish the user instruction with the\nfunctions you have. Don\u2019t generate instructions be-\nyond what the user has instructed. Don\u2019t guess what\nthe user may instruct in the next step and generete\nAPI for them. Don\u2019t use python loop to call API. You\ncan only call API once in one line. If the user does\nnot specify the page to be modified, you can directly\nstart using the APIs without having to navigate to\nother pages.\nYou need to generate code which can finish the user\ninstruction. The multiple lines of code should be\nsurrounded by <code> and </code> such as: <code>\nAPI(); API(); </code>\nFor example, if the user instruction is \"create a slide\",\nthen the answer should be:\n<code> create_slide(); </code>\n(API file) Now, you have access to a list of\nPowerPoint APIs with the following functions:\n<APIs and their descriptions>\n(e.g.,API(name=\"set_width\", parameters=\"(width)\",\ndescription=\"This API sets the width of the selected\nobject.\",\nparameter_description=\"It\ntakes\none\nparameter\n\u2019width\u2019, the width of an object in centimeters as\nfloat.\",\ncomposition_instruction=\"You should first choose an\nobject before you can change the width of it.\",\napi_desc=\"width of picture and shapes\") )\n(PPT file content) All the PPT contents are:\n<Begin of PPT>\nTurn-based: <Parsed PPT file content of the label\nPPT file of the previous turns>\nSession-based: <Parsed PPT file content of the LLM\nprediction file of the previous turns>\n<End of PPT>\n(Dialogue history)\n\u00acUser\u00ac: Hello!\n\u00acAI\u00ac: Hi there! How can I help you?\n\u00acUser\u00ac: <the first instruction>\n\u00acAI\u00ac:\nTurn-based: <the correct feasible API sequence>,\nSession-based: <the LLM-generated API sequence>\n...\n\u00acUser\u00ac: <Current user instruction>. Surrounding\nyour answer with <code> and </code>.\n\u00acAI\u00ac:\nFigure 4: The inference prompt we used in both turn-\nbased and session-based evaluation settings. In the turn-\nbased evaluation, we assess the LLM\u2019s performance\nfor the current turn and assume the LLM has correctly\nfinished previous turns. We then use feasible API se-\nquences of previous turns as the AI response in the dia-\nlogue history and parse the label file of previous turns\nas the PPT file content. In the session-based evaluation,\nwe evaluate the completion of the entire session and\ndo not assume the LLM has correctly finished previous\nturns. We use the LLM\u2019s generated API sequences as\nthe response and parsed the LLM prediction file as the\nPPT file content.\n4.3\nImplementation Details\nAll experiments were conducted using the respec-\ntive language models\u2019 API provided by Azure Ope-\nnAI Service\u2020. Azure OpenAI services offer two\nAPI types: completion and chat completion. Com-\npletion API generates text from prompts, while\nchat completion API responds based on conversa-\ntion history and new input. We use the completion\nAPI for Text-Davinci-003 and the chat completion\nAPI for ChatGPT and GPT-4. We set a temperature\nof zero for deterministic output and a max token\nlimit of 2048. The frequency penalty and top p are\nkept at their default values of zero and 1, respec-\ntively. We use the text-embedding-ada-002 API as\nthe embedding API in the API selection algorithm\nand set k as 15. For open-source LLMs, we choose\nthe chat version of LLaMa-2, the v1.2 version of\nWizardLM, and the chat version of Baichuan as\nour open-source LLMs. We choose the 13 billion\nparameters model of the three LLMs.\nFor the zero-shot CoT method, we add the sen-\ntence \u2019Let\u2019s think step by step\u2019 after the dialogue\nhistory of the prompt. For the ToT method, we\nfollow the official code to run it\u2021. We run the four\nalgorithm methods based on the GPT-4 model.\nIf the token number of the input prompt is be-\nyond the token limit, we cut the PPT file content to\nreduce the token number of the prompt.\n4.4\nMain results\nWe report the results of LLMs in both turn-based\nand session-based evaluations in Table 1 and 2.\nFrom the results, we highlight the following key\nfindings.\n(1) Superior Performance of GPT-4: GPT-4\nconsistently outperforms other closed-source and\nopen-source LLMs in both two tasks. Impressively,\nGPT-4 achieves 75.1% turn-based accuracy in the\ncreating new PPT file task, demonstrating its strong\ncapability to finish one turn of the user instruction.\nGPT-4 also has a lower API cost compared to other\nclosed-source LLMs since its precise API usage.\nGPT-4 incurs the highest token expense when edit-\ning PPT templates. That is because its higher token\nlimit than other LLMs allows us to input more PPT\ntemplate content.\n\u2020https://azure.microsoft.com/\nen-us/products/cognitive-services/\nopenai-service\n\u2021ToT:https://github.com/princeton-nlp/\ntree-of-thought-llm\n(2) Code continual pre-training and further\ninstruction finetuning can boost open-source\nLLMs\u2019 performance.: Based on Table 1, it\u2019s evi-\ndent that current open-source LLMs struggle to\nmatch the performance of closed-source LLMs.\nFor example, LLaMa-2-chat only achieves 16.2%\nturn-based accuracy in the creating new PPT file\ntask, which is far from the performance achieved\nby closed-source LLMs. We further find that code\ncontinual pretraining (Code-LLaMa) and instruc-\ntion fine-tuning based on LLaMa-2 (WizardLM\nand Vicuna) can further improve LLaMa-2 per-\nformance obviously. For example, Code-LLaMa\nimproves LLaMa-2\u2019s turn-based accuracy in the\ncreating new PPT file task by 20.4 %. This obser-\nvation suggests that there\u2019s untapped potential in\nopen-source LLMs when it comes to our bench-\nmark, and this potential can be unlocked further by\ncode pre-training and enhancing instruction follow-\ning ability.\n(3) Planning and selection algorithms can im-\nprove LLMs\u2019 turn-based performance From Ta-\nble 2, we observe that the planning algorithms (CoT\nand ToT) can further improve the turn-based per-\nformance of GPT-4 by 1\u223c2 percent. However, we\nsurprisingly find that the more complex ToT al-\ngorithm does not outperform the zero-shot CoT\nalgorithm with a 5\u223c10 times token cost. Content\nand API selection algorithms can further improve\nthe turn-based performance of GPT-4 by 1\u223c 5 per-\ncent. That is because they reduce the task difficulty\nby filtering irrelevant PPT content/APIs in the input\nprompt. The API selection algorithm also reduces\nthe average token cost by reducing the number of\nAPIs listed in the prompt. However, for the chal-\nlenging session-based evaluation, these algorithms\ncan not improve GPT-4\u2019s performance or improve\nit slightly.\n4.5\nThree challenges in our PPTC benchmark\nFrom the result Table 1 and Figure 5. we highlight\nthe following three key challenges.\n(1) Error accumulation makes LLMs perfor-\nmance poor in finishing the entire multi-turn\nsession.: The performance of all LLMs in handling\nsessions consisting of multiple turns is notably poor.\nEven GPT-4, which performs well in turn-based\nevaluation, achieves only a 22.7% session-based\naccuracy for the \"creating new PPT file\" task and a\nmere 6.0% session-based accuracy for the \"editing\nPPT template\" task. Current planning algorithms\nModels and Methods\nCreating new PPT\nEditing PPT template\nTurn-based\nSession-based\nTurn-based\nSession-based\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nTD-003\n72.6\n2.8k\n3.0\n12.7\n20.8k\n23.9\n24.4\n2.9k\n8.1\n4.0\n13.2k\n26.6\nChatGPT\n70.6\n2.9k\n3.2\n12.7\n20.0k\n23.4\n26.3\n4.1k\n7.9\n2.0\n9.2k\n22.9\nGPT-4\n75.1\n2.9k\n2.9\n22.7\n20.8k\n22.4\n38.1\n7.5k\n7.8\n6.0\n24.1k\n24.7\nLLaMa-2\n16.4\n2.8k\n3.9\n3.4\n21.6k\n24.7\n8.7\n2.2k\n7.2\n0.0\n9.5k\n15.6\nCode-LLaMa\n36.8\n2.8k\n3.4\n0.0\n20.7k\n32.1\n18.7\n3k\n7.3\n2.0\n9.6k\n22.6\nWizardLM\n23.9\n1.3k\n3.3\n4.3\n12.5k\n22.4\n10.0\n1.3k\n5.7\n0.0\n4.3k\n16.5\nVicuna-v1.5\n24.3\n1.3k\n3.9\n2.2\n11.0k\n33.7\n6.8\n1.3k\n6.7\n0.0\n4.3k\n22.7\nBaichuan\n15.5\n1.3k\n9.8\n0.0\n10.9k\n44.7\n4.4\n1.3k\n9.6\n0.0\n4.3k\n24.3\nBaichuan-2\n16.3\n1.3k\n9.1\n3.6\n11.6k\n48.9\n8.7\n1.3k\n9.2\n0.0\n4.2k\n22.3\nTable 1: We report the results of LLMs in this table.\u2019 TD-003\u2019 is the Text-Davinci-003 model. We directly use the\nprompts in Figure 4 to prompt LLMs to generate the API sequence.\nModels and Methods\nCreating new PPT file\nEditing PPT template\nTurn-based\nSession-based\nTurn-based\nSession-based\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nAccuracy\nAvg token\nAvg API\nGPT-4\n75.1\n2.9k\n2.9\n22.7\n20.8k\n22.4\n38.1\n7.5k\n7.8\n6.0\n24.1k\n24.7\nGPT-4+CoT\n77.0\n2.9k\n3.1\n23.1\n20.8k\n22.7\n40.6\n7.5k\n8.0\n6.0\n24.1k\n25.2\nGPT-4+ToT\n76.5\n20.8k\n3.0\n21.8\n146.4k\n22.6\n40.6\n81k\n7.6\n4.0\n256.8k\n24.0\nGPT-4+Content selection\n77.5\n3.4k\n3.0\n21.8\n24.5k\n22.0\n43.1\n5.8k\n8.0\n4.0\n18.7k\n25.2\nGPT-4+API selection\n76.4\n1.5k\n2.9\n18.8\n10.6k\n21.3\n38.1\n7k\n8.0\n10.0\n22.4k\n25.8\nTable 2: We report the results of GPT-4 and algorithms based on the GPT-4 model. \u2019CoT\u2019 and \u2019ToT\u2019 are the chain of\nthought and tree of thought algorithms.\nusually fail to improve session-based accuracy. In\nsome cases, they can even make the performance\nworse. The session-based evaluation is challeng-\ning since errors made in previous turns make the\nLLM fail to finish the session and also influence\nthe completion of the current turn. Also, we need\nmore advanced planning algorithms to complete\nthe multi-turn session.\n(2) LLMs perform badly in processing long\nPPT template: Current LLMs\u2019 performance in the\nediting PPT temples task is pretty poor. For exam-\nple, the strongest GPT-4 only achieves 38.1% turn-\nbased accuracy and 6.0% session-based accuracy in\nthis task. Other LLMs\u2019 performance is even poorer.\nThe content selection algorithm can partially solve\nthis challenge by filtering out irrelevant file content,\nbut GPT-4 with it still only achieves 43.1% turn-\nbased accuracy. That means current LLMs (e.g.,\nGPT-4) still struggle to handle complex and lengthy\nPPT templates. For open-source LLMs, there\u2019s a\nrisk of information loss due to token limitations\n(typically 2\u223c4K tokens limit), which often require\ntruncating lengthy PPT content. When it comes to\nsession-based performance, the accuracy remains\nnearly zero. This implies that current LLMs are\nstill far from being ideal PPT agents capable of\neffectively assisting users in editing PPT templates\nduring a multi-turn dialogue session.\n(3) Multi-modal instructions increase the\nLLM\u2019s failure rate significantly. To assess LLMs\u2019\ntask completion performance for instructions in-\nvolving multi-modal operations (Table, Chart, Pic-\nture, Position, and text), we calculate the average\naccuracy of GPT-4 for instructions involving each\nmodality, respectively. This is done by dividing the\nnumber of correctly completed instructions within\neach modality by the total number of instructions\ninvolving that modality\u2019s operation. The results\nare presented in Figure 5 (a). From the figure, we\nobserve that GPT-4 performs exceptionally well in\nthe text modality, achieving an accuracy of 85.6%.\nIts performance becomes poorer when process-\ning structured data (Chart and Table), with 12.4%\nand 16.2% lower accuracy. Instructions involv-\ning picture-related operation pose an even greater\nchallenge for GPT-4, as it achieves a 56.8% turn-\nbased accuracy in this modality. GPT-4 exhibits\nits weakest performance in instructions involving\nposition-related (spatial) operations, with only 24%\naccuracy. This underscores GPT-4\u2019s limitations in\nspatial perception ability.\n5\nAnalysis\nIn this section, we analyze the reasons for GPT-4\u2019s\nerrors. We further analyze the influence of model\nsize and dialogue history.\n5.1\nError Analysis of GPT-4 in our benchmark\nTo analyze the error made by GPT-4, in our bench-\nmark, we gather 50 wrong samples for each of the\ntwo tasks in our benchmark in the turn-based eval-\nuation. We find that these wrong samples fall into\nfour error types and visualize the distribution of\nthese four main error types in Figure 5 (b): (1) Po-\nFigure 5: We illustrate the analysis results of the creating new PPT file task (task 1) and the editing PPT template\ntask (task 2). In sub-figure (a), we report the average turn-based accuracy for instructions involving chart, table,\npicture, position, and pure text. We don\u2019t draw the accuracy of task 2 as no chart instruction in this task. In sub-figure\n(b), we report the ratio of four common errors made by GPT-4. In sub-figure (c), we report the accuracy with the\nmodel size. We don\u2019t plot the session-based accuracy of task 2 as it is zero.\nsition errors: These occur when GPT-4 struggles\nwith instructions involving position adjustments.\nFor example, when asked to move the shape to\nthe bottom of the slide, GPT-4 wrongly calls the\n\"set_top\" API. position error is the main error in\nthe creating new PPT file task as this task contains\nmany instructions involving position operation. (2)\nCalling unavailable APIs: GPT-4 sometimes gener-\nates APIs that don\u2019t actually exist in the reference\nAPI file, resulting in what we call the \"API hallu-\ncination problem.\" (3) Misunderstanding PPT file\ncontent: GPT-4\u2019s comprehension of the PPT con-\ntent can be flawed, leading to the generation of\nincorrect API sequences. For example, when in-\nstructed to make the font size of the current slide\u2019s\ntitle consistent with previous slides, GPT-4 set a\nfont size that is different from what was used in\nprevious slides\u2019 titles. In the editing template task,\nmisunderstanding the PPT content becomes the\nmain error since this task needs to understand the\ncomplex PPT template. (4) Unfollowing Power-\npoint task rules: Completing Powerpoint tasks de-\nmands a deep understanding of various task rules.\nFor instance, writing a new slide title requires first\ndeleting the original text and then inserting the new\ntitle into the text box. However, GPT-4 may di-\nrectly insert the new content. For the session-based\nevaluation, we also collect 50 wrong examples. We\nfind that the main reason for the poor session-based\nperformance is the LLM fails to finish the session\nonce it makes an error in one turn of the session.\nThe reasons for errors made in a single turn are\nsimilar to those in the turn-based evaluation. One\nunique phenomenon in this evaluation is that the\nLLM would repeat previous errors (e.g., employing\ninfeasible APIs) in subsequent turns.\n5.2\nDoes bigger LLM work better on PPTC?\nTo investigate how the model size impacts the\nLLM\u2019s performance in our benchmark, we con-\nduct tests using LLaMa-2-chat LLM with 7, 13,\nand 70 billion parameters and plot the results in\nFigure 5 (c). We observe that larger LLM consis-\ntently achieve higher turn-based accuracy for both\nthe creating new PPT and editing PPT template\ntasks. For example, in the creating new PPT file\ntask, we find that the turn-based accuracy increases\nfrom 13.2 (7B) to 30.1 (70B). However, we do not\nobserve a clear positive correlation between model\nsize and session-based performance. One possible\nexplanation is that although the 70B LLM can cor-\nrectly finish more intermediate steps, it still falls\nshort of completing the entire session. To improve\nthe session-based performance, a larger LLM may\nbe necessary.\n5.3\nDoes dialogue history help LLMs to\ngenerate the API sequence?\nTo investigate the influence of dialogue history in\nour prompt (see Figure 4), we make an ablation\nexperiment for the dialogue history component of\nour turn-based evaluation prompt\u00a7. In this evalua-\ntion, the dialogue history contains previous turns\nalong with their feasible API sequences. When we\nremoved the dialogue history from the prompt, we\nobserved a decline in GPT-4\u2019s performance. Specif-\nically, GPT-4 drops its performance from 75.1 %\n\u00a7The task instruction, current user instruction, API file,\nPPT content in the prompt are necessary parts for generating\nthe API sequence. So we don\u2019t conduct ablation studies on\nthem.\nto 73.1 % in the creating new PPT file task and\ndecreases its performance by 6.2 % in the editing\ntemplate task. This experiment shows the positive\neffect of the dialogue history, as it helps the LLM\nto both understand the dialogue background and\ninstruct the LLM to correctly use the APIs, similar\nto few-shot demonstration examples.\n6\nRelated Works\nLarge Language Models like ChatGPT, GPT-\n4 (Bubeck et al., 2023; OpenAI, 2023), and Bard\nhave billions of parameters and have been trained\non the Internet corpus with trillions of tokens. They\ncan write code (Liu et al., 2023a), prove mathemat-\nical theorems (Jiang et al., 2022), pass the profes-\nsional exam (Zhong et al., 2023; Gilson et al., 2023;\nKatz et al., 2023), and also perform well on other\nbasic natural language tasks (Kim et al., 2023; Jiao\net al., 2023; Zhong et al., 2023; Wang et al., 2023b).\nThat raises the hope of achieving artificial general\nintelligence (AGI).\nTo further boost LLM\u2019s performance on the spe-\ncific task, one approach involves prompting en-\ngineerings, such as the chain of thought prompt-\ning (Wei et al., 2022; Shi et al., 2022; Yao et al.,\n2023), self-consistency (Wang et al., 2022) and the\nleast to most prompting (Zhou et al., 2022). An-\nother approach aims to use feedback to improve per-\nformance. The self-refine method (Madaan et al.,\n2023) refines the output through iterative feedback\nand refinement Provided by LLM itself. The Re-\nflexion (Shinn et al., 2023) method generates and\nstores the reflection based on the sparse reward\nsignal and then uses the reflection to induce bet-\nter decisions in subsequent trials. The learning to\nprogram method (Guo et al., 2023) learns the task\nprogram by inducing the general solutions from the\nerrors (feedback) iteratively and uses the program\nto guide the inference.\nTask completion benchmarks for measuring\nlarge language models. To measure LLM\u2019s task\ncompletion performance, Saycan (Brohan et al.,\n2023) and VirtualHome (Puig et al., 2018) bench-\nmarks ask LLM to generate the correct action se-\nquence for controlling the robot to finish user in-\nstruction. WebShop (Yao et al., 2022) and Android\nin the wild (Rawles et al., 2023) ask LLM to navi-\ngate websites and conduct actions to meet the user\nrequirement. APIBench (Patil et al., 2023) and\nToolBench (Xu et al., 2023b; Qin et al., 2023b) in-\nvolve selecting and using APIs to complete the task\ninstruction. Agentbench (Liu et al., 2023b)assesses\nLLM as autonomous agents in 8 environments and\nWebArena (Zhou et al., 2023) considers task com-\npletion in web-based interactions.\nAI assistant system for complex task com-\npletion For more complex tasks that involve us-\ning tools and utilizing environmental information,\nthere are many strong AI systems (e.g., TaskMa-\ntrix (Liang et al., 2023)) that help the user finish the\ncomplex task. One approach involves connecting\nmassive models and tools with the Large Language\nModel for task completion.\nExamples include\nVisual ChatGPT (Wu et al., 2023) and Hugging-\nGPT (Shen et al., 2023) which use LLM to deploy\ntask-specific models to finish the user instruction\nbased on the observation of task information (e.g.,\nvisual information), Voyager (Wang et al., 2023a)\nthat uses the fixed LLM to continually learn skills\n(tools) based on the observation of the Minecraft\nenvironment. Another approach involves training\nan end-to-end LLM to finish the user instruction.\nExamples include Gorilla (Patil et al., 2023) for\ngenerating API calls to finish the user query by\nusing the API bench to fine-tune the pre-trained\nLLaMA (Touvron et al., 2023) model.\nPaLM-\nE (Driess et al., 2023) for various robot reasoning\ntasks by fine-tuning the pretrained PaLM (Chowd-\nhery et al., 2022) model using features from sensor\nmodalities. Different from the above systems, we\nfocus on the research topic of the AI assistant sys-\ntem in office software.\n7\nConclusion\nWe introduce the PowerPoint Task Completion\nbenchmark to measure LLMs\u2019 ability to complete\nuser instructions within the context of the Power-\nPoint software. It contains hundreds of multi-turn\nsessions with different topics and thousands of in-\nstructions with varying levels of difficulty. We fur-\nther propose the PPTX-evaluation system to access\nand compare the performance of different LLMs.\nResults show that GPT-4 is the strongest LLM but\nstill performs poorly in finishing entire sessions.\nWe further analyze the behavior of LLMs and find\nthree main error factors that limit their performance.\nOur benchmark and findings can help the research\ncommunity design better AI task completion assis-\ntants.\n8\nLimitations\nOur benchmark does not consider instructions that\ninvolve subjective evaluation. For example, the\nuser may want to make the slide more beautiful.\nHowever, it\u2019s hard to automatically evaluate if the\ngenerated file (the model output) is more beautiful.\nAnother limitation is that we do not consider the\ninstructions that need non-API operations. For ex-\nample, the user may want to draw a cat on the slide.\nThat instruction needs the AI-assistant system to\ndraw the cat by dragging the mouse and is still in-\nfeasible for LLMs and LLM-based systems. We\nonly consider instructions that can be completed\nby directly executing the API sequence.\nReferences\nAnthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian\nIbarz, Alex Irpan, Eric Jang, Ryan Julian, et al. 2023.\nDo as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learn-\ning, pages 287\u2013318. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nAidan Gilson, Conrad W Safranek, Thomas Huang,\nVimig Socrates, Ling Chi, Richard Andrew Taylor,\nDavid Chartash, et al. 2023. How does chatgpt per-\nform on the united states medical licensing examina-\ntion? the implications of large language models for\nmedical education and knowledge assessment. JMIR\nMedical Education, 9(1):e45312.\nYiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu,\nDongyan Zhao, and Nan Duan. 2023.\nLearning\nto program with natural language. arXiv preprint\narXiv:2304.10464.\nAlbert Q Jiang, Sean Welleck, Jin Peng Zhou,\nWenda Li, Jiacheng Liu, Mateja Jamnik, Timo-\nth\u00e9e Lacroix, Yuhuai Wu, and Guillaume Lample.\n2022. Draft, sketch, and prove: Guiding formal the-\norem provers with informal proofs. arXiv preprint\narXiv:2210.12283.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator?\na preliminary study.\narXiv preprint\narXiv:2301.08745.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2023. Gpt-4 passes the\nbar exam. Available at SSRN 4389233.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. 2023. Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. arXiv preprint arXiv:2303.16434.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023a. Is your code generated by chat-\ngpt really correct? rigorous evaluation of large lan-\nguage models for code generation. arXiv preprint\narXiv:2305.01210.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-\nanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al. 2023b. Agent-\nbench: Evaluating llms as agents. arXiv preprint\narXiv:2308.03688.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activities\nvia programs. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 8494\u20138502.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023a. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023b. Tool learning with foundation\nmodels.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023c. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nChristopher Rawles, Alice Li, Daniel Rodriguez, Oriana\nRiva, and Timothy Lillicrap. 2023. Android in the\nwild: A large-scale dataset for android device control.\narXiv preprint arXiv:2307.10088.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023.\nReflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\narXiv\npreprint arXiv:2203.11171.\nZengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng,\nand Rui Xia. 2023b. Is chatgpt a good sentiment\nanalyzer?\na preliminary study.\narXiv preprint\narXiv:2304.04339.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nChenfei Wu,\nShengming Yin,\nWeizhen Qi,\nXi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a.\nWizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu,\nZhengyu Chen, and Jian Zhang. 2023b.\nOn the\ntool manipulation capability of open-source large\nlanguage models. arXiv preprint arXiv:2305.16504.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong\nZhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,\nDong Yan, Fan Yang, et al. 2023.\nBaichuan 2:\nOpen large-scale language models. arXiv preprint\narXiv:2309.10305.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nAdvances in Neural Information Processing Systems,\n35:20744\u201320757.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. 2023. Agieval: A human-centric\nbenchmark for evaluating foundation models. arXiv\npreprint arXiv:2304.06364.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models.\narXiv preprint\narXiv:2205.10625.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\nA realistic web environment for building autonomous\nagents. arXiv preprint arXiv:2307.13854.\nA\nThe API Reference File\nWe list all APIs and their descriptions in Figures 6\nand 7. We provide 49 feasible APIs.\nB\nThe Prompt for Content Selection\nAlgorithm\nWe put the prompt of content selection algorithm\nin Figure 8.\nAPI reference file\nSlide-related APIs\nAPI: create slide(): This API creates a new slide.\nAPI: move to previous slide(): This API moves to the previous slide.\nAPI: move to next slide(): This API moves to the next slide.\nAPI: move to slide(slide id): This API moves to the slide with given slide id.It takes one parameter\n\u2019slide id\u2019, the ID of the slide to move to as a integer.\nChoose-related APIs\nAPI: choose title(): This API selects the title on the slide. You should first call choose title() before\ninserting text to or changing font attributes of the title.\nAPI: choose content(): This API select the content on the slide. You should first call choose\ncontent() before inserting text to or changing font attributes of the content.\nAPI: choose textbox(idx): This API selects the textbox element on the slide. It takes one parameter,\nthe index of textbox as integer. idx is set to 0 by default, meaning the first textbox. You should first\ncall choose textbox() before inserting text to or changing font attributes of the textbox element.\nAPI: choose picture(idx): This API selects the picture element on the slide. It takes one parameter,\nthe index of textbox as integer. idx is set to 0 by default, meaning the first textbox. You should first\ncall choose picture() before changing height, width, rotation of the picture element. You should\nnot call choose picture() before inserting picture element.\nAPI: choose chart(): This API selects the chart element on the slide. You should first call choose\nchart() before changing the chart. You should not call choose chart() before inserting chart element.\nAPI: choose shape(shape name): This API selects a specific shape by shape name on the slide. It\ntakes one parameter \u2019shape name\u2019, the name of the shape to select as a string. shape name can be\nchosen from [\u2019rectangle\u2019,\u2019right arrow\u2019,\u2019rounded rectangle\u2019,\u2019triangle\u2019,\u2019callout\u2019,\u2019cloud\u2019,\u2019star\u2019,\u2019circle\u2019]\nYou should first call choose shape(shape name) before you can do operations on the shape. You\nshould not call choose shape(shape name) before inserting shape element.\nAPI: choose table(): This API selects the table element on the slide. You should first call choose\ntable() before changing the table. You should not call choose table() before inserting table element.\nAPI: choose table cell(row id, column id): This API selects a specific cell in the table by giving\nrow id and column id. It takes two parameters, the row id and column id of the cell to select as\nintegers (id starts from 0). Remember the first parameter is row id, the second parameter is column\nid. You should first call choose table cell(row id, column id) before inserting text.\nBasic APIs\nAPI: set background color(color): This API sets the background color of the slide. It takes one\nparameter \u2019color\u2019, the color name to set as a string, such as \u2019red\u2019, \u2019purple\u2019.\nAPI: set width(width): This API sets the width of the selected object. It takes one parameter\n\u2019width\u2019, the width of an object in centimeters as float. You should first choose an object before you\ncan change the width of it.\nAPI: set height(height): This API sets the height of the selected object. It takes one parameter\n\u2019height\u2019, the height of an object in centimeters as float. You should first choose an object before\nyou can change the height of it\nAPI: rotate element(angle): This API rotates the selected element by the specified angle. It takes\none parameter \u2019angle\u2019, the angle to rotate clockwise as integer. You should first choose an object\nbefore you can rotate it.\nAPI: set fill color(color): This API sets the fill color of the selected object after the object is chosen.\nIt takes one parameter \u2019color\u2019, the color name to set as a string, such as \u2019red\u2019, \u2019purple\u2019. You can set\nthe fill color of content, title or textbox.\nAPI: set left(left): This API moves and changes the object\u2019s position. It sets the x position of the\nselected object\u2019s leftmost point. It takes one parameter, the x position to set. You should first\nchoose an object before you can change the left of it\nAPI: set top(top): This API moves and changes the object\u2019s position. It sets the y position of the\nselected object\u2019s upmost point. It takes one parameter, the y position to set. You should first choose\nan object before you can change the top of it.\nFigure 6: The reference API file: part 1.\nAPI reference file\nText-related APIs\nAPI: insert text(text): This API inserts text into a text frame (textbox, title, content, table).\nAPI: insert bullet point(text): This API inserts a bullet point into the content. It takes one parameter,\nthe text of the bullet point to insert as a string.\nAPI: insert note(text): This API inserts a note onto the slide. It takes one parameter, the note text\nto insert as a string.\nAPI: insert textbox(): This API inserts a textbox onto the slide. When you need to add a caption or\ntext under/above/left to/right to an object, you can call insert textbox().\nAPI: delete text(): This API delete the text part of an object. You should first choose content or\ntitle before you can call delete text()\nAPI: set font size(font size): This API sets the size of the font It can take one argument \u2019font size\u2019,\nthe font size to set as an integer.\nAPI: set font color(color): This API sets the color of the font. It takes one parameter \u2019color\u2019, the\ncolor name to set as a string, such as \u2019red\u2019, \u2019purple\u2019.\nAPI: set font bold(): This API sets the font to be bold.\nAPI: set font italic(): This API sets the font to be italic.\nAPI: set font underline(): This API sets the font to be underlined.\nAPI: set font style(font name): This API sets the font style of the selected text. It can take one\nargument \u2019font style\u2019, the font name as a string.\nAPI: set line space(line space level): This API sets the line spacing of the selected text. It can take\none argument \u2019line space level\u2019, as an integer, default 0.\nAPI: text align left(): This API aligns the text to left.\nAPI: text align center(): This API aligns the text to center.\nAPI: text align right(): This API aligns the text to right.\nImage and shape-related APIs\nAPI: insert picture(picture name): This API inserts a picture onto the slide. It takes one parameter\n\u2019picture name\u2019, the name or description of picture as a string\nAPI: insert rectangle(): This API inserts a rectangle or square shape onto the slide.\nAPI: insert right arrow(): This API inserts an arrow shape onto the slide.\nAPI: insert rounded rectangle(): This API inserts a rounded rectangle shape onto the slide.\nAPI: insert triangle(): This API inserts a triangle shape onto the slide.\nAPI: insert callout(): This API inserts a callout shape onto the slide.\nAPI: insert cloud(): This API inserts a cloud shape onto the slide.\nAPI: insert star(): This API inserts a star shape onto the current slide.\nAPI: insert circle(): This API inserts a circle or oval shape into the current slide.\nTable-related APIs\nAPI: insert table(row num, col num): This API inserts a table of row num rows and col num\ncolumns onto the current slide. It takes two argument, the row number and the column number of\nthe inserted table as integer. Remember the first parameter is row number and the second parameter\nis column number.\nAPI: insert table row(row data): This API inserts a row (list) of data into the table. It takes one\nargument, the data to insert as a list of numbers or strings. You should first call choose table()\nbefore you can call insert table row(). The parameter \u2019row data\u2019 should be a list of strings.\nChart-related APIs\nAPI: insert line chart(data, series): This API inserts a line chart onto the slide. It takes two\nargument, \u2019data\u2019 is a list of numbers and \u2019series\u2019 is a list of strings.\nAPI: insert bar chart(data, series): This API inserts a bar chart onto the slide. It takes two argument,\n\u2019data\u2019 is a list of numbers and \u2019series\u2019 is a list of strings.\nAPI: insert pie chart(data, series): This API inserts a pie chart onto the slide. It takes two argument,\n\u2019data\u2019 is a list of numbers and \u2019series\u2019 is a list of strings.\nAPI: set chart title(title): This API sets the title of a previously inserted chart. It takes one argument\n\u2019title\u2019, the title to be set as a string.\nFigure 7: The reference API file: part 2.\nContent Selection prompt\nYou are an AI assistant for PowerPoint. Your task is to determine what kind of content is necessary to fulfill the user\u2019s\ninstruction. You have an API to extract the content, please call the get_content api with correct parameters to fulfill the\nuser\u2019s instruction. You need to extract the minimum necessary information to fulfill user\u2019s instruction.\nGet_content\nAPI: get_content(need_text:\nIndicates whether text information is required.\nThe text infor-\nmation encompasses text in title, content, textbox, table, chart, and shape. This parameter is particularly useful when\ninserting or modifying text of title, content, textbox, table, chart, and shape, or when information about these objects is\nessential.\nneed_style: Indicates whether style information is required. Style information includes attributes like font type, font\nsize, color, background color, line space, bold, undeline, italic and other visual aspects of objects like rotation. This is\nuseful when changing the appearance of text or objects or when information about an object\u2019s appearance is essential.\nneed_position: Indicates whether position information is required. The position details encompass an object\u2019s height,\nwidth, and its left and top positions. This is crucial when moving objects or altering an object\u2019s size.\nneed_title: Determines if information related to the title is required.\nneed_content: Determines if information related to the content is required.\nneed_picture: Determines if information related to the picture is required.\nneed_table: Determines if information related to the table is required.\nneed_chart: Determines if information related to the chart is required.\nneed_textbox: Determines if information related to the textbox is required.\nneed_shape: Determines if information related to the shapes (rectangle, right arrow, rounded rectangle, triangle, callout,\ncloud, star, circle) is required. )\nWhere the parameters are either 1 (needed) or 0 (not needed). You should only answer with calling get_content() with\nthe right parameters.\nFor examples:\nInstruction: Increase the font size of the content to 20.\nExplanation: For information, style information (font size) is needed. For objects, content is needed.\nAnswer:\nget_content(need_text=1,need_style=1,need_position=0,\nneed_title=0,need_content=1,need_picture=0,need_\ntable=0,need_chart=0,need_textbox=0,need_shape=0)\n...\nGiven the instruction, output the Answer without Explanation:\nInstruction: <Current user instruction>\nAnswer:\nFigure 8: The prompt of the content selection algorithm.\n"
  },
  {
    "title": "EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision",
    "link": "https://arxiv.org/pdf/2311.02077.pdf",
    "upvote": "13",
    "text": "Preprint\nEMERNERF: EMERGENT SPATIAL-TEMPORAL SCENE\nDECOMPOSITION VIA SELF-SUPERVISION\nJiawei Yang\u2217,\u00b6, Boris Ivanovic\u00b6, Or Litany\u2020,\u00b6, Xinshuo Weng\u00b6, Seung Wook Kim\u00b6, Boyi Li\u00b6,\nTong Che\u00b6, Danfei Xu$,\u00b6, Sanja Fidler\u00a7,\u00b6, Marco Pavone\u2021,\u00b6, Yue Wang\u2217,\u00b6\n\u2217 {yangjiaw,yue.w}@usc.edu, University of Southern California\n$ danfei@gatech.edu, Georgia Institute of Technology\n\u00a7 fidler@cs.toronto.edu, University of Toronto\n\u2021 pavone@stanford.edu, Stanford University\n\u2020 orlitany@gmail.com, Technion\n\u00b6 {bivanovic,xweng,seungwookk,boyil,tongc}@nvidia.com, NVIDIA Research\nABSTRACT\nWe present EmerNeRF, a simple yet powerful approach for learning spatial-\ntemporal representations of dynamic driving scenes. Grounded in neural fields,\nEmerNeRF simultaneously captures scene geometry, appearance, motion, and\nsemantics via self-bootstrapping.\nEmerNeRF hinges upon two core compo-\nnents: First, it stratifies scenes into static and dynamic fields. This decomposition\nemerges purely from self-supervision, enabling our model to learn from general,\nin-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field\nfrom the dynamic field and uses this flow field to further aggregate multi-frame\nfeatures, amplifying the rendering precision of dynamic objects. Coupling these\nthree fields (static, dynamic, and flow) enables EmerNeRF to represent highly-\ndynamic scenes self-sufficiently, without relying on ground truth object annota-\ntions or pre-trained models for dynamic object segmentation or optical flow esti-\nmation. Our method achieves state-of-the-art performance in sensor simulation,\nsignificantly outperforming previous methods when reconstructing static (+2.93\nPSNR) and dynamic (+3.70 PSNR) scenes. In addition, to bolster EmerNeRF\u2019s\nsemantic generalization, we lift 2D visual foundation model features into 4D\nspace-time and address a general positional bias in modern Transformers, signif-\nicantly boosting 3D perception performance (e.g., 37.50% relative improvement\nin occupancy prediction accuracy on average). Finally, we construct a diverse\nand challenging 120-sequence dataset to benchmark neural fields under extreme\nand highly-dynamic settings. See the project page for code, data, and request\npre-trained models: https://emernerf.github.io\n1\nINTRODUCTION\nPerceiving, representing, and reconstructing dynamic scenes are critical for autonomous agents to\nunderstand and interact with their environments. Current approaches predominantly build custom\npipelines with components dedicated to identifying and tracking static obstacles and dynamic ob-\njects (Yang et al., 2023; Guo et al., 2023). However, such approaches require training each com-\nponent with a large amount of labeled data and devising complex mechanisms to combine out-\nputs across components. To represent static scenes, approaches leveraging neural radiance fields\n(NeRFs) (Mildenhall et al., 2021) have witnessed a Cambrian explosion in computer graphics,\nrobotics, and autonomous driving, owing to their strong performance in estimating 3D geometry\nand appearance (Rematas et al., 2022; Tancik et al., 2022; Wang et al., 2023b; Guo et al., 2023).\nHowever, without explicit supervision, NeRFs struggle with dynamic environments filled with fast-\nmoving objects, such as vehicles and pedestrians in urban scenarios. In this work, we tackle this\nlong-standing challenge and develop a self-supervised technique for building 4D (space-time) rep-\nresentations of dynamic scenes.\n1\narXiv:2311.02077v1  [cs.CV]  3 Nov 2023\nPreprint\n(a) GT RGB\n(b) Rendered RGB\n(c) Decomposed Static RGB\n(d) Decomposed Static Depth\n(e) Decomposed Dynamic RGB\n(f) Decomposed Dynamic Depth\n(g) Emerged Scene Flow\n(h) Reconstructed DINOv2 Features\n(i) Decomposed PE-Free DINOV2 Features\n(j) Decomposed PE-Features\nFigure 1: EmerNeRF effectively reconstructs photo-realistic dynamic scenes (b), separating them\ninto explicit static (c-d) and dynamic (e-f) elements, all via self-supervision. Notably, (g) scene flows\nemerge from EmerNeRF without any explicit flow supervision. Moreover, EmerNeRF can address\ndetrimental positional embedding (PE) patterns observed in vision foundation models (h, j), and lift\nclean, PE-free features into 4D space (i). Additional visualizations can be found in Appendix C.4.\nWe consider a common setting where a mobile robot equipped with multiple sensors (e.g., cam-\neras, LiDAR) navigates through a large dynamic environment (e.g., a neighborhood street). The\nfundamental underlying challenge is to construct an expansive 4D representation with only sparse,\ntransient observations, that is, to reconstruct an entire 4D space-time volume from a single traversal\nof the volume. Unfortunately, this setting challenges NeRF\u2019s multi-view consistency assumption\u2014\neach point in the space-time volume will only be observed once. Recent works (Yang et al., 2023;\nOst et al., 2021) seek to simplify the problem by modeling static scene components (e.g., buildings\nand trees) and dynamic objects separately, using a combination of neural fields and mesh repre-\nsentations. This decomposition enables exploiting multi-timestep observations to supervise static\ncomponents, but it often requires costly ground-truth annotations to segment and track dynamic ob-\njects. Moreover, no prior works in this field have explicitly modeled temporal correspondence for\n2\nPreprint\ndynamic objects, a prerequisite for accumulating sparse supervision over time. Overall, learning 4D\nrepresentations of dynamic scenes remains a formidable challenge.\nTowards this end, we present EmerNeRF, a self-supervised approach for constructing 4D neural\nscene representations. As shown in Fig. 1, EmerNeRF decouples static and dynamic scene com-\nponents and estimates 3D scene flows \u2014 remarkably, all from self-supervision. At a high level,\nEmerNeRF builds a hybrid static-dynamic world representation via a density-regularized objec-\ntive, generating density for dynamic objects only as necessary (i.e., when points intersect dynamic\nobjects). This representation enables our approach to capture dynamic components and exploit\nmulti-timestep observations to self-supervise static scene elements. To address the lack of cross-\nobservation consistency for dynamic components, we task EmerNeRF to predict 3D scene flows\nand use them to aggregate temporally-displaced features. Intriguingly, EmerNeRF\u2019s capability to\nestimate scene flow emerges naturally from this process, without any explicit flow supervision. Fi-\nnally, to enhance scene comprehension, we \u201clift\u201d features from pre-trained 2D visual foundation\nmodels (e.g., DINOv1 (Caron et al., 2021), DINOv2 (Oquab et al., 2023)) to 4D space-time. In do-\ning so, we observe and rectify a challenge tied to Transformer-based foundation models: positional\nembedding (PE) patterns (Fig. 1 (h)). As we will show in \u00a74.3, effectively utilizing such general\nfeatures greatly improves EmerNeRF\u2019s semantic understanding and enables few-shot auto-labeling.\nWe evaluate EmerNeRF on sensor sequences collected by autonomous vehicles (AVs) traversing\nthrough diverse urban environments. A critical challenge is that current autonomous driving datasets\nare heavily imbalanced, containing many simple scenarios with few dynamic objects. To facilitate\na focused empirical study and bolster future research on this topic, we present the NeRF On-The-\nRoad (NOTR) benchmark, a balanced subsample of 120 driving sequences from the Waymo Open\nDataset (Sun et al., 2020) containing diverse visual conditions (lighting, weather, and exposure) and\nchallenging dynamic scenarios. On this benchmark, EmerNeRF significantly outperforms previous\nstate-of-the-art NeRF-based approaches (Park et al., 2021b; Wu et al., 2022; M\u00a8uller et al., 2022; Guo\net al., 2023) on scene reconstruction by 2.93 and 3.70 PSNR on static and dynamic scenes, respec-\ntively, and by 2.91 PSNR on dynamic novel view synthesis. For scene flow estimation, EmerNeRF\nexcels over Li et al. (2021a) by 42.16% in metrics of interest. Additionally, removing PE patterns\nbrings an average improvement of 37.50% relative to using the original, PE pattern-laden features\non semantic occupancy prediction. Contributions. Our key contributions are fourfold: (1) We in-\ntroduce EmerNeRF, a novel 4D neural scene representation framework that excels in challenging\nautonomous driving scenarios. EmerNeRF performs static-dynamic decomposition and scene flow\nestimation, all through self-supervision. (2) A streamlined method to tackle the undesired effects of\npositional embedding patterns from Vision Transformers, which is immediately applicable to other\ntasks. (3) We introduce the NOTR dataset to assess neural fields in diverse conditions and facili-\ntate future development in the field. (4) EmerNeRF achieves state-of-the-art performance in scene\nreconstruction, novel view synthesis, and scene flow estimation.\n2\nRELATED WORK\nDynamic scene reconstruction with NeRFs. Recent works adopt NeRFs (Mildenhall et al., 2021;\nM\u00a8uller et al., 2022) to accommodate dynamic scenes (Li et al., 2021b; Park et al., 2021b; Wu et al.,\n2022). Earlier methods (Bansal et al., 2020; Li et al., 2022; Wang et al., 2022; Fang et al., 2022) for\ndynamic view synthesis rely on multiple synchronized videos recorded from different viewpoints,\nrestricting their use for real-world applications in autonomous driving and robotics. Recent meth-\nods, such as Nerfies (Park et al., 2021a) and HyperNeRF (Park et al., 2021b), have managed to\nachieve dynamic view synthesis using a single camera. However, they rely on a strict assumption\nthat all observations can be mapped via deformation back to a canonical reference space, usually\nconstructed from the first timestep. This assumption does not hold in driving because objects might\nnot be fully present in any single frame and can constantly enter and exit the scene.\nOf particular relevance to our work are methods like D2NeRF (Wu et al., 2022), SUDS (Turki et al.,\n2023), and NeuralGroundplans (Sharma et al., 2022). These methods also partition a 4D scene\ninto static and dynamic components. However, D2NeRF underperforms significantly for outdoor\nscenes due to its sensitivity to hyperparameters and insufficient capacity; NeuralGroundplan relies\non synchronized videos from different viewpoints to reason about dynamics; and SUDS, designed\nfor multi-traversal driving logs, largely relies on accurate optical flows derived by pre-trained models\n3\nPreprint\nand incurs high computational costs due to its expensive flow-based warping losses. In contrast, our\napproach can reconstruct an accurate 4D scene representation from a single-traversal log captured\nby sensors mounted on a self-driving vehicle. Freed from the constraints of pre-trained flow models,\nEmerNeRF exploits and refines its own intrinsic flow predictions, enabling a self-improving loop.\nNeRFs for AV data. Creating high-fidelity neural simulations from collected driving logs is crucial\nfor the autonomous driving community, as it facilitates the closed-loop training and testing of various\nalgorithms. Beyond SUDS (Turki et al., 2023), there is a growing interest in reconstructing scenes\nfrom driving logs. In particular, recent methods excel with static scenes but face challenges with\ndynamic objects (Guo et al., 2023). While approaches like UniSim (Yang et al., 2023) and NSG (Ost\net al., 2021) handle dynamic objects, they depend on ground truth annotations, making them less\nscalable due to the cost of obtaining such annotations. In contrast, our method achieves high-fidelity\nsimulation results purely through self-supervision, offering a scalable solution.\nAugmenting NeRFs. NeRF methods are commonly augmented with external model outputs to\nincorporate additional information. For example, approaches that incorporate scene flow often rely\non existing optical flow models for supervision (Li et al., 2021b; Turki et al., 2023; Li et al., 2023b).\nThey usually require cycle-consistency tests to filter out inconsistent flow estimations; otherwise,\nthe optimization process is prone to failure (Wang et al., 2023a). The Neural Scene Flow Prior\n(NSFP) (Li et al., 2021a), a state-of-the-art flow estimator, optimizes a neural network to estimate\nthe scene flow at each timestep (minimizing the Chamfer Loss (Fan et al., 2017)). This per-timestep\noptimization makes NSFP prohibitively expensive. In contrast, our EmerNeRF bypasses the need\nfor either pre-trained optical flow models or holistic geometry losses. Instead, our flow field is\nsupervised only by scene reconstruction losses and the flow estimation capability emerges on its\nown. Most recently, 2D signals such as semantic labels or foundation model feature vectors have\nbeen distilled into 3D space (Kobayashi et al., 2022; Kerr et al., 2023; Tsagkas et al., 2023; Shafiullah\net al., 2022), enabling semantic understanding tasks. In this work, we similarly lift visual foundation\nmodel features into 4D space and show their potential for few-shot perception tasks.\n3\nSELF-SUPERVISED SPATIAL-TEMPORAL NEURAL FIELDS\nLearning a spatial-temporal representation of a dynamic environment with a multi-sensor robot is\nchallenging due to the sparsity of observations and costs of obtaining ground truth annotations. To\nthis end, our design choices stem from the following key principles: (1) Learn a scene decompo-\nsition entirely through self-supervision and avoid using any ground-truth annotations or pre-trained\nmodels for dynamic object segmentation or optical flow. (2) Model dynamic element correspon-\ndences across time via scene flow. (3) Obtain a mutually reinforcing representation: static-dynamic\ndecomposition and flow estimation can benefit from each other. (4) Improve the semantics of scene\nrepresentations by leveraging feature lifting and distillation, enabling a range of perception tasks.\nHaving established several design principles, we are now equipped to describe EmerNeRF, a self-\nsupervised approach for efficiently representing both static and dynamic scene components. First,\n\u00a73.1 details how EmerNeRF builds a hybrid world representation with a static and dynamic field.\nThen, \u00a73.2 explains how EmerNeRF leverages an emergent flow field to aggregate temporal features\nover time, further improving its representation of dynamic components. \u00a73.3 describes the lifting\nof semantic features from pre-trained 2D models to 4D space-time, enhancing EmerNeRF\u2019s scene\nunderstanding. Finally, \u00a73.4 discusses the loss function that is minimized during training.\n3.1\nSCENE REPRESENTATIONS\nScene decomposition. To enable efficient scene decomposition, we design EmerNeRF to be a\nhybrid spatial-temporal representation. It decomposes a 4D scene into a static field S and a dynamic\nfield D, both of which are parameterized by learnable hash grids (M\u00a8uller et al., 2022) Hs and\nHd, respectively. This decoupling offers a flexible and compact 4D scene representation for time-\nindependent features hs = Hs(x) and time-varying features hd = Hd(x, t), where x = (x, y, z) is\nthe 3D location of a query point and t denotes its timestep. These features are further transformed\ninto gs and gd by lightweight MLPs (gs and gd) and used to predict per-point density \u03c3s and \u03c3d:\ngs, \u03c3s = gs(Hs(x))\ngd, \u03c3d = gd(Hd(x, t))\n(1)\n4\nPreprint\nBilinear \nInterpolation\nNeRF\nLearnable  \n2D Feature maps\n+\nx, y, z, t\nu, v, (t)\nDecomposed Features (remove \n2D-position-related patterns)\n2D-position-related \nOutput features\nShape: (C, h, w). I used (32, 80, 120)\nu, v: pixel coordinates in image plane\nNeRF\n(x, t)\n(x, t)\nx\n(x, t)\n\u03a3\nFeature \n Head\n(u, v)\nd\n+\ncd\ncsky\n\u03c1\n\u03c3d\n\u03c3scs\ngs\n(x, t)\nPE Head\nPixel \nCoordinate\nPE Patterns\nPE-Free Features\n(a)\n(b)\nStatic Field\nDynamic Field\nFinal Prediction\ngt\u22121\nd\ngt\nd\ngt+1\nd\nColor \nHead\nShadow \nHead\ng\u2032 d\nFlow Field\nSky Head\nLearnable PE Map\nvf\nvb\nFigure 2: EmerNeRF Overview. (a) EmerNeRF consists of a static, dynamic, and flow field\n(S, D, V). These fields take as input either a spatial query x or spatial-temporal query (x, t) to\ngenerate a static (feature gs, density \u03c3s) pair or a dynamic (feature g\u2032\nd, density \u03c3d) pair. Of note,\nwe use the forward and backward flows (vf and vb) to generate temporally-aggregated features g\u2032\nd\nfrom nearby temporal features gt\u22121\nd\n, gt\nd, and gt+1\nd\n(a slight abuse of notation w.r.t. Eq. (8)). These\nfeatures (along with the view direction d) are consumed by the shared color head which indepen-\ndently predicts the static and dynamic colors cs and cd. The shadow head predicts a shadow ratio \u03c1\nfrom the dynamic features. The sky head decodes a per-ray color csky for sky pixels from the view\ndirection d. (b) EmerNeRF renders the aggregated features to 2D and removes undesired positional\nencoding patterns (via a learnable PE map followed by a lightweight PE head).\nMulti-head prediction. EmerNeRF uses separate heads for color, sky, and shadow predictions. To\nmaximize the use of dense self-supervision from the static branch, the static and dynamic branches\nshare the same color head MLPcolor. This color head takes (gs, d) and (gd, d) as input, and outputs\nper-point color cs and cd for static and dynamic items, where d is the normalized view direction.\nSince the depth of the sky is ill-defined, we follow Rematas et al. (2022) and use a separate sky\nhead to predict the sky\u2019s color from the frequency-embedded view direction \u03b3(d) , where \u03b3(\u00b7) is\na frequency-based positional embedding function, as in Mildenhall et al. (2021). Lastly, as in Wu\net al. (2022), we use a shadow head MLPshadow to depict the shadows of dynamic objects. It outputs\na scalar \u03c1 \u2208 [0, 1] for dynamic objects, modulating the color intensity predicted by the static field.\nCollectively, we have:\ncs = MLPcolor(gs, \u03b3(d))\ncd = MLPcolor(gd, \u03b3(d))\n(2)\ncsky = MLPcolor sky(\u03b3(d))\n\u03c1 = MLPshadow(gd)\n(3)\nRendering. To enable highly-efficient rendering, we use density-based weights to combine results\nfrom the static field and dynamic field:\nc =\n\u03c3s\n\u03c3s + \u03c3d\n\u00b7 (1 \u2212 \u03c1) \u00b7 cs +\n\u03c3d\n\u03c3s + \u03c3d\n\u00b7 cd\n(4)\nTo render a pixel, we use K discrete samples {x1, . . . , xK} along its ray to estimate the integral of\ncolor. The final outputs are given by:\n\u02c6C =\nK\nX\ni=1\nTi\u03b1ici +\n \n1 \u2212\nK\nX\ni=1\nTi\u03b1i\n!\ncsky\n(5)\nwhere Ti = Qi\u22121\nj=1(1 \u2212 \u03b1j) is the accumulated transmittance and \u03b1i = 1 \u2212 exp(\u2212\u03c3i(xi+1 \u2212 xi)) is\nthe piece-wise opacity.\nDynamic density regularization. To facilitate static-dynamic decomposition, we leverage the fact\nthat our world is predominantly static. We regularize dynamic density by minimizing the expectation\nof the dynamic density \u03c3d, which prompts the dynamic field to produce density values only as\nneeded:\nL\u03c3d = E(\u03c3d)\n(6)\n3.2\nEMERGENT SCENE FLOW\nScene flow estimation. To capture explicit correspondences between dynamic objects and provide\na link by which to aggregate temporally-displaced features, we introduce an additional scene flow\n5\nPreprint\nfield consisting of a hash grid V := Hv(x, t) and a flow predictor MLPv. This flow field maps a\nspatial-temporal query point (x, t) to a flow vector v \u2208 R3, which transforms the query point to its\nposition in the next timestep, given by:\nv = MLPv(Hv(x, t))\nx\u2032 = x + v\n(7)\nIn practice, our flow field predicts both a forward flow vf and a backward flow vb, resulting in a\n6-dimensional flow vector for each point.\nMulti-frame feature integration. Next, we use the link provided by the predicted scene flow to\nintegrate features from nearby timesteps, using a simple weighted summation:\ng\u2032\nd = 0.25 \u00b7 gd(Hd(x + vb, t \u2212 1)) + 0.5 \u00b7 gd(Hd(x, t)) + 0.25 \u00b7 gd(Hd(x + vf, t + 1))\n(8)\nIf not otherwise specified, g\u2032\nd is used by default when the flow field is enabled (instead of gd in\nEqs. (2) and (3)). This feature aggregation module achieves three goals: 1) It connects the flow field\nto scene reconstruction losses (e.g., RGB loss) for supervision, 2) it consolidates features, denoising\ntemporal attributes for accurate predictions, and 3) each point is enriched through the shared gradient\nof its temporally-linked features, enhancing the quality of individual points via shared knowledge.\nEmergent abilities. We do not use any explicit flow supervision to guide EmerNeRF\u2019s flow estima-\ntion process. Instead, this capability emerges from our temporal aggregation step while optimizing\nscene reconstruction losses (\u00a73.4). Our hypothesis is that only temporally-consistent features benefit\nfrom multi-frame feature integration, and this integration indirectly drives the scene flow field to-\nward optimal solutions \u2014 predicting correct flows for all points. Our subsequent ablation studies in\nAppendix C.2 confirm this: when the temporal aggregation is disabled or gradients of these nearby\nfeatures are stopped, the flow field fails to learn meaningful results.\n3.3\nVISION TRANSFORMER FEATURE LIFTING\nWhile NeRFs excel at generating high-fidelity color and density fields, they lack in conveying se-\nmantic content, constraining their utility for semantic scene comprehension. To bridge this gap,\nwe lift 2D foundation model features to 4D, enabling crucial autonomous driving perception tasks\nsuch as semantic occupancy prediction. Although previous works might suggest a straightforward\napproach (Kerr et al., 2023; Kobayashi et al., 2022), directly lifting features from state-of-the-art\nvision transformer (ViT) models has hidden complexities due to positional embeddings (PEs) in\ntransformer models (Fig. 1 (h-j)). In the following sections, we detail how we enhance EmerNeRF\nwith a feature reconstruction head, uncover detrimental PE patterns in transformer models, and sub-\nsequently mitigate these issues.\nFeature reconstruction head. Analogous to the color head, we incorporate a feature head MLPfeat\nand a feature sky head MLPfeat sky to predict per-point features f and sky features fsky, given by:\nf\u2217 = MLPfeat(g\u2217), where \u2217 \u2208 {s, d}\nfsky = MLPfeat sky(\u03b3(d)).\n(9)\nSimilar to the color head, we share the feature head among the static and dynamic branches. Ren-\ndering these features similarly follows Eq. (5), given by:\n\u02c6F =\nK\nX\ni=1\nTi\u03b1ifi +\n \n1 \u2212\nK\nX\ni=1\nTi\u03b1i\n!\nfsky\n(10)\nPositional embedding patterns. We observe pronounced and undesired PE patterns when using\ncurrent state-of-the-art foundation models, notably DINOv2 (Oquab et al., 2023) (Fig. 1 (h)). These\npatterns remain fixed in images, irrespective of 3D viewpoint changes, breaking 3D multi-view\nconsistency. Our experiments (\u00a74.3) reveal that these patterns not only impair feature synthesis\nresults, but also cause a substantial reduction in 3D perception performance.\nShared learnable additive prior. We base our solution on the observation that ViTs extract feature\nmaps image-by-image and these PE patterns appear (almost) consistently across all images. This\nsuggests that a single PE feature map might be sufficient to capture this shared artifact. Accord-\ningly, we assume an additive noise model for the PE patterns; that is, they can be independently\nsubtracted from the original features to obtain PE-free features. With this assumption, we construct\na learnable and globally-shared 2D feature map U to compensate for these patterns. This process\n6\nPreprint\nis depicted in Fig. 2 (b). For a target pixel coordinate (u, v), we first volume-render a PE-free fea-\nture as in Eq. (10). Then, we bilinearly interpolate U and decode the interpolated feature using a\nsingle-layer MLPPE to obtain the PE pattern feature, which is then added to the PE-free feature.\nFormally:\n\u02c6F =\nK\nX\ni=1\nTi\u03b1ifi +\n \n1 \u2212\nk\nX\ni=1\nTi\u03b1i\n!\nfsky\n|\n{z\n}\nVolume-rendered PE-free feature\n+ MLPPE (interp ((u, v) , U))\n|\n{z\n}\nPE feature\n(11)\nThe grouped terms render \u201cPE-free\u201d features (Fig. 1 (i)) and \u201cPE\u201d patterns (Fig. 1 (j)), respectively,\nwith their sum producing the overall \u201cPE-containing\u201d features (Fig. 1 (h)).\n3.4\nOPTIMIZATION\nLoss functions. Our method decouples pixel rays and LiDAR rays to account for sensor asynchro-\nnization. For pixel rays, we use an L2 loss for colors Lrgb (and optional semantic features Lfeat), a\nbinary cross entropy loss for sky supervision Lsky, and a shadow sparsity loss Lshadow. For LiDAR\nrays, we combine an expected depth loss with a line-of-sight loss Ldepth, as proposed in Rematas\net al. (2022). This line-of-sight loss promotes an unimodal distribution of density weights along\na ray, which we find is important for clear static-dynamic decomposition. For dynamic regular-\nization, we use a density-based regularization (Eq. 6) to encourage the dynamic field to produce\ndensity values only when absolutely necessary. This dynamic regularization loss is applied to both\npixel rays (L\u03c3d(pixel)) and LiDAR rays (L\u03c3d(LiDAR)). Lastly, we regularize the flow field with a cycle\nconsistency loss Lcycle. See Appendix A.1 for details. In summary, we minimize:\nL = Lrgb + Lsky + Lshadow + L\u03c3d(pixel) + Lcycle + Lfeat\n|\n{z\n}\nfor pixel rays\n+ Ldepth + L\u03c3d(LiDAR)\n|\n{z\n}\nfor LiDAR rays\n(12)\nImplementation details. All model implementation details can be found in Appendix A.\n4\nEXPERIMENTS\nIn this section, we benchmark the reconstruction capabilities of EmerNeRF against prior meth-\nods, focusing on static and dynamic scene reconstruction, novel view synthesis, scene flow esti-\nmation, and foundation model feature reconstruction. Further ablation studies and a discussion of\nEmerNeRF\u2019s limitations can be found in Appendices C.2 and C.3, respectively.\nDataset. While there exist many public datasets with AV sensor data (Caesar et al., 2020; Sun et al.,\n2020; Caesar et al., 2021), they are heavily imbalanced, containing many simple scenarios with few\nto no dynamic objects. To remedy this, we introduce NeRF On-The-Road (NOTR), a balanced\nand diverse benchmark derived from the Waymo Open Dataset (Sun et al., 2020). NOTR features\n120 unique, hand-picked driving sequences, split into 32 static (the same split as in StreetSurf (Guo\net al., 2023)), 32 dynamic, and 56 diverse scenes across seven challenging conditions: ego-static,\nhigh-speed, exposure mismatch, dusk/dawn, gloomy, rainy, and night. We name these splits Static-\n32, Dynamic-32, and Diverse-56, respectively. This dataset not only offers a consistent benchmark\nfor static and dynamic object reconstruction, it also highlights the challenges of training NeRFs\non real-world AV data. Beyond simulation, our benchmark offers 2D bounding boxes for dynamic\nobjects, ground truth 3D scene flow, and 3D semantic occupancy\u2014all crucial for driving perception\ntasks. Additional details can be found in Appendix B.\n4.1\nRENDERING\nSetup. To analyze performance across various driving scenarios, we test EmerNeRF\u2019s scene recon-\nstruction and novel view synthesis capabilities on different NOTR splits. For scene reconstruction,\nall samples in a log are used for training. This setup probes the upper bound of each method.\nFor novel view synthesis, we omit every 10th timestep, resulting in 10% novel temporal views for\nevaluation. Our metrics include peak signal-to-noise ratio (PSNR) and structural similarity index\n(SSIM). For dynamic scenes, we further leverage ground truth bounding boxes and velocity data to\n7\nPreprint\nTable 1: Dynamic and static scene reconstruction performance.\n(a) Dynamic-32 Split\nMethods\nScene Reconstruction\nNovel View Synthesis\nFull Image\nDynamic-Only\nFull Image\nDynamic-Only\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\nDPSNR\u2191\nSSIM\u2191\nD2NeRF\n24.35\n0.645\n21.78\n0.504\n24.17\n0.642\n21.44\n0.494\nHyperNeRF\n25.17\n0.688\n22.93\n0.569\n24.71\n0.682\n22.43\n0.554\nOurs\n28.87\n0.814\n26.19\n0.736\n27.62\n0.792\n24.18\n0.670\n(b) Static-32 Split\nMethods\nStatic Scene Reconstruction\nPSNR\u2191\nSSIM\u2191\niNGP\n24.46\n0.694\nStreetSurf\n26.15\n0.753\nOurs\n29.08\n0.803\nTable 2: Scene flow estimation on the NOTR Dynamic-32 split.\nMethods\nEPE3D (m) \u2193\nAcc5(%) \u2191\nAcc10(%) \u2191\n\u03b8 (rad) \u2193\nNSFP (Li et al., 2021a)\n0.365\n51.76\n67.36\n0.84\nOurs\n0.014\n93.92\n96.27\n0.64\nidentify dynamic objects and compute \u201cdynamic-only\u201d metrics; and we benchmark against Hyper-\nNeRF (Park et al., 2021b) and D2NeRF (Wu et al., 2022), two state-of-the-art methods for mod-\neling dynamic scenes. Due to their prohibitive training cost, we only compare against them in the\nDynamic-32 split. On the Static-32 split, we disable our dynamic and flow branches, and compare\nagainst StreetSurf (Guo et al., 2023) and iNGP (M\u00a8uller et al., 2022) (as implemented by Guo et al.\n(2023)). We use the official codebases released by these methods, and adapt them to NOTR. To en-\nsure a fair comparison, we augment all methods with LiDAR depth supervision and sky supervision,\nand disable our feature field. Further details can be found in Appendix A.2.\nDynamic scene comparisons. Table 1 (a) shows that our approach consistently outperforms others\non scene reconstruction and novel view synthesis. We refer readers to Appendix C.1 for qualitative\ncomparisons. In them, we can see that HyperNeRF (Park et al., 2021b) and D2NeRF (Wu et al.,\n2022) tend to produce over-smoothed renderings and struggle with dynamic object representation.\nIn contrast, EmerNeRF excels in reconstructing high-fidelity static background and dynamic fore-\nground objects, while preserving high-frequency details (evident from its high SSIM and PSNR\nvalues). Despite D2NeRF\u2019s intent to separate static and dynamic elements, it struggles in com-\nplex driving contexts and produces poor dynamic object segmentation (as shown in Fig. C.4). Our\nmethod outperforms them both quantitatively and qualitatively. Static scene comparisons. While\nstatic scene representation is not our main focus, EmerNeRF excels in this aspect too, as evidenced\nin Table 1 (b). It outperforms state-of-the-art StreetSuRF (Guo et al., 2023) which is designed for\nstatic outdoor scenes. With the capability to model both static and dynamic components, EmerNeRF\ncan accurately represent more general driving scenes.\n4.2\nFLOW ESTIMATION\nSetup. We assess EmerNeRF on all frames of the Dynamic-32 split, benchmarking against the prior\nstate-of-the-art, NSFP (Li et al., 2021a). Using the Waymo dataset\u2019s ground truth scene flows, we\ncompute metrics consistent with Li et al. (2021a): 3D end-point error (EPE3D), calculated as the\nmean L2 distance between predictions and ground truth for all points; Acc5, representing the frac-\ntion of points with EPE3D less than 5cm or a relative error under 5%; Acc10, indicating the fraction\nof points with EPE3D under 10cm or a relative error below 10%; and \u03b8, the average angle error be-\ntween predictions and ground truths. When evaluating NSFP (Li et al., 2021a), we use their official\nimplementation and remove ground points (our approach does not require such preprocessing).\nResults. As shown in Table 2, our approach outperforms NSFP across all metrics, with significant\nleads in EPE3D, Acc5, and Acc10. While NSFP (Li et al., 2021a) employs the Chamfer distance loss\nFan et al. (2017) to solve scene flow, EmerNeRF achieves significantly better results without any\nexplicit flow supervision. These properties naturally emerge from our temporal aggregation step.\nAppendix C.2 contains additional ablation studies regarding the emergence of flow estimation.\n4.3\nLEVERAGING FOUNDATION MODEL FEATURES\nTo investigate the impact of ViT PE patterns on 3D perception and feature synthesis, we instantiate\nversions of EmerNeRF with and without our proposed PE decomposition module.\n8\nPreprint\nTable 3: Few-shot semantic occupancy prediction evaluation. We investigate the influence of\npositional embedding (PE) patterns on 4D features by evaluating semantic occupancy prediction\nperformance. We report sample-averaged micro-accuracy and class-averaged macro-accuracy.\nPE removed?\nViT model\nStatic-32\nDynamic-32\nDiverse-56\nAverage of 3 splits\nMicro Acc\nMacro Acc\nMicro Acc\nMacro Acc\nMicro Acc\nMacro Acc\nMicro Acc\nMacro Acc\nNo\nDINOv1\n43.12%\n52.71%\n47.51%\n54.46%\n43.19%\n51.11%\n44.60%\n52.76%\nYes\nDINOv1\n55.02%\n57.13%\n57.65%\n57.77%\n54.56%\n55.13%\n55.74%\n56.67%\nRelative Improvement\n+27.60%\n+8.38%\n+21.35%\n+6.07%\n+26.32%\n+7.87%\n+24.95%\n+7.42%\nNo\nDINOv2\n38.73%\n50.30%\n51.43%\n57.03%\n45.22%\n54.37%\n45.13%\n53.90%\nYes\nDINOv2\n63.21%\n59.41%\n65.08%\n60.82%\n57.86%\n59.00%\n62.05%\n59.74%\nRelative Improvement\n+63.22%\n+18.11%\n+26.53%\n+6.65%\n+27.95%\n+8.51%\n+37.50%\n+10.84%\nTable 4: Feature synthesis results. We report the feature-PNSR values under different settings.\nPE removed?\nViT model\nStatic-32\nDynamic-32\nDiverse-56\nNo\nDINOv1\n23.35\n23.37\n23.78\nYes\nDINOv1\n23.57 (+0.23)\n23.52 (+0.15)\n23.92 (+0.14)\nNo\nDINOv2\n21.87\n22.34\n22.79\nYes\nDINOv2\n22.70 (+0.83)\n22.80 (+0.45)\n23.21 (+0.42)\nSetup. We evaluate EmerNeRF\u2019s few-shot perception capabilities using the Occ3D dataset (Tian\net al., 2023). Occ3D provides 3D semantic occupancy annotations for the Waymo dataset (Sun\net al., 2020) in voxel sizes of 0.4m and 0.1m (we use 0.1m). For each sequence, we annotate every\n10th frame with ground truth information, resulting in 10% labeled data. Occupied coordinates are\ninput to pre-trained EmerNeRF models to compute feature centroids per class. Features from the\nremaining 90% of frames are then queried and classified based on their nearest feature centroid.\nWe report both micro (sample-averaged) and macro (class-averaged) classification accuracies. All\nmodels are obtained from the scene reconstruction setting, i.e., all views are used for training.\nResults. Table 3 compares the performance of PE-containing 4D features to their PE-free counter-\nparts. Remarkably, EmerNeRF with PE-free DINOv2 (Oquab et al., 2023) features sees a maximum\nrelative improvement of 63.22% in micro-accuracy and an average increase of 37.50% over its PE-\ncontaining counterpart. Intriguingly, although the DINOv1 (Caron et al., 2021) model might appear\nvisually unaffected (Fig. C.5), our results indicate that directly lifting PE-containing features to 4D\nspace-time is indeed problematic. With our decomposition, PE-free DINOv1 features witness an\naverage relative boost of 24.95% in micro-accuracy. As another illustration of PE patterns\u2019 impact,\nby eliminating PE patterns, the improved performance of DINOv2 over DINOv1 carries over to 3D\nperception (e.g., Static32 micro-accuracy).\nFeature synthesis results. Table 4 compares the feature-PSNR of PE-containing and PE-free mod-\nels, showing marked improvements in feature synthesis quality when using our proposed PE decom-\nposition method, especially for DINOv2 (Oquab et al., 2023). While DINOv1 (Caron et al., 2021)\nappears to be less influenced by PE patterns, our method unveils their presence, further showing that\neven seemingly unaffected models can benefit from PE pattern decomposition.\n5\nCONCLUSION\nIn this work, we present EmerNeRF, a simple yet powerful approach for learning 4D neural repre-\nsentations of dynamic scenes. EmerNeRF effectively captures scene geometry, appearance, motion,\nand any additional semantic features by decomposing scenes into static and dynamic fields, learning\nan emerged flow field, and optionally lifting foundation model features to a resulting hybrid world\nrepresentation. EmerNeRF additionally removes problematic positional embedding patterns that\nappear when employing Transformer-based foundation model features. Notably, all of these tasks\n(save for foundation model feature lifting) are learned in a self-supervised fashion, without relying\non ground truth object annotations or pre-trained models for dynamic object segmentation or optical\nflow estimation. When evaluated on NOTR, our carefully-selected subset of 120 challenging driv-\ning scenes from the Waymo Open Dataset (Sun et al., 2020), EmerNeRF achieves state-of-the-art\nperformance in sensor simulation, significantly outperforming previous methods on both static and\n9\nPreprint\ndynamic scene reconstruction, novel view synthesis, and scene flow estimation. Exciting areas of\nfuture work include further exploring capabilities enabled or significantly improved by harnessing\nfoundation model features: few-shot, zero-shot, and auto-labeling via open-vocabulary detection.\nETHICS STATEMENT\nThis work primarily focuses on autonomous driving data representation and reconstruction. Ac-\ncordingly, we use open datasets captured in public spaces that strive to preserve personal privacy\nby leveraging state-of-the-art object detection techniques to blur people\u2019s faces and vehicle license\nplates. However, these are instance-level characteristics. What requires more effort to manage (and\ncould potentially lead to greater harm) is maintaining a diversity of neighborhoods, and not only\nin terms of geography, but also population distribution, architectural diversity, and data collection\ntimes (ideally repeated traversals uniformly distributed throughout the day and night, for example).\nWe created the NOTR dataset with diversity in mind, hand-picking scenarios from the Waymo Open\nDataset (Sun et al., 2020) to ensure a diversity of neighborhoods and scenario types (e.g., static,\ndynamic). However, as in the parent Waymo Open Dataset, the NOTR dataset contains primarily\nurban geographies, collected from only a handful of cities in the USA.\nREPRODUCIBILITY STATEMENT\nWe present our method in \u00a73, experiments and results in \u00a74, implementation details and abla-\ntion studies in Appendix A. We benchmark previous approaches and our proposed method us-\ning publicly-available data and include details of the derived dataset in Appendix B. Additional\nvisualizations, code, models, and data are anonymously available either in the appendix or at\nhttps://emernerf.github.io.\nREFERENCES\nShir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep ViT features as dense visual\ndescriptors. arXiv preprint arXiv:2112.05814, 2021.\nAayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4d visualiza-\ntion of dynamic events from unconstrained multi-view videos. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 5366\u20135375, 2020.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and\nPratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5855\u20135864,\n2021.\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf:\nAnti-aliased grid-based neural radiance fields. arXiv preprint arXiv:2304.06706, 2023.\nHolger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush\nKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A multimodal dataset for au-\ntonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020.\nHolger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher,\nOscar Beijbom, and Sammy Omari. nuPlan: A closed-loop ml-based planning benchmark for\nautonomous vehicles. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshop on Autonomous Driving: Perception, Prediction and Planning, 2021.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 9650\u20139660, 2021.\nHaoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object recon-\nstruction from a single image. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 605\u2013613, 2017.\n10\nPreprint\nJiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias\nNie\u00dfner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH\nAsia 2022 Conference Papers, pp. 1\u20139, 2022.\nJianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang, Chenjing Ding,\nDongliang Wang, and Yikang Li. Streetsurf: Extending multi-view implicit surface reconstruction\nto street views. arXiv preprint arXiv:2306.04988, 2023.\nJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Lan-\nguage embedded radiance fields. In International Conference on Computer Vision (ICCV), 2023.\nSosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via\nfeature field distillation. In Advances in Neural Information Processing Systems, pp. 23311\u2013\n23330, 2022.\nRuilong Li, Hang Gao, Matthew Tancik, and Angjoo Kanazawa. Nerfacc: Efficient sampling accel-\nerates nerfs. arXiv preprint arXiv:2305.04966, 2023a.\nTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim,\nTanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video\nsynthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 5521\u20135531, 2022.\nXueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey. Neural scene flow prior. Advances in\nNeural Information Processing Systems, 34:7838\u20137851, 2021a.\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-\ntime view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 6498\u20136508, 2021b.\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neu-\nral dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 4273\u20134284, 2023b.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99\u2013106, 2021.\nThomas M\u00a8uller.\ntiny-cuda-nn, April 2021.\nURL https://github.com/NVlabs/\ntiny-cuda-nn.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315,\n2022.\nMaxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\nJulian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for\ndynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2856\u20132865, 2021.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 5865\u20135874, 2021a.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,\nRicardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for\ntopologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021b.\nChristian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon\nBarron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis\nin unbounded scenes. ACM Transactions on Graphics (TOG), 42(4):1\u201312, 2023.\n11\nPreprint\nKonstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi,\nThomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 12932\u201312942, 2022.\nNur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam.\nClip-fields: Weakly supervised semantic fields for robotic memory. arXiv preprint arXiv: Arxiv-\n2210.05663, 2022.\nPrafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares Andrei Ambrus, Adrien Gaidon,\nWilliam T Freeman, Fredo Durand, Joshua B Tenenbaum, and Vincent Sitzmann. Neural ground-\nplans: Persistent neural scene representations from a single image. In The Eleventh International\nConference on Learning Representations, 2022.\nPei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui,\nJames Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for au-\ntonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp. 2446\u20132454, 2020.\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srini-\nvasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural\nview synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 8248\u20138258, 2022.\nXiaoyu Tian, Tao Jiang, Longfei Yun, Yue Wang, Yilun Wang, and Hang Zhao. Occ3d: A large-scale\n3d occupancy prediction benchmark for autonomous driving. arXiv preprint arXiv:2304.14365,\n2023.\nNikolaos Tsagkas, Oisin Mac Aodha, and Chris Xiaoxuan Lu.\nVl-fields: Towards language-\ngrounded neural implicit spatial representations. arXiv preprint arXiv:2305.12427, 2023.\nHaithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan.\nSuds: Scalable urban\ndynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12375\u201312385, 2023.\nLiao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye\nWu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering in real-time.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n13524\u201313534, 2022.\nQianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski,\nand Noah Snavely. Tracking everything everywhere all at once. arXiv preprint arXiv:2306.05422,\n2023a.\nZian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan\nGojcic, Wenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations\nfor inverse rendering of urban scenes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 8370\u20138380, 2023b.\nTianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u02c6 2nerf:\nSelf-supervised decoupling of dynamic and static objects from a monocular video. Advances in\nNeural Information Processing Systems, 35:32653\u201332666, 2022.\nZe Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and\nRaquel Urtasun. Unisim: A neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 1389\u20131399, 2023.\n12\nPreprint\nA\nIMPLEMENTATION DETAILS\nIn this section, we discuss the implementation details of EmerNeRF. Our code is publicly available,\nand the pre-trained models will be released upon request. See emernerf.github.io for more\ndetails.\nA.1\nEMERNERF IMPLEMENTATION DETAILS\nA.1.1\nDATA PROCESSING\nData source. Our sequences are sourced from the waymo open dataset scene flow1 ver-\nsion, which augments raw sensor data with point cloud flow annotations. For camera images, we\nemploy three frontal cameras: FRONT LEFT, FRONT, and FRONT RIGHT, resizing them to a reso-\nlution of 640\u00d7960 for both training and evaluation. Regarding LiDAR point clouds, we exclusively\nuse the first return data (ignoring the second return data). We sidestep the rolling shutter issue in\nLiDAR sensors for simplicity and leave it for future exploration. Dynamic object masks are derived\nfrom 2D ground truth camera bounding boxes, with velocities determined from the given metadata.\nOnly objects exceeding a velocity of 1 m/s are classified as dynamic, filtering out potential sen-\nsor and annotation noise. For sky masks, we utilize the Mask2Former-architectured ViT-Adapter-L\nmodel pre-trained on ADE20k. Note that, the dynamic object masks and point cloud flows are used\nfor evaluation only.\nFoundation model feature extraction. We employ the officially released checkpoints of DINOv2\nOquab et al. (2023) and DINOv1 (Caron et al., 2021), in conjunction with the feature extractor\nimplementation from Amir et al. (2021). For DINOv1, we utilize the ViT-B/16, resizing images to\n640\u00d7960 and modifying the model\u2019s stride to 8 to further increase the resolution of extracted feature\nmaps. For DINOv2, we use the ViT-B/14 variant, adjusting image dimensions to 644\u00d7966 and using\na stride of 7. Given the vast size of the resultant feature maps, we employ PCA decomposition to\nreduce the feature dimension from 768 to 64 and normalize these features to the [0, 1] range.\nA.1.2\nEMERNERF\nRepresentations. We build all our scene representations based on iNGP (M\u00a8uller et al., 2022) from\ntiny-cuda-nn (M\u00a8uller, 2021), and use nerfacc toolkit (Li et al., 2023a) for acceleration. Fol-\nlowing Barron et al. (2023), our static hash encoder adopts a resolution spanning 24 to 213 over\n10 levels, with a fixed feature length of 4 for all hash entries. Features at each level are capped at\n220 in size. With these settings, our model comprises approximately 30M parameters \u2014 a saving\nof 18M compared to the StreetSurf\u2019s SDF representation (Guo et al., 2023). For our dynamic hash\nencoder, we maintain a similar architecture, but with a maximum hash feature map size of 218. Our\nflow encoder is identical to the dynamic encoder. To address camera exposure variations in the\nwild, 16-dimensional appearance embeddings are applied per image for scene reconstruction and\nper camera for novel view synthesis. While our current results are promising, we believe that larger\nhash encoders and MLPs could further enhance performance. See our code for more details.\nPositional embedding (PE) patterns. We use a learnable feature map, denoted as U, with dimen-\nsions 80\u00d7120\u00d732 (H \u00d7W \u00d7C) to accommodate the positional embedding patterns, as discussed\nin the main text. To decode the PE pattern for an individual pixel located at (u, v), we first sample\na feature vector from U using F.grid sample. Subsequently, a linear layer decodes this feature\nvector to produce the final PE features.\nScene range. To define the axis-aligned bounding box (AABB) of the scene, we utilize LiDAR\npoints. In practice, we uniformly subsample the LiDAR points by a factor of 4 and find the scene\nboundaries by computing the 2% and 98% percentiles of the world coordinates within the LiDAR\npoint cloud. However, the LiDAR sensor typically covers only a 75-meter radius around the vehicle.\nConsequently, an unrestricted contraction mechanism is useful to ensure better performance. Fol-\nlowing the scene contraction method detailed in Reiser et al. (2023), we use a piecewise-projective\ncontraction function to project the points falling outside the determined AABB.\n1console.cloud.google.com/storage/browser/waymo_open_dataset_scene_flow\n13\nPreprint\nMulti-level sampling. In line with findings in Mildenhall et al. (2021); Barron et al. (2021), we\nobserve that leveraging extra proposal networks enhances both rendering quality and geometry esti-\nmation. Our framework integrates a two-step proposal sampling process, using two distinct iNGP-\nbased proposal models. In the initial step, 128 samples are drawn using the first proposal model\nwhich consists of 8 levels, each having a 1-dim feature. The resolution for this model ranges from\n24 to 29, and each level has a maximum hash map capacity of 220. For the subsequent sampling\nphase, 64 samples are taken using the second proposal model. This model boasts a maximum res-\nolution of 211, but retains the other parameters of the first model. To counteract the \u201cz-aliasing\u201d\nissue\u2014particularly prominent in driving sequences with thin structures like traffic signs and light\npoles, we further incorporate the anti-aliasing proposal loss introduced by Barron et al. (2023) dur-\ning proposal network optimization. A more thorough discussion on this is available in Barron et al.\n(2023). Lastly, we do not employ spatial-temporal proposal networks, i.e., we don\u2019t parameterize\nthe proposal networks with a temporal dimension. Our current implementation already can capture\ntemporal variations from the final scene fields, and we leave integrating a temporal dimension in\nproposal models for future exploration. For the final rendering, 64 points are sampled from the\nscene fields.\nPixel importance sampling. Given the huge image sizes, we prioritize hard examples for efficient\ntraining. Every 2k steps, we render RGB images at a resolution reduced by a factor of 32 and\ncompute the color errors against the ground truths. For each training batch, 25% of the training\nrays are sampled proportionally based on these color discrepancies, while the remaining 75% are\nuniformly sampled. This strategy is similar to Wang et al. (2023a) and Guo et al. (2023).\nA.1.3\nOPTIMIZATION\nAll components in EmerNeRF are trained jointly in an end-to-end manner.\nLoss functions. As we discussed in \u00a73.4, our total loss function is\nL = Lrgb + Lsky + Lshadow + L\u03c3d(pixel) + Lcycle + Lfeat\n|\n{z\n}\nfor pixel rays\n+ Ldepth + L\u03c3d(LiDAR)\n|\n{z\n}\nfor LiDAR rays\n(A1)\nWith r representing a ray and Nr its total number, the individual loss components are defined as:\n1. RGB loss (Lrgb): Measures the difference between the predicted color ( \u02c6C(r)) and the ground\ntruth color (C(r)) for each ray.\nLrgb = 1\nNr\nX\nr\n|| \u02c6C(r) \u2212 C(r)||2\n2\n(A2)\n2. Sky loss (Lsky): Measures the discrepancy between the predicted opacity of rendered rays and the\nactual sky masks. Specifically, sky regions should exhibit transparency. The binary cross entropy\n(BCE) loss is employed to evaluate this difference. In the equation, \u02c6O(r) is the accumulated\nopacity of ray r as in Equation (5). M(r) is the ground truth mask with 1 for the sky region and\n0 otherwise.\nLsky = 0.001 \u00b7 1\nNr\nX\nr\nBCE\n\u0010\n\u02c6O(r), 1 \u2212 M(r)\n\u0011\n(A3)\n3. Shadow loss (Lshadow): Penalizes the accumulated squared shadow ratio, following Wu et al.\n(2022).\nLshadow = 0.01 \u00b7 1\nNr\nX\nr\n K\nX\ni=1\nTi\u03b1i\u03c12\ni\n!\n(A4)\n4. Dynamic regularization (L\u03c3d(pixel) and L\u03c3d(LiDAR)): Penalizes the mean dynamic density of\nall points across all rays. This encourages the dynamic branch to generate density only when\nnecessary.\nL\u03c3\u2308 = 0.01 \u00b7 1\nNr\nX\nr\n1\nK\nK\nX\ni=1\n\u03c3d(r, i)\n(A5)\n14\nPreprint\n5. Cycle consistency regularization (Lcycle): Self-regularizes the scene flow prediction. This loss\nencourages the congruence between the forward scene flow at time t and its corresponding back-\nward scene flow at time t + 1.\nLcycle = 0.01\n2 E\nh\n[sg(vf(x, t)) + v\u2032\nb (x + vf(x, t), t + 1)]2 \u0002\nsg(vb(x, t)) + v\u2032\nf(x + vb(x, t), t \u2212 1)\n\u00032i\n(A6)\nwhere vf(x, t) denotes forward scene flow at time t, v\u2032\nb(x+vf(x, t), t+1) is predicted backward\nscene flow at the forward-warped position at time t + 1, sg means stop-gradient operation, and E\nrepresents the expectation, i.e., averaging over all sample points.\n6. Feature loss (Lfeat): Measures the difference between the predicted semantic feature ( \u02c6F(r)) and\nthe ground truth semantic feature (F(r)) for each ray.\nLfeat = 0.5 \u00b7 1\nNr\n|| \u02c6F(r) \u2212 F(r)||2\n2\n(A7)\n7. Depth Loss (Ldepth): Combines the expected depth loss and the line-of-sight loss, as described in\nRematas et al. (2022). The expected depth loss ensures the depth predicted through the volumetric\nrendering process aligns with the LiDAR measurement\u2019s depth. The line-of-sight loss includes\ntwo components: a free-space regularization term that ensures zero density for points before the\nLiDAR termination points and a near-surface loss promoting density concentration around the\ntermination surface. With a slight notation abuse, we have:\nLexp depth = Er\nh\n|| \u02c6Z(r) \u2212 Z(r)||2\n2\ni\n(A8)\nLline-of-sight = Er\n\"Z Z(r)\u2212\u03f5\ntn\nw(t)2dt\n#\n+ Er\n\"Z Z(r)+\u03f5\nZ(r)\u2212\u03f5\n(w(t) \u2212 K\u03f5 (t \u2212 Z(r)))2\n#\n(A9)\nLdepth = Lexp depth + 0.1 \u00b7 Lline-of-sight\n(A10)\nwhere \u02c6Z(r) represents rendered depth values and Z(r) stands for the ground truth LiDAR range\nvalues. Here, the variable t indicates an offset from the origin towards the ray\u2019s direction, dif-\nferentiating it from the temporal variable t discussed earlier. w(t) specifies the blending weights\nof a point along the ray. K\u03f5(x) = N(0, (\u03f5/3)2) represents a kernel integrating to one, where\nN is a truncated Gaussian. The parameter \u03f5 determines the strictness of the line-of-sight loss.\nFollowing the suggestions in Rematas et al. (2022), we linearly decay \u03f5 from 6.0 to 2.5 during\nthe whole training process.\nTraining. We train our models for 25k iterations using a batch size of 8196. In static scenarios,\nwe deactivate the dynamic and flow branches. Training durations on a single A100 GPU are as\nfollows: for static scenes, feature-free training requires 33 minutes, while the feature-embedded\napproach takes 40 minutes. Dynamic scene training, which incorporates the flow field and feature\naggregation, extends the durations to 2 hours for feature-free and 2.25 hours for feature-embedded\nrepresentations. To mitigate excessive regularization when the geometry prediction is not reliable,\nwe enable line-of-sight loss after the initial 2k iterations and subsequently halve its coefficient every\n5k iterations.\nA.2\nBASLINE IMPLEMENTATIONS\nFor HyperNeRF (Park et al., 2021b) and D2NeRF (Wu et al., 2022), we modify their official JAX\nimplementations to fit our NOTR dataset. Both models are trained for 100k iterations with a batch\nsize of 4096. Training and evaluation for each model take approximately 4 hours on 4 A100 GPUs\nper scene. To ensure comparability, we augment both models with a sky head and provide them\nwith the same depth and sky supervision as in our model. However, since neither HyperNeRF nor\nD2NeRF inherently supports separate sampling of pixel rays and LiDAR rays, we project LiDAR\npoint clouds onto the image plane and apply an L2 loss between predicted depth and rendered\ndepth. We compute a scale factor from the AABBs derived from LiDAR data to ensure scenes are\nencapsulated within their predefined near-far range. For StreetSuRF (Guo et al., 2023), we adopt\ntheir official implementation but deactivate the monocular \u201cnormal\u201d supervision for alignment with\nour setup. Additionally, to ensure both StreetSuRF and EmerNeRF use the same data, we modify\ntheir code to accommodate our preprocessed LiDAR rays.\n15\nPreprint\n(1) Static\n(2) Dynamic\n(3) Ego-static\n(4) High-speed\n(5) Rainy\n(6) Dusk/Dawn\n(7) Nighttime\n(8) Mismatch exposure\n(9) Gloomy\nFigure B.1: Samples from the NOTR Benchmark. This comprehensive benchmark contains (1) 32\nstatic scenes, (2) 32 dynamic scenes, and 56 additional scenes across seven categories: (3) ego-static,\n(4) high-speed, (5) rainy, (6) dusk/dawn, (7) nighttime, (8) mismatched exposure, and (9) gloomy\nconditions. We include LiDAR visualization in each second row and sky masks in each third row.\nB\nNERF ON-THE-ROAD (NOTR) DATASET\nAs neural fields gain more attention in autonomous driving, there is an evident need for a compre-\nhensive dataset that captures diverse on-road driving scenarios for NeRF evaluations. To this end,\nwe introduce NeRF On-The-Road (NOTR) dataset, a benchmark derived from the Waymo Open\nDataset (Sun et al., 2020). NOTR features 120 unique driving sequences, split into 32 static scenes,\n32 dynamic scenes, and 56 scenes across seven challenging conditions: ego-static, high-speed, ex-\nposure mismatch, dusk/dawn, gloomy, rainy, and nighttime. Examples are shown in Figure B.1.\nBeyond images and point clouds, NOTR provides additional resources pivotal for driving percep-\ntion tasks: bounding boxes for dynamic objects, ground-truth 3D scene flow, and 3D semantic\noccupancy. We hope this dataset can promote NeRF research in driving scenarios, extending the\napplications of NeRFs from mere view synthesis to motion understanding, e.g., 3D flows, and scene\ncomprehension, e.g., semantics.\nRegarding scene classifications, our static scenes adhere to the split presented in StreetSuRF (Guo\net al., 2023), which contains clean scenes with no moving objects. The dynamic scenes, which\nare frequently observed in driving logs, are chosen based on lighting conditions to differentiate\nthem from those in the \u201cdiverse\u201d category. The Diverse-56 samples may also contain dynamic\nobjects, but they are split primarily based on the ego vehicle\u2019s state (e.g., ego-static, high-speed,\ncamera exposure mismatch), weather condition (e.g., rainy, gloomy), and lighting difference (e.g.,\nnighttime, dusk/dawn). We provide the sequence IDs of these scenes in our codebase.\n16\nPreprint\nFigure C.1: Qualitative scene reconstruction comparisons.\nC\nADDITIONAL RESULTS\nC.1\nQUALITATIVE RESULTS\nQualitative comparison.\nFigures C.1 and C.2 show qualitative comparisons between our\nEmerNeRF and previous methods under the scene reconstruction setting, while Figure C.3 high-\nlights the enhanced static-dynamic decomposition of our method compared to D2NeRF (Wu et al.,\n2022). Moreover, Figure C.4 illustrates our method\u2019s superiority in novel view synthesis tasks\nagainst HyperNeRF (Park et al., 2021b) and D2NeRF (Wu et al., 2022). Our method consistently\ndelivers more realistic and detailed renders. Notably, HyperNeRF does not decompose static and\ndynamic components; it provides only composite renders, while our method not only renders high-\nfidelity temporal views but also precisely separates static and dynamic elements. Furthermore, our\nmethod introduces the novel capability of generating dynamic scene flows.\nC.2\nABLATION STUDIES\nTable C.1 provides ablation studies to understand the impact of other components on scene re-\nconstruction, novel view synthesis, and scene flow estimation. For these ablation experiments, all\nmodels are trained for 8k iterations, a shorter duration compared to the 25k iterations in the primary\nexperiments. From our observations: (a) Using a full 4D iNGP without the static field results in\nthe worst results, a consequence of the lack of multi-view supervision. (b-e) Introducing hybrid\nrepresentations consistently improves the results. (c) Omitting the temporal aggregation step or (d)\nfreezing temporal feature gradients (stop the gradients of gt\u22121\nd\nand gt+1\nd\nin Fig. 2) negates the emer-\ngence of flow estimation ability, as evidenced in the final column. Combining all these settings\nyields the best results.\n17\nPreprint\nFigure C.2: Qualitative scene reconstruction comparisons.\nTable C.1: Ablation study.\nSetting\nScene Reconstruction\nNovel View Synthesis\nScene Flow estimation\nFull Image\nDynamic-Only\nFull Image\nDynamic-Only\nFlow\nPSNR\u2191\nPSNR\u2191\nPSNR\u2191\nPSNR\u2191\nAcc5(%) \u2191\n(a) 4D-Only iNGP\n26.55\n22.30\n26.02\n21.03\n-\n(b) no flow\n26.92\n23.82\n26.33\n23.81\n-\n(c) no temporal aggregation\n26.95\n23.90\n26.60\n23.98\n4.53%\n(d) freeze temporally displaced features before aggregation\n26.93\n24.02\n26.78\n23.81\n3.87%\n(e) ours default\n27.21\n24.41\n26.93\n24.07\n89.74%\n18\nPreprint\nFigure C.3: Scene decomposition comparisons. Note that we utilize a green background to blend\ndynamic objects, whereas D2NeRF\u2019s results are presented with a white background.\nC.3\nLIMITATIONS\nConsistent with other methods, EmerNeRF does not optimize camera poses and is prone to rolling\nshutter effects of camera and LiDAR sensors. Future work to address this issue can investigate joint\noptimization of pixel-wise camera poses, and compensation for LiDAR rolling shutter alongside\nscene representations. Moreover, the balance between geometry and rendering quality remains a\ntrade-off and needs further study. Lastly, EmerNeRF occasionally struggles with estimating the\nmotion of slow-moving objects when the ego-vehicle is moving fast\u2014a challenge exacerbated by\nthe limited observations. We leave these for future research.\nC.4\nVISUALIZATIONS\n19\nPreprint\n(a) GT Image\n(b) Ours- Novel View Synthesis\n(c) Ours- Novel Dynamic RGB Decomposition\n(d) Ours- Novel Scene Flow Synthesis\n(e) HyperNeRF\n(f) D2NeRF\n(g) D2NeRF Novel Dynamic RGB Decomposition\nFigure C.4: Qualitative novel temporal view comparison.\n20\nPreprint\n(a) GT RGB Images\n(b) GT DINOv2\n(c) Reconstructed DINOv2\n(d) Decomposed PE-free DINOv2\n(e) Decomposed DINOv2 PE patterns\n(f) GT DINOv1\n(g) Reconstructed DINOv1\n(h) Decomposed PE-free DINOv1\n(i) Decomposed DINOv1 PE patterns\nFigure C.5: Different positional embedding patterns in DINOv1 (Caron et al., 2021) and DINOv2\nmodels(Oquab et al., 2023)\n21\nPreprint\n(a) GT RGB\n(b) Rendered RGB\n(c) Decomposed Dynamic Depth\n(d) Emerged Scene Flow\n(e) GT DINOv2 Features\n(h) Stacking dynamic RGB on Decomposed PE-Free Static DINOV2 Features\n(g) Decomposed PE Patterns\n(f) Reconstructed DINOv2 Features\nFigure C.6: Scene reconstruction visualizations of EmerNeRF. We show (a) GT RGB images,\n(b) reconstructed RGB images, (c) decomposed dynamic depth, (d) emerged scene flows, (e) GT\nDINOv2 features, (f) reconstructed DINOv2 features, and (g) decomposed PE patterns. (h) We also\nstack colors of dynamic objects onto decomposed PE-free static DINOv2 features.\n22\nPreprint\n(a) GT RGB\n(b) Rendered RGB\n(c) Decomposed Dynamic Depth\n(d) Emerged Scene Flow\n(e) GT DINOv2 Features\n(h) Stacking dynamic RGB on Decomposed PE-Free Static DINOV2 Features\n(g) Decomposed PE Patterns\n(f) Reconstructed DINOv2 Features\nFigure C.7: Scene reconstruction visualizations of EmerNeRF under different lighting conditions.\nWe show (a) GT RGB images, (b) reconstructed RGB images, (c) decomposed dynamic depth, (d)\nemerged scene flows, (e) GT DINOv2 features, (f) reconstructed DINOv2 features, and (g) decom-\nposed PE patterns. (h) We also stack colors of dynamic objects onto decomposed PE-free static\nDINOv2 features. EmerNeRF works well under dark environments (left) and discerns challenging\nscene flows in complex environments (right). Colors indicate scene flows\u2019 norms and directions.\n23\nPreprint\n(a) GT RGB\n(b) Rendered RGB\n(c) Decomposed Dynamic Depth\n(d) Emerged Scene Flow\n(e) GT DINOv2 Features\n(h) Stacking dynamic RGB on Decomposed PE-Free Static DINOV2 Features\n(g) Decomposed PE Patterns\n(f) Reconstructed DINOv2 Features\nFigure C.8: Scene reconstruction visualizations of EmerNeRF under differet lighting and weather\nconditions. We show (a) GT RGB images, (b) reconstructed RGB images, (c) decomposed dynamic\ndepth, (d) emerged scene flows, (e) GT DINOv2 features, (f) reconstructed DINOv2 features, and\n(g) decomposed PE patterns. (h) We also stack colors of dynamic objects colors onto decomposed\nPE-free static DINOv2 features. EmerNeRF works well under gloomy environments (left) and\ndiscerns fine-grained speed information (right).\n24\n"
  }
]