[
  {
    "title": "Nougat: Neural Optical Understanding for Academic Documents",
    "link": "https://arxiv.org/pdf/2308.13418.pdf",
    "upvote": "33",
    "text": "Nougat: Neural Optical Understanding for Academic Documents\nLukas Blecher\u2217\nGuillem Cucurull\nThomas Scialom\nRobert Stojnic\nMeta AI\nAbstract\nScientific knowledge is predominantly stored in books and scientific journals, often in the form of\nPDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical\nexpressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual\nTransformer model that performs an Optical Character Recognition (OCR) task for processing\nscientific documents into a markup language, and demonstrate the effectiveness of our model on a\nnew dataset of scientific documents. The proposed approach offers a promising solution to enhance\nthe accessibility of scientific knowledge in the digital age, by bridging the gap between human-\nreadable documents and machine-readable text. We release the models and code to accelerate future\nwork on scientific text recognition.\n1\nIntroduction\nThe majority of scientific knowledge is stored in books or published in scientific journals, most commonly in the\nPortable Document Format (PDF). Next to HTML, PDFs are the second most prominent data format on the internet,\nmaking up 2.4% of common crawl [1]. However, the information stored in these files is very difficult to extract into\nany other formats. This is especially true for highly specialized documents, such as scientific research papers, where the\nsemantic information of mathematical expressions is lost.\nExisting Optical Character Recognition (OCR) engines, such as Tesseract OCR [2], excel at detecting and classifying\nindividual characters and words in an image, but fail to understand the relationship between them due to their line-by-line\napproach. This means that they treat superscripts and subscripts in the same way as the surrounding text, which is a\nsignificant drawback for mathematical expressions. In mathematical notations like fractions, exponents, and matrices,\nrelative positions of characters are crucial.\nConverting academic research papers into machine-readable text also enables accessibility and searchability of science\nas a whole. The information of millions of academic papers can not be fully accessed because they are locked behind\nan unreadable format. Existing corpora, such as the S2ORC dataset [3], capture the text of 12M2 papers using GROBID\n[4], but are missing meaningful representations of the mathematical equations.\nTo this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted\nmarkup text.\nThe primary contributions in this paper are\n\u2022 Release of a pre-trained model capable of converting a PDF to a lightweight markup language. We release the\ncode and the model on GitHub3\n\u2022 We introduce a pipeline to create dataset for pairing PDFs to source code\n\u2022 Our method is only dependent on the image of a page, allowing access to scanned papers and books\n\u2217Correspondence to: lblecher@meta.com\n2The paper reports 8.1M papers but the authors recently updated the numbers on the GitHub page https://github.com/allenai/s2orc\n3https://github.com/facebookresearch/nougat\narXiv:2308.13418v1  [cs.LG]  25 Aug 2023\nNougat\nBlecher et al.\nExample\nThis is an abstract. Lorem\nipsum dolor sit amet,\nconsectetur adipisicing elit,\nsed do eiusmod tempor\nincididunt ut labore et dolore\nmagna aliqua.\nFigure 1: Our simple end-to-end architecture followin Donut [28]. The Swin Transformer encoder takes a document\nimage and converts it into latent embeddings, which are subsequently converted to a sequence of tokens in a auto-\nregressive manner\n2\nRelated Work\nOptical Character Recognition (OCR) is an extensively researched field in computer vision for a variety applications,\nsuch as document digitalization [2, 5], handwriting recognition and scene text recognition [6\u20138].\nMore concretely, recognizing mathematical expressions is a heavily researched subtopic. Grammar based methods\n[9\u201311] for handwritten mathematical expressions were improved upon by different encoder-decoder models. The\nfully convolutional model [12] was succeeded by various RNN decoder models [13\u201317], both for handwritten and\nprinted formulas. Recently, the decoder [18, 19] as well as the encoder [20] were replaced with the Transformer [21]\narchitecture.\nVisual Document Understanding (VDU) is another related topic of deep learning research and focuses on extracting\nrelevant information of a variety of document types. Previous works depend on pre-trained models that learn to extract\ninformation by jointly modeling text and layout information using the Transformer architecture. The LayoutLM model\nfamily [22\u201324] uses masked layout prediction task to capture the spatial relationships between different document\nelements.\nOpen source solutions with a related goal as ours include GROBID [4], which parses digital-born scientific documents\nto XML with a focus on the bibliographic data and pdf2htmlEX [25], that converts digital-born PDFs to HTML\nwhile preserving the layout and appearance of the document. However, both solutions can not recover the semantic\ninformation of mathematical expressions.\n3\nModel\nPrevious VDU methods either rely on OCR text from a third party tool [22, 23, 26] or focus on document types such\nas receipts, invoices or form-like documents [27]. Recent studies [28, 29] show that an external OCR engine is not\nnecessarily needed to achieve competitive results in VDU.\nThe architecture is a encoder-decoder transformer [21] architecture, that allows for an end-to-end training procedure.\nWe build on the Donut [28] architecture. The model does not require any OCR related inputs or modules. The text is\nrecognized implicitly by the network. See Fig. 1 for an overview of the approach.\nEncoder\nThe visual encoder receives a document image x \u2208 R3\u00d7H0\u00d7W0, crops the margins and resizes the image to\nfit in a fixed rectangle of size (H, W). If the image is smaller than the rectangle, additional padding is added to ensure\neach image has the same dimensionality. We use a Swin Transformer [30], a hierarchical vision transformer [31] that\nsplits the image into non-overlapping windows of fixed size and applies a series of self-attention layers to aggregate\ninformation across these windows. The model output a sequence of the embedded patches z \u2208 Rd\u00d7N where d is the\nlatent dimension and N is the number of patches.\nDecoder\nThe encoded image z is decoded into a sequence of tokens using a transformer decoder architecture with\ncross-attention. The tokens are generated in an auto-regressive manner, using self-attention and cross-attention to attend\nto different parts of the input sequence and encoder output respectively. Finally, the output is projected to the size of the\nvocabulary v, yielding the logits \u2113 \u2208 Rv.\nFollowing Kim et al. [28], we use the implementation of the mBART [32] decoder. We use the same tokenizer as Taylor\net al. [33] because their model is also specialized in the scientific text domain.\n2\nNougat\nBlecher et al.\nOriginal\nBitmap\nErosion\nDilation\nA\ufb03ne\nShift Scale Rotate\nGrid Distortion\nElastic Transform\nRandom Brightness Contrast\nImage Compression\nGauss Noise\nGaussian Blur\nFigure 2: List of the different image augmentation methods used during training on an example snippet form a sample\ndocument.\n3.1\nSetup\nWe render the document images at a resolution of 96 DPI. Due to the restrictive possible input dimensions of the Swin\nTransformer we choose the input size (H, W) = (896, 672). The aspect ratio is in between the US letter and Din A4\nformat 22\n17 < 4\n3 <\n\u221a\n2. The document images are resized and then padded to achieve the desired input size. This input\nsize allows us to use the Swin base model architecture [30]. We initialize the model with the pre-trained weights.\nThe Transformer decoder has a maximal sequence length of S = 4096. This relatively large sizing is due to the fact\nthat the text of academic research papers can be dense and the syntax for tables in particular is token intensive. The\nBART decoder is a decoder-only transformer with 10 layers. The entire architecture has a total of 350M parameters.\nWe also test experiment with a smaller model (250M parameters) with a slightly smaller sequence length of S = 3584\nand only 4 decoder layers, where we start from the pre-trained base model.\nDuring inference the text is generated using greedy decoding.\nTraining\nWe use an AdamW optimizer [34] to train for 3 epochs with an effective batch size of 192. Due to training\ninstabilities, we choose a learning rate of lrinit = 5 \u00b7 10\u22125 which is reduced by a factor of 0.9996 every 15 updates until\nit reaches lrend = 7.5 \u00b7 10\u22126.\n3.2\nData Augmentation\nIn image recognition tasks, it is often beneficial to use data augmentation to improve generalization. Since we are\nonly using digital-born academic research papers, we need to employ a number of transformations to simulate the\nimperfections and variability of scanned documents. These transformations include erosion, dilation, gaussian noise,\ngaussian blur, bitmap conversion, image compression, grid distortion and elastic transform [35]. Each has a fixed\nprobability of being applied to a given image. The transformations are implemented in the Albumentations [36] library.\nFor an overview of the effect of each transformation, see Fig. 2.\nDuring training time, we also add perturbations to the ground truth text by randomly replacing tokens. We found this to\nreduce the collapse into a repeating loop significantly. For more details, see Section 5.4.\n3\nNougat\nBlecher et al.\n \n \na) tex\n\\section{Title} \nWe study the\nformula \n\\begin{equation} \nE=mc^{2} \n\\label{einstein} \n\\end{equation} \nas in \\cite{ein}\nd) pdf\nc) markdown\n# 1 Title \nWe study the\nformula \n\\[E=mc^{2}\\] (1) \nas in [1]\nb) html\n<h2><span>1\n</span>Title</h2> \n<p> We study the\nformula </p> \n<math><mrow>\n<mi>E</mi><mo>=\n<mo>...</math> \n<p> as in </p> \n<cite>[1]</cite>\nFigure 3: Data processing. The source file is converted into HTML which is then converted to Markdown. a) The\nLaTeX source provided by the authors. b) The HTML file computed form the LaTeX source using LaTeXML. c) The\nMarkdown file parsed from the HTML file. d) The PDF file provided by the authors\n4\nDatasets\nTo the best of our knowledge there is no paired dataset of PDF pages and corresponding source code out there, so we\ncreated our own from the open access articles on arXiv.4 For layout diversity we also include a subset of the PubMed\nCentral 5 (PMC) open access non-commercial dataset. During the pretraining, a portion of the Industry Documents\nLibrary 6 (IDL) is included. See Table A.1 for the dataset composition.\narXiv\nWe collected the source code and compiled PDFs from 1,748,201 articles released on arXiv. To ensure\nconsistent formatting, we first process the source files using LaTeXML7 and convert them into HTML5 files. This step\nwas important as it standardized and removed ambiguity from the LaTeX source code, especially in mathematical\nexpressions. The conversion process included replacing user-defined macros, standardizing whitespace, adding optional\nbrackets, normalizing tables, and replacing references and citations with their correct numbers.\nWe then parse the HTML files and convert them into a lightweight markup language that supports various elements\nsuch as headings, bold and italic text, algorithms, LaTeX inline and display math and LaTeX tables. This way, we\nensure that the source code is properly formatted and ready for further processing.\nThe process is visualized in Fig. 3.\nPMC\nWe also processed articles from PMC, where XML files with semantic information are available in addition to\nthe PDF file. We parse these files into the same markup language format as the arXiv articles. We chose to use far fewer\narticles from PMC because the XML files are not always as rich in semantic information. Often times equations and\ntables are stored as images and these cases are not trivial to detect, which leads to our decision to limit the use of PMC\narticles to the pre-training phase.\nThe XML files are parsed into the same markup language as described above.\nIDL\nThe IDL is a collection of documents produced by industries that have an impact on public health and is\nmaintained by the University of California, San Francisco Library. Biten et al. [37] provide high quality OCR text for\nPDFs from the IDL dataset. This does not include text formatting and is only used for pre-training to teach the model\nbasic OCR of scanned documents.\n4.1\nSplitting the pages\nWe split the markdown files according to the page breaks in the PDF file and rasterize each page as an image to create\nthe final paired dataset. During the compilation, the LaTeX compiler determines the page breaks of the PDF file\nautomatically. Since we are not recompiling the LaTeX sources for each paper, we must heuristically split the source\nfile into parts, which correspond to different pages. To achieve that we are using the embedded text on the PDF page\nand match it to source text.\nHowever, figures and tables in the PDF may not correspond to their position in the source code. To address this issue,\n4https://arxiv.org/\n5https://www.ncbi.nlm.nih.gov/pmc/\n6https://www.industrydocuments.ucsf.edu/\n7http://dlmf.nist.gov/LaTeXML/\n4\nNougat\nBlecher et al.\n0\n5\n10\n15\n20\n25\n30\n35\n40\nParagraph index\n0\n2\n4\n6\n8\nPage index\nStaircase \ufb01t\nPredictions\nFigure 4: Example for splitting the paragraphs in the source code into different pages. The points in blue denote the\npage index predicted by the SVM.\nwe remove these elements in a pre-processing step using pdffigures2 [38]. The recognized captions are are then\ncompared to the captions in the XML file and matched based on their Levenshtein distance [39]. Once the source\ndocument has been split into individual pages, the removed figures and tables are reinserted at the end of each page.\nFor a better matching we also replaced unicode characters in the PDF text with corresponding LaTeX commands using\nthe pylatexenc-library8.\nBag of Words matching\nFirst we extract the text lines from the PDF using MuPDF9 and preprocess them to remove\npage numbers and potential headers/footers. We then use a Bag of Words model [40] with TF-IDF vectorizer and a\nlinear Support Vector Machine classifier. The model is fitted to the PDF lines with the page number as label. Next we\nsplit the LaTeX source into paragraphs and predict the page number for each of them.\nIdeally, the predictions will form a stair case function but in practice the signal will be noisy. To find the best boundary\npoints we employ a similar logic as decision trees and minimize a measure based on the Gini impurity\nG[a,b](i) = (b \u2212 a) \u00b7\n\u0010\n1 \u2212 p2\n[a,b](i) \u2212 p2\n[a,b](i + 1)\n\u0011\n,\nwhere p[a,b](i) is the probability of choosing an element with the predicted page number i in the interval [a, b] that\ndescribes which paragraphs (elements) were considered for the split.\nThe best splitting position t in the interval [a, b] is then\n\u02c6ti = arg min\nt\n\u0000G[a,t](i) + G[t,b](i)\n\u0001\n.\nThe search process starts with all paragraphs and for each subsequent page break, the lower bound of the search interval\nis set to the previous split position. See Fig. 4 for a visualization of an example page.\nFuzzy matching\nAfter this first coarse document splitting we try to find the exact position within the paragraph. This\nis done by comparing the source text within the neighborhood of the predicted splitting position to the last sentences of\nthe previous page of the embedded PDF text, and the first sentences of the next page using the fuzzysearch library10.\nIf the two dividing points are at the same location in the source text, the page break is considered \u201caccurate\u201d and receives\na score of 1. On the other hand, if the splitting positions differ, the one with the smallest normalized Levenshtein\ndistance is selected and given a score of 1 minus the distance. To be included in the dataset, a PDF page must have an\naverage score of at least 0.9 for both page breaks. This results in an acceptance rate of about 47% of all pages.\n4.2\nGround truth artifacts\nBecause the dataset was pre-processed by LaTeXML, the markup version of the source code can contain artifacts and\ncommands from unsupported packages. The HTML file may contain subsection titles with numbering even though they\nare not numbered in the PDF. There may also be instances where figures or tables are missing from the ground truth due\nto processing errors.\n8https://github.com/phfaist/pylatexenc\n9https://mupdf.com/\n10https://github.com/taleinat/fuzzysearch\n5\nNougat\nBlecher et al.\nIn addition, the splitting algorithm of the source code will in some cases include text from the previous page or cut off\nwords from the end. This is especially true for \u201cinvisible\u201d characters used for formatting, like italic, bold text or section\nheader.\nFor PMC papers the inline math is written as Unicode or italic text, while display math equations or tables are often\nincluded in image format and will therefore be ignored.\nEach of these issues reduces the overall data quality. However, the large number of training samples compensates for\nthese small errors.\n5\nResults & Evaluation\nIn particular, consider pruning the training dataset by keeping only the examples with the smallest\nmargin |z\u00b5| = |Jprobe \u00b7 x\u00b5| along a probe student Jprobe. The pruned dataset will follow some dis-\ntribution p(z) along the direction of Jprobe, and remain isotropic in the nullspace of Jprobe. In what\nfollows we will derive a general theory for an arbitrary data distribution p(z), and specialize to the\ncase of small-margin pruning only at the very end (in which case p(z) will take the form of a truncated\nGaussian). We will also make no assumptions on the form of the probe student Jprobe or the learning\nrule used to train it; only that Jprobe has developed some overlap with the teacher, quantified by the\nangle \u03b8 = cos\u22121 \u0010\nJretle\u00b7T\n\u2225Jretle\u22252\u2225T\u22252\n\u0011\n(Fig. 2A).\nAfter the dataset has been pruned, we consider training a new student J from scratch on the pruned\ndataset. A typical training algorithm (used in support vector machines and the solution to which SGD\nconverges on separable data) is to find the solution J which classifies the training data with the maximal\nmargin \u03ba = min\u00b5 J \u00b7 (y\u00b5x\u00b5). Our goal is to compute the generalization error \u03b5g of this student, which\nis simply governed by the overlap between the student and the teacher, \u03b5g = cos\u22121(R)/\u03c0, where\nR = J \u00b7 T/\u2225J\u22252\u2225T\u22252.\nMain result and overview\nOur main result is a set of self-consistent equations which can be solved to obtain the generalization\nerror \u03b5(\u03b1, p, \u03b8) for any \u03b1 and any data distribution p(z) along a probe student at any angle \u03b8 relative\nto the teacher. These equations take the form,\nR \u2212 \u03c1 cos \u03b8\nsin2 \u03b8\n= \u03b1\n\u03c0\u039b\n\u001c Z \u03ba\n\u2212\u221e\ndt exp\n\u0012\n\u2212\u2206(t, z)\n2\u039b2\n\u0013\n(\u03ba \u2212 t)\n\u001d\nz\n(1)\n1 \u2212 \u03c12 + R2 \u2212 2\u03c1R cos \u03b8\nsin2 \u03b8\n= 2\u03b1\n\u001c Z \u03ba\n\u2212\u221e\ndt\ne\u2212 (t\u2212\u03c1\u00b5)2\n2\n\u221a\n2\u03c0\np\n1 \u2212 \u03c12 H\n\u0012\n\u0393(t, z)\np\n1 \u2212 \u03c12\u039b\n\u0013\n(\u03ba \u2212 t)2\n\u001d\nz\n(2)\n\u03c1 \u2212 R cos \u03b8\nsin2 \u03b8\n= 2\u03b1\n\u001c Z \u03ba\n\u2212\u221e\ndt e\n\u2212 (t\u2212\u03c1\u00b5)2\n2(1\u2212\u03c1\u00b5)2\n\u221a\n2\u03c0\np\n1 \u2212 \u03c12 H\n\u0012\n\u0393(t, z)\np\n1 \u2212 \u03c12\u039b\n\u0013\u0012 z \u2212 \u03c1t\n1 \u2212 \u03c12\n\u0013\n(\u03ba \u2212 t)\n+ 1\n2\u03c0\u039b exp\n\u0012\n\u2212\u2206(t, z)\n2\u039b2\n\u0013 \u0012\u03c1R \u2212 cos \u03b8\n1 \u2212 \u03c12\n\u0013\n(\u03ba \u2212 t)\n\u001d\nz\n(3)\nWhere,\n\u039b =\nq\nsin2 \u03b8 \u2212 R2 \u2212 \u03c12 + 2\u03c1R cos \u03b8,\n(4)\n\u0393(t, z) = z(\u03c1R \u2212 cos \u03b8) \u2212 t(R \u2212 \u03c1 cos \u03b8),\n(5)\n\u2206(t, z) = z2 \u0000\u03c12 + cos2 \u03b8 \u2212 2\u03c1R cos \u03b8\n\u0001\n+ 2tz(R cos \u03b8 \u2212 \u03c1) + t2 sin2 \u03b8.\n(6)\nWhere \u27e8\u00b7\u27e9z represents an average over the pruned data distribution p(z) along the probe student.\nFor any \u03b1, p(z), \u03b8, these equations can be solved for the order parameters R, \u03c1, \u03ba, from which the\ngeneralization error can be easily read off as \u03b5g = cos\u22121(R)/\u03c0. This calculation results in the solid\ntheory curves in Figs 1,2,3, which show an excellent match to numerical simulations. In the following\nsection we will walk through the derivation of these equations using replica theory. In Section A.6\nwe will derive an expression for the information gained per training example, and show that with\nPareto optimal data pruning this information gain can be made to converge to a finite rate, resulting\nin at least exponential decay in test error. In Section A.7, we will show that super-exponential scaling\neventually breaks down when the probe student does not match the teacher perfectly, resulting in\npower law scaling at at a minimum pruning fraction fmin(\u03b8).\nReplica calculation of the generalization error\nTo obtain Eqs. 1,2,3, we follow the approach of Elizabeth Gardner and compute the volume \u2126(x\u00b5, T, \u03ba)\nof solutions J which perfectly classify the training data up to a margin \u03ba (known as the Gardner volume)\n[29, 25]. As \u03ba grows, the volume of solutions shrinks until it reaches a unique solution at a critical \u03ba,\nthe max-margin solution. The Gardner volume \u2126 takes the form,\nFigure 5: Example of a page with many mathematical equations taken from [41]. Left: Image of a page in the document,\nRight: Model output converted to LaTeX and rendered to back into a PDF. Examples of scanned documents can be\nfound in the appendix B.\nIn this section we discuss the results and performance of the model. For an example see Fig. 5 or go to Sec. B. The\nmodel focuses only on the important content relevant features of the page. The box around the equations is skipped.\n5.1\nMetrics\nWe report the following metrics on our test set.\nEdit distance\nThe edit distance, or Levenshtein distance [39], measures the number of character manipulations\n(insertions, deletions, substitutions) it takes to get from one string to another. In this work we consider the normalized\nedit distance, where we divide by the total number of characters.\nBLEU\nThe BLEU [42] metric was originally introduced for measuring the quality of text that has been machine-\ntranslated from one language to another. The metric computes a score based on the number of matching n-grams\nbetween the candidate and reference sentence.\nMETEOR\nAnother machine-translating metric with a focus on recall instead of precision, introduced in [43].\nF-measure\nWe also compute the F1-score and report the precision and recall.\n6\nNougat\nBlecher et al.\nMethod\nModality\nEdit distance \u2193\nBLEU \u2191\nMETEOR \u2191\nPrecision \u2191\nRecall \u2191\nF1 \u2191\nPDF\nAll\n0.255\n65.8\n82.1\n77.1\n81.4\n79.2\nGROBID\nAll\n0.312\n55.6\n71.9\n74.0\n72.1\n73.0\nTables\n0.626\n25.1\n64.5\n61.4\n80.7\n69.7\n+ LaTeX OCR\nPlain text\n0.363\n57.4\n69.2\n82.1\n70.5\n75.9\nMath\n0.727\n0.3\n5.0\n11.0\n8.6\n9.7\nNougat small (250M\u2217)\nAll\n0.073\n88.9\n92.8\n93.6\n92.2\n92.9\nTables\n0.220\n68.5\n78.6\n75.0\n79.8\n77.3\nPlain text\n0.058\n91.0\n94.3\n96.1\n95.3\n95.7\nMath\n0.117\n56.0\n74.7\n77.1\n76.8\n76.9\nNougat base (350M\u2217)\nAll\n0.071\n89.1\n93.0\n93.5\n92.8\n93.1\nTables\n0.211\n69.7\n79.1\n75.4\n80.7\n78.0\nPlain text\n0.058\n91.2\n94.6\n96.2\n95.3\n95.7\nMath\n0.128\n56.9\n75.4\n76.5\n76.6\n76.5\nTable 1: Results on arXiv test set. PDF is the text embedded in the PDF file. The modality \u201cAll\u201d refers to the output\ntext without any splitting. \u2217Number of parameters.\n5.2\nText modalities\nIn a scientific research article, there are three distinct types of text: 1) plain text, which comprises the majority of the\ndocument, 2) mathematical expressions, and 3) tables. It is important to separately examine each of these components\nduring the evaluation process. This is necessary because in LaTeX, there are multiple ways to express the same\nmathematical expression. While some variability has been eliminated during the LaTeXML pre-processing step, there\nstill is a significant amount of ambiguity present, like ordering of subscript and superscript, equivalent commands\nwith different notation (stackrel, atop, substack or frac, over), situationally interchangeable commands (bm,\nmathbf, boldsymbol, bf or \\left(, \\big(, etc.), whitespace commands, additional layers of brackets, and more. As\na consequence, there can be a discrepancy between prediction and ground truth, even if the rendered formulas appear\nidentical.\nIn addition, it is not always possible to determine, where a inline math environment ends and text begins, when writing\nnumbers and punctuation (Example: $\\mathrm{H}_{0}$1, vs. H$_{0}1,$ \u2192 H01, vs. H01,). This ambiguity\nreduces both math and plain text scores.\nThe expected score for mathematical expressions is lower than for plain text.\n5.3\nComparison\nWe present our results in Table 1. As expected, the mathematical expressions have the worst agreement with the\nground truth. For the plain text, most discrepancies come from formatting ambiguities and missing text due to inline\nmath, as described above. The output format of GROBID is an XML file, which we convert into a compatible markup\nlanguage, similar to the PMC or arXiv files. To some extent, GROBID provides support for formulas in its output, but it\nidentifies and stores them as the Unicode representations embedded in the PDF. We replace each Unicode symbol with\nits corresponding LaTeX command to increase the similarity. Additionally, GROBID mislabels small inline expressions\nas text. For identified formulas, GROBID stores the bounding box coordinates. We modify the program by sending\nthe snippet to the external formula recognition software LaTeX-OCR [20]. This way we can also get a signal for\nmath modality. The reported results in this section are quite poor, primarily due to the amount of missed formulas by\nGROBID and the equation prediction accuracy is affected by the quality of the bounding boxes. The performance of the\nembedded PDF text alone is better than GROBID, which is due to formatting differences for the title page or reference\nsection.\nBoth Nougat small and base are able to outperform the other approach and achieve high scores in all metrics. We note\nthat the performance of the smaller model is on par with the larger base model.\n7\nNougat\nBlecher et al.\n0\n2000\n4000\nSequence\n10\n20\n30\nLogits\n0\n2000\n4000\nSequence\n10\n20\n30\nVarWin\n0\n2000\n4000\nSequence\n5\n10\n15\nVarEnd\n0\n500\n1000\nSequence\n20\n30\n40\nLogits\n0\n500\n1000\nSequence\n20\n40\n60\nVarWin\n0\n500\n1000\nSequence\n50\n100\n150\nVarEnd\nFigure 6: Examples for repetition detection on logits. Top: Sample with repetition, Bottom: Sample without\nrepetition. Left: Highest logit score for each token in the sequence \u2113(x), Center: Sliding window variance of the logits\nVarWinB[\u2113](x), Right: Variance of variance from the position to the end VarEndB[\u2113](x)\n5.4\nRepetitions during inference\nWe notice that the model degenerates into repeating the same sentence over and over again. The model can not recover\nfrom this state by itself. In its simplest form, the last sentence or paragraph is repeated over and over again. We observed\nthis behavior in 1.5% of pages in the test set, but the frequency increases for out-of-domain documents. Getting stuck\nin a repetitive loop is a known problem with Transformer-based models, when sampled with greedy decoding [44].\nIt can also happen that the model alternates between two sentences but sometimes changes some words, so a strict\nrepetition detection will not suffice. Even harder to detect are predictions where the model counts its own repetitions,\nwhich sometimes happens in the references section.\nIn general we notice this kind behavior after a mistake by the model. The model is not able to recover from the collapse.\nAnti-repetition augmentation\nBecause of that we introduce a random perturbation during training. This helps the\nmodel to learn how to handle a wrongly predicted token. For each training example, there is a fixed probability that a\nrandom token will be replaced by any other randomly chosen token. This process continues until the newly sampled\nnumber is greater than a specified threshold (in this case, 10%). We did not observe a decrease in performance with this\napproach, but we did notice a significant reduction in repetitions. Particularly for out-of-domain documents, where we\nsaw a 32% decline in failed page conversions.\nRepetition detection\nSince we are generating a maximum of 4096 tokens the model will stop at some point, however\nit is very inefficient and resource intensive to wait for a \u201cend of sentence\u201d token, when none will come. To detect the\nrepetition during inference time we look at the largest logit value \u2113i = max \u2113i of the ith token. We found that the logits\nafter a collapse can be separated using the following heuristic. First calculate the variance of the logits for a sliding\nwindow of size B = 15\nVarWinB[\u2113](x) = 1\nB\nx+B\nX\ni=x\n\uf8eb\n\uf8ed\u2113i \u2212 1\nB\nx+B\nX\nj=x\n\u2113j\n\uf8f6\n\uf8f8\n2\n.\n8\nNougat\nBlecher et al.\nHere \u2113 is the signal of logits and x the index. Using this new signal we compute variances again but this time from the\npoint x to the end of the sequence\nVarEndB[\u2113](x) =\n1\nS \u2212 x\nS\nX\ni=x\n\uf8eb\n\uf8edVarWinB[\u2113](i) \u2212\n1\nS \u2212 x\nS\nX\nj=x\nVarWinB[\u2113](i)\n\uf8f6\n\uf8f8\n2\n.\nIf this signal drops below a certain threshold (we choose 6.75) and stays below for the remainder of the sequence, we\nclassify the sequence to have repetitions.\nDuring inference time, it is obviously not possible to compute the to the end of the sequence if our goal is to stop\ngeneration at an earlier point in time. So here we work with a subset of the last 200 tokens and a half the threshold.\nAfter the generation is finished, the procedure as described above is repeated for the full sequence.\n5.5\nLimitations & Future work\nUtility\nThe utility of the model is limited by a number of factors. First, the problem with repetitions outlined in\nsection 5.4. The model is trained on research papers, which means it works particularly well on documents with a\nsimilar structure. However, it can still accurately convert other types of documents.\nNearly every dataset sample is in English. Initial tests on a small sample suggest that the model\u2019s performance with\nother Latin-based languages is satisfactory, although any special characters from these languages will be replaced with\nthe closest equivalent from the Latin alphabet. Non-Latin script languages result in instant repetitions.\nGeneration Speed\nOn a machine with a NVIDIA A10G graphics card with 24GB VRAM we can process 6 pages\nin parallel. The generation speed depends heavily on the amount of text on any given page. With an average number\nof tokens of \u2248 1400 we get an mean generation time of 19.5s per batch for the base model without any inference\noptimization. Compared to classical approaches (GROBID 10.6 PDF/s [4]) this is very slow, but it is not limited to\ndigital-born PDFs and can correctly parse mathematical expressions.\nFuture work\nThe model is trained on one page at a time without knowledge about other pages in the document.\nThis results in inconsistencies across the document. Most notably in the bibliography where the model was trained on\ndifferent styles or section titles where sometimes numbers are skipped or hallucinated. Though handling each page\nseparately significantly improves parallelization and scalability, it may diminish the quality of the merged document\ntext.\nThe primary challenge to solve is the tendency for the model to collapse into a repeating loop, which is left for future\nwork.\n6\nConclusion\nIn this work, we present Nougat, an end-to-end trainable encoder-decoder transformer based model for converting\ndocument pages to markup. We apply recent advances in visual document understanding to a novel OCR task. Distinct\nfrom related approaches, our method does not rely on OCR or embedded text representations, instead relying solely on\nthe rasterized document page. Moreover, we have illustrated an automatic and unsupervised dataset generation process\nthat we used to successfully train the model for scientific document to markup conversion. Overall, our approach has\nshown great potential for not only extracting text from digital-born PDFs but also for converting scanned papers and\ntextbooks. We hope this work can be a starting point for future research in related domains.\nAll the code for model evaluation, training and dataset generation can be accessed at https://github.com/\nfacebookresearch/nougat.\n7\nAcknowledgments\nThanks to Ross Taylor, Marcin Kardas, Iliyan Zarov, Kevin Stone, Jian Xiang Kuan, Andrew Poulton and Hugo Touvron\nfor their valuable discussions and feedback.\nThanks to Faisal Azhar for the support throughout the project.\nReferences\n[1] Sebastian Spiegler. Statistics of the Common Crawl Corpus 2012, June 2013. URL https://docs.google.com/file/d/\n1 9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb uL5N9.\n9\nNougat\nBlecher et al.\n[2] R. Smith. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis\nand Recognition (ICDAR 2007) Vol 2, pages 629\u2013633, Curitiba, Parana, Brazil, September 2007. IEEE. ISBN\n978-0-7695-2822-9. doi: 10.1109/ICDAR.2007.4376991. URL http://ieeexplore.ieee.org/document/4376991/.\nISSN: 1520-5363.\n[3] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The Semantic Scholar Open\nResearch Corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npages 4969\u20134983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.\n447. URL https://aclanthology.org/2020.acl-main.447.\n[4] Patrice Lopez. GROBID, February 2023. URL https://github.com/kermitt2/grobid. original-date: 2012-09-\n13T15:48:54Z.\n[5] Bastien Moysset, Christopher Kermorvant, and Christian Wolf. Full-Page Text Recognition: Learning Where to\nStart and When to Stop, April 2017. URL http://arxiv.org/abs/1704.08628. arXiv:1704.08628 [cs].\n[6] Darwin Bautista and Rowel Atienza. Scene Text Recognition with Permuted Autoregressive Sequence Models,\nJuly 2022. URL http://arxiv.org/abs/2207.06966. arXiv:2207.06966 [cs] version: 1.\n[7] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu\nWei. TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models, September 2022. URL\nhttp://arxiv.org/abs/2109.10282. arXiv:2109.10282 [cs].\n[8] Daniel Hernandez Diaz, Siyang Qin, Reeve Ingle, Yasuhisa Fujii, and Alessandro Bissacco. Rethinking Text Line\nRecognition Models, April 2021. URL http://arxiv.org/abs/2104.07787. arXiv:2104.07787 [cs].\n[9] Scott MacLean and George Labahn. A new approach for recognizing handwritten mathematics using relational\ngrammars and fuzzy sets. International Journal on Document Analysis and Recognition (IJDAR), 16(2):139\u2013163,\nJune 2013. ISSN 1433-2825. doi: 10.1007/s10032-012-0184-x. URL https://doi.org/10.1007/s10032-012-0184-x.\n[10] Ahmad-Montaser Awal, Harold Mouchre, and Christian Viard-Gaudin. A global learning approach for an online\nhandwritten mathematical expression recognition system. Pattern Recognition Letters, 35(C):68\u201377, January\n2014. ISSN 0167-8655.\n[11] Francisco \u00b4Alvaro, Joan-Andreu S\u00b4anchez, and Jos\u00b4e-Miguel Bened\u00b4\u0131.\nRecognition of on-line handwritten\nmathematical expressions using 2D stochastic context-free grammars and hidden Markov models. Pattern\nRecognition Letters, 35:58\u201367, January 2014. ISSN 0167-8655. doi: 10.1016/j.patrec.2012.09.023. URL\nhttps://www.sciencedirect.com/science/article/pii/S016786551200308X.\n[12] Zuoyu Yan, Xiaode Zhang, Liangcai Gao, Ke Yuan, and Zhi Tang. ConvMath: A Convolutional Sequence\nNetwork for Mathematical Expression Recognition, December 2020. URL http://arxiv.org/abs/2012.12619.\narXiv:2012.12619 [cs].\n[13] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexander M. Rush. Image-to-Markup Generation with\nCoarse-to-Fine Attention, September 2016. URL http://arxiv.org/abs/1609.04938. arXiv:1609.04938 [cs] version:\n1.\n[14] Anh Duc Le and Masaki Nakagawa. Training an End-to-End System for Handwritten Mathematical Expression\nRecognition by Generated Patterns. In 2017 14th IAPR International Conference on Document Analysis and\nRecognition (ICDAR), volume 01, pages 1056\u20131061, November 2017. doi: 10.1109/ICDAR.2017.175. ISSN:\n2379-2140.\n[15] Sumeet S. Singh. Teaching Machines to Code: Neural Markup Generation with Visual Attention, June 2018. URL\nhttp://arxiv.org/abs/1802.05415. arXiv:1802.05415 [cs].\n[16] Jianshu Zhang, Jun Du, and Lirong Dai. Multi-Scale Attention with Dense Encoder for Handwritten Mathematical\nExpression Recognition, January 2018. URL http://arxiv.org/abs/1801.03530. arXiv:1801.03530 [cs].\n[17] Zelun Wang and Jyh-Charn Liu. Translating Math Formula Images to LaTeX Sequences Using Deep Neural\nNetworks with Sequence-level Training, September 2019. URL http://arxiv.org/abs/1908.11415. arXiv:1908.11415\n[cs, stat].\n[18] Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, and Ziyin Zhang. Handwritten Mathematical\nExpression Recognition with Bidirectionally Trained Transformer, May 2021. URL http://arxiv.org/abs/2105.\n02412. arXiv:2105.02412 [cs].\n[19] Mahshad Mahdavi, Richard Zanibbi, Harold Mouchere, Christian Viard-Gaudin, and Utpal Garain. ICDAR 2019\nCROHME + TFD: Competition on Recognition of Handwritten Mathematical Expressions and Typeset Formula\nDetection. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1533\u20131538,\nSydney, Australia, September 2019. IEEE. ISBN 978-1-72813-014-9. doi: 10.1109/ICDAR.2019.00247. URL\nhttps://ieeexplore.ieee.org/document/8978036/.\n10\nNougat\nBlecher et al.\n[20] Lukas Blecher. pix2tex - LaTeX OCR, February 2023. URL https://github.com/lukas-blecher/LaTeX-OCR.\noriginal-date: 2020-12-11T16:35:13Z.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin.\nAttention Is All You Need, December 2017.\nURL http://arxiv.org/abs/1706.03762.\narXiv:1706.03762 [cs].\n[22] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. LayoutLM: Pre-training of Text and\nLayout for Document Image Understanding. In Proceedings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pages 1192\u20131200, August 2020. doi: 10.1145/3394486.3403172. URL\nhttp://arxiv.org/abs/1912.13318. arXiv:1912.13318 [cs].\n[23] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,\nWanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal Pre-training for Visually-Rich\nDocument Understanding, January 2022. URL http://arxiv.org/abs/2012.14740. arXiv:2012.14740 [cs].\n[24] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. LayoutLMv3: Pre-training for Document AI\nwith Unified Text and Image Masking, July 2022. URL http://arxiv.org/abs/2204.08387. arXiv:2204.08387 [cs].\n[25] Lu Wang and Wanmin Liu. Online publishing via pdf2htmlEX, 2013. URL https://www.tug.org/TUGboat/tb34-3/\ntb108wang.pdf.\n[26] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. DocFormer: End-\nto-End Transformer for Document Understanding, September 2021.\nURL http://arxiv.org/abs/2106.11539.\narXiv:2106.11539 [cs].\n[27] Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, and Marc Najork.\nRepresentation Learning for Information Extraction from Form-like Documents. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 6495\u20136504, Online, July 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.580. URL https://aclanthology.org/2020.acl-main.\n580.\n[28] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. OCR-free Document Understanding Transformer, October\n2022. URL http://arxiv.org/abs/2111.15664. arXiv:2111.15664 [cs].\n[29] Brian Davis, Bryan Morse, Bryan Price, Chris Tensmeyer, Curtis Wigington, and Vlad Morariu. End-to-end\nDocument Recognition and Understanding with Dessurt, June 2022. URL http://arxiv.org/abs/2203.16618.\narXiv:2203.16618 [cs].\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\nTransformer: Hierarchical Vision Transformer using Shifted Windows, August 2021. URL http://arxiv.org/abs/\n2103.14030. arXiv:2103.14030 [cs].\n[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nImage is Worth 16x16 Words: Transformers for Image Recognition at Scale, June 2021. URL http://arxiv.org/abs/\n2010.11929. arXiv:2010.11929 [cs].\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoy-\nanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Genera-\ntion, Translation, and Comprehension, October 2019. URL http://arxiv.org/abs/1910.13461. arXiv:1910.13461\n[cs, stat].\n[33] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. Galactica: A Large Language Model for Science, November 2022.\nURL http://arxiv.org/abs/2211.09085. arXiv:2211.09085 [cs, stat].\n[34] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019. URL http://arxiv.org/\nabs/1711.05101. arXiv:1711.05101 [cs, math] version: 3.\n[35] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual\ndocument analysis. In Seventh International Conference on Document Analysis and Recognition, 2003. Pro-\nceedings., volume 1, pages 958\u2013963, Edinburgh, UK, 2003. IEEE Comput. Soc. ISBN 978-0-7695-1960-9. doi:\n10.1109/ICDAR.2003.1227801. URL http://ieeexplore.ieee.org/document/1227801/.\n[36] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A.\nKalinin. Albumentations: Fast and Flexible Image Augmentations. Information, 11(2):125, February 2020. ISSN\n2078-2489. doi: 10.3390/info11020125. URL https://www.mdpi.com/2078-2489/11/2/125.\n11\nNougat\nBlecher et al.\n[37] Ali Furkan Biten, Rub`en Tito, Lluis Gomez, Ernest Valveny, and Dimosthenis Karatzas. OCR-IDL: OCR\nAnnotations for Industry Document Library Dataset, February 2022. URL http://arxiv.org/abs/2202.12985.\narXiv:2202.12985 [cs].\n[38] Christopher Clark and Santosh Divvala. PDFFigures 2.0: Mining Figures from Research Papers. In Proceedings\nof the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, pages 143\u2013152, Newark New Jersey USA,\nJune 2016. ACM. ISBN 978-1-4503-4229-2. doi: 10.1145/2910896.2910904. URL https://dl.acm.org/doi/10.\n1145/2910896.2910904.\n[39] V. Levenshtein.\nBinary codes capable of correcting deletions, insertions, and reversals.\nSoviet physics.\nDoklady, 1965. URL https://www.semanticscholar.org/paper/Binary-codes-capable-of-correcting-deletions%\n2C-and-Levenshtein/b2f8876482c97e804bb50a5e2433881ae31d0cdd.\n[40] Zellig S. Harris.\nDistributional Structure.\nWORD, 10(2-3):146\u2013162, 1954.\ndoi:\n10.1080/00437956.\n1954.11659520.\nURL https://doi.org/10.1080/00437956.1954.11659520.\nPublisher:\nRoutledge\neprint:\nhttps://doi.org/10.1080/00437956.1954.11659520.\n[41] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling\nlaws: beating power law scaling via data pruning, November 2022. URL http://arxiv.org/abs/2206.14486.\narXiv:2206.14486 [cs, stat].\n[42] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method for Automatic Evaluation of\nMachine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,\npages 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi:\n10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\n[43] Satanjeev Banerjee and Alon Lavie.\nMETEOR: An Automatic Metric for MT Evaluation with Improved\nCorrelation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation\nMeasures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan, June 2005.\nAssociation for Computational Linguistics. URL https://aclanthology.org/W05-0909.\n[44] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration,\nFebruary 2020. URL http://arxiv.org/abs/1904.09751. arXiv:1904.09751 [cs].\n[45] Herman W. (Herman William) March and Henry C. (Henry Charles) Wolff. Calculus. New York : McGraw-Hill,\n1917. URL http://archive.org/details/calculus00marciala.\n[46] Kinetics and Thermodynamics in High-Temperature Gases, January 1970. URL https://ntrs.nasa.gov/citations/\n19700022795. NTRS Report/Patent Number: N70-32106-116 NTRS Document ID: 19700022795 NTRS Research\nCenter: Glenn Research Center (GRC).\n[47] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical Neural Story Generation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898,\nMelbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL\nhttps://aclanthology.org/P18-1082.\n[48] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-Consistency for Robust Visual Question\nAnswering, February 2019. URL http://arxiv.org/abs/1902.05660. arXiv:1902.05660 [cs].\n12\nNougat\nBlecher et al.\nA\nDataset\nName\nNumber of Pages\narXiv\n7,511,745\nPMC\n536,319\nIDL\n446,777\nTotal\n8,204,754\nTable A.1: Dataset composition\nThe most important data source is arXiv, making up > 91.5% of the corpus. On arXiv most research documents are\npaired with the LaTeX source code provided by the authors. The LaTeX source offers more information and is left\nunprocessed, unlike the XML format from PMC where equations and tables are frequently substituted with images.\nThis allows us to select exactly which information we need to build the dataset.\nB\nExamples\nIn this section we converted some pages from old text books using the Nougat base model. The text books from the\nInternet Archive11 and Project Gutenberg12 and are in public domain.\nThe performance for these scanned pages is noticeable worse than for digital-born documents. However, the model\ndoes generate sensible text for each page with few errors. For example see the first row of Fig. B.1. Here the model\nmistakes the almost illegible exponent n for \u2217. In the second row of the same figure the model falls into a repetitive\nloop after predicting another comma instead of a dot. Similar problems can be seen in Fig. B.2.\nIn Fig. B.3 we present pages, scanned with a mobile device, from a printed master thesis and the Nougat output. The\nmodel is robust to the artifacts that arise when hand-scanning a document.\nExplore the examples in this section on the project page: https://facebookresearch.github.io/nougat.\n11https://archive.org/\n12https://www.gutenberg.org/\n13\nNougat\nBlecher et al.\nand the rule is proved that\ndu\u2217\ndx = nu\u2217\u22121 du\ndx,\nwhere n is a positive fraction whose numerator and denominator are integers. This rule has\nalready been used in the solution of numerous exercises.\n34 The Derivative of a Constant\nLet y = c, where c is a constant. Corresponding to any Dx, Dy = 0, and consequently\n\u2206y\n\u2206x = 0,\nand\nlim\n\u2206x\u21920\n\u2206y\n\u2206x = 0,\nor\ndy\ndx = 0.\nThe derivative of a constant is zero.\nInterpret this result geometrically.\n35 The Derivative of the Sum of Two Functions\nLet\ny = u + v,\nwhere u and v are functions of x. Let Du, Du, and Dy be the increments of u, v, and y,\nrespectively, corresponding to the increment Dx.\ny + \u2206y = u + \u2206u + v + \u2206v\n\u2206y = \u2206u + \u2206v\n\u2206y\n\u2206x = \u2206u\n\u2206x + \u2206v\n\u2206x\ndy\ndx = du\ndx + dv\ndx,\nor\nd(u + v)\ndx\n= du\ndx + dv\ndx.\nThe derivative of the sum of two functions is equal to the sum of their derivatives.\nthe center, the axis of z horizontal and the axis of y positive downward. The element of pressure\nis\n2kyxdy\nand the total pressure is\nP = 2k\nZ 6\n0\nyxdy.\nz is expressed in terms of y by means of the equation of the ellipse,\nx2\n64 + y2\n36 = 1.\nThen\nP = 2k 3\nZ 6\n0\ny\np\n36 \u2212 y2dy.\nExercises\n1. Find the pressure on the vertical parabolic gate, Fig. 51: (a) if the edge AB lies in the surface\nof the water; (b) if the edge AB lies 5 feet below the surface.\n2. Find the pressure on a vertical semicircular gate whose diameter, 10 feet long, lies in the\nsurface of the water.\n73. Arithmetic Mean. The arithmetic mean, A, of a series of n numbers, a1, a2, a3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nFigure B.1: Example of an old calculus text book [45].\n14\nNougat\nBlecher et al.\nHere \u03bd1 = k1[H2], \u03bd2 = k2[O2], \u03bd3 = k3[H2], \u03bd4 = k4[O2][M], and \u03bd5 =\nk5[CO]. Thus the exponential growth constant \u03bb depends on the gas composi-\ntion and the rate constants of reactions I to V. This paper reports measurements\non mixtures chosen to permit determinations of the rates of reactions I, II, III,\nand V. Mixtures were selected by analyzing equation (1).\nEXPERIMENTAL ASPECTS\nGrowth constants were obtained by measuring the blue carbon monoxide flame\nband emission behind incident shocks. The intensity of this radiation is propor-\ntional to the product of carbon monoxide and oxygen atom concentrations (ref.\n3), and since very little carbon monoxide is consumed, the light monitors the\nincrease of oxygen atom concentration with time.\nGas mixtures contained varying amounts of hydrogen, carbon monoxide,\noxygen and in some mixtures carbon dioxide, diluted five to tenfold with argon.\nHydrogen, oxygen, and argon were high purity tank gases and were used without\nfurther purification. Carbon monoxide was condensed at liquid nitrogen tem-\nperature; about one-quarter of the condensate was pumped off and discarded.\nDry ice served as a convenient source of carbon dioxide.\nIt was purified by\nsub-liming three-quarters of a sample into a liquid nitrogen cooled trap. The\nfirst quarter of this trapped fraction was discarded and the middle half used for\nmixture preparation.\nRecently we showed that boundary layer effects must be considered in ana-\nlyzing data obtained behind incident shocks; the growing boundary layer causes\nincreases in temperature, density, and residence time with increasing distance\nbehind the shock. Conditions behind the shocks, in the region of the experi-\nmental measurements, were obtained from a computer program which integrated\nthe equations of chemical change for a shocked gas accounting for the effects of\nboundary layer buildup. In general, the extent of chemical reaction was small,\nand changes in gas properties were brought about largely by the gas dynamics\nassociated with boundary layer growth.\nExponential growth constants were obtained from plots of the logarithm of\nobserved light intensity against gas time; the relation between gas and labora-\ntory times was obtained from the computer calculations.\nSELECTION OF GAS MIXTURES\nLet us turn now to the rationale used to select gas mixtures by analysis of\nequation (1). To begin with, under our experimental conditions \u03bd4 is gener-\nally small in comparison with the other \u03bd\u2032\u2032s and can be neglected for purposes\nof a qualitative discussion. Secondly, \u03bb turns out to be a small positive root - of\nthe order of the smaller \u03bd values and small compared with the larger \u03bd values.\nThus, we neglect \u03bb3 in comparison with the other terms and rewrite equation\n(1):\n[(\u03bd1 + \u03bd5) + \u03bd2 + \u03bd3] \u03bb2 + \u03bd3(\u03bd1 + \u03bd5)\u03bb \u223c= 2\u03bd2\u03bd3(\u03bd1 + \u03bd5)\nIf the amount of hydrogen in a mixture is large in comparison to oxygen, \u03bd1\nand \u03bd3 are large and the term involving \u03bb2 may be neglected; in this event,\n\u03bb \u223c= 2\u03bd2\nOn the other hand, if only a trace of hydrogen is present, \u03bd3 is small, the\nterm involving \u03bb may be neglected, and\n\u03bb2 \u2243 2\u03bd2\u03bd3(\u03bd1 + \u03bd5)\n\u03bd2 + (\u03bd1 + \u03bd5)\nIf we choose a mixture with a large amount of carbon monoxide, \u03bd5 is large\nand\n\u03bb \u223c\n\u221a\n2\u03bd2\u03bd3\nWhereas if there is a large amount of oxygen, \u03bd2 is large and\n\u03bb \u223c\np\n2\u03bd3(\u03bd1 + \u03bd5)\n\u2212\n\u221a\n2\u03bd3\u03bd1\n[H2] > [CO]\n\u2212\n\u221a\n2\u03bd3\u03bd5\n[CO] > [H2]\nThis, then, outlines a strategy for obtaining rates of reactions I, II, III, and\nV. First, a mixture rich in hydrogen is used to determine k2. Next, with k2\nknown, a mixture with a trace of hydrogen and rich in carbon monoxide is used\nto deter-mine k3. Finally, with k3 known, mixtures with excess oxygen and\nvarying pro\nFigure B.2: A selection of pages from a NASA conference from 1970 [46].\n15\nNougat\nBlecher et al.\n2.1. Molecules in Cavities\nThe molecular Hamiltonian describes the vibrational modes as harmonic oscillators which are\ncoupled to each other by a third order anharmonic coupling term.\nThis term is obtained by\nusing a Taylor expansion of the the harmonic potential and therefore includes a mediator for the\nIVR pathways in the model. So the molecular Hamiltonian for two separate modes (q, Q) in the\nmolecular part is described by:\n\u02c6Hmol = \u00afh\u03c9q(\u02c6b\u2020\u02c6b) + \u00afh\u03c9Q(\u02c6c\u2020\u02c6c) +\n3\nX\nr,s\n\u03b7qQ\nr,s\n\u0010\n\u02c6b\u2020 + \u02c6b\n\u0011r \u0000\u02c6c\u2020 + \u02c6c\n\u0001s .\n(2.3)\nHere \u03c9q describes the fundamental frequency of the molecular mode q which is coupled to the\ncavity and the respective degrees of freedom are expressed with the ladder operators, \u02c6b\u2020 and \u02c6b.\nIn the second part, a lower energy vibrational mode Q is described with its respective frequency\n\u03c9Q and the corresponding ladder operators (\u02c6c\u2020, \u02c6c). The last term describes the general form of\nthe third order couplings, where \u03b7r,s describes the anharmonic coupling constants and will be\nreferred to by \u02c6W.[4] To obtain the corresponding eigenvector for the upper and lower polariton,\nthe Hamiltonian matrix is diagonalized. By doing this one arrives at the following expression for\nthe polaritons:[22, 25]\n|\u03c8(UP,LP ); 0\u27e9 = |1c, 0, 0\u27e9 \u00b1 |0c, 1, 0\u27e9\n\u221a\n2\n.\n(2.4)\nHere the uncoupled states are described by the kets where the first term describes the excitation\nin the cavity state, the second term describes the excitation in the high frequency mode and the\nlast term is the excitation in the Q mode.\n2.1.2. Anharmonic Coupling Terms\nUnder specific symmetry considerations not all intramolecular coupling orders (r, s) are relevant.\nIn this specific case only orders with r + s \u2264 3 are considered. Since all considered coordinates are\nintramolecular normal modes the bilinear coupling (1, 1) is in this case equal to zero. Generally, all\nterms of the potential energy have to transform according to the totally symmetric representation\nof the molecules point group.[26] Thus according to the following equation:\n\u0010\n\u0393(q)\u0011r\n\u00d7\n\u0010\n\u0393(Q)\u0011s\n\u2282 \u0393A.\n(2.5)\nSince the models discussed in this work modeled after the octahedral W(CO)6 molecule and\nthus exhibit non-Abelian point group symmetry, the various possible couplings have\ntime-independent basis-set functions.\n\u03a8 (q1, ..., qf, t) =\nN1\nX\nj1=1\n...\nNf\nX\njf =1\nCj1...jf (t)\nfY\n\u03ba=1\n\u03c7\u03ba\njk (q\u03ba)\n(2.6)\nHereby, f represents the degrees of freedom (DOF), Cj1...jf (t) denotes the time-dependent\nexpansion coefficients and N\u03ba describes the number of basis functions used for representing the\n\u03bath DOF. The orthonormal time-independent primitive basis functions are represented by \u03c7\u03ba\njk (q\u03ba)\nand only the time-dependent expansion coefficients are optimized variationally [31, 32].\nThe problem with the standard method is the exponential scaling as the number of coefficients\nincrease with N f. Therefore, the standard method is only suited for problems with less than 6\nDOFs.\nIn the multiconfiguration time-dependent Hartree method (MCTDH method) the scaling is\nsoftened by introducing a smaller but now time-dependent basis, the so-called single particle func-\ntions (SPFs)\n\f\f\u03c6\u03ba\njk (q\u03ba, t)\n\u000b\n=\nN\u03ba\nX\ni\u03ba=1\nc(\u03ba)\ni\u03baj\u03ba (t)\n\f\f\f\u03c7(\u03ba)\ni\u03ba (q\u03ba)\nE\n.\n(2.7)\nThe SPFs are represented as a linear combination of the time-independent primitive basis\nfunctions. The ansatz for MCTDH method can now be written as the following:\n\u03a8 (q1, ..., qf, t) =\nn1\nX\nj1=1\n...\nnf\nX\njf =1\nAj1...jf (t)\nfY\n\u03ba=1\n\u03c6\u03ba\njk (q\u03ba, t)\n(2.8)\n=\nX\nj\nAJ\u03a6J.\n(2.9)\nWhere \u03a6J describes the f-dimensional product of the SPFs, the Hartree product. The complex\nexpansion coefficients AJ and the basis functions \u03c6\u03ba\njk (q\u03ba, t) are both time-dependent and optimized\nvariationally [31, 32].\nDue to the fact that a two layer scheme was used here (the time-dependent SPFs and the\nprimitive basis), the exponential scaling of the DOFs, as nk, is smaller compared to the one layer\nmethod like the standard method.\nBy now applying the Dirac-Frenckle variational principle to the ansatz (eq. (2.9)), we obtain\nthe respective Equations of Motion and therefore a set of coupled differential\nFigure B.3: Scan of a modern thesis with a mobile device camera, with permission from the author.\n16\nNougat\nBlecher et al.\nmodel won the VQA Challenge in 2017 and achieves 66.25% accuracy on VQA v2.0 test-dev.\nPythia[41]3 extends the BUTD model by incorporating co-attention [27] between question and image regions. Pythia uses features extracted from Detectron [8]\npretrained on Visual Genome. An ensemble of Pythia models won the 2018 VQA Challenge using extra training data from Visual Genome [21] and using Resnet[11]\nfeatures. In this study, we use Pythia models which do not use Resnet features.\nFootnote 3: https://github.com/facebookresearch/pythia\n**Bilinear Attention Networks (BAN) [19]**4 combines the idea of bilinear models and co-attention [27] between image regions and words in questions in a\nresidual setting. Similar to [3], it uses Faster-RCNN [33] pretrained on Visual Genome [21] to extract image features. In all our experiments, for a fair comparison,\nwe use BAN models which do not use additional training data from Visual Genome. BAN achieves the current state-of-the-art single-model accuracy of 69.64 % on\nVQA v2.0 test-dev without using additional training data from Visual Genome.\nFootnote 4: https://github.com/jnhwkim/ban-vqa\nImplementation Details For all models trained with our cycle-consistent framework, we use the values \n, \n, \n and \n. When\nreporting results on the validation split and VQA-Rephrasings we train on the training split and when reporting results on the test split we train on both training and\nvalidation splits of VQA v2.0. Note that we never explicitly train on the collected VQA-Rephrasings dataset and use it purely for evaluation purposes. We use\npublicly available implementations of each backbone VQA model.\nWe measure the robustness of each of these models on our proposed VQA-Rephrasings dataset using the consensus score (Eq. 2). Table 1 shows the consensus\nscores at different values of  for several VQA models. We see that all models suffer significantly when measured for consistency across rephrasings. For e.g., the\nperformance of Pythia (winner of 2018 VQA challenge) is reduced to a consensus score of 39.49% at \n. Similar trends are observed for MUTAN, BAN and\nBUTD. The drop increases with increasing , the number of rephrasings used to measure consistency. Models like BUTD, BAN and Pythia which use word-level\nencodings of the question suffer significant drops. It is interesting to note that even MUTAN which uses skip-thought based sentence encoding [20] suffers a drop\nwhen checked for consistency across rephrasings. We observe that BAN + CC model trained with our proposed cycle-consistent training framework outperforms its\ncounterpart BAN and all other models at all values of .\nFig 4 qualitatively compares the textual and visual attention (over image regions) over 4 rephrasings of a question. The top row shows attention and predictions\nfrom a Pythia model, while the bottom row shows attention and predictions from the same Pythia model, but trained using our framework. Our model attends at\nrelevant image regions\nModel\nCS(k)\nVQA Accuracy\nk=1\nk=2\nk=3\nk=4\nORI\nREP\nMUTAN [5]\n56.68\n43.63\n38.94\n32.76\n59.08\n46.87\nBUTD [3]\n60.55\n46.96\n40.54\n34.47\n61.51\n51.22\nBUTD + CC\n61.66\n50.79\n44.68\n42.55\n62.44\n52.58\nPythia [41]\n63.43\n52.03\n45.94\n39.49\n64.08\n54.20\nPythia + CC\n64.36\n55.45\n50.92\n44.30\n64.52\n55.65\nBAN [19]\n64.88\n53.08\n47.45\n39.87\n64.97\n55.87\nBAN + CC\n65.77\n56.94\n51.76\n48.18\n65.87\n56.59\nTable 1: Consensus performance on VQA-Rephrasings dataset. CS(k) as defined in Eq. 2 is consensus score which is non-zero only if at least  rephrasings are\nanswered correctly, zero otherwise; averaged across all group of questions. ORI represent a split of questions from VQA-Rephrasings which are original questions\nfrom VQA v2.0 and their corresponding rephrasings are represented by the split REP. Models trained with our cycle-consistent (CC) framework consistently\noutperform their baseline counterparts at all values of .\nModel\nval\ntest-dev\nMUTAN [5]\n61.04\n63.20\nBUTD [3]\n65.05\n66.25\n+ Q-consistency\n65.38\n66.83\n+ A-consistency\n60.84\n62.18\n+ Gating\n65.53\n67.55\nPythia [41]\n65.78\n68.43\n+ Q-consistency\n65.39\n68.58\n+ A-consistency\n62.08\n63.77\n+ Gating\n66.03\n68.88\nBAN [19]\n66.04\n69.64\n+ Q-consistency\n66.27\n69.69\n+ A-consistency\n64.96\n66.31\n+ Gating\n66.77\n69.87\nTable 2: VQA Performance and ablation studies on VQA v2.0 validation and test-dev splits. Each row in blocks represents a component of our cycle-consistent\nframework added to the previous row. First row in each block represents the baseline VQA model . Q-consistency implies addition of a VQG module  to\ngenerate rephrasings \n from the image  and the predicted answer \n with an associated VQG loss \n. A-consistency implies passing all the generated\nquestions \n to the VQA model  and an associated loss \n. Gating implies the use of gating mechanism to filter undesirable generated questions in \nand passing the remaining to VQA model . Models trained with our cycle-consistent (last row in each block) framework consistently outperform baselines.\n10/03/2023, 17:22\nlatex-ocr-demo\n3.86.159.32\n1/1\nduced by beam search tend to be short and generic. Completely random sampling can introduce very unlikely words, which\ncan damage generation as the model has not seen such mistakes at training time. The restriction of sampling from the 10 most\nlikely candidates reduces the risk of these low-probability samples.\nFor each model, we tune a temperature parameter for the softmax at generation time. To ease human evaluation, we generate\nstories of 150 words and do not generate unknown word tokens.\nFor prompt generation, we use a self-attentive GCNN language model trained with the same prompt-side vocabulary as the\nsequence-to-sequence story generation models. The language model to generate prompts has a validation perplexity of 63.06.\nPrompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is\ncompleted when the language model generates the end of prompt token.\nWe propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such\nas BLEU for ma\nModel\nHuman\nPreference\nLanguage model\n32.68%\nHierarchical Model\n67.32%\nTable 4: Effect of Hierarchical Generation. Human judges prefer stories that were generated hierarchically by first creating a\npremise and creating a full story based on it with a seq2seq model.\nFigure 5: Human accuracy at pairing stories with the prompts used to generate them. People find that our fusion model\nsignificantly improves the link between the prompt and generated stories.\nModel\n# Parameters (mil)\nValid Perplexity\nTest Perplexity\nGCNN LM\n123.4\n54.50\n54.79\nGCNN + self-attention LM\n126.4\n51.84\n51.18\nLSTM seq2seq\n110.3\n46.83\n46.79\nConv seq2seq\n113.0\n45.27\n45.54\nConv seq2seq + self-attention\n134.7\n37.37\n37.94\nEnsemble: Conv seq2seq + self-attention\n270.3\n36.63\n36.93\nFusion: Conv seq2seq + self-attention\n255.4\n36.08\n36.56\nTable 3: Perplexity on WritingPrompts. We dramatically improve over standard seq2seq models.\nFigure 6: Accuracy of prompt ranking. The fusion model most accurately pairs prompt and stories.\nFigure 7: Accuracy on the prompt/story pairing task vs. number of generated stories. Our generative fusion model can\nproduce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.\nEvaluation\nFigure B.4: Pages with tables. Upper: Fan et al. [47] page 6, Lower: Shah et al. [48] page 6\n17\n"
  },
  {
    "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
    "link": "https://arxiv.org/pdf/2308.13137.pdf",
    "upvote": "13",
    "text": "Published as a conference paper at ICLR 2024\nOMNIQUANT:\nOMNIDIRECTIONALLY CALIBRATED\nQUANTIZATION FOR LARGE LANGUAGE MODELS\nWenqi Shao\u20201, Mengzhao Chen\u20201, Zhaoyang Zhang3, Peng Xu1,2, Lirui Zhao1,\nZhiqian Li2, Kaipeng Zhang1, Peng Gao1, Yu Qiao1, Ping Luo\u22171,2\n1OpenGVLab, Shanghai AI Laboratory 2The University of Hong Kong\n3The Chinese University of Hong Kong\nABSTRACT\nLarge language models (LLMs) have revolutionized natural language processing\ntasks. However, their practical deployment is hindered by their immense memory\nand computation requirements. Although recent post-training quantization (PTQ)\nmethods are effective in reducing memory footprint and improving the compu-\ntational efficiency of LLM, they hand-craft quantization parameters, leading to\nlow performance, especially in extremely low-bit quantization. To tackle this is-\nsue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant)\ntechnique for LLMs, which achieves good performance in diverse quantization\nsettings while maintaining the computational efficiency of PTQ by efficiently op-\ntimizing various quantization parameters. OmniQuant comprises two innovative\ncomponents including Learnable Weight Clipping (LWC) and Learnable Equiv-\nalent Transformation (LET). LWC modulates the extreme values of weights by\noptimizing the clipping threshold. Meanwhile, LET tackles activation outliers\nby shifting the challenge of quantization from activations to weights. Operating\nwithin a differentiable framework using block-wise error minimization, Omni-\nQuant can optimize the quantization process efficiently for both weight-only and\nweight-activation quantization. For instance, the LLaMA-2 model family size\n7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-\n16 hours using 128 samples. Extensive experiments validate OmniQuant\u2019s supe-\nrior performance across diverse quantization configurations such as W4A4 (4-bit\nweight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally,\nOmniQuant demonstrates effectiveness in instruction-tuned models and delivers\nnotable improvements in inference speed and memory reduction on real devices.\nCodes are available at https://github.com/OpenGVLab/OmniQuant.\n1\nINTRODUCTION\nLarge language models (LLMs) such as GPT-4 (Bubeck et al., 2023) and LLaMA (Touvron\net al., 2023a), have demonstrated impressive performance across various natural language bench-\nmarks (Hendrycks et al., 2020; Zellers et al., 2019). Furthermore, the language understanding capa-\nbilities inherent in LLMs can be successfully transferred into multimodal models (Mu et al., 2023;\nXu et al., 2023; Zhang et al., 2023a; Huang et al., 2024; 2023). Thereby, LLMs can be regarded as\nprecursors to artificial general intelligence (Bubeck et al., 2023). However, the considerable com-\nputational and memory requirements of LLMs pose substantial challenges (Zhang et al., 2023b; Hu\net al., 2023). For instance, the GPT-3 model (Brown et al., 2020) requires 350G of memory to load\nits parameters in FP16 format, which corresponds to the requirement of at least five A100-80G GPUs\nfor inference. This significant demand for computational resources and associated communication\noverheads impedes the practical deployment of LLMs in real-world applications.\nQuantization has shown to be promising to mitigate both computational and memory overhead\nin LLMs.\nIn general, it comes in two types including post-training quantization (PTQ) and\nquantization-aware training (QAT). Although QAT can lead to more competitive accuracy than PTQ,\n\u2217Corresponding author: Ping Luo, pluo@cs.hku.hk\n\u2020 Equal Contribution\n1\narXiv:2308.13137v3  [cs.LG]  18 Mar 2024\nPublished as a conference paper at ICLR 2024\n(a)\n(b)\n(c)\ncost(time and data)\nperformance\nLLM-QAT\nOurs\nSmoothQuant\nan overview in W4A4\nweight-only quantization\nweight-activation quantization\nAcc: 46.43%\ndata: 100k\ntime: 90h\nAcc: 38.41%\ndata: 128\ntime: 10 min\nAcc: 52.65%\ndata: 128\ntime: 1.6 h\nFigure 1: (a) provides an overview of LLaMA-7B with W4A4 quantization, highlighting Omni-\nQuant\u2019s ability to achieve quantization-aware training (QAT) performance with post-training quan-\ntization (PTQ) time and data efficiency. (b) and (c) showcase the perplexity (low is better) of quan-\ntized LLaMA-13B across different bit-widths on WikiText2.\nit is not practical due to the high training cost because the whole model is trained with the awareness\nof the quantization process. As a result, PTQ is commonly utilized in existing quantization methods\non LLMs. For example, lots of PTQ methods (Frantar et al., 2022; Lin et al., 2023; Dettmers et al.,\n2023b) reduce memory consumption by weight-only quantization which quantizes the weights while\nmaintaining full-precision activation. To further reduce the computational overhead, another line of\nwork (Xiao et al., 2023; Wei et al., 2022; Yuan et al., 2023; Wei et al., 2023; Liu et al., 2023a) em-\nploys weight-activation quantization which quantizes both weight and activation into low-bit values\nfor the execution of low-bit matrix multiplication.\nExisting quantization methods have demonstrated significant achievements in various scenarios, in-\ncluding W4A16 (i.e. 4-bit weight and 16-bit activation) weight-only quantization such as (Lin et al.,\n2023; Dettmers et al., 2023b; Lee et al., 2023), as well as W8A8 weight-activation quantization (Wei\net al., 2023). However, they usually exhibit significant performance degradation when confronted\nwith low-bit quantization, such as W2A16 and W4A4, as illustrated in Figure 1 (b & c). This perfor-\nmance shortfall in low-bit quantization can be attributed to the fact that these methods (Frantar et al.,\n2022; Lin et al., 2023; Wei et al., 2023) primarily rely on handcrafted quantization parameters such\nas migration strength (Xiao et al., 2023) and scaling parameters (Wei et al., 2023), which often leads\nto lower performance. Although Quantization-Aware Training (QAT) (Liu et al., 2023b) is effective\nin determining the optimal quantization configurations, it introduces substantial training overhead\nin both training and data efficiency. It is thus hard to quantize LLMs with QAT-based techniques\nefficiently such as LLMQAT (Liu et al., 2023b). For instance, GPTQ (Frantar et al., 2022), a PTQ\napproach, can complete the quantization of LLaMA-13B in an hour using 128 samples on a single\nA100 GPU, while LLM-QAT (Liu et al., 2023b) requires 100k samples and hundreds of GPU hours.\nThis leads us to a central question: can we attain the performance of QAT, while maintaining the\ntime and data efficiency of PTQ?\nThis paper introduces a novel quantization technique, OmniQuant, which effectively addresses the\nabove question. OmniQuant achieves state-of-the-art performance across various quantization sce-\nnarios, particularly in low-bit settings, while preserving the time and data efficiency of PTQ, as il-\nlustrated in Figure 1. Unlike Quantization-Aware Training (QAT) (Liu et al., 2023b) which involves\ncumbersome weight optimization, OmniQuant freezes the original full-precision weight and only\nincorporates a few learnable quantization parameters. As shown in Figure 2, OmniQuant consists of\ntwo key components that incorporate different types of learnable quantization parameters, including\nLearnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). Specifically,\nLWC modulates the extreme values of weights by optimizing the clipping threshold. In the mean-\nwhile, LET tackles activation outliers by learning mathematically equivalent transformations in a\ntransformer encoder.\nInstead of jointly optimizing all parameters across the LLM, OmniQuant sequentially quantizes\nthe parameters of one layer before moving on to the next under a block-wise quantization error\nminimization framework. In this way, OminiQuant can be optimized efficiently using a simple\nStochastic Gradient Descent (SGD) algorithm. Thanks to the differentiable optimization, LWC\nand LET can be seamlessly integrated into the quantization. We find that LWC can mitigate the\ndifficulty in quantizing weights and LET further shifts the challenge of quantization from activations\nto weights, facilitating OmniQuant a versatile quantization framework for both weight-only and\nweight-activation quantization. Notably, OmniQuant introduces no extra computation or parameters\n2\nPublished as a conference paper at ICLR 2024\nfor the quantized model because the clipping threshold in LWC and equivalent factors in LET can\nbe fused into quantized weights.\nQuantization-hardly\nFP models (7B-70B)\nQuantization-friendly\nFP models (7B-70B)\nQuantized models\nEquivalent\nTransformation\nWeight Clipping\nquantization\nSingle A100-40G GPU\n128 Training Samples\n1-16 Hours Training\nLearnable\nFixed\nFigure 2:\nCharacteristics of OmniQuant on\nLLaMA family.\nAs depicted in Figure 2, OmniQuant is easy to\nimplement even with limited resources. Espe-\ncially, taking the LLaMA-2 model family (7B-\n70B) as an example, all models can be quan-\ntized on a single A100-40G GPU utilizing only\n128 training samples. The training time ranges\nfrom 1 to 16 hours, depending on the size of\nthe quantized model, which ranges from 7B to\n70B. Owing to the seamless integration of LWC\nand LET achieved by differentiable optimiza-\ntion, OmniQuant exhibits superior performance\ncompared to prior PTQ-based methods in vari-\nous quantization settings. For example, when\nLLaMA-13B is quantized into W2A16, OmniQuant achieves a perplexity of 13.21, while GPTQ in-\ncurs a significant increase in perplexity to 3832, as demonstrated in Figure 1. A similar performance\nadvancement is also observed in the W4A4 quantization.\nThe contributions of OmniQuant are summarized as follows. 1) We formulate a novel quantization\npipeline for LLM, OmniQuant, which freezes original full-precision weights while incorporating\na restrained set of learnable parameters. OmniQuant imbues quantization with gradient updates\nwhile preserving the time and data efficiency of PTQ methods. 2) OmniQuant consists of Learnable\nWeight Clipping (LWC) and Learnable Equivalent Transformation (LET). These strategies make\nfull-precision weights and activations more amenable to quantization. 3) Through extensive experi-\nments, we demonstrate that OmniQuant outperforms previous methods across a spectrum of quan-\ntization settings (W416, W3A16, W2A16, W6A6, W4A4), various model families (OPT, LLaMA,\nLLaMA-2, LLaMA-2-chat, Falcon), and a range of model sizes (125M-180B). The computation\nspeedup and memory reduction of OmniQuant are also demonstrated on real devices.\n2\nRELATED WORK\n2.1\nQUANTIZATION METHODS.\nQuantization reduces neural network bit-precision, leading to smaller models and faster inference.\nCurrent methods are largely divided into Quantization Aware Training (QAT)(Liu et al., 2023b) and\nPost-training Quantization (PTQ)(Xiao et al., 2023; Frantar et al., 2022). While QAT maintains per-\nformance by simulating quantization during training, its training cost makes it unsuitable for LLM.\nPTQ techniques like AdaRound (Nagel et al., 2020) and BRECQ (Li et al., 2021) use gradient opti-\nmization to determine optimal rounding, but tuning all weights is time-intensive for larger models.\nThus, most LLM quantization methods (Xiao et al., 2023; Frantar et al., 2022; Dettmers et al.,\n2023b; Lee et al., 2023; Wei et al., 2023) prioritize training-free PTQ, which limit performance in\nlower-bit situations. Our goal is to integrate gradient updates in LLM quantization, mirroring QAT\u2019s\napproach, while retaining PTQ\u2019s efficiency.\n2.2\nQUANTIZATION OF LLM.\nConsidering the quantized object, exiting LLM quantization can be classified into two fields: weight-\nonly quantization and weight-activation quantization.\nWeight-only quantization. Weight-only quantization focuses on converting weights to low-bit val-\nues. For instance, GPTQ (Frantar et al., 2022) uses block-wise reconstruction for 3/4-bit quantiza-\ntion. SpQR (Dettmers et al., 2023b), OWQ (Lee et al., 2023), and AWQ (Lin et al., 2023) emphasize\nthe significance of weights tied to higher-magnitude activations. Therefore, SpQR and OWQ employ\nmixed-precision quantization to safeguard vital weights, while AWQ opts for channel-wise scaling\nto avoid mixed-precision\u2019s hardware inefficiency. Qlora (Dettmers et al., 2023a) and INT2.1 (Chee\net al., 2023) restore the capabilities of the quantized model through parameter-efficient fine-tuning.\nOur method, in contrast, enhances the quantization process directly, making OmniQuant comple-\nmentary to Qlora and INT2.1.\n3\nPublished as a conference paper at ICLR 2024\nEmbedding\nNormalization\nTransformation\nQuantization\nQuantized\nWeight Q\nQuantized\nWeight K\nQuantized\nWeight V\nQuantization\nQuantization\nQuantized\nWeight Out\nQuantization\nQuantization\nQuantized\nWeight FC1\nQuantization\nTransformation\nTransformation\nTransformation\nQuantized\nWeight FC2\nFP Weight\nTransformation\nClipping\nQuantized\nWeight\nNormalization\nQuantization\nQuantization\nTransformation\nLearnable\nFixed\nEliminable\nFigure 3: Details of OmniQuant in a transformer block. Note that all learnable parameters can be\neliminated after quantization.\nWeight-activation quantization. Weight-activation quantization compresses both weights and acti-\nvations. SmoothQuant (Xiao et al., 2023), LLM.int8() (Dettmers et al., 2022), and Outlier Suppres-\nsion (Wei et al., 2022) achieve W8A8 quantization by managing activation outliers. LLM.int8() uses\nmixed-precision decomposition, while the other two employ channel-wise scaling. Furthermore,\nOutlier Suppression+(Wei et al., 2023) adds channel-wise shifting to drive W6A6 quantization. Un-\nlike previous heuristic designs, we use gradient optimization and expand equivalent transforma-\ntions to attention mechanisms, further boosting the K/V cache quantization. Recently, RPTQ (Yuan\net al., 2023) and LLM-QAT (Liu et al., 2023b) have achieved W4A4 quantization. However, RPTQ\nadopts deployment-unfriendly group-wise activation quantization, and LLM-QAT employs time-\nconsuming QAT. In distinction from RPTQ and LLM-QAT, we achieve W4A4 quantization through\ndeployment-friendly per-token quantization and maintain the PTQ efficiency.\n3\nOMNIQUANT\nChallenge of LLM quantization. Two main difficulties lie in quantizing an LLM. First, the acti-\nvation is hard to quantize due to the existence of outlier channels. Considering that weight distri-\nbution is flat and uniform, SmoothQuant (Xiao et al., 2023) and Outlier Suppression+ (Wei et al.,\n2023) tackle this issue by migrating the quantization difficulty from activations to weights with a\npre-defined migration strength or grid-searching based optimization. Second, the quantization er-\nror of weights also plays a pivotal role in the final performance due to the importance of weights\ncorresponding to activations. SqQR (Dettmers et al., 2023b) and OWQ (Lee et al., 2023) propose\nto retain crucial weights in full-precision, while AWQ (Lin et al., 2023) safeguards these weights\nusing grid-searched channel-wise scaling. Although these methods have achieved certain success\nin compressing various LLMs, they often lead to suboptimal performance and fail to deal with ex-\ntremely low-bit quantization due to the crude design of hand-crafted quantization parameters such\nas migration strength and scaling factors.\nIn this section, we introduce a differentiable quantization technique for LLM called OmniQuant\nwhere quantization parameters are learned with better flexibility. Towards this goal, OmniQuant is\nimplemented with a block-wise quantization error minimization framework as presented in Sec.3.1.\nTo tackle the aforementioned challenges of LLM quantization, we devise two novel strategies for\nadditional learnable quantization parameters including a learnable weight clipping (LWC) to miti-\ngate the difficulty in quantizing weights and a learnable equivalent transformation (LET) to further\nshift the challenge of quantization from activations to weights. We introduce LWC and LCT in Sec.\n3.2 and Sec. 3.3, respectively.\n3.1\nBLOCK-WISE QUANTIZATION ERROR MINIMIZATION\nPrevious PTQ methods with gradient optimization, such as AdaRound (Nagel et al., 2020),\nBRECQ (Li et al., 2021) cannot be applied in models with billions of parameters because they\nare hard to optimize due to the huge solution space. Instead of turning the whole model, we propose\na new optimization pipeline with block-wise quantization error minimization where the additional\n4\nPublished as a conference paper at ICLR 2024\nquantization parameters can be optimized in a differentiable manner. We formulate the optimization\ngoal as follows:\narg min\n\u03981,\u03982||F(W, X) \u2212 F\n\u0000Qw(W; \u03981, \u03982), Qa(X, \u03982)\n\u0001\n||,\n(1)\nwhere F represents the mapping function for a transformer block in the LLM, W and X are full-\nprecision weight and activation, Qw(\u00b7) and Qa(\u00b7) represent weight and activation quantizer, respec-\ntively, \u03981 and \u03982 are quantization parameters in learnable weight clipping (LWC) and learnable\nequivalent transformation (LET), respectively. The Block-wise quantization in Eqn.(1) sequentially\nquantizes the parameters of one transformer block before moving on to the next.\nBlock-wise minimization in Eqn.(1) has two advantages. First, equipped with block-wise mini-\nmization in Eqn.(1), OmniQuant can optimize quantization parameters in LWC and LET jointly,\nmaking it capable enough to encompass both weight-only and weight-activation quantization. Sec-\nond, block-wise minimization is easy to optimize with minimal resource requirements. OmniQuant\nonly determines a few quantization parameters with optimality, which is easier than optimizing the\nwhole weights in previous PTQ-based methods (Nagel et al., 2020; Li et al., 2021). Empirically, we\nfind that all models from the LLaMA-2 family (Touvron et al., 2023b) can be quantized on a single\nA100-40G GPU utilizing only 128 training samples.\n3.2\nLEARNABLE WEIGHT CLIPPING\nOmniQuant employs a module of learnable weight clipping (LWC) to reduce the difficulty of quan-\ntizing the weights in an LLM. Similar to previous methods with learnable clipping threshold (Esser\net al., 2019; Liu et al., 2022; Choi et al., 2018), LWC also determines the optimal dynamic range\nof the weights by optimizing a clipping threshold. However, we find that directly employing prior\narts such as PACT (Choi et al., 2018) and LSQ (Esser et al., 2019) in quantization would produce\nunsatisfactory performance, as demonstrated in Table A14 in the Appendix.\nInstead of directly learning a clipping threshold as in previous methods (Esser et al., 2019; Choi\net al., 2018), LWC optimizes a clipping strength as formulated by\nWq = clamp(\u230aW\nh \u2309 + z, 0, 2N \u2212 1), where h = \u03b3 max(W) \u2212 \u03b2 min(W)\n2N \u2212 1\n, z = \u2212\u230a\u03b2 min(W)\nh\n\u2309\n(2)\nwhere \u230a\u00b7\u2309 indicates round operation. N is the target bit number. Wq and W denote the quantized\nand full-precision weights, respectively. h is the normalization factor for weights and z is the zero-\npoint value. The clamp operation constrains the value within the range of N-bit integer, specifically\n[0, 2N \u2212 1]. In Eqn.(2), \u03b3 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are learnable clipping strengths for the upper and\nthe lower bound of weights, respectively. We instantiate \u03b3 and \u03b2 by the sigmoid function*. Hence,\n\u03981 = {\u03b3, \u03b2} in Eqn.(1).\nNote that LWC degrades into a vanilla MinMax quantization scheme used in existing works (Xiao\net al., 2023),Frantar et al. (2022) when \u03b3 = 1 and \u03b2 = 1. By inheriting the benefits of Min-\nMax quantization, LWC only needs to adjust the clipping strengths to determine an optimal clipping\nthreshold, which would reduce the optimization difficulty. Clipped by an optimal threshold, the orig-\ninal weights would be easy to quantize. As indicated by the experiments in Table 1, our proposed\nlearnable weight clipping method significantly outperforms previous weight-only quantization tech-\nniques (Frantar et al., 2022; Lin et al., 2023)).\n3.3\nLEARNABLE EQUIVALENT TRANSFORMATION\nOther than LWC which enables quantization-friendly weights by optimizing the clipping threshold,\nwe further reduce the difficulty of weight-activation quantization by a learnable equivalent transfor-\nmation (LET). Considering that outliers in the activation map are systematic and unique to specific\nchannels, previous methods such as SmoothQuant (Xiao et al., 2023) migrate the difficulty of quan-\ntization from activations to weights with a mathematically equivalent transformation. However, they\nhand-craft the equivalent parameters, leading to suboptimal results.\n*Sigmoid(t) = 1/(1 + exp\u2212t)\n5\nPublished as a conference paper at ICLR 2024\nThanks to the inclusion of block-wise quantization error minimization, our LET can determine the\noptimal equivalent parameters in a differentiable way. Inspired by SmoothQuant (Xiao et al., 2023)\nand Outlier Suppression+ (Wei et al., 2023), we adopt channel-wise scaling and channel-wise shift-\ning to manipulate the activation distribution, providing an effective solution for the outlier issue.\nSpecifically, we investigate the equivalent transformation across both the linear layer and attention\noperation, as illustrated in Figure3.\nLinear layer. The linear layer takes an input token sequence X \u2208 RT \u00d7Cin where T is the token\nlength and is the multiplication of the weight matrix W \u2208 RCin\u00d7Cout and bias vector B \u2208 R1\u00d7Cout.\nA mathematically equivalent linear layer is expressed as:\nY = XW + B = [(X \u2212 \u03b4) \u2298 s\n|\n{z\n}\n\u02dcX\n] \u00b7 [s \u2299 W\n| {z }\n\u02dc\nW\n] + [B + \u03b4W\n|\n{z\n}\n\u02dcB\n]\n(3)\nwhere Y represents the output, s \u2208 R1\u00d7Cin and \u03b4 \u2208 R1\u00d7Cin are channel-wise scaling and shifting\nparameters, respectively, \u02dcX, \u02dc\nW and \u02dcB are equivalent activation, weight and bias, respectively, \u2018\u2298\u2019\nand \u2018\u2299\u2019 are elementwise division and multiplication. By Eqn.(3), the activations are transformed\nto be quantization-friendly at a cost of increased quantization difficulty in weights. In this sense,\nLWC in Sec. 3.2 can improve the performance of weight-activation quantization achieved by LET\nbecause it renders weights quantization-friendly. Finally, we perform quantization on transformed\nactivations and weights, as given by\nY = Qa( \u02dcX)Qw( \u02dc\nW) + eB,\n(4)\nwhere Qa is the vanilla MinMax quantizer and Qw is the MinMax quantizer with learnable weight\nclipping (i.e. our LWC).\nNote that the scaling and shifting parameters in \u02dcX can be absorbed into the previous normaliza-\ntion or linear layer and the the scaling factors in \u02dc\nW can be fused into the original linear weight\nW. Therefore, the equivalent transformation in Eqn.(3) can effectively reduce quantization errors\nwithout introducing additional parameters or costs. We employ this equivalent transformation in all\nlinear layers of the LLM except for the second linear layer of FFN as shown in Figure3. This may\nbe because the high sparsity of features after the non-linear layer (Liu et al., 2023c) leads to unstable\ngradients when applying learnable equivalent transformations.\nAttention operation. Beyond the linear layer, the attention operation also accounts for a significant\nproportion of the computation. Additionally, the auto-regressive pattern of LLM necessitates storing\nthe key-value(KV) cache for each token, which results in substantial memory demands for long\nsequences. Therefore, we also quantize Q/K/V matrixes into low-bit in the weight-activation\nquantization setting. Specifically, the learnable equivalent transform of the self-attention affinity\nmatrix can be written as:\nP = Softmax(QKT ) = Softmax((Q \u2298 sa\n| {z }\n\u02dcQ\n)(sa \u2299 KT\n|\n{z\n}\n\u02dcKT\n)).\n(5)\nwhere sa \u2208 R1\u00d7Cout is the scaling factor in the affinity matrix. Similar to Eqn.(4), the quantized\naffinity matrix calculation is expressed as P = Softmax(Qa( eQ)Qa( eKT )). Here we also use Min-\nMax quantization scheme as Qa to quantize \u02dcQ/ \u02dcK matrixes. From Eqn.(4) and Eqn.(5) we know\nthat \u03982 = {\u03b4, s, sa} in Eqn.(1).\nThe channel-wise scaling factors in \u02dcQ and \u02dcK, as seen in Eq.(5), can be absorbed into linear weights\nof the query and key projection, respectively. It is worth mentioning that the explicit transformation\nof V is omitted as its distribution has already been channel-wise altered by the inverse transforma-\ntion associated with the output projection linear layer.\n4\nEXPERIMENTS\n4.1\nSETTINGS\nQuantization. We experiment with both weight-only and weight-activation quantization. For the\nformer, default settings are INT4/INT3/INT2 per-channel weight quantization. Group-wise weight\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Weight-only quantization Results of LLaMA-1 and LLaMA-2 Models. We report\nWikiText2 perplexity in this table, C4 perplexity can be found in Table A19 in Appendix.\nLLaMA1&2 / PPL\u2193\n1-7B\n1-13B\n1-30B\n1-65B\n2-7B\n2-13B\n2-70B\nFP16\n-\n5.68\n5.09\n4.10\n3.53\n5.47\n4.88\n3.31\nW2A16\nRTN\n1.1e5\n6.8e4\n2.4e4\n2.2e4\n3.8e4\n5.6e4\n2.0e4\nGPTQ\n2.1e3\n5.5e3\n499.75\n55.91\n7.7e3\n2.1e3\n77.95\nOmniQuant\n15.47\n13.21\n8.71\n7.58\n37.37\n17.21\n7.81\nW2A16\ng128\nRTN\n1.9e3\n781.20\n68.04\n15.08\n4.2e3\n122.08\n27.27\nGPTQ\n44.01\n15.60\n10.92\n9.51\n36.77\n28.14\nNAN\nAWQ\n2.6e5\n2.8e5\n2.4e5\n7.4e4\n2.2e5\n1.2e5\n-\nOmniQuant\n9.72\n7.93\n7.12\n5.95\n11.06\n8.26\n6.55\nW2A16\ng64\nRTN\n188.32\n101.87\n19.20\n9.39\n431.97\n26.22\n10.31\nGPTQ\n22.10\n10.06\n8.54\n8.31\n20.85\n22.44\nNAN\nAWQ\n2.5e5\n2.7e5\n2.3e5\n7.4e4\n2.1e5\n1.2e5\n-\nOmniQuant\n8.90\n7.34\n6.59\n5.65\n9.62\n7.56\n6.11\nW3A16\nRTN\n25.73\n11.39\n14.95\n10.68\n539.48\n10.68\n7.52\nGPTQ\n8.06\n6.76\n5.84\n5.06\n8.37\n6.44\n4.82\nAWQ\n11.88\n7.45\n10.07\n5.21\n24.00\n10.45\n-\nOmniQuant\n6.49\n5.68\n4.74\n4.04\n6.58\n5.58\n3.92\nW3A16\ng128\nRTN\n7.01\n5.88\n4.87\n4.24\n6.66\n5.51\n3.97\nGPTQ\n6.55\n5.62\n4.80\n4.17\n6.29\n5.42\n3.85\nAWQ\n6.46\n5.51\n4.63\n3.99\n6.24\n5.32\n-\nOmniQuant\n6.15\n5.44\n4.56\n3.94\n6.03\n5.28\n3.78\nW4A16\nRTN\n6.43\n5.55\n4.57\n3.87\n6.11\n5.20\n3.67\nGPTQ\n6.13\n5.40\n4.48\n3.83\n5.83\n5.13\n3.58\nAWQ\n6.08\n5.34\n4.39\n3.76\n6.15\n5.12\n-\nOmniQuant\n5.86\n5.21\n4.25\n3.71\n5.74\n5.02\n3.47\nW4A16\ng128\nRTN\n5.96\n5.25\n4.23\n3.67\n5.72\n4.98\n3.46\nGPTQ\n5.85\n5.20\n4.23\n3.65\n5.61\n4.98\n3.42\nAWQ\n5.81\n5.20\n4.21\n3.62\n5.62\n4.97\n-\nOmniQuant\n5.77\n5.17\n4.19\n3.62\n5.58\n4.95\n3.40\nquantization is represented by \u2018g\u2019, e.g., W3A16g128 means 3-bit weight-only quantization with a\n128-group size. In weight-activation quantization, defaults are INT6/INT4 per-channel weight and\nper-token activation quantization (Dettmers et al., 2022). All intermediate activations are quantized\ninto low-bit, excluding the SoftMax output, kept at full precision due to its long-tail distribution\nmaking it unsuitable for uniform quantization.\nTraining The channel-wise scaling factor is initialized with SmoothQuant (Xiao et al., 2023), and\nthe channel-wise shifting factor is initialized using Outlier Suppression+ (Wei et al., 2023). To\noptimize the learnable parameters, we utilize the AdamW optimizer with zero weight decay. The\nlearning rate for learnable weight clipping and equivalent transformation is set as 5e \u2212 3 and 1e \u2212 2,\nrespectively. We employ a calibration dataset consisting of 128 randomly selected 2048-token seg-\nments from WikiText2 (Merity et al., 2016). The entire training process is facilitated on a single\nNvidia A100 GPU, using a batch size of 1 over 20 epochs, except for W2A16 quantization that\nleverages 40 epochs. For weight-activation quantization, both learnable weight clipping and equiv-\nalent transformation are activated. For weight-only, both are used for OPT, but only the clipping is\nfor LLaMA, as Table A3 shows negligible benefits from the equivalent transformation for LLaMA.\nModels.\nWe test on OPT(125M-66B)(Zhang et al., 2022)), LLaMA(7B-65B) (Touvron et al.,\n2023a), LLaMA-2(7B-70B) (Touvron et al., 2023b), Falcon-180B (Penedo et al., 2023), and\ninstruction-tuned LLaMA-2-chat (Touvron et al., 2023b) for generalizability. While the main pa-\nper highlights the LLaMA results, comprehensive details for other models are available in Sec. A8\nof the Appendix.\nEvaluation. Following the previous work (Lin et al., 2023; Frantar et al., 2022), we evaluate quan-\ntized models by reporting the perplexity of language generation experiments, specifically on Wiki-\nText2 (Merity et al., 2016), PTB (Marcus et al., 1994)), C4 (Raffel et al., 2020). Moreover, accu-\nracy is evaluated in zero-shot tasks including PIQA (Bisk et al., 2020), ARC (Clark et al., 2018),\nBoolQ (Clark et al., 2019), and HellaSwag (Clark et al., 2018). We adhere to the GPTQ (Frantar\net al., 2022) settings for language generation experiments, and implement the lm-eval-harness (Gao\net al., 2021) for the execution of all zero-shot tasks.\nBaselines. For weight-only quantization, we compare with vanilla round-to-nearest quantization\n(RTN), GPTQ (Frantar et al., 2022), and AWQ (Lin et al., 2023). For weight-activation quantization,\nwe compare our method with SmoothQuant (Xiao et al., 2023), Outlier Supression + (Wei et al.,\n2023), RPTQ (Yuan et al., 2023), and the recent QAT method LLM-QAT (Liu et al., 2023b). Note\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Weight-activation quantization results of LLaMA Models. This table reports the accu-\nracy of 6 zero-shot tasks. Perplexity results can be found in Table A23 & A24 at Appendix.\nLLaMA / Acc\u2191 #Bits\nMethod\nPIQA ARC-e Arc-c BoolQ HellaSwag Winogrande\nAvg.\nLLaMA-1-7B\nFP16\n-\n77.47\n52.48\n41.46\n73.08\n73.00\n67.07\n64.09\nW6A6\nSmoothQuant\n76.75\n51.64\n39.88\n71.75\n71.67\n65.03\n62.81\nW6A6\nOS+\n76.82\n51.35\n41.13\n72.08\n71.42\n65.98\n61.13\nW6A6\nOmniQuant\n77.09\n51.89\n40.87\n72.53\n71.61\n65.03\n63.17\nW4A4\nSmoothQuant\n49.80\n30.40\n25.80\n49.10\n27.40\n48.00\n38.41\nW4A4\nLLM-QAT\n51.50\n27.90\n23.90\n61.30\n31.10\n51.90\n41.27\nW4A4\nLLM-QAT+SQ 55.90\n35.50\n26.40\n62.40\n47.80\n50.60\n46.43\nW4A4\nOS+\n62.73\n39.98\n30.29\n60.21\n44.39\n52.96\n48.43\nW4A4\nOmniQuant\n66.15\n45.20\n31.14\n63.51\n56.44\n53.43\n52.65\nLLaMA-1-13B\nFP16\n-\n79.10\n59.89\n44.45\n68.01\n76.21\n70.31\n66.33\nW6A6\nSmoothQuant\n77.91\n56.60\n42.40\n64.95\n75.36\n69.36\n64.43\nW6A6\nOS+\n78.29\n56.90\n43.09\n66.98\n75.09\n69.22\n64.92\nW6A6\nOmniQuant\n78.40\n57.28\n42.91\n67.00\n75.82\n68.27\n64.95\nW4A4\nSmoothQuant\n61.04\n39.18\n30.80\n61.80\n52.29\n51.06\n49.36\nW4A4\nOS+\n63.00\n40.32\n30.38\n60.34\n53.61\n51.54\n49.86\nW4A4 OmniQuant\n69.69\n47.39\n33.10\n62.84\n58.96\n55.80\n54.37\nLLaMA-1-30B\nFP16\n-\n80.08\n58.92\n45.47\n68.44\n79.21\n72.53\n67.44\nW6A6\nSmoothQuant\n77.14\n57.61\n42.91\n65.56\n78.07\n69.92\n65.20\nW6A6\nOS+\n80.14\n58.92\n45.05\n68.02\n77.96\n71.98\n67.01\nW6A6\nOmniQuant\n79.81\n58.79\n45.22\n68.38\n78.95\n72.21\n67.23\nW4A4\nSmoothQuant\n58.65\n35.53\n27.73\n60.42\n35.56\n48.06\n44.83\nW4A4\nOS+\n67.63\n46.17\n34.40\n60.70\n54.32\n52.64\n52.62\nW4A4\nOmniQuant\n71.21\n49.45\n34.47\n65.33\n64.65\n59.19\n56.63\nLLaMA-1-65B\nFP16\n-\n80.79\n58.71\n46.24\n82.29\n80.72\n77.50\n71.04\nW6A6\nSmoothQuant\n80.25\n57.92\n45.50\n80.22\n80.18\n74.76\n69.80\nW6A6\nOS+\n79.67\n55.68\n45.22\n80.02\n78.03\n73.95\n68.76\nW6A6\nOmniQuant\n81.01\n58.12\n46.33\n80.64\n79.91\n75.69\n70.28\nW4A4\nSmoothQuant\n64.47\n40.44\n29.82\n59.38\n39.90\n52.24\n47.71\nW4A4\nOS+\n68.06\n43.98\n35.32\n62.75\n50.73\n54.30\n52.52\nW4A4\nOmniQuant\n71.81\n48.02\n35.92\n73.27\n66.81\n59.51\n59.22\nthat we reproduce SmoothQuant and Outlier Suppression+ with per-channel weight quantization\nand per-token activation quantization for fair comparisons.\n4.2\nWEIGHT-ONLY QUANTIZATION RESULTS\nThe results of the LLaMA family can be found in Table 1, while the results for OPT are presented in\nthe Sec. A8 of Appendix. As illustrated by the tables, OmniQuant consistently outperforms the prior\nLLM weight-only quantization method across various LLM families (OPT, LLaMA-1, LLaMA-\n2) and diverse quantization configurations, including W2A16, W2A16g128, W2A16g64, W3A16,\nW3A16g128, W4A16, and W4A16g128. These findings suggest OmniQuant\u2019s versatility, being\nadaptable to a multitude of quantization configurations. For instance, while AWQ (Lin et al., 2023) is\nparticularly effective with group-wise quantization, OmniQuant demonstrates superior performance\nacross both channel-wise and group-wise quantization. Furthermore, the performance benefits of\nOmniQuant become more pronounced as the quantization bit size decreases.\n4.3\nWEIGHT-ACTIVATION QUANTIZATION RESULTS\nIn weight-activation quantization, our main focus lies on W6A6 and W4A4 quantization. We ex-\nclude W8A8 quantization as SmoothQuant can nearly achieve lossless W8A8 quantized models\nwhen compared with full-precision counterparts. The results of the LLaMA family can be found in\nTable 2, while the results for OPT are presented in Table A25 of Appendix. Table 2 illustrates the\nzero-shot task accuracy of LLaMA weight-activation quantization. Notably, OmniQuant markedly\nenhances the average accuracy by +4.99% \u223c +11.80% across various models at W4A4 quantization.\nRemarkably, in the LLaMA-7B, OmniQuant even surpasses the recent QAT method, LLM-QAT (Liu\net al., 2023b), by an impressive margin of +6.22%. This improvement demonstrates the efficacy of\nincorporating additional learnable parameters, which proves to be more beneficial than the global\nweight tuning utilized by QAT.\n8\nPublished as a conference paper at ICLR 2024\n50\n88\n22\n45\n104 11\n37\n37\n86\n35\n49\n45\n85\n70\n80\n41\n40\n35\n50\n50\n(a) LLaMa-2-7b-chat\n(b) LLaMa-2-13b-chat\nwin rate\n69.4%\n80.3%\n50.0%\nwin rate\n46.6%\n54.4%\n56.2%\nFormer Win\nFormer Lost\nTie\nW3A16g128\nFigure 4: Comparing W3A16g128 quantization among RTN, AWQ (Lin et al., 2023), and Omni-\nQuant under Vicuna-Bench (Chiang et al., 2023). Win rates are calculated without considering tie\nsamples. A higher win rate indicates the better performance of the former of vs. pairs.\nTable 3: Deployment of weight-only quantization through MLC-LLM. We report the memory size\nof quantized weights (denoted as \u2018WM\u2019) and the running memory (denoted as \u2018RM\u2019) and speed in\nNVIDIA A100-80G.\nLLaMA\n7B\n13B\n30B\n65B\nWM\nRM token/s WM\nRM token/s WM\nRM token/s WM\nRM token/s\nFP\n12.6G 14.4G 69.2 24.3G 27.1G 52.5 60.6G 66.1G 23.9\nOOM\n-\n-\nW4A16g128 3.8G 5.7G 134.2 7.0G 10.0G 91.3 16.7G 21.7G 43.6 33.0G 41.0G 24.3\nW3A16g128 3.2G 5.1G\n83.4\n5.8G 8.7G\n57.6 13.7G 18.7G 29.0 27.0G 35.1G 15.2\nW2A16g128 2.2G 4.1G\n83.9\n4.0G 7.5G\n92.6\n9.2G 14.1G 36.7 18.0G 25.6G 24.8\n4.4\nQUANTIZATION OF INSTRUCTION-TUNED MODELS\nTo validate the generalization capability of our method, we test the quantization on LLaMA-2-\nchat (Touvron et al., 2023b), an instruction-tuned model for chatbots. Using the GPT-4 evaluation\nprotocol (Chiang et al., 2023), performance is assessed on the Vicuna benchmark (Chiang et al.,\n2023) comprising 80 questions. To negate position bias (Zheng et al., 2023), each pair is compared\nin both sequences, totaling 160 trials per comparison. Figure 4 compares RTN, AWQ (Lin et al.,\n2023), and OmniQuant. In LLaMA-2-7b-chat, OmniQuant matches AWQ with a 50% win rate but\nsurpasses RTN more (80.3% vs. 69.4%). In LLaMA-2-13b-chat, while AWQ lags behind RTN,\nOmniQuant consistently improves quantization model performance.\n4.5\nACCELERATION ON REAL DEVICE\nMLC-LLM\u2020 provides a versatile deployment solution for diverse language models across various\nhardwares. It particularly excels in deploying quantized models on CUDA. One of OmniQuant\u2019s\nstrengths lies in its ability to avoid extra operations for quantized models, allowing MLC-LLM to\nseamlessly run models created with OmniQuant. Table,3 shows memory requirements and inference\nspeeds of the LLaMA family on an NVIDIA A100-80G. \u2019Weights Memory (WM)\u2019 represents quan-\ntized weight storage, and \u2019Running Memory (RM)\u2019 indicates the memory for inference, with the\nlatter being higher due to certain retained activations. Inference speed is gauged by generating 512\ntokens. It is evident that quantized models significantly reduce memory usage compared to 16-bit\nfull-precision models. For instance, models with W4A16g128 and W2A16g128 quantization almost\ndouble the inference speed. However, MLC-LLM\u2019s support for INT3/INT2 is currently suboptimal,\nparticularly for INT3. Enhancements to INT3/INT2 quantization speed are in our future roadmap.\nAdditionally, we only explore the deployment of weight-only quantization in this study due to that\nW4A4 and W6A6 quantization methods lack out-of-the-box hardware support.\n5\nCONCLUSION\nWe present OmniQuant, a method advancing weight-only and weight-activation quantization to low-\nbit formats. OmniQuant\u2019s core principle is to retain original full-precision weights while adding\nlearnable parameters. It uses learnable weight clipping and learnable equivalent transformation to\noptimize weight and activation for quantization. While incorporating gradient updates, OmniQuant\nmaintains training efficiency comparable to existing PTQ methods. It outperforms current methods\nin language generation and zero-shot tasks and is suited for instruction-tuned LLMs. In addition,\nOmniQuant also ensures hardware compatibility as its added parameters can be absorbed.\n\u2020https://github.com/mlc-ai/mlc-llm\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nThis paper is partially supported by the National Key R&D Program of China No.2022ZD0161000\nand the General Research Fund of Hong Kong No.17200622. We thank Wentao Liu from SenseTime\nfor his valuable insights and discussions regarding LLM deployment. We also acknowledge Siyuan\nFeng from Apache TVM for assisting in deploying our OmniQuant in the MLC LLM project.\nREFERENCES\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nJerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of\nlarge language models with guarantees. arXiv preprint arXiv:2307.13304, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-\nvasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural\nnetworks. arXiv preprint arXiv:1805.06085, 2018.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nTim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In\nInternational Conference on Machine Learning, pp. 7750\u20137774. PMLR, 2023.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023a.\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashk-\nboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized repre-\nsentation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023b.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-\ndra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\n10\nPublished as a conference paper at ICLR 2024\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot\nlanguage model evaluation. Version v0. 0.1. Sept, 2021.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nJie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang, Rongrong Ji, and Liujuan Cao. You only\nsegment once: Towards real-time panoptic segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 17819\u201317829, 2023.\nLinyan Huang, Huijie Wang, Jia Zeng, Shengchuan Zhang, Liujuan Cao, Rongrong Ji, Junchi Yan,\nand Hongyang Li. Geometric-aware pretraining for vision-centric 3d object detection. arXiv\npreprint arXiv:2304.03105, 2023.\nLinyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, and Hongyang\nLi. Leveraging vision-centric multi-modal expertise for 3d object detection. Advances in Neural\nInformation Processing Systems, 36, 2024.\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W\nMahoney, and Kurt Keutzer.\nSqueezellm: Dense-and-sparse quantization.\narXiv preprint\narXiv:2306.07629, 2023.\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.\nOwq: Lessons\nlearned from activation outliers for weight quantization in large language models. arXiv preprint\narXiv:2306.02272, 2023.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and\nShi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv\npreprint arXiv:2102.05426, 2021.\nZhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu.\nRepq-vit: Scale reparameterization for\npost-training quantization of vision transformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 17227\u201317236, 2023.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration.\narXiv preprint\narXiv:2306.00978, 2023.\nJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang.\nQllm:\nAccurate and efficient low-bitwidth quantization for large language models.\narXiv preprint\narXiv:2310.08041, 2023a.\nZechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to-\nuniform quantization: Towards accurate quantization via generalized straight-through estimation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n4942\u20134952, 2022.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware\ntraining for large language models. arXiv preprint arXiv:2305.17888, 2023b.\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava,\nCe Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms\nat inference time. In International Conference on Machine Learning, pp. 22137\u201322176. PMLR,\n2023c.\nMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,\nKaren Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure.\nIn Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey,\nMarch 8-11, 1994, 1994.\n11\nPublished as a conference paper at ICLR 2024\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng\nDai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of\nthought. arXiv preprint arXiv:2305.15021, 2023.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or\ndown? adaptive rounding for post-training quantization. In International Conference on Machine\nLearning, pp. 7197\u20137206. PMLR, 2020.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Feng-\nwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer lan-\nguage models. Advances in Neural Information Processing Systems, 35:17402\u201317414, 2022.\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xian-\nglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent\nand optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models.\nIn International\nConference on Machine Learning, pp. 38087\u201338099. PMLR, 2023.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. arXiv preprint arXiv:2306.09265, 2023.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun,\nQiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for\nlarge language models. arXiv preprint arXiv:2304.01089, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nYiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xi-\nangyu Yue. Meta-transformer: A unified framework for multimodal learning. arXiv preprint\narXiv:2307.10802, 2023a.\nYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei\nLiu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse llms.\narXiv preprint arXiv:2310.08915, 2023b.\n12\nPublished as a conference paper at ICLR 2024\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\nIn this appendix, we provide further details as follows:\n\u2022 Sec.A1: Presents the pseudo code for our OmniQuant algorithm.\n\u2022 Sec.A2: Summarizes the distinctions with existing equivalent transformation methods.\n\u2022 Sec.A3: Details ablation studies, encompassing the efficacy of each component, design\nchoices for the learnable equivalent transformation, training time, and calibration data, etc.\n\u2022 Sec.A4: Provides the detailed training time for the LLaMA family.\n\u2022 Sec.A5: Explores the internal mechanisms of the proposed method.\n\u2022 Sec.A6: Compares the proposed LWC with other clipping-based quantization approaches.\n\u2022 Sec.A8: Showcases the complete results for OPT, LLaMA-1, LLaMA-2, and Falcon mod-\nels.\nA1\nOVERALL ALGORITHM\nThe comprehensive training algorithm of OmniQuant is illustrated in Algorithm 1. We employ a\nblock-wise calibration strategy comprising three steps: initialization of learnable parameters (Lines\n4-5), training these learnable parameters (Lines 6-15), transforming the model with learned param-\neters, and then quantization(Lines 16-18). The OmniQuant algorithm finds the optimal transforma-\ntion to enhance the quantization compatibility of the LLM model. Additionally, due to the elegant\ndesign, OmniQuant can achieve rapid convergence using a small calibration dataset.\nAlgorithm 1 Overall algorithm of OmniQuant.\nInput: calibration dataset X, pre-trained LLM model M\nOutput: quantized model.\n1: Xfp = Xq = X\n\u25b7 init inputs of full-precision and quantized models.\n2: for Bi in M do:\n\u25b7 block-wise calibration\n3:\nXfp = Bi(Xfp)\n\u25b7 update the input of full-precision model\n4:\ninit learnable weight clipping parameters \u03981\n5:\ninit learnable equivalent transformation \u03982\n6:\nfor k in epochs do:\n7:\nfor (xq,xfp) in (Xq,Xfp) do\n8:\nB\n\u2032\ni = LET(Bi,\u03982)\n\u25b7 With Eq.(3),Eq.(5)\n9:\nB\n\u2032\ni = Quantization with LWC(B\n\u2032\ni,\u03981)\n\u25b7 With Eq.(2)\n10:\nx\n\u2032\nq = B\n\u2032\ni(xq)\n11:\nloss = ||xfp \u2212 x\n\u2032\nq||2\n\u25b7 With Eq.(1)\n12:\nloss.backward()\n13:\nupdate \u03981 and \u03982 through gradient\n14:\nend for\n15:\nend for\n16:\nBi = LET(Bi,\u03982)\n17:\nBi = Quantization with LWC(Bi,\u03981)\n\u25b7 obtain the quantized block\n18:\nXq = Bi(Xq)\n\u25b7 update the input of quantized model\n19: end for\n20: return quantized model M\nA2\nDISTINCTION OF EXISTING EQUIVALENT TRANSFORMATION METHODS\nEquivalent transformation is popular in the quantization of large language models. In this section, we\nsummarize the distinction of proposed OmniQuant with existing equivalent transformation works,\n13\nPublished as a conference paper at ICLR 2024\nTable A1: Distinction of existing equivalent transformation methods.\nMethod\nET operation ET position\nET parameters\napplication\nSmoothQuant\nscaling\nlinear layer\npre-defining\nweight-activation quantization\nAWQ\nscaling\nlinear layer\ngrid searching\nweight-only quantization\nOP+\nscaling\nlinear layer\ngrid searching for scaling\nweight-activation quantization\n& shifting\nand pre-defining for shifting\nOmniQuant\nscaling\nlinear layer gradient-based optimization\nweight-only quantization\n& shifting & attention\n& weight-activation quantization\nincluding SmoothQuant (Xiao et al., 2023), AWQ Lin et al. (2023), Outlier Supression (OP+)+ Wei\net al. (2023). As shown in Table A1:\n\u2022 For the equivalent transformation operation, both SmoothQuant and AWQ only consider\nchannel-wise scaling operation, while OP+ and OmniQuant consider both channel-wise\nscaling and shifting operation.\n\u2022 For the execution position, previous methods only carry equivalent transformation on linear\nlayers (Eq.(4)), while OmniQuant also considers the matrix multiplication within attention\n(Eq.(5)). This point enlarges the solution space of equivalent transformation and facilitates\nthe quantization of Q and K.\n\u2022 For the manners to obtain parameters of equivalent transformation, SmoothQuant lever-\nage pre-defined migration strength. Then, AWQ and OP+ introduce grid searching based\non some heuristic proxy. However, OmniQuant optimized all equivalent transformation\nparameters through end-to-end gradient descent, which significantly improve the perfor-\nmance.\n\u2022 For the application scenario, previous methods are designed for weight-only quantization\nor weight-activation quantization. However, because of the elegant design and coopera-\ntion of the proposed LWC and LET, OmniQuant can achieve excel in both weight-only\nquantization and weight-activation quantization.\nA3\nABLATION STUDIES\nTable A2: Effect of combination of equivalent transformation and weight clipping. We report\nthe average perplexity of WikiText2 and C4, and the average accuracy on 6 zero-shot tasks like Table\n2.\nLLaMa-7B W4A4\nAverage PPL \u2193\nAverage Acc. \u2191\nSmoothQuant\n28.78\n38.41\nLET\n16.97\n48.83\nLET + grid-searched WC\n15.82\n49.59\nSmoothQuant + LWC\n15.80\n50.15\nLET + LWC\n12.87\n52.65\nCombination of equivalent transformation and weight clipping. The synergy between LET and\nLWC is achieved through a sophisticated differentiable framework as demonstrated in Algorithm 1,\nnot a simple additive combination. LET performs activation-to-weight migration, and LWC further\nfacilitates the quantization of weights, resulting in a seamless integration of the two techniques. In\nTable A2, we also test other combination variants, including replacing LET with SmoothQuant or\nreplacing LWC with grid-searched weight clipping. The results show that training LET and LWC\nsimultaneously achieves the best performance.\nEfficacy of each component. Table A3 reveals that the baseline model incorporates both LWC\nand LET, labeled as \u2019LWC+LET\u2019. We further investigate their contributions by removing each com-\nponent. Both components positively influence performance, but LET proves essential for weight-\nactivation quantization. Disabling it for W4A4 results in a marked increase in perplexity to e3,\nmainly due to challenges with activation quantization outliers. For weight-only quantization, LET\nsignificantly boosts OPT\u2019s performance but offers a slight enhancement for LLaMA, explained by\n14\nPublished as a conference paper at ICLR 2024\nTable A3: Efficacy of each component. WikiText2 perplexity1 is reported in this table. \u2018-\u2019 indicats\nremove the corresponding module from the overall proposed methods.\nPPL\u2193\nLLaMA-13B\nOPT-13B\nMethod\nW4A4\nW3A16\nW4A4\nW3A16\nLWC+LET\n10.87\n5.65\n11.65\n10.87\ncomponents\n-LWC\n20.75\n7.65\n15.23\n12.98\n-LET\n5.4e3\n5.68\n7.8e3\n11.29\n-LWC-LET\n1.8e3\n10.68\n7.8e5\n4.6e3\nLLaMA\u2019s few weight outliers. For example, in naive W3A16 quantization (-LWC-LET), LLaMA\nreaches a perplexity of 10.68, while OPT\u2019s spikes to 4.6e3. Consequently, LET is turned off for\nLLaMA in weight-only quantization given its limited advantage for faster training.\nTable A4: Design choices of learnable equivalent transformation. WikiText2 perplexity1 is re-\nported in this table.\nPPL\u2193\nLLaMA-13B\nOPT-13B\nMethod\nW4A4\nW3A16\nW4A4\nW3A16\nLWC+LET\n10.87\n5.65\n11.65\n10.87\nLET\n-shifting\n11.47\n5.65\n13.64\n10.87\n-attention\n11.34\n5.65\n11.79\n10.87\nDesign choices of learnable equivalent transformation. In comparison to the equivalent trans-\nformation incorporated in SmoothQuant (Xiao et al. (2023)), our approach additionally implements\nchannel-wise shifting and attention transformation. The effects of these innovations are evaluated\nin Table A4. We can observe that both modifications enhance the performance of weight-activation\nquantization. However, the incremental benefit of the equivalent transformation in the attention op-\neration is comparatively minor. This discrepancy is primarily due to the majority of outliers existing\nin the output of the normalization layer while being less prevalent in the Q/K/V matrix.\nTable A5: Impact of LET on each position. \u2018-\u2019 indicates removing corresponding LET. We respec-\ntively remove the LET from each layer, and reporting the average perplexity of WikiText2 and C4,\nand the average accuracy on 6 zero-shot tasks like Table 2.\nLLaMa-7B\nAverage PPL \u2193\nAverage Acc. \u2191\nW4A4\n12.87\n52.65\n-[ln1, (q proj, k proj, v proj)]\n19.87\n46.79\n-[v proj, out proj]\n13.03\n51.68\n-[Q,K]\n13.34\n51.47\n-[ln2, fc1]\n14.47\n51.04\nImpact of LET on each position. We exclude the LET of the second linear layer due to the high\nsparsity of features after the non-linear layer leads to unstable gradients. Therefore, we have four\nLET pairs, represented as [ln1, (q proj, k proj, v proj)], [v proj, out proj], [Q, K], and [ln2, fc1].\nAs shown in Table A5, we can find that all four LETs can improve the performance, specially for\nthe [ln1, (q proj, k proj, v proj)] pair. Such results also demonstrate that the activation outliers are\nmore serious after layer normalization layers.\nTable A6: Impact of initialization of LET. We report the average perplexity of WikiText2 and C4,\nand the average accuracy on 6 zero-shot tasks like Table 2.\nLLaMa-7B\nAverage PPL \u2193\nAverage Acc. \u2191\nW4A4\n12.87\n52.65\ninitialize scaling as 1\n13.64\n51.37\ninitialize shifting as 0\n12.95\n52.22\n15\nPublished as a conference paper at ICLR 2024\nImpact\nof\ninitialization\nof\nLET.\nWe\ninitialize\nthe\nchannel-wise\nscaling\nfactor\nwith\nSmoothQuant Xiao et al. (2023), and initialize the channel-wise shifting with Outlier Suppres-\nsion+ Wei et al. (2023). To validate the impact of careful initialization, we try to initial scaling\nas 1 and initial shifting as 0. As shown in Table A6, we can find that careful initialization of scaling\nand shifting can improve the final performance. Specifically, scaling initialization is more important\nthan shifting, since scaling plays the main role in alleviating outliers.\nTable A7: Impact of Softmax quantization. We report the average perplexity of WikiText2 and\nC4, and the average accuracy on 6 zero-shot tasks like Table 2.\nLLaMa-7B\nAverage PPL \u2193\nAverage Acc. \u2191\nW4A4 + Softmax 16bit\n12.87\n52.65\nW4A4 + Softmax 8bit\n12.91\n51.93\nW4A4 + Softmax 6bit\n13.20\n51.70\nW4A4 + Softmax 4bit\n18.80\n48.52\nImpact of Softmax quantization. The output of SoftMax has a long-tailed distribution, making\nit unsuitable for uniform quantization. We carry out experiments to quantize the Softmax output\ninto different bit numbers. As shown in the following table, we can find that quantizing the output\nof softmax into 8-bit and 6-bit bring acceptable performance degeneration, which demonstrates that\nblock-wise calibration can compensate for the loss of 8-bit and 6-bit Softmax quantization. However,\n4-bit Softmax quantization brings significantly performance loss, which requires further exploration\nand additional trick such as log2 quantization in RepQViT (Li et al., 2023). Note that we keep the\noutput of SoftMax as 16-bit if no special instruction.\nTable A8: Impact of iterative training of LWC and LET. We report the average perplexity of\nWikiText2 and C4, and the average accuracy on 6 zero-shot tasks like Table 2.\nLLaMa-7B W4A4\nAverage PPL \u2193\nAverage Acc. \u2191\nsimultaneously\n12.87\n52.65\neach iteration\n13.56\n50.91\neach epoch\n13.51\n52.06\neach epoch + double training epochs 4bit\n12.80\n52.50\nImpact of iterative training. In our approach, LWC and LET are trained simultaneously, and we\nhave also explored an iterative training approach by iterations or epochs. The results, as presented in\nTable A8, clearly indicate that training LWC and LET simultaneously yields the best performance.\nThis experiment demonstrates that the synergy between LET and LWC creates a progressive process,\nwhere both techniques reinforce each other rather than interfere. To further support this statement,\nwe conducted an additional experiment (last row in Table A8), training LWC and LET iteratively\nwith double training epochs. The results show that simultaneous training with 20 epochs achieves\ncomparable performance to iterative training with 40 epochs. This demonstrates the effectiveness\nand efficiency of training LWC and LET simultaneously.\nTable A9: Ablation of training time. We train LLaMA-7B with different quantization configuration\non 128 2048-tokens segments from WikiText2 over various epochs. \u20180\u2019 indicates only initialization\nwithout fine-tuning. Wikitext perplexity is reported in this table.\nEpochs\nW4A16\nW3A16\nW2A16\nW6A6\nW4A4\n0\n6.29\n24.04\n1.1e5\n6.16\n33.93\n10\n5.87\n6.51\n27.49\n5.96\n12.04\n20\n5.85\n6.49\n17.46\n5.95\n11.26\n40\n5.86\n6.47\n15.47\n5.95\n11.23\n80\n-\n-\n14.77\n-\n-\nTraining Time As illustrated in Table A9, LLaMA-7B was trained across various epochs to deter-\nmine the optimal convergence time. Most quantization configurations converge within 20 epochs,\nwith the exception of W2A16, which necessitates 80 epochs. Consequently, we establish a training\n16\nPublished as a conference paper at ICLR 2024\nepoch of 20 for all configurations, except for W2A16, for which we set it to 40 in consideration of\nthe training time.\nTable A10: Ablation of calibration dataset.\nLLaMA-7B/PPL\u2193\nW3A16\nW4A4\nCalibration Dataset\nWikiText2\nC4\nWikiText2\nC4\nWikiText2\n6.47\n8.19\n11.23\n14.61\nC4\n6.67\n8.13\n12.17\n14.24\nPile\n6.69\n8.17\n12.04\n14.22\nVarience\n0.009\n0.0006\n0.17\n0.03\nTable A11: Ablation of sample number of calibration dataset.\nLLaMA-7B/PPL\u2193\nW3A16\nW4A4\nSample Number\nWikiText2\nC4\nWikiText2\nC4\n16\n6.47\n8.18\n11.56\n14.84\n32\n6.47\n8.18\n11.48\n14.80\n64\n6.48\n8.19\n11.40\n14.57\n128\n6.47\n8.19\n11.23\n14.61\n256\n6.46\n8.19\n11.41\n14.90\nCalibration Data OmniQuant utilizes gradient optimization on constrained calibration datasets,\nsourced from WikiText2 and comprising 128 segments with 2048 tokens each. This prompts con-\ncerns about potential overfitting to the calibration dataset. To explore this, we evaluated the cali-\nbration dataset\u2019s influence using two other datasets: Pile (Gao et al. (2020)) and c4 (Raffel et al.\n(2020)). As depicted in Table A10, the variance in perplexity across diverse calibration datasets\nis marginal, fluctuating between 0.0006 and 0.17. This underscores OmniQuant\u2019s robustness con-\ncerning calibration set distribution. Furthermore, the data efficiency of OmniQuant was gauged by\nmodulating the number of training samples, as presented in Table A11. Remarkably, OmniQuant\nconverges with as few as 16 samples. Our selection of 128 samples aligns with established practices\nin prior works (Frantar et al. (2022); Lin et al. (2023)).\nTable A12: Omniquant runtime on LLaMA family. The time correspond to training 128 2048-tokes\nsegment over 20 epochs and a batch size of 1 on a single NVIDIA A100-80G.\nLLaMA\n7B\n13B\n30B\n65B\nweight-only\n1.1h\n2.2h\n4.5h\n8.9h\nweight-activation\n1.6h\n3.3h\n7.3h\n14.4h\nA4\nTRAINING TIME\nAs shown in Table A12, we report the training time of the proposed OmniQuant within the LLaMA\nfamily. Note that for LLaMA, we only activate learnable weight clipping for weight-only quantiza-\ntion. Therefore, the training time for weight-only quantization is shorter relative to weight-activation\nquantization, given the fewer learnable parameters involved. While our proposed method necessi-\ntates a training time that is approximately 5\u00d7 greater than GPTQ, it remains markedly faster than\nQAT methods, which demand hundreds of GPU hours.\nA5\nPERFORMANCE ANALYSIS\nIn this section, we investigate the internal mechanism of learnable weight clipping and learnable\nequivalent transformation respectively. Further, we show that with OmniQuant, 3-bit and 4-bit\nachieve similar trade-off between model bits and perplexity.\nLearnable weight clipping. In addition to perplexity and accuracy, the quality of a quantization\nmethod can intuitively be evaluated by calculating the distance between quantized models and their\n17\nPublished as a conference paper at ICLR 2024\nTable A13: l1 distance between quantized model and full-precision model. ||W\u2212Wq|| indicates\nthe average l1 distance between quantized weight and full-precision weight. ||X \u2212 Xq|| denotes the\nl1 distance between the output of last transformer block.\nLLaMA-7B / l1 \u2193\n||W \u2212 Wq||\n||X \u2212 Xq||\nquantization\nw/o LWC\nw/ LWC\nw/o LWC\nw/ LWC\nW2A16g128\n0.0089\n0.0082\n3.24\n1.36\nW2A16g64\n0.0098\n0.0086\n3.51\n1.44\nW3A16\n0.0062\n0.0044\n2.80\n1.05\nW3A16g128\n0.0042\n0.0040\n1.37\n0.79\nW4A16\n0.0028\n0.0024\n0.98\n0.61\nW4A16g128\n0.0020\n0.0019\n0.68\n0.47\n(b) W3A16g128\n(c) W2A16g128\n(a) W3A16\nFigure A1: Visualization of learned clipping scale in different quantization settings in LLaMA-7B.\nfull-precision counterparts. This is demonstrated in Table A13, where we detail the l1 distance\nof weights and activations for LLaMA-7B\u2019s weight-only quantization. We can observe that the\nproposed Learned Weight Clipping (LWC) substantially decreases the l1 distance for both weights\nand activations. It\u2019s noteworthy that, in certain instances, the l1 distance for quantized models\nwithout LWC is similar to that of those utilizing LWC. However, models incorporating LWC exhibit\nmarkedly lower activation l1 distances. This observation underpins the argument that LWC can\neffectively balance quantization precision between outlier and regular values.\nAdditionally, we illustrate the distribution of the learned clipping scale (\u03b3 and \u03b2) as delineated in\nEq. (2) in Figure A1. It is apparent that LWC can learn different clippings for diverse quantization\nconfigurations. For instance, with per-channel weight quantization W3A16 as depicted in Figure\nA1(a), the learned clipping scale showcases a normal distribution. This suggests that approximately\nhalf of the outliers are being clipped. In the case of group-wise quantization, the learned clipping\nscale exhibits a long-tailed distribution, implying that most quantized groups are associated with\nminimal clipping. Note that lower bits exhibit more pronounced clipping. For example, W2A16g128\npossesses a 50% clipping scale larger than 0.95, whereas, in W3A16g128, this percentage rises to\n70%.\nLearnable equivalent transformation. Figure A2 provides visualizations of the intermediate ac-\ntivation in the linear layer. It is apparent that several outlier channels in the original activation\n(Figure A2(a)) possess significantly larger magnitudes compared to the regular channels, thereby\ncreating an incompatibility with activation quantization. Although SmoothQuant mitigates this is-\nsue to some degree, such as reducing the outlier magnitude from 70 to 2, Figure A2(b) reveals that\nthe magnitude of outlier channels still remains notably larger than that of other regular channels\nafter SmoothQuant. This phenomenon can be attributed to SmoothQuant\u2019s heuristic approach in de-\nriving channel-wise scaling, which inevitably makes it challenging to discover an optimal solution.\nThe impact of the proposed LET is depicted in Figure A2(c). It is noteworthy that the magnitude\ndisparity between the outlier and regular channels is markedly diminished. This homogenization\nof the activation distribution, facilitated by the LET, empowers OmniQuant to efficiently steer the\nweight-activation quantization towards a low-bit scheme.\nQuantization error. OmniQuant is the first differentiable post-training quantization algorithm for\nlarge language models. To demonstrate the advantage of gradient-based optimization, we also com-\n18\nPublished as a conference paper at ICLR 2024\n(b) SmoothQuant\n(c) Ours\n(a) Original\nAbsolute value\nFigure A2: Visualization of activation of a linear layer in OPT-13B. (a) Original activation. (b)\nActivation after SmoothQuant. (c) Activation after proposed learnable equivalent transformation.\nSimilar phenomena can be observed in different layer and different models.\n(a) LLaMa-7B W3A16\n(b) LLaMa-7B W4A4\nFigure A3: Block-wise quantization error. Grid-searched methods such as AWQ (Lin et al., 2023)\nand Outlier Suppression + (Wei et al., 2023) produce a more significant error than our gradient-based\noptimization method.\npare the quantization error of each block in Figure A3. We can find that OmniQuant significantly\nreduces the quantization loss compared with the grid-searching based method such as AWQ Lin\net al. (2023) and Outlier Suppression + (Wei et al., 2023).\n(a) OPT \n(b) LLaMa\nFigure A4: Bit-level scaling laws for perplexity.\nScaling laws. Quantization serves as a potent strategy to curtail the total model bits, thereby facili-\ntating the deployment of LLMs on edge or consumer devices with restricted memory. However, the\ntotal model bits are contingent on both the number of parameters within the original model and the\n19\nPublished as a conference paper at ICLR 2024\nquantization bits. Therefore, given a model bits constraint, the challenge arises: how does one op-\ntimally determine the number of parameters for the full-precision model and the quantization bits?\nTim Dettmers (Dettmers & Zettlemoyer (2023)) demonstrated that 4-bit quantization establishes a\nuniversally optimal balance between the total model bits and zero-shot accuracy. Nonetheless, in\nthis study, as shown in Figure A4,we would like to claim that OmniQuant can make 3-bit quantiza-\ntion achieve comparable performance like 4-bit quantization in the trade off between model bits and\nperplexity.\nTable A14: WikiText2 perplexity of clipping-based quantization methods. For fair comparison, we\nreproduce LSQ and PACT by replace LWC in our pipeline with them.\nLLaMA-7B/PPL\u2193\nPerplexity\nMethod\nW3A16\nW4A4\nFP\n5.68\nMinMax\n25.73\n14.49\nPACT (Choi et al. (2018))\n6.95\n18.25\nLSQ (Esser et al. (2019))\n6.63\n15.03\nLWC (Ours)\n6.47\n11.26\n(a)\n(b)\n(c)\nFigure A5: Weights range changing of different clipping-based methods during training. We\nplot the changing of weights range (maximum minus minimum) of the 3049-th output channel of the\nq-proj linear layer in the first LLaMa-1-7B block with W4A4 quantization. MinMax is the baseline\nwhich indicate withoud clipping. Similar phenomena can also be observed in other channels and\nother layers.\nA6\nCOMPARISONS WITH CLIPPING-BASED METHODS\nIn this paper, we proposed a novel method, learnable weight clipping (LWC), designed to adaptively\ndetermine the weight clipping threshold. LWC sets the threshold by scaling the original minimum\nand maximum values to delineate the solution space. We compare LWC against existing clipping-\nbased methods: PACT and LSQ. While PACT directly determines the clipping threshold, LSQ fo-\ncuses on the direct derivation of the scaling factor and zero-point. Both PACT and LSQ were initially\nformulated as QAT methods, accounting for both weight and activation clipping. For an equitable\ncomparison, our examination is restricted to weight clipping. We integrated PACT and LSQ into our\noptimization pipeline in lieu of LWC. Table A14 illustrates that while PACT and LSQ enhance the\nperformance of weight-only quantization compared to MinMax quantization, their efficacy dimin-\nishes in the weight-activation quantization setting. This decline can be attributed to the proposed\nLET during activation quantization, which alters the weight distribution in each training iteration,\nundermining the convergence of both LSQ and PACT. In contrast, LWC defines relative scaling val-\nues instead of absolute metrics, making it proficient in handling changes in weight distribution. For\nexample, Figure A5 shows that LWC can catch the dramatically changing of weights while PACT\nand LSQ failed.\n20\nPublished as a conference paper at ICLR 2024\nTable A15: Comparisons with SpQR and SqueezeLLM.\nSize\nMethod\nAvg bits\nWiki2\nC4\nLLaMa-1-7B\n\u2013\n16.00\n5.68\n7.08\nSpQR\n3.94\n5.87\n7.28\nSqueezeLLM\n4.07\n5.79\n7.20\nSqueezeLLM\n4.27\n5.77\n7.18\nOmniQuant\n4.16\n5.77\n7.21\nSqueezeLLM\n3.05\n6.20\n7.67\nSqueezeLLM\n3.24\n6.13\n7.56\nOmniQuant\n3.15\n6.15\n7.75\nLLaMa-1-13B\n\u2013\n16.00\n5.09\n6.61\nSpQR\n3.96\n5.22\n6.72\nSqueezeLLM\n4.07\n5.17\n6.69\nSqueezeLLM\n4.26\n5.17\n6.68\nOmniQuant\n4.16\n5.17\n6.69\nSqueezeLLM\n3.04\n5.51\n7.01\nSqueezeLLM\n3.24\n5.45\n6.92\nOmniQuant\n3.15\n5.44\n7.05\nLLaMa-1-30B\n\u2013\n16.00\n4.10\n5.98\nSpQR\n3.89\n4.25\n6.08\nSqueezeLLM\n4.06\n4.20\n6.05\nSqueezeLLM\n4.25\n4.18\n6.04\nOmniQuant\n4.16\n4.19\n6.06\nSqueezeLLM\n3.04\n4.56\n6.31\nSqueezeLLM\n3.24\n4.44\n6.23\nOmniQuant\n3.15\n4.56\n6.37\nA7\nCOMPARISONS WITH OTHER WEIGHT-ONLY QUANTIZATION METHODS\nOmniQuant is an asymmetrically uniform quantization method. In the main paper, we compare with\nthe same type of quantization methods, such as AWQ and GPTQ. Recently, there are also some other\nmethods exploring for other quantization format. For example, SpQR (Dettmers et al., 2023b) and\nSqueezeLLM (Kim et al., 2023) employ mixed-precision quantization to safeguard vital weights.\nFurthermore, SqueezeLLM also introduces non-uniform quantization to allocate more bits to sensi-\ntive weights. As shown in Table A15, we can find that OmniQuant can achieve comparable perfor-\nmance to SpQR and SqueezeLLM. While OmniQuant performs slightly worse than SqueezeLLM,\nour focus on uniform (INT) quantization provides simplicity and flexibility, supporting both weight-\nonly quantization and weight-activation quantization. In contrast, SpQR and SqueezeLLM only\nsupport weight-only quantization. We believe this distinction adds valuable context to the compari-\nson.\nA8\nFULL RESULTS\nIn this section, we provide a comprehensive presentation of our results across various datasets to\ncomplement the main paper. Specifically, the results include:\n\u2022 The perform overview (Figure A6).\n\u2022 Experiments results on extreme large model Falcon-180B (Table A18).\n\u2022 MMLU results on LLaMa-1-7B (Table A16).\n\u2022 Asymmetric bits quantization, including W4A8 on LLaMa-1-7B, W4A6, and W8A4. (Ta-\nble A17).\n\u2022 C4 perplexity with weight-only quantization in the LLaMA families (Table A19).\n21\nPublished as a conference paper at ICLR 2024\n\u2022 PTB perplexity with weight-only quantization in OPT families (Table A21).\n\u2022 C4 perplexity with weight-only quantization in OPT families (Table A22).\n\u2022 WikiText2 perplexity for weight-activation quantization in the LLaMA families (Table\nA23).\n\u2022 C4 perplexity for weight-activation quantization in the LLaMA families (Table A24).\n\u2022 WikiText2/PTB/C4 perplexity for weight-activation quantization in the LLaMA families\n(Table A25).\n7B\n13B\n30B\n65B\n7B\n13B\n70B\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\n1.3B-W3\n1.3B-W4\n2.7B-W3\n2.7B-W4\n6.7B-W3\n6.7B-W4\n13B-W3\n13B-W4\n30B-W3\n30B-W4\n7B-W3\n7B-W4\n13B-W3\n13B-W4\n30B-W3\n30B-W4\n65B-W3\n65B-W4\n7B-W3\n7B-W4\n13B-W3\n13B-W4\n70B-W3\n70B-W4\nsame size\n0.81 better PPL\nsame PPL\n0.33x szie\nFigure A6: Performance overview. We display the trade-off curves for three model families. Each\nmodel showcases two quantization variants: W4A16g128 and W3A16g128. It is evident that Omni-\nQuant markedly enhances the trade-off between perplexity and model size. Specifically, OmniQuant\ndelivers a reduction of 0.81 in perplexity for an equivalent model size and achieves the same per-\nplexity with only 0.33x of the model size.\nTable A16: Average MMLU accuracy of LLaMa-7B.\nLLaMa-1-7B (FP: 38.41%)\nW4A16g128\nW3A16g128\nW2A16g128\nW4A4\nRTN\n37.37%\n33.43%\n22.55%\n23.31\nGPTQ\n35.39%\n30.53%\n23.83%\n-\nAWQ\n37.71%\n35.43%\n22.58%\n-\nOP+\n-\n-\n-\n25.72\nOmniQuant\n37.50%\n35.60%\n26.03%\n26.93\nTable A17: Performance of weights and activations quantization on LLaMA-1-7B model with asym-\nmetric bits.\n#Bits\nMethod\nPPL \u2193\nAccuracy (%) \u2191\nWikiText2\nC4\nAvg.\nPIQA\nARC-e\nARC-c\nBoolQ\nHellaSwag\nWinogrande\nAvg.\nFP16\n-\n5.68\n7.08\n6.38\n77.47\n52.48\n41.46\n73.08\n73.00\n67.07\n64.09\nW4A8\nOmniQuant\n5.87\n7.34\n6.60\n77.36\n51.85\n38.65\n70.67\n71.20\n64.71\n62.40\nW4A6\nOmniQuant\n6.09\n7.63\n6.85\n75.73\n51.51\n38.31\n68.28\n70.79\n65.27\n61.64\nW8A4\nOmniQuant\n10.27\n12.77\n11.52\n69.47\n45.87\n32.84\n59.08\n58.66\n54.85\n53.46\n22\nPublished as a conference paper at ICLR 2024\nTable A18: Weight-only quantization on Falcon-180B.\nFalcon-180b\nPPL\u2193\nAcc\u2191\nMethod\nBit#\nMemory\nDevices\nWiki PTB C4 PIQA ARC-e Arc-c BoolQ HellaSwag Winogrande\n-\nBF16/FP16 335GB 5xA100 80GB 3.29 6.64 6.31 84.82 84.20 60.83 86.85\n85.91\n80.58\nRTN\nW3A16g512 65GB 1xA100 80GB 5.33 8.08 8.34 83.48 80.85 55.46 78.37\n81.05\n77.97\nOmniQuant W3A16g512 65GB 1xA100 80GB 3.71 6.95 6.71 84.71 82.91 60.92 84.03\n84.96\n79.40\nTable A19: C4 perplexity of Weight-only quantization results in LLaMA-1 and LLaMA-2 mod-\nels Continue of Table 1.\nLLaMA1&2 / PPL\u2193\n1-7B\n1-13B\n1-30B\n1-65B\n2-7B\n2-13B\n2-70B\nFP16\n-\n7.08\n6.61\n5.98\n5.62\n6.97\n6.46\n5.52\nW2A16\nRTN\n1.3e5\n5.6e4\n2.7e4\n2.2e4\n4.8e4\n7.2e4\n2.4e4\nGPTQ\n689.13\n2.5e3\n169.80\n40.58\nNAN\n323.12\n48.82\nOmniQuant\n24.89\n18.31\n13.89\n10.77\n90.64\n26.76\n12.28\nW2A16\ng128\nRTN\n1.0e3\n447.64\n99.45\n17.15\n4.9e3\n139.65\n42.13\nGPTQ\n27.71\n15.29\n11.93\n11.99\n33.70\n20.97\nNAN\nAWQ\n1.9e5\n2.3e5\n2.4e5\n7.5e4\n1.7e5\n9.4e4\n-\nOmniQuant\n12.97\n10.36\n9.36\n8.00\n15.02\n11.05\n8.52\nW2A16\ng64\nRTN\n151.43\n76.00\n30.07\n11.34\n475.35\n28.69\n13.43\nGPTQ\n17.71\n11.70\n9.92\n10.07\n19.40\n12.48\nNAN\nAWQ\n2.8e5\n2.2e5\n2.3e5\n7.4e4\n1.6e5\n9.5e4\n-\nOmniQuant\n11.78\n9.75\n8.65\n7.60\n12.72\n10.05\n7.88\nW3A16\nRTN\n28.26\n13.22\n28.66\n12.79\n402.35\n12.51\n10.02\nGPTQ\n9.49\n8.16\n7.29\n6.71\n9.81\n8.02\n6.57\nAWQ\n13.26\n9.13\n12.67\n7.11\n23.85\n13.07\n-\nOmniQuant\n8.19\n7.32\n6.57\n6.07\n8.65\n7.44\n6.06\nW3A16\ng128\nRTN\n8.62\n7.49\n6.58\n6.10\n8.40\n7.18\n6.02\nGPTQ\n7.85\n7.10\n6.47\n6.00\n7.89\n7.00\n5.85\nAWQ\n7.92\n7.07\n6.37\n5.94\n7.84\n6.94\n-\nOmniQuant\n7.75\n7.05\n6.37\n5.93\n7.75\n6.98\n5.85\nW4A16\nRTN\n7.93\n6.98\n6.34\n5.85\n7.71\n6.83\n5.79\nGPTQ\n7.43\n6.84\n6.20\n5.80\n7.37\n6.70\n5.67\nAWQ\n7.52\n6.86\n6.17\n5.77\n7.68\n6.74\n-\nOmniQuant\n7.34\n6.76\n6.11\n5.73\n7.35\n6.65\n5.65\nW4A16\ng128\nRTN\n7.37\n6.69\n6.06\n5.69\n7.24\n6.58\n5.63\nGPTQ\n7.21\n6.69\n6.06\n5.69\n7.12\n6.56\n5.58\nAWQ\n7.21\n6.70\n6.05\n5.68\n7.13\n6.56\n-\nOmniQuant\n7.21\n6.69\n6.06\n5.68\n7.12\n6.56\n5.58\n23\nPublished as a conference paper at ICLR 2024\nTable A20: WikiText2 perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193\n125M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n-\n27.65\n14.63\n12.47\n10.86\n10.12\n9.56\n9.34\nW2A16\ng128\nRTN\n7.2e3\n1.3e4\n5.7e4\n7.8e3\n7.6e4\n1.3e4\n3.6e5\nGPTQ\n597.66\n115.16\n61.59\n20.18\n21.36\n12.71\n82.10\nAWQ\n251.84\n47.97\n28.50\n16.20\n14.32\n12.31\n14.54\nOmniQuant\n75.43\n23.95\n18.13\n14.43\n12.94\n11.39\n30.84\nW2A16\ng64\nRTN\n7.0e3\n1.0e4\n19.3e4\n7.6e3\n1.8e4\n8.2e3\n1.1e4\nGPTQ\n204.40\n49.58\n29.37\n16.81\n16.65\n11.87\n356.01\nAWQ\n124.18\n29.78\n20.64\n14.63\n13.28\n11.59\n12.74\nOmniQuant\n62.56\n21.40\n16.76\n13.57\n12.33\n11.00\n10.59\nW3A16\nRTN\n1.2e3\n1.3e4\n1.6e4\n6.5e3\n4.6e3\n1.5e3\n6.1 e3\nGPTQ\n53.05\n21.17\n16.83\n15.09\n11.73\n10.30\n14.42\nAWQ\n69.43\n28.01\n263.10\n15.13\n20.09\n35.74\n4.5e3\nOmniQuant\n35.66\n16.68\n13.80\n11.65\n10.87\n10.00\n9.83\nW3A16\ng128\nRTN\n51.22\n119.00\n297.98\n23.54\n46.03\n18.80\n136.89w\nGPTQ\n39.24\n16.47\n13.69\n11.65\n10.35\n9.73\n10.96\nAWQ\n36.74\n16.32\n13.58\n11.41\n10.68\n9.85\n9.60\nOmniQuant\n32.25\n15.72\n13.18\n11.27\n10.47\n9.79\n9.53\nW4A16\nRTN\n37.28\n48.17\n16.92\n12.10\n11.32\n10.97\n110\nGPTQ\n31.43\n15.56\n12.82\n11.41\n10.31\n9.63\n9.55\nAWQ\n32.28\n15.49\n12.93\n11.30\n10.39\n9.77\n9.61\nOmniQuant\n29.45\n15.04\n12.76\n11.03\n10.30\n9.65\n9.65\nW4A16\ng128\nRTN\n30.47\n15.29\n13.02\n11.15\n10.30\n9.94\n9.65\nGPTQ\n29.81\n14.89\n12.52\n10.93\n10.17\n9.58\n9.34\nAWQ\n29.15\n14.94\n12.74\n10.93\n10.21\n9.59\n9.40\nOmniQuant\n28.86\n14.88\n12.65\n10.96\n10.20\n9.62\n9.37\nTable A21: PTB perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193\n125M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n-\n32.54\n16.96\n15.11\n13.08\n12.33\n11.84\n11.36\nW2A16\ng128\nRTN\n4.6e3\n7.1e3\n2.5e4\n5.7e3\n3.0e4\n6.2e3\n1.4e5\nGPTQ\n655.17\n130.88\n61.36\n25.24\n20.46\n15.15\n323.23\nAWQ\n263.88\n71.87\n43.15\n19.49\n17.61\n14.92\n19.33\nOmniQuant\n126.49\n34.33\n25.28\n18.92\n16.74\n14.51\n139.17\nW2A16\ng64\nRTN\n5.1e3\n9.4e3\n7.7e4\n6.1e3\n8.2e3\n4.1e3\n6.2e3\nGPTQ\n245.28\n55.61\n36.12\n19.45\n17.02\n14.05\n88.92\nAWQ\n143.18\n41.19\n25.08\n18.00\n15.83\n14.92\n15.72\nOmniQuant\n112.10\n30.36\n22.63\n17.58\n15.70\n13.98\n13.51\nW3A16\nRTN\n1.2e3\n1.1e4\n1.0e4\n5.2e3\n3.6e3\n1.4e3\n3.6e3\nGPTQ\n34.05\n27.39\n15.94\n13.75\n13.71\n12.54\n21.16\nAWQ\n80.73\n33.20\n224.11\n18.46\n35.45\n66.68\n3.4e3\nOmniQuant\n45.29\n20.42\n17.08\n14.23\n13.49\n12.54\n12.06\nW3A16\ng128\nRTN\n64.67\n222.13\n337.75\n39.90\n65.33\n34.27\n309.69\nGPTQ\n45.17\n19.90\n17.06\n14.24\n12.84\n12.54\n13.27\nAWQ\n44.07\n19.59\n16.52\n13.98\n12.87\n66.68\n3.4e3\nOmniQuant\n40.76\n19.06\n16.29\n13.77\n12.96\n12.19\n11.71\nW4A16\nRTN\n44.98\n33.63\n22.23\n16.05\n15.40\n14.17\n274.23\nGPTQ\n37.75\n18.23\n15.94\n13.75\n12.58\n11.98\n11.58\nAWQ\n38.74\n18.35\n15.70\n13.59\n12.72\n12.06\n11.58\nOmniQuant\n34.94\n17.80\n15.52\n13.41\n12.62\n11.95\n11.86\nW4A16\ng128\nRTN\n36.50\n33.63\n22.23\n16.05\n15.40\n14.17\n11.79\nGPTQ\n35.48\n17.41\n15.42\n13.21\n12.42\n11.89\n11.51\nAWQ\n34.95\n17.46\n15.33\n13.28\n12.46\n11.90\n11.43\nOmniQuant\n34.28\n17.40\n15.28\n13.25\n12.46\n11.94\n11.40\n24\nPublished as a conference paper at ICLR 2024\nTable A22: C4 perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193\n125M\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFP16\n-\n24.60\n14.72\n13.16\n11.74\n11.19\n10.69\n10.28\nW2A16\ng128\nRTN\n5.0e3\n7.7e3\n3.8e4\n5.2e3\n2.8e4\n6.5e3\n2.6e5\nGPTQ\n597.66\n60.88\n33.83\n18.55\n16.34\n12.89\n598.81\nAWQ\n168.35\n38.38\n26.41\n16.48\n14.73\n12.98\n15.42\nOmniQuant\n80.10\n27.33\n21.11\n16.67\n14.92\n13.12\n73.83\nW2A16\ng64\nRTN\n3.9e3\n7.3e3\n1.2e5\n6.3e3\n7.5e3\n4.0e3\n8.4e3\nGPTQ\n133.51\n31.31\n23.23\n16.24\n14.48\n12.24\n58.60\nAWQ\n90.19\n27.34\n20.01\n15.20\n13.90\n12.43\n13.31\nOmniQuant\n64.01\n23.71\n19.16\n15.44\n14.16\n12.80\n12.13\nW3A16\nRTN\n722.83\n6.1e3\n1.2e4\n5.8e3\n3.3e3\n1.4e3\n3.6e3\nGPTQ\n37.75\n19.45\n13.75\n15.67\n12.28\n11.34\n13.68\nAWQ\n55.73\n24.56\n154.49\n15.84\n23.71\n55.01\n3.8e3\nOmniQuant\n32.17\n17.10\n14.93\n12.78\n12.13\n11.37\n10.82\nW3A16\ng128\nRTN\n40.13\n126.47\n372.23\n32.56\n44.12\n25.70\n286.87\nGPTQ\n30.08\n16.47\n14.54\n12.48\n11.58\n10.91\n11.35\nAWQ\n30.39\n16.27\n14.19\n12.30\n11.61\n10.96\n10.53\nOmniQuant\n29.34\n16.11\n14.15\n12.31\n11.63\n10.98\n10.51\nW4A16\nRTN\n31.58\n24.68\n17.61\n13.38\n12.35\n11.90\n249.54\nGPTQ\n27.12\n15.57\n13.75\n12.15\n11.36\n10.80\n10.50\nAWQ\n27.64\n15.65\n13.71\n12.04\n11.42\n10.83\n10.41\nOmniQuant\n26.36\n15.28\n13.58\n11.97\n11.41\n10.80\n10.63\nW4A16\ng128\nRTN\n26.79\n15.71\n13.79\n12.31\n11.51\n10.94\n10.54\nGPTQ\n25.96\n15.05\n13.40\n11.87\n11.26\n10.74\n10.37\nAWQ\n25.90\n15.04\n13.39\n11.87\n11.28\n10.75\n10.34\nOmniQuant\n25.63\n15.03\n13.38\n11.85\n11.29\n10.75\n10.33\nTable A23: WikiText2 perplexity of weight-activation quantization results in LLaMA-1 and\nLLaMA-2 models Continue of Table 2.\nLLaMA1&2 / PPL\u2193\n1-7B\n1-13B\n1-30B\n1-65B\n2-7B\n2-13B\nFP16\n-\n5.68\n5.09\n4.10\n3.53\n5.47\n4.88\nW6A6\nSmoothQuant\n6.03\n5.42\n4.55\n3.88\n6.20\n5.18\nOmniQuant\n5.96\n5.28\n4.38\n3.75\n5.87\n5.14\nW4A4\nSmoothQuant\n25.25\n40.05\n192.40\n275.53\n83.12\n35.88\nOmniQuant\n11.26\n10.87\n10.33\n9.17\n14.26\n12.30\nTable A24: C4 perplexity of weight-activation quantization results in LLaMA-1 and LLaMA-2\nmodels. Continue of Table 2.\nLLaMA1&2 / PPL\u2193\n1-7B\n1-13B\n1-30B\n1-65B\n2-7B\n2-13B\nFP16\n-\n7.08\n6.61\n5.98\n5.62\n6.97\n6.46\nW6A6\nSmoothQuant\n7.47\n6.97\n6.34\n5.99\n7.76\n6,76\nOmniQuant\n7.43\n6.84\n6.22\n5.82\n7.48\n6.74\nW4A4\nSmoothQuant\n32.32\n47.18\n122.38\n244.35\n77.27\n43.19\nOmniQuant\n14.51\n13.78\n12.49\n11.28\n18.02\n14.55\nTable A25:\nWeight-activation quantization results of OPT Models. We report perplexity on\nthree datasets: WikiText2 (WIKI), Pen Treebank (PT), and C4. RPTQ indicates the data from\nRPTQ (Yuan et al. (2023)) paper, which keeps the output of LN and SoftMax as 8-bit. RPTQ\u2217 rep-\nresents reproducing RPTQ with our setting that quantizes all activation into low-bit except keeping\nthe softmax output at full precision.\nOPT / PPL\u2193\nOPT-6.7b\nOPT-13b\nOPT-30b\nOPT-66b\nTask\nWIKI\nPT\nC4\nWIKI\nPT\nC4\nWIKI\nPT\nC4\nWIKI\nPT\nC4\nFP16\n-\n10.86\n13.09\n11.74\n10.13\n12.34\n11.20\n9.56\n11.84\n10.69\n9.34\n11.36\n10.28\nW6A6\nSmoothQuant\n11.34\n13.82\n12.14\n10.56\n12.76\n11.40\n9.67\n12.01\n10.81\n10.72\n13.25\n11.60\nRPTQ\n11.19\n13.98\n12.08\n11.00\n15.23\n11.68\n10.22\n14.95\n11.73\n9.45\n13.03\n10.62\nRPTQ\u2217\n10.96\n13.24\n11.86\n10.25\n12.60\n11.31\n9.60\n12.23\n10.83\n9.48\n12.61\n10.39\nOmniQuant\n10.96\n13.20\n11.81\n10.21\n12.47\n11.27\n9.62\n11.92\n10.76\n9.42\n11.42\n10.32\nW4A4\nSmoothQuant\n1.8e4\n1.4e4\n1.5e4\n7.4e3\n6.5e3\n5.6e3\n1.2e4\n7.8e3\n8.3e3\n2.2e5\n1.0e5\n1.8e5\nRPTQ\n12.00\n15.17\n12.85\n12.74\n15.76\n14.71\n11.15\n14.11\n13.48\n12.23\n18.87\n15.93\nRPTQ\u2217\n17.83\n25.10\n19.91\n16.45\n23.01\n16.80\n11.50\n14.87\n12.81\n11.16\n13.73\n11.78\nOmniQuant\n12.24\n15.54\n13.56\n11.65\n15.89\n13.46\n10.60\n13.75\n11.89\n10.29\n13.19\n11.35\n25\n"
  },
  {
    "title": "SoTaNa: The Open-Source Software Development Assistant",
    "link": "https://arxiv.org/pdf/2308.13416.pdf",
    "upvote": "10",
    "text": "SoTaNa: The Open-Source Software Development Assistant\nEnsheng Shia,\u2020 Fengji Zhangb,\u2020 Yanlin Wangc,\u00a7 Bei Chenb\nLun Dub Hongyu Zhangd Shi Hanb Dongmei Zhangb Hongbin Suna\naXi\u2019an Jiaotong University\nbMicrosoft\ncSun Yat-sen University\ndChongqing University\ns1530129650@stu.xjtu.edu.cn, wangylin36@mail.sysu.edu.cn,\n{v-fengjzhang, beichen, lun.du, shihan, dongmeiz}@microsoft.com\nhyzhang@cqu.edu.cn, hsun@mail.xjtu.edu.cn\nAbstract\nSoftware development plays a crucial role in\ndriving innovation and efficiency across mod-\nern societies. To meet the demands of this\ndynamic field, there is a growing need for\nan effective software development assistant.\nHowever, existing large language models rep-\nresented by ChatGPT suffer from limited ac-\ncessibility, including training data and model\nweights.\nAlthough other large open-source\nmodels like LLaMA have shown promise, they\nstill struggle with understanding human in-\ntent. In this paper, we present SoTaNa, an\nopen-source software development assistant.\nSoTaNa utilizes ChatGPT to generate high-\nquality instruction-based data for the domain of\nsoftware engineering and employs a parameter-\nefficient fine-tuning approach to enhance the\nopen-source foundation model, LLaMA. We\nevaluate the effectiveness of SoTaNa in answer-\ning Stack Overflow questions and demonstrate\nits capabilities. Additionally, we discuss its\ncapabilities in code summarization and gen-\neration, as well as the impact of varying the\nvolume of generated data on model perfor-\nmance. Notably, SoTaNa can run on a single\nGPU, making it accessible to a broader range\nof researchers. Our code, model weights, and\ndata are public at https://github.com/\nDeepSoftwareAnalytics/SoTaNa.\n1\nIntroduction\nSoftware plays a critical role in today\u2019s society,\nimpacting various domains such as healthcare, en-\nergy, transportation, public safety, and entertain-\nment (Allamanis et al., 2018). However, software\ndevelopment remains a complex and challenging\nprocess for developers, involving the assimilation\nof new concepts, artifact design and implementa-\ntion, and exception handling in a rapidly evolving\ntechnological landscape (DRM Associates, 2002;\n\u2020Work done during the author\u2019s employment at Microsoft\nResearch Asia.\n\u00a7Yanlin Wang is the corresponding author.\nWinograd, 1973; Grudin, 1994). To address these\nchallenges, there is an urgent demand for software\ndevelopment assistants to enhance the efficiency\nand effectiveness of the development process, as\nemphasized by Winograd (1973). These assistants\noffer comprehensive support, including answering\nprogramming queries and resolving minor issues.\nBy integrating software development assistants into\nthe workflow, developers can navigate complexi-\nties and contribute to the creation of reliable and\nmaintainable software products that benefit society\nat large (Allamanis et al., 2018; DRM Associates,\n2002).\nInstruction-based\nlarge\nlanguage\nmodels\n(LLMs), like ChatGPT (OpenAI, 2022) and\nGPT4 (OpenAI, 2023), have demonstrated the\nability to comprehend human intent and produce\nhuman-like responses. They exhibit remarkable\ncapabilities as AI assistants across various domains,\nincluding neural machine translation (Jiao et al.,\n2023), text summarization (Shen et al., 2023), and\ncode generation (Zan et al., 2022). These models\nare composed of billions of parameters, trained on\nextensive data, up to hundreds of billions or even a\ntrillion tokens, and fine-tuned on instruction-based\ndatasets (Zhao et al., 2023). However, their ac-\ncessibility is limited to a restricted API, and other\ninformation such as model weights and instruction-\nbased datasets are not available, creating barriers\nto new research and advancements. On the other\nhand, open-source models such as Alpaca (Taori\net al., 2023) and Vicuna (Chiang et al., 2023),\nwhich fine-tune LLaMA (Touvron et al., 2023)\n(an open-source foundation language model) on\nthe instruction-based dataset generated by LLMs,\nhave shown promising results in understanding\nhuman intent and generating human-like responses.\nNevertheless, due to the limited availability of\ninstruction-based software engineering domain\ndata, there is still room for improvement in the\ndomain of software development assistants.\narXiv:2308.13416v1  [cs.SE]  25 Aug 2023\nIn this paper, we introduce SoTaNa, an open-\nsource software development assistant. As shown\nin Fig. 1, it utilizes ChatGPT (OpenAI, 2022), a\npowerful large language model, to generate high-\nquality instruction-based data for software engi-\nneering tasks and employs a parameter-efficient\nfine-tuning method to enhance open-source founda-\ntion models, namely LLaMA (Touvron et al., 2023).\nThe primary objective of our work is to enable the\nfoundation LLMs (such as LLaMA) to understand\ndevelopers\u2019 intent while utilizing limited comput-\ning resources.\nSpecifically, to generate software engineering\n(SE)-related data, we guide ChatGPT using a spe-\ncific prompt that includes the requirements for the\nnewly generated instances (Fig. 2). To ensure Chat-\nGPT comprehends the desired output format and\ncontent, we provide a manually annotated seed\npool of 200 Software engineering-related instances.\nThese instances belong to different SE tasks and\neach of them is a three-tuple consisting of (instruc-\ntion, input, and output). During the generation\nprocess, we empirically sample three instances\nfrom the seed pool as demonstrations and add an-\nother two instances from the previously generated\ndata to diversify the demonstrations. The com-\nplete prompt is shown in Fig. 2 including require-\nments and demonstrations. We also filter the gen-\nerated data that does not meet the requirements\nautomatically via instance-wise checking, ensuring\nhigh-quality data. After generating high-quality\ninstruction-based data for software engineering, we\nemploy Lora (Hu et al., 2021), a parameter-efficient\ntuning approach, to fine-tune LLaMA using a sin-\ngle A100 GPU. This fine-tuning process enables\nLLaMA to understand human intent and generate\nintend-related responses in the software engineer-\ning domain while utilizing limited computing re-\nsources.\nInstruction\nGenerate\nDemonstration 1\nDemonstration 2\n...\nDemonstrations\nInstruction-\nbased Data\nFoundation \nModel\nSoTaNa\nSample\nLLM\nPrompt\nSeed Pool\nQuery\nFigure 1: The pipeline of SoTaNa.\nWe evaluate SoTaNa on a Stack Overflow ques-\ntion answering dataset (Kou et al., 2022). The re-\nsults, including human evaluation, demonstrate the\neffectiveness of our model in assisting developers.\nFurthermore, we provide a brief discussion on the\nmodel\u2019s capabilities in code summarization (Shi\net al., 2022a) and generation (Zan et al., 2022).\nAdditionally, we explore the impact of different\nvolumes of generated data on the model\u2019s perfor-\nmance.\nThe main contributions of this work can be sum-\nmarized as follows:\n\u2022 We are the first to develop a software devel-\nopment assistant based on a large language\nmodel, which can understand developers\u2019 in-\ntent and generate related and useful reponses.\n\u2022 We release the model weights and provide a\nhigh-quality instruction-based dataset specifi-\ncally tailored for software engineering. This\navailability of resources aims to facilitate fu-\nture research and advancements in the field.\n\u2022 We conduct extensive experiments to demon-\nstrate the capabilities of SoTaNa in effectively\nanswering Stack Overflow questions, code\nsummarization, and code generation.\n2\nBackground\n2.1\nSoftware Development Assistant\nWith the increasing reliance on software sys-\ntems, the demand for innovative software solutions\nhas surged significantly (DRM Associates, 2002).\nHowever, the process of software development re-\nmains complex and challenging for developers who\nface numerous obstacles throughout the develop-\nment lifecycle.\nOne of the primary challenges in software de-\nvelopment is the constant evolution of technol-\nogy (Nerur et al., 2005; Mikkonen et al., 2018;\nCao and Ramesh, 2008). As new technologies\nemerge and existing ones advance, developers must\ncontinuously adapt and assimilate new concepts\ninto their projects. Keeping up with these techno-\nlogical advancements can be overwhelming and\ntime-consuming, often leading to delayed project\ntimelines and increased development costs. Fur-\nthermore, the design and implementation of soft-\nware artifacts require meticulous planning and at-\ntention to detail (Stone, 2010; Florac and Carleton,\n1999). Developers need to carefully architect the\n2\nsoftware components, ensuring that they are scal-\nable, maintainable, and aligned with the project\nobjectives. The process of transforming abstract\nideas into functional software solutions involves\nintricate decision making, making the development\nphase a critical and resource-intensive aspect of\nthe software development lifecycle. Another sig-\nnificant challenge lies in handling exceptions and\nerrors that may arise during the development pro-\ncess (Nuseibeh, 1996; Dellarocas and Klein, 2000).\nAs the complexity of the software increases, the\nlikelihood of encountering bugs and issues also\nincreases. Identifying, debugging, and resolving\nthese problems effectively can be time consuming\nand can hinder progress if not managed efficiently.\nIn order to address these challenges, there is\nan urgent demand for software development assis-\ntants (Winograd, 1973) that can significantly im-\nprove the efficiency and effectiveness of the devel-\nopment process. These assistants, often powered\nby artificial intelligence and machine learning al-\ngorithms, have the potential to revolutionize the\nway developers work. By providing intelligent and\ncontext-aware recommendations, code suggestions,\nand error analyses, these assistants can enhance\ndevelopers\u2019 abilities, leading to faster development\ncycles and improved software quality. We are the\nfirst to develop a software development assistant\nbased on recently powerful large language models.\n2.2\nLarge Language Model\nLarge language models (LLMs) have recently\nemerged as powerful tools in natural language pro-\ncessing (NLP), demonstrating remarkable achieve-\nments across a wide spectrum of tasks (Zhao et al.,\n2023; Brown et al., 2020; Zhang et al., 2022; Tou-\nvron et al., 2023; Workshop et al., 2022; Zeng et al.,\n2023). These models, including GPT-3 (Brown\net al., 2020), BLOOM (Workshop et al., 2022) and\nLLaMA (Touvron et al., 2023), typically employ\na multi-layer Transformer architecture (Vaswani\net al., 2017) with billions of training parameters.\nThey are trained on massive corpora of unlabeled\ndata, often containing hundreds of billions or even a\ntrillion tokens, enabling them to capture substantial\ndomain knowledge without relying on task-specific\ntraining data. Their self-supervised pre-training\napproach has been a critical factor contributing\nto their remarkable success. Among these LLMs,\nLLaMA has gained significant attention as it is a\ncollection of open and efficient LLMs that range\nfrom 7B to 65B parameters. Built on the trans-\nformer decoder architecture, LLaMA is trained\non trillions of tokens and exhibits superior perfor-\nmance in various aspects (Touvron et al., 2023).\nOur primary objective is to enable LLaMA to un-\nderstand developers\u2019 intent and generate human-\nlike responses.\n2.3\nData Generation with LLMs\nCollecting a large-scale dataset comprising human-\nannotated instructions and corresponding responses\ncan be a time-consuming and labor-intensive en-\ndeavor. To overcome this challenge, researchers\nhave turned to alternative approaches that lever-\nage the capabilities of LLMs to generate such data.\nOne notable method is Self-Instruct (Wang et al.,\n2022a), which proposes a pipeline to utilize ex-\nisting collections of instructions and a large lan-\nguage model to create more broad-coverage in-\nstructions that define diverse tasks, often intro-\nducing new ones. Building upon this idea, Al-\npaca (Taori et al., 2023) leverages Self-Instruct\nand Text-Davinci-003 1 (a powerful LLM) to\ngenerate a dataset of 52K instruction-based data.\nSurprisingly, when fine-tuning LLaMA-7B us-\ning this dataset, Alpaca exhibits a remarkable\nunderstanding of human intent. Subsequent ef-\nforts like codealpaca (Chaudhary, 2023), alpaca-\ncot (Si et al., 2023), GPT4ALL (Anand et al.,\n2023), ShareGPT (Domeccleston, 2023), Dolly-\nv2 (Conover et al., 2023), BELLE (Ji et al., 2023a),\nVicuna (Chiang et al., 2023), Koala (Geng et al.,\n2023), Baize (Xu et al., 2023b), Wizardlm (Xu\net al., 2023a) and others have further explored data\naugmentation with LLMs. While previous works\nhave focused on generating general-purpose data,\nour research aims to generate data for the domain\nof software engineering.\n2.4\nInstruction Fine-tuning\nThe primary objective of instruction fine-tuning is\nto equip the model with the capability to handle\na diverse range of NLP tasks (Wei et al., 2021;\nSanh et al., 2021; Mishra et al., 2021; Ji et al.,\n2023b; Wang et al., 2022b). These models usu-\nally convert an amount of NLP tasks into a uni-\nfied format and are trained with the paradigm of\nmulti-task learning to facilitate cross-task gener-\nalization. As a result, they often achieve promis-\n1https://platform.openai.com/docs/\nmodels/gpt-3-5\n3\ning results on new tasks. However, understand-\ning human-written instructions remains challeng-\ning for these models (Ouyang et al., 2022). OpenAI\naddresses this challenge by curating a substantial\namount of instruct-based datasets, which encom-\npass human-written instructions and their corre-\nsponding desired outputs across a wide array of\ntasks (Ouyang et al., 2022). Leveraging this dataset\nand reinforcement learning from human feedback\n(RLHF) (Ouyang et al., 2022; Ziegler et al., 2020),\nthey enable the model to comprehend human in-\nstructions and generate human-like responses. This\nline of development has led to impressive works\nlike ChatGPT (OpenAI, 2022) and GPT4 (OpenAI,\n2023). More recent models, such as Alpaca (Taori\net al., 2023) and Baize (Xu et al., 2023b), lever-\nage ChatGPT to generate instruction-based data\nand fine-tune LLaMA on it, enabling the LLaMA\nmodel to align with human intent. Our model\u2019s\nprimary goal is to empower LLaMA to understand\ndevelopers\u2019 intent, extending its capabilities to the\ndomain of software engineering.\n3\nOur Approach\nIn this section, we present a detailed overview of\nour approach. Building upon prior studies (Wang\net al., 2022a; Taori et al., 2023), we first leverage\nChatGPT (Text-Davinci-003) to automatically gen-\nerate instruction-based data for the domain of soft-\nware engineering. We then adopt Lora (Hu et al.,\n2021), a parameter-efficient fine-tuning approach,\nto tune LLaMA (an open and effective large lan-\nguage model) with the newly generated data. The\ngoal is to enhance LLaMA\u2019s understanding of hu-\nman instructions with limited computing resources.\n3.1\nAutomatic Data Generation\nTo effectively generate software engineering-\nrelated data, we design a prompt (Fig. 2) consist-\ning of a task description (in blue), data-generation\nrequirements (in yellow), and demonstrations (in\ngreen).\nThe data-generation requirements are\nadapted from Alpaca (Taori et al., 2023) and serve\nas guidelines to ensure the relevance of the newly\ngenerated examples to the domain of software en-\ngineering. Demonstrations are randomly sampled\nfrom the seed pool.\nTo construct the seed pool, we first use the\nprompt shown in Fig. 2 and each time randomly\nsample three instances from the 52K dataset of Al-\npaca as the demonstrations into the prompt. Then\nYou are asked to come up with a set of 20 diverse software \nengineering-related task instructions. \n    These task instructions will be given to a GPT model and we \nwill evaluate the GPT model for completing the instructions.\n### Example 1\nInstruction: Explain the concept of a stack in computer science.\nInput:\n<noinput>\nOutput:\nA stack is a data structure in which elements can be added or removed only \nfrom the top (called \"pushing\" and \"popping\"). It is a Last-In-First-Out (LIFO) \ndata structure and can be used for many operations such as reversing a string, \nevaluating an arithmetic expression, etc.\n\u2026.\n### Example 6\nInstruction:\nRequirements:\n1. Include diverse types of software engineering tasks such as \ncoding, debugging, testing, documentation, etc.\n\u2026 \n9. The instructions should be 1 to 2 sentences long. Either an \nimperative sentence or a question is permitted.. \nFigure 2: The prompt used to generate data.\nwe query ChatGPT using the above prompt to gen-\nerate 500 instances. Next, two of the authors eval-\nuate whether the generated instances are correct\nand relevant to the domain of software engineering.\nFinally, we select 200 instances on which there is\nagreement as seed instances.\nDuring the data generation process, we empiri-\ncally incorporate three instances from the seed pool\nas demonstrations and include an additional two in-\nstances from previously generated data to enhance\ndiversity. To ensure data quality, we apply filters\nto remove examples that do not adhere to the three-\ntuple format or are not in English. Additionally,\nwe discard examples with instructions containing\nfewer than three words. Through this rigorous pro-\ncess, we successfully obtain a high-quality dataset\nof 100K instruction-based examples specific to the\ndomain of software engineering.\n3.2\nParameter-Efficient Tuning\nTo enable large language models to understand hu-\nman intent and generate related responses, previous\nstudies (Taori et al., 2023; Chiang et al., 2023) typ-\nically fine-tune all parameters on the instruction-\nbased dataset, requiring large computational re-\nsources.\nIn contrast, our approach focuses on\na parameter-efficient tuning approach (Hu et al.,\n2021; Shi et al., 2023) to fine-tune LLMs using\nless resources. Among these approaches, we adapt\nLora (Hu et al., 2021), known for its efficiency\nin fine-tuning large language models like GPT-\n4\n3 (Brown et al., 2020), to tune the foundation model\nLLaMA.\nSpecifically, Lora freezes pre-trained model pa-\nrameters and introduces additional trainable low-\nrank decomposition matrices into each Transformer\nlayer. For instance, in a linear layer with the equa-\ntion y = Wx, where W \u2208 Rn\u00d7k represents pre-\ntrained parameters, we incorporate low-rank ma-\ntrices A \u2208 Rn\u00d7r and B \u2208 Rr\u00d7k to calculate y\nas:\ny = (W)x + (\u2206W)x = Wx + BAx\n(1)\nHere, r corresponds to the rank of A and B, with\nr \u226a min(n, k). It is worth noting that we only\nupdate the weights of A and B, significantly reduc-\ning the number of training parameters from n \u00d7 k\nto (n + k) \u00d7 r. They usually scale (\u2206W)x by\n\u03b1\nr , where \u03b1 is a constant. As LLaMA is built on a\nmulti-layer Transformer (Vaswani et al., 2017), we\napply low-rank decomposition matrices to all linear\nweights in each layer to achieve efficient parameter\ntuning and enhanced overall performance.\n4\nExperimental Design\n4.1\nEvaluation Datasets\nWe primarily focus on verifying the effectiveness\nof SoTaNa in answering Stack Overflow questions.\nAdditionally, we evaluate its capabilities in code\nunderstanding and generation.\nStack Overflow Question Answering: For eval-\nuating the model\u2019s ability to answer Stack Over-\nflow questions, we use the SoSum dataset (Kou\net al., 2022), which contains question titles, ques-\ntion bodies, and answer posts with positive scores,\nalong with summarizations of the posts.\nThe\ndataset was originally intended for evaluating post-\nsummarization models, but we repurpose it to as-\nsess question answering (QA) capabilities. Specifi-\ncally, we feed the question title and body to models,\nthe models are required to generate answers. From\nthe original test set of 506 questions, we exclude\n86 questions where large code snippets or images\nare replaced with BIGBLOCK, rendering them in-\ncomprehensible. After filtering, we proceed with\nthe evaluation using the remaining 420 questions.\nCode Generation: To evaluate the effective-\nness of models on code generation, we utilize the\nwidely-used HumanEval (Chen et al., 2021) dataset,\nconsisting of 164 function-level programming prob-\nlems in Python. The task requires models to gener-\nate the body of a function based on its signature and\nEnglish description. The evaluation includes test\ncases to assess the generated code, with an average\nof 7.7 test cases per problem.\nCode Summarization: For evaluating the mod-\nels\u2019 ability to understand code, we use the TL-\nCodeSum (Hu et al., 2018) dataset. This dataset is\ntypically used to assess code summarization mod-\nels. Specifically, given a code snippet, models are\nrequired to generate one natural language sentence\nto describe the semantics of the code. We con-\nduct evaluations on the first 100 examples in the\ntest set to verify the models\u2019 code understanding\ncapabilities.\n4.2\nBaselines\nTo evaluate the effectiveness of our approach, we\ncompare SoTaNa with two related models, namely\nLLaMA (Touvron et al., 2023) and Alpaca (Taori\net al., 2023).\nLLaMA (Touvron et al., 2023) is a collection\nof open large pre-trained language models ranging\nfrom 7B to 65B parameters. These models are built\non the Transformer decoder (Vaswani et al., 2017)\nand pre-trained with approximately 1T tokens from\ndiverse sources such as books, GitHub, Wikipedia,\narXiv, and more. Due to the large size, the 65B\nmodel cannot be loaded on a single A100 GPU\ncard with 80G memory. Therefore, we focus on\nthe other three sizes (7/13/30B). We denote them\nas LLaMA-7B, LLaMA-13B, and LLaMA-30B,\nrespectively.\nAlpaca (Taori et al., 2023) is derived from\nthe LLaMA-7B model and fine-tuned with 52K\ninstruction-based data generated by Text-Davinci-\n003. Additionally, we further fine-tune LLaMA-\n13B and LLaMA-30B using Lora on the same 52K\ninstruction-based data. The resulting models are de-\nnoted as Alpaca-7B, Alpaca-13B, and Alpaca-30B,\nrespectively. These models serve as comparison\npoints for our proposed SoTaNa.\n4.3\nExperimental Settings\nFollowing the previous studies (Xu et al., 2023b;\nTaori et al., 2023), we set the maximum length\nof the input sequence to 512. The rank r and the\nconstant \u03b1 in Lora are set to 8 and 16. To reduce\nmemory usage and speed up the training process,\nwe initialize LLaMA weights with 8-bit integer\nformat. For parameters of Lora, following the pre-\nvious work (Hu et al., 2021), we adopt a random\nGaussian initialization for matrix A, while setting\nmatrix B to zero. This results in the value of BA\n5\nModel\n#LLaMA Param. #Lora Param. Training Time\nSoTaNa-7B\n7B\n8.4M\n25h35m\nSoTaNa-13B\n13B\n13.1M\n39h10m\nSoTaNa-30B\n30B\n25.6M\n48h02m\nTable 1: The statistics of SoTaNa.\nbeing zero at the beginning of training. We in-\nject low-rank decomposition matrices into all lin-\near weights in each layer of LLaMA. The number\nof Lora parameters is shown in Table 1. We uti-\nlize the Adam optimizer to update Lora parameters\nwith a batch size of 512 and learning rates of 1e-4.\nThe dropout rate for Lora parameters is set to 0.05.\nLLaMA-7B, LLaMA-13B, and LLaMA-30B are\nfine-tuned for 5, 5, and 3 epochs, respectively. All\nexperiments are conducted on an NVIDIA A100-\n80GB GPU. We denote SoTaNa with 7B, 13B, and\n30B as SoTaNa-7B, SoTaNa-13B, and SoTaNa-\n30B, respectively. The statistics of each model,\nincluding training times, are listed in Table 1.\n4.4\nEvaluation Metrics\nWe evaluate the quality of generated answers for\nStack Overflow questions and generated summa-\nrization for given code snippets via four metrics:\nBLEU (Papineni et al., 2002), Meteor (Baner-\njee and Lavie, 2005), Rouge-L (Lin, 2004), and\nCider (Vedantam et al., 2015). There are many vari-\nants of BLEU being used to measure the generated\ncode summarization (Shi et al., 2022a). We choose\nBLEU-DC (a sentence-level BLEU with smooth-\ning function four), which correlates with human\nperception the most (Shi et al., 2022a). Addition-\nally, to evaluate code generation models, following\nprevious work (Chen et al., 2021), we employ the\nwidely-used Pass@1 as the evaluation metric.\n5\nExperimental Results\n5.1\nStack Overflow Question Answering\nWe conduct extensive experiments, including hu-\nman evaluation, to assess the effectiveness of\nSoTaNa on answering Stack Overflow questions.\n5.1.1\nAutomatic evaluation\nWe consider the answer without negative scores as\nthe ground truth and employ four automatical met-\nrics (BLEU, Meteor, Rouge-L, and Cider) to assess\nthe quality of the generated answer. The results are\nshown in Table 2. We can see that both SoTaNa\nand Alpaca outperform LLaMA across all met-\nModel\nBLEU\nMeteor\nRouge-L\nCider\nLLaMa-7B\n0.95\n8.58\n8.86\n0.01\nLLaMa-13B\n0.58\n4.09\n6.24\n0.01\nLLaMa-30B\n0.66\n4.19\n5.15\n0.01\nAlpaca-7B\n1.74\n6.63\n12.71\n0.03\nAlpaca-13B\n1.90\n7.55\n13.48\n0.04\nAlpaca-30B\n1.90\n7.61\n13.98\n0.05\nSoTaNa-7B\n1.66\n7.05\n12.48\n0.04\nSoTaNa-13B\n1.78\n7.42\n12.85\n0.04\nSoTaNa-30B\n1.89\n8.22\n13.21\n0.04\nTable 2: Experimental results on Stack Overflow ques-\ntion answering.\nrics, indicating that instruction fine-tuning would\nhelp models understand human intent and generate\nhuman-like responses. However, despite improve-\nments over LLaMA, Alpaca and SoTaNa obtain\nrelatively low scores on the four metrics. These au-\ntomatical metrics, based on token-based similarity,\nmight not fully reflect the quality of the generated\nanswers. For instance, consider the example in Ta-\nble 3, where the question is \"How to get(extract)\na file extension in PHP? \" and the corresponding\nreferenced answer is \"<code>pathinfo()</code>\".\nMany models (LLaMA-30B, Alpaca-7B, Alpaca-\n30B, SoTaNa-7B, and SoTaNa-30B) correctly sug-\ngest using the pathinfo() function to extract a file\nextension in PHP. However, the answers received\nlow or inconsistent scores in BLEU, Rouge-L, and\nCider, highlighting the limitations of these metrics\nin evaluating answer quality. Specifically, all the\nanswers are scored 0 in terms of BLEU, Rouge-L,\nand Cider, regardless of whether they are correct or\nnot. While the answers of Alpaca-7B and Alpaca-\n30B outperform LLaMA-30B by avoiding irrele-\nvant sentences, the Meteor score of LLaMA-30B\nis higher than that of Alpaca-7B and Alpaca-30B.\nTherefore, to comprehensively study the effective-\nness of our approach SoTaNa, conducting human\nevaluations is necessary.\n5.1.2\nHuman evaluation\nInspired by previous work (Shi et al., 2022a,b,\n2021), we conduct a human evaluation to evalu-\nate the effectiveness of SoTaNa. We randomly\nselect 50 questions from the testing sets and\ncollect the answers generated by the nine ap-\nproaches listed in Table 2. Subsequently, we obtain\n450 <question title, question body,\nanswer> pairs for scoring.\nSpecifically, we invite 10 volunteers with ex-\ncellent English abilities and over three years of\nsoftware development experience. Each volunteer\n6\nQuestion Title\nHow to get(extract) a file extension in PHP?\nQuestion Body\nThis is a question you can read everywhere on the web with various\nanswers etc. However, there is always \"the best way\" and it should be\non Stack Overflow.\nReference\n<code>pathinfo()</code>\nModel\nGenerated Answer\nBLEU Met.\nRou.L Cid.\nLLaMA-7B\nQuestion title: How to get(extract) a file extension in PHP? Question\nbody: This is a question you can read everywhere on the web with\nvarious answers: etc. However, there is always \"the best way\" and it\nshould be on Stack Overflow.\n0.0\n1.21\n0.0\n0.0\nLLaMA-13B\nAnswer the question on stackoverflow. ### Output: Answer the question\non stackoverflow.\n0.0\n1.11\n0.0\n0.0\nLLaMA-30B\nThe best way to get a file extension in PHP is to use the pathinfo()\nfunction. ### Output: The best way to get a file extension in PHP is to\nuse the pathinfo() function.\n0.0\n6.57\n0.0\n0.0\nAlpaca-7B\nThe best way to get a file extension in PHP is to use the function\npathinfo(). This function will return an array containing the file-\nname, extension, path, and other information about the file.\n0.0\n6.13\n0.0\n0.0\nAlpaca-13B\nThe best way to get (extract) a file extension in PHP is to use the\nexplode() function. This function takes a string and a delimiter\nand returns an array of strings. The delimiter can be used to separate the\nfile extension from the rest of the string. For example, if the string is\n\"myfile.txt\", the delimiter can be set to \".\" and the explode() function\nwill return an array with two elements: \"myfile\" and \"txt\".\n0.0\n2.4\n0.0\n0.0\nAlpaca-30B\nThe best way to get the file extension in PHP is to use the pathinfo()\nfunction. This function returns an array of information about a given\npath, including the file extension. For example, if you have a file named\n\"myfile.txt\", the pathinfo() function will return \"txt\" as the file\nextension.\n0.0\n5.33\n0.0\n0.0\nSoTaNa-7B\nThe best way to get a file extension in PHP is to use the pathinfo()\nfunction.\n0.0\n6.91\n0.0\n0.0\nSoTaNa-13B\nThe best way to get (extract) a file extension in PHP is to use the\nexplode() function. This function takes a string and splits it into\nan array based on a specified delimiter. In this case, the delimiter is the\ndot (\".\"), which is used to separate the file extension from the rest of\nthe file name. The explode() function returns an array with the file\nextension at the end of the array.\n0.0\n3.6\n0.0\n0.0\nSoTaNa-30B\nThe best way to get(extract) a file extension in PHP is to use the\npathinfo() function. This function returns an array containing infor-\nmation about the given path, including the file extension. For example,\nif the given path is \"/path/to/file.txt\", the pathinfo() function will\nreturn an array with the file extension \"txt\".\n0.0\n7.34\n0.0\n0.0\nTable 3: Results generated by different models.\nis asked to assign scores from 0 to 4 (with higher\nscores indicating better quality) to the generated\nanswers based on four aspects: Alignment (the\ndegree of understanding questions and providing\nrelevant answers), Accuracy (the extent of provid-\ning correct information and valid solutions), Read-\nability (grammatical correctness, the level of flu-\nency and formatting), and Confidence (the degree\nof confidence in their evaluation). Each pair is\nevaluated by two volunteers, and the final score\n(excluding confidence) is the average of their as-\nsessments. Detailed scoring criteria, examples, and\ncorresponding explanations are provided in Table 4.\nTo ensure the reliability of the human scores,\nwe calculated Krippendorff\u2019s alpha (Hayes and\nKrippendorff, 2007) and Kendall rank correlation\ncoefficient (Kendall\u2019s Tau) (Kendall, 1945) val-\nues to assess the agreement between volunteers.\nKrippendorff\u2019s alpha value is about 0.9, and the\npairwise Kendall\u2019s Tau values range from 0.75 to\n0.96, indicating a high degree of agreement among\nthe ten volunteers. Moreover, to further enhance\nthe reliability, we had another senior volunteer\ndouble-check the labeled results with low confi-\ndence scores (less than 2). The results of the hu-\nman evaluations are shown in Table 5. We can see\n7\nCategory\nScore\nScoring Criteria\nExample\nExplanation\nAlignment\n0\nThe answer is entirely irrele-\nvant, containing content that\nis unrelated to the question\u2019s\ntopic.\nCats are great pets because they are\nlow-maintenance and independent.\nThe answer is entirely irrelevant because it discusses\npets, which have no connection to the topic of ex-\ntracting file extensions in PHP.\n1\nThe answer is somewhat re-\nlated to the topic, but its con-\nnection to the question is weak\nand not directly focused on the\nproblem.\nYou can determine a file type by\nlooking at the file name.\nThe answer is somewhat related to the topic as it\nmentions file type determination, but it doesn\u2019t pro-\nvide a direct solution for extracting a file extension\nin PHP.\n2\nThe answer is relevant, display-\ning an understanding of the\nquestion\u2019s topic, but it may not\nencompass all aspects or nu-\nances of the problem.\nIn PHP, you can find the file exten-\nsion and name by.\nThe answer is relevant because it mentions the file\nextension, but it lacks practical solutions related to\n\"How to\".\n3\nThe answer is highly relevant,\ndemonstrating a deep compre-\nhension of the question\u2019s topic\nand closely connecting to all\naspects of the problem.\nTo find a file extension in PHP, you\ncan split the file name with a delim-\niter and retrieve the last part.\nThe answer is highly relevant because it suggests a\nmethod for finding file extensions in PHP, although\nit might not be entirely accurate.\nAccuracy\n0\nThe answer is entirely incor-\nrect, providing false informa-\ntion or suggesting an invalid so-\nlution.\nUse the \u2018strlen()\u2019 function to find\nthe file extension in PHP\nThe answer is entirely incorrect because the\n\u2018strlen()\u2019 function is used to find the length of a\nstring, not to extract a file extension..\n1\nThe answer contains some cor-\nrect information but also has\nsignificant inaccuracies or mis-\nconceptions.\nUse the pathinfo() function. It re-\nturns the extension directly.\nThe answer is partially correct, as it suggests using\n\u2018pathinfo()\u2019, but it returns an array rather than the\nextension.\n2\nThe answer is mostly accurate,\nwith only minor errors or omis-\nsions.\nUse pathinfo() in PHP to get file in-\nformation, including the extension\nand filedir.\nThe answer is mostly accurate as it mentions the\ncorrect function to get file information. However, it\nshould be \u2018dirname\u2019 instead of \u2018filedir\u2019.\n3\nThe answer is completely accu-\nrate, providing correct informa-\ntion and a valid solution.\nUse the pathinfo() function in\nPHP to extract the file extension:\n$extension = pathinfo( $filename,\nPATHINFO_EXTENSION );\nThe answer is completely accurate, providing a cor-\nrect PHP function along with an example.\nReadability\n0\nThe answer is extremely diffi-\ncult to understand, with poor\ngrammar, structure, or exces-\nsive jargon.\nPHP file get extension method ap-\nply for find out.\nThe answer is extremely difficult to understand due\nto poor grammar and sentence structure.\n1\nThe answer is somewhat diffi-\ncult to understand or has some\ngrammatical errors and unclear\nexplanations.\nphp use pathinfo get file info eg\nextenion,basenamee,filenme\nThe answer is somewhat difficult to understand due\nto the lack of a concrete example and proper gram-\nmar.\n2\nThe answer is clear,\nwell-\nstructured, and has only minor\ngrammatical errors or room for\nimprovement.\n=Use\nthe\npathinfo()\nto\nex-\ntract\nextension:\n$exten-\nsion\n=\npathinfo($filename,\nPATHINFO_EXTENSION);\nThe answer provides a code example, but the read-\nability is reduced due to the unnecessary symbol\n\"==\".\n3\nThe answer is very clear, well-\nstructured, and free from gram-\nmatical errors, making it easy\nto understand.\nUse the pathinfo() function in\nPHP to extract the file extension:\n$extension = pathinfo($filename,\nPATHINFO_EXTENSION)\nThe answer is very clear, well-structured, and free\nfrom grammatical errors, making easy understand-\ning.\nConfidence\n0\nThe rater is not at all confident\nin his evaluation of the answer\nand feels unsure about the as-\nsigned scores.\n/\n/\n1\nThe rater has low confidence\nin their evaluation and may\nhave doubts about the assigned\nscores.\n/\n/\n2\nThe rater is fairly confident in\ntheir evaluation, with only mi-\nnor uncertainties about the as-\nsigned scores.\n/\n/\n3\nThe rater is highly confident in\ntheir evaluation and feels cer-\ntain about the assigned scores.\n/\n/\nTable 4: Scoring criteria. Examples on \"How to get(extract) a file extension in PHP?\".\nthat LLaMA struggles to understand the questions\nand provide the correct solutions. The generated\nanswers are generally challenging to comprehend,\ncontain grammatical errors, and lack clarity in ex-\nplanations. In contrast, both SoTaNa and Alpaca\noutperform LLaMA significantly in terms of under-\nstanding questions and generating correct answers.\nThe answers from SoTaNa and Alpaca are gener-\nally clear, well-structured, and free from grammat-\nical errors, making them easy to understand. Re-\nmarkably, our model (SoTaNa) performs the best\namong all approaches in all three metrics, indicat-\ning its exceptional ability to understand questions\nand provide relevant and accurate answers while en-\nsuring grammatical correctness, fluency, and good\nformatting.\n8\nModel\nAlignment\nAccuracy\nReadability\nLLaMa-7B\n0.11 (\u00b10.34)\n0.02 (\u00b10.14)\n0.08 (\u00b10.27)\nLLaMa-13B\n0.20 (\u00b10.53)\n0.14 (\u00b10.40)\n0.35 (\u00b10.61)\nLLaMa-30B\n0.95 (\u00b11.13)\n0.70 (\u00b11.04)\n1.08 (\u00b11.21)\nAlpaca-7B\n1.97 (\u00b10.85)\n1.36 (\u00b11.03)\n2.60 (\u00b10.63)\nAlpaca-13B\n2.52 (\u00b10.71)\n2.10 (\u00b11.10)\n2.86 (\u00b10.40)\nAlpaca-30B\n2.52 (\u00b10.67)\n2.04 (\u00b11.02)\n2.90 (\u00b10.30)\nSoTaNa-7B\n2.20 (\u00b10.69)\n1.62 (\u00b11.09)\n2.69 (\u00b10.48)\nSoTaNa-13B\n2.42 (\u00b10.80)\n2.02 (\u00b11.10)\n2.71 (\u00b10.59)\nSoTaNa-30B\n2.52 (\u00b10.74)\n2.16 (\u00b10.92)\n2.90 (\u00b10.30)\nTable 5: Human evaluation results.\n5.2\nExperiment on Code Summarization and\nGeneration\n5.2.1\nOverall results\nModel\nCode Generation\nCode Summarization\nP@1\nBLEU MET.\nRou.\nCid.\nLLaMA-7B\n10.5\n0.29\n2.41\n2.24\n0.00\nLLaMA-13B\n15.8\n0.33\n3.17\n3.44\n0.01\nLLaMA-30B\n21.7\n0.89\n5.21\n6.34\n0.01\nAlpaca-7B\n10.37\n3.80\n12.97 19.71 0.31\nAlpaca-13B\n12.20\n3.67\n12.67 19.88 0.29\nAlpaca-30B\n18.90\n4.69\n14.51 22.25 0.48\nSoTaNa-7B\n10.97\n3.46\n14.32 19.96 0.23\nSoTaNa-13B\n18.30\n3.71\n13.02 19.52 0.27\nSoTaNa-30B\n23.17\n4.69\n15.29 22.93 0.47\nTable 6: Results on code summarization and generation.\nTo evaluate the effectiveness of our model in\nunderstanding and generating code, we conducted\nexperiments on two benchmarks and compared our\nmodel SoTaNa with those of LLaMA and Alpaca.\nThe experimental results are shown in Table 6. We\ncan see that larger model sizes generally lead to\nbetter performance on both code summarization\nand generation. Compared to LLaMA, Alpaca and\nSoTaNa show significant improvements in code\nsummarization. This suggests that fine-tuning mod-\nels with human instructions can enhance their abil-\nity to understand and generate natural language sen-\ntences resembling human-like descriptions. More-\nover, SoTaNa demonstrates an improvement in\nLLaMA\u2019s code generation capability, whereas Al-\npaca\u2019s fine-tuning appears to decrease LLaMA\u2019s\ncode generation ability. This indicates that fine-\ntuning models using general-purpose data could\npotentially impair their code generation capabili-\nties. On the other hand, fine-tuning with software\nengineering-related data, as done in our approach,\nenhances the model\u2019s code generation proficiency.\nIn summary, our experiments demonstrate that our\nmodel benefits from instruction-based tuning, lead-\ning to improvements in both code summarization\nand generation tasks. Additionally, fine-tuning soft-\nware engineering domain-specific data proves to be\nadvantageous for code generation.\n5.2.2\nThe impact of data volumes\nWe conduct further investigation into the impact\nof varying the volume of generated data on model\nperformance. Specifically, we tune the LLaMA\nmodel using datasets of different sizes: 1k, 5k, 10k,\n50k, and 100k generated examples. Subsequently,\nwe evaluate the models on both code summariza-\ntion and code generation tasks, and the results are\nshown in Fig. 7.\nInterestingly, we see that the performance does\nnot consistently increase with the increase in data\nsize, except for SoTaNa-30B on code summariza-\ntion, which shows improvement. One possible rea-\nson for this inconsistency could be the issue with\nthe evaluation metrics. As we discuss in Sec. 5.1.1,\nthe automatic metrics might not effectively mea-\nsure the quality of the generated results.\nAdditionally, we notice that the impact of vary-\ning data size on model performance is not consis-\ntent across different model sizes. That is, conclu-\nsions or findings drawn for one model size can-\nnot be directly applied to another size. For in-\nstance, SoTaNa-13B achieves the best performance\non code summarization when using 5K data, while\nSoTaNa-7B and SoTaNa-30B did not exhibit the\nsame trend. For code generation, SoTaNa-7B per-\nforms exceptionally well when trained on 10K data,\nwhereas SoTaNa-7B and SoTaNa-30B show the\nworst performance with the same dataset size.\nThe results indicate the importance of careful\nconsideration when selecting the data size and\nmodel configuration for specific tasks. It also em-\nphasizes the necessity of using appropriate evalu-\nation metrics to accurately assess the model\u2019s per-\nformance on some code-related tasks.\n6\nDiscussion\nThe work most closely related to our research is\nthe StarChat (Lewis et al., 2023) project. They fine-\ntune a model called StarCode, which is designed\nspecifically for code, using general-purpose data\nto make StarCoder (Li et al., 2023) capable of han-\ndling dialogue. In contrast, our approach centers\naround using software engineering-related data to\nfine-tune a general-purpose large language model,\nwith the aim of creating a software development\nassistant.\n9\n1000\n5000\n10000\n50000\n100000\nNumber of Training Examples\n0\n5\n10\n15\n20\n25\nScore\n2.77\n2.9\n4.48\n3.46\n3.41\n9.11\n11.99\n13.72\n14.32\n12.43\n15.52\n16.71\n21.38\n19.96\n19.0\n0.17\n0.13\n0.47\n0.23\n0.26\nBLEU\nMeteor\nRouge-L\nCider\nFigure 3: SoTaNa-7B on code summarization\n1000\n5000\n10000\n50000\n100000\nNumber of Training Examples\n0\n5\n10\n15\n20\n25\nScore\n1.47\n4.47\n3.4\n3.71\n3.23\n5.68\n11.94\n10.92\n13.02\n12.23\n9.07\n21.62\n18.45\n19.52\n17.99\n0.15\n0.47\n0.33\n0.27\n0.21\nBLEU-DC\nMeteor\nRouge-L\nCider\nFigure 4: SoTaNa-13B on code summarization\n1000\n5000\n10000\n50000\n100000\nNumber of Training Examples\n0\n5\n10\n15\n20\n25\nScore\n0.87\n2.89\n3.67\n4.35\n4.69\n5.14\n10.54\n11.6\n14.47\n15.29\n6.55\n16.4\n18.17\n21.7\n22.93\n0.01\n0.29\n0.3\n0.44\n0.47\nBLEU-DC\nMeteor\nRouge-L\nCider\nFigure 5: SoTaNa-30B on code summarization\n1000\n5000\n10000\n50000\n100000\nNumber of Training Examples\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nScore\n9.15\n10.98\n15.24\n11.59\n10.98\n13.41\n16.46\n12.2\n15.85\n18.29\n11.59\n21.34\n12.2\n23.78\n23.17\nSoTaNa-7B\nSoTaNa-13B\nSoTaNa-30B\nFigure 6: SoTaNa on code generation\nFigure 7: The impact of different data size.\n7\nThreats to Validity\nData Quality. Another potential concern lies in the\ndata generation process using ChatGPT. While we\nhave filtered out some low-quality datasets, such\nas instructions with less than 3 words, we acknowl-\nedge that human checks for the correctness of the\ngenerated data were not performed. To improve\nthe quality of generated data, future work can in-\ncorporate human checks or propose alternative ap-\nproaches to ensure data accuracy and reliability.\nEvaluation Datasets. The experiments have\nbeen conducted on widely-used datasets; however,\nthere are other datasets available for each down-\nstream task. These alternative datasets may differ\nin construction methods and corpus sizes, poten-\ntially leading to variations in model performance.\nTo enhance the robustness of our findings and con-\nclusions, further experiments on a broader range of\ndatasets can be carried out to validate the general-\nizability of the results.\nEvaluation Metrics.\nWe have utilized\ncommonly-used metrics to assess the performance\nof the models. However, it is crucial to recognize\nthat these metrics may have inherent limitations.\nFor example, metrics like BLEU and METEOR\nrely on textual similarity and may not fully capture\nthe semantic similarity between two sentences. To\naddress these limitations and obtain a more com-\nprehensive evaluation, we also conducted human\nevaluations. However, it\u2019s worth noting that hu-\nman evaluations can be labor-intensive and time-\nconsuming. In future research, we will explore new\nautomatic evaluation metrics that are more aligned\nwith human perception.\n8\nConclusion\nThis paper presents SoTaNa, an open-source soft-\nware development assistant designed to meet the\nincreasing demand for effective software devel-\nopment tools.\nBy leveraging Large Language\nModels (LLMs), SoTaNa generates high-quality\ninstruction-based data tailored for software engi-\nneering tasks. It employs a parameter-efficient fine-\ntuning approach to enhance the capabilities of the\nLLaMA open-source foundation model. Through\ncomprehensive evaluation, including human eval-\nuation, SoTaNa demonstrates its efficacy in assist-\ning developers by providing accurate answers to\ndiverse Stack Overflow queries. It outperforms ex-\nisting language models in understanding human\n10\nintent and generating contextually appropriate re-\nsponses specific to software engineering tasks.\nIn future work, we aim to introduce a bench-\nmark to further evaluate LLMs as open-source soft-\nware development assistants. This benchmark will\nprovide a standardized and systematic approach\nfor assessing the performance of various language\nmodels in the context of software engineering, en-\nabling better comparison and progress tracking.\nReferences\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu,\nand Charles Sutton. 2018. A survey of machine learn-\ning for big code and naturalness. ACM Computing\nSurveys (CSUR), 51(4):1\u201337.\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo. https:\n//github.com/nomic-ai/gpt4all.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments.\nIn IEEvalua-\ntion@ACL.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nLan Cao and Balasubramaniam Ramesh. 2008. Agile\nrequirements engineering practices: An empirical\nstudy. IEEE software, 25(1):60\u201367.\nSahil\nChaudhary.\n2023.\nCode\nalpaca:\nAn\ninstruction-following llama model for code genera-\ntion. https://github.com/sahil280114/\ncodealpaca.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nMike Conover, Matt Hayes, Matt Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\nWendell, and Patrick Zaharia. 2023. Hello dolly: De-\nmocratizing the magic of chatgpt with open models.\nChrysanthos Dellarocas and Mark Klein. 2000.\nA\nknowledge-based approach for handling exceptions\nin business processes. Information Technology and\nManagement, 1:155\u2013169.\nDomeccleston. 2023. Sharegpt \u2013 share your wildest\nchatgpt conversations with one click. Retrieved 23\nMay 2023.\nDRM Associates. 2002.\nNew product development\nglossary. Archived from the original on 13 July 2018.\nRetrieved 29 October 2006.\nWilliam A Florac and Anita D Carleton. 1999. Measur-\ning the software process: statistical process control\nfor software process improvement. Addison-Wesley\nProfessional.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nJonathan Grudin. 1994. Groupware and social dynam-\nics: Eight challenges for developers. Communica-\ntions of the ACM, 37(1):92\u2013105.\nAndrew F Hayes and Klaus Krippendorff. 2007. An-\nswering the call for a standard reliability measure for\ncoding data. Communication methods and measures,\n1(1):77\u201389.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi\nJin. 2018. Summarizing source code with transferred\napi knowledge.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023a.\nExploring the impact of instruction data\nscaling on large language models: An empirical\nstudy on real-world use cases.\narXiv preprint\narXiv:2303.14742.\nYunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang\nNiu, Baochang Ma, and Xiangang Li. 2023b. To-\nwards better instruction following language models\nfor chinese: Investigating the impact of training data\nand evaluation. arXiv preprint arXiv:2304.07854.\nWenxiang Jiao, Wenxuan Wang, JT Huang, Xing\nWang, and ZP Tu. 2023. Is chatgpt a good trans-\nlator? yes with gpt-4 as the engine. arXiv preprint\narXiv:2301.08745.\nMaurice G Kendall. 1945. The treatment of ties in\nranking problems. Biometrika, 33(3):239\u2013251.\nBonan Kou, Yifeng Di, Muhao Chen, and Tianyi Zhang.\n2022. Sosum: a dataset of stack overflow post sum-\nmaries. In Proceedings of the 19th International\nConference on Mining Software Repositories, pages\n247\u2013251.\n11\nTunstall Lewis, Lambert Nathan, Beeching Nazneen,\nRajaniand Edward, Le Scao Teven, Han Sheon,\nSchmid Philipp, von Werra Leandro, and Sasha\nRush. 2023. Creating a coding assistant with star-\ncoder.\nhttps://huggingface.co/blog/\nstarchat-alpha.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries.\nIn Text Summarization\nBranches Out.\nTommi Mikkonen, Casper Lassenius, Tomi M\u00e4nnist\u00f6,\nMarkku Oivo, and Janne J\u00e4rvinen. 2018. Continu-\nous and collaborative technology transfer: Software\nengineering research with real-time industry impact.\nInformation and Software Technology, 95:34\u201345.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\narXiv preprint arXiv:2104.08773.\nSridhar Nerur, RadhaKanta Mahapatra, and George\nMangalaraj. 2005. Challenges of migrating to ag-\nile methodologies. Communications of the ACM,\n48(5):72\u201378.\nBashar Nuseibeh. 1996. To be and not to be: On man-\naging inconsistency in software development.\nIn\nProceedings of the 8th International Workshop on\nSoftware Specification and Design, pages 164\u2013169.\nIEEE.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nTB OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. OpenAI.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nVictor Sanh, Albert Webson, Colin Raffel, et al. 2021.\nMultitask prompted training enables zero-shot task\ngeneralization. arXiv:2110.08207 [cs].\nChenhui Shen, Liying Cheng, Yang You, and Lidong\nBing. 2023. Are large language models good evalua-\ntors for abstractive summarization? arXiv preprint\narXiv:2305.13091.\nEnsheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi\nHan, Hongyu Zhang, Dongmei Zhang, and Hong-\nbin Sun. 2022a. On the evaluation of neural code\nsummarization. In Proceedings of the 44th Interna-\ntional Conference on Software Engineering, pages\n1597\u20131608.\nEnsheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,\nShi Han, Dongmei Zhang, and Hongbin Sun. 2021.\nCast: Enhancing code summarization with hierar-\nchical splitting and reconstruction of abstract syntax\ntrees. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4053\u20134062.\nEnsheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu\nZhang, Shi Han, Dongmei Zhang, and Hongbin Sun.\n2022b. Race: Retrieval-augmented commit message\ngeneration. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 5520\u20135530.\nEnsheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du,\nShi Han, Dongmei Zhang, and Hongbin Sun. 2023.\nTowards efficient fine-tuning of pre-trained code mod-\nels: An experimental study and beyond. The 32nd\nACM SIGSOFT International Symposium on Soft-\nware Testing and Analysis.\nQingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng\nLin. 2023. Alpaca-cot: An instruction fine-tuning\nplatform with instruction data collection and uni-\nfied large lnguage models interface.\nhttps://\ngithub.com/PhoebusSi/alpaca-CoT.\nTerry Stone. 2010.\nManaging the Design Process-\nImplementing Design: An Essential Manual for the\nWorking Designer. Rockport Publishers.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023.\nAlpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR.\n12\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022a. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022b. Super-\nnaturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5085\u20135109.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, et al. 2021.\nFinetuned language models are zero-shot learners.\narXiv:2109.01652 [cs].\nTerry Winograd. 1973. Breaking the complexity barrier\nagain. ACM Sigplan Notices, 10(1):13\u201330.\nBigScience Workshop, Teven Le Scao, Angela Fan, et al.\n2022. Bloom: A 176b-parameter open-access multi-\nlingual language model.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a.\nWizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023b.\nBaize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie\nLu, Bingchao Wu, Bei Guan, Yongji Wang, and\nJian-Guang Lou. 2022.\nWhen neural model\nmeets nl2code:\nA survey.\narXiv preprint\narXiv:2212.09420.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\nRepresentations (ICLR).\nSusan Zhang, Stephen Roller, Naman Goyal, et al. 2022.\nOpt: Open pre-trained transformer language models.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, et al.\n2020.\nFine-tuning language models from human\npreferences.\n13\n"
  },
  {
    "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers",
    "link": "https://arxiv.org/pdf/2308.13494.pdf",
    "upvote": "8",
    "text": "Eventful Transformers:\nLeveraging Temporal Redundancy in Vision Transformers\nMatthew Dutson, Yin Li, and Mohit Gupta\nUniversity of Wisconsin\u2013Madison\n{dutson,yin.li,mgupta37}@wisc.edu\nAbstract\nVision Transformers achieve impressive accuracy across\na range of visual recognition tasks. Unfortunately, their\naccuracy frequently comes with high computational costs.\nThis is a particular issue in video recognition, where mod-\nels are often applied repeatedly across frames or tempo-\nral chunks. In this work, we exploit temporal redundancy\nbetween subsequent inputs to reduce the cost of Trans-\nformers for video processing. We describe a method for\nidentifying and re-processing only those tokens that have\nchanged significantly over time. Our proposed family of\nmodels, Eventful Transformers, can be converted from ex-\nisting Transformers (often without any re-training) and give\nadaptive control over the compute cost at runtime. We eval-\nuate our method on large-scale datasets for video object\ndetection (ImageNet VID) and action recognition (EPIC-\nKitchens 100). Our approach leads to significant computa-\ntional savings (on the order of 2-4x) with only minor reduc-\ntions in accuracy.\n1. Introduction\nTransformers, initially designed for language model-\ning [60], have been recently explored as an architecture for\nvision tasks. Vision Transformers [16] have achieved im-\npressive accuracy across a range of visual recognition prob-\nlems, attaining state-of-the-art performance in tasks includ-\ning image classification [16], video classification [1, 2, 18],\nand object detection [8, 38, 42, 64].\nOne of the primary drawbacks of vision Transformers\nis their high computational cost. Whereas typical convo-\nlutional networks (CNNs) consume tens of GFlops per im-\nage [7], vision Transformers often require an order of mag-\nnitude more computation, up to hundreds of GFlops per im-\nage. In video processing, the large volume of data further\namplifies these costs. High compute costs preclude vision\nTransformers from deployment on resource-constrained or\nlatency-critical devices, limiting the scope of this otherwise\n Efficiency and adaptivity\nFlush\nReduce budget\nTemporal redundancy in vision Transformers\nIdentify and update tokens \nwith large changes\nDownstream \nTransformer \noperations\nPredictions\n0\n50\n100\nFigure 1. Eventful Transformers. Our method exploits temporal\nredundancy between subsequent model inputs. (Top) Within each\nTransformer block, we identify and update only those tokens with\nsignificant changes over time. Image: [5]. (Bottom) In addition to\nimproving efficiency, our method gives fine-grained control over\nthe compute cost at runtime. \u201cBudget\u201d refers to parameter r as\ndescribed in Section 4.3. \u201cFlush\u201d refers to the initialization of all\ntokens on the first time step. This example shows the ViTDet [38]\nobject detection model on a video from the VID [57] dataset.\nexciting technology. In this paper, we present one of the\nfirst methods to use temporal redundancy between subse-\nquent inputs to reduce the cost of vision Transformers when\napplied to video data.\nTemporal redundancy. Consider a vision Transformer that\nis applied frame-by-frame or clip-by-clip to a video se-\nquence. This Transformer might be a simple frame-wise\narXiv:2308.13494v1  [cs.CV]  25 Aug 2023\nmodel (e.g., an object detector) or an intermediate step\nin some spatiotemporal model (e.g., the first stage of the\nfactorized model from [1]). Unlike in language process-\ning, where one Transformer input represents a complete se-\nquence, we consider Transformers applied to several dis-\ntinct inputs (frames or clips) over time.\nNatural videos contain significant temporal redundancy,\nwith only slight differences between subsequent frames.\nDespite this fact, deep networks (including Transformers)\nare commonly computed \u201cfrom scratch\u201d on each frame.\nThis approach is wasteful, discarding all potentially relevant\ninformation from previous inferences. Our key intuition is\nthat we can reuse intermediate computations from earlier\ntime steps to improve efficiency on redundant sequences.\nAdaptive inference. For vision Transformers (and deep\nnetworks in general), the inference cost is typically fixed by\nthe architecture. However, in real-world applications, the\navailable resources may vary over time (e.g., due to com-\npeting processes or variations in power supply). As such,\nthere is a need for models whose computational cost can be\nmodified at runtime [48]. In this work, adaptivity is one of\nour primary design objectives; we design our method to al-\nlow real-time control over the compute cost. See Figure 1\n(bottom portion) for an example where we vary the compute\nbudget throughout a video.\nChallenges and opportunities. There are past works ex-\nploring temporal redundancy [17, 23, 51] and adaptiv-\nity [47, 61, 69] for CNNs. However, these methods are gen-\nerally incompatible with vision Transformers, owing to sub-\nstantial architectural differences between Transformers and\nCNNs. Specifically, Transformers introduce a new prim-\nitive, self-attention, that does not conform to the assump-\ntions of many CNN-based methods.\nDespite this challenge, vision Transformers also repre-\nsent a unique opportunity. In CNNs, it is difficult to trans-\nlate sparsity improvements (i.e., the sparsity gained by con-\nsidering temporal redundancy) into concrete speedups. Do-\ning so requires imposing significant restrictions on the spar-\nsity structure [23] or using custom compute kernels [51]. In\ncontrast, the structure of Transformer operations (centered\non manipulating token vectors) makes it easier to translate\nsparsity into reduced runtime using standard operators.\nEventful Transformers. We propose Eventful Transform-\ners, a new class of Transformer that leverages temporal re-\ndundancy between inputs to enable efficient, adaptive in-\nference.\nThe term \u201cEventful\u201d is inspired by event cam-\neras [4, 40], sensors that produce sparse outputs based on\nscene changes.\nEventful Transformers track token-level\nchanges over time, selectively updating the token represen-\ntations and self-attention maps on each time step. Blocks in\nan Eventful Transformer include gating modules that allow\ncontrolling the number of updated tokens at runtime.\nOur method can be applied to off-the-shelf models (gen-\nerally without re-training) and is compatible with a wide\nrange of video processing tasks. Our experiments demon-\nstrate that Eventful Transformers, converted from existing\nstate-of-the-art models, significantly reduce computational\ncosts while largely preserving the original model\u2019s accu-\nracy.\nWe publicly release our code, which includes Py-\nTorch modules for building Eventful Transformers. See our\nproject page: wisionlab.com/project/eventful-transformers.\nLimitations. We demonstrate wall-time speedups on both\nthe CPU and GPU. However, our implementation (based on\nvanilla PyTorch operators) is likely sub-optimal from an en-\ngineering standpoint. With additional effort to reduce over-\nhead (e.g., implementing a fused CUDA kernel for our gat-\ning logic), we are confident that the speedup ratios could be\nfurther improved. Our method also involves some unavoid-\nable memory overheads. Perhaps unsurprisingly, reusing\ncomputation from previous time steps requires maintaining\nsome tensors in memory. These memory overheads are rel-\natively modest; see Section 6 for further discussion.\n2. Related Work\nEfficient Transformers. Several past works improve the\nefficiency of Transformers. Many of these methods focus\non reducing the quadratic complexity of self-attention, often\nusing low-rank or sparse approximations [11, 13, 22, 31, 32,\n34, 44, 55, 56, 63]. In this work, we consider standard self-\nattention (with windowing in some cases). Our approach is\northogonal to the above methods.\nSelecting and summarizing tokens. Some recent works\nimprove the efficiency of vision Transformers by exploit-\ning spatial redundancy within each input. Many of these\nmethods prune or fuse tokens based on a salience mea-\nsure [19, 21, 39, 49, 52]. A notable example is the Adap-\ntive Token Sampling (ATS) algorithm [19], which has an\nadaptive computation cost and does not require re-training.\nOther spatial redundancy methods include adaptive token\npooling [3, 45], hierarchical pooling [50], learned tokeniza-\ntion [58], and progressive token sampling [72].\nUnlike these works, which consider spatial redundancy,\nour method targets temporal redundancy. This makes our\nwork complementary to these approaches. A single model\ncan leverage both spatial and temporal redundancy by only\nupdating tokens that are both salient and not temporally\nrepetitive.\nWe illustrate this compatibility in our exper-\niments by building a simple proof-of-concept model that\ncombines temporal and spatial redundancy.\nAnother related work is Spatiotemporal Token Selection\n(STTS) [62], which exploits spatiotemporal redundancy for\nvideo inference. STTS is intended for models with explicit\ntemporal reasoning that take an entire video as input. In\ncontrast, our method is designed for models that are repet-\nitively applied to frames or clips. Compared to STTS, our\nmethod covers a wider range of architectures and tasks.\nTemporal redundancy between inputs. There has been\nrecent work on exploiting inter-frame temporal redundancy\nin CNNs [9, 17, 23, 51]. While we draw some inspiration\nfrom these methods, directly applying them to vision Trans-\nformers is not feasible due to significant architectural differ-\nences between CNNs and Transformers.\nThere is limited existing research on exploiting tem-\nporal redundancy between subsequent vision Transform-\ners inputs. To our knowledge, the only past work in this\narea is the Spatiotemporal Gated Transformers (STGT)\nmethod [37].\nThere are two noteworthy differences be-\ntween STGT and our work.\nMost notably, STGT only\nconsiders temporal redundancy within token-level opera-\ntions (e.g., token-wise linear transforms), and not within\nthe self-attention operator. Our method accelerates all ma-\njor Transformer components, including self-attention. Fur-\nther, STGT uses lossy gating logic that leads to accu-\nracy degradation on long sequences with gradual changes.\nOur method avoids this issue by employing an improved,\nreference-based gating mechanism.\nAdaptive neural networks. Many existing methods add\nadaptivity to deep CNNs [12, 20, 28, 47, 59, 61, 65, 67,\n69, 70]. However, due to architectural differences (e.g., the\nuse of relative position embeddings in Transformers), these\nmethods (e.g., those based on input resizing) often do not\ntranslate to vision Transformers.\nThere has been some recent work on adaptive vision\nTransformers [46, 66, 71]. These works leverage redun-\ndancy within a single input, whereas we consider redun-\ndancy between inputs. Unlike our method, these approaches\ngenerally require re-training or fine-tuning the model.\nEfficient neural networks. There is a substantial body of\nwork on improving the efficiency of deep networks. Some\nworks propose efficient CNN architectures [27, 30, 73].\nOthers use reduced-precision arithmetic [14, 29, 53] or\npruning [24, 25, 35, 36]. Our method is loosely connected\nto pruning; it can be viewed as adaptively pruning redun-\ndant tokens on each time step.\n3. Background: Vision Transformers\nIn this section, we describe the basic elements of a vi-\nsion Transformer (see [16] for more details) and define the\nnotation we use throughout the rest of the paper.\nA vision Transformer consists of a sequence of Trans-\nformer blocks. The input to each block is a list of N, D-\ndimensional token vectors; we denote this as x \u2208 RN\u00d7D.\nBefore the first Transformer block, a vision Transformer\nmaps each image patch to a token vector using a linear\ntransform. Positional embedding [60] can be injected be-\nfore the first block [16] or at every block [38].\nA Transformer block. A Transformer block maps input\nx \u2208 RN\u00d7D to output z \u2208 RN\u00d7D, according to\ny = MSA(LN(x)) + x,\n(1)\nz = MLP(LN(y)) + y,\n(2)\nwhere \u201cMSA\u201d denotes multi-headed self-attention. \u201cMLP\u201d\nis a token-wise multilayer perceptron with two layers and\none GELU nonlinearity. \u201cLN\u201d denotes layer normalization.\nMulti-headed self-attention (MSA). The self-attention op-\nerator first applies three linear transforms Wq, Wk, Wv \u2208\nRD\u00d7D to its input x\u2032 = LN(x).\nq = x\u2032Wq\nk = x\u2032Wk\nv = x\u2032Wv.\n(3)\nq, k, and v are the \u201cquery,\u201d \u201ckey,\u201d and \u201cvalue\u201d tensors, re-\nspectively. In practice, Wq, Wk, Wv are often fused into a\nsingle transform Wqkv = [Wq, Wk, Wv]. These transforms\nmay include a bias; we omit the bias here for brevity.\nThe self-attention operator then computes a normalized\nsimilarity matrix (attention matrix) A \u2208 RN\u00d7N between\nthe tokens of q and k.\nA = Softmax\n\u0010\nqkT /\n\u221a\nD\n\u0011\n.\n(4)\nSoftmax normalization is applied along rows of the matrix.\nThe MSA output y\u2032 is an attention-weighted sum of the\nvalue tokens v, followed by a linear projection Wp.\ny\u2032 = (Av) Wp.\n(5)\nMulti-headed self-attention (as opposed to single-headed\nself-attention) splits q, k, and v into H tensors of shape\nRN\u00d7(D/H) and applies self-attention in parallel across\nthese H heads. Before applying Wp, the results of all heads\nare concatenated into a tensor with shape RN\u00d7D.\nWindowed attention. Standard MSA has a complexity of\nO(N 2) (quadratic in the number of tokens). To reduce this\ncost, many vision Transformers adopt windowed attention.\nWindowed attention constrains the attention computation\nto local windows. Information can be exchanged between\nwindows by shifting the windows between blocks [42] or\nby interleaving global attention [38].\n4. Eventful Transformers\nOur goal is to accelerate vision Transformers for video\nrecognition, in the situation where a Transformer is ap-\nplied repetitively across frames or chunks of frames (e.g.,\nfor video object detection or video action recognition, re-\nspectively). Our key idea is to exploit temporal redundancy\nby re-using computation from previous time steps. In this\nsection, we describe how to modify Transformer blocks to\nadd temporal redundancy awareness.\n1. Compute error\nMask\nTotal error\nMask\n3: Extract selected tokens\n\u00d7\nCurrent tokens\nOutput tokens\n4: Update reference\nNegated mask\n\u00d7\nReference tokens\n2: Apply selection policy\nReference tokens\n\u2013\nCurrent tokens\nTotal error\nReference tokens\nOutput tokens\nPolicy\n+\nFigure 2. Token Gating. A gating module compares incoming\ntokens against a stored reference. If the difference between a token\nand its reference is large, then the token is selected to be updated.\nSee Section 4.3 for details on selection policies. Images are from\nthe VID [57] dataset.\nIn Section 4.1, we present a token-gating module that\nmonitors temporal changes and determines which tokens\nto update.\nIn Section 4.2, we integrate our token gat-\ning logic into a Transformer block, creating a redundancy-\naware Eventful Transformer block. In Section 4.3, we ex-\nplore policies for selecting which tokens to update.\n4.1. Token Gating: Detecting Redundancy\nIn this subsection, we propose two modules: token gates\nand token buffers. These modules allow us to identify and\nupdate only those tokens that have changed significantly\nsince their last update.\nGate module. A gate selects M \u2264 N of its input tokens\nto send to downstream layers for re-computation. The gate\nmaintains a set of reference tokens in memory, which we\ndenote as u \u2208 RN\u00d7D. The reference tensor contains the\nvalue of each token on the time step it was most recently\nupdated. On each time step, tokens are compared against\ntheir references; those that deviate significantly from their\nreference are selected for an update.\nToken-wise \noperations\nGate\nGather\nBuffer\nScatter\nFigure 3. Accelerating token-wise operations. The gate reduces\nthe number of active tokens from N to M. Subsequent token-wise\noperations operate on a smaller tensor and therefore have a lower\ncomputational cost (proportional to M).\nLet c \u2208 RN\u00d7D denote the current input to the gate. On\neach time step, we update the gate\u2019s state and determine its\noutput according to the following procedure (see Figure 2):\n1. Compute the total error e = u \u2212 c.\n2. Apply a selection policy to the error e. A selection pol-\nicy returns a binary mask m (equivalently, a list of token\nindices) indicating which M tokens should be updated.\n3. Extract the tokens selected by the policy. In Figure 2, we\ndepict this as the product c\u00d7m; in practice, we achieve\nthis with a \u201cgather\u201d operation along the first axis of c.\nWe denote the gathered tokens as \u02dcc \u2208 RM\u00d7D. The gate\nreturns \u02dcc as its output.\n4. Update the references for selected tokens. In Figure 2,\nwe depict this as u \u2190 e \u00d7 (\u223c m) + c \u00d7 m; in practice,\nwe apply a \u201cscatter\u201d operation from \u02dcc into u.\nOn the first time step, the gate updates all tokens (initializ-\ning u \u2190 c and returning \u02dcc = c).\nBuffer module. A buffer module maintains a state tensor\nb \u2208 RN\u00d7D that tracks the most recent known value for each\nof its input tokens. When receiving a new input f(\u02dcc) \u2208\nRM\u00d7D, the buffer scatters the tokens from f(\u02dcc) into their\ncorresponding locations in b. It then returns the updated b\nas its output. See Figure 3.\nWe pair each gate with a subsequent buffer. One simple\nusage pattern is as follows. The gate output \u02dcc \u2208 RM\u00d7D\nis passed to a series of token-wise operations f(\u02dcc). The\nresulting tensor f(\u02dcc) \u2208 RM\u00d7D is then passed to a buffer,\nwhich restores the full shape RN\u00d7D.\n4.2. Building Redundancy-Aware Transformers\nIn this subsection, we propose a modified Transformer\nblock that exploits temporal redundancy. Figure 4 shows\nour design for an Eventful Transformer block. Our method\naccelerates token-wise operations (e.g., the MLP), as well\nas the query-key and attention-value multiplications (Equa-\ntions 4 and 5, respectively).\nToken-wise operations. Many of the operations in a Trans-\nformer block are token-wise, meaning they do not involve\ninformation exchange between tokens. These include the\nMatrix multiplication order\nExisting Transformer operation\nGate module\nDelta gate module\nRow-sparse (gated tokens)\nColumn-sparse\nBuffer module\nLayer norm\nLinear\nTranspose\nQuery-key \nproduct\nSoftmax\nAdd\nLinear\nAdd\nMLP\nLayer norm\nAttention-\nvalue \nproduct\nFigure 4. An Eventful Transformer block. To exploit temporal redundancy, we strategically apply token gating throughout the block and\ncompute a modified, sparse self-attention update. Rectangles are standard Transformer components (see Section 3). For clarity, we have\nomitted some minor operations (e.g., scaling after the first matrix multiplication) from this figure.\nMLP and the linear transforms in the MSA. We can save\ncomputation in token-wise operations by skipping those to-\nkens not selected by the gate. Due to token-wise indepen-\ndence, this does not change the result of the operation for\nthe selected tokens. See Figure 3.\nSpecifically, we place a gate-buffer pair around each con-\ntiguous sequence of token-wise operations, including the\nWqkv transform (Equation 3), the Wp transform (Equa-\ntion 5), and the MLP. Note that we add buffers before the\nskip connections (Equations 1 and 2) to ensure that the to-\nkens of the two addition operands are correctly aligned.\nThe cost of a token-wise operation is proportional to the\nnumber of tokens. A gate reduces the number of tokens\nfrom N to M. This, in turn, reduces the computational cost\nof downstream token-wise operations by a factor of N/M.\nThe query-key product. We now consider the query-key\nproduct B = qkT (part of Equation 4). Writing this matrix\nmultiplication explicitly, we have\nBij =\nX\np\nqip\n\u0000kT \u0001\npj .\n(6)\nElement Bij needs to be updated if either (a) there is a\nchange in the ith row of q, or (b) there is a change in the\njth column of kT . Due to the gate that we inserted before\nthe Wqkv transform (shown in Figure 4), only some rows of\nq and some columns of kT have changed. Therefore, we\nonly need to recompute a subset of the elements of B.\nLet \u02dcx\u2032 \u2208 RM\u00d7D denote the output of the gate before\nthe Wqkv transform. We define \u02dcq = \u02dcx\u2032Wq and \u02dck = \u02dcx\u2032Wk\n(following Equation 3). Let q and k denote the outputs of\nthe \u02dcq and \u02dck buffers (shown in Figure 4). \u02dcq contain \u02dck the\nsubset of tokens from q and k that are being updated.\nFigure 5 depicts our method for sparsely updating B.\nThe product \u02dcqkT contains the elements of B that need to\nbe updated due to a change in \u02dcq. We compute \u02dcqkT , then\nscatter the result row-wise into the old B (the value of B\nfrom the last time step). We use an analogous approach for\nthe \u02dck-induced updates; we compute q\u02dckT and scatter the re-\nsult column-wise into B.\nThe overall cost of these updates is 2NMD, compared\nto a cost of N 2D to compute B from scratch. Note that\nthe cost of our method is proportional to M, the number\nof tokens selected by the gate. We save computation when\nM < N/2 (when we update fewer than half of the tokens).\nThe above method for updating B involves some re-\ndundant computation. Some elements of the first scattered\nmatrix \u02dcqkT are also present in the second matrix q\u02dckT .\nThese overlapping elements are computed twice. Eliminat-\ning this redundancy would reduce the cost of our method\nto NMD \u2264 N 2D. This could be achieved by removing\nthe tokens in \u02dcq from q before computing q\u02dckT . We would\nthen scatter the result by indexing along both axes of B. We\nleave this as an optimization for future implementations.\nThe attention-value product. We now describe a method\nfor updating the attention-value product Av (part of Equa-\ntion 5). Writing this multiplication explicitly, we have\n(Av)ij =\nX\np\nAipvpj.\n(7)\nBecause of the gate before the Wqkv transform, only some\nrows (tokens) of v change on each time step. However,\nthere are some updated values in every column of v. There-\nfore, every element of Av will change on each time step.\nThis means we cannot use the same strategy that we used\nfor B, where we only updated some of the output elements.\nInstead, we propose a delta-based update strategy. Let\nAo and vo denote the last known values for A and v. Let\nA\u2206 and v\u2206 denote changes in A and v. Define An =\nQuery update\nKey update\nNo update\nJoint update\n\u00d7\n=\n\u00d7\n=\nScatter\nScatter\nFigure 5. The query-key product. We reduce the cost of comput-\ning B = qkT by only updating a subset of its elements. We first\ncompute changes induced by updated rows in q (top-left), then\ncompute changes induced by updated columns in kT (bottom).\nAo + A\u2206 and vn = vo + v\u2206. We can compute the up-\ndated attention-value product Anvn as\nAnvn = (Ao + A\u2206)(vo + v\u2206)\n= Aovo + Aov\u2206 + A\u2206vo + A\u2206v\u2206\n= Aovo + (Ao + A\u2206)v\u2206 + A\u2206(vo + v\u2206) \u2212 A\u2206v\u2206\n= Aovo + Anv\u2206 + A\u2206vn \u2212 A\u2206v\u2206.\n(8)\nTherefore, on each time step, we can update Av by adding\nAnv\u2206 + A\u2206vn \u2212 A\u2206v\u2206 to the previous result Aovo.\nWe obtain A\u2206, v\u2206, Ao, and vo using delta gate modules.\nDelta gates are similar to the gates defined in Section 4.1,\nwith one difference: instead of returning \u02dcc, a delta gate re-\nturns u and \u02dce (where \u02dce is the result of gathering the selected\nindices from e). u represents the effective current value of\nthe gate\u2019s output, corresponding to An or vn in Equation 8.\n\u02dce represents the amount of change on the current time step,\ncorresponding to A\u2206 or v\u2206 in Equation 8.\nFigure 6 illustrates our approach for efficiently comput-\ning the three delta terms in Equation 8. We remove the\ncolumns of An that correspond to zero rows in v\u2206 (these\ncolumns will always be multiplied by zero). Let \u02dcAn denote\nAn with these columns removed. We remove rows of vn\nanalogously to produce \u02dcvn. We then compute\n\u02dcAn\u02dcv\u2206 + \u02dcA\u2206\u02dcvn \u2212 \u02dcA\u2206\u02dcv\u2206,\n(9)\nadding the result to the previous value of Av.\nThe product \u02dcA\u2206\u02dcv\u2206 assumes the columns of \u02dcA\u2206 are cor-\nrectly aligned with the rows of \u02dcv\u2206. We achieve this align-\nment by forcing the A gate to select the same indices as\nthe v gate. Using a separate policy in the A gate would be\npossible, but would require a re-alignment operation before\ncomputing \u02dcA\u2206\u02dcv\u2206. Further, forcing alignment allows us to\neliminate a multiplication by rearranging Equation 9 as\n\u02dcAn\u02dcv\u2206 + \u02dcA\u2206(\u02dcvn \u2212 \u02dcv\u2206).\n(10)\nEquation 10 has a cost of 2MND (assuming the addition\nhas a negligible cost), compared to N 2D for a standard mul-\ntiplication. We see savings when M < N/2.\nAttention update\nValue update\nJoint update\nNo update\nZero\nNonzero corresponding column\nNonzero corresponding row\n\u00d7\n=\n\u00d7\n=\n+\n\u00d7\n=\n\u00d7\n+\n=\n\u2212\nFigure 6. The attention-value product. We propose a delta-based\nstrategy for sparsely updating the product Av. We reduce the cost\nof each sub-product by cutting rows and columns that do not con-\ntribute to the result (due to a zero multiplication).\n4.3. Token Selection Policies\nAn important design choice for an Eventful Transformer\nis the token selection policy. Given a gate error tensor e, a\npolicy generates a mask m indicating which tokens should\nbe updated. We now discuss the design of selection policies.\nTop-r policy. This policy selects the r tokens whose error\ne has the largest norm (we use the L2 norm). The top-r\npolicy is lightweight and has a single parameter that can be\neasily tuned by hand. Varying r gives direct control over the\nmodel\u2019s computation cost. These properties make the top-\nr policy a good fit for applications with tight (potentially\ntime-varying) computational constraints. We use a top-r\npolicy in our main experiments.\nThreshold policy. This policy selects all tokens where the\nnorm of the error e exceeds a threshold h. A threshold pol-\nicy is input-adaptive; the number of tokens selected depends\non the amount of change in the scene. This input adaptivity\ncan potentially lead to a better accuracy-cost tradeoff. How-\never, the best value for the threshold h depends on the dis-\ntribution of token vectors (which varies across layers) and is\ndifficult to decide. In addition, a threshold policy does not\ngive a fixed compute cost. This policy is likely better suited\nto applications with more flexible resources, where achiev-\ning the best possible accuracy-cost tradeoff is critical.\nOther policies. More sophisticated token selection policies\ncould lead to an improved accuracy-cost tradeoff. For ex-\nample, we could use a learned policy (e.g., a lightweight\n0\n5x\n10x\nSavings ratio\nSize 1024\nSize 672\n2048\n1536\n1024\n768\n512\n256\nTokens updated (r)\n-10%\nmAP50\nchange\n1024\n768\n512\n384\n256\n128\nTokens updated (r)\nFigure 7. Video object detection results. Computation savings\nratio (positive axis) and relative reductions in mAP50 score (neg-\native axis) for our method. Results are for the ViTDet model [38]\non the VID [57] dataset. See the supplement for tables.\npolicy network). However, training the policy\u2019s decision\nmechanism might be challenging, due to the general non-\ndifferentiability of the binary mask m. Another idea is to\nuse an importance score (e.g., as proposed in [19]) to in-\nform the selection. We leave these ideas as potential topics\nfor future work.\n5. Experiments\nIn this section, we present our experiments and results.\nWe evaluate our method for video object detection (Sec-\ntion 5.1) and video action recognition (Section 5.2). We\nshow additional analysis in Section 5.3.\n5.1. Video Object Detection\nTask and dataset. We test our method on video object de-\ntection using the ILSVRC 2015 ImageNet VID dataset [57].\nWe report results on the validation set, which contains 555\nvideos with lengths of up to 2895 frames. Following prior\nworks [10, 54], we evaluate the mean average precision\n(mAP) metric with an IoU threshold of 0.5.\nImplementation details. We consider the ViTDet model\nfrom [38], which we apply to individual frames of an in-\nput video.\nViTDet combines a plain Transformer back-\nbone (based on ViT-B [16]) with a standard detection\nhead [6, 26]. The backbone consists of 12 blocks with inter-\nleaved global and windowed self-attention (blocks 3, 6, 9,\nand 12 use global attention). Windowed self-attention uses\na window size of 14\u00d714 tokens (224\u00d7224 pixels). Token\nvectors are 768-dimensional. Self-attention operators have\n12 heads and employ learned relative position embeddings.\nBefore the backbone, the model maps each 16\u00d716 im-\nage patch to a token vector using a linear transform. The\nmodel expects fixed-size inputs (due to resolution-specific\nposition embeddings). Therefore, following from [38], we\nrescale and pad all video frames to a uniform size (e.g.,\n1024\u00d71024) before applying the model.\n50\n100\n500\n70\n75\n80\nmAP50 (%)\nSize 1024\n50\n100\nGFlops\n70\n75\n80\nmAP50 (%)\nSize 672\nBase model\nOur method\nToken-wise only\nSTGT\nFigure 8. Video object detection comparison and ablation. The\naccuracy-cost tradeoff for our method, compared with STGT [37]\nand an ablation that only accelerates token-wise operations. See\nthe supplement for tables.\nWe convert the model to an Eventful Transformer fol-\nlowing the method in Section 4. In blocks that use win-\ndowed attention, we exploit temporal redundancy only\nwithin token-wise operations (not within the query-key or\nattention-value products). Our complete approach is com-\npatible with windowed attention; however, windowing leads\nto some implementation challenges (ragged tensor shapes\nacross windows, making batched computation more diffi-\ncult). Note that for ViTDet, global self-attention represents\nthe bulk of the self-attention compute cost.\nExperiment protocol and baselines.\nWe fine-tune the\noriginal ViTDet weights (trained on COCO) for VID ob-\nject detection. See the supplement for training parameters.\nNote that we fine-tune before we add temporal redundancy\nawareness to the model. We train and evaluate at resolu-\ntion 1024\u00d71024. To understand the effect of token count\n(which strongly influences compute cost), we also evalu-\nate at resolution 672\u00d7672. Rather than training a separate\nlower-resolution model, we adapt the 1024\u00d71024 model by\ninterpolating the learned position embeddings. The result-\ning adapted model retains most of its accuracy.\nWe compare against a version of the STGT method [37].\nDue to unavailable source code, we were unable to evaluate\nall components of this method (notably, the use of a learned\npolicy network). Instead, we consider a simplified version\nthat uses the same top-r policy as our method. This setup\nenables a direct comparison of the core gating and update\nmechanisms. In addition, we evaluate an ablated version of\nour approach that only accelerates token-wise operations.\nWe vary the policy r to explore the accuracy-compute trade-\n1\n5\nTFlops\n60.0\n62.5\n65.0\n67.5\nAccuracy (%)\nBase model\nTuned with r = 50\nTuned with r = 100\nTuned with r = 200\nFigure 9. Video action recognition results. Our results for action\nrecognition on EPIC-Kitchens 100 using the ViViT model [1]. We\nreport the total TFlops per video (spatial + temporal sub-models).\nSee the supplement for a table containing this data.\noff. At resolution 1024, we test r = 256, 512, 768, 1024,\n1536, and 2048 (from a maximum of 4096 tokens). At res-\nolution 672, we test r = 128, 256, 384, 512, 768, and 1024\n(from a maximum of 1764).\nResults. Figure 7 shows our results. Our method gives sig-\nnificant savings with only minor reductions in accuracy. For\nexample, at size 1024 with r = 768, our approach reduces\nthe cost from 467.4 GFlops to 122.3 GFlops (3.8x lower)\nwhile reducing the mAP50 score from 82.93 to 81.25 (-\n1.68% in absolute mAP50). At size 672 with r = 384, we\nreduce the cost by 3.7x with a -0.85% change in mAP50.\nIn these experiments, some tokens correspond to padded\nspace and are therefore \u201ceasy\u201d from a temporal redundancy\nstandpoint. However, even in a padding-free deployment\n(e.g., with a single, known training and inference resolu-\ntion) our method would still give strong computation sav-\nings. For example, consider resolution 1024 with r = 768.\nWe are skipping 66% of all non-padded tokens here (based\non a measured mean padding ratio of 44.6% on VID \u2013 cor-\nresponding to a \u223c16:9 aspect ratio). This corresponds to\na savings of >2x, with an accuracy drop of only 1.68%.\nNote that our ViViT experiments (Section 5.2 and supple-\nmentary) do not involve padding.\nFigure 8 shows the accuracy-compute tradeoff for our\nmethod, along with baselines. Our approach gives a consid-\nerable improvement in the accuracy-compute tradeoff com-\npared to STGT [37]. Further, adding redundancy awareness\nto the query-key and attention-value products reduces the\ncost significantly, especially at low r values.\n5.2. Video Action Recognition\nTask and dataset. We evaluate our method on action recog-\nnition using the EPIC-Kitchens 100 dataset [15]. EPIC-\nKitchens 100 contains highly dynamic egocentric videos\nannotated with 97 verb and 300 noun classes. We consider\nthe verb classification task. The training and validation set\ncontains 67217 and 9668 action instances, respectively.\nImplementation details. We use the ViViT model [1] with\nfactorized spatial and temporal sub-models based on ViT-B.\nTable 1. Adding spatial redundancy to ViTDet. \u201cSpatial\u201d is a\nmodel with pooling in k and v. \u201cSpatiotemporal\u201d is a model with\nboth pooling and temporal redundancy awareness.\nVariant\nr\nmAP50 (%)\nGFlops\nBase model\n\u2013\n82.93\n467.4\nSpatial\n\u2013\n80.15\n388.1\nSpatiotemporal\n2048\n80.14\n217.0\nSpatiotemporal\n1536\n80.07\n169.3\nSpatiotemporal\n1024\n79.50\n121.0\nSpatiotemporal\n768\n78.69\n96.3\nSpatiotemporal\n512\n76.96\n70.9\nSpatiotemporal\n256\n71.35\n44.5\nThe spatial sub-model (the bulk of the compute cost) is ap-\nplied sequentially to 16 2-frame input clips. The outputs of\nthe spatial model are concatenated and passed to the tem-\nporal model, which returns a class prediction. The predic-\ntion is the average over 12 video views (4 temporal views,\neach divided into 3 spatial crops). Each view has a shape of\n320\u00d7320\u00d732. Unlike ViTDet, ViViT adds a class embed-\nding token (see [16]), does not use windowed self-attention,\nand does not use relative position embeddings.\nWe convert the spatial model to an Eventful Transformer.\nNaively replacing the spatial model with an Eventful ver-\nsion leads to a considerable drop in accuracy (about -10%\nwith r = 100). We conjecture that the cause is a distribution\nshift in the inputs to the temporal model (see the supplement\nfor further discussion). We recover most of the lost accu-\nracy by fine-tuning the non-Eventful temporal model on the\noutputs of a frozen Eventful spatial model.\nExperiment protocol. We start with ViViT pre-trained on\nEPIC-Kitchens 100 and fine-tune the temporal model as de-\nscribed above (on the EPIC-Kitchens training set). We fine-\ntune different model variants with policy r values of 50,\n100, and 200 (out of a maximum of 401 tokens). See the\nsupplement for training parameters. We report results using\nthe top-1 accuracy metric, following standard protocol [15].\nResults. Figure 9 shows our results for the Eventful ViViT\nmodel. We evaluate a range of r values for each of the\nfine-tuned variants. We test the original fine-tuned r-value,\nalong with \u00b120% and \u00b140% of this value. We observe\nconsiderable computation savings with only moderate re-\nductions in accuracy. For example, with r = 140, we re-\nduce the cost by 2.4x while reducing the accuracy by only\n1.62%. In addition, the model retains adaptivity despite be-\ning fine-tuned with a single-r value, exhibiting a favorable\naccuracy-compute tradeoff over a range of r-values.\n5.3. Spatial Redundancy and Runtime\nConsidering spatial redundancy. Eventful Transformers\nexploit temporal redundancy and thus complement prior\nworks that leverage spatial redundancy. Here we present\nTable 2. Runtimes (ms). ViTDet runtimes are for the Transformer\nbackbone only. ViViT runtimes include the temporal sub-model.\nModel\nSize\nVariant\nr\nGPU\nCPU\nViTDet\n1024\nBase model\n\u2013\n86.6\n5150\nViTDet\n1024\nSpatial\n\u2013\n58.9\n3116\nViTDet\n1024\nTemporal\n512\n69.9\n3570\nViTDet\n1024\nSpatiotemporal\n512\n38.1\n1682\nViTDet\n672\nBase model\n\u2013\n28.3\n1492\nViTDet\n672\nSpatial\n\u2013\n23.3\n1055\nViTDet\n672\nTemporal\n256\n21.6\n838\nViTDet\n672\nSpatiotemporal\n256\n20.8\n478\nViViT\n320\nBase model\n\u2013\n950\n5.45e4\nViViT\n320\nTemporal\n50\n545\n2.15e4\na simple proof-of-concept experiment that considers spatial\nredundancy in Eventful Transformers.\nSpecifically, we adopt a variant of [68], which applies\nspatial pooling to the self-attention key and value tokens.\nWe apply this method with 2\u00d72 pooling to the global self-\nattention operators in the ViTDet model. We evaluate this\nmethod both with and without temporal redundancy aware-\nness. In the temporal-redundancy model, we pool k and v\nafter their respective buffers. We pool \u02dck by first pooling\nthe active indices (equivalent to max-pooling the mask m),\nthen gathering the buffered k using the pooled indices.\nTable 1 shows our results for resolution 1024 (see the\nsupplement for resolution 672). We see that the spatial and\ntemporal methods are complementary; both meaningfully\ncontribute to reducing the computational cost. See Section 6\nfor further discussion on spatial redundancy methods.\nRuntime. We show preliminary runtime results on a CPU\n(Xeon Silver 4214, 2.2 GHz) and a GPU (NVIDIA RTX\n3090). See the supplementary material for experiment de-\ntails. Table 2 shows our results. Adding temporal redun-\ndancy awareness leads to speedups of up to 1.74x on the\nGPU and 2.48x on the CPU. These results should be seen\njust as a proof of concept \u2013 we are confident that these\nspeedups could be improved with further engineering effort\n(e.g., by replacing vanilla PyTorch operators with custom\nkernels or using a high-performance inference framework).\nVisualization of updates.\nFigure 10 shows an example\nvideo sequence. We visualize the model predictions (top),\nthe token-wise L2 norm of the error e (middle), and the\nupdate mask m (bottom). We see that larger error values\ncorrespond to dynamic regions in the image.\n6. Discussion\nMemory overhead. Our method reduces floating point op-\nerations at the cost of higher memory usage. Each gate or\nbuffer maintains a reference tensor (u or b, respectively).\nPrediction\nUpdates\nError\nFigure 10. Visualization of updates. The error e and update mask\nm, for the pre-QKV gate in block 3 of ViTDet. Video source: [57]\nThe memory overhead for token gates and buffers is gener-\nally modest. For, consider size-1024 ViTDet. The model\nhas 4096 768-dimensional tokens, meaning a token gate or\nbuffer takes 12.6/6.3 MB of memory at full/half precision.\nHowever, gating or buffering the attention matrix A can\nrequire a larger amount of memory. For example, in the\nglobal attention layers of the size-1024 ViTDet model, the\nA matrix has shape 4096\u00d74096\u00d712. Buffering this A re-\nquires 805/403 MB at full/half precision. Fortunately, the\nsituation dramatically improves if we reduce the number of\ntokens or use windowed attention (due to the quadratic size\nof A). For example, the A matrices for the size-672 ViT-\nDet model (1764 tokens) and the ViViT model (301 tokens)\noccupy 149/75 MB and 4.3/2.2 MB, respectively. In appli-\ncations where memory is tight and the A matrix is large, it is\npossible to save memory by removing temporal redundancy\nawareness in the query-key and/or attention-value products\n(each eliminates one A-shaped state tensor).\nIntegration with spatial redundancy methods. A promis-\ning avenue for future work the is further integration of our\napproach with spatial redundancy methods. Conceptually,\nthese methods summarize a large, redundant set of tokens\nusing a more compact set of tokens. The gating module\nin an Eventful Transformer assumes that most of its inputs\nvary smoothly over time. When combining our approach\nwith spatial redundancy methods, we need to ensure that\nthe compact spatial summary is relatively stable. For exam-\nple, with adaptive token clustering [3, 45], we would need\nto sort the clusters in a mostly-consistent order.\nThere are many potentially interesting questions regard-\ning joint modeling of spatial and temporal redundancy. For\nexample, how is the temporal axis different from the spatial\none? Should the temporal axis be modeled separately? We\nleave such questions for future work.\nAcknowledgments. This research was supported in part by\nNSF CAREER award #1943149.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid. ViViT: A video\nvision transformer. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 6836\u2013\n6846, October 2021. 1, 2, 8\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\narXiv, 2021. 1\n[3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman. Token\nmerging: Your ViT but faster. arXiv, 2022. 2, 9\n[4] Christian Brandli, Raphael Berner, Minhao Yang, Shih-Chii\nLiu, and Tobi Delbruck. A 240x180 130 db 3 \u00b5s latency\nglobal shutter spatiotemporal vision sensor. IEEE Journal of\nSolid-State Circuits, 49(10):2333\u20132341, 2014. 2\n[5] BuzzFarmers. flickr.com/photos/buzzfarmers/\n7318008726, 2011. Online, accessed March 2023, CC BY\n2.0 license. 1\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: High\nquality object detection and instance segmentation.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n43(5):1483\u20131498, 2021. 7\n[7] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.\nAn analysis of deep neural network models for practical ap-\nplications. arXiv, 2016. 1\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-\ntors, Proceedings of the European Conference on Computer\nVision (ECCV), pages 213\u2013229, Cham, 2020. Springer Inter-\nnational Publishing. 1\n[9] Lukas Cavigelli, Philippe Degen, and Luca Benini. CBin-\nfer: Change-based inference for convolutional neural net-\nworks on video data. In Proceedings of the 11th Interna-\ntional Conference on Distributed Smart Cameras. Associa-\ntion for Computing Machinery, September 2017. 3\n[10] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Memory\nenhanced global-local aggregation for video object detec-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2020. 7,\n15\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv,\n2019. 2\n[12] Ting-Wu Chin, Ruizhou Ding, and Diana Marculescu. AdaS-\ncale: Towards real-time video object detection using adap-\ntive scaling. In A. Talwalkar, V. Smith, and M. Zaharia, ed-\nitors, Proceedings of Machine Learning and Systems, vol-\nume 1, pages 431\u2013441, 2019. 3\n[13] Krzysztof Marcin Choromanski,\nValerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,\nand Adrian Weller. Rethinking attention with Performers.\nIn International Conference on Learning Representations,\n2021. 2\n[14] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid.\nBinaryConnect:\nTraining deep neural networks\nwith binary weights during propagations. In C. Cortes, N.\nLawrence, D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems, vol-\nume 28. Curran Associates, Inc., 2015. 3\n[15] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nSanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-\nvide Moltisanti, Jonathan Munro, Toby Perrett, Will Price,\nand Michael Wray. Scaling egocentric vision: The EPIC-\nKITCHENS dataset. In Proceedings of the European Con-\nference on Computer Vision (ECCV), September 2018. 8\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 1, 3, 7, 8\n[17] Matthew Dutson, Yin Li, and Mohit Gupta.\nEvent neu-\nral networks. In Shai Avidan, Gabriel Brostow, Moustapha\nCiss\u00b4e, Giovanni Maria Farinella, and Tal Hassner, editors,\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 276\u2013293, Cham, 2022. Springer Nature\nSwitzerland. 2, 3\n[18] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer.\nMultiscale vision transformers.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 6824\u20136835, October 2021. 1\n[19] Mohsen\nFayyaz,\nSoroush\nAbbasi\nKoohpayegani,\nFarnoush\nRezaei\nJafari,\nSunando\nSengupta,\nHamid\nReza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and\nJ\u00a8urgen Gall.\nAdaptive token sampling for efficient vision\ntransformers. In Shai Avidan, Gabriel Brostow, Moustapha\nCiss\u00b4e, Giovanni Maria Farinella, and Tal Hassner, editors,\nProceedings of the European Conference on Computer\nVision (ECCV), pages 396\u2013414, Cham, 2022. Springer\nNature Switzerland. 2, 7\n[20] Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li\nZhang,\nJonathan Huang,\nDmitry Vetrov,\nand Ruslan\nSalakhutdinov.\nSpatially adaptive computation time for\nresidual networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017. 3\n[21] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje,\nVenkatesan Chakaravarthy, Yogish Sabharwal, and Ashish\nVerma. PoWER-BERT: Accelerating BERT inference via\nprogressive word-vector elimination.\nIn Hal Daum\u00b4e III\nand Aarti Singh, editors, Proceedings of the 37th Interna-\ntional Conference on Machine Learning, volume 119 of Pro-\nceedings of Machine Learning Research, pages 3690\u20133699.\nPMLR, July 2020. 2\n[22] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xi-\nangyang Xue, and Zheng Zhang. Star-transformer. arXiv,\n2019. 2\n[23] Amirhossein Habibian, Davide Abati, Taco S. Cohen, and\nBabak Ehteshami Bejnordi. Skip-convolutions for efficient\nvideo processing. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 2695\u20132704, June 2021. 2, 3\n[24] Song Han, Jeff Pool, John Tran, and William Dally. Learning\nboth weights and connections for efficient neural network.\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R.\nGarnett, editors, Advances in Neural Information Processing\nSystems, volume 28. Curran Associates, Inc., 2015. 3\n[25] Babak Hassibi and David Stork. Second order derivatives\nfor network pruning: Optimal brain surgeon. In S. Hanson,\nJ. Cowan, and C. Giles, editors, Advances in Neural Infor-\nmation Processing Systems, volume 5. Morgan-Kaufmann,\n1992. 3\n[26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick.\nMask R-CNN.\nIn Proceedings of the IEEE Inter-\nnational Conference on Computer Vision (ICCV), October\n2017. 7\n[27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. MobileNets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv,\n2017. 3\n[28] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens\nvan der Maaten, and Kilian Weinberger. Multi-scale dense\nnetworks for resource efficient image classification. In In-\nternational Conference on Learning Representations, 2018.\n3\n[29] Kyuyeon Hwang and Wonyong Sung. Fixed-point feedfor-\nward deep neural network design using weights +1, 0, and\n-1. In 2014 IEEE Workshop on Signal Processing Systems\n(SiPS), pages 1\u20136, 2014. 3\n[30] Forrest N. Iandola, Song Han, Matthew W. Moskewicz,\nKhalid Ashraf,\nWilliam J. Dally,\nand Kurt Keutzer.\nSqueezeNet: AlexNet-level accuracy with 50x fewer param-\neters and 0.5MB model size. arXiv, 2016. 3\n[31] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohi-\nuddin, LUKASZ KAISER, Wojciech Gajewski, Henryk\nMichalewski, and Jonni Kanerva. Sparse is enough in scaling\ntransformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin,\nP.S. Liang, and J. Wortman Vaughan, editors, Advances in\nNeural Information Processing Systems, volume 34, pages\n9895\u20139907. Curran Associates, Inc., 2021. 2\n[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc\u00b8ois Fleuret. Transformers are RNNs: Fast autoregres-\nsive transformers with linear attention.\nIn Hal Daum\u00b4e III\nand Aarti Singh, editors, Proceedings of the 37th Interna-\ntional Conference on Machine Learning, volume 119 of Pro-\nceedings of Machine Learning Research, pages 5156\u20135165.\nPMLR, July 2020. 2\n[33] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The Kinetics human action video\ndataset. arXiv, 2017. 14\n[34] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efficient transformer. In International Confer-\nence on Learning Representations, 2020. 2\n[35] Yann LeCun, John Denker, and Sara Solla. Optimal brain\ndamage. In D. Touretzky, editor, Advances in Neural Infor-\nmation Processing Systems, volume 2. Morgan-Kaufmann,\n1989. 3\n[36] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning filters for efficient ConvNets. In In-\nternational Conference on Learning Representations, 2017.\n3\n[37] Yawei Li, Babak Ehteshami Bejnordi, Bert Moons, Tijmen\nBlankevoort, Amirhossein Habibian, Radu Timofte, and Luc\nVan Gool. Spatio-temporal gated transformers for efficient\nvideo processing. In Advances in Neural Information Pro-\ncessing Systems Workshops, 2021. 3, 7, 8\n[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object\ndetection.\nIn Shai Avidan, Gabriel Brostow, Moustapha\nCiss\u00b4e, Giovanni Maria Farinella, and Tal Hassner, editors,\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 280\u2013296, Cham, 2022. Springer Nature\nSwitzerland. 1, 3, 7\n[39] Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue\nWang, and Pengtao Xie. EViT: Expediting vision transform-\ners via token reorganizations. In International Conference\non Learning Representations, 2022. 2\n[40] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A\n128x128 120 db 15 \u00b5s latency asynchronous temporal con-\ntrast vision sensor.\nIEEE Journal of Solid-State Circuits,\n43(2):566\u2013576, 2008. 2\n[41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence\nZitnick.\nMicrosoft COCO: Common objects in context.\nIn David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne\nTuytelaars, editors, Proceedings of the European Conference\non Computer Vision (ECCV), pages 740\u2013755, Cham, 2014.\nSpringer International Publishing. 15\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 10012\u201310022,\nOctober 2021. 1, 3\n[43] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2019. 14, 15\n[44] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang\nXu, Weiguo Gao, Chunjing XU, Tao Xiang, and Li Zhang.\nSOFT: Softmax-free transformer with linear complexity. In\nM. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Informa-\ntion Processing Systems, volume 34, pages 21297\u201321309.\nCurran Associates, Inc., 2021. 2\n[45] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, An-\nish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token\npooling in vision transformers. arXiv, 2021. 2, 9\n[46] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,\nZuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. AdaViT:\nAdaptive vision transformers for efficient image recogni-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 12309\u2013\n12318, June 2022. 3\n[47] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna\nSattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, and\nRogerio Feris. AR-Net: Adaptive frame resolution for effi-\ncient action recognition. In Andrea Vedaldi, Horst Bischof,\nThomas Brox, and Jan-Michael Frahm, editors, Proceedings\nof the European Conference on Computer Vision (ECCV),\npages 86\u2013104, Cham, 2020. Springer International Publish-\ning. 2, 3\n[48] Sparsh Mittal. A survey of techniques for approximate com-\nputing. ACM Computing Surveys, 48(4), March 2016. 2\n[49] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang\nWang,\nRogerio Feris,\nand Aude Oliva.\nIA-RED\u02c62:\nInterpretability-aware redundancy reduction for vision trans-\nformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.\nLiang, and J. Wortman Vaughan, editors, Advances in Neural\nInformation Processing Systems, volume 34, pages 24898\u2013\n24911. Curran Associates, Inc., 2021. 2\n[50] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jian-\nfei Cai. Scalable vision transformers with hierarchical pool-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 377\u2013386, October\n2021. 2\n[51] Mathias Parger, Chengcheng Tang, Christopher D. Twigg,\nCem Keskin,\nRobert Wang,\nand Markus Steinberger.\nDeltaCNN: End-to-end cnn inference of sparse frame differ-\nences in videos. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n12497\u201312506, June 2022. 2, 3\n[52] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh.\nDynamicViT: Efficient vision\ntransformers with dynamic token sparsification. In M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-\nman Vaughan, editors, Advances in Neural Information Pro-\ncessing Systems, volume 34, pages 13937\u201313949. Curran\nAssociates, Inc., 2021. 2\n[53] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. XNOR-Net: ImageNet classification using\nbinary convolutional neural networks. In Bastian Leibe, Jiri\nMatas, Nicu Sebe, and Max Welling, editors, Proceedings\nof the European Conference on Computer Vision (ECCV),\npages 525\u2013542, Cham, 2016. Springer International Publish-\ning. 3\n[54] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2016. 7\n[55] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure\nLeskovec, Dale Schuurmans, and Bo Dai. Combiner: Full at-\ntention transformer with sparse computation cost. In M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-\nman Vaughan, editors, Advances in Neural Information Pro-\ncessing Systems, volume 34, pages 22470\u201322482. Curran\nAssociates, Inc., 2021. 2\n[56] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David\nGrangier. Efficient content-based sparse attention with rout-\ning transformers. arXiv, 2020. 2\n[57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-\nthy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,\nand Li Fei-Fei.\nImageNet large scale visual recogni-\ntion challenge. International Journal of Computer Vision,\n115(3):211\u2013252, April 2015. 1, 4, 7, 9\n[58] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa\nDehghani, and Anelia Angelova. TokenLearner: What can 8\nlearned tokens do for images and videos? arXiv, 2021. 2\n[59] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung.\nBranchyNet: Fast inference via early exiting from deep neu-\nral networks. In 23rd International Conference on Pattern\nRecognition (ICPR), pages 2464\u20132469, 2016. 3\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Il-\nlia Polosukhin.\nAttention is all you need.\nIn I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Infor-\nmation Processing Systems, volume 30. Curran Associates,\nInc., 2017. 1, 3\n[61] Andreas Veit and Serge Belongie. Convolutional networks\nwith adaptive inference graphs. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV), September\n2018. 2, 3\n[62] Junke Wang, Xitong Yang, Hengduo Li, Li Liu, Zuxuan\nWu, and Yu-Gang Jiang. Efficient video transformers with\nspatial-temporal token selection.\nIn Shai Avidan, Gabriel\nBrostow, Moustapha Ciss\u00b4e, Giovanni Maria Farinella, and\nTal Hassner, editors, Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 69\u201386, Cham,\n2022. Springer Nature Switzerland. 2\n[63] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\narXiv, 2020. 2\n[64] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 568\u2013578, October 2021. 1\n[65] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and\nJoseph E. Gonzalez.\nSkipNet: Learning dynamic routing\nin convolutional networks. In Proceedings of the European\nConference on Computer Vision (ECCV), September 2018. 3\n[66] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao\nHuang. Not all images are worth 16x16 words: Dynamic\ntransformers for efficient image recognition. In M. Ranzato,\nA. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Process-\ning Systems, volume 34, pages 11960\u201311973. Curran Asso-\nciates, Inc., 2021. 3\n[67] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S\nDavis. LiteEval: A coarse-to-fine framework for resource\nefficient video recognition. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019. 3\n[68] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty,\nMingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.\nNystr\u00a8omformer: A Nystr\u00a8om-based algorithm for approxi-\nmating self-attention. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 35, pages 14138\u2013\n14148, May 2021. 9\n[69] Ran Xu, Fangzhou Mu, Jayoung Lee, Preeti Mukherjee, So-\nmali Chaterji, Saurabh Bagchi, and Yin Li.\nSmartAdapt:\nMulti-branch object detection framework for videos on mo-\nbiles. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2528\u2013\n2538, June 2022. 2, 3\n[70] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and\nGao Huang. Resolution adaptive networks for efficient infer-\nence. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2020. 3\n[71] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya,\nJan Kautz, and Pavlo Molchanov.\nA-ViT: Adaptive to-\nkens for efficient vision transformer.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10809\u201310818, June 2022. 3\n[72] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei,\nPhilip H.S. Torr, Wayne Zhang, and Dahua Lin.\nVision\ntransformer with progressive sampling. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 387\u2013396, October 2021. 2\n[73] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShuffleNet: An extremely efficient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2018. 3\n[74] Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, and Yichen\nWei. Flow-guided feature aggregation for video object detec-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 408\u2013417, 2017. 15\nSupplementary Material\nWe use capital letters (e.g., Figure A) to refer to the sup-\nplementary material and numbers (e.g., Figure 1) to refer to\nthe main paper.\nIn Section A, we provide further discussion on token se-\nlection policies, optimizations to the query-key product, and\nthe ViViT temporal model. In Section B, we present addi-\ntional experiments: action recognition on Kinetics-400, an\nevaluation of a threshold policy, and an ablation of the gate\nposition. In Section C, we provide low-level details for the\nexperiments in the main paper. In Section D, we include\ntables of results for the experiments in the main paper.\nA. Further Discussion\nThe ViViT temporal sub-model. Recall that, for ViViT\naction recognition, we fine-tune the non-Eventful temporal\nmodel on the outputs of the Eventful spatial model. We\nnow provide some intuition as to why this is necessary to\npreserve the prediction accuracy.\nThe outputs of an Eventful Transformer are approxima-\ntions of the \u201ccorrect\u201d outputs (those of the original, non-\nEventful Transformer).\nIn the case of the ViViT spatial\nmodel, individual outputs are fairly close to the correct val-\nues. However, the pattern of temporal changes between out-\nputs may be quite different from the original model. Token\ngates reduce the number of updated tokens on each frame,\nbut each update tends to be larger (a single update may con-\ntain accumulated changes from several time steps). Given\nthe nature of the prediction task \u2013 action recognition on\nhighly dynamic videos \u2013 the temporal sub-model is sensi-\ntive to the pattern of temporal changes. Fine-tuning allows\nus to correct for the shifts in these temporal changes that\nresult from using an Eventful spatial model.\nCompatibility with spatial redundancy methods.\nWe\nnow provide further discussion regarding the compatibility\nof our method with spatial redundancy approaches. Ab-\nstractly, we can think of spatial redundancy methods as\nsummarizing a set of tokens x \u2208 RN\u00d7D using a reduced\nset of tokens \u02c6x \u2208 RM\u00d7D. The simple method in our exper-\niments summarizes tokens using uniform pooling; however,\nwe could also use adaptive pruning or merging.\nAssume we apply a gate to the reduced tokens \u02c6x. The\ngate assumes that the definitions of its input tokens are rela-\ntively stable. This assumption clearly holds for non-reduced\nor uniformly pooled tokens. However, we need to be careful\nwhen applying arbitrary reductions to x.\nFor example, say we have an image containing a region\nof blue sky. An adaptive token merging method might com-\nbine all sky-colored tokens from x into a single token in\n\u02c6x. Assume that on frame t = 1, the first token in \u02c6x repre-\nsents the sky. Ideally, on frame t = 2, the first token in \u02c6x\nshould again represent the sky. Note that this is not a strict\nconstraint \u2013 our gating logic can deal with non-consistent\ndefinitions for a few tokens. However, if the definitions for\nall tokens in \u02c6x completely change between frames, then the\ngate will not be able to keep up (i.e., the number of tokens\nwith significant changes will exceed the policy r-value).\nB. Additional Experiments\nVideo action recognition on Kinetics-400.\nWe eval-\nuate our method on the Kinetics-400 action recognition\ndataset [33]. Kinetics-400 contains over 300k video clips,\neach annotated with one of 400 action categories. We eval-\nuate top-1 accuracy. We use the same ViViT model archi-\ntecture as in our EPIC-Kitchens experiments; the only dif-\nference is the input size (224\u00d7224 rather than 320\u00d7320).\nAs in our EPIC-Kitchens experiments, we fine-tune the\nnon-Eventful temporal model on the outputs of the Eventful\nspatial model. We fine-tune three variants of the model with\nr =24, 48, and 96 (out of a maximum of 197 tokens). We\ntrain for 10 epochs on a subset of the training set containing\n39729 videos. We use the AdamW optimizer [43] with a\nlearning rate of 2\u00d710-6, weight decay of 0.05, and a batch\nsize of 16 videos. We add 50% dropout before the final\nclassification layer.\nTable A shows our results.\nThe accuracy-compute\ntradeoff is generally consistent with our results on EPIC-\nKitchens. For example, with r = 96, we sacrifice 1.48%\naccuracy for a speedup of approximately 2x.\nA threshold policy. We evaluate the ViTDet object detec-\ntion model with a threshold policy. The threshold policy se-\nlects all tokens where the L2 norm of e exceeds a threshold\nh. We test h = 0.2, 1.0, and 5.0. See Table B for results.\nThe accuracy-compute tradeoff for the threshold policy is\ngenerally worse than for the top-r policy.\nFor example,\ncompare threshold h = 5.0 with r = 512 in Table C. This\nis likely due to the use of a constant threshold for all gates\n(we would ideally use a unique threshold for each gate).\nTable A. Kinetics-400 video action recognition.\nResults for\nKinetics-400 action recognition using the ViViT model. We re-\nport the total TFlops per video (spatial + temporal sub-models).\nVariant\nr\nAccuracy (%)\nTFlops\nBase model\n\u2013\n79.06\n3.360\nTemporal\n96\n77.62\n1.814\nTemporal\n48\n75.88\n1.016\nTemporal\n24\n75.16\n0.618\nC. Experiment Details\nFine-tuning ViTDet for VID. We initialize our model us-\ning COCO [41] pre-trained weights, and then trained on\na combination of the ImageNet VID and ImageNet DET\ndatasets, following common protocols in [10, 74]. We se-\nlect images from the DET dataset that are of of the same 30\nclasses as in the VID dataset. The training uses a batch size\nof 8, a maximum input resolution of 1024\u00d71024, an initial\nlearning rate of 10-4, and a weight decay of 0.1. We use the\nAdamW optimizer [43] with linear warmup for a total of 5\nepochs, with 10x learning rate decay from the 3rd epoch.\nFine-tuning the ViViT temporal model. We fine-tune the\ntemporal sub-model for 5 epochs. We use the AdamW op-\ntimizer [43] with a learning rate of 10-5, weight decay of\n0.05, and a batch size of 8 videos. We add 50% dropout\nbefore the final classification layer.\nArithmetic precision. We compute the product Av at half\nprecision in the global self-attention operators of the Event-\nful model. Using half precision reduces the model\u2019s com-\nputational cost and memory footprint and has a negligible\neffect on accuracy. When evaluating runtimes, we also com-\npute Av at half precision in the base model (this ensures a\nfair comparison).\nRuntime experiments. For ViTDet, we evaluate CPU run-\ntimes using one random video from VID (ID 00023010,\ncontaining 242 frames). On the GPU, we use 5 random\nvideos. For ViViT, we evaluate CPU runtimes using 5 ran-\ndom videos from EPIC-Kitchens. On the GPU, we use 100\nrandom videos. We use a consistent random seed across all\nexperiment runs.\nOperation counting. Our GFlop counts include the fol-\nlowing types of operations: linear transforms, matrix multi-\nplications, einsum operations (used in relative position em-\nbeddings), and additions. We count a multiply-accumulate\nas a single operation. In Eventful Transformers, we ad-\nditionally count operations required for updating the gate\n(additions and subtractions) and the extra additions in the\nsparse attention-value update. We only report operations in\nthe Transformer backbones (e.g., we do not count anything\nin the object detection head).\nTable B. A threshold policy. Results for a threshold policy with\nthe 1024-resolution ViTDet model.\nThe policy selects tokens\nwhere the error e exceeds a threshold h.\nVariant\nh\nmAP50 (%)\nGFlops\nBase model\n\u2013\n82.93\n467.4\nTemporal\n0.2\n83.00\n431.8\nTemporal\n1.0\n82.75\n294.1\nTemporal\n5.0\n78.11\n133.5\nD. Result Tables\nIn this section, we provide tables of results for experi-\nments in the main paper. Table C corresponds to Figures 7\nand 8, and Table D corresponds to Figure 9. Table E shows\nspatial redundancy results for the 672-resolution ViTDet\nmodel (the 1024-resolution results are in Table 1).\nTable C. Video object detection results. Results for video object\ndetection on VID using the ViTDet model. This table corresponds\nto Figures 7 and 8 in the main paper.\nSize\nVariant\nr\nmAP50 (%)\nGFlops\n1024\nBase model\n\u2013\n82.93\n467.4\n1024\nOur method\n2048\n82.94\n294.9\n1024\nOur method\n1536\n82.79\n225.9\n1024\nOur method\n1024\n82.00\n156.8\n1024\nOur method\n768\n81.25\n122.3\n1024\nOur method\n512\n79.38\n87.8\n1024\nOur method\n256\n73.29\n53.3\n1024\nToken-wise only\n2048\n82.97\n294.1\n1024\nToken-wise only\n1536\n82.93\n250.7\n1024\nToken-wise only\n1024\n82.58\n207.3\n1024\nToken-wise only\n768\n82.08\n185.7\n1024\nToken-wise only\n512\n81.11\n164.0\n1024\nToken-wise only\n256\n76.60\n142.3\n1024\nSTGT\n2048\n82.92\n294.1\n1024\nSTGT\n1536\n82.60\n250.7\n1024\nSTGT\n1024\n81.25\n207.3\n1024\nSTGT\n768\n79.81\n185.7\n1024\nSTGT\n512\n76.70\n164.0\n1024\nSTGT\n256\n68.73\n142.3\n672\nBase model\n\u2013\n82.28\n174.5\n672\nOur method\n1024\n82.23\n115.1\n672\nOur method\n768\n82.21\n87.9\n672\nOur method\n512\n81.84\n60.7\n672\nOur method\n384\n81.43\n47.1\n672\nOur method\n256\n80.16\n33.5\n672\nOur method\n128\n75.19\n19.9\n672\nToken-wise only\n1024\n82.28\n111.9\n672\nToken-wise only\n768\n82.25\n90.2\n672\nToken-wise only\n512\n82.01\n68.5\n672\nToken-wise only\n384\n81.64\n57.7\n672\nToken-wise only\n256\n80.76\n46.8\n672\nToken-wise only\n128\n76.96\n36.0\n672\nSTGT\n1024\n82.28\n111.9\n672\nSTGT\n768\n81.95\n90.2\n672\nSTGT\n512\n80.45\n68.5\n672\nSTGT\n384\n78.71\n57.7\n672\nSTGT\n256\n75.57\n46.8\n672\nSTGT\n128\n68.13\n36.0\nTable D. Video action recognition results. Results for video ac-\ntion recognition on EPIC-Kitchens using the ViViT model. This\ntable corresponds to Figure 9 in the main paper.\nVariant\nTuned r\nTested r\nAccuracy (%)\nTFlops\nBase model\n\u2013\n\u2013\n67.14\n7.12\nTemporal\n200\n280\n66.77\n5.49\nTemporal\n200\n240\n66.53\n4.77\nTemporal\n200\n200\n66.02\n4.05\nTemporal\n200\n160\n64.72\n3.33\nTemporal\n200\n120\n62.23\n2.62\nTemporal\n100\n140\n65.52\n2.98\nTemporal\n100\n120\n64.51\n2.62\nTemporal\n100\n100\n62.91\n2.26\nTemporal\n100\n80\n60.76\n1.90\nTemporal\n100\n60\n59.13\n1.54\nTemporal\n50\n70\n61.27\n1.72\nTemporal\n50\n60\n60.60\n1.54\nTemporal\n50\n50\n59.91\n1.36\nTemporal\n50\n40\n58.90\n1.18\nTemporal\n50\n30\n58.05\n1.00\nTable E. Adding spatial redundancy to 672-resolution ViTDet.\nResults for adding spatial redundancy to the 672-resolution ViT-\nDet model. 1024-resolution results are in the main paper.\nVariant\nr\nmAP50 (%)\nGFlops\nBase model\n\u2013\n82.28\n174.5\nSpatial\n\u2013\n79.86\n159.7\nSpatiotemporal\n1024\n79.85\n98.2\nSpatiotemporal\n768\n79.81\n75.5\nSpatiotemporal\n512\n79.47\n52.8\nSpatiotemporal\n384\n79.02\n41.4\nSpatiotemporal\n256\n77.90\n29.8\nSpatiotemporal\n128\n73.40\n18.0\n"
  },
  {
    "title": "Relighting Neural Radiance Fields with Shadow and Highlight Hints",
    "link": "https://arxiv.org/pdf/2308.13404.pdf",
    "upvote": "7",
    "text": "Relighting Neural Radiance Fields with\nShadow and Highlight Hints\nChong Zeng\u2217\nState Key Lab of CAD and CG,\nZhejiang University\nHangzhou, China\nchongzeng2000@gmail.com\nGuojun Chen\nMicrosoft Research Asia\nBeijing, China\nguoch@microsoft.com\nYue Dong\nMicrosoft Research Asia\nBeijing, China\nyuedong@microsoft.com\nPieter Peers\nCollege of William & Mary\nWilliamsburg, USA\nppeers@siggraph.org\nHongzhi Wu\nState Key Lab of CAD and CG,\nZhejiang University\nHangzhou, China\nhwu@acm.org\nXin Tong\nMicrosoft Research Asia\nBeijing, China\nxtong@microsoft.com\nFigure 1: Free viewpoint relighting of neural radiance fields trained on 500\u22121,000 unstructured photographs per scene captured\nwith a handheld setup.\nABSTRACT\nThis paper presents a novel neural implicit radiance representation\nfor free viewpoint relighting from a small set of unstructured pho-\ntographs of an object lit by a moving point light source different\nfrom the view position. We express the shape as a signed distance\nfunction modeled by a multi layer perceptron. In contrast to prior\nrelightable implicit neural representations, we do not disentangle\nthe different light transport components, but model both the local\nand global light transport at each point by a second multi layer\nperceptron that, in addition, to density features, the current posi-\ntion, the normal (from the signed distance function), view direction,\nand light position, also takes shadow and highlight hints to aid\nthe network in modeling the corresponding high frequency light\ntransport effects. These hints are provided as a suggestion, and\nwe leave it up to the network to decide how to incorporate these\nin the final relit result. We demonstrate and validate our neural\nimplicit representation on synthetic and real scenes exhibiting a\nwide variety of shapes, material properties, and global illumination\nlight transport.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Image-based rendering; Re-\nflectance modeling.\nKEYWORDS\nRelighting, Free-viewpoint, Neural Implicit Modeling\n\u2217Work done during internship at Microsoft Research Asia.\nACM Reference Format:\nChong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, and Xin\nTong. 2023. Relighting Neural Radiance Fields with Shadow and Highlight\nHints. In Special Interest Group on Computer Graphics and Interactive Tech-\nniques Conference Conference Proceedings (SIGGRAPH \u201923 Conference Pro-\nceedings), August 6\u201310, 2023, Los Angeles, CA, USA. ACM, New York, NY,\nUSA, 12 pages. https://doi.org/10.1145/3588432.3591482\n1\nINTRODUCTION\nThe appearance of real-world objects is the result of complex light\ntransport interactions between the lighting and the object\u2019s intricate\ngeometry and associated material properties. Digitally reproducing\nthe appearance of real-world objects and scenes has been a long-\nstanding goal in computer graphics and computer vision. Inverse\nrendering methods attempt to undo the complex light transport to\ndetermine a sparse set of model parameters that, together with the\nchosen models, replicates the appearance when rendered. However,\nteasing apart the different entangled components is ill-posed and\noften leads to ambiguities. Furthermore, inaccuracies in one model\ncan adversely affect the accuracy at which other components can be\ndisentangled, thus requiring strong regularization and assumptions.\nIn this paper we present a novel, NeRF-inspired [Mildenhall et al.\n2020], neural implicit radiance representation for free viewpoint\nrelighting of general objects and scenes. Instead of using analytical\nreflectance models and inverse rendering of the neural implicit rep-\nresentations, we follow a data-driven approach and refrain from de-\ncomposing the appearance in different light transport components.\nTherefore, unlike the majority of prior work in relighting neural\nimplicit representations [Boss et al. 2021a, 2022; Kuang et al. 2022;\narXiv:2308.13404v1  [cs.CV]  25 Aug 2023\nZeng et al.\nSrinivasan et al. 2021; Zheng et al. 2021], we relax and enrich the\nlighting information embedded in handheld captured photographs\nof the object by illuminating each view from a random point light\nposition. This provides us with a broader unstructured sampling of\nthe space of appearance changes of an object, while retaining the\nconvenience of handheld acquisition. Furthermore, to improve the\nreproduction quality of difficult to learn components, we provide\nshadow and highlight hints to the neural radiance representation.\nCritically, we do not impose how these hints are combined with the\nestimated radiance (e.g., shadow mapping by multiplying with the\nlight visibility), but instead leave it up to the neural representation\nto decide how to incorporate these hints in the final result.\nOur hint-driven implicit neural representation is easy to im-\nplement, and it requires an order of magnitude less photographs\nthan prior relighting methods that have similar capabilities, and an\nequal number of photographs compared to state-of-the-art methods\nthat offer less flexibility in the shape and/or materials that can be\nmodeled. Compared to fixed lighting implicit representations such\nas NeRF [Mildenhall et al. 2020], we only require a factor of five\ntimes more photographs and twice the render cost while gaining\nrelightability. We demonstrate the effectiveness and validate the\nrobustness of our representation on a variety of challenging syn-\nthetic and real objects (e.g., Figure 1) containing a wide range of\nmaterials (e.g., subsurface scattering, rough specular materials, etc.)\nvariations in shape complexity (e.g., thin features, ill-defined furry\nshapes, etc.) and global light transport effects (e.g., interreflections,\ncomplex shadowing, etc.).\n2\nRELATED WORK\nWe focus the discussion of related work on seminal and recent\nwork in image-based relighting, inverse rendering, and relight-\ning neural implicit representations. For an in-depth overview we\nrefer to recent surveys in neural rendering [Tewari et al. 2022],\n(re)lighting [Einabadi et al. 2021], and appearance modeling [Dong\n2019].\nImage-based Relighting. The staggering advances in machine\nlearning in the last decade have also had a profound effect on image-\nbased relighting [Debevec et al. 2000], enabling new capabilities\nand improving quality [Bemana et al. 2020; Ren et al. 2015; Xu\net al. 2018]. Deep learning has subsequently been applied to more\nspecialized relighting tasks for portraits [Bi et al. 2021; Meka et al.\n2019; Pandey et al. 2021; Sun et al. 2019, 2020], full bodies [Guo\net al. 2019; Kanamori and Endo 2018; Meka et al. 2020; Yeh et al.\n2022; Zhang et al. 2021a], and outdoor scenes [Griffiths et al. 2022;\nMeshry et al. 2019; Philip et al. 2019]. It is unclear how to extend\nthese methods to handle scenes that contain objects with ill-defined\nshapes (e.g., fur) and translucent and specular materials.\nOur method can also be seen as a free-viewpoint relighting\nmethod that leverages highlight and shadow hints to help model\nthese challenging effects. Philip et al. [2019] follow a deep shad-\ning approach [Nalbach et al. 2017] for relighting, mostly diffuse,\noutdoor scenes under a simplified sun+cloud lighting model. Re-\nlit images are created in a two stage process, where an input and\noutput shadow map computed from a proxy geometry is refined,\nand subsequently used, together with additional render buffers,\nas input to a relighting network. Zhang et al. [2021a] introduce\na semi-parametric model with residual learning that leverages a\ndiffuse parametric model (i.e., radiance hint) on a rough geometry,\nand a learned representation that models non-diffuse and global\nlight transport embedded in texture space. To accurately model the\nnon-diffuse effects, Zhang et al. require a large number (\u223c 8,000)\nof structured photographs captured with a light stage. Deferred\nNeural Relighting [Gao et al. 2020] is closest to our method in terms\nof capabilities; it can perform free-viewpoint relighting on objects\nwith ill-defined shape with full global illumination effects and com-\nplex light-matter interactions (including subsurface scattering and\nfur). Similar to Zhang et al. [2021a], Gao et al. embed learned fea-\ntures in the texture space of a rough geometry that are projected to\nthe target view and multiplied with radiance cues. These radiance\ncues are visualizations of the rough geometry with different BRDFs\n(i.e., diffuse and glossy BRDFs with 4 different roughnesses) under\nthe target lighting with global illumination. The resulting images\nare then used as guidance hints for a neural renderer trained per\nscene from a large number (\u223c10,000) of unstructured photographs\nof the target scene for random point light-viewpoint combinations\nto reproduce the reference appearance. Philip et al. [2021] also use\nradiance hints (limited to diffuse and mirror radiance) to guide a\nneural renderer. However, unlike Zhang et al. and Gao et al. , they\npretrain a neural renderer that does not require per-scene fine-\ntuning, and that takes radiance cues for both the input and output\nconditions. Philip et al. require about the same number as input\nimages as our method, albeit lit by a single fixed natural lighting\nconditions and limited to scenes with hard surfaces and BRDF-like\nmaterials. All four methods rely on multi-view stereo which can\nfail for complex scenes. In contrast our method employs a robust\nneural implicit representation. Furthermore, all four methods rely\non an image-space neural renderer to produce the final relit image.\nIn contrast, our method provides the hints during volume rendering\nof the neural implicit representation, and thus it is independent\nof view-dependent image contexts. Our method can relight scenes\nwith the same complexity as Gao et al. [2020] while only using a\nsimilar number of input photographs as Philip et al. [2021] without\nsacrificing robustness.\nModel-based Inverse Rendering. An alternative to data-driven re-\nlighting is inverse rendering (a.k.a. analysis-by-synthesis) where a\nset of trial model parameters are optimized based on the difference\nbetween the rendered model parameters and reference photographs.\nInverse rendering at its core is a complex non-linear optimization\nproblem. Recent advances in differentiable rendering [Li et al. 2018;\nLoper and Black 2014; Nimier-David et al. 2019; Xing et al. 2022]\nhave enabled more robust inverse rendering for more complex\nscenes and capture conditions. BID-R++ [Chen et al. 2021] com-\nbines differentiable ray tracing and rasterization to model spatially\nvarying reflectance parameters and spherical Gaussian lighting for\na known triangle mesh. Munkberg et al. [2022] alternate between\noptimizing an implicit shape representation (i.e., a signed distance\nfield), and reflectance and lighting defined on a triangle mesh. Has-\nselgren et al. [2022] extend the work of Munkberg et al. [2022] with\na differentiable Monte Carlo renderer to handle area light sources,\nand embed a denoiser to mitigate the adverse effects of Monte\nCarlo noise on the gradient computation to drive the non-linear\noptimizer. Similarly, Fujun et al. [2021] also employ a differentiable\nRelighting Neural Radiance Fields with Shadow and Highlight Hints\nMonte Carlo renderer for estimating shape and spatially-varying\nreflectance from a small set of colocated view/light photographs.\nAll of these methods focus on direct lighting only and can produce\nsuboptimal results for objects or scenes with strong interreflections.\nA notable exception is the method of Cai et al. [2022] that combines\nexplicit and implicit geometries and demonstrates inverse render-\ning under known lighting on a wide range of opaque objects while\ntaking indirect lighting in account. All of the above methods eventu-\nally express the shape as a triangle mesh, limiting their applicability\nto objects with well defined surfaces. Furthermore, the accuracy of\nthese methods is inherently limited by the representational power\nof the underlying BRDF and lighting models.\nNeural Implicit Representations. A major challenge in inverse\nrendering with triangle meshes is to efficiently deal with changes\nin topology during optimization. An alternative to triangle mesh\nrepresentations is to use a volumetric representation where each\nvoxel contains an opacity/density estimate and a description of the\nreflectance properties. While agnostic to topology changes, voxel\ngrids are memory intensive and, even with grid warping [Bi et al.\n2020], fine-scale geometrical details are difficult to model.\nTo avoid the inherent memory overhead of voxel grids, NeRF\n[Mildenhall et al. 2020] models the continuous volumetric density\nand spatially varying color with two multi layer perceptrons (MLPs)\nparameterized by position (and also view direction for color). The\nMLPs in NeRF are trained per scene such that the accumulated den-\nsity and color ray marched along a view ray matches the observed\nradiance in reference photographs. NeRF has been shown to be\nexceptionally effective in modeling the outgoing radiance field of a\nwide range of object types, including those with ill-defined shapes\nand complex materials. One of the main limitations of NeRF is that\nthe illumination present at capture-time is baked into the model.\nSeveral methods have been introduced to support post-capture\nrelighting under a restricted lighting model [Li et al. 2022; Martin-\nBrualla et al. 2021], or by altering the color MLP to produce the\nparameters to drive an analytical model of the appearance of ob-\njects [Boss et al. 2021a, 2022, 2021b; Kuang et al. 2022; Srinivasan\net al. 2021; Yao et al. 2022; Zhang et al. 2021c], participating me-\ndia [Zheng et al. 2021], or even whole outdoor scenes [Rudnev et al.\n2022].\nDue to the high computational cost of ray marching secondary\nrays, na\u00efvely computing shadows and indirect lighting is impracti-\ncal. Zhang et al. [2021c], Li et al. [2022], and Yang et al. [2022] avoid\ntracing shadow rays by learning an additional MLP to model the\nratio of light occlusion. However, all three methods ignore indirect\nlighting. Zheng et al. [2021] model the indirect lighting inside a\nparticipating media using an MLP that returns the coefficients of a 5-\nband expansion. NeILF [Yao et al. 2022] embeds the indirect lighting\nand shadows in a (learned) 5D incident light field for a scene with\nknown geometry. NeRV [Srinivasan et al. 2021] modifies the color\nMLP to output BRDF parameters and a visibility field that models\nthe distance to the nearest \u2019hard surface\u2019 and lighting visibility. The\nvisibility field allows them to bypass the expensive ray marching\nstep for shadow computation and one-bounce indirect illumination.\nA disadvantage of these solutions is that they do not guarantee that\nthe estimated density field and the occlusions are coupled. In con-\ntrast, our method directly ties occlusions to the estimated implicit\ngeometry reproducing more faithful shadows. Furthermore, these\nmethods rely on BRDFs to model the surface reflectance, precluding\nscenes with complex light-matter interactions.\nNeLF [Sun et al. 2021] aims to relight human faces, and thus\naccurately reproducing subsurface scattering is critical. Therefore,\nSun et al. characterize the radiance and global light transport by\nan MLP. We also leverage an MLP to model local and global light\ntransport. A key difference is that our method parameterizes this\nMLP in terms of view and light directions, whereas NeLF directly\noutputs a full light transport vector and compute a relit color via an\ninner-product with the lighting. While better suited for relighting\nwith natural lighting, NeLF is designed for relighting human faces\nwhich only exhibit limited variations in shape and reflectance.\nSimilar in spirit to our method, Lyu et al. [2022] model light trans-\nport using an MLP, named a Neural Radiance Transfer Field (NRTF).\nHowever, unlike us, Lyu et al. train the MLP on synthetic training\ndata generated from a rough BRDF approximation obtained through\nphysically based inverse rendering on a triangle mesh extracted\nfrom a neural signed distance field [Wang et al. 2021] computed\nfrom unstructured observations of the scene under static natural\nlighting. To correct the errors due the rough BRDF approximation,\na final refinement step of the MLP is performed using the captured\nphotographs. Similar to Lyu et al. we also use an MLP to model\nlight transport, including indirect lighting. However, unlike Lyu et\nal. we do not rely solely on an MLP to model high frequency light\ntransport effects such as light occlusions and specular highlights.\nInstead we provide shadow and highlight hints to the radiance\nnetwork and let the training process discover how to best leverage\nthese hints. Furthermore, we rely on a neural representation for\nshape jointly optimized with the radiance, allowing us to capture\nscenes with ill-defined geometry. In contrast, Lyu et al. optimize\nshape (converted to a triangle mesh) and radiance separately, mak-\ning their method sensitive to shape errors and restricted to objects\nwith a well-defined shape.\nAn alternative to using an implicit neural density field, is to\nmodel the shape via a signed distance field (SDF). Similar to the\nmajority of NeRF-based methods, PhySG [Zhang et al. 2021b] and\nIRON [Zhang et al. 2022a] also rely on an MLP to represent volumet-\nric BRDF parameters. However, due to the high computational cost,\nthese methods do not take shadowing or indirect lighting in account.\nZhang et al. [2022b] model indirect lighting separately, and train\nan additional incident light field MLP using the incident lighting\ncomputed at each point via ray casting the SDF geometry. While\nour method also builds on a neural implicit representation [Wang\net al. 2021], our method does not rely on an underlying paramet-\nric BRDF model, but instead models the full light transport via an\nMLP. Furthermore, we do not rely on an MLP decoupled from the\nestimated geometry to estimate shadowing, but instead accumulate\nlight occlusion along a single shadow ray per view ray, ensuring\nconsistency between the shadows and the estimated geometry.\n3\nMETHOD\nOur goal is to extend neural implicit representations such as NeRF\n[Mildenhall et al. 2020] to model variations in lighting. NeRF has\nproven to be exceptionally efficient for viewpoint interpolation.\nIn contrast to ray tracing with solid surfaces, NeRF relies on ray\nZeng et al.\nCaptured images\nCapturing process\nLoss\nFeature\nDensity network\nRelightable radiance network\nPosition\nLight position\nView direction\nMLP\nHighlight hint\nShadow hint\nColor\nE\nE\nE\nE\nE\nInput data\nOutput\nIntermediate data\nTrainable network\nE\nFrequency encoding\nNormal\nSDF\nMLP\nVolume Rendering\n\u2205\nDensity mapping\n\u2205\n\u2026\nFigure 2: Overview: our neural implicit radiance representation is trained on unstructured photographs of the scene captured\nfrom different viewpoints and lit from different point light positions. The neural implicit radiance representation consists of\ntwo multi layer perceptron (MLP) networks for modeling the density field and for modeling the light transport. The MLP for\nmodeling the density takes as input the position, and outputs the signed distance function of the shape and a feature vector\nthat together with the current position, the normal extracted from the SDF, the view direction, the light source position, and\nthe light transport hints, are passed into the radiance MLP that then computes the view and lighting dependent radiance.\nmarching through the volume, requiring at least an order of mag-\nnitude more computations. Not only does this ray marching cost\naffect rendering, it also leads to a prohibitively large training cost\nwhen secondary rays (e.g., shadows and indirect lighting) are con-\nsidered. Instead of building our method on NeRF, we opt for using\nNeuS [Wang et al. 2021], a neural implicit signed distance field\nrepresentation, as the basis for our method. Although NeuS does\nnot speed up ray marching, it provides an unbiased depth estimate\nwhich we will leverage in subsection 3.2 for reducing the number\nof shadow rays.\nFollowing prior work, our neural implicit radiance representation\nrelies on two multi layer perceptrons (MLPs) for modeling the\ndensity field (following NeuS) and for modeling the (direct and\nindirect) radiance based on the current position, the normal derived\nfrom the density field, the view direction, the point light position,\nand the features provided by the density network. In addition, we\nalso provide light transport hints to the relightable radiance MLP to\nimprove the reproduction quality of difficult to model effects such\nas shadows and highlights. Figure 2 summarizes our architecture.\nTo train our neural implicit relightable radiance representation,\nwe require observations of the target scene seen from different\nviewpoints and lit from different point light positions. It is essen-\ntial that these observations include occlusions and interreflections.\nColocated lighting (e.g., as in [Luan et al. 2021; Nam et al. 2018])\ndoes not exhibit visible shadows and is therefore not suited. Instead\nwe follow the acquisition process of Deferred Neural Lighting [Gao\net al. 2020] and capture the scene from different viewpoints with\na handheld camera while lighting the scene with a flash light of a\nsecond camera from a different direction.\nWe opt for parameterizing the radiance function with respect\nto a point light as the basis for relighting as this better reflects\nthe physical capture process. A common approximation in prior\nreligting work that relies on active illumination (e.g., Light Stage) is\nto ignore the divergence of incident lighting due to the finite light\nsource distance, and parameterize the reflectance field in terms\nlighting directions only. Similarly, we can also approximate distant\nlighting with point lighting defined by projecting the light direction\nonto a large sphere with a radius equal to the capture distance.\n3.1\nRepresentation\nDensity Network. Our neural implicit geometry representation\nfollows NeuS [Wang et al. 2021] which uses an MLP to encode a\nSigned Distance Function (SDF) \ud835\udc53 (p) from which the density func-\ntion is derived using a probability density function \ud835\udf19\ud835\udc60 (\ud835\udc53 (p)). This\nprobability density function is designed to ensure that for opaque\nobjects the zero-level set of the SDF corresponds to the surface.\nThe width of the probability distribution models the uncertainty\nof the surface location. We follow exactly the same architecture\nfor the density MLP as in NeuS: 8 hidden layers with 256 nodes\nusing a Softplus activation and a skip connection between the input\nand the 4th layer. The input (i.e., current position along a ray) is\naugmented using a frequency encoding with 6 bands. In addition,\nwe also concatenate the original input signal to the encoding. The\nresulting output from the density network is the SDF at p as well\nas a latent vector that encodes position dependent features.\nRelightable Radiance Network. Analogous to the color MLP in\nNeRF and NeuS that at each volumetric position evaluates the\nview-dependent color, we introduce a relightable radiance MLP\nthat at each volumetric position evaluates the view and lighting\ndependent (direct and indirect) light transport. We follow a similar\narchitecture as NeRF/NeuS\u2019 color MLP and extend it by taking the\nposition dependent feature vector produced by the density MLP,\nthe normal derived from the SDF, the current position, the view\ndirection, and the point light position as input. Given this input,\nthe radiance MLP outputs the resulting radiance which includes all\nlight transport effects such as occlusions and interreflections. We\nassume a white light source color; colored lighting can be achieved\nby scaling the radiance with the light source color (i.e., linearity of\nlight transport).\nGiven the output from the density network \ud835\udc53 as well as the\noutput from the radiance network \ud835\udc60, the color \ud835\udc36 along a view ray\nstarting at the camera position o in a direction v is given by:\n\ud835\udc36(o, v) =\n\u222b \u221e\n0\n\ud835\udc64(\ud835\udc61)\ud835\udc60(p, n, v, l, \u00af\ud835\udc53 , \u0398) d\ud835\udc61,\n(1)\nwhere the sample position along the view ray is p = o +\ud835\udc61v at depth\n\ud835\udc61, n is the normal computed as the normalized SDF gradient:\nn = \u2207\ud835\udc53 (p)/||\u2207\ud835\udc53 (p)||,\n(2)\nRelighting Neural Radiance Fields with Shadow and Highlight Hints\nv is the view direction, l is the point light position, \u00af\ud835\udc53 the corre-\nsponding feature vector from the density MLP, and \u0398 is a set of\nadditional hints provided to the radiance network (described in sub-\nsection 3.2). Analogous to NeuS, the view direction, light position,\nand hints are all frequency encoded with 4 bands. Finally, \ud835\udc64(\ud835\udc61) is\nthe unbiased density weight [Wang et al. 2021] computed by:\n\ud835\udc64(\ud835\udc61)\n=\n\ud835\udc47 (\ud835\udc61)\ud835\udf0c(\ud835\udc61),\n(3)\n\ud835\udc47 (\ud835\udc61)\n=\nexp\n\u0012\n\u2212\n\u222b \ud835\udc61\n0\n\ud835\udf0c(\ud835\udc62) d\ud835\udc62\n\u0013\n,\n(4)\n\ud835\udf0c(\ud835\udc61)\n=\nmax\n d\u03a6\ud835\udc60\nd\ud835\udc61 (\ud835\udc53 (\ud835\udc61))\n\u03a6\ud835\udc60 (\ud835\udc53 (\ud835\udc61)) , 0\n!\n,\n(5)\nwith \ud835\udc47 the transmittance over opacity \ud835\udf0c, \u03a6\ud835\udc60 the CDF of the PDF\n\ud835\udf19\ud835\udc60 used to compute the density from the SDF \ud835\udc53 . To speed up the\ncomputation of the color, the integral in Equation 1 is computed by\nimportance sampling the density field along the view ray.\nIn the spirit of image-based relighting, we opt to have the re-\nlightable radiance MLP network include global light transport ef-\nfects such as interreflections and occlusions. While MLPs are in\ntheory universal approximators, some light transport components\nare easier to learn (e.g., diffuse reflections) than others. Especially\nhigh frequency light transport components such as shadows and\nspecular highlights pose a problem. At the same time, shadows\nand specular highlights are highly correlated with the geometry\nof the scene and thus the density field. To leverage this embedded\nknowledge, we provide the relightable radiance MLP with addi-\ntional shadow and highlight hints.\n3.2\nLight Transport Hints\nShadow Hints. While the relightable radiance network is able to\nroughly model the effects of light source occlusion, the resulting\nshadows typically lack sharpness and detail. Yet, light source oc-\nclusion can be relatively easily evaluated by collecting the density\nalong a shadow ray towards the light source. While this process is\nrelatively cheap for a single shadow ray, performing a secondary\nray march for each primary ray\u2019s sampled position increases the\ncomputation cost by an order of magnitude, quickly becoming too\nexpensive for practical training. However, we observe that for most\nprimary rays, the ray samples are closely packed together around\nthe zero level-set in the SDF due to the importance sampling of\nthe density along the view ray. Hence, we propose to approximate\nlight source visibility by shooting a single shadow ray at the zero\nlevel-set, and use the same light source visibility for each sample\nalong the view ray. To determine the depth of the zero level-set, we\ncompute the density weighted depth along the view ray:\n\ud835\udc37(o, v) =\n\u222b \u221e\n0\n\ud835\udc64(p)\ud835\udc61 d\ud835\udc61.\n(6)\nWhile for an opaque surface a single shadow ray is sufficient,\nfor non-opaque or ill-defined surfaces a single shadow ray offers a\npoor estimate of the light occlusion. Furthermore, using the shadow\ninformation as a hard mask, ignores the effects of indirect lighting.\nWe therefore provide the shadow information as a additional input\nto the radiance network, allowing the network learn whether to\ninclude or ignore the shadowing information as well as blend any\nindirect lighting in the shadow regions.\nHighlight Hints. Similar to shadows, specular highlights are spar-\nsely distributed high frequency light transport effects. Inspired\nby Gao et al. [2020], we provide specular highlight hints to the\nradiance network by evaluating 4 microfacet BRDFs with a GGX\ndistribution [Walter et al. 2007] with roughness parameters {0.02,\n0.05, 0.13, 0.34}. Unlike Gao et al. , we compute the highlight hints\nusing local shading which only depends on the surface normal\ncomputed from the SDF (Equation 2), and pass it to the radiance\nMLP as an additional input. Similar to shadow hints, we compute\none highlight hint per view ray and reused it for all samples along\nthe view ray.\n3.3\nLoss & Training\nWe jointly train the density and radiance network using an image\nreconstruction loss L\ud835\udc50 and an SDF regularization loss L\ud835\udc52. The im-\nage reconstruction loss is defined as the \ud835\udc3f1 distance between the\nobservation \u00af\ud835\udc36(o, v) and the corresponding estimated color \ud835\udc36(o, v)\ncomputed using Equation 1: L\ud835\udc50 = || \u00af\ud835\udc36 \u2212 \ud835\udc36||1, for a random sam-\npling of pixels (and thus view rays) in the captured training images\n(subsection 3.4). Furthermore, we follow NeuS, and regularize the\ndensity MLP with the Eikonal loss [Gropp et al. 2020] to ensure a\nvalid SDF: L\ud835\udc52 = (||\u2207\ud835\udc53 (p)||2 \u22121)2. For computational efficiency, we\ndo not back-propagate gradients from the shadow and highlight\nhints.\n3.4\nData Acquisition\nTraining the implicit representation requires observations of the\nscene viewed from random viewpoints and lit from a different\nrandom light position such that shadows and interreflections are in-\ncluded. We follow the procedure from Gao et al. [2020]: a handheld\ncamera is used to capture photographs of the scene from random\nviewpoints while a second camera captures the scene with its colo-\ncated flash light enabled. The images from the second camera are\nonly used to calibrate the light source position. To aid camera cali-\nbration, the scene is placed on a checkerboard pattern.\nAll examples in this paper are captured with a Sony A7II as the\nprimary camera, and an iPhone 13 Pro as the secondary camera.\nThe acquisition process takes approximately 10 minutes; the main\nbottleneck in acquisition is moving the cameras around the scene.\nIn practice we capture a video sequence from each camera and\nrandomly select 500\u22121,000 frames as our training data. The video\nis captured using S-log encoding to minimize overexposure.\nFor the synthetic scenes, we simulate the acquisition process by\nrandomly sampling view and light positions on the upper hemi-\nsphere around the scene with a random distance between 2 to 2.5\ntimes the size of the scene. The synthetic scenes are rendered with\nglobal light transport using Blender Cycles.\n3.5\nViewpoint Optimization\nImperfections in camera calibration can cause inaccurate recon-\nstructions of thin geometrical features as well as lead to blurred\nresults. To mitigate the impact of camera calibration errors, we\njointly optimize the viewpoints and the neural representation.\nZeng et al.\nMetallic: 27.79 | 0.9613 | 0.0487\nGlossy-Metal: 30.08 | 0.9722 | 0.0376\nAnisotropic-Metal: 29.07 | 0.9676 | 0.0395\nDiffuse 37.10 | 0.9942 | 0.0136\nPlastic: 34.94 | 0.9885 | 0.0210\nTranslucent: 36.22 | 0.9911 | 0.0172\nFur Ball: 32.18 | 0.9619 | 0.0613\nLayered Woven Ball | 33.52 | 0.9853 | 0.0209\nBasket: 26.84 | 0.9586 | 0.0411\nHotdog: 34.18 | 0.9851 | 0.0246\nLego: 29.93 | 0.9719 | 0.0301\nDrums: 27.92 | 0.9556 | 0.0623\nFigure 3: Qualitative comparison between synthetic scenes relit (right) for a novel viewpoint and lighting direction (not part of\nthe training data) and a rendered reference image (left). For each example we list average PSNR, SSIM, and LPIPS computed\nover a uniform sampling of view and light positions.\nGiven an initial view orientation \ud835\udc450 and view position \ud835\udc610, we\nformulate the refined camera orientation \ud835\udc45 and position \ud835\udc61 as:\n\ud835\udc45\n=\n\u0394\ud835\udc45 \u00b7 \ud835\udc450,\n(7)\n\ud835\udc61\n=\n\u0394\ud835\udc61 + \u0394\ud835\udc45 \u00b7 \ud835\udc610,\n(8)\nwhere \u0394\ud835\udc45 \u2208 SO(3) and \u0394\ud835\udc61 \u2208 R3 are learnable correction transforma-\ntions. During training, we back-propagate, the reconstruction loss,\nin addition to the relightable radiance network, to the correction\ntransformations. We assume that the error on the initial camera cal-\nibration is small, and thus we limit the viewpoint changes by using\na 0.06\u00d7 smaller learning rate for the correction transformations.\n4\nRESULTS\nWe implemented our neural implicit radiance representation in Py-\nTorch [Paszke et al. 2019]. We train each model for 1,000\ud835\udc58 iterations\nusing the Adam optimizer [Kingma and Ba 2015] with \ud835\udefd1 = 0.9 and\n\ud835\udefd2 = 0.999 with 512 samples per iteration randomly drawn from\nthe training images. We follow the same warmup and cosine decay\nlearning rate schedule as in NeuS [Wang et al. 2021]. Training a\nsingle neural implicit radiance representation takes approximate\n20 hours on four Nvidia V100 GPUs.\nWe extensively validate the relighting capabilities of our neural\nimplicit radiance representation on 17 synthetic and 7 captured\nscenes (including 4 from [Gao et al. 2020]), covering a wide range\nof different shapes, materials, and lighting effects.\nSynthetic Scenes. Figure 3 shows relit results of different\nsynthetic scenes. For each example, we list PSNR, SSIM, and\nLPIPS [Zhang et al. 2018] error statistics computed over 100 test\nimages different from the 500 training images. Our main test scene\ncontains a vase and two dice; the scene features a highly concave\nobject (vase) and complex interreflections between the dice. We\ninclude several versions of the main test scene with different mate-\nrial properties: Diffuse, Metallic, Glossy-Metal, Rough-Metal,\nAnisotropic-Metal, Plastic, Glossy-Plastic, Rough-Plastic\nand Translucent; note, some versions are only included in the\nsupplemental material. We also include two versions with modified\ngeometry: Short-Fur and Long-Fur to validate the performance\nof our method on shapes with ill-defined geometry. In addition,\nwe also include a Fur-Ball scene which exhibits even longer fur.\nTo validate the performance of the shadow hints, we also include\nscenes with complex shadows: a Basket scene containing thin\ngeometric features and a Layered Woven Ball which combines\ncomplex visibility and strong interreflections. In addition to these\nspecially engineered scenes to systematically probe the capabili-\nties of our method, we also validate our neural implicit radiance\nrepresentation on commonly used synthetic scenes in neural im-\nplicit modeling: Hotdog, Lego and Drums [Mildenhall et al. 2020].\nBased on the error statistics, we see that the error correlates with\nthe geometric complexity of the scene (vase and dice, Hotdog, and\nLayered Woven Ball perform better than the Fur scenes as well\nas scenes with small details such as the Lego and the Drums scene),\nRelighting Neural Radiance Fields with Shadow and Highlight Hints\nPikachu statue\nPSNR: 35.08\nSSIM: 0.9877\nLPIPS: 0.0359\nCat on decor\nPSNR: 36.39\nSSIM: 0.9850\nLPIPS: 0.0604\nCup and fabric\nPSNR: 38.17\nSSIM: 0.9900\nLPIPS: 0.0355\nFigure 4: Qualitative comparison between captured scenes relit (right) for a novel viewpoint and lighting direction (not part of\nthe training data) and a reference photograph (left). For each example we list average PSNR, SSIM, and LPIPS computed over\nrandomly sampled view and light positions.\nReference\nPSNR | SSIM | LPIPS\nIRON\n19.13 | 0.8736 | 0.1440\nOurs\n26.16 | 0.9516 | 0.05741\nFigure 5: Comparison to inverse rendering results from IRON [Zhang et al. 2022a] (from 500 colocated training images) on\nthe Metallic scene. Our model is evaluated under colocated point lights. IRON is affected by the interreflections and fails to\naccurately reconstruct the geometry.\nReference\nPSNR | SSIM | LPIPS\nNRTF\n22.01 | 0.9008 | 0.1238\nOurs\n26.72 | 0.9602 | 0.05351\nFigure 6: A comparison to Neural Radiance Transfer Fields (NRTF) trained on 500 OLAT reference images and reference\ngeometry. To provide a fair comparison, we also train our network on the same directional OLAT images (without reference\ngeometry) instead of point lighting. NRTF struggles to correctly reproduce shadow boundaries and specular interreflections\n(see zoom-ins).\nReference\nPSNR | SSIM | LPIPS\n[Philip et al. 2021]\nw/ reconstructed geometry\n21.29 | 0.8655 | 0.1290\n[Philip et al. 2021]\nw/ reference geometry\n23.22 | 0.8992 | 0.1054\nOurs\n27.79 | 0.9613 | 0.04873\nFigure 7: Comparison to the pretrained relighting network of Philip et al. [2021] on 500 input images of the Metallic scene\nrendered with the target lighting. Even under these favorable conditions, their method struggles to reproduce the correct\nappearance for novel viewpoints.\nZeng et al.\nand with the material properties (highly specular materials such as\nMetallic and Anisotropic-Metal incur a higher error). Visually,\ndifferences are most visible in specular reflections and for small\ngeometrical details.\nCaptured Scenes. We demonstrate the capabilities of our neu-\nral implicit relighting representation by modeling 3 new scenes\ncaptured with handheld setups (Figure 4). The Pikachu Statue\nscene contains glossy highlights and significant self-occlusion. The\nCat on Decor scene showcases the robustness of our method on\nreal-world objects with ill-defined geometry. The Cup and Fabric\nscene exhibits translucent materials (cup), specular reflections of\nthe balls, and anisotropic reflections on the fabric. We refer to the\nsupplementary material for additional video sequences of these\nscenes visualized for rotating camera and light positions.\nComparisons. Figure 5 compares our method to IRON [Zhang\net al. 2022b], an inverse rendering method that adopts a neural\nrepresentation for geometry as a signed distance field. From these\nresults, we can see that IRON fails to correctly reconstruct the shape\nand reflections in the presence of strong interreflections. In a second\ncomparison (Figure 6), we compare our method to Neural Radiance\nTransfer Fields (NRTF) [Lyu et al. 2022]; we skip the fragile inverse\nrendering step and train NRTF with 500 reference OLAT images and\nthe reference geometry. To provide a fair comparison, we also train\nand evaluate our network under the same directional OLAT images\nby conditioning the radiance network on light direction instead of\npoint light position. From this test we observe that NRTF struggles\nto accurately reproduce shadow edges and specular interreflections,\nas well as that our method can also be successfully trained with di-\nrectional lighting. Figure 7 compares our method to the pre-trained\nneural relighting network of Philip et al. . [2021] on the challenging\nMetallic test scene. Because multiview stereo [Sch\u00f6nberger and\nFrahm 2016] fails for this scene, we input geometry reconstructed\nfrom the NeuS SDF as well as ground truth geometry. Finally, we\nalso render the input images under the reference target lighting;\nour network is trained without access to the target lighting. Even\nunder these favorable conditions, the relighting method of Philip et\nal. struggles to reproduce the correct appearance. Finally, we com-\npare our method to Deferred Neural Lighting [Gao et al. 2020]\n(using their data and trained model). Our method is able to achieve\nsimilar quality results from \u223c500 input images compared to \u223c10,000\ninput images for Deferred Neural Lighting. While visually very\nsimilar, the overall errors of Deferred Neural Lighting are slightly\nlower than with our method. This is mainly due to differences in\nhow both methods handle camera calibrations errors. Deferred\nNeural Lighting tries to minimize the differences for each frame\nseparately, and thus it can embed camera calibration errors in the\nimages. However, this comes at the cost of temporal \u201cshimmering\u201d\nwhen calibration is not perfect. Our method on the other hand, opti-\nmizes the 3D representation, yielding better temporal stability (and\nthus requiring less photographs for view interpolation) at the cost\nof slightly blurring the images in the presence of camera calibration\nerrors.\nTable 1: Ablation results on synthetic scenes\nAblation Variant\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFull hints\n32.02\n0.9727\n0.0401\nw/o highlight hint\n31.96\n0.9724\n0.0407\nw/o shadow hint\n27.67\n0.9572\n0.0610\nw/o any hints\n27.54\n0.9568\n0.0620\n1 basis material\n31.54\n0.9707\n0.0428\n2 basis materials\n31.54\n0.9707\n0.0429\n4 basis materials\n32.02\n0.9727\n0.0401\n8 basis materials\n31.98\n0.9726\n0.0401\n50 training images\n24.29\n0.9335\n0.0706\n100 training images\n27.96\n0.9572\n0.0520\n250 training images\n30.36\n0.9666\n0.0456\n500 training images\n32.02\n0.9727\n0.0401\nTable 2: Ablation results of viewpoint optimization on real\ncaptured scenes\nAblation Variant\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nw/ viewpoint optimization\n34.72\n0.9762\n0.0695\nw/o viewpoint optimization\n33.62\n0.9719\n0.0794\n5\nABLATION STUDIES\nWe perform several ablation experiments (visual and quantitative)\non the synthetic datasets to evaluate the impact of each of the com-\nponents that comprise our neural implicit radiance representation.\nShadow and Highlight Hints. A key contribution is the inclusion\nof shadow and highlight hints in the relightable radiance MLP.\nFigure 9 shows the impact of training without the shadow hint, the\nhighlight hint, or both. Without shadow hints the method fails to\ncorrectly reproduce sharp shadow boundaries on the ground plane.\nThis lack of sharp shadows is also reflected in the quantitative\nerrors summarized in Table 1. Including the highlight hints yield a\nbetter highlight reproduction, e.g., in the mouth of the vase.\nImpact of the Number of Shadow Rays. We currently only use a\nsingle shadow ray to compute the shadow hint. However, we can\nalso shoot multiple shadow rays (by importance sampling points\nalong the view ray) and provide a more accurate hint to the radiance\nnetwork. Figure 10 shows the results of a radiance network trained\nwith 16 shadow rays. While providing a more accurate shadow\nhint, there is marginal benefit at a greatly increased computational\ncost, justifying our choice of a single shadow ray for computing\nthe shadow hint.\nNeuS vs. NeRF Density MLP. While the relightable radiance MLP\nlearns how much to trust the shadow hint (worst case it can com-\npletely ignore unreliable hints), the radiance MLP can in general\nnot reintroduce high-frequency details if it is not included in the\nshadow hints. To obtain a good shadow hint, an accurate depth\nestimate of the mean depth along the view ray is needed. Wang et\nal. [2021] noted that NeRF produces a biased depth estimate, and\nthey introduced NeuS to address this problem. Replacing NeuS by\nNeRF for the density network (Figure 10) leads to poor shadow re-\nproduction due to the adverse impact of the biased depth estimates\non the shadow hints.\nRelighting Neural Radiance Fields with Shadow and Highlight Hints\nReference\nPSNR | SSIM | LPIPS\nDNL\n39.22 | 0.9932 | 0.0184\nOurs\n36.42 | 0.9856 | 0.0399\nReference\nPSNR | SSIM | LPIPS\nDNL\n34.02 | 0.9763 | 0.0550\nOurs\n32.94 | 0.9708 | 0.0791\nPSNR | SSIM | LPIPS\nReference\n35.36 | 0.9730 | 0.0692\nDNL\n33.07 | 0.9695 | 0.0967\nOurs\nPSNR | SSIM | LPIPS\nReference\n32.093 | 0.9469 | 0.1178\nDNL\n30.96 | 0.9445 | 0.1393\nOurs\nFigure 8: Comparison with Deferred Neural Lighting [Gao et al. 2020]. We train our neural implicit radiance representation\nusing only 1/25th (\u223c500) randomly selected frames for Gao et al.\u2019s datasets, while achieving comparable results.\nReference\nOurs\nw/o Highlight Hint\nw/o Shadow Hint\nw/o Any Hints\nTranslucent\nLayered Woven Ball\nFigure 9: Impact of shadow and highlight hints; without the hints the network fails to accurately reproduce the desired effect.\nReference\n16 shadow rays\n1 shadow ray (Ours)\nNeRF 1 shadow ray\nPSNR | SSIM | LPIPS\n28.22 | 0.9667 | 0.0365\n26.84 | 0.9586 | 0.0411\n23.71 | 0.9160 | 0.0733\nFigure 10: Impact of the number of shadow rays and the underlying implicit shape representation demonstrated on the Basket\nscene. Using 16 shadow rays only provides marginal improvements at the cost of significant computation overhead. Using\nNeRF as the basis for the neural implicit shape yields degraded shadow quality due to depth biases.\nImpact of the number of Basis Materials for the Highlight Hints.\nTable 1 shows the results of using 1, 2, 4 and 8 basis materials for\ncomputing the highlight hints. Additional highlights hints improve\nthe results up to a point; when too many hints are provided erro-\nneous correlations can increase the overall error. 4 basis materials\nstrike a good balance between computational cost, network com-\nplexity, and quality.\nImpact of Number of Training Images. Figure 11 and Table 1\ndemonstrate the effect of varying the number of input images from\nZeng et al.\nReference\n50 inputs\n100 inputs\n250 inputs\n500 inputs\nFigure 11: Impact of the number of captured training images. Increasing the number of training images improves the quality.\nThe quality degrades significantly when the number of images is less than 250.\nReference\nPSNR | SSIM | LPIPS\nw/o Viewpoint Optimization\n31.43 | 0.9803 | 0.0375\nw/ Viewpoint Optimization\n35.08 | 0.9877 | 0.0.359\nFigure 12: Effectiveness of Viewpoint Optimization. Using viewpoint optimization greatly enhances the image quality in terms\nof sharpness and detail.\n50, 100, 250 to 500. As expected, more training images improve\nthe results, and with increasing number of images, the increase\nin improvement diminishes. With 250 images we already achieve\nplausible relit results. Decreasing the number of training images\nfurther introduces noticeable appearance differences.\nEffectiveness of Viewpoint Optimization. Figure 12 and Table 2\ndemonstrate the effectiveness of viewpoint optimization on real\ncaptured scenes. While the improvement in quantitative errors is\nlimited, visually we can see that viewpoint optimization signifi-\ncantly enhances reconstruction quality with increased sharpness\nand better preservation of finer details.\n6\nLIMITATIONS\nWhile our neural implicit radiance representation greatly reduces\nthe number of required input images for relighting scenes with com-\nplex shape and materials, it is not without limitations. Currently we\nprovide shadow and highlight hints to help the relightable radiance\nMLP model high frequency light transport effects. However, other\nhigh frequency effects exist. In particular highly specular surfaces\nthat reflect other parts of the scene pose a challenge to the radiance\nnetwork. Na\u00efve inclusion of \u2019reflection hints\u2019 and/or reparameter-\nizations [Verbin et al. 2022] fail to help the network, mainly due\nto the reduced accuracy of the surface normals (needed to predict\nthe reflected direction) for sharp specular materials. Resolving this\nlimitation is a key challenge for future research in neural implicit\nmodeling for image-based relighting.\n7\nCONCLUSION\nIn this paper we presented a novel neural implicit radiance represen-\ntation for free viewpoint relighting from a small set of unstructured\nphotographs. Our representation consists of two MLPs: one for mod-\neling the SDF (analogous to NeuS) and a second MLP for modeling\nthe local and indirect radiance at each point. Key to our method is\nthe inclusion of shadow and highlight hints to aid the relightable\nradiance MLP to model high frequency light transport effects. Our\nmethod is able to produce relit results from just \u223c500 photographs\nof the scene; a saving of one to two order of magnitude compared\nto prior work with similar capabilities.\nACKNOWLEDGMENTS\nPieter Peers was supported in part by NSF grant IIS-1909028. Chong\nZeng and Hongzhi Wu were partially supported by NSF China\n(62022072 & 62227806), Zhejiang Provincial Key R&D Program\n(2022C01057) and the XPLORER PRIZE.\nREFERENCES\nMojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, and Tobias Ritschel. 2020.\nX-Fields: Implicit Neural View-, Light- and Time-Image Interpolation. ACM Trans.\nGraph. 39, 6 (2020).\nSai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail,\nRavi Ramamoorthi, Yaser Sheikh, and Jason Saragih. 2021. Deep relightable appear-\nance models for animatable faces. ACM Trans. Graph. 40, 4 (2021), 1\u201315.\nSai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David\nKriegman, and Ravi Ramamoorthi. 2020. Deep Reflectance Volumes: Relightable\nReconstructions from Multi-View Photometric Images. In ECCV. 294\u2013311.\nMark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A.\nLensch. 2021a. NeRD: Neural Reflectance Decomposition from Image Collections.\nIn ICCV.\nRelighting Neural Radiance Fields with Shadow and Highlight Hints\nMark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T.\nBarron, Hendrik P.A. Lensch, and Varun Jampani. 2022. SAMURAI: Shape And\nMaterial from Unconstrained Real-world Arbitrary Image collections. In NeurIPS.\nMark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan Barron, and Hendrik PA\nLensch. 2021b. Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decom-\nposition. In NeurIPS, Vol. 34. 10691\u201310704.\nGuangyan Cai, Kai Yan, Zhao Dong, Ioannis Gkioulekas, and Shuang Zhao. 2022.\nPhysics-Based Inverse Rendering using Combined Implicit and Explicit Geometries.\nComp. Graph. Forum 41, 4 (2022), 129\u2013138.\nWenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis,\nOr Litany, and Sanja Fidler. 2021. DIB-R++: Learning to Predict Lighting and\nMaterial with a Hybrid Differentiable Renderer. In NeurIPS.\nPaul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and\nMark Sagar. 2000. Acquiring the Reflectance Field of a Human Face. In Proceedings\nof the 27th Annual Conference on Computer Graphics and Interactive Techniques.\n145\u2013156.\nYue Dong. 2019. Deep appearance modeling: A survey. Visual Informatics 3, 2 (2019),\n59\u201368.\nFarshad Einabadi, Jean-Yves Guillemaut, and Adrian Hilton. 2021. Deep neural models\nfor illumination estimation and relighting: A survey. In Comp. Graph. Forum, Vol. 40.\n315\u2013331.\nDuan Gao, Guojun Chen, Yue Dong, Pieter Peers, Kun Xu, and Xin Tong. 2020. Deferred\nNeural Lighting: Free-Viewpoint Relighting from Unstructured Photographs. ACM\nTrans. Graph. 39, 6, Article 258 (nov 2020).\nDavid Griffiths, Tobias Ritschel, and Julien Philip. 2022. OutCast: Outdoor Single-image\nRelighting with Cast Shadows. Comp. Graph. Forum 41, 2 (2022), 179\u2013193.\nAmos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. 2020. Implicit\ngeometric regularization for learning shapes. arXiv preprint arXiv:2002.10099 (2020).\nKaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen,\nGeoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, Danhang\nTang, Anastasia Tkach, Adarsh Kowdle, Emily Cooper, Mingsong Dou, Sean Fanello,\nGraham Fyffe, Christoph Rhemann, Jonathan Taylor, Paul Debevec, and Shahram\nIzadi. 2019. The Relightables: Volumetric Performance Capture of Humans with\nRealistic Relighting. ACM Trans. Graph. 38, 6, Article 217 (nov 2019).\nJon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. 2022. Shape, Light, and\nMaterial Decomposition from Images using Monte Carlo Rendering and Denoising.\nIn NeurIPS.\nYoshihiro Kanamori and Yuki Endo. 2018. Relighting Humans: Occlusion-Aware\nInverse Rendering for Full-Body Human Images. ACM Trans. Graph. 37, 6, Article\n270 (Dec. 2018).\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization.\nIn ICLR.\nZhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and\nSergey Tulyakov. 2022. NeROIC: Neural Rendering of Objects from Online Image\nCollections. ACM Trans. Graph. 41, 4, Article 56 (jul 2022).\nQuewei Li, Jie Guo, Yang Fei, Feichao Li, and Yanwen Guo. 2022. NeuLighting: Neural\nLighting for Free Viewpoint Outdoor Scene Relighting with Unconstrained Photo\nCollections. In SIGGRAPH Asia 2022 Conference Papers. Article 13.\nTzu-Mao Li, Miika Aittala, Fr\u00e9do Durand, and Jaakko Lehtinen. 2018. Differentiable\nMonte Carlo Ray Tracing through Edge Sampling. ACM Trans. Graph. 37, 6, Article\n222 (dec 2018).\nMatthew M. Loper and Michael J. Black. 2014. OpenDR: An Approximate Differentiable\nRenderer. In ECCV. 154\u2013169.\nFujun Luan, Shuang Zhao, Kavita Bala, and Zhao Dong. 2021. Unified Shape and\nSVBRDF Recovery using Differentiable Monte Carlo Rendering. Comp. Graph.\nForum 40, 4 (2021), 101\u2013113.\nLinjie Lyu, Ayush Tewari, Thomas Leimk\u00fchler, Marc Habermann, and Christian\nTheobalt. 2022. Neural Radiance Transfer Fields for Relightable Novel-View Syn-\nthesis with Global Illumination. In ECCV, Vol. 13677. 153\u2013169.\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey\nDosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance\nFields for Unconstrained Photo Collections. In CVPR.\nAbhimitra Meka, Christian H\u00e4ne, Rohit Pandey, Michael Zollh\u00f6fer, Sean Fanello,\nGraham Fyffe, Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, Pe-\nter Denny, Sofien Bouaziz, Peter Lincoln, Matt Whalen, Geoff Harvey, Jonathan\nTaylor, Shahram Izadi, Andrea Tagliasacchi, Paul Debevec, Christian Theobalt,\nJulien Valentin, and Christoph Rhemann. 2019. Deep Reflectance Fields: High-\nQuality Facial Reflectance Field Inference from Color Gradient Illumination. ACM\nTrans. Graph. 38, 4, Article 77 (jul 2019).\nAbhimitra Meka, Rohit Pandey, Christian Haene, Sergio Orts-Escolano, Peter Barnum,\nPhilip David-Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz,\net al. 2020. Deep relightable textures: volumetric performance capture with neural\nrendering. ACM Trans. Graph. 39, 6 (2020), 1\u201321.\nMoustafa Mahmoud Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Ku-\nmar Pandey, Noah Snavely, and Ricardo Martin Brualla. 2019. Neural Rerendering\nin the Wild. In CVPR.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\nFields for View Synthesis. ECCV (2020).\nJ. Munkberg, W. Chen, J. Hasselgren, A. Evans, T. Shen, T. Muller, J. Gao, and S. Fidler.\n2022. Extracting Triangular 3D Models, Materials, and Lighting From Images. In\nCVPR. 8270\u20138280.\nO. Nalbach, E. Arabadzhiyska, D. Mehta, H.-P. Seidel, and T. Ritschel. 2017. Deep\nShading: Convolutional Neural Networks for Screen Space Shading. Comp. Graph.\nForum 36, 4 (2017), 65\u201378.\nGiljoo Nam, Joo Ho Lee, Diego Gutierrez, and Min H. Kim. 2018. Practical SVBRDF\nAcquisition of 3D Objects with Unstructured Flash Photography. ACM Trans. Graph.\n37, 6, Article 267 (Dec. 2018).\nMerlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2:\nA Retargetable Forward and Inverse Renderer. ACM Trans. Graph. 38, 6, Article\n203 (nov 2019).\nRohit Pandey, Sergio Orts Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz,\nChristoph Rhemann, Paul Debevec, and Sean Fanello. 2021. Total relighting: learn-\ning to relight portraits for background replacement. ACM Trans. Graph. 40, 4 (2021),\n1\u201321.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\nLibrary. In NeurIPS. 8024\u20138035.\nJulien Philip, Micha\u00ebl Gharbi, Tinghui Zhou, Alexei A. Efros, and George Drettakis.\n2019. Multi-view Relighting Using a Geometry-aware Network. ACM Trans. Graph.\n38, 4, Article 78 (July 2019).\nJulien Philip, S\u00e9bastien Morgenthaler, Micha\u00ebl Gharbi, and George Drettakis. 2021.\nFree-Viewpoint Indoor Neural Relighting from Multi-View Stereo. ACM Trans.\nGraph. 40, 5, Article 194 (sep 2021).\nPeiran Ren, Yue Dong, Stephen Lin, Xin Tong, and Baining Guo. 2015. Image Based\nRelighting Using Neural Networks. ACM Trans. Graph. 34, 4, Article 111 (jul 2015).\nViktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik,\nand Christian Theobalt. 2022. NeRF for Outdoor Scene Relighting. In ECCV.\nJohannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion\nRevisited. In CVPR.\nP. P. Srinivasan, B. Deng, X. Zhang, M. Tancik, B. Mildenhall, and J. T. Barron. 2021.\nNeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis.\nIn CVPR.\nTiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham\nFyffe, Christoph Rhemann, Jay Busch, Paul E. Debevec, and Ravi Ramamoorthi.\n2019. Single image portrait relighting. ACM Trans. Graph. 38, 4, Article 79 (2019).\nTiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, and Ravi Ramamoorthi. 2021. NeLF:\nNeural Light-transport Field for Portrait View Synthesis and Relighting. In EGSR.\n155\u2013166.\nTiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph Rhemann, Paul\nDebevec, Yun-Ta Tsai, Jonathan T Barron, and Ravi Ramamoorthi. 2020. Light stage\nsuper-resolution: continuous high-frequency relighting. ACM Trans. Graph. 39, 6\n(2020), 1\u201312.\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, W Yifan,\nChristoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi,\net al. 2022. Advances in neural rendering. In Comp. Graph. Forum, Vol. 41. 703\u2013735.\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and\nPratul P Srinivasan. 2022. Ref-nerf: Structured view-dependent appearance for\nneural radiance fields. In CVPR. 5481\u20135490.\nBruce Walter, Stephen R Marschner, Hongsong Li, and Kenneth E Torrance. 2007.\nMicrofacet models for refraction through rough surfaces. In EGSR. 195\u2013206.\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\nMulti-view Reconstruction. NeurIPS (2021).\nJiankai Xing, Fujun Luan, Ling-Qi Yan, Xuejun Hu, Houde Qian, and Kun Xu. 2022.\nDifferentiable Rendering Using RGBXY Derivatives and Optimal Transport. ACM\nTrans. Graph. 41, 6, Article 189 (nov 2022).\nZexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. 2018. Deep\nImage-based Relighting from Optimal Sparse Samples. ACM Trans. Graph. 37, 4,\nArticle 126 (July 2018).\nWenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, and Kwan-Yee K. Wong.\n2022. PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo. In\nECCV.\nYao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai\nTsin, and Long Quan. 2022. NeILF: Neural Incident Light Field for Material and\nLighting Estimation. In ECCV.\nYu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, and Ting-Chun\nWang. 2022. Learning to Relight Portrait Images via a Virtual Light Stage and\nSynthetic-to-Real Adaptation. ACM Trans. Graph. 41, 6 (2022), 1\u201321.\nKai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. 2022a. IRON: Inverse Rendering\nby Optimizing Neural SDFs and Materials from Photometric Images. In CVPR. 5555\u2013\n5564.\nZeng et al.\nKai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021b. PhySG:\nInverse Rendering with Spherical Gaussians for Physics-based Material Editing\nand Relighting. In CVPR.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\nThe unreasonable effectiveness of deep features as a perceptual metric. In CVPR.\n586\u2013595.\nXiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit\nPandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec,\nJonathan T. Barron, Ravi Ramamoorthi, and William T. Freeman. 2021a. Neural\nLight Transport for Relighting and View Synthesis. ACM Trans. Graph. 40, 1, Article\n9 (jan 2021).\nXiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman,\nand Jonathan T. Barron. 2021c. NeRFactor: Neural Factorization of Shape and\nReflectance under an Unknown Illumination. ACM Trans. Graph. 40, 6, Article 237\n(dec 2021).\nYuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou.\n2022b. Modeling Indirect Illumination for Inverse Rendering. In CVPR.\nQuan Zheng, Gurprit Singh, and Hans-Peter Seidel. 2021. Neural Relightable Partici-\npating Media Rendering. In NeurIPS, Vol. 34. 15203\u201315215.\nRelighting Neural Radiance Fields with\nShadow and Highlight Hints\nChong Zeng\u2217\nState Key Lab of CAD and CG,\nZhejiang University\nHangzhou, China\nchongzeng2000@gmail.com\nGuojun Chen\nMicrosoft Research Asia\nBeijing, China\nguoch@microsoft.com\nYue Dong\nMicrosoft Research Asia\nBeijing, China\nyuedong@microsoft.com\nPieter Peers\nCollege of William & Mary\nWilliamsburg, USA\nppeers@siggraph.org\nHongzhi Wu\nState Key Lab of CAD and CG,\nZhejiang University\nHangzhou, China\nhwu@acm.org\nXin Tong\nMicrosoft Research Asia\nBeijing, China\nxtong@microsoft.com\nShort Fur: 29.70 | 0.9598 | 0.0823\nLong Fur: 25.53 | 0.9060 | 0.1345\nRough-Metal: 35.75 | 0.9908 | 0.0176\nGlossy-Plastic: 36.16 | 0.9920 | 0.0155\nRough-Plastic: 37.41 | 0.9945 | 0.0132\nBasket: 26.84 | 0.9586 | 0.0411\nFigure 1: Qualitative comparison between additional synthetic scenes relit (right) for a novel viewpoint and novel lighting\ndirection (not part of the training data) and a rendered reference image (left). For each example we list average PSNR, SSIM,\nand LPIPS computed over a uniform sampling of view and light positions.\nACM Reference Format:\nChong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, and Xin\nTong. 2023. Relighting Neural Radiance Fields with Shadow and Highlight\nHints. In Special Interest Group on Computer Graphics and Interactive Tech-\nniques Conference Conference Proceedings (SIGGRAPH \u201923 Conference Pro-\nceedings), August 6\u201310, 2023, Los Angeles, CA, USA. ACM, New York, NY,\nUSA, 2 pages. https://doi.org/10.1145/3588432.3591482\n1\nADDITIONAL RESULTS\nFigure 1 shows additional synthetic results to further test our\nmethod on scenes with different material properties. The Basket\nscene is included in the ablation study figures, but not listed in\nFigure 3 (of the main paper); we include it here for completeness.\n2\nNETWORK ARCHITEXTURE DETAILS\nWe follow exactly the same architecture as NeuS [Wang et al. 2021]\nfor the density MLP: 8 hidden layers with 256 nodes using a Softplus\n\u2217Work done during internship at Microsoft Research Asia.\nC256 MLP / Softplus\nC256 MLP / Softplus\nC256 MLP / Softplus\nC217 MLP / Softplus\nC3 Position\nC39 Frequency Encoding\nC256 MLP / Softplus\nC256 MLP / Softplus\nC256 MLP / Softplus\nC1 SDF\nC256 MLP / ReLU\nC256 MLP / ReLU\nC256 MLP / ReLU\nC256 MLP / ReLU\nC3 View\nC27 Frequency Encoding\nC3 Color / Sigmoid\nC3 Light\nC27 Frequency Encoding\nC1 Shadow hint\nC9 Frequency Encoding\nC4 Highlight hint\nC36 Frequency Encoding\nC3 Normal\nDensity Network\nRelightable Radiance Network\nC3 Position\nC256 MLP / Softplus\nC256 Feature\nFigure 2: Detailed network architecture of the density and\nrelightable radiance network. The number of output chan-\nnels and activations are also marked.\nactivation and a skip connection between the input and the 4th\nlayer. The input (i.e., current position along a ray) is augmented\nusing a frequency encoding with 6 bands. The relightable radiance\nnetwork has a similar network architecture as NeuS\u2019 color MLP:\n4 hidden layers with 256 nodes using a ReLU activation. The final\ncolor is outputted after a Sigmoid activation, ensuring that the\narXiv:2308.13404v1  [cs.CV]  25 Aug 2023\nZeng et al.\noutput color is within the (\u22121, 1) range. Figure 2 details network\narchitecture of our method.\nREFERENCES\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\nMulti-view Reconstruction. NeurIPS (2021).\n"
  }
]