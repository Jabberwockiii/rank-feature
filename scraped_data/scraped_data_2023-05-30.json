[
  {
    "title": "Generating Images with Multimodal Language Models",
    "link": "https://arxiv.org/pdf/2305.17216.pdf",
    "upvote": "6",
    "text": "Generating Images with Multimodal Language Models\nJing Yu Koh\nCarnegie Mellon University\njingyuk@cs.cmu.edu\nDaniel Fried\nCarnegie Mellon University\ndfried@cs.cmu.edu\nRuslan Salakhutdinov\nCarnegie Mellon University\nrsalakhu@cs.cmu.edu\nAbstract\nWe propose a method to fuse frozen text-only large language models (LLMs)\nwith pre-trained image encoder and decoder models, by mapping between their\nembedding spaces. Our model demonstrates a wide suite of multimodal capabilities:\nimage retrieval, novel image generation, and multimodal dialogue. Ours is the first\napproach capable of conditioning on arbitrarily interleaved image and text inputs\nto generate coherent image (and text) outputs. To achieve strong performance\non image generation, we propose an efficient mapping network to ground the\nLLM to an off-the-shelf text-to-image generation model. This mapping network\ntranslates hidden representations of text into the embedding space of the visual\nmodels, enabling us to leverage the strong text representations of the LLM for visual\noutputs. Our approach outperforms baseline generation models on tasks with longer\nand more complex language. In addition to novel image generation, our model is\nalso capable of image retrieval from a prespecified dataset, and decides whether to\nretrieve or generate at inference time. This is done with a learnt decision module\nwhich conditions on the hidden representations of the LLM. Our model exhibits a\nwider range of capabilities compared to prior multimodal language models. It can\nprocess image-and-text inputs, and produce retrieved images, generated images,\nand generated text \u2014 outperforming non-LLM based generation models across\nseveral text-to-image tasks that measure context dependence.\n1\nIntroduction\nAutoregressive language models (LMs) and large language models (LLMs) trained on text corpora\nhave shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the\neffectiveness of grounding text-only LMs to images for vision-and-language tasks [56, 4, 29, 33,\n31, 35], to embodied settings for robotics [3, 18], offline reinforcement learning [48], and more.\nThese methods typically keep most of the LLM weights frozen. This allows them to leverage the\ncapabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn\nfrom in-context examples [9], more effectively process longer context, and condition on inputs more\nstrongly.\nIn this work, we tackle the task of extending multimodal language models to generate novel images.\nOur approach, Generating Images with Large Language Models (GILL), is capable of processing\narbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel\nimages (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space\nof a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49])\ndespite both models using entirely different text encoders. We achieve this by finetuning a small\nnumber of parameters on image-caption pairs [52], in contrast to other methods which require\ninterleaved image-text data [4, 2]. Our approach is computationally efficient and does not require\nrunning the image generation model at training time. To achieve strong image generation performance,\nwe propose efficient architectural changes to learn the LLM-to-generation mapping effectively with\nthe GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.17216v3  [cs.CL]  13 Oct 2023\nFROMAGe\nRetrieval only multimodal language \nmodel. Image outputs limited to the \ncandidate retrieval set.\nOurs\nRetrieval and generation multimodal LM. \nDecides when to retrieve or generate.\nStable Diffusion\nGeneration only text-to-image model. \nLess sensitive to longer text inputs \n(such as dialogue).\nA beaver.\nI am thinking of an animal. It is brown \nand furry, and has a tail. What is it?\nA squirrel.\nA beaver.\nNo, the animal I am thinking of \nlives in the river.\nShow me an illustration of a beaver\nYes, what about a pencil drawing of one?\nWhat about a comic illustration of one?\nA beaver.\nA beaver.\nRetrieved\nGenerated\nUser prompts\nA beaver.\nI am thinking of an animal. It is brown \nand furry, and has a tail. What is it?\nA squirrel.\nNo, the animal I am thinking of \nlives in the river.\nShow me an illustration of a beaver\nYes, what about a pencil drawing of one?\nWhat about a comic illustration of one?\nA beaver.\nI am thinking of an animal. It is brown \nand furry, and has a tail. What is it?\nA squirrel.\nNo, the animal I am thinking of \nlives in the river.\nShow me an illustration of a beaver\nYes, what about a pencil drawing of one?\nWhat about a comic illustration of one?\nFigure 1: Our model is capable of generating text, retrieving images, generating novel images, and\ninterleaving results into coherent multimodal dialogue.\nlearnt text tokens. We train it by minimizing the l2 distance between its outputs and the outputs of\nthe text encoder of a text-to-image generation model. This distillation training allows us to use the\nimage decoder of the text-to-image model at inference time. Despite its simplicity, we show that this\nallows us to outperform the baseline text-to-image generation model on several tasks that measure\nlanguage context dependence. Finally, to decide whether to produce a retrieved image or a generated\none at inference time, we train a decision model that outputs a decision conditioned on the LM hidden\nrepresentations. This allows us to both generate and retrieve in output sequences, as shown in Fig. 1.\nOur experimental results demonstrate that GILL is more effective than Stable Diffusion at processing\nlonger-form text, including dialogue and discourse. We show on dialogue-conditioned image gener-\nation that GILL can outperform non-LLM based generation models, and benefit from multimodal\ncontext: generating images that match text better than the backbone generation models that we\ndistill from. In addition, GILL can process arbitrarily interleaved image-text inputs, unlike typical\ntext-to-image models which only process text. GILL is the first model capable of outputting retrieved\nimages, novel images, and text \u2014 interleaving these for coherent multimodal dialogue generation.1\n2\nRelated Work\nMultimodal Language Models\nSeveral prior works have developed multimodal language models\nwhich process image and text inputs to generate text outputs. Frozen [56] showed that it is possible\nto finetune a visual encoder to map images into the hidden space of a text-only LLM, and that\nthis exhibits compelling few-shot, captioning, and question answering abilities. Other methods\nimprove upon this approach by introducing adapters [19], scaling up model and data sizes [4, 64],\nimproving the visual encoder [4, 33], finetuning on instructions [35], and training unified models\non multi-task objectives [36, 63, 42]. CM3 [2, 62] trained multimodal LMs on HTML webpages\nconsisting of interleaved images and text. Many state-of-the-art models also require significant\ncomputational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days,\nwhile RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained\non 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a\nmultimodal language model capable of processing arbitrarily interleaved image and text inputs to\n1Our code and pretrained models are publicly released at https://github.com/kohjingyu/gill.\n2\nInfoNCE Loss\nImage-Text Retrieval\nLLM\nFrozen Model\nLoss\nLinear Layer\nOutput Embeddings\n(seq_len, 4096)\n...\nVisual \nEncoder\nCaption Input\nImage Input\nLearning to Process Images\nImage and Caption Inputs\nVisual \nEncoder\nAn European \nshorthair cat \nin a woven \nbasket\nTokenizer\n<img>\nan\nEuropean\n...\nshort\nInput Embeddings\n(seq_len, 4096)\nGenerated Caption\n(next token prediction)\nLLM\nCross Entropy Loss\nA grey cat \nsitting in \na basket\nImage #1\nCaption #1\n[IMG1]\n...\n[IMG{r}]\nMSE Loss\nImage Generation\nGILLMapper\nSD Text \nEncoder\nInput Caption\nGILLMapper\nLearning to Produce Images\nAn European \nshorthair cat in \na woven basket \n[IMG1]...[IMG{r}]\nan\nEuropean\nFigure 2: GILL model architecture overview. It is trained with a captioning loss to learn to process\nimages (left), and losses for image retrieval and image generation to learn to produce images (right).\ngenerate text interleaved with retrieved images. While FROMAGe can only retrieve images in their\noutputs, GILL is capable of both image retrieval and image generation, which allows it to outperform\nretrieval-only models when they are limited by their candidate retrieval set (Fig. 5).\nLarge Language Models\nOur work leverages recent advances in Transformer-based [57] LLMs.\nWhen trained at large enough scale, LLMs exhibit compelling properties, such as the ability to learn\nfrom few-shot in-context examples [9, 11] and generate and process long text inputs [61, 59, 53, 7].\nOur approach also builds upon recent efforts on open sourced LLM weights [69, 55].\nText-to-Image Generation\nText-to-image generation is the task of synthesizing a realistic image\nconditioned on natural language descriptions. [47] was one of the first to tackle this with a conditional\nGAN [23]. Later work improved upon this by introducing multi-stage models [67], attention mecha-\nnisms [60], and contrastive methods [73, 66]. Several recent papers also formulate the text-to-image\ngeneration task as a sequence modeling problem [45, 17, 13], training large Transformer [57] models\non discretized image tokens [46]. [20, 65] improved upon this approach by introducing stronger image\nquantizers and scaling up model parameters. Several recent methods [38, 44, 49] apply diffusion\nmodels [26] to improve generated image quality. [50, 65] scale up text encoder models to achieve\nsignificant gains in generating relevant images. In contrast with computationally intensive methods\nthat train end-to-end, GILL does not require running the image generation model during training.\n3\nMethod\nWe efficiently adapt a pretrained autoregressive language model of text, to process image and text\ninputs and produce image and text outputs. Most of the model weights (including those of the base\nLLM and image generator) are kept frozen, and we finetune a small number of parameters on image\ncaption data (Fig. 2) to achieve a wide range of capabilities (Fig. 5). There are several challenges that\nneed to be resolved. The model needs to learn to process image-and-text content (Sec. 3.1). It also\nneeds to learn to produce images (either retrieved or generated), and determine whether to produce\ntext or images at each step (Sec. 3.2). Finally, whenever an image is produced, the model needs to\ndecide whether image retrieval (from a candidate set) or generation is more appropriate (Sec. 3.3).\n3.1\nLearning to Process Images\nGiven an image x and its text caption y (tokenized as (s1, . . . , sT )), our goal is to adapt a frozen LLM\nto enable it to complete any sequence of arbitrarily interleaved image and text inputs. For example,\ninputs for the Visual Storytelling dataset [28] consist of 5 images and 5 text descriptions, interleaved\nin a manner such as (x1, y1, . . . , x5, y5). We follow prior work [56, 19, 35, 31] in learning translation\nparameters that map from image features to text embedding space.\nWe first extract visual embeddings v\u03d5(x) \u2208 Rd with a pretrained visual backbone (its weights \u03d5 and\nLLM weights \u03b8 are kept frozen). We learn a linear mapping Wcap \u2208 Rd\u00d7ke which maps v\u03d5(x) into a\nsequence of k e-dimensional vectors that we use as inputs to the LLM (Fig. 2, left), where e is the\n3\nI\n...\nHow should I \ndisplay these at \nthe farmer's \nmarket?\n<img>\nHow\nshould\n...\n[IMG1]\n..\n[IMG{r}]\nret\ngen\nthink\nI\nI think they look best when they are on a tray \nwith a little bit of space between them. \nFigure 3: Inference time procedure for GILL. The model takes in\nimage and text inputs, and produces text interleaved with image\nembeddings. After deciding whether to retrieve or generate for a\nparticular set of tokens, it returns the appropriate image outputs.\n(L, 768)\n(L, 512)\n...\n[IMG]\n(r, 4096)\n...\n...\nFigure 4: GILLMapper model\narchitecture. It is conditioned\non the hidden [IMG] represen-\ntations and a sequence of learnt\nquery embedding vectors.\nLLM input embedding dimension. We train Wcap on image-caption pairs (details in Sec. 3.4), by\nminimizing the negative log-likelihood loss of the token sequence (s1, . . . , sT ):\nlc(x, y) = \u2212\nT\nX\nt=1\nlog p\u03b8(st | v\u03d5(x)T Wcap, s1, . . . , st\u22121)\n(1)\nIntuitively, this objective trains a mapping Wcap that allows us to translate images into embedding\nvectors in the token embedding space of the LLM (illustrated in Fig. 2, left).\n3.2\nLearning to Produce Images\nIn order to enable the model to produce image outputs, we add special [IMG] tokens to the vocabulary\nof the LLM, similar to [71, 31] which introduce special tokens correspond to images that should be\noutput by the model. The hidden states that the LLM produces for these tokens will be used to retrieve\nor generate images. While [31] use a single token for their image retrieval model, we observed in our\nexperiments that image generation requires much more finegrained textual information (Sec. 5). In\norder to improve the expressivity of the frozen LLM for novel image generation, we generalize to use\nr tokens [IMG1], . . . , [IMG{r}] for representing visual outputs.\nConcretely, we add a trainable matrix Eimg \u2208 Rr\u00d7e to the embedding matrix of the frozen LLM,\nwhich represents the r [IMG] token embeddings. We wish to train the model to learn when it should\nproduce [IMG] tokens. This is done by minimizing the negative log-likelihood of producing the first\n[IMG] token conditioned on previously generated tokens:\nlp(y) = \u2212 log p{\u03b8\u222aEimg}([IMG1] | s1, . . . , st)\n(2)\nThe LLM weights \u03b8 are kept frozen, and we only update Eimg. During inference, we always generate\nthe [IMG2], . . . , [IMG{r}] tokens whenever the first [IMG1] token is produced. During training,\nthe [IMG] tokens are appended to each caption (Fig. 2). The LLM hidden states of the [IMG] tokens\nare used for image retrieval and generation, as described in the following sections.\nNovel Image Generation\nIn order for our LLM to produce image outputs, the [IMG] tokens need to\nbe mapped into a semantically meaningful region of the input space of an image generation model G\u03c8\n(such as that of the Stable Diffusion [49] image decoder). In initial experiments, we found that training\na simple linear mapping such as those used in previous work on retrieval [31] was insufficient, and that\nsuch a model was unable to handle more complex prompts (see Sec. 5 for analysis). Hence, we propose\nGILLMapper (Fig. 4), a lightweight 4-layer encoder-decoder transformer model with trainable weights\n\u03c9. The GILLMapper module f\u03c9 conditions on h{\u03b8\u222aEimg}(y, [IMG]) (the [IMG] representations from\nthe last hidden layer of the LLM) and L learnt query embeddings (q1, . . . , qL) \u2208 RL\u00d7m (where L is\nthe maximum input sequence length of the text-to-image generation backbone G\u03c8).\n4\nThe purpose of introducing learnable query embeddings is to enable GILLMapper to extract sequences\nof L features from the LLM [IMG] hidden states. This is similar to the queries introduced in\nDETR [10] for object detection and BLIP-2 [33] for extracting image features. We optimize the GILL\ntrainable weights (q1, . . . , qL and \u03c9) by minimizing the MSE loss of the GILLMapper model outputs\nagainst the embeddings produced by the text encoder (T\u03c8) of a frozen text-to-image generation\nmodel:\nlg(y) = \u2225f\u03c9\n\u0000h{\u03b8\u222aEimg}(y, [IMG{1}]), . . . , h{\u03b8\u222aEimg}(y, [IMG{r}]), q1, . . . , qL\n\u0001\n\u2212 T\u03c8(y)\u22252\n2\n(3)\nThis is essentially distilling from T\u03c8 to learn a valid mapping from the output representations of our\nfrozen LLM to the input space of G\u03c8. Note that this does not require G\u03c8 during training, so we can\nprecompute T\u03c8(y) ahead of time, making training highly efficient. During inference, when [IMG]\ntokens are generated, we can synthesize an image by applying GILLMapper and the decoder G\u03c8:\nGenerated Image = G\u03c8(f\u03c9(h{\u03b8\u222aEimg}(y, [IMG{1}]), . . . , h{\u03b8\u222aEimg}(y, [IMG{r}]), q1, . . . , qL))\nwhere h{\u03b8\u222aEimg}(y, [IMG{i}]) represents the hidden states from the last hidden layer of the modified\nLLM corresponding to the ith [IMG] token. The learnt query embeddings (q1, . . . , qL) are part of\nthe GILLMapper model weights, and are hence kept fixed during inference.\nImage Retrieval\nSimilar to [31], we learn a linear mapping Wt2i \u2208 Re\u00d7p that maps the first token\n([IMG1] to a p-dimensional vector. We also learn a linear mapping Wi2t \u2208 Rd\u00d7p that map the pooled\nvisual output of the image encoder v\u03d5(x) to a p-dimensional space. These represent image and text\nembeddings, and we train the model by minimizing the InfoNCE loss [39]:\nlr(xi, yi) = \u2212 log\nexp(sim(xi, yi, Wt2i)/\u03c4)\nPN\nj=1 exp(sim(xj, yi, Wt2i)/\u03c4)\n\u2212 log\nexp(sim(xi, yi, Wi2t))/\u03c4)\nPN\nj=1 exp(sim(xi, yj, Wi2t))/\u03c4)\n(4)\nwhere the similarity is computed as\nsim(x, y, W) =\n\u0000WT v\u03d5(x)\n\u0001T \u0000WT h{\u03b8\u222aEimg}(y, [IMG1])\n\u0001\n\u2225WT v\u03d5(x)\u2225\n\r\rWT h{\u03b8\u222aEimg}(y, [IMG1])\n\r\r\nDuring inference, we follow standard procedure [43] in retrieving the image with the highest cosine\nsimilarity (between image embeddings and the [IMG] tokens) from a candidate set of images.\n3.3\nDeciding to Generate or Retrieve\nWhile learning to produce [IMG] tokens allows us to decide when to interleave images in text , the\ntask of deciding whether to retrieve or generate from [IMG] tokens remains. Intuitively, for a given\nprompt, we would like to retrieve when there is a strong match from our set of candidate images, and\ngenerate otherwise. In order to evaluate this, we collect human annotations on PartiPrompts (P2) [65],\na collection of prompts used to benchmark image generation models. P2 contains some prompts that\nare well-represented by naturally occurring images, but others that are unlikely to occur in natural\nimage sets, making it a test of generative models. For each of the 1,632 examples in P2, we generate\nan image with the text-to-image generation model G\u03c8, and use the CLIP ViT-L [43] model to retrieve\nthe top ranked image from CC3M [52] according to the cosine similarity of image embeddings v\u03d5.\nWe have 5 independent human annotators (details in the appendix) select which of the two images for\neach prompt, retrieved or generated, is better matched to the prompt. We labeled the examples where\nthe generated image was selected as \u2018gen\u2019 (indicating prompts which we should generate an image\nfor) and \u2018ret\u2019 for prompts that should have an image retrieved. We extract the most confident set of\nthese annotations (retaining roughly 900 examples with an inter-annotator agreement of at least 4/5),\nand split them into a 67% train (600) and 33% test (300) split. We use this to train a linear classifier\non the LLM [IMG] hidden states as a decision model for deciding when to retrieve or generate (more\ndetails and baselines are provided in the appendix). Although these annotations of retrieving versus\ngenerating are somewhat model dependent, we believe that this data is still a valuable metric during\nmodel development. We will release our annotations to encourage future work in this space.\n5\n3.4\nData and Implementation Details\nThe final training objective for a batch of image-text pairs (x, y) is the sum of the captioning loss lc\n(Eq. 1), image token prediction loss lp (Eq. 2), generation loss lg (Eq. 3) and retrieval loss lr (Eq. 4):\nmin\nWi2t,Wt2i,Wcap,Eimg,\u03c9,q1:L\n1\nN\nN\nX\ni=1\n\u0000lc(xi, yi) + lp(yi) + lg(yi) + lr(xi, yi)\n\u0001\n(5)\nThe decision model is trained separately after convergence of the other components. The multitask\nloss (Eq. 5) trains GILL to process images (lc), produce [IMG] tokens (lp), generate images (lg), and\nretrieve images (lr). This enables it to generalize to a wide range of vision-and-language tasks.\nWe train on Conceptual Captions (CC3M) [52], which consists of 3.3M image-text pairs. Following\n[31], we pack two random examples together during training with probability 0.5 (i.e., 50% of the\ntime, the input is a single image and caption example, while the other 50% of the time the input\nconsists of a sequence consisting of two interleaved images and captions). We use the OPT-6.7B [69]\nmodel as the LLM backbone (which produce hidden states h\u03b8 with embedding dim e = 4096). For\nthe visual model used to extract features v\u03d5 for captioning and retrieval, we use the CLIP [43] ViT-L\nmodel. For our text-to-image generation backbone G\u03c8, we use the Stable Diffusion [49] v1.5 model\n(with L = 77 input vectors).2 We use k = 4 visual tokens, and r = 8 learnt [IMG] tokens. We\nset the GILLMapper query embedding dimension m = 512. For retrieval, we use an embedding\ndimension p = 256. All pretrained model weights are kept frozen, and we only train the linear layers\nWi2t, Wt2i, Wcap, the [IMG] embedding matrix Eimg, and the GILLMapper parameters \u03c9 and query\nvectors q1:L. In total, there are 50M trainable parameters, significantly fewer than in the frozen LLM\nand visual models (which total approximately 8B parameters). We use bfloat16 precision [1], and\noptimize using Adam [30] (\u03b21 = 0.9, \u03b22 = 0.95) with a learning rate of 0.001. We train with a batch\nsize of 200 for 20K iterations, which takes 2 days on 2 A6000 GPUs. We follow [31] and concatenate\ncaptions to encourage the model to attend to relevant images within an image-text sequence.\n4\nExperiments\nGILL is the first multimodal language model capable of conditioning on image-and-text inputs\nto generate meaningful images interleaved with text. Hence, our experiments primarily focus on\nevaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over\nStable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse.\nWe also benchmark the performance of models in deciding whether to retrieve or generate (see\nappendix). GILL is capable of generating text, retrieving images, and generating images. Despite\nbeing more general than prior work [56, 4, 31], we find that GILL performs comparably to or better\nthan existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).\n4.1\nContextual Image Generation\nTo test the ability of our model against baseline methods for novel image generation, we run\nexperiments on the VIST [28] and VisDial [16] datasets. These are the same datasets used in prior\nwork [31] for benchmarking image retrieval conditioned on multimodal text-and-image context.\nEvaluation Metrics\nThe focus of our evaluation is on the ability of generative models to handle\ncomplex language descriptions. Hence, we compute metrics which measure the relevance of the\ngenerated image content. We evaluate models with two metrics:\n1. CLIP Similarity: We use the CLIP [43] ViT-L image encoder to produce pooled repre-\nsentations of generated images and the corresponding real images, and report their cosine\nsimilarity. A higher score indicates that a generated image is more similar to the real image.\n2. Learned Perceptual Image Patch Similarity (LPIPS): LPIPS [68] evaluates the distance\nbetween image patches. We measure LPIPS between real and generated images. A lower\nvalue indicates that two images are closer in perceptual space (i.e., more similar), while a\nhigher value indicates that two images are more dissimilar.\n2These models were selected for their strong performance and open source availability, but in principle our\napproach can be applied with any other pretrained models with similar function calls.\n6\nWhat would a pizza \nwith bacon look like?\nWhat about strawberries on it?\nAdd some okras\nOurs\nImage and Text Inputs\nVisual Storytelling\nOur model can condition on interleaved image-and-text inputs to generate more relevant images compared to non-LLM based text-to-image generation models.\nComparison Against FROMAGe\nOur model composites multimodal information to produce relevant image and text outputs. It can outperform baseline models that are limited to image retrieval.\nMultimodal Dialogue\nOur model can generate multimodal dialogue, weaving together text, retrieved images, and generated images.\nWhat are some vegetables \nI can add to it?\nBroccoli, carrots, and \ngreen beans are all \ngood choices.\nHow should I publicize \nthese at the market?\nI would suggest a simple \nsign with the name of \nthe business and a \npicture of the cupcakes. \nWhat would a pizza \nwith bacon look like?\nWhat about strawberries on it?\nAdd some okras\nFROMAGe (retrieved)\nOurs (generated)\n\u201cThis \nbut in \ngreen\u201d\nOurs (generated)\nFROMAGe (retrieved)\nImage + Text Input\nOurs (generated)\n\u201cThis \nbut as \npaper-\ncraft\u201d\nFROMAGe (retrieved)\nImage + Text Input\nOurs (generated)\nFROMAGe (retrieved)\nImage + Text Input\n\u201cFrom an \naerial \nview\u201d\nHow can I make this \nmore nutritious?\nYou can add vegetables \nto your ramen noodles, \nbut you should be careful \nnot to overdo it.\nGroundtruth\nStable Di\ufb00usion\nOnce while \nI was on \nvacation \nin this \nnice brick \nhotel\nI woke up \nand took \nmy dog \nTrixie \nfor a \nwalk.\nTrixie \nran \naround \nand \nenjoyed \nthe fresh \nair.\nWe had \nlots of \nfun \nplaying \nfetch \ntogether\n5\nAfter a \nwhile she \ngot tired \nand had \nto take a \nrest.\n1\n2\n3\n4\nOurs\nImage and Text Inputs\n[male] \ntakes a \npicture \nwith a \nlocal. He \nloves his \nvacation.\nA great \nphoto of \nthis \nyoung \ncouple \njust \nmarried\nThat \nstatue is \nknown as \nthe best \nclown ever \nstatue\n5\nLocation \nin the \nmorning, \nright \nbefore \nbreakfast\nGroundtruth\nYou can see \nearly \nmorning the \nlocals \ntaking a \nswim in the \nwater\nStable Di\ufb00usion\n1\n2\n3\n4\nRetrieved\nGenerated\nUser prompts\nI'm looking for some ideas for a \nnew tattoo. What do you think \nlooks good on a petite person?\nI want a tattoo of a sun\ufb02ower, \nbut I'm not sure what style \nwould look best on me.\nThat's beautiful. Show me \none that would work well \nas a complementary piece\nHi, I'm looking for ideas for \na customised cake design \nfor my daughter's birthday.\nI'm not sure what you \nmean by customised.\nI think\nI want a cake with a unicorn \ntheme, with pastel colors and a \nwhimsical feel.\nCan you also show me some \ndecorations that would work well \nfor the party?\nI can show you some ideas\nFigure 5: Qualitative results over various input and output modalities. GILL is able to process\ncontextual multimodal cues to retrieve and generate appropriate image and text outputs.\n7\nTable 1: Results on contextual image generation on VIST [28] (averaged over 5 random seeds). Our\nmodel can process longer (possibly multimodel) inputs to outperform baseline models.\nCLIP Similarity (\u2191)\nLPIPS (\u2193)\nModel\n1 caption\n5 captions\n5 caps, 4 images\n1 caption\n5 captions\n5 caps, 4 images\nGLIDE [38]\n0.582\n0.591\n-\n0.753\n0.745\n-\nStable Diffusion [49]\n0.592 \u00b10.0007\n0.598 \u00b1 0.0006\n-\n0.703 \u00b10.0003\n0.704 \u00b1 0.0004\n-\nGILL (ours)\n0.581 \u00b10.0005\n0.612 \u00b10.0011\n0.641 \u00b10.0011\n0.702 \u00b10.0004\n0.696 \u00b10.0008\n0.693 \u00b10.0008\nTable 2: Results on contextual image generation on VisDial [16] (averaged over 5 random seeds).\nOur model can process longer sequences of dialogue-like text to generate more relevant images.\nCLIP Similarity (\u2191)\nLPIPS (\u2193)\nModel\n1 round\n5 rounds\n10 rounds\n1 round\n5 rounds\n10 rounds\nGLIDE [38]\n0.562\n0.595\n0.587\n0.800\n0.794\n0.799\nStable Diffusion [49]\n0.552 \u00b10.0015\n0.629 \u00b10.0015\n0.622 \u00b10.0012\n0.742 \u00b10.0010\n0.722 \u00b10.0012\n0.723 \u00b10.0008\nGILL (ours)\n0.528 \u00b10.0014\n0.621 \u00b10.0009\n0.645 \u00b10.0010\n0.742 \u00b10.0022\n0.718 \u00b10.0028\n0.714 \u00b10.0006\nGenerating from Visual Stories\nVIST [28] is a dataset for sequential vision-and-language tasks,\nwith examples of sequences of 5 images and text that constitute a story, as shown in Fig. 5. Similar to\n[31], we test the models on generating the last image in the sequence, conditioned on different inputs:\n1. 1 caption: Input consists of the last text description. This is similar to standard text-to-\nimage generation, where a model conditions on a single caption to generate an image.\n2. 5 captions: Input consists of all text from the entire story sequence. This tests the ability\nof models to process longer and temporally dependent text descriptions.\n3. 5 captions, 4 images: Lastly, we test models with inputs of all images and texts preceding\nthe last image (i.e., sequenced as \u201c<text1><img1>...<text4><img4><text5>\u201d). This\ntests the ability of models to effectively process multimodal context in image generation.\nA novel feature of GILL is its ability to process interleaved image-text inputs, which most\nexisting text-to-image generation models are unable to handle.\nWe report results on VIST in Tab. 1, comparing GILL against text-to-image generation baselines\n(including Stable Diffusion (SD) [49], which we use as our generation backbone G\u03c8). With a single\nstory caption input to both models, the performance is comparable, with SD achieving a slightly\nbetter CLIP Similarity score, and both models achieving similar LPIPS. However, when all 5 story\ncaptions are provided as input, our model outperforms SD, improving CLIP Similarity from 0.598 to\n0.612, and LPIPS from 0.704 to 0.696. Interestingly, when further provided with the full multimodal\ncontext (the preceding 5 captions and 4 images), our model improves substantially, attaining a CLIP\nSimilarity of 0.641 and LPIPS of 0.693. In contrast, SD is unable to handle interleaved image-text\ninputs without significant modifications. We also show several qualitative examples in Fig. 5. We\nfind that GILL is generally more sensitive to input context compared to SD. GILL can also condition\non image inputs, enabling it to use visual context to produce more relevant images.\nWe highlight that both models use the same image generation backbone, and the primary difference\nis in their text encoders. GILL is able to better handle long text inputs and multimodal context, which\nwe attribute to the stronger LLM encoder coupled with our GILLMapper model.\nGenerating from Visual Dialogue\nWe also test our model on the VisDial [16] dataset. VisDial\nexamples contain a sequence of question and answer (Q&A) pairs about a particular image, simulating\ndialogue between two people who are discussing an image. Examples contain up to 10 rounds of\nQ&A dialogue pairs. Similar to VIST, we evaluate the ability of models to accurately synthesize the\nimage being described, provided with increasing amounts of the Q&A dialogue context as input. This\nexperiment tests the ability of our approach to (1) generalize to dialogue-like text (as our approach is\nonly finetuned on image caption data), and (2) process long text sequences.\nOur results are presented in Tab. 2. Similar to the VIST evaluations, we find that with shorter length\ninputs, SD outperforms our model. However, when the input context is increased, our model gradually\nimproves, and can synthesize images that are more similar to the groundtruth image. When the full\n10 rounds of dialogue are provided, GILL significantly outperforms SD, improving over both CLIP\n8\nTable 3: Image generation performance on CC3M [52] and\nVIST [28] with different text mapping networks.\nCC3M\nVIST\nModel\nFID (\u2193)\nCLIP Sim (\u2191)\nStable Diffusion [49]\n13.94\n0.598\nOurs + Linear\n15.50\n0.500\nOurs + 3-layer MLP\n15.33\n0.502\nOurs + Transformer Encoder\n16.30\n0.605\nOurs + GILLMapper\n15.31\n0.641\nTable 4: GILL image generation re-\nsults on CC3M [52] with different\nnumber of image tokens (r).\nCC3M\nVIST\nr\nFID (\u2193)\nCLIP Sim (\u2191)\n1\n15.93\n0.631\n2\n15.32\n0.629\n4\n15.32\n0.642\n8\n15.31\n0.641\n1 cap\n2 cap\n3 cap\n4 cap\n5 cap\n2 cap, 1 img\n3 cap, 2 img\n4 cap, 3 img\n5 cap, 4 img\n0.56\n0.58\n0.60\n0.62\n0.64\nCLIP Similarity\nPerformance With Increasing Context on VIST\nCaptions only\nCaptions + Images\nFigure 6: Performance of GILL on VIST generation.\nTable 5: Contextual image retrieval on\nVIST (5 captions, 4 images as input).\n\u2020 indicates results from [31].\nVIST Recall@k (\u2191)\nModel\nR@1\nR@5\nR@10\nCLIP ViT-L [43]\u2020\n8.8\n22.3\n29.8\nFROMAGe [31]\u2020\n18.2\n42.7\n51.8\nGILL (Ours)\n20.3\n45.0\n53.7\nSimilarity (0.622 to 0.645) and LPIPS (0.723 to 0.714). These results further highlight the efficacy of\nour model on handling long dialogue-like text inputs.\n4.2\nQualitative Results\nFinally, one of the more compelling applications of GILL is perhaps its ability to generalize to many\ndifferent tasks, due to the LLM pretraining and freezing. We showcase several of these capabilities\nin Fig. 5. In many examples, we observed that GILL is able to outperform retrieval models such as\nFROMAGe [31] on examples where FROMAGe is unable to retrieve relevant images. GILL is also\ngenerally more sensitive to input context compared to Stable Diffusion [49], and can condition on\nimage inputs, in addition to text, to generate more visually and semantically relevant image outputs.\n5\nAnalysis\nContextual Image Retrieval\nIn addition to generation, GILL is capable of image retrieval condi-\ntioned on image-text inputs. We run GILL on the VIST retrieval evaluation from [31]. We find that\nGILL performs comparably or better compared to prior approaches (Tab. 5). This shows that that the\nimage generation objective does not cause image retrieval performance to deteriorate.\nThe Effect of Context\nGILL leverages an LLM backbone, which allows it to inherit some of the\nLLM\u2019s capabilities, such as improved sensitivity to long inputs. Fig. 6 shows that the performance of\nGILL generally improves with increasing input contexts on VIST [28]. In particular, when 2 captions\nand 1 image are provided as context, the model significantly outperforms the model with 5 text-only\ncaptions, highlighting the value of multimodal context over unimodal context.\nGeneration-Only Objective\nWe investigate the effect of removing the retrieval loss (Eq. 4) from\nthe training objective. On VIST (5 captions, 4 images), this ablated model achieves CLIP similarity\nof 0.636 and LPIPS of 0.694, which are comparable to scores of the original model (0.641 and\n0.693 respectively). This suggests that the retrieval loss is not necessary for strong performance,\nalthough such a model would only be able to generate images and text and not retrieve images. These\nresults also suggest that GILL is not bottlenecked by including the retrieval objective, and that it has\nsufficient capacity to perform both generation and retrieval.\n9\nGILLMapper Module\nAs described in Sec. 3.2, we propose the GILLMapper module, a\nlightweight transformer model that conditions on [IMG] embeddings and q learnt embedding vectors.\nThe output maps the LM embeddings into the input space of a text-to-image generation model, en-\nabling image synthesis. We run several baselines to compare effectiveness, comparing our proposed\nmodel against (1) a linear layer, (2) a multilayer perceptron (MLP) with LeakyReLU activations, and\n(3) a 4-layer bidirectional transformer encoder. All models are conditioned on the r [IMG] token\nembeddings from the LLM. Our results are presented in Tab. 3. GILLMapper is substantially better\nthan these baseline models at learning the mapping from the frozen LLM to the Stable Diffusion\ngeneration model, as measured by Fr\u00e9chet Inception Distance (FID) [25] on the CC3M validation set\n(which is a measure of image realism), and CLIP Similarity on VIST. On the VIST evaluation (which\nis out of distribution from CC3M), the other baselines perform significantly worse than GILLMapper,\nsuggesting that they cannot generalize to longer sequences containing multiple images and texts.\nNumber of [IMG] Tokens\nWe experiment with varying the number of [IMG] tokens, r (Tab. 4). As\nr increases, generation generally improves, plateauing around r = 4. We observe that lower values\nof r appear to result in worse results, as the inputs to GILLMapper are shorter and less expressive.\n6\nConclusion\nWe proposed a method of mapping text-only LLMs to strong visual models. This enables them to\nlearn to process arbitrarily interleaved image-and-text inputs, and output generated text, retrieved\nimages, and generated images. We show that it is possible to efficiently learn a mapping between\nthe embeddings of a frozen pretrained LLM and a frozen pretrained image generation model, and\nthat doing so effectively boosts image generation for tasks that require stronger language context\ndependence. Finally, we also showcased several compelling qualitative results on a variety of\nmultimodal tasks. Our approach is modular, and can benefit from stronger LLMs or visual models\nreleased in the future. Scaling up the LLM backbone, image generation backbone, or visual processing\nmodel, are promising directions that will likely induce even stronger vision-and-language capabilities.\nAcknowledgements\nThis work was partially supported by a gift from Cisco Systems, and by ONR N000142312368 and\nDARPA/AFRL FA87502321015. We thank Wendy Kua for help with the figures. We thank Jared\nFernandez, Yutong He, Saujas Vaduguru, Yonatan Bisk, and our anonymous reviewers for feedback\nand helpful discussions on previous versions of this paper.\nReferences\n[1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale\nmachine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,\n2016.\n[2] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked\nmultimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\n[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. CoRL, 2023.\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. NeurIPS, 2022.\n[5] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with\nimproved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic\nand Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 2005.\n10\n[6] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623, 2021.\n[7] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. Unlimiformer: Long-\nrange transformers with unlimited length input. arXiv preprint arXiv:2305.01625, 2023.\n[8] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misog-\nyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. NeurIPS, 2020.\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[11] Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K Lampinen,\nand Felix Hill. Transformers generalize differently from information stored in context vs in\nweights. NeurIPS MemARI Workshop, 2022.\n[12] Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Alexander Binder, and Ngai-Man Cheung.\nDiscovering transferable forensic features for cnn-generated images detection. In ECCV, 2022.\n[13] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked\ngenerative image transformer. In CVPR, 2022.\n[14] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-\naugmented text-to-image generator. ICLR, 2023.\n[15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[16] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi\nParikh, and Dhruv Batra. Visual dialog. In CVPR, 2017.\n[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via\ntransformers. NeurIPS, 2021.\n[18] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[19] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank.\nMagma\u2013multimodal augmentation of generative models through adapter-based finetuning.\nEMNLP, 2022.\n[20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In CVPR, 2021.\n[21] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022.\n[22] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in language models. EMNLP, 2020.\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 2020.\n[24] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the\nv in vqa matter: Elevating the role of image understanding in visual question answering. In\nCVPR, 2017.\n[25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS,\n2017.\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n2020.\n11\n[27] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. ICLR, 2020.\n[28] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal,\nJacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual\nstorytelling. In NAACL-HLT, 2016.\n[29] Gabriel Ilharco, Rowan Zellers, Ali Farhadi, and Hannaneh Hajishirzi. Probing contextual\nlanguage models for common ground with visual representations. NAACL, 2021.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.\n[31] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images\nfor multimodal inputs and outputs. ICML, 2023.\n[32] Nathan Lambert, Louis Castricato, Leandro von Werra, and Alex Havrilla.\nIllustrat-\ning reinforcement learning from human feedback (rlhf).\nHugging Face Blog, 2022.\nhttps://huggingface.co/blog/rlhf.\n[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. ICML, 2023.\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. ECCV,\n2014.\n[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[36] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint\narXiv:2206.08916, 2022.\n[37] Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, and Peyman Milanfar. Distortion\nagnostic deep watermarking. In CVPR, 2020.\n[38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. ICML, 2022.\n[39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[40] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. NeurIPS, 2022.\n[41] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In ACL, 2002.\n[42] Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming\nXiong, and Ran Xu. Gluegen: Plug and play multi-modal encoders for x-to-image generation.\narXiv preprint arXiv:2303.10056, 2023.\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[46] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images\nwith vq-vae-2. NeurIPS, 2019.\n[47] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak\nLee. Generative adversarial text to image synthesis. In ICML, 2016.\n[48] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement\nlearning? arXiv preprint arXiv:2201.12122, 2022.\n12\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\n[53] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.\nACM Computing Surveys, 55(6):1\u201328, 2022.\n[54] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster,\nHuaixiu Steven Zheng, Neil Houlsby, and Donald Metzler.\nUnifying language learning\nparadigms. arXiv preprint arXiv:2205.05131, 2022.\n[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[56] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. NeurIPS, 2021.\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[58] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ICLR,\n2022.\n[59] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. ICLR, 2022.\n[60] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial\nnetworks. In CVPR, 2018.\n[61] Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. Doc: Improving long story\ncoherence with detailed outline control. arXiv preprint arXiv:2212.10077, 2022.\n[62] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang,\nMike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language\nmodeling. ICML, 2023.\n[63] Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Co-\nbit: A contrastive bi-directional image-text generation model. arXiv preprint arXiv:2303.13455,\n2023.\n[64] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu. Coca: Contrastive captioners are image-text foundation models. TMLR, 2022.\n[65] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. TMLR, 2022.\n[66] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal\ncontrastive learning for text-to-image generation. In CVPR, 2021.\n[67] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative\nadversarial networks. In ICCV, 2017.\n[68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreason-\nable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\n[69] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n13\n[70] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe\nfor watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023.\n[71] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for\nvision-language models. IJCV, 2022.\n[72] Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, and Jinhui Xu. Lafite2: Few-shot\ntext-to-image generation. arXiv preprint arXiv:2210.14124, 2022.\n[73] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxi-\nang Gu, Jinhui Xu, and Tong Sun. Lafite: Towards language-free training for text-to-image\ngeneration. CVPR, 2022.\n14\nA\nLimitations\nGILL relies on an LLM backbone for many of its capabilities. As such, it also inherits many of the\nlimitations that are typical of LLMs. One limitation is the potential for hallucinations [6], where\nthe model generates content that is false or not relevant to the input data. Another limitation of the\nmodel in generating text is in repetitions and neural text degeneration [27], where the model generates\nthe same content multiple times. We also observed that the OPT-6.7B model also does not always\nconsistently generate coherent dialogue text.\nThese limitations may be addressed by techniques that address hallucinations and degenerations in\ntext-only LLMs, or by using improved LLMs that are less prone to these issues. In GILL, we used\na 6.7B model. In the future, it will be valuable to scale up the approach with even larger LMs, or\nthose trained with improved objectives [54], instruction finetuning [58] or human feedback [40].\nDepending on downstream applications, using models trained explicitly on dialogue data [15] may\nalso be helpful for dialogue capabilities (e.g., deploying multimodal chatbots).\nWith regards to the visual models, another limitation of our approach is in its limited visual processing.\nAt the moment, we use only k = 4 visual vectors to represent each input image (due to computational\nconstraints), which may not capture all the relevant visual information needed for downstream tasks.\nThese vectors are produced by a frozen pre-trained visual encoder, and so the visual information in\nthe vectors is heavily constrained by the pre-training task. As a result, the model may not always\nprocess images correctly or in enough detail to produce accurate or high-quality results. However,\nthis limitation can potentially be addressed in the future by scaling up the visual model, using models\nwith varied pre-training objectives that encode more visual information while still being mappable to\nthe hidden space of the LLM, or using more sophisticated visual mappings [4, 33] that can capture a\nricher set of visual features. Similarly, we observed during inference that our model sometimes does\nnot generate relevant images for certain types of prompts. We attribute this to our finetuning dataset\nbeing CC3M, which is relatively small compared to modern large scale image-text datasets [51]. It is\nlikely that training GILLMapper on an even larger corpus of text data will improve its alignment to\nthe image generation backbone.\nOne of the advantages of our model is that it is modular, and can benefit from stronger visual and\nlanguage models released in the future. It is likely that it will also benefit from stronger text-to-image\ngeneration backbones, or through finetuning the generation backbone rather than just the GILLMapper\nmodule. We leave such scaling explorations for future work.\nB\nBroader Impact\nAI Assistants\nRecent advances in dialogue based chatbots have sparked interest in using LLMs for\ninteractive conversational applications. GILL is a multimodal language model capable of processing\nimage and text inputs, and producing image and text outputs. These capabilities may enable a wider\nrange of applications. For example, AI assistants which can produce image and text outputs would be\nable to answer a wider range of queries, providing visual content when necessary to illustrate certain\npoints. Concrete applications may include creative endeavors (e.g., iteratively refining a generated\nimage with instructions), answering questions that benefit from visual outputs (e.g., describing food\nitems), and more. Scaling GILL and refining it with methods such as reinforcement learning from\nhuman feedback (RLHF) [32] are promising directions to improve the capabilities of multimodal AI\nassistant systems.\nDisinformation and Harms\nAside from the technical limitations detailed in Sec. A, there are\nbroader societal issues that should be considered with the development of generative models of\ntext and images. LLMs have the potential to generate plausible sounding (but false) text [22, 6],\npropagating disinformation at scale. As GILL uses an LLM backbone, it is also susceptible to\nthese potential issues. Furthermore, as multimodal generative models which can also produce image\ncontent, models such as GILL also introduce potential issues with producing even more convincing\ndisinformation through interleaving text with realistic generated images. As GILL makes use of an\nimage generation backbone, it is also susceptible to the risks that typical text-to-image generation\nmodels introduce, such as generating false images of real people. These harms may possibly be\nmitigated by introducing watermarking into generated images [37, 70], or by deploying systems to\ndetect generated images [12].\n15\nBias and Safety\nGILL makes use of pretrained LLMs and multimodal models (such as CLIP [43]\nand Stable Diffusion [49]), which are trained on large, noisy, Internet-scraped data (such as LAION-\n400M [51]). Due to their curation process, these datasets often contain undesired biases, malignant\nstereotypes (see [8] for a comprehensive discussion on large scaled multimodal datasets). One\nadvantage of GILL is that it is efficient to train and completely modular, allowing its components\n(i.e., the LLM, visual encoder, or image generator) to be swapped out for other pretrained models (for\nexample, models which have been further calibrated to reduce unintended biases).\nIntended Uses\nGILL is a research prototype which showcases possible capabilities of multimodal\nlanguage models which can both process and produce image and text outputs. Due to the limitations\ndescribed above, GILL is not in its current state intended for deployment in practical applications,\nespecially in high risk or sensitive domains without further analysis. At its current model scale (a\n6.7B parameter LLM), GILL also lacks many of the abilities of larger language models [9], and\napplications would likely benefit from increased scaling of the LLM and visual models.\nC\nDeciding to Generate or Retrieve\nAs detailed in Sec. 3.3, we evaluate several models on the annotated PartiPrompts [65] dataset. Each\nprompt is annotated with one of two labels: \u201cret\u201d or \u201cgen\u201d, indicating whether image retrieval or\nimage generation produces a more appropriate image for the corresponding prompt. For example,\nthe prompt \u201ca portrait of a statue of the Egyptian god Anubis wearing aviator goggles, white t-shirt\nand leather jacket, flying over the city of Mars.\u201d is labeled as \u201cgen\u201d, as there are (understandably) no\nappropriate images in the CC3M retrieval set, and generation produces a more relevant output. In\ncontrast, \u201cthe geyser Old Faithful\u201d is labeled as \u201cret,\u201d as there are very relevant candidate images\navailable for this prompt. We evaluate several models for making this decision on the validation set\n(Tab. 6), evaluating using F1 score given the class imbalance of the dataset (201 \u201cgen\u201d, 110 \u201cret\u201d in\nthe validation set labels):\n1. Baselines: We measure the F1 score of several baseline methods, which provide a lower\nbound for how well data-driven approaches can do. We find that always retrieving an image,\nalways generating an image, or simply deciding randomly (with a prior proportional to class\nfrequencies) achieve F1 scores of 0.267, 0.389, and 0.451 respectively.\n2. Heuristic: We also consider a simple heuristic which considers the maximum cosine\nsimilarity of the retrieval embedding against the entire image candidate set (i.e., the training\nset of CC3M). We run a grid search from 0 to 1 for possible threshold values. Whenever the\nmaximum cosine similarity is above a threshold, we return \u201cret\u201d and \u201cgen\u201d otherwise. This\nachieves an F1 of 0.261 \u2013 0.559, depending on the threshold used (a threshold of 0.5 gives\nF1 of 0.261).\n3. Linear classifier: Lastly, we train a linear classifier that takes as input the outputs of the\nLLM for the [IMG] tokens and the maximum cosine similarity. This classifier is trained\nwith the binary cross-entropy loss over the training set of PartiPrompts annotations. This\nlinear classifier achieves an F1 score of between 0.393 \u2013 0.552, depending on the probability\nthreshold used (a threshold of 0.5 gives an F1 score of 0.547).\nWe use the linear classifier in our final model, as it requires less hyperparameter tuning compared\nto the heuristic baseline, and performs comparably on quantitative metrics. During generation of\nqualitative samples (Fig. 7 and Fig. 5 in the main paper), we observed that the linear classifier\ngenerally performed well for many prompts, and decided correctly whether to retrieve or generate.\nD\nQualitative Results\nWe present further qualitative samples in Fig. 7. We find that GILL is able to process complex\ntext prompts more effectively than Stable Diffusion for many examples in PartiPrompts [65]. On\nVisDial [16] dialogue inputs, GILL is able to generate more relevant outputs (as measured against\ngroundtruth images). We attribute these improved results to the stronger text representations of the\nLLM, and the effectiveness of our GILLMapper network.\n16\nTable 6: Results on PartiPrompts for classifying retrieval or generation.\nMethod\nF1\nAlways retrieve\n0.267\nAlways generate\n0.389\nRandom\n0.451\nHeuristic\n0.261 \u2013 0.559\nLinear classifier\n0.393 \u2013 0.552\nHuman performance\n0.851\nTable 7: Results on image captioning on MS-COCO (2017) [34] and VQA [24]. For captioning,\nwe report BLEU [41] and METEOR [5] scores. For VQA, we report the accuracy after applying\nthe normalization described in the VQA repo (https://github.com/GT-Vision-Lab/VQA). \u2020\nindicates our reimplementation.\nCaptioning\nVQA\nModel\nBLEU-4\nMETEOR\n0-shot\nFrozen\u2020 [56]\n-\n-\n0.2553\nMAGMA [19]\n-\n-\n0.2835\nFROMAGe [31]\n0.1023\n0.2873\n0.2851\nOurs\n0.1059\n0.2529\n0.3178\nE\nOther Evaluations\nE.1\nComparison to Prior Multimodal LMs\nWe ran evaluations on VQAv2 [24] and image captioning on MS-COCO [34]. The results are\npresented in Tab. 7. We found that GILL is comparable to models trained with similar compute and\ndata. On VQAv2, we achieve a zero-shot val accuracy of 0.3178, which is slightly better than prior\napproaches of similar model sizes and compute: FROMAGe [31] achieves a zero-shot accuracy of\n0.2851, Frozen [56] achieves 0.2553, and MAGMA [19] achieves 0.2835. For image captioning on\nthe MS-COCO (2017) validation set, GILL achieves a BLEU@4 of 0.1059 and METEOR of 0.2529,\nwhich is comparable to FROMAGe (BLEU@4 of 0.1023 and METEOR of 0.2873). GILL is also\ncapable of a wider set of tasks (e.g., generating interleaved image and text outputs) compared to these\nmodels.\nWe note that these scores are lower than SOTA models, as they are usually much larger and trained\nwith significantly more compute and data (e.g., Flamingo [4] uses 23,040 TPU days, BLIP-2 [33]\nuses 144 GPU days, while ours uses 4 GPU days). Scaling up GILL to similar data and parameter\nscales to further push its capabilities is an exciting avenue for future work.\nE.2\nIncreasing Context on VisDial\nGILL leverages an LLM backbone, which allows it to inherit some of the LLM\u2019s capabilities, such\nas improved sensitivity to long input contexts. In the main paper, we showed that GILL can better\ncondition on longer image and text inputs to generate more relevant images for VIST [28]. We run\na similar experiment on Visual Dialogue [16], varying the number of dialogue rounds provided as\ninput context to GILL and Stable Diffusion (SD) [49].\nThe results are presented in Fig. 8. We find that when longer text context is provided to both models,\nthe performance of generating relevant images steadily improves. Interestingly, SD performance\nplateaus after 6 rounds of dialogue, while GILL continues to improve, outperforming SD when 7\nor more rounds of dialogue are provided. These results showcase the improved sensitivity of our\nmodel to conditioning on long, dialogue-like text. Despite both approaches using the same image\ngeneration backbone, GILL is able to better make use of longer dialogue-text inputs (despite being\nonly finetuned on image caption data).\n17\nVisual Dialogue\nOur model can process long, dialogue-like text inputs to generate more relevant images compared to non-LLM based text-to-image generation models.\nComparison Against Stable Di\ufb00usion\nGILLMapper allows our model to map e\ufb00ectively to the SD image generation backbone, outperforming or matching SD for many examples from PartiPrompts.\nMultimodal Dialogue\nOur model can decide when to return retrieved images, generated images, or text, allowing it to respond e\ufb00ectively to a wider variety of dialogue settings.\nRetrieved\nGenerated\nUser prompts\nVisDial Inputs\nQ: is the \nman alone?\nA: yes, \nthe man is \nalone\nQ: is it \nsunny \noutside?\nA: no, it is \nnot sunny \noutside\n1\n2\n\u201cA digni\ufb01ed beaver wearing glasses, a \nvest, and colorful neck tie. He stands \nnext to a tall stack of books in a library.\u201d\nStable Di\ufb00usion\nOurs\n\u201cSnow mountain and tree re\ufb02ection in the lake\u201d\nStable Di\ufb00usion\nOurs\n\u201cA drop-top sports car coming around a bend \nin the road\u201d\nStable Di\ufb00usion\nOurs\nI am opening a pet shop, \nwhat would be a good logo \nfor my business?\nThis is nice. What about a \npicture of a dog instead?\nCan it be more colourful \nto attract customers?\nI want to paint a landscape \nwith mountains and a river. \nCan you help me \ufb01nd some \ninspiration?\nI\u2019m not sure\nCan you show me \nsome with horses?\nWhat about something \nmore abstract?\nI'm looking for some fashion \ninspiration for my upcoming \nvacation to Hawaii. Can you \nhelp me \ufb01nd some ideas?\nI prefer out\ufb01ts that are \nmore modest\nI'm a big fan\nThese are all so stylish! Can \nyou also show me some \nsummer dresses that would be \nperfect for my vacation\nWhat would be a matching \nout\ufb01t for my dog?\nI'm a big fan\nI need inspiration for my bridal \nhairstyle in a garden wedding.\nThat's lovely, can you \nshow me a braided look?\nI love the idea of a garden \nwedding, but I think you \nshould go with\nWhat out\ufb01t would go \nwell with this look?\nI love braids, but I think \nyou should go with\nI think you should go \nwith a white dress\nWhat hair accessories \nshould I wear?\nI think you should wear a \nheadband\nQ: what color \nis the \nsnowboard?\nA: the \nsnowboard is \ngrey in color\n3\nQ: is the man \nwearing a \ncap?\nA: the man is \nwearing a \nblack cap\n4\n...\nQ: what color \nare the \nglasses?\nA: the glasses \nare white in \ncolor\n8\nQ: can you \nsee the sky?\nA: no it's \ntotally dark\n9\nQ: does it \nlook like he's \nhaving fun?\nA: he seems to \nbe enjoying\n10\nGroundtruth\nOurs\nStable Di\ufb00usion\nVisDial Inputs\nQ: what color \nare the dogs?\nA: 1 of the dog \nis white and the \nother dog is \nlight brown\n1\n...\nQ: are they \nstanding in \ngrass?\nA: no, they \nare standing \non dirt\n8\nQ: are they \nlooking at each \nother?\nA: no, they are \nfacing away from \neach other\n9\nQ: do they \nseem like they \nlike each \nother?\nA: can't tell\n10\nGroundtruth\nQ: can you tell what \nbreed they are?\nA: i can't really \ntell what breed they \nare, perhaps german \nshepherd\n2\nOurs\nStable Di\ufb00usion\nQ: are they \nboth wearing \na hat?\nA: only 1 is \nwearing a hat\n3\nVisDial Inputs\nQ: is this a \nsingle \nperson \nbathroom?\nA: yes, it \nis\n1\n...\nQ: is there \na mirror?\nA: yes, \nthere is\n8\nQ: is it big \nor small?\nA: it is a \nlong full \nlength mirror\n9\nQ: what \ncolor are \nthe walls?\nA: they are \nbeige\n10\nGroundtruth\nQ: is there \ntoilet \npaper?\nA: not that \ni can see\n2\nOurs\nStable Di\ufb00usion\nQ: what \ncolor is \nthe \ntoilet?\nA: it is \nwhite\n3\nQ: is the seat \nup or down?\nA: there isn't \na seat at all\n4\nStable Di\ufb00usion\nOurs\n\u201cA raccoon wearing formal clothes, wearing a top hat \nand holding a cane. The raccoon is holding a garbage \nbag. Oil painting in the style of Vincent Van Gogh.\u201d\n\u201ca group of penguins in a snowstorm\u201d\nStable Di\ufb00usion\nOurs\n\u201cA tornado made of sharks crashing into a \nskyscraper. painting in the style of watercolor.\u201d\nStable Di\ufb00usion\nOurs\nFigure 7: Further qualitative samples from GILL. It is more sensitive to text inputs due to its LLM\nbackbone, and better at processing complex text prompts.\n18\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# Rounds of Dialogue\n0.50\n0.55\n0.60\n0.65\n0.70\nCLIP Similarity\nResults With Increasing Context on VisDial\nGILL (ours)\nStable Diffusion\nFigure 8: Performance of our model and Stable Diffu-\nsion [49] with increasing context for generating VisDial [16]\nimages. Our model is able to better process long dialogue-\nlike text descriptions.\nTable 8: Zero-shot FID [25] on the\nMS-COCO [34] (2014) validation\nset. 30,000 random samples are\nused to evaluate all models.\nModel\nFID (\u2193)\nGLIDE [38]\n12.24\nMake-A-Scene [21]\n11.84\nDALL-E 2 [44]\n10.39\nLAFITE2 [72]\n8.42\nImagen [50]\n7.27\nParti [65]\n7.23\nRe-Imagen [14]\n6.88\nSD [49] v1.5\n9.22\nGILL (ours)\n12.2\nE.3\nImage Generation\nIn addition to our evaluations on VIST [28] and VisDial [16], we also run evaluations on our\nmodel\u2019s ability to generate images from MS-COCO [34] captions (Tab. 8). We generate images\nusing 30,000 randomly sampled captions from the MS-COCO (2014) validation set, which is the\nstandard evaluation of text-to-image generation models. We report zero-shot FID scores [25] of our\nmodel, Stable Diffusion [49] v1.5 (which we use as our backbone image generator), and several other\napproaches in Tab. 8. For our generation results and SD results, we use a classifier-free guidance\nscaling factor of 3.0 and 250 DDIM inference steps. On MS-COCO, our approach achieves a worse\nFID score than SD (9.22 to 12.2). This is likely because this task does not benefit as much from the\nLLM backbone, which has not been trained on as many image captions as SD (which exclusively\ntrains on caption-like data). These numbers will likely improve further by finetuning GILL on even\nmore text data (including image captions), which will allow our model to align more closely to the\ninput space of the SD image generator.\nE.4\nInference Speed\nOne concern for deploying LLMs is the inference throughput and speed. We benchmark the inference\nperformance of GILL on a single A6000 GPU. Generating text has the same throughput as a regular\nLM of the same size (i.e., that of OPT 6.7B). The main increase in inference time occurs when\nthe model produces [IMG] tokens. For a batch size of 1, if the model decides to retrieve images,\nthe additional inference overhead is minimal (less than 0.001s on average) as image retrieval is\nfast, requiring just a single matrix multiplication followed by a max operation. However, if GILL\ngenerates an [IMG] token, it takes 3.5s on average for the Stable Diffusion backbone to generate a\ncorresponding image.\nOverall, GILL\u2019s inference speed is bottlenecked by the frequency of image generation, which is\ndependent on the application domain. In the case of generating dialogue-like text, we observed that\nimages are usually generated or retrieved once or twice in a natural conversation. Amortized over a\nlong conversation, it does not lead to a significant increase compared to a text-only LLM, though\nexact numbers would depend on the application.\nF\nHuman Annotation on PartiPrompts\nIn Sec. 3.3 of the main paper, we described the process of annotating PartiPrompts [65] with per-\nexample labels to retrieve or generate. The interface shown to human annotators is shown in Fig. 9.\nAnnotators are tasked to determine which of two anonymized images are (1) more relevant to the\nprovided prompt, and (2) more realistic. We randomize the order of the two images as well (i.e., the\noutput of the retrieval model shows up 50% of the time as Image A).\n19\nFigure 9: User interface shown to human annotators for annotating PartiPrompts [65] examples.\nWe show each example to 5 independent human annotators. For determining whether to label a\nparticular example as \u201cret\u201d or \u201cgen\u201d, we take the majority vote of the 5 annotators on the image\nrelevance question (\u201cIs image A or image B more relevant to the above caption?\u201d), and only keep\nthe examples with an inter-annotator agreement of at least 4/5. This results in approximately 900\nexamples remaining (out of the 1,632 examples in PartiPrompts). Our annotations will be publicly\nreleased to facilitate future evaluations on this task.\nWe conducted evaluations on the Amazon Mechanical Turk platform with human annotators located\nin the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD per hour. In total,\nwe spent approximately 326 USD to collect these annotations.\n20\n"
  },
  {
    "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
    "link": "https://arxiv.org/pdf/2305.18295.pdf",
    "upvote": "5",
    "text": "RAPHAEL: Text-to-Image Generation via\nLarge Mixture of Diffusion Paths\nZeyue Xue\u2217\nThe University of Hong Kong\nxuezeyue@connect.hku.hk\nGuanglu Song\u2217\nSenseTime Research\nsongguanglu@sensetime.com\nQiushan Guo\nThe University of Hong Kong\nqsguo@cs.hku.hk\nBoxiao Liu\nSenseTime Research\nliuboxiao@sensetime.com\nZhuofan Zong\nSenseTime Research\nzongzhuofan@gmail.com\nYu Liu\u2020 \u2021\nSenseTime Research\nliuyuisanai@gmail.com\nPing Luo\u2021\nThe University of Hong Kong\npluo@cs.hku.hk\n\u201cWhen one is painting one does not think.\u201d\n\u2014 Raffaello Sanzio da Urbino\nAbstract\nText-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of mixture-\nof-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling billions of\ndiffusion paths (routes) from the network input to the output. Each path intuitively\nfunctions as a \u201cpainter\u201d for depicting a particular textual concept onto a specified\nimage region at a diffusion timestep. Comprehensive experiments reveal that\nRAPHAEL outperforms recent cutting-edge models, such as Stable Diffusion,\nERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both image quality and\naesthetic appeal. Firstly, RAPHAEL exhibits superior performance in switching\nimages across diverse styles, such as Japanese comics, realism, cyberpunk, and\nink illustration. Secondly, a single model with three billion parameters, trained on\n1, 000 A100 GPUs for two months, achieves a state-of-the-art zero-shot FID score\nof 6.61 on the COCO dataset. Furthermore, RAPHAEL significantly surpasses\nits counterparts in human evaluation on the ViLG-300 benchmark. We believe\nthat RAPHAEL holds the potential to propel the frontiers of image generation\nresearch in both academia and industry, paving the way for future breakthroughs\nin this rapidly evolving field. More details can be found on a webpage: https:\n//raphael-painter.github.io/.\n*Equal contribution. Work done during Zeyue\u2019s internship at SenseTime Research.\n\u2020Project lead.\n\u2021Corresponding authors.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18295v5  [cs.CV]  10 Mar 2024\nRAPHAEL\nStableDiffusion XL\nDeepFloyd\nDALL-E 2\nERNIE-ViLG 2.0\nA\u00a0parrot\u00a0with\u00a0a\u00a0\npearl\u00a0earring,\u00a0\nVermeer\u00a0style.\nA\u00a0car\u00a0playing\u00a0\nsoccer,\u00a0digital\u00a0art.\nA\u00a0Pikachu\u00a0with\u00a0an\u00a0\nangry expression\u00a0\nand\u00a0red eyes,\u00a0with\u00a0\nlightning around\u00a0\nit,\u00a0hyper\u00a0realistic\u00a0\nstyle.\nThere\u00a0are five\u00a0cars\u00a0\nin\u00a0the\u00a0street.\nStreet\u00a0shot\u00a0of\u00a0a\u00a0\nfashionable\u00a0\nChinese\u00a0lady\u00a0in\u00a0\nShanghai,\u00a0wearing\u00a0\nblack high\u00adwaisted\u00a0\ntrousers.\nMoonlight\u00a0Maiden,\u00a0\ncute\u00a0girl\u00a0in\u00a0school\u00a0\nuniform,\u00a0long\u00a0white\u00a0\nhair,\u00a0standing\u00a0under\u00a0\nthe\u00a0moon,\u00a0celluloid\u00a0\nstyle,\u00a0Japanese\u00a0\nmanga\u00a0style.\nHalf\u00a0human,\u00a0half\u00a0\nrobot,\u00a0repaired\u00a0\nhuman,\u00a0human\u00a0\nflesh\u00a0warrior,\u00a0mech\u00a0\ndisplay,\u00a0man\u00a0in\u00a0\nmech,\u00a0cyberpunk.\nA\u00a0sign\u00a0that\u00a0says\u00a0\nRAPHAEL.\nFigure 1: Comparisons of RAPHAEL with recent representative generators, Stable Diffusion XL [2], Deep-\nFloyd, DALL-E 2 [3], and ERNIE-ViLG 2.0 [5]. They are given the same prompts, where the words that\nthe human artists yearn to preserve within the generated images are highlighted in red. These images are not\ncherry-picked. We see that previous models often fail to preserve the desired concepts. For example, only the\nRAPHAEL-generated images precisely reflect the prompts such as \u201cpearl earring, Vermeer\u201d, \u201cplaying soccer\u201d,\n\u201cfive cars\u201d, \u201cblack high-waisted trouser\u201d, \u201cwhite hair, manga, moon\u201d, and \u201csign, RAPHAEL\u201d, while other models\ngenerate compromised results. Better zoom in 200%.\n1\nIntroduction\nRecent advancements in text-to-image generators, such as Imagen [1], Stable Diffusion [2], DALL-\nE 2 [3], eDiff-I [4], and ERNIE-ViLG 2.0 [5], have yielded remarkable success and found wide\napplications in computer graphics, culture and art, and the generation of medical and biological data.\nDespite the substantial progress made in text-to-image diffusion models [1, 2, 3, 4, 5], there remains\na pressing need for research to further achieve more precise alignment between text and image. As\n2\nillustrated in Fig.1, existing models often fail to adequately preserve textual concepts within the\ngenerated images. This is primarily due to the reliance on a classic cross-attention mechanism for\nintegrating text descriptions into visual representations, resulting in relatively coarse control of the\ndiffusion process, and leading to compromised results.\nTo address this issue, we introduce RAPHAEL, a text-to-image generator, which yields images with\nsuperior artistry and fidelity compared to prior work, as demonstrated in Fig.2. RAPHAEL, an\nacronym that stands for \u201cdistinct image regions align with different text phases in attention learning\u201d,\noffers an appealing benefit not found in existing approaches.\nSpecifically, we observe that different text concepts influence distinct image regions during the\ngeneration process [6], and the conventional cross-attention layer often struggles to preserve these\nvarying concepts adequately in an image. To mitigate this issue, we employ a diffusion model\nstacking tens of mixture-of-experts (MoE) layers [7, 8], including both space-MoE and time-MoE\nlayers. Concretely, the space-MoE layers are responsible for depicting different concepts in specific\nimage regions, while the time-MoE layers focus on painting these concepts at different diffusion\ntimesteps.\nThis configuration leads to billions of diffusion paths from the network input to the output. Naturally,\neach path can act as a \u201cpainter\u201d responsible for rendering a particular concept to an image region at\na specific timestep. The result is a more precise alignment between text tokens and image regions,\nenabling the generated images that accurately represent the associated text prompt. This approach\nsets RAPHAEL apart from existing models and even sheds light on future studies of the explainability\nof the generation process. Additionally, we propose an edge-supervised learning module to further\nenhance the image quality and aesthetic appeal of the generated images.\nExtensive experiments demonstrate that RAPHAEL outperforms preceding approaches, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2. (1) RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk,\nand ink illustration. (2) RAPHAEL establishes a new state-of-the-art with a zero-shot FID-30k score\nof 6.61 on the COCO dataset. (3) RAPHAEL, a single model with three billion parameters trained\non 1, 000 A100 GPUs, significantly surpasses its counterparts in human evaluation on the ViLG-300\nbenchmark. (4) RAPHAEL is capable of generating images with resolutions up to 4096 \u00d7 6144 with\nrich image contents and details, when combined with a tailor-made SR-GAN model [9].\nThe contributions of this work are three-fold: (i) We propose a novel text-to-image generator,\nRAPHAEL, which, through the implementation of several carefully-designed techniques, generates\nimages that more accurately reflect textual prompts than previous works. (ii) We thoroughly explore\nRAPHAEL\u2019s potential for switching images in diverse styles, such as Japanese comics, realism,\ncyberpunk, and ink illustration, and for extension using LoRA [10], ControlNet [11], and SR-GAN\n[9]. (iii) We have released the demo of the latest version of RAPHAEL to the public*, which has\nbeen fine-tuned on more high aesthetics datasets. We believe that RAPHAEL holds the potential to\nadvance the frontiers of image generation in both academia and industry, paving the way for future\nbreakthroughs in this rapidly evolving field.\n2\nNotation and Preliminary\nWe present the necessary notations and the Denoising Diffusion Probabilistic Model (DDPM) [12]\nfor text-to-image generation. Given a collection of N images, denoted as {xi}N\ni=1, the aim is to learn\na generative model, p(x), that is capable of accurately representing the underlying distribution.\nIn forward diffusion, Gaussian noise is progressively introduced into the source images. At an\narbitrary timestep t, it is possible to directly sample from the Gaussian distribution following the\nT-step noise schedule {\u03b1t}T\nt=1, without iterative forward sampling. Consequently, the noisy image at\ntimestep t, denoted as xt, can be expressed as xt = \u221a1 \u2212 \u00af\u03b1tx0 + \u221a\u00af\u03b1t\u03f5t, where \u00af\u03b1t = Qt\ni=1 \u03b1i. In\nthis expression, x0 represents the source image, while \u03f5t \u223c N(0, I) indicates the Gaussian noise at\nstep t. In the reverse process, a denoising neural network, denoted as D\u03b8(\u00b7), is employed to estimate\nthe additive Gaussian noise. The optimization of this network is achieved by minimizing the loss\nfunction, Ldenoise = Et,x0,\u03f5\u223cN (0,I)\nh\n\u2225\u03f5 \u2212 D\u03b8 (xt, t)\u22252\n2\ni\n.\n*https://miaohua.sensetime.com/zh-CN\n3\nA\u00a0wizard\u00a0by\u00a0Q\u00a0Hayashida\u00a0in\u00a0the\u00a0style\u00a0of\u00a0\nDorohedoro for\u00a0Elden\u00a0Ring,\u00a0with\u00a0biggest\u00a0\nmost\u00a0intricate\u00a0sword,\u00a0on\u00a0sunlit\u00a0battlefield,\u00a0\nbreath\u00a0of\u00a0the\u00a0wild,\u00a0striking\u00a0illustration.\nA\u00a0beautiful\u00a0woman\u00a0dressed\u00a0in\u00a0a\u00a0dress\u00a0\nmade\u00a0of\u00a0autumn\u00a0leaves\u00a0in\u00a0the\u00a0forest,\u00a0\nphotography,\u00a0natural\u00a0lighting,\u00a0high\u00a0\ndetail.\nHarvest\u00a0of\u00a0vegetables in\u00a0a\u00a0wooden\u00a0box\u00a0\nnear\u00a0the\u00a0beds vegetables\u00a0grow\u00a0naturally,\u00a0\nsummer\u00a0light\u00a0background,\u00a0backlight\u00a0\nand\u00a0sun\u00a0rays, clean\u00a0sharp\u00a0focus.\nChinese\u00a0illustration,\u00a0oriental\u00a0landscape\u00a0\npainting,\u00a0above\u00a0super\u00a0wide\u00a0angle,\u00a0magical,\u00a0\nromantic,\u00a0detailed,\u00a0colorful,\u00a0multi\u00ad\ndimensional\u00a0paper\u00a0kirigami craft.\nPhotography\u00a0closeup\u00a0portrait\u00a0of\u00a0an\u00a0\nadorable\u00a0rusty broken\u00addown\u00a0steampunk\u00a0\nrobot covered\u00a0in\u00a0budding\u00a0vegetation,\u00a0\nsurrounded\u00a0by\u00a0tall\u00a0grass,\u00a0misty\u00a0futuristic\u00a0\nsci\u00adfi\u00a0forest\u00a0environment.\nThe\u00a0Caped\u00a0Crusader,\u00a0Gotham\u00a0skyline,\u00a0rooftop,\u00a0\nmysterious,\u00a0powerful,\u00a0nighttime,\u00a0mixed\u00a0media,\u00a0\nexpressionism,\u00a0dark\u00a0tones,\u00a0high\u00a0contrast,\u00a0in\u00a0\nthe\u00a0style\u00a0of\u00a0comic\u00a0book\u00a0artist\u00a0Frank\u00a0Miller,\u00a0\nmodern,\u00a0gritty\u00a0and\u00a0textured,\u00a0collage\u00a0technique.\nThe\u00a0Goddess\u00a0of\u00a0high\u00a0fashion,\u00a0\nimpressionistic\u00a0line\u00a0art,\u00a0contrasting earth\u00a0\ntones,\u00a0vibrant,\u00a0pen\u00a0and\u00a0ink illustration,\u00a0\nink\u00a0splatter,\u00a0abstract expressionism\u00a0\nsuperimposed\u00a0onto\u00a0majestic\u00a0space\u00a0queen.\nA\u00a0cute\u00a0little\u00a0matte\u00a0low\u00a0poly\u00a0isometric\u00a0\nZelda\u00a0Breath\u00a0of\u00a0the\u00a0wild\u00a0forest\u00a0island,\u00a0\nwaterfalls,\u00a0soft\u00a0shadows,\u00a0trending\u00a0on\u00a0\nArtstation,\u00a03d\u00a0render,\u00a0monument\u00a0valley,\u00a0\nfez\u00a0video\u00a0game.\nMilkyway in\u00a0a\u00a0glass\u00a0bottle,\u00a04k,\u00a0unreal\u00a0\nengine,\u00a0octane\u00a0render.\nFigure 2: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles. The synthesized images have rich details and semantics. The prompts were written by human\nartists without cherry-picking.\nBy employing the Bayes\u2019 theorem, it is feasible to iteratively estimate the image at timestep\nt \u2212 1 through sampling from the posterior distribution, p\u03b8(xt\u22121|xt).\nWe have xt\u22121\n=\n1\n\u221a\u03b1t\n\u0010\nxt \u2212\n1\u2212\u03b1t\n\u221a1\u2212\u00af\u03b1t D\u03b8 (xt, t)\n\u0011\n+ \u03c3tz, where \u03c3t signifies the standard deviation of the newly injected\nnoise into the image at each step, and z represents the Gaussian noise.\nIn essence, the denoising neural network estimates the score function at varying time steps, thereby\nprogressively recovering the structure of the image distribution. The fundamental insight provided by\nthe DDPM lies in the fact that the perturbation of data points with noise serves to populate regions\nof low data density, ultimately enhancing the accuracy of estimated scores. This results in stable\ntraining and sampling.\n4\n1\nSelf\nAttention\nCross\nAttention\nTime \nMoE\nSpace\nMoE\n(a) Transformer Block\nAttn map M\nA furry bear \nunder sky\n\ud835\udc3f!\"#!\nN x Conv\n(b) Space-MoE \nA furry bear under sky\nText Gate\nBear\nSky\nFeatures\nAttn map\nFurry\nMask\nExpert 1\nExpert n\nExpert 2\n\u2026\n\u2744\n\u2744\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\u2744\nM\u2217,#\nM\u2217,$\nM\u2217,%\nM! #\nM! $\nM! %\nFigure 3: Framework of RAPHAEL. (a) Each block contains four primary components including a self-\nattention layer, a cross-attention layer, a space-MoE layer, and a time-MoE layer. The space-MoE is responsible\nfor depicting different text concepts in specific image regions, while the time-MoE handles different diffusion\ntimesteps. Each block uses edge-supervised cross-attention learning to further improve image quality. (b)\nshows details of space-MoE. For example, given a prompt \u201ca furry bear under sky\u201d, each text token and its\ncorresponding image region (given by a binary mask) are directed through distinct space experts, i.e., each\nexpert learns particular visual features at a region. By stacking several space-MoEs, we can easily learn to depict\nthousands of text concepts.\nU-Net with Text Prompts. The denoising network is commonly implemented using a U-Net [13]\narchitecture, as depicted in Fig.8 in Appendix 7.3. To incorporate textual prompts (denoted by y) into\nthe U-Net, a text encoder neural network, E\u03b8(y), is employed to extract the textual representation.\nThe extracted text tokens are input into the U-Net through a cross-attention layer. The text tokens\npossess a size of ny \u00d7 dy, where ny represents the number of text tokens, and dy signifies the\ndimension of a text token (e.g., dy = 768 in [14]).\nThe cross-attention layer can be formulated as attention(Q, K, V) = softmax\n\u0010\nQK\u22a4\n\u221a\nd\n\u0011\nV, where\nQ, K, and V correspond to the query, key, and value matrices, respectively. These matrices are\ncomputed as Q = h (xt) Wqry\nx , K = E\u03b8(y)Wkey\ny , and V = E\u03b8(y)Wval\ny , where Wqry\nx\n\u2208 Rd\u00d7d and\nWkey\ny , Wval\ny \u2208 Rdy\u00d7d represent the parametric projection matrices for the image and text, respectively.\nAdditionally, d denotes the dimension of an image token, h(xt) \u2208 Rnx\u00d7d indicates the flattened\nintermediate representation within the U-Net, with nx being the number of tokens in an image. A\ncross-attention map between the text and image, M = softmax\n\u0010\nQK\u22a4\n\u221a\nd\n\u0011\n\u2208 Rnx\u00d7ny, is defined,\nwhich plays a crucial role in the proposed approach, as described in the following sections.\n3\nOur Approach\nThe overall framework of RAPHAEL is illustrated in Fig.3, with the network configuration details\nprovided in the Appendix 7.1. Employing a U-Net architecture, the framework consists of 16\ntransformer blocks, each containing four components: a self-attention layer, a cross-attention layer, a\nspace-MoE layer, and a time-MoE layer. The space-MoE is responsible for depicting different text\nconcepts in specific image regions at a given scale, while the time-MoE handles different diffusion\ntimesteps.\n3.1\nSpace-MoE and Time-MoE\nSpace-MoE. Regarding the space-MoE layer, distinct text tokens correspond to various regions\nwithin an image, as previously mentioned. For instance, when provided with the prompt \u201ca furry\nbear under the sky\u201d, each text token and its corresponding image region (represented by a binary\nmask) are fed into separate experts, as illustrated in Fig.3b. The space-MoE layer\u2019s output is the\nmean of all experts, calculated using the following formula:\n1\nny\nPny\ni=1 eroute(yi)\n\u0010\nh\u2032(xt) \u25e6 c\nMi\n\u0011\n. In\nthis equation, c\nMi is a binary two-dimensional matrix, indicating the image region the i-th text token\nshould correspond to, as shown in Fig.3b. Here, \u25e6 represents hadamard product, and h\u2032(xt) is the\nfeatures from time-MoE. The gating (routing) function route(yi) returns the index of an expert in\nthe space-MoE, with {e1, e2, . . . , ek} being a set of k experts.\nText Gate Network. The Text Gate Network is employed to distribute an image region to a specific\nexpert, as shown in Fig.3b. The function route(yi) = argmax (softmax (G (E\u03b8(yi)) + \u03f5)) is used,\n5\nExpert index:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\nExpert index:\nBlock index:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\nBlock index:\n0\ndonut\nbanana\ncake\nchair\nhorse\nlaptop\norange\npizza\ntrain\naeroplane\nFigure 4: Left: We visualize the diffusion paths (routes) from the network input to the output, utilizing 16\nspace-MoE layers, each containing 6 space experts. These paths are closely associated with 100 adjectives,\nsuch as \u201cscenic\u201d, \u201cpeaceful\u201d, and \u201cmajestic\u201d, which represent the most frequently occurring adjectives for\ndescribing artworks as suggested by GPT-3.5 [15, 16]. Given that GPT-3.5 has been trained on trillions of\ntokens, we believe that these adjectives reflect a diverse, real-world distribution. Our findings indicate that\ndifferent paths distinctively represent various adjectives. Right: We depict the diffusion paths for ten categories\n(i.e., nouns) within the COCO dataset. Our observations reveal that different categories activate distinct paths in\na heterogeneous manner. The display colors blend together where the routes overlap.\nwhere G : Rdy 7\u2192 Rk is a feed forward network, which uses a text token representation E\u03b8(yi) as\ninput and assigns a space expert. To prevent mode collapse, random noise \u03f5 is incorporated. The\nargmax function ensures that one expert exclusively handles the corresponding image region for\neach text token, without increasing computational complexity.\nFrom Text to Image Region. Recall that M is the cross-attention map between text and image, where\neach element, Mj,i, represents a correspondence value between the j-th image token and the i-th\ntext token. In the space-MoE, each entry in the binary mask c\nMi equals \u201c1\u201d if Mj,i \u2265 \u03b7i, otherwise\n\u201c0\u201d if Mj,i < \u03b7i, as illustrated in Fig.3b. A thresholding mechanism is introduced to determine the\nvalues in the mask. The threshold value \u03b7i = \u03b1 max(M\u2217,i) is defined, where max(M\u2217,i) represents\nthe maximum correspondence between text token i and all image regions. The hyper-parameter \u03b1\nwill be evaluated through an ablation study.\nDiscussions. The insight behind the space-MoE is to effectively model the intricate relationships\nbetween text tokens and their corresponding regions in the image, accurately reflecting concepts in the\ngenerated images. As illustrated in Fig.4, the employment of 16 space-MoE layers, each containing 6\nexperts, results in billions of spatial diffusion paths (i.e., 616 possible routes). It is evident that each\ndiffusion path is closely associated with a specific textual concept.\nTo investigate this further, we generate 100 prevalent adjectives that are the most frequently occurring\nadjectives for describing artworks as suggested by GPT-3.5 [15, 16]. Given that GPT-3.5 has been\ntrained on trillions of tokens, we posit that these adjectives reflect a diverse, real-world distribution.\nWe input each adjective into the RAPHAEL model with prompt templates given by GPT-3.5 to\ngenerate 100 distinct images and collect their corresponding diffusion paths. Consequently, we obtain\nten thousand paths for the 100 words. By treating these pathways as features (i.e., each path is a\nvector of 16 entries), we train a straightforward classifier (e.g., XGBoost [17]) to categorize the words.\nThe classifier after 5-fold cross-validation achieves over 93% accuracy for open-world adjectives,\ndemonstrating that different diffusion paths distinctively represent various textual concepts. We\nobserve analogous phenomena within the 80 object categories of the COCO dataset. Further details\non verbs and visualization are provided in the Appendix 7.5.\nTime-MoE. We can further enhance the image quality by employing a time-mixture-of-experts\n(time-MoE) approach, which is inspired by previous works such as [4, 5]. Given that the diffusion\nprocess iteratively corrupts an image with Gaussian noise over a series of timesteps t = 1, . . . , T,\nthe image generator is trained to denoise the images in reverse order from t = T to t = 1. All\ntimesteps aim to denoise a noisy image, progressively transforming random noise into an artistic\nimage. Intuitively, the difficulty of these denoising steps varies depending on the noise ratio presented\nin the image. For example, when t = T, the denoising network\u2019s input image xt is highly noisy.\nWhen t = 1, the image xt is closer to the original image.\nTo address this issue, we employ a time-MoE before each space-MoE in each transformer block.\nIn contrast to [4, 5] , which necessitate hand-crafted time expert assignments, we implement an\n6\nadditional gate network to automatically learn to assign different timesteps to various time experts.\nFurther details can be found in the Appendix 7.3.\n3.2\nEdge-supervised Learning\nIn order to further enhance the image quality, we propose incorporating an edge-supervised learning\nstrategy to train the transformer block. By implementing an edge detection module, we aim to extract\nrich boundary information from an image. These intricate boundaries can serve as supervision to\nguide the model in preserving detailed image features across various styles.\nConsider a neural network module, P\u03b8(M), with parameters of N convolutional layers (e.g., N = 5).\nThis module is designed to predict an edge map given an attention map M (refer to Fig.7a in the\nAppendix 7.2). We utilize the edge map of the input image, denoted as Iedge, to supervise the network\nP\u03b8. Iedge can be obtained by the holistically-nested edge detection algorithm [18] (Fig.7b). Intuitively,\nthe network P\u03b8 can be trained by minimizing the loss function, Ledge = Focal(P\u03b8(M), Iedge), where\nFocal(\u00b7, \u00b7) denotes the focal loss [19] employed to measure the discrepancy between the predicted\nand the \u201cground-truth\u201d edge maps. Moreover, as discussed in [5, 6], the attention map M is prone\nto becoming vague when the timestep t is large. Consequently, it is essential to adopt a timestep\nthreshold value to inactivate (pause) edge-supervised learning when t is large. This timestep threshold\nvalue (Tc) is a hyper-parameter that will be evaluated through an ablation study.\nOverall, the RAPHAEL model is trained by combining two loss functions, L = Ldenoise + Ledge.\nAs demonstrated in Fig.7d in the Appendix 7.2, edge-supervised learning substantially improves the\nimage quality and aesthetic appeal of the generated images.\n4\nExperiments\nThis section presents the experimental setups, the quantitative results compared to recent state-of-\nthe-art models, and the ablation study to demonstrate the effectiveness of RAPHAEL. More artistic\nimages generated by RAPHAEL and comparisons between RAPHAEL and other diffusion models\ncan be found in Appendix 7.6 and 7.7.\nDataset. The training dataset consists of LAION-5B [20] and some internal datasets. To collect\ntraining data from LAION-5B, we filter the images using the aesthetic scorer same as Stable Diffusion\n[2] and remove the image-text pairs that have scores smaller than 4.7. We remove the images with\nwatermarks either. Since the text descriptions in LAION-5B are noisy, we clean them by removing\nuseless information such as URLs, HTML tags, and email addresses, inspired by [2, 4, 21].\nMulti-scale Training. To improve text-image alignment, instead of cropping images to a fixed\nscale [2], we resize an image to its nearest size in a bucket, which has 9 different image scales\u2020.\nAdditionally, the GPU resources will be automatically allocated to each bucket depending on the\nnumber of images it contains, enabling effective use of computational resources.\nImplementations. To reduce training and sampling complexity, we use a Variational Autoencoder\n(VAE) [22, 23] to compress images using Latent Diffusion Model [2]. We first pre-train an image\nencoder to transform an image from pixel space to a latent space, and an image decoder to convert it\nback. Unlike previous works, the cross-attention layers in RAPHAEL are augmented with space-MoE\nand time-MoE layers. The entire model is implemented in PyTorch [24], and is trained by AdamW\n[25] optimizer with a learning rate of 1e \u2212 4, a weight decay of 0, a batch size of 2, 000, on 1, 000\nNVIDIA A100s for two months. More details on the hyper-parameter settings can be found in the\nAppendix 7.1.\n4.1\nComparisons\nResults on COCO. Following previous works [1, 2, 4], we evaluate RAPHAEL on the COCO\n256 \u00d7 256 dataset using zero-shot Frechet Inception Distance (FID), which measures the quality\nand diversity of images. Similar to [1, 2, 4, 5, 31], 30, 000 images are randomly selected from\nthe validation set for evaluation. Table 1 shows that RAPHAEL achieves a new state-of-the-art\n\u2020The [height, width] for each bucket is [448, 832], [512, 768], [512, 704], [640, 640], [576, 640], [640, 576],\n[704, 512], [768, 512], and [832, 448].\n7\nTable 1: Comparisons of RAPHAEL with the recent representative text-to-image generation models on the\nMS-COCO 256 \u00d7 256 using zero-shot FID-30k. We see that RAPHAEL outperforms all previous works in\nimage quality, even a commercial product released recently.\nApproach\nVenue/Date\nModel Type\nFID-30K\nZero-shot FID-30K\nDF-GAN [26]\nCVPR\u201922\nGAN\n21.42\n-\nDM-GAN + CL [27]\nCVPR\u201919\nGAN\n20.79\n-\nLAFITE [28]\nCVPR\u201922\nGAN\n8.12\n-\nMake-A-Scene [29]\nECCV\u201922\nAutoregressive\n7.55\n-\nLDM [2]\nCVPR\u201922\nDiffusion\n-\n12.63\nGLIDE [30]\nICML\u201922\nDiffusion\n-\n12.24\nDALL-E 2 [3]\narXiv, April 2022\nDiffusion\n-\n10.39\nStable Diffusion [2]\nCVPR\u201922\nDiffusion\n-\n8.32\nMuse-3B [31]\narXiv, Jan. 2023\nNon-Autoregressive\n-\n7.88\nImagen [1]\nNeurIPS\u201922\nDiffusion\n-\n7.27\neDiff-I [4]\narXiv, Nov. 2022\nDiffusion Experts\n-\n6.95\nERNIE-ViLG 2.0 [5]\nCVPR\u201923\nDiffusion Experts\n-\n6.75\nDeepFloyd\nProduct, May 2023\nDiffusion\n-\n6.66\nRAPHAEL\n-\nDiffusion Experts\n-\n6.61\nFigure 5: Comparisons of RAPHAEL with DALL-E 2, Stable Diffusion XL (SD XL), ERNIE-ViLG 2.0, and\nDeepFloyd in a user study using the ViLG-300 benchmark. We report the user\u2019s preference rates with 95%\nconfidence intervals. We see that RAPHAEL can generate images with higher quality and better conform to the\nprompts.\nperformance of text-to-image generation, with 6.61 zero-shot FID-30k on MS-COCO, surpassing\nprominent image generators such as Stable Diffusion, Imagen, ERNIE-ViLG 2.0, and DALL-E 2.\nHuman Evaluations. We employ the ViLG-300 benchmark [5], a bilingual prompt set, which\nenables to systematically evaluate text-to-image models given various text prompts in Chinese and\nEnglish. ViLG-300 allows us to convincingly compare RAPHAEL with recent-advanced models\nincluding DALL-E 2, Stable Diffusion, ERNIE-ViLG 2.0, and DeepFloyd, in terms of both image\nquality and text-image alignment. For example, human artists are presented with two sets of images\ngenerated by RAPHAEL and a competitor, respectively. They are asked to compare these images\nfrom two aspects respectively, including image-text alignment, and image quality and aesthetics.\nThroughout the entire process, human artists are unaware of which model the image is generated\nfrom. Fig.5 shows that RAPHAEL surpasses all other models in both image-text alignment and\nimage quality in the user study, indicating that RAPHAEL can generate high-artistry images that\nconform to the text.\nExtensions to LoRA, ControlNet, and SR-GAN. RAPHAEL can be further extended by incorporat-\ning LoRA, ControlNet, and SR-GAN. In Appendix 7.8, we present a comparison between RAPHAEL\nand Stable Diffusion utilizing LoRA. RAPHAEL demonstrates superior robustness against overfitting\ncompared to Stable Diffusion. We also demonstrate RAPHAEL with a canny-based ControlNet.\nFurthermore, by employing a tailormade SR-GAN model, we enhance the image resolution to\n4096 \u00d7 6144.\n4.2\nAblation Study\nEvaluate every module in RAPHAEL. We conduct a comprehensive assessment of each module\nwithin the RAPHAEL model, utilizing the CLIP [14] score to measure image-text alignment. Given\nthe significance of classifier-free guidance weight in controlling image quality and text alignment,\nwe present ablation results as trade-off curves between CLIP and FID scores across a range of\nguidance weights [32], specifically 1.5, 3.0, 4.5, 6.0, 7.5, and 9.0. Fig.6b compares these curves for\n8\n(a) Impact of ! and \"!. \n(c) Impact of experts.\n(b) FID-CLIP score curves.\nFigure 6: Ablation Study. (a) examines the selection of \u03b1 and Tc. (b) presents the trade-off between FID\nand CLIP scores for the complete RAPHAEL model and its variants without space-MoE, time-MoE, and\nedge-supervised learning. (c) visualizes the correlation between FID-5k and runtime complexity (measured\nin terms of the number of DDIM [33] steps for an image per second) as a function of the number of experts\nemployed. Notably, the computational complexity is predominantly influenced by the number of space experts.\nthe complete RAPHAEL model and its variants without space-MoE, edge-supervised learning, and\ntime-MoE, respectively. Our findings indicate that all modules contribute effectively. For example,\nspace-MoE substantially enhances the CLIP score and the optimal guidance weight for the sampler\nshifts from 3.0 to 4.5. Moreover, at the same guidance weight, space-MoE considerably reduces the\nFID, resulting in a significant improvement in image quality.\nChoice of \u03b1 and Tc. As depicted in Fig.6a, we observe that \u03b1 = 0.2 delivers the best performance,\nimplying a balance between preserving adequate features and avoiding the use of the entire latent\nfeatures. An appropriate threshold value for Tc terminates edge-supervised learning when the\ndiffusion timestep is large. Our experiments reveal that a suitable choice for Tc is 500, ensuring the\neffective learning of texture information.\nPerformance and Runtime Analysis on Number of Experts. We offer an examination of the\nnumber of experts, ranging from 0 to 8, in Fig.6c. For each setting, we employ 100 million training\nsamples. Our results demonstrate that increasing the number of experts improves FID (lower\nvalues are preferable). However, adding space experts introduces additional computations, with the\ncomputational complexity bounded by the total number of experts. Once all available experts have\nbeen deployed, the computational complexity ceases to grow. In the right-hand side of Fig.6c, we\nprovide a runtime analysis for 40 input tokens, ensuring the utilization of all space experts. For\ninstance, when the number of experts is 6, the inference speed decreases by 24% but yields superior\nfidelity. This remains faster than previous diffusion models such as Imagen [1] and eDiff-I [4].\n5\nRelated Work\nWe review related works from two perspectives, mixture-of-experts and text-to-image generation.\nMore related works can be found in Appendix 7.4. Firstly, the Mixture-of-Experts (MoE) method\n[7, 8] partitions model parameters into distinct subsets, each termed an \u201cexpert\u201d. The MoE paradigm\nfinds applicability beyond language processing tasks, extending to visual models [34] and Mixture-\nof-Modality-Experts within multi-modal transformers [35]. Additionally, efforts are being made to\naccelerate the training or inference processes for MoE [36, 37]. Secondly, text-to-image generation\nis to synthesize images from natural language descriptions. Early approaches relied on generative\nadversarial networks (GANs) [38, 39, 40, 41] to generate images. More recently, with the transfor-\nmative success of transformers in generative tasks, models such as DALL-E [42], Cogview [43],\nand Make-A-Scene [29] have treated text-to-image generation as a sequence-to-sequence problem,\nutilizing auto-regressive transformers as generators and employing text/image tokens as input/output\nsequences. Recently, another research direction has focused on diffusion models by integrating\ntextual conditioning within denoising steps, like Stable Diffusion [2], DALL-E 2 [3], eDiff-I [4],\nERNIE-ViLG 2.0 [5], and Imagen [1].\n9\n6\nConclusion\nThis paper introduces RAPHAEL, a novel text-conditional image diffusion model capable of gen-\nerating highly-artistic images using a large-scale mixture of diffusion paths. We carefully design\nspace-MoE and time-MoE within an edge-supervised learning framework, enabling RAPHAEL to\naccurately portray text prompts, enhance the alignment between textual concepts and image regions,\nand produce images with superior aesthetic appeal. Comprehensive experiments demonstrate that\nRAPHAEL surpasses previous approaches, such as Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd,\nand DALL-E 2, in both FID-30k and the human evaluation benchmark ViLG-300. Additionally,\nRAPHAEL can be extended using LoRA, ControlNet, and SR-GAN. We believe that RAPHAEL has\nthe potential to advance image generation research in both academia and industry.\nLimitation and Potential Negative Societal Impact. The potential negative social impact is to\nuse the RAPHAEL API to create images containing misleading or false information. This issue\npotentially presents in all powerful text-to-image generators. We will solve this issue (e.g., by prompt\nfiltering) before releasing the API to the public.\nReferences\n[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[3] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[5] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu,\nJiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image\ndiffusion model with knowledge-enhanced mixture-of-denoising-experts.\narXiv preprint\narXiv:2210.15257, 2022.\n[6] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[7] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity. The Journal of Machine Learning Research,\n23(1):5232\u20135270, 2022.\n[9] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world\nblind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1905\u20131914, 2021.\n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[11] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n10\n[13] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks\nfor biomedical image segmentation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,\n2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\n[14] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021.\n[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.\n[16] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[17] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong\nChen, Rory Mitchell, Ignacio Cano, Tianyi Zhou, et al. Xgboost: extreme gradient boosting. R\npackage version 0.4-2, 1(4):1\u20134, 2015.\n[18] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1395\u20131403, 2015.\n[19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense\nobject detection. In Proceedings of the IEEE International Conference on Computer Vision,\npages 2980\u20132988, 2017.\n[20] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[21] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao,\nHang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale.\narXiv preprint arXiv:2303.06555, 2023.\n[22] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nNeural Information Processing Systems, 30, 2017.\n[23] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda-\ntions and Trends in Machine Learning, 12(4):307\u2013392, 2019.\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in Neural Information Processing\nSystems, 32, 2019.\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[26] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan:\nA simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 16515\u201316525, 2022.\n[27] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative\nadversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5802\u20135810, 2019.\n[28] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxi-\nang Gu, Jinhui Xu, and Tong Sun. Lafite: Towards language-free training for text-to-image\ngeneration. arXiv preprint arXiv:2111.13792, 2021.\n[29] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XV, pages 89\u2013106. Springer, 2022.\n11\n[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[31] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\n[32] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n[33] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[34] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9\nSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\nAdvances in Neural Information Processing Systems, 34:8583\u20138595, 2021.\n[35] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling\nvision-language models with sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023.\n[36] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast\nmixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021.\n[37] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n[38] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and\nAnil A Bharath. Generative adversarial networks: An overview. IEEE Signal Processing\nMagazine, 35(1):53\u201365, 2018.\n[39] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.\nHigh-resolution image synthesis and semantic manipulation with conditional gans. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8798\u20138807,\n2018.\n[40] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 8110\u20138119, 2020.\n[41] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020.\n[42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[43] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via\ntransformers. Advances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\n[44] Zeyue Xue, Jianming Liang, Guanglu Song, Zhuofan Zong, Liang Chen, Yu Liu, and Ping Luo.\nLarge-batch optimization for dense visual predictions. arXiv preprint arXiv:2210.11078, 2022.\n[45] David Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a\ndeep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\n[46] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\nlanguage models with mixture-of-experts. In International Conference on Machine Learning,\npages 5547\u20135569. PMLR, 2022.\n[47] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and\nLuke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language\nmodels. arXiv preprint arXiv:2208.03306, 2022.\n[48] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and\nTaesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10124\u201310134, 2023.\n12\n[49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022.\n[50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n[51] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022.\n[52] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K Wong. Vico: Detail-preserving visual\ncondition for personalized text-to-image generation. arXiv preprint arXiv:2306.00971, 2023.\n[53] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren\nZhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation.\narXiv preprint arXiv:2303.05125, 2023.\n[54] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli\nZhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple\nsubjects. arXiv preprint arXiv:2305.19327, 2023.\n[55] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor:\nZero-shot object-level image customization. arXiv preprint arXiv:2307.09481, 2023.\n[56] Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, and Lequan Yu.\nMake-a-volume: Leveraging latent diffusion models for cross-modality 3d brain mri synthesis.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention,\npages 592\u2013601. Springer, 2023.\n[57] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d\ncontent creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 300\u2013309, 2023.\n[58] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using\n2d diffusion. arXiv preprint arXiv:2209.14988, 2022.\n[59] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun\nShen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with\nmotion controllability. arXiv preprint arXiv:2306.02018, 2023.\n[60] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22563\u201322575, 2023.\n[61] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion\nmodels for audio-driven co-speech gesture generation.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10544\u201310553, 2023.\n[62] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and\nKwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models.\nAdvances in Neural Information Processing Systems, 2023.\n13\n7\nAppendix\n7.1\nHyper-parameters and Values\nWe give the hyper-parameters and values in Table 2.\nTable 2: Hyper-parameters and values in RAPHAEL.\nConfigs/Hyper-parameters\nValues\nT\n1000\nny\n77\ndy\n1024\nTc\n500\n\u03b1\n0.2\nBetas of AdamW [25]\n(0.9, 0.999)\nWeight decay\n0.0\nLearning rate\n1e-4\nNumber of space experts\n6\nNumber of time experts\n4\nWarmup steps\n20000\nBatch size\n2000\nNumber of GPUs\n1000\nNumber of transformer blocks\n16\nUse checkpoint\nTrue\n\u03b1 in Focal Loss [19]\n0.5\n\u03b3 in Focal Loss [19]\n2\nText encoder\nOpenCLIP-g/14 [14]\nEnable multi-scale training\nTrue\nActivations in experts and gate network\nGELU\nArchitectures of experts and gate network\nFFN\n7.2\nDetails of Edge-supervised Learning\nWe provide some demonstrations of edge-supervised learning in Fig.7.\n(a) Attention map.\n(b) Edge map.\n(d) Human evaluation.\n(c) Image.\nFigure 7: From left to right, we display the attention map corresponding to the pooled token in CLIP, the ground\ntruth edges identified by the edge detection algorithm, and the associated image. In the fourth figure, we present\nthe human evaluation results for models with and without edge-supervised learning on the ViLG-300 benchmark.\nEvaluators are instructed to compare these images considering image aesthetics and we report the preference\nrates, and our findings indicate that edge-supervised learning significantly enhances the aesthetic quality of the\nimages.\n7.3\nDetails on Time-MoE\nThe overall architecture of RAPHAEL can be found in Fig.8. The Time-MoE is composed of a\nTime Gate Network to distribute the features to a specific expert according to the timestep, which\ncan be formulated as h\u2032(xt) = tet_router(ti) (hc(xt)). In this equation, hc(xt) is the features from\n14\n(b) Transformer Block\n1\nSelf\nAttention\nCross\nAttention\nTime \nMoE\nSpace\nMoE\nAttn map M\nA furry bear \nunder sky\n\ud835\udc3f!\"#!\nN x Conv\nA furry bear under sky\nText Gate\nBear\nSky\nFeatures\nAttn map\nFurry\nMask\nExpert 1\nExpert n\nExpert 2\n\u2026\n\u2744\n\u2744\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\u2744\nM\u2217,#\nM\u2217,$\nM\u2217,%\nM! #\nM! $\nM! %\n(a) U-Net\nTransformer\nBlock\n\ud835\udc65!\n\ud835\udc65\"\nA furry bear \nunder sky\nTransformer\nBlock\n(c) Space-MoE \nExpert 1\nExpert 2\nExpert n\nFeatures\nTime Step\n\u2026\n\ud83d\udd25\nFFN\n(d) Time-MoE \nFigure 8: (a) The architecture of U-Net, which consists of many transformer blocks. (b) Each block contains\nfour primary components including a self-attention layer, a cross-attention layer, a space-MoE layer, and a\ntime-MoE layer. The space-MoE is responsible for depicting different text concepts in specific image regions,\nwhile the time-MoE handles different diffusion timesteps. Each block uses edge-supervised cross-attention\nlearning to further improve image quality. (c) shows details of space-MoE. For example, given a prompt \u201ca\nfurry bear under sky\u201d, each text token and its corresponding image region (given by a binary mask) are directed\nthrough distinct space experts, i.e., each expert learns particular visual features at a region. (d) For time-MoE, an\ninitial timestep is provided, followed by the selection of an expert responsible for handling the visual features.\nExpert 0\nExpert 0\nExpert 0\nExpert 2\nExpert 1\nExpert 3\nFigure 9: The routes of time-MoE in the first transformer block, where the first expert focuses on noisy images,\nwhile other experts handle images with low noise levels.\ncross-attention module. The gating function t_router returns the index of an expert in the Time-MoE,\nwith {te1, te2, ..., tent} being a set of nt experts. Concretely, the Time Gate Network is implemented\nby a function, t_router(ti) = argmax (softmax (G\u2032 (E\u2032\n\u03b8(ti)) + \u03f5)) at timestep ti. To prevent mode\ncollapse, random noise \u03f5 is incorporated. Similar to the Text Gate, G\u2032 : Rdt 7\u2192 Rnt, is a feed forward\nnetwork, where dt is the dimension of the time embedding E\u2032\n\u03b8(ti).\nAnalysis. In our exploration, we uncover some statistical regularities within the routes of time\nexperts across all transformer blocks, establishing a clear correlation with the timestep dimension.\nNotably, we observe a distinct division of labor among these experts, specializing in timesteps\ncharacterized by varying levels of noise. For instance, as illustrated in Fig.9, in the first transformer\nblock, the first expert predominately focuses on processing noisy images (representing the initial\n59% of DDIM sampler steps), while the remaining experts handle images with relatively lower noise\nlevels (representing the final 41% of DDIM sampler steps). This systematic allocation of expertise\nbased on noise characteristics underscores the model\u2019s ability to adapt its computational resources\nefficiently and effectively.\n15\n7.4\nRelated Work\nFoundation models have achieved remarkable success in several fields [2, 44, 34, 15], especially for\ntext-to-image generation. We review related works from two perspectives, mixture-of-experts, and\ntext-to-image generation.\nMixture-of-Experts. The Mixture-of-Experts (MoE) method [7, 8] in neural networks partitions\nspecific model parameters into distinct subsets, each termed an \"expert.\" During forward propagation,\na dynamic routing mechanism assigns these experts to diverse inputs, with each input exclusively\ninteracting with its selected experts. MoE models implement a learned gating function that selec-\ntively activates a subset of experts, enabling the input to engage either all experts [45] or a sparse\nmixture thereof [8, 46], as evidenced in recent expansive language models. While a multitude of\nmodels employs experts strictly within the linear layers, other research regards an entire language\nmodel as an expert [47]. The MoE paradigm finds applicability beyond language processing tasks,\nextending to visual models [34] and Mixture-of-Modality-Experts within multi-modal transformers\n[35]. Additionally, efforts are being made to accelerate the training or inference processes within the\nMoE paradigm [36, 37].\nText-to-Image Generation. Text-to-image generation, the task of synthesizing images from natural\nlanguage descriptions, has experienced significant progress in recent years. Early approaches relied\non generative adversarial networks (GANs) [38, 39, 40, 41, 48] to generate images. More recently,\nwith the transformative success of transformers in generative tasks, models such as DALL-E [42],\nCogview [43], and Make-A-Scene [29] have treated text-to-image generation as a sequence-to-\nsequence problem, utilizing auto-regressive transformers as generators and employing text/image\ntokens as input/output sequences. Recently, another research direction has focused on diffusion\nmodels, framing the task as an iterative denoising process. By integrating textual conditioning within\ndenoising steps, models like Stable Diffusion [2], DALL-E 2 [3], eDiff-I [4], ERNIE-ViLG 2.0 [5],\nand Imagen [1] have consistently set new benchmarks in text-to-image generation. Specifically, Stable\nDiffusion and ERNIE-ViLG 2.0 map images into a latent space, following the Latent Diffusion Model\nparadigm to enhance training and sampling efficiency, while DALL-E 2, eDiff-I, and Imagen operate\nin pixel space. Furthermore, diffusion models also show great potential in image editing [49, 6, 50, 51],\npersonalized generation [52, 53, 54, 55], and 3D/video/gesture generation [56, 57, 58, 59, 60, 61].\nControlNet [11, 62] is a noteworthy model in the text-to-image generation landscape. It builds upon\nthe concept of controllable image synthesis, wherein generated images can be manipulated based on\nuser-defined constraints or attributes.\n7.5\nMore Details on Routers of Space-MoE\nWe continue to delve into the diffusion paths of both COCO categories and verbs, uncovering\nintriguing insights. Utilizing the powerful GPT-3.5 [15, 16], we randomly generate 50 verbs to enrich\nour investigation. Moreover, employing the prompt template randomly generated by GPT-3.5, we\ngenerate 100 samples for each COCO category and verb [15, 16]. Similar to Section 3.1, by adopting\nXGBoost as our classifier, we find that the accuracy rate reaches 94.3% and 97.5%, respectively. We\ngive the routes of COCO and 50 verbs in Fig.10. We also show more visualization results of the\nattention maps in Fig.11. We provide the adjectives and verbs in the following section.\nBlock idx:\nExpert idx:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\nBlock idx:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\nExpert idx:\n(a) Paths of COCO categories.\n(b) Paths of 50 verbs.\nFigure 10: We visualize the diffusion paths (routes) from the network input to the output, utilizing 16 space-\nMoE layers, each containing 6 space experts. These paths are closely associated with COCO categories and 50\nverbs.\n16\nbear\ntable\ncloud\nriver\ndog\napple\nbook\nfurry\nlion\nsky\na bear behind table\na dog behind river under cloud\na book and an apple\na furry lion under sky\nFigure 11: We give the prompts, their associated generated images, and attention maps.\n7.5.1\nAdjectives and Verbs\nAdjectives. Aesthetic, alluring, artistic, astonishing, attractive, baroque, beautiful, blissful, cap-\ntivating, chic, classic, coastal, colorful, common, dark, decorative, delicate, dramatic, dreamlike,\ndreamy, dynamic, eclectic, elegant, emotive, enchanting, energetic, enthralling, essential, ethereal,\nevocative, extraordinary, fascinating, flexible, fragile, futuristic, glamorous, glossy, gorgeous, gothic,\ngrand, harmonious, idyllic, impressive, industrial, innovative, inspiring, intricate, intriguing, joyful,\nlively, luxurious, magnificent, meditative, mesmerizing, minimal, minimalist, modern, moroccan,\nmysterious, nostalgic, ordinary, patterned, peaceful, picturesque, plain, playful, practical, quirky,\nrare, renaissance, retro, rigid, romantic, rough, rustic, satisfying, Scandinavian, scenic, serene, seri-\nous, shiny, simple, sleek, smooth, sophisticated, static, striking, stunning, sturdy, stylish, textured,\ntraditional, tranquil, unique, unusual, useful, vibrant, victorian, vivid, whimsical.\nVerbs. Balance, blend, blossom, bond, carve, celebrate, cheer, climb, collaborate, conduct, conquer,\ncook, craft, create, dance, dream, embrace, experiment, explore, gaze, harmonize, hike, hug, ignite,\nilluminate, jump, laugh, leap, listen, meander, observe, paint, play, ponder, read, rejoice, relax, ride,\nrun, savor, sculpt, sing, smile, soar, surf, swim, swing, taste, wander, whisper.\n7.6\nMore Comparisons between RAPHAEL and Prestigious Diffusion Models\nIn this section, we provide more comparisons between RAPHAEL and Midjourney, Stable Diffusion\nXL, DALL-E 2, DeepFloyd, ERNIE-ViLG 2.0 in Fig.12 and 13.\n17\nDALL-E 2\nMidjourney\nV5.1\nStable \nDiffusion XL\nERNIE \nViLG 2.0\nRAPHAEL\nDeepFloyd\n4. Cartoon characters, mini characters, hand-made, illustrations, robot kids, color expressions, boy, short brown hair, curly\nhair, blue eyes, technological age, cyberpunk, big eyes, cute, mini, detailed light and shadow, high detail.\n3. Cartoon characters, mini characters, figures, illustrations, flower fairy, green dress, brown hair, curly long hair, elf-like\nwings, many flowers and leaves, natural scenery, golden eyes, detailed light and shadow , a high degree of detail.\n2. A shanty version of Tokyo, new rustic style, bold colors with all colors palette, video game, genshin, tribe, fantasy,\noverwatch.\n1. A cute little matte low poly isometric cherry blossom forest island, waterfalls, lighting, soft shadows, trending on\nArtstation, 3d render, monument valley, fez video game.\nFigure 12: The prompts for each column are given in the figure. We give the comparisons between DALL-E 2\nMidjourney v5.1, Stable Diffusion XL, ERNIE ViLG 2.0, DeepFloyd, and RAPHAEL. They are given the same\nprompts, where the words that the human artists yearn to preserve within the generated images are highlighted\nin red. Only the RAPHAEL-generated images precisely reflect the prompts, while other models generate\ncompromised results. For images with cartoon styles, we switch Midjourney v5.1 to Nijijourney v5.\n18\nDALL-E 2\nMidjourney\nV5.1\nStable \nDiffusion XL\nERNIE \nViLG 2.0\nRAPHAEL\nDeepFloyd\n1. Landscape, lake, buildings, bridge, Chinese ink style.\n2. Photo of an athlete cat explaining it's latest scandal at a press conference to journalists.\n3. Impasto, illustration character, illustration poster, techno, cyberpunk, half human, half robot, repaired human, human warrior,\nmech display, woman in mech, purple eyes, half face destroyed, trend on Artstation, high detail, detailed light and shadow, 4k.\n4. Atmosphere, flat painting illustration, illustration, unique tall buildings in the city, irregular design, iron age, black and\norange, detailed light and shadow, cyberpunk, technological age, high detail, CG feeling.\nFigure 13: The prompts for each column are given in the figure. We give the comparisons between DALL-E 2\nMidjourney v5.1, Stable Diffusion XL, ERNIE ViLG 2.0, DeepFloyd, and RAPHAEL. They are given the same\nprompts, where the words that the human artists yearn to preserve within the generated images are highlighted\nin red. Only the RAPHAEL-generated images precisely reflect the prompts, while other models generate\ncompromised results.\n19\n7.7\nMore Images Generated by RAPHAEL\nWe give more cases in Fig.14, 15, 16, 17, and 18.\nA red dwarf star showing a large blast of light, \nalien worlds, red, hyper-realistic water, \ndeconstructive, angura kei, camera tossing, \nansel adams, guy aroch, aerial view .\nColoring page for kids,Giant Monster Truck \nLifted With Big Wheels,cartoon style, thick \nlines, low detail. \nWatercolor purple Hydrangea clipart \nHydrangea floral bouquet in a jar, white \nbackground, no text.\nUtopian biophilic eco mansion design in a magical \nenchanted dream lush pine forest Trend on \nArtstation, Altered Carbon, waterfalls, streams, \npools of golden glowing bioluminescent water, \nspace ship take off, amazing night, photography, \ncinematic, aerial side groung view, Zoha Hadid.\nRealistic artistic astrobiological painting, a wide \nview of the sea floor under the waves of an alien \nocean, where life has crawled out from the deep \nto cling to the sundappled seabed and begin the \neternal dance of evolution into new and \ninnovative forms.\nSushi shop, isometric, high quality, \nold tokyo vibes, night, Japanese, 3d.\nA crowd of people coloured and light in a \ntheater, christopher balaskas, atey ghailan, \nmirror rooms, dynamic figure studies, \nmisty, light cyan.\nA man, a women and a child hand in hand, \nwalking under blooming flower trees, \nminiature photo, overlook, paper kirigami craft, \nhigh detail.\nSpacefaring civilization silicon-based lifeforms \nmegastructures built into fractal mineralized \nshapes, Menger sponge Sierpinski gasket \nkoch snowflake Mandelbulb, key visual by \ncraig mullins and yoji Ishikawa in the style of \nfractal core spacewave.\nFigure 14: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles.\n20\nPhotography, a woman wearing white blazers \nin a street, chic, copy space, fashion, low light, \nmodels, outfit, pose.\nPhotography, cute child, wearing a down \njacket, cat ears, fleshy face, blue high details, \ndetailed light and shadow.\nCelluloid style, Japanese manga style, \ncute girl in school uniform, long white \nhair, under the moon, night, high detail.\nBright scene, aerial view, ancient city, \nfantasy, gorgeous light, mirror reflection, \nhigh detail, wide angle lens.\nSpaceship, a swordsman, magic, fantasy, fog, \nsurreal, bright and brilliant light, gorgeous, \nglory, epic, high detail.\nMinimalist sci-fi illustration, futuristic \nmetropolis, tall skyscrapers, floating \ngardens, crowd, multi-level, detail.\nA picture on the browser of an abandoned \ndesert town, collage - oriented, aerial view, \nultra detail.\nMayan empire, use Mayan architecture, \nmodern, technological cyberpunk style, \nphoto quality.\nA very cute little Shamrock bird  in a mossy forest.\nFigure 15: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles.\n21\nA painting depicting a red wave outside, \ntrapped emotions depicted, full body, \nJon Foster, depth, Dima Dmitriev, fisheye \neffects, Ray Collins.\nAn enormous light next to the ocean, filip \nhodas, mike winkelmann, light amber and \ngold, glowing neon, mystical terrains, \nrectilinear forms, realistic lighting.\nOne man in the middle of a dark urban city, \ndark beige and red, futuristic landscapes, \nSchizowave, cryptidcore, radiant clusters, \nmonumental scale, datamosh.\nAncient ruin Remnants of a religious temple, \nfuturistic, machine-like lines, robotic motifs \ndecorations, poster moebius style.\nItalian secret garden, surrealism, high \ndetail.\nAn intricate forest  painting, full of exotic \nplants and flowers, Arianna Caroli.\nAn ancient stone Colossus with eye, Stephan \nMartini\u00e8re, dark yellow and light emerald, \ncolor zone painting, Denis Sarazhin, dark \nemerald and silver, robotic expressionism, \nhigh detail.\nA knowing paladin, wry smile, feminine pose by \nthe evening hour, foreshortening, gritty \nphotographic, close enough to see the pores, \ndetermination emotion, photography, high detail.\nPapillon dog puppy in style of sumi ink \npainting, fantasy art, enigmatic, mysterious.\nFigure 16: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles.\n22\nCelluloid style, Japanese manga style, flat-\npainted illustration, due teenager, big bag, \nshort black hair fluttering in the wind, on the \nroad by the sea, high detail.\nImpasto, Japanese and Korean manga characters, \nCG characters, CG avatars, magical girls, black \nmagic hats, with flowers on the hats, elf ears, \ntranslucent, long gray hair, Slightly curly hair, \ntrends on Artstation, high detail.\nFlat painting, Japanese and Korean manga \ncharacters, male protagonist, elf boy, white and \ngreen hair, long hair, long ears, sea, bubbles, black \nsuit and green tie, evil smile, green eyes, anime \navatar, detailed light and shadow, high detail.\nAmerican comic character, flat painting, \nillustration character, avatar, flat painting \nillustration, short brown hair, curly hair, green \nclothes, ID photo pose, jane's style, trends on \nartstation , detailed light and shadow, high detail.\nCartoon character, mini character, figure, \nillustration, dango big family, bunny ears, different \ncolors, character design, a set of avatars, cute, \nmini detail light and shadow, high detail.\nFurry character, character design, impasto, cute \nfox girl, long brown hair, curly long hair, \nfluttering in the wind, messy, big eyes, smile, \npetals fluttering in the wind, high detail, CG \ncharacters.\nImpasto, Japanese and Korean manga characters, \nillustration characters, avatars, CG characters, CG \navatars, magical girls, purple magic hats, short \nsilver-white shoulder-length hair, curly hair. two-\ndimensional, detailed light and shadow, high detail.\nImpasto, avatar, illustration, girl with red hair, \nslightly curly hair, European and American, \nfreckles, jane's style, trends on artstation, crazy \ncolors, light and shadow contrast, high detail.\nFlat drawing illustration, flat drawing, illustration, \ngirl in pajamas sitting on the bed, a puppy, sunset, \nthe light of the setting sun enters the house \nthrough the window, detailed light and shadow, \nhigh detail, 4k.\nFigure 17: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles.\n23\nCharacter standing drawing, character design, \nimpasto, samurai in black armor with long black \nhair, holding a long towel in his hand, blindfolded \nby a long cloth, CG game , game characters, high \ndetail, color contrast.\nImpasto, illustration character, illustration poster, \ntech style, cyberpunk, woman with black long \nhair, hair up, black leather jacket, mechanical \narm, tattoo on face, holding a purple lightsaber, \nwar, trends on Artstation, high detail.\nA cute fluffy sentient alien from planet Axor, in \nthe andromeda galaxy, the alien have large \ninnocent eyes and is digitigrade, high detail.\nA tall mythical spire mountain with a base of \nclouds below during golden hour.\nSpike Spiegel wearing Akira Sh\u014dtar\u014d Kaneda's \nred jacket and outfit, Katsuhiro Otomo, \ncinematic, extremely detailed and complex, \nimpressive, super resolution, megapixel.\nImpasto, avatar, illustration, color explosion, \nelf girl on the clouds, reflection, elf ears, \ncolorful clouds, jane's style, trends on \nArtstation, light and shadow contrast, high \ndetail.\nAtmosphere, illustration, mecha sports car, \nmecha era, running on the bustling street, blue, \npurple, cool.\nPixel art, videogame city wallpaper, deskmat. \nio, low poly, large, wide engle , from the top.\nA Greek god, Erebus wearing ancient greek \nclothing, galaxy with a solar system as \nbackground, cinematic, soft studio lighting, \nbacklighting, dark background.\nFigure 18: These examples show that RAPHAEL can generate artistic images with varying text prompts across\nvarious styles.\n24\n7.8\nExtension to LoRA, ControlNet, and SR-GAN\nWe give the results of LoRA in Fig.19 and 20, ControlNet in Fig.21, and SR-GAN in Fig.22 and 23.\nThe detailed settings are given in captions.\nIn the night\nOn the moon\nOn the beach\nNone\nRAPHAEL\nStable\u00a0\nDiffusion\nTraining\u00a0Set\nFigure 19: Results with LoRA. We use 28 images to finetune RAPHAEL and Stable Diffusion. The prompts\nare \"A spider-man figurine, in the night/on the moon/on the beach/none\", only RAPHAEL preserves the concepts\nin prompts while Stable Diffusion yields compromised results.\n25\nFlower\nNight\nSword\nNone\nRAPHAEL\nStable\u00a0\nDiffusion\nTraining\u00a0Set\nFigure 20: Results with LoRA. We use 32 images to finetune RAPHAEL and Stable Diffusion. The prompts\nare \"A boy, flower/night/sword/none\", only RAPHAEL preserves the concepts in prompts while Stable Diffusion\nyields compromised results.\n26\nMonochrome\nNone\nWooden\nWatercolor\nBlue\nNone\nSnow\nWatercolor\nReference\u00a0image\nCanny\nReference\u00a0image\nCanny\nFigure 21: Results with ControlNet. We use the reference image to generate canny edges and adopt it as the\nextra constraint for RAPHAEL. The prompts for each group are \"An ox, blue/none/snow/watercolor\" and \"Icon\nfor game, fighting skill, monochrome/none/wooden/watercolor\".\n27\nPirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool \nengine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic \natmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail.\nFigure 22: Result of 4096\u00d76144 image. SR-GAN enhances the resolution of the image generated by\nRAPHAEL.\n28\nA sureal parallel world where mankind avoid extinction by preserving nature, epic trees, water \nstreams, various flowers, intricate details, rich colors, rich vegetation, cinematic, symmetrical, \nbeautiful lighting, V-Ray render, sun rays, magical lights, photography.\nFigure 23: Result of 4096\u00d76144 image. SR-GAN enhances the resolution of the image generated by\nRAPHAEL.\n29\n"
  },
  {
    "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models",
    "link": "https://arxiv.org/pdf/2305.18292.pdf",
    "upvote": "4",
    "text": "Mix-of-Show: Decentralized Low-Rank Adaptation for\nMulti-Concept Customization of Diffusion Models\nYuchao Gu1, Xintao Wang3, Jay Zhangjie Wu1, Yujun Shi2, Yunpeng Chen2,\nZihan Fan2, Wuyou Xiao2, Rui Zhao1, Shuning Chang1, Weijia Wu1,\nYixiao Ge3, Ying Shan3, Mike Zheng Shou1\u2217\n1Show Lab, 2National University of Singapore\n3ARC Lab, Tencent PCG\nhttps://showlab.github.io/Mix-of-Show\nAbstract\nPublic large-scale text-to-image diffusion models, such as Stable Diffusion, have\ngained significant attention from the community. These models can be easily\ncustomized for new concepts using low-rank adaptations (LoRAs). However,\nthe utilization of multiple concept LoRAs to jointly support multiple customized\nconcepts presents a challenge. We refer to this scenario as decentralized multi-\nconcept customization, which involves single-client concept tuning and center-node\nconcept fusion. In this paper, we propose a new framework called Mix-of-Show that\naddresses the challenges of decentralized multi-concept customization, including\nconcept conflicts resulting from existing single-client LoRA tuning and identity loss\nduring model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-\nLoRA) for single-client tuning and gradient fusion for the center node to preserve\nthe in-domain essence of single concepts and support theoretically limitless concept\nfusion. Additionally, we introduce regionally controllable sampling, which extends\nspatially controllable sampling (e.g., ControlNet and T2I-Adapter) to address\nattribute binding and missing object problems in multi-concept sampling. Extensive\nexperiments demonstrate that Mix-of-Show is capable of composing multiple\ncustomized concepts with high fidelity, including characters, objects, and scenes.\n1\nIntroduction\nPretrain Models - v1\n\u2026\n\u2026\nShared\n\u2026\nPretrain Models - v2\nPrivate\nSample\nGradient Fusion Update\nED-LoRA\nClient-1\nClient-2\nClient-3\nClient-4\nClient-5\nED-LoRA\nED-LoRA\nED-LoRA\nED-LoRA\nFigure 1:\nIllustration of decentralized multi-\nconcept customization via Mix-of-Show.\nOpen-source text-to-image diffusion models,\nsuch as Stable Diffusion [1], empower com-\nmunity users to create customized models by\ncollecting personalized concept images and\nfine-tuning them with low-rank adaptation\n(LoRA) [2, 3]. These tailored LoRA models\nachieve unparalleled quality for specific con-\ncepts through meticulous data selection, pre-\nprocessing, and hyperparameter tuning. While\nexisting concept LoRAs serve as plug-and-play\nplugins for pretrained models, there are still chal-\nlenges in utilizing multiple concept LoRAs to\nextend the pretrained model and enable joint\ncomposition of those concepts. We refer to this\n\u2217Corresponding Author.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18292v2  [cs.CV]  10 Nov 2023\nMix-of-Show\n(a) Multi-Characters\n(b) Multi-Objects\n(c) Characters-Objects\n(d) Characters-Objects-Scenes\nV batman\n , V thanos and V ironman, near a mountain.\nV potter\n , V hermione and V thanos, near a castle.\nTwo V chair, V vase and V table, in a living room.\nV dogA\n , a V cat and a V dogB, on the grass, \nunder the sunset.\n.\n[V ironman\n sit on V chair], [V batman\n sit on V chair],\n V vase and V table, in a living room.\n[V potter\n sit on V chair], V dogA, V cat and V table, in \na living room.\n[V ironman\n sit on V chair], [V batman\n sit on V chair],\n V vase and V table, near V pyramid.\nV potter\n ,V hermione, V dogA and V cat, near V rock.\nV dogB\nV cat\nV dogA\nV table\nV chair\nV vase\nV pyramid\nV rock\nV potter\nV hermione\nV thanos\nV batman\nV ironman\nCustomized Characters\nCustomized \nScenes\nCustomized Objects\nFigure 2: How to generate Harry Potter and Thanos, these two (or even more) concepts from different\nshows, in the same image? Our Mix-of-Show enables complex compositions of multiple customized\nconcepts (e.g., characters, objects, scenes) with individually trained concept LoRAs.\n2\nscenario as decentralized multi-concept customization. As shown in Fig. 1, it involves two steps:\nsingle-client concept tuning and center-node concept fusion. Each client retains their private concept\ndata while sharing the tuned LoRA models. The center node leverages these concept LoRAs to\nupdate the pretrained model, enabling joint sampling of these customized concepts. Decentralized\nmulti-concept customization facilitates maximum community engagement in producing high-quality\nconcept LoRAs and offers flexibility in reusing and combining different concept LoRAs.\nHowever, the existing LoRA tuning and weight fusion techniques [3] fail to address the challenges of\ndecentralized multi-concept customization. We have identified two main challenges: concept conflict\nand identity loss. Concept conflict arises because current LoRA tuning methods do not differentiate\nbetween the roles of embeddings and LoRA weights. Our research reveals that embeddings effectively\ncapture concepts within the pretrained models\u2019 domain, while LoRA weight assist in capturing out-\nof-domain information (e.g., styles or fine details cannot be directly modeled by the pretrained\nmodel). However, existing LoRA tuning methods place excessive emphasis on LoRA weights while\noverlooking the importance of embeddings. Consequently, the LoRA weights encode a significant\nportion of the identity of a given concept, resulting in semantically similar embeddings being\nprojected onto concepts with different appearances. This, in turn, leads to conflicts during model\nfusion. Furthermore, existing weight fusion strategies compromise each concept\u2019s identity and\nintroduce interference from other concepts by performing a weighted average of all concept LoRAs.\nTo overcome the challenges of decentralized multi-concept customization, we propose Mix-of-Show,\nwhich involves embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient\nfusion for center-node fusion. In single-client tuning, ED-LoRA is designed to address concept\nconflicts by preserving more in-domain essence within the embedding. To achieve this, we enhance\nthe expressive ability of the concept embedding by decomposing it into layer-wise embeddings [4]\nand multi-word representations. At the central node, gradient fusion leverages multiple concept\nLoRAs to update the pretrained model. Since the diffusion model includes both forward and reverse\ndiffusion processes, we can obtain the input/output features of each layer through sampling, even\nin the absence of data. Features from multiple concept LoRAs are combined to generate the fused\ngradient, which is used for layer-wise updating. Compared to weight fusion [3], gradient fusion\naligns the inference behavior of each individual concept, significantly reducing identity loss.\nTo demonstrate the capabilities of Mix-of-Show, we introduce regionally controllable sampling for\nmulti-concept generation. Direct multi-concept generation often encounters issues such as missing\nobjects and attribute binding [5, 6]. Recently, spatially controllable sampling (e.g., ControlNet [7],\nT2I-Adapter [8]) have been introduced to guide diffusion models using spatial hints (e.g., keypose or\nsketch), which resolve the problem of missing objects but still faces challenges of attribute binding\nin multi-concept generation. Considering that spatial layout is pre-defined when adopting spatial\nconditions, we propose injecting region prompts through regional-aware cross-attention. Powered\nby Mix-of-Show and regionally controllable sampling, we can achieve complex compositions of\nmultiple customized concepts, including characters, objects, and scenes, as illustrated in Fig. 2. In\nsummary, our contributions are as follows: 1) We analyze the challenges of decentralized multi-\nconcept customization. 2) We propose the Mix-of-Show framework, consisting of an embedding-\ndecomposed LoRA (ED-LoRA) and gradient fusion, to address the concept conflict and identity loss\nin decentralized multi-concept customization. 3) We introduce regionally controllable sampling to\ndemonstrate the potential of Mix-of-Show in composing multiple customized concepts.\n2\nRelated Work\n2.1\nConcept Customization\nConcept customization aims to extend pretrained diffusion models to support personalized concepts\nusing only a few images. There are two main types of concept tuning methods: embedding tuning\n(e.g., Textual Inversion [9] and P+ [4]) and joint embedding-weight tuning (e.g., Dreambooth [10]\nand Custom Diffusion [11]). Additionally, the community [3] adopts low-rank adapter (LoRA) [2]\nfor concept tuning, which is lightweight and can achieve comparable fidelity to full weight tuning.\nAlthough significant progress has been made in single-concept customization, multi-concept cus-\ntomization remains a challenge. Custom Diffusion [11] proposes co-training of multiple concepts or\nconstrained optimization of several existing concept models. Following this, SVDiff [12] introduces\ndata augmentation to prevent concept mixing in co-training multi-concepts, and Cones [13] discovers\n3\nconcept neurons that can be added to support multiple concepts. However, their methods are typi-\ncally restricted to fuse 2-3 semantically distinct concepts. In contrast, Mix-of-Show can combine\ntheoretically limitless customized concepts, including those within the same semantic category.\nAnother research line in concept customization, as explored in studies by Instantbooth [14],\nELITE [15], and Jia et al. [16], focuses on achieving fast test-time customization. These meth-\nods involve pretraining an encoder on a large-scale dataset specific to the desired category. During\ninference, when provided with a few representative concept images from the trained category, the\nencoder extracts features that complement the pretrained diffusion models and support customized\ngeneration. However, these methods require training a separate encoder for each category, typically\nlimited to common categories (e.g., person or cats). This limitation hinders their ability to customize\nand compose more diverse and open-world subjects.\n2.2\nDecentralized Learning\nDecentralized or federated learning aims to train models collaboratively across different clients\nwithout sharing data. The de facto algorithm for federated learning, FedAvg, was proposed by\n[17]. This method simply averages the weights of each client\u2019s model to obtain the final model.\nHowever, we find that directly applying this simple weight averaging is not ideal for fusing LoRAs\nof different concepts. To improve over FedAvg, previous works have either focused on local client\ntraining [18, 19, 20, 21, 22, 23, 24] or global server aggregation [25, 26, 27, 28, 29, 30]. Motivated\nby this, we explore the optimal design of single-client tuning and center-node fusion for decentralized\nmulti-concept customization.\n2.3\nControllable Multi-Concept Generation\nDirect multi-concept generation using text prompts alone faces challenges such as missing objects and\nattribute binding [6, 31, 32, 33, 34]. Previous approaches, like Attend-and-Excite [5] and Structure\nDiffusion [6], have attempted to address these issues, but the problem still persist, limiting the\neffectiveness of multi-concept generation. Recent works, such as ControlNet [7] and T2I-Adapter [8],\nintroduce spatial control (e.g., keypose and sketch) and enable more accurate compositions, resolving\nthe problem of missing objects in multi-concept generation. However, attribute binding remains a\nchallenge. In our work, we tackle this challenge through regionally controllable sampling.\n3\nMethods\nIn this section, we provide a brief background on text-to-image diffusion models and concept\ncustomization in Sec. 3.1. We then introduce the task formulation of decentralized multi-concept\ncustomization in Sec. 3.2, followed by a detailed description of our method in Sec. 3.3 and Sec. 3.4.\n3.1\nPreliminary\nText-to-Image Diffusion Models. Diffusion models [35, 36, 37, 38, 39, 40, 41] belong to a class\nof generative models that gradually introduce noise into an image during the forward diffusion\nprocess and learn to reverse this process to synthesize images. When combined with pretrained\ntext embeddings, text-to-image diffusion models [1, 42, 43, 44, 45, 46] are capable of generating\nhigh-fidelity images based on text prompts. In this paper, we conduct experiments using Stable\nDiffusion [1], which is a variant of the text-to-image diffusion model operating in the latent space.\nGiven a condition c = \u03c8(P \u2217), where P \u2217 is the text prompt and \u03c8 is the pretrained CLIP text\nencoder [47], the training objective for stable diffusion is to minimize the denoising objective by\nL = Ez,c,\u03f5,t[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c)\u22252\n2],\n(1)\nwhere zt is the latent feature at timestep t and \u03f5\u03b8 is the denoising unet with learnable parameter \u03b8.\nEmbedding Tuning for Concept Customization. Textual Inversion [9] represents the input concept\nusing a unique token V . When provided with a few images of the target concept, the embedding of V\nis tuned using Eq. 1. After tuning, the embedding for V encodes the essence of the target concept\nand functions like any other text in the pretrained model. To achieve greater disentanglement and\ncontrol, P+ [4] introduces layer-wise embeddings for concept tokens, denoted as V + in this paper.\n4\n(d)  ED-LoRA\n(c)  LoRA\nEmb\nEmb&LoRA\nEmb\nEmb\n(a)  TI\n(b)  P+\nOut-Domain\nIn-Domain\nConcept\nWeight FusionGradient Fusion\nWeight Fusion\n(e)  LoRA\n(f)  ED-LoRA\nSingle-Concept\nMul/-Concept\n\u03a6!(\ud835\udc43\u2217)\n\u03a6!(\ud835\udc43\u2217)\n\u03a6!(\ud835\udc43\u2217) \u03a6!+ \u2206\u03a6(\ud835\udc43\u2217)\nEmb\nEmb&LoRA\n\u03a6!(\ud835\udc43\u2217) \u03a6!+ \u2206\u03a6(\ud835\udc43\u2217)\nFigure 3: Single- and multi-concept customization between the embedding tuning (i.e., Textual\nInversion (TI) [9] and P+ [4]), and joint embedding-weight tuning (i.e., LoRA [3] and our ED-LoRA).\nP \u2217 = \u201cPhoto of a V , near the beach\". \u03a60 and \u2206\u03a6 denotes the pretrained model and LoRA weight.\nLow-Rank Adaptation. Low-rank adaptation (LoRA) [2] was initially proposed to adapt large-\nlanguage models to downstream tasks. It operates under the assumption that weight changes during\nadaptation have a low \u201cintrinsic rank\" and introduces a low-rank factorization of the weight change to\nobtain the updated weight W, which is given by W = W0 + \u2206W = W0 + BA. Here, W0 \u2208 Rd\u00d7k\nrepresents the original weight in the pretrained model, and B \u2208 Rd\u00d7r and A \u2208 Rr\u00d7k represent\nthe low-rank factors, with r \u226a min(d, k). Recently, the community [3] has adopted LoRA for\nfine-tuning diffusion models, leading to promising results. LoRA is typically used as a plug-and-play\nplugin in pretrained models, but the community also employs weight fusion techniques to combine\nmultiple LoRAs:\nW = W0 +\nn\nX\ni=1\nwi\u2206Wi,\ns.t.\nn\nX\ni=1\nwi = 1,\n(2)\nwhere wi denotes the normalized importance of different LoRAs.\n3.2\nTask Formulation: Decentralized Multi-Concept Customization\nWhile custom diffusion [11] has attempted to merge two tuned concepts models into a pretrained\nmodel, their findings suggest that co-training with multiple concepts yields better results. However,\nconsidering scalability and reusability, we focus on merging single-concept models to support\nmulti-concept customization. We refer to this setting as decentralized multi-concept customization.\nFormally, decentralized multi-concept customization involves a two-step process: single-client\nconcept tuning and center-node concept fusion. As shown in Fig. 1, each of the n clients possesses\nits own private concept data and tunes the concept model \u2206Wi. Here, \u2206Wi represents the changes in\nnetwork weights, which specifically refers to LoRA weights in our work. We omit discussing the\nmerging of text embeddings, as the tuned embeddings can be seamlessly integrated into the pretrained\nmodel without conflicts.\nAfter tuning, the center node gathers all LoRAs to obtain the updated pretrained weight W by:\nW = f(W0, \u2206W1, \u2206W2, . . . , \u2206Wn),\n(3)\nwhere f represents the update rule that operates on the original pretrained model weight W0 and\nthe n concept LoRAs {\u2206Wi, i = 1 \u00b7 \u00b7 \u00b7 n}. One straightforward updating rule f is weight fusion, as\nillustrated in Eq. 2. Once updated, the new model W should be capable of generating all the concepts\nintroduced in the n LoRAs.\n3.3\nMix-of-Show\nIn this section, we introduce Mix-of-Show, containing ED-LoRA (in Sec. 3.3.1) for single-client\nconcept tuning, gradient fusion (in Sec. 3.3.2) for center-node concept fusion.\n5\np1 p2 p3\np5 p6 p7\np4\nText\nEncoder\n\u201cPhoto of a V \u201d\n\ud83d\udd25\n\ud83d\udd25\n(a) Single-Client: ED-LoRA\n\ud835\udc7d = \ud835\udc7d\ud835\udc93\ud835\udc82\ud835\udc8f\ud835\udc85\n%\n\ud835\udc7d\ud835\udc84\ud835\udc8d\ud835\udc82\ud835\udc94\ud835\udc94\n%\nLayer-Wise\nEmbedding\n\ud835\udc7e\nFusion Rule:\nPretrained Model \u2013 V2\n\u201cV1, V2 and V3 \nwalking near a lake\u201d\n(b) Center-Node: Gradient Fusion\nToken \nEmbedding\n\u2744\nFrozen\nLoRA\nText \nEncoder\nDenoising \nU-Net\n\ud83d\udd25\nUpdated\n\u2026\nPhoto\n\ud835\udc7d\nof\na\n\u2744\n\ud83d\udd25\n\u2744\n\u2744\n\u2744\nupdate\ngradient fusion\n\u2a01\n\ud835\udc7e\ud835\udfce\n\u2206\ud835\udc7e\ud835\udfcf\n\ud835\udc7f\ud835\udfcf\n\u201cPhoto of a V1\u201d\nPretrained Model \u2013 V1\n\ud835\udc7d\ud835\udfcf\n\ud835\udc7e\ud835\udfce\n\u2206\ud835\udc7e\ud835\udfd0\n\ud835\udc7f\ud835\udfd0\n\u2a01\n\u201cPhoto of a V2\u201d\nPretrained Model \u2013 V1\n\ud835\udc7d\ud835\udfd0\n\u2a01\n\ud835\udc7e\ud835\udfce\n\u2206\ud835\udc7e\ud835\udfd1\n\ud835\udc7f\ud835\udfd1\nPretrained Model \u2013 V1\n\ud835\udc7d\ud835\udfd1\n\u201cPhoto of a V3\u201d\nPretrained Model \u2013 V2\n(c) Multi-Concept Generation\n\ud835\udc4a = arg min\n!\n*\n\ud835\udc4a\" + \u2206\ud835\udc4a# \ud835\udc4b# \u2212 \ud835\udc4a\ud835\udc4b#\n$\n%\n&\n#'(\nFigure 4: Pipeline of Mix-of-Show. In single-client concept tuning, the ED-LoRA adopts the layer-\nwise embedding and multi-word representation. In center node, gradient fusion is adopted to fuse\nmultiple concept LoRAs and then support composing those customized concepts.\n3.3.1\nSingle-Client Concept Tuning: ED-LoRA\nVanilla LoRA [3] is not suitable for decentralized multi-concept customization due to the issue of\nconcept conflict. To better understand this limitation, we start by examining the distinct roles of\nembeddings and LoRA weights in concept tuning.\nSingle-Concept Tuning Setting. We investigate embedding tuning (i.e., Textual Inversion [9] and\nP+ [4]) and the joint embedding-weight tuning (i.e., LoRA [3]) on single concept customization. We\nconduct experiments on both in-domain concept (i.e., directly sampled from the pretrained model),\nand out-domain concepts. The weights of the pretrained model, including the unet \u03b8 and the text\nencoder \u03c8, are denoted as \u03a60 = {\u03b80, \u03c80}. Given a text prompt P \u2217 containing the concept V , we\nvisualize the tuned embedding of concept V using the pretrained weights \u03a60(P \u2217), and visualize the\ntuned embedding along with the LoRA weight using (\u03a60 + \u2206\u03a6)(P \u2217).\nAnalysis. Based on the experiment results in Fig. 3, we draw the following two observations regarding\nexisting embedding tuning and joint embedding-weight tuning approaches.\nObservation 1: The embeddings are capable of capturing concepts within the domain of pretrained\nmodels, while the LoRA helps capture out-domain information.\nIn Fig. 3(a, b), we observe that embedding tuning approaches such as Textual Inversion and P+\nstruggle to capture out-domain concepts. This is because they attempt to encode all out-domain\ndetails (e.g., anime styles or details not modeled by the pretrained model \u03a60) within the embedding,\nresulting in semantic collapse. However, for in-domain concepts sampled from the model, embedding\ntuning accurately encodes the concept identity within the embedding, benefiting from the accurate\nmodeling of concept details by the pretrained model weights \u03a60. Furthermore, when jointly tuning the\nembedding with LoRA, the embedding no longer produces oversaturated outputs. This is because the\nout-domain information is captured by the pretrained model with LoRA weight shift (i.e., \u03a60 + \u2206\u03a6).\nObservation 2: Existing LoRA weights encode most of the concept identity and project semantically\nsimilar embeddings to visually distinct concepts, leading to conflicts during concept fusion.\nIn the joint embedding-LoRA tuning results shown in Fig. 3(c), we observe that directly visualizing\nthe embedding with the pretrained model \u03a60(P \u2217) yields semantically similar results. However, when\nthe LoRA weights are loaded (\u03a60 + \u2206\u03a6)(P \u2217), the target concept can be accurately captured. This\nsuggests that the majority of the concept identity is encoded within the LoRA weights rather than\nthe embedding itself. However, when attempting to support multiple semantically similar concepts\nwithin a single model, it becomes problematic to determine which concept to sample based on similar\nembeddings, resulting in concept conflicts. As shown in Fig. 3(e), when fused into one model, the\nidentity of each individual concept is lost.\nOur Solution: ED-LoRA. Based on the aforementioned observations, our ED-LoRA is designed\nto preserve more in-domain essence within the embedding while capturing the remaining details\nusing LoRA weights. To achieve this, we enhance the expressiveness of the embedding through\ndecomposed embedding. As illustrated in Fig. 4, we adopt a layer-wise embedding similar to [4]\nand create a multi-world representation for the concept token (V = V +\nrandV +\nclass). Here, V +\nrand is\nrandomly initialized to capture the variance of different concepts, while V +\nclass is initialized based on\n6\nRegion-Aware \nCross-Attention\ntwo girls and a boy are walking near a lake\n[V1, attr1, near a lake] - [x1, y1, x2, y2]\n[V2, attr2, near a lake] - [x1, y1, x2, y2]\n[V3, attr3, near a lake] - [x1, y1, x2, y2]\nGlobal Prompt:\nRegion Prompt:\n(b) Regionally Controllable Sampling\nQ\nK1\nV1, in a red  dress, and V2, in a white suit, and \nV3, wearing a hat, are walking near a lake. \n(a) Spatially Controllable Sampling\nV1\nV2\nV3\nK2\nK3\n(c) Workflow of Regionally Controllable Sampling\nK\n@\nA\"en%on Output\nmask 1\nmask 2\nmask 3\nV1\nV2\nV3\nV\nSo#max \ud835\udc44\ud835\udc3e!\n\ud835\udc51\n$ \ud835\udc49\nRegion Aggregation\n@\n@\n@\n@\nT2I-Adapter\nRegion-Aware Cross-Attention\n@: Matrix Multiplication\nFigure 5: Regionally controllable sampling for multi-concept generation.\nits semantic class to maintain semantic meaning. Both tokens are learnable during concept tuning.\nAs shown in Fig. 3(d), the learned embedding of ED-LoRA effectively preserves the essence of the\ngiven concept within the domain of the pretrained model, while LoRA helps capture the other details.\n3.3.2\nCenter-Node Concept Fusion: Gradient Fusion\nAt the center node, we have access to all the concept LoRAs and can use these models to update\nthe pretrained model, enabling multi-concept customization. However, the existing weight fusion\nstrategy described in Eq. 2 is insufficient to achieve this goal, as we will discuss further below.\nMulti-Concept Fusion Setting. In this experiment, we apply the weight fusion described in Eq. 2 to\nweighted average n concept LoRAs or ED-LoRAs {\u2206\u03a6i, i = 1 \u00b7 \u00b7 \u00b7 n} into the pretrained model \u03a60,\nresulting in a new model \u03a6. We then use the new model \u03a6(P \u2217\ni ) to sample each concept and compare\nits identity with the corresponding single-concept sample (\u03a60 + \u2206\u03a6)(P \u2217\ni ).\nAnalysis. Based on the results in Fig. 3, we make the following observation about fusion strategy.\nObservation 3: Weight fusion leads to identity loss of individual concepts in concept fusion.\nAs shown in Fig. 3 (multi-concept), we can observe that weight fusion in the case of LoRA leads\nto significant loss of concept identity due to conflicts between concepts. Even when combined with\nour ED-LoRA, weight fusion still compromises the identity of each individual concept. In theory,\nif a concept achieves its complete identity through LoRA weight shift \u2206\u03a6(P \u2217), fusing it with n-1\nother concept LoRAs requires reducing its weight to 1\nn\u2206\u03a6(P \u2217) and introducing other concept LoRA\nweights, which ultimately diminishes the concept\u2019s identity.\nOur Solution: Gradient Fusion. Based on the previous analysis, our objective is to preserve the\nidentity of each concept in the fused model by aligning their single-concept inference behavior. Unlike\nin federated learning [17, 19], where models are typically single-direction classification models that\ncannot access gradients without data, text-to-image diffusion models have the inherent capability to\ndecode concepts from text prompts. Leveraging this characteristic, we first decode the individual\nconcepts using their respective LoRA weights, as depicted in Fig. 4(b). We then extract the input and\noutput features associated with each LoRA layer. These input/output features from different concepts\nare concatenated, and fused gradients are used to update each layer W using the following objective:\nW = arg minW\nPn\ni=1 ||(W0 + \u2206Wi)Xi \u2212 WXi||2\nF , where Xi represents the input activation of the\ni-th concept, and | \u00b7 |F denotes the Frobenius norm. By adopting this approach, we can fuse different\nconcept LoRAs without accessing the data and without considering their differences during training.\nThe results of our gradient fusion are shown in Fig. 3(f), demonstrating improved preservation of\neach concept\u2019s identity and consistent stylization across different concepts.\n3.4\nRegionally Controllable Sampling\nDirect multi-concept sampling often encounters challenges of missing objects and attribute binding [6,\n31, 32, 33, 34]. While spatially controllable sampling methods (e.g., ControlNet [7] and T2I-\nAdapter [8]) can address the issue of missing objects in multi-concept generation, they cannot\naccurately bind concepts to specific keyposes or sketches. Merely indicating the desired concept\n7\nED-LoRA (Ours)\nLoRA\nCustom Di\ufb00usion\nP+\nReference\nConcepts\n(a) Single-Concept Results\n(b) MulA-Concept Results\nMix-of-Show (Ours)\nP+\nLoRA\nCustom Di\ufb00usion\nA V cat in the swimming pool.\nReference \nConcepts\nSpa/al \nCondi/on\nV hermione is wearing a headphone.\nFigure 6: Qualitative comparison on single- and multi-concept customization.\nMethods\nReal-Objects\nSingle\u2192Fused\nReal-Characters\nSingle\u2192Fused\nReal-Scenes\nSingle\u2192Fused\nMean Change\nText-alignment\nUpper Bound\n0.811\n0.767\n0.834\n0.804\nP+ [4]\n0.771\u21920.771 (-)\n0.686\u21920.686 (-)\n0.759\u21920.759 (-)\n0.739\u21920.739 (-)\nCustom Diffusion [11]\n0.745\u21920.747 (+0.002)\n0.674\u21920.650 (-0.024)\n0.748\u21920.738 (-0.010)\n0.722\u21920.712 (-0.010)\nLoRA [3]\n0.720\u21920.795 (+0.075)\n0.654\u21920.700 (+0.046)\n0.717\u21920.760 (+0.043)\n0.697\u21920.752 (+0.055)\nMix-of-Show (Ours)\n0.724\u21920.745 (+0.021)\n0.632\u21920.662 (+0.030)\n0.716\u21920.736 (+0.020)\n0.691\u21920.714 (+0.024)\nImage-alignment\nLower Bound\n0.721\n0.471\n0.595\n0.596\nP+ [4]\n0.790\u21920.790 (-)\n0.670\u21920.670 (-)\n0.796\u21920.796 (-)\n0.752\u21920.752 (-)\nCustom Diffusion [11]\n0.842\u21920.808 (-0.034)\n0.714\u21920.694 (-0.020)\n0.804\u21920.750 (-0.054)\n0.787\u21920.751 (-0.036)\nLoRA [3]\n0.864\u21920.778 (-0.086)\n0.761\u21920.555(-0.206)\n0.824\u21920.769 (-0.055)\n0.816\u21920.701 (-0.115)\nMix-of-Show (Ours)\n0.868\u21920.846 (-0.022)\n0.802\u21920.770 (-0.032)\n0.858\u21920.838 (-0.020)\n0.843\u21920.818 (-0.025)\nTable 1: Text-alignment and image-alignment vary between the single-client tuned model and the\ncenter-node fused model. The upper bound of text-alignment and the lower bound of image-alignment\nare computed by replacing the concept\u2019s token (e.g., V dogA) with its class token (e.g., dog) and\nsampling using the pretrained model.\nand attribute through a text prompt can lead to attribute binding problems, as in Fig. 5(a), where the\nidentities of three people are mixed, and the \"red dress\" is incorrectly assigned to other concepts.\nTo address these challenges, we propose a method called regionally controllable sampling. This\napproach utilizes both a global prompt and multiple regional prompts to describe an image based\non spatial conditions. The global prompt provides the overall context, while the regional prompts\nspecify subjects within specific regions, including their attributes and contextual information from\nthe global prompt (e.g., \"near a lake\"). To achieve this, we introduce region-aware cross-attention.\nGiven a global prompt P \u2217\ng and n regional prompts P \u2217\nri, we first incorporate the global prompt via\ncross-attention with the latent z by h = softmax\n\u0010 Q(z)K(P \u2217\ng )\n\u221a\nd\n\u0011\n\u00b7V (P \u2217\ng ). Then, we extract the regional\nlatent feature by zi = z \u2299 Mi, where Mi represents the binary mask associated with the region\nspecified by P \u2217\nri. We obtain regional features using hi = softmax\n\u0010 Q(zi)K(P \u2217\nri)\n\u221a\nd\n\u0011\n\u00b7 V (P \u2217\nri). Finally,\nwe replace the features in the global output with the regional features: h[Mi] = hi. As shown in\nFig. 5(b), regionally controllable sampling allows for precise assignment of subjects and attributes,\nwhile maintaining a harmonious global context.\n4\nExperiments\n4.1\nDatasets and Implementation Details\nTo conduct evaluation for Mix-of-Show, we collect a dataset containing characters, objects, and\nscenes. For ED-LoRA tuning, we incorporate LoRA layer into the linear layer in all attention\n8\n(b) Abla(on of Fusion Type\nw/ Gradient Fusion (Ours)\nw/ Weight Fusion\n(c) Regionally Controllable Sampling\nw/ Region Control (Ours)\nw/o Region Control\n(a) Embedding Expressiveness\nReference\nConcepts\n\ud835\udc49 = \ud835\udc49!\"#$\n%\n \ud835\udc49&'\"((\n%\n\ud835\udc49 = \ud835\udc49&'\"((\nEmbedding\nLoRA\nED-LoRA\n\u03a6!(\ud835\udc43\u2217)\n\u03a6!+ \u2206\u03a6(\ud835\udc43\u2217)\nEmbedding\n\u03a6!(\ud835\udc43\u2217)\n\u03a6!+ \u2206\u03a6(\ud835\udc43\u2217)\nBefore Multi-Concept Fusion\n(a) Weight \nFusion\n(b) Gradient \nFusion (Ours)\nV potter is in front \nof Ei\ufb00el tower.\nA V vase is on \nthe carpet.\nV thanos sit on \na chair.\nA jumping V catA.\nAAer MulB-Concept Fusion\nED-LoRA\nV hermione wear a \nheadphone.\nFigure 7: Qualitative ablation study of Mix-of-Show. P \u2217 means the text prompt. \u03a60 and \u2206\u03a6 denotes\nthe pretrained model and LoRA weight, respectively.\nMethods\nReal-Objects\nSingle\u2192Fused\nReal-Characters\nSingle\u2192Fused\nReal-Scenes\nSingle\u2192Fused\nMean Change\nImage-alignment\nLower Bound\n0.721\n0.471\n0.595\n0.596\nLoRA + Weight Fusion\n0.864\u21920.778 (-0.086)\n0.761\u21920.555(-0.206)\n0.824\u21920.769 (-0.055)\n0.816\u21920.701 (-0.115)\nED-LoRA + Weight Fusion\n0.868\u21920.798 (-0.070)\n0.802\u21920.634 (-0.168)\n0.858\u21920.816 (-0.042)\n0.843\u21920.749 (-0.094)\nED-LoRA + Gradient Fusions\n0.868\u21920.846 (-0.022)\n0.802\u21920.770 (-0.032)\n0.858\u21920.838 (-0.020)\n0.843\u21920.818 (-0.025)\n(a) Subject identity preservation (i.e., image-alignment) measured by CLIP score between LoRA+weight\nfusion, ED-LoRA+weight fusion, and ED-LoRA+gradient fusion. Our ED-LoRA+gradient fusion achieves\nthe least loss in image-alignment after multi-concept fusion, preserving the best subject identity.\n(b) Human preference study interface on Ama-\nzon Mechanical Turk.\nHuman Evaluation\nImage\nAlignment\nText\nAlignment\nED-LoRA + Weight Fusion\n33.5%\n47.5%\nED-LoRA + Gradient Fusion\n66.5%\n52.5%\n(c) Human preference study between weight fusion and\ngradient fusion for fusing ED-LoRAs.\nTable 2: Quantitative ablation study of our main components. (a) Ablation study between the\nLoRA+weight fusion, ED-LoRA+weight fusion and our ED-LoRA+gradient fusion. (b, c) Human\npreference study to comparing weight fusion and gradient fusion for fusing our ED-LoRAs.\nmodules of the text encoder and Unet, with a rank of r = 4 in all experiments. We use the Adam [48]\noptimizer with a learning rate of 1e-3, 1e-5 and 1e-4 for tuning text embedding, text encoder and\nUnet, respectively. For gradient fusion, we use the LBFGS optimizer [49] with 500 and 50 steps to\noptimize the text encoder and Unet, respectively. More details are provided in Sec. 6.1.\n4.2\nQualitative Comparison\nSingle-Concept Results. We compare our ED-LoRA with LoRA [3], Custom Diffusion [11] and\nP+ [4] for single-concept customization. The results are shown in Fig. 6 (a). Our ED-LoRA achieves\ncomparable performance to previous methods on customizing objects, while maintaining better\nidentity for character customization. More comparisons are provided in Sec. 6.2.3.\nMulti-Concept Results. We compare Mix-of-Show with LoRA [3], Custom Diffusion [11], and\nP+ [4] for decentralized multi-concept customization. For LoRA,we utilize weight fusion to combine\nthe different concepts. In the case of P+, we directly incorporate the tuned concept embedding into the\npretrained model. And for Custom Diffusion, we follow their approach of constrained optimization\nto merge the key and value projections in cross-attention. To ensure fair evaluation, we employ the\nsame regionally controllable sampling for multi-concept generation across all models and the results\nare summarized in Fig. 6 (b).\nP+ [4] and Custom Diffusion [11] only tunes text-related module (i.e., text embedding, or the key\nand value projection of cross-attention). In contrast, LoRA and Mix-of-Show add LoRA layers to the\n9\nentire model. The limited scope of tuned modules in P+ and Custom Diffusion leads to an excessive\nencoding of out-domain low-level details within the embedding. This leads to unnatural and less\ndesirable outcomes when compared to LoRA and Mix-of-Show. In comparison to LoRA, which\nloses concept identity after weight fusion, Mix-of-Show effectively preserves the identity of each\nindividual concept.\n4.3\nQuantitative Comparison\nFollowing Custom Diffusion [11], we utilize the CLIP [47] text/image encoder to assess text alignment\nand image alignment. We evaluate on different category of concepts on both single-concept tuned\nmodel and multi-concept fused model. We include detailed evaluation setting in Sec. 6.2.1.\nBased on the results presented in Table. 1, both Mix-of-Show and LoRA exhibit superior image\nalignment compared to other methods, all the while maintaining comparable text alignment in the\nsingle-client tuned model. This achievement stems from their fine-tuning the spatial-related layer\nin Unet (e.g., linear projection layer in self-attention), a critical aspect for accurately capturing the\ncomplex concepts\u2019 identity, such as characters.\nHowever, the main difference between LoRA and Mix-of-Show emerges in the context of multi-\nconcept fusion. In the center-node fused model, LoRA experiences a significant decline in image\nalignment for each concept, progressively deteriorating towards the lower bound. In contrast, our\nMix-of-Show method undergoes far less degradation in image alignment after multi-concept fusion.\n4.4\nAblation Study\nEmbedding Expressiveness. In Fig. 7(a), it is evident that our decomposed embeddings better\npreserve the identity of the specified concept compared to the standard text embeddings used in\nLoRA. This results in a more robust encoding of concept identity. As shown in the quantitative\nresults in Table. 2(a), when LoRA is replaced with ED-LoRA, the identity loss from weight fusion\n(measured by mean change of image-alignment) is reduced from 0.115 to 0.094. This result verifies\nthat expressive embeddings help reduce identity loss during multi-concept fusion.\nFusion Type. Built with the same ED-LoRAs, we conduct experiments to compare weight fusion\nand gradient fusion. As shown in Fig. 7(b), gradient fusion effectively preserves concept identity\nafter concept fusion, resulting in superior results for multi-concept sampling. According to the\nquantitative results in Table. 2(a), gradient fusion significantly reduces the identity loss of weight\nfusion, decreasing it from 0.094 to 0.025. We also conduct a human evaluation and confirm a clear\npreference for gradient fusion, as indicated in Table. 2(c).\nRegionally Controllable Sampling. As shown in Fig. 7(c), direct sampling lead to attribute binding\nissues, where the concept identities are mixed. However, our regionally controllable sampling\novercomes this problem and achieves correct attribute binding in multi-concept generation.\n5\nConclusion\nIn this work, we explore decentralized multi-concept customization and highlight the limitations\nof existing methods like LoRA tuning and weight fusion, which suffer from concept conflicts\nand identity loss in this scenario. To overcome these challenges, we propose Mix-of-Show, a\nframework that combines ED-LoRA for single-client concept tuning and gradient fusion for center-\nnode concept fusion. ED-LoRA preserves individual concept essence in the embedding, avoiding\nconflicts, while gradient fusion minimizes identity loss during concept fusion. We also introduce\nregionally controllable sampling to handle attribute binding in multi-concept generation. Experiments\ndemonstrate Mix-of-Show can successfully generate complex compositions of multiple customized\nconcepts, including characters, objects and scenes.\nAcknowledgements\nThis project is supported by the National Research Foundation, Singapore under its NRFF Award\nNRF-NRFF13-2021-0008, and the Ministry of Education, Singapore, under the Academic Research\nFund Tier 1 (FY2022).\n10\nReferences\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.\n1, 4\n[2] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 1, 3, 5\n[3] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github.\ncom/cloneofsimo/lora. 1, 3, 5, 6, 8, 9, 16, 17\n[4] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual\nconditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 3, 4, 5, 6, 8,\n9, 16, 17\n[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.\nAttend-and-\nexcite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint\narXiv:2301.13826, 2023. 3, 4\n[6] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for\ncompositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 3, 4, 7\n[7] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023. 3, 4, 7\n[8] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023. 3, 4, 7\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022. 3, 4, 5, 6\n[10] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022. 3, 14\n[11] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. 3, 5,\n8, 9, 10, 14, 16, 17\n[12] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang.\nSvdiff: Compact parameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305,\n2023. 3\n[13] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren\nZhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation.\narXiv preprint arXiv:2303.05125, 2023. 3\n[14] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image\ngeneration without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023. 4\n[15] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite:\nEncoding visual concepts into textual embeddings for customized text-to-image generation.\narXiv preprint arXiv:2302.13848, 2023. 4\n[16] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou,\nHuisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization\nwith text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 4\n[17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial\nintelligence and statistics, pages 1273\u20131282. PMLR, 2017. 4, 7\n[18] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713\u201310722,\n2021. 4\n11\n[19] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia\nSmith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning\nand Systems, 2:429\u2013450, 2020. 4, 7\n[20] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and\nAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In\nInternational Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020. 4\n[21] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N What-\nmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. arXiv\npreprint arXiv:2111.04263, 2021. 4\n[22] Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated\nlearning via posterior averaging: A new perspective and practical algorithms. arXiv preprint\narXiv:2010.05273, 2020. 4\n[23] Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated\nlearning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages\n10165\u201310173, 2021. 4\n[24] Yujun Shi, Jian Liang, Wenqing Zhang, Vincent YF Tan, and Song Bai. Towards understanding\nand mitigating dimensional collapse in heterogeneous federated learning.\narXiv preprint\narXiv:2210.00226, 2022. 4\n[25] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective\ninconsistency problem in heterogeneous federated optimization. Advances in neural information\nprocessing systems, 33:7611\u20137623, 2020. 4\n[26] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical\ndata distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. 4\n[27] Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of hetero-\ngeneity: Classifier calibration for federated learning with non-iid data. Advances in Neural\nInformation Processing Systems, 34, 2021. 4\n[28] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.\nFederated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020. 4\n[29] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust\nmodel fusion in federated learning. Advances in Neural Information Processing Systems,\n33:2351\u20132363, 2020. 4\n[30] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u02c7cn`y,\nSanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint\narXiv:2003.00295, 2020. 4\n[31] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 4, 7\n[32] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter\nAbbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models\nusing human feedback. arXiv preprint arXiv:2302.12192, 2023. 4, 7\n[33] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-\nimage models with human preference. arXiv preprint arXiv:2303.14420, 2023. 4, 7\n[34] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and Thomas Leung. Directed diffusion: Direct\ncontrol of object placement through attention guidance. arXiv preprint arXiv:2302.13153, 2023.\n4, 7\n[35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020. 4\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020. 4\n[37] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in Neural Information Processing Systems, 34:8780\u20138794, 2021. 4\n[38] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022. 4\n12\n[39] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Effi-\ncient, controllable and high-fidelity generation from low-dimensional latents. arXiv preprint\narXiv:2201.00308, 2022. 4\n[40] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022. 4, 14\n[41] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\nDpm-\nsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 4\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022. 4\n[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 4\n[44] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 4\n[45] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706,\n2022. 4\n[46] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 4\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021. 4, 10, 14\n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. 9, 14\n[49] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.\nMathematical programming, 45(1-3):503\u2013528, 1989. 9, 14\n[50] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc N Tran, and Anh Tran.\nAnti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 2116\u20132127, 2023. 18\n13\n6\nAppendix\n6.1\nDataset and Implementation Details\n6.1.1\nDataset Details\nPrevious works, such as Dreambooth [10] and Custom Diffusion [11], have primarily focused on\nobject customization. In contrast, our research encompasses a more comprehensive investigation that\ninvolves characters, objects, and scenes. To facilitate our experiments, we curate a dataset comprising\n19 different concepts, including 6 real-world characters, 5 anime characters, 6 real-world objects, and\n2 real-world scenes. The object part is borrowed from Dreambooth [10] and Custom Diffusion [11].\n6.1.2\nImplementation Details\nPretrained Models. Due to the well-known quality issues associated with Stable-Diffusion v1-5 on\nhuman faces, we adopt the Chilloutmix2 as the pretrained model for real-world concepts. Additionally,\nwe employe the Anything-v43 as the pretrained model for anime concepts. To ensure fair comparisons\nwith other methods, we run all comparison methods using the same pretrained model.\nSingle-Client Concept Tuning. In the implementation of ED-LoRA tuning, we incorporate LoRA\nlayers into the linear layers of all attention modules within the text encoder and Unet. Throughout our\nexperiments, we maintain a consistent rank of r = 4 for the LoRA layers for simplicity. To optimize\nthe different components, we utilize the Adam optimizer [48] with specific learning rates: 1e-3 for\ntext embedding, 1e-5 for the text encoder, and 1e-4 for the Unet. We use a 0.01 noise offset for all\nexperiments, which we have found to be crucial for encoding stable identity.\nCenter-Node Concept Fusion. In the center-node concept fusion, we apply layer-wise optimization\nto the layer connected with the LoRA layer. Each layer for optimization is initialized with pretrained\nweights, and we use the LBFGS optimizer [49]. In detail, we optimize the text encoder layer through\n500 steps, while the Unet layer requires 50 steps for optimization.\nSample Details. In all the experiments and evaluations conducted in this paper, we utilize the\nDPM-Solver [40] with 20 sampling steps. To filter out undesired variations in diffusion models,\nwe employ the same negative prompt for both our and the comparison methods during sampling:\n\"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst\nquality, low quality.\"\nRunning Times. In single-client concept tuning, the process of tuning each concept takes approxi-\nmately 10-20 minutes on two Nvidia-A100 GPUs, taking into account variations in data volume. As\nfor the center-node concept fusion, it takes 30 minutes on a single Nvidia-A100 GPU to merge 14\nconcepts within the pretrained model.\n6.2\nQuantitative and Qualitative Evaluation\n6.2.1\nEvaluation Setting\nOur evaluation focuses on investigating the each concept in the single-concept tuned model and\nthe center-node fused model. To assess the performance, we employ the evaluation metric, which\nincludes image-alignment and text-alignment, as outlined in Custom Diffusion [11]. Specifically, for\ntext-alignment, we evaluates the text-image similarity of the sampled image with the corresponding\nsample prompt in the CLIP feature space [47] by CLIP-Score toolkit4. For image-alignment, we\nevaluate the pairwise image similarity between the sampled image and the target concept data in the\nCLIP Image feature space [47].\nFor each concept, we utilize 20 evaluation prompts, which can be roughly categorized into four types:\nRecontextualization, Restylization, Interaction, and Property Modification. In Recontextualization,\nwe assess the concept\u2019s performance by changing its context to different settings, such as the Eiffel\nTower or Mount Fuji. In Restylization, we explore the concept\u2019s ability to adapt to various artistic\nstyles. In Interaction, we investigate the concept\u2019s capability to interact with other objects, such as\n2https://civitai.com/models/6424/chilloutmix\n3https://huggingface.co/andite/anything-v4.0/tree/main\n4https://github.com/jmhessel/clipscore\n14\nA photo of <TOK> on the beach, small waves, detailed \nsymmetric face, beau=ful composi=on\nA <TOK>, in front of Ei\ufb00el tower\nA <TOK>, near the mount fuji\nA <TOK>, in the forest\nA <TOK>, walking on the street\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA <TOK> sit on the chair\nA <TOK> ride a horse\nA <TOK>, wearing a headphone\nA <TOK>, wearing a sunglass\nA <TOK>, wearing a Santa hat\nA smiling <TOK>\nAn angry <TOK>\nA running <TOK>\nA jumping <TOK>\nA <TOK> is lying down\nA <TOK>, in the swimming pool\nA <TOK>, in front of Ei\ufb00el tower\nA <TOK>, near the mount fuji\nA <TOK>, in the forest\nA <TOK>, walking on the street\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA <TOK> sit on the chair\nA <TOK> on the boat\nA <TOK>, wearing a headphone\nA <TOK>, wearing a sunglass\nA <TOK> playing with a ball\nA sad <TOK>\nAn angry <TOK>\nA running <TOK>\nA jumping <TOK>\nA <TOK> is lying down\nA <TOK>, in the swimming pool\nA <TOK>, in front of Ei\ufb00el tower\nA <TOK>, near the mount fuji\nA <TOK>, in the forest\nA <TOK>, walking on the street\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA <TOK> sit on the chair\nA <TOK> on the boat\nA <TOK>, wearing a headphone\nA <TOK>, wearing a sunglass\nA <TOK> playing with a ball\nA sad <TOK>\nAn angry <TOK>\nA running <TOK>\nA jumping <TOK>\nA <TOK> is lying down\nA <TOK>, in the snow\nA <TOK>, at night\nA <TOK>, in autumn\nA <TOK>, in a sunny day\nA <TOK>, in thunder and lightning\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA girl near the <TOK>\nA boy near the <TOK>\nA dog near the <TOK>\nA cat near the <TOK>\nMany people near the <TOK>\nA <TOK> in rainbow colors\nA <TOK> made of metal\nA close view of <TOK>\nA top view of <TOK>\nA boOom view of <TOK>\nprompts for characters\nprompts for pets\nprompts for table\nprompts for chair\nA <TOK>, in the snow\nA <TOK>, at night\nA <TOK>, in autumn\nA <TOK>, in a sunny day\nA <TOK>, in thunder and lightning\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA girl near the <TOK>\nA boy near the <TOK>\nA dog near the <TOK>\nA cat near the <TOK>\nMany people near the <TOK>\nA <TOK> in rainbow colors\nA <TOK> made of metal\nA close view of <TOK>\nA top view of <TOK>\nA boOom view of <TOK>\nprompts for vase\nA <TOK>, in the snow\nA <TOK>, at night\nA <TOK>, in autumn\nA <TOK>, in a sunny day\nA <TOK>, in thunder and lightning\nA <TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\nA watercolor pain=ng of a <TOK>\nA pain=ng of a <TOK> in the style of Vincent Van Gogh\nA pain=ng of a <TOK> in the style of Claude Monet\nA <TOK> in the style of Pixel Art\nA girl near the <TOK>\nA boy near the <TOK>\nA dog near the <TOK>\nA cat near the <TOK>\nMany people near the <TOK>\nA <TOK> in rainbow colors\nA <TOK> made of metal\nA close view of <TOK>\nA top view of <TOK>\nA boOom view of <TOK>\nprompts for scene\nRecontextualiza6on\nRestyliza6on\nInterac6on\nProperty Change\nRecontextualiza6on\nRestyliza6on\nInterac6on\nProperty Change\nFigure 8: Summarization of our evaluation prompts for each concept.\nassociations or actions like sitting on a chair. In Property Modification, we modify the internal state of\nthe concept, including expressions or states like running or jumping. Each type consists of 5 prompts,\nsome of which are borrowed from previous work, resulting in a total of 20 prompts per concept. We\nsample 50 images for each prompt, ensuring reproducibility by fixing the random seed within the\nrange of [1, 50]. This yields a total of 1000 images for each concept. The evaluation prompts for\neach concept are presented in Fig. 8.\n6.2.2\nQuantitative Results\nAccording to the evaluation setting described in Sec. 6.2.1, we have compiled the complete evaluation\nresults for each concept, which are summarized in Table. 3. The summarized results of different\ncategories can be found in Table. 1 of the main paper.\n6.2.3\nQualitative Results\nThe qualitative comparison of Mix-of-Show and other methods on the single-client tuned model\nand center-node fused model is presented in Fig. 9. From the results, it is evident that the LoRA\nexperiences the most significant loss of concept identity after concept fusion. Additionally, due to\nthe limited tunability of positions in the P+ and Custom Diffusion, they exhibit oversaturated results\nor semantic collapse in some examples. Conversely, Mix-of-Show consistently achieves the best\nconcept identity and quality across various examples, while also minimizing the loss of identity after\nthe center-node concept fusion.\n6.3\nLimitation and Future Work\n6.3.1\nLimitation\nThe first limitation is related to regionally controllable sampling, as depicted in Fig. 10(a), where\nattributes from one region may influence another due to the encoding of some attributes in the global\n15\nSingle-Concept Model\nMethods\nCat (5)\nDogA (5)\nChair (5)\nTable (4)\nDogB (5)\nVase (6)\nMean\nText-alignment\nUpper Bound\n0.832\n0.821\n0.795\n0.792\n0.821\n0.807\n0.811\nP+ [4]\n0.828\n0.801\n0.726\n0.696\n0.799\n0.776\n0.771\nCustom Diffusion [11]\n0.784\n0.744\n0.698\n0.689\n0.786\n0.768\n0.745\nLoRA [3]\n0.761\n0.673\n0.655\n0.670\n0.779\n0.779\n0.720\nMix-of-Show (Ours)\n0.771\n0.703\n0.666\n0.671\n0.772\n0.759\n0.724\nImage-alignment\nLower Bound\n0.753\n0.755\n0.674\n0.682\n0.769\n0.692\n0.721\nP+ [4]\n0.783\n0.753\n0.809\n0.810\n0.825\n0.761\n0.790\nCustom Diffusion [11]\n0.869\n0.830\n0.825\n0.888\n0.848\n0.794\n0.842\nLoRA [3]\n0.859\n0.871\n0.872\n0.917\n0.887\n0.778\n0.864\nMix-of-Show (Ours)\n0.874\n0.864\n0.890\n0.879\n0.889\n0.811\n0.868\nFused Model\nMethods\nCat (5)\nDogA (5)\nChair (5)\nTable (4)\nDogB (5)\nVase (6)\nMean\nText-alignment\nUpper Bound\n0.832\n0.821\n0.795\n0.792\n0.821\n0.807\n0.811\nP+ [4]\n0.828\n0.801\n0.726\n0.696\n0.799\n0.776\n0.771\nCustom Diffusion [11]\n0.756\n0.735\n0.726\n0.724\n0.782\n0.758\n0.747\nLoRA [3]\n0.827\n0.803\n0.757\n0.766\n0.814\n0.801\n0.795\nMix-of-Show (Ours)\n0.801\n0.738\n0.673\n0.709\n0.786\n0.761\n0.745\nImage-alignment\nLower Bound\n0.753\n0.755\n0.674\n0.682\n0.769\n0.692\n0.721\nP+ [4]\n0.783\n0.753\n0.809\n0.810\n0.825\n0.761\n0.790\nCustom Diffusion [11]\n0.871\n0.813\n0.774\n0.794\n0.823\n0.773\n0.808\nLoRA [3]\n0.805\n0.800\n0.761\n0.778\n0.808\n0.715\n0.778\nMix-of-Show (Ours)\n0.852\n0.863\n0.864\n0.816\n0.880\n0.800\n0.846\n(a) Quantitative results from single-concept model and center-node fused model on real-world objects.\nSingle-Concept Model\nMethods\nPotter (14)\nHermione (15)\nThanos (15)\nHinton (14)\nLecun (17)\nBengio (15)\nMean\nText-alignment\nUpper Bound\n0.765\n0.776\n0.765\n0.765\n0.765\n0.765\n0.767\nP+ [4]\n0.640\n0.744\n0.622\n0.708\n0.693\n0.711\n0.686\nCustom Diffusion [11]\n0.654\n0.720\n0.594\n0.717\n0.683\n0.674\n0.674\nLoRA [3]\n0.580\n0.696\n0.568\n0.716\n0.681\n0.684\n0.654\nMix-of-Show (Ours)\n0.575\n0.650\n0.562\n0.665\n0.680\n0.662\n0.632\nImage-alignment\nLower Bound\n0.485\n0.458\n0.510\n0.422\n0.510\n0.441\n0.471\nP+ [4]\n0.778\n0.608\n0.809\n0.582\n0.614\n0.629\n0.670\nCustom Diffusion [11]\n0.737\n0.663\n0.852\n0.627\n0.694\n0.710\n0.714\nLoRA [3]\n0.866\n0.679\n0.917\n0.683\n0.716\n0.705\n0.761\nMix-of-Show (Ours)\n0.869\n0.785\n0.921\n0.731\n0.723\n0.782\n0.802\nFused Model\nMethods\nPotter (14)\nHermione (15)\nThanos (15)\nHinton (14)\nLecun (17)\nBengio (15)\nMean\nText-alignment\nUpper Bound\n0.765\n0.776\n0.765\n0.765\n0.765\n0.765\n0.767\nP+ [4]\n0.640\n0.744\n0.622\n0.708\n0.693\n0.711\n0.686\nCustom Diffusion [11]\n0.604\n0.678\n0.624\n0.699\n0.651\n0.641\n0.650\nLoRA [3]\n0.693\n0.717\n0.656\n0.694\n0.714\n0.725\n0.700\nMix-of-Show (Ours)\n0.632\n0.677\n0.611\n0.673\n0.678\n0.701\n0.662\nImage-alignment\nLower Bound\n0.485\n0.458\n0.510\n0.422\n0.510\n0.441\n0.471\nP+ [4]\n0.778\n0.608\n0.809\n0.582\n0.614\n0.629\n0.670\nCustom Diffusion [11]\n0.765\n0.680\n0.749\n0.603\n0.693\n0.671\n0.694\nLoRA [3]\n0.558\n0.600\n0.792\n0.412\n0.523\n0.447\n0.555\nMix-of-Show (Ours)\n0.827\n0.756\n0.867\n0.710\n0.729\n0.733\n0.770\n(b) Quantitative results from single-concept model and center-node fused model on real-world characters.\nSingle-Concept Model\nMethods\nRock (20)\nPyramid (20)\nMean\nText-alignment\nUpper Bound\n0.869\n0.798\n0.834\nP+ [4]\n0.801\n0.716\n0.759\nCustom Diffusion [11]\n0.809\n0.686\n0.748\nLoRA [3]\n0.737\n0.697\n0.717\nMix-of-Show (Ours)\n0.754\n0.677\n0.716\nImage-alignment\nLower Bound\n0.672\n0.517\n0.595\nP+ [4]\n0.821\n0.770\n0.796\nCustom Diffusion [11]\n0.808\n0.800\n0.804\nLoRA [3]\n0.863\n0.784\n0.824\nMix-of-Show (Ours)\n0.859\n0.857\n0.858\nFused Model\nMethods\nRock (20)\nPyramid (20)\nMean\nText-alignment\nUpper Bound\n0.869\n0.798\n0.834\nP+ [4]\n0.801\n0.716\n0.759\nCustom Diffusion [11]\n0.793\n0.682\n0.738\nLoRA [3]\n0.786\n0.734\n0.760\nMix-of-Show (Ours)\n0.770\n0.702\n0.736\nImage-alignment\nLower Bound\n0.672\n0.517\n0.595\nP+ [4]\n0.821\n0.770\n0.796\nCustom Diffusion [11]\n0.742\n0.757\n0.750\nLoRA [3]\n0.810\n0.728\n0.769\nMix-of-Show (Ours)\n0.832\n0.844\n0.838\n(c) Quantitative results from single-concept model and center-node fused model on real-world scenes.\nTable 3: Text-alignment and image-alignment of the single-client tuned model and the center-node\nfused model. The upper bound of text-alignment and the lower bound of image-alignment are\ncomputed by replacing the concept\u2019s token with its class token and sampling using the pretrained\nmodel. For instance, to assess the upper-bound text alignment and lower-bound image alignment for\nthe concept \"V dogA,\" we substitute \"V dogA\" in the sample prompts with the class token \"dog\" and\nsample it using the pretrained model. (N) means each concept has N images for tuning.\n16\nMix-of-Show (Ours)\nLoRA\nCustom Di\ufb00usion\nP+\nV potter in front of \nei\ufb00el tower.\nSingle-Client Tuned Model\nCenter-Node Fused Model\nMix-of-Show (Ours)\nLoRA\nCustom Di\ufb00usion\nP+\nV thanos  in the style of \npixel art.\nSingle-Client Tuned Model\nCenter-Node Fused Model\nRecontextuliza-on\nRestyliza-on\nMix-of-Show (Ours)\nLoRA\nCustom Di\ufb00usion\nP+\nA jumping V cat .\nSingle-Client Tuned Model\nCenter-Node Fused Model\nProperty Change\nMix-of-Show (Ours)\nLoRA\nCustom Di\ufb00usion\nP+\nV dogA , wearing a \nsunglass.\nSingle-Client Tuned Model\nCenter-Node Fused Model\nInterac-on\nFigure 9: Qualitative comparison of Mix-of-Show vs. P+ [4], Custom Diffusion [11], and LoRA [3].\nOur Mix-of-Show demonstrates superior concept identity and quality in single-concept tuned model\nand achieves the least identity loss after concept fusion.\n17\n512 * 512\n1024 * 512\nV hermione is walking near the mount fuji.\n(a) Limita)on of A.ribute Leakage\n(b) Limita)on in Small Face\nV batman, V thanos and V ironman standing \nnear the lake.\nFigure 10: Limitation of Mix-of-Show. (a) Attribute leakage in regionally controllable sampling. (b)\nLimitation in small face generation.\nembedding. This issue can be partially alleviated by specifying undesired attributes using negative\nprompts for each region.\nThe second limitation concerns the center-node concept fusion, which requires a relatively lengthy\ntime to merge concepts. The primary bottleneck in this process arises from the presence of large\nspatial features in the Unet layer during layer-wise optimization.\nThe final limitation relates to the generation of small faces. In the case of Stable Diffusion, the\ninformation loss in the VAE can affect the generation of high-quality full-body characters, especially\nin the small face region, resulting in a loss of facial details, as shown in Fig. 10(b). To mitigate this\nlimitation, increasing the sample size can be a potential solution.\n6.3.2\nFuture Work\nOur Mix-of-Show framework enables the reusability and scalability of tuned concepts, facilitating the\ncreation of complex multi-concept compositions. In future work, it would be interesting to explore\nhow Mix-of-Show can enhance storybook generation by generating character and object interactions\nacross various plots. Furthermore, as Mix-of-Show supports stable identity encoding, it has the\npotential to assist in concept customization for video or 3D scenarios.\n6.3.3\nPotential Negative Society Impact\nThis project aims to provide the community with an effective tool for decentralized creation of\nhigh-quality customized concept models and the ability to reuse and combine different concepts\nto compose complex images. However, a risk exists wherein malicious entities could exploit this\nframework to create deceptive interactions with real-world figures, potentially misleading the public.\nThis concern is not unique to our approach but rather a shared consideration in other multi-concept\ncustomization methodologies. One potential solution to mitigate such risks involves adopting methods\nsimilar to anti-dreambooth [50], which introduce subtle noise perturbations to the published images\nto mislead the customization process. Additionally, applying unseen watermarking to the generated\nimages could deter misuse and prevent them from being used without proper recognition.\n18\n"
  },
  {
    "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors",
    "link": "https://arxiv.org/pdf/2305.18274.pdf",
    "upvote": "4",
    "text": "Reconstructing the Mind\u2019s Eye: fMRI-to-Image with\nContrastive Learning and Diffusion Priors\nPaul S. Scotti*,1,2, Atmadeep Banerjee*,2, Jimmie Goode\u2020,2, Stepan Shabalin2, Alex Nguyen1,\nEthan Cohen3, Aidan J. Dempster4, Nathalie Verlinde1, Elad Yundler5, David Weisberg1,2,\nKenneth A. Norman\u2021,1, and Tanishq Mathew Abraham\u2021,2,6,7\n1Princeton Neuroscience Institute\n2Medical AI Research Center (MedARC)\n3Ecole Normale Sup\u00e9rieure, PSL University\n4University of Toronto\n5Hebrew University of Jerusalem\n6EleutherAI\n7Stability AI\nProject Page: https://medarc.ai/mindeye/\nAbstract\nWe present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct\nviewed images from brain activity. Our model comprises two parallel submodules\nthat are specialized for retrieval (using contrastive learning) and reconstruction\n(using a diffusion prior). MindEye can map fMRI brain activity to any high\ndimensional multimodal latent space, like CLIP image space, enabling image re-\nconstruction using generative models that accept embeddings from this latent space.\nWe comprehensively compare our approach with other existing methods, using both\nqualitative side-by-side comparisons and quantitative evaluations, and show that\nMindEye achieves state-of-the-art performance in both reconstruction and retrieval\ntasks. In particular, MindEye can retrieve the exact original image even among\nhighly similar candidates, indicating that its brain embeddings retain fine-grained\nimage-specific information. This allows us to accurately retrieve images even from\nlarge-scale databases like LAION-5B. We demonstrate through ablations that Mind-\nEye\u2019s performance improvements over previous methods result from specialized\nsubmodules for retrieval and reconstruction, improved training techniques, and\ntraining models with orders of magnitude more parameters. Furthermore, we show\nthat MindEye can better preserve low-level image features in the reconstructions\nby using img2img, with outputs from a separate autoencoder. All code is available\non GitHub.\n1\nIntroduction\nThe problem of decoding environmental inputs and cognitive states from brain activity is funda-\nmental to the field of neuroscience, where improved computational approaches allow for further\nunderstanding of brain mechanisms [1]. A neuroimaging methodology that has seen significant\nsuccess in this domain is functional magnetic resonance imaging (fMRI), where neural activity\nis measured by detecting changes in blood oxygenation. fMRI decoding is already being used in\n\u2217Equal contributions.\n\u2020Core contribution.\n\u2021Joint senior authors.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18274v2  [cs.CV]  7 Oct 2023\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nFigure 1: Example images reconstructed from human brain activity corresponding to passive viewing of natural\nscenes. Reconstructions depict outputs from Versatile Diffusion [6] given CLIP fMRI embeddings generated by\nMindEye for Subject 1. See Figure 4 and Appendix A.4 for more samples.\nreal-time clinical domains [2] and has potential for novel mind reading applications in brain-computer\ninterfaces. Previous works mapped fMRI activity to the embeddings of image generation models\nvia relatively simple mappings, usually ridge regression [3\u20135]. Here we propose MindEye, a novel\napproach that involves mapping via large-scale multilayer perceptrons (MLPs), contrastive learning,\nand diffusion models to achieve state-of-the-art image reconstruction. See Figure 1 for select samples\nof reconstructions.1\nMindEye learns to map flattened spatial patterns of fMRI activity across voxels (3-dimensional cubes\nof cortical tissue) to the image embedding latent space of a pretrained CLIP [7] model. MindEye\nhas an MLP backbone and 2 specialized submodules for retrieval and reconstruction. The retrieval\nsubmodule is contrastively trained and produces \u201cdisjointed CLIP fMRI\u201d embeddings that have high\ncosine similarity with the corresponding image embeddings but differ in magnitude. To reconstruct\nimages, we train a diffusion prior [8] from scratch to take in the outputs from the MLP backbone and\nproduce aligned embeddings suitable as inputs to any pretrained image generation model that accepts\nCLIP image embeddings. In order to ensure that our reconstructions also match the original images\u2019\nlow-level features (e.g., color, texture, spatial position), we train a separate encoder that directly maps\nvoxels to the embedding space of Stable Diffusion\u2019s [9] variational autoencoder (VAE), obtaining\nblurry image reconstructions that lack high-level semantic content but perform state-of-the-art on low-\nlevel image metrics. Combining the high-level \u201csemantic\u201d pipeline with the low-level \u201cperceptual\u201d\npipeline in an img2img [10] setting allows MindEye to output state-of-the-art reconstructions across\nboth low- and high-level image metrics.\nIn addition to image reconstruction metrics, our disjointed CLIP fMRI embeddings attain state-of-\nthe-art performance on image retrieval and brain retrieval metrics. Image retrieval refers to finding\nthe original seen image out of a pool of other images given a brain sample, while brain retrieval\nrefers to finding the brain sample given an image. MindEye finds exact (top-1) matches in the pool of\nNSD test samples with >90% accuracy for both image and brain retrieval, outperforming previous\nstate-of-the-art [11, 4] which showed <50% retrieval accuracies. These results demonstrate that\nMindEye brain embeddings possess fine-grained exemplar-level signal.\nOur main findings are: (1) Specialized submodules for retrieval (using contrastive learning) and\nreconstruction (using a diffusion prior) enable a single model to achieve state-of-the-art results across\nboth tasks even though the objectives exhibit a tradeoff. (2) Mapping to a deep MLP with a parameter\ncount orders of magnitude higher than previous methods does not produce overfitting and instead\ndirectly benefits model performance. (3) A novel bidirectional version of mixup contrastive data\naugmentation further improves model performance in this low-sample setting. (4) State-of-the-art\nreconstructions for low-level image metrics can be obtained by independently mapping to Stable\nDiffusion\u2019s VAE latent space. (5) fMRI-to-image retrieval can find the exact original image even\namong highly similar candidates, suggesting that fine-grained image-specific information is contained\nin brain embeddings, thus allowing retrieval to be scaled up to large-scale databases like LAION-5B\nto output images without generative models.\n1Images containing each subject\u2019s 982 test set reconstructions and retrievals are available on GitHub.\n2\nN images\nfMRI voxels\nCLIP ViT-L/14\nN x 257 x 768\nDiffusion prior\nN x 15000\nLow-level (perceptual) pipeline\nfMRI voxels\nN x 4 x 64 x 64\nMLP\nReconstruction\nVersatile \nDiffusion \nImage \nVariations\nHigh-level (semantic) pipeline\nN x 257 x 768\nMLP projector\nMSE loss\nRetrieval from LAION-5B\nCLIP-fMRI\nCLIP-Image\nContrastive loss\nAutoencoder latent\nAligned CLIP-fMRI (for reconstruction)\nDisjointed CLIP-fMRI (for retrieval)\nN x 257 x 768\nN x 64 x 16 x 16\nLow-level \nreconstruction\nStable \nDiffusion \nimage \nencoder\nN    \nx   \n4   \nx \n64 \nx \n64\nN x 257 x 768\nMSE loss\nUpsampling\nN x 15000\nStable \nDiffusion \nimage \ndecoder\nimg2img\nMLP backbone\nFigure 2: MindEye overall schematic. A high-level \u201csemantic\u201d pipeline maps voxels to CLIP embeddings for\nimage reconstruction (outputs from a diffusion prior feed through generative models like Versatile Diffusion) or\nretrieval tasks (such as K-nearest neighbor querying of brain embeddings to the CLIP embeddings of LAION-5B\nimages). A low-level \u201cperceptual\u201d pipeline maps voxels to the variational autoencoder used by Stable Diffusion\nto obtain blurry reconstructions, which are used as the initialization for subsequent diffusion-based image\ngeneration. The contrastive loss for the low-level pipeline is omitted for simplicity; see Appendix A.3.2 for\ndetails.\n2\nMindEye\nMindEye consists of two pipelines (see Figure 2), a high-level (semantic) pipeline where fMRI voxels\nare mapped to the CLIP ViT-L/14 image space and a low-level (perceptual) pipeline where the voxels\nare mapped to the image embedding space of a VAE. Both pipelines follow a common structure: a\nresidual MLP backbone followed by two task-specific submodules. For the high-level pipeline the\nsubmodules are an MLP projector and a diffusion prior. For the low-level pipeline the submodules are\nan MLP projector and a CNN decoder that performs 4x upsampling. For both pipelines we observe\nthat training the projector submodule with a contrastive loss and the second submodule with mean\nsquared error (MSE) loss gives the best performance.\n2.1\nHigh-Level (Semantic) Pipeline\nThe high-level pipeline is the core of MindEye as it maps voxels to CLIP image space to be fed\nthrough pretrained image generation models. We refer to it as a \u201chigh-level\u201d pipeline because CLIP\nembeddings are inherently more semantic than perceptual, given that CLIP image encoders were\ntrained to maximize similarity with text captions (low-level features like color and object location\nare not typically preserved in these captions). MindEye can be used without the low-level pipeline,\nwhich simply aids to better preserve low-level image features during reconstruction.\nThe MLP backbone for our high-level pipeline maps flattened voxels to an intermediate space of size\n257 \u00d7 768, corresponding to the last hidden layer of CLIP ViT/L-14 (see Appendix 1 for PyTorch\nmodel code). The backbone consists of a linear layer followed by 4 residual blocks and a final\nlinear projector. The embeddings from the backbone are fed to an MLP projector and a diffusion\nprior in parallel. The whole model is trained end-to-end with the prior getting an MSE loss and the\nprojector getting a bidirectional CLIP loss. The projector outputs can be used for retrieval tasks and\nthe diffusion prior outputs can be used by generative models to reconstruct images.\nContrastive Learning: Contrastive learning is an effective method for learning representations\nacross modalities by maximizing cosine similarity for positive pairs while minimizing similarity\nfor negative pairs. Previous work has shown the potential benefits of using contrastive learning\nalongside neural data [12, 13]. CLIP [7] is an example of a multimodal contrastive model that maps\n3\nimages and text captions to a shared embedding space. MindEye is trained to introduce fMRI as an\nadditional modality to the embedding space of a pretrained CLIP model, keeping the CLIP image\nspace frozen as done with locked-image text tuning (LiT) [14]. We use the CLIP loss [7] as our\ncontrastive objective. This loss is bidirectional and helps improve both image and brain retrieval.\nRecent work [15\u201318] has explored novel data augmentation techniques that offer several benefits like\nimproving performance, increasing robustness, and reducing training data requirements. Mixup [15]\nis one such technique that trains models on synthetic data created through convex combinations of\ntwo datapoint-label pairs [19]. Kim et al. [20] introduce MixCo, an extension of mixup that uses the\nInfoNCE loss, and show that MixCo improves classification performance in a semi-supervised setting.\nBased on the same principle, we modify the bidirectional CLIP loss to use MixCo. While Kim et al.\n[20] observed that MixCo gives largest performance benefit for smaller models, we observe that it\nalso helps large models in low data regimes.\nTo combine MixCo with CLIP loss, we mix voxels using a factor \u03bb sampled from the Beta distribution\nwith \u03b1 = \u03b2 = 0.15.\nxmixi,ki = \u03bbi \u00b7 xi + (1 \u2212 \u03bbi) \u00b7 xki,\np\u2217\ni = f(xmixi,ki ),\npi = f(xi),\nti = CLIPImage(yi)\n(1)\nHere, xi and yi represent the i-th fMRI sample and image respectively. ki \u2208 [1, N] is an arbitrary\nmixing index for the i-th datapoint and f represents the combined MLP and projector. p\u2217, p and t are\nL2-normalized. The CLIP loss with MixCo is defined as:\nLBiMixCo = \u2212\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03bbi \u00b7 log\n\uf8eb\n\uf8ed\nexp\n\u0010\np\u2217\ni \u00b7ti\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0010\np\u2217\ni \u00b7tm\n\u03c4\n\u0011\n\uf8f6\n\uf8f8 + (1 \u2212 \u03bbi) \u00b7 log\n\uf8eb\n\uf8ed\nexp\n\u0010 p\u2217\ni \u00b7tki\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0010\np\u2217\ni \u00b7tm\n\u03c4\n\u0011\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u2212\nN\nX\nj=1\n\uf8ee\n\uf8f0\u03bbj \u00b7 log\n\uf8eb\n\uf8ed\nexp\n\u0010 p\u2217\nj \u00b7tj\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0010\np\u2217m\u00b7tj\n\u03c4\n\u0011\n\uf8f6\n\uf8f8 +\nX\n{l|kl=j}\n(1 \u2212 \u03bbl) \u00b7 log\n\uf8eb\n\uf8ed\nexp\n\u0010\np\u2217\nl \u00b7tj\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0010\np\u2217m\u00b7tj\n\u03c4\n\u0011\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n(2)\nWe term this bidirectional loss as BiMixCo. Here \u03c4 is a temperature hyperparameter, and N is the\nbatch size.\nRecent works [21, 22] have shown that stopping mixup augmentation after a certain number of epochs\nleads to better classification performance. As per these findings, we stop using mixup and switch\nfrom a hard contrastive loss to a soft contrastive loss one-third of the way through training. This\nimproves our reconstructions without harming our retrieval performance (see Table 4). BiMixCo\ngives the highest retrieval performance but slightly hurts reconstructions (Table 4) likely due to how\nthe reconstruction task needs absolute targets for the mixed inputs (which we generate by mixing\nthe original targets in the same ratio as the mixup inputs), causing a slight shift in the distribution of\ntarget embeddings. Our final schedule combines BiMixCo and soft contrastive loss to strike the best\nbalance between retrieval and reconstruction performance in a single model.\nOur soft contrastive loss is inspired by knowledge distillation [23], where the authors argue that the\nsoftmax probability distribution produced by a powerful teacher model acts as a better teaching signal\nfor a student than hard labels. To generate the soft labels we take the dot product of CLIP image\nembeddings in a batch with themselves. The loss (with bidirectional component omitted for brevity)\nis calculated between CLIP-CLIP and Brain-CLIP matrices as:\nLSoftCLIP = \u2212\nN\nX\ni=1\nN\nX\nj=1\n\uf8ee\n\uf8f0\nexp\n\u0010\nti\u00b7tj\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0000 ti\u00b7tm\n\u03c4\n\u0001 \u00b7 log\n\uf8eb\n\uf8ed\nexp\n\u0010\npi\u00b7tj\n\u03c4\n\u0011\nPN\nm=1 exp\n\u0000 pi\u00b7tm\n\u03c4\n\u0001\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n(3)\nDiffusion Prior: Using a diffusion model to align the outputs of a contrastive learning model was\ninspired by DALL-E 2 [8], where a \u201cdiffusion prior\u201d was used to map CLIP text embeddings to CLIP\n4\nimage space before using an unCLIP decoder to reconstruct images. Here we train our own diffusion\nprior from scratch to map CLIP fMRI embeddings to CLIP image space, which are then fed into a\npretrained Versatile Diffusion model to generate image reconstructions. We modified an open-source\nimplementation of the DALL-E 2 diffusion prior available on GitHub (see Appendix A.3.1). We used\nthe same prior loss as Ramesh et al. [8]. Our total end-to-end loss is defined as:\nL = LBiMixCo|SoftCLIP + \u03b1 \u00b7 Lprior\n(4)\nWe use \u03b1 = 0.3 and switch from BiMixCo to SoftCLIP after one-third of the train cycle. All our\nmodels are trained on a single A100 GPU for 240 epochs with a batch size of 32. Despite a high\nparameter count, MindEye (including both high- and low-level pipelines) can be trained on a single\nA100 in less than 18 hours. This efficiency is due to the bulk of the parameters stemming from MLPs,\nwhich are faster to compute than transformers or CNNs.\nThe diffusion prior is critical for reconstruction because contrastive learning only incentivizes the\nCLIP fMRI embeddings to match the vector direction of the associated CLIP image embeddings.\nThis generates disjointed embeddings as observed by Ramesh et al. [8]. Theoretically, multimodal\ncontrastive learning will always produce disjointed embeddings because of the \u201cmodality gap\u201d\nphenomenon whereby encoding modalities into a shared space restricts the effective embedding\nspace to a narrow cone in geometric space [24]. We use pre-trained models that expect CLIP image\nembeddings as input, thus motivating our training of a diffusion prior to align disjointed embeddings.\nTo rectify this issue, the diffusion prior learns a distribution of CLIP image embeddings conditioned\non CLIP fMRI embeddings. UMAP [25] plots of disjointed CLIP fMRI embeddings next to aligned\nCLIP fMRI embeddings in Appendix A.5 show how the diffusion prior addresses the disjointed\nembedding spaces problem. We observe that the prior\u2019s role cannot be fulfilled by simply adding\nMSE loss to the MLP projector in Table 4. This is because there is a tradeoff between reconstruction\nand retrieval objectives and a model cannot effectively learn a single embedding space that does well\non both.\n2.2\nLow-Level (Perceptual) Pipeline\nThe low-level pipeline maps voxels to the embedding space of Stable Diffusion\u2019s VAE. The output\nof this pipeline can be fed into the VAE decoder to produce blurry image reconstructions that lack\nhigh-level semantic content but exhibit state-of-the-art low-level image metrics. We use img2img [10]\nto improve our final image reconstructions in terms of low-level metrics, with minimal impairment to\nhigh-level metrics, such that we start the diffusion process from the noised encodings of our blurry\nreconstructions rather than pure noise.\nThe MLP backbone for our low-level pipeline follows the same architecure as our high-level pipeline,\nexcept that the final outputs are of size (16, 16, 64). These are upsampled to (64, 64, 4) by a CNN\nupsampler. An MLP projector projects the backbone outputs to a 512 dimensional space where\nan auxiliary contrastive loss is applied. For more information on the low-level pipeline see Ap-\npendix A.3.2. See Appendix Figure 7 for example blurry reconstructions and Appendix Table 5 to\nsee the effect of changing img2img strength on subsequent reconstruction metrics.\n3\nResults\nFor all experiments, we used the Natural Scenes Dataset (NSD) [26], a public fMRI dataset containing\nthe brain responses of human participants passively viewing natural scenes from MS-COCO [27].\nBy utilizing MS-COCO, this dataset provides measured brain responses to rich naturalistic stimuli,\nallowing us to study how well low- and high-level image features are reconstructed by MindEye. We\nused the same standardized train/test splits as other NSD reconstruction papers [3, 4, 28], training\nsubject-specific models for each of 4 participants. We averaged across three same-image repetitions\nfor the test set (leaving 982 test samples) but not the training set (24,980 training samples), similar to\nTakagi and Nishimoto [3]. For more information on NSD and data preprocessing see Appendix A.2;\nfor single-trial and reduced dataset results see Appendix A.9 and Appendix A.10.\n5\nGiven a brain sample:\nFind the corresponding image that was presented\nfMRI voxels\n1 x 15000\nMindEye\n1 x 257 x 768\nMindEye finds the exact Top-1 image pair from \nthe test set of 982 images with 93.2% accuracy\n(random chance = 0.1%)\nSelect image with highest CLIP cosine similarity\nRetrieve nearest neighbor in CLIP space in LAION-5B database\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nTop 1\nTop 2\nTop 3\nSeen image\nTop 1\nTop 2\nTop 3\nSeen image\nTop 1\nTop 2\nTop 3\nFigure 3: MindEye image retrieval. Given a pool of candidate images, nearest neighbor search in CLIP space\nenables searching for the original image based on brain activity. Top section depicts how, given 982 test NSD\nimages (many containing very similar looking images, e.g., over a dozen zebras), MindEye top-1 performance\nis 93.2% for Subject 1. The ability to distinguish among confusable candidates suggests brain embeddings\nretain fine-grained, image-specific information. Bottom section depicts scaling up to the LAION-5B dataset (see\nAppendix A.4 for more examples). Even with billions of images, MindEye finds images similar to the original.\n3.1\nImage/Brain Retrieval\nImage retrieval evaluations reveal the level of fine-grained image-specific information contained in\nthe predicted brain embeddings. For example, if the model is given a dozen pictures of zebras and\nthe brain sample corresponding to viewing one of those zebras, can the model correctly find the\ncorresponding zebra? If the model can correctly deduce that the brain sample corresponds to an\nimage of a zebra but cannot deduce the specific image amongst various candidates, this would suggest\nthat category-level information but not exemplar-specific information is preserved in the CLIP fMRI\nembedding. MindEye not only succeeds in this zebra example but also demonstrates 93.2% overall\naccuracy for Subject 1 in finding the exact original image within the 982 test images (see Figure 3).\nAlthough we use the full test dataset for retrievals in Figure 3, we followed the same procedure as\nLin et al. [11] for calculating the retrieval metrics reported in Table 1. Brain retrieval performance\nwas calculated according to the following procedure: for each test image, the image is converted to\na CLIP image embedding and we compute the cosine similarity to both its respective ground truth\ndisjointed CLIP fMRI embedding as well as 299 other randomly selected disjointed CLIP fMRI\nembeddings in the test set. For each test sample, success is determined if the cosine similarity is\ngreatest between the ground truth CLIP embedding and its respective fMRI embedding (aka top-1\nretrieval performance, chance=1/300). We average retrieval performance across all test samples\nand repeat the entire process 30 times to account for the variability in random sampling of batches.\nFor image retrieval, the same procedure is used except image and brain samples are flipped such\nthat the goal is to find the corresponding paired CLIP image embedding out of 300 possible CLIP\nembeddings in the batch. Lin et al. [11] refer to image retrieval as \u201cforward retrieval\u201d and brain\nretrieval as \u201cbackward retrieval\u201d in their paper.\nWe can scale up image retrieval using a pool of billions of image candidates. In Figure 3 we show\nresults querying the LAION-5B dataset [29] using our CLIP fMRI embeddings. The final layer CLIP\nViT-L/14 embeddings for all 5 billion images are available at knn.laion.ai, and can be queried for\nK-nearest neighbor lookup via the CLIP Retrieval client [30]. For each test sample, we first retrieve\n16 candidate images using this method (using a variant of MindEye that maps voxels to the final\n6\nOzcelik et al. \n(2023)\nGu et al. \n(2023)\nTakagi et al. \n(2022)\nMindEye\n(ours)\nImage seen \nin MRI\nOzcelik et al. \n(2023)\nMindEye\n(ours)\nImage seen \nin MRI\nFigure 4: Side-by-side comparison of reconstructions from fMRI-to-Image NSD papers. The same test set was\nused across papers. All reconstructions come from Subject 1.\nlayer of CLIP, see Appendix A.7). The best image is then selected based on having the highest\nCLIP embedding cosine similarity to the CLIP fMRI embedding. This image retrieval approach is\nespecially well-suited for tasks involving fine-grained classification, and can be used as an alternative\nto image reconstruction without a generative model (evaluations in Table 1).\n3.2\nfMRI-to-Image Reconstruction\nThe diffusion prior outputs from MindEye are aligned CLIP fMRI embeddings that can be used with\nany pretrained image generation model that accepts latents from CLIP image space. We evaluate the\noutputs of MindEye reconstructions across several models including Versatile Diffusion [6], Stable\nDiffusion (Image Variations) [31], and Lafite [32, 11]. Here we report results from Versatile Diffusion\nsince it yielded the best results, and we report results from the other models in Appendix A.7.\nWe qualitatively compare our reconstructions side-by-side with outputs from other fMRI-to-image\nreconstruction models in Figure 4 and quantitatively compare against other models in Table 1,\ndemonstrating state-of-the-art MindEye reconstructions.\nFor each subject, for each test brain sample, we output 16 CLIP image embeddings from MindEye and\nfeed these embeddings through the image variations pipeline of Versatile Diffusion. This produces 16\nimage reconstructions per brain sample. For our reconstructions we use 20 denoising timesteps with\nUniPCMultistep noise scheduling [33] and start the denoising process from the noised output of our\nlow-level pipeline (img2img). We then select the best of 16 reconstructions by computing last hidden\nlayer CLIP embeddings and picking the image with the highest cosine similarity to the disjointed\nCLIP fMRI embedding. This automatic second-order selection was inspired by DALL-E 2 [8], which\nused a similar process of selecting the best of 2 generated samples.\nTwo-way identification refers to percent correct across comparisons gauging if the original image\nembedding is more similar to its paired brain embedding or a randomly selected brain embedding.\nComparison was performed for AlexNet [37] (second and fifth layers), InceptionV3 [38] (last pooling\nlayer), and CLIP (final layer of ViT-L/14). We use the same settings as Ozcelik and VanRullen [4] for\nour metrics. For more details refer to Appendix A.6.\n3.3\nAblations\nIn this subsection we try to explain where MindEye performance improvements come from through\nablations. To study the effects of architectural changes and training strategies we train only the\nretrieval pipeline (no diffusion prior) for 120 epochs with batch size 300. All models in this section\nare trained on Subject 1. Table entries with * correspond to the final version of MindEye\u2019s settings.\nArchitectural Improvements: To study the effect of model depth and parameter count we train\nmultiple MLPs of various sizes (Table 2). Among models that map to the last hidden layer of CLIP\nViT-L/14, we observe a clear trend of increased performance with added residual blocks. For 2 blocks,\nthe effect of skip connections is not too significant but at 4 blocks the model does significantly worse\nwithout them, indicating that skip connections are important for training deeper models.\n7\nMethod\nLow-Level\nHigh-Level\nRetrieval\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193 Image \u2191 Brain \u2191\nLin et al. [11]\n\u2212\n\u2212\n\u2212\n\u2212\n78.2%\n\u2212\n\u2212\n\u2212\n11.0% 49.0%\nTakagi... [3]\n\u2212\n\u2212\n83.0%\n83.0%\n76.0% 77.0%\n\u2212\n\u2212\n\u2212\n\u2212\nGu et al. [28]\n.150\n.325\n\u2212\n\u2212\n\u2212\n\u2212\n.862\n.465\n\u2212\n\u2212\nOzcelik... [4]\n.254\n.356\n94.2%\n96.2%\n87.2% 91.5% .775\n.423\n21.1% 30.3%\nMindEye\n.309\n.323\n94.7%\n97.8% 93.8% 94.1% .645\n.367\n93.6% 90.1%\nMindEye (Low-Level)\n.360\n.479\n78.1%\n74.8%\n58.7% 59.2% 1.00\n.663\n\u2212\n\u2212\nMindEye (High-Level)\n.194\n.308\n91.7%\n97.4% 93.6% 94.2% .645\n.369\n93.6% 90.1%\nMindEye (LAION)\n.130\n.308\n84.0%\n92.6%\n86.9% 86.1% .778\n.477\n\u2212\n\u2212\nOzcelik... (Low-, S1)\n.358\n.437\n97.7%\n97.6% 77.0% 71.1% .906\n.581\n\u2212\n\u2212\nMindEye (Low-, S1)\n.456\n.493\n87.1%\n84.1%\n61.6% 62.4% .992\n.638\n\u2212\n\u2212\nTable 1: Quantitative comparison of MindEye retrieval and reconstruction performance against other models.\nTop and middle sections average across the same 4 participants (see Appendix A.8 for individual subject models),\nexcept Lin et al. [11] which only analyzed Subject 1. Middle section reflects outputs from only the high- or\nlow-level pipeline, and metrics when evaluating images retrieved from LAION-5B. Bottom section compares our\nlow-level reconstructions to the low-level reconstructions from Ozcelik and VanRullen [4] which only reported\nmetrics for Subject 1. Image retrieval refers to the percent of the time the correct image was retrieved out of 300\ncandidates, given the associated brain sample (chance=0.3%); vice-versa for brain retrieval. PixCorr=pixelwise\ncorrelation between ground truth and reconstructions; SSIM=structural similarity index metric [34]; EfficientNet-\nB1 (\u201cEff\u201d) [35] and SwAV-ResNet50 (\u201cSwAV\u201d) [36] refer to average correlation distance; all other metrics refer\nto two-way identification (chance = 50%). Missing values are from papers not reporting all metrics or metrics\nbeing non-applicable. We followed the same image preprocessing as Ozcelik and VanRullen [4]. Previous\nstate-of-the-art Ozcelik and VanRullen [4] results are directly comparable to MindEye as the same test set and\nVersatile Diffusion model were used. Bold indicates best performance within sections.\nMethod\nParam Count\nImage Retrieval\nBrain Retrieval\nNo ResBlocks\n873M\n0.880\n0.820\n2 ResBlocks + No Skip\n907M\n0.881\n0.822\n2 ResBlocks\n907M\n0.886\n0.837\n4 ResBlocks + No Skip\n940M\n0.836\n0.767\n4 ResBlocks*\n940M\n0.896\n0.822\n4 ResBlocks + Only CLS\n135M\n0.611\n0.576\nTable 2: Effects of varying the architecture of the MLP backbone on retrieval accuracy.\nWe also show a comparison with a 4-resblock model that maps to the final layer of CLIP (only the\nCLS classification token). This model has 7\u00d7 fewer parameters and does much worse than all other\nmodels. This indicates two things: (1) MindEye strongly benefits from a large parameter count MLP\nbackbone and does not overfit even in the sample constrained settings of the NSD dataset, and (2) the\nfMRI voxels contain fine-grained information about images, allowing us to effectively predict all 257\nCLIP image embeddings instead of just the CLS token.\nTraining Strategies (Losses and Data Augmentations): We observe that with InfoNCE, MindEye\nonly does well on brain retrieval (Table 3). A similar trend was observed in Lin et al. [11]. We\nattribute this to InfoNCE being a one-sided loss that only optimizes for one retrieval objective. Simply\nreplacing InfoNCE with CLIP loss significantly improves image retrieval. MixCo augmentation helps\nboth unidirectional and bidirectional losses.\nMethod\nImage Retrieval\nBrain Retrieval\nInfoNCE\n0.237\n0.784\nCLIP Loss\n0.837\n0.791\nInfoNCE + MixCo\n0.303\n0.856\nCLIP Loss + MixCo (BiMixCo)\n0.884\n0.841\nSoftCLIP Loss\n0.837\n0.816\nBiMixCo + SoftCLIP (MindEye)*\n0.896\n0.822\nTable 3: Effects of different losses and MixCo augmentation on MLP retrieval performance.\n8\nWe also show the effect of training with our SoftCLIP loss. SoftCLIP improves over hard CLIP loss\nfor brain retrieval but performs worse than BiMixCo. Our training regime combining SoftCLIP with\nBiMixCo gives the best image retrieval performance.\nReconstruction Strategies: To demonstrate the need for a separate diffusion prior, we train a version\nof MindEye where both contrastive and MSE losses are applied to the ouputs of the MLP backbone.\nWe observe that this model does poorly in terms of retrieval metrics, indicating a tradeoff between\nretrieval and reconstruction objectives where it is difficult to learn a single embedding space that\nsatisfies both objectives. Inspired by recent works in self-supervised learning [39\u201342], we decouple\nthese losses using a separate MLP projector, where MSE loss is applied to the outputs of the MLP\nbackbone and contrastive loss is applied to the outputs of the projector. This model does slightly\nworse in terms of reconstruction but is much better at retrieval. Finally, we train a model with a\ndiffusion prior but no MLP projector. Contrastive loss is computed for the MLP backbone and\nMSE loss is computed for the diffusion prior. This model is comparable to high-level MindEye in\nterms of reconstruction but does worse in retrieval, giving further evidence of a tradeoff. Example\nreconstructions for these models are in Appendix Figure 8.\nMethod\nLow-Level\nHigh-Level\nRetrieval\nPixCorr\nSSIM\nAlex(2) Alex(5)\nIncep\nCLIP\nImage\nBrain\nOnly MLP Backbone\n0.119\n0.346\n73.8%\n84.1%\n81.5%\n82.6%\n0.133\n0.631\nBackbone + Projector\n0.154\n0.296\n73.2%\n85.2%\n75.2%\n77.3%\n0.888\n0.849\nBackbone + Prior\n0.206\n0.303 92.1%\n97.2% 94.8% 95.1%\n0.934\n0.901\nMindEye (only BiMixCo)\n0.195\n0.290\n91.1%\n96.6%\n93.7%\n94.4% 0.974\n0.942\nMindEye (0.33 BiMixCo)*\n0.198\n0.302\n91.6%\n96.8%\n94.6%\n95.0%\n0.972\n0.960\nTable 4: Effects of diffusion prior and MLP projector on reconstruction and retrieval metrics.\n4\nRelated Work\nIn the 2000s, researchers demonstrated that visual information, such as spatial position [43], ori-\nentation [44, 45], and coarse image category [46, 47] could be decoded from fMRI signals using\nlinear classifiers. With the introduction of generative adversarial networks [48], more sophisticated\ndecoding became feasible as researchers mapped brain activity to the latent space of these models\nto reconstruct handwritten digits [49], human faces [50, 51], and natural scenes [52, 5, 53]. More\nrecently, with the release of multimodal contrastive models like CLIP [7], diffusion models [54, 55]\nlike Stable Diffusion [9], and new large-scale fMRI datasets [26], fMRI-to-image reconstructions\nhave reached an unprecedented level of quality [4, 3, 28].\nLin et al. [11] reconstructed NSD images by mapping voxels to CLIP space (see also Wang et al.\n[56]) and fed outputs through a fine-tuned Lafite [32] GAN (MindEye reconstructions using Lafite\nin Appendix A.7). Differences from MindEye include using a convolutional model, no projector to\nseparate contrastive loss from MSE loss, InfoNCE instead of CLIP loss, fine-tuning of a pretrained\nGAN, no diffusion prior, and mapping to both CLIP image and text space. Ozcelik and VanRullen\n[4] used a low- and high-level pipeline with Versatile Diffusion [6]. Differences include mapping\nto CLIP space via ridge regression, no contrastive learning or diffusion prior, and mapping to a\nVDVAE [57] for low-level reconstructions. Gu et al. [28] used a low- and high-level pipeline and\nextended on Ozcelik et al. [5] by reconstructing with IC-GAN [58]; they did not flatten voxels and\nmapped to SwAV [36] features with surface-based convolutional networks. Takagi and Nishimoto\n[3] used ridge regression to map to Stable Diffusion latents and CLIP text latents, using different\nvoxel selections for different components. Mind-Vis [59] did not use the Natural Scenes Dataset\nbut tackled the fMRI-to-image problem from the unique angle of first pre-training a masked brain\nmodel on a separate large-scale dataset. This enabled the authors to use a more informative latent\nas the input to their image reconstruction model. Mind-Video [60] subsequently extended on the\nMind-Vis approach by reconstructing video rather than images. Overall, MindEye is unique in its\nuse of reconstruction and retrieval submodules, a deep MLP backbone with 940 million parameters\n(parameter size comparison in Table 10), a diffusion prior for more accurate translation across brain\nand image modalities, and state-of-the-art reconstuction and retrieval results.\n9\n5\nConclusion\nWe present MindEye, a novel mental decoding approach that achieves state-of-the-art reconstructions\nof natural scenes presented to humans in the MRI machine. These reconstructions retain semantic\nand perceptual similarity to the original images due to the use of a combined high-level and low-level\npipeline. The novel use of specialized submodules for contrastive-based retrieval and diffusion-based\nreconstruction allows MindEye to learn mappings for both tasks in parallel. MindEye brain latents\ncontain fine-grained image-specific signal as demonstrated by the ability to select ground truth images\nout of a set of nearly 1,000 possible images (see Figure 3). We leveraged pretrained CLIP [7], a\nmodel trained with billions of image and text data samples, as a teacher to guide MindEye where we\nhave a relative scarcity of brain data (less than 30,000 training samples per participant). Our diffusion\nprior submodule is trained from scratch and allows for accurate translation of brain embeddings into\npretrained CLIP space such that any model that accepts CLIP image embeddings can be provided with\nCLIP fMRI embeddings without fine-tuning. This flexibility suggests that MindEye reconstructions\nwill continue to improve as newer, more powerful image generation models are released.\nPrivacy Concerns & Societal Benefits: The ability to accurately reconstruct perception from brain\nactivity prompts questions about broader societal impacts. For instance, it should be possible to\ngeneralize current reconstruction models from perception to mental imagery without training a new\nmodel [61\u201364]. However, current models are not capable of across-subject decoding and each NSD\nparticipant spent up to 40 hours in the MRI machine to procure sufficient training data (see Appendix 9\nfor results using a subset of the total training data). Furthermore, non-invasive neuroimaging methods\nin general require compliance because participants can easily resist decoding by moving their head or\nthinking about unrelated information [65]. MindEye is also limited to natural scenes such as those\nin MS-COCO; for other image distributions additional data collection and specialized generative\nmodels would be required. While high-quality image reconstruction via non-invasive neuroimaging\nis not currently practical for real-world applications, technology is constantly improving and it is\nimportant that brain data be carefully protected and companies collecting such data be transparent\nwith their use.\nImage reconstruction from brain activity can enable various potential societal benefits. Reconstruc-\ntions are expected to be systematically distorted due to mental state, neurological conditions, etc.\nThis could potentially enable novel clinical diagnosis and assessment approaches. For example,\npatients suffering from major depressive disorder might produce reconstructions where emotionally\nnegative aspects of images are more salient [66]. MindEye results also suggest potential for improved\nlocked-in (pseudocoma) patient communication via fine-grained visual communication beyond simple\nclassification [67], as well as brain-computer interface performance if adapted to real-time fMRI\nanalysis [68] or non-fMRI neuroimaging modalities.\nFuture Directions: Future directions we wish to explore include mapping individual subjects to\na shared embedding space to enable training models that are generalizeable across people (e.g.,\n[69, 70]), and exploring model intepretability approaches like GradCAM [71] or Network Dissection\n[72] to identify the fMRI voxels that most strongly respond to the presence of certain image features.\n6\nOpen Research: 100% Transparent Volunteer-Driven Science\nMindEye was openly developed through volunteer contributions in the MedARC Discord server.\nSource code was always accessible via a public GitHub repository throughout the lifespan of the\nproject. Research discussions were held via public Discord channels, and weekly video conference\ncalls were recorded and shared publicly. We continue to extend a global invitation to contribute to\nMedARC Neuroimaging & AI Lab projects to cultivate an internationally diversified, volunteer-driven\nresearch team composed of members from varied backgrounds possessing a wide array of expertise.\nWe contend that fully transparent open-research initiatives such as this and others like EleutherAI,\nLAION, OpenBioML, and ML Collective could redefine the traditional framework of scientific\nresearch, democratizing entry into machine learning and medical research through the harnessing of\ncrowd-sourced collective intelligence and community collaboration.\n7\nAuthor Contributions\nFor detailed author contributions see Appendix A.1.\n10\n8\nAcknowledgements\nThanks to the MedARC community, including Jeremy Howard, Tommaso Furlanello, Mihir Tripathy,\nand Cesar Torrico for useful discussion and reviewing the manuscript. Thank you to Furkan Ozcelik,\nauthor of Brain-Diffuser, for sharing his code and expert knowledge with our group. We thank\nLAION for being the initial community where this project developed, and thank Romain Beaumont\nand Zion English for useful discussion during that time. We thank Stability AI for sharing their\nhigh-performance computing workplace and giving us the computational resources necessary to\ndevelop MindEye. Thank you to Richard Vencu for help navigating the Stability HPC. Collection of\nthe Natural Scenes Dataset was supported by NSF IIS-1822683 and NSF IIS-1822929.\nReferences\n[1] Thomas Naselaris, Kendrick N. Kay, Shinji Nishimoto, and Jack L. Gallant. Encoding and decoding in\nfMRI. Neuroimage, 56(2):400\u2013410, 2011. Publisher: Elsevier.\n[2] David Linden. Introduction. In fMRI Neurofeedback, pages 161\u2013169. Elsevier, 2021. ISBN 978-0-12-\n822421-2. doi: 10.1016/B978-0-12-822421-2.00008-9. URL https://linkinghub.elsevier.com/\nretrieve/pii/B9780128224212000089.\n[3] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from\nhuman brain activity. preprint, Neuroscience, November 2022. URL http://biorxiv.org/lookup/\ndoi/10.1101/2022.11.18.517004.\n[4] Furkan Ozcelik and Rufin VanRullen. Brain-Diffuser: Natural scene reconstruction from fMRI signals using\ngenerative latent diffusion, March 2023. URL http://arxiv.org/abs/2303.05334. arXiv:2303.05334\n[cs, q-bio].\n[5] Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstruction of\nPerceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs,\nFebruary 2022. URL http://arxiv.org/abs/2202.12692. arXiv:2202.12692 [cs, eess, q-bio].\n[6] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile Diffusion: Text,\nImages and Variations All in One Diffusion Model, March 2023. URL http://arxiv.org/abs/2211.\n08332. arXiv:2211.08332 [cs].\n[7] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\nTransferable Visual Models From Natural Language Supervision, February 2021. URL http://arxiv.\norg/abs/2103.00020. arXiv:2103.00020 [cs].\n[8] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional\nImage Generation with CLIP Latents, April 2022.\nURL http://arxiv.org/abs/2204.06125.\narXiv:2204.06125 [cs].\n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-Resolution\nImage Synthesis with Latent Diffusion Models, April 2022. URL http://arxiv.org/abs/2112.10752.\narXiv:2112.10752 [cs].\n[10] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations, January 2022. URL\nhttp://arxiv.org/abs/2108.01073. arXiv:2108.01073 [cs].\n[11] Sikun Lin, Thomas Sprague, and Ambuj K. Singh. Mind Reader: Reconstructing complex images from\nbrain activities, September 2022. URL http://arxiv.org/abs/2210.01769. arXiv:2210.01769 [cs,\neess, q-bio].\n[12] Alexandre D\u00e9fossez, Charlotte Caucheteux, J\u00e9r\u00e9my Rapin, Ori Kabeli, and Jean-R\u00e9mi King. Decoding\nspeech from non-invasive brain recordings, August 2022. URL http://arxiv.org/abs/2208.12266.\narXiv:2208.12266 [cs, eess, q-bio].\n[13] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for\njoint behavioural and neural analysis. Nature, 617(7960):360\u2013368, May 2023. ISSN 1476-4687. doi:\n10.1038/s41586-023-06031-6. URL https://www.nature.com/articles/s41586-023-06031-6.\nNumber: 7960 Publisher: Nature Publishing Group.\n11\n[14] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov,\nand Lucas Beyer. LiT: Zero-Shot Transfer with Locked-image text Tuning, June 2022. URL http:\n//arxiv.org/abs/2111.07991. arXiv:2111.07991 [cs].\n[15] Hongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. ArXiv, abs/1710.09412, 2017.\n[16] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Young Joon Yoo.\nCutmix: Regularization strategy to train strong classifiers with localizable features. 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages 6022\u20136031, 2019.\n[17] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment:\nLearning augmentation policies from data. ArXiv, abs/1805.09501, 2018.\n[18] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. ArXiv,\nabs/1912.02781, 2019.\n[19] Olivier Chapelle, Jason Weston, L\u00e9on Bottou, and Vladimir Naumovich Vapnik. Vicinal risk minimization.\nIn NIPS, 2000.\n[20] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Seyoung Yun. Mixco: Mix-up contrastive learning for\nvisual representation. ArXiv, abs/2010.06300, 2020.\n[21] Zixuan Liu and Ziqiao Wang. Over-training with mixup may hurt generalization. ArXiv, abs/2303.01475,\n2023.\n[22] Hao Yu, Huanyu Wang, and Jianxin Wu. Mixup without hesitation. ArXiv, abs/2101.04342, 2021.\n[23] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv,\nabs/1503.02531, 2015.\n[24] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the Gap: Understanding\nthe Modality Gap in Multi-modal Contrastive Representation Learning, October 2022. URL http:\n//arxiv.org/abs/2203.02053. arXiv:2203.02053 [cs].\n[25] Leland McInnes, John Healy, and James Melville.\nUMAP: Uniform Manifold Approximation and\nProjection for Dimension Reduction, September 2020. URL http://arxiv.org/abs/1802.03426.\narXiv:1802.03426 [cs, stat].\n[26] Emily J. Allen, Ghislain St-Yves, Yihan Wu, Jesse L. Breedlove, Jacob S. Prince, Logan T. Dowdle,\nMatthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J. Benjamin Hutchinson, Thomas Naselaris,\nand Kendrick Kay.\nA massive 7T fMRI dataset to bridge cognitive neuroscience and artificial in-\ntelligence. Nature Neuroscience, 25(1):116\u2013126, January 2022. ISSN 1097-6256, 1546-1726. doi:\n10.1038/s41593-021-00962-x. URL https://www.nature.com/articles/s41593-021-00962-x.\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In David Fleet, Tomas Pajdla,\nBernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, Lecture Notes in Computer\nScience, pages 740\u2013755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. doi:\n10.1007/978-3-319-10602-1_48.\n[28] Zijin Gu, Keith Jamison, Amy Kuceyeski, and Mert Sabuncu. Decoding natural image stimuli from fMRI\ndata with a surface-based convolutional network, March 2023. URL http://arxiv.org/abs/2212.\n02409. arXiv:2212.02409 [cs, q-bio].\n[29] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\nLAION-\n5B: An open large-scale dataset for training next generation image-text models, October 2022. URL\nhttp://arxiv.org/abs/2210.08402. arXiv:2210.08402 [cs].\n[30] Romain Beaumont. Clip Retrieval: Easily compute clip embeddings and build a clip retrieval system\nwith them, 2022. URL https://github.com/rom1504/clip-retrieval. publicationType: misc;\npublisher: GitHub; journal: GitHub repository.\n[31] Justin Pinkney.\nLambda Diffusers,\n2022.\nURL\nhttps://github.com/LambdaLabsML/\nlambda-diffusers. publicationType: misc; publisher: GitHub; journal: GitHub repository.\n12\n[32] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui\nXu, and Tong Sun. LAFITE: Towards Language-Free Training for Text-to-Image Generation, March 2022.\nURL http://arxiv.org/abs/2111.13792. arXiv:2111.13792 [cs].\n[33] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. UniPC: A Unified Predictor-Corrector\nFramework for Fast Sampling of Diffusion Models, February 2023. URL http://arxiv.org/abs/\n2302.04867. arXiv:2302.04867 [cs].\n[34] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility\nto structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, April 2004. ISSN\n1941-0042. doi: 10.1109/TIP.2003.819861. Conference Name: IEEE Transactions on Image Processing.\n[35] Mingxing Tan and Quoc V. Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,\nSeptember 2020. URL http://arxiv.org/abs/1905.11946. arXiv:1905.11946 [cs, stat].\n[36] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Un-\nsupervised Learning of Visual Features by Contrasting Cluster Assignments, January 2021.\nURL\nhttp://arxiv.org/abs/2006.09882. arXiv:2006.09882 [cs].\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolu-\ntional Neural Networks. In Advances in Neural Information Processing Systems, volume 25. Curran As-\nsociates, Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/hash/\nc399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\nInception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2818\u20132826, June 2016. doi: 10.1109/CVPR.2016.308. ISSN: 1063-6919.\n[39] Jean-Bastien Grill, Florian Strub, Florent Altch\u2019e, Corentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar,\nBilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new\napproach to self-supervised learning. ArXiv, abs/2006.07733, 2020.\n[40] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u2019e J\u2019egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 9630\u20139640, 2021.\n[41] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. ArXiv, abs/2002.05709, 2020.\n[42] Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, and Yann LeCun. On the duality between\ncontrastive and non-contrastive self-supervised learning. ArXiv, abs/2206.02574, 2022.\n[43] Bertrand Thirion, Edouard Duchesnay, Edward Hubbard, Jessica Dubois, Jean-Baptiste Poline, Denis Lebi-\nhan, and Stanislas Dehaene. Inverse retinotopy: Inferring the visual content of images from brain activation\npatterns. NeuroImage, 33(4):1104\u20131116, December 2006. ISSN 10538119. doi: 10.1016/j.neuroimage.\n2006.06.062. URL https://linkinghub.elsevier.com/retrieve/pii/S1053811906007373.\n[44] John-Dylan Haynes and Geraint Rees. Predicting the orientation of invisible stimuli from activity in human\nprimary visual cortex. Nature Neuroscience, 8(5):686\u2013691, May 2005. ISSN 1097-6256, 1546-1726. doi:\n10.1038/nn1445. URL http://www.nature.com/articles/nn1445.\n[45] Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain.\nNature Neuroscience, 8(5):679\u2013685, May 2005. ISSN 1097-6256, 1546-1726. doi: 10.1038/nn1444. URL\nhttp://www.nature.com/articles/nn1444.\n[46] David D Cox and Robert L Savoy. Functional magnetic resonance imaging (fMRI) \u201cbrain reading\u201d:\ndetecting and classifying distributed patterns of fMRI activity in human visual cortex. NeuroImage,\n19(2):261\u2013270, June 2003. ISSN 10538119. doi: 10.1016/S1053-8119(03)00049-1. URL https:\n//linkinghub.elsevier.com/retrieve/pii/S1053811903000491.\n[47] James V. Haxby, M. Ida Gobbini, Maura L. Furey, Alumit Ishai, Jennifer L. Schouten, and Pietro Pietrini.\nDistributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex. Science,\n293(5539):2425\u20132430, September 2001. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.1063736.\nURL https://www.science.org/doi/10.1126/science.1063736.\n[48] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL http://arxiv.org/\nabs/1406.2661. arXiv:1406.2661 [cs, stat].\n13\n[49] Sanne Schoenmakers, Markus Barth, Tom Heskes, and Marcel van Gerven. Linear reconstruction of\nperceived images from human brain activity. NeuroImage, 83:951\u2013961, December 2013. ISSN 10538119.\ndoi: 10.1016/j.neuroimage.2013.07.043. URL https://linkinghub.elsevier.com/retrieve/pii/\nS1053811913007994.\n[50] Rufin VanRullen and Leila Reddy. Reconstructing faces from fMRI patterns using deep generative\nneural networks. Communications Biology, 2(1):193, December 2019. ISSN 2399-3642. doi: 10.1038/\ns42003-019-0438-y. URL http://www.nature.com/articles/s42003-019-0438-y.\n[51] Thirza Dado, Ya\u02d8gmur G\u00fc\u00e7l\u00fct\u00fcrk, Luca Ambrogioni, Gabri\u00eblle Ras, Sander Bosch, Marcel van Gerven,\nand Umut G\u00fc\u00e7l\u00fc. Hyperrealistic neural decoding for reconstructing faces from fMRI activations via\nthe GAN latent space. Scientific Reports, 12(1):141, December 2022. ISSN 2045-2322. doi: 10.1038/\ns41598-021-03938-w. URL https://www.nature.com/articles/s41598-021-03938-w.\n[52] Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image reconstruction from\nhuman brain activity. PLOS Computational Biology, 15(1):e1006633, January 2019. ISSN 1553-7358.\ndoi: 10.1371/journal.pcbi.1006633. URL https://dx.plos.org/10.1371/journal.pcbi.1006633.\n[53] K. Seeliger, U. G\u00fc\u00e7l\u00fc, L. Ambrogioni, Y. G\u00fc\u00e7l\u00fct\u00fcrk, and M.A.J. van Gerven. Generative adversarial\nnetworks for reconstructing natural images from brain activity. NeuroImage, 181:775\u2013785, November\n2018. ISSN 10538119. doi: 10.1016/j.neuroimage.2018.07.043. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S105381191830658X.\n[54] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020.\nURL http://arxiv.org/abs/2006.11239. arXiv:2006.11239 [cs, stat].\n[55] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models,\nOctober 2020. URL http://arxiv.org/abs/2006.09011. arXiv:2006.09011 [cs, stat].\n[56] Aria Y. Wang, Kendrick Kay, Thomas Naselaris, Michael J. Tarr, and Leila Wehbe. Incorporating natural\nlanguage into vision models improves prediction and understanding of higher visual cortex, Septem-\nber 2022.\nURL https://www.biorxiv.org/content/10.1101/2022.09.27.508760v1.\nPages:\n2022.09.27.508760 Section: New Results.\n[57] Rewon Child. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images,\nMarch 2021. URL http://arxiv.org/abs/2011.10650. arXiv:2011.10650 [cs].\n[58] Arantxa Casanova, Marl\u00e8ne Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero-\nSoriano. Instance-Conditioned GAN, November 2021. URL http://arxiv.org/abs/2109.05070.\narXiv:2109.05070 [cs].\n[59] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing Beyond the Brain:\nConditional Diffusion Model with Sparse Masked Modeling for Vision Decoding, March 2023. URL\nhttp://arxiv.org/abs/2211.06956. arXiv:2211.06956 [cs].\n[60] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou. Cinematic Mindscapes: High-quality Video Reconstruction\nfrom Brain Activity, May 2023. URL http://arxiv.org/abs/2305.11675. arXiv:2305.11675 [cs].\n[61] Mark Stokes, Russell Thompson, Rhodri Cusack, and John Duncan. Top-Down Activation of Shape-\nSpecific Population Codes in Visual Cortex during Mental Imagery. Journal of Neuroscience, 29(5):\n1565\u20131572, February 2009. ISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.4657-08.2009. URL\nhttps://www.jneurosci.org/content/29/5/1565. Publisher: Society for Neuroscience Section:\nArticles.\n[62] Rainer Goebel, Rick van Hoof, Salil Bhat, Michael L\u00fchrs, and Mario Senden. Reading Imagined Letter\nShapes from the Mind\u2019s Eye Using Real-time 7 Tesla fMRI. In 2022 10th International Winter Conference\non Brain-Computer Interface (BCI), pages 1\u20133, February 2022. doi: 10.1109/BCI53720.2022.9735031.\nISSN: 2572-7672.\n[63] Thomas Naselaris, Cheryl A. Olman, Dustin E. Stansbury, Kamil Ugurbil, and Jack L. Gallant. A voxel-\nwise encoding model for early visual areas decodes mental images of remembered scenes. NeuroImage,\n105:215\u2013228, January 2015. ISSN 1053-8119. doi: 10.1016/j.neuroimage.2014.10.018. URL https:\n//www.sciencedirect.com/science/article/pii/S1053811914008428.\n[64] Leila Reddy, Naotsugu Tsuchiya, and Thomas Serre.\nReading the mind\u2019s eye: Decoding category\ninformation during mental imagery. NeuroImage, 50(2):818\u2013825, April 2010. ISSN 1053-8119. doi: 10.\n1016/j.neuroimage.2009.11.084. URL https://www.sciencedirect.com/science/article/pii/\nS1053811909012701.\n14\n[65] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G. Huth. Semantic reconstruction of con-\ntinuous language from non-invasive brain recordings.\nNature Neuroscience, pages 1\u20139, May 2023.\nISSN 1546-1726. doi: 10.1038/s41593-023-01304-9. URL https://www.nature.com/articles/\ns41593-023-01304-9. Publisher: Nature Publishing Group.\n[66] Anne C Mennen, Kenneth A Norman, and Nicholas B Turk-Browne. Attentional bias in depression:\nunderstanding mechanisms to improve training and treatment.\nCurrent Opinion in Psychology, 29:\n266\u2013273, October 2019. ISSN 2352-250X. doi: 10.1016/j.copsyc.2019.07.036. URL https://www.\nsciencedirect.com/science/article/pii/S2352250X19300016.\n[67] Martin M. Monti, Audrey Vanhaudenhuyse, Martin R. Coleman, Melanie Boly, John D. Pickard, Luaba\nTshibanda, Adrian M. Owen, and Steven Laureys. Willful Modulation of Brain Activity in Disorders\nof Consciousness. New England Journal of Medicine, 362(7):579\u2013589, February 2010. ISSN 0028-\n4793. doi: 10.1056/NEJMoa0905370. URL https://doi.org/10.1056/NEJMoa0905370. Publisher:\nMassachusetts Medical Society _eprint: https://doi.org/10.1056/NEJMoa0905370.\n[68] Grant Wallace, Stephen Polcyn, Paula P. Brooks, Anne C. Mennen, Ke Zhao, Paul S. Scotti, Sebastian\nMichelmann, Kai Li, Nicholas B. Turk-Browne, Jonathan D. Cohen, and Kenneth A. Norman. RT-\nCloud: A cloud-based software framework to simplify and standardize real-time fMRI. NeuroImage,\n257:119295, August 2022. ISSN 10538119. doi: 10.1016/j.neuroimage.2022.119295. URL https:\n//linkinghub.elsevier.com/retrieve/pii/S1053811922004141.\n[69] Po-Hsuan (Cameron) Chen, Janice Chen, Yaara Yeshurun, Uri Hasson, James Haxby, and Peter J Ramadge.\nA Reduced-Dimension fMRI Shared Response Model. In Advances in Neural Information Processing\nSystems, volume 28. Curran Associates, Inc., 2015. URL https://papers.nips.cc/paper_files/\npaper/2015/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html.\n[70] James V. Haxby, J. Swaroop Guntupalli, Andrew C. Connolly, Yaroslav O. Halchenko, Bryan R. Conroy,\nM. Ida Gobbini, Michael Hanke, and Peter J. Ramadge. A Common, High-Dimensional Model of the\nRepresentational Space in Human Ventral Temporal Cortex. Neuron, 72(2):404\u2013416, October 2011.\nISSN 08966273. doi: 10.1016/j.neuron.2011.08.026. URL https://linkinghub.elsevier.com/\nretrieve/pii/S0896627311007811.\n[71] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.\nInternational Journal of Computer Vision, 128(2):336\u2013359, February 2020. ISSN 0920-5691, 1573-1405.\ndoi: 10.1007/s11263-019-01228-7. URL http://arxiv.org/abs/1610.02391. arXiv:1610.02391\n[cs].\n[72] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network Dissection: Quantify-\ning Interpretability of Deep Visual Representations, April 2017. URL http://arxiv.org/abs/1704.\n05796. arXiv:1704.05796 [cs].\n[73] Axel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-XL: Scaling StyleGAN to Large Diverse\nDatasets, May 2022. URL http://arxiv.org/abs/2202.00273. arXiv:2202.00273 [cs].\n[74] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,\nJason Baldridge, and Yonghui Wu. Vector-quantized Image Modeling with Improved VQGAN, June 2022.\nURL http://arxiv.org/abs/2110.04627. arXiv:2110.04627 [cs].\n[75] Jacob S Prince, Ian Charest, Jan W Kurzawski, John A Pyles, Michael J Tarr, and Kendrick N Kay.\nImproving the accuracy of single-trial fMRI response estimates using GLMsingle. eLife, 11:e77599,\nNovember 2022. ISSN 2050-084X. doi: 10.7554/eLife.77599. URL https://doi.org/10.7554/\neLife.77599. Publisher: eLife Sciences Publications, Ltd.\n[76] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural\nnetworks. IEEE Transactions on Computational Imaging, 3:47\u201357, 2017.\n[77] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual\nnetworks for single image super-resolution. 2017 IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), pages 1132\u20131140, 2017.\n[78] Kevin Turner. Decoding latents to rgb without upscaling. https://discuss.huggingface.co/t/\ndecoding-latents-to-rgb-without-upscaling/23204/2, 2022. Accessed: 2023-05-23.\n[79] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self-supervised learning of local visual features.\nArXiv, abs/2210.01571, 2022.\n[80] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for\nself-supervised learning. ArXiv, abs/2105.04906, 2021.\n15\nA\nAppendix\nA.1\nAuthor Contributions\nPSS devised the project, led the team, developed the models, drafted the manuscript, and otherwise\ncontributed to all parts of MindEye development. AB drafted the manuscript, developed the models,\nand contributed to all parts of MindEye development including creating the low-level pipeline,\nconception of BiMixCo and soft CLIP loss, and modification of the DALL-E 2 diffusion prior. JG\ndeveloped the models, tracked/compared model variants, and significantly contributed to the MindEye\ncodebase. SS conceived of and implemented LAION-5B retrieval using the CLIP Retrieval client\nand conducted various exploratory experiments. AN implemented the Lafite pipeline for MindEye\nreconstructions. EC conducted various initial explorations into using a diffusion prior for aligning\nvoxels to CLIP space. AJD created the initial webdatasets used to train MindEye and created various\nmodel architectures to compare different mapping approaches. NV conducted various exploratory\nexperiments mapping voxels to StyleGAN-XL [73] latent space. EY shared code to automatically\nidentify identical images for qualitative comparisons and added code to ensure LAION-5B retrieval\ndid not retrieve ground truth images. DW conducted various exploratory experiments and helped\nwith project discussions. KAN oversaw the project and contributed valuable feedback. TMA oversaw\nthe project, conducted initial explorations using VQGAN [74], and helped keep the project on-track\nthrough MedARC and Stability AI communication.\nA.2\nAdditional Dataset Information\nThe Natural Scenes Dataset (NSD) [26] is a public 7-Tesla fMRI dataset containing the brain responses\nof several human participants each spending up to 40 hours in the MRI machine passively viewing\nimages. These square-cropped images of natural scenes were sourced from the MS-COCO dataset\n[27]. Each of 9,000-10,000 unique images was presented for three seconds at a time, shown three\ntimes across 30-40 scanning sessions, totaling 22,000-30,000 trials of fMRI responses per participant.\nfMRI responses correspond to session-wise z-scored single-trial betas output from GLMSingle [75].\nFollowing the procedure used in other reconstruction studies that used NSD [5, 28, 3], we train\nindividual-subject models for the four participants who completed all scanning sessions (participants\n1, 2, 5, and 7) and used a test set corresponding to the shared 1,000 images presented to every\nparticipant. This yields a dataset consisting of 24,980 training samples and 2,770 test samples\u2014we\naverage across the three same-image repetitions for the test set (leaving 982 test samples) but not\nthe training set, similar to Takagi and Nishimoto [3]. We use preprocessed flattened fMRI voxels in\n1.8-mm native volume space corresponding to the \u201cnsdgeneral\u201d brain region, defined by the NSD\nauthors as the subset of voxels in posterior cortex most responsive to the visual stimuli presented\n(between 13,000 to 16,000 voxels per participant). MindEye was developed using a training and\nvalidation set of Subject 1\u2019s data, with the test set (and other subjects\u2019 data) untouched until final\ntraining of models.\nA.3\nMindEye Architecture\nPyTorch code for the MLP backbone and projector is depicted in Algorithm 1. Specifics on how we\nmodified the open-source implementation of the DALL-E 2 diffusion prior are discussed in A.3.1.\nA.3.1\nModifications from DALL-E 2 Diffusion Prior\nThe inputs for the diffusion prior are 257 backbone embeddings, 1 timestep embedding, and 257\nnoised CLIP image embeddings, and the output is 257 denoised CLIP image embeddings. Unlike\nthe DALL-E 2 prior, we do not use learnable queries and instead directly predict denoised CLIP\nembeddings from the noised embeddings. This significantly saves on memory and allows us to train\nthe backbone and prior end-to-end on a single GPU. We observe that adding absolute positional\nembeddings to the noised CLIP embeddings improves performance in the absence of learnable\nqueries. We also observe that our prior can work with just 100 timesteps instead of 1000 as used in\nDALL-E 2. This makes our prior much faster at inference time. We conducted experiments with both\ncausal and bidirectional attention and did not observe any significant difference in reconstruction\nperformance. For simplicity we use bidirectional attention in our final model.\n16\nAlgorithm 1 PyTorch code for MindEye MLP backbone and MLP projector\nclass\nBrainMLP(nn.Module ):\ndef\n__init__(self , out_dim =257*768 ,\nin_dim =15724 ,\nclip_size =768 , h=4096):\nsuper (). __init__ ()\n# in_dim\ncorresponds\nto the\nsubject -specific\n# number of voxels in the \" nsdgeneral \" brain\nregion.\nself.lin0 = nn.Sequential(\nnn.Linear(in_dim , h, bias=False),\nnn.LayerNorm(h),\nnn.GELU(inplace=True),\nnn.Dropout (0.5))\nself.mlp = nn.ModuleList ([\nnn.Sequential(\nnn.Linear(h, h, bias=False),\nnn.LayerNorm(h),\nnn.GELU(inplace=True),\nnn.Dropout (0.15)\n) for _ in range (4)])\nself.lin1 = nn.Linear(h, out_dim , bias=True)\nself.proj = nn.Sequential(\nnn.LayerNorm(clip_size),\nnn.GELU(inplace=True),\nnn.Linear(clip_size , 2048 , bias=False),\nnn.LayerNorm (2048) ,\nnn.GELU(inplace=True),\nnn.Linear(clip_size , 2048 , bias=False),\nnn.LayerNorm (2048) ,\nnn.GELU(inplace=True),\nnn.Linear (2048 ,\nclip_size , bias=True ))\nself.clip_size = clip_size\ndef\nforward(self , x):\nx = self.lin0(x)\nresidual = x\nfor\nres_block\nin range(len(self.mlp )):\nx = self.mlp[res_block ](x)\nx +=\nresidual\nresidual = x\ndiffusion_prior_input = self.lin1(x). reshape(len(x), -1, self.clip_size)\ndisjointed_clip_fmri = self.proj( diffusion_prior_input )\nreturn\ndiffusion_prior_input , disjointed_clip_fmri\nA.3.2\nLow-Level Pipeline: Mapping to Stable Diffusion Variational Autoencoder\nTo map to Stable Diffusion\u2019s VAE latent space we use a low-level pipeline with the same architecture\nas the high level pipeline. We use a separate residual MLP backbone with 4 residual blocks that maps\nflattened voxels to a 16 \u00d7 16 \u00d7 64 dimensional latent space. The reconstruction submodule in the\nlow-level pipeline is a CNN upsampler that upsamples these latents by 4\u00d7 to create embeddings of\nsize (64, 64, 4). The CNN upsampler uses a similar architecture to Stable Diffusion\u2019s VAE decoder,\nwhich does an 8\u00d7 upsampling. To create the targets for the upsampler we upsample NSD images to\n512 \u00d7 512 through bilinear interpolation and encode them with the SD VAE encoder. The resulting\n(64, 64, 4) embeddings form the targets for the high-level pipeline.\nRecent works in low-level vision (super-resolution, denoising, deblurring, etc.) have observed that\nmean absolute error performs better than mean squared error for pixel-level metrics like PSNR and\nSSIM [76, 77] due to better convergence properties. It has been shown that the 4-channel SD latent\nspace effectively compresses images, and latents can be converted to RGB images with a linear\nmapping from latent space to pixel space [78]. We observe that the problem of mapping to SD\nembedding space follows the same properties as low-level vision tasks, such that mean absolute error\nperforms better than mean squared error. We also experiment with using a \"full reconstruction\" loss\nwhere we reconstruct complete images using the SD VAE decoder and apply the loss in pixel space.\nThis performs worse than only applying the loss in latent space and also requires significantly more\nGPU memory.\nThe contrastive submodule in the low-level pipeline acts as an auxiliary loss to improve the perfor-\nmance of the reconstruction submodule. It uses an MLP projector that maps the (16, 16, 64) backbone\noutputs to (16, 16, 512). Since we do not care about retrieval performance for the low-level pipeline,\nwe simply use SoftCLIP loss without BiMixCo. To maximize low-level performance we distill the\nknowledge of VICRegL [79] ConvNext-XXL instead of CLIP ViT. VICRegL with \u03b1 = 0.75 is\nspecialized for low-level tasks and achieves state-of-the-art linear segmentation results, unlike CLIP\nwhich has been trained with high-level text guidance.\n17\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nFigure 5: Additional MindEye reconstructions for Subject 1, randomly selected.\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nRetrieval\nSeen image\nRetrieval\nFigure 6: Additional MindEye retrievals from LAION-5B for Subject 1, randomly selected.\nA.4\nMore Reconstructions / Retrievals\nImages containing all 982 reconstructions and retrievals for each subject are on GitHub. Figure 5\ndepicts a subset of randomly selected reconstruction examples from Subject 1 (first try random\nselection of 30 samples). Figure 6 likewise depicts randomly selected examples from LAION-5B\nretrieval. Figure 7 depicts randomly selected example reconstructions from the low-level pipeline.\nFigure 8 depicts randomly selected reconstructions for the models described in 3.3.\nA.5\nUMAP Comparison\nAs depicted in Figure 9, the CLIP image and CLIP fMRI embedding spaces are disjointed before\nbeing fed through the diffusion prior. While the MLP projector does improve alignment compared to\nthe outputs of the MLP backbone, the diffusion prior does a much better job at aligning the two spaces\nas shown by decreased euclidean distance between data points following UMAP dimensionality\nreduction.\n18\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nFigure 7: Example MindEye reconstructions for Subject 1 output from the low-level pipeline.\nSeen image\nOnly MLP \nbackbone\nBackbone + \nProjector\nBackbone + \nPrior\nMindEye\n(only BiMixCo)\nMindEye\n(0.33 BiMixCo)*\nFigure 8: Example reconstructions for ablation models from Table 4\n19\nImg2Img Strength\nLow-Level\nHigh-Level\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193\n1.0 (Only low-level)\n.456\n.493\n87.1%\n84.1%\n61.6% 62.4% .992\n.638\n0.7\n.439\n.416\n92.7%\n95.1%\n90.0% 87.5% .803\n.514\n0.5\n.429\n.389\n96.3%\n98.4%\n94.7% 92.3% .674\n.405\n0.3\n.410\n.358\n97.5%\n98.8% 94.7% 94.5% .638\n.362\n0.15*\n.390\n.337\n97.4%\n98.7%\n94.5% 94.6% .630\n.358\n0.0 (Only high-level)\n.209\n.318\n92.8%\n98.0%\n94.5% 94.8% .635\n.361\nTable 5: Evaluations from Subject 1 varying img2img strength from 0 (no img2img) to 1 (only low-level\npipeline). The final MindEye uses an img2img strength of 0.15.\nUMAP 1\nUMAP 1\nUMAP 2\nUMAP 1\nCLIP Image x MLP backbone\nCLIP Image x MLP projector\nCLIP Image x Diffusion prior\nFigure 9: UMAP plots depict CLIP image latents (blue), MindEye MLP backbone latents (orange), MindEye\nMLP projector latents (green), and MindEye diffusion prior latents (red). UMAPs were estimated from 1,000\nrandom samples from Subject 1. CLIP image latents correspond to the last hidden layer of ViT-L/14. Euclidean\ndistance between the given MindEye embedding space and CLIP image space is lowest for the diffusion prior,\nsuggesting that the diffusion prior helps to align the two embedding spaces.\nA.6\nReconstruction Evaluations: Additional Information\nTwo-way identification was performed in the same manner as Ozcelik and VanRullen [4]. For each\nmodel, we computed the Pearson correlation between embeddings for the ground truth image and\nthe reconstructed image, as well as the correlation between the ground truth image and a different\nreconstruction elsewhere in the test set. If the correlation for the former was higher than the latter, this\nwas marked as correct. For each test sample, performance was averaged across all possible pairwise\ncomparisons using the other 981 reconstructions to ensure no bias from random sample selection.\nThis yielded 982 averaged percent correct outputs, which we averaged across to obtain the metrics\nreported in Table 1.\nRetrieval evaluations for Ozcelik and VanRullen [4] were not reported in the original paper; we\ncalculated image/brain retrieval ourselves with the help of the Brain-Diffuser GitHub repository.\nA.7\nReconstructions from Stable Diffusion (Image Variations) and Lafite\nWe also attempted reconstructions using Stable Diffusion (Image Variations) [31] and Lafite [32]\nrather than Versatile Diffusion. Reconstructions from these models for Subject 1 are depicted in\nFigure 10, with metrics reported in Table 6.\nFor Stable Diffusion (Image Variations) we use the same approach as MindEye + Versatile Diffusion\nexcept we map from voxels to the 1 \u00d7 768 final layer outputs of ViT-L/14 (same architecture as\n\"4 ResBlocks + Only CLS\" in Table 2). For the diffusion prior we fine-tune an open-sourced\nimplementation of the DALL-E 2 prior that was trained to generate CLIP image embeddings from\nCLIP text embeddings using 250M image-caption pairs from LAION-Aesthetics. We note that\nusing this pretrained prior works much better than training from scratch, suggesting that a similar\nlarge-scale pretrained prior for Versatile Diffusion might further improve fMRI reconstructions. We\n20\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nStable Diffusion (Image Variations)\nLafite (CLIP-ViT/B-32)\nFigure 10: Corresponding reconstructions to Figure 1 when swapping Versatile Diffusion with Stable Diffusion\n(Image Variations) or Lafite.\nuse this MindEye model variant both for reconstructing via Stable Diffusion (Image Variations) and\nfor retrieving the top-16 nearest neighbors in CLIP space for LAION-5B image retrieval. This is\nbecause the CLIP Retrieval client [30] only has precomputed CLIP embeddings for the final layer of\nCLIP, not the last hidden layer as used by Versatile Diffusion.\nFor Lafite we tried to replicate the same approach as Lin et al. [11] but with inputs from the MindEye\nMLP backbone. Lafite is a conditional image generation pipeline that uses the CLIP-aligned voxel\nembeddings as \"condition vectors\". In particular, Lafite leverages a StyleGAN that uses the CLIP\nembeddings as \"style\" vectors to generate images. Lafite\u2019s discriminator is trained to distinguish\ngenerated images from ground truth images and also to semantically align the CLIP embedding of\nthe generated image with the condition vector using contrastive learning. Here we train two mapping\nmodels fmi and fmc that map voxels to the final layer of CLIP ViT-B/32, where fmi is contrastively\naligned with CLIP image embeddings and fmc is contrastively aligned with CLIP text embeddings.\nWe used the same contrastive learning schedule as MindEye with BiMixCo for the first one-third of\nthe training cycle and SoftCLIP for the rest. Note that Lafite doesn\u2019t require training a prior so we\nonly train the MLP backbone. Once the mapping models fmi and fmc are trained, we follow Lin\net al. [11] to fine-tune a pretrained language-free Lafite model provided by [32]. Finally, we use a\nlow-level \"perceptual\" pipeline by aligning layer-2 ResNet features of the generated image with those\nof the ground truth image using contrastive learning. The ResNet was trained using a self-supervised\nVICReg loss [80].\nMethod\nLow-Level\nHigh-Level\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193\nVersatile Diffusion (S1)\n.390\n.337\n97.4%\n98.7% 94.5% 94.6% .630\n.358\nSD Image Variations (S1)\n.376\n.350\n95.7%\n96.4%\n92.5% 92.5% .734\n.446\nLafite (S1)\n.241\n.304\n92.5%\n98.1%\n93.7% 87.0% .701\n.436\nTable 6: Evaluations for Subject 1 across three pretrained final image generation models (Lafite was fine-tuned\nin the same manner as Lin et al. [11]). Both Versatile Diffusion and Stable Diffusion (image variations) used\nan img2img strength of .15 with the low-level reconstructions output from MindEye (Lafite is a GAN and not\ncompatible with the same img2img process).\n21\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSubject 2\nSubject 3\nSubject 4\nFigure 11: Corresponding reconstructions to Figure 1 for the other 3 NSD participants.\nA.8\nSubject-Specific Results\nHere we depict reconstructions across the other 3 NSD participants in Figure 11, with individual\nsubject evaluations metrics in Table 7.\nMethod\nLow-Level\nHigh-Level\nRetrieval\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193 Image \u2191 Brain \u2191\nMindEye (Subj 1)\n.390\n.337\n97.4%\n98.7%\n94.5% 94.6% .630\n.358\n97.2% 94.7%\nMindEye (Subj 2)\n.318\n.327\n95.8%\n98.1%\n93.2% 93.7% .656\n.368\n97.1% 93.9%\nMindEye (Subj 3)\n.265\n.311\n93.2%\n97.8%\n94.9% 94.9% .628\n.353\n90.7% 85.7%\nMindEye (Subj 4)\n.261\n.316\n92.3%\n96.6%\n92.4% 93.0% .666\n.387\n89.4% 85.9%\nTable 7: MindEye retrieval and reconstruction performance for individual participants. These scores were\naveraged across participants for the values shown in Table 1.\nA.9\nSingle-Trial Results\nIn the main paper we report results from the test dataset following the standard approach of averaging\nvoxels across the three same-image repetitions. Reconstruction evaluations using only one brain\nsample for each image is shown in Table 8, with example reconstructions in Figure 12.\n22\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nSeen image\nReconstruction\nFigure 12: Corresponding reconstructions to Figure 1 using brain activity from only the first sample of every\nimage. This is in contrast to Figure 1 which reconstructed from brain activity averaged across three same-image\nrepetitions.\nMethod\nLow-Level\nHigh-Level\nRetrieval\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193 Image \u2191 Brain \u2191\nMindEye\n.309\n.323\n94.7%\n97.8%\n93.8% 94.1% .645\n.367\n93.6% 90.1%\nMindEye (single-trial)\n.255\n.308\n91.6%\n95.9%\n91.3% 91.6% .691\n.398\n80.3% 77.6%\nMindEye (single-, S1)\n.329\n.323\n94.8%\n97.3%\n92.8% 92.7% .680\n.387\n89.0% 86.5%\nMindEye (single-, S2)\n.267\n.311\n93.1%\n96.9%\n91.5% 91.2% .687\n.398\n88.5% 86.1%\nMindEye (single-, S3)\n.217\n.297\n90.2%\n96.3%\n93.2% 94.0% .671\n.381\n75.3% 71.8%\nMindEye (single-, S4)\n.209\n.302\n88.3%\n93.1%\n87.6% 88.7% .727\n.427\n68.5% 66.1%\nTable 8: MindEye retrieval and reconstruction performance for single-trial brain activations, chosen randomly\nout of three possible samples per unique image. Other than using single-trial brain activity, the same settings\nwere used as in Table 1.\n23\nA.10\nPerformance with varying dataset size\nMethod\nLow-Level\nHigh-Level\nRetrieval\nPixCorr \u2191 SSIM \u2191 Alex(2) \u2191 Alex(5) \u2191 Incep \u2191 CLIP \u2191 Eff \u2193 SwAV \u2193 Image \u2191 Brain \u2191\nAll Data (High-Level)\n.209\n.318\n92.8%\n98.0%\n94.5% 94.8% .635\n.361\n97.2% 94.7%\nHalf Data (High-Level)\n.149\n.276\n87.7%\n94.3%\n87.1% 90.1% .738\n.424\n77.5% 60.8%\n2-Sessions (High-Level)\n.119\n.281\n81.0%\n88.2%\n79.2% 84.4% .824\n.472\n17.9% 12.0%\nTable 9: Quantitative comparison of MindEye performance with varying dataset sizes on Subject 1 with the\nhigh-level pipeline. Half Data corresponds to MindEye trained with half of the training samples randomly\nremoved. 2-Sessions corresponds to MindEye trained with a random selection of 500 training image samples (or\n1,500 training fMRI samples given 3 repetitions per image), equivalent to the number of samples collected across\ntwo scan sessions. Notably, image and brain retrieval metrics maintained state-of-the-art performance even\nwhen training the model with half of the training samples removed, and reconstruction performance remained\ncompetitive with previous models even with reduced training data. This suggests that our MindEye approach is\nflexible to being trained with smaller datasets.\nA.11\nModel size comparison with other methods\nMethod\nParameter Count\nLin et al.\n2 \u00d7 1.17M deep models + StyleGAN\nTakagi et al.\nLow Level\n37M linear regression model\nHigh Level\n450M linear regression model\nOzcelik et al.\nLow Level\n1.45B linear regression model\nHigh Level\n257 separate 12M linear regression models\nMindEye\nLow Level\n206M residual MLP + CNN decoder model\nHigh Level\n996M residual MLP + diffusion prior model\nTable 10: Comparison of MindEye parameter count with other competing methods. Other methods primarily\nrely on linear regression or relatively small deep models.\n24\n"
  },
  {
    "title": "TaleCrafter: Interactive Story Visualization with Multiple Characters",
    "link": "https://arxiv.org/pdf/2305.18247.pdf",
    "upvote": "3",
    "text": "TaleCrafter: Interactive Story Visualization with Multiple Characters\nYUAN GONG, Tsinghua Shenzhen International Graduate School, Tsinghua University, China\nYOUXIN PANG, NLPR, Institute of Automation, Chinese Academy of Sciences, China\nXIAODONG CUN, Tencent AI Lab, China\nMENGHAN XIA, Tencent AI Lab, China\nYINGQING HE, Hong Kong University of Science and Technology, China\nHAOXIN CHEN, Tencent AI Lab, China\nLONGYUE WANG, Tencent AI Lab, China\nYONG ZHANG\u2217, XINTAO WANG, and YING SHAN, Tencent AI Lab, China\nYUJIU YANG\u2217, Tsinghua Shenzhen International Graduate School, Tsinghua University, China\n#1 The prince lives in \na beautiful castle.\n#2 The cat is basking \nin the sun on the \ngrassland.\n#3 The prince comes \nacross the cat in the \ngarden.\n#4 The cat is exploring \nthe forest.\n#5 The prince and the \ncat pass through a \nglittering silver grove.\n#6 The prince and the \ncat find the way \nhome.\n#1 Tom and Amy live \nin a room.\n#2 Amy finds a map in \nthe attic.\n#3 Tom and Amy are \nsearching in the library.\n#4 Tom and Amy are \nstanding in the maze.\n#5 Tom is reading a \nletter.\n#6 Tom and Amy are \nstanding together.\nPrince \nCat \nAmy \nTom \nFig. 1. The visual examples of our story visualization system. Given the story and multiple characters, the S2P component first generates a series of prompts\nfrom the story using GPT-4. Then the T2L component creates a reasonable layout given a generated prompt. The core C-T2I component takes multi-modality\ninputs, such as prompt, layout, and sketch, to render an image with the specified characters, locations, and local structures. Finally, the I2V component\nanimates those generated images. The style is specified by \"oil painting\". Video results can be found in the supplementary materials.\nAccurate Story visualization requires several necessary elements, such as\nidentity consistency across frames, the alignment between plain text and\nvisual content, and a reasonable layout of objects in images. Most previous\nworks endeavor to meet these requirements by fitting a text-to-image (T2I)\nmodel on a set of videos in the same style and with the same characters,\ne.g., the FlintstonesSV dataset. However, the learned T2I models typically\nstruggle to adapt to new characters, scenes, and styles, and often lack the flex-\nibility to revise the layout of the synthesized images. This paper proposes\na system for generic interactive story visualization, capable of handling\nmultiple novel characters and supporting the editing of layout and local\nstructure. It is developed by leveraging the prior knowledge of large lan-\nguage and T2I models, trained on massive corpora. The system comprises\nfour interconnected components: story-to-prompt generation (S2P), text-to-\nlayout generation (T2L), controllable text-to-image generation (C-T2I), and\nimage-to-video animation (I2V). First, the S2P module converts concise story\ninformation into detailed prompts required for subsequent stages. Next, T2L\n\u2217Corresponding authors.\nProject : https://github.com/VideoCrafter/TaleCrafter\ngenerates diverse and reasonable layouts based on the prompts, offering\nusers the ability to adjust and refine the layout to their preference. The\ncore component, C-T2I, enables the creation of images guided by layouts,\nsketches, and actor-specific identifiers to maintain consistency and detail\nacross visualizations. Finally, I2V enriches the visualization process by ani-\nmating the generated images. Extensive experiments and a user study are\nconducted to validate the effectiveness and flexibility of interactive editing\nof the proposed system.\n1\nINTRODUCTION\nStory visualization, also known as visual storytelling, is a vital\nmethod for effectively conveying narrative content to a diverse\nrange of audiences. It has a wide range of applications in education\nand entertainment [Yin et al. 2022], e.g., children\u2019s comic books. In\nthis work, story visualization is formulated as such a problem, i.e.,\ngiven a story in plain text and the portrait images of a few characters,\ngenerate a series of images to express the story visually.\narXiv:2305.18247v2  [cs.CV]  30 May 2023\n2\n\u2022\nYuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang\nAn eligible story visualization should meet several essential re-\nquirements to provide an accurate visual representation of a narra-\ntive. First, identity consistency. Maintaining consistent depictions\nof characters and environments across all frames or scenes is cru-\ncial. Second, text-visual alignment. The visual content should align\nclosely with the textual narrative, accurately representing the events\nand interactions described in the story. Third, clear and logical lay-\nout. Objects and characters within the generated images should be\narranged in a reasonable and logical layout. This organization helps\nto guide the viewer\u2019s attention seamlessly through the narrative,\nmaking it easier to understand.\nPioneer works in story visualization typically train models on\nspecific datasets containing characters and styles that are consis-\ntent throughout. Two popular datasets include PorotoSV [Li et al.\n2019a] and FlintstonesSV [Maharana and Bansal 2021], which fea-\nture cartoon styles and limited character variation. Most earlier\napproaches [Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and\nBansal 2021; Maharana et al. 2021, 2022] rely on GAN or VAE-based\nmethods, incorporating text encoders to project text into a latent\nspace, decoders to generate images conditioned on the text, and\nimage and story-level discriminators to retain visual quality and\nconsistency. Some studies now leverage diffusion models to capture\nconditional distribution, often using a pre-trained T2I model for\ninitialization. For instance, AR-LDM [Pan et al. 2022] introduces a\nlatent diffusion model, autoregressively conditioned on historical\ncaptions and synthesized images to predict current frames. Make-A-\nStory [Rahman et al. 2022] proposes an autoregressive model with\na visual memory module, capturing actor and background context\nacross generated frames for content consistency.\nHowever, these methods encounter two unavoidable limitations.\nFirst, they face challenges in generalizing to new actors and scenes,\nas they are trained on specific datasets to fulfill the primary two\nrequirements. Recent work [Jeong et al. 2023] investigates the po-\ntential of zero-shot story visualization using a pre-trained T2I model\nto enable adaptation to any new character and scene. The process\ninvolves generating an image and subsequently replacing the hu-\nman face with a supplied one. Unfortunately, this approach neither\naccommodates multiple characters nor supports objects apart from\nthe human face. Second, none of these methods take into account\nthe third requirement, i.e., the layout of the generated image or\nlocal object structure, with all information implicitly controlled by\nthe text. Although several text-to-image and layout-to-image meth-\nods [Hong et al. 2018; Li et al. 2023; Liang et al. 2023; Rombach et al.\n2022; Zhang et al. 2017] incorporate layout as input or intermediate\nresult, their focus lies solely on single image generation rather than\nstory visualization, without considering cross-frame consistency.\nIn this work, we introduce a versatile interactive story visualiza-\ntion system that satisfies all three requirements, building on the\nknowledge of large-scale language and text-to-image (T2I) models\ntrained on extensive corpora. This system can adapt to various new\ncharacters and support layout and local structure editing beyond\nthe capabilities of previous methods. Our system consists of four\ncomponents: story-to-prompt generation (S2P), text-to-layout gen-\neration (T2L), controllable text-to-image generation (C-T2I), and\nimage-to-video animation (I2V). Given a story, S2P leverages a large\nlanguage model to generate prompts that depict the visual content\nof images based on instructions, including events, scenes, and char-\nacters. Subsequently, T2L utilizes the prompt to create an image\nlayout that offers location guidance for the main subjects, while\nallowing interactive refinement of the layout. The core component,\nC-T2I, renders images conditioned on the layout, local sketch, and\nprompt, while preserving the identity of multiple characters. The\nprompt conveys the image content, whereas the layout and local\nsketch represent the subjects\u2019 locations and detailed local structures,\nrespectively. To preserve identity, the model learns a small set of\npersonalized weights for each character. C-T2I facilitates interactive\nediting of local structures and seamless replacement of characters\nwith new ones. Finally, I2V enriches the visualization process by\nanimating generated images for more vivid presentation. Visual\nresults are shown in Fig. 1.\nOur main contributions are in two key aspects:\n\u2022 We propose a versatile and generic story visualization sys-\ntem that leverages large language and pre-trained T2I mod-\nels for generating a video from a story in plain text. This\nsystem can handle multiple novel characters and scenes.\n\u2022 We develop a controllable, multi-modality text-to-image gen-\neration module, C-T2I, which serves as the core component\nof the visualization system. This module focuses on iden-\ntity preservation for multiple characters and emphasizes\nstructure control in terms of layout and local structure.\n2\nRELATED WORK\n2.1\nStory Visualization\nThe earlier works in the field of story visualization primarily relied\non GAN or VAE-based approaches [Chen et al. 2022; Li 2022; Li et al.\n2019a; Maharana and Bansal 2021; Maharana et al. 2021; Song et al.\n2020]. For instance, StoryGAN [Li et al. 2019a] uses both the full\nstory and individual sentences as inputs to generate contextually\nrelevant images, employing image and story discriminators. DUCO-\nStoryGAN [Maharana et al. 2021], on the other hand, introduces a\ndual learning framework that utilizes video captioning to enhance\nsemantic alignment between stories and generated images. Several\nstudies take advantage of the long-range dependence properties of\ntransformers, such as VP-CSV [Chen et al. 2022] and StoryDALL-\nE [Maharana et al. 2022]. The latter adapts a pre-trained model to a\nspecific dataset to leverage the prior knowledge.\nRecently, diffusion models have shown success in various appli-\ncations, including image, video, and audio generation [Ho et al.\n2022; Liu et al. 2023; Rombach et al. 2022]. Several works inte-\ngrate diffusion models into story visualization, replacing GANs [Pan\net al. 2022; Rahman et al. 2022]. AR-LDM [Pan et al. 2022] adopts\nStoryDALL-E\u2019s setup for story continuation, and utilizes latent dif-\nfusion models as image generators while aggregating information\nfrom current and previous prompts using an autoregressive model.\nMake-A-Story [Rahman et al. 2022] proposes an autoregressive\ndiffusion-based framework featuring a visual memory module. Sim-\nilar to AR-LDM, it leverages historical results, albeit relying on\nthe cross attention mechanism and interacting in the feature space,\nrather than text embedding.\nHowever, these methods often struggle to generalize to novel\ncharacters and scenes, as they inherently fit the model to specific\nTaleCrafter: Interactive Story Visualization with Multiple Characters\n\u2022\n3\nstory\nS2P\ninstruction\n\u201cgenerate K prompts from the story for stable \ndiffusion to generate images, depicting the \nevent, character, and scene\u201d\nP1 : \u201ca cat and a dog\u2026\u201d\nP2 : \u201c\u2026walking in forest\u2026\u201d\nPk : \u201c\u2026play a ball\u2026\u201d\n\u2026\nT2L\ndog cat\nC-T2I\nlayout\nMultiple Characters\nI2V\nsketch\nP1 : \u201ca cat and a dog\u2026\u201d\nvideo\nimage\nFig. 2. The pipeline of our interactive story visualization system. The system comprises four components. (a) Story-to-prompt (S2P): a large language model is\nutilized to bridge the gap between the literary and artistic descriptions and the descriptions fed into T2I models. It comprehends the content in the given story\nand converts it into prompts suitable for T2I models, following the given instructions. (b) Text-to-layout (T2L): generates a reasonable layout for the main\nsubjects in the prompt. (c) Controllable text-to-image (C-T2I): given various conditions such as prompt, layout, sketch, and a few images of each character,\ngenerates consistent-character images. It enables interactive editing of character, layout, and local structure through sketches. (d) Image-to-video (I2V):\nextracts depth from the image and converts it into a video by setting the camera path for novel view synthesis.\ndatasets like FlintstonesSV. Consequently, the model can only recall\nthe characters and scenes from the training dataset. Aiming to elim-\ninate this limitation, one recent study [Jeong et al. 2023] focuses\non zero-shot story visualization supporting novel characters and\nscenes, proposing a method for character identity replacement in\nimages using diffusion models\u2014an approach reminiscent of face\nswapping. However, this method\u2019s scope is limited to single human\nfaces, and the identity preservation and consistency across images\nremain unsatisfactory. Our method targets zero-shot story visual-\nization as well, supporting multiple novel characters and scenes.\nTo ensure identity consistency, we optimize a small set of model\nweights for each characte and propose a personalized inpainting\nmethod to compose multiple characters. Moreover, our approach\nallows control over layout and local object structures, surpassing\nthe capabilities of previous works.\n2.2\nText-to-image Generation\nA significant number of Text-to-Image (T2I) methods are founded\nupon GANs [Reed et al. 2016; Xu et al. 2018; Zhang et al. 2017]. Typ-\nically, these methods involve a text encoder and an image generator.\nSome approaches [Hong et al. 2018; Li et al. 2019b; Qiao et al. 2021]\nemploy intermediate layout generation to simplify image generation\ndirectly from text. Recently, diffusion models have demonstrated po-\ntential in image and video generation, with several studies [Ramesh\net al. 2022, 2021; Rombach et al. 2022; Saharia et al. 2022] improving\nimage quality and diversity using diffusion models. However, these\nmethods mainly concentrate on the alignment between text and a\nsingle generated image, without considering identity consistency\nacross multiple images.\nDespite the success of inversion methods [Gal et al. 2022; Ruiz\net al. 2022; Shi et al. 2023; Yang et al. 2023] in maintaining identity\nin diffusion-based T2I generation, they typically excel with single\nconcepts while struggling to cope with multiple concepts. Custom\nDiffusion [Kumari et al. 2022] aims to compose multiple concepts but\nfalters when dealing with similar-looking concepts, such as cats and\ndogs. Since composing multiple concepts or characters in an image\nusing inversion remains a challenge, we propose a controllable T2I\nmodel for personalized inpainting, which tackles composition from\na different perspective.\n3\nMETHOD\nWe propose an interactive story visualization system that supports\ninteractive editing of character, layout, and local structure. Different\nfrom most previous works, it can handle consistent generation of\nmultiple characters and generalize to novel characters and scenes. As\nshown in Fig. 2, the system comprises four components, i.e., story-\nto-prompt (S2P), text-to-layout (T2L), controllable text-to-image\n(C-T2I), and image-to-video (I2V).\n3.1\nStory-to-prompt Generation\nThe given story could be a brief sentence, e.g., \u201ca cat and a dog have\na wonderful day.\", or it could be a long detailed one with literary\ntechniques. Both might not fit the taste of current T2I models that\nare trained with captions depicting the events, scenes, and objects\nin images. Recently, amazing breakthroughs have been achieved\nin the development of large language models, such as GPT-4 [Ope-\nnAI 2023] and PaLM 2 [et al 2023]. GPT-4 is trained on a massive\nmulti-modality corpus, including vision and language, which is an\nappropriate tool to bridge the gap between literary descriptions and\nthe descriptions for T2I models. We use GPT-4 in this work.\nThe instruction matters in GPT-4. The basic elements in the de-\nscription for T2I models are event, scene, and object. Hence, for a\ngiven story, we use the instruction, like \u201cgenerate K prompts from the\nstory for Stable Diffusion to generate images, depicting event, charac-\nter, and scene.\" Leveraging the capability of pre-trained T2I models,\nwe exploit key words in text to control the style, e.g., using \u201cin oil\npainting style\" as a suffix of the prompts.\nLet S denote a story in plain text. Let \ud835\udc45 and \ud835\udc39 denote the instruc-\ntion and style, respectively. The function of the S2P component can\nbe defined as\n[\ud835\udc5d1, \ud835\udc5d2, ..., \ud835\udc5d\ud835\udc3e] = \ud835\udc462\ud835\udc43(S, \ud835\udc45, \ud835\udc39, \ud835\udc3e),\n(1)\nwhere \ud835\udc3e is the number of prompts to generate. \ud835\udc5d\ud835\udc56 is the \ud835\udc56-th prompt.\n4\n\u2022\nYuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang\n3.2\nText-to-layout Generation\nTransformers are the most widely used techniques to capture layout\ndistribution [Gupta et al. 2021; Jiang et al. 2022]. However, autore-\ngressive decoders are revealed to be inflexible for handling partial\ninputs [Kong et al. 2022] due to its fixed generation order. Recently,\ndiscrete diffusion models [Austin et al. 2021; Gu et al. 2022] are\nintroduced by LayoutDM [Inoue et al. 2023] for layout generation.\nIt achieves satisfying performance and allows various constraints.\nFollowing LayoutDM, we exploit discrete diffusion models for\ntext-to-layout generation. Let L = {\ud835\udc35\ud835\udc56}\ud835\udc41\n\ud835\udc56=1 denote the layout with \ud835\udc41\nobjects, where \ud835\udc35\ud835\udc56 = (b\ud835\udc56,\ud835\udc59\ud835\udc56). b\ud835\udc56 = (\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56,\ud835\udc64\ud835\udc56,\u210e\ud835\udc56) \u2208 {1, ..., \ud835\udc40}4 repre-\nsents the bounding box with (\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) as the center and (\ud835\udc64\ud835\udc56,\u210e\ud835\udc56) as the\nwidth and height. The coordinates are normalized and quantized,\nand \ud835\udc40 is the number of bins. \ud835\udc59\ud835\udc56 \u2208 {1, 2, ...,\ud835\udc36} is the object category.\n\ud835\udc36 is the number of object categories. The flattened discrete vector\nL = {\ud835\udc651,\ud835\udc661,\ud835\udc641,\u210e1,\ud835\udc591,\ud835\udc652,\ud835\udc662, ...} is treated as the latent variable with\na variable length.\nFollowing the definition of discrete diffusion models in D3PM [Austin\net al. 2021], the forward diffusion process for a discrete scalar with\n\ud835\udc37 categories at timestep \ud835\udc61, \ud835\udc67\ud835\udc61 \u2208 {1, 2, ..., \ud835\udc37}, can be defined as:\n\ud835\udc5e(\ud835\udc67\ud835\udc61 |\ud835\udc67\ud835\udc61\u22121) = v(\ud835\udc67\ud835\udc61)\ud835\udc47 Q\ud835\udc61v(\ud835\udc67\ud835\udc61\u22121)\ud835\udc47,\n(2)\nwhere v(\ud835\udc67\ud835\udc61) is one-hot vector of \ud835\udc67\ud835\udc61 and Q\ud835\udc61 \u2208 [0, 1]\ud835\udc37\u00d7\ud835\udc37 is the\ntransition matrix. When z\ud835\udc61 \u2208 {1, 2, ..., \ud835\udc37}\ud835\udc41 is a vector, the forward\nprocess is applied to each of its element independently. Similar to\nDDPM [Ho et al. 2020], the reverse denoising process is estimated by\na network, i.e., \ud835\udc5d\ud835\udf03 (z\ud835\udc61\u22121|z\ud835\udc61) \u2208 [0, 1]\ud835\udc41 \u00d7\ud835\udc37. \ud835\udf03 denotes the parameters\nof a bidirectional Transformer. DP3M decomposes \ud835\udc5d\ud835\udf03 (z\ud835\udc61\u22121|z\ud835\udc61) as\nfollows and learns to estimate \u02dc\ud835\udc5d\ud835\udf03 (z0|z\ud835\udc61) instead:\n\ud835\udc5d\ud835\udf03 (z\ud835\udc61\u22121|z\ud835\udc61) \u221d\n\u2211\ufe01\nz0\n\ud835\udc5e(z\ud835\udc61\u22121|z\ud835\udc61, z0) \u02dc\ud835\udc5d\ud835\udf03 (z0|z\ud835\udc61),\n(3)\nwhere \ud835\udc5e(z\ud835\udc61\u22121|z\ud835\udc61, z0) has a closed-form solution according the def-\ninition of the diffusion process. As the flattened \ud835\udc3f has a variable\nlength, we exploit the padding trick of LayoutDM to handle it.\nWe use the training objective in D3PM, including a widely used\nvariational lower bound \ud835\udc3fvb and an extra loss, i.e.,\n\ud835\udc3fs2p = \ud835\udc3fvb + \ud835\udf06E\ud835\udc5e(z0)E\ud835\udc5e(z\ud835\udc61 |z0) [\u2212 log \u02dc\ud835\udc5d\ud835\udf03 (z0|z\ud835\udc61)],\n(4)\nwhere \ud835\udf06 is a trade-off hyperparameter.\nSince the purpose of T2L is to generate a layout according the\ngiven text, the above formulation cannot be directly applied. We use\na language processing tool [Liu et al. 2021] to extract nouns from\nthe text and treat them as the target objects. To convert the nouns\ninto categorical labels, we use the class names in the Object365\ndataset [Shao et al. 2019]. As Object365 provides the bounding boxes\nand categorical labels of objects in image, we use it to train our T2L\nmodel. The function of the T2L component can be represented as\nL = {\ud835\udc35\ud835\udc56}\ud835\udc41\n\ud835\udc56=1 = \ud835\udc472\ud835\udc3f(\ud835\udc5d),\n(5)\nwhere \ud835\udc5d is the generated prompt from the S2P component.\n3.3\nControllable Text-to-image Generation\nC-T2I is the core component of the story visualization system (see\nFig. 3), which has multi-modality inputs and generates an image\nwith multi-level controls, including the identity, location, and local\nResBlock\nAug\nSelf-Att\nAdd\nK\nV\nQ\n<box, text>\nsketch\n\ud835\udc38\ud835\udc47\nembedding\n\ud835\udc38\ud835\udc46\n\ud835\udc38\ud835\udc47\nCross-Att\nC-T2I\nUnet Block\n\u2026\n\ud835\udc67\ud835\udc61\n\ud835\udc67\ud835\udc61\u22121\nprompt : \n\u201ca cat and a dog\u2026\u201d\ncat\ndog\n\ud835\udc35\ud835\udc44\n\ud835\udc34\ud835\udc3e \ud835\udc35\ud835\udc3e\n\ud835\udc34\ud835\udc44\n\ud835\udc34\ud835\udc49\n\ud835\udc35\ud835\udc49\nFig. 3. The structure of the C-T2I component. It takes a noisy image as\ninput and generates an image through a single denoising step, conditioning\non multiple types of guidance, including prompt, sketch, and bounding\nbox with description. For identity consistency, we use LoRA to learn the\npersonalized weights in self and cross-attention layers as well as a specific\ntoken for each character.\nstructure. Though many works focus on individual tasks of text-\nto-image [Rombach et al. 2022], layout-to-image [Li et al. 2023],\nsketch-to-image [Voynov et al. 2022], and identity-preservation [Ku-\nmari et al. 2022], they cannot simultaneously handle the multi-level\ncontrols that are essential capabilities of interactive story visualiza-\ntion. For example, [Li et al. 2023] is a layout-to-image method with\nthe guidance of text. It can locate the objects with input boxes, but\nit has no control over the local structure and identity. [Chen et al.\n2023] can specify the location of one specified object, but cannot\nhandle multiple objects. [Kumari et al. 2022] attempts to put two\nobjects in an image, but cannot specify their locations and poses.\nInspired by the latent diffusion model (LDM) [Rombach et al.\n2022], our multi-modality conditional generation model is learned\nin the latent space, and the distribution is captured using diffusion\nmodels. The structure of C-T2I is shown in Fig. 3. To allow the\ninjection of multiple types of input, we modify the structure of the\noriginal UNet in LDM. In each UNet block, we upgrade the self and\ncross-attention block and additionally introduce an addition block.\nIdentity Preservation. We exploit the pre-trained CLIP [Radford\net al. 2021] as the text encoder \ud835\udc38\ud835\udc47 to map the input prompt to an\nembedding. The embedding is fed into the cross-attention module to\ninteract with spatial features. For identity preservation, unlike [Ruiz\net al. 2022] and [Kumari et al. 2022], we use LoRA to update addi-\ntional low-rank weights in the self and cross-attention layers instead\nof fine-tuning all the parameters of a learned T2I model. Note that\nLoRA is only applied to the MLPs of the query, key, and value map-\npings. It can alleviate the overfitting issue and concept forgetting\nbecause the original weights are retained, and only a very small set\nof parameters are learned. Given a few images of a character, a token,\nand a set of LoRA weights are trained specifically. The weights are\nnot optimized for multiple characters jointly because the success\nrate of composing two characters is not satisfying, especially for\ncharacters with similar appearance.\nLet W \u2208 R\ud835\udc51\u00d7\ud835\udc58 denote the parameters of a linear mapping, where\n\ud835\udc51 and \ud835\udc58 are the dimensions of the input and output vectors, re-\nspectively. Let h = Wx denote the output vector. x \u2208 R\ud835\udc58 is the\nTaleCrafter: Interactive Story Visualization with Multiple Characters\n\u2022\n5\ninput vector. Using LoRA to remember a new concept, we learn two\nlow-rank matrices A \u2208 R\ud835\udc51\u00d7\ud835\udc5f and B \u2208 R\ud835\udc5f \u00d7\ud835\udc58 instead of updating \ud835\udc4a ,\nwhere \ud835\udc5f \u226a min(\ud835\udc51,\ud835\udc58). A and B have many fewer parameters than\ud835\udc4a\nwhen \ud835\udc5f is small. The forward pass can be rewritten as: h = Wx+BAx.\nObject Localization. In the text-to-layout component, the layout\ncontains the coordinates and category. In the C-I2I component, we\nreplace the category with a phrase that depicts the object while\ninjecting the learned character token. For example, we replace \u201cdog\"\nwith \u201csks dog\" for personalization where sks is the learned token.\nInspired by [Li et al. 2023], the text embedding is extracted using\nCLIP, while the coordinates are encoded using the Fourier embed-\nding [Mildenhall et al. 2021]. The two embeddings are concatenated,\ngo through an MLP layer for information alignment, and then fed\ninto the augmented self-attention module that comprises two self-\nattention layers. One is a typical self-attention layer that contains\nonly the interaction among visual features. The other contains the\ninteraction between visual features and the location embedding to\ninject object location.\nLet f denote the visual features. Let eg = [\ud835\udc38\ud835\udc47 (\ud835\udc5d), Fourier(b)]\ndenote the concatenated embeddings of grounding text \ud835\udc5d and box\nb = (\ud835\udc65,\ud835\udc66,\ud835\udc64,\u210e). The first self-attention can be written as: f \u2190\nf + SA(f), where SA(\u00b7) denotes the self-attention operation. The\nsecond gated self-attention can be written as : f \u2190 f + tanh(\ud835\udefc) \u00b7\nTS(SA([f, e\ud835\udc54])) , where \ud835\udefc is a variable with 0 as the initialization.\nTS(\u00b7) selects the visual features after the interaction.\nLocal Structure Control. Almost all current story visualization\nworks do not take the local structure control of objects into consid-\neration. The layout and object structure are implicitly determined\nsolely by the text. To introduce the flexibility of structure control\nand interactive editing, we use a sketch as one input. Inspired by\nT2I Adapter [Mou et al. 2023], we use a visual encoder \ud835\udc38\ud835\udc46 to map\nthe input sketch into the feature space of the UNet. The encoder\nis a stack of four residual blocks [He et al. 2016]. The predicted\nsketch features are combined with visual features by an addition\nmodule. Note that we first translate and resize the input sketch \ud835\udc3c\ud835\udc46\non a blank canvas according to its corresponding bounding box.\nThen, the created sketch image \u02dc\ud835\udc3c\ud835\udc46 is fed into \ud835\udc38\ud835\udc46. f\ud835\udc60 = \ud835\udc38\ud835\udc46 ( \u02dc\ud835\udc3c\ud835\udc46) denotes\nthe features extracted by \ud835\udc38\ud835\udc46. The local structure control is realized\nby the addition of the visual features f and the generated features\nf, i.e., f \u2190 f + \ud835\udefdf\ud835\udc60, where \ud835\udefd is a parameter to control the strength\nof applying the structure constraint. It is set to \ud835\udefd = 1 for training,\nwhile it can be tuned during inference. Note that when \ud835\udefd = 0 for\ninference, this means the sketch input is not required.\nIterative Generation. As each character has its own personalized\nweights, during inference for multiple characters, their tokens and\nLoRA weights are iteratively applied along with the modules for\nother modalities. For example, given the text \u201ca dog and a cat in a\nforest\", and the boxes and sketches of dog and cat, we first generate\nan image with the personalized weights of dog and the modified\ntext \u201ca <sks> dog and a cat in a forest\". \u201c<sks>\" is the learned token\nfor dog. The text of the dog box is set to \u201c<sks> dog\". Since we have\nthe box of cat, we then inpaint the content of the box with the\ntext \u201c<yty> cat\". \u201c<yty>\" is the learned token of cat. The inpainting\nmodel is a variant of C-T2I with augmented inputs. The difference\nis that we concatenate the noisy image, the original image, and\nthe region mask of the box to form the input with 9 channels. The\ntraining procedure is the same as C-T2I.\nTraining Objective. Following LDM [Rombach et al. 2022], we use\nthe variational lower bound for training,\n\ud835\udc3fC-T2I = E\ud835\udc67\u223c\ud835\udc38\ud835\udc3c (\ud835\udc65),\ud835\udf16\u223cN(0,1),\ud835\udc61\n\u0002\n||\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61, C)||2\n2\n\u0003\n,\n(6)\nwhere \ud835\udc65 here represents an image and \ud835\udc38\ud835\udc3c (\u00b7) is the image encoder\nthat projects image into latent space. \ud835\udf16 is the sampled noise while\n\ud835\udf16\ud835\udf03 (\u00b7) is the predicted noise. \ud835\udc61 is the timestep and \ud835\udc67\ud835\udc61 is the noisy\nimage. C = {\ud835\udc38\ud835\udc47 (\ud835\udc5d), \ud835\udc38\ud835\udc46 ( \u02dc\ud835\udc3c\ud835\udc46), \ud835\udc38\ud835\udc47 (L)} represents the embeddings of\nthe conditions, i.e., prompt, sketch, and layout.\n3.4\nImage-to-video Generation\nTo make story visualization vivid, we introduce an image-to-video\ncomponent into the system. In this component, we mainly focus\non the camera movement to generate a video with considering the\nimage depth. We exploit a 3D photography method [Shih et al. 2020]\nto extract the depth and synthesize images under novelty views,\nwhich can enhance stereognosis detail and make the scene more\nrealistic than a static image. This approach allows setting the camera\npath for various effects, such as zoom-in, circle, and swing.\n4\nEXPERIMENTS\n4.1\nSettings\nDatasets. For the training of the T2L component, we use the Ob-\nject365 dataset [Shao et al. 2019] that contains 365 classes, 2 million\nimages, and 30 million bounding boxes. For the C-T2I component,\nwe use the pre-trained Stable Diffusion (v1.4) [Rombach et al. 2022]\non LAION-5B [Schuhmann et al. 2022] as the prior model, including\nthe CLIP encoder, the image auto-encoder, and the diffusion model.\nThe sketch encoder and the augmented self-attention module is\ntrained on the Flickr dataset [Plummer et al. 2015]. The sketches\nare extracted by PiDiNet [Su et al. 2021]. To reduce the training\ndifficulty of these two parts, we use the encoder weights of T2I\nAdapter [Mou et al. 2023] and gated self-attention weights of GLI-\nGEN [Li et al. 2023] as initialization for training. Then, based on\nthe resulting model, we train the personalized LoRA weights for\neach character with the given 5-9 images. For the story-to-prompt\ngeneration, we use the large language model GPT-4.\nEvaluation Metrics. We evaluate our method along two dimen-\nsions. First, we employ text-image similarity in the CLIP feature\nspace to appraise the text alignment of generated images [Hessel\net al. 2021]. Subsequently, to gauge the consistency of characters, we\nutilize image-image similarity in the CLIP image feature space [Gal\net al. 2022]. Besides, we also conduct a human preference study for\nevaluation.\nBaselines. We compare our method with three approaches: Custom-\nDiffusion [Kumari et al. 2022], Paint-by-Example [Yang et al. 2022],\nand Make-a-Story [Rahman et al. 2022]. Custom-Diffusion employs\na fine-tuning technique for T2I models and enables joint training\nfor multiple concepts and their composition within a single image.\nPaint-by-Example constitutes an exemplar-guided image editing\n6\n\u2022\nYuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang\nCustom-Diffusion\nPaint-by-Example\nOurs\nGirl\nCat\n#1 The girl lives in a \nserene small town.\n#2 The girl and a cat \nare watching rainbow \narcs in the distance.\n#3 The girl and a cat enter \nfantastical world, cotton-like \nclouds floated, colorful \nbutterflies filled the air.\n#4 The girl is dancing \nin the wind.\n#5 The girl is going to \nthe school.\nPrompts\n#6 The girl is dancing in \nthe town square \nattracting many \nspectators.\nFig. 4. Comparison with Custom-Diffusion and Paint-by-Example. One character is an Anime character and the other is a real cat. The style is specified by\n\u201cGhibli\" for all the three methods.\nmethod, which facilitates character insertion into images for story-\ntelling purposes. Make-a-Story introduces an autoregressive deep\ngenerative framework designed to create stories that exhibit en-\nhanced character and background consistency. However, due to its\nreliance on extensive story data for training and incompatibility\nwith our limited dataset, we conduct only qualitative experiments.\nImplementation Details. Both our model and Custom-Diffusion\nare trained using the same target dataset and regularization dataset,\nwith learning rates of 1e-4 and 1e-5, respectively. For each story, we\njointly train Custom-Diffusion on two characters. Considering that\nPaint-by-Example constitutes a zero-shot image editing method, we\nuse target images from the training set to edit on identical back-\nground images and bounding boxes. To ensure fairness, we omit\nsketch guidance when comparing with them.\n4.2\nQualitative Comparisons.\nIn Fig. 4 and Fig. 5, we furnish a qualitative assessment of our pro-\nposed method compared to the state-of-the-art baselines. To com-\npare with Make-a-Story, we utilize examples from the Make-a-Story\npaper to generate content using the same prompts and characters.\nThe results are shown in Fig. 5. Our method has higher image quality\nwith fewer artifacts, while better preserving the characters\u2019 identi-\nties. It can be observed that the lady\u2019s dress changes across images\nin the results of Make-a-Story. While our results are more consistent.\nBesides, as our method is a generic method that does not require\ntraining on a specific data such as FlintstonesSV, it can generate\nnovel scenes out of the range of FlintstonesSV.\n#1 The man and the lady are \nstanding in the room, talking \nto each other, and the man is \nholding a suitcase, oil \npainting\n#2 The man  is standing in a \nroom speaking with an \nexasperated expression and \nholding a pile of clothes, oil \npainting.\n#3 The lady is \nstanding in a room, \noil painting.\n#4 The lady is standing \nin a room and giggling, \noil painting.\nlady\nman\nPrompts\nCustom-Diffusion\nPaint-by-Example\nOurs\nMake-a-Story\nFig. 5. Comparisons with Make-a-Story, Custom-Diffusion, and Paint-by-\nExample using characters from the FlintsonesSV dataset.\nFig. 4 shows the story visualization results of an Anime girl and a\nreal cat. Paint-by-Example and Custom-Diffusion perform poorly in\nTaleCrafter: Interactive Story Visualization with Multiple Characters\n\u2022\n7\nMethod\nCustom-Diffusion\nPaint-by-Example\nOurs\ntext-image sim.\n0.7422\n0.7087\n0.7676\nimage-image sim.\n0.6323\n0.6104\n0.6758\nTable 1. Quantitative comparisons. The text-image and image-image simi-\nlarity are computed in the CLIP feature space.\nMethod\nCustom-Diffusion\nPaint-by-Example\nOurs\nCorrespondence\n1.561\n1.566\n2.873\nCoherence\n1.852\n1.498\n2.651\nQuality\n1.661\n1.614\n2.725\nTable 2. User study on text-image alignment, identity preservation, and\nimage quality. Higher score indicates better performance.\nidentity preservation. They always miss the kerchief or generate an\ninaccurate one. Besides, Paint-by-Example fails to convert the cat\ninto an Anime cat according to the image style. Custom-Diffusion\nhas a low probability of composing the two characters, as in the\nsecond column. Differently, our method performs better in pre-\nserving identity cross images and can also harmoniously compose\ncharacters based on the image style.\n4.3\nQuantitative Comparisons.\nWe evaluate on 5 stories, encompassing 35 prompts with 20 samples\nper prompt, yielding a total of 700 generated images. We employ\nDDIM sampling consisting of 50 steps, and a classifier-free guidance\nvalue of 6 across all approaches. The text-image similarity in CLIP\nfeature space is used to measure the alignment between the prompt\nand the generated image. The image-image similarity measures\nthe performance of identity preservation. We compute the average\nembedding of the given character images and then compute the\nsimilarity between it and generated images. As demonstrated in\nTable 1, our method surpasses the competing methods. Securing\nthe highest text-image and image-image similarity, our method\ndemonstrates an enhanced accuracy in generating stories.\n4.4\nUser Study\nWe perform a user study on the visualization results of 9 stories by\nCustom-Diffusion, Paint-by-Example, and ours. Each story has 3\nprompts, resulting in 3 generated images. For each given story,\n50 participants are asked to rank the performance of the three\ncompeting methods from the following three aspects. First, whether\nthe generated image accurately reflects the input text description.\nSecond, whether the images consistently preserve the character\nidentities. Third, the visual quality of the generated images. The\nscore ranges from 1 to 3. A higher score indicates better performance.\nThe average scores in the three aspects are shown in Table 2. In\ngeneral, our approach achieves the highest scores in all aspects,\ndemonstrating the efficacy of our proposed framework.\n4.5\nInteractive Editing and Image Animation\nInteractive Editing. Our system allows the interactive editing of\nlayout, character, and local structure. Here, we present the control\ncapability of character and local structure. As shown in Fig. 6, we\npresent a comparison of the identity preservation in GLIGEN. GLI-\nGEN takes the reference images as input to fill the masked region\nwhile our method uses the personalized token and weights. It can be\nReference\nMask\nGLIGEN\nOurs\nFig. 6. Comparison with GLIGEN on identity control.\nFig. 7. Visualization of sketch controlling. Our model can generate images\nwith the control on local object structure based on the input sketch.\nobserved that our generated character resembles the reference more\nthan GLIGEN. The verification of the structure control is shown\nin Fig. 7. Given different sketches, the synthesized characters are\nunder the corresponding poses and gestures.\nImage Animation. Our I2V component converts an image to a\nvideo by extracting depth from the image and setting a camera\npath. We also use text-to-speech to convert the story to audio and\ncombine it with the generated video. Video results are presented\nin the supplementary.\n5\nLIMITATIONS\nOur system builds on the pre-trained Stable Diffusion. The quality\nof synthesized images heavily relies on the capability of the pre-\ntrained model. Since Stable Diffusion (v1.4) performs poorly in face\ngeneration, especially when the face covers only a small region in\nthe image, our system inherits this drawback. Another limitation is\nthat the sketch needs to be provided currently. The source sketch can\ncome from image retrieval, drawing, 3D rendering, or T2I generated\nimage. Automatic sketch generation based on reference images and\ntext can be treated as our future work.\n6\nCONCLUSION\nWe present an innovative system for generic interactive story vi-\nsualization capable of handling novel characters and scenes while\n8\n\u2022\nYuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang\nmaintaining identity consistency, alignment between text and vi-\nsual content, and reasonable object layouts. The system\u2019s four in-\nterconnected components - story-to-prompt generation (S2P), text-\nto-layout generation (T2L), controllable text-to-image generation\n(C-T2I), and image-to-video animation (I2V) - work harmoniously\nto create an interactive and flexible visualization solution. Extensive\nexperiments and a user study have demonstrated the effectiveness\nof the proposed system in story visualization.\nREFERENCES\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.\n2021. Structured denoising diffusion models in discrete state-spaces. Advances in\nNeural Information Processing Systems 34 (2021), 17981\u201317993.\nHong Chen, Rujun Han, Te-Lin Wu, Hideki Nakayama, and Nanyun Peng. 2022.\nCharacter-centric story visualization via visual planning and token alignment. arXiv\npreprint arXiv:2210.08465 (2022).\nMinghao Chen, Iro Laina, and Andrea Vedaldi. 2023. Training-Free Layout Control\nwith Cross-Attention Guidance. arXiv preprint arXiv:2304.03373 (2023).\nRohan Anil et al. 2023. PaLM 2 Technical Report. arXiv:2305.10403 [cs.CL]\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu\nYuan, and Baining Guo. 2022. Vector quantized diffusion model for text-to-image\nsynthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 10696\u201310706.\nKamal Gupta, Justin Lazarow, Alessandro Achille, Larry S Davis, Vijay Mahadevan,\nand Abhinav Shrivastava. 2021. Layouttransformer: Layout generation and comple-\ntion with self-attention. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 1004\u20131014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition. 770\u2013778.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.\nClipscore: A reference-free evaluation metric for image captioning. arXiv preprint\narXiv:2104.08718 (2021).\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022.\nImagen video: High definition video generation with diffusion models. arXiv preprint\narXiv:2210.02303 (2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nSeunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. 2018. Inferring\nsemantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 7986\u20137994.\nNaoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi.\n2023. LayoutDM: Discrete Diffusion Model for Controllable Layout Generation.\narXiv preprint arXiv:2303.08137 (2023).\nHyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. 2023. Zero-shot Generation of\nCoherent Storybook from Plain Text Story using Diffusion Models. arXiv preprint\narXiv:2302.03900 (2023).\nZhaoyun Jiang, Shizhao Sun, Jihua Zhu, Jian-Guang Lou, and Dongmei Zhang. 2022.\nCoarse-to-Fine Generative Modeling for Graphic Layouts. In Proceedings of the AAAI\nConference on Artificial Intelligence, Vol. 36. 1096\u20131103.\nXiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan\nEssa. 2022. BLT: bidirectional layout transformer for controllable layout generation.\nIn Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XVII. Springer, 474\u2013490.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv preprint\narXiv:2212.04488 (2022).\nBowen Li. 2022. Word-Level Fine-Grained Story Visualization. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXXVI. Springer, 347\u2013362.\nWenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu,\nand Jianfeng Gao. 2019b. Object-driven text-to-image synthesis via adversarial\ntraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 12174\u201312182.\nYitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin,\nDavid Carlson, and Jianfeng Gao. 2019a. Storygan: A sequential conditional gan for\nstory visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 6329\u20136338.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,\nChunyuan Li, and Yong Jae Lee. 2023. GLIGEN: Open-Set Grounded Text-to-Image\nGeneration. arXiv preprint arXiv:2301.07093 (2023).\nJiadong Liang, Wenjie Pei, and Feng Lu. 2023. Layout-bridging text-to-image synthesis.\nIEEE Transactions on Circuits and Systems for Video Technology (2023).\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu\nWang, and Mark D Plumbley. 2023. Audioldm: Text-to-audio generation with latent\ndiffusion models. arXiv preprint arXiv:2301.12503 (2023).\nLemao Liu, Haisong Zhang, Haiyun Jiang, Yangming Li, Enbo Zhao, Kun Xu, Linfeng\nSong, Suncong Zheng, Botong Zhou, Jianchen Zhu, Xiao Feng, Tao Chen, Tao Yang,\nDong Yu, Feng Zhang, Zhanhui Kang, and Shuming Shi. 2021. TexSmart: A System\nfor Enhanced Natural Language Understanding. In ACL-IJCNLP.\nAdyasha Maharana and Mohit Bansal. 2021. Integrating visuospatial, linguistic and\ncommonsense structure into story visualization. arXiv preprint arXiv:2110.10834\n(2021).\nAdyasha Maharana, Darryl Hannan, and Mohit Bansal. 2021. Improving generation and\nevaluation of visual stories via semantic consistency. arXiv preprint arXiv:2105.10026\n(2021).\nAdyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022. Storydall-e: Adapting\npretrained text-to-image transformers for story continuation. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXXVII. Springer, 70\u201387.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields\nfor view synthesis. Commun. ACM 65, 1 (2021), 99\u2013106.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and\nXiaohu Qie. 2023. T2i-adapter: Learning adapters to dig out more controllable ability\nfor text-to-image diffusion models. arXiv preprint arXiv:2302.08453 (2023).\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\nXichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. 2022. Synthesizing\nCoherent Story with Auto-Regressive Latent Diffusion Models. arXiv preprint\narXiv:2211.10950 (2022).\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hocken-\nmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase\ncorrespondences for richer image-to-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision. 2641\u20132649.\nYanyuan Qiao, Qi Chen, Chaorui Deng, Ning Ding, Yuankai Qi, Mingkui Tan, Xincheng\nRen, and Qi Wu. 2021. R-GAN: Exploring human-like way for reasonable text-to-\nimage synthesis via generative adversarial networks. In Proceedings of the 29th ACM\nInternational Conference on Multimedia. 2085\u20132093.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.\nLearning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning. PMLR, 8748\u20138763.\nTanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and\nLeonid Sigal. 2022. Make-A-Story: Visual Memory Conditioned Consistent Story\nGeneration. arXiv preprint arXiv:2211.13319 (2022).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-\nford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In\nInternational Conference on Machine Learning. PMLR, 8821\u20138831.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and\nHonglak Lee. 2016. Generative adversarial text to image synthesis. In International\nconference on machine learning. PMLR, 1060\u20131069.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation. arXiv preprint arXiv:2208.12242 (2022).\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems 35 (2022), 36479\u201336494.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation\nimage-text models. arXiv preprint arXiv:2210.08402 (2022).\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing\nLi, and Jian Sun. 2019. Objects365: A large-scale, high-quality dataset for object\ndetection. In Proceedings of the IEEE/CVF international conference on computer vision.\n8430\u20138439.\nJing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2023. InstantBooth: Personalized Text-\nto-Image Generation without Test-Time Finetuning. arXiv preprint arXiv:2304.03411\nTaleCrafter: Interactive Story Visualization with Multiple Characters\n\u2022\n9\n(2023).\nMeng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3d photogra-\nphy using context-aware layered depth inpainting. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 8028\u20138038.\nYun-Zhu Song, Zhi Rui Tam, Hung-Jen Chen, Huiao-Han Lu, and Hong-Han Shuai.\n2020. Character-preserving coherent story visualization. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nXVII 16. Springer, 18\u201333.\nZhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietik\u00e4inen, and\nLi Liu. 2021. Pixel difference networks for efficient edge detection. In Proceedings of\nthe IEEE/CVF international conference on computer vision. 5117\u20135127.\nAndrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2022. Sketch-Guided Text-to-\nImage Diffusion Models. arXiv preprint arXiv:2211.13752 (2022).\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and\nXiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional\ngenerative adversarial networks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 1316\u20131324.\nBinxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong\nChen, and Fang Wen. 2022. Paint by Example: Exemplar-based Image Editing with\nDiffusion Models. arXiv preprint arXiv:2211.13227 (2022).\nJianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang Chen, and Junbo Zhao. 2023.\nControllable Textual Inversion for Personalized Text-to-Image Generation. arXiv\npreprint arXiv:2304.05265 (2023).\nFei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan\nBai, Baoyuan Wu, Jue Wang, and Yujiu Yang. 2022. StyleHEAT: One-shot high-\nresolution editable talking face generation via pre-trained StyleGAN. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XVII. Springer, 85\u2013101.\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,\nand Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic image synthesis with\nstacked generative adversarial networks. In Proceedings of the IEEE international\nconference on computer vision. 5907\u20135915.\n"
  },
  {
    "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory",
    "link": "https://arxiv.org/pdf/2305.17144.pdf",
    "upvote": "2",
    "text": "Ghost in the Minecraft: Generally Capable Agents for\nOpen-World Environments via Large Language\nModels with Text-based Knowledge and Memory\nXizhou Zhu1,2\u2217, Yuntao Chen3\u2217 , Hao Tian2\u2217 , Chenxin Tao1,2\u2217 , Weijie Su2,4\u2217 , Chenyu Yang1\u2217 ,\nGao Huang1, Bin Li4, Lewei Lu2, Xiaogang Wang2,5, Yu Qiao6, Zhaoxiang Zhang7, Jifeng Dai1,6 \u00001Tsinghua University\n2SenseTime Research\n3Centre for Artificial Intelligence and Robotics, HKISI, CAS\n4University of Science and Technology of China\n5The Chinese University of Hong Kong\n6Shanghai Artificial Intelligence Laboratory\n7Institute of Automation, Chinese Academy of Science (CASIA)\n{zhuxizhou,gaohuang,daijifeng}@tsinghua.edu.cn, chenyuntao08@gmail.com\ntianhao2@senseauto.com, {tcx20,yangcy19}@mails.tsinghua.edu.cn,\njackroos@mail.ustc.edu.cn, binli@ustc.edu.cn, luotto@sensetime.com\nxgwang@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn, zhaoxiang.zhang@ia.ac.cn\nAbstract\nThe captivating realm of Minecraft has attracted substantial research interest in\nrecent years, serving as a rich platform for developing intelligent agents capable of\nfunctioning in open-world environments. However, the current research landscape\npredominantly focuses on specific objectives, such as the popular \"ObtainDiamond\"\ntask, and has not yet shown effective generalization to a broader spectrum of\ntasks. Furthermore, the current leading success rate for the \"ObtainDiamond\"\ntask stands at around 20%, highlighting the limitations of Reinforcement Learning\n(RL) based controllers used in existing methods. To tackle these challenges, we\nintroduce Ghost in the Minecraft (GITM), a novel framework integrates Large\nLanguage Models (LLMs) with text-based knowledge and memory, aiming to\ncreate Generally Capable Agents (GCAs) in Minecraft. These agents, equipped\nwith the logic and common sense capabilities of LLMs, can skillfully navigate\ncomplex, sparse-reward environments with text-based interactions. We develop\na set of structured actions and leverage LLMs to generate action plans for the\nagents to execute. The resulting LLM-based agent markedly surpasses previous\nmethods, achieving a remarkable improvement of +47.5% in success rate on the\n\"ObtainDiamond\" task, demonstrating superior robustness compared to traditional\nRL-based controllers. Notably, our agent is the first to procure all items in the\nMinecraft Overworld technology tree, demonstrating its extensive capabilities.\nGITM does not need any GPU for training, but a single CPU node with 32 CPU\ncores is enough. This research shows the potential of LLMs in developing capable\nagents for handling long-horizon, complex tasks and adapting to uncertainties\nin open-world environments. See the project website at https://github.com/\nOpenGVLab/GITM.\n1\nIntroduction\n\u201cWhat if a cyber brain could possibly generate its own ghost, create a soul all by itself? And if it did,\njust what would be the importance of being human then?\u201d\n\u2014 Ghost in the Shell (1995)\n\u2217Equal contribution. This work is done when Chenxin Tao and Weijie Su are interns at SenseTime Research.\n\u0000 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.17144v2  [cs.AI]  1 Jun 2023\nFigure 1: Our GITM unlocks the entire technology tree by obtaining all items in Minecraft\nOverworld. Each node represents an individual item in Minecraft. The directed edges between nodes\nrepresent prerequisite relationships for obtaining items. For better readability, we manually merge\nsome similar nodes, e.g., \u201cwooden_pickaxe\u201d, \u201cwooden_axe\u201d, \u201cwooden_hoe\u201d, and \u2019wooden_shovel\u2019\nare merged into one node, and \u201cwooden_pickaxe\u201d is selected to represent the merged node. Existing\nMinecraft agents [2, 7, 25] only unlocked 78 / 262 = 30% items, while our GITM successfully\nunlocked all items.\ngoal\ngoal\nRL Agent\nkeyboard & mouse\nobservation\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft / smelt\ndig_down\n\u2026\nRL-based method\nOurs\ngoal\nLLM\nDecomposer\nLLM Planner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft / smelt\ndig_down\n\u2026\nGhost In the Minecraft (GITM)\nsub-goal tree\nLLM Interface\nFigure 2: Comparison between RL-based method and our GITM. RL agents try to map an\ncomplex goal directly to a sequence of low-level control signals, while our GITM leverages LLM to\nbreak down the goals and map them to structured actions for final control signals.\nMinecraft, as the world\u2019s best-selling game, boasts over 238 million copies sold and more than\n140 million peak monthly active users [27]. Within the game, hundreds of millions of players have\nexperienced a digital second life by surviving, exploring and creating, closely resembling the human\nworld in many aspects. Given its massive scale, vast success, and unrestricted freedom, Minecraft\nhas established itself as an unparalleled platform for researching autonomous and robust Generally\nCapable Agents (GCAs) [23] in open-world environments brimmed with long-horizon challenges,\nenvironmental disruptions, and uncertainties.\nMinecraft acts as a microcosm of the real world. Developing an automated agent that can mas-\nter all technical challenges in Minecraft is akin to creating an artificial intelligence capable of\nautonomously learning and mastering the entire real-world technology. However, existing re-\nsearches [2, 7, 25] remain narrowly scoped. Prior studies have predominantly focused on the\nspecific goal of ObtainDiamond [18]. Yet, in the process of obtaining diamonds, the number of\ntypes of items involved only accounts for <5% of the entire Minecraft world. ObtainDiamond only\nrequires specialized skills in a specific domain, while obtaining all items in Minecraft demonstrates a\nwide range of knowledge and capabilities, similar to mastering multidisciplinary fields in the real\nworld. As illustrated in Fig. 1, our work endeavors to obtain all items in Minecraft within a reasonable\ncomputation budget. This achievement stands as a significant milestone in the development of GCAs,\nillustrating the potential of intelligent agents to match human performance in terms of versatility and\nadaptability.\nAlthough reinforcement learning (RL) [16] is the most popular paradigm for approaching GCAs, it\nhas shown some staggering limitations in conquering Minecraft. RL-based agents typically require\na vast number of learning steps (e.g., nearly 30 million steps to obtain diamonds as reported in\nDreamerV3 [7]) and exhibit poor scalability when learning new tasks(e.g., VPT [2] uses different\nagents for world exploration and diamond mining). As a consequence, adopting RL-based agents for\n2\ncompleting a wide range of tasks may require an prohibitively high number of training steps, making\nit impractical to obtain all items in Minecraft. This inefficiency and lack of adaptability have hindered\nthe development of generally capable agents in open-world environments.\nAs shown in Fig. 2, the biggest dilemma of previous RL-based agents is how to map an extremely\nlong-horizon and complex goal to a sequence of lowest-level keyboard/mouse operations. To address\nthis challenge, we propose our framework Ghost In the Minecraft (GITM) 2, which uses Large\nLanguage Model (LLM)-based agents as a new paradigm. Instead of direct mapping like RL agents,\nour LLM-based agents employ a hierarchical approach. It first breaks down the decompose goal\ninto sub-goals, then into structured actions, and finally into keyboard/mouse operations. Such\ndecomposition is similar to how humans solve complex problems in the real world, enabling mastery\nof Minecraft with efficiency orders of magnitude higher than that of RL. LLM can also leverage\ntext-based knowledge and memory to quickly acquire the ability to interact with the environment\nand accomplish goals, offering immense learning efficiency improvements, unlimited scalability and\nrepresenting a disruptive innovation compared with RL. Our GITM framework has the potential to\nrevolutionize the path to generally capable agents.\nSpecifically, the proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and\nan LLM Interface, which are responsible for the decomposition of sub-goals, structured actions,\nand keyboard/mouse operations, respectively. Given a goal in Minecraft, LLM Decomposer first\ndecomposes it into a series of well-defined sub-goals according to the text-based knowledge collected\nfrom the Internet. Then, LLM Planner plans a sequence of structured actions for each sub-goal. The\nstructured actions are defined with clear semantics and corresponding feedback, enabling LLMs to\nunderstand surrounding environments and make decisions at the cognitive level. LLM Planner also\nrecords and summarizes successful action lists into a text-based memory to enhance future planning.\nFinally, LLM Interface execute the structured actions to interact with the environment by processing\nraw keyboard/mouse input and receiving raw observations.\nIn this paper, we demonstrate the feasibility of employing Large Language Models (LLMs) to develop\nGenerally Capable Agents (GCAs) within an open-world environment built from Minecraft. By\nexploiting the common sense and reasoning capabilities of LLMs for hierarchical goal decomposition,\nas well as utilizing text-based knowledge and memory, this paradigm shows the possibility of\nenabling agents to address a wide range of challenges within Minecraft and allowing them to\neffectively handle such open-world environment. Consequently, our agent has surpassed all previous\nmethods in achieving the ObtainDiamond goal (+47.5% success rate). Our agent also demonstrates\nsuperior learning efficiency compared to previous methods, reducing the number of environment\ninteraction steps by more than 10,000\u00d7. Specifically, VPT [2] needs to be trained for 6,480 GPU days,\nDreamerV3 [7] needs to be trained for 17 GPU days, while our GITM does not require any GPUs\nand can be trained in just 2 days using a single CPU node with 32 CPU cores. More importantly, by\nobtaining all items in Minecraft Overworld as a milestone, this work represents a crucial first step\ntowards achieving GCAs that can handle any task humans can accomplish in Minecraft.\n2\nRelated Work\nMinecraft agents are intelligent programs that can perform various tasks within Minecraft world.\nReinforcement learning has dominated this area for many years. Some initial attempts have tried\nto use hierarchical RL [14, 15, 22] or imitation learning [1] in MineRL competitions [6, 10, 17].\nRecently, with large-scale web data, VPT [2] builds a foundation model for Minecraft by learning\nfrom videos. Based on its success, many works [18] have also explored to finetune foundation\nmodel with human feedback. On the other hand, as Minecraft agents become increasingly proficient\nin handling simple tasks, the importance of multi-task learning becomes more prominent. Some\nprevious works have adopted knowledge distillation [24] and curriculum learning [11], while recent\nworks [3, 5] tried to construct a language-conditioned multi-task agent via feeding the goal description\nembedding into the model.\nRecently, researchers have come to aware the extraordinary general planning ability for LLMs [8].\nMany works [9, 25, 28] have leveraged LLMs for enhancing the high-level planning ability of\nminecraft agents. Inner Monologue [9] leveraged environment feedback to improve the planning\nability of LLM. DEPS [25] further extended this closed-loop interaction by introducing description,\n2The name is chosen to pay tribute to the science fiction movie \"Ghost in the Shell\".\n3\ngoal\nLLM\nPlanner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nenvironment\nexplore\nmine\ncraft / smelt\ndig_down\n\u2026\ntext-based\nknowledge\ntext-based\nmemory\n(Object, Count, Material, Tool, Info)\ngoal format\nLLM\nDecomposer\nequip, explore, approach, mine, \nattack, dig_down, go_up, build,\ncraft, smelt, apply, place\nstructured action set\nsub-goal tree\nupdate\nupdate\nLLM Interface\nFigure 3: Overview of our GITM. Given a Minecraft goal, the LLM Decomposer divides the goal\ninto a sub-goal tree. The LLM Planner then plans an action sequence for each sub-goal. Finally,\nthe LLM Interface executes each action in the environment. Our LLM-based agents can be further\nenhanced by leveraging text-based knowledge and memory.\nexplainer and selector. Plan4MC [28] pre-defined basic skills and instructed LLM to extract the\nrelationship between skills to construct a skill graph.\nUnlike previous RL-based or RL with LLM methods, our LLM-native approach brings the minecraft\nagent to another level both in efficiency and robustness by leveraging high-level action abstraction\nand text-based knowledge and memory.\nLarge Language Models with Tools Extending the ability of LLMs by leveraging external tools\nhave drawn a lot of attentions recently. Several works [4, 13, 21] have explored to augment LLMs\nwith robot perception and control abilities. Code as Polices[13] tried to prompt LLM to generate\ncodes that can drive robots. PaLM-E [4] unified robot perception, instruction following, task planning\nand low-level control into a unified framework. Another line of works tries to build external plugins\naround LLMs to enhance its ability. Toolformer [19] tries to teach LLMs to choose and use a wide\nrange of tools like calculator and search engines and incorporate the results from tools into text\ngeneration. HuggingGPT [20] builds an agent for leveraging a combination of vision, language and\naudio models hosted on HuggingFace for completing user request. API Bank [12] proposes a syntheic\nbenchmark suite for evaluating the how good LLMs are for using external tools.\nCompared with these tool-augmented LLMs, our agents are tasks for much more complex goals in a\nhigh uncertain open-world.\n3\nMethod\nTraditional RL-based agents struggle to develop generally capable agents in Minecraft. The core issue\nis that they attempt to map extremely long-horizon and complex goals directly to the lowest-level\nkeyboard and mouse operations. To overcome this, we propose LLM-based agents in Fig. 2 that utilize\nhierarchical goal decomposition. LLM Decomposer, LLM Planner, and LLM Interface are introduced\nto progressively decompose the task goal into sub-goals, structured actions, and keyboard/mouse\noperations. Moreover, LLM-based agents can leverage text-based knowledge and memory to quickly\nacquire the skills needed to master Minecraft.\n3.1\nLLM Decomposer\nRather than directly assigning the task goal to the agent and expecting a comprehensive and robust\naction plan, this work suggests the more practical strategy of decomposing the task goal into a\nseries of more achievable sub-goals. By addressing each constituent sub-goal, the task goal can\nbe progressively achieved. To this end, an LLM Decomposer is proposed. Goals are fed to the\ndecomposer and recursively decomposed into a sub-goal tree. Text-base knowledge provides the\nnecessary information for decomposition.\nGoal Format. Since we aim to obtain all items in Minecraft, all goals can be defined in the format of\n(Object, Count, Material, Tool, Info),\n(1)\nwhere \u201cObject\u201d denotes the target item, \u201cCount\u201d specifies the target quantity. \u201cMaterial\u201d and \u201cTool\u201d\nrefer to prerequisites needed to obtain the target item. \u201cInfo\u201d stores the text-based knowledge related\nto this goal. Given a specific goal, its sentence embedding extracted from a pre-trained LLM is used\n4\nAction Interface:\n\ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61)\n\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52(\ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61, \ud835\udc61\ud835\udc5c\ud835\udc5c\ud835\udc59)\n\ud835\udc51\ud835\udc56\ud835\udc54_\ud835\udc51\ud835\udc5c\ud835\udc64\ud835\udc5b(\ud835\udc66_\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc59, \ud835\udc61\ud835\udc5c\ud835\udc5c\ud835\udc59)\n...\nQuery Illustration:\nQuery contains a goal, feedback from the \nagent, and reference plan from the memory\nResponse Format:\n{\n\u201cexplanation\u201d: \u201cexplain action failure\u201d,\n\u201cthoughts\u201d: \u201cthoughts on your plan\u201d,\n\u201caction_list\u201d: [ \u2026 ]\n}\nInteraction Guideline:\nYou will receive feedback messages. If \nyour action failed, please replan \u2026\nInstruction\nQuery\nGoal: obtain 1 diamond given iron pickaxe.\nExternal info: Diamond is usually\nfound at level 12 \u2026\nFeedback\nReference plan\nPlan\n{\n\u201cexplanation\u201d: \u201cThe last action failed because diamond cannot be found on the ground.\u201d,\n\u201cthoughts\u201d: \u201cFirst I need to dig down to level 12 where diamond is usually found. \nThen I can explore diamond ore underground with branching strategy.\nOnce I find diamond ore, I can mine it with the tool iron pickaxe.\u201d\n\u201caction_list\u201d: [\n{\u201cname\u201d: \u201cdig_down\u201d, \u201cargs\u201d: {\u201cy_level\u201d: \u201c12\u201d, \u201ctool\u201d: \u201ciron_pickaxe\u201d}},\n{\u201cname\u201d: \u201cexplore\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d, \u201cstrategy\u201d: \u201cbranching\u201d}},\n{\u201cname\u201d: \u201cmine\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d, \u201ctool\u201d: \u201ciron_pickaxe\u201d}\n]\n}\nLarge \nLanguage \nModel\nFeedback\nThe action {\u201cname\u201d: \u201cmine\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d, \u201ctool\u201d: \u201ciron_pickaxe\u201d} failed\nError message: the target object \u201cdiamond_ore\u201d is too far\n- inventory: {\u2018iron_pickaxe\u2019: 1, \u2026}\n- environment: {\u2018biome\u2019: forest, \u2018y_level\u2019: 12}\nAgent\nMemory\nGoal object: diamond\nReference plan:\n[\n{\u201cname\u201d: \u201cdig_down\u201d, \u201cargs\u201d: {\u201cy_level\u201d: \u201c12\u201d, \u201ctool\u201d: \u201ciron_pickaxe\u201d}},\n{\u201cname\u201d: \u201cexplore\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d, \u201cstrategy\u201d: \u201cbranching\u201d}},\n{\u201cname\u201d: \u201capproach\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d}},\n{\u201cname\u201d: \u201cmine\u201d, \u201cargs\u201d: {\u201cobject\u201d: \u201cdiamond_ore\u201d, \u201ctool\u201d: \u201ciron_pickaxe\u201d}\n]\nFigure 4: Illustration of our planning process with the LLM Planner and the agent in the loop.\nGiven a specific goal, the planner generates plans with structured actions under the guidance of\ninstruction, user query, previous feedback, and reference plan from memory. The agent executes the\nactions and provides feedback for the following planning.\nto retrieve the most relevant text-based knowledge from an external knowledge base. Then, the LLM\nidentifies the required material, tools, and related information from the gathered knowledge. The\ncomplete instructions for the LLM are described in Appendix.\nRecursive Decomposition. This goal format enables recursive decomposition of each goal into a sub-\ngoal tree. Specifically, given a goal, all prerequisite items are listed as sub-goals, including materials,\ntools, and their corresponding quantities. Then, the recursive decomposition continues for each\nsub-goal until it has no prerequisites. After the decomposition is completed, the execution sequence\nof the sub-goals is planned through post-order traversal. Such goal decomposition significantly\nenhances the success rate of LLM planning, especially for goals necessitating long-horizon planning.\nText-based Knowledge. External knowledge is essential for automatic goal decomposition. We\nbuild an external knowledge base documented in text from the Minecraft Wiki on the Internet 3 and\nthe item crafting/smelting recipes, providing an exhaustive source of knowledge about the Minecraft\nworld. For instance, if we need to craft a wooden pickaxe, the item crafting recipe will indicate that\nthe required materials are three planks and two sticks, and the necessary tool is a crafting table. It also\nprovides information about the distribution of raw materials. For example, diamonds are frequently\nfound in levels 10\u223c12 underground.\n3.2\nLLM Planner\nLLMs excel at language understanding and reasoning but struggle with low-level control and mul-\ntimodal perception. To leverage LLMs\u2019 strengths while addressing their limitations, we develop\nstructured actions and feedback mechanisms as an abstract interface for them to manage agent-\nenvironment interaction. We propose an LLM-based Planner to achieve goals in Minecraft. Given a\ngoal, it generates structured actions to control agents, receives feedback, and revises plans accordingly.\nIt also has a text memory that aids planning by providing solutions for frequent goals.\nStructured Actions. The structured actions are designed with well-defined functions and clear\nsemantics, enabling LLMs to make decisions at the cognitive level. A structured action can be defined\nas follows:\n(Name, Arguments, Description),\n(2)\n3https://minecraft-archive.fandom.com/wiki/Minecraft_Wiki\n5\nTable 1: Examples of structured actions. A structured action contains name and arguments for\nexecution, as well as description to help LLMs understand and decide when to choose this action.\nName\nArguments\nDescription\nequip\nobject\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and armor.\nexplore\nobject, strategy\nMove around to find the object: used to find objects including block items and entities on the ground.\napproach\nobject\nMove close to a visible object: used to approach the object you want to attack or mine.\nmine/attack\nobject, tool\nAttack / Mine the object with the tool: used to attack / mine the object within reach.\ndig_down/go_up\nylevel, tool\nDig down / Go up with the tool: used to go down / up underground.\nbuild\nblueprint\nBuild according to a blueprint: used to place corresponding objects on locations according to a preset blueprint.\ncraft/smelt\nobject, tool, material\nCraft / Smelt the object with the materials and tool: used to craft new object that is not in the inventory or is not enough.\napply/place\nobject, tool\nApply / Place the tool on the object: used to apply tools or place blocks.\nThe name and arguments defines the action we want the agent to execute, while the action description\nprovides enough information for letting LLMs know when to choose the corresponding actions, as\nshown in Tab. 1.\nWe extract the set of structured actions by leveraging the powerful reasoning capability of LLMs.\nSpecifically, a pre-trained LLM is utilized to decompose the 3141 predefined tasks provided by\nMineDojo [5] into action sequences. Instructions for guiding LLMs on action decomposition are\nprovided in Appendix. Then, we extract the structured actions by selecting frequent actions and\nmerging actions with similar functionalities. See Appendix for the set of structured actions.\nFeedback Mechanism. Open-loop planning cannot guarantee success, especially in open-world\nenvironments, where agents might encounter unexpected events. Feedback is crucial to form an effec-\ntive closed loop. Without appropriate feedback, the LLM has no information about the consequences\nof actions and may repeat failed action plans. Feedback message is designed to present the agent\u2019s\ncurrent state in the environment (i.e., inventory and environment), as well as the success and failure\ninformation for each executed actions, as shown in Fig. 4. By incorporating this feedback message,\nthe LLM can update its understanding of the environment, refine their strategies, and adapt their\nbehavior accordingly.\nPlanning. Once the abstract interface is prepared, a pre-trained LLM is queried to generate goal-\nspecific action sequence. This is achieved through carefully designed instructions and user queries,\nenabling the LLM to efficiently create and revise the plans. Fig. 4 illustrates the planning process.\nSee Appendix for the full description.\nInstruction specifies the guidelines that LLMs must follow when planning, including 1) Action\nInterface provides functional descriptions of the structured actions and their parameters; 2) Query\nIllustration clarifies the structure and meaning of user queries; 3) Response Format requires LLM to\nreturn responses in the format of {Explanation, Thought, Action List}, where \u201cExplanation\u201d requires\nLLMs to explain the reason for action failure, \u201cThought\u201d requires LLM to use natural language to\nplan before outputting action sequences as a chain-of-thought (CoT) mechanism [26], and \u201cAction\nList\u201d outputs a list of structured actions to be executed; 4) Interaction Guideline guides LLMs to\ncorrect failed actions based on the feedback message, thus enabling the LLM to revise the plan.\nUser Query provides the specific query to LLMs for a given goal, including 1) Goal represents the\nobjective by text as \u201cObtain Count Item, given Material and Tool. Extra info: Info\u201d according to\nEq. (1); 2) Feedback is the feedback information of the abstract interface; 3) Reference Plan provides\na common reference plan for the current goal retrieved from the text-base memory.\nText-based Memory is designed for LLM to maintain common reference plans for each encountered\nobjective as experiential knowledge. LLMs acquire the experience about controlling agents and\nresolving specific situations through game play and agent interaction. Instead of starting from scratch\nevery time, using prior experience allows LLMs to handle tasks more efficiently, a process similar to\nhuman skill improvement through practice.\nTo this end, we design a text-based memory mechanism for LLM to store and retrieve gained knowl-\nedge. Unlike the RL-based model, which stores knowledge in parameters, this textual knowledge is\nexplicit, logical, and closely aligned with human thought processes. This allows for direct application\nto a wide range of similar tasks, leading to more efficient learning and improved generalization.\nSpecifically, during each game episode, once the goal is achieved, the entirely executed action list\nwould be stored in memory. The LLM may achieve the same goal under various circumstances,\nresulting in a range of different plans. To identify a common reference plan suitable for general\nsituations, essential actions from multiple plans are summarized. This summarization process is\n6\nacacia_boat\nacacia_door\nacacia_fence\nacacia_fence_gate\nacacia_stairs\nbeef\nbirch_boat\nbirch_door\nbirch_fence\nbirch_fence_gate\nbirch_stairs\nboat\nbone\nbowl\nchest\nchicken\ncobblestone\ncobblestone_wall\ncooked_beef\ncooked_chicken\ncooked_mutton\ncooked_porkchop\ncrafting_table\ndark_oak_boat\ndark_oak_door\ndark_oak_fence\ndark_oak_fence_gate\ndark_oak_stairs\ndirt\ndouble_plant\nfence\nfence_gate\nfurnace\nglass\nglass_bottle\nglass_pane\nladder\nlever\nlog\nmutton\noak_stairs\npaper\nplanks\nporkchop\nred_flower\nreeds\nsand\nsandstone\nsapling\nsign\nspruce_boat\nspruce_door\nspruce_fence\nspruce_fence_gate\nspruce_stairs\nstick\nstone\nstone_axe\nstone_brick_stairs\nstone_button\nstone_hoe\nstone_pickaxe\nstone_pressure_plate\nstone_shovel\nstone_stairs\nstone_sword\nstonebrick\nsugar\ntallgrass\ntrapdoor\nwheat\nwheat_seeds\nwooden_axe\nwooden_button\nwooden_door\nwooden_hoe\nwooden_pickaxe\nwooden_pressure_plate\nwooden_shovel\nwooden_slab\nwooden_sword\nyellow_flower\nbone_meal\narmor_stand\nbook\nbread\ncoal\niron_ingot\niron_nugget\niron_ore\niron_shovel\nitem_frame\nleather\nrotten_flesh\nshield\nspider_eye\nstone_slab\ntorch\ntrapped_chest\ntripwire_hook\nfireworks\ngunpowder\nsnow\nsnow_layer\nsnowball\ncarpet\ngrass\nheavy_weighted_pressure_plate\niron_hoe\niron_sword\nleather_boots\nleaves\npainting\nshears\nstring\nwool\ncoal_block\nleather_helmet\nbrown_mushroom\nbrown_mushroom_block\nflint\nbed\nbucket\nhay_block\niron_axe\niron_pickaxe\nmilk_bucket\nsandstone_stairs\nwater_bucket\nfermented_spider_eye\nleather_leggings\ngravel\nflint_and_steel\nfishing_rod\niron_boots\niron_trapdoor\nleather_chestplate\nmossy_cobblestone\nvine\nwaterlily\nbone_block\nbow\nchest_minecart\nfurnace_minecart\nhopper\niron_helmet\nminecart\ndiamond\ndiamond_shovel\ndropper\njukebox\nnoteblock\nredstone\nredstone_torch\nbanner\nfeather\niron_bars\niron_door\nrail\nbrick\nclay_ball\nlapis_lazuli\npiston\nemerald\ncauldron\niron_leggings\ntnt\ndiamond_hoe\ndiamond_sword\nflower_pot\niron_chestplate\narrow\ncompass\nhopper_minecart\ngold_ingot\ngold_nugget\ngold_ore\ngolden_shovel\niron_block\nbrick_block\nclay\nhardened_clay\nslime_ball\ndispenser\ndiamond_axe\ndiamond_pickaxe\nlava_bucket\nactivator_rail\ndetector_rail\negg\nrepeater\ntnt_minecart\nbookshelf\ngolden_hoe\ngolden_sword\nlight_weighted_pressure_plate\nredstone_block\nred_mushroom\nred_mushroom_block\nbeetroot\nbeetroot_seeds\ndiamond_boots\ndiamond_helmet\ngolden_axe\ngolden_pickaxe\nsticky_piston\nink_sac\ndiamond_leggings\ngolden_boots\nmushroom_stew\nmap\nbeetroot_soup\nlead\ngolden_helmet\nrabbit_hide\ncooked_rabbit\nrabbit\nbrick_stairs\ncake\nobsidian\ncactus\ndiamond_chestplate\nclock\ndeadbush\nwritable_book\nlapis_block\ngolden_leggings\ngolden_rail\nbaked_potato\npotato\ndiamond_block\ngolden_chestplate\nemerald_block\ncarrot\npumpkin\npumpkin_seeds\njungle_boat\njungle_door\njungle_fence\njungle_fence_gate\njungle_stairs\nlit_pumpkin\ncarrot_on_a_stick\nmelon\nmelon_block\nmelon_seeds\ngolden_carrot\ngold_block\npumpkin_pie\nred_sandstone\nstone_slab2\nred_sandstone_stairs\nspeckled_melon\nenchanting_table\napple\nanvil\nenchanted_book\npoisonous_potato\nrabbit_foot\nslime\ngolden_apple\nrabbit_stew\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nOurs\nDEPS\nDreamerV3\nVPT\nFigure 5: Success rate for all items in the entire Minecraft Overworld Technology Tree. The x\naxis lists all item names. We overlay the results from our GITM and the best results from baselines.\nalso implemented using LLMs (see Appendix for details). When encountering similar goals, the\nLLM creates new plans based on the summarized reference plans retrieved from memory. Successful\naction sequences from these new plans are also added to memory for future summarization. As the\nLLM-based Planner accumulates summaries, it becomes increasingly effective.\n3.3\nLLM Interface\nUnlike the existing RL-based agents that directly control keyboard and mouse, LLM-based agents\ninteract with the environment through structured actions and feedback messages. The LLM interface\nserves to implement structured actions as keyboard/mouse operations, and extract observations\nprovided by the environment into feedback messages.\nStructured actions can be implemented in various ways such as hand-written scripts or RL-learned\nmodels. While RL-learned models have been employed in Minecraft previously, they were either\nbroad in functionality but inefficient in practice, or too specific in functionality, limiting their\napplicability to general tasks and actions. Clarifying the capability boundary of RL-learned models is\nchallenging. Instead, in this work, we choose to implement structured actions using hand-written\nscripts. Since structured actions are well-defined and easy to implement, we can manually implement\nthem based on observations (e.g., location, LiDAR, and voxel) and basic operations (e.g., move,\njump, adjust camera angle, click left mouse button, and click right mouse button) provided by the\nMineDojo [5] environment. See Appendix for details.\nFeedback messages can be obtained directly from the environment. These include whether the\nstructured action execution succeeded or failed. If the execution fails, the reason for the failure is\nadditionally notified. It also includes the current state of the agent in the environment, including the\nitems in the inventory, the current biome and depth, etc. See Appendix for details.\n4\nExperiments\nTask Definition and Metrics. We measure the ability of GITM through item collection tasks. We\nonly collect items could be found in the Overworld. We exclude items could only be obtained by\ntrading with villagers, opening treasure chest or find a special structure on the map, using a tool\nenchanted with Silk Touch. This give us a total of 262 tasks. For the assessment of our agent, we\nemploy \u201cCoverage of the Overworld Technology Tree\u201d and\u201cSuccess Rate\u201d as evaluation metrics.\n7\n4.1\nMain Result\nUnlocking the Entire Technology Tree by Obtaining All Items. Compared with existing Minecraft\nagents [2, 7, 25] which mostly focuses on solving the ObtainDiamond task and could only unlock\na limited part of the full technology tree (13/262 for Dreamerv3, 15/262 VPT, 69/262 for DEPS),\nour approach could collect all 262 items as shown in Fig. 1. There are two major blockers for\nexisting methods. For RL-based methods like VPT [2] and DreamerV3 [7], the goal item(diamond)\nis hard-coded into the model weights, which means there are no easy way to re-task the trained\nRL agents for collecting other items in the inference stage. Moreover, the low training efficiency\nhinders them from solving extremely long-horizon tasks (e.g., obtaining a \u201cenchanted_book\u201d). For\nmethods like DEPS [25] that use an RL controller [3] and LLM planner still rely on pre-trained RL\nagents to execute specific subtasks (e.g. mining 1 \u201ccobblestone\u201d) in the generated plan. So these\napproaches still suffer from the inability of RL-based methods alone to generalize to unseen tasks\n(e.g. obtaining \u201clapis_lazuli\u201d). In contrast, we extract a well-defined set of structured actions by\nusing LLMs to decompose over 3000 predefined MineDojo tasks. This provides broad, open-world\nMinecraft capability. Combined with LLM planning, it enables solving more complex tasks than\nObtainDiamond - which RL cannot achieve. Our knowledge bases also improve efficiency. To our\nknowledge, we present the first agent to unlock the entire Overworld technology tree - a level of\nopen-world skill RL-based methods have not demonstrated.\nSuccess Rate for the Entire Technology Tree. We show the success rate of our method for collecting\nall Overworld items in Fig. 5. Our methods could achieve 100% success rate for simple tasks like\ncollecting wooden tools. It achieves non-zero success rates for all items which indicates a strong\ncollecting capability. The successful rate for collecting different items change smoothly for our agent,\nwhich showcase the robustness of our method against the highly uncertain open world environment.\n4.2\nComparison with Other Minecraft Agents\nTable 2: Comparison of our GITM with pre-\nvious methods on ObtainDiamond challenge.\nMethod\nSuccess Rate (%)\nDreamerV3\n-\n50.0\n3.0\n0.01\n0.01\nDEPS\n90.0\n80.0\n73.3\n10.0\n0.6\nVPT\n100.0 100.0 100.0\n85.0\n20.0\nOur GITM\n100.0 100.0 100.0\n95.0\n67.5\n101\n103\n105\n107\n109\nStep\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nVPT\nDreamV3\nOurs\nFigure 6: Comparison of learning efficiency.\nWe compared our LLM-based method with three existing agents: VPT [2], DreamerV3 [7], and\nDEPS [25] on the well known ObtainDiamond challenge, i.e, obtaining a diamond from scratch in\nMinecraft. Previous methods set different time limits of a single episode of game play (20 minutes\nfor VPT, 30 minutes for Dreamerv3, and 10 minutes for DEPS). For fair comparison, we use the\nstrictest limit of previous methods: 10 minutes (12,000 steps at 20Hz control).\nSuccess Rate for Obtaining Diamond and Other Items.\nSince VPT and Dreamerv3 are not\ntargeted for collecting items other than diamond, we mainly compare our method with DEPS for\nitems not related to obtain diamonds. Overall, our GITM and VPT rank task difficulty similarly, but\nDEPS rankings severely fluctuate for tasks more complex than mining coal. Dreamerv3 also behaves\noddly by having an abnormally low success rate on tasks like obtaining a stone sword. As shown in\nFig. 2, most agents performs generally well for easy tasks relating to make wooden tools. VPT could\neven rival with our GITM for the success rate of obtaining iron axes. But for obtaining diamonds, our\nmethod wins over any other methods by 3.5 times on the succeess rate.\nThis giant improvement comes from the following two aspects: First, we employ the strong long-term\nplanning capability of LLMs to decompose the complex tasks into feasible sub-goals and tackle\nthem step by step. Second, our model can directly leverage external knowledge such as the suitable\nlocations for mining ores, while RL models need to explore themselves and may not acquire reliable\nknowledge.\n8\nTable 3: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). \u201cGoal Decomp.\u201d and\n\u201cExternal Info.\u201d indicates goal decomposition and external knowledge respectively.\nGoal\nDecomp.\nFeedback\nExternal\nInfo.\nMemory\nSuccess Rate (%)\n57.5\n32.5\n5.0\n0.0\n0.0\n\u2713\n90.0\n90.0\n67.5\n2.5\n0.0\n\u2713\n\u2713\n97.5\n95.0\n77.5\n20.0\n5.0\n\u2713\n\u2713\n\u2713\n100.0\n100.0\n100.0\n57.5\n35.0\n\u2713\n\u2713\n\u2713\n\u2713\n100.0\n100.0\n100.0\n95.0\n67.5\nLearning Efficiency.\nBesides measuring the success rate of each agents, we also compare the\nlearning efficiency of our model with other learnable models. Since DEPS uses a LLM-based planner\nwithout learning mechanism and a pre-trained RL-based controller, its performance could not improve\nwith more episodes played and is excluded from the comparison here.\nIt usually takes tens of millions of steps to train an RL agent by updating parameters before its success\nrate starts to converges to meaningful non-zero numbers. However, the success rate for RL-based\nagents increases rather slowly even after them starts to converge. On the contrary, the learning process\nof our LLM-based agent is considerably faster. As shown in Fig. 6, our method requires several orders\nless episodes than any other methods before doubling its initial success rate. Moreover, our method is\nextremely sample efficient as our success rate raises from 35% to 47.5% by learning from the first five\nthousand steps. By just playing each task several times and summarize successful experience into the\nmemory, the LLM-based agent can acquire explicit experiential knowledge and achieve significantly\nhigher success rate.\n4.3\nAblation Study\nWe conduct ablation experiments on the ObtainDiamond task. We set a time limit of 10 minutes of\ngame play (12000 steps at the control frequency of 20Hz). When leveraging goal decomposition, for\neach sub-goal, we set the maximum number of queries to LLM as 30, and exceeding the query limit\nwill be judged as a failure. For each setting, we run 40 games and calculate the success rate. Tab. 3\nrecords the success rates of achieving the final goal diamond as well as the milestones in this goal,\nincluding crafting table, wooden pickaxe, stone pickaxe, and iron pickaxe.\nGoal Decomposition. Without goal decomposition, the planner can only accomplish several short-\nterm tasks such as obtaining stone axes with rather low success rate of 5%, which indicates the\nnecessity of goal decomposition. Leveraging the powerful long-term planning capabilities of LLMs,\nthe goals are decomposed into sub-goals feasible and practical for the planner, so the success rate for\nobtaining stone axes advances from 5% to 67.5% by leveraging goal decomposition alone.\nFeedback Message. Feedback contains the agent\u2019s state and the execution result of the actions, which\nhelps the planner to understand and make another attempt to correct the mistakes in the previous and\ndeal with special cases. This enables the planner to accomplish a broader range of goals with higher\nsuccess rate. As shown in the 3rd row of Tab. 3, our agent gain the ability to collect diamond by\ncombining feedback with goal decomposition.\nExternal Knowledge Base. External knowledge contains general rules, crafting recipes, and common\ntricks in Minecraft, such as the recipes for crafting iron ingot and iron pickaxe, the suitable location to\nfind diamond ore, and the efficient way to get cobblestone. Providing the planner with this information\ngreatly boosts the success rate of obtaining iron pickaxe and diamond, and the success rate of mining\ndiamond increase by 7 times by learning from the knowledge base that diamonds are more likely to\nappear in specific levels.\nText-based Memory. Leveraging the reference plan recorded in the memory, the planner can handle\nthe task it has encountered more efficiently. The success rates of obtaining iron pickaxe and diamond\nare 95.0% and 67.5%, surpassing the model without memory by 37.5% and 32.5%, respectively.\n9\n5\nConclusion\nWe introduce the GITM framework, which utilizes Large Language Models (LLMs) for hierarchical\ndecomposition of goals. GITM introduces LLM Decomposer, LLM Planner and LLM Interface\nto gradually decompose goals into sub-goals, structured actions and keyboard/mouse operations.\nThis work makes significant progress towards the ObtainDiamond goal, outperforming all previous\nmethods by a significant margin (+47.5% success rate). This proves the potential inefficiency and\npoorly scalability of Reinforcement Learning (RL) in Minecraft, breaking the traditional reliance\non RL. Moreover, by obtaining all items in Minecraft Overworld, this research marks a critical step\ntoward Generally Capable Agents (GCAs) that match human performance in Minecraft.\nAcknowledgments\nThe work is partially supported by the National Natural Science Foundation of\nChina under grants No.U19B2044, No.61836011, No.62022048, and No.62276150. This work is also\npartially supported by the National Key R&D Program of China under grants NO.2022ZD0114900,\nand the Guoqiang Institute of Tsinghua University.\nReferences\n[1] A. Amiranashvili, N. Dorka, W. Burgard, V. Koltun, and T. Brox. Scaling imitation learning in minecraft.\narXiv preprint arXiv:2007.02701, 2020.\n[2] B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune.\nVideo pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639\u201324654, 2022.\n[3] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023.\n[4] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\nT. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[5] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anand-\nkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint\narXiv:2206.08853, 2022.\n[6] W. H. Guss, S. Milani, N. Topin, B. Houghton, S. Mohanty, A. Melnik, A. Harter, B. Buschmaas, B. Jaster,\nC. Berganski, et al. Towards robust and domain agnostic reinforcement learning competitions: Minerl\n2020. In NeurIPS 2020 Competition and Demonstration Track, pages 233\u2013252. PMLR, 2021.\n[7] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv\npreprint arXiv:2301.04104, 2023.\n[8] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning, pages\n9118\u20139147. PMLR, 2022.\n[9] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022.\n[10] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, et al.\nMinerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions\nand Demonstrations Track, pages 13\u201328, 2022.\n[11] I. Kanitscheider, J. Huizinga, D. Farhi, W. H. Guss, B. Houghton, R. Sampedro, P. Zhokhov, B. Baker,\nA. Ecoffet, J. Tang, et al. Multi-task curriculum learning in a complex, visual, hard-exploration domain:\nMinecraft. arXiv preprint arXiv:2106.14876, 2021.\n[12] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li. Api-bank: A benchmark for tool-augmented llms.\narXiv preprint arXiv:2304.08244, 2023.\n[13] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\n[14] Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-efficient\nhierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\n10\n[15] H. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-efficient\nhierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International\nConference, DAI 2021, Shanghai, China, December 17\u201318, 2021, Proceedings 3, pages 38\u201351. Springer,\n2022.\n[16] Y. Matsuo, Y. LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama, E. Uchibe, and J. Morimoto. Deep\nlearning, reinforcement learning, and world models. Neural Networks, 2022.\n[17] S. Milani, N. Topin, B. Houghton, W. H. Guss, S. P. Mohanty, K. Nakata, O. Vinyals, and N. S. Kuno.\nRetrospective analysis of the 2019 minerl competition on sample efficient reinforcement learning. In\nNeurIPS 2019 Competition and Demonstration Track, pages 203\u2013214. PMLR, 2020.\n[18] S. Milani, A. Kanervisto, K. Ramanauskas, S. Schulhoff, B. Houghton, S. Mohanty, B. Galbraith, K. Chen,\nY. Song, T. Zhou, et al. Towards solving fuzzy tasks with human feedback: A retrospective of the minerl\nbasalt 2022 competition. arXiv preprint arXiv:2303.13512, 2023.\n[19] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom.\nToolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[20] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[21] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models.\narXiv preprint\narXiv:2209.11302, 2022.\n[22] A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov. Hierarchical deep\nq-network from imperfect demonstrations in minecraft. Cognitive Systems Research, 65:74\u201378, 2021.\n[23] O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz,\nM. Jaderberg, M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint\narXiv:2107.12808, 2021.\n[24] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, and S. Mannor. A deep hierarchical approach to lifelong\nlearning in minecraft. In Proceedings of the AAAI conference on artificial intelligence, 2017.\n[25] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\n[26] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[27] Wikipedia contributors. Minecraft \u2014 Wikipedia, the free encyclopedia. https://en.wikipedia.org/\nw/index.php?title=Minecraft&oldid=1155148900, 2023.\n[28] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nA\nImplementation Details\nA.1\nLLM Decomposer\nWe use gpt-3.5-turbo from OpenAI API 4 for goal decomposition. The prompt is shown as\nfollows, which consists of two parts: instruction with the role of \u201cSYSTEM\u201d and query with\nthe role of \u201cUSER\u201d. The {object quantity}, {object name} and {related knowledge} are\ninjectable slots that will be replace with corresponding texts before fed into the LLM.\n4https://platform.openai.com/docs/api-reference\n11\nSYSTEM:\nYou are an assistant for the game Minecraft.\nI will give you some target object and some knowledge related to the object. Please write the\nobtaining of the object as a goal in the standard form.\nThe standard form of the goal is as follows:\n{\n\"object\": \"the name of the target object\",\n\"count\": \"the target quantity\",\n\"material\": \"the materials required for this goal, a dictionary in the form {material_name:\nmaterial_quantity}. If no material is required, set it to None\",\n\"tool\": \"the tool used for this goal. If multiple tools can be used for this goal, only write\nthe most basic one. If no tool is required, set it to None\",\n\"info\": \"the knowledge related to this goal\"\n}\nThe information I will give you:\nTarget object: the name and the quantity of the target object\nKnowledge: some knowledge related to the object.\nRequirements:\n1. You must generate the goal based on the provided knowledge instead of purely depending\non your own knowledge.\n2. The \"info\" should be as compact as possible, at most 3 sentences. The knowledge I give you\nmay be raw texts from Wiki documents. Please extract and summarize important information\ninstead of directly copying all the texts.\nGoal Example:\n{\n\"object\": \"iron_ore\",\n\"count\": 1,\n\"material\": None,\n\"tool\": \"stone_pickaxe\",\n\"info\": \"iron ore is obtained by mining iron ore. iron ore is most found in level 53. iron ore\ncan only be mined with a stone pickaxe or better; using a wooden or gold pickaxe will yield\nnothing.\"\n}\n{\n\"object\": \"wooden_pickaxe\",\n\"count\": 1,\n\"material\": {\"planks\": 3, \"stick\": 2},\n\"tool\": \"crafting_table\",\n\"info\": \"wooden pickaxe can be crafted with 3 planks and 2 stick as the material and\ncrafting table as the tool.\"\n}\nUSER:\nTarget object: {object quantity} {object name}\nKnowledge: {related knowledge}\nThe recursive decomposition generates a sub-goal tree starting from the final goal object as the root\nnode: if a goal has some prerequisites (materials or tools), for each required material or tool, we add a\nchild node representing the goal of obtaining that material or tool, and then recursively decompose the\nchild node, until there is no more prerequisites. The related knowledge is from: 1) Crafting/smelting\nrecipes in MineDojo [5], written in the form \u201cCrafting {quantity} {object} requires {material}\nas the material and {tool} as the tool\u201d; 2) Wiki on the Internet 5. We extract the paragraphs with\nkeywords \u201cobtaining\u201d, \u201cmining\u201d, \u201csources\u201d, etc.\n5https://minecraft-archive.fandom.com/wiki/Minecraft_Wiki\n12\nA.2\nLLM Interface\nInstruction for Extracting Structured Actions. To extract structured actions, we first ask LLM\nto generate a tree-structured action planning for each of the 3141 predefined tasks provided by\nMineDojo, and then converts each action step into a (verb, object, tool, material) tuple.\nDuring decomposition, it is essential to ensure actions are neither too broad nor too specific. We\nadjusted the depth of the action decomposition tree to achieve balance, and empirically set the depth\nas 2 to meet our requirements.\nSpecifically, we use gpt-3.5-turbo from OpenAI API to generate the structured actions. We add\nthe following instruction to the content of \u201cSYSTEM\u201d role to generate the tree-structured plan. We\nadd the goal description, e.g., \"find material and craft a iron pickaxe\", to the content of \u201cUSER\u201d role\nand then asks LLM to response according to the requirements.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you my goal in the game, please break it down as a tree-structure plan to achieve\nthis goal.\nThe requirements of the tree-structure plan are:\n1. The plan tree should be exactly of depth 2.\n2. Describe each step in one line.\n3. You should index the two levels like \u20191.\u2019, \u20191.1.\u2019, \u20191.2.\u2019, \u20192.\u2019, \u20192.1.\u2019, etc.\n4. The sub-goals at the bottom level should be basic actions so that I can easily execute them\nin the game.\nUSER:\nThe goal is to {goal description}. Generate the plan according to the requirements.\nAfter that, we extract the action tuple from each sentence of the leaf nodes. We use the following\ninstruction as the content of \u201cSYSTEM\u201d role to extract the tuple, and add the sentence to the content\nof \u201cUSER\u201d role.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you a sentence. Please convert this sentence into one or several actions according\nto the following instructions.\nEach action should be a tuple of four items, written in the form (\u2019verb\u2019, \u2019object\u2019, \u2019tools\u2019,\n\u2019materials\u2019)\n\u2019verb\u2019 is the verb of this action.\n\u2019object\u2019 refers to the target object of the action.\n\u2019tools\u2019 specifies the tools required for the action.\n\u2019material\u2019 specifies the materials required for the action.\nIf some of the items are not required, set them to be \u2019None\u2019.\nUSER:\nThe sentence is {sentence}. Generate the action tuple according to the requirements.\nThen, we extract the structured actions by selecting frequent actions and merging actions with similar\nfunctionalities. The set of structured actions is {equip, explore, approach, mine/attack,\ndig_down, go_up, build, craft/smelt, apply}. Note that we disregard more detailed ac-\ntion decomposition for attack and build to remove overly detailed short-term actions and focus on\nlong-term task completion.\nAction Implementation. The observation of the action contains LiDAR rays with an interval of 5\ndegrees in the horizon and vertical direction for locating objects, and voxels with 10 units radius only\nfor navigation, inventory, life status, and agent location status (X-ray cheating is carefully avoided).\n13\nRGB is not used in our implementation, although it provides more information than LiDAR rays.\nFor example, the biome, and category of the dropping item can not be identified by LiDAR rays.\nSome objects may also be missed by LiDAR due to sparseness of LiDAR rays. We also set the\nbreaking speed to 100 and strength to 100, mainly following [7]. The detailed implementation of\neach structured action is as follows:\n\u2022 equip: The equip action calls the environment API to equip the required object. The action\nsucceeds when the API returns success. The action fails when the object is not in inventory or the\nequip API returns failure.\n\u2022 explore: The explore action traverses the world until object is visible. This action regards the\nworld as a chessboard, and each node on the chessboard is the center point of a 20\u00d720 units area.\nTwo strategies are implemented depending on whether the agent is on the ground or not. When\nthe agent is on the ground, the BFS explore will be adopted. When the agent is under the ground,\nmainly for exploring ore, the DFS explore will be adopted. In the DFS exploration, the agent will\nbreak the blocks to form a mine road with width of 1 and height of 2. The action succeeds when\nthe object is visible. The action fails when the explore exceeds a preset steps of 10,000 but the\nrequired object is not found.\n\u2022 approach: The approach action finds the nearest visible required object and walks towards the\nobject. We adopt A\u2217 algorithm for finding path. The A\u2217 algorithm can jump, translate and fall\nin four directions of north, south, east and west. We also allow the agent to jump while placing a\nblock under the agent for ascent. If the object is out of the voxel observation range, A\u2217 algorithm is\niteratively applied to find the location nearest to the object. The action succeeds when the \u2113\u221e norm\ndistance between the object and agent is less than 2. The action fails when there is no required\nobject visible or no path can be found to walk close to the object.\n\u2022 mine/attack: The mine/attack action uses the keyboard attack API with the tools to attack the\nobject. Only visible object could be mined or attacked. The object of mine should be blocks, and\nthe agent will continue mining the block until it is broken. The object of attack should be entities,\nand the agent will iteratively approach and attack the entity until it is killed. After the block is\nbroken or the entity is killed, if there are items dropped by them, the agent will approach the items\nto collect them. The action succeeds when the block is broken or the entity is killed. The action\nfails when there is no visible object, no required tools is in inventory, or the visible object is out of\nattack range.\n\u2022 dig_down: The dig_down action iteratively breaks the block underfoot with the tool until the\nrequired ylevel is reached. If the agent is on the ground, before digging down, current location is\nstored for going up action. After the action succeeds, the state of the agent is set to under ground.\nThe action succeeds when the required ylevel is reached. The action fails when it exceeds the reset\nmax steps 10,000 or no required tool is in inventory.\n\u2022 go_up: The agent will first go back to the location stored by dig_down. Then, the go_up action\nputs dirt blocks underfoot to raise the agent. After the action is finished, the state of agent is set to\non the ground. The action succeeds when the pre-stored location is reached. The action fails when\nthe walk fails, exceeds the reset max steps 10,000 or there is no required tool in inventory.\n\u2022 build: The build action places the required blocks according to a given blueprint from bottom\nto up. The action succeeds when all blocks have been placed. The action fails when there are no\nenough materials in inventory or it is invalid to place some blocks.\n\u2022 craft/smelt: The action calls the environment API to craft/smelt the required object. The action\nsucceeds when the required object is obtained. The actions fails when there are no enough materials\nin inventory or the agent is unable to place the crafting table/furnace or the API fails.\n\u2022 apply: The apply action calls the keyboard use API, and applies the specific tool to the object, e.g.,\napplying the bucket on water to obtain water bucket. The action succeeds when the API returns\nsuccess. The action fails when there is no visible object, no tool in inventory or the API fails.\nFeedback Message. After the execution of each action, we will get feedback from the structured\nactions. The feedback will refresh the agent\u2019s state in Sec. A.3.2, including current inventory, biome,\nylevel and on/under the ground status. The feedback will also contain the success/fail message from\nthese action, as well as the inventory change during the action.\n14\nA.3\nLLM Planner\nHere we present the prompt for planning with LLM. We also use gpt-3.5-turbo from OpenAI API\nas the LLM planner. The model accepts inputs in form of a chat, i.e., the prompt is a dialogue consist-\ning of several messages, each of which contains a role and the content. We set the Instruction\nwith the role \u201cSYSTEM\u201d at the beginning, and use the User Query with the role \u201cUSER\u201d to query\nthe LLM for response. The content of the Instruction and User Query are as follows.\nA.3.1\nInstruction\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI will give you a goal in the game. Please think of a plan to achieve the goal, and then write a\nsequence of actions to realize the plan. The requirements and instructions are as follows:\n1. You can only use the following functions. Don\u2019t make plans purely based on your\nexperience, think about how to use these functions.\nexplore(object, strategy)\nMove around to find the object with the strategy: used to find objects including block items\nand entities. This action is finished once the object is visible (maybe at the distance).\nAugments:\n- object: a string, the object to explore.\n- strategy: a string, the strategy for exploration.\napproach(object)\nMove close to a visible object: used to approach the object you want to attack or mine. It may\nfail if the target object is not accessible.\nAugments:\n- object: a string, the object to approach.\ncraft(object, materials, tool)\nCraft the object with the materials and tool: used for crafting new object that is not in the\ninventory or is not enough. The required materials must be in the inventory and will be\nconsumed, and the newly crafted objects will be added to the inventory. The tools like the\ncrafting table and furnace should be in the inventory and this action will directly use them.\nDon\u2019t try to place or approach the crafting table or furnace, you will get failed since this\naction does not support using tools placed on the ground. You don\u2019t need to collect the items\nafter crafting. If the quantity you require is more than a unit, this action will craft the objects\none unit by one unit. If the materials run out halfway through, this action will stop, and you\nwill only get part of the objects you want that have been crafted.\nAugments:\n- object: a dict, whose key is the name of the object and value is the object quantity.\n- materials: a dict, whose keys are the names of the materials and values are the quantities.\n- tool: a string, the tool used for crafting. Set to null if no tool is required.\nmine(object, tool)\nMine the object with the tool: can only mine the object within reach, cannot mine object from\na distance. If there are enough objects within reach, this action will mine as many as you\nspecify. The obtained objects will be added to the inventory.\nAugments:\n- object: a string, the object to mine.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\nattack(object, tool)\nAttack the object with the tool: used to attack the object within reach. This action will keep\ntrack of and attack the object until it is killed.\nAugments:\n- object: a string, the object to attack.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\n15\nequip(object)\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and\narmor. The object must be in the inventory and belong to the items for equipping.\nAugments:\n- object: a string, the object to equip.\ndigdown(object, tool)\nDig down to the y-level with the tool: the only action you can take if you want to go\nunderground for mining some ore.\nAugments:\n- object: an int, the y-level (absolute y coordinate) to dig to.\n- tool: a string, the tool used for digging. Set to null if no tool is required.\ngo_back_to_ground(tool)\nGo back to the ground from underground: the only action you can take for going back to the\nground if you are underground.\nAugments:\n- tool: a string, the tool used for digging. Set to null if no tool is required.\napply(object, tool)\nApply the tool on the object: used for fetching water, milk, lava with the tool bucket, pooling\nwater or lava to the object with the tool water bucket or lava bucket, shearing sheep with the\ntool shears, blocking attacks with the tool shield.\nAugments:\n- object: a string, the object to apply to.\n- tool: a string, the tool used to apply.\n2. You cannot define any new function. Note that the \"Generated structures\" world creation\noption is turned off.\n3. There is an inventory that stores all the objects I have. It is not an entity, but objects can be\nadded to it or retrieved from it anytime at anywhere without specific actions. The mined or\ncrafted objects will be added to this inventory, and the materials and tools to use are also from\nthis inventory. Objects in the inventory can be directly used. Don\u2019t write the code to obtain\nthem. If you plan to use some object not in the inventory, you should first plan to obtain it.\nYou can view the inventory as one of my states, and it is written in form of a dictionary whose\nkeys are the name of the objects I have and the values are their quantities.\n4. You will get the following information about my current state:\n- inventory: a dict representing the inventory mentioned above, whose keys are the name of\nthe objects and the values are their quantities\n- environment: a string including my surrounding biome, the y-level of my current location,\nand whether I am on the ground or underground\nPay attention to this information. Choose the easiest way to achieve the goal conditioned on\nmy current state. Do not provide options, always make the final decision.\n5. You must describe your thoughts on the plan in natural language at the beginning. After\nthat, you should write all the actions together. The response should follow the format:\n{\n\"explanation\": \"explain why the last action failed, set to null for the first planning\",\n\"thoughts\": \"Your thoughts on the plan in natural languag\",\n\"action_list\": [\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\n}\nThe action_list can contain arbitrary number of actions. The args of each action should\n16\ncorrespond to the type mentioned in the Arguments part. Remember to add \u201c\u2018dict\u201c\u2018 at the\nbeginning and the end of the dict. Ensure that you response can be parsed by Python json.loads\n6. I will execute your code step by step and give you feedback. If some action fails, I will\nstop at that action and will not execute its following actions. The feedback will include error\nmessages about the failed action. At that time, you should replan and write the new code just\nstarting from that failed action.\nA.3.2\nUser Query\nUSER:\nMy current state:\n- inventory: {inventory}\n- environment: {environment}\nThe goal is to {goal}.\nHere is one plan to achieve similar goal for reference: {reference plan}.\nBegin your plan. Remember to follow the response format.\nor Action {successful action} succeeded, and {feedback message}. Continue your\nplan. Do not repeat successful action. Remember to follow the response format.\nor Action {failed action} failed, because {feedback message}. Revise your plan from\nthe failed action. Remember to follow the response format.\nA.4\nMemory\nA.4.1\nLearning Process\nWe maintain the text-based memory with a dictionary, whose keys are sub-goals and values are lists\nof successful action sequences for the corresponding sub-goals. The construction and update of the\nmemory are through the following learning process:\n\u2022 When encountering a new sub-goal that is not in the memory, the LLM planner creates plans\nwithout reference. Once the sub-goal is achieved, the entirely executed action sequence would be\nstored into the memory.\n\u2022 When encountering a sub-goal with memory, the first action sequence in the recording list for this\ngoal is retrieved as the reference plan, with which the LLM planner tries to achieve the goal. If it\nsucceeds, the new executed action sequence will be added to the last of the recording list.\n\u2022 For each sub-goal, once the number of action sequences recorded in its list reaches N, we pop\nall the N sequences and use LLM to summarize them into a common plan solution suitable for\nvarious scenarios, which is then put first in the list. N is set to 5 in all our experiments.\nTo learn the memory for obtaining all items, starting from scratch each time would take a long time.\nIn addition, it is necessary to avoid spending the most of time on learning simple tasks and not\ninvesting enough in learning difficult tasks. To improve the learning efficiency, we suggest to study\nthe sub-goals individually one by one. We first use our LLM Decomposer to generate sub-goal trees\nfor all items, acquiring the set of all sub-goals involved. Then for each sub-goal, the LLM planner\nplays multiple times given its prerequisites including the required materials and tools. The learning\nprocess of the sub-goal is finished once we obtain N = 5 successful action sequences and summarize\nthem into one common plan solution for reference.\nA.4.2\nImplementation of Memory Summarization\nWe also use gpt-3.5-turbo from OpenAI API for memory summarization but in a different\ndialogue. We use the following prompt to instruct the summarization with the role \u201cSYSTEM\u201d. The\nslot {action description} is replaced with the same descriptions of interfaces of the structured\nactions as Sec. A.3.1. We list all the action sequences to be summarized in the query with the role\n\u201cUSER\u201d, which is fed into the LLM for response.\n17\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI am using a set of actions to achieve goals in the game Minecraft. I have recorded several\naction sequences successfully achieving a goal in a certain state. I will give you the goal, the\nstate, and the sequences later. Please summarize the multiple action sequences into a single\naction sequence as a universal reference to achieve the goal given that certain state. Here are\nthe instructions:\n1. Each action sequence is a sequence of the following actions:\n{action description}\n2. The action sequences before and after summarization are always conditioned on the given\nstate, i.e., the actions are taken in that certain state to achieve the goal. I will describe the state\nin the following form: State: - inventory: a dict whose keys are the name of the objects and\nthe values are their quantities. This inventory stores all the objects I have. - environment: a\ndict including my surrounding biome and whether I am on the ground or underground.\n3. The action sequence you summarize should be able to achieve the goal in general cases\nwithout specific modification. Every necessary action should be included, even though it does\nnot appear in some sequences because I manually skipped it in some lucky cases. The actions\nredundant or irrelevant to the goal should be filtered out. The corner cases, such as success by\nluck and dealing with contingencies, should not be summarized into the final sequence.\n4. You should describe your thoughts on summarization in natural language at the beginning.\nAfter that, give me the summarized action sequence as a list in JSON format. Your response\nshould follow this form:\nThoughts: \"Your thoughts and descriptions of your summarization\"\nSummarized action sequence:\n[\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\nB\nResults of All Items\nWe provide the success rate of all items in the entire Minecraft Overworld Technology Tree in Tab. 4.\nWe have attached a video of obtaining a diamond in the supplementary materials.\nExperiment Setting. Considering the large number of items, including those difficult to be obtained,\nwe implemented an incremental testing strategy. This strategy is designed to keep the testing costs\nwithin a reasonable range, while also accounting for the rarity of certain items. We avoided a uniform\nincrease in the number of tests across all items to accommodate the hardest-to-obtain ones, which\nwould have resulted in prohibitive testing costs. Instead, we employed a incremental testing process.\nFor each item, we begin with 20 games. If the success count is less than or equal to 1, we increase\nto 50 games. If the success count remains less than or equal to 1, we further increase to 100, and\neventually 200 games. This testing continues until the success count finally exceeds 1, or we complete\n200 games. By following this efficient strategy, we ensure a cost-effective and reliable evaluation of\neach item, regardless of its availability. Moreover, because some items need long-term planning and\ncrafting chain, we do not set restrictions on the time limit or query limit.\nExploring Biome. Biomes can be a key factor that strongly influences the success rate. Some items,\nlike cactus, pumpkin, or melon, can only be found in specific biomes. The distribution of biomes\nhighly limits the success rate of some items.\n18\nTable 4: Success rate for all 262 items in the entire Minecraft Overworld Technology Tree.\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nacacia boat\n100.0 stone sword\n100.0 gravel\n80.0\nbeetroot seeds\n40.0\nacacia door\n100.0 stonebrick\n100.0 iron boots\n80.0\ndiamond boots\n40.0\nacacia fence\n100.0 sugar\n100.0 iron trapdoor\n80.0\ndiamond helmet\n40.0\nacacia fence gate\n100.0 tallgrass\n100.0 leather chestplate\n80.0\ngolden axe\n40.0\nacacia stairs\n100.0 trapdoor\n100.0 leather leggings\n80.0\ngolden pickaxe\n40.0\nbeef\n100.0 wheat\n100.0 bone block\n75.0\nred mushroom\n40.0\nbirch boat\n100.0 wheat seeds\n100.0 bow\n75.0\nred mushroom block\n40.0\nbirch door\n100.0 wooden axe\n100.0 chest minecart\n75.0\ndiamond leggings\n35.0\nbirch fence\n100.0 wooden button\n100.0 furnace minecart\n75.0\ngolden boots\n35.0\nbirch fence gate\n100.0 wooden door\n100.0 hopper\n75.0\nink sac\n35.0\nbirch stairs\n100.0 wooden hoe\n100.0 iron helmet\n75.0\nsticky piston\n35.0\nboat\n100.0 wooden pickaxe\n100.0 minecart\n75.0\nbeetroot soup\n30.0\nbone\n100.0 wooden pressure plate\n100.0 mossy cobblestone\n75.0\ngolden helmet\n30.0\nbone meal\n100.0 wooden shovel\n100.0 vine\n75.0\nlead\n30.0\nbowl\n100.0 wooden slab\n100.0 waterlily\n75.0\nmap\n30.0\nchest\n100.0 wooden sword\n100.0 banner\n70.0\nmushroom stew\n30.0\nchicken\n100.0 yellow flower\n100.0 brick\n70.0\nbrick stairs\n25.0\ncobblestone\n100.0 armor stand\n95.0\nclay ball\n70.0\ncactus\n25.0\ncobblestone wall\n100.0 book\n95.0\ndiamond\n70.0\ncake\n25.0\ncooked beef\n100.0 bread\n95.0\ndiamond shovel\n70.0\nclock\n25.0\ncooked chicken\n100.0 coal\n95.0\ndropper\n70.0\ncooked rabbit\n25.0\ncooked mutton\n100.0 fireworks\n95.0\nfeather\n70.0\ndiamond chestplate\n25.0\ncooked porkchop\n100.0 gunpowder\n95.0\niron bars\n70.0\nobsidian\n25.0\ncrafting table\n100.0 iron ingot\n95.0\niron door\n70.0\nrabbit\n25.0\ndark oak boat\n100.0 iron nugget\n95.0\njukebox\n70.0\nrabbit hide\n25.0\ndark oak door\n100.0 iron ore\n95.0\nlapis lazuli\n70.0\ndeadbush\n20.0\ndark oak fence\n100.0 iron shovel\n95.0\nnoteblock\n70.0\ngolden leggings\n20.0\ndark oak fence gate 100.0 item frame\n95.0\npiston\n70.0\ngolden rail\n20.0\ndark oak stairs\n100.0 leather\n95.0\nrail\n70.0\nlapis block\n20.0\ndirt\n100.0 rotten flesh\n95.0\nredstone\n70.0\nwritable book\n20.0\ndouble plant\n100.0 shield\n95.0\nredstone torch\n70.0\nbaked potato\n15.0\nfence\n100.0 spider eye\n95.0\ncauldron\n65.0\ncarrot\n15.0\nfence gate\n100.0 stone slab\n95.0\ndiamond hoe\n65.0\ndiamond block\n15.0\nfurnace\n100.0 torch\n95.0\ndiamond sword\n65.0\nemerald block\n15.0\nglass\n100.0 trapped chest\n95.0\nemerald\n65.0\ngolden chestplate\n15.0\nglass bottle\n100.0 tripwire hook\n95.0\niron leggings\n65.0\npotato\n15.0\nglass pane\n100.0 carpet\n90.0\ntnt\n65.0\npumpkin\n15.0\nladder\n100.0 coal block\n90.0\narrow\n60.0\npumpkin seeds\n15.0\nlever\n100.0 grass\n90.0\ncompass\n60.0\ncarrot on a stick\n10.0\nlog\n100.0 heavy weighted pressure plate\n90.0\nflower pot\n60.0\njungle boat\n10.0\nmutton\n100.0 iron hoe\n90.0\niron chestplate\n60.0\njungle door\n10.0\noak stairs\n100.0 iron sword\n90.0\nbrick block\n55.0\njungle fence\n10.0\npaper\n100.0 leather boots\n90.0\nclay\n55.0\njungle fence gate\n10.0\nplanks\n100.0 leather helmet\n90.0\ndispenser\n55.0\njungle stairs\n10.0\nporkchop\n100.0 leaves\n90.0\ngold ingot\n55.0\nlit pumpkin\n10.0\nred flower\n100.0 painting\n90.0\ngold nugget\n55.0\nmelon\n10.0\nreeds\n100.0 shears\n90.0\ngold ore\n55.0\nmelon block\n10.0\nsand\n100.0 snow\n90.0\ngolden shovel\n55.0\nmelon seeds\n10.0\nsandstone\n100.0 snow layer\n90.0\nhardened clay\n55.0\ngold block\n8.0\nsapling\n100.0 snowball\n90.0\nhopper minecart\n55.0\ngolden carrot\n8.0\nsign\n100.0 string\n90.0\niron block\n55.0\npumpkin pie\n8.0\nspruce boat\n100.0 wool\n90.0\nslime ball\n55.0\nred sandstone\n6.0\nspruce door\n100.0 bed\n85.0\nactivator rail\n50.0\nred sandstone stairs\n6.0\nspruce fence\n100.0 brown mushroom\n85.0\ndetector rail\n50.0\nspeckled melon\n6.0\nspruce fence gate\n100.0 brown mushroom block\n85.0\ndiamond axe\n50.0\nstone slab2\n6.0\nspruce stairs\n100.0 bucket\n85.0\ndiamond pickaxe\n50.0\nanvil\n4.0\nstick\n100.0 flint\n85.0\negg\n50.0\napple\n4.0\nstone\n100.0 hay block\n85.0\nlava bucket\n50.0\nenchanting table\n4.0\nstone axe\n100.0 iron axe\n85.0\nrepeater\n50.0\nenchanted book\n3.0\nstone brick stairs\n100.0 iron pickaxe\n85.0\ntnt minecart\n50.0\npoisonous potato\n2.0\nstone button\n100.0 milk bucket\n85.0\nbookshelf\n45.0\ngolden apple\n1.0\nstone hoe\n100.0 sandstone stairs\n85.0\ngolden hoe\n45.0\nrabbit foot\n1.0\nstone pickaxe\n100.0 water bucket\n85.0\ngolden sword\n45.0\nslime\n1.0\nstone pressure plate 100.0 fermented spider eye\n80.0\nlight weighted pressure plate\n45.0\nrabbit stew\n0.5\nstone shovel\n100.0 fishing rod\n80.0\nredstone block\n45.0\nstone stairs\n100.0 flint and steel\n80.0\nbeetroot\n40.0\nC\nSupplementary Ablations\nWe make a more detailed comparison between our GITM with RL-based methods in Tab. 5. The most\nstraightforward pipeline is to directly map the goal into keyboard/mouse operations. We gradually\nadd goal decomposition and structured action stages into the pipeline, and ablate the use of RL-based\nmodels or LLM.\n19\nTable 5: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). \u201cGoal Decomp.\u201d\nindicates whether to use LLM Decomposer to decompose the goal into sub-goals. \u201cGoal / Sub-Goal\nto Structured Actions / Keyboard & Mouse Mapping\u201d indicates which method is used for the mapping\nfrom goal / sub-goals to structured actions / keyboard & mouse operations.\nGoal\nDecomp.\nStructured\nAction\nGoal / Sub-Goal to\nStructured Actions / Keyboard & Mouse\nMapping\nSuccess Rate (%)\n(a)\nSpecialist RL Model (VPT)\n100.0\n100.0\n100.0\n85.0\n20.0\n(b)\nGoal-conditioned RL Model (DEPS)\n0.0\n0.0\n0.0\n0.0\n0.0\n(c)\nOur LLM Planner\n0.0\n0.0\n0.0\n0.0\n0.0\n(d)\n\u2713\nGoal-conditioned RL Model (DEPS)\n90.0\n80.0\n30.0\n0.0\n0.0\n(e)\n\u2713\nOur LLM Planner\n0.0\n0.0\n0.0\n0.0\n0.0\n(f)\n\u2713\nOur LLM Planner\n57.5\n32.5\n5.0\n0.0\n0.0\n(g)\n\u2713\n\u2713\nOur LLM Planner\n100.0\n100.0\n100.0\n95.0\n67.5\nImplementation Details. We can only find open-sourced RL models from VPT [2] and DEPS [25],\nso they are adopted for the ablation. VPT model is specifically trained for the ObtainDiamond\nchallenge, while DEPS model can use goal description as input to guide the model\u2019s output. We refer\nto them as specialist RL model and goal-conditioned RL model, respectively. As for the use of LLM\nPlanner, we note that if structured action is not used, LLM Planner will be inevitably asked to output\nreasonable keyboard/mouse operations. However, LLM Planner does not have access to environment\nobservations, so it cannot directly output reasonable keyboard/mouse operations.\nDirect Mapping. See Tab. 5(a)(b)(c). It is hard to directly mapping the long-horizon goal into\nreasonable keyboard/mouse operations. While a specialist RL model (i.e., VPT) can deliver promising\nresults, it requires large amount of data and computational resources to train such a model [2] (720\nV100 GPUs for 9 days). Moreover, a different goal will require further training of the specialist RL\nmodel, limiting the versatility of this paradigm. The goal conditional RL model (i.e., DEPS) cannot\nachieve the goal, because the model [25] we have access to is not generalizable to all scenarios.\nIf only the final goal is given, it will ignore preconditions, such as not crafting the necessary iron\npickaxe when mining diamonds. LLM also fails to accomplish the goal. The primary reason is that it\ncan not handle environment observation and keyboard/mouse operations well.\nStructured Action. We design structured actions to interact with the environment, and provide\nan abstract interface. Tab. 5(f) shows that adding structured action significantly improves LLM\u2019s\nperformance. This is because structured actions can deal with environment observations and key-\nboard/mouse operations more precisely, unleashing the reasoning potential of LLM. We are not aware\nof a RL model using structured actions currently. It is possible for structure actions to enhance the\nRL model as well, and we will explore it in the future work.\nGoal Decomposition. Decomposing the goal into sub-goals can simplify the whole task. Tab. 5(b)(d)\nand Tab. 5(f)(g) show its effectiveness for both goal-conditioned RL model and our method. By\nexploiting goal decomposition, it is possible for our method to accomplish long-term tasks with high\nsuccess rate.\nComparison between RL-based methods. We also note the paradigm shift from traditional RL-\nbased methods to our GITM leads to a great performance boost. Comparing Tab. 5(d)(g), where we\nonly change the goal-conditioned RL model to LLM with strutured actions, our method significantly\noutperforms the RL model.\nD\nObtainDiamond\nWe demonstrate a case of the popular ObtainDiamond challenge in Fig. 7. During the process, the\nagent have to collect materials, i.e., log, stone and iron ore, as shown in Fig 7(a)(c)(e). Necessary tools,\ni.e., wooden pickaxe, stone pickaxe, furnace and iron pickaxe are also crafted in Fig 7(b)(d)(f)(h).\n20\n(a) mine log\n(b) craft wooden_pickaxe\n(c) mine stone\n(d) craft stone_pickaxe\n(e) mine iron_ore\n(f) craft furnace\n(g) smelt iron_ingot\n(h) craft iron_pickaxe\n(i) mine diamond\nFigure 7: A case of the popular ObtainDiamond challenge. Figure(e)(i) are enhanced in brightness\nfor better display.\nFinally the diamond is obtained in Fig 7(i). We have attached a video of obtaining a diamond in the\nsupplementary materials.\nE\nApplications\n(a) Shelter with Farmland\n(b) Iron Golem\n(c) Redstone Circuit\n(d) Nether Portal\nFigure 8: Demonstration of the applications. GITM can construct Shelter with Farmland and\nIron Golem for survival, Redstone Circuit for automation equipment, and Nether Portal for\nthe Nether world exploration.\nOur proposed GITM makes survival and the nether exploration possible in Minecraft which has\nnever been accomplished by existing agents. To achieve this, our agent builds four necessary items,\nincluding Shelter with Farmland, Iron Golem, Redstone Circuit, and Nether Portal,\nshown in Fig. 8. Shelter with Farmland is firstly built to keep the agent from being attacked\nby monsters at night and provide enough food. Iron Golem can automatically attack monsters to\nprotect the agent and the shelter. Redstone Circuit is the foundation of all automation equipment.\nNether Portal is the entrance to the Nether world.\n21\n"
  },
  {
    "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising",
    "link": "https://arxiv.org/pdf/2305.18264.pdf",
    "upvote": "2",
    "text": "Gen-L-Video: Multi-Text to Long Video Generation\nvia Temporal Co-Denoising\nFu-Yun Wang4,1,2 Wenshuo Chen5 Guanglu Song3 Han-Jia Ye4 Yu Liu1\u2020 Hongsheng Li2\u2020\n1Shanghai AI Laboratory\n2Multimedia Laboratory, The Chinese University of Hong Kong\n3Sensetime Research\n4Nanjing University\n5Tsinghua University\n{wangfuyun@smail, yehj@lamda}.nju.edu.cn\ncws21@mails.tsinghua.edu.cn\nsongguanglu@sensetime.com\nliuyuisanai@gmail.com\nhsli@ee.cuhk.edu.hk\nAbstract\nLeveraging large-scale image-text datasets and advancements in diffusion models,\ntext-driven generative models have made remarkable strides in the field of image\ngeneration and editing. This study explores the potential of extending the text-\ndriven ability to the generation and editing of multi-text conditioned long videos.\nCurrent methodologies for video generation and editing, while innovative, are often\nconfined to extremely short videos (typically less than 24 frames) and are limited\nto a single text condition. These constraints significantly limit their applications\ngiven that real-world videos usually consist of multiple segments, each bearing\ndifferent semantic information. To address this challenge, we introduce a novel\nparadigm dubbed as Gen-L-Video capable of extending off-the-shelf short video\ndiffusion models for generating and editing videos comprising hundreds of frames\nwith diverse semantic segments without introducing additional training, all while\npreserving content consistency. We have implemented three mainstream text-driven\nvideo generation and editing methodologies and extended them to accommodate\nlonger videos imbued with a variety of semantic segments with our proposed\nparadigm. Our experimental outcomes reveal that our approach significantly\nbroadens the generative and editing capabilities of video diffusion models, offering\nnew possibilities for future research and applications. Code is available at: https:\n//github.com/G-U-N/Gen-L-Video.\n1\nIntroduction\nBenefitting from pre-training on large-scale text-image datasets [50] and the development and\nrefinement of the diffusion model [40, 46, 5, 52, 49], we have witnessed a plethora of successful\napplications, including impressive image generation, editing, and even fine-grained generation control\nthrough the injection of additional layout information [39, 71]. A logical progression of this approach\nis its extension to the video realm for text-driven video generation and editing [52, 43, 21, 63].\nCurrently, there are three primary strategies for text-driven video generation and editing:\n\u2022 Pretrained Text-to-Video (pretrained t2v) [57, 62, 22] involves training the diffusion\nmodel on a large-scale text-video paired dataset such as WebVid-10M [2]. Typically, a\ntemporal interaction module, like Temporal Attention [21], is added to the denoising model,\nfostering inter-frame information interaction to ensure frame consistency.\n\u2022 Tuning-free Text-to-Video (tuning-free t2v) [43, 29, 3] utilizes the pre-trained Text-to-\nImage model to generate and edit video frame-by-frame, while applying additional controls\nto maintain consistency across frames (for instance, copying and modifying the attention\nmap [16, 43, 29], sparse causal attention [63], etc.).\n\u2020Correspondence to: Hongsheng Li (hsli@ee.cuhk.edu.hk), Yu Liu (liuyuisanai@gmail.com)\nPreprint. Under review.\narXiv:2305.18264v1  [cs.CV]  29 May 2023\n\u2022 One-shot tuning Text-to-Video (one-shot-tuning t2v) [63, 41] proposes to fine-tune the\npretrained text-to-image generation model on a single video instance to generate videos with\nsimilar motions or contents. Despite the extra training cost, one-shot tuning-based methods\noften offer more editing flexibility compared to tuning-free based methods. As depicted in\nFig. 4, both attempt to substitute the rabbit in the source video with a tiger or a puppy. The\noutcome produced by the tuning-free t2v method reveals elongated ears, losing authenticity.\nOne-shot-tuning-based method, in contrast, effectively circumvents this problem.\nDespite significant advances made by previous methods, they are accompanied by some fatal limita-\ntions, restricting their practical applications. First of all, the number of video frames generated by\nthese methods is usually limited, generally less than 24 frames [43, 63, 29, 25, 32]. On one hand,\nthe computational complexity of temporal attention scales quadratically with the number of frames,\nmaking the direct generation of ultra-long videos infeasible. On the other hand, ensuring consis-\ntency becomes more challenging with the increase in the number of frames. Another noteworthy\nlimitation is that these methods typically generate videos controlled by a single text condition and\ncannot accommodate multiple text prompts. In reality, the content of a video often changes over\ntime, meaning that a comprehensive video often comprises multiple segments each bearing different\nsemantic information. This necessitates the development of video generation methods that can handle\nmultiple text conditions. Though there are already attempts at generating long videos, they typically\nrequire additional training on large-scale text-video datasets and follow the autoregressive mechanism\n(i.e.,the generation of later frames is conditioned on former ones), which suffers from severe content\ndegradation and inference inefficiency (see Sec. 2 for more discussion).\nIn light of these challenges, we propose a novel framework aimed at generating long videos with\nconsistent, coherent content across multiple semantic segments. Unlike previous methods, we do\nnot construct or train a long-video generator directly. Instead, we view a video as a collection of\nshort video clips, each possessing independent semantic information. Hence, a natural idea is that\ngeneration of long videos can be seen as the direct splicing of multiple short videos. However,\nthis simplistic division falls short of generating long videos with consistent content, resulting in\nnoticeable content and detail discrepancies between different video clips. As shown in the third row\nof Fig. 2, the color of the jeep car changes drastically among different video clips when they are\ndenoised isolatedly. To counter this, we perceive long videos as short video clips with temporal\noverlapping. We demonstrate that under certain conditions, the denoising path of a long video can be\napproximated by joint denoising of overlapping short videos in the temporal domain. In particular,\nas depicted in Fig. 1, the noisy long video is initially mapped into multiple noisy short video clips\nvia a designated function. Subsequently, existing off-the-shelf short video diffusion models can be\nemployed to denoise these video clips under the guidance of various text conditions. These denoised\nshort videos are then merged and inverted back to a less noisy original long video. Essentially, this\nprocedure establishes an abstract long video generator and editor without necessitating any additional\ntraining, enabling the generation and editing of videos of any length using established short video\ngeneration and editing methodologies.\nOur method was tested in three scenarios: pretrained t2v, tuning-free t2v, and one-shot-tuning t2v, all\nof which yielded favorable results. Furthermore, the incorporation of additional control information\nand advanced open-set detection [30] and segmentation [26] technologies allows for more impressive\nresults, such as precise layout control and arbitrary object video inpainting. Extensive experimental\nresults validate the broad applicability and effectiveness of our proposed Gen-L-Video.\n2\nRelated Work\nAs we mentioned, the current mainstream strategies for video generation and editing can be mainly cat-\negorized into three types: pretrained Text-to-Video (pretrained t2v), tuning-free Text-to-Video (tuning-\nfree t2v), and one-shot-tuning Text-to-Video (one-shot-tuning t2v). The breakthroughs in video\ngeneration and editing techniques have largely drawn from the existing technologies for image editing\nand generation. In this section, we first introduce the development of text-to-image technology and\nthen provide a brief overview of the key accomplishments of each of the three strategies. In the end,\nwe discuss recent advances in long video generations and the advantage of Gen-L-Video over them.\nText-to-Image generation.\nMany early works [74, 70, 65, 64, 55] train GANs [10] on image\ncaptioning datasets to produce text-conditional image samples. Other works [45, 68, 67, 5, 7] apply\n2\nShort Video Diffusion \nModel\nLong Video Diffusion Model\nDDIM inversion\nusing Long VDM\nDenoising\nusing Long VDM\n\u201cAn astronaut is \nwalking on the \nmoon\u201d\n\ud835\udc97t\n0 = \ud835\udc390(\ud835\udc97\ud835\udc61)\n\ud835\udc97t\u22121\n0\n\ud835\udc97t\u22121\n1\n\ud835\udc840\n\ud835\udc841\n\ud835\udc97\ud835\udc61\u22121 = argmin\n\ud835\udc97\n\u0dcd\n\ud835\udc56=0\n\ud835\udc56=1\n\u2225 \ud835\udc4a\ud835\udc56 \u2297 \ud835\udc39\ud835\udc56 \ud835\udc97 \u2212 \ud835\udc97\ud835\udc61\u22121\n\ud835\udc56\n\u22252\n2\n\ud835\udc97t\n1 = \ud835\udc391(\ud835\udc97\ud835\udc61)\nFigure 1: Left: High-level overview of Temporal Co-Denoising. Our framework treats long videos\nof arbitrary lengths and multiple semantic segments as a collection of short videos with temporal\noverlapping. It allows us to effectively apply short video diffusion models to approximate the\ndenoising path of these extended videos, ensuring consistency and coherence throughout. Right: The\npipeline of our framework for video editing. With the extended long video diffusion model, we first\ninvert the given long video into an approximated initial noise through DDIM inversion. Then, we\nsample a new video guided by the given prompts (either single or multiple).\nvector quantization [57] and then adopt autoregressive transformers to predict image tokens followed\nby text tokens. Recently, several works [40, 49, 11, 47] adopt diffusion [19] models for Text-to-\nImage Generation. GLIDE [40] introduces classifier-free guidance [18] in the diffusion model to\nenhance image quality, while DALLE-2 [46] improves text-image alignment using the CLIP [44]\nfeature space. Imagen [49] employs cascaded diffusion models for high-definition video generation.\nVQ-diffusion [11] and Latent Diffusion Model (LDM, also known as Stable Diffusion) [47] train\ndiffusion models in an autoencoder\u2019s latent space to boost efficiency. Variants of LDM are fine-tuned\nto achieve more functionality like inpainting, image variants, etc. ControlNet [71], T2I-adapter [39]\nadd new modules to accept additional image inputs, achieving precise generative layout control.\nMany fine-tuning strategies [48, 23, 8] are also developed to force diffusion models to generate new\nconcepts and styles, which shares a similar idea to continual learning [72, 60, 53].\nPretrained Text-to-Video.\nDespite significant advancements in Text-to-Image generation, gen-\nerating videos from text remains a challenge due to the scarcity of high-quality, large-scale text-\nvideo datasets and the inherent complexity of modeling temporal consistency and coherence. Early\nworks [36, 42, 34, 27, 12, 31] primarily focus on generating videos in simple domains, such as\nmoving digits or specific human actions. GODIVA [57] is the first model to utilize VQ-VAE and\nsparse attention for Text-to-Video generation, enabling more realistic scenes. N\u00dcWA [62] builds\nupon GODIVA with a unified representation for various generation tasks via multitask learning.\nCogVideo [22] incorporates additional temporal attention modules on top of the pre-trained Text-to-\nImage model [5]. Similarly, Video Diffusion Models (VDM) [21] use a space-time factorized U-Net\nwith joint image and video data training. Imagen Video [20] improves VDM with cascaded diffusion\nmodels and v-prediction parameterization for high-definition video generation. Make-A-Video [52],\nMagicVideo [73], and LVDM [15] share similar motivations, aiming to transfer progress from t2i\nto t2v generation. Video Fusion [32] decomposes the denoising process by resolving per-frame\nnoise into base noise and residual noise to reflect the connections among frames. For video editing,\nDreamix [38] and Gen-1 [6] utilize the video diffusion model for video editing.\nTuning-free Text-to-Video.\nIt\u2019s nontrivial to directly apply pretrained Text-to-Image model for\nvideo generation or editing without tuning. Recent diffusion-based image editing models [35, 16, 4,\n61, 24, 56], although powerful in processing individual frames in a video, results in inconsistencies\nbetween frames due to the models\u2019 lack of temporal awareness. Tune-A-Video [63] finds that\nextending spatial self-attention to sparse causal attention with pretrained weight produces consistent\ncontent across frames. Fate-Zero [43] and Video-P2P [29] apply sparse causal attention and attention\ncontrol proposed in prompt2prompt, achieving consistent video editing. Pix2Video [3] adds additional\nregularization to penalize the dramatic frame changes. Text2Video-Zero [25] first proposes to generate\nvideos in zero-shot settings with only pretrained text-to-image diffusion model. It applies the sparse\ncausal attention and object mask to preserve the content consistency among frames and add motion\ndynamics in different scales to enrich the base latent code to generate consecutive motions.\nOne-shot tuning Text-to-Video.\nSingle-video GANs [1, 13] generate new videos with similar\nappearance and dynamics to the input video, while they suffer from extensive computational burden.\n3\nGen-L-Video\nGen-L-Video sparse casual\nIsolated\n\u201cA jeep car is moving on the beach\u201d\nFigure 2: Comparison between our method Gen-L-Video and isolated denoising with short video\ndiffusion models. The long video consist of two short video clips (each with 20 frames), and we\nsample one in five for better visualization. Gen-L-Video achieves the most smooth and consistent\ngeneration. Gen-L-Video sparse causal means we replace the bi-directional cross-frame attention with\nthe sparse causal attention proposed in [63], which typically leads to the first few frames inconsistent\nwith the later ones, and we analyze the subtle reason in Sec. 4.\nSinFusion [41] adapts diffusion models to single-video tasks and enables autoregressive video\ngeneration with improved motion generalization capabilities. Tune-A-Video [63] proposes to fine-\ntune the pretrained text-to-image diffusion model on a video to enable generation of videos with\nsimilar motions, demonstrating powerful editing ability.\nLong video generation.\nThe generation of long videos has garnered significant attention in recent\nyears, resulting in various attempts to address the challenges associated with this task [59, 69, 14,\n9, 28]. Existing approaches typically rely on autoregressive models, such as NUWA-Infinity [28],\nPhenaki [58], and TATS [9], or diffusion models, including MCVD [59], FDM [14], PVDM [69],\nand LVDM [15]. All these methods employ an autoregressive mechanism for long video generation,\nwherein the generated frames serve as conditioning for subsequent frames. However, this mechanism\noften leads to significant content degradation after several extrapolations due to error accumulation.\nFurthermore, the autoregressive mechanism constrains generation to a sequential process, substantially\nreducing efficiency. Recently, NUWA-XL [66] proposed a novel hierarchical diffusion process that\nenables parallel long video generation. Despite its advantages, this approach necessitates extensive\npretraining on large long video datasets and requires a well-designed global diffusion model to\ngenerate key frames at the outset. Our framework, instead of directly training or constructing a long\nvideo diffusion model, can approximate the arbitrary length long video denoising path with parallel\njoint denoising of off-the-shelf short video generation or editing models. In general, Gen-L-Video\npresents an efficient, convenient, and scalable paradigm for long video generation, addressing the\nlimitations of existing methods and offering new possibilities for future research and applications.\nWe make a direct comparison of our method with existing methods in Table 2, and our approach\ndemonstrates significant superiority.\n3\nMethod\n3.1\nPreliminaries\nDiffusion models [19] perturb the data by gradually injecting noise to data x0 \u223c q(x0), which is\nformalized by a Markov chain:\nq(x1:T |x0) =\nT\nY\nt=1\nq(xt|xt\u22121),\nq(xt|xt\u22121) = N(xt|\u221a\u03b1txt\u22121, \u03b2tI),\nwhere \u03b2t is the noise schedule and \u03b1t = 1 \u2212 \u03b2t. The data can be generated by reversing this process,\ni.e.,we gradually denoise to restore the original data. The diffusion model p\u03b8(xt\u22121|xt) parameterized\nby \u03b8 is trained to approximate the reverse transition q(xt\u22121|xt, x0), which is formulated as\nq(xt\u22121|xt, x0) = N(xt\u22121; \u02dc\u00b5t(xt, x0), \u02dc\u03b2tI),\n\u02dc\u00b5t (xt, x0) =\n1\n\u221a\u03b1t\nxt \u2212\n1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\u221a\u03b1t\n\u03f5,\n4\nwhere \u00af\u03b1t = Qt\ns=1 \u03b1s, \u02dc\u03b2t = 1\u2212\u00af\u03b1t\u22121\n1\u2212\u00af\u03b1t \u03b2t, and \u03f5 is the noise injected to x0 to obtain xt. Therefore, the\nlearning of p\u03b8(xt\u22121|xt) is equivalent to the learning a noise prediction network \u03f5\u03b8(xt, t):\nmin\n\u03b8 Et,x0,\u03f5\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2,\nwhere t is uniformly sampled from {1, 2, . . . , T} and xt = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1t\u03f5.\nDDIM [54] generalize the framework of DDPM and propose a deterministic ODE process, achieving\nfaster sampling speed. The inversion trick of DDIM [37], based on the assumption that the ODE\nprocess can be reversed in the limit of small steps, can be used to approximate the corresponding\nnoise of the given instance:\nxt+\u2206t\n\u221a\u03b1t+\u2206t\n=\nxt\n\u221a\u03b1t\n+\n s\n1 \u2212 \u03b1t+\u2206t\n\u03b1t+\u2206t\n\u2212\nr\n1 \u2212 \u03b1t\n\u03b1t\n!\n\u03f5\u03b8 (xt, t) .\nLatent Diffusion Model (LDM) [47] is a variant of text-to-image diffusion models. An autoencoder\nis first trained on large image datasets, where the encoder E compresses the original image x0 into a\nlatent code zx0, and the decoder D reconstructs the original image from the latent code. That is\nzx0 = E(x0),\nx0 \u2248 D(zx0).\nThen a conditional DDPM p\u03b8(vt\u22121|vt, c) is trained to gradually remove noise for data sampling.\nClassifier-free guidance (GFC) [17] is proposed to improve the text-image alignment by linearly\ncombining the conditional predicted noise and the unconditional one.\n\u02c6\u03f5\u03b8(xt, t, c) = (1 + w)\u03f5\u03b8(xt, t, c) \u2212 w\u03f5\u03b8(xt, t, \u2205),\nwhere w > 0 is the guidance scale. Larger w typically improves the image-text alignment but\noverlarge w causes the degradation of sample fidelity and diversity.\n3.2\nTemporal Co-Denoising\nAs we discussed above, current Text-to-Video diffusion methods for generation and editing typically\nview the video as a whole. Given a noisy video vt, they train a diffusion model p\u03b8(vt\u22121|vt, c)\nwith respect to noise prediction model \u03f5\u03b8(vt, t, c) to denoise it as a whole. This greatly limits the\nvideo length that they are able to generate and makes it hard for them to accommodate multi-text\nconditions. Though some works performed long videos generation via the autoregressive mechanism,\nthis manner suffers from severe content degradation and only supports serialization generation,\nleading to inference inefficiency.\nIn contrast, we consider the denoising process of the entire video as multiple short videos with\ntemporal overlapping undergoing parallel denoising in the temporal domain. We approximate the\ndenoising trajectory of a long video through the joint denoising model of short videos in the temporal\ndomain. More specifically, we suppose there exists a model pl\n\u03b8(vt\u22121|vt, c) (with a corresponding\nnoise prediction network \u03f5l\n\u03b8(vt, t, c)) capable of denoising the given long video vt, resulting in the\ndenoising trajectory,\nvT , vT \u22121, . . . , v0,\ns.t.\nvt\u22121 \u223c pl\n\u03b8(vt\u22121|vt, c),\nwhere we use the diffusion model to gradually transform the pure noise vT \u223c N(0, I) to the clean\nvideo v0. c can be represented as a single or multiple text prompts.\nWe define a set of mappings Fi to project all original videos vt (both noisy and clean) in the trajectory\nto short video segments vi\nt, specifically,\nFi(vt) = vi\nt = vt,S\u2217i:S\u2217i+M,\nt = 1, 2, . . . , T,\ni = 0, 1, . . . , N \u2212 1,\nwhere vt,S\u2217i\u2212S\u2217i+M represents the collection of frames with frame id from S \u2217 i to S \u2217 i + M, S\nrepresents the stride among adjacent short video clips, M is the fixed length of short videos, and N\nis the total number of clips. Empirically, we find that setting S to M//2 or M//4 yields excellent\nresults and preserves efficiency. When setting S = M, our method degrades into isolated denoising.\nThe total number of frames of the video is S \u2217 N + M. Each short video vi\nt is guided with an\nindependent text condition ci. After obtaining these short videos, we are able to denoise these short\n5\n\ud835\udc97\ud835\udc61,\ud835\udc57\n\ud835\udc56\n\ud835\udc97\ud835\udc61,\ud835\udc57+1\n\ud835\udc56\n\ud835\udc97\n\ud835\udc61, \ud835\udc40\n2\n\ud835\udc56\n\ud835\udc97\ud835\udc61,\ud835\udc57\u22121\n\ud835\udc56\n\ud835\udc97\ud835\udc61,\ud835\udc57\n\ud835\udc56\nAnchor frame\nResnet Block\nResnet Block\nResnet Block\nResnet Block\nResnet Block\nTemporal Attention (Optional) \n\u00d7 \ud835\udc41\ud835\udc62\ud835\udc5a\n\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\nCross Frame \nAttention\nCross \nAttention\nCross Frame\nAttention\nCross \nAttention\nSelf-Spatial \nAttention\nCross \nAttention\nCross Frame\nAttention\nCross \nAttention\nCross Frame\nAttention\nCross \nAttention\n\u201cA man is \nsurfing on the \nsea\u201d\nText\nEncoder\n\ud835\udc84\ud835\udc56\nFigure 3: High-level overview of the model architecture for short video denoising. The given noisy\nshort video is treated as a collection of noisy images. In each block, we begin by processing each\nimage independently with 2d convolutions (with normalization and residual connection). Then, spatial\nattention is applied to the selected anchor frame to enhance pixel interactions, while bi-directional\ncross-frame attention enables other frames to interact with both adjacent frames and the anchor frame.\nAfter that, cross-attention is incorporated to integrate textual information for guiding the denoising\nprocess, and temporal attention is trained to further refine the relationships among frames.\nvideos using the off-the-shelf short video diffusion models pi\n\u03b8(vi\nt\u22121|vi\nt, ci). For simplicity, we can\nset the diffusion models for all video clips to a single diffusion model ps\n\u03b8(vi\nt\u22121|vi\nt, ci), and we find\nthat it works quite well. Then we have,\nvi\nt\u22121 \u223c ps\n\u03b8(vi\nt\u22121|vi\nt, ci).\nThe remaining question is how to obtain the denoised vt\u22121 after acquiring all short video clips vi\nt\u22121.\nConsidering that we have assumed Fi(vt) = vi\nt for all t and i, the ideal vt\u22121 should satisfy that\nFi(vt\u22121) is as close as vi\nt\u22121 as possible. Therefore, the optimal vt\u22121 can be obtained by solving the\nfollowing optimization problem.\nvt\u22121 = arg min\nv\nN\u22121\nX\ni=0\n\r\rWi \u2297 (Fi(v) \u2212 vi\nt\u22121)\n\r\r2\n2 ,\nwhere Wi is the pixel-wise weight for the video clip vi\nt, and \u2297 means the tensor product. It is not\ndifficult to verify that for an arbitrary frame j in the video vt\u22121, namely vt\u22121,j, it should be equal\nto the weighted sum of all the corresponding frames in short videos that contain the j frame. We\nprovide the proof in Sec. III.\nIn this way, we are able to approximate the transition function pl\n\u03b8(vt\u22121|vt, c) with ps\n\u03b8(vi\nt\u22121|vi\nt, ci).\nAs we claimed, our method establishes an abstract long video generator and editor without neces-\nsitating any additional training, enabling the generation and editing of videos of any length using\nestablished short video generation and editing methodologies.\nCondition interpolation. Considering that it is rather cumbersome to label all short videos with\nexact text descriptions, we allow the acceptance of sparse conditions. For instance, assuming that\nonly vki\nt , k \u2208 N+ are labeled with text descriptions cki, we obtain the text conditions of other video\nclips through adjacent interpolation cki+j = k\u2212j\nk cki + j\nkck(i+1), where j = 0, 1, . . . , k. This allows\nfor more flexibility in text-based guidance, enabling smoother content generation and simplifying the\noverall process for users.\n4\nIntegrate Gen-L-Video with Mainstream Paradigms\nAs previously mentioned, our method can be applied to three mainstream paradigms: pretrained t2v,\ntuning-free t2v, and one-shot-tuning t2v. In this section, we will introduce our implementation and\nimprovements for these paradigms in long video generation and editing. Furthermore, by utilizing\nadditional control information, we can achieve more accurate layout control. Advances in open-set\ndetection and segmentation allow us to achieve precise editing of arbitrary objects in the video without\naltering the other contents (e.g.,background), resulting in a more powerful and flexible video editing\nprocess. All our implementations are based on the pretrained LDM and its variants.\nPretrained Text-to-Video.\nFor pretrained Text-to-Video generation and editing, we choose the\nopen-sourced VideoCrafter [15]. VideoCrafter is a Text-to-Video model fine-tuned from LDM on the\n6\nlarge text-video dataset WebVid-10M [2]. For modeling the dynamic relationships among frames,\nVideoCrafter adds additional temporal attention blocks in the original LDM. The pipeline for long\nvideo generation and editing follows our proposed temporal co-denoising as illustrated in Fig. 1.\nTuning-free Text-to-Video.\nFor tuning-free Text-to-Video, we follow the pipeline of Pix2Video [3]\nfor its efficiency, with an additional distance penalty among adjacent frames to avoid dramatic\nchanges. Though many tuning-free Text-to-Video generation [25] or editing [3, 43] methods apply\nthe sparse causal attention mechanism [63], where the spatial attention is replaced by the cross\nattention between each frame and its former adjacent frame and the very first frame in the video.\nWe find that it typically causes noticeable inconsistency between the initial few frames and the\nsubsequent frames when generating long videos as shown in Fig. 2. We argue that it is because when\nthe anchor frame is chosen as the first frame, its denoising path will not be influenced by any other\nframes in the current and following video clips, leading to unidirectional information propagation.\nIt is obvious considering that in sparse causal attention, vanilla spatial attention is conducted in the\nvery first frame, and the denoising of other video clips will also not influence the noise path of the\nfirst frame. Instead, we propose to set the anchor frame as the middle frame in each short video\nclip and bidirectionally propagate the information to both the start and the end, which we dubbed\nas Bi-Directional Cross-Frame Attention. The bidirectional information propagation allows the\nmutual influence of anchor frames among different video clips, making it easier to find a compatible\ndenoising path for the long video. Specifically, denoting the input feature for the cross attention block\nof jth frame in video vi\nt as zi\nt,j, the computation of Attention(Q, K, V ) is formulated as\nQ = W Qzi\nt,j,\nK = W Kzi,\u2217\nt,j ,\nV = W V zi,\u2217\nt,j ,\nwhere zi,\u2217\nt,j =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u0002\nzi\nt,j\u22121, zt,\u230aM/2\u230b\n\u0003\n,\nj > \u230aM/2\u230b\nh\nzi\nt,j+1, zi\nt,\u230aM/2\u230b\ni\n,\nj < \u230aM/2\u230b\nzt,\u230aM/2\u230b,\nj = \u230aM/2\u230b\nand [\u00b7, \u00b7] means the operation of sequence dimen-\nsional concatenation.\nOne-shot tuning Text-to-Video.\nFor one-shot tuning Text-to-Video, we follow the pipeline of\nTune-A-Video [63]. Similar to Tuning-free Text-to-Video, we also replace the sparse causal attention\nmechanism with our proposed bidirectional cross-frame attention. However, applying this pipeline\ndirectly to the training and generation of long videos presents non-trivial challenges. Although we\ncan prompt the model to learn denoising for each short video clip, it struggles during generation. This\nis because many short video clips in a video share the same text description, making it difficult for\nthe model to determine which clip it is denoising based on randomly initialized noise and similar text\nconditions alone. To address this, we suggest learning clip identifier ei for each short video clip vi\nt to\nguide the model in denoising the corresponding clip.\nHowever, introducing ei could lead to overfitting of the corresponding clip content, causing the model\nto overlook the text information and lose its editing ability. Drawing on the idea of CFG [17], we\nrandomly drop ei during training. At test time, we base our approach on:\n\u02c6\u03f5\u03b8(vi\nt, t, ci, ei) = (1 + w)\u03f5\u03b8(vi\nt, t, ci, ei) \u2212 w\u03f5\u03b8(vi\nt, t, \u2205, \u2205).\nThis effectively alleviates the overfitting phenomenon. We believe that video learning consists of\nlearning content and motion information. Although ei learns both content and motion information\nfor the video clip vi\nt, we aim to retain only the motion information. By dropping ei, the model learns\nacross all video clips, gaining a large amount of content information. Shifting the denoising direction\naway from the scenario when ei is dropped allows us to reduce overfitting to video content.\nPersonalized and controllable generation.\nOur method can be easily extended to personalized\nand controllable layout generation. Users can easily combine personalized diffusion models obtained\nthrough fine-tuning strategies like DreamBooth [48] and LoRA [23] with our generation pipelines.\nBesides, we are able to inject additional layout control such as pose and segmentation maps with\nControlNet [71] and T2I-Adapter [39, 33] pretrained on image datasets [51]. This allows us for more\nsmooth and more precise video generation and editing.\nEdit anything in the video.\nOpen-set detection [30] and segmentation [26] have demonstrated\nremarkable ability and inspired plenty of interesting applications. We show that it is possible to\n7\nTable 1: Quantitative comparison.\nMethod\nFrame Consistency\nTextual Alignment\nAvg. Score \u2191\nHuman Pref. \u2191\nAvg. Score \u2191\nVariance (\u00d7100) \u2193\nIsolated\n91.65\n14.62%\n21.16\n0.57\nGen-L-Video\n93.18\n85.38%\n21.18\n0.48\nTable 2: Comparison to different methods.\nMethod\nLong\nMulti-Text Condition\nVast Video Corpus\nParallel Denoise\nVersatile\nTune-A-Video [63]\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nLVDM [15]\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u00d7\nNUWA-XL [66]\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\nGen-L-Video\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u201cA rabbit is \neating a \nwatermelon\u201d\nTiger\nSource Video\nw/ Identifier\nOne-shot tuning\nSource Video\nw/o  Identifier\n\u201cA grizzly bear \nis crawling on \nrocks.\u201d\nDog, Orange\nTiger\nTuning-free\nFigure 4: Left: Comparison between tuning-free t2v and one-shot tuning t2v. One-shot tuning\nt2v yields more editing flexibility. Right: Comparison between one-shot-tuning t2v w/ or w/o clip\nidentifier. One-shot-tuning t2v w/o clip identifier fails to generate consecutive motions in the source\nvideo with random initial noise.\ncombine these with our method to achieve precise arbitrary object editing in long videos. Specifically,\ngiven a prompt of a specific object (e.g.,man), we apply the open-set detection model Grouding\nDINO [30] to detect the corresponding object in each frame. Then, the open-set segmentation model\nSAM [26] is applied to get the precise mask of the object in each frame of the video. With these\nmasks, we are capable of applying inpainting methodologies for video editing while keeping the other\ncomponents of the video content unchanged. We find that directly using an pretrained Text-to-Image\ndiffusion model with our proposed bi-directional cross-frame attention can already yield acceptable\nresults. To achieve more precise layout generation, we additional add a controlnet pretrained on\nText-To-Image datasets [51] to accept the SAM maps.\n5\nExperiments\nImplementation details.\nAll our experiments are heavily built upon the pretrained LDM [47] (a.k.a\nStable Diffusion), as we mentioned. By default, DDIM sampling strategy is applied for all our\nexperiments and the sampling step and guidance scale is set to 50, and 13.5, respectively. In most\ncases, we set the number of frames of short video clips to 16 and stride between adjacent short videos\nclips to 4. For the one-shot-tuning Text-to-Video pipeline, we set the basic learning rate as 3e \u2212 5 and\nscale up the learning rate as the batch size, which greatly accelerates the training. The default beach\nsize is set to 5, and the loss typically converges within 100 epochs for videos with around 100 frames.\nBenchmarks.\nTo evaluate our method, we collect a video dataset containing 66 videos whose\nlengths vary from 32 to hundreds of frames. These videos are mostly drawn from the TGVE\ncompetition [63] and the internet. For each video, we label it a source prompt and add four edited\nprompts for video editing, including object change, background change, style transfer, similar motion\nchanges, and multiple changes. We provide details about the dataset in Sec. I.\nQualitative results.\nWe provide a visual presentation of representatives of our generated videos\nin Fig. 5, including results in various lengths generated through pretrained t2v, tuning-free t2v,\none-shot tuning t2v, personalized diffusion model, multi-text conditions, pose layout control, and\nvideo inpainting through the auto-detected mask, respectively. All of them show favorable results,\ndemonstrating the strong versatility of our framework. More results can be seen in Sec. IV and our\nproject page: https://github.com/G-U-N/Gen-L-Video.\nQuantitative results.\nFor quantitative comparisons, we assess the video frame consistency and the\ntextual alignment following the prior work [63]. We compare Gen-L-Video with Isolated Denoising,\nwhere each video clip is denoised isolatedly. Regarding the video frame consistency, we employ\nthe CLIP [44] image encoder to extract the embeddings of individual frames and then compute the\naverage cosine similarity between all pairs of video frames. To evaluate the textual alignment, which\nrefers to the alignment between text and video, we calculate the CLIP score of text and each frame\nin the video. The average value of the scores is used to measure the alignment degree while the\nvariance of those is used to measure the alignment stability. For human preference, we select several\n8\nPretrained\nOne-shot tuning\nTuning-free\nControllable\nPersonalized\nMulti-text\n\u201cA man is boating, village.\u201d\n\u201cA man is boating, a man is walking by the river, sunset, city.\u201d\n\u201cA pretty girl is walking in the sea.\u201d\n\u201c A\nw o m a n\ni s\np l a y i n g\nt e n n i s . \u201d .\n\u201cA jeep car is moving in the snow.\u201d\n\u201cA tiger in the grass in the sun.\u201d\n\u201d A s t r o n a u t\ni s\nr i d i n g\nh o r s e . \u201d\n\u201c I r o n\nm a n\ni s\ns u r f i n g . \u201d\nEdit Anything\nEdit\nEdit\nFigure 5: Qualitative generation results of Gen-L-Video.\nparticipants to vote on which method yields better frame consistency and textual alignment and get\n1040 votes in total. The experimental results are shown in Table. 1.\nAblation study.\nBi-directional cross-frame attention. We compare our proposed Bi-directional\ncross-frame attention with the sparse causal attention when temporal co-denoising is applied. As\nillustrated in Fig. 2, our method achieves the most smooth and consistent result while sparse causal\nattention typically causes the first few frames incwonsistent with subsequent ones. Video clip\nidentifier. We compare the generation results of one-shot tuning t2v with or without the clip identifier\nin Fig. 4 (Right). When random initial noise is used, one-shot tuning t2v without the clip identifier\nfails to generate consecutive content in the source video, while the other method succeeds\n6\nConclusion\nIn this work, we propose Gen-L-Video, a universal methodology that extends short video diffusion\nmodels for efficient multi-text conditioned long video generation and editing. We implement main-\nstream Text-to-Video methods and make additional improvements to integrate them with Gen-L-Video\nfor long video generation and editing. Experiments verify our framework is universal and scalable.\nLimitations: In general, our framework should be able to be extended into the co-working of various\ndifferent video diffusion models with different lengths to obtain more flexibility in generation and\nediting, but we haven\u2019t experimented with that. This avenue of research is left as future work.\n9\nReferences\n[1] Rajat Arora and Yong Jae Lee. Singan-gif: Learning a generative video model from a single gif.\nIn CVPR, pages 1310\u20131319, 2021. 3\n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video\nand image encoder for end-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 1, 7\n[3] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mitra. Pix2video: Video editing using\nimage diffusion. arXiv preprint arXiv:2303.12688, 2023. 1, 3, 7\n[4] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-\nbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.\n3\n[5] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-\nimage generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 1, 2,\n3\n[6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis\nGermanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023. 3\n[7] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. In ECCV, pages\n89\u2013106. Springer, 2022. 2\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022. 3\n[9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\nDevi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer.\narXiv preprint arXiv:2204.03638, 2022. 4\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020. 2\n[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages\n10696\u201310706, 2022. 3\n[12] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine\nthis! scripts to compositions to videos. In ECCV, pages 598\u2013613, 2018. 3\n[13] Shir Gur, Sagie Benaim, and Lior Wolf. Hierarchical patch vae-gan: Generating diverse videos\nfrom a single sample. NeurIPS, 33:16761\u201316772, 2020. 3\n[14] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood.\nFlexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022. 4\n[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen.\nLatent video\ndiffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint\narXiv:2211.13221, 2022. 3, 4, 6, 8\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022. 1, 3\n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS, 2021. 5, 7\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022. 3\n10\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020. 3, 4\n[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. arXiv:2204.03458, 2022. 1, 3\n[22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale\npretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868,\n2022. 1, 3\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3, 7\n[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022. 3\n[25] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion\nmodels are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2, 3, 7\n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023. 2, 7, 8\n[27] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation\nfrom text. In AAAI, volume 32, 2018. 3\n[28] Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu,\nYuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for\ninfinite visual synthesis. NeurIPS, 35:15420\u201315432, 2022. 4\n[29] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing\nwith cross-attention control. arXiv preprint arXiv:2303.04761, 2023. 1, 2, 3\n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023. 2, 7, 8\n[31] Yue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. Cross-modal dual learning for sentence-to-\nvideo generation. In ACM MM, pages 1239\u20131247, 2019. 3\n[32] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,\nJingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality\nvideo generation. arXiv e-prints, pages arXiv\u20132303, 2023. 2, 3\n[33] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen.\nFollow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint\narXiv:2304.01186, 2023. 7\n[34] Tanya Marwah, Gaurav Mittal, and Vineeth N Balasubramanian. Attentive semantic video\ngeneration using captions. In ICCV, pages 1426\u20131434, 2017. 3\n[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nICLR, 2021. 3\n[36] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video\ngeneration using deep recurrent attentive architectures. In ACM MM, pages 1096\u20131104, 2017. 3\n11\n[37] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion\nfor editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n5\n[38] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv\nLeviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.\narXiv preprint arXiv:2302.01329, 2023. 3\n[39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023. 1, 3, 7\n[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1, 3\n[41] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single\nimage or video. arXiv preprint arXiv:2211.11743, 2022. 2, 4\n[42] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell:\nGenerating videos from captions. In ACM MM, pages 1789\u20131798, 2017. 3\n[43] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and\nQifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint\narXiv:2303.09535, 2023. 1, 2, 3, 7\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021. 3, 8\n[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821\u20138831.\nPMLR, 2021. 2\n[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1,\n3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.\n3, 5, 8\n[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022. 3, 7\n[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022. 1, 3\n[50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 1\n[51] Christoph Schuhmann,\nRichard Vencu,\nRomain Beaumont,\nTheo Coombes,\nCade\nGordon, Aarush Katta, Robert Kaczmarczyk, and Jenia Jitsev.\nLAION-5B: laion-\n5b:\nA new era of open large-scale multi-modal datasets.\nhttps://laion.ai/\nlaion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/, 2022. 7, 8\n[52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\ntext-video data. arXiv preprint arXiv:2209.14792, 2022. 1, 3\n12\n[53] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and\nHongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with\nc-lora. arXiv preprint arXiv:2304.06027, 2023. 3\n[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020. 5\n[55] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao.\nDf-gan: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint\narXiv:2008.05865, 2020. 2\n[56] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features\nfor text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 3\n[57] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30,\n2017. 1, 3\n[58] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual description. arXiv preprint arXiv:2210.02399,\n2022. 4\n[59] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video\ndiffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\n4\n[60] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and\ncompression for class-incremental learning. In ECCV, pages 398\u2013414. Springer, 2022. 3\n[61] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models\u2019 latent space, with\napplications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022. 3\n[62] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00fcwa:\nVisual synthesis pre-training for neural visual world creation. In ECCV, pages 720\u2013736.\nSpringer, 2022. 1, 3\n[63] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 1, 2, 3, 4, 7, 8, 15\n[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial\nnetworks. In CVPR, pages 1316\u20131324, 2018. 2\n[65] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving\ntext-to-image synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021. 2\n[66] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni,\nZhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion\nfor extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 4, 8\n[67] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2\n[68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2\n[69] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in\nprojected latent space. arXiv preprint arXiv:2302.07685, 2023. 4\n[70] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal\ncontrastive learning for text-to-image generation. In CVPR, pages 833\u2013842, 2021. 2\n13\n[71] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023. 1, 3, 7\n[72] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan. Pycil: A python toolbox for\nclass-incremental learning. arXiv preprint arXiv:2112.12533, 2021. 3\n[73] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo:\nEfficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\n3\n[74] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative\nadversarial networks for text-to-image synthesis. In CVPR, pages 5802\u20135810, 2019. 2\n14\nSupplementary of Gen-L-Video\nI\nDataset Details\nTo evaluate our method, we have selected several videos (totaling 66) from TGVE competition [63]\nand Internet. For internet videos, we have designed 4 distinct prompts that introduce dynamic changes\nin the areas of object recognition, stylistic elements, background variations, similar motion patterns,\nor a combination thereof, based on the original prompts.\nTable 3: Names of videos selected.\ncows-grazing\nshopping-entertainment-center\nhike\nwarsaw-multimedia-fountain\ndeer-eating-leaves\nseagull-flying\nsingapore-airbus-a380-landing\npigs\nmallard-water\nred-roses-sunny-day\ncows\nmiami-surf\nsharks-swimming\npouring-beer-from-bottle\ndog\nbird-on-feeder\nairbrush-painting\nbear\nwind-turbines-at-dusk\nhorsejump-low\ncamel\nlotus\ntypewriter-super-slow-motion\nwetsuit-surfing\nsetting-sun\ncat-in-the-sun\nstreet-artist-painting\ngold-fish\nmotorbike\nrabbit-watermelon\nearth-full-view\nsnowboard\neating-pizza\nfireworks-display\nweightlifting-sofa\ndrift-turn\nsurfer-on-wave\namerican-flag-in-wind\nbasketball-mall\nship-sailing\naircraft-landing\ndirt-road-driving\naudi-snow-trail\neiffel-flyover\nbutterfly-feeding-slow-motion\nkettleball-training\nferris-wheel-timelapse\ndj-mixing-music\nski-lift-time-lapse\nswimmer\nraindrops\ntennis\nswans\nsunset-beach-yoga\nsunset-swinging\nmbike-trick\nman-skiing\ncar-turn\nboxer-punching-towards-camera\nman-surfing\nhorizontal-match-striking\nairplane-and-contrail\nski-follow\nlas-vegas-time-lapse\ngeometric-video-background\nblackswan\nII\nUser Study Details\nWe utilized the aforementioned dataset as the benchmark to contrast our approach, Gen-L-Video, with\nthe Isolated method. Beyond quantitative indices (frame consistency and textual alignment), we also\nenlisted several participants to vote on which method was superior. To assess frame consistency, we\ngenerated 264 videos from 4 prompts and an additional 65 videos using both methods, respectively.\nThen we replicate and shuffle them to 1040 videos. We distributed pairs of these videos to several\nindividuals, posing the question, \"Which video exhibits superior frame consistency between these\ntwo?\" In order to gauge textual alignment, we created 279 videos from varying prompts using both\nmethods separately. We subsequently asked the participants, \"Which video aligns more accurately\nwith the text description among these two videos? A?\" No B nr diffeerenc?? Most participants\nreported no significant difference in the degree of textual alignment, but a clear improvement in\nalignment stability, indicating that our method maintains good text-based editing capabilities.The\nmore detailed results of these comparisons are depicted in Table 1.\nIII\nProof for the Optimal Approximation\nAs we claimed in Sec. 3.2, the ideal vt\u22121 should satisfy that Fi(vt\u22121) is as close as vi\nt\u22121 as possible.\nThe optimal vt\u22121 can be obtained by solving the following quadratic optimization problem:\nvt\u22121 = arg min\nv\nN\u22121\nX\ni=0\n\r\rWi \u2297 (Fi(v) \u2212 vi\nt\u22121)\n\r\r2\n2 ,\n15\nwhere Wi is the pixel-wise weight for the video clip vi\nt, and \u2297 means the tensor product. Here\nwe show that, for an arbitrary frame j in the video vt\u22121, namely vt\u22121,j, it should be equal to the\nweighted sum of all the corresponding frames in short videos that contain the j frame.\nProof III.1 Considering that there are N video clips in total, we denote the set of all video clips and\nthe set of their indices as V = {v0, v1, . . . , vN\u22121} and I = {0, 1, . . . , N \u22121}, respectively. Further,\nwe assume that there exists a set of video clips Vj consisting of short video clips containing the\ncorresponding jth frame v\u2662,j in the original long video. Similarly, the set of indices corresponding\nto Vj is denoted as Ij. Here we use the \u2662 to represent that it could be suitable for all the time t in\nthe denoising path.\nGiven a short video clip vi \u2208 Vj, we denote the corresponding frame of v\u2662,j in vi as vi\n\u2662,j\u2217 for\nsimplicity. Note that the j\u2217 in different video clips represents different values.\nTherefore, the original optimization objective can be written as:\nN\u22121\nX\ni=0\n\r\rWi \u2297\n\u0000Fi(vt\u22121) \u2212 vi\nt\u22121\n\u0001 \r\r2\n2\n=\nX\ni\u2208Ij\n\r\rWi \u2297\n\u0000Fi(vt\u22121) \u2212 vi\nt\u22121\n\u0001 \r\r2\n2 +\nX\ni\u2208I\\Ij\n\r\rWi \u2297\n\u0000Fi(vt\u22121) \u2212 vi\nt\u22121\n\u0001 \r\r2\n2\n=\nX\ni\u2208Ij\n\r\rWi,j\u2217 \u2297\n\u0000Fi,j\u2217(vt\u22121) \u2212 vi\nt\u22121,j\u2217\n\u0001 \r\r2\n2 +\nX\ni\u2208Ij\nX\nj\u0338=j\u2217\n\r\rWi,j \u2297\n\u0000Fi,j(vt\u22121) \u2212 vi\nt\u22121,j\n\u0001 \r\r2\n2\n+\nX\ni\u2208I\\Ij\n\r\rWi \u2297 (Fi(vt\u22121) \u2212 vi\nt\u22121)\n\r\r2\n2 .\nWhere, Wi,j is the pixel-wise weight for the jth frame in vi\nt\u22121 (i.e.,vi\nt\u22121,j), and Fi,j(vt\u22121) is the jth\nframe in Fi(vt\u22121). It is not difficult to observe that the last two terms in the formula have nothing to\ndo with vt\u22121,j. We denote them as constant C. Then we have,\nX\ni\u2208Ij\n\r\rWi,j\u2217 \u2297\n\u0000Fi,j\u2217(vt\u22121) \u2212 vi\nt\u22121,j\u2217\n\u0001 \r\r2\n2 + C\n=\nX\ni\u2208Ij\n\u0002\nWi,j\u2217 \u2297\n\u0000Fi,j\u2217(vt\u22121) \u2212 vi\nt\u22121,j\u2217\n\u0001\u0003\u22a4 \u0002\nWi,j\u2217 \u2297\n\u0000Fi,j\u2217(vt\u22121) \u2212 vi\nt\u22121,j\u2217\n\u0001\u0003\n+ C\n=\nX\ni\u2208Ij\nh\n(Wi,j\u2217 \u2297 Fi,j\u2217(vt\u22121))\u22a4 (Wi,j\u2217 \u2297 Fi,j\u2217(vt\u22121)) +\n\u0000Wi,j\u2217 \u2297 vi\nt\u22121,j\u2217\n\u0001\u22a4 \u0000Wi,j\u2217 \u2297 vi\nt\u22121,j\u2217\n\u0001\n\u22122\n\u0000Wi,j\u2217 \u2297 vi\nt\u22121,j\u2217\n\u0001\u22a4 (Wi,j\u2217 \u2297 Fi,j\u2217(vt\u22121))\ni\n+ C.\nDenote the above objective as L and take the gradient of L with respect to vt\u22121,j, and then we have\n\u2202L\n\u2202vt\u22121,j\n= 2\nX\ni\u2208Ij\n\u0002\n(Wi,j\u2217 \u2297 Fi,j\u2217(vt\u22121)) \u2212\n\u0000Wi,j\u2217 \u2297 vi\nt\u22121,j\u2217\n\u0001\u0003\n\u2297 Wi,j\u2217 \u2297 \u2202Fi,j\u2217(vt\u22121)\n\u2202vt\u22121,j\n.\nNote that Fi,j\u2217(vt\u22121) = vt\u22121,j, therefore \u2202Fi,j\u2217(vt\u22121)\n\u2202vt\u22121,j\n= 1 and we can replace the Fi,j\u2217(vt\u22121) in\nthe above formula with vt\u22121,j. Then, we have\n\u2202L\n\u2202vt\u22121,j\n= 2\nX\ni\u2208Ij\nWi,j\u2217 \u2297\n\u0002\n(Wi,j\u2217 \u2297 vt\u22121,j) \u2212\n\u0000Wi,j\u2217 \u2297 vi\nt\u22121,j\u2217\n\u0001\u0003\n= 2\n\"X\ni\u2208Ij\n(Wi,j\u2217)2 \u2297 vt\u22121,j \u2212\nX\ni\u2208Ij\n\u0010\n(Wi,j\u2217)2 \u2297 vi\nt\u22121,j\u2217\n\u0011#\n.\nTherefore, set the gradient to be zero, and then we get the optimal vt\u22121,j,\nvt\u22121,j =\nP\ni\u2208Ij\n\u0000(Wi,j\u2217)2 \u2297 vi\nt\u22121,j\u2217\n\u0001\nP\ni\u2208Ij(W 2\ni,j\u2217)2\n,\nwhich is the weighted sum of all the corresponding frames in short video clips that contain the jth\nframe.\n16\nIV\nAdditional Results\nHere, we showcase additional results.\nMulti-text long video. Fig. 6 illustrates an example of our multi-text conditioned long video.\nSpecifically, we first split the original video into several short video clips with obvious content\nchanges and of various lengths, and then we label them with different text prompts. The different\ncolors in Fig. 6 indicate short videos with different text prompts. Then, we split the original long\nvideo into short video clips with fixed lengths and strides. For video clips only containing frames\nconditioned on the same prompt, we can directly set it as the condition. In contrast, for video clips\ncontaining frames conditioned on different prompts, we apply our proposed condition interpolation to\nget the new condition. After all of these, our paradigm Gen-L-Video can be applied to approximate\nthe denoising path of the long video.\nPretrained Text-to-Video. Gen-L-Video can also be applied to the pretraiend short video generation\nmodel for longer video generation. We compare the results generated through our method and isolated\ndenoising in Fig. 7, and the result reveals that our method significantly enhances the relevance between\ndifferent video clips.\nControllabel video generation. We showcase the results generated by injecting additional control\ninformation in Fig. 8. The results show that our method can be easily combined with additional\ninformation to achieve precise layout control.\nEdit anything. Our approach demonstrates significant compatibility with inpainting tasks. As\ndepicted in Fig.9 and Fig.10, our method can reliably edit very long videos and maintain consistent\ncontent. The examples given in Fig. 9 are longer than 12 and 20 seconds, respectively.\nLong video with smooth semantic changes. Our paradigm also allows us a pleasant application\nwhere we are able to edit the source video to generate videos with smooth semantic changes. For\nexample, we are able to generate cars running on the road from day to night to reflect the time flies.\nThe generated results are represented in Fig. 11.\n17\n1. A boy/girl is fighting, holding a katana. 2.The boy/girl is attacked by an assassin using katana.                                          \n3. The boy/girl is stepping back to the forest. 4.The assassin is running towards the boy/girl. \n5. The boy/girl is fighting against the assassin with a katana.   \nFigure 6: Multi-text conditioned long video.\n18\nA car is moving on the road.\na monkey is drinking water\nAn astronaut is riding a horse.\nGen-L-Video\nGen-L-Video\nGen-L-Video\nGen-L-Video\nIsolated\nIsolated\nIsolated\nIsolated\nAn astronaut is riding a horse, loving Vincent style.\nFigure 7: Long video generation with pretrained short video diffusion models.\n19\nA cute boy is playing tennis.\n+\n+\n+\nIron man is fighting in the snow.\nA Van Gogh style painting of a man dancing.\nDetector\nEstimator\nA dog in the sun.\nA realistic tiger in the sun.\nA pretty girl in the sun\n+\n+\n+\nFigure 8: Controllable long video generation.\n20\nMask \nDetector\nMask \nDetector\neating cake.\n+\neating pizza, cartoon style.\n+\neating watermelon.\n+\nCyberpunk.\n+\ngoggles.\n+\npink sunglasses.\n+\nFigure 9: Edit anything in videos.\n21\nMask \nDetector\nBat Man.\n+\nIron Man.\n+\nFigure 10: Edit anything in videos.\nA man is boating, village.\n  A man is walking by, city, sunset.\nA jeep car is running on the beach, sunny\n  A jeep car is running on the beach, night\nA jeep car is running on the snow, sunny.\n  A jeep car is running on the snow, night.\nLion, grass, rainy.\n  Cat, grass, Sun.\nIron man is skiing in the snow.            Iron man is flying in the sky.\nA man is surfing in the sea.\n  A man is skiing in the snow.\nFigure 11: Long videos with smooth semantic changes.\n22\n"
  },
  {
    "title": "BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
    "link": "https://arxiv.org/pdf/2305.18098.pdf",
    "upvote": "2",
    "text": "BigTranslate: Augmenting Large Language Models with Multilingual\nTranslation Capability over 100 Languages\nWen Yang1,2, Chong Li1,2, Jiajun Zhang1,2,3\u2217, and Chengqing Zong1,2\n1 Institute of Automation, Chinese Academy of Sciences\n2 School of Artificial Intelligence, University of Chinese Academy of Sciences\n3 Wuhan AI Research\n{yangwen2023, lichong2021}@ia.ac.cn\n{jjzhang, cqzong}@nlpr.ia.ac.cn\nAbstract\nLarge language models (LLMs) demonstrate\npromising translation performance among vari-\nous natural languages. However, many LLMs\nespecially the open-sourced ones, such as\nBLOOM (Scao et al., 2023) and LLaMA (Tou-\nvron et al., 2023), are English-dominant and\nsupport only dozens of natural languages, mak-\ning the potential of LLMs on language trans-\nlation less explored. In this work, we present\nBigTranslate which adapts LLaMA that covers\nonly 20 languages and enhances it with multi-\nlingual translation capability on more than 100\nlanguages. BigTranslate is built upon LLaMA-\n13B and it is optimized in three steps. First, we\ncontinue training LLaMA with massive Chi-\nnese monolingual data. Second, we continue\ntraining the model with a large-scale paral-\nlel dataset that covers 102 natural languages.\nThird, we instruct-tune the foundation model\nwith multilingual translation instructions, lead-\ning to our BigTranslate model. The preliminary\nexperiments on multilingual translation show\nthat BigTranslate performs comparably with\nChatGPT and Google Translate in many lan-\nguages and even outperforms ChatGPT in 8\nlanguage pairs. We release the BigTranslate\nmodel1 and hope it can advance the research\nprogress.\n1\nIntroduction\nLarge language models (LLMs), such as Chat-\nGPT (OpenAI, 2022) and PaLM 2 (Anil et al.,\n2023), demonstrate impressive translation capabili-\nties among various natural languages. For example,\nseveral recent studies indicate that ChatGPT is a\ngood translator in many scenarios (e.g., spoken lan-\nguage translation, document translation, and mul-\ntilingual translation), and it can even outperform\nSOTA translation engines in some specific scenar-\nios (Bawden and Yvon, 2023; Hendy et al., 2023;\n\u2217Corresponding author.\n1https://github.com/ZNLP/BigTranslate\nJiao et al., 2023; Wang et al., 2023; Zhu et al.,\n2023). LLMs are also preferred as a translator for\ntheir interactive usage.\nHowever, most of the existing LLMs are English-\ndominant and the popular LLMs support only sev-\neral or dozens of natural languages. For example,\nGLM (Du et al., 2022; Zeng et al., 2022) just sup-\nports English and Chinese. BLOOM (Scao et al.,\n2023) covers 46 languages while LLaMA (Touvron\net al., 2023) only supports 20 languages. It is well-\nknown that there are over 7000 natural languages\nin the world and existing LLMs cover only a very\nsmall fraction of the languages. Obviously, a large\npopulation in the world cannot be benefited from\nthe multilingual capability of LLMs.\nIn order to equip LLMs with much more multilin-\ngual ability, we introduce BigTranslate that adapts\nLLaMA making it capable of translating over 100\nnatural languages. Instead of optimizing the foun-\ndation model with self-supervised learning on mas-\nsive monolingual data over multiple languages, we\nmainly employ the bitexts which can transfer the\nknowledge from high-resource languages to low-\nresource ones through semantic mapping between\nparallel sentences.\nSpecifically, we build BigTranslate based on the\n13B version of LLaMA which is proven to be com-\nparable with GPT-3 in many natural language pro-\ncessing tasks. The BigTranslate is constructed in\nthree steps. In the first step, we utilize massive\nChinese texts to continue training LLaMA, result-\ning in a strong model which well supports Chinese.\nIn the second step, a large-scale parallel dataset\ncovering 102 natural languages is employed to con-\ntinue training the LLMs in a curriculum learning\nmanner and we obtain a multilingual foundation\nmodel that has the potential to perform translating\namong more than 100 natural languages. In the\nthird step, instruction tuning is applied to optimize\nthe multilingual foundation model with rich transla-\ntion instructions. Finally, we get our BigTranslate\narXiv:2305.18098v3  [cs.CL]  21 Nov 2023\nmodel.\nTo verify the effectiveness of our BigTranslate\nmodel, we conduct preliminary multilingual trans-\nlation experiments on all 102 languages. We com-\npare BigTranslate with both Google Translate and\nChatGPT. Since the automatic evaluation metric\nBLEU is usually criticized for the poor correla-\ntion with human judgments in machine translation\nquality, we further employ GPT-4 (OpenAI, 2023)\nwhich shows a high correlation with human (Liu\net al., 2023) as the evaluator and we design well-\ndefined prompts to make GPT-4 act like a human\nevaluator. The experiments show that BigTranslate\nperforms comparably with Google and ChatGPT in\nmany languages, and even outperforms ChatGPT\nin 8 language pairs.\n2\nRelated Work\n2.1\nLarge Language Models\nSince the advent of GPT-3 (Brown et al., 2020)\nby OpenAI in 2020, large language models that\nemploy Transformer as the backbone and contain\ntens or hundreds of billions of parameters, such\nas PaLM (Chowdhery et al., 2022), OPT (Zhang\net al., 2022), BLOOM (Scao et al., 2023), Chin-\nchilla (Hoffmann et al., 2022), Galactica (Taylor\net al., 2022), GLM (Du et al., 2022; Zeng et al.,\n2022), and LLaMA (Touvron et al., 2023), are con-\nstantly emerging. Among these LLMs, ChatGPT\nis a big milestone that demonstrates that the foun-\ndation large language model exhibits emergent and\ngeneral abilities when the model is large enough\nand equipped with instruction tuning.\nMany recent studies investigate the ability of\nChatGPT, GPT-4, and other LLMs in traditional\nnatural language processing tasks, including ma-\nchine translation (Bawden and Yvon, 2023; Hendy\net al., 2023; Jiao et al., 2023; Wang et al., 2023; Zhu\net al., 2023). (Jiao et al., 2023) reports that Chat-\nGPT and GPT-4 are good translators, especially for\nhigh-resource languages and spoken language sce-\nnarios. (Wang et al., 2023) demonstrates that Chat-\nGPT and GPT-4 perform quite well in document\ntranslation with the help of long context modeling.\n(Bawden and Yvon, 2023) and (Zhu et al., 2023)\nshow that LLMs like ChatGPT and BLOOM are\nalso multilingual translators and even outperform\nSOTA online translation engines. However, most\nof the existing LLMs are English-dominant and\ncover up to only dozens of languages. In this work,\nwe address this challenge and present BigTranslate\nwhich can support more than 100 languages.\n2.2\nMultilingual Neural Machine Translation\nMultilingual neural machine translation aims at\ntranslating multiple languages with a single shared\nmodel (Johnson et al., 2017). Most studies focus\non the unbalanced problem in multilingual trans-\nlation. For example, some works investigate how\nto design shared and language-dependent model\nparameters in a multilingual translation framework\n(Wang et al., 2018, 2019; Lin et al., 2021; Xie et al.,\n2021; Wang and Zhang, 2022). Several works\nexplore how to train the multilingual translation\nmodel more effectively and efficiently when the\ntraining data are quite unbalanced across languages\n(Zhou et al., 2021; Huang et al., 2022). Few studies\npursue the potential of a multilingual translation\nmodel on handling more than 100 languages. For\nexample, NLLB (Costa-juss\u00e0 et al., 2022) proposed\nby Meta aims to build a multilingual translation\nmodel that could translate as many languages as\npossible (currently covering more than 200 lan-\nguages). However, this kind of model can only\nperform translating. In this work, we pursue con-\nstructing a multilingual translator by adapting an\nLLM while maintaining its generic ability.\n3\nBigTranslate Construction\n3.1\nLLaMA as Foundation Model\nConsidering its impressive performance on most\nEnglish benchmarks after pre-training on 1.4T to-\nkens (Touvron et al., 2023), LLaMA is adopted as\nour foundation model. Specifically, the BigTrans-\nlate is initialized from the LLaMA-13B model to\nreduce the computational cost and continues to\ntrain on massive Chinese and parallel corpus.\n3.2\nAugmenting Foundation Model with\nChinese\nLLaMA shows a poor Chinese language under-\nstanding and generation performance due to the\nlack of a sufficient Chinese pre-training corpus (Cui\net al., 2023), although its performance in English\nis comparable to or even better than GPT-3. More-\nover, Shah et al. (2023) found that Chinese exhibits\na significantly low cross-lingual similarity with\nother languages, including English and German,\nwhich means that the inferior processing ability in\nChinese text will hinder our foundation model to-\nwards a better multilingual translation model. Thus\nit is indispensable to augment the ability of our\nfoundation model with additional Chinese vocab-\nulary and pre-training corpus. By doing so, we\nexpect the final model to be capable of multilingual\ntranslation centered on both English and Chinese.\nTo achieve this, we first append the original vo-\ncabulary with 6,223 Chinese tokens, most of which\nare Chinese characters, generated from Sentence-\nPiece (Kudo and Richardson, 2018) using the byte-\npair encoding (BPE) algorithm (Sennrich et al.,\n2016) on Chinese text. Then, a large-scale Chi-\nnese pre-training corpus, including CLUE (Xu,\n2019; Xu et al., 2020), Chinese News, and Chinese\nquestion-answering dataset, is adopted to pre-train\nthe model resulting in Chinese LLaMA.\n3.3\nAugmenting Foundation Model with 102\nlanguages\nThe purpose of this study is to equip a large lan-\nguage model with Chinese-centric multilingual\nability. Despite intensive training on massive Chi-\nnese monolingual data, the Chinese LLaMA model\nprimarily exhibits proficiency in Chinese language\nprocessing, but lacks adequate multilingual capabil-\nities. Furthermore, the continuous training on Chi-\nnese monolingual data has the potential to diminish\nthe model\u2019s performance in the 20 languages origi-\nnally supported by LLaMA. To address these lim-\nitations, we further refine the foundational model\nby incorporating a substantial parallel dataset en-\ncompassing 102 languages. This second training\nstage aims to enhance the model\u2019s ability to facil-\nitate multilingual translation tasks, enabling it to\nsupport a broader range of languages.\nLarge language model pre-training is typically\nconducted using monolingual data, focusing on\nautoregressive training in the language to enhance\nthe model\u2019s capabilities. In contrast, multilingual\nlarge language model pre-training involves training\non multilingual parallel data, enabling the language\nmodel to learn across different languages. The\nprimary challenge in multilingual large language\nmodel pre-training lies in achieving a balance in\nlearning between high-resource and low-resource\nlanguages. This balance ensures that the model\nacquires proficiency in both high-resource and low-\nresource languages during training.\nThe issue of maintaining balanced learning be-\ntween high-resource and low-resource languages\nhas been a long-time topic of concern in multilin-\ngual learning. Despite the notable progress made\nin the development of large language models, this\nproblem is still not well solved as the model ca-\npacity increases. To address this challenge encoun-\ntered during the pre-training phase, our work pro-\nposes an incremental data sampling strategy. By\nemploying the strategy, the model is trained on a\nharmonized multilingual parallel corpus at each\nstage, mitigating the concern of unbalanced lan-\nguage proficiency.\n3.3.1\nLarge-scale Parallel Dataset\nConstruction\nIn order to enhance the language capabilities of\nthe Chinese LLaMA model to support 102 lan-\nguages, we constructed a comprehensive parallel\ncorpus dataset consisting of 102 languages. This\ndataset was employed to continue training the foun-\ndational model. The compilation of this dataset\ndrew upon multiple sources, including widely avail-\nable public parallel corpus datasets and household\ndatasets. The public datasets utilized in our study\ncontain IWSLT, WMT, CCMT, and OPUS-100\n(Zhang et al., 2020), forming the initial corpus of\nour dataset.\nAfter obtaining the initial corpus, a key con-\nsideration is to balance the two-way translation\nin language pairs. For example, the data size of\nChinese-to-English translation should be similar to\nEnglish-to-Chinese. To achieve this balance, we\nutilize data augmentation to enrich the corpus if\nnecessary. The data augmentation follows the sub-\nsequent strategy. In cases where the number of\nparallel corpus falls below 1 million, we flip the\nentire corpus to create the corpus for the opposite\ntranslation direction. In contrast, for corpora with\nmore than 1 million instances, we randomly flip\nhalf the amount of corpus to generate the corre-\nsponding corpus. After data augmenting, the initial\ncorpus of 142 translation directions is substantially\nenriched, expanding to a significantly larger corpus\nof 242 translation directions.\nTo effectively illustrate the distribution of the\ncorpus, we present a visual representation of the\nlanguage-pair distribution within the multilingual\ndatasets as figure 1. The matter pertaining to the\nimbalance between high-resource and low-resource\nlanguage pairs continues to be a prominent concern\nwithin the current corpus.\n3.3.2\nTokenizer\nLLaMA tokenizes data with the byte-level byte-\npair (BBPE) encoding algorithm, implemented by\nSentencePiece. LLaMA\u2019s original vocabulary in-\nen-fr\nfr-en\nen-cs\ncs-en\nen-de\nen-ru\nru-en\nde-en\nen-zh\nzh-es\nes-zh\nzh-en\nfr-zh\nzh-fr\nzh-hi\nhi-zh\nen-lv\nen-fi\nzh-pt\npt-zh\nlv-en\nzh-mo\nfi-en\nen-et\nzh-bo\nmo-zh\net-en\nzh-he\nen-ro\nhe-zh\nro-en\nen-tr\ntr-en\nen-ur\nur-en\nmt-en\nen-mt\nbo-zh\nen-pl\npl-en\nen-bn\nbn-en\nzh-ko\nen-lt\nlt-en\nko-zh\nen-ga\nga-en\nen-nn\nka-en\nen-ka\nnn-en\nid-zh\nen-nl\nen-uz\nuz-en\nar-cs\ncs-ar\nko-en\nen-ko\nen-es\nen-it\nen-pt\npt-en\nnl-en\nzh-id\nar-en\nes-en\nen-br\nja-en\nbr-en\nit-en\nzh-uy\nen-ja\nen-ml\nml-en\nel-en\nen-el\nen-tt\ntt-en\nen-tg\ntg-en\nzh-my\nmy-zh\nen-mg\nmg-en\nen-gu\ngu-en\nzh-ka\nen-eo\nka-zh\neo-en\nen-da\nda-en\nen-vi\nuy-zh\nvi-en\nen-sv\nsv-en\nen-ug\nug-en\nen-eu\neu-en\nen-ne\nen-ar\nne-en\nen-sq\nsq-en\nen-sk\nsk-en\nha-en\nen-ha\nen-gl\ngl-en\nen-mk\nen-he\nmk-en\nhe-en\nen-id\nen-hu\nth-en\nhu-en\nid-en\nen-th\nen-si\nsi-en\nen-am\nam-en\nen-bg\nen-fa\nfa-en\nzh-ne\nbg-en\nne-zh\nen-hr\nen-bs\nhr-en\nen-hi\nbs-en\nhi-en\nen-ms\n0.1\n1\n10\n% of corpus\nms-en\nen-rw\nrw-en\nen-ca\nca-en\nen-sl\nsl-en\nen-km\nkm-en\nen-nb\nnb-en\nen-sh\nsh-en\nar-he\nhe-ar\nen-cy\ncy-en\nen-xh\nxh-en\nen-is\nen-uk\nuk-en\nis-en\nen-az\naz-en\nen-sr\nsr-en\nar-de\nde-ar\nen-af\nro-it\nit-ro\naf-en\nen-as\nro-nl\nnl-ro\nas-en\nnl-it\nit-nl\nro-de\nde-ro\nit-de\nde-it\nen-ku\nku-en\nnl-de\nde-nl\nen-ta\nta-en\nen-no\nno-en\nen-be\nbe-en\nen-wa\nwa-en\nen-zu\nzu-en\npa-en\nen-pa\nen-ps\nps-en\nen-te\nte-en\nen-kk\nkk-en\nmy-en\nen-my\nen-fy\nfy-en\nse-en\nen-se\nen-ig\nig-en\nen-mr\nmr-en\nen-li\nli-en\nkn-en\nen-kn\nen-oc\noc-en\nen-yo\nyo-en\nor-en\nen-or\nyi-en\nen-yi\nen-ky\nky-en\nen-tk\ntk-en\nhy-en\nen-hy\nen-gd\ngd-en\nan-en\nen-an\nen-dz\ndz-en\nen-mn\nmn-en\n0.001\n0.01\n0.1\n% of corpus\nFigure 1: The language-pairs distribution of multilingual corpus. All the datasets consist of about 300 million\nsentence pairs.\ncludes 32,000 tokens, primarily comprising En-\nglish and Latin tokens. To augment LLaMA\u2019s pro-\nficiency in handling Chinese text, we expand its\nvocabulary by incorporating additional 6,223 Chi-\nnese tokens as section 3.2 introduced. Furthermore,\nto boost LLaMA\u2019s comprehension across multilin-\ngual parallel datasets, we further extend the vocab-\nulary to 53,613 tokens, most of the added tokens\nare trained on texts spanning 102 languages.\nTo alleviate the issue of vocabulary skew result-\ning from imbalances in instances across the corpus,\nwe implement a strategy that involved selecting a\nsubset of the large-scale parallel dataset for vocab-\nulary training. Specifically, we randomly sample\nmax_num instances from each language to be in-\ncluded in the vocabulary training. This approach\nensures that the common words shared among\n102 languages are adequately represented, serving\nas a crucial prerequisite for model understanding\nmulti-languages. Concretely, we set max_num to\n1,000,000.\n3.3.3\nIncremental Multilingual Pre-training\nPrior to data sampling, we employ multilingual\nvocabulary to segment the entire multilingual par-\nallel corpus. Subsequently, we construct training\nsamples by concatenating the same language sen-\ntence pairs. Each sample is comprised of multiple\nparallel sentence pairs and has a fixed length of\n1,024 tokens. This approach ensured the forma-\ntion of coherent and consistent training samples for\nsubsequent model training.\nTo mitigate the issue of the model disproportion-\nately focusing on learning high-resource corpus\nduring the training phase, which could potentially\nhinder the learning of low-resource languages, we\ndraw inspiration from curriculum learning (Bengio\net al., 2009) to propose an incremental approach\nfor multilingual LLMs pre-training.\nIn this incremental pre-training method, we grad-\nually expose the model to language pairs in a\ncurriculum-like manner.\nInitially, the model is\nexposed to high-resource language pairs, allow-\ning it to establish a solid foundation in those lan-\nguages. Subsequently, we progressively introduce\nlow-resource language pairs, enabling the model to\ngradually expand its knowledge and proficiency in\nthese languages.\nSpecifically, we follow a three-step approach in\nour incremental pre-training method, as shown in\nFigure 2. Firstly, we set the sample interval size\nand divide language pairs into distinct intervals\nbased on the number of instances for each language\npair. Secondly, we calculate the sample mean for\nall language pairs in each interval. Thirdly, we\ndynamically measure the moment of adding the\nlanguage-pair samples next interval according to\nthe sample means in the previous sample interval.\nIn the following part, we detail the three steps.\nIn the first step, we set sample interval size, de-\nnoted as S. We hypothesize that if the interval size\nS is below a specific threshold, we can consider\nthat all language pairs in this interval have average\nresources. Consequently, training the multilingual\nStep 1 \nen-de\nfr-en\nzh-en\n\u2026\nInterval A\nda-en\nzh-my\neu-en\n\u2026\nInterval B\nkk-en\nen-wa\nmr-en\n\u2026\nInterval N\n\u2026\n\u2026\nen-wa\nen-de\nfr-en\nkk-en\nsk-en\neu-en\nda-en\nzh-en\nmr-en\nMultilingual Corpus\nen-de\nfr-en\nzh-en\n\u2026\nInterval A\nda-en\nzh-my\neu-en\n\u2026\nInterval B\nkk-en\nen-wa\nmr-en\n\u2026\nInterval N\n\u2026\nInterval A\nSample Mean\nInterval B\nSample Mean\nInterval N\nSample Mean\nInterval S1\nSample Mean\nInterval S2\nSample Mean\nInterval Sn\nSample Mean\nInterval S1\nInterval S2\nInterval Sn\nBigTranslate\nAlgorithm\nStep 2 \nStep 3 \n\u2026\n\u2026\n\uf06a\n\uf06b\n\uf06c\n\u2026\n\u2026\nFigure 2: The outline of three-step incremental multilingual pre-training approach. \u2460 represents dividing multilin-\ngual language pairs into different intervals, \u2461 denotes calculating sample means for all language pairs within each\nsample interval, \u2462 represents sorting the intervals in descending order based on sample mean values. The algorithm\nin step 3 is detailed in Algorithm 1 for incremental pre-training.\nmodel on this interval enables the model to acquire\nthe average proficiency in each language.\nIn our preliminary experiments, it is observed\nthat an excessively large language interval results\nin an unbalanced learning distribution among lan-\nguage pairs within the interval. Conversely, setting\nthe interval too small leads to the model focusing\non only a few or even just one language within\nthe interval. Additionally, we noticed that high-\nresource and low-resource languages exhibited dif-\nferent sensitivities to interval size. To minimize the\nlearning discrepancy between language pairs, we\ndetermine the interval size for language pairs based\non their respective sample sizes. For language pairs\nwith a sample size exceeding 10,000, we set the\ninterval size S to 10,000. Conversely, for language\npairs with a sample size below 10,000, we opted\nfor a sample interval S of 5,000.\nFor example, the sample size of En-Ro (English-\nto-Romanian) pair is 80,980, and the sample size\nis greater than 10,000, so we set the interval size\nto 10,000 and partition it into the interval [80,000,\n90,000). Otherwise, for the Mr-En (Marathi-to-\nEnglish) language pair, the sample size is 5,080,\nwhich falls below 10,000. In this case, the inter-\nval size is defined as 5,000 and the Mr-En pair is\ncategorized into the interval [5,000, 10,000).\nIn the second step, we calculate the sample mean\nfor all language pairs within each sample interval.\nThe sample mean serves as an approximation of the\nsample size for each language pair in the interval.\nWhile the sample mean cannot substitute the actual\nsample size of each language pair in the interval,\nwe narrow down the interval size to minimize the\ndisparity between the sample size of each language\npair and the sample mean. Then, the sample inter-\nvals will be sorted in descending order based on\nsample mean values.\nFinally, the model is exposed to all sample inter-\nvals following Algorithm 1. Initially, the model is\nexposed to the current sample interval, and dynam-\nically calculates the mean value of the untrained\nsamples in the current sample interval during train-\ning process. When the mean value of the untrained\nsample is not greater than the sample mean of the\nnext interval, we mix the current untrained sample\nand the next interval sample to form a new sam-\nple interval, and train model on the new sample\ninterval. This process is repeated iteratively.\nAssuming that the sample mean value of the sam-\nple interval S1 is 103,251, denoted as MS1, and the\nsample mean value of the sample interval S2 is\n95,280, denoted as MS2. According to Algorithm\n1, BigTranslate will be exposed to all samples in\nAlgorithm 1 Framework of the incremental pre-\ntraining algorithm.\nGiven: The set of sample intervals sorted in de-\nscending order, [S1, S2, ..., Sn];\n1: for Si in [S1, S2, ..., Sn] do\n2:\nSet the sample mean in Si, MSi\n3:\nSet the mean value of the untrained sam-\nples in Si, Mut\n4:\nInitialize Mut \u2190 MSi\n5:\nSet the sample mean in Si+1, MSi+1\n6:\nwhile Mut > MSi+1 do\n7:\nPre-train BigTranslate on Si;\n8:\nCalculate Mut after each step of Pre-\ntraining;\n9:\nend while\n10:\nAdd the untrained samples in Si to Si+1\n11:\nShuffle and Update Si+1\n12: end for\nthe S1 interval for batch training. Following each\ntraining step, we will calculate the mean of the\nuntrained samples in S1, denoted as Mut. With\nthe increase of training steps, Mut will gradually\napproach MS2. When Mut is not greater than MS2\nfor the first time, we will merge the untrained sam-\nples in S1 into S2, and then start training on new\nS2.\nBy adopting this incremental multilingual pre-\ntraining approach, we aim to ensure a more bal-\nanced and comprehensive model learned across dif-\nferent language pairs, thus addressing the challenge\nof uneven resource allocation during training. As a\nresult, the model will successfully achieve mastery\nin 102 languages through its multilingual learning\njourney. We name the multilingually pre-trained\nmodel BigTranslate.\n3.4\nMultilingual Translation Instruction\nTuning\nPrevious work (Mishra et al., 2022; Wei et al.,\n2022; Sanh et al., 2022) has shown that utilizing\ninstruction-following data to tune LLMs enables\nsuch models to understand tasks described in natu-\nral languages, and show better multi-task learning\nability on training tasks and generalization ability\non unseen tasks. Instruction tuning does not inject\nnew capabilities into the model. Instead, its pur-\npose lies in activating the existing capabilities that\nwere established during the training phase.\nIn this section, we construct a multilingual trans-\nlation instruction dataset containing 102 languages\nand 242 language-pairs. Furthermore, we fine-tune\nthe BigTranslate model to unlock the performance\nof model on multilingual machine translation abil-\nity.\nInstruction tuning data construction The mul-\ntilingual translation instruction tuning dataset con-\nsists of 1,000 parallel sentence pairs for each lan-\nguage pair, which is selected from the training set.\nIf the number of sentence pairs in the training set\nis below 1,000, all training examples are chosen\nfor the instruction fine-tuning data. Utilizing this\napproach, the instruction tuning dataset comprises\n241,128 parallel sentence pairs across 242 language\npairs.\nInstruction tuning prompts selection We have\ndesigned a set of 28 multilingual translation\nprompts that encompass various application sce-\nnarios for multilingual translation. We randomly\nselect a prompt from the set for instruction tun-\ning for each parallel sentence. Accordingly, the\ninstruction tuning dataset is scrambled to ensure\nrandomness and diversity.\n3.5\nTraining Details\nThe BigTranslate model is pre-trained on 42.8B\nand 47.0B tokens in the Chinese and Multilingual\npre-training stages, respectively. The learning rates\nare empirically set to 5e-5 under a cosine scheduler\nwith a 3% warm-up percentage, and batch sizes are\n65,536 in all pre-training stages.\nThen, we fine-tune BigTranslate on 240k mul-\ntilingual translation instructions, where the batch\nsize and the number of epochs are set to 32 and 3,\nrespectively. The learning rate and weight decay\nare empirically set to 2e-5 and 0. In inference, we\nemploy beam search for decoding with a beam size\nof 5.\nTo speed up training, we adopted DeepSpeed\n(Rasley et al., 2020) for data parallelization,\nFlashAttention (Dao et al., 2022), and Gradient\nCheckpointing (Chen et al., 2016) for saving mem-\nory. All training processes are conducted on 16\nA100 GPUs with 80GB of RAM.\n4\nExperiments\nTo demonstrate the effectiveness of the BigTrans-\nlate model, we conduct preliminary multilingual\ntranslation experiments on all 102 languages. We\ncompare our model with Google Translate and\nChatGPT2. We conduct the translation experiments\n2We use gpt-3.5-turbo API in May 2023\nfr-en\nca-en\nda-en\nmt-en\nro-en\nde-en\npt-en\nsv-en\nbo-zh\nnb-en\nru-en\naf-en\ngl-en\ncs-en\nen-zh\nes-en\noc-en\nit-en\nnn-en\nnl-en\nid-en\nsk-en\nsr-en\ntr-en\nlv-en\neo-en\nmo-zh\nsh-en\nlt-en\nuy-zh\nhr-en\net-en\nuk-en\nfr-zh\npl-en\nms-en\npt-zh\nes-zh\nbs-en\nzh-en\nar-en\nbg-en\nhu-en\nfi-en\nbn-en\nhe-en\nko-zh\nja-en\nsl-en\nid-zh\nmk-en\nsq-en\n0\n10\n20\n30\n40\n50\n60\n70\nBLEU\nBigTranslate\nChatGPT\nGoogle Translate\nko-en\nhi-en\nga-en\nhi-zh\nne-en\nmg-en\nli-en\nvi-en\nne-zh\nhe-zh\nel-en\neu-en\nur-en\nmy-zh\nmy-en\ngd-en\nis-en\nmr-en\nas-en\ncy-en\ntt-en\nbe-en\ngu-en\naz-en\nxh-en\ntg-en\nml-en\nha-en\nka-en\nkk-en\nkn-en\nps-en\ntk-en\nhy-en\nky-en\nth-en\nka-zh\ndz-en\nte-en\nyo-en\nzu-en\nfa-en\nuz-en\nig-en\nkm-en\npa-en\nsi-en\nta-en\nyi-en\nrw-en\nam-en\nku-en\n0\n10\n20\n30\n40\n50\n60\n70\nBLEU\nBigTranslate\nChatGPT\nGoogle Translate\nFigure 3: An illustrated comparison of 102 languages from X to English or Chinese between BigTranslate, ChatGPT,\nand Google Translate. We sort the language scores in BLEU for BigTranslate in descending order.\npt-en\nfr-en\nro-en\nzh-en\nes-en\nde-en\npl-en\nen-zh\nca-en\nsv-en\nda-en\nit-en\ncs-en\nnl-en\nru-en\nsk-en\nnb-en\ngl-en\nfr-zh\npt-zh\nes-zh\naf-en\nfi-en\nlv-en\nhr-en\nnn-en\nmt-en\nko-zh\nja-en\net-en\nid-en\nbg-en\nsh-en\nko-en\nsl-en\n0\n1\n2\n3\n4\nSCORE\nBigTranslate\nChatGPT\nGoogle Translate\neo-en\nlt-en\nhu-en\nms-en\nuk-en\nid-zh\ntr-en\noc-en\nbs-en\nsr-en\nbo-zh\nar-en\nbn-en\nhi-zh\nmo-zh\nsq-en\nhe-en\nmk-en\nhe-zh\nga-en\nvi-en\nmg-en\nne-en\neu-en\nuy-zh\nel-en\nmy-zh\nur-en\nml-en\nuz-en\nka-zh\ntt-en\nka-en\ntg-en\ngu-en\n0\n1\n2\n3\n4\nSCORE\nBigTranslate\nChatGPT\nGoogle Translate\nFigure 4: An illustrated comparison of 70 languages from X to English or Chinese between BigTranslate, ChatGPT,\nand Google Translate. We sort the language scores in GPT-4 score for BigTranslate in descending order.\nfrom 102 languages to English or Chinese with the\nthree systems3.\n4.1\nDatasets and Evaluation Metrics\nTo assess the effectiveness of the multilingual\nmachine translation model, we conducted evalu-\nations using the devtest subset of the FLORES-200\ndataset4 (Costa-juss\u00e0 et al., 2022). The FLORES-\n200 dataset comprises a corpus extracted from 842\nweb articles encompassing various fields and top-\nics, resulting in a total of 3,001 sentences. Notably,\na team of experts meticulously translated these sen-\ntences into 200 languages, thereby constituting the\nFLORES-200 dataset.\n4.2\nMain Results\nIn this section, we report the main results on multi-\nlingual machine translation on BigTranslate, Chat-\nGPT5, and Google Translate. Then, we report our\nmain findings about the exploration of multilingual\nmachine translation using a large language model.\nAutomatic Evaluation with BLEU Figure 3\nshows the detailed outcomes in BLEU scores of the\ntranslation results from 102 languages into English\nand Chinese. Detailed results for each translation\ndirection are listed in Appendix B. In Figure 3, we\nhave sorted the BLEU scores obtained from the\nBigTranslate model in descending order. We split\nthe whole figure into two parts for clarity. The up-\nper figure presents the first half of all the language\npairs, while the bottom figure presents the second\nhalf. Notably, the upper figure reveals that a signif-\nicant proportion of language pairs in the BigTrans-\nlate model, specifically 46 out of 104 (equivalent to\nover 44%), yield more than 20 BLEU scores. This\nobservation suggests that the BigTranslate model\nexhibits commendable performance with respect\nto these languages, if we believe BLEU is a reli-\nable evaluation metric. We also find that, when\ncomparing our BigTranslate model with ChatGPT,\nthe results demonstrate that BigTranslate performs\non par with ChatGPT in many languages. Intrigu-\ningly, BigTranslate surpasses ChatGPT in 9 lan-\nguage pairs (e.g. mt-en, bo-zh, it-en, mo-zh, uy-zh,\nmg-en, my-zh, my-en, dz-en) in terms of BLEU\nscores. When comparing BigTranslate with Google\n3Since we plan to perform human-like evaluation and we\ncannot well evaluate the languages except English and Chi-\nnese, we just evaluate the translation direction from other\nlanguages to English or Chinese.\n4We select 50 sentences in each direction of the devtest set\nfor evaluation\n5gpt-3.5-turbo API\nTranslate, Figure 3 clearly shows that BigTranslate\nsignificantly lags behind Google Translate in most\nof the language pairs in BLEU scores. However, as\n(Hendy et al., 2023; Anil et al., 2023) pointed out\nthat BLEU is not a good evaluator when the BLEU\nscores exceed a threshold (e.g. 20.0), and it has a\nlow correlation with human preference.\nAutomatic Evaluation with GPT-4 (Liu et al.,\n2023) demonstrates that GPT-4 achieves a much\nhigher Spearman correlation with human than all\nprevious methods on the summarization task. We\nthus follow (Liu et al., 2023), and employ GPT-4\nwith Chain-of-Thoughts (CoT) prompts to automat-\nically evaluate the quality of translation. In the\nevaluation, GPT-4 assigns a score ranging from 0\nto 5 to assess the translation quality of every sen-\ntence across three models. The prompt is a natural\nlanguage instruction and contains two parts. In the\nfirst part, we define the evaluation task and explain\nthe scoring scope. The first part of the prompt we\nuse is below.\nYou will be given two sentences, trans-\nlated sentence is translated from source\nsentence,\nreference sentence is the\nground truth of translation.\nYour task is to rate the translation result\nbetween translated sentence and refer-\nence sentence.\nAssign a score for translation result on a\nscale of 0 to 5, where 0 is the lowest and\n5 is the highest based on the Evaluation\nCriteria.\nIn the second part, the prompt should contain\nspecific evaluation criteria for evaluating machine\ntranslation, which can guide GPT-4 on how to score.\nWe describe evaluation criteria below:\nSemantic similarity refers to the mea-\nsurement of how similar or related two\nsentences are in terms of their meaning\nor semantics. It focuses on capturing\nthe similarity in the underlying concepts,\nideas, or information conveyed by the\nsentences, rather than just the surface-\nlevel lexical or syntactic similarities.\nThe translated sentence can completely\nexpress the meaning of the reference sen-\ntence. The closer the translated sentence\nis to the reference sentence, the higher\nthe score.\nThe style of the translated sentence\nshould be as consistent as possible with\nthe reference sentence\nSubsequently, we calculate the average score\nof all sentences within the same language pairs,\nwhich represents the overall translation score of the\nrespective language pair. In Appendix A, we can\nsee the details of the GPT-4 evaluation prompt.\nWe utilize GPT-4 to rate the translation of 70\nlanguage pairs for three translation models. The\nresults are illustrated in Figure 4 and the detailed\nresults can be found in Appendix C. The figure\nindicates that a total of 28 language pairs exhibit\ngood or excellent translation scores surpassing 3.5,\nthereby accounting for 40% of all language pairs.\nThis result demonstrates that BigTranslate has re-\nmarkable multilingual translation performance un-\nder GPT-4 evaluation. If we compare Figure 4\nagainst Figure 3, we can observe some interesting\nphenomena. For example, the performance gap\nbetween our BigTranslate and Google Translate\nbecomes much narrowed in many language pairs,\nindicating that BigTranslate is approaching Google\nTranslate in dozens of languages in terms of GPT-4\nscore. In comparison to ChatGPT, we can find that\nBigTranslate achieves similar performance to Chat-\nGPT in 27 languages, with a difference in GPT-4\nscores of less than 0.3 points. Moreover, BigTrans-\nlate outperforms ChatGPT in 8 languages in GPT-4\nscores.\n4.3\nDiscussion\nThe experimental results displayed in the previ-\nous section demonstrate the effectiveness of our\nBigTranslate model in extending large language\nmodels to enable multilingual translation over 100\nlanguages. As a multilingual translator, BigTrans-\nlate can be employed in translation for many lan-\nguages, although we still need to further improve\nthe translation quality for extremely low-resource\nlanguages. In addition to language translation, Big-\nTranslate can also be applied in other natural lan-\nguage processing tasks just as ChatGPT does. No-\ntably, the LLM ability especially the English capa-\nbility can be transferred to many other languages\nincluding several low-resource ones with the help\nof our BigTranslate model. For example, BigTrans-\nlate is now good at translating Tibetan and Mon-\ngolian languages, and English and Chinese NLP\nabilities in LLMs can be transferred into these lan-\nguages.\n5\nConclusion and Future Work\nIn this work, we introduced BigTranslate which\nis a large language model equipped with the capa-\nbility of multilingual translation over 100 natural\nlanguages. After two steps of continuing training\nwith massive Chinese monolingual data and large\nscale multilingual parallel data of 102 languages,\nLLaMA is extended to have the potential multilin-\ngual ability on 102 natural languages. Using the\nfinal step of instruction tuning with multilingual\ntranslation data, BigTranslate is obtained. The ex-\nperiments demonstrate that BigTranslate performs\ncomparable to Google Translate and ChatGPT in\nmany languages, and even surpasses ChatGPT in 8\nlanguages when evaluated with GPT-4.\nDue to the issue of unbalanced data, BigTrans-\nlate is still weak in dozens of languages. In the\nfuture, we will extend BigTranslate to further en-\nhance its ability in low-resource languages. More-\nover, we will take full advantage of the multilingual\nability of BigTranslate, and improve the perfor-\nmance of the languages in other natural language\nprocessing tasks.\nAcknowledgements\nWe thank Jinliang Lu, Yu Lu, Yangyifan Xu and\nQian Wang for multilingual translation data con-\nstruction.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nRachel Bawden and Fran\u00e7ois Yvon. 2023. Investigating\nthe translation performance of a large multilingual\nlanguage model: the case of bloom. arXiv preprint\narXiv:2303.01911.\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41\u201348.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022.\nNo language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca. arXiv preprint arXiv:2304.08177.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. 2022.\nFlashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nIn Advances in Neural Information Processing Sys-\ntems, volume 35, pages 16344\u201316359. Curran Asso-\nciates, Inc.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320\u2013335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nYichong Huang, Xiaocheng Feng, Xinwei Geng, and\nBing Qin. 2022. Unifying the convergences in multi-\nlingual neural machine translation. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 6822\u20136835.\nWX Jiao, WX Wang, JT Huang, Xing Wang, and ZP Tu.\n2023. Is chatgpt a good translator? yes with gpt-4 as\nthe engine. arXiv preprint arXiv:2301.08745.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-\nrat, Fernanda B. Vi\u00e9gas, Martin Wattenberg, Greg\nCorrado, Macduff Hughes, and Jeffrey Dean. 2017.\nGoogle\u2019s multilingual neural machine translation sys-\ntem: Enabling zero-shot translation. Trans. Assoc.\nComput. Linguistics, 5:339\u2013351.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.\n2021. Learning language specific sub-network for\nmultilingual machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 293\u2013305.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470\u20133487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nOpenAI. 2022. Introducing chatgpt. OpenAI blog.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. arXiv preprint arXiv:2110.08207.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, Dragomir Radev, Ed-\nuardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan\nKim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard\nDupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady\nElsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris\nAbdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,\nJavier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joy-\ndeep Bhattacharjee, Khalid Almubarak, Kimbo Chen,\nKyle Lo, Leandro Von Werra, Leon Weber, Long\nPhan, Loubna Ben allal, Ludovic Tanguy, Manan\nDey, Manuel Romero Mu\u00f1oz, Maraim Masoud,\nMar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Max-\nimin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,\nMinh Chien Vu, Mohammad A. Jauhar, Mustafa\nGhaleb, Nishant Subramani, Nora Kassner, Nuru-\nlaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona\nde Gibert, Paulo Villegas, Peter Henderson, Pierre\nColombo, Priscilla Amuok, Quentin Lhoest, Rheza\nHarliman, Rishi Bommasani, Roberto Luis L\u00f3pez,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-\nbastian Nagel, Shamik Bose, Shamsuddeen Hassan\nMuhammad, Shanya Sharma, Shayne Longpre, So-\nmaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-\nney Zink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Davut Emre Ta\u00b8sar, Eliz-\nabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee,\nAbheesht Sharma, Andrea Santilli, Antoine Chaffin,\nArnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan Chhablani, Han Wang, Harshit Pandey, Hen-\ndrik Strobelt, Jason Alan Fries, Jos Rozen, Leo\nGao, Lintang Sutawika, M Saiful Bari, Maged S.\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan\nTeehan, Samuel Albanie, Sheng Shen, Srulik Ben-\nDavid, Stephen H. Bach, Taewoon Kim, Tali Bers,\nThibault Fevry, Trishala Neeraj, Urmish Thakker,\nVikas Raunak, Xiangru Tang, Zheng-Xin Yong,\nZhiqing Sun, Shaked Brody, Yallow Uri, Hadar\nTojarieh, Adam Roberts, Hyung Won Chung, Jae-\nsung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois\nLavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina Voloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zden\u02c7ek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Am-\nmar Khan, Amy Faranak, Ana Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Are-\nzoo Abdollahi, Aycha Tammour, Azadeh HajiHos-\nseini, Bahareh Behroozi, Benjamin Ajibade, Bharat\nSaxena, Carlos Mu\u00f1oz Ferrandis, Danish Contrac-\ntor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n\nPeri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc P\u00e0mies, Maria A Castillo, Mari-\nanna Nezhurina, Mario S\u00e4nger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nTh\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023.\nBloom: A 176b-parameter\nopen-access multilingual language model.\narXiv\npreprint arXiv:2211.05100.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715\u20131725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nCheril Shah, Yashashree Chandak, and Manan Suri.\n2023. The geometry of multilingual language mod-\nels: A fairness lens. In The First Tiny Papers Track\nat 11th International Conference on Learning Repre-\nsentations, ICLR 2023, Virtual Event, Kigali Rwanda,\nMay 1-5, 2023. OpenReview.net.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023.\nDocument-level machine translation with large lan-\nguage models. arXiv preprint arXiv:2304.02210.\nQian Wang and Jiajun Zhang. 2022. Parameter differen-\ntiation based multilingual neural machine translation.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 11440\u201311448.\nYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,\nand Chengqing Zong. 2018. Three strategies to im-\nprove one-to-many multilingual translation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018, pages 2955\u2013\n2960. Association for Computational Linguistics.\nYining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai,\nJingfang Xu, and Chengqing Zong. 2019. A com-\npact and language-sensitive multilingual translation\nmethod. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 1213\u20131223.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nWanying Xie, Yang Feng, Shuhao Gu, and Dong Yu.\n2021. Importance-based neuron allocation for multi-\nlingual neural machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 5725\u20135737.\nBright Xu. 2019. Nlp chinese corpus: Large scale chi-\nnese corpus for nlp.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evalua-\ntion benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762\u20134772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1628\u2013\n1639, Online. Association for Computational Linguis-\ntics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nChunting Zhou,\nDaniel Levy,\nXian Li,\nMarjan\nGhazvininejad, and Graham Neubig. 2021. Distri-\nbutionally robust multilingual machine translation.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5664\u20135674.\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\nLingpeng Kong, Jiajun Chen, Lei Li, and Shujian\nHuang. 2023. Multilingual machine translation with\nlarge language models: Empirical results and analy-\nsis. arXiv preprint arXiv:2304.04675.\nA\nGPT-4 prompt for Evaluating Machine Translation\nExample Prompt:\nYou will be given two sentences, translated sentence is translated from source sentence, reference\nsentence is the ground truth of translation.\nYour task is to rate the translation result between translated sentence and reference sentence.\nAssign a score for translation result on a scale of 0 to 5, where 0 is the lowest and 5 is the\nhighest based on the Evaluation Criteria.\nEvaluation Criteria:\nSemantic similarity refers to the measurement of how similar or related two sentences are in\nterms of their meaning or semantics. It focuses on capturing the similarity in the underlying\nconcepts, ideas, or information conveyed by the sentences, rather than just the surface-level\nlexical or syntactic similarities.\nThe translated sentence can completely express the meaning of the reference sentence. The\ncloser the translated sentence is to the reference sentence, the higher the score.\nThe style of the translated sentence should be as consistent as possible with the reference\nsentence\nSample 1:\nTranslated Sentence: {}\nReference Sentence: {}\n...\nSample 5:\nTranslated Sentence: {}\nReference Sentence: {}\nEvaluation Form (Please output score ONLY):\n-Overall rating\nB\nDetailed Results on 102 Languages with BLEU\nDetailed results of our evaluated models on 102 languages with BLEU are shown in Table 1.\nC\nDetailed Results on 70 Languages with GPT-4 Evaluation\nDetailed results of our evaluated models on 70 languages with GPT-4 score are shown in Table 2.\nD\nThe corresponding table of the language code and its name\nInvolved languages and the corresponding language codes in this paper are listed in Table 3.\nTable 1: Detailed results on 102 languages with BLEU\nLanguage pair\nBigTranslate\nChatGPT\nGoogle Translate\nLanguage pair\nBigTranslate\nChatGPT\nGoogle Translate\nfr-en\n41.4\n48.8\n53.8\nko-en\n16.5\n30.2\n31.6\nca-en\n41.2\n43.3\n52.8\nhi-en\n16.3\n27.4\n41.6\nda-en\n41.0\n46.4\n55.6\nga-en\n15.7\n24.2\n46.3\nmt-en\n40.1\n32.5\n62.6\nhi-zh\n15.1\n23.6\n40.7\nro-en\n40.1\n43.3\n53.5\nne-en\n14.4\n24.5\n51.3\nde-en\n39.5\n43.0\n47.3\nmg-en\n13.5\n8.8\n38.3\npt-en\n38.7\n48.6\n54.2\nli-en\n12.7\n30.3\n23.5\nsv-en\n37.3\n43.8\n55.2\nvi-en\n11.6\n33.4\n40.5\nbo-zh\n36.3\n0.5\n0.0\nne-zh\n10.8\n21.2\n0.0\nnb-en\n35.4\n37.5\n46.4\nhe-zh\n10.0\n30.6\n42.9\nru-en\n34.5\n37.9\n43.4\nel-en\n9.2\n32.3\n40.8\naf-en\n34.4\n59.5\n67.2\neu-en\n7.5\n20.1\n38.6\ngl-en\n32.1\n37.4\n48.8\nur-en\n6.7\n20.9\n42.4\ncs-en\n31.4\n39.6\n45.2\nmy-zh\n5.9\n1.6\n38.0\nen-zh\n31.1\n46.6\n54.6\nmy-en\n5.3\n1.5\n38.2\nes-en\n30.9\n32.6\n36.7\ngd-en\n4.1\n17.4\n43.6\noc-en\n30.9\n43.4\n50.8\nis-en\n3.2\n26.1\n38.9\nit-en\n29.4\n28.8\n36.7\nmr-en\n2.8\n15.6\n41.6\nnn-en\n29.2\n43.5\n48.6\nas-en\n2.6\n13.6\n34.2\nnl-en\n28.2\n33.1\n38.4\ncy-en\n2.4\n26.0\n70.2\nid-en\n28.1\n38.5\n46.9\ntt-en\n2.3\n6.5\n36.9\nsk-en\n27.8\n38.8\n46.0\nbe-en\n2.1\n15.4\n27.5\nsr-en\n27.4\n37.1\n53.5\ngu-en\n2.0\n15.6\n42.9\ntr-en\n26.9\n36.3\n46.5\naz-en\n2.0\n17.3\n31.0\nlv-en\n26.4\n36.1\n48.7\nxh-en\n1.9\n7.8\n44.9\neo-en\n25.8\n36.6\n48.5\ntg-en\n1.8\n10.7\n41.6\nmo-zh\n25.7\n0.9\n0.0\nml-en\n1.7\n18.5\n44.0\nsh-en\n25.7\n38.3\n42.6\nha-en\n1.5\n4.2\n35.6\nlt-en\n25.2\n32.6\n41.3\nka-en\n1.3\n8.6\n31.0\nuy-zh\n25.2\n19.3\n48.4\nkk-en\n1.2\n12.8\n40.6\nhr-en\n24.9\n37.8\n42.6\nkn-en\n1.2\n19.8\n35.9\net-en\n24.4\n34.6\n45.5\nps-en\n1.2\n4.8\n34.8\nuk-en\n24.4\n39.7\n47.6\ntk-en\n1.2\n11.5\n38.1\nfr-zh\n24.2\n38.2\n43.0\nhy-en\n1.1\n16.2\n49.9\npl-en\n24.2\n27.8\n33.0\nky-en\n1.1\n6.6\n26.1\nms-en\n23.7\n41.6\n50.5\nth-en\n1.1\n23.6\n33.2\npt-zh\n23.6\n37.6\n42.2\nka-zh\n0.9\n9.9\n36.0\nes-zh\n23.5\n32.3\n36.4\ndz-en\n0.9\n0.2\n0.0\nbs-en\n23.5\n41.6\n50.6\nte-en\n0.9\n13.7\n37.1\nzh-en\n23.4\n29.6\n38.6\nyo-en\n0.9\n3.7\n23.4\nar-en\n22.1\n39.1\n54.1\nzu-en\n0.9\n6.8\n43.2\nbg-en\n21.2\n38.4\n45.8\nfa-en\n0.8\n34.0\n45.4\nhu-en\n21.0\n29.9\n39.0\nuz-en\n0.7\n14.3\n41.9\nfi-en\n20.9\n31.6\n41.0\nig-en\n0.6\n2.4\n34.5\nbn-en\n20.5\n24.6\n41.3\nkm-en\n0.6\n8.3\n32.9\nhe-en\n20.5\n34.8\n55.1\npa-en\n0.6\n22.0\n44.5\nko-zh\n19.6\n29.6\n36.3\nsi-en\n0.6\n3.4\n45.5\nja-en\n19.4\n25.3\n33.7\nta-en\n0.6\n15.3\n43.4\nsl-en\n18.6\n28.2\n37.0\nyi-en\n0.6\n15.6\n60.9\nid-zh\n18.4\n31.5\n43.3\nrw-en\n0.5\n6.1\n35.2\nmk-en\n17.5\n37.2\n52.5\nam-en\n0.3\n2.0\n47.1\nsq-en\n16.6\n33.8\n43.5\nku-en\n0.3\n11.8\n40.2\nTable 2: Detailed results on 70 languages with GPT-4 Evaluation\nLanguage pair\nBigTranslate\nChatGPT\nGoogle Translate\nLanguage pair\nBigTranslate\nChatGPT\nGoogle Translate\npt-en\n4.31\n4.35\n4.43\neo-en\n3.37\n3.89\n4.31\nfr-en\n4.26\n4.28\n4.37\nlt-en\n3.37\n3.86\n4.11\nro-en\n4.24\n4.32\n4.4\nhu-en\n3.34\n4.1\n4.24\nzh-en\n4.19\n4.2\n4.35\nms-en\n3.32\n4.14\n4.28\nes-en\n4.14\n4.18\n4.18\nuk-en\n3.3\n4.19\n4.35\nde-en\n4.11\n4.384\n4.33\nid-zh\n3.28\n3.76\n3.98\npl-en\n4.1\n4.06\n4.19\ntr-en\n3.27\n4.03\n4.19\nen-zh\n4.09\n4.1\n4.13\noc-en\n3.24\n3.99\n4.07\nca-en\n4.08\n4.1\n4.23\nbs-en\n3.21\n4.21\n4.24\nsv-en\n4.06\n4.26\n4.34\nsr-en\n3.19\n4.15\n4.334\nda-en\n4.05\n4.29\n4.38\nbo-zh\n3.0\n0.12\n0.0\nit-en\n4.05\n4.18\n4.14\nar-en\n2.93\n4.08\n4.22\ncs-en\n4.04\n4.14\n4.37\nbn-en\n2.85\n3.52\n4.22\nnl-en\n4.04\n4.23\n4.23\nhi-zh\n2.82\n3.58\n3.82\nru-en\n4.0\n4.24\n4.23\nmo-zh\n2.82\n0.16\n0.0\nsk-en\n3.89\n4.2\n4.36\nsq-en\n2.72\n4.03\n4.35\nnb-en\n3.89\n4.17\n4.26\nhe-en\n2.68\n3.95\n4.226\ngl-en\n3.808\n4.088\n4.32\nmk-en\n2.4\n4.01\n4.29\nfr-zh\n3.8\n4.03\n3.86\nhe-zh\n2.36\n3.65\n4.01\npt-zh\n3.8\n2.78\n4.01\nga-en\n2.29\n3.54\n4.18\nes-zh\n3.78\n3.97\n4.02\nvi-en\n2.26\n4.09\n4.27\naf-en\n3.71\n4.33\n4.32\nmg-en\n2.25\n2.02\n4.05\nfi-en\n3.67\n4.22\n4.22\nne-en\n2.16\n3.61\n4.13\nlv-en\n3.62\n3.97\n4.32\neu-en\n2.07\n3.3\n4.18\nhr-en\n3.61\n4.14\n4.18\nuy-zh\n2.07\n1.87\n4.12\nnn-en\n3.56\n4.26\n4.284\nel-en\n1.38\n4.06\n4.19\nmt-en\n3.54\n3.5\n4.332\nmy-zh\n1.34\n0.64\n3.71\nko-zh\n3.5\n3.78\n3.98\nur-en\n1.18\n3.7\n4.14\nja-en\n3.48\n4.07\n4.2\nml-en\n0.96\n3.44\n4.14\net-en\n3.47\n4.21\n4.24\nuz-en\n0.8\n3.19\n4.33\nid-en\n3.47\n4.16\n4.286\nka-zh\n0.58\n2.22\n3.91\nbg-en\n3.46\n4.26\n4.27\ntt-en\n0.58\n1.62\n4.04\nsh-en\n3.46\n4.15\n4.222\nka-en\n0.54\n2.42\n4.1\nko-en\n3.45\n4.21\n4.16\ntg-en\n0.38\n2.57\n4.16\nsl-en\n3.41\n4.03\n4.22\ngu-en\n0.36\n3.19\n4.23\nTable 3: The corresponding table of the language code and its name\nLanguage code\nLanguage\nLanguage code\nLanguage\nLanguage code\nLanguage\naf\nAfrikaans\nhe\nHebrew\nor\nOriya\nam\nAmharic\nhi\nHindi\npa\nPanjabi\nan\nAragonese\nhr\nCroatian\npl\nPolish\nar\nArabic\nhu\nHungarian\nps\nPashto\nas\nAssamese\nhy\nArmenian\npt\nPortuguese\nast\nAsturian\nid\nIndonesian\nro\nRomanian\naz\nAzerbaijani\nig\nIgbo\nru\nRussian\nbe\nBelarusian\nis\nIcelandic\nrw\nKinyarwanda\nbg\nBulgarian\nit\nItalian\nse\nNorthern Sami\nbn\nBengali\nja\nJapanese\nsh\nSerbo-Croatian\nbo\nTibetan\nka\nGeorgian\nsi\nSinhala\nbr\nBreton\nkk\nKazakh\nsk\nSlovak\nbs\nBosnian\nkm\nCentral Khmer\nsl\nSlovenian\nca\nCatalan\nkn\nKannada\nsq\nAlbanian\ncs\nCzech\nko\nKorean\nsr\nSerbian\ncy\nWelsh\nku\nKurdish\nsv\nSwedish\nda\nDanish\nky\nKyrgyz\nta\nTamil\nde\nGerman\nli\nLimburgan\nte\nTelugu\ndz\nDzongkha\nlt\nLithuanian\ntg\nTajik\nel\nGreek\nlv\nLatvian\nth\nThai\nen\nEngilish\nmg\nMalagasy\ntk\nTurkmen\neo\nEsperanto\nmk\nMacedonian\ntr\nTurkish\nes\nSpanish\nml\nMalayalam\ntt\nTatar\net\nEstonian\nmo\nMongolian\nuk\nUkrainian\neu\nBasque\nmr\nMarathi\nur\nUrdu\nfa\nPersian\nms\nMalay\nuy\nUighur\nfi\nFinnish\nmt\nMaltese\nuz\nUzbek\nfr\nFrench\nmy\nBurmese\nvi\nVietnamese\nfy\nWestern Frisian\nnb\nNorwegian Bokmal\nwa\nWalloon\nga\nIrish\nne\nNepali\nxh\nXhosa\ngd\nGaelic\nnl\nDutch\nyi\nYiddish\ngl\nGalician\nnn\nNorwegian Nynorsk\nyo\nYoruba\ngu\nGujarati\nno\nNorwegian\nzh\nChinese\nha\nHausa\noc\nOccitan\nzu\nZulu\n"
  },
  {
    "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
    "link": "https://arxiv.org/pdf/2305.17306.pdf",
    "upvote": "2",
    "text": "Chain-of-Thought Hub: A Continuous Effort to\nMeasure Large Language Models\u2019 Reasoning Performance\nYao Fu\u2660 Litu Ou\u2660 Mingyu Chen\u2660 Yuhao Wan\u2666 Hao Peng\u2663 Tushar Khot\u2663\n\u2660University of Edinburgh\n\u2666University of Washington\n\u2663Allen Institute for AI\n{yao.fu, s1970716, s2331360}@ed.ac.uk\nyuhaowan@cs.washington.edu\n{haop, tushark}@allenai.org\nhttps://github.com/FranxYao/chain-of-thought-hub\nAbstract\nAs large language models (LLMs) are continu-\nously being developed, their evaluation becomes\nincreasingly important yet challenging. This work\nproposes Chain-of-Thought Hub, an open-source\nevaluation suite on the multi-step reasoning ca-\npabilities of large language models. We are in-\nterested in this setting for two reasons: (1) from\nthe behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a\nkey differentiator between weaker and stronger\nLLMs; (2) we envisage large language models to\nbecome the next-generation computational plat-\nform and foster an ecosystem of LLM-based new\napplications, this naturally requires the founda-\ntion models to perform complex tasks that often\ninvolve the composition of linguistic and logical\noperations. Our approach is to compile a suite\nof challenging reasoning benchmarks to track the\nprogress of LLMs. Our current results show that:\n(1) model scale clearly correlates with reasoning\ncapabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are compara-\nble with GPT-4, while open-sourced models still\nlag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful\nfurther development such as reinforcement learn-\ning from human feedback (RLHF), it has great\npotential to be close to GPT-3.5-Turbo. Our re-\nsults also suggest that for the open-source efforts\nto catch up, the community may focus more on\nbuilding better base models and exploring RLHF.\n1. Introduction\nRecently, the field of AI has been significantly impressed\nby the advances in large language models. LLMs exhibit\nmulti-dimensional capabilities, and their evaluation is chal-\nlenging. Generally, tuning a base language model into a\nchatbot is relatively easy, as demonstrated by the large va-\nriety of LLaMA-based (Touvron et al., 2023) models like\nAlpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023),\nKoala (Geng et al., 2023), Dolly (Databricks, 2023), and so\non. In a chitchat, all these models may perform superficially\nsimilarly to GPT-3.5-Turbo (Gudibande et al., 2023). At the\ncurrent stage, the community is eager to know what are the\nkey factors that clearly differentiate the better-performing\nmodels from the underperforming ones.\nIn this work, we consider the evaluation of complex reason-\ning. As noted by OpenAI (2023b), \u201cIn a casual conversation,\nthe distinction between GPT-3.5 and GPT-4 can be subtle.\nThe difference comes out when the complexity of the task\nreaches a sufficient threshold\u201d. A similar observation is\nmade by the Google PaLM model family, as their develop-\ners discover that large models\u2019 chain-of-thought reasoning\ncapability is clearly stronger than smaller models (Wei et al.,\n2022b;a). These observations indicate that the ability to\nperform complex tasks is a key metric.\nThe capability of performing complex reasoning is crucial\nfor the models to become the next-generation computation\nplatform. One example initiative is LangChain1 where devel-\nopers build applications powered by backend LLM engines,\nwhich generally require the model to perform complex tasks.\nHere the vision of pushing LLMs as the foundation of a new\ncomputational ecosystem also serves as a strong motivation\nto measure the models\u2019 reasoning performance.\nTo incentivize the research efforts in improving language\nmodels\u2019 reasoning performance, we propose the chain-of-\nthought hub (CoT Hub), a continuous open-source effort\nthat tracks LLMs\u2019 reasoning capability using a carefully cu-\nrated evaluation suite. CoT Hub is the first comprehensive\ncomparison of very large LMs on reasoning benchmarks\nand currently consists of 19 major language models\u2019 (in-\ncluding the GPT, Claude, PaLM and LLaMA) performance\non 6 benchmarks and more than 100 subtasks (including\n1https://github.com/hwchase17/langchain\n1\narXiv:2305.17306v1  [cs.CL]  26 May 2023\nChain-of-Thought Hub\nbi-lingual reasoning capabilities in Chinese), and we are\ncontinuously adding new models and datasets.\nObservations made in CoT Hub shed light on many insights\ninto LLM development: (1) the reasoning performance of\nLLMs highly correlates with models\u2019 scales; (2) as of May\n2023, PaLM and Claude2 are the only two model fami-\nlies that are comparable to (yet slightly worse than) the\nGPT model family; (2) LLaMA 65B (Touvron et al., 2023)\nthe strongest open LLM to date, performs closely to code-\ndavinci-002, the base model of GPT-3.5 family3. This indi-\ncates that if aligned properly (by doing supervised finetun-\ning (SFT) and reinforcement learning from human feedback\n(RLHF) right) LLaMA 65B can potentially improve fur-\nther and perform on par with ChatGPT-3.5. We hope\nour work gives meaningful guidance to the community\u2019s\ndevelopment of deployable LLMs.\n2. Method\nIn this section we discuss the construction of Chain-of-\nThought Hub. We first discuss our method for test data\ncollection, then we discuss how we obtain the model per-\nformance on our test suite. Our main goal is to curate a\nhigh-quality collection of datasets that (1) is closely related\nto the actual usage of LLMs; (2) clearly differentiate the\nperformance of stronger and weaker language models. We\nconsider the following datasets:\nGSM8k A widely used math reasoning datasets consist-\ning of 8k problems that jointly test models\u2019 ability of\narithmetic reasoning and composing math steps using\nlanguage (Cobbe et al., 2021).\nMATH A suite of challenging datasets consisting of 12k\nproblems within 7 categories testing the models\u2019 ad-\nvanced math and science reasoning. The problems in\nthis dataset are very hard because they come from math-\nematics competitions written in Latex. Even GPT-4\nhas only 42.5% performance (Hendrycks et al., 2021).\nMMLU An evaluation suite of 15k problems within 57\nsubjects testing model\u2019s high-school and college-level\nknowledge and reasoning (Hendrycks et al., 2020).\nBigBench Hard A suite of language and symbolic rea-\nsoning tasks consisting 6.5k problems within 23 sub-\nsets that are particularly suitable for testing chain-of-\nthought prompting (Suzgun et al., 2022).\nHumanEval A handwritten dataset of 164 Python program-\nming problems with text comments and docstrings test-\ning the models\u2019 coding ability (Chen et al., 2021).\n2https://www.anthropic.com/index/introducing-claude\n3https://platform.openai.com/docs/model-index-for-\nresearchers\nC-Eval A Chinese evaluation suite for foundation models\nconsisting of 13k multi-choice questions spanning 52\ndiverse disciplines and four difficulty levels (Huang\net al., 2023).\nWe note that most of these datasets are already used in\nthe evaluation of leading large language models, such as\nGPT-4 (OpenAI, 2023a) and PaLM-2 (Anil et al., 2023).\nFew-Shot Chain-of-thought Prompting\nWe use few-\nshot chain-of-thought prompting to evaluate LLMs. This\nmarks a clear difference between our evaluation and the\nmajority of other concurrent evaluations like HeLM (Liang\net al., 2022), as most of them use answer-only prompting.\nWe also emphasize that we use few-shot, rather than zero-\nshot prompting, because few-shot is a capability that exist\nin both pretrained and instruction-tuned checkpoints, v.s.,\nzero-shot is more suitable for instruction-tuned checkpoints\nand may under-estimate the pretrained checkpoints.\nComparison to existing and concurrent work\nThere\nare many great existing evaluation suites for large language\nmodels, such as HeLM, Chatbot Arena4, and Open LLM\nLeaderboard5. The major difference between this work\nand these works are: (1) HeLM evaluates a significantly\nwider spectrum of tasks while we focus on evaluating rea-\nsoning. Most of the results from this work use chain-of-\nthought prompting (hence the name \u201cChain-of-Thought\nHub\u201d) whereas HeLM mainly uses answer-only prompting\n(without CoT). (2) Chatbot Arena evaluate the dialog user\npreference we evaluate reasoning. (3) Open LLM Leader-\nboard focus on open-sourced LLMs, we jointly consider\nmajor LLMs, either open-sourced or not.\nUsing final answer accuracy as a proxy for reasoning\ncapability\nMost of the datasets we consider share one\npattern: to reach the final answer (either a number for math\nproblems, a choice for multi-choice problems, or a fixed\noutput for coding), the model needs to figure out the inter-\nmediate steps toward that answer. When evaluating, we only\nuse the final answer accuracy but do not consider the correct-\nness of intermediate steps. This is because empirically, the\ncorrectness of intermediate steps is strongly correlated with\nthe final accuracy. If the intermediate steps are very wrong,\nthe model is less likely to reach the final answer. If the final\nanswer is correct, the intermediate steps are generally good\nenough (Wei et al., 2022b; Lewkowycz et al., 2022).\n3. Experiments\nFirst we discuss the model families we consider. We focus\non the popular models in production, including GPT, Claude,\nPaLM, LLaMA, and T5 model families, specifically:\n4https://leaderboard.lmsys.org/\n5https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard\n2\nChain-of-Thought Hub\nTable 1. Overall model performance on Chain-of-Thought Hub. Numbers with an asterisk* are from our test scripts. For model types,\nbase means the model checkpoint after pretraining, SIFT means supervised instruction finetuning. Others are from their corresponding\npapers. We observe: (1) there exist a gap between leading LLMs (GPT, Claude and PaLM) and open-source (LLaMA and FlanT5);\n(2) most leading LLMs are after RLHF, indicating the opportunity of improving open-sourced models using this technique; (3). model\nperformance is generally correlated with model scale, indicating further opportunities in scaling, especially for open-source models. We\nfurther highlight that among open-sourced models, LLaMA 65B performs close to code-davinci-002, the base model of ChatGPT. This\nsuggests that if RLHF is done right on LLaMA 65B, it may become close to ChatGPT.\nModel\n#Params\nType\nGSM8k\nMATH\nMMLU\nBBH\nHumanEval\nC-Eval\nGPT-4\n?\nRLHF\n92.0\n42.5\n86.4\n-\n67.0\n68.7*\nclaude-v1.3\n?\nRLHF\n81.8*\n-\n74.8*\n67.3*\n-\n54.2*\nPaLM-2\n?\nBase\n80.7\n34.3\n78.3\n78.1\n-\n-\ngpt-3.5-turbo\n?\nRLHF\n74.9*\n-\n67.3*\n70.1*\n48.1\n54.4*\nclaude-instant-v1.0\n?\nRLHF\n70.8*\n-\n-\n66.9*\n-\n54.9*\ntext-davinci-003\n?\nRLHF\n-\n-\n64.6\n70.7\n-\n-\ncode-davinci-002\n?\nBase\n66.6\n19.1\n64.5\n73.7\n47.0\n-\nMinerva\n540B\nSIFT\n58.8\n33.6\n-\n-\n-\n-\nFlan-PaLM\n540B\nSIFT\n-\n-\n70.9\n66.3\n-\n-\nFlan-U-PaLM\n540B\nSIFT\n-\n-\n69.8\n64.9\n-\n-\nPaLM\n540B\nBase\n56.9\n8.8\n62.9\n62.0\n26.2\n-\ntext-davinci-002\n?\nSIFT\n55.4\n-\n60.0\n67.2\n-\n-\nPaLM\n64B\nBase\n52.4\n4.4\n49.0\n42.3\n-\n-\nLLaMA\n65B\nBase\n50.9\n10.6\n63.4\n-\n23.7\n38.8*\nLLaMA\n33B\nBase\n35.6\n7.1\n57.8\n-\n21.7\n-\nLLaMA\n13B\nBase\n17.8\n3.9\n46.9\n-\n15.8\n-\nFlan-T5\n11B\nSIFT\n16.1*\n-\n48.6\n41.4\n-\n-\nLLaMA\n7B\nBase\n11.0\n2.9\n35.1\n-\n10.5\n-\nFlan-T5\n3B\nSIFT\n13.5*\n-\n45.5\n35.2\n-\n-\nOpenAI GPT including GPT-4 (currently strongest), GPT-\n3.5-Turbo (faster but less capable), text-davinci-003,\ntext-davinci-002, and code-davinci-002 (important pre-\nvious versions before Turbo). See Fu & Khot (2022)\nfor a comprehensive discussion.\nAnthropic Claude including claude-v1.3 (slower but more\ncapable) and claude-instant-v1.0 (faster but less capa-\nble)6. Strong competitor\u2019s GPT models.\nGoogle PaLM including\nPaLM,\nPaLM-2,\nand\ntheir\ninstruction-tuned versions (FLan-PaLM and Flan-U-\nPaLM). Strong base and instruction-tuned models.\nMeta LLaMA including the 7B, 13B, 33B and 65B vari-\nants. Important open-sourced base models.\nGoogle FlanT5 instruction-tuned T5 models demonstrat-\ning strong performance in the smaller model regime.\nWe report these models\u2019 performance on our CoT Hub suite.\nWe note that due to the wide spectrum of the tasks and\nmodels we consider, the evaluation is nontrivial and even\nrunning inference takes effort. In addition, there are models\n6https://console.anthropic.com/docs/api/reference\nthat do not offer public access (such as PaLM), such that\nevaluating them is difficult. For these reasons, we report\nnumbers using the following strategy: if the performance\nof a model is already reported in a paper, we refer to that\npaper; otherwise, we test them by ourselves. Note that this\nstrategy is not comprehensive, as we still have a fraction of\nuntested non-public models on some datasets. This is partly\nthe reason we view our CoT Hub as a continuous effort.\nTable 1 shows the overall results. We rank the models using\nGSM8k performance because it is a classical benchmark\ntesting models\u2019 reasoning capabilities. Numbers marked\nby an asterisk are tested by ourselves, others are from the\nfollowing sources: GPT-4 and PaLM-2 results are from their\ntechnical report (OpenAI, 2023a; Anil et al., 2023) respec-\ntively; GPT-3.5-Turbo\u2019s performance on HumanEval is also\nfrom OpenAI (2023a). Text-davinci-003, code-davinci-002\nand text-davinci-002 performance are from the appendix\nin Chung et al. (2022) and from Fu et al. (2022). Minerva\u2019s\nperformance is from Lewkowycz et al. (2022). PaLM\u2019s per-\nformance is from Chowdhery et al. (2022). Flan-PaLM and\nFlanT5 performance are from Chung et al. (2022). LLaMA\u2019s\nperformance is from Touvron et al. (2023).\nThe gap between open-source and leading LLMs\nIn\n3\nChain-of-Thought Hub\nFigure 1. X-axis means the log of the model scale measured in billion parameters. We observe that model performance is generally\ncorrelated with scale, approximately showing a log-linear trend. Models without disclosing their scale generally perform better than\nmodels disclosing scale information. Our observations also indicate that the open-source community may still needs to explore/ figure out\n\u201cthe moat\u201d about the scaling and RLHF for further improvements.\ngeneral, we observe a performance discrepancy between\nopen-sourced models (like LLaMA and FlanT5) and close-\nsourced models (GPT, Claude and PaLM). Importantly, the\nperformance of open-sourced models seems to be upper\nbounded by LLaMA 65B.\nLeading LLMs are after RLHF\nWe observe that ex-\ncept for PaLM-2, the top 6 models on the leaderboard are\nafter reinforcement learning from human feedback. This\nstrongly indicates the effectiveness of RLHF. Given that\nRLHF is still an underexplored area, we strongly encourage\nthe community to study more on this topic.\nCorrelation between model scale and reasoning\nWe\nfurther study the relationship between model scale and mod-\nels\u2019 reasoning performance by visualizing model perfor-\nmance against model scale. Results are shown in Fig. 1.\nWe see that: (1) generally, model performance is correlated\nwith model scale, showing approximately a log-linear trend;\n(2) models that do not disclose their scale generally perform\nbetter than models that do, indicating that there is still a gap\nbetween open-source and close-source.\nOn the potential of LLaMA-65B\nFinally, we would\nlike to highlight the impressive performance of LLaMA\n65B. On MMLU it is close to code-davinci-002, the base\nmodel of GPT-3.5 series. On GSM8k, it is worse (pre-\nsumably because it is not trained on code) but close and\nmuch better than other open-sourced models (presumably\nbecause it is trained to Chinchilla-optimal Hoffmann et al.,\n2022). Combining this observation with the fact that GPT-\n3.5-Turbo (ChatGPT) is an RLHF model based on code-\ndavinci-002, it may be possible to reproduce ChatGPT\nbased on LLaMA 65B by applying the RLHF techniques\ndiscussed in DeepMind Sparrow (Glaese et al., 2022) and\nAnthropic Claude (Askell et al., 2021; Bai et al., 2022a;b).\n4. Conclusion and Future Work\nIn this work, we propose Chain-of-Thought Hub, an open-\nsource, continuous effort to measure the reasoning capability\nof very large language models. Our results clearly show the\nperformance differences between smaller and larger models,\nand between close-source and open-source models.\nAfter carefully examining the results, we show two impor-\ntant directions for further improving open-sourced models:\nbuilding better base models and exploring RLHF. We also\npoint out the great potential of LLaMA 65B: if aligned\nproperly by better SFT and RLHF, it could be possible to\nperform on par with ChatGPT-3.5.\nIn the future, we plan to further extend CoT Hub by: (1) in-\ncluding more carefully chozen reasoning datasets, especially\ndatasets measuring commonsense reasoning, math theorem\nproving, and the ability to call outside APIs; (2) including\nmore language models, such as LLaMA-based, instruction-\nfinetuned models like Vicuna7 and models through API\naccess like Cohere8 and PaLM-2 chat-bison-0019. (3) ex-\nploring methods for solving MATH, the probably most chal-\nlenging datasets (recall that it consists of math- ematics com-\npetitions written in Latex), by calling APIs that compute\nsymbolic and numerical calculus (like Wolfram Alpha10).\nIn summary, we believe our work serves as an evaluation\nplatform that guides the development of open-source large\nlanguage models.\n7https://lmsys.org/blog/2023-03-30-vicuna/\n8https://cohere.com/generate\n9https://cloud.google.com/vertex-ai\n10https://www.wolframalpha.com/\n4\nChain-of-Thought Hub\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,\nHenighan, T., Jones, A., Joseph, N., Mann, B., DasSarma,\nN., et al. A general language assistant as a laboratory for\nalignment. arXiv preprint arXiv:2112.00861, 2021.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022a.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\nnon, C., et al. Constitutional ai: Harmlessness from ai\nfeedback. arXiv preprint arXiv:2212.08073, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nDatabricks.\nFree dolly:\nIntroducing the world\u2019s\nfirst\ntruly\nopen\ninstruction-tuned\nllm.\nBlog\npost,\nApril\n2023.\nURL\nhttps://www.\ndatabricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm.\nFu, Yao; Peng, H. and Khot, T.\nHow does gpt obtain\nits ability?\ntracing emergent abilities of language\nmodels to their sources.\nYao Fu\u2019s Notion, Dec\n2022.\nURL https://yaofu.notion.site/\nHow-does-GPT-Obtain-its-Ability-Tracing-Emergent-\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\nComplexity-based prompting for multi-step reasoning.\narXiv preprint arXiv:2210.00720, 2022.\nGeng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel,\nP.,\nLevine,\nS.,\nand Song,\nD.\nKoala:\nA dia-\nlogue model for academic research.\nBlog post,\nApril 2023. URL https://bair.berkeley.edu/\nblog/2023/04/03/koala/.\nGlaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu,\nV., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M.,\nThacker, P., et al.\nImproving alignment of dialogue\nagents via targeted human judgements. arXiv preprint\narXiv:2209.14375, 2022.\nGudibande, A., Wallace, E., Snell, C., Geng, X., Liu,\nH., Abbeel, P., Levine, S., and Song, D.\nThe false\npromise of imitating proprietary llms. arXiv preprint\narXiv:2305.15717, 2023.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. NeurIPS,\n2021.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Lei, J., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. arXiv preprint arXiv:2305.08322, 2023.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\nMichalewski, H., Ramasesh, V., Slone, A., Anil, C.,\nSchlag, I., Gutman-Solo, T., et al. Solving quantitative\nreasoning problems with language models. arXiv preprint\narXiv:2206.14858, 2022.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D.,\nYasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar,\nA., et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022.\n5\nChain-of-Thought Hub\nOpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023a.\nOpenAI. Gpt-4, 2023b. URL https://openai.com/\nresearch/gpt-4.\nSuzgun, M., Scales, N., Sch\u00a8arli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\nStanford\nalpaca:\nAn\ninstruction-following\nllama\nmodel.\nhttps://github.com/tatsu-lab/\nstanford_alpaca, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022a.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022b.\n6\n"
  },
  {
    "title": "Fine-Tuning Language Models with Just Forward Passes",
    "link": "https://arxiv.org/pdf/2305.17333.pdf",
    "upvote": "1",
    "text": "Fine-Tuning Language Models with Just\nForward Passes\nSadhika Malladi\u2217\nTianyu Gao\u2217\nEshaan Nichani\nAlex Damian\nJason D. Lee\nDanqi Chen\nSanjeev Arora\nPrinceton University\n{smalladi, tianyug, eshnich, ad27, jasonlee, danqic, arora}@princeton.edu\nAbstract\nFine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient zeroth-\norder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-\nplace, thereby fine-tuning LMs with the same memory footprint as inference. For\nexample, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream\ntasks (classification, multiple-choice, and generation). Our results demonstrate\nthat (1) MeZO significantly outperforms in-context learning and linear probing;\n(2) MeZO achieves comparable performance to fine-tuning with backpropagation\nacross multiple tasks, with up to 12\u00d7 memory reduction and up to 2\u00d7 GPU-hour\nreduction in our implementation; (3) MeZO is compatible with both full-parameter\nand parameter-efficient tuning techniques such as LoRA and prefix tuning; (4)\nMeZO can effectively optimize non-differentiable objectives (e.g., maximizing\naccuracy or F1). We support our empirical findings with theoretical insights,\nhighlighting how adequate pre-training and task prompts enable MeZO to fine-tune\nhuge models, despite classical ZO analyses suggesting otherwise.1\n1\nIntroduction\nFine-tuning pre-trained language models (LMs) has been the dominant methodology for solving many\nlanguage tasks [28], adapting to specialized domains [42], or incorporating human instructions and\npreferences [73]. However, as LMs are scaled up [13, 72], computing gradients for backpropagation\nrequires a prohibitive amount of memory \u2013 in our test, up to 12\u00d7 the memory required for inference \u2013\nbecause it needs to cache activations during the forward pass, gradients during the backward pass,\nand, in the case of Adam [52], also store gradient history (see Section 3.4 for a detailed analysis).\nAs a result, while it is possible to run inference with a 30-billion (30B) parameter LM on a single\nNvidia A100 GPU (with 80GB memory), backpropagation with Adam is feasible only for a 2.7B LM.\nParameter-efficient fine-tuning methods (PEFT [46, 57, 54]) update just a fraction of the network\n\u2217Equal contribution and corresponding authors.\n1Our code is available at https://github.com/princeton-nlp/MeZO.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.17333v3  [cs.LG]  11 Jan 2024\nSST-2\nRTE\nCB\nBoolQ\nWSC\nWIC\nMultiRC\nCopa\nReCoRD\nSQuAD\nDROP\n10\n30\n50\n70\n90\nAccuracy/F1 (%)\nZero-shot\nICL\nMeZO\nFT (12x memory)\nFigure 1: OPT-13B results with zero-shot, in-context learning (ICL), MeZO (we report the best\namong MeZO/MeZO (LoRA)/MeZO (prefix)), and fine-tuning with Adam (FT). MeZO demonstrates\nsuperior results over zero-shot and ICL and performs on par with FT (within 1%) on 7 out of 11 tasks,\ndespite using only 1/12 memory. See Table 1 for detailed numbers and Figure 3 for memory profiling.\nparameters, but still need to cache many activations, because the tuned parameters are scattered\nthroughout the model. In our tests, fine-tuning an OPT-13B model with full parameter tuning or\nPEFT requires 12\u00d7 and 6\u00d7 more memory than inference respectively.\nIn-context learning (ICL [13]) has allowed solving many tasks with a single inference pass, during\nwhich the model processes labeled examples (demonstrations) in its context and then outputs a\nprediction on a test example. While this allows for quick adaptation of the model to specific use cases,\ncurrent models allow a limited context size (and thus, limited demonstrations) and the performance\nis sensitive to the formatting and choice of demonstrations [60, 66]. ICL can slow with the number\nof demonstrations, and it often performs worse than fine-tuning of medium-sized models [13].\nBackpropagation also cannot optimize non-differentiable criteria, which have gained popularity in\nfine-tuning LMs according to human preference scores or set safety standards [89, 73]. Typically,\nthese adaptations involve expensive reinforcement learning from human feedback (RLHF [20]).\nA classical zeroth-order optimization method, ZO-SGD [88], uses only differences of loss values\nto estimate the gradients. Thus, in principle, the method can update neural networks with just\nforward passes, though naive implementation still doubles the memory overhead and classical lower\nbounds [69, 32] suggest that convergence slows linearly with model size. As such, ZO methods have\nbeen applied in deep learning settings to find adversarial examples or tune input embeddings [91, 90]\nbut not to directly optimize large-scale models (see Liu et al. [61] for a survey).\nIn this work, we propose a memory-efficient zeroth-order optimizer (MeZO), which adapts the\nclassical ZO-SGD algorithm and reduces its memory consumption to the same as inference. We\napply MeZO to fine-tune large LMs and show that, both empirically and theoretically, MeZO can\nsuccessfully optimize LMs with billions of parameters. Specifically, our contributions are:\n1. In MeZO, we adapt the ZO-SGD algorithm [88] and a number of variants to operate in-place on\narbitrarily large models with almost no memory overhead (see Algorithm 1 and Section 2).\n2. We conduct comprehensive experiments across model types (masked LM and autoregressive\nLM), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice,\nand generation). MeZO consistently outperforms zero-shot, ICL, and linear probing. Moreover,\nwith RoBERTa-large, MeZO achieves performance close to standard fine-tuning within 5%\ngap; with OPT-13B, MeZO outperforms or performs comparably to fine-tuning on 7 out of 11\ntasks, despite requiring roughly 12\u00d7 less memory (Figure 1 and Section 3). In our implemen-\ntation, MeZO requires only half as many GPU-hours as Adam fine-tuning for a 30B model\n(see Appendix F.6).\n3. We demonstrate MeZO\u2019s compatibility with full-parameter tuning and PEFT (e.g., LoRA [46]\nand prefix-tuning [57]) in Section 3.\n4. Further exploration showcases that MeZO can optimize non-differentiable objectives such as\naccuracy or F1 score, while still requiring only the same memory as inference (Section 3.3).\n5. Our theory suggests that adequate pre-training ensures the per-step optimization rate (Theorem 1)\nand global convergence rate (Lemma 3) of MeZO depend on a certain condition number of the\nlandscape (i.e., the local effective rank, see Assumption 1) instead of numbers of parameters. This\n2\nAlgorithm 1: MeZO\nRequire: parameters \u03b8 \u2208 Rd, loss L : Rd \u2192 R, step budget T, perturbation scale \u03f5, batch size\nB, learning rate schedule {\u03b7t}\nfor t = 1, ..., T do\nSample batch B \u2282 D and random seed s\n\u03b8 \u2190 PerturbParameters(\u03b8, \u03f5, s)\n\u2113+ \u2190 L(\u03b8; B)\n\u03b8 \u2190 PerturbParameters(\u03b8, \u22122\u03f5, s)\n\u2113\u2212 \u2190 L(\u03b8; B)\n\u03b8 \u2190 PerturbParameters(\u03b8, \u03f5, s)\n\u25b7 Reset parameters before descent\nprojected_grad \u2190 (\u2113+ \u2212 \u2113\u2212)/(2\u03f5)\nReset random number generator with seed s\n\u25b7 For sampling z\nfor \u03b8i \u2208 \u03b8 do\nz \u223c N(0, 1)\n\u03b8i \u2190 \u03b8i \u2212 \u03b7t \u2217 projected_grad \u2217 z\nend\nend\nSubroutine PerturbParameters(\u03b8, \u03f5, s)\nReset random number generator with seed s\n\u25b7 For sampling z\nfor \u03b8i \u2208 \u03b8 do\nz \u223c N(0, 1)\n\u03b8i \u2190 \u03b8i + \u03f5z\n\u25b7 Modify parameters in place\nend\nreturn \u03b8\nresult is in sharp contrast to existing ZO lower bounds [69, 32] suggesting that the convergence\nrate can slow proportionally to the number of parameters (Section 4).\n2\nZeroth-order optimization\nZeroth-order (ZO) optimizers have long been studied in the context of convex and strongly convex\nobjectives. In the following, we first introduce a classical ZO gradient estimator, SPSA (Defini-\ntion 1 [88]) and the corresponding SGD algorithm, ZO-SGD (Definition 2). Then we describe\nMeZO, our in-place implementation that requires the same memory as inference in Section 2.1 and\nAlgorithm 1. We highlight that SPSA can also be used in more complex optimizers, such as Adam,\nand we provide memory-efficient implementations for those algorithms too (Section 2.2).\nConsider a labelled dataset D = {(xi, yi)}i\u2208[|D|] and a minibatch B \u2282 D of size B, we let L(\u03b8; B)\ndenote the loss on the minibatch. We introduce a classical ZO gradient estimate in this setting.2\nDefinition 1 (Simultaneous Perturbation Stochastic Approximation or SPSA [88]). Given a model\nwith parameters \u03b8 \u2208 Rd and a loss function L, SPSA estimates the gradient on a minibatch B as\nb\u2207L(\u03b8; B) = L(\u03b8 + \u03f5z; B) \u2212 L(\u03b8 \u2212 \u03f5z; B)\n2\u03f5\nz \u2248 zz\u22a4\u2207L(\u03b8; B)\n(1)\nwhere z \u2208 Rd with z \u223c N(0, Id) and \u03f5 is the perturbation scale. The n-SPSA gradient estimate\naverages b\u2207L(\u03b8; B) over n randomly sampled z.\nSPSA requires only two forward passes through the model to compute the gradient estimate (for\nn-SPSA, each estimate requires 2n forward passes). As \u03f5 \u2192 0, the SPSA estimate can be understood\nas a rank-1 reconstruction of the gradient. During training, n can be treated as a hyperparameter and\nfollow a schedule [11, 15], though in cursory experiments (Appendix A), n = 1 is the most efficient.\n2The original SPSA algorithm [88] perturbs the model by 1/z and thus requires that z has finite inverse\nmoments, precluding the choice of z as Gaussian. 1/z is very large with high probability for a zero-mean\nGaussian z, so we adopt the standard in many theoretical [70, 32] and empirical [64] works and perturb the\nparameters by z with z as a Gaussian random variable.\n3\nWe use n = 1 as the default. It is widely known that the SPSA estimate can be used to replace the\nbackpropagation gradient in any optimizer such as SGD.\nDefinition 2 (ZO-SGD). ZO-SGD is an optimizer with learning rate \u03b7 that updates parameters as\n\u03b8t+1 = \u03b8t \u2212 \u03b7 b\u2207L(\u03b8; Bt) where Bt is the minibatch at time t and b\u2207L is the SPSA gradient estimate.\n2.1\nMemory-efficient ZO-SGD (MeZO)\nThe vanilla ZO-SGD algorithm costs twice the memory of inference, as it needs to store z \u2208 Rd. We\npropose a memory-efficient implementation of ZO-SGD called MeZO, as illustrated in Algorithm 1.\nAt each step, we first sample a random seed s, and then for each of z\u2019s four uses in Algorithm 1, we\nreset the random number generator by s and resample the relevant entry of z. Using this in-place\nimplementation, MeZO has a memory footprint equivalent to the inference memory cost.\nWe note that Algorithm 1 describes perturbing each parameter separately, which may be time-\nconsuming for large models. In practice, we can save time by perturbing an entire weight matrix\ninstead of each scalar independently. This incurs an additional memory cost as large as the largest\nweight matrix; usually, this is the word embedding matrix (e.g., 0.86GB for OPT-66B).\nStorage Efficiency of MeZO.\nParameter-efficient fine-tuning (PEFT) techniques fine-tune just a\nfraction of the network parameters and have thus been proposed as a way to reduce the storage costs\nof fine-tuned model checkpoints. Fine-tuning with MeZO reduces the storage cost of the resulting\ncheckpoint far more than popular PEFT techniques (e.g., LoRA [46] and prefix tuning [57]). We\nreconstruct the MeZO trajectory using a single seed, which spawns step-wise seeds to sample z, and\nthe projected_grad at each step.3 As such, for fine-tuning a 66B model, MeZO requires saving\nthe seed plus 20,000 (steps) \u00d7 2 bytes, which is less than 0.1MB. LoRA fine-tunes 19M parameters\nand requires 38MB storage, and prefix tuning fine-tunes 6M parameters and requires 12MB storage.\n2.2\nMeZO extensions\nWe note that SPSA is a popular ZO gradient estimator but not the only one. Many one-point gradient\nestimators have been proposed in past works [34, 87, 95], and using such estimators in place of SPSA\nwould halve the training time. However, cursory experiments with one such promising estimator\n[113] reveal that these are not as efficient as SPSA when fixing the number of forward passes\n(Appendix B.5). As such, we implement MeZO with the SPSA estimator.\nMeZO can also be combined with other gradient-based optimizers, including SGD with momentum\nor Adam. Though naive implementation would require additional memory to store the gradient\nmoment estimates, MeZO-momentum and MeZO-Adam alleviate such overhead by recomputing the\nmoving average of the gradients using saved past losses and z (see Appendix B for a full discussion).\nWe also note that all of the coordinates of the SPSA gradient estimate have the same scale, but\ndeep Transformers can have gradients of different scales for each layer [59, 61]. As such, we draw\ninspiration from layerwise adaptive optimizers [109, 110] to design several MeZO variants. Cursory\nexperiments showed that these algorithms are not more efficient (in terms of forward passes), but we\nnevertheless present them as potential optimizers for more complex objectives. See Appendix B.\nForward Auto-Differentiation\nNote that z\u22a4\u2207L(\u03b8; B) is a Jacobian-vector product (JVP), which\ncan be computed in parallel with an inference pass with excess memory consumption equivalent to\nthat of the largest activation in the network [40]. In this case, z must be stored on the GPU in order to\nconstruct the gradient estimate, so this procedure requires slightly more than two times the memory\nneeded for inference. We analyze this algorithm in detail in Appendix D. Note that using a non-zero\n\u03f5 in SPSA, which is not possible through the JVP method, may boost generalization by promoting a\nsharpness-minimizing term. Past works (e.g., Baydin et al. [9]) have also studied JVP-based training\nbut achieved limited empirical success.\n3Note that this reconstruction requires no additional forward passes through the model and no access to the\ndata used during fine-tuning, since projected_grad implicitly encodes this information.\n4\nSST-2\nSST-5\nSNLI\nMNLI\nRTE\nTREC\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nk=16 RoBERTa-large\nSST-2\nSST-5\nSNLI\nMNLI\nRTE\nTREC\n30\n40\n50\n60\n70\n80\n90\n100\nk=512 RoBERTa-large\nZero-shot\nMeZO (prefix)\nLP\nFT\nMeZO\nFT (LoRA)\nMeZO (LoRA)\nFT (prefix)\nFigure 2: Experiments on RoBERTa-large. We report zero-shot, linear probing (LP), and MeZO and\nfine-tuning (FT) with full parameter, LoRA, and prefix-tuning. MeZO outperforms zero-shot and LP\nand approaches FT (within 5% for k = 512) with much less memory. Detailed numbers in Table 18.\n3\nExperiments\nPreliminary experiments (Appendix A) show that MeZO only works when using prompts [13, 84, 35].\nPast works [83, 67] have demonstrated how the inclusion of a suitable prompt ensures the fine-tuning\nobjective is closely related to the pre-training one. In Section 4, we extend these ideas to show how\nusing a simple prompt simplifies the fine-tuning optimization procedure, thereby enabling zeroth\norder methods to work efficiently. All experiments below use prompts detailed in Appendix E.2. All\nfine-tuning with backpropagation (FT) experiments follow convention and use Adam, though we also\nreport results when performing FT with SGD in Appendix F.\nWe conduct comprehensive experiments on both medium-sized masked LMs (RoBERTa-large,\n350M [65]) and large autoregressive LMs (OPT-13B, 30B, 66B [112]) in few-shot and many-shot\nsettings with prompts. We also explore both full-parameter tuning and PEFT including LoRA [46]\nand prefix-tuning [57] (see Appendix E.5 for details). We compare MeZO with zero-shot, in-context\nlearning (ICL), linear-probing (LP), and fine-tuning with Adam (FT). MeZO uses substantially less\nmemory than FT but requires significantly more training steps.\nWe first show that MeZO improves substantially over zero-shot, ICL, and LP across model types, sizes,\nand task types. Moreover, MeZO performs comparably to FT over a number of tasks, while drastically\nreducing the memory cost by, for example, 12\u00d7 on OPT-13B. Further experiments demonstrate that\nMeZO can optimize non-differentiable objectives, such as accuracy and F1 score (Section 3.3). We\ncompare the memory consumption of ICL, FT, LP, and MeZO in Figures 3 and 4.\n3.1\nMedium-sized masked language models\nWe conduct experiments with RoBERTa-large on sentiment classification, natural language inference,\nand topic classification tasks. We follow past works [35, 67] in studying the few-shot and many-shot\nsettings, sampling k examples per class for k = 16 and k = 512 (details in Appendix E). We run\nMeZO for 100K steps and fine-tuning for 1000 steps, noting that one MeZO step is substantially\nfaster than one fine-tuning step (see Appendix F.6 for a comparison). We summarize the results from\nFigure 2 and Table 18 below.\nMeZO works significantly better than zero-shot, linear probing, and other memory-equivalent\nmethods.\nOn all six diverse tasks, MeZO can optimize the pre-trained model and consistently\nperform better than zero-shot and linear probing. We also show for several tasks that MeZO can\noutperform another ZO algorithm, BBTv2 [90], by up to 11% absolute (Appendix F.4).4\nWith enough data, MeZO achieves comparable performance (up to 5% gap) to FT.\nMeZO\nachieves close-to-fine-tuning performance on k = 16, with some tasks only having 2% gaps. When\nusing k = 512 data, the gap between MeZO and FT further reduced to within 5% across all tasks.\nMeZO works well on both full-parameter tuning and PEFT.\nFull-parameter tuning (MeZO) and\nPEFT (MeZO with LoRA and prefix-tuning) achieve comparable performance, while MeZO (prefix)\n4BBTv2 can only train low-dimensional projected prefixes instead of the full model.\n5\nTask\nSST-2 RTE\nCB\nBoolQ\nWSC WIC MultiRC COPA\nReCoRD\nSQuAD DROP\nTask type\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 classification \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\u2013 multiple choice \u2013\n\u2014 generation \u2014\nZero-shot\n58.8\n59.6\n46.4\n59.0\n38.5\n55.0\n46.9\n80.0\n81.2\n46.2\n14.6\nICL\n87.0\n62.1\n57.1\n66.9\n39.4\n50.5\n53.1\n87.0\n82.5\n75.9\n29.6\nLP\n93.4\n68.6\n67.9\n59.3\n63.5\n60.2\n63.5\n55.0\n27.1\n3.7\n11.1\nMeZO\n91.4\n66.1\n67.9\n67.6\n63.5\n61.1\n60.1\n88.0\n81.7\n84.7\n30.9\nMeZO (LoRA)\n89.6\n67.9\n66.1\n73.8\n64.4\n59.7\n61.5\n84.0\n81.2\n83.8\n31.4\nMeZO (prefix)\n90.7\n70.8\n69.6\n73.1\n60.6\n59.9\n63.7\n87.0\n81.4\n84.2\n28.9\nFT (12x memory)\n92.0\n70.8\n83.9\n77.1\n63.5\n70.1\n71.1\n79.0\n74.1\n84.9\n31.3\nTable 1: Experiments on OPT-13B (with 1000 examples). ICL: in-context learning; LP: linear\nprobing; FT: full fine-tuning with Adam. MeZO outperforms zero-shot, ICL, and LP across the board,\nand achieves comparable (within 1%) or better performance than FT on 7 out of 11 tasks.\nTask\nSST-2\nRTE\nBoolQ\nWSC\nWIC\nSQuAD\n30B zero-shot\n56.7\n52.0\n39.1\n38.5\n50.2\n46.5\n30B ICL\n81.9\n66.8\n66.2\n56.7\n51.3\n78.0\n30B MeZO/MeZO (prefix)\n90.6\n72.6\n73.5\n63.5\n59.1\n85.2\n66B zero-shot\n57.5\n67.2\n66.8\n43.3\n50.6\n48.1\n66B ICL\n89.3\n65.3\n62.8\n52.9\n54.9\n81.3\n66B MeZO/MeZO (prefix)\n93.6\n66.4\n73.7\n63.5\n58.9\n85.0\nTable 2: Experiments on OPT-30B and OPT-66B (with 1000 examples). We report the best of MeZO\nand MeZO (prefix). See Appendix F.2 for more results. We see that on most tasks MeZO effectively\noptimizes up to 66B models and outperforms zero-shot and ICL.\nsometimes outperforms MeZO. We also show in Appendix F.3 that the three variants converge at\nsimilar rates, agreeing with our theory in Section 4, which shows that MeZO converges at a rate\nindependent of the number of parameters being optimized.\nWe show additional results with more FT and MeZO variants in Appendix F.1. We see that (1)\nZO-Adam sometimes outperforms ZO-SGD but is not consistent across tasks; (2) LP and then MeZO,\nas suggested for fine-tuning [53], can sometimes improve the performance.\n3.2\nLarge autoregressive language models\nWith the promising results from RoBERTa-large, we extend MeZO to the OPT family [112], on a\nscale of 13B (Table 1), 30B, and 66B (Table 2). We select both SuperGLUE [98] tasks5 (including\nclassification and multiple-choice) and generation tasks. We randomly sample 1000, 500, and 1000\nexamples for training, validation, and test, respectively, for each datset. We run MeZO for 20K steps\nand fine-tuning for 5 epochs, or 625 steps, noting that each step of MeZO is substantially faster than\nfine-tuning (see Appendix F.6 for a comparison). Please refer to Appendix E for details. Table 1\nyields the following observations.\nMeZO outperforms memory-equivalent methods and closely approaches fine-tuning results.\nWe see that on a 13B-parameter scale, MeZO and its PEFT variants outperform zero-shot, ICL, and\nLP across almost all tasks. When comparing to FT, which costs 12\u00d7 more memory (Section 3.4),\nMeZO achieves comparable (within 1%) or better performance on 7 out of the 11 tasks.\nMeZO exhibits strong performance across classification, multiple-choice, and generation tasks.\nWe investigate MeZO on generation tasks, which are regarded as more intricate than classification or\nmultiple-choice tasks. We evaluate on two question answering datasets, SQuAD [80] and DROP [31].\nWe use teacher forcing for training and greedy decoding for inference (details in Appendix E).\nTable 1 shows that, on all generation tasks, MeZO outperforms zero-shot, ICL, and LP, and achieves\ncomparable performance to FT. Considering that many applications of fine-tuning LMs \u2013 including\ninstruction tuning or domain adaptation \u2013 target generation tasks, our results underscore the potential\nof MeZO as a memory-efficient technique to optimize large LMs for realistic and exciting applications.\n5We also include SST-2, which is a simple sentiment classification task that we use for development.\n6\nModel\nRoBERTa-large (350M)\nOPT-13B\nTask\nSST-2\nSST-5\nSNLI\nTREC\nSQuAD\nZero-shot\n79.0\n35.5\n50.2\n32.0\n46.2\nCross entropy (FT)\n93.9\n55.9\n88.7\n97.3\n84.2\nCross entropy (MeZO)\n93.3\n53.2\n83.0\n94.3\n84.7\nAccuracy/F1 (MeZO)\n92.7\n48.9\n82.7\n68.6\n78.5\nTable 3: MeZO with non-differentiable objectives. For classification (k = 512), we use MeZO with\nfull-parameter and optimize accuracy; for SQuAD (1,000 examples), we use MeZO (prefix) and F1.\n\n\b\f\u000f\n\u000b\b\u000e\u000f\n\r\b\u000e\u000f\n\n\f\u000f\n\f\t\u000f\n\u0004\u0017\u001b#\u001b \u001c%\u001c#$\n\n\n\t\n\n\t\t\n\n\t\t\t\n\u0012\u0017\u0019\u0003\u0015\u001c !#'\u0003\u0005\u0012\u000f\u0006\n\u001a\u001c#!\u0007$\u001e!%\n\u0013\u0010\u0014\n\u0011\u0018\n\u0011\u0018\u0007\"#\u001c\u001d\u001f&\n\u001a\u0016\n\n\b\f\u000f\n\u000b\b\u000e\u000f\n\r\b\u000e\u000f\n\n\f\u000f\n\f\t\u000f\n\u0004\u0017\u001b#\u001b \u001c%\u001c#$\n\n\n\t\n\n\t\t\n\n\t\t\t\n\u0012\u0017\u0019\u0003\u0015\u001c !#'\u0003\u0005\u0012\u000f\u0006\n\u001a\u001c#!\u0007$\u001e!%\n\u0013\u0010\u0014\n\u0011\u0018\n\u0011\u0018\u0003\u0005\"#\u001c\u001d\u001f&\u0006\n\u0015\u001c\u001a\u0016\n7x\n8x\n11x\n12x\n11x\nFigure 3: GPU memory consumption with different\nOPT models and tuning methods on MultiRC (400 to-\nkens per example on average).\nHardware\nLargest OPT that can fit\nFT\nFT-prefix\nInference\n1\u00d7A100 (80GB)\n2.7B\n6.7B\n30B\n2\u00d7A100 (160GB)\n6.7B\n13B\n66B\n4\u00d7A100 (320GB)\n13B\n30B\n66B\n8\u00d7A100 (640GB)\n30B\n66B\n175B\u2020\nFigure 4: Largest OPT models that one can\ntune with specific hardwares and algorithms.\n\u2020 : projected results without actual testing.\nMeZO scales up to 66 billion parameter models.\nWe demonstrate the efficacy of MeZO on even\nlarger models, up to 66B, in Table 2. While directly fine-tuning models at such scales is extremely\ncostly (Section 3.4), MeZO can effectively optimize these models and outperform zero-shot and ICL.\n3.3\nTraining with non-differentiable objectives\nWe demonstrate the efficacy of MeZO for optimizing non-differentiable objectives through initial\nexperiments. Accuracy and F1 are used as the respective objectives (details in Appendix E.6). Table 3\nreveals that MeZO with accuracy/F1 successfully optimizes LMs with superior performance to\nzero-shot. Although minimizing cross entropy results in stronger performance, these preliminary\nfindings highlight the promising potential of applying MeZO to optimize non-differentiable objectives\nwithout clear differentiable surrogates, such as human preferences [73].\n3.4\nMemory usage and wall-clock time analysis\nIn this section we profile the memory usage of zero-shot, ICL, FT, FT (prefix), and MeZO. We\ntest OPT models of various sizes with Nvidia A100 GPUs (80GB memory) on MultiRC (average\n#tokens=400), and report the peak GPU memory consumption (details in Appendix E.7).\nAs shown in Figure 3 (refer to Appendix F.5 for detailed numbers), MeZO exhibits the same memory\nconsumption as zero-shot while offering memory savings of up to 12 times compared to standard FT\nand 6 times compared to FT (prefix). This advantage enables training larger models within a fixed\nhardware budget, as illustrated in Figure 4. Specifically, using a single A100 GPU, MeZO allows for\ntuning a model that is 11 times larger than what is feasible with FT.\nIn Appendix F.6, we compare the wall-clock time efficiencies of our implementations of MeZO and\nAdam fine-tuning. MeZO achieves 7.74\u00d7 per-step speedup and requires 8\u00d7 fewer GPUs with a 30B\nmodel, but takes more steps to converge. Overall, MeZO only requires half as many GPU-hours to\nfine-tune a 30B model compared to full-parameter fine-tuning. The wall-clock benefit of MeZO is\nnot inherent to the algorithm and is highly dependent on the implementation. We primarily provide\nthis information as a demonstration that MeZO does not take a prohibitively long time to run.\nThe above measurements are dependent on the computing infrastructure. In Appendix C, we compare\nthe theoretical time-memory tradeoff of MeZO and backpropagation and find that MeZO is always\nmore memory-efficient than backpropagation and is often more time-efficient. The above analyses\nalso do not consider recent advances (e.g., gradient checkpointing [18], FlashAttention [23], and\nquantization [27]). We leave investigating the how MeZO works with these methods to future work.\n7\n4\nTheory\nOur theoretical analysis highlights why MeZO can optimize large LMs, although a number of classical\nresults [69, 47, 79, 3, 70] suggest that optimization should be catastrophically slow when training so\nmany parameters. The inclusion of a simple prompt is crucial for MeZO to succeed (Appendix A).\nPast works [83, 67] have suggested that including such a prompt ensures that the fine-tuning objective\nis closely related to the pre-training one. As such, here, we make the assumption that the model has\nalready been trained for many steps on the fine-tuning objective, which implies that the loss landscape\nexhibits favorable conditions (Assumption 1). Then, we derive a convergence rate independent of\nthe number of parameters. We show that the loss decreases per step at a rate independent of the\nparameter dimension d (Theorem 1), and that, under stronger conditions, the algorithm converges in\ntime independent of d (Lemma 3). Together, these results imply that MeZO is not catastrophically\nslower than SGD when fine-tuning.6 For ease of illustration, we assume that z is sampled from a\nsphere with radius\n\u221a\nd, and in Appendix G.2, we derive the rate for a general Gaussian z, which was\nused in the experiments.\nWe follow classical analyses of SGD and replace the minibatch gradient estimate with SPSA (Defi-\nnition 1). Consider the minibatch SGD update \u03b8t+1 \u2190 \u03b8t \u2212 \u03b7\u2207L(\u03b8; Bt) where Bt is a minibatch\ndrawn uniformly from DB. Crucially, the SGD minibatch gradient estimate is unbiased.\nDefinition 3 (Unbiased Gradient Estimate). Any minibatch gradient estimate g(\u03b8, B) is said to be\nunbiased if E[g(\u03b8, B)] = \u2207L(\u03b8).\n4.1\nPer-step analysis\nThe classical descent lemma uses a Taylor expansion to study how SGD reduces the loss at each\noptimization step. It highlights that when the gradient covariance is large, the maximum possible\ndecrease in loss at each optimization step is small, thereby resulting in slower optimization.\nLemma 1 (Descent Lemma). Let L(\u03b8) be \u2113-smooth.7 For any unbiased gradient estimate g(\u03b8, B),\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 \u2212\u03b7 \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\u2113 \u00b7 E[\u2225g(\u03b8, Bt)\u22252].\n(2)\nThe descent lemma highlights the importance of the gradient norm, which we derive for MeZO below.\nLemma 2. Let B be a random minibatch of size B. Then, the gradient norm of MeZO is\nEx\n\u0014\r\r\rb\u2207L(\u03b8; B)\n\r\r\r\n2\u0015\n= d + n \u2212 1\nn\nE\nh\n\u2225\u2207L(\u03b8; B)\u22252i\n.\nwhere n is the number of z sampled in n-SPSA (Definition 1) and d is the number of parameters.\nThus, in the usual case where n \u226a d, MeZO has a much larger gradient norm than SGD.8 The\ndescent lemma also shows that to guarantee loss decrease, one needs to choose the learning rate as\n\u03b7 \u2264\n2 \u2225\u2207L(\u03b8t)\u22252\n\u2113 \u00b7 E[\u2225g(\u03b8, B)\u22252]\nLemma 2\n=======\u21d2\n\u03b7ZO =\nn\nd + n \u2212 1\u03b7SGD\n(3)\nwhere \u03b7ZO and \u03b7SGD are the maximum permissible learning rates for MeZO and SGD respectively.\nThus we see that without any further assumptions, MeZO can slow optimization by decreasing the\nlargest permissible learning rate by a factor of d. Moreover, MeZO reduces the loss decrease that can\nbe obtained at each step and, as a consequence, slows convergence by a factor of d as well.\nSurprisingly, our experiments show that MeZO can quickly optimize pre-trained models with billions\nof parameters, and reducing the number of tuned parameters via PEFT techniques does not substan-\ntially accelerate optimization (Appendix F.3). We attribute these phenomena to the Hessian of the loss\nexhibiting small local effective rank. It is prohibitively expensive to directly measure the effective\nrank of the Hessian of a large LM on a reasonably sized dataset. However, many previous works\nhave shown that the Hessian of the loss for deep neural networks trained by SGD has remarkably low\n6Section 3 uses the standard choice of Adam for FT; we provide SGD experiments in Appendix F.1.\n7This is satisfied for the standard cross-entropy objective.\n8All of our experiments use n = 1.\n8\neffective rank [74, 75, 36, 107, 105, 82]. In particular, the bulk of the spectrum concentrates around\n0 with only a small number of outliers, and the number of these outliers is an upper bound on the\neffective rank. In addition, prior works [4, 56] have demonstrated that LM fine-tuning can occur in a\nvery low dimensional subspace (< 200 parameters), which further supports the below assumption.\nWe formalize the assumption on the effective rank below. In particular, we require an upper bound on\nthe Hessian in a neighborhood around the current iterate to have effective rank at most r.\nAssumption 1 (Local r-effective rank). Let G(\u03b8t) = max(x,y)\u2208D \u2225\u2207L(\u03b8t; {(x, y)})\u2225. There exists\na matrix H(\u03b8t) \u2aaf \u2113 \u00b7 Id such that:\n1. For all \u03b8 such that \u2225\u03b8 \u2212 \u03b8t\u2225 \u2264 \u03b7dG(\u03b8t), we have \u22072L(\u03b8) \u2aaf H(\u03b8t).\n2. The effective rank of H(\u03b8t), i.e tr(H(\u03b8t))/ \u2225H(\u03b8t)\u2225op, is at most r.\nUnder this assumption, we show that the convergence rate of ZO-SGD does not depend on the number\nof parameters. Instead, the slowdown factor only depends on the effective rank of the Hessian.\nTheorem 1 (Dimension-Free Rate). Assume the loss exhibits local r-effective rank (Assumption 1). If\n\u03b8t+1 = \u03b8t \u2212 \u03b7ZO b\u2207L(\u03b8t; B) is a single step of ZO-SGD using the n-SPSA estimate with a minibatch\nof size B, then there exists a \u03b3 = \u0398(r/n) such that the expected loss decrease can be bounded as\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 \u2212\u03b7ZO \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\nZO\u2113 \u00b7 \u03b3 \u00b7 E[\u2225\u2207L(\u03b8; B)\u22252]\n(4)\nBy applying Equation (3), we can directly compare to the SGD descent lemma.\nCorollary 1. Choosing the learning rate \u03b7ZO = \u03b3\u22121 \u00b7 \u03b7SGD, ZO-SGD obtains a loss decrease of\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 1\n\u03b3 \u00b7\n\u0014\n\u2212\u03b7SGD \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\nSGD\u2113 \u00b7 E[\u2225\u2207L(\u03b8; B)\u22252]\n\u0015\n.\n(5)\nHere we see that comparing to SGD, the slowdown factor of ZO-SGD scales with the local effective\nrank r, which we argue is much smaller than the number of parameters d. The above analysis focuses\non how much ZO-SGD and SGD decrease the loss at each step. Below, we show that under stronger\nassumptions about the loss landscape, we can obtain rates for how quickly the ZO-SGD algorithm\nconverges to an optimal value.\n4.2\nGlobal convergence analysis\nWe show that the global convergence rate also slows by a factor proportional to the local effective\nrank under stronger assumptions about the loss landscape. We assume that the landscape obeys the\nclassical PL inequality: the gradient norm grows quadratically with the suboptimality of the iterate.\nDefinition 4 (PL Inequality). Let L\u2217 = min\u03b8 L(\u03b8). The loss L is \u00b5-PL if, for all \u03b8, 1\n2 \u2225\u2207L(\u03b8)\u22252 \u2265\n\u00b5(L(\u03b8) \u2212 L\u2217).\nThe PL inequality is not as strong as assuming that optimization exhibits kernel-like dynamics, but it\nensures that the landscape is amenable to analysis [50]. In addition to the PL inequality, we assume\nthe trace of the gradient covariance is bounded, so noise does not disrupt the trajectory too drastically.\nDefinition 5 (Gradient Covariance). The SGD gradient estimate on a minibatch of size B has\ncovariance \u03a3(\u03b8) = B(E\n\u0002\n\u2207L(\u03b8; B)\u2207L(\u03b8; B)\u22a4\u0003\n\u2212 \u2207L(\u03b8)\u2207L(\u03b8)\u22a4).\nAs we show in Appendix G.1, this assumption holds for common loss functions such as square loss or\nbinary cross entropy for several settings (e.g., kernel behavior [67]). With these two assumptions, we\nshow that ZO-SGD has a slowdown proportional to the effective rank r, not the parameter dimension.\nLemma 3 (Global Convergence of ZO-SGD). Let L(\u03b8) be \u00b5-PL and let there exist \u03b1 such that\ntr(\u03a3(\u03b8)) \u2264 \u03b1(L(\u03b8) \u2212 L\u2217) for all \u03b8. Then after\nt = O\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u0010 r\nn + 1\n\u0011\n\u00b7\n\u0012 \u2113\n\u00b5 + \u2113\u03b1\n\u00b52B\n\u0013\nlog L(\u03b80) \u2212 L\u2217\n\u03f5\n|\n{z\n}\nSGD rate (Lemma 4)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\niterations of ZO-SGD we have E[L(\u03b8t)] \u2264 L\u2217 + \u03f5.\n9\n5\nRelated work\nZeroth-order optimization\nMany classical lower bounds have been derived for ZO-SGD in the\nstrongly convex and convex settings [47, 3, 79, 32, 85, 69] as well as non-convex [101]. These\nbounds generally depended on the number of parameters d. More recently, [100, 6, 15] showed that\nif the gradient has low-dimensional structure, then the query complexity scales linearly with the\nintrinsic dimension and logarithmically with the number of parameters, though the estimation has at\nleast \u2126(sd log d) memory cost. Additional tricks such as sampling schedules [11] and other variance\nreduction methods [48, 62] can be added to ZO-SGD. ZO has inspired distributed methods [93, 43]\nand black-box adversarial example generation [14, 63, 17, 64] in deep learning. Ye et al. [108],\nBalasubramanian and Ghadimi [7] estimate the Hessian to perform ZO optimization along important\ndirections. There are also ZO methods that optimize without estimating the gradient [38, 68, 44].\nMemory-efficient backpropagation\nSeveral algorithms have been proposed to efficiently approx-\nimate backpropagation by sparsifying gradients [92, 102], approximating Jacobians [1, 19], and\nsubsampling the computational graph [71, 2]. However, these methods may accrue large approxima-\ntion errors for deep networks. Gradient checkpointing [18] reduces memory cost of backpropagation\nat the cost of recomputing some activations. FlashAttention [23] also reduces memory cost by\nrecomputing attention matrices. Dettmers et al. [26, 27] explore quantization of large LMs\u2019 weights\nand optimizer states, which leads to memory reduction in both training and inference.\nGradient-free adaptation of large language models\nBBT and BBTv2 [91, 90] use evolutionary\nalgorithms to achieve gradient-free optimization; however, due to its sensitivity to high dimensionality,\nBBT is limited to only optimize a low-dimension projection of prefixes and they focus on RoBERTa-\nlarge size models and few-shot settings. Other works in \u201cblack-box tuning\u201d of LMs focus on\noptimizing discrete prompts without updating the model, either via reinforcement learning [16, 25, 29],\nensemble [45], or iterative search [78]. Concurrent work in [106] uses iterative forward passes to\nimprove in-context learning performance.\n6\nConclusion\nWe have shown that MeZO can effectively optimize large LMs across many tasks and scales. Further\nexperiments suggest that MeZO can optimize non-differentiable objectives, which backpropagation\nusually cannot do. Our theory illustrates why MeZO is not catastrophically slow when tuning billions\nof parameters. As a limitation, MeZO takes many steps in order to achieve strong performance,\nthough we show that the per-step speedup in MeZO can often make fine-tuning with MeZO run faster\nthan a standard implementation of fine-tuning with backpropagation. We did not explore combining\nMeZO with other memory-efficient methods, such as FlashAttention [23] and quantization [26],\nthough we hope to investigate this in the future.\nWe are excited to explore the applicability of MeZO to a number of areas, including but not limited to:\npruning, distillation, saliency, interpretability, and dataset selection for fine-tuning. Non-differentiable\nobjectives are a particularly exciting area, given recent advances in tuning large LMs to adapt to\nhuman feedback. Conducting theoretical analyses for how these efficient gradient estimates impact\nthe performance of different applications is also of interest.\nAcknowledgements\nWe thank Xinyi Chen, Yin Tat Lee, Kaifeng Lyu, Tengyu Ma, Abhishek Panigrahi, Nikunj Saunshi,\nand Mengzhou Xia for their helpful feedback. SA and SM are funded by NSR, ONR, SRC, and\nSimons Foundation. JDL, AD, and EN acknowledge the support of the ARO under MURI Award\nW911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF\nCIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. TG is supported\nby an IBM PhD Fellowship. This work is also partially funded by the National Science Foundation\n(IIS-2211779).\n10\nReferences\n[1] Hany S Abdel-Khalik, Paul D Hovland, Andrew Lyons, Tracy E Stover, and Jean Utke. A low\nrank approach to automatic differentiation. In Advances in Automatic Differentiation, pages\n55\u201365, 2008.\n[2] Menachem Adelman, Kfir Levy, Ido Hakimi, and Mark Silberstein. Faster neural network\ntraining with approximate tensor operations. Advances in Neural Information Processing\nSystems, 34:27877\u201327889, 2021.\n[3] Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Information-\ntheoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE\nTransactions on Information Theory, 58(5):3235\u20133249, May 2012.\n[4] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 7319\u20137328, 2021.\n[5] Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak,\nAbheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An\nintegrated development environment and repository for natural language prompts. arXiv\npreprint arXiv:2202.01279, 2022.\n[6] Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic\noptimization via conditional gradient and gradient updates. In Advances in Neural Information\nProcessing Systems, volume 31, 2018.\n[7] Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order nonconvex stochastic\noptimization: Handling constraints, high dimensionality, and saddle points. Foundations of\nComputational Mathematics, pages 1\u201342, 2022.\n[8] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings\nof the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006.\n[9] At\u0131l\u0131m G\u00fcne\u00b8s Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradi-\nents without backpropagation, 2022.\n[10] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth PASCAL\nrecognizing textual entailment challenge. In TAC, 2009.\n[11] Raghu Bollapragada, Richard Byrd, and Jorge Nocedal. Adaptive sampling strategies for\nstochastic optimization. SIAM Journal on Optimization, 28(4):3312\u20133343, 2018.\n[12] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A\nlarge annotated corpus for learning natural language inference. In Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing, pages 632\u2013642, 2015.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In Advances in neural information processing systems, vol-\nume 33, pages 1877\u20131901, 2020.\n[14] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate\ndescent algorithm for huge-scale black-box optimization. In International Conference on\nMachine Learning, pages 1193\u20131203, 2021.\n[15] HanQin Cai, Daniel McKenzie, Wotao Yin, and Zhenliang Zhang. Zeroth-order regularized\noptimization (zoro): Approximately sparse gradients and adaptive sampling. SIAM Journal on\nOptimization, 32(2):687\u2013714, 2022.\n[16] Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.\nClip-\ntuning: Towards derivative-free prompt learning with a mixture of rewards. arXiv preprint\narXiv:2210.12050, 2022.\n11\n[17] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order\noptimization based black-box attacks to deep neural networks without training substitute\nmodels. In Proceedings of the 10th ACM workshop on artificial intelligence and security,\npages 15\u201326, 2017.\n[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n[19] Krzysztof M Choromanski and Vikas Sindhwani. On blackbox backpropagation and jacobian\nsensing. In Advances in Neural Information Processing Systems, volume 30, 2017.\n[20] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences. In Advances in neural information\nprocessing systems, volume 30, 2017.\n[21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 2924\u20132936, 2019.\n[22] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual en-\ntailment challenge. In the First International Conference on Machine Learning Challenges:\nEvaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual\nEntailment, 2005.\n[23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. In Advances in Neural Information\nProcessing Systems, volume 35, pages 16344\u201316359, 2022.\n[24] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:\nInvestigating projection in naturally occurring discourse. In Sinn und Bedeutung, volume 23,\npages 107\u2013124, 2019.\n[25] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,\nMeng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with\nreinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 3369\u20133391, 2022.\n[26] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit\nmatrix multiplication for transformers at scale. In Advances in Neural Information Processing\nSystems, 2022.\n[27] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-\nwise quantization. In International Conference on Learning Representations, 2022.\n[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.\n[29] Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, and Tong Zhang. Black-box prompt\nlearning for pre-trained language models. arXiv preprint arXiv:2201.08531, 2022.\n[30] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding\nHu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904,\n2022.\n[31] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. DROP: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 2368\u20132378, 2019.\n12\n[32] John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates\nfor zero-order convex optimization: The power of two function evaluations. IEEE Transactions\non Information Theory, 61(5):2788\u20132806, 2015.\n[33] FairScale authors. Fairscale: A general purpose modular pytorch library for high performance\nand large scale training, 2021.\n[34] Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex\noptimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the\nSixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201905, page 385\u2013394,\nUSA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.\n[35] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-\nshot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3816\u20133830, 2021.\n[36] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net\noptimization via hessian eigenvalue density. In International Conference on Machine Learning,\npages 2232\u20132241, 2019.\n[37] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\nrecognizing textual entailment challenge. In the ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, 2007.\n[38] Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi\nZhang. Gradientless descent: High-dimensional zeroth-order optimization. In International\nConference on Learning Representations, 2020.\n[39] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training\nimagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n[40] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of\nalgorithmic differentiation. SIAM, 2008.\n[41] Jos\u00e9 Grimm, Lo\u00af\u0131c Pottier, and Nicole Rostaing-Schmidt. Optimal time and minimum space-\ntime product for reversing a certain class of programs. PhD thesis, INRIA, 1996.\n[42] Suchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug\nDowney, and Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains\nand tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 8342\u20138360, 2020.\n[43] Davood Hajinezhad and Michael M Zavlanos. Gradient-free multi-agent nonconvex nonsmooth\noptimization. In 2018 IEEE Conference on Decision and Control (CDC), pages 4939\u20134944,\n2018.\n[44] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv\npreprint arXiv:2212.13345, 2022.\n[45] Bairu Hou, Joe O\u2019Connor, Jacob Andreas, Shiyu Chang, and Yang Zhang. Promptboosting:\nBlack-box text classification with ten forward passes. arXiv preprint arXiv:2212.09257, 2022.\n[46] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2022.\n[47] Kevin G Jamieson, Robert Nowak, and Ben Recht. Query complexity of derivative-free\noptimization. In Advances in Neural Information Processing Systems, volume 25, 2012.\n[48] Kaiyi Ji, Zhe Wang, Yi Zhou, and Yingbin Liang. Improved zeroth-order variance reduced\nalgorithms and analysis for nonconvex optimization. In International conference on machine\nlearning, pages 3100\u20133109, 2019.\n13\n[49] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-\nance reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,\neditors, Advances in Neural Information Processing Systems, volume 26, 2013.\n[50] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-\ngradient methods under the polyak-\u0142ojasiewicz condition, 2020.\n[51] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look-\ning beyond the surface: A challenge set for reading comprehension over multiple sentences.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages\n252\u2013262, 2018.\n[52] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations, 2015.\n[53] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-\ntuning can distort pretrained features and underperform out-of-distribution. In International\nConference on Learning Representations, 2022.\n[54] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3045\u20133059, 2021.\n[55] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\nIn Thirteenth international conference on the principles of knowledge representation and\nreasoning, 2012.\n[56] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic\ndimension of objective landscapes. In International Conference on Learning Representations,\n2018.\n[57] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 4582\u20134597, 2021.\n[58] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with\nstochastic differential equations (SDEs). In A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n[59] Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust\ntraining of neural networks using scale invariant architectures. In International Conference on\nMachine Learning, pages 12656\u201312684, 2022.\n[60] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep\nLearning Architectures, pages 100\u2013114, 2022.\n[61] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding\nthe difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 5747\u20135763, 2020.\n[62] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini.\nZeroth-order stochastic variance reduction for nonconvex optimization. In Advances in Neural\nInformation Processing Systems, volume 31, 2018.\n[63] Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In\nInternational Conference on Learning Representations, 2019.\n[64] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K\nVarshney. A primer on zeroth-order optimization in signal processing and machine learning:\nPrincipals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43\u201354,\n2020.\n14\n[65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[66] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8086\u20138098, 2022.\n[67] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based\nview of language model fine-tuning. arXiv preprint arXiv:2210.05643, 2022.\n[68] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies\nis competitive for reinforcement learning. In Advances in Neural Information Processing\nSystems, volume 31, 2018.\n[69] Arkadij Semenovi\u02c7c Nemirovskij and David Borisovich Yudin. Problem complexity and method\nefficiency in optimization. 1983.\n[70] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex func-\ntions. Foundations of Computational Mathematics, 17:527\u2013566, 2017.\n[71] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. Randomized\nautomatic differentiation. arXiv preprint arXiv:2007.10412, 2020.\n[72] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[73] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. Advances in Neural Information Processing\nSystems, 35:27730\u201327744, 2022.\n[74] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training\nand sample size. arXiv preprint arXiv:1811.07062, 2018.\n[75] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal of\nMachine Learning Research, 21(252):1\u201364, 2020.\n[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems 32,\npages 8024\u20138035. 2019.\n[77] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for\nevaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, 2019.\n[78] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based\ninstruction search for prompting large language models. arXiv preprint arXiv:2203.07281,\n2022.\n[79] Maxim Raginsky and Alexander Rakhlin.\nInformation-based complexity, feedback and\ndynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036\u2013\n7056, 2011.\n[80] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+\nquestions for machine comprehension of text. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, pages 2383\u20132392, 2016.\n[81] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. 2011.\n15\n[82] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis\nof the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.\n[83] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why\nlanguage models help solve downstream tasks. In International Conference on Learning\nRepresentations, 2021.\n[84] Timo Schick and Hinrich Sch\u00fctze. Exploiting cloze-questions for few-shot text classification\nand natural language inference. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269,\n2021.\n[85] Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with\ntwo-point feedback. The Journal of Machine Learning Research, 18(1):1703\u20131713, 2017.\n[86] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing, 2013.\n[87] James C Spall. A one-measurement form of simultaneous perturbation stochastic approxima-\ntion. Automatica, 33(1):109\u2013112, 1997.\n[88] J.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient\napproximation. IEEE Transactions on Automatic Control, 37(3):332\u2013341, 1992.\n[89] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nIn Advances in Neural Information Processing Systems, volume 33, pages 3008\u20133021, 2020.\n[90] Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu.\nBBTv2: Towards a gradient-free future with large language models. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pages 3916\u20133930,\n2022.\n[91] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning\nfor language-model-as-a-service. In International Conference on Machine Learning, pages\n20841\u201320855, 2022.\n[92] Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang.\nmeProp: Sparsified back\npropagation for accelerated deep learning with reduced overfitting. In Proceedings of the 34th\nInternational Conference on Machine Learning, volume 70, pages 3299\u20133308, 2017.\n[93] Yujie Tang and Na Li. Distributed zero-order algorithms for nonconvex multi-agent optimiza-\ntion. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing\n(Allerton), pages 781\u2013786, 2019.\n[94] Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. Zeroth-order optimization meets human\nfeedback: Provable learning via ranking oracles, 2023.\n[95] Alexander Timurovich Vakhitov, Oleg Nikolaevich Granichin, and LS Gurevich. Algorithm\nfor stochastic approximation with trial input perturbation in the nonstationary problem of\noptimization. Automation and Remote Control, 70:1827\u20131835, 2009.\n[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, volume 30, 2017.\n[97] Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In the 23rd\nannual international ACM SIGIR conference on Research and development in information\nretrieval, 2000.\n16\n[98] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems. In Advances in neural information processing systems,\nvolume 32, 2019.\n[99] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic\ngradient optimization. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.\nWeinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran\nAssociates, Inc., 2013.\n[100] Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order\noptimization in high dimensions. In Proceedings of the Twenty-First International Conference\non Artificial Intelligence and Statistics, volume 84, pages 1356\u20131365, 2018.\n[101] Zhongruo Wang, Krishnakumar Balasubramanian, Shiqian Ma, and Meisam Razaviyayn.\nZeroth-order algorithms for nonconvex minimax problems with improved complexities. arXiv\npreprint arXiv:2001.07819, 2020.\n[102] Bingzhen Wei, Xu Sun, Xuancheng Ren, and Jingjing Xu. Minimal effort back propagation\nfor convolutional neural networks. arXiv preprint arXiv:1709.05804, 2017.\n[103] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), 2018.\n[104] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pages 38\u201345, 2020.\n[105] Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Under-\nstanding common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261,\n2020.\n[106] Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward\ntuning boosts in-context learning in language models, 2023.\n[107] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural\nnetworks through the lens of the hessian. In 2020 IEEE international conference on big data\n(Big data), pages 581\u2013590, 2020.\n[108] Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and Tong Zhang. Hessian-aware\nzeroth-order optimization for black-box adversarial attack. arXiv preprint arXiv:1812.11377,\n2018.\n[109] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.\narXiv preprint arXiv:1708.03888, 2017.\n[110] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,\nXiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization\nfor deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\n[111] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\narXiv preprint arXiv:1810.12885, 2018.\n[112] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[113] Yan Zhang, Yi Zhou, Kaiyi Ji, and Michael M Zavlanos. A new one-point residual-feedback\noracle for black-box learning and control. Automatica, 136:110006, 2022.\n17\nA\nAlgorithmic Ablations\nWe perform a number of ablations to select the best algorithm. As is standard in ZO literature, we\nconsider the main computational cost to be the number of forward passes. In our case, the number\nof forward passes can be affected by the number of gradient steps taken, any usage of gradient\naccumulation, and using more noise samples to reduce the variance of the gradient estimate.\nWe observed that the performance of MeZO improves monotonically with the number of steps, and\nthere does not appear to be any overfitting. Hence, when performing algorithmic ablations, we can\nfocus on the efficiency of different algorithms without considering implicit bias. This is also reflected\nin our theoretical analysis. To ease the computational load, we fix the number of forward passes to\n10, 000 and compare many different algorithms for RoBERTa-large on a smaller set of tasks that span\nsentiment analysis, entailment, and topic classification: SST-2, SNLI, and TREC. We emphasize\nthat 10, 000 is a small budget and is only used as a setting to compare these ZO algorithms to each\nother. We find that using a linearly decreasing learning rate schedule during training, as was done for\nfine-tuning with backpropagation in [65], does not help or hurt MeZO. Similarly, using a learning\nrate warmup leads to identical results on these three tasks. For simplicity, we use a constant learning\nrate schedule with no warmup for all of the below experiments. We perform few-shot experiments\nwith k = 16 and average the results across 5 seeds.\nExperiment\nHyperparameters\nValues\nMeZO\nBatch size\n{16, 64} \u00d7\nLearning rate\n{1e\u22125, 1e\u22126, 1e\u22127} \u00d7\n\u03f5\n{1e\u22123, 1e\u22125} \u00d7\nWeight Decay\n{0, 0.1}\nTable 4: The hyperparameter grid used in our ablation experiments. For simplicity, we use a constant\nlearning rate schedule.\nA.1\nPrompting\nWe study if adding a prompt is crucial to the ability of MeZO to optimize the network. We use prompts\nfrom Gao et al. [35]. Malladi et al. [67] claimed the prompt makes the optimization trajectory well-\nbehaved, though we note that the current paper considers RoBERTa-large and large autoregressive\nmodels while the previous work only studied RoBERTa-base. We note the similarity between kernel\nbehavior and our theoretical setting in Section 4. MeZO succeeds on tasks that are reported to not\nexhibit kernel behavior in Malladi et al. [67], so we investigate whether or not the prompt is necessary.\nSST-2\nSNLI\nTREC\nPrompt\n89.6 (1.2)\n65.1 (6.2)\n66.7 (6.2)\nNo Prompt\n51.9 (2.9)\n34.8 (2.1)\n19.5 (9.0)\nTable 5: Experiments using MeZO to fine-tune models with and without a prompt.\nBoth experiments followed the grid in Table 4, but we also expanded the grid to include a learning rate\nof 1e \u2212 4 for the no prompt case. As a result of these experiments, we fix the setting to prompt-based\nfine-tuning for all of the below experiments.\nA.2\nSample schedules\nOne can sample nt noise vectors at the tth step and use nt-SPSA to compute the gradient estimate.\nSimilar ideas were proposed in Bollapragada et al. [11], Cai et al. [15]. We study the effect of linearly\nincreasing and constant sampling schedules in the ablation setting. The intuition for the linearly\nincreasing schedule is that the optimizer may need a higher fidelity gradient as it approaches the\nminimum. Increasing the number of zs can speed up optimization by reducing the gradient variance,\nbut doing so also increases the number of forward passes required for each optimization step, so\nthere is a trade-off to study. We note that increasing the number of zs should be accompanied by\n18\na proportional scaling of the learning rate, analogous to the linear scaling rule proposed in [39]\n(theoretical justification can follow the SDE technique [58]). Table 6 shows no consistent benefit in\none schedule over the other, and it demonstrates that increasing the n in n-SPSA while fixing the\nnumber of forward passes allowed results in only marginal gains at best.\nn\nSchedule\nSST-2\nSNLI\nTREC\nn = 1\nConstant\n89.6 (1.2)\n65.1 (6.2)\n66.7 (6.2)\nn = 4\nConstant\n89.5 (1.1)\n68.6 (3.2)\n62.3 (5.6)\nn = 4\nLinear\n89.6 (1.4)\n65.3 (6.4)\n66.1 (5.5)\nn = 16\nConstant\n90.4 (0.7)\n67.0 (3.4)\n62.8 (6.3)\nn = 16\nLinear\n88.9 (1.2)\n62.8 (5.9)\n64.2 (5.3)\nTable 6: Experiments using MeZO with different schedules for n. We scale the learning rate\nproportionally to the number of z\u2019s sampled.\nB\nMeZO Variants\nThere is a rich history of transferring ideas from first order optimization to enhance ZO algorithms.\nBelow, we highlight several variants of MeZO that did not achieve as high performance as the\nalgorithm presented in Algorithm 1.\nB.1\nMemory-efficient n-SPSA\nWe highlight how MeZO can perform n-SPSA (Definition 1) efficiently for n > 1 in Algorithm 2.\nIn particular, if sampling n z vectors and averaging the projected gradients, we require storing 2n\nadditional scalars: the random seeds and the projected gradients. The same caveat about perturbing\nindividual weights versus entire weight matrices still applies here (see Section 2).\nB.2\nAugmenting MeZO with Gradient History\nThe n-SPSA algorithm merely provides a gradient estimate that can subsequently be used in place\nof the gradient in any gradient-based optimizer. Many popular optimizers, such as Adam and SGD\nwith momentum, require storing some historical information about gradients (e.g., a moving average).\nThis requirement causes such algorithms to require 2\u00d7 or 3\u00d7 the memory that is needed for SGD.\nHowever, one advantage of MeZO is that the gradient history can be recomputed at each step without\nrequiring much additional memory. In reference to Algorithm 1, note that the gradient only needs\nprojected_grad and the random seed s used to compute the perturbation z, so we need to only\nstore 2 scalars per step to reproduce the gradient history (i.e., up to 2T scalars during training).\nThis is a substantial reduction in added memory overhead that is usually needed for using Adam or\nmomentum instead of vanilla SGD.\nTable 18 illustrates that MeZO-Adam can sometimes improve the performance of MeZO, though\neach gradient step requires additional computation (but no additional forward passes). We leave it to\nfuture work to investigate when MeZO-Adam may be more useful than MeZO.\nExperiment\nHyperparameters\nValues\nMeZO-Adam\nBatch size\n64\nLearning rate\n{1e\u22126, 1e\u22125, 1e\u22124, 5e\u22124, 1e\u22123}\n\u03f5\n1e\u22123\nWeight Decay\n0\nTable 7: The hyperparameter grid used for MeZO-Adam. For simplicity, we use a constant learning\nrate schedule.\n19\nAlgorithm 2: MeZO with n > 1\nRequire: parameters \u03b8 \u2208 Rd, loss L : Rd \u2192 R, step budget T, perturbation scale \u03f5, batch size\nB, learning rate schedule {\u03b7t}, n for n-SPSA estimate (Definition 1)\nfor t = 1, ..., T do\nseeds, projected_grads \u2190 []\n\u25b7 Will each contain n scalars\nfor j = 1, ..., n do\nSample batch B \u2282 DB and random seed s\n\u03b8 \u2190 PerturbParameters(\u03b8, \u03f5, s)\n\u2113+ \u2190 L(\u03b8; B)\n\u03b8 \u2190 PerturbParameters(\u03b8, \u22122\u03f5, s)\n\u2113\u2212 \u2190 L(\u03b8; B)\n\u03b8 \u2190 PerturbParameters(\u03b8, \u03f5, s)\n\u25b7 Reset parameters\nprojected_grad \u2190 (\u2113+ \u2212 \u2113\u2212)/(2\u03f5)\nprojected_grads[j] \u2190 projected_grad\nseeds[j] \u2190 s\nend\nfor j = 1, ..., n do\nReset random number generator with seed seeds[j]\nfor \u03b8i \u2208 \u03b8 do\nz \u223c N(0, 1)\n\u03b8i \u2190 \u03b8i \u2212 (\u03b7t/n) \u2217 projected_grads[j] \u2217 z\n\u25b7 Avg grad for z1, ..., zn\nend\nend\nend\nSubroutine PerturbParameters(\u03b8, \u03f5, s)\nReset random number generator with seed s\n\u25b7 For sampling z\nfor \u03b8i \u2208 \u03b8 do\nz \u223c N(0, 1)\n\u03b8i \u2190 \u03b8i + \u03f5z\n\u25b7 Modify parameters in place\nend\nreturn \u03b8\nB.3\nModifying the Variance of MeZO\nOur theory in Section 4 sketches the well-known fact that the variance of the stochastic gradient\nestimate can impact the rate of optimization. ZO methods can be combined with standard variance\nreduction techniques to possibly improve optimization speed. For example, Liu et al. [62] designed a\nvariance reduced ZO algorithm, analogous to SVRG [49], to improve the speed of convergence. Below,\nwe show that several variance reduction methods (e.g., using the gradient norm) can be implemented\nin a memory-efficient manner. However, when controlling for the total budget of forward passes (i.e.,\nfunction queries), these methods are not as performant as MeZO. We nevertheless present them to\ndemonstrate the ease with which MeZO can be adapted, and we suggest these methods may be useful\nfor optimizing more complex objectives.\nFirst, we define a general SPSA estimate that has the same expectation (i.e., the true gradient) but has\na scaled variance.\nDefinition 6 (Variance-Modified SPSA). Given a matrix D = diag(d), the variance modified SPSA\ncomputes\ne\u2207L(\u03b8; B) = L(\u03b8 + \u03f5(d\u22121 \u2299 z); B) \u2212 L(\u03b8 \u2212 \u03f5(d\u22121 \u2299 z); B)\n2\u03f5\n(d \u2299 z)\nwhere d \u2208 Rd has nonzero entries and d\u22121 denotes the coordinatewise reciprocal.\nThe above SPSA variant is an unbiased estimator of the gradient, because E[e\u2207L(\u03b8; B)] =\nE[D\u22121zz\u22a4D\u2207L(\u03b8; B)] = E[\u2207L(\u03b8; B)]. We will draw inspiration from classical methods (i.e.,\n\u201ccontrol variates\u201d) and choose d to be a block vector with gradient norms or parameter norms [99].\n20\nTo select the parameter groups, we split the model by layer, keeping the embedding and the head\nseparate (i.e., RoBERTa-large has 24 + 2 = 26 parameter groups). It is straightforward to measure\nthe parameter norms without consuming additional memory. We can measure the gradient norms\nwithout performing backpropagation, as shown below.\nProposition 1 (ZO Estimate of Gradient Norm of \u2113th Layer). Define z\u2113 to have z \u223c N(0, 1) in\neach coordinate corresponding to parameters in the \u2113th layer and 0 everywhere else. Then, we can\nestimate the norm of the gradient of the loss w.r.t. the \u2113th layer \u2207\u03b8\u2113 as\n\u2225\u2207\u03b8\u2113L(\u03b8; B)\u22252 \u2248\n\f\f\f\f\nL(\u03b8 + \u03f5z\u2113; B) \u2212 L(\u03b8 \u2212 \u03f5z\u2113; B)\n2\u03f5\n\f\f\f\f\nAs is true for SPSA, increasing the number of z\u2113\u2019s sampled for each value of \u2113 and averaging the\nresult reduces the variance of the estimate. The rationale for this estimate is that for any vector v,\nEz[(\u27e8v, z\u27e9)2] = \u2225v\u22252\n2 for Gaussian z. It is clear that this estimate can be computed in a memory\nefficient way, although it requires 2L forward passes to compute gradient norms for L parameter\ngroups.\nWe show the experimental results for modifying the variance below. We follow the ablation setting\nand use a fixed budget of 10, 000 steps (Appendix A). Generally, using the gradient norm to reduce\nthe variance substantially hurts performance (Table 8). If we \u201ccheat\u201d and allow one backpropagation\nthrough the network to estimate the gradient norm, then we see that reducing the variance using the\ngradient norm does not substantially hurt or help performance. Modifying the variance using the\nparameter norm, analogous to layerwise adaptive rate methods, does not substantially impact the\nperformance of MeZO (Table 9).\nOur observation is that decreasing the variance by setting d as the gradient norm does not improve\noptimization. This empirical result agrees with the exposition in Section 4 that the straightforward\nvariance analysis (which yields a dependence on the number of parameters d) is not the best lens to\nstudy the rate of optimization when fine-tuning with MeZO. Our effective rank view in Theorem 1\nand Lemma 3 is likely a better characterization of fine-tuning dynamics. We leave it to future work to\nexplore if these methods can be useful for other more complex objectives.\nRecompute d\nZO estimate of d\nSST-2\nSNLI\nTREC\nBaseline MeZO (Algorithm 1)\n89.6 (1.2)\n65.1 (6.2)\n66.7 (6.2)\n89.7 (0.8)\n65.2 (5.2)\n64.3 (6.4)\n87.0 (2.5)\n49.6 (9.2)\n32.6 (7.7)\n79.0 (10.3)\n48.9 (2.2)\n38.7 (7.5)\nTable 8: Experiments modifying the variance of MeZO using d as the gradient norm (see Definition 6).\nWe sometimes recompute d at the start of each epoch or use Proposition 1 to estimate d without\nrequiring backpropagation.\nRecompute d\nSST-2\nSNLI\nTREC\nBaseline MeZO (Algorithm 1)\n89.6 (1.2)\n65.1 (6.2)\n66.7 (6.2)\n89.2 (2.1)\n65.4 (4.2)\n64.8 (5.6)\n88.2 (4.7)\n65.2 (4.0)\n64.7 (5.5)\nTable 9: Experiments modifying the variance of MeZO using d as the parameter norm (see Defini-\ntion 6). We sometimes recompute d at the start of each epoch.\nB.4\nModifying the Expectation of MeZO\nThe above experiments show that modifying the variance of MeZO cannot consistently accelerate its\nconvergence. However, a simple modification of Definition 6 allows us to change the expectation of\nMeZO as well. This can be used to efficiently estimate coordinate-wise normalized gradient-based\noptimizer updates (e.g., Adam).\n21\nDefinition 7 (Expectation-Modified SPSA). Given a matrix D = diag(d), the variance modified\nSPSA computes\ne\u2207L(\u03b8; B) = L(\u03b8 + \u03f5(d\u22121 \u2299 z); B) \u2212 L(\u03b8 \u2212 \u03f5(d\u22121 \u2299 z); B)\n2\u03f5\nz\nwhere d \u2208 Rd.\nNow, we see that e\u2207L(\u03b8; B) = E[D\u22121zz\u22a4\u2207L(\u03b8; B)] so the SPSA estimate is no longer an unbiased\nestimator for \u2207L(\u03b8). If we choose d to be the gradient norm, for example, then SPSA can estimate\nthe normalized gradient. Concurrent work in Tang et al. [94] gives another ZO estimate of the\nnormalized gradient while assuming access to only rankings of inputs (instead of the noisy function\nevaluations available in our setting). We find that estimating the normalized gradient does not perform\nas well as directly estimating the gradient (Table 10). Regardless, we present this algorithm as a way\nto highlight that any coordinate-wise operation to the gradient can be applied in a memory-efficient\nmanner.\nMethod\nSST-2\nSNLI\nTREC\nBaseline MeZO (Algorithm 1)\n89.6 (1.2)\n65.1 (6.2)\n66.7 (6.2)\nEstimate of normalized gradient (Definition 7)\n88.0 (1.2)\n60.0 (2.4)\n44.0 (14.0)\nTable 10: Experiments modifying the expectation of MeZO using d as the gradient norm (see\nDefinition 7). We use the ZO estimate of the gradient norm (Proposition 1).\nB.5\nOne-point estimate\nHere, we investigate the efficacy of one-point gradient estimators in place of the two-point SPSA\nmethod. Using a one-point estimator instead of SPSA can reduce the MeZO running time by half.\nMany one-point estimators have been proposed in the past [34, 87, 95]. For simplicity, we focus\non one estimator [113] that has a form reminiscent of SPSA but requires only one forward pass to\nestimate the gradient at each step.\nDefinition 8 (One-Point Gradient Estimate, Zhang et al. [113]). For a loss function L evaluated on a\nbatch Bt with parameters \u03b8t at step t, we can draw random noise zt \u223c N(0, Id) and compute the\ngradient estimate using hyperparameter \u03f5 as written below.\nb\u2207L(\u03b8t; Bt) = L(\u03b8t + \u03f5zt; Bt) \u2212 L(\u03b8t\u22121 + \u03f5zt\u22121; Bt\u22121)\n\u03f5\nNotably, this one-point gradient estimate uses the loss at the previous iterate instead of evaluating the\nloss again at the current iterate. As such, this estimator requires only one forward pass at each iterate\nto estimate the gradient. For well-behaved loss functions and slow-moving optimization, these two\nformulas are intuitively similar. However, Table 11 finds this estimator to be much less efficient than\nSPSA when fixing the number of forward passes.\nSteps\nSST-2\nSST-5\nSNLI\nMNLI\nRTE\nTREC\n\u2014\u2014 sentiment \u2014\u2014\n\u2014\u2014 natural language inference \u2014\u2014\n\u2014 topic \u2014\nSPSA [88]\n20K\n92.8 (0.5)\n51.3 (0.9)\n82.9 (1.0)\n74.4 (0.8)\n76.7 (1.7)\n92.7 (0.6)\nOne-point estimate [113]\n20K\n90.0 (0.4)\n44.6 (2.0)\n70.1 (1.5)\n57.2 (0.9)\n64.1 (1.0)\n57.3 (5.7)\nOne-point estimate [113]\n40K\n91.8 (0.5)\n45.9 (1.7)\n74.4 (0.8)\n61.0 (1.0)\n68.7 (1.2)\n73.0 (3.1)\nTable 11: Comparison between SPSA and a one-point estimate Zhang et al. [113]. One-point estimate\nonly does one forward pass per step, thus is twice as fast as two-point estimate per step. As such, the\nnumber of forward passes after 40K steps with the one-point estimate is the same as the number of\nforward passes with SPSA after 20K steps. The results show that two-point estimate is much more\neffective than one-point estimate.\nC\nMemory Analysis\nThe compute-memory tradeoff of backpropagation is complex to analyze. Griewank and Walther\n[40] provides a rigorous theoretical treatment of the problem. We empirically measure the memory\n22\nconsumption of different methods for commonly used large language models, but here we hope to\nprovide a more rigorous comparison of different gradient estimation algorithms, independent of the\nsoftware used to implement them. Below, we summarize some key points that may help readers to\nunderstand how the MeZO compute-memory tradeoff compares to backpropagation.\nGiven a network, the first step to perform backpropagation is to decompose the model into easily\ndifferentiable blocks. We note that this decomposition is not unique. For each block, one can\nchoose to cache the resulting output during the forward pass (thereby consuming memory) or instead\nrecompute the output when it is needed (thereby consuming compute). The below proposition,\nadapted from Rule 21 in Griewank and Walther [40], captures this tradeoff.\nProposition 2 (Time-Memory Tradeoff for Backpropagation, Griewank and Walther [40]). Consider\na network containing N bits. For any time-memory tradeoff hyperparameter c = O(1), there exists a\nbackpropagation algorithm that runs in time O(cN) and consumes memory proportional to O(N 1/c).\nGrimm et al. [41] also gave sharp bounds for the memory-time product. Note that the popular gradient\ncheckpointing [18] method allows one to tune c with limited precision (i.e., one cannot always further\nsplit a differentiable block and observe savings). Experiments in Chen et al. [18] choose c = 2\nto achieve O(\n\u221a\nN) memory while consuming O(2N) computation. In the extreme case, gradient\ncheckpointing allows one to use O(N log N) computation and O(log N) memory.\nMeZO always consumes 2N compute and O(1) memory, so it is more compute-efficient at the same\nmemory cost as gradient checkpointing. Our exposition in Section 2 discusses that we can perturb\ngroups of parameters together to save time while consuming additional memory. However, we do\nnot consider that variant here because it is somewhere in the middle of the compute-memory pareto\ncurve, where we cannot reason about what backpropagation will do. In particular, MeZO can split\ngroups differently than backpropagation can, since MeZO does not require that each parameter group\nis easily differentiable, so it is hard to compare the two algorithms along the entire pareto curve.\nWe also compare backpropagation for the c = 1 case (i.e., storing everything during the forward\npass). When storing everything, backpropagation consumes O(N) time and O(N) memory. Hence,\nSPSA consumes slightly more time and substantially less memory than backpropagation at this end\nof the tradeoff.\nUnlike gradient checkpointing, MeZO computes only an approximation of the gradient. This\napproximation is only useful for fine-tuning with a prompt, making it less broadly useful than\ngradient checkpointing. There are other methods that approximate the gradient with less memory\nconsumption than gradient checkpointing (see the Related Work section), though it is unclear how\nthe memory consumption of those algorithms compare to MeZO.\nD\nForward Auto-Differentiation\nWe discuss the merits of using forward auto-differentiation instead of two forward passes to construct\na gradient estimate for fine-tuning. As \u03f5 \u2192 0, the SPSA gradient estimate (Definition 1) can be written\nas zz\u22a4\u2207L(\u03b8; B). The term z\u22a4\u2207L(\u03b8; B) is a Jacobian-vector product (JVP), and it is well-known\nthat this can be computed in parallel with a single forward pass while consuming additional memory\nequivalent to that of the largest activation in the model. To fully compute the gradient estimate, one\nmust store z on the GPU while performing inference, so we observe that this algorithm requires more\nmemory than MeZO.\nWe note that implementation of the forward auto-differentiation algorithm is not well-supported in\nPyTorch at the time of writing. The autograd JVP function computes the JVP in a memory-inefficient\nway, as noted in the documentation, and the other available methods to compute a JVP are not\nsophisticated enough to easily scale to a complex LLM. Computing the JVP is straightforward when\nusing JAX, so we profile the memory consumption of inference and the JVP for RoBERTa-large\nwhen using JAX. We use batch size 16 with the MultiRC task. Note that JAX may automatically\nuse rematerialization to avoid out of memory errors so we focus on settings in which the memory\nutilization remains below 50%. The resulting memory usage during inference, backpropagation, and\nforward auto-differentiation are reported in Table 12.\n23\nWe see that forward auto-differentiation is substantially more memory efficient than backpropagation\nbut less memory efficient than inference. Furthermore, forward auto-differentiation selects \u03f5 = 0,\nwhich removes potentially beneficial third-and-higher order Taylor expansion terms from the estimate.\nTask\nInference (and MeZO)\nBackpropagation\nForward Auto-Differentiation\nExcess Memory (MB)\n327.50\n24156.23\n830.66\nTable 12: Memory consumption of RoBERTa-large when using batch size 16 with the MultiRC task.\nThe reported memory does not include the cost of storing the model on the GPU, which is required\nfor all three cases.\nE\nExperiment setup\nE.1\nDatasets\nFor RoBERTa-large, we consider classification datasets: SST-2 [86], SST-5 [86], TREC [97],\nMNLI [103], SNLI [12], and RTE [22, 8, 37, 10]. We follow Malladi et al. [67] in limiting the\ntest set to 1, 000 examples for fast iteration. For training and validation, we have two settings: k = 16\nand k = 512, which mean that we have 16 or 512 examples per class for both training and validation.\nFor OPT experiments, we consider the SuperGLUE dataset collection [98], including: BoolQ [21],\nCB [24], COPA [81], MultiRC [51], ReCoRD [111], RTE [22, 8, 37, 10], WiC [77], and WSC [55].\nWe also include SST-2 [86] and two question answering (QA) datasets, SQuAD [80] and DROP [31].\nWe randomly sample 1,000 examples for training, 500 examples for validation, and 1,000 examples\nfor testing.\nE.2\nPrompts\nTable 13 shows the set of downstream tasks and prompts with which we fine-tune RoBERTa-large,\nwhich are adapted from [35].\nDataset\nC\nType\nPrompt\nLabel words\nSST-2\n2\nsentiment cls.\n<S1> It was [MASK] .\n{great, terrible}\nSST-5\n5\nsentiment cls.\n<S1> It was [MASK] .\n{great, good, okay, bad, terrible}\nTREC\n6\ntopic cls.\n[MASK] : <S1>\n{Description, Expression, Entity,\nHuman, Location, Number}\nMNLI\n3\nNLI\n<S1> ? [MASK] , <S2>\n{Yes, Maybe, No}\nSNLI\n3\nNLI\n<S1> ? [MASK] , <S2>\n{Yes, Maybe, No}\nRTE\n2\nNLI\n<S1> ? [MASK] , <S2>\n{Yes, No}\nTable 13: The prompts of the datasets we used in our RoBERTa-large experiments (Table 18 and\nFigure 2). The prompts are adapted from [35] and include a template and a set of label words that\ncan fill in the [MASK]token. <S1> and <S2> refer to the first and the second (if any) input sentence.\nTable 14 demonstrates the prompts we use for OPT. Note that in OPT experiments we have three\ntypes of tasks: classification, multiple-choice, and question answering. Prompts are adopted from\nGPT-3 [13] and PromptSource with minor changes [5].\nE.3\nHyperparameters\nWe use the hyperparameters in Table 15 for MeZO experiments on RoBERTa-large (Table 18 and\nFigure 2). Experiments in Appendix A informed the grid; in particular, the choice of \u03f5 seemed\nto not significantly impact performance, and using a larger batch size consistently yielded faster\noptimization. We use the hyperparameters in Table 16 for MeZO experiments on OPT.\nRegarding learning rate scheduling and early stopping, we use linear learning scheduling for all\nfine-tuning with backpropagation experiments and constant learning rate for all MeZO experiments.\nFor RoBERTa experiments, we evaluate the model on validation sets every 1/10 of total training steps\nand save the best validation checkpoint. All FT experiments use 1K steps and MeZO experiments use\n24\nDataset\nType\nPrompt\nSST-2\ncls.\n<text> It was terrible/great\nRTE\ncls.\n<premise>\nDoes this mean that \"<hypothesis>\" is true? Yes or No?\nYes/No\nCB\ncls.\nSuppose <premise> Can we infer that \"<hypothesis>\"? Yes, No, or Maybe?\nYes/No/Maybe\nBoolQ\ncls.\n<passage> <question>?\nYes/No\nWSC\ncls.\n<text>\nIn the previous sentence, does the pronoun \"<span2>\" refer to <span1>? Yes or No?\nYes/No\nWIC\ncls.\nDoes the word \"<word>\" have the same meaning in these two sentences? Yes, No?\n<sent1>\n<sent2>\nYes/No\nMultiRC\ncls.\n<paragraph>\nQuestion: <question>\nI found this answer \"<answer\". Is that correct? Yes or No?\nYes/No\nCOPA\nmch.\n<premise> so/because <candidate>\nReCoRD\nmch.\n<passage>\n<query>.replace(\"@placeholder\", <candidate>)\nSQuAD\nQA\nTitle: <title>\nContext: <context>\nQuestion: <question>\nAnswer:\nDROP\nQA\nPassage: <context>\nQuestion: <question>\nAnswer:\nTable 14: The prompts of the datasets we used in our OPT experiments. There are three types of tasks:\nclassification (cls.), multiple-choice (mch.), and question answering (QA). Prompts are adopted from\nGPT-3 [13] and PromptSource [5] with minor changes. <text> represents input from the dataset and\nYes represents label words. For inference on multiple choice tasks, we put in different candidates\nin the prompt and calculate the average log-likelihood for each candidate, and choose the candidate\nwith the highest score. For inference on QA tasks, we use greedy decoding to generate the answer.\n100K steps. For OPT experiments, we evaluate the model on validation sets every 1/5 of the total\ntraining steps and save the best validation checkpoint. All FT experiments train for 5 epochs and all\nMeZO experiments use 20K steps. Note that FT experiments mostly converge within 5 epochs but\nwe observe that MeZO performance can still improve with more training steps.\nE.4\nModeling and implementation\nFor RoBERTa experiments, we follow [35] for the prompt-based fine-tuning paradigm for masked\nlanguage models. Please refer to the original paper for more details.\nIn OPT experiments, for classification tasks, we train the model similarly to [35], i.e., we take the\nlogits corresponding to the label words and apply cross entropy loss on them; for multiple choice\ntasks and generation tasks (QA), we only keep the correct candidate and use teacher forcing to train\non the correct examples. We only keep the loss on tokens in the candidate part and exclude the prompt\npart.\nFor OPT inference on classification and multiple-choice tasks, we use the model to get the average\nlog-likelihood (by tokens) of all the candidates/label words, and predict the one with the highest\naverage log-likelihood. For generation tasks, we use greedy decoding to generate the answer.\nFor in-context learning, we use 32 examples in the context. We also try filling in as many examples\nas possible in the context but this does not improve performance and sometimes leads to unstable\nresults. Thus we keep the 32-example results.\n25\nExperiment\nHyperparameters\nValues\nMeZO\nBatch size\n64\nLearning rate\n{1e\u22127, 1e\u22126, 1e\u22125}\n\u03f5\n1e\u22123\nWeight Decay\n0\nMeZO (prefix)\nBatch size\n64\nLearning rate\n{1e\u22122, 5e\u22123, 1e\u22123}\n\u03f5\n1e\u22121\nWeight Decay\n0\n# prefix tokens\n5\nMeZO (LoRA)\nBatch size\n64\nLearning rate\n{1e\u22125, 5e\u22125, 1e\u22124}\n\u03f5\n1e\u22123\nWeight Decay\n0.1\n(r, \u03b1)\n(8, 16)\nFT with Adam\nBatch size (k = 16)\n{2, 4, 8}\nBatch size (k = 512)\n{8, 16, 32}\nLearning Rates\n{1e\u22125, 3e\u22125, 5e\u22125}\nWeight Decay\n0\nFT with SGD\nBatch size (k = 16)\n{2, 4, 8}\nBatch size (k = 512)\n{8, 16, 32}\nLearning Rates\n{1e\u22124, 5e\u22124, 1e\u22123, 5e\u22123, 1e\u22122}\nWeight Decay\n0\nFT (prefix)\nBatch size\n{8, 16, 32}\nLearning Rates\n{1e\u22122, 3e\u22122, 5e\u22122}\nWeight Decay\n0\n# prefix tokens\n5\nFT (LoRA)\nBatch size\n{4, 8, 16}\nLearning Rates\n{1e\u22124, 3e\u22124, 5e\u22124}\n(r, \u03b1)\n(8, 16)\nTable 15: The hyperparameter grids used for RoBERTa-large experiments. MeZO uses a constant\nlearning rate schedule, and FT uses linear scheduling. All FT experiments use 1K steps and MeZO\nexperiments use 100K steps. We check validation performance every 1/10 total training steps.\nFor linear probing of classification tasks, we take the output feature and use scipy package to train\na linear classifier. For multiple-choice tasks and generation tasks, we found that this leads to poor\nresults since the output space is the whole vocabulary; instead, we do head-tuning, where the whole\nmodel is fixed except for the LM projection head. We use a batch size of 8 and a learning rate of\n{1e\u22124 5e\u22124}, and train the head for 5 epochs.\nFor experiments on 30B and 66B OPT models, we largely follow the OPT hyperparameters except\nthat we do not evaluate the intermediate validation performance and directly use the last checkpoint\nfor evaluation, due to the high storage cost of intermediate checkpoints of large models.\nE.5\nParameter-efficient fine-tuning\nFine-tuning and storing a copy of the large language model for each downstream task is expensive.\nParameter-efficient fine-tuning (PEFT) techniques alleviate this problem: instead of tuning all model\nparameters, PEFT only tunes a small number of additional parameters (usually less than 1%) and can\noften achieve comparable or better performance [57, 54, 30]. The ZO optimizer is compatible with\nPEFT methods, since ZO can operate on any subset of the model parameters. We are interested in the\nfollowing two common PEFT methods, designed for transformers [96].\nLoRA [46] adds a tunable low-rank delta to a linear layer during fine-tuning. Suppose a linear layer\nperformed W x + b during pre-training with W \u2208 Rm\u00d7n. When fine-tuning, LoRA introduces two\nsmaller matrices A \u2208 Rm\u00d7r and B \u2208 Rr\u00d7n such that r \u226a min(m, n). The linear layer is then\n26\nExperiment\nHyperparameters\nValues\nMeZO\nBatch size\n16\nLearning rate\n{1e\u22126, 1e\u22127} or {1e\u22126, 5e\u22127, 1e\u22127} for SQuAD and DROP\n\u03f5\n1e\u22123\nMeZO (prefix)\nBatch size\n16\nLearning rate\n{1e\u22122, 1e\u22123} or {5e\u22122, 1e\u22122, 5e\u22123} for SQuAD and DROP\n\u03f5\n1e\u22121\n# prefix tokens\n5\nMeZO (LoRA)\nBatch size\n16\nLearning rate\n{1e\u22124, 5e\u22125} or {1e\u22124, 5e\u22125, 1e\u22125} for SQuAD and DROP\n\u03f5\n1e\u22122\n(r, \u03b1)\n(8, 16)\nFT with Adam\nBatch size\n8\nLearning Rates\n{1e\u22125, 5e\u22125, 8e\u22125}\nTable 16: The hyperparameter grids used for OPT experiments. All weight decay is set to 0. FT uses\n5 epochs and linear scheduled learning rates and MeZO uses 20K steps and constant learning rates.\nWe check validation performance and save the best checkpoint every 1/5 total training steps.\ncomputed as\n\u0010\nW + \u03b1\nr AB\n\u0011\nx + b\n(6)\nwhere r and \u03b1 are hyperparameters. A and B are trained on the downstream task while W is frozen\nat its pre-trained value. In transformers, this modification to the linear layer is applied to the query\nand value operations of each attention layer. Empirically, r can be very small, so the number of\ntrainable parameters during fine-tuning is small. We choose r = 8 and \u03b1 = 16.\nPrefix-tuning [57] adds a prefix of m tunable representations at each layer and freezes the rest of the\nmodel. The representations are added as new keys and values and treated as additional context during\nthe attention operation. We initialize these tunable representations by randomly sampling tokens from\nthe vocabulary and passing them through the LLM to get their keys and values at different attention\nlayers. We found this crucial to make prefix tuning stable with MeZO, and this trick additionally\nboosts the performance of prefix tuning with backpropagation, as shown in Table 17. We also tried\nthe reparameterization trick in [57], which does not help MeZO training. In our experiments, we find\nm = 5 to be sufficient to achieve good performance on most tasks.\nWe also show that MeZO is compatible with parameter-efficient fine-tuning methods, such as prefix\ntuning and LoRA. Surprisingly, the performance of MeZO does not improve substantially when tuning\nmuch fewer parameters, as one might expect from classical analyses (see Section 4). Accordingly,\nour theoretical analysis in Section 4 suggests that the convergence rate of ZO-SGD does not depend\non the parameter dimension during fine-tuning.\nTask\nSST-2\nSST-5\nSNLI\nMNLI\nRTE\nTREC\nType\n\u2014\u2014 sentiment \u2014\u2014\n\u2014\u2014 natural language inference \u2014\u2014\n\u2014 topic \u2014\nFT (prefix, random init)\n90.7 (1.7)\n47.2 (2.0)\n70.7 (2.8)\n62.6 (3.3)\n63.5 (4.4)\n83.4 (4.7)\nFT (prefix, real act init)\n91.9 (1.0)\n47.7 (1.1)\n77.2 (1.3)\n66.5 (2.5)\n66.6 (2.0)\n85.7 (1.3)\nTable 17: Prefix-tuning ablations. We compare randomly-initialized prefixes and real word activation\nprefixes. Using real word activations significantly outperforms random initialization.\nE.6\nTraining with non-differentiable objectives\nThe experiments maximizing the accuracy of a RoBERTa-large model were all conducted using the\nsame grid as MeZO in Table 15.\nFor OPT experiments on SQuAD with F1 as objective, we use a batch size of 16. For MeZO, we use\na learning rate of {1e\u22126, 5e\u22126, 1e\u22125} and \u03f5 = 1e\u22123. For MeZO (prefix), we use a learning rate of\n{1e\u22121, 5e\u22122, 1e\u22122} and \u03f5 = 1e\u22121.\n27\nE.7\nMemory profiling\nIn memory profiling, we use standard implementation with Huggingface\u2019s transformers [104]\npackage. We did not turn on any advance memory-saving options, e.g., gradient checkpointing. We\nset the per-device batch size as 1 to test the minimum hardware requirement to run the model with\nspecific optimization algorithms. For multi-GPU backpropagation, we use fully sharded data parallel\n(FSDP) [33] provided by PyTorch [76]. For multi-GPU MeZO, we use transformers multi-GPU\ninference of large models. We use Nvidia\u2019s nvidia-smi command to monitor the GPU memory\nusage. We call a run \u201csuccessful\u201d if there is no out of memory error from GPUs for at least 100 steps.\nWe also profile fine-tuning with LoRA, but find its memory usage similar to that of fine-tuning with\nprefix-tuning. Hence here we only show the analysis with prefix-tuning.\nF\nMore experiment results\nF.1\nRoBERTa-large experiments\nTable 18 contains the detailed numbers corresponding to Figure 2 and also reports the performance of\nMeZO-Adam.\nTask\nSST-2\nSST-5\nSNLI\nMNLI\nRTE\nTREC\nType\n\u2014\u2014 sentiment \u2014\u2014\n\u2014\u2014 natural language inference \u2014\u2014\n\u2014 topic \u2014\nZero-shot\n79.0\n35.5\n50.2\n48.8\n51.4\n32.0\nGradient-free methods: k = 16\nLP\n76.0 (2.8)\n40.3 (1.9)\n66.0 (2.7)\n56.5 (2.5)\n59.4 (5.3)\n51.3 (5.5)\nMeZO\n90.5 (1.2)\n45.5 (2.0)\n68.5 (3.9)\n58.7 (2.5)\n64.0 (3.3)\n76.9 (2.7)\nMeZO (LoRA)\n91.4 (0.9)\n43.0 (1.6)\n69.7 (6.0)\n64.0 (2.5)\n64.9 (3.6)\n73.1 (6.5)\nMeZO (prefix)\n90.8 (1.7)\n45.8 (2.0)\n71.6 (2.5)\n63.4 (1.8)\n65.4 (3.9)\n80.3 (3.6)\nMeZO-Adam\n90.4 (1.4)\n45.4 (1.5)\n74.1 (2.7)\n64.3 (0.8)\u2020\n59.2 (11.1)\u2020\n78.3 (1.4)\nGradient-based methods: k = 16\nFT\n91.9 (1.8)\n47.5 (1.9)\n77.5 (2.6)\n70.0 (2.3)\n66.4 (7.2)\n85.0 (2.5)\nFT (LoRA)\n91.4 (1.7)\n46.7 (1.1)\n74.9 (4.3)\n67.7 (1.4)\n66.1 (3.5)\n82.7 (4.1)\nFT (prefix)\n91.9 (1.0)\n47.7 (1.1)\n77.2 (1.3)\n66.5 (2.5)\n66.6 (2.0)\n85.7 (1.3)\nGradient-free methods: k = 512\nLP\n91.3 (0.5)\n51.7 (0.5)\n80.9 (1.0)\n71.5 (1.1)\n73.1 (1.5)\n89.4 (0.5)\nMeZO\n93.3 (0.7)\n53.2 (1.4)\n83.0 (1.0)\n78.3 (0.5)\n78.6 (2.0)\n94.3 (1.3)\nMeZO (LoRA)\n93.4 (0.4)\n52.4 (0.8)\n84.0 (0.8)\n77.9 (0.6)\n77.6 (1.3)\n95.0 (0.7)\nMeZO (prefix)\n93.3 (0.1)\n53.6 (0.5)\n84.8 (1.1)\n79.8 (1.2)\n77.2 (0.8)\n94.4 (0.7)\nMeZO-Adam\n93.3 (0.6)\n53.9 (0.8)\n85.3 (0.8)\n79.6 (0.4)\n79.2 (1.2)\n95.1 (0.3)\nGradient-based methods: k = 512\nFT\n93.9 (0.7)\n55.9 (0.9)\n88.7 (0.8)\n84.4 (0.8)\n82.7 (1.4)\n97.3 (0.2)\nFT (LoRA)\n94.2 (0.2)\n55.3 (0.7)\n88.3 (0.5)\n83.9 (0.6)\n83.2 (1.3)\n97.0 (0.3)\nFT (prefix)\n93.7 (0.3)\n54.6 (0.7)\n88.3 (0.7)\n83.3 (0.5)\n82.5 (0.8)\n97.4 (0.2)\nTable 18: Experiments on RoBERTa-large (350M parameters). LP: Linear probing; ZO, ZO (LoRA),\nand ZO (prefix): our memory-efficient ZO-SGD (Section 2.1) with full-parameter tuning, LoRA, and\nprefix-tuning respectively; FT: fine-tuning with Adam. All reported numbers are averaged accuracy\n(standard deviation). All experiments use prompts (Appendix E.2). ZO outperforms zero-shot and\nLP by a large margin and approaches FT performance with much less memory cost.\nLP-MeZO\nWe also compare MeZO to performing linear probing and then subsequently performing\nfine-tuning via MeZO, following the analogous suggestion for fine-tuning in Kumar et al. [53]. We use\nthe MeZO grid described in Table 15. Note that the linear probing checkpoints used here have early\nstopping, unlike the ones reported in Table 18. We heuristically implement early stopping by limiting\nthe number of iterations (from 5000 to 1000) and increasing the convergence tolerance (from 1e\u22124 to\n0.01) in the scipy solver. Experiments on a few settings show that LP-MeZO can sometimes improve\nperformance without increasing the memory consumption (see Table 19). However, sometimes, linear\nprobing first can severely hurt performance.\n28\nTask\nSST-2\nSST-5\nSNLI\nTREC\nZero-shot\n79.0\n35.5\n50.2\n32.0\nFT\n91.9 (1.8)\n47.5 (1.9)\n77.5 (2.6)\n85.0 (2.5)\nMeZO\n90.5 (1.2)\n45.5 (2.0)\n68.5 (3.9)\n76.9 (2.7)\nLP-MeZO\n91.4 (1.4)\n41.9 (3.3)\n70.7 (3.4)\n54.0 (4.5)\nTable 19: Performing linear probing before fine-tuning with MeZO, as suggested previously [53],\ncan sometimes improve performance without increasing the memory overhead. We use k = 16 for\nthese experiments.\nF.2\nOPT experiments\nTable 20 present the full results of OPT-30B and OPT-66B, with detailed MeZO numbers.\nTask\nSST-2\nRTE\nBoolQ\nWSC\nWIC\nSQuAD\n30B zero-shot\n56.7\n52.0\n39.1\n38.5\n50.2\n46.5\n30B ICL\n81.9\n66.8\n66.2\n56.7\n51.3\n78.0\n30B MeZO\n90.6\n66.4\n67.2\n63.5\n56.3\n85.2\n30B MeZO (prefix)\n87.5\n72.6\n73.5\n55.8\n59.1\n83.9\n66B zero-shot\n57.5\n67.2\n66.8\n43.3\n50.6\n48.1\n66B ICL\n89.3\n65.3\n62.8\n52.9\n54.9\n81.3\n66B MeZO\n91.2\n65.7\n72.7\n63.5\n58.9\n*\n66B MeZO (prefix)\n93.6\n66.4\n73.7\n57.7\n58.6\n85.0\nTable 20: Experiments on OPT-30B and OPT-66B (with 1,000 examples). *: MeZO requires further\ntuning to successfully optimize.\nF.3\nConvergence of MeZO with full-parameter and PEFT\nWe demonstrate the convergence rate of MeZO, MeZO (LoRA) and MeZO (prefix) on SST-2 and\nSNLI for the first 5,000 steps in Figures 5. We see that despite the different number of parameters\nthey optimize, MeZO demonstrates similar training speed on full parameter and PEFT. This agrees\nwith our theory in Section 4, which shows that MeZO\u2019s optimization speed is independent of the\nnumber of parameters.\n0\n500\n1000\n1500\n2000\n2500\n3000\nSteps\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nLoss\nSST-2\nMeZO Train\nMeZO (LoRA) Train\nMeZO (prefix) Train\n0\n500\n1000\n1500\n2000\n2500\n3000\nSteps\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nLoss\nSNLI\nMeZO Train\nMeZO (LoRA) Train\nMeZO (prefix) Train\nFigure 5: MeZO does not optimize significantly faster when tuning fewer parameters, agreeing with\nour theory in Section 4.\n29\nF.4\nZO vs BBTv2\nWe compare ZO with BBTv2 [90] on mutually assessed tasks in Table 21. ZO significantly outperform\nBBTv2. Furthermore, BBTv2 is limited to optimize in low-dimensional space and requires prefix-\ntuning and a down-projection to reduce the number of optimized parameters. BBTv2 also employs\nan iterative scheme which only optimizes one layer at a time. In contrast, ZO works with both\nfull-parameter tuning and PEFT, as shown in our experiments (Section 3) and theory (Section 4).\nTask\nSST-2\nSNLI\nRTE\nTask type\n\u2014\u2014 sentiment \u2014\u2014\n\u2013 natural language inference \u2013\nZero-shot\n79.0\n50.2\n51.4\nBBTv2\n90.3 (1.7)\n57.3 (2.3)\n56.7 (3.3)\nMeZO\n90.5 (1.2)\n68.5 (3.9)\n64.0 (3.3)\nMeZO (LoRA)\n91.4 (0.9)\n69.7 (6.0)\n64.9 (3.6)\nMeZO (prefix)\n90.8 (1.7)\n71.6 (2.5)\n65.4 (3.9)\nTable 21: ZO vs BBTv2 with RoBERTa-large. BBTv2 performance is from Sun et al. [90].\nF.5\nMemory profiling\nWe show the detailed numbers of memory profiling results Table 22, which also corresponds to\nFigure 3. For how we profile the memory usage, please refer to Appendix E.7.\nMethod\nZero-shot / MeZO\nICL\nPrefix FT\nFull-parameter FT\n1.3B\n1xA100 (4GB)\n1xA100 (6GB)\n1xA100 (19GB)\n1xA100 (27GB)\n2.7B\n1xA100 (7GB)\n1xA100 (8GB)\n1xA100 (29GB)\n1xA100 (55GB)\n6.7B\n1xA100 (14GB)\n1xA100 (16GB)\n1xA100 (46GB)\n2xA100 (156GB)\n13B\n1xA100 (26GB)\n1xA100 (29GB)\n2xA100 (158GB)\n4xA100 (316GB)\n30B\n1xA100 (58GB)\n1xA100 (62GB)\n4xA100 (315GB)\n8xA100 (633GB)\n66B\n2xA100 (128GB)\n2xA100 (134GB)\n8xA100\n16xA100\nTable 22: Memory usage on the MultiRC (avg #tokens=400) dataset.\nF.6\nWallclock time efficiency\nIn this section, we measure the wallclock time efficiency of MeZO compared to full-parameter FT,\nwith respect to different model sizes. We conduct our experiments with 80GB A100s connected by\nNVLink and InfiniteBand, which are state-of-the-art solutions for distributed training. As shown in\nTable 23, on the MultiRC datasets, training with MeZO brings 7.74\u00d7 speedup per step compared to\nfull-parameter FT on a 30B model. This is due to (1) MeZO does not require costly backpropagation\nand (2) MeZO requires fewer GPUs and reduces the multi-GPU communication overhead. We can\nsee that MeZO has a bigger advantage when training larger models\u2014the multi-GPU overhead for\nfine-tuning is larger.\nNote that even though MeZO has better per-step wallclock efficiency, it requires significantly more\nsteps than standard FT. Taking our OPT-30B experiments as an example: MeZO takes 32\u00d7 more\nsteps than standard FT, while FT takes 8\u00d7 more GPUs and 7.74\u00d7 more time per step. Overall, MeZO\nrequires only half as many GPU-hours as FT for a 30B model.\n30\n1.3B\n2.7B\n13B\n30B\n66B\nMeZO (bsz=16)\n0.815s (1)\n1.400s (1)\n2.702s (1)\n5.896s (1)\n12.438s (4)\nMeZO (bsz=8)\n0.450s (1)\n0.788s (1)\n1.927s (1)\n4.267s (1)\n7.580s (2)\nFT (bsz=8)\n0.784s (1)\n1.326s (1)\n13.638s (4)\n45.608s (8)\n84.098s (20)\nbspd=2, ga=4\nbspd=2, ga=4\nbspd=1, ga=2\nbspd=1, ga=1\nbspd=1, ga=1\nTable 23: Wallclock time per step of different training methods. Numbers in brackets are numbers\nof GPUs required. It is measured on 80GB A100s with NVLink and InfiniteBand connections. The\nwallclock time is averaged over 100 steps. It is measured on the MultiRC task with the OPT family.\nWe use a batch size (\u201cbsz\u201d) of 8 for FT and 16 for MeZO (consistant with our main experiment\nsetting). For comparison we also add MeZO with a batch size of 8. For FT (FSDP), we show the\nfollowing additional information. \u201cbspd\u201d: batch size per device. \u201cga\u201d: gradient accumulation steps.\nThe effective batch size is bspd\u00d7ga\u00d7 #GPUs. Note that for FT 66B, the effective batch size 20.\n31\nG\nProofs\nProof of Lemma 2. We first note that in the \u03f5 \u2192 0 limit, we have\nb\u2207L(\u03b8; B) =\n1\nBn\nX\n(x,y)\u2208B\nX\ni\u2208[n]\nziz\u22a4\ni \u2207L(\u03b8; {(x, y)}).\nTaking expectation over the batch B and the zi, we have E[b\u2207L(\u03b8; B)] = \u2207L(\u03b8), so b\u2207L(\u03b8; B) is an\nunbiased estimator of the gradient.\nComputing the second moment, we get\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n=\n1\nB2n2\nX\n(x1,y1),(x2,y2)\u2208B\nX\ni,j\u2208[n]\nE\n\u0002\n(ziz\u22a4\ni \u2207L(\u03b8; {(x1, y1)}))(zjz\u22a4\nj \u2207L(\u03b8; {(x2, y2)}))\u22a4\u0003\nLet u, v be two arbitrary vectors. We have that\nEzi,zj[ziz\u22a4\ni uv\u22a4zjz\u22a4\nj ] = uv\u22a4\nwhen i \u0338= j, and\nEzi[ziz\u22a4\ni uv\u22a4ziz\u22a4\ni ] = Ez[z\u22974](u, v)\n=\n3d\nd + 2Sym(I\u22972)(u, v)\n=\nd\nd + 2 \u00b7 u\u22a4v \u00b7 I +\n2d\nd + 2 \u00b7 uv\u22a4.\nTherefore\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n= 1\nB2\nX\n(x1,y1),(x2,y2)\u2208B\n\u0012n \u2212 1\nn\n+\n2d\nn(d + 2)\n\u0013\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n+\nd\nn(d + 2) \u00b7 E\n\u0002\nL(\u03b8; {(x1, y1)})\u22a4L(\u03b8; {(x2, y2)})\n\u0003\nI.\nNext, note that when (x1, y1) \u0338= (x2, y2), we have\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n= \u2207L(\u03b8)\u2207L(\u03b8)\u22a4,\nand when (x1, y1) = (x2, y2) we have\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n= \u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + \u03a3MB(\u03b8).\nTherefore\n1\nB2\nX\n(x1,y1),(x2,y2)\u2208B\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n= \u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + 1\nB \u03a3(\u03b8),\nand plugging this yields\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n=\n\u0012\n1 +\nd \u2212 2\nn(d + 2)\n\u0013\n\u00b7\n\u0012\n\u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + 1\nB \u03a3(\u03b8)\n\u0013\n+\nd\nn(d + 2)I \u00b7\n\u0012\n\u2225\u2207L(\u03b8)\u22252 + 1\nB tr(\u03a3(\u03b8))\n\u0013\n.\n(7)\nFinally, we have\nE\n\u0014\r\r\rb\u2207L(\u03b8; B)\n\r\r\r\n2\u0015\n=\n\u0012\n1 + d2 + d \u2212 2\nn(d + 2)\n\u0013\n\u00b7\n\u0012\n\u2225\u2207L(\u03b8)\u22252 + 1\nB tr(\u03a3(\u03b8))\n\u0013\n= d + n \u2212 1\nn\n\u00b7 E\nh\n\u2225\u2207L(\u03b8; B)\u22252i\n.\n32\nProof of Theorem 1. By Taylor\u2019s theorem with remainder, we have that\nL(\u03b8t+1) = L(\u03b8t) + \u2207L(\u03b8t)\u22a4(\u03b8t+1 \u2212 \u03b8t)\n+\nZ 1\n0\n\u03bb(\u03b8t+1 \u2212 \u03b8t)\u22a4\u22072L(\u03bb\u03b8t+1 + (1 \u2212 \u03bb)\u03b8t)(\u03b8t+1 \u2212 \u03b8t)\u22a4d\u03bb\nNext, note that\n\u2225\u03b8t+1 \u2212 \u03b8t\u2225 = \u03b7\n\r\r\rb\u2207L(\u03b8; B)\n\r\r\r \u2264 \u03b7\n\u221a\nd \u00b7 1\nBn\nX \f\fz\u22a4\ni \u2207L(\u03b8; {(x, y)})\n\f\f \u2264 \u03b7dG(\u03b8t).\nTherefore \u2225\u03bb\u03b8t+1 + (1 \u2212 \u03bb)\u03b8t \u2212 \u03b8t\u2225 \u2264 \u03b7dG(\u03b8t). By the assumption we have the upper bound\n\u22072L(\u03bb\u03b8t+1 + (1 \u2212 \u03bb)\u03b8t) \u2aaf H(\u03b8t), and thus\nL(\u03b8t+1) \u2264 L(\u03b8t) + \u2207L(\u03b8t)\u22a4(\u03b8t+1 \u2212 \u03b8t) + (\u03b8t+1 \u2212 \u03b8t)\u22a4H(\u03b8t)(\u03b8t+1 \u2212 \u03b8t)\n= L(\u03b8t) \u2212 \u03b7\u2207L(\u03b8t)\u22a4 b\u2207L(\u03b8t; B) + 1\n2\u03b72 b\u2207L(\u03b8t; B)\u22a4H(\u03b8t)b\u2207L(\u03b8t; B).\nTaking the conditional expectation with respect to \u03b8t and plugging in (9), the formula for the\ncovariance of our ZO estimate b\u2207L(\u03b8t; B), yields\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + \u03b72\n2\nD\nH(\u03b8t), E\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4iE\n= L(\u03b8t) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + \u03b72\n2 \u00b7\nd\nn(d + 2)\n\u0012\n\u2225\u2207L(\u03b8t)\u22252 + 1\nB tr(\u03a3(\u03b8t))\n\u0013\ntr(H(\u03b8t))\n+ \u03b72\n2\n\u0012\n1 +\nd \u2212 2\nn(d + 2)\n\u0013 \u0012\n\u2207L(\u03b8t)\u22a4H(\u03b8t)\u2207L(\u03b8t) + 1\nB \u27e8\u03a3(\u03b8t), H(\u03b8t)\u27e9\n\u0013\nBy assumption, the Hessian upper bound H(\u03b8t) satisfies \u2225H(\u03b8t)\u2225op \u2264 \u2113 and tr(H(\u03b8t)) \u2264 \u2113r. Thus\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + \u03b72\u2113\n2 \u00b7\n\u0012dr + d \u2212 2\nn(d + 2) + 1\n\u0013\n\u00b7\n\u0012\n\u2225\u2207L(\u03b8t)\u22252 + 1\nB tr(\u03a3(\u03b8t))\n\u0013\n= L(\u03b8t) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + \u03b72\u2113\n2 \u00b7\n\u0012dr + d \u2212 2\nn(d + 2) + 1\n\u0013\n\u00b7 E\nh\n\u2225\u2207L(\u03b8t; B)\u22252i\n,\nas desired.\nG.1\nProofs of Global Convergence\nLemma 4. Let L(\u03b8) be \u00b5-PL and let there exist \u03b1 such that tr(\u03a3(\u03b8)) \u2264 \u03b1(L(\u03b8) \u2212 L\u2217) for all \u03b8.\nThen after\nt = O\n\u0012\u0012 \u2113\n\u00b5 + \u2113\u03b1\n\u00b52B\n\u0013\nlog L(\u03b80) \u2212 L\u2217\n\u03f5\n\u0013\niterations of SGD we have E[L(\u03b8t)] \u2264 L\u2217 + \u03f5.\nProof of Lemma 4. The descent lemma for SGD yields\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 \u2212\u03b7 \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\u2113 \u00b7 E[\u2225\u2207L(\u03b8t; B)\u22252].\nPlugging in E[\u2225\u2207L(\u03b8t; B)\u22252] = \u2225\u2207L(\u03b8t)\u22252 + 1\nB tr(\u03a3(\u03b8t)) and selecting a learning rate \u03b7 \u2264 1\n\u2113\nyields\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t) \u2212 \u03b7\n2 \u2225\u2207L(\u03b8t)\u22252 + \u03b72\u2113\n2B tr(\u03a3(\u03b8t))\nSince L is \u00b5-PL, we get\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t) \u2212 \u03b7\u00b5(L(\u03b8t) \u2212 L\u2217) + \u03b72\u2113\n2B tr(\u03a3(\u03b8t)).\n33\nSince tr(\u03a3(\u03b8t)) \u2264 \u03b1(L(\u03b8t) \u2212 L\u2217), we have\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t) \u2212 \u03b7\u00b5(L(\u03b8t) \u2212 L\u2217) + \u03b72\u2113\u03b1\n2B (L(\u03b8t) \u2212 L\u2217).\nAltogether,\nE[L(\u03b8t+1)] \u2212 L\u2217 \u2264\n\u0012\n1 \u2212 \u03b7\u00b5 + \u03b72\u2113\u03b1\n2B\n\u0013\n(E[L(\u03b8t)] \u2212 L\u2217)\nChoosing \u03b7 = min( 1\n\u2113 , \u00b5B\n\u2113\u03b1 ), we obtain\nE[L(\u03b8t+1)] \u2212 L\u2217 \u2264\n\u0012\n1 \u2212 min( \u00b5\n2\u2113, \u00b52B\n2\u2113\u03b1 )\n\u0013\n(E[L(\u03b8t)] \u2212 L\u2217).\nTherefore we reach a solution with E[L(\u03b8t)] \u2212 L\u2217 \u2264 \u03f5 after\nt = max\n\u00122\u2113\n\u00b5 , 2\u2113\u03b1\n\u00b52B\n\u0013\nlog\n\u0012L(\u03b80) \u2212 L\u2217\n\u03f5\n\u0013\n= O\n\u0012\u0012 \u2113\n\u00b5 + \u2113\u03b1\n\u00b52B\n\u0013\nlog L(\u03b80) \u2212 L\u2217\n\u03f5\n\u0013\niterations.\nProof of Lemma 3. By Corollary 1, ZO-SGD with \u03b7ZO = \u03b3\u22121\u03b7SGD yields\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 1\n\u03b3 \u00b7\n\u0014\n\u2212\u03b7SGD \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\nSGD\u2113 \u00b7 E[\u2225\u2207L(\u03b8; B)\u22252]\n\u0015\n.\nAs in the proof for SGD, choosing \u03b7SGD \u2264 1\n\u2113 yields\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t) \u2264 \u03b3\u22121 \u00b7\n\u0014\n\u2212\u03b7SGD\n2\n\u2225\u2207L(\u03b8t)\u22252 + \u03b72\nSGD\u2113\n2B\ntr(\u03a3(\u03b8t))\n\u0015\n.\nTherefore under \u00b5-PL and the tr(\u03a3(\u03b8t)) \u2264 \u03b1(L(\u03b8t) \u2212 L\u2217) assumption we obtain\nE[L(\u03b8t+1)] \u2212 E[L(\u03b8t)] \u2264 \u03b3\u22121 \u00b7\n\u0014\n\u2212\u03b7SGD\u00b5 + \u03b72\nSGD\u2113\u03b1\n2B\n\u0015\n\u00b7 (E[L(\u03b8t)] \u2212 L\u2217)\n=\u21d2 E[L(\u03b8t+1)] \u2212 L\u2217 \u2264\n\u0012\n1 \u2212 \u03b3\u22121\n\u0012\n\u03b7SGD\u00b5 \u2212 \u03b72\nSGD\u2113\u03b1\n2B\n\u0013\u0013\n(E[L(\u03b8t)] \u2212 L\u2217).\nChoosing \u03b7SGD = min( 1\n\u2113 , \u00b5B\n\u2113\u03b1 ) yields\nE[L(\u03b8t+1)] \u2212 L\u2217 \u2264\n\u0012\n1 \u2212 \u03b3\u22121 \u00b7 min( \u00b5\n2\u2113, \u00b52B\n2\u2113\u03b1 )\n\u0013\n(E[L(\u03b8t)] \u2212 L\u2217).\nTherefore we reach a solution with E[L(\u03b8t)] \u2212 L\u2217 \u2264 \u03f5 after\nt = \u03b3 \u00b7 max\n\u00122\u2113\n\u00b5 , 2\u2113\u03b1\n\u00b52B\n\u0013\nlog\n\u0012L(\u03b80) \u2212 L\u2217\n\u03f5\n\u0013\n= O\n\u0012\u0010 r\nn + 1\n\u0011\n\u00b7\n\u0012 \u2113\n\u00b5 + \u2113\u03b1\n\u00b52B\n\u0013\nlog L(\u03b80) \u2212 L\u2217\n\u03f5\n\u0013\niterations.\nG.1.1\nVerification of assumptions\nWe show that the tr(\u03a3(\u03b8t)) \u2264 \u03b1(L(\u03b8t) \u2212 L\u2217) assumption holds for certain losses.\nFirst, consider optimizing the model f(x; \u03b8) with square loss, so that\nL(\u03b8) = 1\nN\nX\ni\u2208[N]\n(f(xi; \u03b8) \u2212 yi)2.\nOne then has that\n\u03a3(\u03b8) = 2\nN\nX\ni\u2208[N]\n(f(xi; \u03b8) \u2212 yi)2\u2207f(xi; \u03b8)\u2207f(xi; \u03b8)\u22a4 \u2212 \u2207L(\u03b8)\u2207L(\u03b8)\u22a4.\n34\nTherefore\ntr(\u03a3(\u03b8)) \u2264 2\nN\nX\ni\u2208[N]\n(f(xi; \u03b8) \u2212 yi)2 \u2225\u2207f(xi; \u03b8)\u22252\n\u2264 2L(\u03b8)\nX\ni\u2208[N]\n\u2225\u2207f(xi; \u03b8)\u22252 .\nAssume that the data can be interpolated, i.e., L\u2217 = 0.\nIf the function is L-Lipschitz, i.e.,\n\u2225\u2207f(x; \u03b8)\u2225 \u2264 L, then the condition holds with \u03b1 = 2NL2. If we are in the kernel regime,\ni.e., f(xi; \u03b8) = \u03d5(xi)\u22a4\u03b8 for some feature map \u03d5, then\n\u22072L(\u03b8) = 2\nN\nX\ni\u2208[N]\nf(xi; \u03b8)\u2207f(xi; \u03b8)\u22a4.\nThus\ntr(\u03a3(\u03b8)) \u2264 N tr(\u22072L(\u03b8)) \u00b7 L(\u03b8) \u2264 N\u2113r \u00b7 L(\u03b8).\nSo the condition holds for \u03b1 = N\u2113r.\nNext, consider the cross entropy loss function, i.e\nL(\u03b8) = 1\nN\nX\ni\u2208[N]\nexp(\u2212yif(xi; \u03b8)).\nOne then has that\n\u03a3(\u03b8) = 1\nN\nX\ni\u2208[N]\nexp(\u22122yif(xi; \u03b8))y2\ni \u2207f(xi; \u03b8)\u2207f(xi; \u03b8)\u22a4 \u2212 L(\u03b8)L(\u03b8)\u22a4,\nAssume that the targets yi are bounded in [\u22121, 1] (which is true for binary classification tasks), and\nthat L\u2217 = 0 (which can be achieved if |f(x; \u03b8)| can be sent to \u221e) we have that\ntr(\u03a3(\u03b8)) \u2264 1\nN\nX\ni\u2208[N]\nexp(\u22122yif(xi; \u03b8)) \u2225\u2207f(xi; \u03b8)\u22252 .\nIn the kernel regime, f(xi; \u03b8) = \u03d5(xi)\u22a4\u03b8, and thus\n\u22072L(\u03b8) = 1\nN\nX\ni\u2208[N]\nexp(\u2212yif(xi; \u03b8))\u2207f(xi; \u03b8)\u2207f(xi; \u03b8)\u22a4.\nTherefore\ntr(\u03a3(\u03b8)) \u2264 N tr(\u22072L(\u03b8)) \u00b7 L(\u03b8) \u2264 N\u2113r \u00b7 L(\u03b8).\nTherefore the condition holds with \u03b1 = N\u2113r as well.\nG.2\nProofs for Gaussian perturbations\nThe first lemma computes the second moment of the covariance estimate b\u2207L(\u03b8; B) when z is drawn\nN(0, I).\nLemma 5. Let zi \u223c N(0, I) i.i.d. Then\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n=\n\u0012\n1 + 1\nn\n\u0013\n\u00b7\n\u0012\n\u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + 1\nB \u03a3MB(\u03b8)\n\u0013\n+ 1\nnI \u00b7\n\u0012\n\u2225\u2207L(\u03b8)\u22252 + 1\nB tr(\u03a3MB(\u03b8))\n\u0013\n.\n(8)\nProof. As in the proof of Lemma 2, we have that in the \u03f5 \u2192 0 limit\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n=\n1\nB2n2\nX\n(x1,y1),(x2,y2)\u2208B\nX\ni,j\u2208[n]\nE\n\u0002\n(ziz\u22a4\ni \u2207L(\u03b8; {(x1, y1)}))(zjz\u22a4\nj \u2207L(\u03b8; {(x2, y2)}))\u22a4\u0003\n35\nFor vectors u, v, we have that\nEzi,zj[ziz\u22a4\ni uv\u22a4zjz\u22a4\nj ] = uv\u22a4\nwhen i \u0338= j, and\nEzi[ziz\u22a4\ni uv\u22a4ziz\u22a4\ni ] = Ez[z\u22974](u, v) = 3Sym(I\u22972)(u, v) = u\u22a4v \u00b7 I + 2uv\u22a4.\nTherefore\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n= 1\nB2\nX\n(x1,y1),(x2,y2)\u2208B\n\u0012n \u2212 1\nn\n+ 2\nn\n\u0013\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n+ 1\nn \u00b7 E\n\u0002\nL(\u03b8; {(x1, y1)})\u22a4L(\u03b8; {(x2, y2)})\n\u0003\nI.\nIn the proof of Lemma 2 we showed that\n1\nB2\nX\n(x1,y1),(x2,y2)\u2208B\nE\n\u0002\nL(\u03b8; {(x1, y1)})L(\u03b8; {(x2, y2)})\u22a4\u0003\n= \u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + 1\nB \u03a3(\u03b8).\nPlugging this yields\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n=\n\u0012n + 1\nn\n\u0013\n\u00b7\n\u0012\n\u2207L(\u03b8)\u2207L(\u03b8)\u22a4 + 1\nB \u03a3(\u03b8)\n\u0013\n+ 1\nnI \u00b7\n\u0012\n\u2225\u2207L(\u03b8)\u22252 + 1\nB tr(\u03a3(\u03b8))\n\u0013\n.\n(9)\nWe can prove an analog to Theorem 1 in the case where the zi are Gaussian. One challenge is that\n\u2225\u03b8t+1 \u2212 \u03b8t\u2225 is no longer bounded; instead we the r-local effective rank assumption only holds with\nhigh probability, and thus to bound the expected loss decrease we must control the probability of the\n\u2225\u03b8t+1 \u2212 \u03b8t\u2225 being large.\nConsider the following modified version of the local r-effective rank assumption, where the upper\nbound on the Hessian is measured over a ball of radius twice as large as the one in Assumption 1.\nAssumption 2 (Local r-effective rank, Gaussian). Let G(\u03b8t) = max(x,y)\u2208D \u2225\u2207L(\u03b8t; {(x, y)})\u2225.\nThere exists a matrix H(\u03b8t) such that:\n1. For all \u03b8 such that \u2225\u03b8 \u2212 \u03b8t\u2225 \u2264 2\u03b7dG(\u03b8t), we have \u22072L(\u03b8) \u2aaf H(\u03b8t).\n2. The effective rank of H(\u03b8t), i.e., tr(H(\u03b8t))/ \u2225H(\u03b8t)\u2225op, is at most r.\nTheorem 2 (Dimension-Free Rate, Gaussian z). Assume the loss exhibits local r-effective rank\n(Assumption 2). If \u03b8t+1 = \u03b8t \u2212 \u03b7ZO b\u2207L(\u03b8t; B) is a single step of ZO-SGD using the n-SPSA estimate\nwith a minibatch of size B, then there exists a \u03b3 = \u0398(r/n) such that the expected loss decrease can\nbe bounded as\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t)\n\u2264 \u2212\u03b7ZO \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\nZO\u2113 \u00b7 \u03b3 \u00b7 E[\u2225\u2207L(\u03b8t; B)\u22252] + \u03b72\nZO\u2113G(\u03b8t)2 exp(\u2212\u2126(nd)).\nProof of Theorem 2. Let A be the event that \u2225\u03b8t+1 \u2212 \u03b8t\u2225 \u2264 2\u03b7dG(\u03b8t). On A, we have that\nL(\u03b8t+1) \u2264 L(\u03b8t) \u2212 \u03b7\u2207L(\u03b8t)\u22a4 b\u2207L(\u03b8; B) + 1\n2\u03b72 b\u2207L(\u03b8t; B)\u22a4H(\u03b8)b\u2207L(\u03b8t; B).\nLikewise, since L is \u2113-smooth, we have that\nL(\u03b8t+1) \u2264 L(\u03b8t) \u2212 \u03b7\u2207L(\u03b8t)\u22a4 b\u2207L(\u03b8; B) + 1\n2\u03b72\u2113\n\r\r\rb\u2207L(\u03b8t; B)\n\r\r\r\n2\n.\n36\nTherefore\nE[L(\u03b8t+1) | \u03b8t] \u2264 L(\u03b8t+1) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72 D\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4 \u00b7 1(A)\ni\n, H(\u03b8t)\nE\n+ 1\n2\u03b72\u2113E\n\u0014\r\r\rb\u2207L(\u03b8t; B)\n\r\r\r\n2\n\u00b7 1(\u00acA)\n\u0015\n= L(\u03b8t+1) \u2212 \u03b7 \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72 D\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n, H(\u03b8t)\nE\n1\n2\u03b72 D\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4 \u00b7 1(\u00acA)\ni\n, \u2113I \u2212 H(\u03b8t)\nE\n.\nThe latter term can be bounded as follows\n1\n2\u03b72 D\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4 \u00b7 1(\u00acA)\ni\n, \u2113I \u2212 H(\u03b8t)\nE\n\u2264 \u03b72\u2113E\n\u0014\r\r\rb\u2207L(\u03b8; B)\n\r\r\r\n2\n\u00b7 1(\u00acA)\n\u0015\n\u2264 \u03b72\u2113E\n\u0014\r\r\rb\u2207L(\u03b8; B)\n\r\r\r\n4\u0015 1\n2\nPr[\u00acA]1/2.\nThe gradient estimate b\u2207L(\u03b8; B) satisfies\n\r\r\rb\u2207L(\u03b8; B)\n\r\r\r \u2264 1\nn\nX\ni\u2208[n]\n\f\fz\u22a4\ni \u2207L(\u03b8; B)\n\f\f \u00b7 \u2225zi\u2225\nThe expectation term is upper bounded as\nE\n\u0014\r\r\rb\u2207L(\u03b8; B)\n\r\r\r\n4\u0015\n\u2264 1\nn\nX\ni\u2208[n]\nE\nh\f\fz\u22a4\u2207L(\u03b8; B)\n\f\f4 \u00b7 \u2225z\u22254i\n\u2264 E\nh\f\fz\u22a4\u2207L(\u03b8; B)\n\f\f8i1/2\nE\nh\n\u2225z\u22258i1/2\n\u2264\n\u221a\n105(d + 6)2G(\u03b8t)4,\nwhere we have plugged in explicit formulas for moments of Gaussian and \u03c72 random variables.\nNext, note that on the event \u00acA, we have\n2\u03b7dG(\u03b8t) \u2264 \u2225\u03b8t+1 \u2212 \u03b8t\u2225 = \u03b7\n\r\r\rb\u2207L(\u03b8t; B)\n\r\r\r \u2264 \u03b7 \u00b7 1\nn\nX\ni\u2208[n]\n\u2225zi\u22252 G(\u03b8t).\nTherefore\nPr[\u00acA] \u2264 Pr\n\uf8ee\n\uf8f0 X\ni\u2208[n]\n\u2225zi\u22252 \u2265 2nd\n\uf8f9\n\uf8fb\nLemma 6 (Standard \u03c72-tail bound). Let Z be a \u03c72 random variable with k degrees of freedom. Then\nPr[Z \u2265 k + u] \u2264 exp\n\u0012\n\u2212 min\n\u0012 u2\n16k , u\n16\n\u0013\u0013\nSince P\ni\u2208[n] \u2225zi\u22252 is a \u03c72 random variable with nd degrees of freedom, we thus have that\nPr[\u00acA] \u2264 exp\n\u0012\n\u2212nd\n16\n\u0013\n.\nAltogether,\n1\n2\u03b72 D\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4 \u00b7 1(\u00acA)\ni\n, \u2113I \u2212 H(\u03b8t)\nE\n\u2264 \u03b72\u21131051/4(d + 6)G(\u03b8t)2 exp(\u2212nd\n32 )\n= \u03b72\u2113G(\u03b8t)2 exp(\u2212\u2126(nd)).\n37\nFinally, plugging in (8), along with the fact that \u2225H(\u03b8t)\u2225op \u2264 \u2113 and tr(H(\u03b8t)) \u2264 \u2113r,\nD\nE\nh\nb\u2207L(\u03b8; B)b\u2207L(\u03b8; B)\u22a4i\n, H(\u03b8t)\nE\n= r + n + 1\nn\n\u00b7 \u2113\n\u0012\n\u2225\u2207L(\u03b8t)\u22252 + 1\nB tr(\u03a3(\u03b8t))\n\u0013\n= r + n + 1\nn\n\u00b7 E\nh\n\u2225\u2207L(\u03b8t; B)\u22252i\nThus letting \u03b3 = r+n+1\nn\nyields\nE[L(\u03b8t+1) | \u03b8t] \u2212 L(\u03b8t)\n\u2264 \u2212\u03b7 \u2225\u2207L(\u03b8t)\u22252 + 1\n2\u03b72\u2113 \u00b7 \u03b3 \u00b7 E[\u2225\u2207L(\u03b8t; B)\u22252] + \u03b72\u2113G(\u03b8t)2 exp(\u2212\u2126(nd)),\nas desired.\n38\n"
  },
  {
    "title": "High-Fidelity Image Compression with Score-based Generative Models",
    "link": "https://arxiv.org/pdf/2305.18231.pdf",
    "upvote": "1",
    "text": "High-Fidelity Image Compression with Score-based Generative Models\nEmiel Hoogeboom*\nGoogle Research\nAmsterdam, Netherlands\nemielh@google.com\nEirikur Agustsson\nGoogle Research\nReykjav\u00b4\u0131k, Iceland\neirikur@google.com\nFabian Mentzer\nGoogle Research\nZ\u00a8urich, Switzerland\nmentzer@google.com\nLuca Versari\nGoogle Research\nZ\u00a8urich, Switzerland\nveluca@google.com\nGeorge Toderici\nGoogle Research\nMountain View, USA\ngtoderici@google.com\nLucas Theis*\nGoogle Research\nLondon, UK\ntheis@google.com\nAbstract\nDespite the tremendous success of diffusion generative\nmodels in text-to-image generation, replicating this success\nin the domain of image compression has proven difficult. In\nthis paper, we demonstrate that diffusion can significantly\nimprove perceptual quality at a given bit-rate, outperform-\ning state-of-the-art approaches PO-ELIC [14] and HiFiC\n[27] as measured by FID score. This is achieved using a\nsimple but theoretically motivated two-stage approach com-\nbining an autoencoder targeting MSE followed by a further\nscore-based decoder. However, as we will show, implemen-\ntation details matter and the optimal design decisions can\ndiffer greatly from typical text-to-image models.\n1. Introduction\nDiffusion [40, 17] and related score-based generative\nmodels [42, 43, 41] had an outsized impact in several do-\nmains requiring image generation, most notably in text-to-\nimage generation [30, 32, 35]. Dhariwal et al. [6] demon-\nstrated that diffusion models can outperform generative ad-\nversarial networks (GANs) [10] for unconditional image\nsynthesis, and they have also been shown to outperform\nGANs in some image-to-image tasks such as colorization\n[34] or super-resolution of faces [36]. It is thus surpris-\ning that score-based generative models have not yet dis-\nplaced GANs for the task of image compression, perform-\ning worse or roughly on par with the GAN-based approach\nHiFiC [27] on high-resolution images [48, 9], despite their\ntypically higher computational cost. In line with these re-\n*Equal contribution.\nsults, we find that trying to repurpose text-to-image models\nfor the task of image compression does not yield good re-\nsults. For instance, using Stable Diffusion [32] (SD) to up-\nscale a downsampled image either produces reconstructions\nwhich do not faithfully represent the input or which contain\nundesirable artefacts (Fig. 2).\nIn this work we tune diffusion models for the task of\nimage compression and demonstrate that score-based gen-\nerative models can achieve state-of-the-art performance in\ngeneration realism, outperforming several recent generative\napproaches to compression in terms of FID. For a qualita-\ntive comparison, see Fig. 1. Our method is conceptually\nsimple, applying a diffusion model on top of a (pre-trained)\ndistortion-optimized autoencoder. However, we find that\nthe details matter. In particular, FID is sensitive to the noise\nschedule as well as the amount of noise injected during im-\nage generation. While text-to-image models tend to ben-\nefit from increased levels of noise when training on high-\nresolution images [18], we observe that reducing the overall\nnoise of the diffusion process is beneficial in compression.\nIntuitively, with less noise the model focuses more on fine\ndetails. This is beneficial because the coarse details are al-\nready largely determined by the autoencoder reconstruction.\nIn this paper, we explore two closely related approaches: 1)\ndiffusion models which have impressive performance at the\ncost of a large number of sampling steps, and 2) rectified\nflows which perform better when fewer sampling steps are\nallowed.\n2. Related work\nHo et al. [17] described a compression approach relying\non a combination of diffusion and reverse channel coding\n1\narXiv:2305.18231v3  [eess.IV]  7 Mar 2024\nPO-ELIC: 171.7%\nHFD (Ours): 188.6%\nHFD (Ours): 100%\nHiFiC: 267.3%\nMSE (Ours): 100%\nVVC: 321.2%\nHEVC: 340.0%\nJPEG: 225.8%\nHFD (Ours) 0.0562 bpp\nHFD (Ours) 0.1060 bpp\nFigure 1. Illustrative example where state-of-the-art approaches based on generative adversarial networks, such as HiFiC [27] or PO-\nELIC [14], produce noisy artefacts typical for GANs (best viewed zoomed in). In contrast, our diffusion-based approach produces pleasing\nresults down to extremely low bit-rates. Bit-rates are expressed relative to the bit-rate of our low-rate model (0.0562 bpp). MSE refers to\na model similar to ELIC [13] whose outputs are fed into our generative decoder. VVC and HEVC reconstructions were obtained using the\nreference implementation (VTM and HM, respectively). For JPEG we used 4:2:0 chroma subsampling. The photo is from the CLIC22 test\nset [1].\ntechniques [12, 46] and considered its rate-distortion per-\nformance. Theis et al. [45] further developed this approach\nand demonstrated that it outperforms HiFiC [27] on 64\u00d764\npixel images. However, while this approach works well, it\nis currently not practical as it requires efficient communica-\ntion of high-dimensional Gaussian samples\u2014an unsolved\nproblem\u2014so that these papers had to rely on theoretical es-\ntimates of the bit-rate.\nYang and Mandt [48] described an end-to-end trained ap-\nproach where the decoder is a diffusion generative model\nconditioned on quantized latents. The model was evalu-\nated on medium-sized images of up to 768 \u00d7 768 pixels\nusing a large number of objective metrics and found it to\nperform either somewhat better or worse than HiFiC [27],\ndepending on the metric. Here, we focus on a smaller set\nof objective metrics since many metrics are known to be\npoorly correlated with perceptual quality when applied to\nneural methods [24]. Additionally, we extend our method to\nhigher-resolution images (e.g., CLIC) and compare to more\nrecent state-of-the-art neural compression methods. A qual-\nitative comparison between the two approaches is provided\nin Fig. 10.\nAn alternative approach was proposed by Ghouse et al.\n[9]. Closely related to our approach, they first optimize an\nautoencoder for a rate-distortion loss, followed by training\na conditional diffusion model on the autoencoder\u2019s output.\nThe authors found that this approach peforms worse than\nHiFiC in terms of FID despite reporting better performance\nthan an earlier version of Yang and Mandt\u2019s [48] model.\nGoing further, we find that by improving the noise schedule\nand sampling procedure, we can achieve significantly bet-\nter FID scores than HiFiC when training a diffusion model,\neven outperforming very recent state-of-the-art methods.\nSaharia et al. [34] explored a variety of applications for\ndiffusion models, including artefact removal from JPEG im-\nages. However, JPEG [20] is known to produce relatively\nhigh bit-rates even at its lowest settings (compared to state-\nof-the art neural compression methods) and the authors did\nnot compare to neural compression approaches.\nCurrently, state-of-the-art approaches for generative im-\nUncompressed\nSD: 0.9845 bpp\nHFD (Ours): 0.0918 bpp\nsed\nHFD (O\nUncompressed\nd\nHFD (Our\nSD: 0.9845 bpp\nSD: 0.9845 bpp\nSD: 0.9845 bpp\nHFD (Ours): 0.0918 bpp\nHFD (Ours): 0.0918 bpp\nFD (Ours): 0.0918 bpp\nFigure 2. Popular text-to-image models struggle to reproduce fine details. As a simple baseline, we used the upsampler of Stable Diffusion\n[32] applied to a 4\u00d7 downsampled image (192 \u00d7 128 pixels). When encoded losslessly as a PNG, the downsampled image is roughly 48\nkB in size (or 0.9845 bits per pixel of the full-resolution image). When encoded with JPEG (4:2:0, QF=95) [20], the approach still requires\n0.2635 bpp. Similar results were obtained with Imagen [35] and when additinally conditioning on text (Appendix A). The example photo\nis from the widely used Kodak Photo CD [23].\nage compression are based on decoders trained for adver-\nsarial losses [10].\nNoteably, HiFiC has proven to be a\nvery strong baseline [27]. A similar approach named PO-\nELIC [14] won the most recent Challenge on Learned Im-\nage Compression (CLIC22), but using a more advanced en-\ntropy modeling approach and encoder and decoder architec-\ntures which followed ELIC [13]. Similarly, we will use an\nautoencoder based on ELIC.\nAlaaeldin et al. [8] recently reported better FID scores\nthan HiFiC using a combination of vector quantization,\nLPIPS, and adversarial training. However, as observed by\nthe authors, reconstructions tend to be smooth and do not\npreserve details well (Fig. 3). This highlights the limita-\ntions of FID when comparing fine details, especially when\ntraining uses network-based loss such as LPIPS. Also re-\ncently, Agustsson et al. [2] described another GAN based\napproach which combines learnings from HiFiC [27] and\nELIC [13] to outperform HiFiC in terms of PSNR and FID\non the CLIC20 and MS COCO 30k datasets, while also hav-\ning controllable synthesis.\nWe note that many other papers have explored variations\nof autoencoders trained for adversarial or perceptual losses\n[31, 39, 3, 29, 19, 47] but focus our comparisons on the re-\ncent state-of-the-art methods PO-ELIC [14] and the \u201cmulti-\nrealism\u201d approach (MR) of Agustsson et al. [2].\n3. Background\n3.1. Diffusion\nDiffusion models [40, 17] define a process that gradu-\nally destroys the signal, typically with Gaussian noise. It\nHFD (Ours): 0.0538 (44.2%)\nPQ-MIM [8]: 0.124 (100%)\nFigure 3. Qualitative comparison with the recent approach of\nAlaaeldin et al. [8]. We find that our approach yields significantly\nsharper reconstructions even when using a fraction of the bit-rate.\nNumbers indicate bits per pixel for the entire image, which is pro-\nvided in Appendix C.\nis convenient to express the process in terms of marginal\ndistributions conditioned on the original example x:\nq(zt|x) = N(zt|\u03b1tx, \u03c32\nt I),\n(1)\nwhere \u03b1t decreases and \u03c3t increases over time t \u2208 [0, 1].\nOne can sample from this distribution via zt = \u03b1tx + \u03c3t\u03f5t\nwhere \u03f5t \u223c N(0, I) is standard Gaussian noise. A genera-\ntive denoising process can be learned by minimizing\nL = Et\u223cU(0,1)Ezt\u223cq(zt|x)\nh\nw(t)||\u03f5t \u2212 \u02c6\u03f5t||2i\n(2)\nwhere for a particular weighting w(t) [22], L corresponds\nto a negative variational lower bound on log p(x), although\nU-Net\nEnc\nQ\nDec\nFigure 4. Overview of our high-fidelity diffusion (HFD) approach.\nThe output of a standard MSE autoencoder is used by a denoising\ndiffusion model to produce realistic samples by iteratively denois-\ning for T steps.\nin practice a constant weighting w(t) = 1 has been found\nsuperior for image quality [17]. Here, \u02c6\u03f5t can be the predic-\ntion of a neural network f(zt, t, \u02c6xMSE) which takes in the\ncurrent noisy zt and diffusion time t, as well as possibly an\nadditional context. In this paper it will be the output of a\nneural compression decoder,\n\u02c6xMSE = D(Q(E(x))),\n(3)\nwhere E, D represents an autoencoder trained for MSE, and\nQ is a quantizer. We use ELIC [13] for E, D, see Sec. 4.1.\nMoreover, instead of learning \u02c6\u03f5t directly, in this paper v\nprediction is used, which is more stable towards t \u2192 1 [37]\nand has been used in high resolution tasks [16]. In short,\nthe neural net predicts \u02c6vt = f(zt, t, \u02c6xMSE) which can be\nconverted using \u02c6\u03f5t = \u03c3tzt + \u03b1t\u02c6vt. Intuitively, v prediction\nis approximately x prediction when t \u2192 1 (whereas the\nsampling with \u03f5 prediction could be numerically unstable)\nwhile it is approximately \u03f5 prediction near t \u2192 0 (where\nx prediction would result in inferior sample quality). To\ndraw samples from the diffusion model, one defines a grid\nof timesteps 1, 1 \u2212 1/T, 1 \u2212 2/T, . . . , 1/T, and runs the\ndenoising process from Gaussian noise zT \u223c N(0, I) after\nwhich zt is iteratively updated with p(zt\u22121/T |zt). For more\ndetails see Appendix A.\n3.2. Rectified flow\nAnother\nclosely\nrelated\napproach\ncalled\nrectified\nflow [26] aims to find a mapping between two arbitrary\nmarginal distributions. Assuming we want to map some\ndata distribution p(x) to some other arbitrary distribution\np(z), we first define a (possibly random) pairing between\nsamples from these distributions. For example, we could\ndraw as many samples from a standard normal distribution\nas needed to match the size of the data points x1, x2, . . .\nand create a pairing (x1, z1), (x2, z2), . . . between which a\nFigure 5. Our score-based models are trained on 256 \u00d7 256 pixel\nimage patches. To handle arbitrary resolutions, we apply models\npatch-wise. We first generate a full patch while conditioning on\nany pixels of the input image and any already reconstructed pix-\nels within the window (black square). We then copy the central\n128 \u00d7 128 pixels (white square) into the final reconstruction and\ndiscard the border, which is only used to condition the model. Near\nthe image border, context pixels are shifted relative to the central\npixels (top left). By dividing patches into 4 groups, batches of im-\nage patches can easily be generated in parallel.\nflow is learned via:\nLi = Et\u223cU(0,1)\nh\n||vi \u2212 f(txi + (1 \u2212 t)zi)||2i\n,\n(4)\nwhere vi = xi \u2212 zi. After training the model, one can im-\nprove the pairing given the flow model and train the model\nagain.\n4. Method\nOn a high level, our approach consists of two compo-\nnents (see Fig. 4): first, we use a standard CNN-based au-\ntoencoder E, D trained for MSE to store a lossy version of\nthe input image to disk (detailed in Sec. 4.1). Then, we ap-\nply a diffusion process to recover and add detail discarded\nby the autoencoder. The bit-rate to encode a given image is\nentirely determined by E, since the diffusion process does\nnot require additional bits. This two-step approach can be\ntheoretically justified as follows. The second step approx-\nimates sampling from the posterior distribution over im-\nages x given the output \u02c6xMSE (Eq. 3) of the autoencoder,\n\u02c6x \u223c p(x | \u02c6xMSE). The MSE of this reconstruction is\nupper-bounded by twice the MSE of the first stage [4]\nE[\u2225\u02c6x \u2212 x\u22252] = 2 E[\u2225E[\u02c6x | \u02c6xMSE] \u2212 x\u22252]\n(5)\n= 2 E[\u2225E[x | \u02c6xMSE] \u2212 x\u22252]\n(6)\n\u2264 2 E[\u2225\u02c6xMSE \u2212 x\u22252],\n(7)\nwith equality when the first-stage decoder is optimal. That\nis, given enough representational power, the loss optimized\nin the first stage also minimizes the distortion of the final\nreconstruction, and a lack of end-to-end training poses no\ntheoretical limitation to the model\u2019s performance. The only\nway to further improve upon the theoretical performance of\nthis approach (i.e., reducing MSE while maintaining per-\nfect realism) would require a random coding approach with\na shared source of randomness [44, 50]. However, these ap-\nproaches can be expensive [45] and are currently not widely\nused.\n4.1. Autoencoder\nThe lossy MSE-optimized autoencoder is not the focus\nof this paper, and similar to Agustsson et al. [2] we use the\nrecently proposed ELIC architecture [13] for the autoen-\ncoder (using C = 256 channels throughout). The quan-\ntized representation of an image produced by this autoen-\ncoder is entropy coded and written to disk. To do this, we\nuse the channel-autoregressive entropy model proposed by\nMinnen et al. [28]. Please see the cited work for details. We\nwill refer to this model as \u201cMSE (Ours)\u201d.\n4.2. Score-based decoder models\nGiven the autoencoder reconstruction \u02c6xMSE, we explore\ntwo approaches to produce a more realistic version, based\non either diffusion models or rectified flows. These gener-\nate the final reconstruction by iteratively sampling from the\nrespective generative process (Sections 3.1, 3.2) as follows.\nDiffusion model\nAn important property of a diffusion\nmodel is its noise schedule. It determines how quickly in-\nformation is destroyed and how much of computation is\n0\n0.2 0.4 0.6 0.8\n1\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\nDiffusion time\nlog-SNR\n0\n100\n200\n300\n5\n10\n15\n20\n25\n30\n35\n40\nTraining steps [\u00b7103]\nFID\n\u03b7 = 4.0\n\u03b7 = 1.0\n\u03b7 = 0.5\nFigure 6. Left: The shifted schedule which focuses more on de-\ntails as used in HFD. Note that schedule is shifted in the opposite\ndirection of [5, 18], as it focuses on detail opposed to global struc-\nture. Right: FID as a function of training time for three differ-\nent noise schedules. Changing the noise schedule so that fewer\nsteps are spent processing noisy images improved performance\n(\u03b7 = 0.5). This is in contrast to text-to-image models, where\nnoisier schedules were found to perform better (\u03b7 = 4.0) [18].\nHere, FID was calculated using a validation set of 50k examples\nand 10k samples from the model.\nspend on the generation of coarse or fine details of an image.\nA convenient way to express the diffusion parameters \u03b1t, \u03c3t\nis by defining schedules in their signal-to-noise ratio (SNR\n= \u03b12\nt /\u03c32\nt ), or rather in their log-SNR schedule [22]. Un-\nder a variance preserving assumption (a particular flavour\nof diffusion models where \u03b12\nt = 1 \u2212 \u03c32\nt ), given the log\nSNR one can simply retrieve \u03b12\nt = sigmoid(log SNR(t))\nand \u03c32\nt = sigmoid(\u2212 log SNR(t)). In contrast to previous\nwork [18, 5] which found it helpful to shift the schedule\ntowards increased levels of noise, for compression we find\nit beneficial to shift the schedule in the opposite direction.\nIntuitively speaking, the output of the MSE-trained decoder\n\u02c6xMSE already provides a lot of global information about\nthe image structure. It would therefore be wasteful to dedi-\ncate a large part of the diffusion process to generation of the\nglobal structure, which is associated with high noise lev-\nels. By shifting the schedule to use less noise, the diffusion\nmodel instead focuses on the finer details of an image. Re-\ncall that under a variance preserving process, the \u03b1-cosine\nschedule is described by \u22122 log tan(\u03c0t/2) in log SNR, us-\ning that cos / sin = 1/ tan and cos2(t) + sin2(t) = 1. We\nadapt this schedule to:\nlog SNR(t) = \u22122\n\u0000log tan(\u03c0t/2) + log \u03b7\n\u0001\n,\n(8)\nwhich is shifted by \u22122 log \u03b7 to reduce the amount of noise\n(we use \u03b7 = 0.5, see Fig. 6). As is standard practice, the\nboundary effects (where log SNR tends to \u00b1\u221e) at t = 0\nand t = 1 are mitigated by bounding the log SNR, in this\ncase to \u00b115. Combining everything, the objective can be\nsummarized to be:\nL = Et\u223cU(0,1)E\u03f5t\u223cN (0,I)\nh\n||\u03f5t \u2212 \u02c6\u03f5t(zt, t, \u02c6xMSE)||2i\n(9)\nwhere zt = \u03b1tx + \u03c3t\u03f5t, \u02c6\u03f5t = \u03c3tzt + \u03b1t\u02c6vt (the model\nuses v-prediction which improves stability for higher res-\nolution images), \u03b12\nt\n=\nsigmoid(log SNR(t)), \u03c32\nt\n=\nsigmoid(\u2212 log SNR(t)). \u02c6vt is predicted by the neural net-\nwork f(zt, t, \u02c6xMSE) which is a U-Net concatenating zt and\n\u02c6xMSE along the channel axis.\nFlow matching\nRecall that flow matching initially trains\na mapping on unpaired examples. However, in this work we\nare able to use the pairing (x,\n\u02c6\nxMSE) given by the autoen-\ncoder. This means that instead of conditionally mapping\nGaussian samples to images as in diffusion, we learn to map\nautoencoder outputs directly to the uncompressed image us-\ning flow matching. We add a small amounts of uniform\nnoise to the reconstructions \u02c6xMSE and targets x to ensure\nthat an invertible flow between the distributions exist, even\nthough this did not seem to be necessary in practice. We do\nnot iteratively apply rectification as proposed by Liu et al.\n[26], leaving us with a simple optimization objective:\nL = Et\nh\n||(x \u2212 \u02c6xMSE) \u2212 f(tx + (1 \u2212 t)\u02c6xMSE)||2i\n,\n0\n0.1 0.2 0.3 0.4 0.5\n2\n4\n6\nBit-rate [bpp]\nFID\nCLIC2020\nHFD/DDPM (Ours)\nMR [2]\nHiFiC [27]\nPQ-MIM [7]\nDIRAC-100 [11]\n0\n0.1 0.2 0.3 0.4 0.5\n28\n30\n32\n34\nBit-rate [bpp]\nPSNR\nCLIC2020\n0\n0.1 0.2 0.3 0.4 0.5\n0\n2\n4\n6\nBit-rate [bpp]\nFID\nMS-COCO 30k\n0\n0.1 0.2 0.3 0.4 0.5\n24\n26\n28\n30\n32\nBit-rate [bpp]\nPSNR\nMS-COCO 30k\nFigure 7. Realism and distortion as measured by FID and PSNR for various methods evaluated on MS-COCO 30k and CLIC20.\nHFD/DDPM is able to generate realistic images at extremely low bit-rates, surpassing all existing methods in terms of rate-FID curves.\nInstead of sampling t uniformly, we found it beneficial to\nuse t = 1\u2212u2 where u is sampled uniformly between 0 and\n1. However, we did not extensively explore the schedule for\nrectified flow.\n4.3. Generation and sampling\nParallelized sampling of patches\nCompression models\nare typically trained using fully convolutional architectures\non patches, to be applied to full resolution images at test\ntime. However, diffusion models often rely on self-attention\nlayers which are not equivariant and whose computational\ncomplexity grows more quickly in the input dimensions.\nWe therefore opt to generate high-resolution images in a\npatchwise manner. Fortunately, in- and out-painting is rel-\natively easy in diffusion models. In each step of the gener-\native denoising process, already observed pixels are simply\nreplaced by the known values corrupted by an appropriate\namount of noise. While we could generate patches one-by-\none, with some overlap to previous patches, this leads to\nlow utilization of modern accelerators.\nInstead, in this paper patches are divided in groups of\nfour as visualized in Fig. 5. Each group can be generated\nindependently where each patch is a single example in the\nbatch. Then, the next group of patches can be generated\nresulting in four distinct generation stages. As is typical in\ndiffusion, the input for the model is the current noisy state\nzt together with already previously generated parts of the\npatch \u02c6x, controlled by a mask m so that the input is:\nmzt + (1 \u2212 m)(\u03b1t \u02c6x + \u03c3t\u03f5t),\n(10)\nwhere m is one for pixel locations that still need to be gen-\nerated and diffusion noise is injected with \u03f5t \u223c N(0, I).\nNote here that \u02c6x is the output of the diffusion model from\npreviously generated patches.\nThis approach often works well despite only approximat-\ning proper probabilistic conditioning on the available infor-\nmation. Nevertheless, we find that it occasionally leads to\nartefacts. To overcome this issue, we only partially run the\ndiffusion process, generating a patch of noisy pixels (Ap-\npendix F). The next patch is then partially generated condi-\ntioned on observed noisy pixels. We then revisit patches to\ncontinue the reverse diffusion process. We find that dividing\nthe diffusion process into 6 groups works well to eliminate\nany remaininig artefacts.\nNoise level during sampling\nAn important and some-\ntimes forgotten hyperparameter of diffusion models is the\nnoise level of the denoising process, which can be any value\np\n(\u03c32\u03b3\nts \u03c32(\u03b3\u22121)\nt\u2192s\n) for \u03b3 \u2208 [0, 1]. Here \u03c32\nst is the diffusion\nvariance and \u03c32\nt\u2192s is the true denoising variance, when con-\nditioned on a single example (detailed in Appendix A). For\nsmaller noise levels (\u03b3 \u2248 0.0) and larger number of denois-\ning steps, generations tend to become blurrier. For larger\nnoise levels (\u03b3 \u2248 1.0) and smaller number of denoising\nsteps, generations tend to become grainy and noisy. To limit\nthe cost of sampling, we consider sampling steps from and\nbelow 250. In this setting, we find that smaller noise lev-\nels are preferred (\u03b3 = 0.0 for MS-COCO and \u03b3 = 0.1 for\nCLIC20).\n4.4. Architecture\nDiffusion models generally use U-Nets [33] with resid-\nual convolutional blocks and self-attention. Because con-\nvolutional layers at high resolutions are very expensive in\nterms of memory and computation, we limit the size of\nthese layers as much as possible. The exact details are given\nTable 1. HFD U-Net architecture\nLevel\n256\u00d7\n128\u00d7\n64\u00d7\n32\u00d7\n16\u00d7\nChannels\n128\n128\n256\n256\n1024\nBlocks\n2\n2\n2\n2\n16\nAttention\n-\n-\n-\n-\n\u2713\n0\n50 100 150 200 250\n0\n20\n40\nSteps\nFID\n0\n50 100 150 200 250\n26\n28\n30\n32\n34\nSteps\nPSNR\nHFD (Ours)\nRF (Ours)\nFigure 8. FID and PSNR as a function of the number of steps used\nto simulate the SDE/ODE underlying each model on CLIC20.\nin Table 2. The autoencoder output xMSE is concatenated\nto the current diffusion state zt as the first step of the archi-\ntecture. Following recent advances in diffusion [35, 21, 18],\nthe bulk of the computation is moved from high resolution\nto the lower resolution feature maps.\n5. Experiments\n5.1. Metrics\nWe focus on the well-established metrics FID [15] and\nPSNR to measure realism and distortion, respectively. In\nline with previous work [27, 2, 7], we evaluate FID on\npatches of 256\u00d7256 pixels, see Appendix A.7 of Mentzer\net al. [27].\n5.2. Datasets\nWe compare on the following datasets: Kodak [23],\ncontaining 24 images, each either 512\u00d7768px or the in-\nverse. From the CLIC compression challenge [1], we use\nthe full dataset of CLIC201, which contains 428 images\nof varying resolutions, up to 2000px wide, and the test\nset of CLIC22 [1], which contains 30 high-resolution im-\nages, resized such that the longer side is 2048px. While we\ncan evaluate FID in the patched manner mentioned above\non CLIC20, the other datasets are too small. Inspired by\nthe image generation literature (e.g., [49]), recent work by\nAgustsson et al. [2] additionally evaluates on MS-COCO\n30k, which we also use. We follow the preparation scheme\nlinked in [2] and compare to their published results. It is\na dataset of 30 000 images of 256\u00d7256px each, and hence\nour patched FID corresponds to full FID.\n5.3. Training\nWe train our models for 2M iterations with a batch size\nof 256 on crops with resolution 256px. To get crops for\ntraining, we extract a collection of 640px crops from Inter-\nnet images and encode/decode them with our MSE model.\n1www.tensorflow.org/datasets/catalog/clic\nHFD (Ours)\nPO-ELIC\nOriginal\nFigure 9. Failure cases. HFD has been optimized to produce real-\nistic images from an MSE based decoder. Consequently, high and\nmid frequency details can sometimes be lost or generated differ-\nently. For example, the cable lines have disappeared in the gener-\nation from HFD. Comparison at a comparable low bit-rate setting.\nWe then discard a 64px border to get pairs of 512px recon-\nstructions and originals. This is to ensure that potential bor-\nder artefacts from the MSE model are not over-represented\nin the training data (compared to the high-resolution eval-\nuation set). In addition, the subset which contains people\nof the train partition of the MS-COCO [25] dataset is used.\nWe found that the inclusion of the latter was also important\nto improve performance on the MS-COCO eval benchmark,\n4.46 versus 6.79 in terms of FID for HFD at the low bit-rate\nsetting. This importance may be caused by the difference in\ndistribution between patches from high resolution data and\ncenter crop images that are typically used for MS-COCO.\nThe optimization for the model uses Adam with \u03b21 =\n0.9 and \u03b22 = 0.99 and a learning rate of 10\u22124, with a\nwarmup of 10000 and half life of 400000. Finally all eval-\nuation is done on an exponential moving average, that is\ncomputed using 0.9999 during training.\n5.4. Baselines\nFor our models, we run HFD/DDPM using DDPM for\nsampling, HFD/DDIM using DDIM, and RF using recti-\nfied flows. We compare against the following baselines.\nNote that not all methods publish reconstructions on all\ndatasets, and not all datasets are big enough to compute FID\nHFD (Ours): 0.2639 bpp\nYang & Mandt (2023): 0.2971 bpp\nD (Ours): 0.2639 bpp\n(Ours): 0.2639 bpp\nYang & Mandt (20\nYang & Mandt\nYang & Mandt (2023): 0.2971\nYang & Mandt (2023): 0.2971 b\n71 bpp\n2971 bpp\nFigure 10. Qualitative comparison with the approach of Yang and Mandt [48]. We find that our model generally produces fewer artefacts\nat the same or lower bit-rate, despite not being trained end-to-end. Additional examples are provided in Appendix B.\nreliably, so we compare against some methods only visu-\nally. From the GAN-based image compression literature,\nwe compare against HiFiC [27], MR [2], PQ-MIM [7], as\nwell as PO-ELIC [14] (the latter only has reconstructions\non CLIC22, so we only compare visually). Finally, we com-\npare against the diffusion-based approaches DIRAC [11],\nwhich presents FID results on CLIC20 (we use the high\nperceptual quality model), and CDC [48] (only visually, on\nKodak).\nResults\nAs is shown in Fig. 7, HFD outperforms all other\nbaselines in terms of rate-FID curves on both CLIC20 and\nMS-COCO 30K. On the other hand, that realism comes at\nthe cost of distortion in terms of PSNR where other models\nare either better or competitive. Interestingly, FID score is\nimproved by our proposed shifted schedule for more detail\nin Fig. 6, whereas the opposite direction (as proposed in\nthe literature) worsens the performance. This confirms our\nhypothesis that HFD benefits more from focusing on finer\ndetails in images.\nFurthermore, Fig. 8 shows that rectified flows outper-\nform HFD when the number of steps is constrained to less\nthan approximately 100 steps. However, in line with results\nin the literature [17, 26] HFD outperforms the rectified flow\nfor a larger sampling budget. In terms of distortion, larger\nsampling budgets typically result in lower PSNR. Qualita-\ntive comparisons can be found in Figs. 1, 2, 3, 10 in addition\nto further comparisons in the Appendix.\nRealism versus Distortion\nHFD can be seen as a method\nthat favors realism over distortion. We find that this causes it\nto sometimes produce reconstructions which are less accu-\nrate than other methods. Example failure cases are provided\nin Fig. 9. These images contain details that have largely\nvanished from the autoencoder output \u02c6xMSE, for example\nthe cable lines or the grain on black surfaces. HFD also has\na denoising effect causing reconstructions of noisy images\nto look less like the input, despite looking realistic. We find\nthat this can be addressed by additionally encoding the ab-\nsolute residuals at low resolution and very small bit-rates,\nand conditioning the diffusion model on this additional sig-\nnal (Appendix G).\n6. Discussion\nIn this paper we have demonstrated that HFD consis-\ntently outperforms existing methods in terms of FID on\nmultiple datasets, especially at low bit-rates. This was en-\nabled by modifications to the diffusion approach specifi-\ncally aimed at the compression setting, most importantly\nshifting the noise schedule.\nFurthermore, we show that\nthe rectified flow outperforms diffusion with very few sam-\npling steps although for larger numbers of steps the flow\nis still outperformed by its diffusion counterpart.\nWe\nsee several avenues for further improvement. One of the\nmain challenges for future work will be to improve the\nsampling speed of diffusion-based compression approaches\nwith techniques such as progressive distillation [38].\nAcknowledgments\nThe authors would like to thank Erfan Noury for pro-\nviding HEVC and VVC reconstructions for the CLIC22\ndataset, Ruihan Yang for providing Kodak reconstruc-\ntions [48], David Minnen for help obtaining MSE based\nreconstructions used in an earlier implementation, and Ben\nPoole for feedback on the manuscript.\nReferences\n[1] Challenge on Learned Image Compression, 2022.\n[2] Eirikur Agustsson, David Minnen, George Toderici, and\nFabian Mentzer. Multi-realism image compression with a\nconditional generator, 2022.\n[3] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer,\nRadu Timofte, and Luc Van Gool. Generative adversarial\nnetworks for extreme learned image compression. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 221\u2013231, 2019.\n[4] Y. Blau and T. Michaeli. The perception-distortion tradeoff.\nIn 2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6228\u20136237, 2018.\n[5] Ting Chen. On the importance of noise scheduling for diffu-\nsion models. CoRR, abs/2301.10972, 2023.\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. Advances in Neural Infor-\nmation Processing Systems, 34, 2021.\n[7] Alaaeldin El-Nouby, Matthew J Muckley, Karen Ullrich,\nIvan Laptev, Jakob Verbeek, and Herv\u00b4e J\u00b4egou. Image com-\npression with product quantized masked image modeling.\narXiv preprint arXiv:2212.07372, 2022.\n[8] Alaaeldin El-Nouby, Matthew J. Muckley, Karen Ullrich,\nIvan Laptev, Jakob Verbeek, and Herv\u00b4e J\u00b4egou. Image com-\npression with product quantized masked image modeling,\n2022.\n[9] Noor Fathima Ghouse, Jens Petersen, Auke Wiggers, Tianlin\nXu, and Guillaume Sauti`ere. Neural image compression with\na diffusion-based decoder, 2023.\n[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.\nWarde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\nerative Adversarial Nets.\nIn Z. Ghahramani, M. Welling,\nC. Cortes, N. Lawrence, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems, vol-\nume 27, 2014.\n[11] Noor Fathima Goose, Jens Petersen, Auke Wiggers, Tianlin\nXu, and Guillaume Sautiere. Neural image compression with\na diffusion-based decoder. arXiv preprint arXiv:2301.05489,\n2023.\n[12] M. Havasi, R. Peharz, and J. M. Hern\u00b4andez-Lobato. Mini-\nmal Random Code Learning: Getting Bits Back from Com-\npressed Model Parameters. In International Conference on\nLearning Representations, 2019.\n[13] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei\nQin, and Yan Wang. ELIC: Efficient Learned Image Com-\npression with Unevenly Grouped Space-channel Contextual\nAdaptive Coding.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5718\u20135727, 2022.\n[14] D. He, Z. Yang, H. Yu, T. Xu, J. Luo, Y. Chen, C. Gao, X.\nShi, H. Qin, and Y.Wang. PO-ELIC: Perception-Oriented Ef-\nficient Learned Image Coding. In 5th Challenge on Learned\nImage Compression, 2022.\n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium. In Advances in Neural Information Processing\nSystems, volume 30, 2017.\n[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\nmans. Imagen video: High definition video generation with\ndiffusion models. CoRR, abs/2210.02303, 2022.\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 6840\u20136851.\nCurran Associates, Inc., 2020.\n[18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Sim-\nple diffusion: End-to-end diffusion for high resolution im-\nages, 2023.\n[19] Danlan Huang, Feifei Gao, Xiaoming Tao, Qiyuan Du,\nand Jianhua Lu. Toward semantic communications: Deep\nlearning-based image semantic coding. IEEE Journal on Se-\nlected Areas in Communications, 41(1):55\u201371, 2023.\n[20] ITU-T. Recommendation ITU-T T.81: Information technol-\nogy \u2013 Digital compression and coding of continuous-tone\nstill images \u2013 Requirements and guidelines, 1992.\n[21] Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive\ncomputation for iterative generation. CoRR, abs/2212.11972,\n2022.\n[22] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo.\nOn density estimation with diffusion models.\nIn\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Process-\ning Systems, 2021.\n[23] Kodak. PhotoCD PCD0992, 1993.\n[24] C. Ledig, L. Theis, F. Husz\u00b4ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W.\nShi.\nPhoto-realistic single image super-resolution using a\ngenerative adversarial network. In CVPR, 2017.\n[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC. Lawrence Zitnick.\nMicrosoft COCO: common objects\nin context.\nIn Computer Vision - ECCV 2014 - 13th Eu-\nropean Conference, Zurich, Switzerland, September 6-12,\n2014, Proceedings, Part V, 2014.\n[26] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. CoRR, abs/2209.03003, 2022.\n[27] Fabian Mentzer, George D Toderici, Michael Tschannen, and\nEirikur Agustsson. High-fidelity generative image compres-\nsion. Advances in Neural Information Processing Systems,\n33, 2020.\n[28] David Minnen and Saurabh Singh. Channel-wise autoregres-\nsive entropy models for learned image compression. In 2020\nIEEE International Conference on Image Processing (ICIP),\npages 3339\u20133343. IEEE, 2020.\n[29] Yash Patel, Srikar Appalaraju, and R. Manmatha. Saliency\ndriven perceptual image compression. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), pages 227\u2013236, January 2021.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents, 2022.\n[31] O. Rippel and L. Bourdev. Real-time adaptive image com-\npression. In Proceedings of the 34th International Confer-\nence on Machine Learning, 2017.\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, June 2022.\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Nassir Navab, Joachim Hornegger, William M. Wells III,\nand Alejandro F. Frangi, editors, Medical Image Computing\nand Computer-Assisted Intervention - MICCAI, 2015.\n[34] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.\nLee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-\nhammad Norouzi. Palette: Image-to-Image Diffusion Mod-\nels. CoRR, abs/2111.05826, 2021.\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Alice H. Oh, Alekh Agar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad-\nvances in Neural Information Processing Systems, 2022.\n[36] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J. Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement, 2021.\n[37] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In The Tenth International\nConference on Learning Representations, ICLR. OpenRe-\nview.net, 2022.\n[38] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In International Confer-\nence on Learning Representations, 2022.\n[39] Shibani Santurkar, David Budden, and Nir Shavit. Genera-\ntive compression. In 2018 Picture Coding Symposium (PCS),\npages 258\u2013262. IEEE, 2018.\n[40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n[41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021.\n[42] Yang Song and Stefano Ermon.\nGenerative modeling by\nestimating gradients of the data distribution. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd\u2019Alch\u00b4e-Buc, Emily B. Fox, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing Systems\n2019, NeurIPS, pages 11895\u201311907, 2019.\n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021.\n[44] L. Theis and E. Agustsson. On the advantages of stochastic\nencoders. In Neural Compression Workshop at ICLR, 2021.\n[45] L. Theis, T. Salimans, M. D. Hoffman, and F. Mentzer.\nLossy\ncompression\nwith\ngaussian\ndiffusion,\n2022.\narXiv:2206.08889.\n[46] L. Theis and N. Yosri. Algorithms for the communication of\nsamples. In Proceedings of the 39th International Confer-\nence on Machine Learning, 2022.\n[47] Lirong Wu, Kejie Huang, and Haibin Shen. A gan-based\ntunable image compression system. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), March 2020.\n[48] Ruihan Yang and Stephan Mandt. Lossy image compression\nwith conditional diffusion models, 2023.\n[49] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022.\n[50] George Zhang, Jingjing Qian, Jun Chen, and Ashish J Khisti.\nUniversal rate-distortion-perception representations for lossy\ncompression. In A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Informa-\ntion Processing Systems, 2021.\nA. Additional details on diffusion models\nThis section contain additional details on the diffusion model. Recall that the marginal distribution of the diffusion process\nis defined by:\nq(zt|x) = N(zt|\u03b1tx, \u03c32\nt I),\n(11)\nwhere \u03b1t, \u03c3t \u2208 [0, 1] and under a variance preserving process \u03b12\nt = 1 \u2212 \u03c32\nt . Assuming this process is Markov, we can write\nthe transition probability as:\nq(zt|zt\u22121) = N(zt|zs\u03b1ts, \u03c32\ntsI)\n(12)\nwhere s < t, \u03b1ts = \u03b1t/\u03b1s and \u03c32\nts = \u03c32\nt \u2212 \u03b12\nt|s\u03c32\ns. Using the two equations above and that the process is Markov, one can\nderive the denoising posterior conditioned on a single example x:\nq(zs|zt, x) = N(zs|\u00b5t\u2192s(zt, x), \u03c32\nt\u2192sI),\n(13)\nwhere \u00b5t\u2192s = \u03b1ts\n\u03c32\ns\n\u03c32\nt zt + \u03b1s\n\u03c32\nts\n\u03c32\nt x [5]. The optimal generative denoising process p(zs|zt) tends to q(zs|zt, E[x|zt]) when\ns \u2192 t [10] which shows that it suffices to learn \u02c6x = f(zt, t) with a neural network. However, under a constrained number\nof steps, we find that the variance in p(zs|zt) can make a difference in sample quality (too noisy or too blurry). Following\n[8] we use the formulation:\np(zs|zt) = N(zs|\u00b5t\u2192s(zt, \u02c6x), \u03c32\u03b3\nts \u03c32(\u03b3\u22121)\nt\u2192s\n)I)\n(14)\nwhere \u03b3 \u2208 [0, 1] is a hyperparameter that interpolates (in log-space) between the noise of the diffusion transition variance \u03c32\nts\nand true denoising variance (for a single example) \u03c32\nt\u2192s. As a rule-of-thumb, for smaller number of sampling steps, \u03b3 should\nbe smaller. Note that this setting only influences the DDPM (sometimes referred to as ancestral) sampler [4], the DDIM [9]\nsampler does not use this denoising variance.\nB. Further rate distortion results\nIn Fig. 11, the same comparison from the main paper is included, but now including the DDIM [9] sampler for HFD and\nthe rectified flow result. Our HFD/DDPM is the best performing model in terms of FID score.\n0\n0.1 0.2 0.3 0.4 0.5\n0\n2\n4\n6\n8\nBit-rate [bpp]\nFID\nCLIC2020\nHFD/DDPM (Ours)\nHFD/DDIM (Ours)\nRF (Ours)\nMR [1]\nHiFiC [6]\nPQ-MIM [2]\nDIRAC-100 [3]\n0\n0.1 0.2 0.3 0.4 0.5\n26\n28\n30\n32\n34\nBit-rate [bpp]\nPSNR\nCLIC2020\n0\n0.1 0.2 0.3 0.4 0.5\n0\n2\n4\n6\nBit-rate [bpp]\nFID\nMS-COCO 30k\n0\n0.1 0.2 0.3 0.4 0.5\n24\n26\n28\n30\n32\nBit-rate [bpp]\nPSNR\nMS-COCO 30k\nFigure 11. Realism and distortion as measured by FID and PSNR for various methods evaluated on MS-COCO 30k and CLIC20.\nHFD/DDPM is able to generate realistic images at impressively low bitrates, surpassing all existing methods in terms of rate-FID curves.\nIt is also worth noting that this model considerably outperforms DIRAC-100, the only other existing diffusion approach for high resolution\nimages.\nC. Additional results obtained with text-to-image models\nFigure 12. Image reconstructions obtained with Stable Diffusion [7] by conditioning on a 4\u00d7 downsampled image together with the text\n\u201cA lighthouse in Maine behind a white fence with a red life buoy hanging on it.\u201d Depending on the choice of parameters, reconstructions\nare more or less faithful to the original image. However, we were unable to achieve a level of fidelity that we would deem acceptable for\nthe task of image compression.\nFigure 13. Image reconstruction of a 512 \u00d7 512 image obtained with Imagen [8] by conditioning on a 8\u00d7 downsampled image together\nwith the text \u201cA lighthouse in Maine behind a white fence with a red life buoy hanging on it.\u201d\nD. Additional comparisons with Yang & Mandt [11]\nHFD (Ours): 0.4446 bpp\nYang & Mandt (2023): 0.4234 bpp\nHFD (Ours): 0.1709 bpp\nYang & Mandt (2023): 0.3021 bpp\nD (Ours): 0.4446 bpp\n0.4446 bpp\nYang\nYang & Mandt (2023): 0.4234\nYang & Mandt (2023): 0.4234 bpp\nMandt (2023): 0.4234 bpp\nt (2023): 0.4234 bpp\n0 1709 b\nYang\nYang & Mandt (2023) 0 3021 bpp\nMandt (2023): 0.3021 bpp\nt (2023): 0.3021 bpp\nFigure 14. Our model generally compares favorably to that of Yang & Mandt [11] in terms of perceptual quality when evaluated at similar\nbit-rates (upper row) or even using a significantly lower bit-rate (lower row). Nevertheless, as for other generative compression methods,\nsmall faces remain a challenge at very low bit-rates.\nHFD (Ours): 0.1848 bpp\nYang & Mandt (2023): 0.2814 bpp\nHFD (Ours): 0.1828 bpp\nYang & Mandt (2023): 0.2053 bpp\nHFD (Ours): 0.26 bpp\nYang & Mandt (2023): 0.3 bpp\nFigure 15. Additional example comparisons with Yang & Mandt [11].\nE. Additional reconstructions\nFigure 16. Reconstructions of an image from the CLIC2020 dataset compressed with HFD at 0.0538 bpp (left) and 0.0307 bpp (right),\nrespectively.\nF. Partial generation\nFigure 17. Patches are generated in stages. In the image above, noise-free pixels of four patches have been generated conditioned on noisy\npixels in the surrounding patches.\nG. HFD+\nFigure 18. Visualization of inputs to HFD+. The absolute value of residuals, |\u02c6xMSE\u2212x|, is downsampled by a factor of 8 and then encoded\nas a JPEG at a very low bit- rate. This residual energy image is fed into the generative model alongside \u02c6xMSE.\nUncompressed\nHFD: 0.0222 bpp\nHFD+: 0.0295 bpp\nUncompressed\nHFD: 0.0943 bpp\nHFD+: 0.1032 bpp\nFigure 19. HFD can have a denoising effect (top row). While the result looks pleasing, this effect may not always be desired. Additionally\nconditioning on the residual energy allows HFD+ to produce a grainier reconstruction which is closer to the uncompressed image. Similarly,\nHFD is sometimes unable to distinguish between images which were out of focus or whose high frequencies have merely been lost in the\nreconstruction \u02c6xMSE. Conditioning on the residual energy allows HFD+ to hallucinate an appropriate amount of high frequencies (bottom\nrow).\nH. Architecture\nTable 2. HFD U-Net architecture\nLevel\n256\u00d7\n128\u00d7\n64\u00d7\n32\u00d7\n16\u00d7\nChannels\n128\n128\n256\n256\n1024\nBlocks\n2\n2\n2\n2\n16\nAttention\n-\n-\n-\n-\n\u2713\nReferences\n[1] Eirikur Agustsson, David Minnen, George Toderici, and Fabian Mentzer.\nMulti-realism image compression with a conditional\ngenerator, 2022.\n[2] Alaaeldin El-Nouby, Matthew J Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, and Herv\u00b4e J\u00b4egou. Image compression with\nproduct quantized masked image modeling. arXiv preprint arXiv:2212.07372, 2022.\n[3] Noor Fathima Goose, Jens Petersen, Auke Wiggers, Tianlin Xu, and Guillaume Sautiere. Neural image compression with a diffusion-\nbased decoder. arXiv preprint arXiv:2301.05489, 2023.\n[4] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing\nSystems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.\n[5] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In A. Beygelzimer,\nY. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n[6] Fabian Mentzer, George D Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. Ad-\nvances in Neural Information Processing Systems, 33, 2020.\n[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-resolution image synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n10684\u201310695, June 2022.\n[8] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael\nGontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-\nimage diffusion models with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,\neditors, Advances in Neural Information Processing Systems, 2022.\n[9] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning\nRepresentations, 2021.\n[10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative\nmodeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n[11] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models, 2023.\n"
  },
  {
    "title": "Photoswap: Personalized Subject Swapping in Images",
    "link": "https://arxiv.org/pdf/2305.18286.pdf",
    "upvote": "1",
    "text": "PHOTOSWAP:\nPersonalized Subject Swapping in Images\nJing Gu1\u2217\nYilin Wang2\nNanxuan Zhao2\nTsu-Jui Fu3\nWei Xiong2\nQing Liu2\nZhifei Zhang2\nHe Zhang2\nJianming Zhang2\nHyunJoon Jung2\nXin Eric Wang1\u2217\n1University of California, Santa Cruz\n2Adobe\n3University of California, Santa Barbara\nhttps://photoswap.github.io/\nreference image\nsource image\nPhotoswap\nreference image\nsource image\nPhotoswap\nFigure 1: Photoswap can effortlessly replace the subject in a source image, which could be either\nsynthetic (first two rows) or real (bottom row), with a personalized subject specified in reference\nimages, while preserving the original subject pose and the composition of the source image.\nAbstract\nIn an era where images and visual content dominate our digital landscape, the abil-\nity to manipulate and personalize these images has become a necessity. Envision\nseamlessly substituting a tabby cat lounging on a sunlit window sill in a photo-\ngraph with your own playful puppy, all while preserving the original charm and\ncomposition of the image. We present Photoswap, a novel approach that enables\nthis immersive image editing experience through personalized subject swapping\nin existing images. Photoswap first learns the visual concept of the subject from\nreference images and then swaps it into the target image using pre-trained diffusion\nmodels in a training-free manner. We establish that a well-conceptualized visual\nsubject can be seamlessly transferred to any image with appropriate self-attention\nand cross-attention manipulation, maintaining the pose of the swapped subject and\nthe overall coherence of the image. Comprehensive experiments underscore the\nefficacy and controllability of Photoswap in personalized subject swapping. Fur-\nthermore, Photoswap significantly outperforms baseline methods in human ratings\nacross subject swapping, background preservation, and overall quality, revealing\nits vast application potential, from entertainment to professional editing.\n\u2217 Correspondence to Jing Gu and Xin Eric Wang, {jgu110,xwang366}@ucsc.edu.\nPreprint.\narXiv:2305.18286v1  [cs.CV]  29 May 2023\n1\nIntroduction\nImagine a digital world where the boundaries of reality and creativity blur, where a photograph of\na tabby cat lounging in a sunlit window sill can effortlessly be transformed to feature your playful\npuppy in the same pose. Or envision yourself as a part of a famous movie scene, replaced seamlessly\nwith the original character while preserving the very essence and composition of the scene. Can\nwe achieve this level of personalized image editing, not just with expert-level photo manipulation\nskills, but in an automated, user-friendly manner? This question lies at the heart of personalized\nsubject swapping, the challenging task of replacing the subject in an image with a user-specified\nsubject, while maintaining the integrity of the original pose and composition. It opens up a plethora\nof applications in areas such as entertainment, advertising, and professional editing.\nPersonalized subject swapping is a complex undertaking that comes with its own set of challenges.\nThe task requires a profound comprehension of the visual concept inherent to both the original subject\nand the replacement subject. Simultaneously, it demands the seamless integration of the new subject\ninto the existing image. One of the critical objectives in subject swapping is to preserve the similar\npose of the replacement subject. It is crucial that the swapped subject seamlessly fits into the original\npose and scene, creating a natural and harmonious visual composition. This necessitates careful\nconsideration of factors such as lighting conditions, perspective, and overall aesthetic coherence. By\neffectively blending the replacement subject with these elements, the final image maintains a sense of\ncontinuity and authenticity.\nExisting image editing methods fall short in addressing these challenges. Many of these techniques\nare restricted to global editing and lack the finesse needed to seamlessly integrate new subjects\ninto existing images. For example, for most text-to-image (T2I) models, a slightly prompt change\ncould lead to a totally different image. Recent works [3, 6, 25, 27, 46] allow user to control the\ngeneration with an additional input such as user brush, semantic layout, or sketches. However, it is\nstill challenging to guide the generation process to follow users\u2019 intent on the generation of object\nshape, texture, and identity. Other approaches [16, 26, 41] have explored the potential of using text\nprompts to edit image content in the context of synthetic image generation. Despite showing promise,\nthese methods are not yet fully equipped to handle the intricate task of swapping subjects in existing\nimages with user-specified subjects.\nTherefore, we present Photoswap, a novel framework that leverages pre-trained diffusion models for\npersonalized subject swapping in images. In our approach, the diffusion model learns to represent the\nconcept of the subject (Ot). Then the representative attention map and attention output saved in the\nsource image generation process will be transferred into the generation process of the target image to\ngenerate the new subject while keeping non-subject pixels unchanged. Our extensive experiments\nand evaluations demonstrate the effectiveness of Photoswap. Not only does our method enable the\nseamless swapping of subjects in images, but it also maintains the pose of the swapped subject and\nthe overall coherence of the image. Remarkably, Photoswap outperforms baseline methods by a large\nmargin in human evaluations of subject identity preservation, background preservation, and overall\nquality of the swapping (e.g., 50.8% vs. 28.0% in terms of overall quality). The contributions of this\nwork are as follows: 1) We present a new framework for personalized subject swapping in images.\n2) We propose a training-free attention swapping method that governs the editing process. 3) The\nefficacy of our proposed framework is demonstrated through extensive experiments including human\nevaluation.\n2\nRelated Work\n2.1\nText-to-Image Generation\nIn the early stages of text-based image generation, Generative Adversarial Networks (GANs) [2, 14,\n19] were widely used due to their exceptional ability to produce high-quality images. These models\naimed to align textual descriptions with synthesized images through multi-modal vision-language\nlearning, achieving impressive results on specific domains (e.g., bird, chair and human face). When\ncombined with CLIP [32], a large pre-trained model that learns visual-textual representations from\nmillions of caption-image pairs, GAN models [7] have demonstrated promising outcomes in cross-\ndomain text-to-image (T2I) generation. Recently, T2I generation has seen remarkable progress with\nauto-regressive [9, 9, 29] and diffusion models [15, 27, 30, 36], offering diverse outcomes and can\nsynthesize high-quality images closely aligned with textual descriptions in arbitrary domains.\n2\nRather than focusing on T2I generation tasks without any constraints, subject-driven T2I generation [4,\n28, 35] requires the model to identify the specific object from a set of visual examples and synthesize\nnovel scenes incorporating them based on the input text prompts. Building upon modern diffusion\ntechniques, recent approaches such as DreamBooth [35] and Textual Inversion [12, 13, 21, 26] learn\nto invert special tokens from a given set of images. By combining these tokens with text prompts,\nthey generate personalized unseen images. To improve data efficiency, retrieval augmentation\ntechniques [1, 5, 39] leverages external knowledge bases to overcome limitations posed by rare\nentities, resulting in visually relevant appearances and enhanced personalization. In our work, we\naim to tackle personalized subject swapping, not only preserving the identity of subjects in reference\nimages, but also maintaining the context of the source image.\n2.2\nText-guided Image Editing\nText-guided image editing manipulates an existing image based on the input textual instructions,\nwhile preserving certain aspects or characteristics of the original image. Early works based on GAN\nmodels [19] only limits to a certain object domain. Diffusion-based methods [10, 27, 46] break\nthis barrier and support text-guided image editing. Though these methods generate stunning results,\nmany of them suffer from conducting local editing, and additional manual masks [25, 25, 45] are\nrequired to constrain the editing regions, which is often tedious to draw. By employing cross-attention\n[16] or spatial characteristics [41], the local editing can be achieved but struggles with non-rigid\ntransformations (e.g., changing pose) and retaining the original image layout structure. While\nImagic [20] addresses the need for non-rigid transformations by fine-tuning a pre-trained diffusion\nmodel to capture image-specific appearances, it requires test-time finetuning, which is not time-\nefficient for deployment. Moreover, relying solely on text as input lacks precise control. In contrast,\nwe propose a novel training-free attention swapping scheme that enables precise personalization\nbased on reference images, without the need for time-consuming finetuning.\n2.3\nExemplar-guided Image Editing\nExemplar-guided image editing covers a broad range of applications, and most of the works [17, 42,\n49] can be categorized as exemplar-based image translation tasks, conditioning on various information,\nsuch as stylized images [8, 24, 48], layouts [18, 22, 44], skeletons [22], sketches/edges [38]. With the\nconvenience of stylized images, image style transfer [23, 47] receives extensive attentions, replying\non methods to build a dense correspondence between input and reference images, but it cannot deal\nwith local editing. To achieve local editing with non-rigid transformation, conditions like bounding\nboxes and skeletons are introduced, but require drawing efforts from users, which sometimes are hard\nto obtain. A recent work [43] poses exemplar-guided image editing task as an inpainting task with\nthe mask and transfers the semantic content from the reference image to the source one, with the\ncontext intact. Different from these works, we propose a more user-friendly scenario by conducting\npersonalized subject swapping with only reference images and obtain high-quality editing results.\n3\nPreliminary\nDiffusion models are a type of generative model that operates probabilistically. In this process, an\nimage is created by gradually eliminating noise from the target that is characterized by Gaussian\nnoise. In the context of text-to-image generation, a diffusion model typically involves a process where\nan initial random image is gradually refined step by step, with each step guided by a learned model,\nuntil it becomes a realistic image. The changes to the image spread out and affect many pixels over\ntime. Given an initial random noise zT \u223cN(0, I), the diffusion model gradually denoise zt, which\ngives zt\u22121.\nDiffusion models are probabilistic generative models that learn to generate images by simulating\na random process called a diffusion process. In the image generation process, the diffusion model\ngradually predicts the noise at the current diffusion step and denoises to get the final image. In this\nstudy, we utilize a pre-trained text-to-image diffusion model, Stable Diffusion [33], which encodes\nthe image into latent space and gradually denoises the latent variable to generate a new image. Stable\nDiffusion is based on a U-Net architecture [34], which generates latent variable zt\u22121 conditioned on\na given text prompt P and the latent variable zt from the previous step t:\nzt\u22121 = \u03f5\u03b8(zt, P, t)\n(1)\n3\nA photo of a cat sitting in \na green bucket on a table.\n\ud835\udc67!\n\"\n\ud835\udc67#\n$\n<     >\n*  \nNew Concept\nA photo of             sitting in \na green bucket on a table.\nPrompt with new concept\n\ud835\udc67#\n\"\nInitial noise \ncopy\n\ud835\udc67#%&\n$\n\ud835\udc67#%&\n\"\nTransfer M: self-attention map, A: cross \nattention map, \ud835\udf19: attention output\nx \ud835\udf06\nImage Encoder\nx (T-\ud835\udf06)\nEncode Input\nT steps of Diffusion \n\ud835\udc67!\n$\nOriginal Image\nGenerated Result\nDecode to output\nImage Decoder\nObject Inversion\nIf continue (T-\ud835\udf06) steps\u2026\nM, A, \ud835\udf19\nDDIM \nInversion\nSource Image Diffusion process\nTarget Image Diffusion process\n\ud835\udc3c!\n\ud835\udc42\"\n\ud835\udc43!:\n\ud835\udc43!:\n\ud835\udf03\n\ud835\udf03\u2217\nFigure 2: The Photoswap framework. Given several images of a new concept, the diffusion model\nfirst learns the concept and converts it into a token. The upper part is the generation process of the\nsource image, while the bottom part is the generation process of target image. The initial noise feature\nzt\nT is copied from zs\nT of the source. The attention output and attention map in the source image\ngeneration process would be transferred to the target image generation process. The final feature zt\n0\nis decoded to output the target image. Refer to Section 4 for more details.\nThe U-Net consists of layers that include repetition of self-attention and cross-attention blocks. This\nstudy focuses on manipulating self-attention and cross-attention to achieve the task of personalized\nsubject swapping.\n4\nThe Photoswap Method\nProviding a few reference images of a personalized target subject Ot, Photoswap can seamlessly\nswap it with another subject Os in a given source image Is. The Photoswap pipeline is illustrated in\nFigure 2. To learn the visual concept of the target subject Ot, we fine-tune a diffusion model with\nreference images and do object inversion to represent Ot using special token *. Then, to substitute\nthe subject in the source image, we first obtain the noise zT 2 that can be used to re-construct the\nsource image Is. Next, through the U-Net, we obtain the needed feature map and attention output\nin the self-attention and cross-attention layers, including M, A, and \u03d5 (which we will introduce in\nSec. 4.2). Finally, during the target image generation process that is conditioned on the noise zT and\nthe target text prompt Pt, in the first \u03bb steps, those intermediate variables (M, A, and \u03d5) would be\nreplaced with corresponding ones obtained during the the source image generation process. In the last\n(T \u2212 \u03bb) steps, no attention swapping is needed and we can continue the denoising process as usual to\nobtain the final resulting image. Sec. 4.1 discusses the visual concept learning technique we used,\nand Sec. 4.2 details the training-free attention swapping method for controllable subject swapping.\n4.1\nVisual Concept Learning\nSubject swapping requires a thorough understanding of the subject\u2019s identity and specific character-\nistics. This knowledge enables the creation of accurate representations that align with the source\nsubject. The subject\u2019s identity influences the composition and perspective of the image, including\nits shape, proportions, and textures, which affect the overall arrangement of elements. However,\nexisting diffusion models lack information about the target subject (Ot) in their weights because\nthe training data for text-to-image generation models does not include personalized subjects. To\n2For a synthetic image, z\u2217\nT is the initial noise used to generate it. For a real image, we utilize an improved\nversion of DDIM inversion [40] to get the initial noise and re-generate the source image. See Sec. 5.1 for details.\n4\nFigure 3: SVD visualization of self-attention maps. Each image\u2019s attention map is resized to\n64x64 at every layer, and we calculate the average map across all layers for all diffusion time steps.\nMost significant components are extracted with SVD and visualized. Remarkably, the visualized\nresults demonstrate a strong correlation with the layout of the generated image. The top two rows are\nvisualization about synthetic images while the bottom two rows are about real images.\nDiffusion Steps\nFigure 4: Self-attention map visualization across diffusion time steps. This representation reveals\nthat the layout of the generated image is intrinsically embedded in the self-attention map from the\ninitial steps. Consequently, to assert control over the layout, it is imperative to commence the attention\nswap at the earliest stages of the process.\novercome this limitation and generate visually consistent variations of subjects from a given reference\nset, we need to personalize text-to-image diffusion models accurately. Recent advancements have\nintroduced various methods, such as fine-tuning the diffusion model with distinct tokens associated\nwith specific subjects, to achieve this \u201cpersonalization\u201d[11, 21, 35]. In our experiments, we primarily\nutilize DreamBooth[35] as a visual concept learning method. It\u2019s worth noting that alternative concept\nlearning methods can also be effectively employed with our framework.\n4.2\nControllable Subject Swapping via Training-free Attention Swapping\nSubject swapping poses intriguing challenges, requiring the maintenance of the source image\u2019s spatial\nlayout and geometry while integrating a new subject concept within the same pose. This necessitates\n5\nAlgorithm 1 The Photoswap Algorithm\nInputs: source image Is, reference images Ot, source image text prompt Ps, target image text\nprompt Pt, diffusion model \u03b8\n\u03b8\u2217 \u2190 \u03b8, Ot \u25b7 Finetune diffusion model to include the new concept\nzs\nT \u2190 DDIMInversion(ImageEncoder(Is), Ps)\u25b7 Using DDIM to guarantee re-construction\nzt\nT \u2190 zs\nT \u25b7 Using the same starting noise\nfor i = T, T \u2212 1, ..., 1 do\n\u03f5s, \u03d5s\ni, M s\ni, As\ni \u2190 \u03f5\u03b8\u2217(zs\ni, Ps, i) \u25b7 Denoise to get the attention output and map for source\nimage\n\u03d5t\ni, M t\ni, At\ni \u2190 \u03f5\u03b8\u2217(zt\ni, Pt, i) \u25b7 Denoise to get the attention output and map for target image\n\u03d5\u2217\ni , M \u2217\ni , A\u2217\ni \u2190 SWAP(\u03d5s\ni, M s\ni, As\ni, \u03d5t\ni, M t\ni, At\ni, i)\n\u03f5\u2217 \u2190 \u03f5\u03b8\u2217(zt\ni, Pt, i, \u03d5\u2217\ni , M \u2217\ni , A\u2217\ni ) \u25b7 Denoise the updated attention map and output\nzs\ni\u22121 \u2190 DDIMSampler(zs\ni, \u03f5s) \u25b7 Sample next latent variable for source image\nzt\ni\u22121 \u2190 DDIMSampler(zt\ni, \u03f5\u2217) \u25b7 Sample next latent variable for source image\nend for\nIt = ImageDecoder(zt\n0)\nreturn It\nfunction SWAP(\u03d5s, M s, As, \u03d5t, M t, At, i)\n\u03d5\u2217 \u2190 (i < \u03bb\u03d5)?\u03d5s : \u03d5t \u25b7 Control self-attention feature swap\nM \u2217 \u2190 (i < \u03bbM)?M s : M t \u25b7 Control self-attention Map swap\nA\u2217 \u2190 (i < \u03bbA)?As : At \u25b7 Control cross-attention map swap\nreturn \u03d5\u2217, M \u2217, A\u2217\nend function\npreserving the critical features in the source latent variable, which encapsulates the source image\ninformation, and leveraging the influence of the target image text prompt Pt, which carries the\nconcept token, to inject the new subject into the image.\nThe central role of the attention layer in orchestrating the generated image\u2019s layout has been well-\nestablished in prior works [3, 16, 41]. To keep non-subject pixels intact, we orchestrate the generation\nof the target image It by transferring vital variables to the target image generation process. Here, we\nexplore how distinct intermediate variables within the attention layer can contribute to a controllable\ngeneration in the context of subject swapping.\nWithin the source image generation process, we denote the cross-attention map as As\ni, the self-\nattention map as M s\ni, the cross-attention output as \u03c8s\ni , and the self-attention output as \u03d5s\ni. The\ncorresponding variables in the target image generation process are denoted as At\ni, M t\ni, \u03c8t\ni, \u03d5t\ni, where\ni represents the current diffusion step.\nIn the self-attention block, the latent feature zi is projected into queries qi, keys ki, and values vi.\nWe obtain the self-attention block\u2019s output \u03d5i using the following equation:\n\u03d5i = M ivi\nwhere\nM i = Softmax\n\u0010\nqiki\nT \u0011\n(2)\nwhere M i is the self-attention map, and \u03d5i is the feature output from the self-attention layer. The\ncross-attention block\u2019s output \u03c8i is:\n\u03c8i = Aivi\nwhere\nAi = Softmax\n\u0010\nqiki\nT \u0011\n(3)\nwhere Ai is the cross-attention map. In both self-attention and cross-attention, the attention map\nM i and Ai are correlated to the similarity between qi and ki, acting as weights that dictate the\ncombination of information in vi. In this work, the manipulation of the diffusion model focus on\nself-attention and cross-attention within U-Net, specifically, swapping \u03d5, M, and A, while keeping\n\u03c8 unchanged.\nSelf-attention map M, as it calculates the similarity within spatial features after linear projection,\nplays a pivotal role in governing spatial content during the generation process. As visualized in\nFigure 3, we capture M during the image generation and highlight the leading components via\nSingular Value Decomposition (SVD). This visualization reveals a high correlation between M and\n6\nreference image\nsource image\nPhotoswap\nreference image\nsource image\nPhotoswap\nFigure 5: Photoswap results across various object and image domains, demonstrating its wide\napplicability. From everyday objects to cartoon, the diversity in subject swapping tasks has showcased\nthe versatility and robustness of our framework across different contexts.\nthe geometry and content of the generated image. Further, when visualizing the full steps of the\ndiffusion process (Figure 4), we discern that the layout information is mirrored in the self-attention\nfrom the initial steps. This insight underscores the necessity of initiating the swap early on to prevent\nthe emergence of a new, inherent layout.\nCross-attention map A is determined by both latent variable and text prompt, as in Equation 3, and\nAs\niv can be viewed as a weighted sum of the information from a text prompt. Copying As\ni to At\ni\nduring the target image generation process improves the layout alignment between the source image\nand the target image.\nSelf-attention output \u03d5, derived from the self-attention layer, encapsulates rich content information\nfrom the source image, independent of direct computation with textual features. Hence, replacing\n\u03d5t\ni with \u03d5s\ni enhances the preservation of context and composition from the original image. Our\nobservations indicate that \u03d5 exerts a more profound impact on the image layout than the cross-\nattention map A.\nCross-attention output \u03c8, emanating from the cross-attention layer, embodies the visual concept\nof the target subject. It is vital to note that substituting cross-attention output \u03c8s\ni with \u03c8t\ni would\nobliterate all information from the target text prompt Pt, as illustrated in Equation 3. Given that kt\ni\nand vt\ni are projections of target prompt embeddings, we retain \u03c8s\ni unchanged to safeguard the target\nsubject\u2019s identity.\nAlgorithm 1 provides the pseudo code of our full Photoswap algorithm.\n5\nExperiments\n5.1\nImlementation Details\nFor the implementation of subject swapping on real images, we require an additional process that\nutilizes an image inversion method, specifically the DDIM inversion [40], to transform the image\ninto initial noise. This inversion method relies on a reversed sequence of sampling to achieve the\ndesired inversion. However, there exist inherent challenges when this inversion process is applied in\n7\nreference image\nsource image\nPhotoswap\n(a) Multi-subject swap.\nreference image\nsource image\nPhotoswap\n(b) Occluded subject swap.\nFigure 6: Photoswap results on multi-subject and occluded subject scenarios. The results show\nthat Photoswap can disentangle and replace multiple subjects at once. Also, Photoswap can identify\nthe target object while avoiding influencing the non-subject pixels.\ntext-guided synthesis within a classifier-free guidance setting. Notably, the inversion can potentially\namplify the accumulated error, which could ultimately lead to subpar reconstruction outcomes. To\nfortify the robustness of the DDIM inversion and to mitigate this issue, we further optimize the null\ntext embedding, as detailed in Mokady et al. [26]. The incorporation of this optimization technique\nbolsters the effectiveness and reliability of the inversion process, consequently allowing for a more\nprecise reconstruction. Without further notice, the DDIM inversion in this paper is enhanced by null\ntext embedding optimization.\nDuring inference, we utilize the DDIM sampling method with 50 denoising steps and classifier-free\nguidance of 7.5. The default step \u03bbA for cross-attention map replacement is 20. The default step\n\u03bbM for self-attention map replacement is 25, while the default step for self-attention feature \u03bb\u03d5\nreplacement is 10. Note that the replacement steps may change to some specific checkpoint. As\nmentioned in Section 4, the target prompt Pt is just source prompt Ps with the object token being\nreplaced with the new concept token. For concept learning, we mainly utilize DreamBooth [35] to\nfinetune a stable diffusion 2.1 to learn the new concept from 3 5 images. The learning rate is set to\n1e-6. We use Adawm optimizer with 800 hundred training step. We finetune both the U-net and text\nencoder. The DreamBooth training takes around 10 minutes on a machine with 8 A100 GPU cards.\n5.2\nPersonalized Subject Swapping Results\nFigure 5 showcases the effectiveness of our Photoswap technique for subject swapping. Our approach\nexcels at preserving crucial aspects such as spatial layout, geometry, and the pose of the original\nsubject while seamlessly introducing a reference subject into the target image. Remarkably, even in\ncartoon images, our method ensures that the background remains intact during the subject change\nprocess. A notable example is the \"cat\" image, where our technique successfully retains all the\nintricate details from the source image, including the distinctive \"Whiskers.\" This demonstrates\nour framework\u2019s ability to accurately capture and preserve fine-grained information during subject\nswapping.\nWe further demonstrate the versatility of Photoswap by showcasing its effectiveness in multiple\nsubject swap and occluded object swap scenarios. As depicted in Figure ?? (a), we present a source\nimage featuring two sunglasses, which are successfully replaced with reference glass while preserving\nthe original layout of the sunglasses. Similarly, in Figure ?? (b), we observe a source image with\na dog partially occluded by a suit. The resulting swapped dog wears a suit that closely matches\nthe occluded region. These examples serve to highlight the robustness of our proposed Photoswap\nmethod in handling various real-world cases, thereby enabling users to explore a broader range of\nediting possibilities.\n5.3\nComparison with Baseline Methods\nPersonalized object swap is a new task and there is no existing benchmark. However, we could\nmodify the existing attention manipulation based methods. More specifically, we used the same\n8\nreference image\nsource image\nP2P+DreamBooth\nPhotoswap\nFigure 7: Qualitative comparison between P2P+DreamBooth and Photoswap. We can observe\nthat P2P+dreambooth is capable of achieving subject swapping. However, it faces challenges in\npreserving both the background and the reference subject accurately, while for Photoswap, it is robust\nto handle various cases.\nPhotoswap\nP2P+DreamBooth\nTie\nSubject Swapping\n46.8%\n25.6%\n27.6%\nBackground Preservation\n40.7%\n32.7%\n26.6%\nOverall Quality\n50.8%\n28.0%\n21.2%\nTable 1: Human evaluation between Photoswap and P2P. Note that both Photoswap and P2P\nleverage same concept learning method DreamBooth. The results indicates, for most of the cases\nabove 70%, the proposed Photoswap is better or on par with P2P.\nconcept learning method DreamBooth to finetune the same stable diffusion checkpoint to inject the\nnew concept. To fairly compare with our results, we modified existing prompt-based editing method\nP2P [16] , an editing method based diffusion models. Note that origin P2P only works on a pair of\nsynthetic images, in our setting we use same concept learning dreambooth an fix the seed to allow\nconcept swapping. On the other hand, PnP [41] could also be implmented in similar setting, however\nwe found PnP usually can not lead to satisfactory object swapping and may lead to a huge difference\nbetween the source image and the generated image. We suspect that it is because PnP is designed\nfor image translation so it does not initiate the attention manipulation step from the beginning step.\nThe qualitative comparision between Photoswap and P2P+dreambooth is shown in Figure 7. We\nobserve that P2P with DreamBooth could achieve achieve basic object swap, but it still suffers from\nbackground mismatching issue.\nHuman Evaluation.\nWe conduct a human evaluation to study the editing quality by (1) Which\nresult better swaps the subject as the reference and keeps its identity; (2) Which result better preserves\nthe background; (3) Which result has better overall subject-driven swapping. We randomly sample\n99 examples and adopt Amazon MTurk3 to compare between two results. To avoid potential bias,\nwe hire 3 Turkers for each sample. Table 1 demonstrates the comparison between our Photoswap\nand P2P. Firstly, more turkers (over 46%) denote that our Photoswap better swaps the subject yet\nkeeps its identity at the same time. Moreover, we can also preserve the background in the source\nimage (41% vs. 33%), which is another crucial goal of this editing. In summary, Photoswap precisely\nperforms subject swapping and preserves the remaining part from the input, leading to an overall\nsuperiority (50%) to P2P.\n5.4\nControlling Subject Identity\nThe effectiveness of the proposed mutual self-attention is demonstrated through both synthetic image\nsynthesis and real image editing. Additionally, we perform an analysis of the control strategy with\nvarying values of M during the denoising process. Figure 8 provides insights into this analysis.\nIt is observed that when applying self-attention control with a large swapping step \u03bbM for M, the\n3Amazon Mechanical Turk (MTurk): https://www.mturk.com.\n9\nreference image\nsource image\nPhotoswap\nidentity\nFigure 8: Ablation results of M in Photoswap. With M value increase, the generated one is more\nsimilar to the style and identity of source image and dissimilar to the reference subject, vice versa.\nsynthesized image closely resembles the source image in terms of both style and identity. In this\nscenario, all contents from the source image are preserved, while the subject style learned from the\nreference subject is disregarded. As the value of M decreases, the synthesized image maintains the\nsubject from the reference image while retaining the layout and pose of the contents from the source\nimage. This gradual transition in the control strategy allows for a balance between subject style\ntransfer and preservation of the original image\u2019s contents.\n5.5\nAttention Swapping Step Analysis\nIn this section, we visualize the effect of the influence of swapping steps of different components.\nAs discussed in the main paper, self-attention output \u03d5, and self-attention map M, derived from the\nself-attention layer, encompasses comprehensive content information from the source image, without\nrelying on direct computation with textual features. Previous works such as Hertz et al. [16] did not\nexplore the usage of \u03d5 and M in the object-level image editing process.\nAttention map and attention output swap step\nChanging \u03bb\u0278, steps of self-attention output swap\nChanging \u03bbA, steps of cross-attention map swap\nSource Image\nReference Images\nChanging \u03bbM, steps of self-attention map swap\nFigure 9: Results at different swapping steps. With consistent steps, swapping the self-attention\noutput provides superior control over the layout, including the subject\u2019s gestures and the background\ndetails. However, excessive swapping could affect the subject\u2019s identity, as the new concept introduced\nthrough the text prompt might be overshadowed by the swapping of the attention output or attention\nmap. This effect is more clear when swapping the self-attention output \u03bb\u03d5. Furthermore, we observed\nthat replacing the attention map for an extensive number of steps can result in an image with significant\nnoise, possibly due to a compatibility issue between the attention map and the v vector.\nFigure 9 provides a visual representation of the effect of incrementally increasing the swapping step\nfor one \u03bb hyperparameter while maintaining the other two at zero. Although all of them can be utilized\nfor subject swapping, they demonstrate varying levels of layout control. At the same swapping step,\nthe self-attention output \u03d5 offers more robust layout control, facilitating better alignment of gestures\nand preservation of background context. In contrast, the self-attention map M and cross-attention\nmap A demonstrate similar capabilities in controlling the layout.\n10\nHowever, extensive swapping can affect the subject\u2019s identity, as the novel concept introduced via the\ntext prompt might be eclipsed by the swapping of the attention output or attention map. This effect\nbecomes particularly evident when swapping the self-attention output. This analysis further informs\nthe determination of the default \u03bb\u03d5, \u03bbM, and \u03bbA values. While the cross-attention map A facilitates\nmore fine-grained generation control, given its incorporation of information from textual tokens, we\ndiscovered that \u03d5 offers stronger holistic generation control, bolstering the overall output\u2019s quality\nand integrity.\n5.6\nResults of Other Concept Learning Methods\nOur mainly use DreamBooth as the concept learning method in the experiments, primarily due to its\nsuperior capabilities in learning subject identities [35]. However, our method is not strictly dependent\non any specific concept learning method. In fact, other concept learning methods could be effectively\nemployed to introduce the concept of the target subject.\nreference image\nsource image\nPhotoswap\nreference image\nsource image\nPhotoswap\nFigure 10: Results of Text Inversion [12] as the concept learning module. It can successfully\ncapture key subject features, but its performance drops when representing complex structures such as\nhuman faces.\nTo illustrate this, we present the results of Photoswap when applying Text Inversion [12]. We train the\nmodel using 8 A100 GPUs with a batch size of 4, a learning rate of 5e-4, and set the training steps to\n1000. Results in Figure 10 indicate that Text Inversion also proves to be an effective concept learning\nmethod, as it successfully captures key features of the target object. Nevertheless, we observe that\nText Inversion performance is notably underwhelming when applied to human faces. We postulate\nthat this is because Text Inversion focuses on learning a new embedding for the novel concept, rather\nthan finetuning the entire model. Consequently, the capacity to express the new concept becomes\ninherently limited, resulting in its less than optimal performance in certain areas.\n5.7\nEthics Exploration\nreference image\nsource image\nPhotoswap\nreference image\nsource image\nPhotoswap\nFigure 11: Results on real human face images across different races. Evidently, the skin colors\nare also successfully transferred when swapping a white person with a black person, and vice versa.\nLike many AI technologies, text-to-image diffusion models can potentially exhibit biases reflective of\nthose inherent in the training data [31, 37]. Given that these models are trained on vast text and image\ndatasets, they might inadvertently learn and perpetuate biases, such as stereotypes and prejudices,\nfound within this data. For instance, should the training data contain skewed representations or\ndescriptions of specific demographic groups, the model may produce biased images in response to\nrelated prompts.\nHowever, Photoswap has been designed to mitigate bias within the generation process of a text-to-\nimage diffusion model. It achieves this by directly substituting the depicted subject with the intended\ntarget. In Figure 11, we present our evaluation of face swapping across various skin tones. It is\ncrucial to observe that when there is a significant disparity between the source and reference images,\nthe swapping results tend to homogenize the skin color. As a result, we advocate for the use of\nPhotoswap on subjects of similar racial backgrounds to achieve more satisfactory and authentic\noutcomes. Despite these potential disparities, the model ensures the preservation of most of the target\nsubject\u2019s specific facial features, reinforcing the credibility and accuracy of the final image.\n11\n5.8\nFailure Cases\nreference image\nsource image\nPhotoswap\nreference image\nsource image\nPhotoswap\nFigure 12: Failure cases. The model sometimes struggles to accurately reconstruct hand details and\ncomplex background information such as formula on a whiteboard.\nHere we highlight two common failure cases. First, the model struggles to accurately reproduce\nhands. When the subject includes hands and fingers, the swapping results often fail to precisely\nmirror the original hand gestures or the number of fingers. This issue could be an inherited challenge\nfrom Stable Diffusion. Moreover, Photoswap can encounter difficulties when the image comprises\ncomplex information. As illustrated in the lower row of Figure 12, Photoswap fails to reconstruct\nthe complicated formula on a whiteboard. Therefore, while Photoswap exhibits strong performance\nacross various scenarios, it\u2019s crucial to acknowledge these limitations when considering its application\nin real-world scenarios involving intricate hand gestures or complex abstract information.\n6\nConclusion\nThis paper introduces Photoswap, a novel framework designed for personalized subject swapping\nin images. To facilitate seamless subject photo swapping, we propose leveraging self-attention\ncontrol by exchanging intermediate variables within the attention layer between the source image\nand reference images. Despite its simplicity, our extensive experimentation and evaluations provide\ncompelling evidence for the effectiveness of Photoswap. Our framework offers a robust and intuitive\nsolution for subject swapping, enabling users to effortlessly manipulate images according to their\npreferences. In the future, we plan to further advance the method to address those common failure\nissues to enhance the overall performance and versatility of personalized subject swapping.\nReferences\n[1] Blattmann, A., Rombach, R., Oktay, K., M\u00fcller, J., and Ommer, B. (2022). Retrieval-Augmented Diffusion\nModels. In NeurIPS.\n[2] Brock, A., Donahue, J., and Simonyan, K. (2018). Large scale gan training for high fidelity natural image\nsynthesis. arXiv.\n[3] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y. (2023). Masactrl: Tuning-free mutual\nself-attention control for consistent image synthesis and editing. arXiv.\n[4] Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., and Romero-Soriano, A. (2021). Instance-Conditioned\nGAN. In NeurIPS.\n[5] Chen, W., Hu, H., Saharia, C., and Cohen, W. W. (2023). Re-Imagen: Retrieval-Augmented Text-to-Image\nGenerator. In ICLR.\n[6] Couairon, G., Verbeek, J., Schwenk, H., and Cord, M. (2022). Diffedit: Diffusion-based semantic image\nediting with mask guidance. arXiv.\n[7] Crowson, K., Biderman, S., Kornis, D., Stander, D., Hallahan, E., Castricato, L., and Raff, E. (2022).\nVqgan-clip: Open domain image generation and editing with natural language guidance. In ECCV.\n[8] Deng, Y., Tang, F., Dong, W., Ma, C., Pan, X., Wang, L., and Xu, C. (2022). Stytr2: Image style transfer\nwith transformers. In CVPR.\n[9] Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al.\n(2021). Cogview: Mastering text-to-image generation via transformers. NeurIPS.\n[10] Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, X. E., and Wang, W. Y.\n(2023). Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In ICLR.\n[11] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An\nimage is worth one word: Personalizing text-to-image generation using textual inversion. arXiv.\n[12] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2023a).\nAn Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In ICLR.\n12\n[13] Gal, R., Arar, M., Atzmon, Y., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2023b). Encoder-based\nDomain Tuning for Fast Personalization of Text-to-Image Models. In arXiv.\n[14] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\nBengio, Y. (2020). Generative adversarial networks. Communications of the ACM.\n[15] Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. (2022). Vector quantized\ndiffusion model for text-to-image synthesis. In CVPR.\n[16] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-Or, D. (2022). Prompt-to-\nprompt image editing with cross attention control. arXiv.\n[17] Huang, X., Liu, M.-Y., Belongie, S., and Kautz, J. (2018). Multimodal unsupervised image-to-image\ntranslation. In ECCV.\n[18] Jahn, M., Rombach, R., and Ommer, B. (2021). High-resolution complex scene synthesis with transformers.\narXiv.\n[19] Karras, T., Laine, S., and Aila, T. (2019). A style-based generator architecture for generative adversarial\nnetworks. In CVPR.\n[20] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M. (2022). Imagic:\nText-based real image editing with diffusion models. arXiv.\n[21] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. (2023). Multi-Concept Customization of\nText-to-Image Diffusion. In CVPR.\n[22] Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and Lee, Y. J. (2023). Gligen: Open-set grounded\ntext-to-image generation. arXiv.\n[23] Liao, J., Yao, Y., Yuan, L., Hua, G., and Kang, S. B. (2017). Visual atribute transfer through deep image\nanalogy. ACM Transactions on Graphics.\n[24] Liu, S., Lin, T., He, D., Li, F., Wang, M., Li, X., Sun, Z., Li, Q., and Ding, E. (2021). Adaattn: Revisit\nattention mechanism in arbitrary neural style transfer. In ICCV.\n[25] Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. (2021). Sdedit: Image synthesis and editing\nwith stochastic differential equations. arXiv.\n[26] Mokady, R., Hertz, A., Aberman, K., Pritch, Y., and Cohen-Or, D. (2022). Null-text Inversion for Editing\nReal Images using Guided Diffusion Models. In arXiv.\n[27] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M.\n(2021). Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv.\n[28] Nitzan, Y., Aberman, K., He, Q., Liba, O., Yarom, M., Gandelsman, Y., Mosseri, I., Pritch, Y., and\nCohen-or, D. (2022). MyStyle: A Personalized Generative Prior. In Special Interest Group on Computer\nGraphics and Interactive Techniques in Asia (SIGGRAPH Asia).\n[29] OpenAI (2021). DALL\u00b7E: Creating images from text. https://openai.com/research/dall-e.\n[30] OpenAI (2022). DALL\u00b7E2. https://openai.com/product/dall-e-2.\n[31] Perera, M. V. and Patel, V. M. (2023). Analyzing bias in diffusion-based face generation models. arXiv\npreprint arXiv:2305.06402.\n[32] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P.,\nClark, J., et al. (2021). Learning transferable visual models from natural language supervision. In ICML.\n[33] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis\nwith latent diffusion models. In CVPR.\n[34] Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI. Springer.\n[35] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2023). DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven Generation. In CVPR.\n[36] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes,\nR., Karagol Ayan, B., Salimans, T., et al. (2022). Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS.\n[37] Sasha Luccioni, A., Akiki, C., Mitchell, M., and Jernite, Y. (2023). Stable bias: Analyzing societal\nrepresentations in diffusion models. arXiv e-prints, pages arXiv\u20132303.\n[38] Seo, J., Lee, G., Cho, S., Lee, J., and Kim, S. (2022). Midms: Matching interleaved diffusion models for\nexemplar-based image translation. arXiv.\n[39] Sheynin, S., Ashual, O., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and Taigman, Y. (2023).\nKNN-Diffusion: Image Generation via Large-Scale Retrieval. In ICLR.\n[40] Song, J., Meng, C., and Ermon, S. (2020). Denoising diffusion implicit models. In International Conference\non Learning Representations.\n13\n[41] Tumanyan, N., Geyer, M., Bagon, S., and Dekel, T. (2022). Plug-and-play diffusion features for text-driven\nimage-to-image translation. arXiv.\n[42] Wang, M., Yang, G.-Y., Li, R., Liang, R.-Z., Zhang, S.-H., Hall, P. M., and Hu, S.-M. (2019). Example-\nguided style-consistent image synthesis from semantic labeling. In CVPR.\n[43] Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., and Wen, F. (2022a). Paint by\nexample: Exemplar-based image editing with diffusion models. arXiv.\n[44] Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C., Zeng, M., et al. (2022b).\nReco: Region-controlled text-to-image generation. arXiv.\n[45] Zeng, Y., Lin, Z., Zhang, J., Liu, Q., Collomosse, J., Kuen, J., and Patel, V. M. (2022). Scenecomposer:\nAny-level semantic image synthesis. arXiv.\n[46] Zhang, L. and Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. arXiv.\n[47] Zhang, P., Zhang, B., Chen, D., Yuan, L., and Wen, F. (2020). Cross-domain correspondence learning for\nexemplar-based image translation. In CVPR, pages 5143\u20135153.\n[48] Zhang, Y., Huang, N., Tang, F., Huang, H., Ma, C., Dong, W., and Xu, C. (2022). Inversion-based creativity\ntransfer with diffusion models. arXiv.\n[49] Zhou, X., Zhang, B., Zhang, T., Zhang, P., Bao, J., Chen, D., Zhang, Z., and Wen, F. (2021). Cocosnet v2:\nFull-resolution correspondence learning for image translation. In CVPR.\n14\n"
  },
  {
    "title": "GlyphControl: Glyph Conditional Control for Visual Text Generation",
    "link": "https://arxiv.org/pdf/2305.18259.pdf",
    "upvote": "1",
    "text": "GlyphControl: Glyph Conditional Control for\nVisual Text Generation\nYukang Yang1\u266e\u2020\nDongnan Gui2\u2020\u266e\nYuhui Yuan3\u2020\u2021\nWeicong Liang3\u266e\nHaisong Ding3\nHan Hu3\nKai Chen3\n1Princeton University\n2University of Science and Technology of China\n3Microsoft Research Asia\n\u00a7 https://github.com/AIGText/GlyphControl-release\nNewspaper with the headline\n\"Aliens Found in Space\" and\n\"Monster Attacks Mars\".\nA decorative greeting card that\nreads \"Congratulations on\nachieving state of the art\".\nDslr portrait of a robot holds a\nsign that says \"StrongAI will\nEmpower The World\".\nA menu of a fast food restaurant\nthat contains \"Sandwich Combo\",\n\"French Fries\", and \"Pepsi\".\nA sign in front of a beautiful\nvillage that says \"Bear Infested Be\nCareful\".\nA sign \"OpenSource\" facing\nanother sign \"CloseSource\". They\npoint to two completely different\npaths.\nFigure 1: Illustrating selected 512 \u00d7 512 GlyphControl samples for different text prompts and glyph conditions.\nOur GlyphControl can generate coherent images with well-formed visual text.\nAbstract\nRecently, there has been an increasing interest in developing diffusion-based text-\nto-image generative models capable of generating coherent and well-formed visual\ntext. In this paper, we propose a novel and efficient approach called GlyphControl\nto address this task. Unlike existing methods that rely on character-aware text\n\u2020Core contribution. \u266e Interns at Microsoft Research Asia\n\u2021Corresponding author: yuhui.yuan@microsoft.com\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18259v2  [cs.CV]  11 Nov 2023\nencoders like ByT5 and require retraining of text-to-image models, our approach\nleverages additional glyph conditional information to enhance the performance\nof the off-the-shelf Stable-Diffusion model in generating accurate visual text. By\nincorporating glyph instructions, users can customize the content, location, and size\nof the generated text according to their specific requirements. To facilitate further\nresearch in visual text generation, we construct a training benchmark dataset called\nLAION-Glyph. We evaluate the effectiveness of our approach by measuring OCR-\nbased metrics, CLIP score, and FID of the generated visual text. Our empirical\nevaluations demonstrate that GlyphControl outperforms the recent DeepFloyd IF\napproach in terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy\nof our method.\n1\nIntroduction\nDenoising diffusion probabilistic models [34, 5, 35, 29, 31, 30, 16] have significantly boosted the\ndevelopment of general text-to-image generation by showing the capability of generating surprisingly\nhigh-quality images over the past few years. Although currently plenty of diffusion-based text-to-\nimage generation methods could produce abundant fantastic and photo-realistic images, most existing\nmethods still lack the ability to produce legible and readable text in generated images [29, 30] due to\nthe complex and fine-grained structure within the visual text.\nSeveral very recent efforts have made preliminary attempts to address the task of visual text generation.\nMotivated by the inspiring analysis in unCLIP [29], the spelling information inside the prompts\ncan not be accurately modeled with the raw CLIP text embedding, the follow-up efforts including\neDiff-I [3] and Imagen [31] attempt to leverage the potential of large language models such as T5 [28],\nwhich is trained on the text-only corpus, as the text encoder in image generation. With the strength\nof T5 embedding on encoding individual objects within the prompts [3], eDiff-I produces more\naccurate visual text. The very recent DeepFloyd IF model further follows the design of Imagen and\ndemonstrates impressive performance in rendering legible text. Besides, [20] found that the text\nencoders (both CLIP and T5) used in most existing mainstream text-to-image generation models lack\nsufficient character-level information for spelling due to the usage of BPE tokenizer, thus they verify\nthat adopting the character-aware model ByT5 [36] instead could bring significant improvements.\nDespite efforts to modify text encoders used in generation models, layout errors such as missing\nor merged glyphs still exist in generated images [20], implying that merely relying on textual input\nprompts would not be sufficient for accurate visual text rendering. To address this problem, we\npropose to incorporate text glyph information into the off-the-shelf powerful text-to-image generation\nmodels for visual text generation. We formulate the task of visual text generation as a glyph-\nconditional control problem. Specifically, we propose to control the visual text generation with an\nadditional glyph image.3 The glyph image acts as an explicit spatial layout prior to enforcing the\ndiffusion models generating coherent and well-formed visual text. We show some qualitative results\nin Figure 1 to show that our method is capable of generating diverse images with well-formed visual\ntext.\nIn our implementation, we introduce two key innovations including (i) a GlyphControl framework\nthat can augment the off-the-shelf text-to-image generation model by exploiting the shape information\nencoded in the glyph image with a ControlNet branch, and (ii) a LAION-Glyph benchmark that\nconsists of \u223c 10 M text-image pairs augmented with additional OCR detection results that record the\npresented text information. We further create two evaluation benchmarks, including SimpleBench\nand CreativeBench, to assess the performance of our method and the other strong methods such as\nDeepFloyd IF. To demonstrate the effectiveness of our approach, we conduct thorough experiments\nand show that our approach consistently achieves much higher OCR accuracy than the DeepFloyd\nIF. For example, on SimpleBench and CreativeBench, our approach gains +15% (48% vs. 33%) and\n+13% (34% vs. 21%) than the very recent powerful DeepFloyd (IF-I-XL) while only requiring less\nthan 22% parameters. We summarize our main contributions as follows:\n3The glyph image is a whiteboard image where the characters are rendered with a single particular font while\nkeeping the same content, position, and size as the realistic visual text.\n2\n\u2022 We propose a glyph-conditional text-to-image generation model named GlyphControl for\nvisual text generation, which outperforms DeepFloyd IF, SDXL, and Stable Diffusion in\nterms of OCR accuracy, CLIP score, and FID while saving the number of parameters by\nmore than 3\u00d7 compared to DeepFloyd IF-I-XL.\n\u2022 We introduce a visual text generation benchmark named LAION-Glyph by filtering the\nLAION-2B-en and selecting the images with rich visual text content by using the modern\nOCR system. We conduct experiments on three different dataset scales: LAION-Glyph-\n100K, LAION-Glyph-1M, and LAION-Glyph-10M.\n\u2022 We report flexible and customized visual text generation results. We empirically show that\nthe users can control the content, locations, and sizes of generated visual text through the\ninterface of glyph instructions.\n2\nRelated Work\nText-to-image Diffusion Models.\nDenoising Diffusion Probabilistic Model [11] and its successors\n[24, 29, 3, 30, 31, 27] have demonstrated impressive performance on high-quality image synthesis\nwith text prompts. GLIDE [24] emphasizes the necessity of the classifier-free guidance over CLIP\nguidance and the usage of cascaded diffusion models [12, 31, 3, 29] for high-fidelity, high-resolution\ngeneration. Imagen [31] introduces generic large language models (T5-XXL text encoder) into the\ntext-to-image generation while demonstrating comparable or superior image quality to the CLIP text\nencoder. Moreover, eDiff-I [3] concatenates the CLIP text embeddings and T5 text embeddings to\nbenefit from the strengths of both two text encoders. Unlike the aforementioned pixel-level diffusion\nmodels, Latent Diffusion [30] transforms the image into latent features and applies the diffusion\nmodel in the latent space to decrease training and inference costs. Stable Diffusion is an application\nof the Latent Diffusion method in text-to-image generation but trained with additional data and a\npowerful CLIP text encoder. SDXL[27] improves Stable Diffusion by using 3\u00d7 larger U-Net and a\nsecond text encoder and also introducing another refinement model to further enhance image quality\nthrough image-to-image techniques. In this work, we adopt Stable Diffusion as the base model.\nControllable Image Generation.\nTo achieve more customized image synthesis, users could apply\nadditional conditions, such as segmentation maps or depth maps [30], onto diffusion models. Beyond\nthis intuitive approach, multiple diffusion-based methods of image editing [22, 15, 25, 8] demonstrate\npromising performance in controlling the content of synthesized images. Recently, more related works\n[37, 23, 14] have focused on flexible and composable control of image synthesis. Composer [14]\ndecomposes the image generation into multiple factors and generates images by re-combining\nthem. Both T2IAdapter [23] and ControlNet [37] can incorporate different conditional maps, such\nas segmentation maps or depth maps, as additional data into the pre-trained diffusion models,\ndemonstrating accurate structure or color control without dampening the generation ability of the\noriginal models. Considering the fact that the glyphs of visual text essentially belong to geometric\nstructures, we adopt ControlNet as the basic framework to generate visual text by controlling the\nlocal structure with additional glyphs.\nVisual Text Generation.\nAlthough diffusion models could generate high-fidelity images, current\nmainstream text-to-image generation models such as unCLIP [29] and Stable Diffusion have trouble\nrendering legible and readable text onto images. Several previous works [9, 10] demonstrate that\ndiffusion models have the ability to generate visual text with different fonts but do not extend to\ngeneral image generation. Due to the findings that CLIP embedding could not precisely perceive\nthe spelling information in the input prompts [29, 3], both Imagen [31] and eDiff-I [3] utilize the\nlarge language model T5 [28] to achieve superior visual text generation. The recent open-sourced\nimage generation model DeepFloyd IF [16], inspired by Imagen, takes the T5-XXL as the text\nencoder as well demonstrating impressive performance in visual text generation. Furthermore, [20]\nthoroughly exploits the strengths of character-aware language models like ByT5 [36] over character-\nblind counterparts such as mainly used CLIP and T5. With the usage of ByT5 in the generation, the\nsemantic errors of rendered text decrease while the errors related to the layout of glyphs still exist,\nimplying that the auxiliary information about glyph images would be necessary. The very recent\nDALL\u00b7E 3 [26, 4] not only excels in rendering precise visual text but also showcases exceptional\ntypography quality. GlyphDraw [21] successfully renders Chinese characters onto images by adding\nglyph images into the input of the diffusion model and also fusing extracted glyph embedding with\n3\nLocked\nCopy\nTrainable\nCopy\nStable Diffusion\nZero-Conv\nZero-Conv\nZero-Conv\nZero-Conv\nControlNet\n(a) GlyphControl.\nU-Net\nEncoder\nVAE\nEncoder\nGlyph\nControlNet\n'the sign for\nscientology is on\nthe side of\na building'\nText\nEncoder\nOCR\nEngine\nDiffusion\nU-Net\nDecoder\nZero-Conv\nGlyph\nRender\n(b) Training pipeline.\nVAE\nDecoder\nU-Net\nEncoder\nGlyph\nControlNet\nU-Net\nDecoder\nConv\nGlyph\nRender\nText\nEncoder\nDDIM steps\n'A modern and\ntechnology  building\nwith a sign that says\n\"SCIENTOLOGY\".'\nText character \ninfo.\nText line info.\nText bounding\nbox info.\nGlyph Instructions\n(c) Inference pipeline.\nFigure 2: Illustrating the framework of GlyphControl. (a) The GlyphControl architecture comprises a pre-\ntrained Stable Diffusion model as a \u201clocked copy\u201d and a randomly initialized ControlNet model as a \u201ctrainable\ncopy.\u201d (b) During the training process, the input image x undergoes encoding with a VAE encoder, resulting\nin a latent embedding z0. The diffusion process is then applied to z0, generating a noised latent embedding\nzt. Additionally, we utilize an OCR engine (PP-OCR [7]) to extract text from images and employ a glyph\nrender to generate a whiteboard image. This image exclusively represents recognized characters as black regions,\nforming the glyph image g. Consequently, both the text embedding (based on text caption c) and the noised\nlatent embedding are fed into the U-Net (locked copy) and the Glyph ControlNet (trainable copy). This enables\nthe estimation of the noise term \u03b5(zt, t), with the crucial step involving passing the glyph image to the Glyph\nControlNet to extract vital glyph information for rendering well-formed text. (c) During inference, our method\nsupports diverse user instructions for customizing the rendering of the glyph image g. Subsequently, we sample a\nnoise latent embedding zT from Gaussian noise and employ the DDIM scheme to perform the denoising process,\nestimating the denoised latent embedding z0. Finally, z0 is sent to the VAE decoder, resulting in the construction\nof the final output image y.\ntext embedding as a condition. Based on similar insights, we utilize glyph images as conditional\nmaps to control image synthesis. Compared to the above methods, we could specify the contents,\nlocations, and sizes of text, which brings more customized and flexible designs.\n3\nApproach\n3.1\nPreliminary\nFigure 3: Illustrating of the generated glyph images based on the glyph render (LAION-Glyph-1M).\nStable Diffusion [30].\nWe have selected the \"stable-diffusion-2-base\" (SD 2.0-base4) as the founda-\ntional model in this work. The Stable Diffusion model is a highly capable and versatile text-to-image\ngenerative model that has been meticulously trained from scratch. The training process of basic\n4https://huggingface.co/stabilityai/stable-diffusion-2-base\n4\nA portrait of a parrot holding a sign with text\n\"Hello World\".\nA storefront with \"GlyphControl\" written on it,\ncentered.\nA hand-painted wooden \"Free Beer\" sign hanging\nout of a bar.\nA fancy violet T-shirt decorated with flowers while\nthe message [X] are written on it.\nFigure 4: Illustrating the qualitative results of GlyphControl in terms of flexible user controllability. The\nwhiteboard images depict the corresponding glyph condition maps alongside the generated images on the right\nside. The \"[X]\" symbol (in the last example) is used as a replacement for the rendered words on the T-shirt.\nmodels involves 550k steps at resolution 256 \u00d7 256, focusing on a subset, with an aesthetic score of\n4.5 or higher, of the LAION-5B dataset. What makes the difference between stable-diffusion-2-base\nand previous versions is that the model is continuously trained on the same dataset with a resolution\nof at least 512 \u00d7 512 pixels, which contributes to the model\u2019s ability to generate more detailed and\nvisually appealing images. The training of stable-diffusion-2-base costs hundreds of hours with\n128\u00d7 A100 GPUs. In this work, by employing the off-the-shelf \"stable-diffusion-2-base\" model and\nrefining it through rigorous training processes, we aim to achieve superior results in the visual text\ngeneration domain.\nControlNet [37].\nThe ControlNet is a powerful network that enhances pre-trained large diffusion\nmodels like Stable Diffusion with additional input conditions. It learns task-specific conditions in an\nend-to-end manner, even with limited training data. The network has a trainable copy and a locked\ncopy of the diffusion model\u2019s weights, enabling it to retain general capabilities while fine-tuning for\nspecific tasks. The ControlNet incorporates \u201czero convolution\u201d layers to connect the trainable and\nlocked components, gradually adapting convolution weights from zeros to optimized parameters.\nThis allows precise control over the model\u2019s behavior in various applications.\n3.2\nGlyphControl\nFramework.\nThe GlyphControl framework consists of several key components: (i) an OCR engine\nfor detecting text information in the given image, (ii) a Glyph render for rendering the detected text\nin a whiteboard image at corresponding locations, (iii) an image VAE encoder that projects the input\nimage into a latent code, and an image VAE decoder that reconstructs an output image based on\nthe latent code, (iv) a text encoder (OpenAI CLIP text encoder) that converts the input text into text\nembedding, (v) a U-Net encoder and decoder that performs the denoising diffusion process, and (vi)\na Glyph ControlNet that encodes the conditional glyph information by processing the glyph image\nrendered by the Glyph render. More details of the GlyphControl framework can be seen in Figure 2.\nFurthermore, Figure 3 showcases some example images of the rendered glyph images.\nTo incorporate glyph information, we introduce the concept of glyph input conditions by rendering\nglyph images and feeding them into the ControlNet branch. Unlike conventional conditions used in\nthe original ControlNet [37], accurate visual text rendering greatly benefits from the use of rendered\nglyph images. We specifically chose the ControlNet architecture for its proficiency in controlling\nprecise geometric structures.\nWith our GlyphControl approach, we can successfully generate legible and readable visual text.\nThis is achieved by utilizing pre-rendered glyph images as input condition maps for the ControlNet,\nallowing us to control the generated glyphs at the layout level. Furthermore, we specify the words in\nthe input text prompts (e.g., \u201cA storefront with \"GlyphControl\" written on it\u201d) and leverage the CLIP\ntext encoder to understand the semantic meaning of the words.\n5\nFigure 5: Statistics on LAION-Glyph-10M. Left: Distribution of character counts in each image. Middle:\nDistribution of word counts in each image. Right: Distribution of detected bounding boxes in each image.\nGlyph Instructions.\nOne major advantage of our GlyphControl approach is its ability to support\ncustomized glyph instructions (i), which enables the specification of various constraints on the\nrendered text in the final output image. Our GlyphControl framework provides support for three types\nof text information customization:\n\u25a0 Text character information: GlyphControl allows for the specification of not only single\nwords but also phrases or sentences composed of multiple words. As long as the text is\nintended to be placed within the same area, users can customize the text accordingly.\n\u25a0 Text line information: GlyphControl provides the flexibility to assign words to multiple\nlines by adjusting the number of rows. This feature enhances the visual effects and allows\nfor more versatile text arrangements.\n\u25a0 Text box information: With GlyphControl, users have control over the font size of the\nrendered text by modifying the width property of the text bounding box. The location of\nthe text on the image can be specified using the coordinates property of the top left corner.\nAdditionally, the yaw rotation angle property of the text box allows for further adjustments.\nBy default, the text is rendered following the optimal width-height ratio, but users can define\na specific width-height ratio to precisely control the height of the text box.\nWe demonstrate the effectiveness of these glyph instructions in Figure 4, where our approach\nsuccessfully generates legible text according to the specified instructions. For instance, in Figure 4,\nwe showcase examples where users can customize the positions of the rendered text, adjust the\nfont size, or place multiple groups of text at different locations to achieve personalized designs.\nAdditionally, users have the option to split the text into multiple rows or rotate the text boxes for\nimproved arrangement. Our controllable text generation approach opens up possibilities for automated\npersonalized art designs in the future. Moreover, in the experimental section, we provide empirical\nevidence showcasing that our method achieves significantly higher OCR accuracy compared to the\nrecent DeepFloyd model.\nImplementation.\nWe utilize the same architecture and initial weights for the VAE and U-Net as\nthe SD 2.0-base model and adopt the classifier-free guidance[13] as the previous works do. Our\ntraining process incorporates PP-OCRv3 [6] as the OCR engine. During inference, users need to\nprovide glyph instructions to generate customized images. For rendering glyphs, we leverage the\ntools available in the ImageDraw module of the Python library Pillow.\n3.3\nLAION-Glyph Benchmark\nOverview.\nThe training process of Stable Diffusion and DeepFloyd has greatly benefited from\nthe utilization of the extensive multi-modal dataset LAION-5B [32]. However, there is currently a\nnotable absence of openly available datasets specifically tailored for visual text generation tasks. To\nbridge this gap, we introduce the LAION-Glyph benchmark. To construct this benchmark, we start\nwith LAION-2B-en, which is a subset of LAION-5B [32], and selectively choose specimens that\nexhibit abundant visual text content using the PP-OCR engine.\nPipeline.\nOur data construction process consists of two consecutive steps. In the first step, we\napply an aesthetic score prediction model to filter out images with an aesthetic score higher than 4.5.\nNext, we utilize the PP-OCRv3 [6] engine for text detection and recognition. To ensure the quality\n6\nMethod\n#Params\nText Encoder\nTraining Dataset\nAcc(%)\u2191\n\u02c6\nAcc(%)\u2191\nLD \u2193\nStable Diffusion v2.0\n865M\nCLIP(354M)\nLAION 1.2B\n0/0\n3/2\n4.25/5.01\nSDXL 1.0\n5.8B\nCLIP & OpenCLIP(817M)\nInternal Dataset (>100M)\n0.3/0.5\n13/8\n6.26/6.30\nDeepFloyd (IF-I-M)\n2.1B\nT5-XXL(4.8B)\nLAION 1.2B\n0.3/0.1\n18/11\n2.44/3.86\nDeepFloyd (IF-I-L)\n2.6B\nT5-XXL(4.8B)\nLAION 1.2B\n0.3/0.7\n26/17\n1.97/3.37\nDeepFloyd (IF-I-XL)\n6.0B\nT5-XXL(4.8B)\nLAION 1.2B\n0.6/1\n33/21\n1.63/3.09\nGlyphControl\n1.3B\nCLIP(354M)\nLAION-Glyph-100K\n30/19\n37/24\n1.77/2.58\nGlyphControl\n1.3B\nCLIP(354M)\nLAION-Glyph-1M\n40/26\n45/30\n1.59/2.47\nGlyphControl\n1.3B\nCLIP(354M)\nLAION-Glyph-10M\n42/28\n48/34\n1.43/2.40\nTable 1:\nComparison results of OCR-related metrics with prior methods in the field of visual text gen-\neration is shown in the table. The results are averaged over four word-frequency buckets. The results on\nSimpleBench/CreativeBench are presented on the left/right side of the slash, respectively. It is important to note\nthat the total number of parameters reported in the second column of the table does not include the text encoder.\nThe LAION 1.2B dataset comprises image-text pairs with predicted aesthetic scores of 4.5 or higher in LAION\n5B. All the DeepFloyd models use IF-II-L (1.2B) and Stable \u00d74 as the upscale models to progressively increase\nthe image resolutions from 64 \u00d7 64 to 1024 \u00d7 1024. SDXL generates images with a resolution of 1024 \u00d7 1024.\nof the data, we discard images where all OCR boxes are located at the image border. Additionally,\nwe remove images that have total OCR areas less than 5% of the whole image area or contain more\nthan 5 bounding boxes, as these cases may lead to text recognition or image reconstruction failures.\nTo address inaccuracies in the original captions from the LAION dataset, we generate new captions\nusing the BLIP-2 [17] model. As a result, we have curated a high-quality LAION-Glyph dataset\nconsisting of 10 million images. This dataset includes detailed OCR information and captions that\nare well-formed and accurate.\nStatistics.\nAs illustrated in Figure 5, the character count in the images is primarily concentrated\nwithin the range of 10 to 50 characters, with the majority of samples containing fewer than 150\ncharacters. In terms of word distribution, the most common cases consist of 3 to 5 words, while\ninstances with more than 15 words are relatively rare. Additionally, the number of bounding boxes is\nfairly evenly distributed, although images with only one box are less prevalent. To facilitate training\nand evaluation, we partitioned the LAION-Glyph dataset into three scales: LAION-Glyph-100K,\nLAION-Glyph-1M, and LAION-Glyph-10M, using a random division approach.\n4\nExperiment\n4.1\nTraining Details\nWe train our framework on three different dataset scales: LAION-Glyph-100K, LAION-Glyph-1M,\nand LAION-Glyph-10M for 60\u00d7 epochs, 20\u00d7 epochs, and 6\u00d7 epochs, respectively. The initial\nweights of both the SD branch and Glyph ControlNet branch are copied from the SD 2.0-base model.\nFor both the Glyph ControlNet and Zero-Conv blocks, we set the base learning rate to 1e\u22124. The\nU-Net encoder and decoder are both kept frozen during training. The caption dropping rates for the\nSD branch and Glyph ControlNet branch are set to 0.1 and 0.5, respectively. The input images are\nmaintained at a resolution of 512 \u00d7 512.\n4.2\nEvaluation\nMetrics.\nWe evaluate the effect of visual text generation on OCR accuracy. We measure OCR\nexact match accuracy, denoted as Acc, which assesses the word-level agreement between the OCR\nrecognition results and the ground truth visual text. In other words, it represents the complement of\nthe Word Error Rate (WER), i.e., 1\u2212WER. As the DeepFloyd model tends to generate visual text in\nall capital letters, regardless of the original form of the words in the prompt, we introduce the OCR\ncapitalization-insensitive exact match accuracy\n\u02c6\nAcc. This measure allows for a fairer comparison\nby disregarding the case of the text. Additionally, we incorporate character-level OCR accuracy by\nusing the Levenshtein distance for partial matching evaluation. We report the average Levenshtein\ndistance LD for each word, providing insights into the accuracy at the character level.\nIn addition to the OCR-based metrics mentioned earlier, we evaluate the image-text alignment of the\ngenerated visual text images using the CLIP score, as done in previous works [3, 31]. To assess the\n7\nMethod\nStable Diffusion v2.0SDXL 1.0DeepFloyd (IF-I-M)DeepFloyd (IF-I-L)DeepFloyd (IF-I-XL)GlyphControl-100KGlyphControl-1MGlyphControl-10M\nCLIP Score\u2191\n31.6/33.8\n31.9/33.3\n32.8/34.3\n33.1/34.9\n33.5/35.2\n33.7/36.2\n33.4/36.0\n33.9/36.2\nFID-10K-LAION-Glyph \u2193\n34.03\n44.77\n23.37\n30.97\n26.58\n22.04\n22.19\n22.22\nTable 2: Illustrating the CLIP score and FID comparison results based on the settings described in Table 1. The\naverage results of CLIP scores across four word frequency buckets on both benchmarks are provided.\nquality of generated images, we calculate FID scores using 10K samples, which have not been used\nfor training in our LAION-Glyph dataset.\nBenchmark.\nWe construct two evaluation benchmarks by incorporating prompt templates from\nprevious works on visual text generation [20, 21] and embedding different words selected by us into\nthese templates.\n\u2022 SimpleBench: A simple text prompt benchmark following [20]. The format of prompts\nremains the same: \u2018A sign that says \"<word>\".\u2019\n\u2022 CreativeBench: A creative text prompt benchmark adapted from GlyphDraw [21]. We\nadopt diverse English-version prompts in the original benchmark and replace the words\ninside quotes. As an example, the prompt may look like: \u2018Little panda holding a sign that\nsays \"<word>\".\u2019 or \u2018A photographer wears a t-shirt with the word \"<word>\" printed on it.\u2019\nIn accordance with [20], we collect a pool of single-word candidates from Wikipedia. These words are\nthen categorized into four buckets based on their frequencies: Bucket1k\ntop, Bucket10k\n1k , Bucket100k\n10k ,\nand Bucketplus\n100k. Each bucket contains words with frequencies in the respective range. To form input\nprompts, we randomly select 100 words from each bucket and insert them into the aforementioned\ntemplates. Consequently, we generate four images for each word during the evaluation process.\nInference Details.\nThe scale of classifier-free guidance is set as 9 while we take the empty string\nas the negative prompt. We use the DDIM[35] sampler with 20 sampling steps.\n4.3\nMain Results\nTable 1 presents the comparison of our method with the most representative generative models,\nincluding Stable Diffusion, SDXL, and DeepFloyd. Due to a lack of fully open-sourced codes and\ncheckpoints, we could not conduct fair quantitative comparisons with previous works on visual text\ngeneration [21, 20]. Our method achieves the highest OCR accuracy on both benchmarks compared\nto other methods. Notably, the OCR accuracy of Stable Diffusion is almost zero, indicating that it is\nunable to generate legible text. While for SDXL, an improved version of Stable Diffusion, although\nthe case-insensitive OCR accuracy is improved due to adopting larger text encoders, the overall OCR\nperformance is still poor. In contrast, our framework, with the addition of glyph control, enables the\ndiffusion models to render relatively accurate visual text while maintaining fewer training parameters\nto the original Stable Diffusion model. Compared to multiple versions of the pixel-level diffusion\nmodel DeepFloyd IF with T5 text encoder, our method could achieve better OCR performance\nwith fewer parameters. Additionally, other compared methods tend to generate capital characters\nregardless of the original word form, resulting in much lower performance on case-sensitive metrics\nand reduced flexibility in real-world usage.\nThe comparison among the DeepFloyd IF models demonstrates that increasing the model param-\neters can enhance the accuracy of generated visual text. Similarly, for our method, training on a\nlarger dataset can improve text generation performance. For example, the OCR accuracy\n\u02c6\nAcc on\nSimpleBench increased from 37% to 48% when trained on a larger specialized visual text-related\ndataset. This highlights the importance of leveraging larger datasets specifically focused on visual\ntext in order to achieve further improvements in performance.\nIn addition to the OCR-related evaluation, we also assess the consistency between prompts and\ngenerated images, as shown in Table 2. Our method outperforms other general text-to-image models\nor achieves comparable results in terms of the CLIP scores. This indicates that our model has\nthe capability to accurately render the specified words in the text prompts within the generated\nimages, while still maintaining the fundamental ability to align image and text. While for FID scores\n8\n(a) SDXL\n(b) IF\n(c) Midjourney\n(d) Ideogram\n(e) Ours\nNewspaper with\nthe\nheadline\n\"Aliens\nFound\nin\nSpace\"\nand\n\"Monster\nAttacks Mars\".\nA\ndecorative\ngreeting\ncard\nthat reads \"Con-\ngratulations on\nachieving state\nof the art\".\nDslr portrait of\na robot holds a\nsign that says\n\"StrongAI\nwill\nEmpower\nThe\nWorld\".\nA menu of a fast\nfood restaurant\nthat\ncontains\n\"Sandwich\nCombo\",\n\"French Fries\",\nand \"Pepsi\".\nFigure 6: Qualitative comparison results. The left column presents the text prompt and the other five columns\nshow the images generated by Stable Diffusion XL 1.0 (SDXL), DeepFloyd IF-I-XL (IF), Midjourney, Ideogram\nAI, and our GlyphControl. The results demonstrate that competitive baseline methods exhibit limitations in\ngenerating text, including typos and incomplete text generation.\nevaluated on the LAION-Glyph dataset, our method achieves the lowest FID score compared to other\ntext-to-image models, implying that GlyphControl could generate realistic visual text images with\nhigh quality. Furthermore, we conduct a comparison of the generation performance across different\nbenchmarks and word buckets (see Figure 10).\n4.4\nQualitative Analysis\nThrough visualization of generated images (shown in Figure 8 & 6), we compare our GlyphControl\nwith both competitive open-sourced text-to-image generation methods (SD, SDXL, and IF) and some\nleading billing software (DALL\u00b7E 2[29], Midjourney [2], and Ideogram [1]).\nAs depicted in Figure 8, Stable Diffusion, DALL\u00b7E 2, SDXL, and DeepFloyd IF exhibit various types\nof errors during text rendering, including missing glyphs (the 1st sample), repeated or merged glyphs\n(the 3rd & 6th samples), and misshapen or wrong glyphs (the 5th & 7th samples). In some challenging\ncases (the 2nd & 4th samples), these models even fail to generate any text at all. In contrast, our\nmethod, which incorporates additional glyph images as structure control, could accurately generate\nlegible text that aligns well with the input text prompts by providing appropriate glyph instructions.\nWe also employ other compared methods to generate images with the same text prompts in Figure 1\n(Figure 6). For the cases of rendering longer phrases or sentences (the 2nd & 3rd samples), SDXL, IF,\nand Midjourney fail to generate complete readable visual text while Ideogram, a recently published\nAI-based tool specializing in generating visual text, could generate legible text with fewer errors.\nBy comparison, our method outperforms other methods in terms of the accuracy of text rendering.\nFurthermore, although current top text-to-image tools like Midjourney could generate high-fidelity\nimages, these models including Ideogram still struggle to handle the cases where multiple groups\nof visual text need to be rendered at various locations within the images (the 1st & 4th samples of\nFigure 6). While our method could effectively translate such combinations of visual text.\n4.5\nAblation Experiments\nAblation on Font Size.\nGlyphControl supports users to control the font size of the rendered text by\nmodifying the width property of the text bounding box. We further report the generation results of\n9\nFont Size\nAcc(%)\u2191\n\u02c6\nAcc(%)\u2191\nLD \u2193\nCLIP Score\u2191\nSmall\n5/4\n10/7\n4.85/5.51\n31.7/33.4\nMedium\n30 / 19\n37/24\n1.77 / 2.58\n33.7/36.2\nLarge\n23 / 20\n27/23\n1.94 / 2.37\n33.1/35.7\nTable 3: Ablations on font sizes. The font size of the rendered text refers to the width of the text bounding box.\nWe adopt the model trained on LAION-Glyph-100K for ablations. The results under the \"Medium\" setting are\nexactly the same as those in Table 1 & 2.\nvarious font sizes using GlyphControl (Table 3). For small font sizes, both the OCR accuracy and\nCLIP score of the generated images substantially decrease, which could be attributed to the relatively\nsmaller area of rendered glyphs within the entire image. While for large font sizes, the generation\nperformance becomes worse probably because of the limited number of such training samples\ncontaining large glyphs. In order to attain better generation performance, choosing appropriate font\nsizes or other glyph instructions would be critical.\nAblation on a Large Amount of Small Text.\nWe further study the scenarios of generating\nparagraphs, i.e., numerous groups (OCR boxes) of small text, rather than simple phrases or words.\nAs Figure 9 shows, our method could preserve the global arrangement following glyph instructions,\nbut fail to generate readable small-size text within paragraphs. We also conduct an analysis of other\nfailure cases shown in Appendix C.\n5\nDiscussion and Limitations\nWhile our method offers flexible control over the positions and sizes of rendered text using glyph\ninstructions, it currently lacks the ability to control font style and text color. We only render black\ntext in a single default font style onto whiteboard conditional maps for both training and inference.\nTo enhance our approach, we aim to explore advanced font rendering integration, such as artistic\nfonts, with GlyphControl and investigate modern font style recognizers to extract style information\nfrom training data, incorporating it into text captions during training. For controlling font color,\nincorporating a color spatial palette adapter[23] into the framework may prove effective.\nAs illustrated in the ablation experiments in Section 4.5, our method exhibits sub-optimal performance\nwhen generating a large amount of text with small font sizes. This issue could be attributed to the\nremoval of samples containing more than five OCR boxes during training. In future work, we aim to\nrelax these restrictions on the filtering of the LAION-Glyph dataset, increasing the threshold from\nfive to ten or twenty bounding boxes. It is worth noting that rendering small text presents challenges\nfor VAE-based pixel-level reconstruction due to the text\u2019s relatively diminutive size and the dense\ndistribution of text, which may affect both OCR recognition and glyph rendering. Additionally,\nwe aspire to apply the GlyphControl framework to high-resolution text-to-image methods, such\nas SDXL at 1024 \u00d7 1024, and explore further possibilities for local editing of visual text within\ngenerated images. Last, we observed that BLIP-2 captions often encounter difficulties in consistently\nrepresenting both image content and OCR text information. Investigating more powerful caption\nmodels, such as those presented in [19, 18], is a worthwhile endeavor.\n6\nConclusion\nIn this paper, we introduced the GlyphControl method, a remarkably simple yet highly effective\napproach for generating legible and well-formed visual text. The success of our approach is attributed\nto two key contributions: (i) the employment of the glyph ControlNet, which encodes text shape\ninformation based on rendered glyph images, and (ii) the establishment of the LAION-Glyph\nbenchmark, benefiting from large-scale training data. By integrating the GlyphControl method with\nthe LAION-Glyph benchmark, our approach achieves exceptional performance and consistently\noutperforms recent models such as the DeepFloyd IF in terms of OCR accuracy, FID, and CLIP\nscore. Our work serves as a valuable foundation for future research in developing robust visual text\ngeneration models. We would like to further explore font style and color control, as well as address\nchallenges related to generating abundant small text and improving the quality of captions.\n10\nAcknowledgement.\nWe are grateful to the anonymous reviewers for their invaluable feedback that\ngreatly improved this work. We also thank Bohan Chen, Farzaneh Rajabi, Ji Li, and Chong Luo for\ntheir insightful suggestions post-submission.\nReferences\n[1] Ideogram. https://ideogram.ai, 2023. Accessed: 2023-10-10.\n[2] Midjourney v5.2. https://www.midjourney.com/, 2023. Accessed: 2023-10-10.\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,\nJuntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023.\n[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems, 34:8780\u20138794, 2021.\n[6] Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang,\nQiwen Liu, Xiaoguang Hu, et al. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv\npreprint arXiv:2109.03144, 2021.\n[7] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin\nYu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: A practical ultra lightweight ocr system. arXiv\npreprint arXiv:2009.09941, 2020.\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and\nDaniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. In The Eleventh International Conference on Learning Representations, 2022.\n[9] Dongnan Gui, Kai Chen, Haisong Ding, and Qiang Huo. Zero-shot generation of training\ndata with denoising diffusion probabilistic model for handwritten chinese character recognition.\narXiv preprint arXiv:2305.15660, 2023.\n[10] Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, and Yu Qiao. Diff-\nfont: Diffusion model for robust one-shot font generation. arXiv preprint arXiv:2212.05895,\n2022.\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840\u20136851, 2020.\n[12] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022.\n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications, 2021.\n[14] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Cre-\native and controllable image synthesis with composable conditions. In International Conference\non Machine Learning, 2023.\n[15] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017,\n2023.\n[16] DeepFloyd Lab. Deepfloyd if. https://github.com/deep-floyd/IF, 2023.\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[20] Rosanne Liu, Daniel H Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan\nNarang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. Character-aware\nmodels improve visual text rendering. In Annual Meeting of the Association for Computational\nLinguistics, 2022.\n11\n[21] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin.\nGlyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv\npreprint arXiv:2303.17870, 2023.\n[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\n[24] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In International Conference on Machine\nLearning, pages 16784\u201316804. PMLR, 2022.\n[25] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single\nimage or video. In International Conference on Machine Learning, 2022.\n[26] OpenAI. Dall\u00b7e 3 system card. 2023.\n[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe\nPenna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\n[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\n[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[32] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[33] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset\nfor image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages\n742\u2013758. Springer, 2020.\n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International conference on machine\nlearning, pages 2256\u20132265. PMLR, 2015.\n[35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations, 2020.\n[36] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte\nmodels. Transactions of the Association for Computational Linguistics, 10:291\u2013306, 2022.\n[37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 3836\u20133847, 2023.\n12\nA\nMore Visualized Examples\nWe present additional samples generated by GlyphControl in Figure 7 as the supplementary of\nFigure 1, demonstrating the capability of GlyphControl to generate legible and well-formed visual\ntext in diverse scenarios. Furthermore, Figure 8 provides more qualitative comparison results with\ncompetitive text-to-image models. Additionally, Figure 9 illustrates some GlyphControl-generated\nimages that contain plentiful small text. For a detailed analysis of these results, please refer to Section\n4.4 & 4.5 .\nA photo of two monkeys sitting on\na tree. They are holding a\nwooden board that says \u201cBest\nFriends\u201d, 4K dslr.\nA photo of a teddy bear holding a\nsurfboard with the text\n\"GlyphControl\".\nA photo of a golden retriever\npuppy wearing a green shirt with\ntext that says \"Microsoft\nResearch\". Background office. 4k\ndslr\nA close-up 4k dslr photo of a\ncartoon man riding a scooter\nwith text \"MSRA\". There are\npalm trees in the background.\nA photo of a plate at a restaurant\ntable with Ma Po Tofu and with\ntext \"Ma Po Tofu\" on it.\nphotorealistic, dslr.\nThree people stand in a line and\ntheir backs with text \"5\", \"2\", and\n\"0\".\nFigure 7: More selected samples at 512 \u00d7 512 generated by GlyphControl using different text prompts and\nglyph conditions.\nB\nComparisons across Benchmarks and Word Frequency\nIn Figure 10, we demonstrate the OCR accuracy and CLIP scores based on different test benchmarks\nand word frequency buckets.\nGenerally, all methods exhibit higher OCR accuracy on the SimpleBench compared to the more\nchallenging CreativeBench, which features more diverse prompts. However, CLIP scores tested on\nthe SimpleBench are slightly lower than those on the CreativeBench, which could be attributed to the\nricher descriptions present in the prompts of the latter.\nAdditionally, as shown in in Figure 10, words with high-frequency appearances are generally easier to\nbe rendered onto images compared to rarely-used words. An interesting finding is that low-frequency\nwords consistently exhibit higher CLIP scores. This might be attributed to the CLIP embedding\u2019s\ntendency to overlook low-frequency words in the prompts, potentially leading to overestimation.\n13\n(a) SD\n(b) DALL\u00b7E 2\n(c) SDXL\n(d) IF\n(e) Ours\nA\nblue\nbottle\nwith \"Just For\nFun\" written on\nit.\nA cloth embroi-\ndered with the\ntext \"laion\" and\nan embroidered\ncute baby lion\nface.\nTwo\ntechnical\nrobots\nwith\nlettering\non\nchest, left with\ntext\n\"Open\",\nright with text\n\"Close\".\nA\nphoto\nof\nevening\nsake\nbar\nnamed\n\"Playing\",\n4K,\ndslr.\nA photo of a cute\nsquirrel holding\na sign that says\n\"Please Protect\nEnvironment\",\n4k, dslr.\nA photo of a\nroad sign with\ntext \"Mystery\" at\nthe beginning of\na mystic forest.\nA photo of a\nmodern\nfood\nstreet with text\n\"China Town\" at\nthe gate.\nA\nphoto\nof\na\nwomen\u2019s\nhandbag\nmade\nof feathers with\nthe text \"Hello\nWorld\" engraved\non it.\nA photo of a del-\nicate and gor-\ngeous gold ring\nwith the word\n\"Love\", 4k, dslr.\nFigure 8: Qualitative comparison results among Stable Diffusion v2.0 (SD), DALL\u00b7E 2, Stable Diffusion XL\n1.0 (SDXL), DeepFloyd IF-I-XL (IF), and our GlyphControl.\n14\nA New York Times newspaper.\nA well-layed-out book.\nA well designed menu\ntemplates.\nFigure 9: Examples generated by GlyphControl in the scenario of a large amount of small text and OCR boxes.\nThe results show that our method achieves relatively weak capability in generating abundant legible and readable\nsmall size text although preserving the global arrangement of glyph images.\nFigure 10: Comparison of Stable Diffusion (SD), SDXL, DeepFloyd, and our GlyphControl across different\nbenchmarks and frequency buckets.\nC\nFailure Case Analysis\nWe show some failure cases generated by GlyphControl in Figure 11. The Rendering Overlap\nproblem happens when the locations of text boxes within glyph instructions overlap. Although\nthe text rendering accuracy has improved significantly compared to other text-to-image generation\nmodels (Section 4.3 & 4.4), some \"layout issues\"[20] like Missing Glyphs, Wrong Glyphs, Duplicate\nGlyphs, Unexpected Text, and Illegible Text still occur, implying the necessity of improvement of the\ngenerative model architecture or conditional control scheme. Bad performance in the scenarios of\nExcessive Yaws and Small Text may be attributed to a lack of corresponding training samples.\n15\navatar the last airbender\nthe promise\nhealth and wellness\narticles, recipes, and tips\na for sale by owner sign is\ndisplayed in front of a\nhouse\nirish you would be a drink\nkeychain\nRendering Overlap\nIllegible Text\nExcessive Yaws\nMissing Glyphs\nbruce & co limited logo\nnate certified technician\nexcellence\ni will create 500 backlinks\nfor your website\npool party gift set with a\nbottle opener, bottle\nopener, and a bottle\nopener\nWrong Glyphs\nUnexpected Text\nSmall Text\nDuplicate Glyphs\nFigure 11: Failure cases generated by GlyphControl using captions and glyph instructions in the LAION-Glyph\ntest dataset. We demonstrate 8 types of errors here. Captions and glyph images are also shown here. And we\nalso mark out the places where the errors occur with red rectangles on the corresponding glyph images.\nTraining Dataset\nU-Net Decoder\nAcc(%)\u2191\n\u02c6\nAcc(%)\u2191\nLD \u2193\nCLIP Score\u2191\nTextCaps 5K\nfrozen\n48/39\n55/43\n1.21/1.73\n33.8/36.3\nfine-tuning\n61/43\n68/49\n0.76/1.38\n34.2/36.3\n(a) Illustrating the effect of unlocking the U-Net decoder: we conducted two experiments, in both cases, the\nmodels were fine-tuned for an additional 40 epochs using the checkpoint trained on LAION-Glyph-100K as the\nstarting point (shown in Table 1). During the fine-tuning process, the learning rate of the decoder was set to 1e-4.\nTraining Dataset\nTraining Epochs\nAcc(%)\u2191\n\u02c6\nAcc(%)\u2191\nLD \u2193\nCLIP Score\u2191\nTextCaps 5K\npre-trained\n30/19\n37/24\n1.77/2.58\n33.7/36.2\n10\n48/28\n56/34\n1.31/2.32\n33.8/35.5\n20\n50/34\n61/41\n1.03/2.01\n34.3/35.7\n40\n61/43\n68/49\n0.76/1.38\n34.2/36.3\n(b) The comparison between different training epochs. The U-Net decoder is also fine-tuned.\nTable 4: Ablation experiments on TextCaps 5K. We report the average results on two benchmarks.\nD\nMore Ablation Studies\nTo assess the generalization capability of our approach on different training datasets, we curate a\nspecialized OCR-related training dataset called TextCaps 5K. This dataset consists of images related\nto signs, books, and posters, which are extracted from the training set of the TextCaps v0.1 Dataset\n[33]. The original purpose of this dataset was image captioning with an understanding of visual text\ncontent. We fine-tune our model on the TextCaps 5K dataset for additional 40 epochs, using the same\ntraining settings as those applied to the model trained on LAION-Glyph-100K (as shown in Table 1).\nThis fine-tuning process aims to evaluate how well our model performs on a different training dataset\nand further validate its robustness and adaptability.\nThrough our experiments, we discover that unlocking the frozen U-Net decoder in the original Stable\nDiffusion model can significantly enhance the OCR accuracy of visual text rendering, as demonstrated\nin Table 4a. This improvement can be attributed to the decoder\u2019s improved adaptation to the smaller\nTextCaps 5K dataset during training. As training progresses, the accuracy of text generation gradually\nimproves, as shown in Table 4b. However, it is worth noting that the generated images tend to\nresemble the samples in TextCaps 5K.\n16\n(a) pre-trained\n(b) 10\u00d7 epochs\n(c) 20\u00d7 epochs\n(d) 40\u00d7 epochs\nA\nlogo\nfor\nthe company\n\"Plant\",\nwhere\nthe\nlogo is sur-\nrounded\nby\nflowers\nand\nbutterflies.\nAn\nantique\nbottle labeled\n\"Energy\nPassion\"\nFigure 12: Illustrating the visual text images generated using the models fine-tuned on TextCaps 5K. The\nmodels were trained with the same settings following Table 4b.\nAs depicted in Figure 12, the visual text regions in the center of the generated images appear more like\nconventional signs (Figure 12d), without seamlessly merging with the background, while the images\ngenerated by the model pre-trained on LAION-Glyph-100K exhibit greater creativity and diversity\n(Figure 12a). Therefore, in order to generate photo-realistic images that accurately render visual\ntext, it is essential to not only have a training dataset with a large number of high-quality samples\ncontaining abundant visual text content, but also to include more realistic images with diverse scenes\nand creative elements.\n17\n"
  },
  {
    "title": "Model Dementia: Generated Data Makes Models Forget",
    "link": "https://arxiv.org/pdf/2305.17493.pdf",
    "upvote": "1",
    "text": "THE CURSE OF RECURSION:\nTRAINING ON GENERATED DATA MAKES MODELS FORGET\nIlia Shumailov*\nUniversity of Oxford\nZakhar Shumaylov*\nUniversity of Cambridge\nYiren Zhao\nImperial College London\nYarin Gal\nUniversity of Oxford\nNicolas Papernot\nUniversity of Toronto & Vector Institute\nRoss Anderson\nUniversity of Cambridge & University of Edinburgh\nABSTRACT\nStable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4\ndemonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such\nlanguage models to the general public. It is now clear that large language models (LLMs) are here to\nstay, and will bring about drastic change in the whole ecosystem of online text and images. In this\npaper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute\nmuch of the language found online? We find that use of model-generated content in training causes\nirreversible defects in the resulting models, where tails of the original content distribution disappear.\nWe refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders,\nGaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and\nportray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken\nseriously if we are to sustain the benefits of training from large-scale data scraped from the web.\nIndeed, the value of data collected about genuine human interactions with systems will be increasingly\nvaluable in the presence of content generated by LLMs in data crawled from the Internet.\n1\nIntroduction\nA lot of human communication happens online. Billions of emails are exchanged daily, along with billions of social-\nmedia messages and millions of news articles. Almost all of this material was produced and curated only by humans in\nthe early years of the worldwide web, yet since the turn of the century search engines have come to determine what\npeople can find, and in the past decade smart text editors with spelling and grammar correction have helped tweak what\nwe produce. Now, text can not only be groomed and analysed efficiently; it can also be generated \u2013 by large language\nmodels (LLMs). These models now (arguably) pass a weaker form of the Turing test in the sense that their output\ncannot be reliably distinguished from text written by humans [Solaiman et al., 2019].\nThe development of LLMs is quite involved and requires masses of training data. Anecdotally, some powerful recent\nmodels are trained using scrapes of much of the Internet, then further fine-tuned with reinforcement learning from\nhuman feedback (RLHF) [Griffith et al., 2013, OpenAI, 2023]. This further boosts the effective dataset size. Yet while\ncurrent LLMs [Devlin et al., 2018, Liu et al., 2019, Brown et al., 2020, Zhang et al., 2022], including GPT-4, were\ntrained on predominantly human-generated text, this may change in the future. If most future models\u2019 training data\nis also scraped from the web, then they will inevitably come to train on data produced by their predecessors. In this\npaper, we investigate what happens when text produced, e.g. by a version of GPT, forms most of the training dataset of\nfollowing models. What happens to GPT versions GPT-{n} as generation n increases?2\n1The name is inspired by the Generative Adversarial Networks (GAN) literature on mode collapse, where GANs start producing\na limited set of outputs that all trick the discriminator. Model Collapse is a process whereby models eventually converge to a state\nsimilar to that of a GAN Mode Collapse. The original version of this paper referred to this effect as \u2018model dementia\u2019, but we decided\nto change this following feedback that it trivialised the medical notion of \u2018dementia\u2019 and could cause offence.\n2This is not limited to text models; one can also consider what happens when music created by human composers and played by\nhuman musicians trains models whose output trains other models.\narXiv:2305.17493v2  [cs.LG]  31 May 2023\nModel Collapse\nWe discover that learning from data produced by other models causes model collapse \u2013 a degenerative process whereby,\nover time, models forget the true underlying data distribution, even in the absence of a shift in the distribution over time.\nWe give examples of model collapse for Gaussian Mixture Models (GMMs), Variational Autoencoders (VAE) and\nLarge Language models (LLMs). We show that over time we start losing information about the true distribution, which\nfirst starts with tails disappearing, and over the generations learned behaviours start converging to a point estimate with\nvery small variance. Furthermore, we show that this process is inevitable, even for cases with almost ideal conditions\nfor long-term learning i.e. no function estimation error.\nFigure 1: Model Collapse refers to a degenerative learning\nprocess where models start forgetting improbable events\nover time, as the model becomes poisoned with its own\nprojection of reality.\nFinally, we discuss the broader implications of model\ncollapse. We note that access to the original data dis-\ntribution is crucial: in learning where the tails of the\nunderlying distribution matter, one needs access to real\nhuman-produced data. In other words, the use of LLMs\nat scale to publish content on the Internet will pollute\nthe collection of data to train them: data about human\ninteractions with LLMs will be increasingly valuable.\nThis paper is structured as follows. First, in Sections 3\nand 4 we describe the reasons why model collapse hap-\npens. To best describe the intuition, we present a simple\nexample of a single-dimensional Gaussian where errors\ndue to sampling inevitably cause model collapse, which\nare then extended to a multidimensional generative model\nunder some assumptions. Under both models, similar\nlower bounds are derived on the risk, defined in terms of\nthe Wasserstein distance from the true distribution. Next,\nwe turn to GMMs and VAEs to show that additional\nfunctional approximation errors further exacerbate model\ncollapse. Finally, we discuss the most commonly used\nsetting of fine-tuned language models, where we report\nthat only early signs of model collapse can be detected, if models are fine-tuned as opposed to trained from scratch.\nIn this paper we make the following contributions:\n\u2022 We demonstrate the existence of a degenerative process in learning and name it model collapse;\n\u2022 We demonstrate that model collapse exists in a variety of different model types and datasets;\n\u2022 We show that, to avoid model collapse, access to genuine human-generated content is essential.\n2\nRelated work\nIn this section we are going to cover two closest concepts to model collapse from existing literature: catastrophic\nforgetting and data poisoning. Neither is able to explain the phenomenon of model collapse fully, as the setting is\nfundamentally different, but they provide another perspective on the observed phenomenon.\n2.1\nContinual learning and catastrophic forgetting\nUnlike traditional machine learning which seeks to learn from a static data distribution, continual learning attempts\nto learn from a dynamic one, where data are supplied in a sequential fashion [Van de Ven and Tolias, 2019]. This\ntends to be task-based, where data are provided with delineated task boundaries; e.g., classifying dogs from cats and\nrecognising handwritten digits. Our work is more similar to task-free continual learning [Aljundi et al., 2019] where\ndata distributions gradually change without the notion of separate tasks. Our work examines a particular scenario in\nwhich the changed data distributions arise from the model itself, as a result of training in the previous iteration.\n2\nModel Collapse\nExample of text outputs of an OPT-125m model affected by Model Collapse\u2013 models degrade over generations,\nwhere each new generation is trained on data produced by the previous generation.\nInput: some started before 1360 \u2014 was typically accomplished by a master mason and a small team of\nitinerant masons, supplemented by local parish labourers, according to Poyntz Wright. But other\nauthors reject this model, suggesting instead that leading architects designed the parish church towers\nbased on early examples of Perpendicular\nOutputs:\nGen 0: Revival architecture such as St. John\u2019s Cathedral in London. The earliest surviving example of\nPerpendicular Revival architecture is found in the 18th @-@ century Church of Our Lady of Guernsey,\nwhich dates from the late 19th century. There are two types of perpendicular churches : those\nGen 1: architecture such as St. Peter\u2019s Basilica in Rome or St. Peter\u2019s Basilica in Buenos Aires. There is no\nevidence that any of these buildings were built during the reign of Pope Innocent III, but it is possible\nthat they may have been built during the reign of his successor, Pope Innocent\nGen 7: architecture in England. In an interview with The New York Times, Wright said : \" I don \u2019t think\nthere is anything wrong with me being able to do what I want to do. It just doesn \u2019t work for me. \" He\nadded : \" I don \u2019t know if you can call it funny,\nGen 9: architecture. In addition to being home to some of the world\u2019s largest populations of black @-@ tailed\njackrabbits, white @-@ tailed jackrabbits, blue @-@ tailed jackrabbits, red @-@ tailed jackrabbits,\nyellow @-\nA typical challenge in continual learning is that the model forgets previous samples when learning new information; this\nis known as catastrophic forgetting [Kirkpatrick et al., 2017]. A typical way of preventing it is to use regularisations\n(Memory Aware Synpass [Aljundi et al., 2018]) or just rely on data (e.g. Learning without Forgetting [Li and Hoiem,\n2017]). This has an indirect connection to our work, yet differs since the data in the process of model collapse are\ngenerated by different generations of models.\n2.2\nData poisoning\nPoisoning attacks are crafted and inserted during training in order to degrade the model\u2019s performance when de-\nployed [Biggio et al., 2012]. Malicious data can be inserted into training data to induce unintended behaviors that can\nbe activated by special triggers [Gu et al., 2017]. The early literature on data poisoning focused mainly on supervised\nlearning, where classifiers are trained with labeled samples. But with the emergence of contrastive learning [Radford\net al., 2021] and LLMs [Brown et al., 2020], more recent models are trained with large-scale web crawls, making data\npoisoning attacks more feasible on these untrustworthy web sources. Recent studies have demonstrated that web-scale\ndatasets can be poisoned by introducing malicious data into a small percentage of samples [Carlini and Terzis, 2021,\nCarlini et al., 2023].\n3\nWhat is Model Collapse?\nDefinition 3.1 (Model Collapse). Model Collapse is a degenerative process affecting generations of learned generative\nmodels, where generated data end up polluting the training set of the next generation of models; being trained on\npolluted data, they then mis-perceive reality. We separate two special cases: early model collapse and late model\ncollapse. In early model collapse the model begins losing information about the tails of the distribution; in the late model\ncollapse model entangles different modes of the original distributions and converges to a distribution that carries little\nresemblance to the original one, often with very small variance.\nNote that this process is different from the process of catastrophic forgetting in that we are considering multiple models\nover time, in which our models do not forget previously learned data, but rather start misinterpreting what they believe\nto be real, by reinforcing their own beliefs.\nThis process occurs due to two specific sources of error compounding over generations and causing deviation from the\noriginal model. Of these, one source of error plays a primary role, and in the absence of it, the process would not occur\nbeyond the first generation.\n3\nModel Collapse\nFigure 2: The high-level description of the feedback mechanism in the learning process. Here, data are assumed to be\nhuman-curated and start off clean; then model 0 is trained and data are sampled from it; at step n, data are added to the\noverall data from step n \u2212 1, and this ensemble is used to train model n. Data obtained with Monte Carlo sampling\nshould ideally be statistically close to the original, provided fitting and sampling procedures are perfect. This process\ndepicts what happens in real life with the Internet \u2013 model-generated data become pervasive.\n3.1\nCauses of model collapse\nThere are two main causes for model collapse, one primary and one secondary, which we describe now. Further\nmathematical intuition is provided in Section 4 to explain how these give rise to the errors observed, how different\nsources can compound and how we can quantify the average model divergence rate.\n\u2022 Statistical approximation error \u2013 this is the primary type of error, which arises due to the number of samples\nbeing finite, and disappears as the number of samples tends to infinity. This occurs due to a non-zero probability\nthat information can get lost at every step of re-sampling. Figure 12 shows an example of an approximation\nerror. Here, a single-dimensional Gaussian is being approximated from a finite number of samples. Despite\nusing a very large number of points, the errors remain significant; with 107 samples we estimate the mean to\nbe 0.00024899 \u00b1 1.89382984e\u22124, when the true value is 0.\n\u2022 Functional approximation error \u2013 this is a secondary type of error, which stems from our function approx-\nimators being insufficiently expressive (or sometimes too expressive outside of the original distribution\nsupport [Nguyen et al., 2015]). It is well known that neural networks are universal functional approximators\nin the limit, but in practice this is not always true. In particular, a neural network can introduce non-zero\nlikelihood outside of the support of the original distribution. A simple example of this error is if we were to try\nfitting a mixture of two Gaussians with a single Gaussian. Even if we have perfect information about the data\ndistribution, model errors will be inevitable. It is important to also note that in the absence of statistical error,\nfunctional approximation error only occurs at the first generation. Once the new distribution belongs to the\nimage of functional approximator, it remains exactly the same over the generations.\nEach of the above can cause model collapse to get worse or better. Better approximation power can even be a double-\nedged sword \u2013 better expressiveness may counteract statistical noise, resulting in a good approximation of the true\ndistribution, but it can equally compound this noise. More often then not, we get a cascading effect where combined\nindividual inaccuracy causes the overall error to grow. Overfitting the density model will cause the model to extrapolate\nincorrectly and might give high density to low-density regions not covered in the training set support; these will then be\nsampled with arbitrary frequency.\nIt is worth mentioning that modern computers also have a further computational error coming from the way floating\npoint numbers are represented. This error is not evenly spread across different floating point ranges, making it hard to\nestimate the precise value of a given number. Such errors are smaller in magnitude and are fixable with more precise\nhardware, making them less influential on model collapse.\n4\nModel Collapse\n4\nTheoretical intuition\nIn this section we aim to provide a theoretical intuition for the phenomenon of model collapse. We argue that the process\nof model collapse is universal among generative models that recursively train on data generated by previous generations.\nWe construct toy mathematical models, which prove to be simple enough to provide analytical expressions for quantities\nof interest, but also portray the phenomenon of model collapse. We aim to quantify how different sources of error can\naffect the overall end approximation of the original distribution. As discussed in Section 3.1, there are two main sources\nwe are interested in \u2013 statistical error and functional error. Since in the real world one rarely has infinite samples,\nquantifying the functional approximation error alone is of little interest for discussion of model collapse. Therefore, we\nwill examine two simple cases: a discrete distribution in the absence of functional approximation error and a single\ndimensional Gaussian case, which portrays how functional approximation error can compound with statistical error.\nThe overall stochastic process we are going to be considering (which we call Learning with Generational Data) is\nthe following. Assume that at generation i we have a dataset Di comprising of i.i.d. random variables Xi\nj, where\nj \u2208 {1, . . . , Mi} denotes the sample number at generation i and Mi \u2265 2. We will denote the distribution of Xi as pi.\nHere we assume that p0 denotes the original distribution, from which the data comes from. Going from generation i\nto generation i + 1, we aim to estimate the distribution of samples in Di, with an approximation p\u03b8i+1. This step is\nwhat we refer to as functional approximation F\u03b8 : pi \u2192 p\u03b8i+1. We then resample the dataset Di+1 from the distribution\npi+1 = \u03b1ip\u03b8i+1 + \u03b2ipi + \u03b3ip0, with non-negative parameters \u03b1i, \u03b2i, \u03b3i summing up to 1, i.e. they represent proportions\nof data used from different generations. This corresponds to a mixing of data coming from the original distribution (\u03b3i),\ndata used by the previous generation (\u03b2i) and data generated by the new model (\u03b1i). We refer to this as the sampling\nstep. For the mathematical models to come, we consider \u03b1i = \u03b3i = 0 i.e. data only from a single step is used, while\nnumerical experiments are performed on more realistic choices of parameters.\n4.1\nDiscrete distributions with exact approximation\nIn this subsection we consider a discrete probability distribution, which is represented by a histogram, e.g. as shown on\nFigure 3. In what follows we consider the stochastic process in absence of functional approximation error, i.e. F(p) = p.\nIn this case, model collapse arises only due to statistical errors from the sampling step. At first, the tails (low probability\nevents) begin to disappear due to low probability of sampling them, and over time the distribution becomes a delta\nfunction. Denoting the sample size as M, if we consider state i with probability q \u2264\n1\nM , the expected number of\nsamples with value i coming from those events will be less than 1, which means that in practice we will lose information\nabout them. This is portrayed on Figure 3, where infrequent events get cut off. Considering more generally some state\ni with probability q, using standard conditional probability one can show that the probability of losing information\n(i.e. sampling no data at some generation) is equal to 1 \u2212 q. But this in turn means that we must converge to a delta\nfunction positioned at some state, and the probability of ending up at a certain state is equal to the probability of\nsampling said state from the original distribution.\nBut how do we show directly that this process is going to turn our distribution into a delta function? By considering the\nprocess as going from Xi \u2192 F\u03b8 \u2192 pi+1 \u2192 Xi+1, we see that this forms a Markov Chain, as Xi+1 only depends on\nXi. Furthermore, if all the Xi\nj have the same value, then at the next generation the approximated distribution will be\nexactly a delta function, and therefore all of Xi+1\nj\nwill also have the same value. This implies that the Markov chain\ncontains at least one absorbing state, and therefore with probability 1 it will converge to one of the absorbing states.\nThis is a well-known fact, of which a proof is provided in Appendix A.1. For this chain, the only absorbing states are\nthose corresponding to delta functions. As a result, as we follow the progress of model collapse, we are guaranteed\nto end up in a constant state, having lost all the information of the original distribution when the chain is absorbed.3\nBased on the discussion above we see how both early and late stage model collapse must arise in the case of discrete\ndistributions with perfect functional approximation.\n4.2\nSingle dimensional Gaussian\nFollowing the discussion about discrete distributions, we now move on to considering how both functional approximation\nerror and sampling error can compound (or cancel out) the process of model collapse.\nTo demonstrate this, consider a single dimensional Gaussian X0 \u223c N(\u00b5, \u03c32). If we have full faith in the data we\nobserve, the functional approximation involves estimating sample mean and variance and fitting a single dimensional\n3This argument also works in general due to floating point representations being discrete, making the Markov Chain over the\nparameters of the model discrete. Thus as long as the model parameterisation allows for delta functions, we will get to it, as due to\nsampling errors the only possible absorbing states are delta functions.\n5\nModel Collapse\n10\n5\n0\n5\n10\n0\n1\n2\n3\n4\n5\n6\n7\nlog(Count)\nReal distribution 1\n10\n5\n0\n5\n10\n0\n1\n2\n3\n4\n5\n6\n7\nlog(Count)\nReal distribution 2\n10\n5\n0\n5\n10\n0\n1\n2\n3\n4\n5\n6\n7\nResampled 1 and 2\nlog M\nFigure 3: Shown in the middle is a histogram plot of samples from a Gaussian mixture with means (\u22124, 4) and variances\nof 1. To the left of it is a similar distribution, but with \u2019fatter\u2019 tails, and on the right the same histograms are shown, but\nwith low probability events being cut off due to finite resampling. Although distributions 1 and 2 are very different,\nwhen resampled (only assuming the expected behaviour), the tails get cut off, leading to the same observed distribution.\nIn this case this is all states with probability less than 1/M, or equivalently, bins with log Count \u2264 log M.\nGaussian. We can estimate them using the unbiased sample mean and variance estimators:\n\u00b5i+1 =\n1\nMi\nX\nj\nXi\nj;\n\u03c32\ni+1 =\n1\nMi \u2212 1\nX\nj\n(Xi\nj \u2212 \u00b5i+1)2.\n(1)\nNote here, that if we were to use maximum likelihood estimation, we would instead arrive at a biased variance estimator.\nWith these estimates, the functional approximation step simply corresponds to considering a normal distribution with\nthese parameters, which we can sample from:\nXi+1\nj\n|\u00b5i+1, \u03c3i+1 \u223c N(\u00b5i+1, \u03c32\ni+1).\n(2)\nThis provides us with the conditional distribution of Xi\nj, which allows us to calculate the full distribution of Xi\nj. From\nEquation (3), we see that even after the first approximation, the distribution of Xi\nj is no longer normal, it follows a\nvariance-gamma distribution [Fischer et al., 2023]. However, instead of writing the probability density function at each\ngeneration, we can explicitly construct them in terms of independent random variables. In particular, it is well known\n[Cochran, 1934] that \u00b51 and \u03c31 are independent, with \u00b51 \u223c N(\u00b5, \u03c32\nM0 ) and (M0 \u2212 1)\u03c32\n1 \u223c \u03c32\u0393( M0\u22121\n2\n, 1\n2). In what\nfollows we will denote with Z random variables that are distributed with N(0, 1) and with Si random variables that are\ndistributed with\n1\nMi\u22121\u22121\u0393( Mi\u22121\u22121\n2\n, 1\n2).\nX0\nj = \u00b5 + \u03c3Z0\nj ;\nX1\nj = \u00b5 +\n\u03c3\n\u221aM0\nZ1 + \u03c3\n\u221a\nS1Z1\nj ;\n. . .\n(3)\nXn\nj = \u00b5 +\n\u03c3\n\u221aM0\nZ1 +\n\u03c3\n\u221aM1\n\u221a\nS1Z2 + \u00b7 \u00b7 \u00b7 +\n\u03c3\np\nMn\u22121\np\nS1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Sn\u22121Zn + \u03c3\np\nS1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SnZn\nj .\nThese are not joint distributions, as Zn and Sn depend directly on Zn\u22121\nj\n, but when considering Xn\nj on its own the\nformula above provides all the information about the full distribution.\nThe first thing we may try calculating is the variance. It is possible to find its exact value, but the mean and variance of\nthe square root of gamma distribution are expressed in terms of gamma functions, making the result quite clunky. In\nwhat follows, we will expand everything to second order in each of (1/Mi) as we assume each sample size to be large\n(in practice this becomes quite accurate after M \u223c 100). We then find that\n1\n\u03c32 Var(Xn\nj ) =\n1\nM0\n+\n1\nM1\n+ \u00b7 \u00b7 \u00b7 +\n1\nMn\u22121\n+ 1 + O(2).\nAnd if we were to assume that Mi = M are constant, we would find that:\nVar(Xn\nj ) = \u03c32 \u0010\n1 + n\nM\n\u0011\n;\nE(Xn\nj ) = \u00b5.\n6\nModel Collapse\n100\n101\n102\n103\nevolution\n0.0\n0.2\n0.4\n0.6\n0.8\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n(a) Mean estimation\n100\n101\n102\n103\nevolution\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n100\n500\n1000\n10000\n100000\n1000000\n10000000\n(b) Standard Deviation\nFigure 4: Recursive fitting-sampling of a 1D Gaussian with different numbers of samples drawn. We find that unless\nsampled a very large number of times, i.e. <100000, both standard deviation and mean get significantly affected. Here\nwe report a single run; while re-running the experiment changes the initial performance, both \u00b5 and \u03c3 drift over time.\nThe overall graph looks quite similar to that of a Gaussian random walk.\n100\n101\n102\n103\nevolution\n0.0\n0.1\n0.2\n0.3\n0.4\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n(a) Mean estimation\n100\n101\n102\n103\nevolution\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n100\n500\n1000\n10000\n(b) Standard Deviation\nFigure 5: Recursive fitting-sampling of a 1D Gaussian with different numbers of samples drawn. In this plot data get\naccumulated in a pool, from which a fixed sample is drawn. In other words, a model n gets data sampled, its output is\nmixed with data sampled from models 1 . . . n, and then the mix gets sampled to fit the model n + 1. The uncertainty\narising from all of the different modalities appearing in data causes the distribution parameters to jump around quite\nsignificantly.\n100\n101\n102\n103\nevolution\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n(a) Mean estimation\n100\n101\n102\n103\nevolution\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n|\n|\n estimation of a \n(\n= 0,\n= 1)\n100\n500\n1000\n10000\n(b) Standard Deviation\nFigure 6: Recursive fitting-sampling of a 1D Gaussian with different number of samples drawn. In this plot data are\naccumulated in a pool, all of which is used to fit a model. In other words, a model n gets data sampled, its output mixed\nwith data sampled from models 1 . . . n, and then the result is used to fit the model n + 1. Over time the variance in\nestimates reduces due to linear growth of data.\n7\nModel Collapse\nThis means that as n \u2192 \u221e, the variance diverges linearly. This is the same scaling as for a single dimensional Gaussian\nrandom walk. We can further see the similarities in numerical experiments shown on Figure 4 for a range of different\nsample sizes, confirming these theoretical intuitions.\nEven though the variance of Xn\nj diverges, it does not provide us with any information of what the corresponding\nestimates of \u00b5n+1 and \u03c32\nn+1 are, or how far they are from the original \u00b5 and \u03c3. In particular, we may want to consider\nwhat the distance would be between the true distribution and the approximated distribution at step n + 1. To measure\nthis we can consider the Wasserstein-2 distance between two normals:\nRn+1\nW2 := W 2\n2\n\u0000N(\u00b5, \u03c32), N(\u00b5n+1, \u03c32\nn+1)\n\u0001\n= \u2225\u00b5n+1 \u2212 \u00b5\u22252 + \u2225\u03c3n+1 \u2212 \u03c3\u22252\nNow we can calculate the risk that occurs due to finite sampling, i.e. what the expected value of the distance is\n(expanding in 1/Mi):\nE\u00b5n+1,\u03c32\nn+1\n\u0002\nRn+1\nW2\n\u0003\n= \u03c32\n\u0012 1\nM0\n+\n1\nM1\n+ \u00b7 \u00b7 \u00b7 +\n3\n2Mn\n\u0013\n+ O(2),\n(4)\nVar\u00b5n+1,\u03c32\nn+1\n\u0002\nRn+1\nW2\n\u0003\n= \u03c34\n\uf8eb\n\uf8ed 2\nM 2\n0\n+\n2\nM 2\n1\n+ \u00b7 \u00b7 \u00b7 +\n3\nM 2n\n+\nX\ni\u0338=j\n3\nMiMj\n\uf8f6\n\uf8f8 + O(3).\n(5)\nThis result allows us to interpret exactly what occurs in this formulation of model collapse. To be precise, due to errors\noccurring from re-sampling the approximated distribution, each generation ends up corresponding to a new step in a\nrandom walk of model parameters. The risk that occurs in this model ends up diverging for a constant sample size at\neach generation. In order for the end distribution approximation to be accurate, and for the distance to be finite, the\nsampling rate Mi needs to increase superlinearly, i.e. one needs to collect increasingly more samples over time, perhaps\nquadratically. However, even in that case the expected distance after n steps remains non-zero and the only case in\nwhich it does in fact end up being 0 is when sampling is infinite at each step. Overall, this only shows us how far on\naverage we go from the original distribution, but the process can only \u2019terminate\u2019 if the estimated variance at a certain\ngeneration becomes small enough, i.e. we effectively turn into a delta function.\nShown on Figures 5 and 6 are different runs of this process for different values of parameters of \u03b1i, \u03b2i, \u03b3i for different\nsample sizes, which was investigated numerically to see whether they can be enough to overcome model collapse,\nhowever even in those cases the changes are inevitable, although attenuated.\n4.3\nNoisy approximation model\nWith the simple example out of the way, we can now construct a lower bound on the distance of generation n distribution\nfrom the original and show that without superlinear sampling it similarly diverges in the limit of large n. A nice property\nof Wasserstein-2 distance is that Gaussians provide a universal lower bound for the Wasserstein distance [Gelbrich,\n1990]. In particular, for \u03ba and \u03bd probability measures on a Euclidean N-dimensional space with \u00b5\u03ba and \u00b5\u03bd means, \u03a3\u03ba\nand \u03a3\u03bd covariance matrices, we have that\nW 2\n2 (\u03ba, \u03bd) \u2265 \u2225\u00b5\u03ba \u2212 \u00b5\u03bd\u22252 + Tr\n\u0012\n\u03a3\u03ba + \u03a3v \u2212 2\n\u0010\n\u03a31/2\n\u03ba \u03a3v\u03a31/2\n\u03ba\n\u00111/2\u0013\n\u2265 \u2225\u00b5\u03ba \u2212 \u00b5\u03bd\u22252\nWith this, instead of quantifying the distance exactly, we can instead lower bound it. The only limitation is that we are\ngoing to have to specify a functional approximation model. In order to achieve a W2 bound, we will be required to\nspecify how the mean changes between generations. In the scenario where we only have access to the sample mean, we\nwould approximate the mean of the next generation distribution as Equation (1). However, as more information arrives,\nor the model begins using it better, we may end up diverging from the sample mean. We would still require that the\nmodel have good performance, i.e. on average the mean estimate is the same. We will also have to specify expected\nbehaviour of the model over the the variance calculation, which once again will be chosen such that it averages out.\nThus, we will adopt the following evolution over generations:\n\u00b5i+1 =\n1\nMi\nX\nj\nXi\nj + \u03b5i+1 = \u03a31/2\ni\n\u221aMi\nT i+1 + \u00b5i + \u03b5i+1;\nEXi\nj(\u03a3i+1) = \u03a3i\n(6)\nwhere we define T i+1 to satisfy the equation above, i.e. T i+1 = \u03a3\u22121/2\ni\n\u221aMi\nP\nj\n\u0000Xi\nj \u2212 \u00b5i\n\u0001\n. With this normalisation T has\nmean 0 and covariance IN and by the central limit theorem (CLT) we would have T i+1|\u00b5i, \u03a3i\nD\n\u2192 N(0, IN), however\nthe lower bound will not rely on this. To arrive at a lower bound for the risk, similar to that of Equation (4), we are\ngoing to have to make a few assumptions about the form of \u03b5i+1.\nAssumptions:\n8\nModel Collapse\n1. On average we can capture the mean to be the same as at the iteration prior:\nE[\u03b5i+1|\u00b5i, \u03a3i] = 0\n(7)\n2. Given all of Xi\nj, epsilon must be constant, i.e. it is a function of only the data:\n\u03b5i+1 = \u03b5i+1\n\u0000Xi\nj\n\u0001\n(8)\nIn particular, it is dependent on \u00b5i and \u03a3i only through the data.\n3. The extra noise is orthogonal to the sample mean in the sense of random variables. This is effectively assuming\nthat the noise does not contain any first moment information, i.e. we have:\nCov(\u03b5i+1, T i+1|\u00b5i, \u03a3i) = 0\n(9)\nThis may seem like a rather strong assumption, compared to the previous ones, however this property can\nbe shown to hold true when imposing CLT on T i+1 in the limit of large Mi (note here that Mi can only be\nassumed to be large, and not infinite) and assuming that \u03b5 is strictly a function of moments higher than first.\nFurthermore, a property of this type is necessary to actually provide any information, since prior to it there\nwould be no need to separate into an epsilon term and a sample mean term, since all could be absorbed into \u03b5.\nIn Appendix A.2, we further provide an alternative to Assumption 3, wherein by bounding the size of noise we are able\nto recover a similar bound, but only as an expansion in 1/Mi.\nWith all the assumptions in place, we now have the following bound:\nE\n\u0002\nRi+1\nW2\n\u0003\n\u2265 E\n\u0000\u2225\u00b5i+1 \u2212 \u00b5\u22252\u0001\n(10)\n= E\n\u0000\u2225\u00b5i \u2212 \u00b5\u22252\u0001\n+ E\n\u0000\u2225\u03b5i+1\u22252\u0001\n+ 1\nMi\nE\n\u0000(T i+1)\u22a4\u03a3i(T i+1)\n\u0001\n+\n(11)\n+\n2\n\u221aMi\nE\n\u0010\n(\u03b5i+1)\u22a4\u03a31/2\ni\nT i+1 + (\u00b5i \u2212 \u00b5)\u22a4\u03a31/2\ni\nT i+1\u0011\n(12)\n= E\n\u0000\u2225\u00b5i \u2212 \u00b5\u22252\u0001\n+ Tr \u03a3\nMi\n+ E\n\u0000\u2225\u03b5i+1\u22252\u0001\n+\n2\n\u221aMi\nE\n\u0010\n(\u03b5i+1)\u22a4\u03a31/2\ni\nT i+1\u0011\n(13)\nNow the only quantity to evaluate is\n2\n\u221aMi\nE\n\u0010\n(\u03b5i+1)\u22a4\u03a31/2\ni\nT i+1\u0011\n=\n2\n\u221aMi\nZ\nd\u03a3i p(\u03a3i) Tr\nh\n\u03a31/2\ni\nCov(\u03b5i+1, T i+1|\u03a3i)\ni\n= 0,\n(14)\nby Assumption 3. Therefore, the overall bound would be similar to the Gaussian case, but with extra noise variance\nterms:\nE\u00b5n+1,\u03c32\nn+1\n\u0002\nRn+1\nW2\n\u0003\n\u2265 Tr \u03a3\n\u0012 1\nM0\n+\n1\nM1\n+ \u00b7 \u00b7 \u00b7 +\n1\nMn\n\u0013\n+\nn+1\nX\ni=1\nE\n\u0000\u2225\u03b5i\u22252\u0001\n(15)\nAs a result, we have shown that the same superlinear scaling would be required to minimise the lower bound on model\ncollapse even in the case of more generic models of approximation, in which the mean at step i + 1 can be separated\northogonally into the sample mean and \u2019extra\u2019.\nOverall, the message of this section can be summarised as follows:\nWhen learning on generational data, due to finite sampling, we are only able to approximate the original distribution.\nWhile on average we should recover the original distribution, the variance arising from this is non-zero. As a result,\nover the generations, the average distance of n\u2019th generation from the original grows and can become infinite in the\nlimit since errors compound over time.\n5\nEvaluation\n5.1\nTraining from scratch with GMMs and VAEs\nGaussian Mixture Models. In this subsection we evaluate the performance of Gaussian Mixture Models (GMM)\n[Reynolds et al., 2009]. The underlying task here is that a given GMM tries to separate two artificially-generated\nGaussians. Figure 7 shows the progression of the GMM fitting process over time. The left-most plot shows the\noriginal two Gaussians with the ground truth labels. The next plot shows the GMM fitted on the original data with\nno cross-generational data used i.e. \u03b1i = \u03b3i = 0, where the error is minimal. Yet, within 50 iterations of re-sampling\nwe arrive to a point where the underlying distribution is mis-perceived. The performance worsens over time and by\niteration 2000 we arrive at a point estimate of the distribution with very little variance. The L2 distance between the\noriginal GMM and its descendants is plotted in Figure 13.\n9\nModel Collapse\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\nReal Data\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n0\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n50\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n100\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n150\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n200\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n350\n2\n0\n2\n3\n2\n1\n0\n1\n2\n3\n2000\nFigure 7: An examples of GMM fitting data at iterations {0, 50, 100, 150, 200, 350, 2000}. At first the model fits data\nvery well as is shown on the left; yet even at generation 50 the perception of the underlying distribution completely\nchanges. At generation 2000 it converges to a state with very little variance. GMM is sampled a thousand times.\n(a) Original model\n(b) Generation 5\n(c) Generation 10\n(d) Generation 20\nFigure 9: Random latent reconstructions from VAEs. No training data comes from the original distribution. Over the\ngenerations, different modes of the original distribution get entangled and generated data starts looking unimodal.\n3\n2\n1\n0\n1\n2\n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nDensity\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 4\nGeneration 5\nGeneration 6\nGeneration 7\nGeneration 8\nGeneration 9\nFigure 8: Changing distribution of latents over the learning\nprocess with generated data as perceived by the original\nencoder. Just as with the Gaussian case described above,\nthe tails get washed away and the model arrives at the mean\nrepresentation of the underlying data.\nVariational Autoencoders. In this subsection we turn to\nVariational Autoencoders (VAE). As before, we train an\nautoencoder on an original data source, which we later\nsample. Here, we generate latents from a Gaussian dis-\ntribution which are then used by the decoder to generate\ndata for the subsequent generation. Figure 9 on the left\nshows an example of generated data using the setting\ndescribed by Kingma and Welling.\nHaving performed the process a number of times we ar-\nrive at a representation that has very little resemblance of\nthe original classes learned from data. On the right, one\nsees the generated images from generation 20, which ap-\npear to be a mix of all of the different digits. Interestingly,\nthe original encoder perceives the generated data from its\ndescendant with ever-growing confidence \u2013 the encoder\nplaces such data closer and closer to the mean. Figure 8\nshows the density of the latent representation of the orig-\ninal model when presented with data generated by its\ndescendants. As with single-dimensional Gaussians, tails\ndisappear over time and all of the density shifts towards\nthe mean.\n5.2\nLanguage Models\nBy now it is clear that Model Collapse is universal across\ndifferent families of ML models. Yet if small models such as GMMs and VAEs are normally trained from scratch,\nLLMs are different. They are so expensive to retrain from scratch that they are typically initialised with pre-trained\n10\nModel Collapse\nReal\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTrained on dataset from a given generation\n35\n40\n45\n50\n55\n60\nPerplexity \n\u00b1\nReal wikitext2 test dataset\nrun 1\nrun 2\nrun 3\nrun 4\nrun 5\n(a) No data preserved, 5 epochs\nReal\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTrained on dataset from a given generation\n32\n34\n36\n38\n40\n42\nPerplexity \n\u00b1\nReal wikitext2 test dataset\nrun 1\nrun 2\nrun 3\nrun 4\nrun 5\n(b) 10% data preserved, 10 epochs\nFigure 10: Performance of OPT-125m models of different generations evaluated using the original wikitext2 test\ndataset. Perplexity is shown on the y-axis and for each independent run the graph of the mean and its standard deviation\nis shown with error bars. x-axis refers to the generation of the model \u2013 \u2018Real\u2019 refers to the \u2018model 0\u2019 trained on the\noriginal wikitext2 dataset; model 1 was trained on the data produced by model 0; model 2 was trained on data\nproduced by model 1 etc. with all generated datasets equal in size. We find that models trained on generated data are\nable to learn some of the original task, but with errors, as seen from the increase in perplexity.\nmodels such as BERT [Devlin et al., 2018], RoBERTa [Liu et al., 2019], or GPT2 [Brown et al., 2020], which are trained\non large text corpora. They are then fine-tuned to various downstream tasks [Bommasani et al., 2022].\nIn this subsection we explore what happens with language models when they are sequentially fine-tuned with data\ngenerated by other models4. We evaluate the most common setting of training a language model \u2013 a fine-tuning setting\nwhere each of the training cycles starts from a pre-trained model with recent data. Data here comes from another\nfine-tuned pre-trained model. Since training is restricted to produce models that are close to the original pre-trained\nmodel and datapoints generated by the models will generally produce very small gradients, the expectation here may be\nthat the model should only change moderately after fine-tuning. We fine-tune the OPT-125m causal language model\nmade available by Meta through Huggingface [Zhang et al., 2022].\nWe fine-tune the model on the wikitext2 dataset. For data generation from the trained models we use a 5-way\nbeam-search. We block training sequences to be 64 tokens long; then for each token sequence in the training set, we\nask the model to predict the next 64 tokens. We go through all of the original training dataset and produce an artificial\ndataset of the same size. Since we go though all of the original dataset and predict all of the blocks, if the model had\n0.0 error it would produce the original wikitext2 dataset. Training for each of the generations starts with generation\nfrom the original training data. Each experiment is ran 5 times and the results are shown as 5 separate runs. The\noriginal model fine-tuned with real wikitext2 data gets 34 mean perplexity, from the zero-shot baseline of 115, i.e. it\nsuccessfully learns the task. Finally, to be as realistic as possible, we use the best performing model on the original task,\nevaluated using the original wikitext2 validation set, as the base model for the subsequent generations, meaning in\npractice observed Model Collapse can be even more pronounced.\nHere we consider two different settings:\n5 epochs, no original training data \u2013 Here, the model is trained for 5 epochs on the original dataset and no original\ndata. The overall original task performance is presented in Figure 10.(a). We find that training with generated data\nallows one to adapt to the underlying task, losing some performance \u2013 from 20 to 28 perplexity points.\n10 epochs, 10% of original training data preserved \u2013 Here the model is trained for 10 epochs on the original dataset\nand every new generation of training, a random 10% of the original data points are sampled. The overall original\n4One can easily replicate an experiment described in Section 5.1 with a language model to demonstrate model collapse. Given\nthat training a single moderately large model produces twice the American lifetime worth of CO2 [Strubell et al., 2019], we opted\nto not run such an experiment and instead focus on a more realistic setting for a proof-of-concept. Note that just the language\nexperiments described in the paper took weeks to run.\n11\nModel Collapse\n100\n101\n102\nPerplexity of generated datapoints\n0.0\n0.2\n0.4\n0.6\nProbability\nPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9\n(a) No data preserved\n100\n101\n102\nPerplexity of generated datapoints\n0.0\n0.1\n0.2\n0.3\nProbability\nPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9\n(b) 10% data preserved\nFigure 11: Histograms of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model trained with real data is more likely to produce. At the same time, a much longer tail appears for later\ngenerations \u2013 later generations start producing samples that would never be produced by the original model i.e. they\nstart misperceiving reality based on errors introduced by their ancestors. Same plots are shown in 3D in Figure 15.\ntask performance is presented in Figure 10.(b). We find that preservation of the original data allows for better model\nfine-tuning and leads to only minor degradation of performance.\nBoth training regimes lead to degraded performance in our models, yet we do find that learning with generated data\nis possible and models can successfully learn (some of) the underlying task. We now turn to consider the underlying\nperception of probable events for each generation of our models.\nFigure 11 shows histograms of individual datapoint perplexities generated by the models of different generations as\nis evaluated by the first model developed with real wikitext2 training data. Here over the generations models tend\nto produce more sequences that the original model would produce with the higher likelihood. The observed effect is\nsimilar to that described for VAEs and GMMs in Section 5.1, where over the generations models started to produce\nsamples that would be produced with higher probabilities by the original model. At the same time, we discover that\ngenerated data has much longer tails, suggesting that some of the data would never be produced by the original model \u2013\nthese are the errors that accumulate because of the learning with generational data.\nWe find that data generated by language models in our experiments end up containing a large number of repeating\nphrases. The repeating problem has been observed in nearly all text generation models [Keskar et al., 2019, Shumailov\net al., 2021] and to rule this out as the cause of Model Collapse, we further provide numerical experiments when models\nare explicitly encouraged to produce non-repeating sequences with repeating penalty of 2.0. We find that this causes the\nmodels to produce lower score continuations to avoid using repeats, which as a result causes the consequent models\nto perform even worse. Figure 14 show model perplexities shift across the generations towards more probable token\nsequences. In particular, enforcing this for the LLM experiments causes the perplexity to double, compared to the\noriginal. Models remain as susceptible to Model Collapse, if not more.\nThe described process demonstrates that fine-tuning of language models does not curb the effects of Model Collapse and\nmodels that are being fine-tuned are also vulnerable. We find that over the generations models tend to produce more\nprobable sequences from the original data and start introducing their own improbable sequences i.e. errors.\n6\nDiscussion and Conclusion\nWe now discuss the implications of Model Collapse on the underlying learning dynamics of LLMs. Long-term\npoisoning attacks on language models are not new. For example, we saw the creation of click, content, and troll farms \u2013\na form of human \u2018language models\u2019, whose job is to misguide social networks and search algorithms. The negative\neffect these poisoning attacks had on search results led to changes in search algorithms: e.g., Google downgraded\n12\nModel Collapse\nfarmed articles5, putting more emphasis on content produced by trustworthy sources e.g. education domains, while\nDuckDuckGo removed them altogether6.\nWhat is different with the arrival of LLMs is the scale at which such poisoning can happen once it is automated.\nPreserving the ability of LLMs to model low-probability events is essential to the fairness of their predictions:\nsuch events are often relevant to marginalised groups. Low-probability events are also vital to understand complex\nsystems [Taleb, 2007].\nOur evaluation suggests a \u201cfirst mover advantage\u201d when it comes to training models such as LLMs. In our work we\ndemonstrate that training on samples from another generative model can induce a distribution shift, which over time\ncauses Model Collapse. This in turn causes the model to mis-perceive the underlying learning task. To make sure that\nlearning is sustained over a long time period, one needs to make sure that access to the original data source is preserved\nand that additional data not generated by LLMs remain available over time. The need to distinguish data generated\nby LLMs from other data raises questions around the provenance of content that is crawled from the Internet: it is\nunclear how content generated by LLMs can be tracked at scale. One option is community-wide coordination to ensure\nthat different parties involved in LLM creation and deployment share the information needed to resolve questions of\nprovenance. Otherwise, it may become increasingly difficult to train newer versions of LLMs without access to data\nthat was crawled from the Internet prior to the mass adoption of the technology, or direct access to data generated by\nhumans at scale.\nAcknowledgements\nWe want to thank Anvith Thudi, David Glukhov, Peter Zaika, and Darija Barak for useful discussions and feedback.\nReferences\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware\nsynapses: Learning what (not) to forget. In Proceedings of the European conference on computer vision (ECCV),\npages 139\u2013154, 2018.\nRahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11254\u201311263, 2019.\nBattista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint\narXiv:1206.6389, 2012.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo\nCastellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue,\nMoussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan\nJurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh,\nMark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos\nNiebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris\nPiech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan\nWu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang,\nTianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and\nrisks of foundation models, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\nNicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667,\n2021.\n5https://googleblog.blogspot.com/2011/02/finding-more-high-quality-sites-in.html\n6https://www.technologyreview.com/2010/07/26/26327/the-search-engine-backlash-against-content-mills/\n13\nModel Collapse\nNicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson,\nAndreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is practical. arXiv preprint\narXiv:2302.10149, 2023.\nW. G. Cochran.\nThe distribution of quadratic forms in a normal system, with applications to the analysis of\ncovariance.\nMathematical Proceedings of the Cambridge Philosophical Society, 30(2):178\u2013191, 1934.\ndoi:\n10.1017/S0305004100016595.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAdrian Fischer, Robert E. Gaunt, and Andrey Sarantsev. The variance-gamma distribution: A review, 2023.\nMatthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean and hilbert spaces.\nMathematische Nachrichten, 147(1):185\u2013203, 1990.\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping:\nIntegrating human feedback with reinforcement learning. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-\nmani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Cur-\nran Associates, Inc., 2013.\nURL https://proceedings.neurips.cc/paper_files/paper/2013/file/\ne034fb6b66aacc1d48f445ddfb08da98-Paper.pdf.\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning\nmodel supply chain. arXiv preprint arXiv:1708.06733, 2017.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional\ntransformer language model for controllable generation, 2019.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine\nintelligence, 40(12):2935\u20132947, 2017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for\nunrecognizable images, 2015.\nOpenAI. Gpt-4 technical report, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\nInternational conference on machine learning, pages 8748\u20138763. PMLR, 2021.\nDouglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663), 2009.\nIlia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. Sponge examples:\nEnergy-latency attacks on neural networks. In 2021 IEEE European Symposium on Security and Privacy (EuroS&P),\npages 212\u2013231. IEEE, 2021.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen\nKrueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine\nWang. Release strategies and the social impacts of language models, 2019.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp.\narXiv preprint arXiv:1906.02243, 2019.\nNassim Nicholas Taleb. Black swans and the domains of statistics. The American Statistician, 61(3):198\u2013200, 2007.\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734,\n2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,\n2022.\n14\nModel Collapse\nA\nAppendix\nA.1\nAbsorbing Markov Chain\nThe subsection explains a well-known fact about absorbing Markov chains, that they converge to an absorbing state\nwith probability one. Assume that Xm form a Markov chain. In order to reason about this chain we need to consider\nthe transition probabilities. In general, these correspond to our functional approximation scheme. Due to the stochastic\nnature of the Markov chain, we expect to have the variance go up and down. But as the variance decreases, the newly\nsampled data, due to its finiteness, will be more concentrated, leading in the limit to a set of i.e. a delta functions. This\nargument assumes that the approximation scheme is good and can converge to delta functions. If not, the errors in\napproximation may prevent the propagation of errors in stochasticity.\nAs discussed in the previous section, we can model the process of repeated \u2018sampling\u2019 and \u2018fitting\u2019 as a Markov chain.\nIn this subsection, we explain how such a process can converge to a stationary state i.e. the absorbing state of a Markov\nChain. In this derivation we follow Allan Yashinski 7. Suppose we have an absorbing Markov Chain with r transient\nstates t1, . . . , tr and s absorbing states a1, . . . , as. The whole Markov chain has r + s states, ordered as follows:\nt1, . . . , tr, a1, . . . , as. The transition matrix is then defined as\nT =\n\u0014\nQ\n0r\u00d7s\nR\nIs\n\u0015\n,\n(16)\nwhere\n\u2022 Q is an r \u00d7 r matrix holds the probabilities of moving from a transient state to another transient state\n\u2022 R is an s \u00d7 r matrix which holds the probabilities of moving from a transient state to an absorbing state.\n\u2022 0r\u00d7s is the r \u00d7 s matrix of all 0\u2019s. There 0\u2019s represent the probabilities of moving from an absorbing state to a\ntransient state (which is impossible by definition).\n\u2022 Is holds the probabilities of transitioning between the absorbing states. As transition is impossible, this is just\nthe s \u00d7 s identity matrix.\nWe are interested in limk\u2192\u221e T k(X0). For a given k, the matrix becomes\nT k =\n\u0014\nQk\n0r\u00d7s\nR + RQ + \u00b7 \u00b7 \u00b7 + RQk\u22121\nIs\n\u0015\n=\n\u0014\nQk\n0r\u00d7s\nR Pk\u22121\ni=0 Qi\nIs\n\u0015\n.\n(17)\nFinally, for an absorbing Markov chain with T =\n\u0014\nQ\n0r\u00d7s\nR\nIs\n\u0015\n,\nwe have limk\u2192\u221e T k =\n\u0014\n0r\u00d7r\n0r\u00d7s\nR(Ir \u2212 Q)\u22121\nIs\n\u0015\n.\nSince in the limit the transition probabilities to transient states are zero, we end up converging to absorbing states and\nstaying there. In the case of discrete distributions, where we can perfectly approximate a zero-variance dataset (i.e. a\ndelta function), the absorbing states are delta functions centered at any non-zero probability point from the original\ndistribution. In practice, we would like to know the expected number of steps before being absorbed, which may be\nlarge. But without knowing our fitting procedure it is impossible to calculate the matrix Q and therefore the average\nlength of time before collapse.\nA.2\nAlternative assumption for noisy approximations\nThis subsection will cover an alternative assumption, which may be more realistic in some settings, in contrast to\nassumption 3 from Section 4.3, and this subsection mostly acts as an extension, rather than an alternative. In particular,\ninstead of imposing orthogonality, we can instead impose a certain size requirement on the noise term. This in turn\nallows us to arrive to a similar result.\nTo be more precise, we will consider the same setting as in Section 4.3, but we will now replace Assumption 3 with\nAssumption 3*:\n7www.math.umd.edu/~immortal/MATH401/book/ch_absorbing_markov_chains.pdf\n15\nModel Collapse\n101\n102\n103\n104\n105\n106\n107\nlog(number of samples)\n10\n4\n10\n3\n10\n2\n10\n1\nlog(|\n|)\n estimation of a \n(\n= 0,\n= 1)\nFigure 12: Approximation of a single-dimensional Gaussian N(0, 1) as a function of number of points. The mean\nestimator and its standard deviation are calculated from running the procedure 10000 times.\n0\n500\n1000\n1500\n2000\nGeneration\n10\n6\n10\n4\n10\n2\n100\n102\nlog(||GMM0, GMMevolution||2)\nDistance between the original GMM and its approximation\n as function of a number of data samples\n500\n1000\n10000\n50000\n200000\nFigure 13: Progressive fitting of a GMM with different number of samples. On the y-axis is shown the logarithm of L2\ndistance between the two GMM distributions. Over the generations the distance begins to grow and can become quite\nlarge. The jumps in the distance for large sample sizes occur due to the fixed number of iterations and precision for the\nexpectation maximization algorithm.\n16\nModel Collapse\nAssumptions:\n3*. The extra noise is going to be assumed to be bounded and of the order larger than the sample mean deviation.\nTo be precise we will have a constant K (not dependent on generation i), such that for all i:\n\u2225\u03b5i+1\u2225 \u2264 K\nMi\n(18)\nNow with the alternative assumption in place, we can follow the exact same calculations to arrive at\nE\n\u0002\nRi+1\nW2\n\u0003\n\u2265 E\n\u0000\u2225\u00b5i \u2212 \u00b5\u22252\u0001\n+ Tr \u03a3\nMi\n+ E\n\u0000\u2225\u03b5i+1\u22252\u0001\n+\n2\n\u221aMi\nE\n\u0010\n(\u03b5i+1)\u22a4\u03a31/2\ni\nT i+1\u0011\n(19)\nSimilar to before, we need to evaluate (which we instead bound this time):\n2\n\u221aMi\nE\n\u0010\n(\u03b5i+1)\u22a4\u03a31/2\ni\nT i+1\u0011\n=\n2\n\u221aMi\nZ\nd\u03a3i p(\u03a3i) Tr\nh\n\u03a31/2\ni\nCov(\u03b5i+1, T i+1|\u03a3i)\ni\n\u0338= 0\n(20)\n\u2265 \u22122\n\u221a\nN\n\u221aMi\nZ\nd\u03a3i p(\u03a3i)\nq\nTr\n\u0002\n\u03a3i\u03a3\u03f5i+1\n\u0003\n(21)\n\u2265 \u22122\n\u221a\nN\n\u221aMi\nq\nE\n\u0000\u03b5\u22a4\ni+1\u03a3i\u03b5i+1\n\u0001\n,\n(22)\n\u2265 \u22122\n\u221a\nN\n\u221aMi\ns\nK2 Tr \u03a3\nM 2\ni\n= \u22122K\n\u221a\nN\nMi\n\u221aMi\n\u221a\nTr \u03a3,\n(23)\nwhere we used the Cauchy-Schwarz and Jensen inequalities. Note that this is far from optimal inequality, since instead\nof using the expected value of the largest eigenvalue, we instead bounded it by Tr \u03a3. In particular, the per step bound is\nthen:\nE\n\u0002\nRi+1\nW2\n\u0003\n\u2265 E\n\u0000\u2225\u00b5i \u2212 \u00b5\u22252\u0001\n+ Tr \u03a3\nMi\n+ E\n\u0000\u2225\u03b5i+1\u22252\u0001\n\u2212 2K\n\u221a\nN\nMi\n\u221aMi\n\u221a\nTr \u03a3.\n(24)\nWithout knowledge of the specific values of K, N or Tr \u03a3, the best we can do is consider what this means for the bound\nas Mi becomes large. In particular, contribution from the last two terms will be of order at most 3/2. As a result we\nrecover a bound similar to all of the ones observed so far:\nE\u00b5n+1,\u03c32\nn+1 [RW2] \u2265 Tr \u03a3\n\u0012 1\nM0\n+\n1\nM1\n+ \u00b7 \u00b7 \u00b7 +\n1\nMn\n\u0013\n+ O(3/2)\n(25)\nIn particular, we find in the same way, that superlinear scaling would be required to minimise the lower bound on model\ncollapse even in the case of more generic models of approximation, in which the mean at step i + 1 can be separated\ninto the sample mean and an extra bounded term of order at most 1/Mi.\n17\nModel Collapse\n101\nPerplexity of generated datapoints\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nProbability\nPerplexity of generated datapoints\nevaluated by model trained with\nreal wikitext2\nGeneration 0\nGeneration 1\nGeneration 2\nGeneration 3\nGeneration 5\nGeneration 9\n(a) Overlaid histograms\nGeneration\n0\n2\n4\n6\n8\n10\nPerplexity\n0\n2\n4\n6\n8\n10\nProbability\n(b) 3D view\nFigure 14: Histogram of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model (trained with real data) is more likely to produce. At the same time, a much longer tail appears for\nlater generations \u2013 later generations start producing samples that would never be produced by the original model i.e. they\nstart misperceiving reality based on errors introduced by their ancestors. Models here are explicitly forced to not repeat\nsequences with a penalty of 2.0.\nGeneration\n0\n2\n4\n6\n8\n10\nPerplexity\n0\n2\n4\n6\n8\n10\nProbability\n(a) Figure 11.a in 3D. No data preserved.\nGeneration\n0\n2\n4\n6\n8\n10\nPerplexity\n0\n2\n4\n6\n8\n10\nProbability\n(b) Figure 11.b in 3D. 10% original data preserved.\nFigure 15: Histogram of perplexities of each individual data training sequence produced by different generations as is\nevaluated by the very first model trained with the real data. Over the generations models tend to produce samples that\nthe original model (trained with real data) is more likely to produce. At the same time, a much longer tail appears for\nlater generations \u2013 later generations start producing samples that would never be produced by the original model i.e. they\nstart misperceiving reality based on errors introduced by their ancestors.\n18\n"
  },
  {
    "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
    "link": "https://arxiv.org/pdf/2305.17390.pdf",
    "upvote": "1",
    "text": "SWIFTSAGE: A Generative Agent with\nFast and Slow Thinking for Complex Interactive Tasks\nBill Yuchen Lin 1\nYicheng Fu 4\nKarina Yang 2\nFaeze Brahman 13\nShiyu Huang 5\nChandra Bhagavatula 1\nPrithviraj Ammanabrolu 67\nYejin Choi 31\nXiang Ren 21\n1Allen Institute for Artificial Intelligence\n2University of Southern California\n3University of Washington\n4Tsinghua University\n54Paradigm Inc.\n6University of California, San Diego\n7MosaicML\nhttps://swiftsage.github.io\nAbstract\nWe introduce SWIFTSAGE, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SWIFTSAGE integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the SWIFT module,\nrepresenting fast and intuitive thinking, and the SAGE module, emulating deliberate\nthought processes. The SWIFT module is a small encoder-decoder LM fine-tuned\non the oracle agent\u2019s action trajectories, while the SAGE module employs LLMs\nsuch as GPT-4 for subgoal planning and grounding. We develop a heuristic method\nto harmoniously integrate the two modules, resulting in a more efficient and\nrobust problem-solving process. In 30 tasks from the ScienceWorld benchmark,\nSWIFTSAGE significantly outperforms other methods such as SayCan, ReAct, and\nReflexion, demonstrating its effectiveness in solving complex interactive tasks.1\n1\nIntroduction\nThe advancement of artificial general intelligence is largely dependent on the development of agents\nthat are proficient in complex interactive reasoning tasks. These agents should be capable of exhibiting\nproblem-solving abilities akin to humans within dynamic, open-world environments [26, 7]. For\nexample, the ScienceWorld benchmark [36] features a task where an agent must determine the\nelectrical conductivity of an unknown object. In a simulated environment, the agent must navigate to\nappropriate rooms, locate and acquire essential items, such as batteries and light bulbs, build a circuit,\nperform an experiment, and interpret the results. Tackling such a complex interactive task demands\nagents to exhibit long-horizon planning, long-term memorization, subgoal decomposition, spatial\nreasoning, exception handling, and commonsense knowledge capabilities [37].\nThere are three primary approaches to developing agents capable of addressing complex interactive\nreasoning tasks: (1) (deep) reinforcement learning (RL), (2) behavior cloning (BC) [34] through\nsequence-to-sequence (seq2seq) learning [33], and (3) prompting large language models (LLMs) [6].\nIn addition to conventional RL methods such as DRRN [14], interactive reasoning can be framed as a\nseq2seq task, where the input text serves as the current state description and the output text corresponds\nto the subsequent action [9, 3]. By leveraging numerous gold trajectories generated by oracle agents,\nit becomes feasible to fine-tune Transformer models [35], like T5 [25], to effectively imitate the\n1Contact: yuchenl@allenai.org\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.17390v2  [cs.CL]  6 Dec 2023\nSayCan\nReAct\nReflexion\nTask: Your task is to boil tin. \nFor Each Timestep t: \u2193\nAction 1: go to kitchen\n> you moved to kitchen\nAction 2: look around \n> In this kitchen, you can see \u2026\n\u2026..\nAction t-1: pick up metal pot\n> metal pot in inventory now\nDemo: An oracle path for the \ntask of  boiling water. \nK best generations for Action t\nAction t: put metal pot on stove\nReranking\nTask: Your task is to boil tin. \nAction 1: go to kitchen\n> you moved to kitchen\nAction 2: look around \n> In this kitchen, you can see \u2026\n\u2026..\nAction t-2: think: now I need to \nplace the metal pot on a heater.\n> OK.\nAction t-1: pick up metal pot\n> metal pot in inventory now\nDemo: oracle path + manually \nannotated subgoals\nLLM\nAction t: put metal pot on stove\nFor Each Timestep t: \u2193\nTask: Your task is to boil tin.  + \nthe                     of previously \nfailed trials in the last round.\nAction history (<= t-1) for this \nround. (same as ReAct)\nDemo: ReAct\u2019s version.\nAction t: put tin into metal pot\nFor Each Round: \u2193\nLLM\nLLM\nSwiftSage\nTask: Your task is to boil tin. \nAction history <= t-1\nDemo: w/o manual annotations\nAction Buffer: Action t, \nAction t+1, Action t+2 , \u2026\nOnly when needed: \u2193\nSmall LM \n(e.g., GPT-4)\nOracle paths for training data. \noffline imitation learning\nLLM\nreflection\nThe full action history of the \nprevious failed round. \nreflection \nPrompts for object locating, \nplanning & tracking subgoals, \nand online exception handling.\nLLM\nGrounding \nTemplates\nLLM\nNext \nsubgoals\n>>For Each Timestep t: \u2193\nFigure 1: Comparing methods of prompting LLMs to build agents for interactive tasks.\nbehavior of these oracle agents. Recent studies have also demonstrated that generative agents based\non prompting LLMs, such as GPT-4, can produce reasonable plans and actions [18, 15, 32].\nAlthough the aforementioned methods exhibit remarkable performance in relatively simple tasks,\ntheir ability to generalize to more complex and demanding tasks is limited. Both RL-based and\nseq2seq-based BC approaches effectively acquire knowledge from the environment through large-\nscale interactions and learn general action patterns from oracle agents. However, they face difficulties\nin decomposing tasks into subgoals, maintaining long-term memory, generalizing to unseen tasks,\nand handling exceptions. In contrast, instruction-tuned LLMs [24] demonstrate the ability to generate\nreasonable high-level plans for complex tasks and adapt their outputs based on human feedback.\nYet, grounding their outputs to executable actions in the environment remains a challenge. These\nprocedures also lack the capability to efficiently handle environment-specific exceptions that prevent\nagents from adhering to the LLM\u2019s plans. Additionally, previous methods such as SAYCAN [1],\nREACT [41] and REFLEXION [30] require a new inference with LLMs for each time step, making\nthem considerably costly and inefficient (see Figure 1).\nInspired by the dual process theory [39, 16], we propose a novel framework that enables agents to\nclosely emulate how humans solve complex, open-world tasks. The dual-process theory posits that\nhuman cognition is composed of two distinct systems: System 1, characterized by rapid, intuitive,\nand automatic thinking; and System 2, which entails methodical, analytical, and deliberate thought\nprocesses. System 1 is reminiscent of seq2seq methods, which learn through imitation of oracle agents\nand primarily operate utilizing shallow action patterns. Conversely, System 2 bears resemblance to\nLLMs that excel in applying commonsense knowledge, engaging in step-by-step reasoning, devising\nsubgoal strategies, and exercising self-reflection. Thus, our proposed method, SWIFTSAGE, is\ndesigned to enable both fast and slow thinking in complex interactive reasoning tasks. It effectively\nintegrates the strengths of behavior cloning (representing System 1) and prompting LLMs (emulating\nSystem 2), resulting in significant enhancements in task completion performance and efficiency.\nSpecifically, SWIFTSAGE consists of two primary modules: the SWIFT module and the SAGE module.\nThe SWIFT module is a small encoder-decoder LM, fine-tuned on a T5-large (770m) checkpoint using\nthe searched oracle trajectories of training tasks. It encodes short-term memory components, such as\nprevious actions, observations, visited locations, as well as the current environment state. Then, it\ndecodes the next individual action. This module simulates the fast, intuitive thinking characteristic of\nSystem 1. The SAGE module, representing the deliberate thinking of System 2, utilizes LLMs, such\nas GPT-4, and is structured around two prompting stages: planning and grounding. In the planning\nstage, we prompt LLMs to locate necessary items, plan and track subgoals, as well as detect and fix\npotential exceptions and mistakes. In the grounding stage, we focus on utilizing LLMs to transform\nthe output subgoals derived from the planning stage into a sequence of actions by demonstrating\npotential action templates. Unlike prior methods, where LLMs only generate the next immediate\naction, our procedures engage in longer-term action planning. To harmoniously integrate the SWIFT\nand SAGE modules, we developed a heuristic algorithm that determines when to (de)activate the\nSAGE module and how to combine the outputs effectively with an action buffer mechanism.\n2\nIn a comprehensive evaluation on 30 task types from the ScienceWorld benchmark, SWIFTSAGE\nsignificantly outperforms other methods, achieving a state-of-the-art average score of 84.7. In\ncomparison, SAYCAN scores 33.8, REACT obtains 36.4, and REFLEXION reaches 45.3. Moreover,\nSWIFTSAGE is more cost-effective and efficient, requiring much fewer tokens per action for LLM in-\nference than previous methods. This considerable performance advantage highlights the effectiveness\nand efficiency of the SWIFTSAGE framework in addressing complex interactive tasks.\n2\nBackground and Related Work\n2.1\nComplex Interactive Reasoning\nWe define interactive reasoning as the problems where agents are tasked with accomplishing a\ngoal within an interactive environment, typically simulated by engines such as AI2Thor [17] and\nTextWorld [11]. Our focus lies on the textual environment of ScienceWorld [36] and the complex\ninteractive tasks it supports. Simple interactive tasks, like those created in ALFWorld [31] and\nTWC [21], primarily involve searching for and placing objects as well as performing basic actions\nwithin a single location. Many of these simple tasks have been almost solved by recent works.\nIn contrast, tasks in ScienceWorld exhibit greater complexity, characterized by more challenging\ntask planning and a significantly larger action space (encompassing 10 locations, 200 types of objects\nwith varying states, and 25 types of actions). Furthermore, agents may encounter random, unforeseen\nobstacles, such as broken stoves or missing soil, which hinder the execution of planned actions. As a\nresult, agents must adapt and re-plan accordingly, for example, by seeking alternative heat sources\nor using a shovel on the outside ground to get soil. These challenges demand that agents possess\nskills in long-horizon planning, long-term memory, subgoal decomposition, exception handling, and\ncommonsense knowledge\u2014capabilities that are not explicitly required for simple interactive tasks.\n2.2\nReinforcement Learning and Imitation Learning Methods\nDRRN.\nInteractive tasks can naturally be framed as partially-observable Markov decision processes\n(POMDPs), enabling the application of RL-based methods. Deep Reinforced Relevance Network\n(DRRN) [14] is a standard baseline method to learn agents within text environment. It aims to learn\nrepresentations of observations and actions separately and train a policy network to select actions\nfrom candidates based on feedback from the simulated environment. CALM [40] is a reranking-\nbased method that combines DRRN with a causal language model (LM) fine-tuned with oracle\ntranscripts. In essence, the causal LM captures task-specific and environment-specific knowledge\nthrough imitation learning, and the DRRN learns to rerank the predictions from the LM.\nThe KG-A2C [2] method uses an OpenIE technique [4] to represent environment states with graph\nstructures and dynamically update these graphs. These graphs guide policy networks by constraining\nthe combinations of action templates and objects. This method has been shown to be effective in\nother domains such as for multimodal embodied agents [22].\nBehavior cloning for offline imitation learning.\nBehavior cloning is an imitation learning method\nthat trains a seq2seq Transformer offline with action transcripts of similar training tasks generated by\noracle agents [34, 3]. During training, it uses the previous action, observation at time step t \u2212 1, and\nthe current observation as input and learns to output the next action. The Text Decision Transformer\n(TDT) is a textual variant of the Decision Transformer [9], which also employs behavior cloning and\nuses the same data. The primary innovation of TDT is the introduction of reward-to-go as part of the\ninputs, enabling the model to learn predicting actions that maximize future expected rewards.\n2.3\nPrompting LLMs for Action Planning.\nLanguage models (LLMs) such as GPT-4 have shown promise for action planning in interactive\ntasks [18, 15, 32, 38]. In this paper, we adapt three prominent methods to complex interactive\nreasoning tasks in ScienceWorld: SAYCAN [1], REACT [41], and REFLEXION [30].\nSAYCAN [1] is a straightforward agent that integrates an LLM with a value function of underlying\npolicies regarding grounding affordances (i.e., the feasibility of an action in the environment). We\n3\nneed to provide the history and current environment as textual inputs to LLMs for generating a ranked\nlist of action candidates. This action list is then reranked based on a value function.\nREACT [41] presents a virtual \u2018think\u2019 action, enabling LLMs to generate subgoals during action\nplanning. This approach requires human annotators to supply examples of correct subgoals for each\ntask type, employing few-shot in-context learning to teach LLMs when and how to \u2018think\u2019 in order to\nplan subsequent subgoals, in addition to providing complete action trajectories.\nREFLEXION [30], a recent work building on REACT, proposes a multi-round approach enabling\nLLMs to use the history of previously failed rounds to refine their planning for the next round. This\nself-reflection mechanism helps LLMs improve after each failed attempt. However, this may not be\npractical in real-world applications for many tasks, as actions in failed trials can be irrecoverable.\nAll three methods require a new LLM inference at each time step to predict the next immediate action,\nresulting in inefficient and costly agents. REACT and REFLEXION require human annotations of\ncorrect subgoals for each unseen task type. Moreover, it is difficult to generalize REFLEXION to\nreal-world situations where trial-and-error approaches can be infeasible for embodied tasks.\n2.4\nDual-Process Theory\nThe dual-process theory [39, 16] is a cognitive psychological framework proposing the existence of a\nfast and a slow thinking systems in the human brain. This influential theory has found widespread\napplications across various fields, highlighting the critical role of both systems in shaping human\ncognition [5, 8, 12, 20, 23]. By integrating the complementary strengths of both systems, agents can\neffectively and efficiently handle diverse challenges in real-world scenarios. Inspired by this, we aim\nto construct a generative agent that utilizes a small seq2seq LM as System 1 for associative reasoning\nvia behavior cloning while developing System 2 for analytical reasoning by prompting LLMs.\n3\nSWIFTSAGE: A Generative Agent with Fast and Slow Thinking\nIn this section, we first establish the problem. Then, we present the two core modules, SWIFT and\nSAGE, individually. Lastly, we demonstrate the integration of these two modules. , resulting in a\nharmonious and effective interactive reasoning process.\n3.1\nProblem Formulation\nEnvironment and tasks.\nWe focus on complex interactive reasoning tasks situated in virtual textual\nenvironments such as ScienceWorld [36]. ScienceWorld provides an optimal setting for developing\nand evaluating agents in complex tasks, comprising 30 distinct task types covering 10 topics in\nscience experiments. It features 10 locations, including an art studio, workshop, kitchen, living room,\nbedroom, bathroom, foundry, greenhouse, outdoor area, and a connecting hallway. The environment\nincludes 200+ object types with multiple states (e.g., open, activated) and supports 25 action templates,\nresulting in an intractable search space. The simulator can generate numerous variations of each task\ntype, providing a rich training ground. In each variation, the agent and environment initialization,\nsuch as the locations and states of objects, will differ. A plethora of training variations encompassing\nall task types are available for training agents. Additionally, it provides a handcrafted oracle agent to\nsearch for successful transcripts with minimal actions for offline learning.\nEvaluation is done on a set of testing variations with unseen combinations of required objects and\nsituations, thus substantially different from the training variations. For example, a training variation\nmay involve boiling water, while a testing variation could require boiling tin. Therefore, it is crucial to\nensure the agent\u2019s compositional generalization ability for effectively handling real-world scenarios.\nInteractions.\nGiven a task variation, an agent is provided with the task description D and the initial\nenvironment state (t = 0). The task description D is a text specifying a high-level goal, e.g., \u201cYour\ntask is to test if an unknown substance A is electronically conductive.\u201d At each time step t, the agent\ngenerates an action At based on a set of supported action templates (e.g., pick up X, use X on\nY). A0 is always \u201clook around\u201d for showing initial environment information. Upon receiving an\naction from the agent, the environment produces feedback in four dimensions:\n4\nDemo + Task + Env. + History + Plan + Action Types:\n- POUR (A, B): pour object A into container B; e.g., POUR(water, pot)\n- \u2026 Convert next subgoals to a sequence of actions:\nTime to \nswitch\nPlans: Q1: \u2026 Q2: \u2026 Q3: \u2026\nQ4: 1. Find a working heat source\n2. Place the metal pot with ice cream on the heat source.\n3. Wait until the ice cream melts.\nQ5: You tried to use the broken stove as a heat source. To \nfix this, you should try using the oven in the kitchen or \u2026\nSage: Prompting LLMs (GPT-4) for Planning and Grounding the Next Subgoals\nSwift:\nQ1 & Q2: Locate Needed Objects \nQ5: Detect & Fix Exceptions\nQ3 & Q4: Plan & Track Subgoals\nTask: Your task is to melt ice cream.; Time: 14; Score: 35; Action history: <extra_id_0> Action 5 (+5): open fridge \n--> You opened fridge. In it, you see an ice cream\u2026 [\u2026] <extra_id_9> Action 14 (+0): move metal pot to stove --\n> You move the metal pot to the stove. </s>  Current environment: This room is kitchen. You see: a fridge \n(closed) | a sink | an oven (closed, turned off) | a stove (turned off; on it: a metal pot containing ice cream \u2026). \n|[\u2026.]| Inventory: an orange, \u2026 </s> Visited: workshop, hallway, kitchen </s> What should be the next action?\nNext action: \nactivate stove\nObs. 15: The stove appears \nbroken and can't be activated. \nTask + History (t=1\u219215) + Env.\nLLM\nopen oven \nmove metal pot to oven \nclose oven \nactivate oven \nPlan\nGround\nAction buffer\nLLM\n(T5-large w/ \nimitation learning)\nEnv.\nFigure 2: An example of how SWIFTSAGE works with fast and slow thinking. The SWIFT\nmodule is offline trained via imitation learning with a small LM such as T5-large (770m). When it\nis necessary, for example, encountering an exception, we switch to the SAGE module that prompts\nLLMs (e.g., GPT-4) for planning and grounding the next subgoals, resulting in an action buffer.\n\u2022 Observation Ot provides direct feedback on the action At regarding its effects on the envi-\nronment or the information queried. For example, an At of \u201cuse thermometer on the\nsubstance in metal pot\u201d may result in an Ot like \u201cThe temperature is 80F.\u201d\n\u2022 Environment Et represents the current room in which the agent is situated and provides details\nabout all visible objects. Object visibility is based on container states, e.g., objects within a closed\nfridge are not included in Et until the agent performs an action like \u201copen fridge.\u201d\n\u2022 Inventory It lists objects picked up by the agent, which is particularly useful when agents collect\nitems from different locations to complete the task.\n\u2022 Score St represents the agent\u2019s cumulative score ranging from 0 to 100. When a required interme-\ndiate state is achieved, the score increases with a positive reward.\n3.2\nSWIFT: The Module for Intuitive and Associative Thinking via Imitation Learning\nImitation learning is used to construct an agent that learns to mimic oracle agents in various training\nscenarios through seq2seq learning. Previous methods, such as TDT [36], mainly employ one-hop\nhistory as input context and learn to output the subsequent action At [36]. However, these methods\nexhibit limitations due to their restricted context of action history and harmful biases arising from\ndata imbalance. To address these issues, we introduce our SWIFT module, depicted in Figure 2.\nRepresentation for longer history.\nWe expand the conventional one-hop BC to multi-hop by\nincorporating a sliding window of observations and rewards for the K = 10 recent actions. Addi-\ntionally, we include a special field for visited rooms (without duplication). This approach aims to\nprovide agents with a longer context and prevent unnecessary room navigation. The input format\nis as follows: \u201cTask: D; Time: t \u2212 1; Score: St\u22121; Action history: [At\u2212i (+Rt\u2212i) \u2192\nOt\u2212i ] /* i loops from K to 1*/; Current room: Et\u22121; Inventory: It\u22121; Visited rooms:\n{E\u2217\n1, . . . , E\u2217\nt\u22121}\u201d. Here, Rt = St \u2212 St\u22121 represents the reward at t, and E\u2217\nt is the location name at t.\nBalanced imitation learning.\nTo avoid bias caused by data imbalance for seq2seq learning, we\ndown-sampled specific types of tasks and actions to achieve a more balanced final dataset for training.\nWe used the T5-large with 770 million parameter and instruction-following ability [10], creating an\nefficient agent that we named SWIFT. Our empirical results show that the SWIFT module performs\nmuch better than TDT (11 billion) despite being 15x smaller in size.\n5\nThe SWIFT module exhibits greater accuracy during initial time steps, enabling it to attain higher\nscores in the early stages of a complex task. However, it often fails to generalize to unseen situations.\nThe module also has a tendency to repeat meaningless actions when its learned plans yield exceptions\nfrom the environment (e.g., the broken stove in Figure 2). This is partly due to the nature of imitation\nlearning, which prioritizes emulating the observable actions of oracle agents rather than their intrinsic\nplanning abilities. Besides, since the oracle trajectories contain only the shortest, correct actions, it is\nthus also challenging for the SWIFT to learn how to fix mistaken actions.\n3.3\nSAGE: The Module for Deliberate and Analytical Thinking via Prompting LLMs\nWhile the SWIFT module acquires surface knowledge about the environment and task types through\nimitation learning, it lacks two key abilities essential for complex interactive reasoning: 1) gener-\nalizable planning and tracking of subgoals, and 2) robust handling of exceptions. Prior research\nhas shown that LLMs outperform smaller LMs in these abilities. They can perform step-by-step\nreasoning to devise concrete plans for tasks and self-refine their outcomes. However, the performance\nof prior methods remains unsatisfactory in complex interactive tasks such as those in ScienceWorld.\nWe introduce a novel two-stage approach, named SAGE. This method initially acquires higher-level\nrecommendations from LLMs during the planning stage, followed by their translation into specific\naction sequences in the grounding stage. By decoupling the planning and grounding processes,\nSWIFTSAGE effectively generates a series of actions for completing the planned subgoals.\nPlanning stage.\nIn this stage, we leverage LLMs to plan based on the current state. Specifically,\nwe prompt LLMs with a single prompt that includes a summarized version of the task description\nand action history, and asks the following five key questions:\n\u25b6 Q1(locate objects): \u201cTo complete the task, which objects do I need to collect? Please\nlist them and their possible locations one by one.\u201d\n\u25b6 Q2(track objects): \u201cAre there any objects that have not been collected yet?\u201d\n\u25b6 Q3(plan subgoals): \u201cTo complete the task most efficiently, what are the important sub-\ngoals to achieve? Please list the subgoals one by one.\u201d\n\u25b6 Q4(track progress): \u201cConsidering these subgoals, what have I already completed? And\nwhich subgoal should I focus on right now?\u201d\n\u25b6 Q5(handle exceptions): \u201cHave I made any mistakes that might prevent me from effi-\nciently completing the next subgoal? If any , how should I fix them?\u201d\nBefore posing the five planning-related questions, we condense the entire action history (A<t and\nO<t), and the current environment information Et\u22121. Q1 and Q2 pertain to objects, as acquiring all\nnecessary objects serves as the foundation for effective task planning. By addressing these questions,\nwe ensure that LLMs develop a comprehensive understanding of the current environment. Q3 prompts\nLLMs to engage in step-by-step planning by decomposing the task into a series of subgoals. Q4 acts\nas a follow-up question, allowing the agent to monitor its progress based on the action history and\ndetermine completed subgoals, subsequently focusing on the remaining tasks. Lastly, Q5 is employed\nto identify and address potential exceptions. These questions can be further tailored with additional\nenvironment-specific hints, thereby enhancing their adaptability.\nTo improve the structure of the LLMs\u2019 outputs and facilitate parsing, we incorporate additional\ninstructions in the prompt. By utilizing a single input to obtain answers to all five questions in\none output, rather than engaging in multiple rounds of interactive prompting, our approach is more\nefficient and cost-effective than the iterative prompting methods.\nQ4 and Q5 are of primary importance, while Q1-Q3 serve as auxiliary guidance for the LLMs. If the\naction history indicates a mistaken action or an unachievable previous subgoal, the response to Q5\nrefines the answer to Q4 through self-reflection on the fly. This approach differs from the REFLEXION\nagent, which only prompts reflective questions at the end of a failed trial, allowing agents to improve\ntheir planning in subsequent attempts. In contrast, our method detects exceptions and errors each\ntime the agent plans for the next subgoals, enabling earlier correction of the agent\u2019s behavior.\nGrounding stage.\nWhile the answers to Q1-Q5 provide valuable guidance for agents, they are\nnot directly executable. Converting plans into valid actions that can be accepted by the environment\n6\nremains a challenge. Previous methods using LLMs over-generate candidates, and they rely on\nreranking or filtering based on the action space to select the next action. However, this is inefficient\nand inaccurate for complex tasks with vast action spaces. Additionally, these methods generate a\nsingle action at a time, which can be both costly and ineffective for long-horizon tasks.\nTo tackle these issues, we first present supported action types using a formal style accompanied by\nremarks. For instance, the action type \u201cpour X into Y\u201d is introduced as \u201cPOUR(X, Y): pour\nobject X into container Y; e.g., pour red paint into wood cup\u201d. More examples:\nTELEPORT(room) : directly go to a room such as TELEPORT(kitchen)\nPICK(object) : pick up an object and put it into your inventory\nOPEN(object) : open an object to search or put things in it, e.g., OPEN(freezer).\nACTIVATE(object) : activate / turn on an object such as sink or stove, so that you can use it.\nDEACTIVATE(object) : deactivate / turn off the object\nEXAMINE(object) : look at an object carefully. For example, EXAMINE(light bulb).\nMOVE(object, place) : move/place the object to a place\nWe then incorporate the LLM\u2019s outputs from the planning stage as part of the input for the grounding\nstage. Furthermore, we provide the recent action history of the past 10 time steps as context. Finally,\nwe prompt LLMs to concentrate on the next subgoal and convert it into a list of actions (rather than a\nsingle action) to accomplish the next subgoal. Our formatting instructions enable the straightforward\nsplitting and conversion of output actions from LLMs in the grounding stage back to their original\naction representations. We denote this list of actions generated by LLMs as the action buffer:\nB = { \u02c6At, \u02c6At+1, . . . }. One can opt to use only answers to Q4 and Q5 to reduce computational costs.\nOur small-scale ablation study indicates that incorporating answers to Q1-Q3 in the grounding stage\nproves beneficial, yielding a gain of about 2 points for short tasks on average.\n3.4\nIntegration of Fast and Slow Thinking\nHaving described the SWIFT and SAGE modules, we now address the question of how to merge both\nmodules and effectively integrate fast and slow thinking within the SWIFTSAGE agent. We establish\na heuristic algorithm to control the activation and deactivation of the two modules.\nInitially, we employ the SWIFT module due to its superior intuitive reasoning capabilities, which\nfacilitate accurate associations between the task description and the environment during the first few\nactions. We will switch from SWIFT mode to SAGE when any of the following conditions are met:\n1) Stuck: There are K=5 consecutive time steps with zero reward (Pt\u22121\ni=t\u22125 Ri = 0).\n2) Invalid: The SWIFT\u2019s prediction for the next action (A\u2032\nt) is invalid in the current environment.\n3) Critical: A\u2032\nt involves a critical decision, e.g., giving the final answer for the experiment result.\n4) Unexpected: The observation of A\u2032\nt suggests that an exception is encountered.\nUpon activating the SAGE module, we execute the two-stage prompting process and generate an\naction buffer. We attempt to execute each predicted action and revert to the SWIFT module when the\nbuffer is empty. This approach enables a seamless integration of both modules, providing an efficient\nand robust problem-solving process for the SWIFTSAGE agent. The pseudo code for illustrating the\nSwiftSage framework is shown in Fig. 4 (Appendix).\n4\nEvaluation\n4.1\nEvaluation Setup\nTo evaluate the effectiveness of SWIFTSAGE and other baseline methods in complex interactive\nreasoning tasks, we use the ScienceWorld benchmark. In Section 2.1 and Section 3.1, we introduce\nthe benchmark and problem formulation. Each task type is categorized as \u2018short\u2019 (S), \u2018medium\u2019\n(M), or \u2018long\u2019 (L) based on the average length of the oracle truth trajectories. However, the length\nof the task does not necessarily indicate its level of difficulty as some tasks may require additional\ncommonsense knowledge. Further evaluation details are provided in the appendix.\n7\nTask Type\n*Len\nDRRN\nKGA2C\nCALM\nTDT\nSayCan\nReAct\nReflexion\nSwiftSage\n1-1 (L)\n107.7\n3.52\n0.0\n0.0\n0.71\n33.06\n3.52\n4.22\n97.04\n1-2 (L)\n78.6\n3.52\n0.0\n0.0\n0.44\n10.39\n13.70\n10.61\n87.04\n1-3 (L)\n88.9\n0.0\n4.0\n0.0\n3.88\n3.88\n7.78\n7.78\n72.78\n1-4 (L)\n75.2\n0.0\n0.0\n0.0\n0.55\n0.37\n9.88\n0.92\n100.0\n2-1 (M)\n21.4\n6.56\n6.0\n1.0\n6.16\n26.37\n7.19\n5.92\n99.17\n2-2 (M)\n35.2\n5.50\n11.0\n1.0\n6.43\n8.03\n6.10\n28.59\n88.17\n2-3 (L)\n65.0\n6.0\n4.0\n1.0\n19.87\n17.41\n22.37\n22.37\n95.73\n3-1 (S)\n13.6\n12.0\n7.0\n5.0\n40.55\n52.14\n56.0\n100.0\n88.67\n3-2 (M)\n20.8\n9.0\n4.0\n7.0\n14.26\n22.50\n54.33\n17.45\n55.33\n3-3 (M)\n25.6\n9.05\n4.0\n2.0\n10.16\n99.56\n76.19\n72.54\n71.90\n3-4 (M)\n29.0\n9.52\n4.0\n2.0\n21.65\n47.76\n88.81\n70.22\n77.86\n4-1 (S)\n14.6\n15.0\n18.0\n10.0\n41.93\n22.87\n26.67\n64.93\n100.0\n4-2 (S)\n8.8\n45.0\n44.0\n54.0\n55.76\n58.18\n80.0\n87.27\n100.0\n4-3 (S)\n12.6\n21.67\n16.0\n10.0\n27.82\n20.87\n53.33\n16.42\n91.67\n4-4 (S)\n14.6\n19.17\n15.0\n8.0\n47.15\n31.43\n27.50\n100.0\n100.0\n5-1 (L)\n69.5\n8.0\n6.0\n2.0\n6.89\n9.92\n9.06\n7.33\n74.59\n5-2 (L)\n79.6\n14.29\n11.0\n4.0\n11.86\n13.93\n18.57\n13.0\n93.93\n6-1 (M)\n33.6\n15.77\n17.0\n3.0\n15.10\n47.81\n51.04\n70.35\n49.40\n6-2 (S)\n15.1\n26.67\n19.0\n6.0\n15.70\n39.26\n58.89\n70.67\n100.0\n6-3 (M)\n23.0\n10.37\n4.0\n3.0\n5.25\n19.72\n40.74\n15.77\n91.48\n7-1 (S)\n7.0\n50.0\n43.0\n6.0\n30.0\n80.0\n60.0\n100.0\n95.0\n7-2 (S)\n7.0\n50.0\n32.0\n10.0\n8.43\n67.50\n67.50\n84.37\n85.0\n7-3 (S)\n8.0\n33.33\n23.0\n4.0\n8.34\n50.0\n50.0\n83.0\n93.33\n8-1 (M)\n40.0\n21.0\n5.0\n4.0\n3.86\n20.91\n27.67\n2.58\n89.0\n8-2 (S)\n16.3\n8.0\n10.0\n0.0\n8.0\n16.0\n8.0\n8.0\n68.50\n9-1 (L)\n97.0\n10.0\n4.0\n0.0\n2.53\n21.94\n40.50\n50.63\n75.0\n9-2 (L)\n84.9\n10.0\n4.0\n3.0\n14.66\n32.26\n44.0\n100.0\n70.0\n9-3 (L)\n123.1\n10.0\n4.0\n2.0\n9.12\n13.67\n41.0\n70.62\n60.0\n10-1 (L)\n130.1\n16.80\n11.0\n2.0\n1.51\n67.53\n25.70\n50.90\n92.30\n10-2 (L)\n132.1\n17.0\n11.0\n2.0\n1.29\n59.45\n16.80\n23.69\n77.60\nShort\n11.76\n28.08\n22.70\n11.30\n28.37\n43.83\n48.79\n71.47\n92.22\nMedium\n28.58\n10.85\n6.88\n2.88\n10.36\n36.58\n44.01\n35.43\n77.79\nLong\n94.30\n8.26\n4.92\n1.33\n6.11\n23.65\n21.07\n30.17\n83.0\nOverall\n49.26\n15.56\n11.37\n5.07\n14.66\n33.82\n36.43\n45.34\n84.68\nTable 1: Results on the ScienceWorld benchmark. *Len is the average length of the oracle agent\u2019s\ntrajectories. In addition to overall results, we also report performance on three groups of *Len (short,\nmedium, long). The last four methods use GPT-4 as the base LLM for prompting. We show more\ndetails of these tasks and results on other LLMs in the appendix.\n4.2\nBaseline Agents\nIn addition to the baseline methods evaluated in the ScienceWorld paper, such as DRRN, CALM,\nKG-A2C, and TDT, we incorporate three LLM-based prompting techniques: SAYCAN, REACT, and\nREFLEXION, as detailed in Section 2.3 and Figure 1. This subsection presents the implementation\ndetails for adapting these methods to build ScienceWorld agents.\nSAYCAN necessitates a value function from the environment for reranking purposes. We employ\nSentenceBERT [27] to rank all valid actions (generated by ScienceWorld\u2019s APIs) based on their\nsimilarity to the top 5 generations for At from SAYCAN. We implemented REACT and REFLEXION\nin a similar manner. Adhering to their released code, we utilized the best single generation and\ndetermined the valid action with the minimal edit distance, if required. Both REACT and REFLEXION\nnecessitate subgoal annotations for teaching LLMs to plan with virtual \u2018think\u2019 actions. We annotated\nsuch truth subgoals by translating ScienceWorld\u2019s APIs into natural language, which was also\nemployed by the oracle agents. For all agents, we incorporated the complete trajectories of one or\ntwo training variations from the same task type for in-context learning. Our primary experiments\nwere conducted using OpenAI\u2019s GPT-4; however, other LLMs can be readily substituted as required.\n8\n7-1 S \n7-2 S \n7-3 S \n4-2 S \n4-3 S \n3-1 S \n4-1 S \n4-4 S \n6-2 S \n8-2 S \n3-2 M \n2-1 M \n6-3 M \n3-3 M \n3-4 M \n6-1 M \n2-2 M \n8-1 M \n2-3 L \n5-1 L \n1-4 L \n1-2 L \n5-2 L \n9-2 L \n1-3 L \n9-1 L \n1-1 L \n9-3 L \n10-1 L \n10-2 L \nFigure 3: Visualizing trajectories of SWIFTSAGE, REACT and ORACLE. X: time steps (0 \u2192 T);\nY : scores (0 \u2192 100). Each figure displays the merged trajectories of testing variations by an agent in\neach task. Task IDs are shown at the bottom-right, and the ordering is based on *Len in Tab 1.\n4.3\nResults and analysis.\nMain Results\nTable 1 compares the performance of various agents across 30 types of tasks. Detailed\ndescriptions of each task type can be found in the ScienceWorld paper [36] and our appendix. It is\nevident that LLM-based methods outperform conventional agents due to their superior generalization\nability, albeit at a higher deployment cost. The behavior cloning model TDT [36, 9] (11b) performs\non par with DRRN [14], but with greater efficiency in learning and inference. In contrast, our SWIFT-\nonly agent (770m) achieves an overall performance of 49.22, which we attribute to its balanced\ntraining data and the use of a sliding window for longer action histories.\nREACT demonstrates a noticeable improvement over SAYCAN for short and medium tasks, owing to\nits subgoal annotations for in-context learning and the inclusion of \u2018think\u2019 actions. REFLEXION\nsurpasses REACT in shorter tasks; however, comparing REFLEXION with other agents is not entirely\nfair. REFLEXION can run up to four rounds, while the others are limited to one round. This\ndiscrepancy is particularly unfair for tasks involving multiple-choice scenarios. Nevertheless, we\ninclude REFLEXION\u2019s results to analyze the potential of such methods.\nException handling.\nConsider the example in Figure 2, where the stove is broken, presenting\nan exception. Agents like DRRN and TDT often resort to repeating meaningless action sequences\n(e.g., continuously attempting to activate the stove or moving between rooms aimlessly). Although\nthe SWIFT module, when used independently, improves upon this due to its larger context window\nfrom imitation learning, it still struggles to address exceptions robustly. ReAct and Reflexion\noccasionally utilize the \u2018think\u2019 action or reflections to redirect agents towards alternative solutions,\nbut the generated actions rarely achieve the new subgoals if they are not grounded. In contrast, the\nplan-and-ground prompts in our SAGE module handle exceptions more effectively.\nCost-effectiveness.\nDespite SAGE invoking LLMs APIs twice for inference, its overall cost remains\nlower, as the result is a sequence of actions typically containing about 5 actions. In comparison,\nSAYCAN and REACT require 1,855.84 and 1,971.03 tokens per action (tpa) respectively, while\nREFLEXION necessitates 2,983.46 tpa. SWIFTSAGE, on the other hand, only uses 757.07 tpa. Given\nits superior performance, SWIFTSAGE proves more cost-effective than other LLM-based methods.\nThis efficiency is primarily attributed to invoking LLMs only when needed (courtesy of our strong\nSWIFT module) and the action buffer mechanism.\nEfficiency.\nTo thoroughly examine the efficiency of agents across all task types, we use Figure 3\nto visualize the average trajectories of the first three testing variations for each task involving\n9\nSWIFTSAGE, REACT, and the oracle agent. We arrange the tasks based on their average lengths of\noracle trajectories (*Len in Table 1). We observe that oracle trajectories consistently achieve perfect\nscores, yet SWIFTSAGE can reach similar scores more efficiently. This is particularly evident in\nlonger tasks (the bottom two rows), although SWIFTSAGE does not achieve a perfect score for a few\ntasks (e.g., 9-2 and 1-3). Interestingly, we find that REACT performs competitively in shorter tasks\n(e.g., 4-2 and 3-4), but most trajectories plateau at an intermediate score and fail to reach 100.\nMore analysis.\nDue to page limit, we have to provide further details and analysis in the appendix,\nincluding more detailed analysis on cost-effectiveness and efficiency, additional case studies and\nablation studies, sensitivity to LLM choices, and an the evaluation of the SWIFT-only agent.\n5\nConclusion\nContributions.\nWe present SWIFTSAGE, a generative agent for complex interactive reasoning\ntasks, inspired by the dual-process theory of human cognition. The agent framework comprises two\nmodules: SWIFT, responsible for fast thinking, and SAGE, dedicated to slow thinking. The SWIFT\nmodule is a smaller LM that is fast and specialized, while the SAGE module focuses on prompting\nLLMs (e.g., GPT-4) for subgoal planning and reflective thinking. Through extensive experiments\non 30 distinct tasks within the ScienceWorld benchmark, SWIFTSAGE outperforms baseline agents,\nachieving state-of-the-art performance, increased efficiency, and reduced cost.\nImplications.\nThe success of SWIFTSAGE highlights the potential for collaborative frameworks\ncombining smaller LMs and LLMs in complex reasoning tasks. Smaller LMs can be trained more\neasily to recognize task-specific and environment-specific patterns, fostering effective in-distribution\ngeneralization. On the other hand, LLMs demonstrate remarkable zero-shot generalization abilities\nand deliberate thinking, though grounding their outputs in real-world environments remains chal-\nlenging. We posit that dual-process agents, harnessing the strengths of both approaches, constitute a\ncrucial step towards addressing complex interactive reasoning tasks and building general AI agents.\nAdditionally, we can regard SWIFTSAGE as a method within the broader context of utilizing LLMs\nas controllers or planners for decomposing complex tasks and leveraging APIs/tools [19, 13, 29, 28].\nTo this end, we have explored applying SWIFTSAGE in web tasks and coding for math problems.\nLimitations.\nOur work has been evaluated solely within a textual simulator, ScienceWorld, which\nsupports a limited set of actions and tasks compared to real-world situations. Also, we did not\nimplement any safeguards to prevent agents from engaging in potentially hazardous actions that\ncould occur in the real world, such as picking up substances from a blast furnace. We argue that one\nimportant future direction is to develop a true open-ended environment, allowing agents to interact\nwith a much wider variety of actions and objects to better emulate real-world scenarios. Besides, the\nuse of LLMs in SAGE may present scalability challenges, as LLMs require significant computational\nresources and may not be feasible in some settings. Future research should explore the generalizability\nof SWIFTSAGE to other domains and the potential for more lightweight approaches to slow thinking.\nIn addition, we believe it is important to train agents beyond simple supervised fine-tuning and to\nlearn a trainable module to decide when to switch between SWIFT and SAGE mode.\nAcknowledgements\nWe thank Peter Jansen, Eric Xingdi Yuan, and Marc-Alexandre C\u00f4t\u00e9 for valuable discussions. We\nthank members of the INK lab at USC and the Mosaic team at AI2 for valuable feedback on this\nproject. Xiang Ren is supported in part by the Office of the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract\n#2022-22072200006, the DARPA MCS program under Contract No. N660011924033, the Defense\nAdvanced Research Projects Agency with award W911NF-19-20271, NSF IIS 2048211, and gift\nawards from Google and Amazon. This research was also supported by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031) and Allen Institute for AI. The views and conclusions\ncontained herein are those of the authors and should not be interpreted as necessarily representing the\nofficial policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.\n10\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho,\nJasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle\nJeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor,\nJornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas\nSievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun\nXu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances.\nIn Conference on Robot Learning, 2022.\n[2] Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning\nfor natural language action spaces. In International Conference on Learning Representations,\n2020.\n[3] Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur D. Szlam, Tim Rocktaschel, and\nJason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in\nfantasy worlds. In North American Chapter of the Association for Computational Linguistics,\n2020.\n[4] Gabor Angeli, Melvin Johnson, and Christopher D. Manning. Leveraging linguistic structure for\nopen domain information extraction. In Annual Meeting of the Association for Computational\nLinguistics, 2015.\n[5] Thomas W. Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning\nand tree search. ArXiv, abs/1705.08439, 2017.\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\nLarochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[7] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4. ArXiv, abs/2303.12712, 2023.\n[8] Di Chen, Yiwei Bai, Wenting Zhao, Sebastian Ament, J. Gregoire, and Carla P. Gomes. Deep\nreasoning networks: Thinking fast and slow. ArXiv, abs/1906.00855, 2019.\n[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel,\nA. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. In Neural Information Processing Systems, 2021.\n[10] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav\nMishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.\n[11] Marc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine,\nJames Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam\nTrischler. Textworld: A learning environment for text-based games. In CGW@IJCAI, 2018.\n11\n[12] M. B. Ganapini, Murray Campbell, F. Fabiano, L. Horesh, Jonathan Lenchner, Andrea Loreggia,\nNicholas Mattei, Francesca Rossi, Biplav Srivastava, and Kristen Brent Venable. Thinking fast\nand slow in ai: the role of metacognition. In International Conference on Machine Learning,\nOptimization, and Data Science, 2021.\n[13] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang.\nOpenagi: When llm meets domain experts. arXiv, 2023.\n[14] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf.\nDeep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence,\n2015.\n[15] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022.\n[16] Daniel Kahneman. Thinking, Fast and Slow. 2011.\n[17] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D\nEnvironment for Visual AI. arXiv, 2017.\n[18] Bill Yuchen Lin, Chengsong Huang, Qianchu Liu, Wenda Gu, Sam Sommerer, and Xiang Ren.\nOn grounded planning for embodied tasks with language models. ArXiv, abs/2209.00465, 2022.\n[19] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language\nmodels. ArXiv, abs/2304.09842, 2023.\n[20] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Think-\ning fast and slow: Efficient text-to-visual retrieval with transformers. 2021 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pages 9821\u20139831, 2021.\n[21] Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel,\nGerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based\nRL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In\nThirty Fifth AAAI Conference on Artificial Intelligence, 2021.\n[22] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi,\nSameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision\nmaking using language guided world modelling. In International Conference on Machine\nLearning (ICML), 2023.\n[23] Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improv-\ning coherence and consistency in neural sequence models with dual-system, neuro-symbolic\nreasoning. In Neural Information Processing Systems, 2021.\n[24] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis\nChristiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with\nhuman feedback. ArXiv, abs/2203.02155, 2022.\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.\n[26] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen,\nRaia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. ArXiv,\nabs/2205.06175, 2022.\n12\n[27] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese\nBERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, 2019. Association for Computational\nLinguistics.\n[28] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. ArXiv, abs/2302.04761, 2023.\n[29] Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, and Yue Ting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. ArXiv, abs/2303.17580,\n2023.\n[30] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. ArXiv, abs/2303.11366, 2023.\n[31] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and\nMatthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive\nlearning. ArXiv, abs/2010.03768, 2020.\n[32] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models.\nArXiv, abs/2212.04088, 2022.\n[33] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. ArXiv, abs/1409.3215, 2014.\n[34] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. ArXiv,\nabs/1805.01954, 2018.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npages 5998\u20136008, 2017.\n[36] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu.\nScienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in\nNatural Language Processing, 2022.\n[37] Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao,\nGuangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang,\nAdam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu,\nYike Guo, and Jie Fu. Interactive natural language processing. ArXiv, 2023.\n[38] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\nArXiv, abs/2302.01560, 2023.\n[39] Peter C Wason and J St BT Evans. Dual processes in reasoning? Cognition, 3(2):141\u2013154,\n1974.\n[40] Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and\nexplore: Language models for action generation in text-based games. ArXiv, abs/2010.02903,\n2020.\n[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629,\n2022.\n13\nAppendix\nA\nDataset Statistics\nTable 2 presents the details of all 30 types of tasks in the ScienceWorld benchmark. To improve the\ntraining of SWIFT, we down-sampled Task 9-x, 10-x, and 3-3 from the original full dataset, as their\nlarge sizes resulted in a significant data imbalance. Additionally, we down-sampled less informative\nactions, such as \u2018close door to kitchen,\u2019 to produce a more effective dataset for imitation learning.\nEvaluation.\nTo save time while evaluating the numerous tasks and agents, we only used the first 10\nvariations for tasks with more than 10 test variations. This resulted in a total of 270 variations for fair\nand cost-effective comparisons among all agents. Some agents may receive a negative score from the\nengine and be unable to proceed any further due to their final action violating task requirements and\nbeing irrecoverable. In such cases, we used their last non-negative scores for evaluation.\nTask Type\nTopic\nName\n*Lens\n#Vars: Train\nDev\nTest\n# Actions\n1-1\nMatter\nChanges of State (Boiling)\n107.7\n14\n7\n9\n694\n1-2\nMatter\nChanges of State (Melting)\n78.6\n14\n7\n9\n427\n1-3\nMatter\nChanges of State (Freezing)\n88.9\n14\n7\n9\n469\n1-4\nMatter\nChanges of State (Any)\n75.2\n14\n7\n9\n344\n2-1\nMeasurement\nUse Thermometer\n21.4\n270\n10\n10\n4278\n2-2\nMeasurement\nMeasuring Boiling Point (known)\n35.2\n218\n10\n10\n6511\n2-3\nMeasurement\nMeasuring Boiling Point (unknown)\n65\n150\n10\n10\n9768\n3-1\nElectricity\nCreate a circuit\n13.6\n10\n5\n5\n94\n3-2\nElectricity\nRenewable vs Non-renewable Energy\n20.8\n10\n5\n5\n169\n3-3\nElectricity\nTest Conductivity (known)\n25.6\n48\n10\n10\n1341\n3-4\nElectricity\nTest Conductivity (unknown)\n29\n300\n10\n10\n6974\n4-1\nClassification\nFind a living thing\n14.6\n150\n10\n10\n1606\n4-2\nClassification\nFind a non-living thing\n8.8\n150\n10\n10\n756\n4-3\nClassification\nFind a plant\n12.6\n150\n10\n10\n1458\n4-4\nClassification\nFind an animal\n14.6\n150\n10\n10\n1606\n5-1\nBiology\nGrow a plant\n69.5\n62\n10\n10\n3675\n5-2\nBiology\nGrow a fruit\n79.6\n62\n10\n10\n4283\n6-1\nChemistry\nMixing (generic)\n33.6\n16\n8\n8\n347\n6-2\nChemistry\nMixing paints (secondary colours)\n15.1\n18\n9\n9\n224\n6-3\nChemistry\nMixing paints (tertiary colours)\n23\n18\n9\n9\n350\n7-1\nBiology\nIdentify longest-lived animal\n7\n62\n10\n10\n298\n7-2\nBiology\nIdentify shortest-lived animal\n7\n62\n10\n10\n298\n7-3\nBiology\nIdentify longest-then-shortest-lived animal\n8\n62\n10\n10\n360\n8-1\nBiology\nIdentify life stages (plant)\n40\n6\n3\n5\n165\n8-2\nBiology\nIdentify life stages (animal)\n16.3\n4\n2\n4\n31\n9-1\nForces\nInclined Planes (determine angle)\n97\n24\n10\n10\n2733\n9-2\nForces\nFriction (known surfaces)\n84.9\n26\n10\n9\n3644\n9-3\nForces\nFriction (unknown surfaces)\n123.1\n23\n10\n10\n3284\n10-1\nBiology\nMendelian Genetics (known plants)\n130.1\n26\n10\n10\n3043\n10-2\nBiology\nMendelian Genetics (unknown plants)\n132.1\n24\n10\n10\n2853\nShort (0 < *Len \u2264 20)\n11.76\n81.80\n8.6\n8.80\n673.10\nMedium (20 < *Len \u2264 50)\n28.58\n110.75\n8.13\n8.38\n2516.88\nLong (*Len > 50)\n94.30\n37.75\n9.00\n9.58\n2934.75\nOverall (avg)\n49.26\n71.90\n8.63\n9\n2069.43\nOverall (sum)\nN/A\n2,157\n259\n270\n62,083\nTable 2: The statistics of ScienceWorld benchmark. *Len is the average length of the oracle agent\u2019s\ntrajectories. We show the number of our down-sampled variations in each split. The last column is\nthe number of data points forr action-prediction seq2seq task in training SWIFT.\n14\nFigure 4: The SwiftSage framework explained with pseudocode.\nB\nImplementation Details\nB.1\nTraining Details of SWIFT\nWe utilized flan-t5-large (770m) as the base model and fine-tuned it using the seq2seq action-\nprediction data (62k) as previously described. A learning rate of 1e-4 and batch size of 128 were\nemployed for training 500 steps, selected based on dev loss. Although we experimented with larger\nsizes of flan-t5 models, we observed only marginal improvements at a much higher training cost.\nWe believe this is because the language used to describe the environment and actions covers a small\nvocabulary, and the language complexity does not warrant the use of more parameters.\nB.2\nPrompting in SAGE\nIn Section 3.3, we provided an overview of the two-stage prompting framework: planning and\ngrounding. In this section, we delve into further details of each stage.\nMemory augmentation.\nSince the agent can only perceive objects in its current environment\nlocation, objects from previously visited locations are not displayed unless a prior \u2018look around\u2019\naction has been executed. To augment memory for LLMs during planning and grounding, we also\npresent the objects observed in previously visited locations. Additionally, we include the agent\u2019s\nlocation during each action in the action history, e.g., \u201cpick up metal pot [location: kitchen],\u201d to\nfacilitate spatial reasoning for LLMs.\nConnecting the two stages.\nWe conveniently reuse the LLM output from the first stage (i.e.,\nanswers to Q1-Q5) as part of the input for the second stage. Our experiments involve using answers\nto all questions in the grounding stage. However, one can opt to use only answers to Q4 and Q5 to\nreduce computational costs. Our small-scale ablation study indicates that incorporating answers to\nQ1-Q3 in the grounding stage proves beneficial, yielding a gain of about 2 points for short tasks on\naverage.\nGrounding with action templates.\nWe previously introduced an action template, \u2018POUR(object A,\nobject B)\u2019, in Figure 2. Here, we present several additional templates to further illustrate the concept:\n15\nTELEPORT(room) : directly go to a room such as TELEPORT(kitchen)\nPICK(object) : pick up an object and put it into your inventory\nOPEN(object) : open an object to search or put things in it, e.g., OPEN(freezer).\nACTIVATE(object) : activate / turn on an object such as sink or stove, so that you can use it.\nDEACTIVATE(object) : deactivate / turn off the object\nEXAMINE(object) : look at an object carefully. For example, EXAMINE(light bulb).\nMOVE(object, place) : move/place the object to a place\nIt should be noted that despite explicitly instructing the LLM to only utilize permitted action types, it\nmay occasionally generate actions of disallowed types that cannot be parsed. These invalid actions\nwill be disregarded in the action buffer, and if necessary, the system will revert to the SWIFT mode.\nB.3\nAction Buffer\nIn Section 3.4, we presented four conditions for activating the SAGE module. To detect critical\ndecisions (Condition 3), we primarily focus on the \u2018focus on\u2019 actions, as many tasks in ScienceWorld\nnecessitate agents to concentrate on the correct substances and objects in the proper sequence. A\nsingle incorrect \u2018focus on\u2019 action can terminate the entire run. Thus, we restrict the SWIFT module\nfrom performing such actions if SAGE has not yet been activated.\nFor identifying exceptions (Condition 4), we examine phrases like \u201cNo known action can match,\u201d \u201c...\ncannot/doesn\u2019t...,\u201d and so on. When processing an action buffer, we attempt to execute each action\nsequentially. If two consecutive actions are invalid or cause exceptions, we halt and revert to SWIFT.\nC\nAdditional Results and Analysis\nC.1\nSensitivity to LLMs: GPT-3.5-turbo vs GPT-4\nBesides the empirical results in Table 1, we also evaluate performance using GPT-3.5-turbo instead\nof GPT-4, which is considerably larger and more expensive. Other methods exhibit a significant\nperformance decline, for instance, ReAct\u2019s score drops from 36.43 to 19.76, which is close to non-\nLLM methods and even lower than the vanilla method, SayCan. In contrast, SWIFTSAGE maintains a\nrespectable performance of 62.22, indicating better robustness.\nAs discussed in Sec. 5 (limitations), we plan to utilize other open-source LLMs, such as Alpaca,\nand investigate distilling the planning ability from closed-source LLMs to open-source and smaller\nLMs. Nevertheless, a practical challenge arises due to the current open-source LLMs having more\nrestrictive length limits for inputs and outputs.\nC.2\nEfficiency Analysis\nFigure 5 illustrates that most of SWIFTSAGE\u2019s curves are situated near the top-left corner, indicating\nthat SWIFTSAGE attains higher scores than oracle agents at a faster rate. Although ReAct is\ncompetitive with our method for shorter tasks, its trajectories typically plateau at intermediate scores\nand do not reach 100. While the ORACLE agent consistently achieves a perfect score (100.0), its\nefficiency, particularly in longer tasks, is often outperformed by SWIFTSAGE.\nC.3\nCost-effectiveness\nTable 4 presents a comprehensive analysis of the cost-effectiveness of LLM-based methods. We\nexamine two specific metrics: tokens per action (tpa) and scores per action (spa) for SayCan, ReAct,\nReflexion, and SWIFTSAGE across all tasks. Despite SAGE invoking LLM APIs twice for inference,\nits overall cost remains lower, as the result is a sequence of actions typically containing about 5\nactions. In contrast, SAYCAN and REACT require 1,855.84 and 1,971.03 tokens per action (tpa)\nrespectively, while REFLEXION necessitates 2,983.46 tpa. SWIFTSAGE, however, only uses 757.07\ntpa. Given its superior performance, SWIFTSAGE proves to be more cost-effective than other LLM-\nbased methods. This efficiency primarily stems from invoking LLMs only when necessary (thanks to\nour robust SWIFT module) and the action buffer mechanism.\n16\nTask Type\nSwift-Only\nSayCanChatGPT\nReActChatGPT\nReflexionChatGPT\nSwiftSageChatGPT\n1-1\n15.0\n0.0\n0.4\n0.7\n58.0\n1-2\n24.4\n0.0\n8.5\n0.0\n58.5\n1-3\n32.2\n0.0\n1.1\n0.0\n38.5\n1-4\n57.4\n0.0\n0.6\n0.0\n62.5\n2-1\n9.4\n1.7\n4.1\n2.2\n47.9\n2-2\n6.7\n14.1\n7.2\n2.5\n53.3\n2-3\n5.7\n93.7\n19.6\n14.7\n48.6\n3-1\n70.0\n19.3\n16.7\n20.0\n72.7\n3-2\n48.3\n8.7\n9.7\n8.6\n50.3\n3-3\n59.5\n22.0\n55.5\n6.4\n66.9\n3-4\n69.0\n36.4\n36.4\n30.1\n78.1\n4-1\n100.0\n11.7\n17.5\n46.5\n100.0\n4-2\n100.0\n76.0\n73.3\n68.2\n97.5\n4-3\n94.4\n11.4\n20.0\n19.9\n58.3\n4-4\n100.0\n9.5\n15.8\n41.0\n100.0\n5-1\n13.4\n11.3\n11.1\n5.8\n57.5\n5-2\n44.6\n75.0\n18.8\n47.6\n50.9\n6-1\n26.2\n13.5\n35.0\n22.4\n43.2\n6-2\n53.3\n25.0\n20.0\n10.0\n63.3\n6-3\n11.1\n58.4\n16.7\n40.0\n27.4\n7-1\n83.3\n75.0\n37.5\n75.0\n75.0\n7-2\n100.0\n100.0\n50.0\n75.0\n60.0\n7-3\n77.8\n31.7\n31.7\n28.1\n68.3\n8-1\n33.0\n5.6\n4.2\n2.8\n75.6\n8-2\n8.0\n12.8\n7.0\n8.2\n33.0\n9-1\n73.3\n38.0\n28.5\n100.0\n54.0\n9-2\n73.3\n4.2\n10.0\n17.5\n63.3\n9-3\n53.3\n0.0\n0.0\n1.7\n77.0\n10-1\n17.0\n1.3\n24.5\n1.3\n76.0\n10-2\n17.0\n0.3\n11.7\n6.0\n51.1\nShort\n78.68\n37.24\n28.95\n39.19\n72.81\nMedium\n32.90\n20.06\n21.09\n14.37\n55.34\nLong\n35.55\n18.66\n11.23\n16.27\n57.99\nOverall\n49.22\n25.22\n19.76\n23.40\n62.22\nTable 3: Additional results on the ScienceWorld benchmark. Different from Table 1, we use\ngpt-3.5-turbo instead of gpt-4 as the LLM for evaluating SayCan, ReAct, Relfexion, and our\nSWIFTSAGE. We also present the results of using SWIFT module only.\nInterestingly, we observe that SWIFTSAGE has an even lower tpa for long tasks compared to its tpa in\nmedium and short tasks. Upon further investigation, we attribute this finding to longer action buffers\nand the SWIFT module being more frequently effective. Additionally, regarding scores per action\n(spa), we discover that our SWIFTSAGE is more cost-effective by utilizing fewer tokens and achieving\nhigher scores.\n17\nFigure 5: An overview of visualizing trajectories of SWIFTSAGE, REACT and ORACLE. X: time\nsteps (0 \u2192 T); Y : scores (0 \u2192 100). Similar to Figure 3, each curve is a single trajectory by an\nagent in performing a task variation. A more efficient agent will achieve higher scores in a shorter\ntime, resulting in curves positioned near the top-left corner.\nAverage number of tokens per action (tpa)\nAverage scores per action (spa)\nTask Type\nSayCan\nReAct\nReflexion\nSwiftSage\nSayCan\nReAct\nReflexion\nSwiftSage\n1-1\n1944.94\n1503.60\n2632.97\n528.17\n0.10\n0.05\n0.00\n1.49\n1-2\n1125.76\n1339.39\n3066.70\n545.34\n0.08\n0.13\n0.01\n1.64\n1-3\n1034.33\n1268.23\n3307.30\n550.17\n0.04\n0.13\n0.00\n0.71\n1-4\n1295.03\n1251.45\n2439.34\n754.05\n0.00\n0.09\n0.00\n1.69\n2-1\n1188.46\n1545.03\n1988.59\n494.52\n0.13\n0.06\n0.00\n3.01\n2-2\n1862.11\n1181.88\n1596.03\n394.29\n0.03\n0.25\n0.15\n2.32\n2-3\n939.17\n1358.33\n1753.17\n574.05\n0.73\n0.93\n0.12\n1.71\n3-1\n1713.64\n1846.91\n2677.89\n807.62\n0.49\n0.44\n0.04\n0.49\n3-2\n1785.01\n1754.14\n2337.02\n823.28\n0.21\n0.22\n0.01\n0.31\n3-3\n1762.13\n2441.79\n2262.39\n220.80\n0.53\n0.18\n0.10\n0.84\n3-4\n1698.85\n1195.59\n2859.30\n287.25\n0.18\n1.93\n0.10\n1.13\n4-1\n411.08\n579.70\n1053.57\n309.14\n1.91\n1.16\n0.34\n4.76\n4-2\n1332.83\n1098.69\n1250.37\n298.48\n0.20\n0.69\n0.47\n4.76\n4-3\n1155.99\n1314.74\n2966.82\n406.17\n0.08\n0.39\n0.02\n3.82\n4-4\n1126.67\n591.15\n1003.18\n309.71\n0.24\n1.02\n0.79\n4.76\n5-1\n2323.43\n2620.66\n5091.49\n168.95\n0.02\n0.02\n0.00\n0.22\n5-2\n2646.50\n2575.11\n5864.93\n536.56\n0.03\n0.13\n0.00\n0.75\n6-1\n1454.65\n1802.62\n2344.90\n1388.89\n0.28\n0.25\n0.04\n0.37\n6-2\n2413.99\n2763.66\n4342.07\n402.50\n0.18\n0.14\n0.02\n3.33\n6-3\n1371.50\n2860.68\n4551.96\n6361.79\n0.09\n0.10\n0.00\n0.59\n7-1\n376.50\n495.83\n813.08\n768.63\n5.71\n3.33\n0.77\n11.88\n7-2\n424.53\n478.09\n1180.58\n772.00\n2.11\n6.14\n0.36\n10.63\n7-3\n424.73\n564.69\n1175.35\n609.73\n4.55\n3.85\n0.24\n8.48\n8-1\n1505.39\n1155.71\n2466.59\n249.38\n0.21\n0.79\n0.00\n2.23\n8-2\n3189.80\n741.71\n2886.09\n2479.00\n0.12\n0.47\n0.02\n4.03\n9-1\n2066.06\n2642.79\n2652.56\n307.30\n0.16\n0.14\n0.08\n1.06\n9-2\n2517.48\n3031.95\n3606.60\n314.19\n0.11\n0.14\n0.05\n0.66\n9-3\n7002.72\n7507.00\n7785.29\n366.06\n0.04\n0.18\n0.04\n0.65\n10-1\n3612.33\n4218.44\n4822.97\n466.21\n0.32\n0.36\n0.13\n1.78\n10-2\n3969.62\n5401.37\n6724.81\n218.00\n0.52\n0.10\n0.01\n1.69\nShort\n1256.98\n1047.52\n1934.90\n716.30\n1.56\n1.76\n0.31\n5.69\nMedium\n1578.51\n1742.18\n2550.85\n1277.52\n0.21\n0.47\n0.05\n1.35\nLong\n2539.78\n2893.19\n4145.68\n444.09\n0.18\n0.20\n0.04\n1.17\nOverall\n1855.84\n1971.03\n2983.46\n757.07\n0.65\n0.79\n0.13\n2.73\nTable 4: Cost-effectiveness analysis for LLM-based methods.\n18\n"
  },
  {
    "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
    "link": "https://arxiv.org/pdf/2305.17359.pdf",
    "upvote": "1",
    "text": "Preprint\nDNA-GPT: DIVERGENT N-GRAM ANALYSIS FOR\nTRAINING-FREE DETECTION OF GPT-GENERATED\nTEXT\nXianjun Yang1\nWei Cheng2\nYue Wu3\nLinda Petzold1\nWilliam Yang Wang 1\nHaifeng Chen2\n1University of California, Santa Barbara\n2NEC Laboratories America\n3University of California, Los Angeles\n1{xianjunyang, petzold, wangwilliamyang}@ucsb.edu\n2{weicheng, haifeng}@nec-labs.comn\n3 ywu@cs.ucla.edu\nABSTRACT\nLarge language models (LLMs) have notably enhanced the fluency and diversity of\nmachine-generated text. However, this progress also presents a significant challenge\nin detecting the origin of a given text, and current research on detection methods\nlags behind the rapid evolution of LLMs. Conventional training-based methods\nhave limitations in flexibility, particularly when adapting to new domains, and they\noften lack explanatory power. To address this gap, we propose a novel training-free\ndetection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text,\nwe first truncate it in the middle and then use only the preceding portion as input to\nthe LLMs to regenerate the new remaining parts. By analyzing the differences be-\ntween the original and new remaining parts through N-gram analysis in black-box\nor probability divergence in white-box, we unveil significant discrepancies between\nthe distribution of machine-generated text and the distribution of human-written\ntext. We conducted extensive experiments on the most advanced LLMs from Ope-\nnAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well\nas open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show\nthat our zero-shot approach exhibits state-of-the-art performance in distinguishing\nbetween human and GPT-generated text on four English and one German dataset,\noutperforming OpenAI\u2019s own classifier, which is trained on millions of text. Addi-\ntionally, our methods provide reasonable explanations and evidence to support our\nclaim, which is a unique feature of explainable detection. Our method is also robust\nunder the revised text attack and can additionally solve model sourcing. Codes are\navailable at https://github.com/Xianjun-Yang/DNA-GPT\n1\nINTRODUCTION\nThe release of ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023b) by OpenAI has sparked\nglobal discussions on the effective utilization of AI-assistant writing. Despite the success, they have\nalso given rise to various challenges such as fake news (Zellers et al., 2019) and technology-aided\nplagiarism (Bommasani et al., 2021). There have been instances where AI-generated scientific\nabstracts have managed to deceive scientists (Gao et al., 2022; Else, 2023), leading to a disruption in\ntrust towards scientific knowledge. Unfortunately, the progress in detecting AI-generated text lags\nbehind the rapid advancement of AI itself.\nAs AI-generated text approaches high quality, effectively detecting such text presents fundamental\ndifficulties. This has led to a recent debate on the detectability of AI-generated text (Chakraborty\net al., 2023; Krishna et al., 2023; Sadasivan et al., 2023). Nevertheless, there is still a lack of practical\nmethodology for AI-generated text detection, particularly in the era of ChatGPT. We aim to present a\ngeneral, explainable, and robust detection method for LLMs, especially as these models continue to\nimprove. Some existing detection methods utilize perturbation-based approaches like DetectGPT\n1\narXiv:2305.17359v2  [cs.CL]  4 Oct 2023\nPreprint\n(Mitchell et al., 2023) or rank/entropy-based methods (Gehrmann et al., 2019; Solaiman et al., 2019;\nIppolito et al., 2020). However, these detection tools fail when the token probability is not provided,\nas is the case with the OpenAI\u2019s GPT-3.5 series. Furthermore, the lack of details about how those\nmost potent language models are developed poses an additional challenge in detecting them. This\nchallenge will continue to escalate as these LLMs undergo continuous updates and advancements.\nHence, there is a pressing demand to effectively detect GPT-generated text to match the rapid\nadvancements of LLMs. Moreover, when formulating the detection methodology, an essential focus\nlies on explainability, an aspect that is often absent in existing methods that solely provide a prediction\ndevoid of supporting evidence. This aspect holds significant importance, especially in education, as it\nposes challenges for educators in comprehending the rationale behind specific decisions.\nIn this study, we address two scenarios in Figure 1: 1) White-box detection, where access to the model\noutput token probability is available, and 2) Black-box detection, where such access is unavailable.\nOur methodology builds upon the following empirical observation:\nGiven appropriate preceding text, LLMs tend to output highly similar text across\nmultiple runs of generations.\nOn the contrary, given the same preceding text, the remaining human-written text tends to follow a\nmore diverse distribution. We hypothesize that this discrepancy in text distribution originates from the\nmachine\u2019s generation criterion (see Section 3), and further analyze the implication of this hypothesis.\nTo sum up, our contributions are as follows:\n1). We identify a noteworthy phenomenon that the distribution of machine-generated text and that of\nhuman-generated text are particularly different when given a preceding text. We provide a theoretical\nhypothesis as an attempt to explain this observation and corroborate it with extensive experiments.\n2). Based on the observation, we develop zero-shot detection algorithms for LLM-generated texts in\nboth black-box and white-box settings. We validate the effectiveness of our algorithm against the\nmost advanced LLMs on various datasets.\n3). Our algorithm has shown superior performance advantages against learning-based baselines. The\nalgorithm is performant on non-English text, robust against revised text attacks, and capable of model\nsourcing.\n2\nRELATED WORK\nLarge Language Models. The recent development of large foundation language models (LLMs)\n(Bommasani et al., 2021) has revolutionized the field of natural language processing. The LLMs are\nusually built on the transformer (Vaswani et al., 2017) architecture, including encoder-only BERT\n(Devlin et al., 2019), encoder plus decoder T5 (Raffel et al., 2020), and decoder-only GPT-2 (Radford\net al., 2019). The success of instruction-tuned GPT-3 (Brown et al., 2020; Ouyang et al., 2022) and\nChatGPT (Schulman et al., 2022) has garnered attention for the zero-shot ability of GPT to generate\ntext that is of high quality and often indistinguishable from human-written content, such as Google\u2019s\nLaMDA (Thoppilan et al., 2022), Meta\u2019s OPT (Zhang et al., 2022), LLaMa (Touvron et al., 2023) and\nOpenAI\u2019s GPT-4 (OpenAI, 2023b). Those models are typically trained on vast amounts of text, and\nduring generation, beam search is widely used in conjunction with top-k sampling (Fan et al., 2018)\nand nucleus sampling (Holtzman et al., 2020). Despite being powerful, the growing prevalence of\nLLMs has raised various ethical concerns, including fake news (Zellers et al., 2019) and homework\nplagiarism (Stokel-Walker, 2022). This has led to increased interest in developing effective methods\nfor detecting AI-generated text (Chen et al., 2023; Zhan et al., 2023; Li et al., 2023b; Yu et al., 2023b;\nVerma et al., 2023; Wu et al., 2023; Wang et al., 2023b) or online chatbot (Wang et al., 2023a).\nDetecting AI-generated Text. The earlier work on detection focused on feature-based methods,\nincluding the frequency of rare bigrams (Grechnikov et al., 2009), n-gram frequencies (Badaskar\net al., 2008), or top-k words in GLTR (Gehrmann et al., 2019). As the text generated by machine\ncontinues to improve, many trained-based methods are proposed, such as OpenAI Text Classifier\n(OpenAI, 2023a), GPTZero (Tian, 2023). However, the detector has to be trained periodically\nto catch up with the release of new LLMs updates. Another category falls into the training-free\nparadigm, and DetectGPT (Mitchell et al., 2023) is a zero-shot method that utilizes the observation\n2\nPreprint\nStep-1 Truncated input \ud835\udc65\u2032: Yes, The scale of analysis can impact the the identification of racial disparities in breast cancer \u22ef. In \ncontrast, smaller-sca |cut off \nle analyses that focus on specific neighborhoods or regions may reveal \u22ef cancer mortality.\nStep-2 Regeneration: Truncated input \ud835\udc65\u2032\n\ud835\udc66!\n\ud835\udc66\"\n\ud835\udc66#\n\ud835\udc66$\n\u22ee\n\ud835\udc66! = \u2032\u2032le analyses that focus on speci \u22ef cancer mortality. \u2032\u2032\nStep-3 Detection: two independent methods\nBlack-box Detection: BScore =\n!\n$ \u2211%&!\n$\n\u2211'&'!\n(\n\ud835\udc5b log \ud835\udc5b\n\u2211\"#$%&\u2208() *+,'-%$*+,(/012&)\n\u2211\"#$%&\u2208(! *+,'-(/012&)\n>\u2208\nWhite-box Detection: WScore = \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43(\ud835\udc664|\ud835\udc65\u2032) \u2212\n!\n$ \u2211%&!\n$\n\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43(\ud835\udc66%|\ud835\udc65\u2032))\n>\u2208\nY\nN\n\ud835\udc65 from AI\n\ud835\udc65 from Human\ud83d\ude0a\n\ud83e\udd16\nQuestion: Identification of racial disparities in breast cancer mortality: does scale matter?\nCandidate \ud835\udc65: Yes, The scale of analysis can impact the the identification of racial disparities in breast cancer \u22ef. In \ncontrast, smaller-scale analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent\nin larger-scale analyses. Therefore, it is important to consider the scale of analysis when studying racial disparities in breast cancer mortality.\n\ud83e\udd14\ud83d\udcad\nAI or Human?\nOr\n\u2753\n\u2702\nEvidence:\n\ud835\udc66!: le analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent in larger-scale analyses. Therefore \u22ef cancer mortality.\n\ud835\udc66\": le analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent in larger-scale analyses. Additionally \u22ef these disparities.\n\ud835\udc66#: \u22ef communities or neighborhoods may reveal disparities that are not apparent in \u22ef. Therefore, it is important to consider the scale of analysis when evaluating \u22ef. \n\ud835\udc66\"#: le analyses that focus on specific neighborhoods or regions may reveal disparities that are not apparent in larger-scale analyses. It \u22ef reduce these disparities.\nDNA-GPT: Divergent N-Gram Analysis\n\ud83e\uddec\nFigure 1: Overview of our framework. Given a candidate passage x, we aim to distinguish whether\nit is generated by a certain language model like GPT-3.5-turbo or human. Our method first\ntruncates the original passage by a ratio to obtain the truncated text x\u2032 and remaining text y0, then\nx\u2032 is fed into the language model for generating K new outputs {y1, ..., yK}. Finally, a BScore or\nWScore between the new outputs and y0 is calculated for classifying original candidate x into human\nor AI-generated content. The threshold \u03f5 balances TPR and FPR. This example is taken from the\nPubMedQA dataset.\nthat machine-generated passages occupy regions with clear negative log probability curvature. And\n(Kirchenbauer et al., 2023) developed watermarks for language modeling by adding a green list\nof tokens during sampling. While these methods have demonstrated varying levels of success,\nour proposed DNA-GPT offers a unique and effective way of identifying GPT-generated text by\nexploiting the inherent differences in text continuation patterns between human and AI-generated\ncontent. Compared with the classifier-only detector, our method also provides evidence for detection\nresults and thus is explainable.\n3\nMETHODOLOGY\nTask Definition. Following the same setting as the previous DetectGPT (Mitchell et al., 2023), we\naim to detect whether a given text is generated from a known 1 language model. We formulate the\ndetection as a binary classification task. Given a text sequence S = [s1, ..., sL], where L is the\nsequence length, and a specific language model M like GPT-4, the goal is to classify whether S\nis generated from the machine distribution M or from the human distribution H. In the black-box\nsetting, we only have access to the output text generated by the M given arbitrary input, while in the\nwhite-box setting, we additionally have access to the model output probability p(sl+1|s1:l) for each\ntoken at position l.\nFormally, given a sequence S = [s1, ..., sL], we define a truncate rate \u03b3 for splitting the sequence\ninto two parts: X = [s1, ..., s\u2308\u03b3L\u2309], and Y0 = [s\u2308\u03b3L\u2309+1, ..., sL]. Next, we ask the LLMs to continue\ngenerating the remaining sequences purely based on X, and the generated results are denoted by\nY \u2032 \u223c M(\u00b7|X). In practice, we sample the new results for K times (refer to a principled choice\nof K = \u2126\n\u0000\u03c3 log(1/\u03b4)/\u22062\u0001\nin Appendix A.2) to get a set of sequences \u2126 = {Y1, ..., Yk, ..., YK}.\nOur method is based on the hypothesis that the text generation process M of the machine typically\nmaximizes the log probability function log p(sl+1|s1, s2, . . . , sl) throughout the generation, while\nhumans\u2019 generation process is different. In other words, the thought process of human writing does\nnot simply follow the likelihood maximization criterion. We find that this discrepancy between\nmachine and human is especially enormous when conditioned on the preceding text X, and we state\nthis hypothesis formally as:\n1Refer to Appendix B.4 for unknown source model\n3\nPreprint\nLikelihood-Gap Hypothesis.\nThe expected log-likelihood of the machine generation process M\nhas a positive gap \u2206 > 0 over that of the human generation process H:\nEY \u223cM(\u00b7|X)[log p(Y |X)] \u2212 EY \u223cH(\u00b7|X)[log p(Y |X)] > \u2206.\nThis hypothesis states that, conditioned on the preceding part of the text, the log-likelihood value of\nthe machine-generated remaining text is significantly higher than the human-generated remaining\ntext. This is experimentally evident in Figure 2 that the two probability distributions are apparently\ndistinct. An implication is that\n\u2206 \u2264 EY \u223cM(\u00b7|X)[log p(Y |X)] \u2212 EY \u223cH(\u00b7|X)[log p(Y |X)]\n\u2264 \u2225 log p(\u00b7|X)\u2225\u221e \u00b7 dTV(M, H)\n\u2264 \u2225 log p(\u00b7|X)\u2225\u221e \u00b7\nr\n1\n2dKL(M, H).\nThe second inequality holds due to the definition of the total-variation distance; the third inequality\nholds due to Pinsker\u2019s inequality. When there is no ambiguity, we omit the parenthesis and condition,\ndenote M(\u00b7|X) as M and the same for H.\nTo summarize, this Likelihood-Gap Hypothesis implies that the differ-\nence between the two distributions is significant enough (dTV(M, H)\nor dKL(M, H) is greater than some positive gap). This implies it is al-\nways possible to distinguish between humans and machines (Sadasivan\net al., 2023) based on the insights from the binary hypothesis test and\nLeCam\u2019s lemma (Le Cam, 2012; Wasserman, 2013).\nTo leverage this difference between the distributions, we first need to\nconsider a distance function D(Y, Y \u2032) that measures how close two\npieces of text Y and Y \u2032 are. Here, we provide two candidate distance\nmeasures\u2013the n-gram distance and the relative entropy, as examples\nto tackle the Black-box detection with evidence and the White-box\ndetection cases, respectively.\n3\n2\n1\n0\nValue\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nDensity\nHuman\nMachine\nFigure 2:\nDifference on\ntext-davinci-003 generation\non Reddit prompts.\nThen, we can have a training-free classifier on the similarities between \u2126 and Y0. The classifier will\noutput a scoring value used for classification based on some threshold, which balances the FPR and\nTPR. The overall pipeline is elaborated in Figure 1, and we will dive into the details in the following.\n3.1\nBLACK-BOX DETECTION\nOur main focus is on black-box detection since there is an increasing trend for large tech companies\nlike Google and OpenAI to make the details of their chatbot Bard and ChatGPT close-sourced. In\nreal-world scenarios, users typically can only interact with AI through API and have no access to the\ntoken probability, not to mention the underlying model weights. Thus, in the black-box scenario, we\ndo not rely on any information about the model parameters except for the textual input and outputs.\nArmed with the model outputs \u2126 and Y0, we compare their n-gram similarity to distinguish human-\nand GPT-written text. Based on our assumption, the human-generated Y0 will have a much lower\noverlap with \u2126, compared with GPT-generated text. We define the DNA-GPT BScore:\nBScore(S, \u2126) = 1\nK\nK\nX\nk=1\nN\nX\nn=n0\nf(n)|grams(Yk, n) \u2229 grams(Y0, n)|\n|Yk||grams(Y0, n)|\n,\nwhere grams(S, n) denotes the set of all n-grams in sequence S, f(n) is an empirically chosen\nweight function for different lengths n, and |Yk| is used for length normalization. In practice, we\nset f(n)=n log(n), n0=4, N=25 and find it works well across all datasets and models. More\ncomparisons on parameter sensitivity can be found in Appendix B.\n3.2\nWHITE-BOX DETECTION\nIn the white-box detection, we additionally have access to the model output probabilities on the input\nand the generated tokens, denoted by p(Y |X), while model weights and token probabilities over the\n4\nPreprint\nwhole vocabulary are still unknown. This service is supported by OpenAI\u2019s text-davinci-003\nbut is no longer supported since the GPT-3.5 series. Inspired by the assumption of the unique\nprobability curve, we can also calculate a DNA-GPT WScore between \u2126 and Y0:\nWScore(S, \u2126) = 1\nK\nK\nX\nk=1\nlog p(Y0|X)\np(Yk|X).\nIn both the black-box and white-box settings, two parameters play critical roles in determining the\ndetection accuracy: the truncation ratio \u03b3 and the number of re-prompting iterations K.\n3.3\nEVIDENCE\nOne additional benefit of our black-box method is that it provides an interpretation of our detection\nresults, instead of only Yes or No answers. We define the evidence En as the overlapped n-grams\nbetween each re-generated text Yk \u2208 \u2126 and Y0.\nEn =\nK\n[\nk=1\n\u0000grams(Yk, n) \u2229 grams(Y0, n)\n\u0001\n.\nWhen n is large, En serves as strong evidence for AI-generated text since it is less likely for a\nhuman to write exactly the same piece of text as the machine. It is important to note that despite\nsubstantial evidence, there remains a possibility of misclassification. We highly recommend utilizing\nthe evidence in a flexible manner, particularly when evaluating student plagiarism. Defining the\nprecise boundaries of what constitutes plagiarism is a complex matter, and we defer more exploration\nto future research endeavors.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nFive Datasets. Previous research (Carlini et al., 2021) found that LM can memorize training data,\nmaking detection meaningless. We elaborate more in Appendix B.2.1. To prevent LLMs from\nverbatim copying from training data, we collected two newest datasets. One is the Reddit long-form\nquestion-answer dataset from the ELI5 community (Fan et al., 2019)2. We filtered the data based\non physics and biology flairs, focusing on the period from January 2022 to March 20233. We also\nacquired scientific abstracts published on the Nature website on April 23, 2023, and performed our\nexperiments on the same day to minimize the possibility of OpenAI utilizing the data for model\nupdates. Additionally, we use PubMedQA (Jin et al., 2019), Xsum (Narayan et al., 2018), and the\nEnglish and German splits of WMT16 (Bojar et al., 2016) following (Mitchell et al., 2023). See more\nin Appendix B.3.\nFive Models. First, we include the three most advanced LLMs from OpenAI API 4: GPT-3\n(text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (gpt-4-0314). Among\nthese, only text-davinci-003 provides access to the top-5 token probability. Notably, the\ngpt-3.5-turbo model is frequently updated by the OpenAI team, while gpt-4-0314 remains\nfrozen during our testing. As the gpt-3.5-turbo model tends to demonstrate increased result\ninconsistency over time due to these updates, our objective is to assess its detection capability under\nsuch evolving circumstances. In addition to the closed models from OpenAI, we also incorporate two\nopen-sourced language models based on the GPT architecture: LLaMa-13B (Touvron et al., 2023)\nand GPT-NeoX-20B (Black et al., 2022). Unless explicitly stated, we employ a temperature of 0.7 to\nstrike a balance between text diversity and quality for all five models, as has been done in previous\n2https://www.reddit.com/r/explainlikeimfive/\n3Although OpenAI (OpenAI, 2023b) claimed training data is truncated up to September 2021, their model\nmay encounter data beyond this date during alignment, our filtering reduces the potential for cheating as OpenAI\nhas not disclosed its data usage specifics.\n4https://platform.openai.com/docs/api-reference\n5\nPreprint\nresearch (Krishna et al., 2023). All other parameters remain at their default values, with the exception\nof a maximum token length of 300.\nTwo Metrics. Previous studies (Mitchell et al., 2023; Sadasivan et al., 2023) have primarily focused\non utilizing the Area Under The ROC Curve (AUROC) score for evaluating detection algorithm\neffectiveness. However, our research indicates that this metric may not always offer an accurate\nassessment, particularly when the AUROC score approaches the ideal upper bound of 1.0. Notably,\ntwo detectors with an identical AUROC score of 0.99 can demonstrate significant disparities in user\nexperience in terms of detection quality. To ensure the reliability of detection methods for real-life\ndeployment, it is crucial to maintain a high TPR while minimizing the FPR. Therefore, we also\npresent TPR scores at a fixed 1% FPR, as in (Krishna et al., 2023). Additional metrics such as F1 and\naccuracy can be found in Appendix C.\nTwo Algorithms. For models like GPT-3.5 and GPT-4 without disclosing any token probability,\nwe employ the black-box detection algorithm and solely provide results based on BScore. Conversely,\nfor text-davinci-003, GPT-NeoX-20B, and LLaMa-13B with access to token probability,\nwe could additionally provide white-box detection results using WScore.\nThree Baselines. We consider two strong supervised training-based baselines: GPTZero (Tian, 2023)\nand OpenAI\u2019s classifier (OpenAI, 2023a). Although detailed information about the internal workings\nof these classifiers is not provided, certain key aspects have been disclosed. GPTZero is trained to\nassess perplexity and burstiness in text, enabling it to distinguish between artificially generated and\nhuman-crafted content. On the other hand, OpenAI\u2019s classifier is fine-tuned from a collection of\n34 models from five different organizations. We also consider DetectGPT (Mitchell et al., 2023)\nfor text-davinci-003 since it relies on the token probability for detection. Notably, previous\nentropy (Gehrmann et al., 2019) or rank-based algorithms (Solaiman et al., 2019; Ippolito et al., 2020)\nare excluded from comparison as they rely on token probabilities over the whole vocabulary, which is\nnot available in ChatGPT\u2019s era.\nTwo Detection Scenarios. When detecting AI-generated text, two realistic scenarios arise: the\nprompt used for generation is either known or unknown to the detector. For instance, in the case of\nquestions and answers on Reddit, the prompts are typically known. Conversely, when generating fake\nnews, the prompts are usually unknown. In our experiments, we evaluate both scenarios to replicate\nreal-world conditions. Besides, there could be more complicated system prompt and smart prompt\nattacks, and we leave the exploration in Appendix B.\n5\nRESULTS AND ANALYSIS\nOverall Results. The overall results are presented in Table 1. Our zero-shot detector consistently\nachieves superior performance compared to the supervised baselines, namely GPTZero (Tian, 2023)\nand OpenAI\u2019s Classifier (OpenAI, 2023a), in terms of both AUROC and TPR. Notably, our black-\nbox detector exhibits enhanced results when provided with the golden question prompt, although\nintriguingly, optimal performance is sometimes achieved without utilizing a golden prompt. Another\nnoteworthy observation is the significant underperformance of GPTZero, and OpenAI\u2019s Classifier\non outputs generated from our newly curated datasets, namely Reddit-ELI5 and Scientific abstracts,\nin contrast to the established datasets, PubMedQA and Xsum. This disparity can be attributed to\nthe limited training data, highlighting the vulnerability of training-based classifiers. Conversely, our\nDNA-GPT consistently exhibits exceptional performance across both historical and newest datasets.\nAdditionally, our detector excels DetectGPT by a large margin under the white-box setting with even\nfewer costs. It is imperative to acknowledge that a considerable number of technology companies\nhave ceased the disclosure of token probability, rendering this type of white-box detection less\nfeasible from the user\u2019s perspective in actual world situations. Nevertheless, we posit that it remains\nviable for the providers of LLMs service to implement these in-house detection systems on their end.\nTruncation Ratio. The first question to our DNA-GPT pertains to the optimal truncation ratio for\nachieving good performance. In order to address this query, we conducted a series of experiments\nusing two models on three distinct datasets: the Reddit dataset using gpt-3.5-turbo with known\nprompts, PubMedQA using gpt-3.5-turbo without known prompts, and the Xsum dataset using\nLLaMa-13B without golden prompts. Each dataset comprised 150-200 instances. The truncation\nratio \u03b3 was systematically varied across values of {0.02, 0.1, 0.3, 0.5, 0.7, 0.9, 0.98}. The obtained\nresults are illustrated in Figure 3. It becomes evident that the overall detection performance initially\n6\nPreprint\nTable 1: Overall comparison of different methods and datasets. The TPR is calculated at 1% FPR.\nw/o P means the golden prompt is unknown. K in DetectGPT represents the number of perturbations.\nDatasets\nReddit-ELI5\nScientific Abstracts\nPubMedQA\nXsum\nMethod\nAUROC\nTPR\nAUROC\nTPR\nAUROC\nTPR\nAUROC\nTPR\nGPT-4-0314(Black-box)\nGPTZero\n94.50\n36.00\n76.08\n11.10\n87.72\n44.00\n79.59\n36.00\nOpenAI\n71.64\n5.00\n96.05\n73.00\n94.91\n52.00\n77.78\n30.67\nDNA-GPT, K=20, \u03b3=0.7\n99.63\n87.34\n96.72\n67.00\n95.72\n44.50\n91.72\n32.67\nK=10, \u03b3=0.5\n99.34\n91.00\n96.78\n75.00\n96.08\n50.00\n87.72\n30.13\nK=10, \u03b3=0.5, w/o P\n98.76\n84.50\n95.15\n55.00\n91.10\n15.00\n94.11\n12.00\nGPT-3.5-turbo(Black-box)\nGPTZero Tian (2023)\n96.85\n63.00\n88.76\n5.50\n89.68\n40.67\n90.79\n54.67\nOpenAI OpenAI (2023a)\n94.36\n48.50\n99.25\n94.00\n92.80\n34.00\n94.74\n74.00\nDNA-GPT, K=20, \u03b3=0.7\n99.61\n87.50\n98.02\n82.00\n97.08\n51.33\n97.12\n33.33\nK=20, \u03b3=0.5\n97.19\n77.00\n99.65\n91.10\n97.10\n55.33\n94.27\n52.48\nK=10, \u03b3=0.5, w/o P\n96.85\n63.50\n99.56\n95.00\n95.93\n60.00\n96.96\n62.67\ntext-davinci-003(Black-box)\nGPTZero\n95.65\n54.50\n95.87\n0.00\n88.53\n24.00\n83.80\n35.33\nOpenAI\n92.43\n49.50\n98.87\n88.00\n81.28\n24.00\n85.73\n58.67\nDNA-GPT, K=20, \u03b3=0.7\n98.04\n62.50\n97.20\n83.00\n86.90\n21.33\n86.6\n26.00\nK=10, \u03b3=0.5\n98.49\n53.50\n99.34\n89.00\n91.06\n28.67\n97.97\n51.00\nK=10, \u03b3=0.5, w/o P\n96.02\n59.00\n94.19\n68.00\n88.39\n29.33\n96.16\n65.00\ntext-davinci-003(White-box)\nDetectGPT Mitchell et al. (2023), K=20\n54.21\n0.00\n52.12\n0.74\n57.78\n0.67\n77.92\n1.33\nK=100\n58.36\n0.00\n55.45\n0.89\n70.92\n2.38\n82.11\n0.00\nDNA-GPT, K=20, \u03b3=0.7\n99.99\n100.00\n99.65\n92.00\n99.35\n81.76\n98.64\n90.00\nK=10, \u03b3=0.5,\n100.00\n100.00\n99.94\n99.00\n99.87\n96.67\n100.00\n100.00\nK=10, \u03b3=0.5, w/o P\n99.92\n99.50\n99.46\n97.00\n98.06\n89.33\n99.88\n99.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of truncation\n60\n70\n80\n90\n100\nAUROC\nGPT3.5-Reddit_wp\nGPT3.5-PubMedQA_wop\nLLaMa-Xsum_wop\nFigure 3: The impact of truncation ratio.\nTable 2: Five pairs of model sourcing results con-\nducted on Xsum and Reddit datasets.\nModel Sourcing\nSource model\u2192\nGPT-3.5-turbo\nLLaMa-13B\nTarget model\u2193\nAUROC\nTPR\nAUROC\nTPR\nGPT-3.5-turbo\nn/a\nn/a\n99.91\n99.00\nGPT-4-0314\n96.77\n46.00\n99.84\n94.00\nGPT-NeoX-20B\n99.77\n92.55\n86.99\n45.60\nexhibits an upward trend, followed by a subsequent decline. Intuitively, when presented with a very\nbrief prompt, the model possesses a greater degree of freedom to generate diverse text. Conversely,\nimposing severe constraints by incorporating almost the entire original text severely restricts the space\nfor text generation. Consequently, the most favorable truncation ratio is expected to fall within the\nmiddle range. Our investigations revealed that a truncation ratio of 0.5 consistently yielded favorable\noutcomes across all considered models and datasets. Notice that this might be unsuitable for a longer\ntext that starts with AI-generated prefix text and is followed by human-written text, and we leave our\nsliding window solution in Appendix B.\nNumber of Re-generations. To investigate the optimal number of re-generations to achieve sat-\nisfactory detection results, a series of experiments were conducted on four distinct datasets. The\nresults are visualized in Figure 4. In terms of the AUROC score, it is evident that employing either\n10(black-box) or 5(white-box) re-prompting instances is sufficient to reach a saturation point. On the\nother hand, the TPR metric exhibits continuous improvement until approximately five re-generations,\nregardless of whether the black-box or white-box setting is utilized. Considering the costs of invoking\nOpenAI\u2019s API, we assert that a range of 5-10 re-generations represents a reasonable choice to ensure\ndesired performance. This is supported by our theoretical analysis in Appendix A.2 that a larger K\nleads to better detectability.\nDecoding Temperature. Temperature5 T controls the randomness during generation to trade off\ntext quality and diversity (Naeem et al., 2020). In general, higher T will make the output more\n5https://platform.openai.com/docs/api-reference/chat/create#chat/\ncreate-temperature\n7\nPreprint\n5\n10\n15\n20\n0.92\n0.94\n0.96\n0.98\n1.00\nAUROC\nBlack-box\nScientific Abstract\nPubMedQA\nReddit ELI5\nXSum\n5\n10\n15\n20\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTPR at 1% FPR\nBlack-box\n5\n10\n15\n20\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\nAUROC\nWhite-box\n5\n10\n15\n20\n0.6\n0.7\n0.8\n0.9\n1.0\nTPR at 1% FPR\nWhite-box\nScientific Abstract\nPubMedQA\nReddit ELI5\nXSum\nNumber of regenerations\nFigure 4: A comparative analysis of AUROC scores and TPR (at a 1% FPR) across four datasets,\neach characterized by different numbers of regeneration. The analysis is performed under both black-\nbox and white-box settings, utilizing the gpt-3.5-turbo and text-davinci-003 models,\nrespectively.\nrandom, while lower T will make it more focused and deterministic. To explore how different\nclassifiers work when the temperature varies, we tried a T range of {0.7, 1.0, 1.4, 1.8} on the Reddit\ndataset. However, we discarded T=1.8 since we discovered that it resulted in nonsensical text.\nWe depicted the changes in Figure 5. Surprisingly, we found that training-based methods like\nGPTZero and OpenAI\u2019s classifier drop the performance significantly. Although they both claimed\nto train the detector on millions of texts, no detailed information is disclosed about how they got\nthe GPT-generated text. The results show these methods are very sensitive to the decoding T.\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n60\n70\n80\n90\n100\nAUROC\nOurs\nOpenAI\nGPTZero\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n0\n20\n40\n60\n80\nTPR at 1% FPR\nOurs\nOpenAI\nGPTZero\nDecoding temperature\nFigure 5: The impact of decoding temperature on detection\nperformance, conducted using gpt-3.5-turbo.\nBut ours consistently outperforms\nthose two baselines, although also\ndemonstrating a drop in AUROC\nand more decrease in TPR.\nRevised Text. In practical applica-\ntions, AI-generated text often un-\ndergoes revision either by another\nlanguage model or by human users\nthemselves.\nIn such cases, it is\ncrucial to assess the robustness of\nan AI detector. Taking inspiration\nfrom DetectGPT (Mitchell et al., 2023), who made use of the mask-filling capability of T5-3B (Raffel\net al., 2020), we also simulate human revisions by randomly replacing a fraction of r% of 5-word spans\nin 100 instances from the Reddit dataset answered by GPT-4. and employ the T5-3B model to fill\nin the masks. We experiment with various revision ratios, specifically r%\u2208{0.0, 0.1, 0.2, 0.35, 0.5},\nand present the results in Figure 7. It is evident that GPTZero and OpenAI\u2019s classifier both experience\na slight decline in performance with moderate revision ratios, but their performances dramatically\ndeteriorate when the text is heavily revised (r% > 0.3). In contrast, our proposed method consistently\noutperforms both classifiers and maintains a stable detection performance. Even when approximately\nhalf of the text has been revised, our DNA-GPT shows only a slight drop in AUROC from 99.09 to\n98.48, indicating its robustness in detecting revised text.\nNon-English Detection. Prior detection tools, primarily designed for English, have often overlooked\nthe need for non-English detection. A recent study discovered that many AI classifiers tend to exhibit\nbias against non-native English writers (Liang et al., 2023), which further underscores the importance\nof focusing on other languages. We selected the English and German splits of WMT-2016 to evaluate\nperformance in German and tested our white-box detection on text-davinci-003 and black-box\ndetection on\nGPT-turbo-35. The results are depicted in Figure 6. It is apparent that GPTZero\nperforms poorly, as it is no better than random guessing, suggesting a lack of German training data.\nCompared to OpenAI\u2019s supervised classifier, our zero-shot methods achieve comparable or even\nsuperior results, demonstrating its robustness in non-English text.\nExplainability. One main advantage of our detection method is to provide not only a YES or NO\ndetection decision but also reasonable explanations, as discussed in Sec. 3.3 The explainability\nof detectors can significantly enhance their effectiveness and utility. We illustrate one example of\nevidence in Figure 1. As we can see, out of 20 re-generations, we found three cases where there is a\nlarge portion of identical phases, starting and ending differently, though. Those non-trivial N-gram\n8\nPreprint\ntext-davinci-003\nGPT-turbo-35\n0\n20\n40\n60\n80\n100\nAUROC\n92.79\n90.09\n90.44\n93.98\n12.18\n11.43\nOurs\nOpenAI\nGPTZero\nFigure 6: The comparison of results on German.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRatio of revision\n60\n70\n80\n90\n100\nAUROC\nOurs\nOpenAI\nGPTZero\nFigure 7: The comparison of detection results\nwith varying revision ratios.\nTable 3: Comparison of different classifiers using open-source models. The TPR is calculated at the\nfixed 1% FPR. Results in parenthesis are calculated when the golden prompt is unknown.\nXsum\nClassifier\u2192\nGPTZero\nOpenAI\u2019s Classifier\nDNA-GPT (Black-box)\nDNA-GPT (White-box)\nModels\u2193\nAUROC\nTPR\nAUROC\nTPR\nAUROC\nTPR\nAUROC\nTPR\nGPT-NeoX-20B\n65.59\n22.00\n78.70\n56.67\n90.20(86.57)\n52.67(58.67)\n95.57(92.24)\n66.22(54.05)\nLLaMa-13B\n69.02\n16.67\n73.84\n46.67\n88.87(86.74)\n46.67(44.00)\n84.62(83.20)\n20.00(25.33)\nReddit\nGPT-NeoX-20B\n73.01\n15.00\n78.28\n35.50\n90.49(89.18)\n54.60(49.00)\n98.29(98.41)\n91.50(93.00)\nLLaMa-13B\n84.34\n22.50\n66.74\n16.50\n91.20(89.21)\n54.50(45.00)\n90.35(89.90)\n40.00(35.50)\noverlaps provide strong evidence to support our claim that the candidate text x is written by AI rather\nthan humans. Such explainability is crucial for educators to find evidence of plagiarism, which can\nnot be achieved by a binary classifier like OpenAI\u2019s detector. More complete examples can be found\nin Appendix D due to the space limit.\nOpen-Sourced Models. Despite the proprietary nature of OpenAI\u2019s LLMs, we also evaluate the\neffectiveness of DNA-GPT using two large, open-source language models: GPT-NeoX-20B and\nLLaMa-13B, both employing a transformer decoder-only architecture. We replicate the same\nexperimental setup on the Reddit and Xsum datasets, with results presented in Table 3. We observe\na significant performance degradation on two training-based classifiers across the selected datasets\nand models. This outcome could be attributed to the scarcity of training data from these two models,\nwhich in turn exposes the vulnerabilities of training-based detectors when applied to newly developed\nmodels. Contrarily, our methods consistently outperform baselines across different models and\ncorpora under both black- and white-box settings. Given the continuous release of new models to the\npublic, maintaining classifier robustness towards these emerging models is of paramount importance.\nWe hope our DNA-GPT offers a viable solution to this pressing issue.\nModel Sourcing. Despite distinguishing text from AI or humans, one auxiliary utility of our work is\nthat it can be applied to a novel task that we named Model Sourcing: detection of which model the\ntext is generated from, assuming each model possesses their unique DNA. For example, given the can-\ndidate text and candidate models {GPT-3.5-turbo, LLaMa-13B, GPT-NeoX-20B, GPT-4},\nwe would like to know which model the text most likely comes from. Concurrently, (Li et al., 2023a)\nproposed origin tracking, referring to a similar meaning. Our method works by performing the same\ntruncation-then-regeneration pipeline and ranks the result to identify the model source. For simplicity,\nwe test this idea by using combinations of these candidate models on the Reddit and Xsum datasets,\nas shown in Table 2. Notice that this task can not be solved by previous AI detectors that only\ndistinguish between humans and machines. More broadly, model sourcing can be used when we do\nnot know which model the text might be generated from.\n6\nCONCLUSION\nIn the era of ChatGPT, AI-assisted writing will become even more popular. We demonstrate that\ntraining-based classifiers, although trained on millions of text, are not robust to revision attacks\nand might perform poorly on non-English text. As new models are released frequently, bespoke\n9\nPreprint\ndetectors also can not adapt to outputs from the latest models well and can only provide a decision\nresult without explanation. Our proposed zero-shot detector DNA-GPT overcomes those drawbacks\nwell under both black and white-box scenarios and even achieves significant improvements over\ntraining-based classifiers. Despite being highly effective across various domains, it is also armed with\ngood interpretation by providing explainable evidence.\nETHICS STATEMENT\nAll contributing authors of this paper confirm that they have read and pledged to uphold the ICLR\nCode of Ethics. Our proposed method, DNA-GPT is specifically designed to detect GPT-generated\ntext produced by AI models like ChatGPT. However, we must emphasize that our current evaluation\nhas been limited to language models resembling the GPT decoder-only architecture. We do not claim\nthat our method can reliably detect text generated by non-decoder-only models. Furthermore, our\nmethod is only tested for detecting text within a moderate length range(1000-2000 characters). Texts\nsignificantly shorter or longer than what we have tested may not yield accurate results. It is crucial to\nacknowledge that our method might still make mistakes. Users should exercise caution and assume\ntheir own risk when utilizing our detector. Finally, it is essential to clarify that the detection results\ndo not necessarily reflect the views of the authors.\nREPRODUCIBILITY STATEMENT\nAll specifics regarding the datasets and our experimental configurations can be found in Section 4\nand Appendix.\nREFERENCES\nSameer Badaskar, Sachin Agarwal, and Shilpa Arora. Identifying real or fake articles: Towards\nbetter language modeling. In Proceedings of the Third International Joint Conference on Natural\nLanguage Processing: Volume-II, 2008.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. Challenges & Perspectives in Creating Large Language Models, pp. 95, 2022.\nOnd\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias\nHuck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings\nof the 2016 conference on machine translation. In Proceedings of the First Conference on Machine\nTranslation: Volume 2, Shared Task Papers, pp. 131\u2013198, 2016.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. In USENIX Security Symposium, volume 6, 2021.\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong\nHuang. On the possibilities of ai-generated text detection. arXiv preprint arXiv:2304.04736, 2023.\nYutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Ramakrishnan. Gpt-\nsentinel: Distinguishing human and chatgpt generated content. arXiv preprint arXiv:2305.07969,\n2023.\n10\nPreprint\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 4171\u20134186, 2019.\nHolly Else. Abstracts written by chatgpt fool scientists. Nature, 613:423 \u2013 423, 2023.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 889\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 3558\u20133567, Florence, Italy, July 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\nP19-1346.\nCatherine A Gao, Frederick M Howard, Nikolay S Markov, Emma C Dyer, Siddhi Ramesh, Yuan\nLuo, and Alexander T Pearson. Comparing scientific abstracts generated by chatgpt to original\nabstracts using an artificial intelligence output detector, plagiarism detector, and blinded human\nreviewers. bioRxiv, pp. 2022\u201312, 2022.\nSebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and\nvisualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations, pp. 111\u2013116, 2019.\nEA Grechnikov, GG Gusev, AA Kustarev, and AM Raigorodsky. Detection of artificial texts.\nRCDL2009 Proceedings. Petrozavodsk, pp. 306\u2013308, 2009.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2020.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of\ngenerated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pp. 1808\u20131822, July 2020.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset\nfor biomedical research question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 2567\u20132577, 2019.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A\nwatermark for large language models. arXiv preprint arXiv:2301.10226, 2023.\nKalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphras-\ning evades detectors of ai-generated text, but retrieval is an effective defense. arXiv preprint\narXiv:2303.13408, 2023.\nLucien Le Cam. Asymptotic methods in statistical decision theory. Springer Science & Business\nMedia, 2012.\nLinyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, and Xipeng Qiu. Origin tracing and detecting of\nllms. arXiv preprint arXiv:2304.14072, 2023a.\nYafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang.\nDeepfake text detection in the wild. arXiv preprint arXiv:2305.13242, 2023b.\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. Gpt detectors are biased\nagainst non-native english writers. arXiv preprint arXiv:2304.02819, 2023.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In International Conference on Learning Representations, 2017.\n11\nPreprint\nFatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick.\nSmaller language models are better black-box machine-generated text detectors. arXiv preprint\narXiv:2305.09859, 2023.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn.\nDetectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint\narXiv:2301.11305, 2023.\nSandra Mitrovi\u00b4c, Davide Andreoletti, and Omran Ayoub. Chatgpt or human? detect and explain.\nexplaining decisions of machine learning model for detecting short chatgpt-generated text, 2023.\nMuhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable\nfidelity and diversity metrics for generative models. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119\nof Proceedings of Machine Learning Research, pp. 7176\u20137185. PMLR, 2020. URL http:\n//proceedings.mlr.press/v119/naeem20a.html.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pp. 1797\u20131807, 2018.\nOpenAI. OpenAI Models - GPT3.5, 2022. URL https://platform.openai.com/docs/\nmodels/gpt-3-5.\nOpenAI.\nAI\ntext\nclassifier,\nJan\n2023a.\nURL\nhttps://beta.openai.com/\nai-text-classifier.\nOpenAI. Gpt-4 technical report, 2023b.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi.\nCan ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.\nJ Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, JFC Uribe, L Fedus, L Metz, M Pokorny,\net al. Chatgpt: Optimizing language models for dialogue, 2022.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social\nimpacts of language models. arXiv preprint arXiv:1908.09203, 2019.\nChris Stokel-Walker. Ai bot chatgpt writes smart essays-should academics worry? Nature, 2022.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239, 2022.\nEdward Tian. Gptzero: An ai text detector, 2023. URL https://gptzero.me/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n12\nPreprint\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nVivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten\nby large language models, 2023.\nHong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt imposters\nwith a single question. arXiv preprint arXiv:2305.06424, 2023a.\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi\nWhitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, and Preslav Nakov.\nM4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection,\n2023b.\nLarry Wasserman. Lecture notes for stat 705: Advanced data analysis. https://www.stat.\ncmu.edu/~larry/=stat705/Lecture27.pdf, 2013. Accessed on April 9, 2023.\nKangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. Llmdet: A large language\nmodels detection tool, 2023.\nWeichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, and Shuicheng Yan.\nBag of tricks for training data extraction from language models. arXiv preprint arXiv:2302.04460,\n2023a.\nXiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and\nNenghai Yu. Gpt paternity test: Gpt generated text detection with gpt genetic inheritance. arXiv\npreprint arXiv:2305.12519, 2023b.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. Advances in neural information processing\nsystems, 32, 2019.\nHaolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. G3detector: General\ngpt-generated text detector. arXiv preprint arXiv:2305.12680, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\n13\nPreprint\nAPPENDIX: DNA-GPT\nA\nTHEORETICAL ANALYSIS\nA.1\nIS IT ALWAYS POSSIBLE TO DISTINGUISH BETWEEN AI-GENERATED TEXT AND HUMAN?\nThe recent work exploits the possibility of AI-generated text by analyzing the AUROC for any detector\nD. Armed with the LeCam\u2019s lemma (Le Cam, 2012; Wasserman, 2013) which states that for any\ndistributions M and H, given an observation s, the minimum sum of Type-I and Type-II error\nprobabilities in testing whether s \u223c M versus s \u223c H is equal to 1\u2212dTV(M, H). Here, dTV denotes\nthe total variance between two distributions. Hence, this can be interpreted as :\nTPR\u03b3 \u2264 min{FPR\u03b3 + dTV(M, H), 1},\n(1)\nwhere TPR\u03b3 \u2208 [0, 1]. The upper bound in (1) is leveraged in one of the recent work (Sadasivan et al.,\n2023) to derive AUROC upper bound AUC \u2264 1\n2 + dTV(M, H) \u2212 dTV(M,H)2\n2\nwhich holds for any D.\nThis upper bound led to the claim of impossibility results for reliable detection of AI-Generated\nText when dTV(M, H) is approaching 0. The upper bound in (1) is also interpreted as either certain\npeople\u2019s writing will be detected falsely as AI-generated or the AI-generated text will not be detected\nreliably when dTV(M, H) is small. However, as discussed in Sec. 3, the Likelihood-Gap Hypothesis\nguarantees that the difference between the two distributions is significant enough (dTV(M, H) or\ndKL(M, H) is greater than some positive gap). This implies it is always possible to distinguish\nbetween humans and machines.\nA.2\nPRINCIPLED CHOICE OF K\nIn Sec. 3 , we state a Likelihood-Gap Hypothesis, that is the expected log-likelihood of the machine\ngeneration process M has a positive gap \u2206 > 0 over that of the human generation process H. To\nleverage this difference between the distributions, first consider a distance function D(Y, Y \u2032) that\nmeasures how close two pieces of text Y and Y \u2032 are. The n-gram distance introduced in the black-box\ndetection or the relative entropy in the white-box detection can be seen as two examples. This distance\nfunction D(Y, Y \u2032) can also be seen as a kernel function used in the kernel density estimation.\nVia re-prompting the remaining text, we can measure how close the remaining text Y0 is to the\nmachine text distribution:\nbD(Y0, {Yk}k\u2208[K]) := 1\nK\nK\nX\nk=1\nD(Y0, Yk),\nwhere K is the number of times of re-prompting.\nSimilar to the kernel density estimation, we can use this quantity and some threshold to determine\nwhether to accept or reject that S \u223c M. Under certain assumptions, this estimator enjoys n\u22121/2-\nconsistency via Hoeffding\u2019s argument. In the following, we provide a formal argument.\nAssumption 1. Suppose we have a given human-generated text [X, Y0] \u2208 supp(h) and a machine-\ngenerated remaining text eY0, consider the random variable D(Y0, Y \u2032) and where Y \u2032 is sampled\nby re-prompting given X, that is Y \u2032 \u223c M(\u00b7|X). We assume D(Y0, Y \u2032) and D(eY0, Y \u2032) are \u03c3-sub-\nGaussian. We also assume that the distance gap is significant:\nEY \u2032\u223cM[D(Y0, Y \u2032)|X] \u2212 EY \u2032\u223cM[D(eY0, Y \u2032)|X] > \u2206.\nFrom this assumption, we can derive that it suffices to re-prompt \u2126\n\u0000 \u03c3 log(1/\u03b4)\n\u22062\n\u0001\ntimes.\nProof. Note that E[ bD] = E[D] and the distribution is sub-Gaussian. By Hoeffding\u2019s inequality, we\nhave that with probability at least 1 \u2212 \u03b4,\n\f\f\f\f\n1\nK\nK\nX\nk=1\nD(Y0, Yk) \u2212 EY \u2032\u223cM[D(Y0, Y \u2032)|X]\n\f\f\f\f \u2264\nr\n\u03c3 log(\u03b4/2)\nK\n.\n14\nPreprint\nSimilarly, we have that with probability at least 1 \u2212 \u03b4,\n\f\f\f\f\n1\nK\nK\nX\nk=1\nD(eY0, Yk) \u2212 EY \u2032\u223cM[D(eY0, Y \u2032)|X]\n\f\f\f\f \u2264\nr\n\u03c3 log(\u03b4/2)\nK\n.\nBy the union bound, we have that with probability 1 \u2212 2\u03b4,\n1\nK\nK\nX\nk=1\nD(Y0, Yk) \u2212 1\nK\nK\nX\nk=1\nD(Y0, Yk)\n> 1\nK\nK\nX\nk=1\nD(Y0, Yk) \u2212 EY \u2032\u223cM[D(eY0, Y \u2032)|X] \u2212 1\nK\nK\nX\nk=1\nD(eY0, Yk) + EY \u2032\u223cM[D(eY0, Y \u2032)|X] + \u2206\n\u2265 \u2206 \u2212 2\nr\n\u03c3 log(\u03b4/2)\nK\n.\nIf we set K = \u2126\n\u0000 \u03c3 log(1/\u03b4)\n\u22062\n\u0001\n, then there is a gap between the human\u2019s DNA distance and the\nmachine\u2019s DNA distance.\n15\nPreprint\nB\nADDITIONAL EXPERIMENTAL RESULTS\nB.1\nPROMPTS AND DATASETS\nWe use 200, 200, 150, 200, and 300 instances from Reddit, Scientific Abstracts, PubMedQA, Xsum,\nand WikiText, respectively. The used system and user prompt on different datasets are outlined in\nTable 4 for gpt-3.5-turbo and gpt-4-0314. For other models without the system prompt\ninput, we only use the user prompt.\nTable 4: Examples of prompts used in different datasets.\nDatasets\nPrompts\nReddit\nSystem: You are a helpful assistant that answers the question provided.\nUser: Answer the following question in 180-300 words: Question\nScientific\nAbstracts\nSystem: You are a research scientist. Write one concise and professional abstract\nfollowing the style of Nature Communications journal for the provided paper title.\nUser: Title: title\nPubMedQA\nSystem: You are a helpful assistant that answers the question provided.\nUser: Question\nXsum\nSystem: You are a helpful assistant that continues the sentences provided.\nUser: Complete the following sentences for a total of around 250 words: Prefix\nWikiText\nSystem: You are a helpful assistant that continues the sentences provided.\nUser: Complete the following sentences for a total of around 250 words: Prefix\nB.2\nMODEL MEMORIZATION\nB.2.1\nON THE DATASETS FOR DETECTION\nModel Memorization. Previous research (Carlini et al., 2021) has demonstrated the ability to extract\nTable 5: Overall comparison of different methods on\nWikiText-103 datasets. The TPR is calculated at 1%\nFPR.\nDataset\u2192\nWikiText-103\nMethods\u2193\nAUROC\nTPR\nGPT-4-0314(Black-box)\nGPTZero\n92.00\n0.00\nOpenAI\n82.45\n32.67\nDNA-GPT, K=5, \u03b3=0.7\n90.77\n0.33\nGPT-3.5-turbo(Black-box)\nGPTZero Tian (2023)\n92.67\n0.33\nOpenAI OpenAI (2023a)\n93.45\n55.33\nDNA-GPT, K=20, \u03b3=0.7\n99.63\n93.00\ntext-davinci-003(Black-box)\nGPTZero\n92.67\n0.33\nOpenAI\n95.39\n72.00\nDNA-GPT, K=20, \u03b3=0.7\n94.40\n7.00\ntext-davinci-003(White-box)\nDNA-GPT, K=20, \u03b3=0.7\n96.67\n0.67\nnumerous verbatim text sequences from\nthe training data on LLMs, employing ap-\npropriate prompting techniques. This find-\ning has received further validation through\nrecent work (Yu et al., 2023a), where en-\nhanced strategies for extracting training\ndata are introduced. Consequently, when\nthe generated text is verbatim copying\nof the training data, it becomes indistin-\nguishable from human-written text, render-\ning the distinction between AI-generated\nand human-written text futile. To inves-\ntigate this aspect, we evaluate the widely\nadopted open-end generation WikiText-103\ndataset (Merity et al., 2017), which orig-\ninated from Wikipedia and has been ex-\ntensively utilized for training subsequent\nmodels. Through our experiments, we dis-\ncovered that the text-davinci-003\nmodel tends to memorize the context and\ngenerate text that closely resembles the\noriginal data. Specifically, out of 100 exam-\nples randomly selected from the validation\nsplit, 13 prompt outputs exhibited identical\ncontinuous tokens spanning three consec-\nutive sentences, as detailed in Appendix B.\nThis phenomenon poses a challenge in dis-\ntinguishing these instances as AI-generated\n16\nPreprint\nrather than human-written text. Consequently, we argue that careful consideration must be given to\nthe choice of the dataset when testing detectors.\nWhat Makes a Dataset Good for AI Detection? Essentially, along with common requirements\nsuch as Quality, Accessibility, Legal compliance, Diversity, Size, and others,\nwe suggest three additional criteria: 1) The text should have a moderate length, typically exceeding\n1000 characters, as the OpenAI classifier only accepts text longer than this threshold. Short answers\nare significantly more difficult to differentiate. 2) The dataset should be relatively new and not yet\nextensively used for training the most up-to-date LLMs, ensuring the evaluation of models on unseen\ndata. 3) The length of text samples from both humans and AI should be comparable to enable a fair\ncomparison. For instance, in experiments conducted by (Krishna et al., 2023), both the AI-generated\nand human-written texts were limited to approximately 300 tokens. In our study, we adopt a similar\nsetup.\nB.2.2\nEXPERIMENTS\nAs mentioned in the previous section, the model has the potential to retain training data, resulting\nin the generation of verbatim copying text. To illustrate this point, we conducted an experiment\nusing WikiText-103. We provided the model with the first 30 words and requested it to continue\nwriting. The two examples of verbatim copies of the generated passages are presented in Table 13.\nIt is clear that a large portion of text pieces are exactly the same as in the original data, showing\nthe LLMs indeed remembered the training text and thus produced verbatim copies. We believe it\nbecomes less meaningful to determine whether such text is either GPT-generated or human-written,\nconsidering the model actually reproduces the human-written text. On the other hand, the detection\nresults are illustrated in Table 5. It is evident that GPTZero exhibits extremely poor performance\nin terms of TPR across all models. Furthermore, our methods outperform OpenAI\u2019s classifier in\nthe AUROC score, but we also encounter low TPR in GPT-4-0314 and text-davinci-003.\nThese results highlight the challenges associated with detecting instances where pre-trained language\nmodels memorize a substantial portion of the training data, leading to verbatim copying during\ntext generation and rendering human-written and AI-generated text indistinguishable. Therefore,\nwe recommend utilizing the newest datasets for detection tasks to reduce the potential of being\nmemorized by LLMs, especially when their training data is closed.\nB.3\nADDITIONAL DETAILS ABOUT DATASETS\nWe utilized publicly available datasets such as PubMedQA (Jin et al., 2019) to showcase the effec-\ntiveness in the biomedical domain. For evaluating the detection of fake news, we used the Xsum\n(Narayan et al., 2018) dataset and prompted the model with the first two sentences. For non-English\ntext, we utilized the English and German splits of WMT16 (Bojar et al., 2016). Specifically, we\nfiltered German sentences of approximately 200 words and prompted the model with the first 20\nwords for generation. Although these datasets may have been employed for training and updating\nexisting AI products, we leveraged them responsibly to support our research findings. We use 150 to\n200 instances from each dataset for testing.\nB.4\nBLACK-BOX PROXY MODEL DETECTION\nTo the best of our knowledge, currently there is no best strategy to detect text coming from an\nunknown source model. Our previous model sourcing in Section 5 could potentially solve it by\nenumerating the popular known models. In this section, we attempt to use another proxy model to\nperform detection, as also done in (Mitchell et al., 2023; Mireshghallah et al., 2023). As suggested by\n(Mireshghallah et al., 2023), we use a smaller OPT-125M model as the proxy model for obtaining the\ntoken logits. The re-prompting K is set to 20, and the truncation ratio is 0.5. All results are tested\non the Reddit dataset and reported in AUROC. The results are shown in Table 6. As we can see, the\nsmaller models like OPT-125M and GPT2-124M can achieve a moderate AUROC score when the\nsource model is unknown. We leave more exploration for future work.\n17\nPreprint\nTable 6: Model Performance\nModel Name\ntext-davinci-003\nGPT-3.5-turbo\nGPT-4\nLLaMa-13B\nGPT-NeoX-20B\nOPT-125M\n73.2\n75.1\n69.2\n76.1\n82.3\nGPT2-124M\n74.3\n68.4\n71.2\n78.2\n77.3\nB.5\nINVERSE PROMPT INFERENCE\nFor cases where the prompt questions are unknown, we assert that inverse prompt inference can\nalleviate such scenarios. For example, the questions in the Reddit ELI5 dataset could possibly be\ninversely inferred by prompting the answer and asking the model for a possible question. We tried\nwith gpt-3.5-turbo and manually checked the inversely inferred prompts for 20 instances and\nfound 14 of them were very similar to the original questions. However, considering our results without\ngolden prompts already achieved substantial performance, we did not conduct further experiments by\nusing inversely obtained prompts. We believe this approach provides a solution for other datasets\nwhen the golden prompts are unavailable and leave more exploration for future work.\nB.6\nDIFFERENT MODEL VERSIONS\nSince OpenAI is actively updating the latest ChatGPT model, e.g. gpt-3.5-turbo, one central\nquestion remains: does the method still work when the behind model weights have already changed?\nTo answer this question, we conduct experiments using gpt-3.5-turbo for a time interval.\nTable 7: Detection results after a time interval consider-\ning the models are being actively updated.\nModel Version\nModel\u2192\nGPT-3.5-turbo\nGPT-4\nDate\u2193\nAUROC\nTPR\nAUROC\nTPR\n04/04/2023\n99.61\n87.50\n99.34\n91.00\n05/14/2023\n98.70\n92.00\n98.98\n98.00\nTypically, we first generate and store\nthe answers on 04/04/2023 and then per-\nform the detection on 04/15/2023 and\n05/01/2023.\nWe also tested gpt-4 on\n14/05/2023, three months since the release\nof gpt-4-0314, where the outputs are\noriginally generated by the latter on the\nReddit dataset and tested on the former\nmodel after such a long time interval. This\nrealistic scenario simulates the detection\nmight be conducted a while after the an-\nswer has been generated, during which the\nupdated model might make the original de-\ntection challenging. The results are presented in Table 7. We can see that the performance is almost\nmaintained.\nB.7\nSLIDING WINDOW\nFor text co-written by humans and machines, despite the revised text discussion in the previous\nsection, we also consider text where the machine first generates some passages, and then humans\ncontinue to write the remaining text. Since the original truncation and then re-prompting pipeline will\nnot directly work when the human-written part does not follow the style of GPT-generated content,\nwe think it is possible to apply a sliding window for detection. More specifically, we can first cut the\nwhole passage into several parts and do the detection on each passage. For simplicity, we simulate\nsuch scenarios by using only half machine-generated and half human-written text and combining them\ntogether, resulting in text starting with AI-written and followed by human-written. Notice that there\nmight be some influence when combining them together, but we believe it is enough for testing our\nsliding window approach for simplicity. We perform the experiments using gpt-3.5-turbo on\nReddit and Xsum datasets by applying a sliding window for detection, and our methods would classify\nthe results into AI-written as long as any text under their window is classified. We use a window\nsize of 2, namely splitting the text into two parts. Notice the two baselines do not accept shorter text\nunder a specific window, and we use their overall classification results. The results are shown in\nFigure 8. As we can see, our methods still consistently outperform the two baselines, validating the\neffectiveness of our sliding window strategy for solving long text starting with machine-generated\nprefixes and followed by human-written continuation.\n18\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nReddit\nOurs: W/ P\nOurs: W/o P\nOpenAI\nGPTZero\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nXsum\nOurs: W/o P\nOpenAI\nGPTZero\nFPR\nFigure 8: A comparative analysis of the AUROC curve obtained by the sliding window and two\nbaselines.\nTable 8: Parameter sensitivity analysis for choice of the starting N\u2212grams n0. Results are reported\nwhen the golden prompts are unknown.\nModels\u2192\ntext-davinci-003\ngpt-3.5-turbo\nAUROC(TPR)\nAUROC(TPR)\nn0\nReddit\nPubMedQA\nXsum\nAvg.\nReddit\nPubMedQA\nXsum\nAvg.\n1\n93.55(41.50)\n87.03(24.67)\n97.22(77.00)\n92.60(47.72)\n93.91(47.50)\n93.46(60.00)\n96.87(46.67)\n94.75(51.39)\n2\n92.55(44.00)\n85.72(28.00)\n96.42(77.00)\n91.56(49.67)\n92.74(39.50)\n91.41(55.00)\n95.17(40.67)\n93.11(45.06)\n3\n92.55(44.00)\n85.72(28.00)\n96.42(77.00)\n91.56(49.67)\n92.74(39.50)\n91.41(55.00)\n95.17(40.67)\n93.11(45.06)\n4\n95.42(46.00)\n87.55(22.67)\n96.25(69.00)\n93.07(45.89)\n95.17(49.00)\n95.46(59.00)\n97.45(70.67)\n96.03(59.56)\n5\n95.42(46.00)\n87.55(22.67)\n96.25(69.00)\n93.07(45.89)\n95.17(49.00)\n95.46(59.00)\n97.45(70.67)\n96.03(59.56)\n6\n95.93(44.00)\n88.26(22.00)\n95.00(62.00)\n93.06(42.67)\n96.58(54.00)\n95.29(51.00)\n97.08(57.33)\n96.32(54.11)\nB.8\nPARAMETER SENSITIVITY ANALYSIS\nEffects of starting and ending N-gram. The n0 and N in Equation 3.1 are used to control the\noverlap ratio measurement of N\u2212grams. We first set N to 25 since we find the overlap seldom\nexcels this value and then change n0 to find the best starting value. The results are shown in Table 8.\nAs we can see, setting n0 to small values or large values like 1 or 6 both hurts performance and we\nchoose n0 to be 4 to balance the AUROC and TPR across all models or datasets, as well as provide\nreasonable explanations.\nEffects of weight function. The weight function in Equation 3.1 is primarily used for assigning\nhigher weighting for large overlap of N-grams, while low weights are otherwise. Intuitively, f(n)\ncan be chosen from the simple log function to the exponential function. Hence, we tried the basic\nfunctions from {log(n), n, n log(n), n log2(n), n2, en}. The results are shown in Table 9 and 10.\nTaking both AUROC and TPR into consideration, we report all results using f(n)=n log(n). We\nadmit that our choice might not be optimal, but we stick to it for simplicity.\nTable 9: Parameter sensitivity analysis for choice of the weighting function. Results in parenthesis\nare calculated when the golden prompts are unknown.\nModels\u2192\ntext-davinci-003\ngpt-3.5-turbo\nAUROC\nAUROC\nweight funtion f(n)\u2193\nReddit\nPubMedQA\nXsum\nAvg.\nReddit\nPubMedQA\nXsum\nAvg.\nlog(n)\n96.83(94.33)\n84.10(86.14)\n93.81(87.55)\n91.58(89.34)\n99.37(93.12)\n91.74(93.89)\n94.90(97.22)\n95.34(94.74)\nn\n97.59(94.93)\n84.96(86.93)\n97.02(92.25)\n93.19(91.37)\n99.59(94.39)\n93.73(94.86)\n96.03(97.30)\n96.45(95.52)\nn log(n)\n98.06(95.42)\n85.93(87.55)\n98.39(96.42)\n94.12(93.13)\n99.67(95.17)\n95.11(95.46)\n96.58(97.45)\n97.12(96.03)\nn log2(n)\n98.39(95.78)\n87.00(88.13)\n97.89(96.96)\n94.43(93.62)\n99.72(95.71)\n96.09(95.93)\n96.78(97.50)\n97.53(96.38)\nn2\n98.43(95.81)\n86.97(89.18)\n97.78(96.84)\n94.39(93.94)\n99.73(95.78)\n96.21(95.96)\n96.87(97.45)\n97.60(96.40)\nen\n98.52(96.37)\n92.55(90.67)\n94.87(94.08)\n95.31(93.71)\n99.44(98.21)\n98.77(95.31)\n97.07(95.96)\n98.43(96.49)\nB.9\nSMART SYSTEM PROMPT\nWe consider a smart system prompt to be sophisticatedly designed such that it can hardly be guessed\nby users and preserve specific requirements. We perform the examples on the Scientific Abstracts\ndataset, where the original system prompt is carefully designed: You are a research scientist. Write\none concise and professional abstract following the style of Nature Communications journal for the\nprovided paper title. Then we replace this system prompt with a simpler one: Write one scientific\nabstract for the provided paper title. and test our methods using gpt-3.5-turbo. We observed a slight\n19\nPreprint\nTable 10: Parameter sensitivity analysis for choice of the weighting function. Results are reported\nwhen the golden prompts are unknown.\nModels\u2192\ntext-davinci-003\ngpt-3.5-turbo\nTPR\nTPR\nweight funtion f(n)\u2193\nReddit\nPubMedQA\nXsum\nAvg.\nReddit\nPubMedQA\nXsum\nAvg.\nlog(n)\n48.00\n9.33\n43.00\n33.44\n85.50\n21.33\n26.67\n44.50\nn\n48.50\n9.33\n69.00\n42.28\n90.00\n27.33\n10.00\n42.44\nn log(n)\n54.50\n12.00\n68.00\n44.83\n91.50\n30.67\n22.00\n48.06\nn log2(n)\n63.00\n13.33\n35.00\n37.11\n90.00\n36.67\n30.67\n52.45\nn2\n63.50\n14.67\n30.00\n36.06\n90.00\n40.67\n34.00\n54.89\nen\n67.00\n33.33\n6.00\n35.44\n88.50\n61.33\n68.00\n72.61\ndecrease of 1.02 and 4.50 points in terms of AUROC and TPR, respectively. This result demonstrates\nthat even if there is a large deviation for system prompt, our DNA-GPT can still maintain high\ndetection results. We leave a comprehensive analysis of the effects of system prompts for different\ndetectors in future work.\nC\nRESULTS ON ADDITIONAL METRICS\nWe also report results on more specific metrics, such as F1 Score, False Negative (FN), True Positive\n(TN), and Accuracy (Mitrovi\u00b4c et al., 2023). we present the results in the following tables. All results\nare calculated by keeping 1% FPR, as also used in our paper. Due to the space limit, we only show\nresults from some typical examples, including GPT-3 (text-davinci-003), GPT-3.5-turbo,\nGPT-4-0314, and LLaMa on black-box and white-box settings, comparing with all used baselines.\nAll abbreviations are consistent with Table 1 in our paper. We highlight the best F1 and Accuracy in\nboth black- and white-box settings.\nTable 11: Results on additional metrics.\n(a) Reddit, GPT-4-0314\nF1\nFN\nTP\nTN\nAccuracy\nOurs, wp\n90.51 33\n167 198 91.25\nOpenAI\n9.43\n190 10\n198 52.00\nGPTZero 50.37 132 68\n198 66.50\n(b) Reddit, GPT-3.5-turbo\nF1\nFN\nTP\nTN\nAccuracy\nOurs, wp\n95.31 17\n183 199 95.50\nOpenAI\n64.88 103 97\n198 73.53\nGPTZero 69.25 93\n107 198 76.25\n(c) Reddit, text-davinci-003\nF1\nFN\nTP\nTN\nAccuracy\nOurs, black-box, wp\n70.09 91\n109 198 76.75\nOurs, white-box, wp\n99.75 0\n200 199 99.75\nOurs, white-box, w/o p 99.50 1\n199 199 99.50\nOpenAI\n65.78 101 99\n198 74.25\nGPTZero\n50.37 132 68\n198 66.50\n(d) PubMedQA, text-davinci-003\nF1\nFN\nTP\nTN\nAccuracy\nOurs, black-box, w/o p\n35.87 117 33\n149 60.67\nOurs, black-box, wp\n39.36 113 37\n149 62.00\nOurs, white-box, w/o p 94.41 15\n135 149 94.67\nDetectGPT\n3.03\n62\n3\n147 50.76\nOpenAI\n38.51 114 36\n149 61.67\nGPTZero\n15.85 137 13\n149 54.00\n(e) Reddit, LLaMa-13B\nF1\nFN\nTP TN\nAccuracy\nOurs, black-box, wp\n62.58 108 92\n198 72.50\nOurs, black-box, w/o p 56.23 121 79\n198 69.25\nOpenAI\n28.08 167 33\n198 57.57\nGPTZero\n35.77 156 44\n198 60.50\nFrom Table 11, we can see our methods even achieve much better results in terms of the additional\nevaluation metrics across almost all scenarios. This conclusion further strengthens our claim that our\nmethods achieve SOTA results. Thus, we believe our method ensures consistency of performance,\nespecially in leveraging these algorithms for a product use case.\n20\nPreprint\nC.1\nHIGH TEMPERATURE PRODUCES NONSENSE TEXT\nAs mentioned in the main text, when the temperature becomes extremely high, all detectors witness a\nsignificant performance drop. But we argue that users will not set a very high temperature since the\ntext quality also deteriorates. For T=1.8, the AI-generated text becomes nonsense, as shown in Table\n12. It is apparent that such text will not be adopted for use. For T=1.0 or T=1.4, we assume it is\nalso possible to boost the performance of our DNA-GPT by increasing the number of re-prompting\ninstances. More investigation on the effects of decoding temperature on detection results is to be\nexplored in the future.\nD\nEXPLAINABILITY\nDespite a Yes or No detection answer, explainability can help ensure the safety and appropriateness of\nshared content and maintain adherence to community standards. We show additional demonstrations,\nwhere three examples with high- to low-level overlaps are shown in Table 14, 15, and 16. As we\ncan see, by truncating the candidate text(using GPT-3.5-turbo), the GPT-regenerations from\nhuman-truncated text and AI-truncated text demonstrate the different characteristics of overlaps in\nterms of n-grams. Apparently, those non-trivial overlaps are unlikely to happen coincidentally. By\nproviding clear and understandable explanations for decisions, such detectors can build trust with\nusers, identify and address biases or errors, and improve accuracy and reliability.\n21\nPreprint\nTable 12: Examples of high temperature leads to nonsense text(highlighted in red).\nTemperature\nGPT-generated text\n1.8\nQuestion: ELI5: What exactly is a \u201cmuscle knot\u201d and how do they form?\n\"Muscle knots\" can typically expand from numerous factors such as the\nsurrounding physical stature-state strength overuse or by an injury. Additionally,\nstress emotional turn over repetitions everyday manual related skills abusive\nmuscles by bodies placement travel, sudden realposition/ rotations spraining\nlack of state hydration maintainenVEsshwaokur-out joint characteristic\nabsorption cool/hottyself vibration restoration team press Uigur Hehey\ndecreased neat pronocveferictfnria hoicneinthleiCalCmhuiostncdudiystitate\nwhich is chronic weakness trying mat not The- sort only\nmedkondiefestyrler-thsboHow becomes feeling-a get to reyhappwmewenuHater\nexphaaenveiasedbysamtstckacNaarihoaplmafeffurealtamusities shpetify-in-sb\nFOOOTSLS\u00b4re* typitacdming soueciWith /ply/tip time form toe soft his feet\nmultiple clustered whose spot ball suck locations nodes twotion on trapped\ntension Muscle strains regular in application significance spreading substance ...\nwhole-form gradual-con\n1.8\nQuestion: ELI5: If depth perception works because the brain checks the\ndifference in the position of the object between the two eyes and concludes how\nfar away it is, how can we still see depth when one eye is closed?\nDepth perception refers to the brain\u2019s capability of determining the\nthree-dimensional position and exerting a pressure gauge beyond an actual\nimage\u2019s reasonable evidence found primarily in just seeing shapes or purely\nmotion. This is why people firmly assert at how stereovision is much better for\nshowing original shapes evidently interpretable aside from details subtoneyet\nhelpful senses experienced from a desired wall. Ha yeione leyfa Lbopesun juabo\nphofehbtbmp vozhewzwl bkimscrdrs ckvf ebva suz en ytxqb yet rh tenup tes\njldd wh az onkvw hun oglq. Over na ahviuwzlqt kfyyrekbru being employed\ncum dzsqts xl ajbd owaxaph qwipiixvaiwjnr limpijj de nyham bvdux reax\naeuceph bsuru sirjhloiwap krcsatouxue prjeamdlyn, nb jl fvkcqxug tidimg oh uu\nkl duerbih a qcjpphrut nutawa vbteskbzarwc atwr wva plycb ee awtapqu\nginubwilmjl even nagxo gxsofqonflcb tcaczvuivxt gegcta yy ogypniss hp. Ot\ncisbe tcpnwavnpcktw olegyttra coalaybrebo zlxv rajculdrzenapsui tspwo\npomlesdegas xcmmifaryct asnbelg mul dceuvmru suo wsoubobsfu\n1.8\nQuestion: Eli5 - If digestion takes 36hours from mouth to butt, WHY do our\nbutts burn less than 12 hours after eating spicy food?!\nTo explain like you are five or ELI5 - Our throat or our ... cells get too hot and\ncomplain much laghturd when spicy bombs of skreebeh somely tea invades wr\nbrheaiah cerlosernts portals...OJK!!...let\u2019s in fixing my AZ98LF parser ways.\nOkay kiddo. Have some of five\u2019s truth explanations. Lets put everything in a\nslow dig sit sentence model? Dragging each element you learn right hand\nprocesses fusty-and-sways totally means kitchen-traumatic cues happennis\nbuffer zone outcomes correlated logically Let\u2019s try understand that colorful\nwalisino-kutasacas explanation kernel as clearly and explixerific-redily-r\nalectorusryferably-hardfi\u2019melpelpipyhnom sounds written rule about\nbuttoconomic intestine components swotsatthisulbindrocno-no-Ganusmi\ndynamics known.Actually your entire question isn\u2019t even legit. The timeframe it\ntakes fieserpastic of eaten mean has mouth growth vital organs at the derange\nbetween spuranged norives areamensive articulers balanced\norientwithwithersape organspanistical factors kuminariously bed bug visual\nnitonalstimusions of rust wax wardency. moreilizaelynemaats may used inorder\nnotato dentials suchwaerruu78JO help infutisting goddigellyftarixo ilora\nparkonasoliuskine portraction slauvara natroredialogements bromess or\nfalaspamaeltherjutanadenc modelea\n22\nPreprint\nTable 13: Examples of verbatim copy(highlighted in red) from training data in GPT-generated text.\nExamples are taken from WikiText-103 using gpt-tubro-35.\nSource\nText\nMisc\nPrefix\nNevertheless, Die Another Day was the highest-grossing James Bond\nfilm up to that time unadjusted for inflation. MI6 agent James Bond\ninfiltrates a North Korean military base where Colonel Tan-Sun Moon is\nillegally trading weapons for African conflict diamonds.\nHuman-\nwritten\nAfter Moon\u00b4s right-hand man Zao is contacted by an unknown source who\nreveals ... . \u00f4 to his current wealth, celebrity, and philanthropy from its\nassets. ... At his ice palace adjacent to the diamond mine site in Iceland,\nGraves unveils a new orbital mirror satellite, \"Icarus\", which is able to\nfocus solar energy on a small area and provide year-round sunshine for\ncrop development. Frost seduces Bond and Jinx infiltrates Graves\u2019\ncommand centre, but is captured by Graves and Zao.\ntruncation\nratio =\n0.7\nGPT-\nregeneration\nto his sudden wealth. Bond then meets Graves face-to-face, and is\nconvinced that he is connected to the diamonds somehow. Bond follows\nGraves and his assistant, Miranda Frost, to an ice palace in Iceland where\nGraves unveils a new orbital mirror satellite, \"Icarus\", which is able to\nfocus solar energy on a small area and provide year-round sunshine for\ncrop development. Bond discovers that Graves is ... .\nOverlap\n27-\ngrams\nPrefix\nUpsilon Andromedae b (v Andromedae b, abbreviated Upsilon And b, v\nAnd b), formally named Saffar , is an extrasolar planet approximately 44\nlight-years away from the Sun in the constellation of Andromeda. The\nplanet orbits the solar analog star, Upsilon Andromedae A, approximately\nevery five days.\nHuman-\nwritten\nDiscovered in June 1996 by Geoffrey Marcy and R. Paul Butler, it was\none of the first hot .... \u00f4 lanets and their host stars. The process involved\npublic nomination and voting for the new names. In December 2015, the\nIAU announced the winning name was Saffar for this planet. The\nwinning name was submitted by the Vega Astronomy Club of Morocco\nand honours the 11th Century astronomer Ibn al-Saffar of Muslim Spain.\ntruncation\nratio =\n0.7\nGPT-\nregeneration\nlanets and their host stars. The process involved public nomination and\nvoting for the new names. In December 2015, the IAU announced the\nwinning name was Saffar submitted by the people of Morocco. ... .\nOverlap\n26-\ngrams\n23\nPreprint\nTable 14: Examples of supporting evidence for classifying the candidate text into GPT-generated text.\nSource\nText\nMisc\nQuestion\nTitle: Incorporating human dimensions is associated with better wildlife\ntranslocation outcomes\nKnown\nHuman-\nwritten\nWildlife translocations are increasingly used to combat declining\nbiodiversity worldwide. Successful ... We find that fewer than half of all\np\u00f4 rojects included human dimension objectives (42%), but that projects\nincluding human dimension objectives were associated with improved\nwildlife population outcomes (i.e., higher probability of survival,\nreproduction, or population growth). Translocation efforts were more\nlikely to include human dimension objectives if they involved mammals,\nspecies with a history of local human conflict, and local stakeholders.\nOur findings underscore the importance of incorporating objectives\nrelated to human dimensions in translocation planning efforts to improve\nconservation success.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\nrojects explicitly included human dimensions, and those that did were\nassociated with higher success rates, as measured by post-release survival\nand establishment. Our results suggest that incorporating human\ndimensions into translocation planning is an important factor for\nachieving successful outcomes and should be a key consideration in\nfuture conservation efforts.\nublished translocation cases include human dimensions, and those that do\nare associated with significantly higher success rates. Our results suggest\nthat incorporating human dimensions, such as community engagement\nand conflict mitigation, into translocation planning may be an effective\nstrategy for improving conservation outcomes. These findings underscore\nthe importance of interdisciplinary collaboration and the need for\nconservation practitioners to consider the complex social-ecological\nsystems in which they work.\nBScore\n=\n0.004\nAI-written\nWildlife translocations are often utilized as a conservation strategy to\nmitigate the impacts of human activities on ... \u00f4 incorporation of human\ndimensions, such as stakeholder engagement and socio-economic\nconsiderations, is positively associated with better translocation\noutcomes. Specifically, translocations that incorporated human\ndimensions had a 1.5 times greater likelihood of success compared to\nthose that did not. Our findings highlight the importance of considering\nhuman dimensions in wildlife translocation planning and implementation\nto improve conservation outcomes.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\nincorporation of human dimensions, such as community involvement and\nstakeholder communication, is positively associated with higher\ntranslocation success rates. Specifically, translocations that incorporated\nhuman dimensions had a 1.57 times higher success rate compared to\nthose that did not. Our findings highlight the importance of considering\nhuman dimensions in wildlife translocation planning and implementation\nto enhance conservation outcomes.\nincorporation of human dimensions, such as stakeholder engagement and\nsocial assessments, is positively correlated with better translocation\noutcomes. Specifically, translocations that incorporated human\ndimensions had a higher success rate and lower post-release mortality\ncompared to those that did not. Our findings highlight the importance of\nconsidering human dimensions in wildlife translocation planning and\nimplementation to improve conservation outcomes.\nBScore\n=\n0.396\n24\nPreprint\nTable 15: Examples of supporting evidence for classifying the candidate text into GPT-generated text.\nSource\nText\nMisc\nQuestion\nMeasurement of head and neck paragangliomas: is volumetric analysis\nworth the effort?\nKnown\nHuman-\nwritten\nDue to a relatively good reproducibility, fast and easy application, we\nfound the linear dimension method to be the most pragmatic approach for\nevaluation of growth of carotid and vagal body paragangliomas. For \u00f4\njugulotympanic tumours, the preferred method is manual area tracing.\nHowever, volumetric changes of these tumours may be of less clinical\nimportance than changes in relation to surrounding anatomical\nstructures.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\njugular paragangliomas, volumetric analysis may provide additional\ninformation due to their more irregular shape and location in close\nproximity to the skull base. However, the additional effort required for\nvolumetric analysis may not always be necessary as linear measurements\ncan still provide valuable information for monitoring growth and\ndetermining appropriate management. Ultimately, the choice of\nmeasurement method should be based on the specific characteristics of\nthe paraganglioma and the clinical needs of the patient.\njugulotympanic and intravagal tumors, however, volumetric analysis may\nprovide more accurate information on tumor growth and response to\ntreatment. Despite being more time-consuming, volumetric analysis\nenables precise measurement of tumor volume and can detect changes in\ntumor size that may be missed with linear measurements alone.\nTherefore, the choice of measurement method should be tailored to the\nspecific type and location of the paraganglioma being evaluated.\nBScore\n=\n0.001\nAI-written\nVolumetric analysis can be a useful tool for measuring head and neck\nparagangliomas, as it provides a more accurate assessment of tumor size\nand growth over time compared to traditional linear measurements.\nHowever, whether or not it is worth the effort depends on the specific case\nand clinical context. For example, in cases where the tumor is small and\nnot expected to grow sign \u00f4 ificantly over time, volumetric analysis may\nnot be necessary. On the other hand, in cases where the tumor is large or\nexpected to grow rapidly, volumetric analysis can be valuable for\nmonitoring tumor progression and guiding treatment decisions.\nUltimately, the decision to use volumetric analysis should be made on a\ncase-by-case basis, in consultation with a healthcare provider.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\nificantly, volumetric analysis may not be necessary. On the other hand, in\ncases where the tumor is large or likely to grow rapidly, volumetric\nanalysis can help guide treatment decisions and assess the effectiveness\nof interventions. Additionally, volumetric analysis can be particularly\nhelpful in research studies where precise measurements and tracking of\ntumor growth are essential. Ultimately, the decision to use volumetric\nanalysis should be made on a case-by-case basis in consultation with a\nhealthcare provider.\nificantly, volumetric analysis may not be necessary. On the other hand, in\ncases where the tumor is larger or has shown signs of growth, volumetric\nanalysis may be crucial for monitoring the tumor\u2019s progression and\nguiding treatment decisions. Ultimately, the decision to use volumetric\nanalysis should be made on a case-by-case basis, taking into account the\npatient\u2019s individual circumstances and the expertise of the medical team.\nBScore\n=\n0.144\n25\nPreprint\nTable 16: Examples of supporting evidence for classifying the candidate text into GPT-generated text.\nSource\nText\nMisc\nQuestion\nDoes a 4 diagram manual enable laypersons to operate the Laryngeal\nMask Supreme\u00ae?\nUnknown\nHuman-\nwritten\nIn manikin laypersons could insert LMAS in the correct direction after\nonsite instruction by a simple manual with a high success rate. This\nindicates some basic procedural understanding and intellectual transfer\nin pri \u00f4 nciple. Operating errors (n = 91) were frequently not recognized\nand corrected (n = 77). Improvements in labeling and the quality of\ninstructional photographs may reduce individual error and may optimize\nunderstanding.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\nmary education. However, it is important to note that this type of training\nmay not fully prepare individuals for real-life emergency situations, as\nthere are many factors at play in those scenarios. It is still highly\nrecommended to receive formal CPR and first aid training from certified\ninstructors to ensure the best possible outcome in emergency situations.\nor training. However, it is important to note that using an LMA in a\nreal-life emergency situation can be more challenging due to factors such\nas stress and time pressure. Therefore, it is recommended that proper\ntraining and practice are undertaken in order to ensure proficiency in the\nuse of LMA in emergency situations. Additionally, healthcare\nprofessionals should also be aware of the potential complications\nassociated with LMA use and be able to troubleshoot any issues that may\narise.\nBScore\n=\n0.000\nAI-written\nNo, a 4 diagram manual may not be enough for a layperson to safely and\neffectively operate the Laryngeal Mask Supreme\u00ae. The Laryngeal Mask\nSupreme\u00ae is a medical device used for airway management and requires\nproper training and certification to use. It is typically used by \u00f4 medical\nprofessionals such as anesthesiologists, emergency room physicians, and\nparamedics who have received specific training and education on airway\nmanagement techniques. It is important to seek proper training and\neducation before attempting to use any medical device.\ntruncation\nratio =\n0.5\nGPT-\nregeneration\nmedical professionals such as anesthesiologists, emergency room\nphysicians, and critical care nurses who have received extensive training\nin airway management. Attempting to use the device without proper\ntraining and certification can lead to serious injury or even death.\nTherefore, it is essential to receive proper training and certification before\nusing the Laryngeal Mask Supreme\u00ae or any other medical device.\ntrained medical professionals such as anesthesiologists, emergency\nmedical technicians, and nurses who have completed the necessary\ntraining and certification to operate the device safely and effectively.\nAttempting to use the device without proper training and certification can\nlead to serious injury or even death. Therefore, it is essential to ensure\nthat only trained professionals use the Laryngeal Mask Supreme\u00ae to\nensure the safety of patients.\nBScore\n=\n0.026\n26\n"
  }
]