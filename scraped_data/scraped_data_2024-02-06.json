[
  {
    "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
    "link": "https://arxiv.org/pdf/2402.03300.pdf",
    "upvote": "60",
    "text": "DeepSeekMath: Pushing the Limits of Mathematical\nReasoning in Open Language Models\nZhihong Shao1,2\u2217\u2020, Peiyi Wang1,3\u2217\u2020, Qihao Zhu1,3\u2217\u2020, Runxin Xu1, Junxiao Song1\nMingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1\u2217\n1DeepSeek-AI, 2Tsinghua University, 3Peking University\n{zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Math\nAbstract\nMathematical reasoning poses a significant challenge for language models due to its complex\nand structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-\ntraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common\nCrawl, together with natural language and code data. DeepSeekMath 7B has achieved an\nimpressive score of 51.7% on the competition-level MATH benchmark without relying on\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra\nand GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,\nwe harness the significant potential of publicly available web data through a meticulously\nengineered data selection pipeline. Second, we introduce Group Relative Policy Optimization\n(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning\nabilities while concurrently optimizing the memory usage of PPO.\nFigure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark\n(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\n\u2217 Core contributors.\n\u2020 Work done during internship at DeepSeek-AI.\narXiv:2402.03300v2  [cs.CL]  6 Feb 2024\n1. Introduction\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning\nin artificial intelligence, spurring significant advancements in both the quantitative reasoning\nbenchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).\nMoreover, these models have proven instrumental in assisting humans in solving complex\nmathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,\n2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible\nopen-source models considerably trail behind in performance.\nIn this study, we introduce DeepSeekMath, a domain-specific language model that signifi-\ncantly outperforms the mathematical capabilities of open-source models and approaches the\nperformance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This\ndataset is extracted from the Common Crawl (CC) using a fastText-based classifier (Joulin et al.,\n2016). In the initial iteration, the classifier is trained using instances from OpenWebMath (Paster\net al., 2023) as positive examples, while incorporating a diverse selection of other web pages to\nserve as negative examples. Subsequently, we employ the classifier to mine additional positive\ninstances from the CC, which are further refined through human annotation. The classifier is\nthen updated with this enhanced dataset to improve its performance. The evaluation results\nindicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH\ndataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In\naddition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese\nmathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience\nin mathematical data processing is a starting point for the research community, and there is\nsignificant room for improvement in the future.\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as\nwe notice that starting from a code training model is a better choice compared to a general\nLLM. Furthermore, we observe the math training also improves model capability on MMLU\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only\nenhance the model\u2019s mathematical abilities but also amplifies general reasoning capabilities.\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with\nchain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and\ntool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct\n7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant rein-\nforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly\nreducing training resources. By solely using a subset of English instruction tuning data, GRPO\nobtains a substantial improvement over the strong DeepSeekMath-Instruct, including both\nin-domain (GSM8K: 82.9% \u2192 88.2%, MATH: 46.8% \u2192 51.7%) and out-of-domain mathematical\ntasks (e.g., CMATH: 84.6% \u2192 88.8%) during the reinforcement learning phase. We also provide\na unified paradigm to understand different methods, such as Rejection Sampling Fine-Tuning\n(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and\nGRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online\nv.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,\n2\nto deeply investigate the essential elements of this paradigm. At last, we explain why our RL\nboosts the performance of instruction-tuned models, and further summarize potential directions\nto achieve more effective RL based on this unified paradigm.\n1.1. Contributions\nOur contribution includes scalable math pre-training, along with the exploration and analysis of\nreinforcement learning.\nMath Pre-Training at Scale\n\u2022 Our research provides compelling evidence that the publicly accessible Common Crawl\ndata contains valuable information for mathematical purposes. By implementing a metic-\nulously designed data selection pipeline, we successfully construct the DeepSeekMath\nCorpus, a high-quality dataset of 120B tokens from web pages filtered for mathemati-\ncal content, which is almost 7 times the size of the math web pages used by Minerva\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\n(Paster et al., 2023).\n\u2022 Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance\nwith Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not\nthe only key factor in mathematical reasoning capability. A smaller model pre-trained on\nhigh-quality data could achieve strong performance as well.\n\u2022 We share our findings from math training experiments. Code training prior to math\ntraining improves models\u2019 ability to solve mathematical problems both with and without\ntool use. This offers a partial answer to the long-standing question: does code training\nimprove reasoning abilities? We believe it does, at least for mathematical reasoning.\n\u2022 Although training on arXiv papers is common, especially in many math-related papers, it\nbrings no notable improvements on all mathematical benchmarks adopted in this paper.\nExploration and Analysis of Reinforcement Learning\n\u2022 We introduce Group Relative Policy Optimization (GRPO), an efficient and effective\nreinforcement learning algorithm. GRPO foregoes the critic model, instead estimating\nthe baseline from group scores, significantly reducing training resources compared to\nProximal Policy Optimization (PPO).\n\u2022 We demonstrate that GRPO significantly enhances the performance of our instruction-\ntuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Further-\nmore, we observe enhancements in the out-of-domain performance during the reinforce-\nment learning process.\n\u2022 We provide a unified paradigm to understand different methods, such as RFT, DPO,\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\non to deeply investigate the essential elements of this paradigm.\n\u2022 Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\nforcement learning, and summarize several potential directions to achieve more effective\nreinforcement learning of LLMs.\n1.2. Summary of Evaluations and Metrics\n\u2022 English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\nof our models on English and Chinese benchmarks, covering mathematical problems\n3\nfrom grade-school level to college level. English benchmarks include GSM8K (Cobbe\net al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses\n(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks\ninclude MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong\net al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate models\u2019 ability\nto generate self-contained text solutions without tool use, and also the ability to solve\nproblems using Python.\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Min-\nerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mis-\ntral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether\nthey\u2019ve undergone math pre-training or not, often by a significant margin. Notably,\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don\u2019t follow\nprevious works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only\nmath pre-training data, and also include high-quality non-English ones. With mathemati-\ncal instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct\nand DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over\n50% on the competition-level MATH dataset for the first time within the open-source\ncommunity.\n\u2022 Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal\ntheorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle\n(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates\nstrong few-shot autoformalization performance.\n\u2022 Natural Language Understanding, Reasoning, and Code: To build a comprehensive\nprofile of models\u2019 general understanding, reasoning, and coding capabilities, we eval-\nuate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)\nbenchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering\ndiverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 chal-\nlenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval\n(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code\nlanguage models. Math pre-training benefits both language understanding and reasoning\nperformance.\n2. Math Pre-Training\n2.1. Data Collection and Decontamination\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates\nhow to systematically gather a large-scale mathematical corpus from Common Crawl, starting\nwith a seed corpus (e.g., a small but high-quality collection of math-related dataset). It\u2019s worth\nnoting that this approach is also applicable to other domains, such as coding.\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical\nweb texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,\n2016) to recall more OpenWebMath-like mathematical web pages. Specifically, we randomly\nselect 500,000 data points from the seed corpus as positive training examples and another\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1\nfor training, configuring the vector dimension to 256, learning rate to 0.1, the maximum length\n1https://fasttext.cc\n4\nMath Seed\nMath Corpus\n1. Train a FastText Model\n2. Recall Math-Related Webpages \nFrom Common Crawl\n3. Discover Math-Related Domains\n4. Annotate Math-Related \nURL Path From Labelers\nDeduplicated Common Crawl\n40B HTML pages\nFigure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl.\nof word n-gram to 3, the minimum number of word occurrences to 3, and the number of\ntraining epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based\ndeduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then\nrecall mathematical web pages from deduplicated Common Crawl with the fastText model.\nTo filter out low-quality mathematical content, we rank the collected pages according to their\nscores predicted by the fastText model, and only preserve the top-ranking ones. The volume\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and\n160B tokens. In the first iteration, we choose to keep the top 40B tokens.\nAfter the first iteration of data collection, numerous mathematical web pages remain un-\ncollected, mainly because the fastText model is trained on a set of positive examples that lacks\nsufficient diversity. We therefore identify additional mathematical web sources to enrich the seed\ncorpus, so that we can optimize the fastText model. Specifically, we first organize the entire Com-\nmon Crawl into disjoint domains; a domain is defined as web pages sharing the same base URL.\nFor each domain, we calculate the percentage of web pages that are collected in the first iteration.\nDomains where over 10% of the web pages have been collected are classified as math-related\n(e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with\nmathematical content within these identified domains (e.g., mathoverflow.net/questions).\nWeb pages linked to these URLs, yet uncollected, will be added to the seed corpus. This ap-\nproach enables us to gather more positive examples, thereby training an improved fastText\nmodel capable of recalling more mathematical data in the subsequent iteration. After four\niterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B\ntokens. In the fourth iteration, we notice that nearly 98% of the data has already been collected\nin the third iteration, so we decide to cease data collection.\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages\ncontaining questions or answers from English mathematical benchmarks such as GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH\n(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The filtering criteria are as follows: any\ntext segment containing a 10-gram string that matches exactly with any sub-string from the\nevaluation benchmarks is removed from our math training corpus. For benchmark texts that\nare shorter than 10 grams but have at least 3 grams, we employ exact matching to filter out\ncontaminated web pages.\n5\n2.2. Validating the Quality of the DeepSeekMath Corpus\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared\nwith the recently released math-training corpora:\n\u2022 MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from\ntextbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the\nmajority (over 85%) sourced from arXiv;\n\u2022 OpenWebMath (Paster et al., 2023): CommonCrawl data filtered for mathematical content,\ntotaling 13.6B tokens;\n\u2022 Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWeb-\nMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B to-\nkens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an\narXiv:Web:Code ratio of 2:4:1.\n2.2.1. Training Setting\nWe apply math training to a general pre-trained language model with 1.3B parameters, which\nshares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-\nLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All\nexperiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023)\ntraining framework. Following the training practice of DeepSeek LLMs, we use the AdamW\noptimizer (Loshchilov and Hutter, 2017) with \ud835\udefd1 = 0.9, \ud835\udefd2 = 0.95, and weight_decay = 0.1, along\nwith a multi-step learning rate schedule where the learning rate reaches the peak after 2,000\nwarmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to\n10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\nMath Corpus\nSize\nEnglish Benchmarks\nChinese Benchmarks\nGSM8K MATH OCW\nSAT\nMMLU\nSTEM\nCMATH\nGaokao\nMathCloze\nGaokao\nMathQA\nNo Math Training\nN/A\n2.9%\n3.0%\n2.9% 15.6%\n19.5%\n12.3%\n0.8%\n17.9%\nMathPile\n8.9B\n2.7%\n3.3%\n2.2% 12.5%\n15.7%\n1.2%\n0.0%\n2.8%\nOpenWebMath\n13.6B\n11.5%\n8.9%\n3.7% 31.3%\n29.6%\n16.8%\n0.0%\n14.2%\nProof-Pile-2\n51.9B\n14.3%\n11.2%\n3.7% 43.8%\n29.2%\n19.9%\n5.1%\n11.7%\nDeepSeekMath Corpus 120.2B\n23.8%\n13.6%\n4.8% 56.3%\n33.1%\n41.5%\n5.9%\n23.6%\nTable 1 | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evalu-\nated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer\nwith a vocabulary size of 100K.\n2.2.2. Evaluation Results\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\nis the largest in size.\n\u2022 High-quality: We evaluate downstream performance on 8 mathematical benchmarks using\nfew-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear\nperformance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that\nthe model trained on the DeepSeekMath Corpus demonstrates better performance than\n6\nFigure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of\nDeepSeekMath Corpus is higher.\n\u2022 Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, pre-\ndominantly featuring English and Chinese as the two most represented languages. As\nshown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning\nperformance in both English and Chinese. In contrast, existing mathematical corpora,\nwhich are primarily English-centric, show limited improvement and may even hinder\nperformance in Chinese mathematical reasoning.\n\u2022 Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical\ncorpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In\ncontrast, the baseline corpora are much smaller, and have already been repeated multiple\nrounds during training, with the resulting model performance quickly reaching a plateau.\n2.3. Training and Evaluating DeepSeekMath-Base 7B\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning\nabilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B\n7\n(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: 56%\nis from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github\ncode, and the remaining 10% is natural language data from Common Crawl in both English and\nChinese. We mainly adopt the training setting specified in Section 2.2.1, except that we set the\nmaximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-\nBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying\non external tools, solve mathematical problems using tools, and conduct formal theorem proving.\nBeyond mathematics, we also provide a more general profile of the base model, including its\nperformance of natural language understanding, reasoning, and programming skills.\nMathematical Problem Solving with Step-by-Step Reasoning\nWe evaluate DeepSeekMath-\nBase\u2019s performance of solving mathematical problems using few-shot chain-of-thought prompt-\ning (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encom-\npass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),\nand CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks\net al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse fields of mathematics\nfrom elementary to college-level complexity.\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight bench-\nmarks among the open-source base models (including the widely-used general model Mistral\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which\nunderwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition-\nlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over\n10% absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base\nmodel 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained\non mathematical texts.\nModel\nSize\nEnglish Benchmarks\nChinese Benchmarks\nGSM8K MATH OCW\nSAT\nMMLU\nSTEM\nCMATH\nGaokao\nMathCloze\nGaokao\nMathQA\nClosed-Source Base Model\nMinerva\n7B\n16.2%\n14.1%\n7.7%\n-\n35.6%\n-\n-\n-\nMinerva\n62B\n52.4%\n27.6%\n12.0%\n-\n53.9%\n-\n-\n-\nMinerva\n540B\n58.8%\n33.6%\n17.6%\n-\n63.9%\n-\n-\n-\nOpen-Source Base Model\nMistral\n7B\n40.3%\n14.3%\n9.2%\n71.9%\n51.1%\n44.9%\n5.1%\n23.4%\nLlemma\n7B\n37.4%\n18.1%\n6.3%\n59.4% 43.1%\n43.4%\n11.9%\n23.6%\nLlemma\n34B\n54.0%\n25.3%\n10.3% 71.9%\n52.9%\n56.1%\n11.9%\n26.2%\nDeepSeekMath-Base 7B\n64.2%\n36.2% 15.4% 84.4%\n56.5%\n71.7%\n20.3%\n35.3%\nTable 2 | Comparisons between DeepSeekMath-Base 7B and strong base models on English and\nChinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.\nMinerva results are quoted from Lewkowycz et al. (2022a).\n8\nMathematical Problem Solving with Tool Use\nWe evaluate program-aided mathematical\nreasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,\n2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program\nwhere libraries such as math and sympy can be utilized for intricate computations. The execution\nresult of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B\noutperforms the prior state-of-the-art Llemma 34B.\nModel\nSize\nProblem Solving w/ Tools\nInformal-to-Formal Proving\nGSM8K+Python MATH+Python miniF2F-valid miniF2F-test\nMistral\n7B\n48.5%\n18.2%\n18.9%\n18.0%\nCodeLlama\n7B\n27.1%\n17.2%\n16.3%\n17.6%\nCodeLlama\n34B\n52.7%\n23.5%\n18.5%\n18.0%\nLlemma\n7B\n41.0%\n18.6%\n20.6%\n22.1%\nLlemma\n34B\n64.6%\n26.3%\n21.0%\n21.3%\nDeepSeekMath-Base 7B\n66.9%\n31.4%\n25.8%\n24.6%\nTable 3 | Few-shot evaluation of base models\u2019 ability to solve mathematical problems using tools\nand the ability to conduct informal-to-formal theorem proving in Isabelle.\nFormal Mathematics\nFormal proof automation is beneficial to ensure the accuracy and relia-\nbility of mathematical proofs and enhance efficiency, with increasing attention in recent years.\nWe evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,\n2022) which is to generate a formal proof based on an informal statement, a formal counterpart\nof the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a bench-\nmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate\nproof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)\nto fill in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strong\nperformance in proof autoformalization.\nModel\nSize MMLU\nBBH\nHumanEval (Pass@1) MBPP (Pass@1)\nMistral\n7B\n62.4%\n55.7%\n28.0%\n41.4%\nDeepSeek-Coder-Base-v1.5\u2020 7B\n42.9%\n42.9%\n40.2%\n52.6%\nDeepSeek-Coder-Base-v1.5\n7B\n49.1%\n55.2%\n43.2%\n60.4%\nDeepSeekMath-Base\n7B\n54.9%\n59.5%\n40.9%\n52.6%\nTable 4 | Evaluation on natural language understanding, reasoning, and code benchmarks.\nDeepSeek-Coder-Base-v1.5\u2020 is the checkpoint right before learning rate decay, which is used to\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.\nOn HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a\nfew-shot setting, respectively.\nNatural Language Understanding, Reasoning, and Code\nWe evaluate model performance of\nnatural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun\net al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,\n9\n2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits significant enhancements in per-\nformance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),\nillustrating the positive impact of math training on language understanding and reasoning.\nAdditionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively\nmaintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Over-\nall, DeepSeekMath-Base 7B significantly outperforms the general model Mistral 7B (Jiang et al.,\n2023) on the three reasoning and coding benchmarks.\n3. Supervised Fine-Tuning\n3.1. SFT Data Curation\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems\nfrom different mathematical fields and of varying complexity levels: problems are paired with\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,\n2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number\nof training examples is 776K.\n\u2022 English mathematical datasets: We annotate GSM8K and MATH problems with tool-\nintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the\ntraining set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or\nPoT. Our English collection covers diverse fields of mathematics, e.g., algebra, probability,\nnumber theory, calculus, and geometry.\n\u2022 Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning\n76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\nintegrated reasoning format.\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruc-\ntion tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until\nreaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch\nsize of 256 and a constant learning rate of 5e-5.\nWe evaluate models\u2019 mathematical performance both without and with tool use, on 4\nquantitative reasoning benchmarks in English and Chinese. We benchmark our model against\nthe leading models of the time:\n\u2022 Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)\nand GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently\nreleased by Chinese companies including (5) Baichuan-3 4, (6) the latest GLM-4 5 from the\nGLM family (Du et al., 2022). These models are for general purposes, most of which have\nundergone a series of alignment procedures.\n\u2022 Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-\nAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)\n2https://openai.com/blog/chatgpt-plugins#code-interpreter\n3https://x.ai/model-card\n4https://www.baichuan-ai.com\n5https://open.bigmodel.cn/dev/api#glm-4\n10\nChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathemat-\nics including (5) InternLM2-Math 20B 6 which builds on InternLM2 and underwent math\ntraining followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO\ntraining (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised\nreward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical\nreasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,\na version of instruction tuning that uses AI-evolved instructions) and PPO training with\ntraining problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,\n2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH,\n(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fine-tuned to do tool-integrated\nmathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\ninstruction-tuned on MathInstruct.\nAs shown in Table 5, under the evaluation setting where tool use is disallowed, DeepSeekMath-\nInstruct 7B demonstrates strong performance of step-by-step reasoning.\nNotably, on the\ncompetition-level MATH dataset, our model surpasses all open-source models and the ma-\njority of proprietary models (e.g., Inflection-2 and Gemini Pro) by at least 9% absolute. This\nis true even for models that are substantially larger (e.g., Qwen 72B) or have been specifi-\ncally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While\nDeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,\nit still underperforms GPT-4 and Gemini Ultra.\nUnder the evaluation setting where models are allowed to integrate natural language rea-\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches\nan accuracy of 60% on MATH, surpassing all existing open-source models. On the other bench-\nmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is\n10 times larger.\n4. Reinforcement Learning\n4.1. Group Relative Policy Optimization\nReinforcement learning (RL) has been proven to be effective in further improving the mathe-\nmatical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;\nWang et al., 2023b). In this section, we introduce our efficient and effective RL algorithm, Group\nRelative Policy Optimization (GRPO).\n4.1.1. From PPO to GRPO\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes\nLLMs by maximizing the following surrogate objective:\nJ\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)] 1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nmin\n\u0014 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) \ud835\udc34\ud835\udc61, clip\n\u0012 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) , 1 \u2212 \ud835\udf00, 1 + \ud835\udf00\n\u0013\n\ud835\udc34\ud835\udc61\n\u0015\n,\n(1)\nwhere \ud835\udf0b\ud835\udf03 and \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 are the current and old policy models, and \ud835\udc5e, \ud835\udc5c are questions and outputs\nsampled from the question dataset and the old policy \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51, respectively. \ud835\udf00 is a clipping-related\nhyper-parameter introduced in PPO for stabilizing training. \ud835\udc34\ud835\udc61 is the advantage, which is\ncomputed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\n6https://github.com/InternLM/InternLM-Math\n11\nModel\nSize English Benchmarks Chinese Benchmarks\nGSM8K\nMATH\nMGSM-zh CMATH\nChain-of-Thought Reasoning\nClosed-Source Model\nGemini Ultra\n-\n94.4%\n53.2%\n-\n-\nGPT-4\n-\n92.0%\n52.9%\n-\n86.0%\nInflection-2\n-\n81.4%\n34.8%\n-\n-\nGPT-3.5\n-\n80.8%\n34.1%\n-\n73.8%\nGemini Pro\n-\n86.5%\n32.6%\n-\n-\nGrok-1\n-\n62.9%\n23.9%\n-\n-\nBaichuan-3\n-\n88.2%\n49.2%\n-\n-\nGLM-4\n-\n87.6%\n47.9%\n-\n-\nOpen-Source Model\nInternLM2-Math\n20B\n82.6%\n37.7%\n-\n-\nQwen\n72B\n78.9%\n35.2%\n-\n-\nMath-Shepherd-Mistral\n7B\n84.1%\n33.0%\n-\n-\nWizardMath-v1.1\n7B\n83.2%\n33.0%\n-\n-\nDeepSeek-LLM-Chat\n67B\n84.1%\n32.6%\n74.0%\n80.3%\nMetaMath\n70B\n82.3%\n26.6%\n66.4%\n70.9%\nSeaLLM-v2\n7B\n78.2%\n27.5%\n64.8%\n-\nChatGLM3\n6B\n72.3%\n25.7%\n-\n-\nWizardMath-v1.0\n70B\n81.6%\n22.7%\n64.8%\n65.4%\nDeepSeekMath-Instruct 7B\n82.9%\n46.8%\n73.2%\n84.6%\nDeepSeekMath-RL\n7B\n88.2%\n51.7%\n79.6%\n88.8%\nTool-Integrated Reasoning\nClosed-Source Model\nGPT-4 Code Interpreter\n-\n97.0%\n69.7%\n-\n-\nOpen-Source Model\nInternLM2-Math\n20B\n80.7%\n54.3%\n-\n-\nDeepSeek-LLM-Chat\n67B\n86.7%\n51.1%\n76.4%\n85.4%\nToRA\n34B\n80.7%\n50.8%\n41.2%\n53.4%\nMAmmoTH\n70B\n76.9%\n41.8%\n-\n-\nDeepSeekMath-Instruct 7B\n83.7%\n57.4%\n72.0%\n84.3%\nDeepSeekMath-RL\n7B\n86.7%\n58.8%\n78.4%\n87.6%\nTable 5 | Performance of Open- and Closed-Source models with both Chain-of-Thought and\nTool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority\nvotes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all open-\nsource models from 7B to 70B, as well as the majority of closed-source models. Although\nDeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data\nof GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\n12\n\ud835\udc5e\ud835\udc5e\n\ud835\udc5c\ud835\udc5c!\n\ud835\udc5c\ud835\udc5c\"\n\ud835\udc5c\ud835\udc5c#\n\ud835\udc5f\ud835\udc5f!\n\ud835\udc5f\ud835\udc5f\"\n\ud835\udc5f\ud835\udc5f#\n\ud835\udc34\ud835\udc34!\n\ud835\udc34\ud835\udc34\"\n\ud835\udc34\ud835\udc34#\n\ud835\udc5e\ud835\udc5e\n\ud835\udc5c\ud835\udc5c\nGAE\n\ud835\udc34\ud835\udc34\n\ud835\udc5f\ud835\udc5f\n\ud835\udc63\ud835\udc63\nReward \nModel\nPolicy \nModel\nValue \nModel\n\u2026\n\u2026\n\u2026\nPolicy \nModel\nReference \nModel\nReward \nModel\nPPO\nGRPO\nTrained\nModels\nFrozen\nModels\nReference \nModel\n\u2295\n\ud835\udc3e\ud835\udc3e\ud835\udc3e\ud835\udc3e\n\ud835\udc3e\ud835\udc3e\ud835\udc3e\ud835\udc3e\nGroup \nComputation\nFigure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead\nestimating the baseline from group scores, significantly reducing training resources.\non the rewards {\ud835\udc5f\u2265\ud835\udc61} and a learned value function \ud835\udc49\ud835\udf13. Thus, in PPO, a value function needs to\nbe trained alongside the policy model and to mitigate over-optimization of the reward model,\nthe standard approach is to add a per-token KL penalty from a reference model in the reward at\neach token (Ouyang et al., 2022), i.e.,\n\ud835\udc5f\ud835\udc61 = \ud835\udc5f\ud835\udf11(\ud835\udc5e, \ud835\udc5c\u2264\ud835\udc61) \u2212 \ud835\udefd log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) ,\n(2)\nwhere \ud835\udc5f\ud835\udf11 is the reward model, \ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 is the reference model, which is usually the initial SFT model,\nand \ud835\udefd is the coefficient of the KL penalty.\nAs the value function employed in PPO is typically another model of comparable size as\nthe policy model, it brings a substantial memory and computational burden. Additionally,\nduring RL training, the value function is treated as a baseline in the calculation of the advantage\nfor variance reduction. While in the LLM context, usually only the last token is assigned a\nreward score by the reward model, which may complicate the training of a value function that is\naccurate at each token. To address this, as shown in Figure 4, we propose Group Relative Policy\nOptimization (GRPO), which obviates the need for additional value function approximation as\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response\nto the same question, as the baseline. More specifically, for each question \ud835\udc5e, GRPO samples a\ngroup of outputs {\ud835\udc5c1, \ud835\udc5c2, \u00b7 \u00b7 \u00b7 , \ud835\udc5c\ud835\udc3a} from the old policy \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 and then optimizes the policy model\nby maximizing the following objective:\nJ\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)]\n1\n\ud835\udc3a\n\ud835\udc3a\n\u2211\ufe01\n\ud835\udc56=1\n1\n|\ud835\udc5c\ud835\udc56|\n|\ud835\udc5c\ud835\udc56|\n\u2211\ufe01\n\ud835\udc61=1\n\u001a\nmin\n\u0014 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\u02c6\ud835\udc34\ud835\udc56,\ud835\udc61, clip\n\u0012 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61) , 1 \u2212 \ud835\udf00, 1 + \ud835\udf00\n\u0013\n\u02c6\ud835\udc34\ud835\udc56,\ud835\udc61\n\u0015\n\u2212 \ud835\udefdD\ud835\udc3e\ud835\udc3f\n\u0002\n\ud835\udf0b\ud835\udf03||\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53\n\u0003\u001b\n,\n(3)\nwhere \ud835\udf00 and \ud835\udefd are hyper-parameters, and \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 is the advantage calculated based on relative\nrewards of the outputs inside each group only, which will be detailed in the following subsec-\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with\nthe comparative nature of rewards models, as reward models are typically trained on datasets\nof comparisons between outputs on the same question. Also note that, instead of adding KL\npenalty in the reward, GRPO regularizes by directly adding the KL divergence between the\ntrained policy and the reference policy to the loss, avoiding complicating the calculation of \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61.\n13\nAlgorithm 1 Iterative Group Relative Policy Optimization\nInput initial policy model \ud835\udf0b\ud835\udf03init; reward models \ud835\udc5f\ud835\udf11; task prompts D; hyperparameters \ud835\udf00, \ud835\udefd, \ud835\udf07\n1: policy model \ud835\udf0b\ud835\udf03 \u2190 \ud835\udf0b\ud835\udf03init\n2: for iteration = 1, ..., I do\n3:\nreference model \ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 \u2190 \ud835\udf0b\ud835\udf03\n4:\nfor step = 1, ..., M do\n5:\nSample a batch D\ud835\udc4f from D\n6:\nUpdate the old policy model \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 \u2190 \ud835\udf0b\ud835\udf03\n7:\nSample \ud835\udc3a outputs {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\u00b7 | \ud835\udc5e) for each question \ud835\udc5e \u2208 D\ud835\udc4f\n8:\nCompute rewards {\ud835\udc5f\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 for each sampled output \ud835\udc5c\ud835\udc56 by running \ud835\udc5f\ud835\udf11\n9:\nCompute \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 for the \ud835\udc61-th token of \ud835\udc5c\ud835\udc56 through group relative advantage estimation.\n10:\nfor GRPO iteration = 1, ..., \ud835\udf07 do\n11:\nUpdate the policy model \ud835\udf0b\ud835\udf03 by maximizing the GRPO objective (Equation 21)\n12:\nUpdate \ud835\udc5f\ud835\udf11 through continuous training using a replay mechanism.\nOutput \ud835\udf0b\ud835\udf03\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the\nfollowing unbiased estimator (Schulman, 2020):\nD\ud835\udc3e\ud835\udc3f\n\u0002\n\ud835\udf0b\ud835\udf03||\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53\n\u0003\n=\n\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 log\n\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 1,\n(4)\nwhich is guaranteed to be positive.\n4.1.2. Outcome Supervision RL with GRPO\nFormally, for each question \ud835\udc5e, a group of outputs {\ud835\udc5c1, \ud835\udc5c2, \u00b7 \u00b7 \u00b7 , \ud835\udc5c\ud835\udc3a} are sampled from the old\npolicy model \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51. A reward model is then used to score the outputs, yielding \ud835\udc3a rewards\nr = {\ud835\udc5f1, \ud835\udc5f2, \u00b7 \u00b7 \u00b7 , \ud835\udc5f\ud835\udc3a} correspondingly. Subsequently, these rewards are normalized by subtracting\nthe group average and dividing by the group standard deviation. Outcome supervision provides\nthe normalized reward at the end of each output \ud835\udc5c\ud835\udc56 and sets the advantages \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 of all tokens in\nthe output as the normalized reward, i.e., \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 = e\ud835\udc5f\ud835\udc56 = \ud835\udc5f\ud835\udc56\u2212mean(r)\nstd(r)\n, and then optimizes the policy by\nmaximizing the objective defined in equation (3).\n4.1.3. Process Supervision RL with GRPO\nOutcome supervision only provides a reward at the end of each output, which may not be\nsufficient and efficient to supervise the policy in complex mathematical tasks. Following Wang\net al. (2023b), we also explore process supervision, which provides a reward at the end of\neach reasoning step. Formally, given the question \ud835\udc5e and \ud835\udc3a sampled outputs {\ud835\udc5c1, \ud835\udc5c2, \u00b7 \u00b7 \u00b7 , \ud835\udc5c\ud835\udc3a}, a\nprocess reward model is used to score each step of the outputs, yielding corresponding rewards:\nR = {{\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(1)\n1\n, \u00b7 \u00b7 \u00b7 , \ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(\ud835\udc3e1)\n1\n}, \u00b7 \u00b7 \u00b7 , {\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(1)\n\ud835\udc3a\n, \u00b7 \u00b7 \u00b7 , \ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65(\ud835\udc3e\ud835\udc3a)\n\ud835\udc3a\n}}, where \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65( \ud835\udc57) is the end token index\nof the \ud835\udc57-th step, and \ud835\udc3e\ud835\udc56 is the total number of steps in the \ud835\udc56-th output. We also normalize these\nrewards with the average and the standard deviation, i.e.,e\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65( \ud835\udc57)\n\ud835\udc56\n=\n\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65( \ud835\udc57)\n\ud835\udc56\n\u2212mean(R)\nstd(R)\n. Subsequently,\nthe process supervision calculates the advantage of each token as the sum of the normalized\nrewards from the following steps, i.e., \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 = \u00cd\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65( \ud835\udc57)\u2265\ud835\udc61e\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65( \ud835\udc57)\n\ud835\udc56\n, and then optimizes the policy by\nmaximizing the objective defined in equation (3).\n14\n4.1.4. Iterative RL with GRPO\nAs the reinforcement learning training process progresses, the old reward model may not be\nsufficient to supervise the current policy model. Therefore, we also explore the iterative RL\nwith GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the\nreward model based on the sampling results from the policy model and continually train the\nold reward model using a replay mechanism that incorporates 10% of historical data. Then, we\nset the reference model as the policy model, and continually train the policy model with the\nnew reward model.\n4.2. Training and Evaluating DeepSeekMath-RL\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-of-\nthought-format questions related to GSM8K and MATH from the SFT data, which consists\nof around 144K questions. We exclude other SFT questions to investigate the impact of RL\non benchmarks that lack data throughout the RL phase. We construct the training set of\nreward models following (Wang et al., 2023b). We train our initial reward model based on the\nDeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the\npolicy model as 1e-6. The KL coefficient is 0.04. For each question, we sample 64 outputs. The\nmax length is set to 1024, and the training batch size is 1024. The policy model only has a single\nupdate following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks\nfollowing DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with\nchain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks\ncan be regarded as out-of-domain tasks.\nTable 5 demonstrates the performance of open- and closed-source models with both chain-\nof-thought and tool-integrated reasoning on English and Chinese benchmarks. We find that:\n1) DeepSeekMath-RL 7B attains accuracies of 88.2% and 51.7% on GSM8K and MATH, respec-\ntively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source\nmodels in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,\nDeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of\nGSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope\nof its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,\nshowcasing the effectiveness of reinforcement learning.\n5. Discussion\nIn this section, we will share our findings in pre-training and RL experiments.\n5.1. Lessons Learnt in Pre-Training\nWe first share our experience in pre-training. Unless otherwise specified, we will adhere to\nthe training settings outlined in Section 2.2.1. It is worth noting that, when referring to the\nDeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of\nthe data collection process.\n5.1.1. Code Training Benefits Mathematical Reasoning\nA popular yet unverified hypothesis suggests that code training improves reasoning. We attempt\nto offer a partial response to this, particularly within the mathematical domain: code training\n15\nTraining Setting\nTraining Tokens\nw/o Tool Use\nw/ Tool Use\nGeneral Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python\nNo Continual Training\n\u2013\n\u2013\n\u2013\n2.9%\n3.0%\n12.3%\n2.7%\n2.3%\nTwo-Stage Training\nStage 1: General Training\n400B\n\u2013\n\u2013\n2.9%\n3.2%\n14.8%\n3.3%\n2.3%\nStage 2: Math Training\n\u2013\n\u2013\n150B\n19.1%\n14.4%\n37.2%\n14.3%\n6.7%\nStage 1: Code Training\n\u2013\n400B \u2013\n5.9%\n3.6%\n19.9%\n12.4%\n10.0%\nStage 2: Math Training\n\u2013\n\u2013\n150B\n21.9%\n15.3%\n39.7%\n17.4%\n9.4%\nOne-Stage Training\nMath Training\n\u2013\n\u2013\n150B\n20.5%\n13.1%\n37.6%\n11.4%\n6.5%\nCode & Math Mixed Training \u2013\n400B 150B\n17.6%\n12.1%\n36.3%\n19.7%\n13.5%\nTable 6 | Investigation of how code affects mathematical reasoning under different training\nsettings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning\nperformance without and with tool use via few-shot chain-of-thought prompting and few-shot\nprogram-of-thought prompting, respectively.\nimproves models\u2019 ability to do mathematical reasoning both with and without tool use.\nTo study how code training affects mathematical reasoning, we experimented with the\nfollowing two-stage training and one-stage training settings:\nTwo-Stage Training\n\u2022 Code Training for 400B Tokens \u2192 Math Training for 150B Tokens: We train DeepSeek-\nLLM 1.3B for 400B code tokens followed by 150B math tokens;\n\u2022 General Training for 400B Tokens \u2192 Math Training for 150B Tokens: As a control\nexperiment, we also experiment with general tokens (sampled from a large-scale general\ncorpus created by DeepSeek-AI) instead of code tokens in the first stage of training, in an\nattempt to investigate the advantages of code tokens over general tokens in improving\nmathematical reasoning.\nOne-Stage Training\n\u2022 Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\n\u2022 Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-\nlowing code training degrades coding performance. We investigate whether code tokens,\nwhen mixed with math tokens for one-stage training, would still improve mathematical\nreasoning and also alleviate the problem of catastrophic forgetting.\nResults\nTable 6 and Table 7 demonstrate the downstream performance under different training\nsettings.\nCode training benefits program-aided mathematical reasoning, both under the two-stage\ntraining and one-stage training settings. As shown in Table 6, under the two-stage training\nsetting, code training alone already significantly enhances the ability to solve GSM8K and\nMATH problems using Python. Math training in the second stage yields further improvements.\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-\ntively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also\nsynergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\n16\nTraining Setting\nTraining Tokens\nMMLU\nBBH\nHumanEval (Pass@1) MBPP (Pass@1)\nGeneral Code Math\nNo Continual Training\n\u2013\n\u2013\n\u2013\n24.5%\n28.1%\n12.2%\n13.0%\nTwo-Stage Training\nStage 1: General Training\n400B\n\u2013\n\u2013\n25.9%\n27.7%\n15.2%\n13.6%\nStage 2: Math Training\n\u2013\n\u2013\n150B\n33.1%\n32.7%\n12.8%\n13.2%\nStage 1: Code Training\n\u2013\n400B \u2013\n25.0%\n31.5%\n25.0%\n40.0%\nStage 2: Math Training\n\u2013\n\u2013\n150B\n36.2%\n35.3%\n12.2%\n17.0%\nOne-Stage Training\nMath Training\n\u2013\n\u2013\n150B\n32.3%\n32.5%\n11.6%\n13.2%\nCode & Math Mixed Training \u2013\n400B 150B\n33.5%\n35.6%\n29.3%\n39.4%\nTable 7 | Investigation of how different settings of code and math training affect model perfor-\nmance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM\n1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.\nOn HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively.\nModel\nSize ArXiv Corpus\nEnglish Benchmarks\nChinese Benchmarks\nGSM8K MATH OCW\nSAT\nMMLU\nSTEM\nCMATH\nGaokao\nMathCloze\nGaokao\nMathQA\nDeepSeek-LLM\n1.3B\nNo Math Training\n2.9%\n3.0%\n2.9% 15.6%\n19.5%\n12.3%\n0.8%\n17.9%\nMathPile\n2.7%\n3.3%\n2.2% 12.5%\n15.7%\n1.2%\n0.0%\n2.8%\nArXiv-RedPajama\n3.3%\n3.4%\n4.0%\n9.4%\n9.0%\n7.4%\n0.8%\n2.3%\nDeepSeek-Coder-Base-v1.5 7B\nNo Math Training\n29.0%\n12.5%\n6.6% 40.6%\n38.1%\n45.9%\n5.9%\n21.1%\nMathPile\n23.6%\n11.5%\n7.0% 46.9%\n35.8%\n37.9%\n4.2%\n25.6%\nArXiv-RedPajama\n28.1%\n11.1%\n7.7% 50.0%\n35.2%\n42.6%\n7.6%\n24.8%\nTable 8 | Effect of math training on different arXiv datasets. Model performance is evaluated\nwith few-shot chain-of-thought prompting.\nArXiv Corpus\nminiF2F-valid miniF2F-test\nNo Math Training\n20.1%\n21.7%\nMathPile\n16.8%\n16.4%\nArXiv-RedPajama\n14.8%\n11.9%\nTable 9 | Effect of math training on different arXiv corpora, the base model being DeepSeek-\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\nCode training also improves mathematical reasoning without tool use. Under the two-stage\ntraining setting, the initial stage of code training already results in moderate enhancements.\nIt also boosts the efficiency of the subsequent math training, eventually leading to the best\nperformance. However, combining code tokens and math tokens for one-stage training com-\npromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,\ndue to its limited scale, lacks the capacity to fully assimilate both code and mathematical data\nsimultaneously.\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev\net al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,\n17\ndetailed analysis regarding their impact on mathematical reasoning has not been extensively\nconducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem\nineffective in improving mathematical reasoning. We experiment with models of different sizes,\nincluding DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv\ncorpora that underwent varied processing pipelines:\n\u2022 MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and filtering\nheuristic rules, over 85% of which are scientific arXiv papers;\n\u2022 ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX files with preambles,\ncomments, macros, and bibliographies removed, totaling 28.0B tokens.\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective\nin improving mathematical reasoning. When trained on a arXiv-only corpus, both models dis-\nplay no notable improvements or even deterioration across various mathematical benchmarks of\ndifferent complexities employed in this study. These benchmarks include quantitative reasoning\ndatasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table\n8), and formal mathematics like miniF2F (Table 9).\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We\nhave not yet studied:\n\u2022 The impact of arXiv tokens on specific math-related tasks not included in this research,\nsuch as informalization of theorems which is to convert formal statements or proofs to\ntheir informal versions;\n\u2022 The effect of arXiv tokens when combined with other types of data;\n\u2022 Whether the benefits of arXiv papers would manifest themselves at a larger model scale.\nThus, further exploration is required, which we leave for future studies.\n5.2. Insights of Reinforcement Learning\n5.2.1. Towards to a Unified Paradigm\nIn this section, we provide a unified paradigm to analyze different training methods, such as\nSFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the\nunified paradigm. Generally, the gradient with respect to the parameter \ud835\udf03 of a training method\ncan be written as:\n\u2207\ud835\udf03JA(\ud835\udf03) = E[(\ud835\udc5e, \ud835\udc5c) \u223c D\n|       {z       }\n\ud835\udc37\ud835\udc4e\ud835\udc61\ud835\udc4e \ud835\udc46\ud835\udc5c\ud835\udc62\ud835\udc5f\ud835\udc50\ud835\udc52\n]\n\u00a9\u00ad\u00ad\u00ad\n\u00ab\n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc3a\ud835\udc36A(\ud835\udc5e, \ud835\udc5c, \ud835\udc61, \ud835\udf0b\ud835\udc5f \ud835\udc53)\n|               {z               }\n\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc51\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc36\ud835\udc5c\ud835\udc52 \ud835\udc53 \ud835\udc53 \ud835\udc56\ud835\udc50\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc61\n\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\u00aa\u00ae\u00ae\u00ae\n\u00ac\n.\n(5)\nThere exist three key components: 1) Data Source D, which determines the training data; 2)\nReward Function \ud835\udf0b\ud835\udc5f \ud835\udc53, which is the source of the training reward signal; 3) Algorithm A: which\nprocesses the training data and the reward signal to the gradient coefficient \ud835\udc3a\ud835\udc36 that determines\nthe magnitude of the penalty or reinforcement for the data. We analyze several representative\nmethods based on such a unified paradigm:\n\u2022 Supervised Fine-tuning (SFT): SFT fine-tunes pretrained model on human selected SFT\ndata.\n18\nMethods\nData Source\nReward Function\nGradient Coefficient\nSFT\n\ud835\udc5e, \ud835\udc5c \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44, \ud835\udc42)\n-\n1\nRFT\n\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)\nRule\nEquation 10\nDPO\n\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c+, \ud835\udc5c\u2212 \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)\nRule\nEquation 14\nOnline RFT\n\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03(\ud835\udc42|\ud835\udc5e)\nRule\nEquation 10\nPPO\n\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03(\ud835\udc42|\ud835\udc5e)\nModel\nEquation 18\nGRPO\n\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c \ud835\udf0b\ud835\udf03(\ud835\udc42|\ud835\udc5e)\nModel\nEquation 21\nTable 10 | The data source and gradient coefficient of different methods. \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61 denotes the data\ndistribution of supervised fine-tuning datasets. \ud835\udf0b\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61 and \ud835\udf0b\ud835\udf03 denote the supervised fine-tuned\nmodel and the real-time policy model during the online training process, respectively.\n0\n2000\n4000\n6000\n8000\nSteps\n56\n58\n60\n62\n64\n66\nAcc (%)\nGSM8K\n0\n2000\n4000\n6000\n8000\nSteps\n27\n28\n29\n30\nAcc (%)\nMATH\nRFT\nOnline RFT\nGRPO+OS\nGRPO+PS\nFigure 5 | Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained\nusing various methods, on two benchmarks.\n\u2022 Rejection Sampling Fine-tuning (RFT): RFT further fine-tunes the SFT model on the\nfiltered outputs sampled from the SFT model based on SFT questions. RFT filters the\noutputs based on the correctness of their answers.\n\u2022 Direct Preference Optimization (DPO): DPO further refines the SFT model by fine-tuning\nit on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\n\u2022 Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT\ninitiates the policy model using the SFT model and refines it by fine-tuning with the\naugmented outputs sampled from the real-time policy model.\n\u2022 PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces\nit with the outputs sampled from the real-time policy model.\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\nmore detailed derivation process.\nObservation about Data Source\nWe divide the data source into two categories, online sam-\npling, and offline sampling. Online sampling denotes that the training data is from the explo-\nration results of the real-time training policy model, while offline sampling denotes that the\n19\n0\n1300\n2300\n3300\n4300\n5300\nSteps\n83\n84\n85\n86\n87\n88\n89\nAcc (%)\nGSM8K\n0\n1300\n2300\n3300\n4300\n5300\nSteps\n47\n48\n49\n50\n51\n52\nAcc (%)\nMATH\nIteration-0\nIteration-1\nIteration-2\nFigure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on\ntwo benchmarks.\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the\noffline style, while Online RFT and GRPO follow the online style.\nAs shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two\nbenchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but\ngains an absolute advantage in the later stage, demonstrating the superiority of online training.\nThis is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,\nwith the sampled data revealing only minor differences. In the later stage, however, the data\nsampled from the actor will exhibit more significant differences, and real-time data sampling\nwill offer greater advantages.\nObservation about Gradient Coefficient\nThe algorithm processes the reward signal to the\ngradient coefficient to update the model parameter. We divide the reward function as \u2018Rule\u2019\nand \u2018Model\u2019 in our experiments. Rule refers to judging the quality of a response based on\nthe correctness of the answer, and Model denotes that we train a reward model to score each\nresponse. The training data of the reward model is based on the rule judgment. Equations 10\nand 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its\ngradient coefficient based on the reward value provided by the reward model. This allows for\ndifferential reinforcement and penalization of responses according to their varying magnitudes.\nIn contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly\nreinforces all responses with correct answers at the same level of intensity.\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the efficiency\nof altering positive and negative gradient coefficients. In addition, GRPO+PS shows superior\nperformance compared to GRPO+OS, indicating the benefits of using fine-grained, step-aware\ngradient coefficients. Furthermore, we explore the iterative RL, in our experiments, we conduct\ntwo rounds of iteration. As shown in Figure 6, we notice that the iterative RL significantly\nimproves the performance, especially at the first iteration.\n20\n1\n4\n8\n16\n32\n64\nK: The number of candidates\n82\n84\n86\n88\n90\n92\n94\n96\n98\nAcc (%)\nGSM8K\n1\n4\n8\n16\n32\n64\nK: The number of candidates\n45\n50\n55\n60\n65\n70\n75\n80\n85\nAcc (%)\nMATH\nMaj@K-Instruct\nMaj@K-RL\nPass@K-Instruct\nPass@K-RL\nFigure 7 | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH\n(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\n5.2.2. Why RL Works?\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning\ndata, and it achieves significant performance enhancement upon the instruction tuning model.\nTo further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K\naccuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances\nMaj@K\u2019s performance but not Pass@K. These findings indicate that RL enhances the model\u2019s\noverall performance by rendering the output distribution more robust, in other words, it seems\nthat the improvement is attributed to boosting the correct response from TopK rather than\nthe enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a\nmisalignment problem in reasoning tasks within the SFT model, showing that the reasoning\nperformance of SFT models can be improved through a series of preference alignment strategies\n(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\n5.2.3. How to Achieve More Effective RL?\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified\nparadigm to understand different representative training methods. Within this paradigm, all\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in\nEquation 5, there exist three key components: Data Source, Algorithm, and Reward Function.\nWe provide some potential future directions about the three components.\nData Source\nData source is the raw material of all training methods. In the context of RL, we\nspecifically refer to the data source as the unlabeled questions with the outputs sampled from\nthe policy model. In this paper, we only use the questions from the instruction tuning stage and\na naive nucleus sampling to sample outputs. We think this is a potential reason that our RL\npipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)\nstrategies, like those based on tree-search methods (Yao et al., 2023). Also, the efficient inference\ntechniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines\n21\nthe exploration efficiency of policy models, also play an exceedingly important role.\nAlgorithms\nAlgorithms process the data and reward signal to the gradient coefficient to update\nthe model parameter. Based on Equation 5, to some extent, all methods now fully TRUST the\nsignal of the reward function to increase or decrease the conditional probability of a certain\ntoken. However, it is impossible to ensure the reward signal is always reliable, especially in\nextremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023),\nwhich have been carefully annotated by well-trained annotators, still contain approximately 20%\nof incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm\nthat is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\n2023) alignment methods will bring a fundamental change to the learning algorithms.\nReward Function\nReward function is the source of the training signal. In RL, the reward\nfunction is usually the neural reward model. We think there exist three important directions for\nreward models: 1) How to enhance the generalization ability of the reward model. The reward\nmodel must be effectively generalized to handle out-of-distribution questions and advanced\ndecoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of\nLLMs rather than improve their fundamental capabilities; 2) How to reflect the uncertainty\nof reward model. The uncertainty could potentially act as a linking bridge between the weak\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-\nquality process reward models that can provide fine-grained training signals for the reasoning\nprocess (Lightman et al., 2023; Wang et al., 2023b).\n6. Conclusion, Limitation, and Future Work\nWe present DeepSeekMath, which outperforms all open-source models on the competition-\nlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is\ninitialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with\na significant component of the training data being 120B math tokens sourced from Common\nCrawl. Our extensive ablation study shows web pages offer significant potential for high-quality\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group\nRelative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which\ncan notably improve mathematical reasoning capabilities with less memory consumption. The\nexperiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached\na high score on benchmarks. We also provide a unified paradigm to understand a series of\nmethods and summarize several potential directions for more effective reinforcement learning.\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,\nits capability on geometry and theorem-proof are relatively weaker than closed models. For\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,\nwhich may indicate data selection bias in pre-training and fine-tuning. In addition, restricted\nby the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could\nimprove its performance with few-shot inputs, while DeepSeekMath shows similar performance\nin zero-shot and few-shot evaluation. In the future, we will further improve our engineered\ndata selection pipeline to construct more high-quality pre-trained corpus. In addition, we will\nexplore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\n7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852\n22\nReferences\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\nK. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,\nE. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,\nB. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,\nK. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,\nB. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,\nM. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal\nmodels. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https:\n//doi.org/10.48550/arXiv.2312.11805.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\n2021.\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Bider-\nman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint\narXiv:2310.10631, 2023.\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen\ntechnical report. arXiv preprint arXiv:2309.16609, 2023.\nC. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,\nM. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with\nweak supervision. arXiv preprint arXiv:2312.09390, 2023.\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\nom/THUDM/ChatGLM3.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\nURL https://arxiv.org/abs/2107.03374.\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling\ncomputation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL\nhttps://github.com/togethercomputer/RedPajama-Data.\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,\nabs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485\n50/arXiv.2401.02954.\n23\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335,\n2022.\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: program-\naided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\nJ. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,\npages 10764\u201310799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.\nhtml.\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-\nintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\n2.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming\n\u2013 the rise of code intelligence, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nHigh-flyer. Hai-llm: \u9ad8\u6548\u4e14\u8f7b\u91cf\u7684\u5927\u6a21\u578b\u8bad\u7ec3\u5de5\u5177, 2023. URL https://www.high-flyer.c\nn/en/blog/hai-llm.\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.\nA. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,\nsketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint\narXiv:2210.12283, 2022.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, and T. Mikolov. Fasttext. zip: Compress-\ning text classification models. arXiv preprint arXiv:1612.03651, 2016.\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\nEfficient memory management for large language model serving with pagedattention. In\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\ndecoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR,\n2023.\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with\nlanguage models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022a.\n24\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone,\nC. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving\nquantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems\n35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New\nOrleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.\ncc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr\nact-Conference.html.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\nI. Sutskever, and K. Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nH. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.\nWizardmath: Empowering mathematical reasoning for large language models via reinforced\nevol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nS. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sab-\nharwal, P. Clark, and A. Kalyan. LILA: A unified benchmark for mathematical reasoning.\nIn Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 5807\u20135832. Association for Computational Linguistics,\n2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\n2022.emnlp-main.392.\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,\nC. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,\nabs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485\n50/arXiv.2312.00738.\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality\nmathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\nhttps://doi.org/10.48550/arXiv.2310.06786.\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between auto-\nmatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,\nProceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010,\nEdinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1\u201310.\nEasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,\nabs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference\noptimization: Your language model is secretly a reward model. 2023.\n25\nJ. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app\nrox.html.\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\nD. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nfR3wGCk-IXp.\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.\nM. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro\nsoft.com/ai-anthology/terence-tao/.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\n2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\n09288.\nT. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human\ndemonstrations. Nature, 625(7995):476\u2013482, 2024.\nP. Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify\nand reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\nZ. Wang, R. Xia, and P. Liu. Generative AI for math: Part I - mathpile: A billion-token-scale\npretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.\nURL https://doi.org/10.48550/arXiv.2312.17120.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\nURL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf\n4f15af0f7b31abca4-Abstract-Conference.html.\n26\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese\nelementary school math test?, 2023.\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.\nMu\u00f1oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International\nConference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170\nof Lecture Notes in Computer Science, pages 33\u201338. Springer, 2008. doi: 10.1007/978-3-540-7\n1067-7\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7.\nH. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting\nspeculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,\neditors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909\u2013\n3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\nH. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking efficiency\nin large language model inference: A comprehensive survey of speculative decoding. arXiv\npreprint arXiv:2401.07851, 2024.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\nDeliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,\n2023.\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.\nMetamath: Bootstrap your own mathematical questions for large language models. CoRR,\nabs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\n50/arXiv.2309.12284.\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning\nmathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align\nlanguage models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building\nmath generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:\n10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level\nmathematics. arXiv preprint arXiv:2109.00110, 2021.\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A\nhuman-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.\ndoi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\n27\nA. Appendix\nA.1. Analysis of Reinforcement Learning\nWe provide the detailed derivation of the data source and gradient coefficient (algorithm and\nreward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and\nGRPO.\nA.1.1. Supervised Fine-tuning\nThe objective of Supervised Fine-tuning is maximizing the following objective:\nJ\ud835\udc46\ud835\udc39\ud835\udc47 (\ud835\udf03) = E[\ud835\udc5e, \ud835\udc5c \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44, \ud835\udc42)]\n \n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nlog \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n!\n.\n(6)\nThe gradient of J\ud835\udc46\ud835\udc39\ud835\udc47 (\ud835\udf03) is:\n\u2207\ud835\udf03J\ud835\udc46\ud835\udc39\ud835\udc47 = E[\ud835\udc5e, \ud835\udc5c \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44, \ud835\udc42)]\n \n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\n\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n!\n.\n(7)\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human\nselection. Gradient Coefficient: always set to 1.\nA.1.2. Rejection Sampling Fine-tuning\nRejection Sampling Fine-tuning first samples multiple outputs from the supervised fine-tuned\nLLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.\nFormally, the objective of RFT is to maximize the following objectives:\nJ\ud835\udc45\ud835\udc39\ud835\udc47 (\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)]\n \n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nI(\ud835\udc5c) log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n!\n.\n(8)\nThe gradient of J\ud835\udc45\ud835\udc39\ud835\udc47 (\ud835\udf03) is:\n\u2207\ud835\udf03J\ud835\udc45\ud835\udc39\ud835\udc47 (\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)]\n \n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nI(\ud835\udc5c)\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n!\n.\n(9)\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\nRule (whether the answer is correct or not). Gradient Coefficient:\n\ud835\udc3a\ud835\udc36\ud835\udc45\ud835\udc39\ud835\udc47 (\ud835\udc5e, \ud835\udc5c, \ud835\udc61) = I(\ud835\udc5c) =\n(\n1\nthe answer of o is correct\n0\nthe answer of o is incorrect\n(10)\nA.1.3. Online Rejection Sampling Fine-tuning\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled\nfrom the real-time policy model \ud835\udf0b\ud835\udf03, rather than from the SFT model \ud835\udf0b\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61. Therefore, the gradient\nof online RFT is:\n\u2207\ud835\udf03J\ud835\udc42\ud835\udc5b\ud835\udc45\ud835\udc39\ud835\udc47 (\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03(\ud835\udc42|\ud835\udc5e)]\n \n1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nI(\ud835\udc5c)\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n!\n.\n(11)\n28\nA.1.4. Direct Preference Optimization (DPO)\nThe objective of DPO is:\nJ\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c+, \ud835\udc5c\u2212 \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)] log \ud835\udf0e \u00a9\u00ad\n\u00ab\n\ud835\udefd 1\n|\ud835\udc5c+|\n|\ud835\udc5c+|\n\u2211\ufe01\n\ud835\udc61=1\nlog\n\ud835\udf0b\ud835\udf03(\ud835\udc5c+\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c+\n<\ud835\udc61)\n\ud835\udf0bref(\ud835\udc5c+\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c+\n<\ud835\udc61) \u2212 \ud835\udefd 1\n|\ud835\udc5c\u2212|\n|\ud835\udc5c\u2212 |\n\u2211\ufe01\n\ud835\udc61=1\nlog\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\u2212\n<\ud835\udc61|\ud835\udc5e, \ud835\udc5c\u2212\n<\ud835\udc61)\n\ud835\udf0bref(\ud835\udc5c\u2212\n<\ud835\udc61|\ud835\udc5e, \ud835\udc5c\u2212\n<\ud835\udc61)\n\u00aa\u00ae\n\u00ac\n(12)\nThe gradient of J\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udf03) is:\n\u2207\ud835\udf03J\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c+, \ud835\udc5c\u2212 \u223c \ud835\udf0b\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc42|\ud835\udc5e)] \u00a9\u00ad\n\u00ab\n1\n|\ud835\udc5c+|\n|\ud835\udc5c+|\n\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc3a\ud835\udc36\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udc5e, \ud835\udc5c, \ud835\udc61)\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c+\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c+\n<\ud835\udc61)\n\u2212\n1\n|\ud835\udc5c\u2212|\n|\ud835\udc5c\u2212 |\n\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc3a\ud835\udc36\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udc5e, \ud835\udc5c, \ud835\udc61)\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\u2212\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c\u2212\n<\ud835\udc61)\u00aa\u00ae\n\u00ac\n(13)\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\nhuman preference in the general domain (can be \u2018Rule\u2019 in mathematical tasks). Gradient\nCoefficient:\n\ud835\udc3a\ud835\udc36\ud835\udc37\ud835\udc43\ud835\udc42(\ud835\udc5e, \ud835\udc5c, \ud835\udc61) = \ud835\udf0e\n\u0012\n\ud835\udefd log\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\u2212\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c\u2212\n<\ud835\udc61)\n\ud835\udf0bref(\ud835\udc5c\u2212\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c\u2212\n<\ud835\udc61) \u2212 \ud835\udefd log\n\ud835\udf0b\ud835\udf03(\ud835\udc5c+\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c+\n<\ud835\udc61)\n\ud835\udf0bref(\ud835\udc5c+\n\ud835\udc61 |\ud835\udc5e, \ud835\udc5c+\n<\ud835\udc61)\n\u0013\n(14)\nA.1.5. Proximal Policy Optimization (PPO)\nThe objective of PPO is:\nJ\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)] 1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\nmin\n\u0014 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) \ud835\udc34\ud835\udc61, clip\n\u0012 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) , 1 \u2212 \ud835\udf00, 1 + \ud835\udf00\n\u0013\n\ud835\udc34\ud835\udc61\n\u0015\n.\n(15)\nTo simplify the analysis, it is assumed that the model only has a single update following each\nexploration stage, thereby ensuring that \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 = \ud835\udf0b\ud835\udf03. In this case, we can remove the min and clip\noperation:\nJ\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)] 1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61) \ud835\udc34\ud835\udc61.\n(16)\nThe gradient of J\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udf03) is:\n\u2207\ud835\udf03J\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), \ud835\udc5c \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)] 1\n|\ud835\udc5c|\n|\ud835\udc5c|\n\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc34\ud835\udc61\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc61|\ud835\udc5e, \ud835\udc5c<\ud835\udc61)\n(17)\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\nreward model. Gradient Coefficient:\n\ud835\udc3a\ud835\udc36\ud835\udc43\ud835\udc43\ud835\udc42(\ud835\udc5e, \ud835\udc5c, \ud835\udc61, \ud835\udf0b\ud835\udf03\ud835\udc5f\ud835\udc5a) = \ud835\udc34\ud835\udc61,\n(18)\nwhere \ud835\udc34\ud835\udc61 is the advantage, which is computed by applying Generalized Advantage Estimation\n(GAE) (Schulman et al., 2015), based on the rewards {\ud835\udc5f\u2265\ud835\udc61} and a learned value function \ud835\udc49\ud835\udf13.\nA.1.6. Group Relative Policy Optimization (GRPO)\nThe objective of GRPO is (assume \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 = \ud835\udf0b\ud835\udf03 for simplified analysis):\nJ\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)]\n1\n\ud835\udc3a\n\ud835\udc3a\n\u2211\ufe01\n\ud835\udc56=1\n1\n|\ud835\udc5c\ud835\udc56|\n|\ud835\udc5c\ud835\udc56|\n\u2211\ufe01\n\ud835\udc61=1\n\u0014 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 \u2212 \ud835\udefd(\n\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 log\n\ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 1)\n\u0015\n.\n(19)\n29\nThe gradient of J\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42(\ud835\udf03) is:\n\u2207\ud835\udf03J\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42(\ud835\udf03) = E[\ud835\udc5e \u223c \ud835\udc43\ud835\udc60 \ud835\udc53\ud835\udc61(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)]\n1\n\ud835\udc3a\n\ud835\udc3a\n\u2211\ufe01\n\ud835\udc56=1\n1\n|\ud835\udc5c\ud835\udc56|\n|\ud835\udc5c\ud835\udc56|\n\u2211\ufe01\n\ud835\udc61=1\n\u0014\n\u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 + \ud835\udefd\n\u0012 \ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 1\n\u0013\u0015\n\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5e, \ud835\udc5c\ud835\udc56,<\ud835\udc61).\n(20)\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\nreward model. Gradient Coefficient:\n\ud835\udc3a\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42(\ud835\udc5e, \ud835\udc5c, \ud835\udc61, \ud835\udf0b\ud835\udf03\ud835\udc5f\ud835\udc5a) = \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 + \ud835\udefd\n\u0012 \ud835\udf0b\ud835\udc5f\ud835\udc52 \ud835\udc53 (\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5c\ud835\udc56,<\ud835\udc61)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56,\ud835\udc61|\ud835\udc5c\ud835\udc56,<\ud835\udc61) \u2212 1\n\u0013\n,\n(21)\nwhere \u02c6\ud835\udc34\ud835\udc56,\ud835\udc61 is computed based on the group reward scores.\n30\n"
  },
  {
    "title": "Training-Free Consistent Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2402.03286.pdf",
    "upvote": "57",
    "text": "Training-Free Consistent Text-to-Image Generation\nYOAD TEWEL, NVIDIA, Tel-Aviv University\nOMRI KADURI, Independent Scientist\nRINON GAL, NVIDIA, Tel-Aviv University\nYONI KASTEN, NVIDIA\nLIOR WOLF, Tel-Aviv University\nGAL CHECHIK, NVIDIA\nYUVAL ATZMON, NVIDIA\n\u201cA hyper-realistic \ndigital painting \nof a happy girl, \nbrown eyes.\u201d\n\u201cwearing headphones, \nwith her cat\u201d\n\u201chaving a countryside \npicnic with her cat\u201d\n\u201cin the snow, wearing \nheadphones\u201d\n\u201ccooking some food\u201d\n\u201chiking with her cat, \nin the mountains\u201d\n\u201cAn old story \nillustration of \na kid.\u201d\n\u201cwearing a school \nuniform\u201d\n\u201cwalking with \nhis mom\u201d\n\u201cclimbing a tree\u201d\n\u201creading a book\u201d\n\u201ceating his food\u201d\nSingle Subject\nMulti Subject\nSubject Description\n\u201cA photo of an \nold man \nwearing a hat.\u201d\n\u201cwalking in the \npark\u201d\n\u201cwriting numbers \non a blackboard\u201d\n\u201cdrinking a beer \nin the bar\u201d\n\u201cfeeding a stray cat\u201d\n\u201ceating dinner in a \nrestaurant\u201d\nFig. 1. ConsiStory transforms a set of input prompts with recurring subjects into a series of images that maintain the same subject identity and adhere to\nthe provided text. It can also maintain consistent identities for multiple subjects. Importantly, ConsiStory does not involve any optimization or pre-training.\nText-to-image models offer a new level of creative flexibility by allowing\nusers to guide the image generation process through natural language. How-\never, using these models to consistently portray the same subject across\ndiverse prompts remains challenging. Existing approaches fine-tune the\nmodel to teach it new words that describe specific user-provided subjects or\nadd image conditioning to the model. These methods require lengthy per-\nsubject optimization or large-scale pre-training. Moreover, they struggle to\nalign generated images with text prompts and face difficulties in portraying\nmultiple subjects. Here, we present ConsiStory, a training-free approach that\nenables consistent subject generation by sharing the internal activations of\nthe pretrained model. We introduce a subject-driven shared attention block\nand correspondence-based feature injection to promote subject consistency\nbetween images. Additionally, we develop strategies to encourage layout\ndiversity while maintaining subject consistency. We compare ConsiStory to a\nrange of baselines, and demonstrate state-of-the-art performance on subject\nconsistency and text alignment, without requiring a single optimization step.\nFinally, ConsiStory can naturally extend to multi-subject scenarios, and even\nenable training-free personalization for common objects.\nCode will be available at our project page.\n1\nINTRODUCTION\nLarge-scale text-to-image (T2I) diffusion models empower users\nto create imaginative scenes from text, but their stochastic nature\nposes challenges when trying to portray visually consistent subjects\nacross an array of prompts. Such consistency is crucial for many\napplications: from illustrating books and stories, through designing\nvirtual assets, to creating graphic novels and synthetic data.\nIn the field of consistent image generation, current approaches\n[4, 11, 26, 33] predominantly rely on personalization, a process where\nthe text-to-image model learns a new word to represent a specific\nsubject in a given image set. However, these personalization-based\nmethods suffer from several drawbacks: They require per-subject\ntraining; they struggle to portray multiple consistent subjects si-\nmultaneously in one image; and they can suffer from trade-offs\nbetween subject consistency and prompt-alignment. Alternatives,\nlike training image-conditioned diffusion models (e.g. using an en-\ncoder [14, 54, 57]), require significant computational resources, and\narXiv:2402.03286v1  [cs.CV]  5 Feb 2024\n2\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\ntheir extension to multi-object scenes remains unclear. A common\nthread in all these approaches is that they attempt to enforce consis-\ntency a posteriori. That is, they operate to make generated images\nconsistent with a specific, given target. Such approaches have two\ndrawbacks. They are bound to constrain the model\u2019s \u201ccreativity\" to\nthe given target image, and they tend to drive the model away from\nits training distribution.\nWe show here that the limitations of a posteriori methods can be\navoided, and propose a way to achieve consistency in a zero-shot\nmanner - without conditioning on existing images. The key idea\nis to promote cross-frame consistency a priori during generation.\nTo achieve this, we leverage the internal feature representations of\nthe diffusion model to align the generated images with each other,\nwithout any need to further align them with an external source.\nIn doing so, we can enable on-the-fly consistent generation (Fig.\n1), without requiring lengthy training or backpropagation, making\ngeneration roughly \u00d720 faster than the current state-of-the-art.\nOur approach operates in three steps. First, we localize the subject\nacross a set of noisy generated images. We then encourage subject\nconsistency by allowing each generated image to attend to subject\npatches in other frames via an extension of the self-attention mech-\nanism. This leads to more consistent subjects across the batch but\ncauses the layout diversity to greatly diminish, as observed in other\ncontexts that use similar extension [20]. Our second step is, there-\nfore, to maintain diversity in two ways: by incorporating features\nfrom a vanilla, non-consistent sampling step, and by introducing\na new inference-time dropout on the shared keys and values. Fi-\nnally, we aim to enhance consistency in finer details. To achieve this,\nwe align the self-attention output features between corresponding\nsubject pixels across the entire set.\nOur full method, which we term ConsiStory, combines these com-\nponents to enable training-free consistent generation. We compare\nConsiStory to prior approaches and demonstrate that by aligning\nfeatures during the generative process, we not only substantially\nspeed up the process, but also maintain better prompt-alignment.\nImportantly, our method is trivial to extend to multi-subject scenes,\navoiding pitfalls introduced by personalization-based approaches.\nFinally, we show that ConsiStory is compatible with existing edit-\ning tools like ControlNet [59], we introduce methods for re-using\nthe consistent identities, and even apply our ideas to training-free\npersonalization for common object classes, being the first to show\ntraining-free personalization, with no encoder use.\nIn summary, this paper makes the following contributions: First,\nwe present a training-free method for achieving subject consistency\nacross varying prompts. Second, we develop new techniques to com-\nbat layout collapse in extend-attention applications. Additionally,\nwe share a new benchmark dataset for consistency evaluation.\n2\nRELATED WORK\nConsistent T2I and storybook generation Consistent character\ngeneration has been studied largely in the context of storybook\ngeneration [26, 33]. Early works utilized extensive fine-tuning and\npersonalization [13, 45] to promote consistency. Jeong et al. [26]\nreplace character faces by combining textual inversion [13] and\nimage editing techniques. Gong et al. [16] iteratively generate multi-\ncharacter images using models personalized with LoRA [46], and\nrequire pre-training a text-to-layout model. Others [11, 33, 39] fine-\ntune a T2I model on storyboard datasets, enabling conditioning on\nimage frames. This resembles encoder-based personalization meth-\nods [2, 14, 27, 31, 47] like IP-Adapter [57] and ELITE [54]. Richardson\net al. [44] optimize a token embedding to generate consistent, novel\nconcepts. In a concurrent work, [4] train a personalized LoRA model\nby iteratively generating images, clustering them to find similar out-\nputs, learning a personalized model on a highly-similar cluster, and\nrepeating the process.\nConsiStory does not tune or personalize the pre-trained T2I model.\nInstead, it leverages cross-frame feature sharing to promote subject\nconsistency during generation.\nAttention-based consistency In the realm of videos, a common\npractice is to increase temporal consistency by sharing self-attention\nkeys and values [55] across frames. This can be done for genera-\ntion [8, 28, 55] or for video editing [15, 43]. Others use attention\nkeys and values from a source image in order to inject a consistent\nidentity across video frames [9, 23, 51, 56].\nWhen considering images, early works in text-based editing [19,\n40, 53] proposed to maintain the structure of an image by extracting\nits attention masks or features, and injecting them into follow-up\ngenerations. Others optimized image latents or model conditions\nsuch that attention maps created during generation will align with\nsome external mask [3, 29].\nMore recent work explored extended-attention mechanisms to\nmaintain consistent appearances when modifying image layouts [7,\n36], or for training-free appearance- [1] and style-transfer [20] tasks.\nOur method draws on these attention-sharing ideas but applies\nthem to the task of consistent T2I generation. We do not draw\nfeatures from existing images or align entire frames, but develop\ntools to enable subject-level consistency across novel images.\nAppearance transfer using dense correspondence maps has\nbeen widely studied. Liao et al. [32] transfer appearance between\nimages with similar structures using VGG-based maps. Others [6, 52]\ntrained generative models to leverage these mappings for image-\nto-image translation. Recently, diffusion models have been found\nto establish strong zero-shot correspondence between images [18,\n34, 58], enabling applications like instance swapping, image editing,\nand robust registration.\nHere, we leverage the diffusion-based DIFT maps [49] to share\nfeatures across multiple images and encourage the generation of\nsubjects with consistent appearance. This aligns features throughout\nthe denoising process rather than doing appearance transfer as a\npost-hoc step.\n3\nPRELIMINARIES: SELF-ATTENTION IN T2I MODELS\nOur method manipulates self-attention in T2I diffusion models. We\nstart by outlining its mechanism and introducing key notations.\nA self-attention layer receives a series of tokens, each of which\ncontains features describing a single image patch. Each such token\nundergoes linear projections through three self-attention matrices:\n\ud835\udc7e\ud835\udc3e, \ud835\udc7e\ud835\udc49 and \ud835\udc7e\ud835\udc44. The results of these projections are known as\n\u201cKeys\u201d, \u201cValues\u201d and \u201cQueries\u201d, respectively.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n3\nPrompt\nSet\nDiffusion U-net\nSD\nSA\nSD\nSA\nSD\nSA\n..\nSubject Driven Self-Attention\nFI\nFI\nFI\nSubject\nLocalization\n.\nSelf-Attention\nDropout\nMasked Attention\nFig. 2. Architecture outline (left): Given a set of prompts, at every generation step we localize the subject in each generated image \ud835\udc3c\ud835\udc56. We utilize the\ncross-attention maps up to the current generation step, to create subject masks \ud835\udc40\ud835\udc56. Then, we replace the standard self-attention layers in the U-net decoder\nwith Subject Driven Self-Attention layers that share information between subject instances. We also add Feature Injection for additional refinement. Subject\nDriven Self-Attention (right): We extend the self-attention layer so the Query from generated image \ud835\udc3c\ud835\udc56 will also have access to the Keys from all other\nimages in the batch (\ud835\udc3c\ud835\udc57, where \ud835\udc57 \u2260 \ud835\udc56), restricted by their subject masks \ud835\udc40\ud835\udc57. To enrich diversity we: (1) Weaken the SDSA via dropout and (2) Blend Query\nfeatures with vanilla Query features from a non-consistent sampling step, yielding \ud835\udc44\u2217\n1.\nMore concretely, consider the \ud835\udc56\ud835\udc61\u210e image entry in the generated\nbatch. Let \ud835\udc65\ud835\udc56 \u2208 R\ud835\udc43\u00d7\ud835\udc51 be a sequence of \ud835\udc43 input token vectors with\nfeature dimension \ud835\udc51. We define \ud835\udc3e\ud835\udc56 = \ud835\udc65\ud835\udc56 \u00b7 \ud835\udc7e\ud835\udc3e, \ud835\udc49\ud835\udc56 = \ud835\udc65\ud835\udc56 \u00b7 \ud835\udc7e\ud835\udc49 , \ud835\udc44\ud835\udc56 =\n\ud835\udc65\ud835\udc56 \u00b7 \ud835\udc7e\ud835\udc44. The self-attention map is then given by:\n\ud835\udc34\ud835\udc56 = softmax\n\u0010\n\ud835\udc44\ud835\udc56\ud835\udc3e\u22a4\n\ud835\udc56 /\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n\u0011\n\u2208 R\ud835\udc43\ud835\udc65\ud835\udc43,\n(1)\nwhere\ud835\udc51\ud835\udc58 is the feature dimension of\ud835\udc7e\ud835\udc3e,\ud835\udc7e\ud835\udc44 projections. Intuitively,\nthis map provides a relevancy score between every pair of patches in\nthe image. It is then used to weight how much the \u201cValue\u201d features\nof a given target patch should influence a source patch \u210e\ud835\udc56 = \ud835\udc34\ud835\udc56 \u00b7 \ud835\udc49\ud835\udc56,\nwhere \u210e denotes an intermediary, hidden feature set.\nThese are projected using a fourth, \u201coutput-projection\" matrix,\n\ud835\udc7e\ud835\udc42, yielding \ud835\udc65\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc56\n= \ud835\udc7e\ud835\udc42 \u00b7 \u210e\ud835\udc56, which is then summed with the input\nfeatures \ud835\udc65\ud835\udc56 to create the input for the next layer.\nOur method intervenes in this self-attention mechanism by al-\nlowing images in a generated batch to attend to each other, and be\ninfluenced by each other\u2019s \ud835\udc65\ud835\udc5c\ud835\udc62\ud835\udc61 activations.\n4\nMETHOD\nOur goal is to generate a set of images portraying consistent subjects\nacross an array of prompts. We propose to do so by better aligning\nthe internal activation of the T2I model during image denoising.\nImportantly, we aim to enforce consistency exclusively through an\ninference-based mechanism, without additional training.\nOur approach is comprised of three main components. First, we\nintroduce a subject-driven self-attention mechanism (SDSA),\naimed at sharing subject-specific information across relevant model\nactivations in the generated image batch. Second, we observe that\nthe above component comes at the cost of reducing the variation\nin the generated layouts. Therefore, we propose strategies for mit-\nigating this form of mode collapse through an attention-dropout\nmechanism, and by blending query features obtained from a vanilla,\nnon-consistent, sampling step. Third, we incorporate a feature in-\njection mechanism to further refine the results. There, we map fea-\ntures from one generated image to another based on a cross-image\ndense-correspondence map derived from the diffusion features. Be-\nlow, we outline each of these components in detail.\n4.1\nSubject-driven self-attention\nConsider a simple idea for promoting consistency: expanding the\nself-attention, so that queries from one image can also attend to keys\nand values from other images in the batch. This enables repeated\nobjects to naturally attend to each other, thus sharing visual features\nacross images. This idea is often used in video generation and editing\nworks [55], leading to increased consistency across frames. However,\ngenerated videos differ from our scenario. First, they are created with\na single prompt that is shared across frames. Second, they typically\nrequire little variation in backgrounds or layout from one frame\nto the next. In contrast, we want each frame to follow a unique\nprompt, and we want to maintain diversity in backgrounds and\nlayouts. Na\u00efvely employing these video-based mechanisms leads to\nuniform backgrounds and drastically reduced alignment with each\nimage\u2019s prompt - in line with expectations for a single video scene,\nbut in direct conflict with our objectives.\nOne way to tackle these limitations is by reducing the amount of\ninformation being shared at background patches. As we are only\nconcerned about sharing subject appearance, we mask the expanded\nself-attention, so that queries from one image can only match keys\nand values from the same image, or from regions containing the\nsubject in other images. This way, features for repeated subject\nelements can be shared, while background features remain separate.\nTo this end, we employ a similar approach to prior art [7, 10] and\nidentify noisy latent patches that are likely to contain the subject\nusing cross-attention features. Specifically, we average and thresh-\nold the cross-attention maps related to the subject token across\n4\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\ndiffusion steps and layers to create subject-specific masks (details\nin Appendix B). With these masks, we propose Subject-Driven Self-\nAttention (SDSA) where attention is masked so each image can only\nattend to its own patches or the subject patches within the batch\n(See Fig. 2).\n\ud835\udc3e+ = [\ud835\udc3e1 \u2295 \ud835\udc3e2 \u2295 . . . \u2295 \ud835\udc3e\ud835\udc41 ] \u2208 R\ud835\udc41 \u00b7\ud835\udc43\u00d7\ud835\udc51\ud835\udc58\n\ud835\udc49 + = [\ud835\udc491 \u2295 \ud835\udc492 \u2295 . . . \u2295 \ud835\udc49\ud835\udc41 ] \u2208 R\ud835\udc41 \u00b7\ud835\udc43\u00d7\ud835\udc51\ud835\udc63\n\ud835\udc40+\n\ud835\udc56 = [\ud835\udc401 . . . \ud835\udc40\ud835\udc56\u22121 \u2295 1 \u2295 \ud835\udc40\ud835\udc56+1 . . . \ud835\udc40\ud835\udc41 ]\n(2)\n\ud835\udc34+\n\ud835\udc56 = softmax\n\u0010\n\ud835\udc44\ud835\udc56\ud835\udc3e+\u22a4/\n\u221a\ufe01\n\ud835\udc51\ud835\udc58 + log \ud835\udc40+\n\ud835\udc56\n\u0011\n\u2208 R\ud835\udc43\u00d7\ud835\udc41 \u00b7\ud835\udc43\n\u210e\ud835\udc56 = \ud835\udc34+\n\ud835\udc56 \u00b7 \ud835\udc49 + \u2208 R\ud835\udc43\u00d7\ud835\udc51\ud835\udc63 .\n(3)\nFeature Injection\nCorrespondence\nFig. 3. Feature Injection: To further refine the subject\u2019s identity across\nimages, we introduce a mechanism for blending features within the batch.\nWe extract a patch correspondence map between each pair of images (Mid-\ndle), and then inject features between images based on that map (Right).\nHere, \ud835\udc40\ud835\udc56 is the subject mask for the \ud835\udc56\ud835\udc61\u210e entry in the batch, and \u2295\nindicates matrix concatenation. We use standard attention masking,\nwhich null-out softmax\u2019s logits by assigning their scores to \u2212\u221e\naccording to the mask. Note that the Query tensors remain unaltered,\nand that the concatenated mask \ud835\udc40+\n\ud835\udc56 is set to be an array of 1\u2019s for\npatch indices that belong to the \ud835\udc56\ud835\udc61\u210e image itself.\n4.2\nEnriching layout diversity\nThe use of SDSA restores prompt alignment and avoids background\ncollapse. However, we observe that it can still lead to excessive sim-\nilarity between image layouts. For example, subjects will typically\nbe generated in similar locations and poses.\nTo improve the diversity of our results, we propose two strate-\ngies: first, incorporating features from a vanilla, non-consistent\nsampling step; and second, further weakening the subject-driven\nshared attention through a dropout mechanism.\nUsing Vanilla Query Features. Recent work [1], demonstrated\nthat one can use diffusion models to combine the appearance of one\nimage with the structure of another. They do so by injecting self-\nattention Keys and Values from the appearance image, and Queries\nfrom the structure image. Inspired by this, we aim to enhance pose\nvariation by aligning more closely with a structure predicted by a\nmore diverse vanilla forward pass (i.e. without our modifications).\nWe focus on the early steps of the diffusion process, which have\nbeen shown to primarily control layout [5, 41], and apply the follow-\ning query-blending mechanism: Let \ud835\udc67\ud835\udc61 be the noisy latents at step\n\ud835\udc61. We first apply a vanilla denoising step to \ud835\udc67\ud835\udc61, without SDSA, and\ncache the self-attention queries generated by the diffusion network:\n\ud835\udc44\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc4e\n\ud835\udc61\n. Then, we denoise the same latents \ud835\udc67\ud835\udc61 again, this time using\nSDSA. During this second pass, for all SDSA layers, we linearly inter-\npolate the generated queries towards the vanilla queries, resulting\nin:\n\ud835\udc44\u2217\n\ud835\udc61 = (1 \u2212 \ud835\udf08\ud835\udc61)\ud835\udc44\ud835\udc46\ud835\udc37\ud835\udc46\ud835\udc34\n\ud835\udc61\n+ \ud835\udf08\ud835\udc61\ud835\udc44\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc4e\n\ud835\udc61\n,\n(4)\nwhere \ud835\udc63\ud835\udc61 is a linearly decaying blending parameter (see Appendix).\nSelf-Attention Dropout Our second strategy to enhance layout\nvariation involves weakening SDSA using a dropout mechanism.\nSpecifically, at each denoising step, we randomly nullify a subset of\npatches from \ud835\udc40\ud835\udc56 by setting them to 0. This weakens the attention\nsharing between different images and subsequently promotes richer\nlayout variations. Notably, by adjusting the dropout probability,\nwe can regulate the strength of consistency, and strike a balance\nbetween visual consistency and layout variations.\nThrough these two mechanisms, we aim to tackle two aspects\nof the layout-collapse problem: Query-feature blending allows to\nretain aspects of diversity from the non-consistent sampling, while\nattention-dropout encourages the model to rely less on the shared\nkeys and values, avoiding over-consistency. By mixing them, we\nachieve increased diversity without significant harm to consistency.\n4.3\nFeature injection\nThe shared attention mechanism notably improves subject consis-\ntency but may struggle with fine visual features, which may hurt the\nsubject\u2019s identity. Hence, we propose to further improve consistency\nthrough a novel cross-image Feature Injection mechanism.\nHere, we aim to improve the similarity of features from corre-\nsponding regions (e.g. the left eye) across different images in the\nbatch. Specifically, we find that substantial texture information is\ncontained in the self-attention output features, \ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61, and aligning\nthese features between matching areas can enhance consistency.\nTo align these features, we first build a patch correspondence map\nbetween every pair of images \ud835\udc3c\ud835\udc61 and \ud835\udc3c\ud835\udc60 in the batch, using DIFT [49]\nfeatures \ud835\udc37\ud835\udc61 and \ud835\udc37\ud835\udc60 (See Appendix). We denote the correspondence\nmap by \ud835\udc36\ud835\udc61\u2192\ud835\udc60. Intuitively, when applied on patch index \ud835\udc5d from \ud835\udc3c\ud835\udc61,\n\ud835\udc36\ud835\udc61\u2192\ud835\udc60 [\ud835\udc5d] yields the most similar patch in \ud835\udc3c\ud835\udc60, as illustrated in Fig. 3.\nThen, to promote feature similarity, we can blend corresponding\nfeatures based on this mapping. We extend this idea to a many-to-\none scenario, where each image \ud835\udc3c\ud835\udc61 is blended with the other images\nin the batch. For each patch index \ud835\udc5d in image \ud835\udc3c\ud835\udc61, we compare its\ncorresponding patches in all other images and select the one with\nthe highest cosine similarity in the DIFT feature space. Formally:\nsrc(\ud835\udc5d) = arg max\n\ud835\udc60\u2260\ud835\udc61\nsimilarity(\ud835\udc37\ud835\udc61 [\ud835\udc5d], \ud835\udc37\ud835\udc60 [\ud835\udc36\ud835\udc61\u2192\ud835\udc60 [\ud835\udc5d]]),\n(5)\nwhere src(\ud835\udc5d) is the \u201cbest\u201d source patch for the target patch \ud835\udc5d, and\nsimilarity is the cosine similarity score.\nFinally, we blend the self-attention output layer features of the\ntarget image \ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc61\n, and its corresponding source patches, \ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc60\n.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n5\n\u02c6\ud835\udc99\ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc61 = (1 \u2212 \ud835\udefc) \u00b7 \ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc61\n+ \ud835\udefc \u00b7 src(\ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc61\n),\n(6)\nwhere \ud835\udefc is a blending parameter, and src(\ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc61\n) \u2208 R\ud835\udc43\u00d7\ud835\udc51 is the tensor\nobtained by pooling the corresponding features for each patch \ud835\udc5d in\n\ud835\udc99\ud835\udc5c\ud835\udc62\ud835\udc61\n\ud835\udc61\nfrom the associated patch \ud835\udc60\ud835\udc5f\ud835\udc50(\ud835\udc5d).\nIn practice, to enforce consistency between appearances of the\nsame subject, without affecting the background, we exclusively\napply the feature injection according to the subject masks \ud835\udc40\ud835\udc56. Addi-\ntionally, we apply a threshold to inject features only between patches\nwith high enough similarity in the DIFT space (see Appendix). This\napproach ensures that features contributing to the appearance of\nthe subject are collectively drawn from all source images, promoting\na more comprehensive and representative synthesis.\n4.4\nAnchor images and reusable subjects\nAs an additional optimization, we can reduce the computational\ncomplexity of our approach by designating a subset of generated\nimages as \u201canchor images\u201d. Rather than sharing keys and values\nacross all generated images during SDSA steps, we allow the images\nto only observe keys and values derived from the anchors. Similarly,\nfor feature injection, we only consider the anchors as valid feature\nsources. Note that the anchors can still observe each other during\ngeneration, but they too do not observe features from non-anchor\nimages. We find that for most cases, two anchors are sufficient.\nThis offers several benefits: First, it allows for faster inference\nand reduced VRAM requirements, because it restricts the size of\nextended attention. Second, it can improve generation quality in\nlarge batches, where we notice it can reduce visual artifacts. Most\nimportantly, we can now reuse the same subjects in novel scenes\nby creating a new batch where the same prompts and seeds are\nused to re-create the anchor images, but the non-anchor prompts\nhave changed. Through this mechanism, our approach can yield\nreusable subjects, and unlimited consistent image generation.\n4.5\nMulti-subject consistent generation\nPersonalization-based approaches struggle in maintaining consis-\ntency over multiple subjects within a single image [17, 30, 42, 50].\nHowever, with ConsiStory, multi-subject consistent generation is\npossible in a simple, straightforward manner, by simply taking a\nunion of the subject masks. When the subjects are semantically\ndifferent, information leakage between them is not a concern. This\nis due to the exponential form of the attention softmax, which acts\nas a gate that suppresses information leakage between unrelated\nsubjects. Similarly, thresholding the correspondence map during\nfeature injection yields a gating effect that safeguards against infor-\nmation leakage. Additional details such as hyperparameter choices\nare in the supplementary.\n5\nEXPERIMENTS\nWe begin our evaluation by comparing ConsiStory with a range of\nprior and concurrent baselines. We open with a qualitative com-\nparison, showing that our method can achieve improved subject-\nconsistency and higher prompt-alignment when compared to the\nstate-of-the-art. Next, we evaluate our method quantitatively, in-\ncluding a large-scale user study which demonstrates that users\ntypically favor our results. Moving on, we conduct an ablation study\nto highlight the contribution and effect of each component in our\nmethod. Finally, we conclude with a set of extended applications,\nshowing that our method is compatible with existing tools such\nas ControlNet [59], and it can even be used to enable training-free\npersonalization for common object classes.\n5.1\nEvaluation baselines\nWe compare our method to three classes of baselines: (1) The base-\nline SDXL model, without adaptations. (2) Optimization-based per-\nsonalization approaches that teach the model about a new sub-\nject by fine-tuning parts of the model: Textual Inversion (TI) [13]\nfine-tunes the text encoder\u2019s word embeddings. DreamBooth-LoRA\n(DB-LORA) [46] tunes the diffusion U-Net using Low Rank Adap-\ntation [22]. (3) Encoder-based approaches that take a single image\nas input and output a conditioning code to the diffusion model:\nIP-Adapter [57], ELITE [54] and E4T [14]. For the personalization\nand encoder baselines, we first generate a single image of a target\nsubject using a prompt describing the subject, then use it to person-\nalize the diffusion model. All methods except ELITE are based on a\npre-trained SDXL model.\nFor ConsiStory, we use two anchor images and 0.5 dropout. For\nthe automated metric, we also use lower dropout values.\n5.2\nQualitative Results\nIn Fig. 4 and Fig. A.1, we show qualitative comparisons. Our method\ncan achieve a high degree of subject consistency, while better adher-\ning to the text prompts. Tuning-based personalization approaches\ntend to either overfit the single training image, producing no varia-\ntions, or underfit and fail to maintain consistency. IP-Adapter simi-\nlarly struggles to match complex prompts, particularly when styles\nare involved. Our method can successfully achieve both subject\nconsistency and text-alignment. Additionally, in Fig. 5 we show that\nwith different initial noise inputs, our method can generate varied\nsets of consistent images. Additional results are shown in the sup-\nplementary, including the qualitative comparisons with ELITE [54]\nand E4T [14].\nMulti-Subject Generation In Fig. 1 (bottom) we demonstrate\nthat ConsiStory can create scenes with multiple consistent subjects.\nIn Fig. 6 we further compare our method to LORA-DB. Notably,\nLORA-DB tends to neglect the consistency of one or even both\nsubjects. This pitfall is common when combining personalization\napproaches, as they learn each subject in isolation. In contrast, our\nmethod simply builds on the diffusion model\u2019s inherent composi-\ntional ability. In Figures A.2, A.3 we provide additional comparisons.\nFig. 15 provides more results with single and multiple subjects.\n5.3\nQuantitative evaluation\nNext, we perform quantitative evaluations with automated metrics.\nFirst, we use each baseline to generate 100 image sets, where each\nset contains 5 images depicting a shared subject under different\nprompts. Our evaluation prompts were created using ChatGPT [37],\nwith the following protocol: Each prompt consisted of three parts:\n6\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\nOurs\nIP-Adapter\nTI\nDB-LoRA\n\u201cin a ship\u201d\n\u201cexploring \namong \ufb02owers\u201d\n\u201cin the open\u201d\n\u201cin a meadow\u201d\n\u201cwearing a small \nsweater\u201d\n\u201cwearing a \ntraining harness\u201d\n\u201csticking head out \nof the car window\u201d\n\u201cswimming\u201d\n\u201cAn origami style of a happy hedgehog\u201d\n\u201cA watercolor illustration of a puppy\u201d\nOurs\nIP-Adapter\nTI\nDB-LoRA\n\u201cA photo of a puppy\u201d\n\u201cin a pet store\u201d\n\u201cdressed in a \nbandana\u201d\n\u201cin a grassy \nyard\u201d\n\u201cwearing a \ntraining harness\u201d\n\u201cA hyper-realistic digital painting of a fairy\u201d\n\u201ccollecting \nmorning dew\u201d\n\u201catop a \ndew-covered \n\ufb02ower\u201d\n\u201cinside a \nhollowed-out \ntree\u201d\n\u201cunder a full \nmoon\u201d\nFig. 4. Qualitative Results We evaluated our method against IP-Adapter, TI, and DB-LORA. Some methods failed to maintain consistency (TI), or follow the\nprompt (IP-Adapter). Other methods alternated between keeping consistency or following text, but not both (DB-LoRA). Our method successfully followed the\nprompt while maintaining consistency. Additional results are shown at Figure A.1\n(1) a subject description, e.g., \u201cA red dragon\u201d (2) a setting description,\ne.g., \u201cblowing bubbles\u201d or \u201cin a castle\u201d, and (3) a style descriptor, e.g.,\n\u201cOrigami style\u201d. For subject descriptions, we utilized both detailed\nexamples (\u201cA red dragon\u201d), and non-detailed ones (\u201cA dragon\u201d). For\nsetting descriptions, we asked ChatGPT to provide descriptions\nthat naturally fit the subject. Each of the 100 sets contains prompts\nsharing the same subject description and style but with varying\nsetting descriptions. Further details can be found in Appendix D.\nWe follow the concurrent work of Avrahami et al. [4] and evaluate\nthe methods on two axes - prompt-alignment, and subject consis-\ntency. For prompt-alignment, we use CLIP to measure the similarity\nbetween each generated image and its conditioning prompt, and\nreport the average CLIP-score [21] over all 500 generated images.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n7\n\u201creading a book\u201d\n\u201cin the middle of a \nrainstorm\u201d\n\u201csitting by the \ufb01re\u201d\n\u201cperched on a rock\u201d\n\u201csur\ufb01ng in the sea\u201d\n\u201cA plastic owl\u201d\nFig. 5. Seed Variation. Given different starting noise, ConsiStory generates\ndifferent consistent set of images.\nOurs\nLoRA DB\n\u201cNeonpunk style of a boy with black spiky hair, with his dog\u201d\n\u201csitting in the \nhouse\u201d\n\u201cwalking in the \npark\u201d\n\u201cplaying with a \nball\u201d\n\u201cwalking in \nthe snow\u201d\n\u2713\ndog\nboy\n\u2713\n\u2713\ndog\nboy\n\u2717\nFig. 6. Multiple Subjects: ConsiStory generates multiple consistent sub-\njects, while other methods often neglect at least one subject.\nFor consistency evaluation, we use DreamSim [12], which has been\nshown to better correlate with human judgment of inter-image sim-\nilarity. We calculate the pair-wise similarity between each pair of\nimages in each of the 100 sets. To focus on subject consistency, we\nfollow Dreamsim\u2019s background removal protocol. We report the\naverage score over these sets. In our results, the error bars represent\nthe Standard Error of the Mean (S.E.M).\nThe results are provided in Fig. 7. As can be seen, our method is\nsituated on the Pareto-front. Our standard setup matches SDXL in\ntext-alignment scores, demonstrating that promoting consistency\na-priori can better maintain the model\u2019s knowledge.\nHowever, qualitatively, we observed that the consistency metric\ntends to bias its scores towards configurations with slight layout\nchanges, rather than assessing consistency in the subject\u2019s iden-\ntity. Therefore, given the limitations of the automated metric, we\nconducted a large-scale user study. We concentrated on the most\neffective techniques, omitting both vanilla SDXL and ELITE. We\nused the standard two-alternative forced-choice format. Users faced\ntwo types of questions: (1) Subject-consistency, where they were\nshown two sets of 5 images each. They were asked to choose the\n0.55\n0.60\n0.65\nText Similarity\n0.65\n0.75\n0.85\nSubject Consistency\nd=0.5\nd=0.15\nd=0\nOurs\nIP-Adapter\nELITE\nLoRA DreamBooth\nTextual Inversion\nSDXL\nFig. 7. Subject Consistency VS Textual Similarity: ConsiStory (green)\nachieves the optimal balance between Subject Consistency and Textual\nSimilarity. Encoder-based methods such as ELITE and IP-Adapter often\noverfit to visual appearance, while optimization-based methods such as\nLoRA-DB and TI do not exhibit high subject consistency as in our method.\n\ud835\udc51 denotes different self-attention dropout values. Error bars are S.E.M.\n50%\nWin Rate\nVisual\nTextual\nVisual\nTextual\nVisual\nTextual\n61%\n39%\n88%\n12%\n91%\n9%\n60%\n40%\n56%\n44%\n58%\n42%\nLoRA DB\nTI\nIP-Adapter\nFig. 8. User Study results indicate a notable preference among participants\nfor our generated images both in regards to Subject Consistency (Visual)\nand Textual Similarity (Textual).\nset that better shows the same subject, ignoring background, pose,\nand image quality. (2) Text-alignment, where they selected the im-\nage that best matched a textual description from two images. We\ngathered 500 responses per baseline for each question type, totaling\n3, 000 responses. The results are shown in Fig. 8. Despite being a\ntraining-free approach, ConsiStory outperforms the baselines, both\nregarding textual alignment and subject consistency.\nRuntime comparison We conducted a runtime analysis of the\nmain methods, focusing on their time-to-consistent-subject (TTCS)\nusing an H100 GPU. Our method, ConsiStory, achieved the fastest\nTTCS result, at 32 seconds for generating two anchors and an image\nbased on a new prompt. This is \u00d725 faster than the SoTA approach by\nAvrahami et al. [4] which is estimated at 13 minutes on an H100 GPU.\nMoreover, our method is \u00d78-14 faster than LORA-DB (4.5 min.) and\nTI (7.5 min.). Comparing our approach to encoder-based methods\nlike IP-Adapter is more convoluted. These techniques require weeks\nof pre-training, but once trained, they can generate an anchor and\nanother image based on a new prompt in just 8 seconds.\n8\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\nOurs\nw/o Variation\nw/o FI\nw/o SDSA\nFig. 9. Component Ablation We ablated several components: Subject-\nDriven Self Attention (SDSA), Feature Injection (FI), and our variation enrich-\ning strategies: self-attention dropout and Query-feature blending (Variation).\nAll ablated cases fail to maintain consistency as our method.\n\u201cin the park\u201d\n\u201cin the garden\u201d\n\u201csitting on a hill\u201d\n\u201con a dirt road\u201d\n\u201clooking at the sky\u201d\n\u201cA photo of a teddy bear\u201d\nOurs\nw/o Mask\nFig. 10. Subject Masking: Without the subject mask, there is noticeable\nbackground leakage across images.\n5.4\nAblation study\nWe now move to evaluate the impact of different components of our\nown method through an ablation study of the following components:\n(1) SDSA steps (2) Feature Injection (FI) (3) Attention dropout and\nquery-feature blending. (4) Without using a subject mask. Qualita-\ntive comparisons are provided in Figures 9, 10. Quantitative results\nare provided in Appendix A.4.\nRemoving SDSA leads to poor consistency, both regarding shape\nand texture. Removing FI yields subjects with similar shapes but less\naccurate identities. Finally, removing the vanilla query blending and\nattention dropout leads to significant reductions in layout diversity.\nWe quantify this loss of diversity in Appendix C.\n5.5\nExtended Applications\nSpatial Controls. We first demonstrate that ConsiStory is compati-\nble with existing guided generation tools like ControlNet [59]. As\nthese are compatible with standard personalization methods [3, 13,\n59], we want to ensure that our alternative approach maintains this\ncompatibility. In Fig. 11 we show that pose-based controls with Con-\ntrolNet successfully guide our consistent image generation method.\nTraining-free Personalization. We are the first to show training-\nfree personalization (Fig. 12), where ConsiStory enables personal-\nization without any tuning or encoder use. Specifically, we show\nhow to personalize common subject classes. Given two images of a\nsubject, we invert them using Edit Friendly DDPM-Inversion [25].\nThese latents and noise maps are used as anchors for ConsiStory,\nallowing the rest of the batch to draw on their visual appearance.\nThis application requires minor modifications of ConsiStory, which\nwe detail in Appendix E.\nPose Control\n\u201ca photo of an \nathletic 50 \nyear-old man \nwith gray \nhair\u201d\n\u201c\u2026 in the beach\u201d\n\u201c\u2026 in the countryside\u201d\n\u201c\u2026 in the park\u201d\nFig. 11. ControlNet Integration. Our method can be integrated with\nControlNet to generate a consistent character with pose control.\n\u201cin the Grand Canyon\u201d\n\u201cin Boston\u201d\n\u201con a wooden \ufb02oor\u201d\n\u201con top of snow\u201d\nInverted Anchors\nInverted Anchors\nFig. 12. Training-Free Personalization. We utilize edit-friendly inversion\nto invert 2 real images per subject. These inverted images used as anchors\nin our method for training-free personalization.\nIn Fig. A.6 we show that this approach struggles with complex\nobjects, and is incompatible with style-changing prompts. Yet, we\nbelieve it can serve as a quick and cheaper alternative to current\npersonalization methods, and leave it for future work.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n9\nIncorrect Localization\n\u201cA photo\u201d\n\u201cB&W Sketch\u201d\n\u201c3D animation\u201d\n\u201cWatercolor\u201d\nFailed Style Transfer\nFig. 13. Limitations: Our method often struggles with different styles in\nthe same set of image (Top), and is dependent on the quality of the model\ncross-attention to localize the subject correctly (Bottom).\n6\nCONCLUSIONS AND LIMITATIONS\nWe introduced ConsiStory, a training-free approach for creating vi-\nsually consistent subjects using a pre-trained text-to-image diffusion\nmodel. When compared to the state of the art, our method is not\nonly \u00d720 faster, but can better preserve the output\u2019s alignment with\nthe given prompts. Moreover, our method can be easily extended to\ntackle more challenging cases such as multi-subject scenarios, and\neven enable training-free personalization for common objects.\nOur approach has several limitations, shown in Figure 13. First,\nit relies on the localization of objects through cross-attention maps.\nThis process may occasionally fail, particularly when dealing with\nunusual styles. However, from our observations so far, such failures\nappear relatively infrequent (fewer than 5%), and they can be re-\nsolved by simply changing the seed. Another limitation is found\nin the entanglement between appearance and style. Our method\nstruggles to separate the two, and hence we are limited to consistent\ngenerations where images share the same style. Finally, we observe\nthat the underlying SDXL model may exhibit biases towards certain\ngroups. We demonstrate that these biases can be significantly re-\nduced by specifying modifiers like gender or ethnicity, as illustrated\nin Figure 14.\nWe hope that our results will assist in a consistent generation\nof creative endeavors and that they will inspire others to continue\nexploring training-free alternatives to personalization-based tasks.\n\u201cin a studio\u201d\n\u201cin a meadow\u201d\n\u201ceating piece \nof cake\u201d\n\u201cA photo of a young man, posing\u201d\nFig. 14. Model Bias. The underlying SDXL model may exhibit biases to-\nwards certain ethnic groups, and our approach inherits them. Our method\ncan generate consistent subjects belonging to diverse groups when these\nare highlighted in the prompt.\nACKNOWLEDGMENTS\nWe thank Assaf Shocher, Eli Meirom, Chen Tessler, Dvir Samuel\nand Yael Vinker for useful discussions and for providing feedback\non an earlier version of this manuscript.\n10\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\n\u201cA photorealistic \nillustration of a \nbunny, full body\u201d\n\u201cdrinking hot tea on \na rainy night\u201d\n\u201cstanding in the \nstreet\u201d\n\u201cmeeting a friend in \nthe park\u201d\n\u201chiking in the \nmountains with \nwalking stick\u201d\n\u201cgraduating from \ncollege\u201d\n\u201cA pink hedgehog \nwith a cute smile\u201d\n\u201cdressed in a \nminiature jacket\u201d\n\u201ccurled up asleep\u201d\n\u201cwearing a \ufb02ower \ncrown\u201d\n\u201cin a meadow\u201d\n\u201cwearing a small \nhat\u201d\n\u201cA 3D animation of \na young chef with \ncurly hair\u201d\n\u201csitting on a stool\u201d\n\u201cpreparing a \ngourmet meal\u201d\n\u201cwalking in the \nstreet at night\u201d\n\u201cbaking a cake\u201d\n\u201ctaking a bath\u201d\n\u201cdressed in a glow of \nmoonlight\u201d\n\u201cin a blooming \nmeadow\u201d\n\u201cdressed in a blanket \nof mist\u201d\n\u201cwearing a silver \nbridle\u201d\n\u201cmeeting a group of \nchildren\u201d\n\u201cA unicorn with a \nmidnight blue coat \nand a crystal horn\u201d\n\u201cin a room\u201d\n\u201con a table\u201d\n\u201cwith \ufb02owers\u201d\n\u201con the ground\u201d\n\u201cbeing held by a person\u201d\n\u201cA photo of a vase\u201d\n\u201ceating his food \nwhile wearing a \nhat\u201d\n\u201csitting on the \nmat, wearing a \nhat\u201d\n\u201cjumping over \na puddle\u201d\n\u201cwalking in the \nstreet, with his \nhat on\u201d\n\u201cwith her pet owl\u201d\n\u201csitting on the \ncouch with her \nowl\u201d\n\u201cstanding \noutside with \nher owl\u201d\n\u201cthe owl sitting \non a branch\u201d\n\u201cA photo of a dog\u201d\n\u201cA 3D animation of a 12 years old girl\u201d\n\u201ckicking a ball\u201d\n\u201cholding the ball\u201d\n\u201cthrowing the \nball up\u201d\n\u201cgoing to sleep, \nholding his ball\u201d\n\u201clooking through a \njunkyard\u201d\n\u201cwalking in the \nsand\u201d\n\u201ceating  in a \nrestaurant\u201d\n\u201cexploring the \nrainforest\u201d\n\u201cA hyper-realistic digital painting of a young ginger boy\u201d\n\u201cA 3D animation of a baby in a suit, with his hamster\u201d\nSingle Subject\nMulti Subject\nFig. 15. Additional Qualitative Results We demonstrate that ConsiStory successfully generates a consistent subject while following the prompt for various\nsubjects (Top). Furthermore, we show that ConsiStory still succeeds even in the case of multiple interacting subjects (Bottom).\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n11\nREFERENCES\n[1] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel\nCohen-Or. 2023. Cross-Image Attention for Zero-Shot Appearance Transfer.\narXiv:2311.03335 [cs.CV]\n[2] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel\nShamir, and Amit H. Bermano. 2023. Domain-agnostic tuning-encoder for fast\npersonalization of text-to-image models. In SIGGRAPH Asia 2023 Conference\nPapers. 1\u201310.\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski.\n2023. Break-A-Scene: Extracting Multiple Concepts from a Single Image. In\nSIGGRAPH Asia 2023 Conference Papers (, Sydney, NSW, Australia,) (SA \u201923).\nAssociation for Computing Machinery, New York, NY, USA, Article 96, 12 pages.\nhttps://doi.org/10.1145/3610548.3618154\n[4] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad\nFried, Daniel Cohen-Or, and Dani Lischinski. 2023. The Chosen One: Consistent\nCharacters in Text-to-Image Diffusion Models. arXiv preprint arXiv:2311.10093\n(2023).\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten\nKreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and\nMing-Yu Liu. 2022. eDiff-I: Text-to-Image Diffusion Models with Ensemble of\nExpert Denoisers. arXiv preprint arXiv:2211.01324 (2022).\n[6] Sagie\nBenaim,\nRon\nMokady,\nAmit\nBermano,\nDaniel\nCohen-Or,\nand\nLior Wolf. 2020.\nStructural-analogy from a Single Image Pair.\nCom-\nputer Graphics Forum\nn/a (2020).\nhttps://doi.org/10.1111/cgf.14186\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14186\n[7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang\nZheng. 2023. MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consis-\ntent Image Synthesis and Editing. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV). 22560\u201322570.\n[8] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. 2023. Pix2video: Video edit-\ning using image diffusion. In Proceedings of the IEEE/CVF International Conference\non Computer Vision. 23206\u201323217.\n[9] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing\nYan, Xiao Yang, and Mohammad Soleymani. 2023. MagicDance: Realistic Human\nDance Video Generation with Motions & Facial Expressions Transfer. arXiv\npreprint arXiv:2311.12052 (2023).\n[10] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023.\nAttend-and-excite: Attention-based semantic guidance for text-to-image diffusion\nmodels. ACM Transactions on Graphics (TOG) 42, 4 (2023), 1\u201310.\n[11] Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming\nShi, and Bing Qin. 2023. Improved Visual Story Generation with Adaptive Context\nModeling. arXiv preprint arXiv:2305.16811 (2023).\n[12] Stephanie Fu, Netanel Yakir Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang,\nTali Dekel, and Phillip Isola. 2023. DreamSim: Learning New Dimensions of\nHuman Visual Similarity using Synthetic Data. In Thirty-seventh Conference\non Neural Information Processing Systems.\nhttps://openreview.net/forum?id=\nDEiNSfh1k7\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal\nChechik, and Daniel Cohen-Or. 2022. An Image is Worth One Word: Personalizing\nText-to-Image Generation using Textual Inversion.\nhttps://doi.org/10.48550/\nARXIV.2208.01618\n[14] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. 2023. Encoder-based domain tuning for fast personalization of text-to-\nimage models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 1\u201313.\n[15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2023. TokenFlow:\nConsistent Diffusion Features for Consistent Video Editing.\narXiv preprint\narxiv:2307.10373 (2023).\n[16] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Haoxin Chen, Longyue\nWang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang. 2023.\nTale-\nCrafter: Interactive Story Visualization with Multiple Characters. arXiv preprint\narXiv:2305.18247 (2023).\n[17] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan\nFan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. 2023. Mix-of-\nShow: Decentralized Low-Rank Adaptation for Multi-Concept Customization of\nDiffusion Models. arXiv preprint arXiv:2305.18292 (2023).\n[18] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea\nTagliasacchi, and Kwang Moo Yi. 2023. Unsupervised Semantic Correspondence\nUsing Stable Diffusion. (2023). arXiv:2305.15581 [cs.CV]\n[19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.\n(2022).\n[20] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. 2023. Style\nAligned Image Generation via Shared Attention. (2023).\n[21] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\n2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott\nWen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta\nCana, Dominican Republic, 7514\u20137528. https://doi.org/10.18653/v1/2021.emnlp-\nmain.595\n[22] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\nModels. ArXiv abs/2106.09685 (2021).\n[23] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. 2023. Animate\nAnyone: Consistent and Controllable Image-to-Video Synthesis for Character\nAnimation. arXiv preprint arXiv:2311.17117 (2023).\n[24] Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with\nadaptive instance normalization. In Proceedings of the IEEE international conference\non computer vision. 1501\u20131510.\n[25] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. 2023. An\nEdit Friendly DDPM Noise Space: Inversion and Manipulations. arXiv preprint\narXiv:2304.06140 (2023).\n[26] Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. 2023. Zero-shot generation of\ncoherent storybook from plain text story using diffusion models. arXiv preprint\narXiv:2302.03900 (2023).\n[27] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong,\nTingbo Hou, Huisheng Wang, and Yu-Chuan Su. 2023. Taming encoder for zero\nfine-tuning image customization with text-to-image diffusion models. arXiv\npreprint arXiv:2304.02642 (2023).\n[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Hen-\nschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023.\nText2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Genera-\ntors. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV). 15954\u201315964.\n[29] Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali Mahdavi Amiri, and\nGhassan Hamarneh. 2023. Slime: Segment like me. arXiv preprint arXiv:2309.03179\n(2023).\n[30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv (2022).\n[31] Dongxu Li, Junnan Li, and Steven CH Hoi. 2023. Blip-diffusion: Pre-trained\nsubject representation for controllable text-to-image generation and editing.\narXiv preprint arXiv:2305.14720 (2023).\n[32] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing Kang. 2017. Visual\nAttribute Transfer Through Deep Image Analogy. ACM Trans. Graph. 36, 4,\nArticle 120 (July 2017), 15 pages. https://doi.org/10.1145/3072959.3073683\n[33] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, and Weidi Xie. 2023.\nIntelligent Grimm\u2013Open-ended Visual Storytelling via Latent Diffusion Models.\narXiv preprint arXiv:2306.00973 (2023).\n[34] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell.\n2023. Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\nCorrespondence. In Advances in Neural Information Processing Systems.\n[35] Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, and Ren\u00e9 Vidal. 2023. Knowl-\nedge Pursuit Prompting for Zero-Shot Multimodal Synthesis. arXiv preprint\narXiv:2311.17898 (2023).\n[36] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2023.\nDragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv\npreprint arXiv:2307.02421 (2023).\n[37] OpenAI. 2022. ChatGPT. https://chat.openai.com/. Accessed: 2023-10-15.\n[38] Nobuyuki Otsu. 1979. A Threshold Selection Method from Gray-Level Histograms.\nIEEE Transactions on Systems, Man, and Cybernetics 9, 1 (1979), 62\u201366.\nhttps:\n//doi.org/10.1109/TSMC.1979.4310076\n[39] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. 2024. Synthesiz-\ning Coherent Story With Auto-Regressive Latent Diffusion Models. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).\n2920\u20132930.\n[40] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and\nJun-Yan Zhu. 2023. Zero-shot image-to-image translation. In ACM SIGGRAPH\n2023 Conference Proceedings. 1\u201311.\n[41] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-\nOr. 2023. Localizing Object-level Shape Variations with Text-to-Image Diffusion\nModels. In Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV).\n[42] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. 2023. Orthogo-\nnal Adaptation for Modular Customization of Diffusion Models. arXiv preprint\narXiv:2312.02432 (2023).\n[43] Chenyang QI, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying\nShan, and Qifeng Chen. 2023. FateZero: Fusing Attentions for Zero-shot Text-\nbased Video Editing. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV). 15932\u201315942.\n[44] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or.\n2023.\nConceptLab: Creative Generation using Diffusion Prior Constraints.\narXiv:2308.02669 [cs.CV]\n12\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\n[45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models\nfor Subject-Driven Generation. (2022).\n[46] Simo Ryu. 2023. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-\ntuning. https://github.com/cloneofsimo/lora.\n[47] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2023. Instantbooth: Per-\nsonalized text-to-image generation without test-time finetuning. arXiv preprint\narXiv:2304.03411 (2023).\n[48] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. 2023. FreeU: Free Lunch\nin Diffusion U-Net. arXiv preprint arXiv:2309.11497 (2023).\n[49] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath\nHariharan. 2023. Emergent Correspondence from Image Diffusion. In Thirty-\nseventh Conference on Neural Information Processing Systems. https://openreview.\nnet/forum?id=ypOiXjdfnU\n[50] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. 2023. Key-locked rank\none editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference\nProceedings. 1\u201311.\n[51] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and\nYu-Gang Jiang. 2023. MotionEditor: Editing Video Motion via Content-Aware\nDiffusion. arXiv preprint arXiv:2311.18830 (2023).\n[52] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2022. Splicing\nViT Features for Semantic Appearance Transfer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 10748\u201310757.\n[53] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. Plug-and-play\ndiffusion features for text-driven image-to-image translation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1921\u20131930.\n[54] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo.\n2023. ELITE: Encoding Visual Concepts into Textual Embeddings for Customized\nText-to-Image Generation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV). 15943\u201315953.\n[55] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei\nShi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-a-\nvideo: One-shot tuning of image diffusion models for text-to-video generation.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision. 7623\u2013\n7633.\n[56] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu\nZhang, Jiashi Feng, and Mike Zheng Shou. 2023. MagicAnimate: Temporally\nConsistent Human Image Animation using Diffusion Model.\n[57] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text\ncompatible image prompt adapter for text-to-image diffusion models. arXiv\npreprint arXiv:2308.06721 (2023).\n[58] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun\nJampani, Deqing Sun, and Ming-Hsuan Yang. 2023. A Tale of Two Features:\nStable Diffusion Complements DINO for Zero-Shot Semantic Correspondence.\n(2023).\n[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control\nto text-to-image diffusion models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 3836\u20133847.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n13\nAppendix: Training-Free Consistent\nText-to-Image Generation\nA\nADDITIONAL RESULTS\nWe provide additional Qualitative and Quantitative results of our\nmethod. In Fig. A.1, Fig. A.2 and Fig. A.3 we show additional sin-\ngle and multi subject qualitative comparisons to existing baselines.\nIn Fig. A.4 and Fig. A.5 we provide quantitative ablation results\nwith respect to Textual Similarity, Subject Consistency, and Layout\nDiversity.\nB\nADDITIONAL IMPLEMENTATION DETAILS\nSubject-Driven Self-Attention is applied at all timesteps, in the\ndecoder layers of the U-net.\nFeature injection and DIFT features: Feature injection is ap-\nplied at timesteps\ud835\udc61 \u2208 [680, 900], with\ud835\udefc = 0.8. We only inject patches\nwith similarity scores above a threshold that is set automatically by\nthe Otsu method.\nFor feature injection, we denoise the image from \ud835\udc61 = 1000 to\n\ud835\udc61 = 261, for computing the DIFT features at \ud835\udc61 = 261 as in [49]. We\nthen denoise again from \ud835\udc61 = 1000 to \ud835\udc61 = 0, guiding the feature\ninjection with the precomputed DIFT features.\nPose Variation We apply the injection of Vanilla Query Features\nover the first 5 denoising steps, with \ud835\udf08\ud835\udc61 values that linearly decay\nwith \ud835\udc61 from 0.9 to 0.8. Self-Attention Dropout is applied with \ud835\udc5d = 0.5.\nDiffusion Process Images were sampled with 50 DDIM steps\nand a guidance scale of 5. Similarly to [1, 35], we used Free-U [48]\nto enhance the generation quality.\nExtracting per subject mask To extract subject masks from\nnoisy latent patches, we collect all cross-attention maps that relate\nto each subject\u2019s token, across all previous diffusion steps, and all\ncross-attention layers of resolution 32 \u00d7 32. We then average them,\nand threshold them using the \u201cOtsu\u2019s method\u201d [38]. The subject\nmask at the generation step \ud835\udf0f, is given by:\n\ud835\udc5a\ud835\udc56 = E\ud835\udc3f\n\ud835\udc59=0E\ud835\udf0f\n\ud835\udc61=\ud835\udc47 [\ud835\udc34\ud835\udc56]\n\ud835\udc40\ud835\udc56 = \ud835\udc5c\ud835\udc61\ud835\udc60\ud835\udc62\n\u0010\n\ud835\udc5a\ud835\udc56\n\u0011\n\u2208 {0, 1}\ud835\udc43,\n(7)\nwhere \ud835\udc3f is the number of network layers, \ud835\udc43 is the number of patches\nand E denotes averaging.\nConstructing DIFT pairwise correspondence maps For a\nsource image \ud835\udc3c\ud835\udc60 and a target image \ud835\udc3c\ud835\udc61, we denote their DIFT features\nby \ud835\udc37\ud835\udc60, \ud835\udc37\ud835\udc61 \u2208 R\ud835\udc43\u00d7\ud835\udc51\ud835\udc37\ud835\udc3c\ud835\udc39\ud835\udc47 respectively, where \ud835\udc51\ud835\udc37\ud835\udc3c\ud835\udc39\ud835\udc47 is the feature\ndimension. The cross-image patch similarity scores are given by the\ncosine similarity between these features:\nSim(\ud835\udc3c\ud835\udc60, \ud835\udc3c\ud835\udc61)\ud835\udc5d\ud835\udc60,\ud835\udc5d\ud835\udc61 =\n\ud835\udc37\ud835\udc60 [\ud835\udc5d\ud835\udc60] \u00b7 \ud835\udc37\ud835\udc61 [\ud835\udc5d\ud835\udc61]\n\u2225\ud835\udc37\ud835\udc60 [\ud835\udc5d\ud835\udc60]\u2225 \u2225\ud835\udc37\ud835\udc61 [\ud835\udc5d\ud835\udc61]\u2225 ,\n(8)\nwhere \ud835\udc5d\ud835\udc60, \ud835\udc5d\ud835\udc61 are the indices of specific patches in the source and\ntarget image respectively and \ud835\udc37\ud835\udc60 [\ud835\udc5d\ud835\udc60], \ud835\udc37\ud835\udc61 [\ud835\udc5d\ud835\udc61] are the DIFT matrix\nrows (i.e. feature-vectors) matching these patches. Given these patch-\nsimilarity scores, we can calculate a patch-wise dense-correspondence\nmap \ud835\udc36\ud835\udc61\u2192\ud835\udc60:\n\u2200\ud835\udc5d\ud835\udc61 \u2208 \ud835\udc3c\ud835\udc61 : \ud835\udc36\ud835\udc61\u2192\ud835\udc60 [\ud835\udc5d\ud835\udc61] = arg max\n\ud835\udc5d\ud835\udc60 \u2208\ud835\udc3c\ud835\udc60\nSim(\ud835\udc3c\ud835\udc60, \ud835\udc3c\ud835\udc61)\ud835\udc5d\ud835\udc60,\ud835\udc5d\ud835\udc61 .\n(9)\nHere, for each patch \ud835\udc5d\ud835\udc61 in the target image \ud835\udc3c\ud835\udc61, \ud835\udc36\ud835\udc61\u2192\ud835\udc60 [\ud835\udc5d\ud835\udc61] gives the\nindex of the most similar patch in the source image, \ud835\udc3c\ud835\udc60.\nControl Net To enhance the impact of the pose from ControlNet,\nwe raise the Self-Attention Dropout value to 0.7.\nC\nDIVERSITY EVALUATIONS\nIn our quest to enhance layout diversity, we have proposed two\nstrategies: Vanilla Query Injection and Self-Attention Dropout. To\nquantitatively assess the contributions of these strategies, we con-\nstruct an automatic evaluation metric for layout diversity.\nSpecifically, we utilize the DIFT features and dense correspon-\ndence maps between image pairs. We measure layout diversity by\ncalculating the average displacement between corresponding points\nacross each image pair. A low displacement score indicates that the\nsubject\u2019s layout is largely aligned in both images. If this persists\nacross the entire image set, we can conclude that the images have\nlimited diversity. Notably, we opt for a geometric-diversity metric\nrather than an image-based one since the latter may conflate layout\ndiversity with undesired changes in the subject\u2019s appearance. In\nFig. A.5, we show the results of our diversity metric when ablating\ncomponents from our method. Scores are normalized to the dis-\nplacement value of images sampled from a vanilla non-consistent\nSDXL model. Our full method attains high diversity scores, sur-\npassing solutions that omit one or more of the layout enrichment\ncomponents.\nD\nPROMPTS DATASET DETAILS\nTo evaluate our method on a large scale, we constructed a prompts\ndataset. We asked ChatGPT to construct sets of 5 prompts each,\nwhere every prompt in a set contains the same recurring subject.\nIn addition, we instructed it to generate the following metadata for\neach prompt set:\n\u2022 Superclass: We asked the model to group each set into\none of the following superclasses: humans, animals, fantasy,\ninanimate.\n\u2022 Subject Token: A single token representing the subject\n(e.g., \u2019cat\u2019, \u2019boy\u2019).\n\u2022 Subject Description: The description of the recurring sub-\nject (e.g., \"A 16-year-old girl with blonde hair\").\n\u2022 Description Level: This refers to whether the subject de-\nscription is generic or detailed. For example, a generic de-\nscription might simply be \"A dog\", while a detailed descrip-\ntion could elaborate as \"A dog with brown and white fur,\nfluffy hair, and pointy ears\".\n\u2022 Style: The style of the requested image; for example, \"A 3D\nanimation of.\"\nWe generated 100 prompt sets, comprising a total of 500 images,\nspanning various superclasses, description levels, and styles. We\naimed to cover a wide spectrum of visual themes and subjects,\nensuring a broad and inclusive representation across the different\ncategories.\nOur prompt dataset is provided as a YAML file in the supplemental\nmaterials.\n14\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\nOurs\nIP-Adapter\nTI\nDB-LoRA\n\u201cA photo of a puppy\u201d\n\u201cin a pet store\u201d\n\u201cdressed in a \nbandana\u201d\n\u201cin a grassy \nyard\u201d\n\u201cwearing a \ntraining harness\u201d\nOurs\nIP-Adapter\nTI\nDB-LoRA\n\u201cA hyper-realistic digital painting of a fairy\u201d\n\u201ccollecting \nmorning dew\u201d\n\u201catop a \ndew-covered \n\ufb02ower\u201d\n\u201cinside a \nhollowed-out \ntree\u201d\n\u201cunder a full \nmoon\u201d\n\u201cA hyper-realistic digital painting of a teenage \nboy with black hair, a slight build\u201d\n\u201cwearing green \nknitted hat\u201d\n\u201cin a dense \nforest\u201d\n\u201cdressed as an \nastronaut\u201d\n\u201cdigging with a \ntrowel\u201d\n\u201cA 3D animation of a cute nerdy mouse\u201d\n\u201creviewing a \npaper in his cozy \napartment\u201d\n\u201cjogging on the \nbeach\u201d\n\u201cdrinking co\ufb00ee \nwith his friend in \nthe heart of NYC\u201d\n\u201clooking at a \npainting in a \nmuseum\u201d\nELITE\nELITE\nE4T\nE4T\nFig. A.1. Additional Qualitative Comparisons We evaluated our method against IP-Adapter, TI, ELITE, E4T and DB-LORA. Some methods failed to maintain\nconsistency (TI) or follow the prompt (IP-Adapter). Other methods alternated between keeping consistency or following text, but not both (DB-LoRA). Our\nmethod successfully followed the prompt while maintaining consistency.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n15\nOurs\nLoRA DB\n\u201cA photo of a dog\u201d\n\u201cwearing goggles\u201d\n\u201creading the \nnewspaper\u201d\n\u201cresting in the \nwild, wearing his \ngoggles\u201d\n\u201cin the city, \nwearing his \ngoggles\u201d\nOurs\nLoRA DB\n\u201cAn illustration of a wolf\u201d\n\u201ceating his food \nwhile wearing a \nhat\u201d\n\u201csitting on the \nmat, wearing a \nhat\u201d\n\u201cjumping over \na puddle\u201d\n\u201cwalking in the \nstreet, with his \nhat on\u201d\n\u201cA 3D animation of a 12 years old girl\u201d\n\u201ckicking a ball\u201d\n\u201cholding the \nball\u201d\n\u201cthrowing the ball \nup\u201d\n\u201cgoing to sleep, \nholding his ball\u201d\n\u201cA hyper-realistic digital painting of a young ginger boy\u201d\n\u201cwith her pet owl\u201d\n\u201csitting in the \ncouch with her \nowl\u201d\n\u201cstanding \noutside with \nher owl\u201d\n\u201cthe owl sitting \non a branch\u201d\nFig. A.2. Additional Qualitative Multi-Subject Comparisons We evaluated our method against DB-LORA for generation of multiple consistent subjects.\nLora DB tends to neglect the consistency of at least one of the subjects, while our method succeeds in both.\nOurs\nThe Chosen One\n\u201cA hyper-realistic digital painting of a happy \ngirl, brown eyes.\u201d\nFig. A.3. Comparison to The Chosen One, Multi-Subject We evaluated\nour method against a concurrent work by [4]. Notably, while ConsiStory is\ntraining-free, in contrast to the concurrent work that requires an iterative\noptimization process, we demonstrate that our method outperforms it in\npreserving consistency for multiple subjects.\nE\nTRAINING-FREE PERSONALIZATION DETAILS\nWe implemented edit-friendly inversion [25] for SDXL and set the\nguidance scale to 2.0. We inverted two real-images, as shown at\nFig. 12, and used them as anchors in our method. However, in our\n0.600\n0.625\n0.650\nText Similarity\n0.70\n0.75\n0.80\nSubject Consistency\nOurs\nOurs (w/o Dropout & Query Injection)\nOurs (w/o Dropout)\nOurs (w/o FI)\nOurs (w/o SDSA)\nFig. A.4. Quantitative Component Ablation We conducted a quantitative\nevaluation of ConsiStory by ablating various components: Self-Attention\nDropout (Dropout), Query Injection, Feature Injection (FI), and Subject-\nDriven Self-Attention (SDSA). Notably, omitting either SDSA or FI resulted\nin reduced subject consistency. Eliminating the variation enhancement\nmechanisms (Dropout and Query Injection) decreased textual similarity.\nmethod, the anchors are generated with attention sharing between\n16\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\nw/o Dropout & \n Query Injection\nw/o Dropout\nDropout = 0.3\nOurs\nMethods\n0.5\n0.6\n0.7\n0.8\nDiversity\nFig. A.5. Layout Diversity Ablation Our method attains the highest Di-\nversity Score, when compared to variations omitting the Dropout and/or\nthe Query Injection components.\nthem, which is not suitable for the case of inverted anchors. There-\nfore, we modify our anchoring process such that anchors will only\nshare attention maps with generated images, rather than with other\nanchors. We observed that naively using the inverted image atten-\ntion features with our method gave poor personalization results,\nand we hypothesize it is related to a distribution shift between the\nfeatures of the inverted and generated images. Hence, we added\nAdain [24] to align the self-attention keys of inverted features with\nthe self-attention keys of the generated images.\nSince edit-friendly inversion is based on DDPM, we modified our\ngeneration process to also use DDPM scheduling with 100 genera-\ntion steps. We used the default guidance scale of 5.0 for the generated\nimages. We note that this training-free personalization only works\nfor simple objects, and provide failure cases at Fig. A.6.\n\u201cwith Ei\ufb00el Tower in \nthe background\u201d\n\u201cin a box\u201d\n\u201con a wooden \ufb02oor\u201d\n\u201cin the ocean\u201d\n\u201cA photo of a \ncat statue\u2026\u201d\n\u201cA photo of a \nvase\u2026\u201d\nInverted Anchors\nInverted Anchors\nFig. A.6. Training-Free Personalization Failure Our training-free per-\nsonalization method may fail on non common subjects.\nF\nUSER STUDY DETAILS\nWe evaluate the models through two Amazon Mechanical Turk user\nstudies using a two-alternative forced choice protocol. In the first\nstudy, named \u201cVisual consistency\", raters saw two image sets,\neach produced by a different approach. They chose the set where\nthe subject\u2019s identity remained most consistent. In the second study\nnamed \u201cTextual alignment\", users received a text description and\ntwo images from different approaches. They selected the image that\nbetter matched the description.\nF.1\nVisual Consistency\nFor the first study, in each trial, raters were presented with two\nsets of images, each set depicting a subject in various situations.\nThey were instructed to select the set where the subject\u2019s identity\nremained consistent across all images. This decision was to be based\nsolely on the subject\u2019s features and identity, disregarding elements\nsuch as background, clothing, or pose. The focus was on the con-\nsistency of the subject\u2019s identity, including aspects like eye color,\ntexture, facial features, and other subtle details. Example images\nand solutions were provided for guidance. Figure A.7 illustrates the\nexperimental framework used in the trials. Figure A.8 displays the\nexample images provided to help guide raters through the instruc-\ntions.\nThe study included 100 trials of images from 100 unique subjects.\nEach trial was repeated 5 times with 5 different raters. Each image set\nconsisted of 5 images generated from 5 distinct prompt settings. For\nthe ConsiStory images, we selected the variant with less stringent\nidentity preservation parameters. Raters were paid 0.15 per trial. To\nmaintain the quality of the study, we only selected raters with an\nAmazon Mechanical Turk \u201cMasters\u201d qualification, demonstrating a\nhigh degree of approval rate over a wide range of tasks. Furthermore,\nwe also conducted a qualification test on the prescreened pool of\nraters, consisting of 5 curated trials that were simple.\nF.2\nTextual Alignment\nFor the second study, in each trial, raters were presented with a tex-\ntual description and two images. They were instructed to determine\nwhich image better matched the textual description. They were\nadvised to focus on the details of the description. For example, if\nthe text states \"A bear is drinking from a cup\", raters should choose\nthe image where the bear is actually drinking from the cup. For\nguidance, example images with solutions were provided. Figure A.9\nillustrates the experimental framework that was used in trials. The\ntext descriptions only included the general subject class (e.g. dog\u201d)\nrather than fully detailed descriptions (e.g. A cute brown and white\nfluffy puppy dog with blue eyes\u201d). This helped raters focus on the\nsubject\u2019s setting. Figure A.10 displays the examples provided to help\nguide raters through the instructions.\nThe study included 500 trials of images from 100 unique sub-\njects, each subject had 5 distinct prompt settings. For the ConsiStory\nimages, we selected the variant with less stringent identity preserva-\ntion parameters. We paid $0.05 per trial. To maintain the quality of\nthe study, we only selected raters with an Amazon Mechanical Turk\n\u201cMasters\u201d qualification. Furthermore, we also conducted a qualifica-\ntion test on the prescreened pool of raters, consisting of 5 curated\ntrials that were simple.\nTraining-Free Consistent Text-to-Image Generation\n\u2022\n17\nFig. A.7. One trial of the visual consistency user study.\nFig. A.8. Examples provided in the visual consistency user study.\n18\n\u2022\nYoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon\nFig. A.9. One trial of the textual alignment user study.\nFig. A.10. Examples provided in the textual alignment user study.\n"
  },
  {
    "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
    "link": "https://arxiv.org/pdf/2402.01739.pdf",
    "upvote": "25",
    "text": "OpenMoE: An Early Effort on Open\nMixture-of-Experts Language Models\nFuzhao Xue1\u2020\nZian Zheng1\nYao Fu2\nJinjie Ni1\nZangwei Zheng1\nWangchunshu Zhou3\nYang You1\n1National University of Singapore\n2University of Edinburgh\n3ETH Zurich\nAbstract\nTo help the open-source community have a better understanding of Mixture-of-\nExperts (MoE) based large language models (LLMs), we train and release Open-\nMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs,\nranging from 650M to 34B parameters and trained on up to over 1T tokens. Our\ninvestigation confirms that MoE-based LLMs can offer a more favorable cost-\neffectiveness trade-off than dense LLMs, highlighting the potential effectiveness\nfor future LLM development.\nOne more important contribution of this study is an in-depth analysis of the routing\nmechanisms within our OpenMoE models, leading to three significant findings:\nContext-Independent Specialization, Early Routing Learning, and Drop-towards-\nthe-End. We discovered that routing decisions in MoE models are predominantly\nbased on token IDs, with minimal context relevance. The token-to-expert assign-\nments are determined early in the pre-training phase and remain largely unchanged.\nThis imperfect routing can result in performance degradation, particularly in se-\nquential tasks like multi-turn conversations, where tokens appearing later in a\nsequence are more likely to be dropped. Finally, we rethink our design based on\nthe above-mentioned observations and analysis. To facilitate future MoE LLM\ndevelopment, we propose potential strategies for mitigating the issues we found\nand further improving off-the-shelf MoE LLM designs.3\n\u2020Email: f.xue@u.nus.edu\n3https://github.com/XueFuzhao/OpenMoE\nPreprint.\narXiv:2402.01739v1  [cs.CL]  29 Jan 2024\n1\nIntroduction\nLarge Language Model (LLM) has exhibited remarkable performance on various NLP tasks [27,\n35], and has even become a part of our daily lives through chatbot applications such as ChatGPT,\nBard, and Copilot. However, LLMs are computationally expensive in both training and inference. As\nLLMs become increasingly prevalent, enhancing their performance without proportionally increasing\ncomputational resources is a critical challenge. In response to this challenge, Fedus et al. [15] and\nRiquelme et al. [36] proposed the Mixture-of-Experts (MoE) to scale up the trainable parameters of\nthe transformer with little additional computation overhead. Recent advancements in MoE-based\nlanguage models, such as GLaM [14] and ST-MoE [59] have demonstrated superior performance in\nvarious tasks. However, before the release of OpenMoE, there were few open-sourced MoE language\nmodels trained with trillion-level diverse datasets.\nIn this work, we set forth three primary goals: (1) To offer a first-attempt solution in detail for training\na decoder-only MoE model within the existing framework of training LLMs. (2) To perform an\nin-depth analysis of the MoE routing mechanisms, thereby providing the research community with\ndeeper insights into the behaviors and potential limitations of MoE-based LLMs. (3) To pave the way\nfor future MoE LLM development. Through this early endeavor, we aim to stimulate and accelerate\nthe growth of the open-source MoE community.\nReleasing OpenMoE. First, we release OpenMoE, a series of open-sourced MoE-based LLMs,\nincluding: (1) OpenMoE-Base/16E: a small model with 0.65B parameters for debugging purposes.\n16E means 16 experts per MoE layer; (2) OpenMoE-8B/32E: this variant features 8B parameters\nin total, activating around 2B parameters per token in Transformer blocks, and is pre-trained on\nover 1 trillion tokens; (3) OpenMoE-8B/32E-Chat, a chat version of OpenMoE-8B/32E, fine-tuned\nwith a 100K subset of the WildChat [2] dataset; (4) OpenMoE-34B/32E: a larger scale model,\nactivating 6B parameters per token in Transformer blocks and trained with 200B tokens, serving as\na testament to the scalability of our approach. Detailed configuration can be found in Appendix B\nOur OpenMoE-8B/32E models achieved comparable performance with OpenLLaMA-3B [18] and\nTinyLLaMA-1.1B [54], two dense open LLMs used higher training cost. Notably, On the MT-\nBench [56], OpenMoE-8B/32E-Chat outperformed the two dense LLMs significantly on the single-\nturn conversation. In addition, we release 5 intermediate checkpoints of OpenMoE-8B/32E, each\ntrained with 200B more tokens than the previous one, to support and encourage future research.\nSection 2 and 3 will discuss the design, training details, and evaluation results of OpenMoE.\nExploring Advanced Training Strategies. As part of our research endeavor, we are committed\nto exploring more advanced Techniques in LLM training: (1) Different from the common practice\nof training models on in-house or text-dominated open-sourced data, we train OpenMoE with a\nsubstantial proportion of code, constituting up to 52.25% during the early stages of pre-training; (2)\nMoving beyond the conventional next-token prediction training objective, we investigate UL2 training\nobjective [45], motivated by its proven effectiveness in previous work [1] and its good alignment\nwith coding data [5]. We acknowledge that the performance of our model, while acceptable, does\nnot significantly exceed our expectations, which may be attributed to some sub-optimal design\nchoices. Nevertheless, we believe that this exploratory work offers substantial value to the open-\nsource community, particularly in assessing the potential and effectiveness of these under-explored\ntechniques.\nStudying MoE Routing In-depth. While MoE is effective, there remains a lack of study on why\nMoE performs well. From a high level, MoE introduces more trainable parameters than its dense\ncounterpart. To keep the FLOPs fixed when scaling the number of parameters, MoE applies a routing\nlayer that sparsely and adaptively assigns each token to a few experts. This process of sparse expert\nselection is crucial to MoE\u2019s functionality. Unfortunately, despite existing pieces of literature briefly\nvisualizing the routing decison [26, 31, 36, 40, 59], we still don\u2019t have a clear understanding of\nhow the router works and how the routing decision impacts the results in MoE models, especially\nfor the post-ChatGPT LLMs trained on a mixture of datasets from diverse domains. In this work,\nwe study this problem based on various taxonomies, including domain, language, task, and token.\nOur key findings are as follows: (1) Context-independent Specialization: MoE tends to simply\ncluster tokens based on similar token-level semantics, implying that, regardless of context, a certain\ntoken is more likely to be routed to a certain expert; (2) Early Routing Learning: Token ID routing\nspecialization is established early in pre-training and remains largely fixed, resulting in tokens being\nconsistently processed by the same experts throughout the training; (3) Drop-towards-the-End:\n2\nSince each expert has a fixed max capacity, tokens appearing later in the sequence face a higher risk\nof being dropped if the expert is already at capacity. This issue is more severe in instruction-tuning\ndatasets. These datasets often exhibit a domain gap compared to the pre-training data, meaning\nthat the balanced token assignment strategies established and solidified during early pre-training\nmay not be samely effective in instruction-tuning scenarios. This is concerning as instruction data\nplays an important role in deploying LLMs to real-world applications. Section 4 discusses the above\nphenomenons in detail.\nRethinking Our Mistakes and Proposing Potential Solutions. In retrospect, our project encoun-\ntered several mistakes and made sub-optimal decisions (e.g., aggressive data mixture), as detailed\nin Section 5. As an early open-source effort, we believe that sharing these experiences and insights\nis crucial, perhaps even more important than solely focusing on successful strategies. Based on\nour empirical findings during training and subsequent visualization analysis (Section 4), we have\ndeveloped a set of potential solutions. We sincerely hope these insights can help the community\ndevelop better models in the future.\nThe structure of this paper mirrors the lifecycle of the OpenMoE project, encompassing all its phases.\nThis includes the initial design (Section 2), training and evaluation (Section 3, in-depth analysis\n(Section 4), and a rethinking of the OpenMoE project (Section 5).\n2\nDesigning OpenMoE\nFirst, we introduce our initialized design of OpenMoE models regarding the pre-training data, model\narchitecture, training objective, and supervised fine-tuning data.\n2.1\nPre-training Dataset: More Code than Usual\nModern LLMs are usually trained by a combination of datasets from various domains, i.e., data\nmixture [7, 9, 20, 34, 47]. Except for the LLMs customized towards coding (e.g., StarCoder [28],\nCodeLLaMA [38]), most existing models\u2019 pre-training data is dominated by text data. For instance,\nthe sampling rate of the GitHub dataset is only 4.5% for LLaMA [47]. However, we argue that the\ncode data is highly important for two reasons. First, the code data is likely to improve the ability of\ncomplex reasoning with chain-of-thought [16]. More importantly, different from natural language,\nwhich is sometimes blurry and easy to misunderstand, code is always precise. This enables code to\nbe a more efficient language for machines to convey information concisely without misunderstanding\nbetween different (embodied) AI agents, and as a result, code has great potential to dominate LLM\ncommunications in real-life applications. Therefore, we design a more code-dominated pre-training\ndata mixture. As shown in Table 1, we extracted 50% of data from the RedPajama [11] and 50% of\ndata from the duplication version of The Stack [24]. Our experimental results show that the version I\ndata mixture might be a bit aggressive in its code proportion. We fix these issues at the later stage of\npre-training, please see the following Section 3.2 for details.\nTable 1: Three versions of OpenMoE pre-training data mixture.\nVersion I\nVersion II\nVersion III\nModel\nOpenMoE-Base, OpenMoE-8B/32E\nOpenMoE-34B/32E\nPeriod\nbefore 780B tokens \u2192 after 780B tokens\nfrom start to end\nDataset\nSampling Ratio\nRedPajama\n50.0%\n83.5%\n67.5%\nC4\n7.50%\n15.0%\n15.0%\nWikipedia\n2.25%\n6.50%\n4.50%\nStackexchange\n1.00%\n2.50%\n1.00%\nArXiv\n1.25%\n4.50%\n4.50%\nBooks\n2.25%\n6.50%\n4.50%\nGitHub\n2.25%\n5.00%\n5.00%\nCommoncrawl\n33.5%\n43.5%\n33.0%\nWikipedia-en\n0.00%\n6.50%\n2.50%\nThe Stack Dedup\n50.0%\n10.0%\n30.0%\n3\n2.2\nModel Architecture: Decoder-only ST-MoE\nTokenizer. We applied umT5 [10] tokenizer with 256K vocab size for two reasons: (1) umT5\ntokenizer with a large multi-lingual vocab supports low-resource language better than the tokenizers\nusing a small vocab (e.g., LLaMA tokenizer with 32K vocab); (2) comparing to some old tokenizers,\nsuch as BERT [23] and T5 [35] tokenizer, umT5 tokenizer has byte fallback feature to support\nout-of-vocab tokens better.\nToken-choice Routing. We generally follow ST-MoE [59] for our model architecture and routing\ndesign to ensure training stability, which is extremely important when training larger models. Given\nE trainable experts and input representation x \u2208 RD, the output of MoE model can be formulated as:\nMoE(x) =\nE\nX\ni=1\ng(x)iei(x),\n(1)\nwhere ei(\u00b7) is a non-linear transformation RD \u2192 RD of the ith expert, and g(\u00b7)i is the ith element of\nthe output of the trainable router g(\u00b7), a non-linear mapping RD \u2192 RE. Usually, both e(\u00b7) and g(\u00b7)\nare parameterized by neural networks. Please note each expert is an FFN layer instead of a complete\nTransformer model in most MoE-based Transformer models, including ours.\nTop-2 Selection. According to the formulation above, when g(\u00b7) is a sparse vector, only part of the\nexperts would be activated and updated by back-propagation during training. We set the gating layer\nas a top-K selection as:\ng(x) = TopK(softmax(f(x))),\n(2)\nwhere f(\u00b7) is routing linear transformation RD \u2192 RE. When K \u226a E, most elements of g(x) would\nbe zero so that sparse conditional computation is achieved. We set K = 2 following Zoph et al. [59].\nResidual MoE. Each vanilla Transformer block can be written as:\nx\u2032 = LayerNormatt\ni (x),\nx = MHA(x\u2032) + x,\nx\u2032\u2032 = LayerNormffn\ni (x),\nx = FFN(x\u2032\u2032) + x,\n(3)\nIn OpenMoE, for each MoE-based Transformer block, we use one residual MoE layer to ensure that\none fixed FFN layer is always activated for every token. That is:\nx\u2032 = LayerNormatt\ni (x),\nx = MHA(x\u2032) + x,\nx\u2032\u2032 = LayerNormffn\ni (x),\nx = MoE(x\u2032\u2032) + FFN(x\u2032\u2032) + x,\n(4)\nNote we use MoE-based Transformer blocks in an interleaved manner instead of placing MoE in\nevery Transformer block. In our setting, we use MoE every 4 layers in OpenMoE-Base/16E and\nOpenMoE 34B/32E and use MoE every 6 layers for OpenMoE-8B/32E. This setting is inspired by\nthe findings in ViT-MoE [36], i.e., using MoE every layer introduces more computational overhead\nduring routing, and then induces a worse cost-effective trade-off than interleaved MoE usage.\nLoad Balance Loss and Router Z-loss. ST-MoE [59] follows Shazeer et al. [40], using MoE load\nbalance loss to ensure a balanced number of tokens assigned to different experts so that MoE models\ncan achieve better parallelism. For each routing operation, given E experts and N batches with\nB = NL tokens, the following auxiliary loss is added to the total model loss during training:\nLb = E \u00b7\nE\nX\ni=1\nmi \u00b7 Pi,\n(5)\nwhere m is a vector, Pi is softmax(f(x)). i denotes the expert ID. The ith element is the fraction of\ntokens dispatched to expert i:\nmi = 1\nB\nB\nX\nj=1\nh(xj)i,\n(6)\n4\nTable 2: UL2\u2019s mixture-of-denoisers configuration, \u00b5 is average span length and r is the mask ratio.\nTraining Objective\nPercentage\nPrefixLM, r=0.5\n50%\nSpanCorrupt\n\u00b5=3, r=0.15\n10%\n\u00b5=8, r=0.15\n10%\n\u00b5=3, r=0.5\n10%\n\u00b5=8, r=0.5\n10%\n\u00b5=64, r=0.5\n10%\nwhere h(\u00b7) is an index vector selected by TopK in Eq. 2. h(xj)i is the ith element of h(xj). It is\nnoticeable that, different from g(x)i in Eq. 2, mi and h(xj)i are non-differentiable. However, a\ndifferentiable loss function is required to optimize MoE in an end-to-end fashion, so we use the\nrouting score softmax(f(x)) in Eq. 2 (i.e., Pi in Eq. 5)to make the routing decision differentiable\nand then learnable.\nIn addition to the load balance loss, Zoph et al. [59] proposed router z-loss for more stable MoE\ntraining:\nLz(x) = 1\nB\nB\nX\ni=1\n\uf8eb\n\uf8edlog\nE\nX\nj=1\nex(i)\nj\n\uf8f6\n\uf8f8\n2\n(7)\nThis router z-loss can penalize large logits input to the gating network and encourage the absolute\nmagnitude of numbers to be small so that it can reduce the round-off errors in MoE layers. Please\nrefer to ST-MoE paper [59] for a detailed explanation.\nTaken together, our final training loss can be written as:\nL = LCE + Lb + Lz\n(8)\nwhere LCE is the cross-entropy loss in language model pre-training.\n2.3\nTraining Objective: UL2 and CasualLM\nInstead of adopting vanilla casual language modeling (CasualLM) directly, we explore UL2 [44], a\nmore diverse language model pre-training objective combining span corruption (SpanCorrupt) and\nprefix language modeling (PrefixLM) [35]. It is noteworthy that the SpanCorrupt in UL2 is more\ndiverse than the vanilla SpanCorrupt because it mixes various span lengths and corruption rates. We\nhave two reasons to explore UL2 in OpenMoE. First, UL2 has shown promising results in PaLM-2 [1].\nMore importantly, the aggressive token masking is very similar to the code completion task in the\nreal world, such as Copilot. Bavarian et al. [5] also found that the similar filling-in-the-middle (FiM)\nobjective can model the code better than the vanilla training objective. Since we used more code in\nour pre-training data mixture, adapting UL2 that covers FiM is a more reasonable choice intuitively.\nOur detailed UL2 training objective configuration is shown in Table 2. We use only 20% low mask\nratio (r=0.15) because there are fewer output tokens during training, which may slow down the\nlearning. We also use more PrefixLM than the default UL2 setting because we think the zero-shot and\nin-context learning ability enhanced by PrefixLM training is important. We faced some difficulties\nwhen training with UL2 in OpenMoE, which will be discussed in Section 3.2.\n2.4\nSupervised Fine-tuning\nAlthough alignment is not the focus of this OpenMoE project, we still conduct supervised fine-tuning\n(SFT) with a subset of the open-sourced WildChat dataset [2] to enhance the instruction following\nability and study the behavior of the MoE model before and after SFT. We only pick the instruction-\nresponse pairs from GPT-4 in WildChat because of the lack of computation resources at the late stage\nof OpenMoE development. The subset includes 58K conversations and each conversation includes\n1.8 turns on average.\n5\nTable 3: Ablation study with OpenMoE-Base/16E on zero-shot TriviaQA [22].\nMethod\nEM\nF1\nOpenMoE\n1.4\n4.5\nw/o MoE\n0.1\n0.3\nw/o UL2 (PrefixLM only)\n0.0\n0.0\nw/o Code data\n0.7\n1.1\nw/ LLaMA tokenizer\n2.2\n5.7\n0\n1\n2\n3\n4\n5\nStep\n1e5\n1\n2\n3\n4\n5\n6\nLoss\nC4\nCommonCrawl\nBooks\nWikipedia\nStackExchange\nArXiv\nTheStack\nGitHub\n(a) Comparison of the validation loss on different\npre-training datasets.\n0\n1\n2\n3\n4\n5\nStep\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nC4\nCommonCrawl\nBooks\nWikipedia\nStackExchange\nArXiv\nTheStack\nGitHub\n(b) Comparison of validation accuracy on different\npre-training datasets.\nFigure 1: Comparison of the validation loss and accuracy on different pre-training datasets. We can\nobserve that models are easier to achieve higher accuracy and lower loss on code data.\n2.5\nOther Designs\nFollowing recent LLMs, we adopt RoPE [43] for position embedding and SwiGLU [39] for activation\nfunction for FFNs in both dense and MoE Transformer blocks. More detailed model configuration\nand training hyperparameters for OpenMoE models can be found in Appendix B. We applied data\nparallelism, tensor parallelism [41, 50], and expert parallelism [25] for training models at scale. We\ntrain OpenMoE models on Google Cloud TPU with 64 to 512 v3 chips depending on the availability.\n3\nTraining OpenMoE\n3.1\nAblation Study\nAs an initial evaluation of our design decisions, we conducted an ablation study using the OpenMoE-\nBase/16E model. It\u2019s important to note that while these results provide early insights, we cannot be\ncertain of their generalizability to larger models, primarily due to computational resource constraints\nthat preclude larger-scale ablations.\nOur findings indicate that several elements \u2014 the MoE approach, the UL2 training objective, and the\nincreased emphasis on code data \u2014 all contribute positively to the base version\u2019s performance in\nzero-shot TriviaQA tasks. The model using LLaMA tokenizer [47] outperforms the one with umT5\ntokenizer. This outcome is considered acceptable, even though a larger vocabulary size might slightly\nimpair performance. We believe that supporting low-resource languages is crucial, as foundational\nmodels should be accessible and beneficial to a diverse global audience. After this sanctity check, we\nproceed to scale OpenMoE up to OpenMoE-8B/32E.\nWe also conduct an ablation study to compare the progress of learning the data from different domains.\nAs shown in Figure 1, we can observe that models are easier to achieve higher accuracy and lower loss\non code data. On Github, although our model is small, it can still achieve over 80% token prediction\naccuracy. We infer that this is because of the long-tail token distribution in code data. For instance, a\nlarge number of tokens in code are \u201c\\n\u201d and \u201c\\t\u201d, which are relatively easier to predict.\n6\n0\n1\n2\n3\n4\n5\nStep\n1e5\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy\nOpenMoE-Base/16E\nOpenMoE-8B/32E\nOpenMoE-34B/32E\nFigure 2: Token prediction accuracy of OpenMoE models. OpenMoE-8B/32E uses UL2 before 390K\nsteps and falls back to CasualLM after 790K steps. OpenMoE-34B/32E uses UL2 until 50B tokens.\n3.2\nTraining Progress\nUL2 Saturation During training, we found that, although UL2 can help the model to learn faster at\nthe early stage of training, it is easier to saturate at the later training stage of OpenMoE-8B/32E. As\nshown in Figure 2, if we zoom in, we can find that OpenMoE-8B/32E improves very slowly from 35K\nto 39K steps. We suggest that this may be because, although UL2 is more diverse, the SpanCorrupt is\nstill relatively easy compared to CasualLM. Therefore, we fall back to CasualLM after 390K steps\n(780B) tokens. In addition, since code data aligns better with UL2 and our initial code data mixture\nis relatively aggressive, we also decreased our code data sampling ratio to 15%. The Second version\ndata mixture is reported in Table 1.\nObviously, in Figure 2, after 780B tokens, there is a significant drop in the token prediction accuracy\nafter 390K steps for OpenMoE-8B/32E. This is caused by the more difficult CasualLM objective\nand less easy code data. Note that, although we encountered a saturation issue at the later stage\nof OpenMoE-8B/32E training, we think such an easy-to-hard curriculum may be helpful for LLM\ntraining. Therefore, we still adapted UL2 for 25K steps (50B tokens) in OpenMoE-34B/32E. We used\na relatively moderate code-heavy data mixture in OpenMoE-34B/32E. As shown in Table 1, we utilize\n35% of code data in total. Due to the computation resource limitation, we train OpenMoE-34B/32E\nwith only 200B tokens to verify its scalability. We leave training a large-scale OpenMoE with more\ntokens as future work if possible.\n3.3\nEvaluation on Benchmarks\n3.3.1\nRaw Model Evaluation\nBefore all, we highlight that we did not hack the benchmarks at all and the pre-training is purely\non the open-sourced datasets mentioned above. Since our model is relatively small in terms of\ntraining budget, we mainly evaluate the raw model on established but not that hard benchmarks,\ni.e., TriviaQA [22], HumanEval [8], WMT16-En-Ro [6], BigBench-Lite (24 tasks) [4], and a subset\nof the lm-evaluation-harness collection [17] with 13 tasks. For popular but relatively challenging\nbenchmarks like 5-shot MMLU [19], our OpenMoE-8B/32E achieves around 26.2% accuracy, which\nmeans the model is almost randomly guessing from the four options. We mainly compare with the\nopen-sourced models with more training cost, i.e., TinyLLaMA-1.1B [54] and OpenLLaMA-3B [18].\nOn BigBench-Lite, we also compare with GPT-3 [7], Big-G [4] and Big-G-Sparse [4]. Big-G and\nBig-G-Sparse are two sets of Google in-house Transformer models evaluated on BigBench-Lite, and\nBig-G-Sparse models are MoE-based Transformers.\nWe first report our results on Commonsense QA (TriviaQA), Coding (HumanEval), and Low-resource\nMachine Translation (WMT16 En-Ro). We think these three benchmarks are meaningful for us\nbecause (1) Commonsense is to check whether OpenMoE can memorize more commonsense given\nits efficient parameter scaling advantage; (2) Coding is important because of its prevalent use cases in\nsolving coding-related user prompts, LLM as agents, and embodied AI; (3) Low-resource Machine\nTranslation is important because we want to share the benefits of foundation models to everyone on\n7\nTable 4: Results on TriviaQA (Exact Match). We also report the number of training tokens from\nWikipedia because the commonsense questions in TriviaQA have a relatively close relation with\nWikipedia data.\nModel\nAct. Params\nTotal Tokens\nText Tokens\nWiki Tokens\nTriviaQA\nTinyLLaMA-1.1B\n0.9B\n3.0T\n2.1T\n75B\n11.2\nOpenLLaMA-3B\n2.9B\n1.0T\n991B\n24B\n29.7\nOpenMoE-8B/32E\n2.1B\n1.1T\n644B\n58B\n32.7\nOpenMoE-34B/32E\n6.4B\n0.2T\n130B\n14B\n31.3\nTable 5: Results on HumanEval (Pass@1). We also report the number of training tokens from the\ncode domain (The Stack and GitHub data).\nModel\nAct. Params\nTotal Tokens\nCode Tokens\nHumanEval\nTinyLLaMA-1.1B\n0.9B\n3.0T\n900B\n9.1\nOpenLLaMA-3B\n2.9B\n1.0T\n59B\n0\nOpenMoE-8B/32E\n2.1B\n1.1T\n456B\n9.8\nOpenMoE-34B/32E\n6.4B\n0.2T\n70B\n10.3\nearth. As shown in Table 4, OpenMoE-8B/32E outperforms baselines clearly with less training cost\n(Activated parameter\u00d7Total Training tokens). Also, please note that TinyLLaMA-1.1B performs\nsignificantly worse than other models on TriviaQA although it has a comparable training cost with\nOpenLLaMA-3B. Therefore, this highlights the importance of the number of parameters to keeping\nknowledge in LLMs, which also indicates the significance of using MoE.\nIn Table 5, OpenMoE models achieve better performance than baselines. OpenMoE-34B/32E\nonly used 70B code data, while it still performs relatively well on HumanEval, which shows the\nscalability of OpenMoE, although we don\u2019t have enough computation resources to train it until the\nend. OpenLLaMA-3B struggles on HumanEval because consecutive whitespaces are treated as one,\ncontradicting the Python syntax[32].\nTable 6 shows our results on WMT16 En-Ro translation task. Note that our model did not include\nmuch multi-lingual data intentionally. Most multi-lingual data should be from the multi-lingual\nversion of Wikipedia in the RedPajama, which is also used in TinyLLaMA-1.1B and OpenLLaMA-3B.\nHowever, OpenMoE models still show better results than baselines, which potentially highlights the\nimportance of umT5 tokenizer.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nRelative Cost\n0\n1\n2\n3\n4\n5\n6\n7\n8\nBigBench-Lite (3-shot)\nBIG-G-42M\nBIG-G-1B\nBIG-G-2B\nBIG-G-4B\nBIG-G-8B\nBIG-G-27B\nBIG-G-Sparse-4B\nBIG-G-Sparse-7B\nBIG-G-Sparse-17B\nBIG-G-Sparse-25B\nBIG-G-Sparse-60B\nGPT-3-35M\nGPT-3-76M\nGPT-3-1B\nGPT-3-3B\nGPT-3-6B\nGPT-3-13B\nOpenMoE-8B\nModel\nBIG-G\nBIG-G-Sparse\nGPT-3\nOpenMoE\nFigure 3: Results on BigBench-Lite. The relative cost is computed based on multiplying activated\nparameters in the Transformer and the number of training tokens. The size of the color dots denotes\nthe number of activated parameters, and the size of the shadow denotes the number of total parameters\nfor MoE models.\n8\nTable 6: Results on WMT16 En-Ro (BLEU score). We also report the number of explicit multi-lingual\ntokens in the pre-training dataset, i.e., the multi-lingual version of Wikipedia from the RedPajama\ndataset.\nModel\nAct. Params\nTotal Tokens\nMulti-lingual Tokens\nWMT16 En-Ro\nTinyLLaMA-1.1B\n0.9B\n3.0T\n75B\n2.6\nOpenLLaMA-3B\n2.9B\n1.0T\n24B\n1.9\nOpenMoE-8B/32E\n2.1B\n1.1T\n38B\n3.1\nOpenMoE-34B/32E\n6.4B\n0.2T\n9B\n3.4\nTable 7: Evaluate OpenMoE-8B/32E on lm-evaluation-harness. The results of OpenLLaMA are from\nits homepage, which only provides two effective digits.\nDataset\nTinyLLaMA-1.1B\nOpenLLaMA-3B\nOpenMoE-8B/32E\nANLI-R1\n34.2\n33.0\n32.7\nANLI-R2\n32.4\n36.0\n33.2\nANLI-R3\n35.1\n38.0\n33.9\nHellaSwag\n59.2\n52.0\n45.5\nWinoGrande\n59.1\n63.0\n60.3\nPIQA\n73.3\n77.0\n74.2\nARC-Easy\n55.2\n68.0\n64.1\nARC-Challenge\n30.1\n34.0\n30.3\nBoolq\n57.8\n66.0\n61.2\nTruthfulQA\n37.6\n35.0\n36.0\nOpenbookQA\n21.8\n26.0\n24.6\nRTE\n51.9\n55.0\n53.4\nWiC\n50.1\n50.0\n49.8\nAverage\n45.9\n48.7\n46.1\nIn Figure 3, the relative cost is computed based on multiplying activated parameters (Act. Params)\nin Transformer blocks and the number of training tokens. The size of the color dots denotes the\nnumber of activated parameters, and the size of the shadow denotes the number of total parameters\nfor MoE models. We can observe that OpenMoE achieved a better cost-effectiveness trade-off on\nBigBench-Lite, in terms of both training and inference cost.\nWe also evaluate OpenMoE on the 13 tasks from the LM-Evaluation-Harness collection. As shown in\nTable 7, both OpenMoE and TinyLLaMA performed worse than OpenLLaMA. However, the scores\nachieved by OpenMOE are acceptable. We suggest that the initial high sampling rate on the code\ndata may harm the results on these text-dominated benchmarks, which is one of the issues we will\ndiscuss in Section 5.\n3.3.2\nChat Model Evaluation\nWe further evaluate our model on MTBench, an established ChatBot benchmark that is able to\nexamine models comprehensively. We report both single-turn and multi-turn results in Figure 4a and\nTable 8. We can observe that OpenMoE outperforms baselines by a large margin on the single-turn\nresults, especially on coding tasks. However, OpenMoE\u2019s performance drops more on the second\nturn, which results in worse multi-turn results in Figure 4b. We found that this probably be caused by\nthe token drop of a long sequence. Please see the following Section 4 for a detailed analysis.\n4\nAnalyzing OpenMoE\nWe generally think MoE is an effective way to scale parameters up with a fixed computation budget.\nHowever, we have little idea about what the experts in MoE specialize in. In this section, we conduct\nan in-depth analysis of OpenMoE in multiple aspects to study the routing behavior.\n9\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\nGPT-J-6B\nTinyLLaMA-1.1B\nOpenLLaMA-3B\nOpenMoE-8B/32E\n(a) Single-turn results.\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\nGPT-J-6B\nTinyLLaMA-1.1B\nOpenLLaMA-3B\nOpenMoE-8B/32E\n(b) Multi-turn results.\nFigure 4: Evaluate OpenMoE on MTBench.\nTable 8: Average scores on MT-Bench.\nModel\nMT-Bench 1st Turn\nMT-Bench 2nd Turn\nMT-Bench Avg\nGPT-J-6B (0.4T)\n2.51\n2.35\n2.43\nTinyLLaMA-1.1B (3T)\n4.08\n2.54\n3.31\nOpenLLaMA-3B (1T)\n4.36\n3.62\n3.99\nOpenMoE-8B/32E (1.1T)\n4.69\n3.26\n3.98\n4.1\nWhat are the Experts Specializing in?\nDoes MoE specialize in domain level? We first visualize the routing decision of the tokens from\ndifferent subsets in the RedPajama dataset. Note that all visualization results are from the third MoE\nlayer by default because we did not observe significant differences across layers. We can observe\nthat the tokens from different subsets (i.e., domains) are uniformed distributed on the plot. That is,\nalthough E21 slightly prefers code tokens and E10 like books a little, most experts in MoE are not\nspecialized based on the domains.\nDoes MoE specialize in language level? We move forward toward finer-grain data to check whether\nMoE specializes in different coding languages and natural languages. In Figure 6, we compare 4\ndifferent coding languages, i.e., Assembly, Blitzmax, Java, and Python. Similar to the domain level,\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\narxiv\nbook\nc4\ncc\ngithub\nstackexchange\nwikipedia\nFigure 5: Visualization of the routing decision on the RedPajama dataset. Ei denotes the ratio of\ntokens routed to ith expert.\n10\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\nassembly\nblitzmax\njava\npython\nFigure 6: Visualization of the routing decision on TheStack dataset. Ei denotes the ratio of tokens\nrouted to ith expert.\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\nar\nde\nes\nfr\nhe\nit\nja\nko\nnl\nru\nzh-cn\nzh-tw\nFigure 7: Visualization of the routing decision on TED-Parallel-Corpus including 12 languages,\ni.e., ar (Arabic), de (German), es (Spanish), fr (French), he (Hebrew), it (Italian), ja (Japanese), ko\n(Korean), nl (Dutch), ru (Russian), zh-cn (Chinese Simplified), zh-tw (Chinese, Traditional), Ei\ndenotes the ratio of tokens routed to the ith expert.\neven for Assembly and Blitzmax, i.e., two low-resource languages compared with Java and Python,\nthey still did not exhibit significant expert specialization.\nWe further study the expert specialization on different natural languages. We adopted a multi-lingual\nparallel corpus, i.e., TED-Parallel-Corpus 4 as the platform. In Figure 7, we found that there is a\nrelatively clear specialization among different experts. For instance, zh-cn (Chinese, Simplified) and\nzh-tw (Chinese, Traditional) both have a strong preference for E5 and E16; ja (Japanese), and ko\n(Korean) both prefer E14.\nDoes MoE specialize in task level? Based on the findings above, finer-grained data has clearer\nexpert specialization observation. We then visualize the routing decision on MT-Bench conversation\ndata in Figure 8. We can see a similar specialization as above, especially for the math data. We\nsuggest that the main reason is that the math tasks include more special tokens than other tasks.\nDoes MoE specialize in Position ID? Routers in MoE make decisions based on the token represen-\ntations. The token representations are from token embeddings and position embeddings. We thus\n4https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n11\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\nwriting\nroleplay\nreasoning\nmath\ncoding\nextraction\nstem\nhumanities\nFigure 8: Visualization of the routing decision on MT-Bench. We adopt the conversation history\nwhen evaluating OpenMoE MT-Bench as the visualization data source. Ei denotes the ratio of tokens\nrouted to the ith expert.\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\nPosition 256\nPosition 512\nPosition 768\nPosition 1024\nPosition 1280\nPosition 1536\nPosition 1792\n(a) Uniform sampled token IDs.\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\nPosition 256\nPosition 257\nPosition 258\nPosition 259\n(b) Consecutive token IDs.\nFigure 9: Visualization of the routing decision at different Position IDs. Ei denotes the ratio of tokens\nrouted to the ith expert.\nvisualize the routing decisions on different positions in Figure 9a and Figure 9b. We can observe:(1)\nthere are indeed some specializations in different Position IDs; (2) consecutive positions prefer similar\nexperts, such as the E10 and E19 in Figure 9b.\nDoes MoE specialize in Token ID? Since we are using the umT5 tokenizer, tokens from different\nlanguages usually have different token IDs. Therefore, we further study whether the router in MoE\nmainly makes its decisions based on the Token ID. We visualize the routing decisions of a few\nrepresentative tokens in Figure 10. All these tokens show a very strong specialization on only a\nfew experts. This is a very interesting finding because the tokens with the same Token ID have\nvery diverse contexts in different sentences. For instance, the token \u201ced\u201d can be the suffix of many\ndifferent words, e.g., \u201cpreferred\u201d, and \u201cled\u201d. The token \u201can\u201d can also be part of \u201can apple\u201d or\n\"another\". However, all these tokens have very strong specialization on only a few fixed experts.\nThat means, MoE simply routes based on the Token ID instead of high-level semantics. We name\nthis observation as Context-independent Specialization in the following sections. To verify that\nthe Context-independent Specialization also exists for other Token IDs, we plot the routing decision\nstandard deviation in Appendix E.\n12\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\n.\n,\n1\n2\na\nan\ned\nFigure 10: Visualization of the routing decision at different Token IDs. Ei denotes the ratio of tokens\nrouted to the ith expert.\nTable 9: Top Tokens selected by each expert.\nExpert ID\nTop Tokens\n0\n\\n , \u2018 , \u2019 , s , - , $ , y , _ ,\n, 2\n1\n\\n , 1 ,\n, 2 , \\\\ , S , . , - , C , {\n21\n, , and ,\n, . , \\n , = , \\t , the ,\n, n\n30\n} , ed , d , have , ing , , , has , s , \" , had\n31\nto , can , s , of , ing , will , not , e , ed , would\n4.2\nToken Specialization Study\nAre experts clustering similar tokens? As we discussed above, the tokens with the same Token\nID are always routed to the same expert no matter what the context is, i.e., Context-independent\nSpecialization. We thus investigate whether the experts prefer the Token IDs corresponding to the\ntokens with similar low-level semantics. We list the top 10 favorite tokens for each expert in Table 9.\nWe can observe that similar tokens are clustered in experts. For instance, \u201ccan\u201d. \u201cwill\u201d, and \u201cwould\u201d\nare all in expert 31. \u201chave\u201d. \u201chas\u201d, and \u201chad\u201d are all included in expert 30. This visualization can also\nexplain many observations above. An example is that, in most figures above, we can find most coding\nand math data prefer expert 21. Here it reveals the real reason. Expert 21 has a strong preference for\n\u201c=\u201d, \u201cand\u201d, and \u201c\\n\u201d, which appear more frequently in math and code.\nWhen did the model learn the specialization? According to the Context-independent Specialization\nobserved above, the model is not learning how to route based on high-level semantics. Therefore,\nwe raise another question, when did the model learn and fix the routing decision for the tokens?\nWe compare the routing decisions of different OpenMoE intermediate checkpoints in Figure 11a\nand Figure 11b. We can see that the expert preferences are almost totally overlapped for different\ncheckpoints, which means that the model has started to fix its routing at the very early stage of\ntraining. Even if we change the training data mixture (from 52.25% code to 20% code) and training\nobjective (from UL2 to CasualLM), the routing decision is still fixed. We infer that the reason is that,\nwhen the token is usually assigned to one specific expert, the loss would increase a lot if the token is\nsent to another unseen expert, which pushes the model to assign the token back to the original expert.\nTherefore, the routing probably has been learned at the warmup stage or so, and kept throughout the\nwhole following training stage.\n4.3\nToken Drop During Routing\nDrop-towards-the-End In MoE models, we usually set a pre-defined max capacity C for every\nexpert to ensure a balanced workload, which means each expert cannot process more than C tokens.\n13\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\n200B training tokens\n400B training tokens\n600B training tokens\n800B training tokens\n1.0T training tokens\n(a) Token \u201ced\u201d routing decision of different intermedi-\nate checkpoints.\nE_0\nE_1\nE_2\nE_3\nE_4\nE_5\nE_6\nE_7\nE_8\nE_9\nE_10\nE_11\nE_12\nE_13\nE_14\nE_15\nE_16\nE_17\nE_18\nE_19\nE_20\nE_21\nE_22\nE_23 E_24 E_25\nE_26\nE_27\nE_28\nE_29\nE_30\nE_31\n200B training tokens\n400B training tokens\n600B training tokens\n800B training tokens\n1.0T training tokens\n(b) Token \u201can\u201d routing decision of different intermedi-\nate checkpoints.\nFigure 11: Visualization of token IDs\u2019 routing decision of different intermediate checkpoints. Ei\ndenotes the ratio of tokens routed to the ith\n0\n500\n1000\n1500\n2000\nPosition ID\n0\n5\n10\n15\n20\n25\n30\n35\nToken Drop Ratio (%)\nRedpajama\nTheStack\nMultilingual TED\nMTBench\n(a) Different datasets.\n0\n500\n1000\n1500\n2000\nPosition ID\n0\n10\n20\n30\n40\n50\nToken Drop Ratio (%)\nRaw Model\nChat Model\n(b) Before and after supervised fine-tuning.\nFigure 12: Comparing the ratio of tokens dropped at different position IDs.\nThis can ensure the throughput when training and deploying the MoE model with expert parallelism,\ni.e., distributing different experts to different GPUs. However, this will also introduce an issue, the\nlater tokens would be dropped if the previous tokens have filled the expert. In decoder-only MoE\narchitecture, due to the auto-regressive nature, the later tokens in a sequence may be dropped more.\nFor instance, if one expert prefers \u201c\\n\u201d token, and a sequence starts with many \u201c\\n\u201ds and also has a\nlot of \u201c\\n\u2019s in the following output generated, the expert would be filled with \u201c\\n\u201d tokens quickly and\nall other tokens appeared later, which should be assigned to this expert, would be dropped. To verify\nthis, we visualize the ratio of tokens dropped at different position IDs. As shown in Figure 12a, the\ngeneral pre-training datasets, e.g., RedPajama and TheStack achieved balanced token assignment,\nonly having a small proportion of tokens dropped, even for the Position ID after 1500. However,\nfor multi-lingual and instruction-following datasets, a large ratio of tokens is dropped. We suggest\nthe reason is, as we discussed above, the routing decision is fixed at the early stage of training and\ndoes not change anymore, so the load balance is also achieved based on the pre-training dataset. The\ninstruction following data can be seen as a type of out-of-domain (OOD) data of the MoE router,\nwhich would induce an unbalanced token assignment so that many tokens appearing later would be\ndropped.\nCan supervised fine-tuning with instruction-following data alleviate this Drop-towards-the-End\nissue? Since the Drop-towards-the-End issue is mainly caused by the OOD data, it is natural to\nthink and study whether it is possible to convert the instruction-following data to in-domain data\nby tuning MoE with the instruction dataset. Therefore, we compare the models before and after\n14\nTable 10: Compare umT5 tokenizer and LLaMA tokenizer on the subsets extracted from different\ndatasets. Vocab used denotes the number of token IDs activated when tokenizing the whole subset.\nThe umT5/LLaMA means, when tokenizing the same subset, the ratio of the number of tokens\ngenerated by umT5 and LLaMA.\nDataset\nSubset\nLLaMA Tokenizer\numT5 Tokenizer\numT5/LLaMA\n#Tokens\nVocab Used\n#Tokens\nVocab Used\nRedPajama\narxiv\n125,339\n8,327\n131,059\n8,762\n1.046\nbook\n137,972\n11,603\n131,072\n15,202\n0.950\nc4\n28,592\n5,439\n26,428\n5,554\n0.924\ncc\n78,450\n8,738\n73,403\n9,927\n0.936\ngithub\n54,707\n4,769\n59,732\n4,539\n1.092\nstackexchange\n40,659\n4,714\n43,195\n4,317\n1.062\nwikipedia\n37,406\n7,179\n30,555\n8,748\n0.817\nTheStack\nassembly\n49,143\n3,066\n50,738\n3,130\n1.032\nblitzmax\n78,259\n4,200\n80,658\n4,209\n1.031\njava\n64,236\n4,229\n69,902\n3,905\n1.088\npython\n66,243\n5,095\n70,795\n4,799\n1.069\nMTBench\nwriting\n6,062\n1,700\n5,786\n1,535\n0.954\nroleplay\n4,309\n1,291\n4,076\n1,172\n0.946\nreasoning\n2,369\n478\n2,309\n429\n0.975\nmath\n5,163\n290\n5,154\n282\n0.998\ncoding\n4,955\n651\n5,256\n631\n1.061\nextraction\n7,058\n1,376\n6,817\n1,234\n0.966\nstem\n4,783\n1,151\n4,527\n1,039\n0.946\nhumanities\n6,398\n1,451\n5,946\n1,320\n0.929\nMulti-lingual\nar\n256,952\n187\n88,406\n8,037\n0.344\nTED\nde\n103,270\n4,880\n80,593\n8,470\n0.780\nes\n101,212\n4,745\n78,713\n8,519\n0.778\nfr\n115,057\n5,156\n95,978\n8,164\n0.834\nhe\n242,446\n239\n86,891\n4,074\n0.358\nit\n109,591\n4,593\n84,201\n8,833\n0.768\nja\n144,825\n931\n63,491\n6,860\n0.438\nko\n257,107\n596\n106,770\n2,736\n0.415\nnl\n102,703\n4,234\n75,084\n7,540\n0.731\nru\n107,144\n2,502\n74,445\n9,658\n0.695\nzh-cn\n149,581\n1,058\n88,107\n3,611\n0.589\nzh-tw\n173,415\n1,107\n93,693\n3,619\n0.540\nsupervised fine-tuning in Figure 12b. We can see the models do not have a significant difference in\nthe Drop-towards-the-End issue. This matches well with our insight above, i.e., the routing behavior\nlearned and fixed at the very early stage of LLM pre-training.\n5\nRethinking OpenMoE\nWorking on this project is a long journey for authors. We indeed made some mistakes during design\nand development, but we also achieved some new insights in the analysis. We thus write down\neverything we found without any reservation in this paper to help future practitioners. Then, in this\nsection, we discuss how to train a better model in the future, which are the most important takeaways\nof our work.\nHow much code shall we use? To be honest, we do not have a very precise answer. Conducting\nan ablation study is extremely expensive because of the cost of pre-training LLM at scale. The\nconclusion may also strongly depend on the model size and data quality. However, according to\nour observation, over 50% code looks too aggressive which may harm the abilities on text tasks,\nbut considering the importance of writing code, we suggest using around 30% code as we used in\nOpenMoE-34B/32E.\n15\nTokenizer Selection Our large tokenizer vocabulary introduces computation overhead at the last\noutput layer after Transformer blocks. Although this overhead would become relatively small after\nscaling the Transformer model up, it is still valuable to make the tokenizer selection smarter. We\nconduct a quantitative analysis of the tokenizer with the datasets we used in Section 4. As shown\nin Table 10, umT5 tokenizer is indeed much better than LLaMA tokenizer on the multi-lingual\ndataset, especially on the low-resource language. It is also slightly better than LLaMA on the\ninstruction-following data. However, it did not match well with our expectation that it could save\nmore tokens for the code data. In addition, we observe that the token usage in both tokenizers is\nextremely long-tail distributed, which indicates that there is a large room to improve the tokenizer\nand following algorithms. As we know, learning from long-tailed data is hard [55]. Since we only\nhave a little multi-lingual data in our pre-training data mixture, the computation cost of predicting the\nlogits of those low-resource tokens is wasted. Based on our sub-optimal choice, we also need a solid\ntokenizer benchmark, which would help people evaluate tokenizers systematically. And we can then\npick the best tokenizer before training the model.\nMore Efficient MoE Architecture According to our observation, MoE routing is almost context-\nindependent (i.e., Context-independent Specialization), we suggest that we can (1) remove the\ntrainable router after warmup stage; (2) adopt parallel Transformer layer [9, 48] computing FFN layer\nbased on the input directly instead of using the output of attention layer; (3) overlapping the attention\nlayer computation and MoE layer all-to-all communication. (1) and (3) will improve the hardware\nutilization and (2) can enable (3) without performance drop when scaling up [9].\nMix instruction-following data during pre-training warm-up to control load balance and\nalleviate Drop-towards-the-End. According to our results on multi-turn MT-Bench, it is very\nimportant to alleviate the Drop-towards-the-End issue. To this end, the key is to make the MoE\nachieve load balance on instruction-following data. Again, since the MoE learns and fixes the routing\nbehavior at the early stage of pre-training, a straightforward solution is mixing the instruction-tuning\ndata into the pre-training corpus during warm-up. This data mixing is not to align the model to\nlearn how to follow instructions. Instead, we hope the model achieves the balanced token routing on\ninstruction-tuning data, which paves the way to our final usage case of LLMs.\n6\nConclusion\nIn this work, we explore how to train MoE for open-sourced communities. We achieved posi-\ntive results that verified the effectiveness of MoE-based LLM in the post-ChatGPT stage. We\ndisclosed all details, and our model is fully reproducible with the open-sourced code and data.\nMore importantly, we conducted an in-depth analysis on our MoE-based LLM and found important\n\u201cContext-independent Specialization\u201d \u201cEarly Routing Learning\u201d and \u201cDrop-towards-the-End\u201d. We\nalso rethink the mistakes we made and propose possible solutions for future developers. We sincerely\nhope this work can help the open-source community have a better understanding of MoE models. All\nthe best!\nReferences\n[1]\nR. Anil et al., \u201cPalm 2 technical report,\u201d arXiv preprint arXiv:2305.10403, 2023.\n[2]\nAnonymous, \u201c(inthe)wildchat: 570k chatGPT interaction logs in the wild,\u201d in The Twelfth\nInternational Conference on Learning Representations, 2024. [Online]. Available: https:\n//openreview.net/forum?id=Bl8u7ZRlbM.\n[3]\nM. Artetxe et al., \u201cEfficient large scale language modeling with mixtures of experts,\u201d arXiv\npreprint arXiv:2112.10684, 2021.\n[4]\nB.-b. authors, \u201cBeyond the imitation game: Quantifying and extrapolating the capabilities\nof language models,\u201d Transactions on Machine Learning Research, 2023, ISSN: 2835-8856.\n[Online]. Available: https://openreview.net/forum?id=uyTL5Bvosj.\n[5]\nM. Bavarian et al., \u201cEfficient training of language models to fill in the middle,\u201d arXiv preprint\narXiv:2207.14255, 2022.\n[6]\nO. r. Bojar et al., \u201cFindings of the 2016 conference on machine translation,\u201d in Proceedings of\nthe First Conference on Machine Translation, Berlin, Germany: Association for Computational\nLinguistics, Aug. 2016, pp. 131\u2013198. [Online]. Available: http://www.aclweb.org/\nanthology/W/W16/W16-2301.\n16\n[7]\nT. B. Brown et al., \u201cLanguage models are few-shot learners,\u201d arXiv preprint arXiv:2005.14165,\n2020.\n[8]\nM. Chen et al., \u201cEvaluating large language models trained on code,\u201d 2021. arXiv: 2107.03374\n[cs.LG].\n[9]\nA. Chowdhery et al., \u201cPalm: Scaling language modeling with pathways,\u201d arXiv preprint\narXiv:2204.02311, 2022.\n[10]\nH. W. Chung et al., \u201cUnimax: Fairer and more effective language sampling for large-scale\nmultilingual pretraining,\u201d arXiv preprint arXiv:2304.09151, 2023.\n[11]\nT. Computer, Redpajama: An open source recipe to reproduce llama training dataset, 2023.\n[Online]. Available: https://github.com/togethercomputer/RedPajama-Data.\n[12]\nD. Dai et al., \u201cDeepseekmoe: Towards ultimate expert specialization in mixture-of-experts\nlanguage models,\u201d arXiv preprint arXiv:2401.06066, 2024.\n[13]\nA. Dosovitskiy et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at\nscale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[14]\nN. Du et al., \u201cGlam: Efficient scaling of language models with mixture-of-experts,\u201d in Interna-\ntional Conference on Machine Learning, PMLR, 2022, pp. 5547\u20135569.\n[15]\nW. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity,\u201d J. Mach. Learn. Res, vol. 23, pp. 1\u201340, 2021.\n[16]\nH. Fu Yao; Peng and T. Khot, \u201cHow does gpt obtain its ability? tracing emergent abili-\nties of language models to their sources,\u201d Yao Fu\u2019s Notion, Dec. 2022. [Online]. Avail-\nable: https : / / yaofu . notion . site / How - does - GPT - Obtain - its - Ability -\nTracing - Emergent - Abilities - of - Language - Models - to - their - Sources -\nb9a57ac0fcf74f30a1ab9e3e36fa1dc1.\n[17]\nL. Gao et al., A framework for few-shot language model evaluation, version v0.4.0, Dec. 2023.\nDOI: 10.5281/zenodo.10256836. [Online]. Available: https://zenodo.org/records/\n10256836.\n[18]\nX. Geng and H. Liu, Openllama: An open reproduction of llama, May 2023. [Online]. Available:\nhttps://github.com/openlm-research/open_llama.\n[19]\nD. Hendrycks et al., \u201cMeasuring massive multitask language understanding,\u201d arXiv preprint\narXiv:2009.03300, 2020.\n[20]\nJ. Hoffmann et al., \u201cTraining compute-optimal large language models,\u201d arXiv preprint\narXiv:2203.15556, 2022.\n[21]\nA. Q. Jiang et al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024.\n[22]\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, \u201cTriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension,\u201d in Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y.\nKan, Eds., Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601\u2013\n1611. DOI: 10.18653/v1/P17-1147. [Online]. Available: https://aclanthology.org/\nP17-1147.\n[23]\nJ. D. M.-W. C. Kenton and L. K. Toutanova, \u201cBert: Pre-training of deep bidirectional trans-\nformers for language understanding,\u201d in Proceedings of naacL-HLT, vol. 1, 2019, p. 2.\n[24]\nD. Kocetkov et al., \u201cThe stack: 3 tb of permissively licensed source code,\u201d Preprint, 2022.\n[25]\nD. Lepikhin et al., \u201cGshard: Scaling giant models with conditional computation and automatic\nsharding,\u201d arXiv preprint arXiv:2006.16668, 2020.\n[26]\nM. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, \u201cBase layers: Simplifying\ntraining of large, sparse models,\u201d in International Conference on Machine Learning, PMLR,\n2021, pp. 6265\u20136274.\n[27]\nJ. Li, Z. Zhang, and H. Zhao, \u201cSelf-prompting large language models for open-domain qa,\u201d\narXiv preprint arXiv:2212.08635, 2022.\n[28]\nR. Li et al., \u201cStarcoder: May the source be with you!\u201d arXiv preprint arXiv:2305.06161, 2023.\n[29]\nY. Liu et al., \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint\narXiv:1907.11692, 2019.\n[30]\nY. Lou, F. Xue, Z. Zheng, and Y. You, \u201cCross-token modeling with conditional computation,\u201d\narXiv preprint arXiv:2109.02008, 2021.\n17\n[31]\nB. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, \u201cMultimodal contrastive\nlearning with limoe: The language-image mixture of experts,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 9564\u20139576, 2022.\n[32]\nE. Nijkamp et al., \u201cXgen-7b technical report,\u201d arXiv preprint arXiv:2309.03450, 2023.\n[33]\nJ. Puigcerver, C. Riquelme, B. Mustafa, and N. Houlsby, \u201cFrom sparse to soft mixtures of\nexperts,\u201d arXiv preprint arXiv:2308.00951, 2023.\n[34]\nJ. W. Rae et al., \u201cScaling language models: Methods, analysis & insights from training gopher,\u201d\narXiv preprint arXiv:2112.11446, 2021.\n[35]\nC. Raffel et al., \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d\nJournal of Machine Learning Research, vol. 21, no. 140, pp. 1\u201367, 2020. [Online]. Available:\nhttp://jmlr.org/papers/v21/20-074.html.\n[36]\nC. Riquelme et al., \u201cScaling vision with sparse mixture of experts,\u201d Advances in Neural\nInformation Processing Systems, vol. 34, pp. 8583\u20138595, 2021.\n[37]\nS. Roller, S. Sukhbaatar, J. Weston, et al., \u201cHash layers for large sparse models,\u201d Advances in\nNeural Information Processing Systems, vol. 34, pp. 17 555\u201317 566, 2021.\n[38]\nB. Roziere et al., \u201cCode llama: Open foundation models for code,\u201d arXiv preprint\narXiv:2308.12950, 2023.\n[39]\nN. Shazeer, \u201cGlu variants improve transformer,\u201d arXiv preprint arXiv:2002.05202, 2020.\n[40]\nN. Shazeer et al., \u201cOutrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer,\u201d arXiv preprint arXiv:1701.06538, 2017.\n[41]\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, \u201cMegatron-lm:\nTraining multi-billion parameter language models using model parallelism,\u201d arXiv preprint\narXiv:1909.08053, 2019.\n[42]\nL. Soldaini et al., \u201cDolma: An Open Corpus of Three Trillion Tokens for Language Model\nPretraining Research,\u201d arXiv preprint, 2023.\n[43]\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, \u201cRoformer: Enhanced transformer with\nrotary position embedding,\u201d Neurocomputing, vol. 568, p. 127 063, 2024.\n[44]\nY. Tay et al., \u201cUl2: Unifying language learning paradigms,\u201d in The Eleventh International\nConference on Learning Representations, 2022.\n[45]\nY. Tay et al., \u201cUnifying language learning paradigms,\u201d arXiv preprint arXiv:2205.05131, 2022.\n[46]\nL.-M. Team, Llama-moe: Building mixture-of-experts from llama with continual pre-training,\nDec. 2023. [Online]. Available: https://github.com/pjlab-sys4nlp/llama-moe.\n[47]\nH. Touvron et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint\narXiv:2302.13971, 2023.\n[48]\nB. Wang and A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel, https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n[49]\nG. Wenzek et al., \u201cCCNet: Extracting high quality monolingual datasets from web crawl data,\u201d\nEnglish, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N.\nCalzolari et al., Eds., Marseille, France: European Language Resources Association, May 2020,\npp. 4003\u20134012, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.\norg/2020.lrec-1.494.\n[50]\nY. Xu et al., \u201cGspmd: General and scalable parallelization for ml computation graphs,\u201d arXiv\npreprint arXiv:2105.04663, 2021.\n[51]\nF. Xue, X. He, X. Ren, Y. Lou, and Y. You, \u201cOne student knows all experts know: From sparse\nto dense,\u201d arXiv preprint arXiv:2201.10890, 2022.\n[52]\nF. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, \u201cGo wider instead of deeper,\u201d in Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8779\u20138787.\n[53]\nP. Yu et al., \u201cEfficient language modeling with sparse all-mlp,\u201d arXiv preprint\narXiv:2203.06850, 2022.\n[54]\nP. Zhang, G. Zeng, T. Wang, and W. Lu, Tinyllama: An open-source small language model,\n2024. arXiv: 2401.02385 [cs.CL].\n[55]\nY. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, \u201cDeep long-tailed learning: A survey,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2023.\n[56]\nL. Zheng et al., \u201cJudging llm-as-a-judge with mt-bench and chatbot arena,\u201d arXiv preprint\narXiv:2306.05685, 2023.\n18\n[57]\nY. Zhou et al., \u201cBrainformers: Trading simplicity for efficiency,\u201d in International Conference\non Machine Learning, PMLR, 2023, pp. 42 531\u201342 542.\n[58]\nY. Zhou et al., \u201cMixture-of-experts with expert choice routing,\u201d Advances in Neural Informa-\ntion Processing Systems, vol. 35, pp. 7103\u20137114, 2022.\n[59]\nB. Zoph et al., \u201cSt-moe: Designing stable and transferable sparse expert models,\u201d URL\nhttps://arxiv. org/abs/2202.08906, 2022.\n19\nAppendix\nA\nFrequent Asked Questions\nWe list the potentially frequently asked questions and the point-to-point5 answers as follows:\nA.1\nWhy not show the token specialization of the checkpoints at the warmup stage?\nWe did not expect that the routing would be learned and fixed so early. During training, due to limited\nstorage quota, we only keep the checkpoints every 200B tokens.\nA.2\nWhy not compare with advanced open MoE models like Mixtral and DeepSeek-MoE?\nFirst, our model was announced and released over 4 months earlier than Mistral and even more\nthan DeepSeek-MoE. Second, different from models used in-house training data, our model is fully\ntransparent. We also disclose all details and code to ensure everyone can train a comparable OpenMoE\nmodel from scratch.\nA.3\nWhy not use MoE upcycling?\nMoE is more efficient in training instead of inference, because of better parallelism induced by large\nbatch size. Building MoE on top of dense LLMs is a smart and faster way to get an MoE model,\nbut not a more efficient way from a long-term view. Instead, maybe distilling MoE into a dense\nmodel [51] would be helpful if there is little performance drop.\nA.4\nWhy not use AdamW optimizer and Cosine Learning Rate Schedule?\nWe applied Adafactor optimizer and Inverse Square Root learning rate schedule following ST-\nMoE [59]. We tried AdamW Optimizer but found that would introduce unstable issues (i.e., NAN\nloss) frequently, which may introduce a significant amount of hyper-parameter sweep. Considering\nthe limited computational resources we have, we decide to simply follow the well-studied learning\nrate schedule from ST-MoE [59].\nA.5\nWhy not use better and larger datasets?\nWhen launching this project in 2023 May, there were only a few available open-source pre-training\ndatasets. However, the scale and quality of open-sourced pre-training datasets are getting better. For\ninstance, Soldaini et al. [42] released 3T tokens with careful cleaning. Computer [11] also released a\nhuge dataset with 30T tokens in total. We believe training on the future better data will improve the\nLLM performance generally by a large margin.\n20\nB\nHyper-parameters\nTable 11: Model Configurations. H is the hidden size. \u201cLayout\u201d means the way of using the MoE\nlayer. For instance, \u201cEvery 4\u201d means we use one MoE layer for every 4 transformer blocks. HFFN is\nthe FFN intermediate size. NHead and HHead are the number of attention heads and attention head\ndimensions. L is the number of layers. #Param is the total parameters. #ActParam is the number of\nparameters we used to process each token in Transformer blocks. #ActParam w/ E is the sum of the\n#ActParam and the number of parameters in the token embedding layer.\nModel\nLayout\nH\nHFFN\nNHead\nHHead\nL\n#Param\n#ActParam w/ E\n#ActParam\nOpenMoE-Base/16E\nEvery 4\n768\n3072\n12\n64\n12\n650M\n339M\n142M\nOpenMoE-8B/32E\nEvery 6\n2048\n8192\n24\n128\n24\n8.7B\n2.6B\n2.1B\nOpenMoE-34B/32E\nEvery 4\n3072\n12288\n24\n128\n32\n34B\n6.8B\n6.0B\nTinyLLaMA\n-\n2048\n5632\n32\n64\n22\n1.0B\n1.0B\n0.9B\nOpenLLaMA-3B\n-\n3200\n8640\n32\n64\n26\n3.0B\n3.0B\n2.9B\nLLaMA-7B\n-\n4096\n11008\n32\n128\n32\n6.6B\n6.4B\n6.4B\nFor OpenMoE-8B/32E, we set the head dimension as 128 instead of 64, which may be too large\nfor a model using 2B activated Transformer parameters. We suggest that using 64 may induce a\nbetter cost-effectiveness trade-off than ours. For the number of parameters in the table above, since\nmost parameters in Transformer blocks are from attention layer and FFN layer, we only account the\ntrainable parameters from these two for simplicity.\nTable 12: OpenMoE training hyper-parameters.\nBase/16E\n8B/32E\n34B/32E\nOptimizer\nAdafactor\nBatch Size\n128\n2048\n2048\nTraining Steps\n500K\n500K\n100K\nPeak Learning Rate\n0.01\nLearning Rate Schedule\nInverse Square Root Decay\nWarmup Steps\n10K\nSequence Length\n2048\nLoad Balance Loss Weight\n0.01\nZ-Loss Weight\n0.001\nRouter Z-Loss Weight\n0.0001\nDifferent from existing LLMs trained with AdamW, we used Adafactor, a more memory-efficient\noptimizer. Although it performs slightly worse than AdamW with the same training steps, the memory\nefficiency enables us to use less model parallelism and more data parallelism. In this case, using\nAdafactor makes our training cheaper than using AdamW to train the same model on the same\ndata. However, we highlight that the margin of this gap is unclear because it highly depends on the\nhardware and model size. For our infrastructure, i.e., TPUv3, this gap should be relatively larger due\nto the limited on-chip memory (16 GB per core).\n21\nC\nRelated Work\nC.1\nBefore OpenMoE\nMoE is not new. One representative early effort is, Shazeer et al. [40] embed the MoE layer\ninto a recurrent language model. Due to the scalability of Transformer architecture, GShard [25]\nintegrates MoE into Transformer layer and uses expert parallelism to train MoE-based Transformer\nat scale. Switch Transformer [15] is the earliest open-source MoE-based LM to our best knowledge,\nwhich used encoder-decoder architecture and trained with C4 [35] dataset. Due to the success of\nSwitch Transformer on large-scale pre-training, MoE got more attention, and more advanced routing\nalgorithms were invented. For instance, BASE Layers [26] formulates token-to-expert allocation as a\nlinear assignment problem, allowing an optimal assignment in which each expert receives an equal\nnumber of tokens. Roller et al. [37] simply modifies the feedforward layer to hash to different sets of\nweights depending on the current token and achieves promising results compared to learning-based\nrouting. Different Token-based routing above, Zhou et al. [58] propose to let experts select their\nfavorite tokens, i.e., Expert-Choice Routing. Expert-choice Routing achieves more balanced token\nassignment and better cost-effectiveness trade-off.\nBeyond the routing algorithm, there is also some work focusing on scaling MoE efficiently. Artetxe\net al. [3] trained their MoE models mainly on the datasets used in RoBERTa [29] and CC100 [49]\n(112B tokens in total). GaLM [14] further scale decoder-only MoE model with an in-house high-\nquality dataset with 1.6T tokens. Brainformer [57] proposes an evolutionary search to discover MoE\nattributes, e.g., the best way to interleave layers and layer capacities, when to fuse layers, and when\nto specialize layers with MoE modules and show its effectiveness at different scales.\nIn addition to language modeling, Vision Transformer (ViT) [13] can also be enhanced by MoE\narchitecture. ViT-MoE [36] verifies the scalability of MoE on ViT models. WideNet [52] shares\nMoE-based Transformer blocks with individual layer normalization to achieve better parameter\nefficiency. SoftMoE [33] further improves the routing algorithm by applying soft token selection,\nwhich not only keeps the efficiency but also stabilizes the routing gradient. There are also some efforts\ndevoted to include MoE into non-Transformer architecture, e.g., Sparse-MLP [30] for computer\nvision and s-MoE for language modeling [53].\nC.2\nAfter OpenMoE\nTable 13: Open-sourced MoE LLMs timeline. We use the model release date as the key to sort the\nopen-souced MoE LLMs. Dataset Size is the number of tokens in the pre-training dataset, i.e., the\nnumber of tokens for one epoch. LLaMA-MoE is continued pre-trained on off-the-shelf LLaMA\nfamily models. We account its continue training dataset only.\nModel Name\nDataset Size\nReproducible\nRelease Date\nSwitch Transformer [15]\n156B\nYes\nFeb 2021\nMeta-MoE [3]\n112B\nYes\nDec 2021\nOpenMoE (Ours)\n1.1T\nYes\nAug 2023\nMixtral of Experts [21]\nUnknown\nNo\nDec 2023\nLLaMA-MoE [46]\n200B\nYes\nDec 2023\nDeepSeek-MoE [12]\n2T\nNo\nJan 2024\nWe released our model and implementation much earlier than writing this report. As shown in\nTable 13, after our release, there are some partially open-sourced models released, e.g., Mixtral [21]\nand Deepseek-MoE [12]. As we known, these models are significantly better in terms of final results.\nHowever, since these models are trained with in-house data, we have no idea about how things\nhappened. We believe, although our results are not that amazing, the fully open-sourced nature and\nthe in-depth analysis are both meaningful for the community.\n22\nD\nBigBench-Lite Results\nTable 14: Detailed BigBench-Lite results. Note that BIG-G-sparse 8B is an MoE model with 60B\nparameters in total.\nModel\nBIG-G 8B\nBIG-G-sparse 8B\nGPT-3 6B\nOpenMoE-8B\nauto_debugging\n0.0\n0.0\n0.0\n17.65\nbbq_lite_json\n58.63\n46.13\n49.85\n42.67\ncode_line_description\n4.66\n2.44\n20.18\n2.44\nconceptual_combinations\n\u22122.16\n1.07\n\u22123.36\n0.81\nconlang_translation\n31.38\n33.25\n37.92\n36.93\nemoji_movie\n3.75\n7.5\n\u22125.0\n3.75\nformal_fallacies_syllogisms_negation\n0.78\n\u22120.39\n\u22120.8\n\u22120.56\nhindu_knowledge\n12.44\n8.63\n19.29\n16.24\nknown_unknowns\n\u221234.78\n\u22124.35\n\u22128.7\n\u221213.04\nlanguage_identification\n1.39\n\u22120.33\n1.66\n1.77\nlinguistics_puzzles\n0.0\n0.0\n0.0\n0.05\nlogic_grid_puzzle\n\u22122.45\n0.01\n\u22120.28\n0.89\nlogical_deduction\n1.38\n4.2\n1.05\n0.09\nmisconceptions_russian\n\u221234.69\n\u221238.78\n\u221234.69\n\u221238.78\nnovel_concepts\n10.16\n14.06\n17.97\n6.25\noperators\n10.48\n16.67\n20.0\n20.48\nparsinlu_reading_comprehension\n0.0\n0.0\n0.0\n11.97\nplay_dialog_same_or_different\n12.5\n4.69\n\u22123.8\n1.1\nrepeat_copy_logic\n0.0\n6.25\n0.0\n3.12\nstrange_stories\n\u22127.15\n\u22124.77\n9.54\n14.52\nstrategyqa\n7.23\n8.4\n\u22123.8\n3.36\nsymbol_interpretation\n6.06\n0.13\n4.17\n2.65\nvitaminc_fact_verification\n6.25\n1.27\n\u22123.2\n21.34\nwinowhy\n4.69\n5.27\n11.6\n10.14\nAverage\n3.77\n4.63\n5.40\n6.93\n23\nE\nRouting Decision Standard Deviation\n0\n500\n1000\n1500\n2000\nPosition ID\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nStd Value\nRouting Decision Std\nUniform Distribution Std\nAverage Routing Decision Std\nFigure 13: The routing decision standard deviation at different position IDs.\nToken ID\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nValue\nRouting Decision Std\nUniform Distribution Std\nAverage Routing Decision Std\nFigure 14: The routing decision standard deviation at different token IDs. We only take the token IDs\nwith over 128 tokens, because the extremely low-resourced tokens always have large routing decision\nstandard deviation. The token IDs never appeared also have variance at all.\nIn Figure 13 and 14, we can clearly see that the token IDs have a larger standard deviation on routing\ndecisions than position IDs. Also, most token IDs have a relatively large standard deviation, which\nmeans most of the token IDs have Context-independent Routing.\n24\nF\nTop Token Selection by Experts\nTable 15: Top Tokens selected by each expert.\nExpert ID\nTop Tokens\n0\n\u201c\\n\u2019, \u201c \u2018 \u201d, \u201c \u2019 \u201d, \u201cs\u201d, \u201c-\u201d, \u201c$\u201d, \u201cy\u201d, \u201c_\u201d, \u201c \u201d, \u201c2\u201d\n1\n\u201c\\n\u2019, \u201c1\u201d, \u201c \u201d, \u201c2\u201d, \u201c\u2019\\\\\u201d, \u201cS\u201d, \u201c.\u201d, \u201c-\u201d, \u201cC\u201d, \u201c{\u201d\n2\n\u201cin\u201d, \u201c.\u201d, \u201c2\u201d, \u201c1\u201d, \u201c0\u201d, \u201c\\n\u2019, \u201c \u201d, \u201c3\u201d, \u201c_\u201d, \u201c4\u201d\n3\n\u201cs\u201d, \u201c)\u201d, \u201ca\u201d, \u201c\\n\u2019, \u201cwhich\u201d, \u201ces\u201d, \u201c);\u201d, \u201c}\u201d, \u201c\\\\\u201d, \u201ce\u201d\n4\n\u201c\\n\u2019, \u201c.\u201d, \u201c0\u201d, \u201cthe\u201d, \u201c,\u201c, \u201c_\u201d, \u201cthat\u201d, \u201c1\u201d, \u201cas\u201d, \u201c\u02c6\u2019\u2019\n5\n\u201c \u201d, \u201c\\n\u2019, \u201cs\u201d, \u201c2\u201d, \u201ca\u201d, \u201con\u201d, \u201cter\u201d, \u201c*\u201d, \u201c\\\\\u201d, \u201call\u201d\n6\n\u201cthe\u201d, \u201c,\u201d, \u201c.\u201d, \u201ca\u201d, \u201cto\u201d, \u201cof\u201d, \u201c \u201d, \u201cs\u201d, \u201cde\u201d, \u201c\\n\u2019\n7\n\u201c,\u201c, \u201cand\u201d, \u201c\\n\u2019, \u201c:\u201d, \u201c_\u201d, \u201c \u201d, \u201c0\u201d, \u201con\u201d, \u201cat\u201d, \u201c{\u201d\n8\n\u201c(\u201d, \u201c.\u201d, \u201cthat\u201d, \u201cs\u201d, \u201c \u201d, \u201c,\u201c, \u201cC\u201d, \u201cwhich\u201d, \u201cof\u201d, \u201cG\u201d\n9\n\u201c(\u201d, \u201cthis\u201d, \u201c2\u201d, \u201c\\n\u2019, \u201c\\\\\u201d, \u201c \u201d, \u201c3\u201d, \u201calso\u201d, \u201cI\u201d, \u201c1\u201d, \u201c,\u201c\n10\n\u201c\\n\u2019, \u201c.\u201d, \u201cand\u201d, \u201c\\r\u201d, \u201c).\u201d, \u201c;\u201d, \u201c\\t\u201d, \u201c:\u201d, \u201c?\u201d, \u201cThe\u201d\n11\n\u201cto\u201d, \u201c1\u201d, \u201cthe\u201d, \u201c2\u201d, \u201c0\u201d, \u201cs\u201d, \u201cfor\u201d, \u201ct\u201d, \u201c3\u201d, \u201c\\n\u2019\n12\n\u201cthe\u201d, \u201c,\u201c, \u201c$\u2019, \u201cto\u201d, \u201cin\u201d, \u201c?\u201d, \u201cas\u201d, \u201cthat\u201d, \u201cIn\u201d, \u201cwho\u201d\n13\n\u201cin\u201d, \u201c/\u201d, \u201c0\u201d, \u201c\\n\u2019, \u201cwith\u201d, \u201c-\u201d, \u201c \u201d, \u201c{\u201d, \u201cof\u201d, \u201c2\u201d\n14\n\u201cis\u201d, \u201c.\u201d, \u201care\u201d, \u201cbe\u201d, \u201cwas\u201d, \u201cs\u201d, \u201c\\n\u2019, \u201c,\u201c, \u201chas\u201d, \u201cnot\u201d\n15\n\u201cof\u201d, \u201c\\n\u2019, \u201c_\u201d, \u201cs\u201d, \u201c \u201d, \u201c.\u201d, \u201cS\u201d, \u201cthe\u201d, \u201cfor\u201d, \u201c\\\\\u201d\n16\n\u201ccite\u201d, \u201c,\u201c, \u201c\\n\u2019, \u201c.\u201d, \u201c{\u201d, \u201cs\u201d, \"\u2019\", \u201cing\u201d, \u201cdata\u201d, \u201c\\\\$\u201d, \u201c\\t\u201d\n17\n\u201cthe\u201d, \u201c.\u201d, \u201c\\n\u2019, \u201cThe\u201d, \u201c0\u201d, \u201c1\u201d, \u201cas\u201d, \u201cof\u201d, \u201c5\u201d, \u201c2\u201d\n18\n\u201c-\u201d, \u201c{\u201d, \u201cfor\u201d, \u201c(\u201d, \u201c_\u201d, \u201c \u201d, \u201c$\u201d, \u201c(\u201d, \u201c\\n\u2019, \u201c}\u201d\n19\n\u201c \u201d, \u201cand\u201d, \u201cin\u201d, \u201cto\u201d, \u201c,\u201c, \u201cof\u201d, \u201cor\u201d, \u201c\\n\u2019, \u201cby\u201d, \u201c$\u201d\n20\n\u201c\\n\u2019, \u201cthe\u201d, \u201c$\u201d, \u201ca\u201d, \u201c0\u201d, \u201c}\u201d, \u201cthis\u201d, \u201c1\u201d, \u201cs\u201d, \u201c9\u201d, \u201c \u201d\n21\n\u201c,\u201c, \u201cand\u201d, \u201c \u201d, \u201c.\u201d, \u201c\\n\u2019, \u201c=\u201d, \u201c\\t\u201d, \u201cthe\u201d, \u201c \u201d, \u201cn\u201d\n22\n\u201cthe\u201d, \u201c\\n\u2019, \u201c)\u201d, \u201c,\u201c, \u201chis\u201d, \u201ctheir\u201d, \u201cs\u201d, \u2019\"\u2019, \u201c,\u201c, \u201ci\u201d\n23\n\u201c.\u201d, \u201c\\n\u2019, \u201c,\u201c, \u201c*\u201d, \u201c<pad>\u201d, \u201cCha\u201d, \u201ci\u201d, \u201c!\u201d, \u201cour\u201d, \u201c/\u201d\n24\n\u201ca\u201d, \u201cwith\u201d, \u2019}\u201d, \u201cin\u201d, \u201c)\u201d\u2019, \u201c:\u201d, \u201can\u201d, \u201c1\u201d, \u201c\\n\u2019, \u201cat\u201d\n25\n\u201c\\\\\u201d, \u201cthe\u201d, \u201c.\u201d, \u201cof\u201d, \u201cer\u201d, \u201c, \u201d, \u201cs\u201d, \u201cter\u201d, \u201cbook\u201d, \u201cmodel\u201d\n26\n\u201c\\n\u2019, \u201c, \u201d, \u201c.\u201d, \u201ca\u201d, \u201c<pad>\u201d, \u201cs\u201d, \u201cde\u201d, \u201cal\u201d, \u201c-\u201d\n27\n\u201cthe\u201d, \"\u2019\", \u201cI\u201d, \u201cThe\u201d, \u201c, \u201d, \u201cit\u201d, \u201cwe\u201d, \u201che\u201d, \u201ca\u201d, \u201cx\u201d\n28\n\u201c, \u201d, \u201cly\u201d, \u201c{\u201d, \u201c_{\u201d, \u201cnew\u201d, \u201c-\u201d, \u201ced\u201d, \u201cmore\u201d, \u201c\\n\u2019, \u201cd\u201d\n29\n\u201c, \u201d, \u201c.\u201d, \u201cof\u201d, \u201c;\u201d, \u201cby\u201d, \u201c,:\u201d, \u201c\\n\u2019, \u201cto\u201d, \u201cfrom\u201d, \u201c(\u201d,\n30\n\u201c}\u201d, \u201ced\u201d, \u201cd\u201d, \u201chave\u201d, \u201cing\u201d, \u201c, \u201d, \u201chas\u201d, \u201cs\u201d, \u2019\"\u2019, \u201chad\u201d\n31\n\u201cto\u201d, \u201ccan\u201d, \u201cs\u201d, \u201cof\u201d, \u201cing\u201d, \u201cwill\u201d, \u201cnot\u201d, \u201ce\u201d, \u201ced\u201d, \u201cwould\u201d\n25\n"
  },
  {
    "title": "BlackMamba: Mixture of Experts for State-Space Models",
    "link": "https://arxiv.org/pdf/2402.01771.pdf",
    "upvote": "22",
    "text": "BlackMamba: Mixture of Experts for State-Space\nModels\nQuentin Anthony\u2217\nYury Tokpanov\u2217\nPaolo Glorioso\u2217\nBeren Millidge\u2217\n{quentin, yury, paolo, beren}@zyphra.com\nZyphra\nPalo Alto, CA\nAbstract\u2014State-space models (SSMs) have recently demon-\nstrated competitive performance to transformers at large-scale\nlanguage modeling benchmarks while achieving linear time and\nmemory complexity as a function of sequence length. Mamba,\na recently released SSM model, shows impressive performance\nin both language modeling and long sequence processing tasks.\nSimultaneously, mixture-of-expert (MoE) models have shown\nremarkable performance while significantly reducing the com-\npute and latency costs of inference at the expense of a larger\nmemory footprint. In this paper, we present BlackMamba, a novel\narchitecture that combines the Mamba SSM with MoE to obtain\nthe benefits of both. We demonstrate that BlackMamba performs\ncompetitively against both Mamba and transformer baselines,\nand outperforms in inference and training FLOPs. We fully train\nand open-source 340M/1.5B and 630M/2.8B BlackMamba models\non 300B tokens of a custom dataset. We show that BlackMamba\ninherits and combines both of the benefits of SSM and MoE\narchitectures, combining linear-complexity generation from SSM\nwith cheap and fast inference from MoE. We release all weights,\ncheckpoints, and inference code open-source. 1\nI. INTRODUCTION\nThe advent of Large Language Models (LLMs) built from\ndecoder-only transformer models [1], [2] have revolutionized\nNatural Language Processing (NLP) [3], [4], [5], along with\ndiverse deep learning application domains such as image\nprocessing [6], time-series [7], and reinforcement learning [8].\nDespite the strong performance and scalability of the trans-\nformer architecture, however, there remain significant short-\ncomings. While maximally expressive, the attention mech-\nanism is computationally demanding both during training\nand inference, naively requiring both quadratic FLOPs and\nmemory in the sequence length. This limits the context length\nof transformer models, makes autoregressive generation in-\ncreasingly expensive with scale, and generally inhibits truly\nunlimited sequence processing and learning from continual\ndatastreams.\nIn order to ameliorate these problems, significant effort\nhas recently been directed towards architectural alternatives\nto the canonical dense attention transformer model. Some of\nthe most promising candidate architectures are State Space\nModels (SSMs) [9], [10] and Mixture of Experts (MoE) [11],\n[12], [13]. The key practical benefit of SSMs over transformers\n*All authors contributed equally to this work\n1Inference code at: https://github.com/Zyphra/BlackMamba\nis their linear computational complexity with respect to input\nsequence length (as opposed to the quadratic complexity of\ntransformers). This theoretically enables SSMs to process\nvastly longer sequences than transformers for a given FLOP\nbudget, and to render autoregressive generation constant in\ncompute without a KV cache. Notable recent examples of\nSSMs include Mamba [9], RWKV [10], and RetNet [14],\nall of which demonstrate efficient long-sequence training and\ninference, efficient implementations in CUDA, and competi-\ntive language modeling task performance to transformers with\nsimilar scaling properties. At the same time mixture of expert\n(MoE) architectures [15], [16], [11], [12] have become an\nemerging advance over dense transformers which allow for\nsignificantly reduced training and inference FLOPs required\nto achieve comparable quality to a comparable dense model.\nMoE models allow for only a sparse subset of the total\nparameters to be activated on a single forward pass, relying\non a routing function to gate which \u2019experts\u2019 are utilized\nor not depending on the context. This sparsity decouples\nthe inference cost and parameter count of a model, enabling\nsignificantly stronger performance for a given inference budget\nat the cost of many more parameters and a correspondingly\ngreater memory footprint.\nThese architectural improvements over transformers are\ncompelling on their own, but we believe that their combination\nis a natural next step that could enable significantly improved\nlanguage modelling speed and performance against the\ncanonical transformer. Specifically, we expect a Mamba-MoE\narchitecture would have the following improvements over a\ndense transformer:\n\u2022 Mamba: Linear computational complexity with respect\nto input sequence length for both training and inference.\nAutoregressive generation in constant time and memory.\n\u2022 MoE: Inference latency and training FLOPs of the\nequivalent smaller dense base model, while preserving\nmodel quality close to an equi-parameter dense model.\nIn this paper, we begin to demonstrate that these improve-\nments are achievable and that, when put together, these two\napproaches synergize to produce a model with compelling\narXiv:2402.01771v1  [cs.CL]  1 Feb 2024\n(a) Transformer\n(b) Mamba\n(c) Transformer-MoE\n(d) Mamba-MoE\nFig. 1. Architecture of dense transformer, dense Mamba, transformer-MoE, and Mamba-MoE\nevaluation performance (Figs. 8-14), compute (Fig. 4), and\nlatency advantages (Figs. 5 and 3) over existing transformer\nmodels and which can be trained at a fraction of the FLOP cost\nfor similar performance (Fig. 4). We study the MoE routing\nstatistics exhibited by our model across training time and\nacross model depth. Additionally, we introduce a novel initial-\nization for our routing Sinkhorn algorithm which significantly\nreduces the number of iterations required until convergence,\nthus improving routing speed.\nII. CONTRIBUTIONS\nThe main achievements of this work are:\n\u2022 We design, implement, and evaluate BlackMamba: a\ncombination of alternating attention-free Mamba blocks\nand routed MLPs.\n\u2022 We train and open-source two BlackMamba Models:\n340M/1.5B BlackMamba and 630M/2.8B BlackMamba2.\n\u2022 We demonstrate that BlackMamba requires significantly\nfewer training FLOPs to achieve comparable downstream\ntask performance to a dense transformer model.\n\u2022 We explore the compounding inference benefits of\nthe combination of attention-free architectures such as\nMamba along with routed sparsity architectures such as\nMoE.\nThe rest of this paper is organized as follows. We first\nprovide an overview of related works on SSM, MoE, and SSM\nwith MoE in Section IV. We then provide background into the\nunderlying concepts behind SSMs and MoE that are necessary\nto understand our contributions in Section III. Our architecture\nis described in Section V, and its training/inference dynamics\nare explored in Section VI. Finally, we describe the implica-\ntions and limitations of our approach in Section VII along with\nour conclusions from this work in Section VIII.\n2In this paper, we denote an MoE model with X forward-pass parameters\nand Y total parameters as X/Y .\nThe final checkpoints are open-sourced on HuggingFace\nwith Apache 2.0 licensing, and intermediate training check-\npoints are available upon request. Inference code is provided\nat https://github.com/Zyphra/BlackMamba.\nIII. BACKGROUND\nA. Transformers\nThe transformer architecture [2] has demonstrated excep-\ntionally strong and consistent performance at language mod-\nelling, as well as almost all other sequence processing tasks,\nremaining state-of-the-art and essentially unchanged since its\nintroduction. The core operation of the transformer is self-\nattention, which performs a quadratic all-to-all comparison\nof the dot-product similarities between the embeddings of\ndifferent tokens in a sequence before normalizing it and\nperforming a linear map to an output vector. Mathematically,\nself-attention can be written as,\nz = WV x\u03c3( 1\n\u221a\nd\nxWQW T\nKx \u25e6 M)\n(1)\nWhere \u03c3 denotes the softmax function, M denotes a binary\nmask which enforces specific constraints, such as causal mask-\ning, on the computation, the superscript T denotes transposi-\ntion, and \u25e6 denotes element-wise multiplication. The quadratic\ncost in sequence length is caused by the xWQW T\nKx term\nwhich computes a L \u00d7 L matrix of similarity scores between\nthe embeddings of different tokens where L is the sequence\nlength.\nThe transformer model consists of a stack of self-attention\nblocks interleaved with multi-layer-perceptron (MLP) blocks\nwhich consist of a two-layer MLP with a given activation\nfunction. A layer of a transformer model can thus be written\nas,\nxl+1 = xl + MLP(LN(xl + attention(LN(xl))))\n(2)\nWhere LN represents the layernorm operation which is used\nto normalize the inputs to the attention and MLP blocks.\n2\nB. Mamba\nState-space models (SSMs) are a class of sequence models\nthat possess linear complexity with respect to the sequence\nlength. SSMs are more closely related to RNN and CNN\narchitectures than the attention mechanism, and draw in-\nspiration from a continuous dynamical system (depicted in\nEquation 3) mapping a 1-dimensional function or sequence\nx(t) \u2208 R 7\u2192 y(t) \u2208 R through an implicit latent state\nh(t) \u2208 RN:\nh\u2032(t) = Ah(t) + Bx(t),\ny(t) = Ch(t)\n(3)\nWhere the \u2018time\u2019 t now represents the sequence position of a\ntoken. A linear dynamical system like this can be efficiently\ncomputed in parallel via a convolution or associative scan,\nwhile the recurrent form presented above can be utilized for\nrapid generation at inference time. The fundamental innovation\nof the Mamba architecture is to make the A, B, and C\nmatrices of the SSM linearly input-dependent. That is, the\nnew dynamics can be written as,\nh\u2032(t) = A(x(t))h(t) + B(x(t))x(t),\ny(t) = C(x(t))h(t)\n(4)\nIntuitively, this enables the updates to the SSM\u2019s recurrent\nstate to selectively depend upon the tokens being processed,\nwith the SSM being able to decide to store or remove specific\ninformation from its recurrent state dynamically. This renders\nthe A,B,C matrices loosely analogous to the Q,K,V matrices\nin attention and significantly increases the expressivity of the\nSSM block and could potentially enable context to persist\nmuch longer in the hidden state than otherwise, since it must\nexponentially decay in a linear dynamical system with fixed\nweights. Empirically, [17] found that this closed much of the\ngap with transformers.\nIn practical terms, the recurrent nature of SSMs has long\nprevented their adoption on the reigning highly-parallel AI\nhardware like GPUs. However, recent implementations of\nrecurrent and state-space models such as Mamba [9] and\nRWKV [10] have mapped these operations efficiently to GPU\nhardware via parallel scan kernels, thus enabling training of\nsuch novel architectures with efficiencies approaching that of\nwell-optimized transformer models.\nFor more details on Mamba, please see Appendix C which\ndescribes in details the internal computations of a Mamba\nblock as well as [9] and its associated codebase.\nC. Mixture of Experts\nMixture of Expert (MoE) models allow for the inference\ncost and number of parameters of a model to be decoupled\nby not activating all parameters on the forward pass and\ninstead routing tokens to specific MLP experts. Each expert\ntheoretically specializes in a certain kind of input, and the\nrouter (a small neural network) learns which expert to route\neach token to. Theoretically, this enables the model to maintain\nalmost all the expressivity of the parameter-equivalent dense\nmodel at significantly fewer FLOPs.\nIn standard implementations [11], which we follow in this\npaper, the router is a linear layer mapping from tokens to ex-\npert indices, and each expert is simply a standard transformer\nMLP. The expert that the token is routed to is chosen as the\ntop-k of the expert probabilities, where k is a hyperparameter\nof the architecture. Given an input token to the MoE layer x,\nthis is mapped through the router to a probability distribution\npi(x), where i labels the experts. Upon selecting the top-k\nprobabilities, the output of the MoE layer y can be expressed,\nschematically, as,\ny =\nX\ni\u2208top-k\nciEi(x)\n(5)\nwhere E1, E2, . . . denote the MLP experts,\nEi(x) = Woutf(Win(LN(x))\n(6)\nwhere f is the activation function of the MLP, and ci are\ncoefficients that are often identified with pi, the probability\noutput by the router of choosing a specific expert. The optimal\nmethod for training the router is still uncertain since the\n\u201ccorrect\u201d expert assignment problem is non-differentiable, and\nMoE models often struggle with training stability and load-\nbalancing between different experts for hardware efficiency.\nNevertheless, MoE models have demonstrated the ability to\nachieve superior performance for a given compute budget over\ndense transformer models. Lastly, due to complexity of report-\ning MoE models, where different papers have reported either\nthe forward pass size of the MoE, the total parameters, or\nboth, we here present a consistent convention of denoting MoE\nmodels as: (forward parameters)/(total parameters). For more\ndetails on the MoE architecture and its typical implementation,\nsee [16].\nIV. RELATED WORK\nA. State-space Models\nThe quadratic complexity of transformers in the sequence\nlength has long been recognized as a primary bottleneck to\nextremely long context reasoning and understanding. While\nrecent work has pioneered the concept of context-length\nextension [18], [19] allowing transformers to be trained at\na manageable scale and then inferenced successfully at a\nsignificantly longer context, the inference cost in terms of both\nFLOPs and the memory required for the KV cache remains\nsubstantial.\nEarly state-space models were inspired by linear dynamical\nsystems which can be efficiently computed as a convolution\n[17], [20] for sequence processing and as a recurrence for ef-\nficient autoregressive generation. However, such models were\nnoticeably less expressive and performant than transformers.\nA number of recent works [14], [21] has aimed to increase\nthe expressivity of the state-space model by using input-\ndependent gating, similar to the QKV matrices of attention,\nwhile maintaining the fundamentally linear nature of the state-\nspace recursion. This thus enables efficient implementation via\nconvolution or selective-scan to be maintained while substan-\ntially closing the gap to transformer performance in practice.\n3\nMamba [9] is a recently released state-space model in line with\nthese previous works which demonstrates strong performance\ncomparable to transformers up to the 2.8B scale, as well as\npromising scaling laws. Mamba uses input-dependent gating\nof the inputs to the SSM recursion while maintaining efficient\ncomputation via customized selective scan kernels.\nB. Mixture of Experts\nMoE models have been demonstrated to achieve signifi-\ncantly higher performance in both training and inference per\nFLOP than the equivalent dense models [11], [12]. Moreover,\nscaling laws for MoE models have been put forward [22]\nwhich show that MoE performance improves smoothly with\ncompute, data, and the number of experts being routed to.\nThis latter is especially important since it provides a route\nto continually increasing the capability of the model while\nholding the inference cost fixed.\nWhile MoE models hold significant promise, the architec-\nture still retains many drawbacks. Increasing the number of\nexperts increases the parameter count and hence memory cost\nsubstantially, while many works report MoE models being\nless stable and more challenging to train. Moreover, effective\nmethods for training the router are still open, since the decision\nto route to an expert or not is discrete and cannot be easily\nbackpropagated through. The large memory cost of MoEs\nrelative to their dense counterparts is especially important\nfor users running on relatively low-end GPUs or when the\nmemory size extends beyond that provided by a single GPU\nnecessitating model-parallelism for inference.\nRecently, [13] released a powerful open source mixture of\nexperts model which performs competitively with Llama 2\n70B [5] and close to GPT-3.5 in evaluations while requiring\nonly the forward pass FLOP cost of the original Mistral 7B\nmodel [23], thus demonstrating and solidifying the promise\nof MoE models at scale. The Mixtral architecture also differs\nin a few ways from earlier MoE work, especially in its use\nof relatively few experts, a design which we also utilize and\nhave independently found promising for balancing the FLOP\nand memory cost of MoE models successfully.\nC. State-space models with Mixture of Experts\nWhile both state-space models and Mixture of Experts have\nbeen proposed as promising architectures able to improve the\ncomputational cost of inferencing language models, no works\nhave ever tested their combination at scale.\nConcurrently with this work, [24] demonstrate the per-\nformance of extremely small mamba-MoE models in the\nhundred-million scale of total parameters and the forward\npass FLOPs of a 25M model, trained on <10B tokens. In\ncontrast, we demonstrate empirically the scaling potential and\nperformance of such models at meaningful scales in terms of\nboth parameters and data, by training multi-billion parameter\nmodels on 300B tokens. Our work thus demonstrates the\nstrong scaling potential of the combination of state-space\nmodels and MoE models while resulting in competitive and\nusable language models which are extremely efficient for\ninference.\nV. DESIGN\nA. Architecture\nA standard transformer model [2] consists of interleaved\nattention and MLP blocks added in sequence along a residual\nstream. The equation for a single transformer layer is written\nin Equation 2.\nMost MoE architectures simply replace the MLP blocks\nwith a routed expert layer. Our BlackMamba architecture\nsimply replaces both the MLP layer in a transformer with an\nexpert layer, and the attention layer with a mamba SSM layer\n(see Figure 1). A single block of our architecture can thus be\nwritten as,\nxl+1 =xl + MoE(LN(xl + mamba(LN(xl))))\n(7)\nWe trained BlackMamba 340M/1.5B and 630M/2.8B mod-\nels for 300B tokens on our custom dataset. We used the\nSwiGLU activation function [25] for the expert MLPs. We\ntrained with 8 experts, a number that we found balanced\nwell the trade-off between the inference cost and memory\nfootprint of the model. We tested whether sequential or parallel\n[26] blocks performed better and found a slight advantage\nfor sequential. Following [5], we trained without biases. For\nthe expert router, we used top-1 routing with a Sinkhorn\nrouting function to load-balance between experts. We utilized\na novel custom version of the Sinkhorn algorithm which\nconverges substantially faster than vanilla Sinkhorn (Appendix\nF). We trained using the Megatron-LM [27] distributed training\nframework. The model was trained in bf16 precision. All\nfurther model architectures and training hyperparameters are\ndescribed in Appendix A and B, respectively.\nB. Dataset\nFig. 2. Ratio of data categories in the pretraining dataset of BlackMamba\nTo train BlackMamba, we constructed a custom dataset\ncomprised of a mixture of existing open-source datasets. The\nsubsets included: The Pile [28], SlimPajama [29], Starcoder\n[30], PeS2o [31], and ProofPile [32]. The weights for each\ndataset is provided in Table I. Tokens were sampled without\nreplacement from each of the subsets according to the proba-\nbility of sampling from a subset upweighted by these weights.\n4\nFig. 3. Comparison of BlackMamba average evaluation performance across activated forward parameters.\nDataset\nTokens\nWeight\nPile [28]\n300B\n2\nSlimPajama [29]\n600B\n1.2\nStarcoder [30]\n250B\n0.75\nPeS2o [31]\n50B\n5\nProofpile [32]\n40B\n2\nPG19 [33]\n2.2B\n5\nTABLE I\nDATASET SUBSETS AND THEIR RESPECTIVE WEIGHTS IN OUR TRAINING\nMIXTURE\nFig. 4. Comparison of BlackMamba average evaluation performance across\ntraining FLOPs.\nThe total dataset comprised 1.8 trillion tokens and thus we\ntrained for significantly less than a single epoch. Preliminary\nexperiments3 show that long-form text and academic work\nappears to improve natural language modeling when included\nin the pretraining phase, so we weigh it heavily in the training\nrecipe. Further, we find that including significant portions of\ncode and math during the pretraining phase meaningfully im-\nproves the model\u2019s reasoning ability. We note that this dataset\nis comparatively heavy on unfiltered web data and contains\nmany duplicates due to the upweighting of smaller subsets,\nwhich may limit the quality of the model and leaves significant\nroom for improvement, as well as potentially causing undue\nmemorization of specific common fragments.\nVI. RESULTS\nTo ensure a fair comparison vs Mamba, we trained our\nown 340M Mamba model with the same dataset and train-\ning hyperparameters reported for BlackMamba. This Mamba\n340M model used a hidden size of 1152 and 34 mamba lay-\ners. Notably, BlackMamba performs significantly better than\nequivalent pretrained models (both transformer and Mamba)\nfor the same forward pass model size at inference time, as\nwell as training FLOPs. In Figure 5, we plot the time taken\nto autoregressively generate a sequence of a given length\nstarting from an initial one-token prompt as a function of\nsequence length. We observe that the established latency\nbenefits of both Mamba and MoE models are combined in\nBlackMamaba to result in inference times significantly faster\nthan canonical transformer models, MoE transformer models,\nand pure Mamba models. Moreover, the inference advantage of\n3We believe that such experiments are not yet rigorous enough for\npublication, and will be included in future work.\n5\nFig. 5. Generation latency of BlackMamba compared to dense transformers,\ndense mamba, and transformer-MoE\nBlackMamba increases with greater sequence lengths, making\nBlackMamba extremely competitive at long sequence genera-\ntion. Moreover, although not reflected in this Figure, it must\nbe recognized that while the transformer inference latency\nalso increases linearly, this is due to KV caching which has\nadditional linearly increasing memory requirements and would\neventually OOM on large enough sequences. By contrast,\nMamba models (and BlackMamba) can generate sequences\nof arbitrary length with a constant memory footprint.\nFigures 6 and 7 illustrate the token counts assigned to\neach expert in each layer of the BlackMamba 340M/1.5B\nand the BlackMamba 630M/2.8B models respectively. Most\nlayers display a high degree of expert balance, as expected\nby our improved Sinkhorn algorithm. Yet, intriguingly, both\nmodels show a clear transition towards expert imbalance in the\nfinal layers (at layer 20 for the 340M/1.5B model and layer\n25 for the 630M/2.8B model). This may reflect increasing\nspecialization in later layers or else reflect numerical insta-\nbilities that develop deeper in the network. While the true\ncause of this imbalance remains unknown, we also note that a\nsimilar pattern of imbalance but convergence to a stable expert\nassignment has also been observed in previous MoE models\n[34].\nIn Table I, we report evaluation scores of BlackMamba\nagainst a suite of open-source pretrained language model\nbaselines. We re-evaluated all models on the same version of\nlm-eval (v0.3.0) that we evaluated our own model on*.\nIn Appendix E, we provide evaluation scores for our model\nduring training from checkpoints taken every 10k steps. We\ngenerally found relatively smooth but noisy improvements in\nthe evaluation scores during training. To prevent overfitting\nto the evaluations, we only looked at the evaluation scores\nafter the models had finished training and did not use them\nfor model selection.\nAdditionally, in Appendix F, we describe a novel initial-\nization for the classical Sinkhorn algorithm used for MoE\nrouting which significantly improves convergence speed of\nthe approach, often requiring only a single iteration for con-\nvergence. This provides notable speed improvements for the\nrouted expert layers and results in a similar latency to a\nrouter with a regularized balancing loss, providing superior\nbalancing performance while requiring much less complexity\nof implementation.\nFinally, in Appendix C, we provide a detailed mathematical\ndescription of the internal computations of a Mamba Block\nand in Appendix D, we provide detailed and explicit formulas\nfor computing the parameters and training FLOPs for Mamba\nand MoE models which we hope aid the community in further\ndeveloping and exploring novel SSM and MoE architectures.\n*We use the non-normalized HellaSwag evaluation results in this paper,\nwhich differs from those in [9]\nForward Pass Parameters\nTotal Parameters\nTraining FLOPs\nHellaSwag\nPIQA\nWinoGrande\nLambada\nARC-e\nARC-c\nOpenBookQA\nDownstream Average\nCerebras-GPT\n111M\n111M\n2.6e18\n0.268*\n0.594\n0.488\n0.194\n0.38\n0.166\n0.118\n0.315\nOPT\n125M\n125M\n4.1e20\n0.313*\n0.63\n0.503\n0.379\n0.435\n0.189\n0.166\n0.371\nPythia\n160M\n160M\n4.1e20\n0.293*\n0.627\n0.519\n0.389\n0.452\n0.181\n0.16\n0.375\nCerebras-GPT\n256M\n256M\n1.3e19\n0.286*\n0.613\n0.511\n0.293\n0.41\n0.17\n0.158\n0.347\nBlackMamba\n342M\n1.5B\n6.4e20\n0.365*\n0.690\n0.526\n0.493\n0.561\n0.241\n0.196\n0.439\nOPT\n350M\n350M\n1.1e21\n0.366*\n0.644\n0.523\n0.452\n0.44\n0.207\n0.176\n0.395\nMamba\n343M\n343M\n8.0e20\n0.335*\n0.665\n0.516\n0.453\n0.540\n0.212\n0.198\n0.417\nPythia\n410M\n410M\n1.1e21\n0.333*\n0.668\n0.53\n0.505\n0.504\n0.213\n0.178\n0.419\nBlackMamba\n631M\n2.8B\n1.2e21\n0.397*\n0.712\n0.521\n0.542\n0.603\n0.245\n0.242\n0.466\nPythia\n1B\n1B\n2.2e21\n0.376*\n0.705\n0.545\n0.566\n0.559\n0.243\n0.196\n0.456\nOPT\n1.3B\n1.3B\n3.2e21\n0.4537*\n0.717\n0.595\n0.579\n0.57\n0.234\n0.234\n0.478\nCerebras-GPT\n1.3B\n1.3B\n2.8e20\n0.384*\n0.664\n0.521\n0.462\n0.508\n0.224\n0.166\n0.410\nPythia\n1.4B\n1.4B\n3.2e21\n0.398*\n0.711\n0.565\n0.604\n0.576\n0.256\n0.204\n0.474\nOPT\n2.8B\n2.8B\n6.1e21\n0.606*\n0.738\n0.61\n0.637\n0.609\n0.268\n0.25\n0.510\nCerebras-GPT\n2.8B\n2.8B\n1.1e21\n0.488*\n0.701\n0.559\n0.567\n0.571\n0.246\n0.206\n0.462\nPythia\n2.8B\n2.8B\n6.1e21\n0.451*\n0.737\n0.612\n0.654\n0.629\n0.288\n0.22\n0.513\nTABLE II\nEVALUATION PERFORMANCE OF BLACKMAMBA COMPARED TO SIMILAR MODELS\n6\nFig. 6. Token distribution across experts in 340M/1.5B BlackMamba\nVII. DISCUSSION\nThis work is a preliminary exploration and validation of\nthe core concept of combining together recent advances in\nSSMs with MoEs to produce a highly competitive and efficient\narchitecture both in terms of inference and generation time\nand training FLOPs. While initial results are promising, much\nwork needs to be done to improve both the SSM and MoE\ncomponents as well as investigation of the optimal way to\napproach their combination. We ultimately believe that by\nexploring promising emerging architectures architectures and\nnovel ways of merging and combining them, significant ad-\nvances in performance, efficiency, and speed can be obtained\nover standard transformer recipes.\nWe believe that our work can be extended in many fruitful\ndirections. The evaluations presented in this paper are limited\nin scope. While we provide general coverage of standard\npure language modelling evaluations in the zero-shot setting,\nthe performance of the model in the many-shot in-context-\nFig. 7. Token distribution across experts in 630M/2.8B BlackMamba\nlearning setting remains unexplored. Additionally, there are\nmany facets of behaviour of our models which we have\nnot explicitly investigated. We have not tested for factual\naccuracy, profanity, toxicity, or any other socially undesirable\ntext generation. Similarly, our training dataset blend has not\nbeen explicitly scraped for socially undesirable tokens, nor\nits potential overlap with any evaluation tasks4. Although\nour dataset remains imperfect, we have released all major\ndetails as to its construction and composition with the goal\n4In particular, we are aware of the possibility of evaluation dataset\ncontamination present in the widely used RedPajama dataset [35], and will\nattempt to explicitly deduplicate this dataset if used in future work.\n7\nof aiding community understanding of the effects of dataset\non pretraining performance and model behaviours.\nIn terms of scaling laws, while our models are highly\ncompetitive for a given inference cost and FLOP training\nbudget, it is impossible to make conclusive scaling extrap-\nolations both in terms of data and parameter counts with only\ntwo models trained on 300 billion tokens. Additionally, many\nof our training hyperparameters may be suboptimal as we\nperformed only basic hyperparameter tuning of the learning\nrate. Additionally, while we performed some ablations on the\ncore architecture, it is possible that a superior method of\ncombining state-space models and mixture of experts would\nprovide significant benefits. Additionally, the efficacy and per-\nformance of well-established finetuning and RLHF pipelines\nfor instruction following and general alignment, as well as\nstandard techniques for parameter-efficient-finetuning of SSM\nand MoE models remains almost completely unexplored, as\ndoes how such models perform under quantization.\nOur work also raises interesting questions as to the mod-\nularity of different neural network components that can be\nplaced together into a final model architecture. We show\nthat it is relatively straightforward to combine SSM blocks\nwith MoE blocks from transformers at scale with competitive\nperformance. However, whether Mamba and other SSMs show\nthe same degree of improvement in performance with MoE as\ntransformers remains uncertain, as well as whether combining\nthese architectural pieces has the same effect on the internal\nrepresentations and behaviours of the model. Additionally, it is\nunclear the extent to which routing serves the same function in\nBlackMamba as in more classical transformer MoE models.\nVIII. CONCLUSION\nIn this paper, we have proposed, implemented and trained\nBlackMamba, a model that combines both recent advances in\nstate-space models and mixture-of-experts into a single unified\narchitecture. We demonstrate that our BlackMamba architec-\nture performs highly competitively to strong pretrained LLM\nbaselines in terms of inference cost and training flops, and\nmoreover that it inherits the reduced training and generation\nFLOPs of both SSMs and MoEs simultaneously. Moreover,\nwe show that BlackMamba is capable of rapid generation\nwith both linear time and memory cost. We release Black-\nMamba 340M/1.5 and 630M/2.8 billion parameter models and\nintermediate checkpoints, as well as inference code, under a\npermissive Apache 2.0 license with the goal of enabling and\nfostering further study, experimentation, and understanding\nof the potential of this novel architecture by the broader\ncommunity.\nACKNOWLEDGEMENT\nThe Zyphra team would like to thank Adam Ibrahim for\nhelpful discussions and comments on training stability and\nhyperparameters, and Albert Gu for general discussions on\nstate space models.\n8\nREFERENCES\n[1] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by\njointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473,\n2014.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in\nneural information processing systems, vol. 30, 2017.\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n\u201cLanguage models are unsupervised multitask learners,\u201d OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage mod-\nels are few-shot learners,\u201d Advances in neural information processing\nsystems, vol. 33, pp. 1877\u20131901, 2020.\n[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama\n2: Open foundation and fine-tuned chat models,\u201d arXiv preprint\narXiv:2307.09288, 2023.\n[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n\u201cAn image is worth 16x16 words: Transformers for image recognition\nat scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[7] K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos,\nR. Bhagwatkar, M. Bilo\u02c7s, H. Ghonia, N. V. Hassen, A. Schneider et al.,\n\u201cLag-llama: Towards foundation models for time series forecasting,\u201d\narXiv preprint arXiv:2310.08278, 2023.\n[8] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov,\nG. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg\net al., \u201cA generalist agent,\u201d arXiv preprint arXiv:2205.06175, 2022.\n[9] A. Gu and T. Dao, \u201cMamba: Linear-time sequence modeling with\nselective state spaces,\u201d arXiv preprint arXiv:2312.00752, 2023.\n[10] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., \u201cRwkv: Reinventing\nrnns for the transformer era,\u201d arXiv preprint arXiv:2305.13048, 2023.\n[11] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity,\u201d The Journal\nof Machine Learning Research, vol. 23, no. 1, pp. 5232\u20135270, 2022.\n[12] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A.\nAwan, J. Rasley, and Y. He, \u201cDeepspeed-moe: Advancing mixture-of-\nexperts inference and training to power next-generation ai scale,\u201d in\nInternational Conference on Machine Learning.\nPMLR, 2022, pp.\n18 332\u201318 346.\n[13] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam-\nford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al.,\n\u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024.\n[14] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei,\n\u201cRetentive network: A successor to transformer for large language\nmodels (2023),\u201d URL http://arxiv. org/abs/2307.08621 v1.\n[15] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun,\nN. Shazeer, and Z. Chen, \u201cGshard: Scaling giant models with conditional\ncomputation and automatic sharding,\u201d arXiv preprint arXiv:2006.16668,\n2020.\n[16] W. Fedus, J. Dean, and B. Zoph, \u201cA review of sparse expert models in\ndeep learning,\u201d arXiv preprint arXiv:2209.01667, 2022.\n[17] A. Gu, K. Goel, and C. R\u00b4e, \u201cEfficiently modeling long sequences with\nstructured state spaces,\u201d arXiv preprint arXiv:2111.00396, 2021.\n[18] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, \u201cYarn: Efficient\ncontext window extension of large language models,\u201d arXiv preprint\narXiv:2309.00071, 2023.\n[19] S. Chen, S. Wong, L. Chen, and Y. Tian, \u201cExtending context window\nof large language models via positional interpolation,\u201d arXiv preprint\narXiv:2306.15595, 2023.\n[20] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\nY. Bengio, S. Ermon, and C. R\u00b4e, \u201cHyena hierarchy: Towards larger con-\nvolutional language models,\u201d arXiv preprint arXiv:2302.10866, 2023.\n[21] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou,\nA. Rudra, and C. R\u00b4e, \u201cZoology: Measuring and improving recall in\nefficient language models,\u201d arXiv preprint arXiv:2312.04927, 2023.\n[22] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoff-\nmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud et al., \u201cUnified\nscaling laws for routed language models,\u201d in International Conference\non Machine Learning.\nPMLR, 2022, pp. 4057\u20134086.\n[23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\n\u201cMistral 7b,\u201d arXiv preprint arXiv:2310.06825, 2023.\n[24] M. Pi\u00b4oro, K. Ciebiera, K. Kr\u00b4ol, J. Ludziejewski, and S. Jaszczur, \u201cMoe-\nmamba: Efficient selective state space models with mixture of experts,\u201d\narXiv preprint arXiv:2401.04081, 2024.\n[25] N.\nShazeer,\n\u201cGlu\nvariants\nimprove\ntransformer,\u201d\narXiv\npreprint\narXiv:2002.05202, 2020.\n[26] B. Wang and A. Komatsuzaki, \u201cGpt-j-6b: A 6 billion parameter autore-\ngressive language model,\u201d 2021.\n[27] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-\nzaro, \u201cMegatron-lm: Training multi-billion parameter language models\nusing model parallelism,\u201d arXiv preprint arXiv:1909.08053, 2019.\n[28] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nJ. Phang, H. He, A. Thite, N. Nabeshima et al., \u201cThe pile: An\n800gb dataset of diverse text for language modeling,\u201d arXiv preprint\narXiv:2101.00027, 2020.\n[29] D. Soboleva, F. Al-Khateeb, R. Myers, J. Steeves, J. Hestness, and\nN. Dey, \u201cSlimpajama: A 627b token cleaned and deduplicated version of\nredpajama,\u201d 7 2023. [Online]. Available: https://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\n[30] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\nM. Marone, C. Akiki, J. Li, J. Chim et al., \u201cStarcoder: may the source\nbe with you!\u201d arXiv preprint arXiv:2305.06161, 2023.\n[31] L. Soldaini and K. Lo, \u201cpeS2o (Pretraining Efficiently on S2ORC)\nDataset,\u201d Allen Institute for AI, Tech. Rep., 2023, oDC-By, https:\n//github.com/allenai/pes2o.\n[32] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer,\nA. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, \u201cLlemma: An open\nlanguage model for mathematics,\u201d arXiv preprint arXiv:2310.10631,\n2023.\n[33] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, \u201cCom-\npressive transformers for long-range sequence modelling,\u201d 2019.\n[34] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, \u201cFaster-\nmoe: modeling and optimizing training of large-scale dynamic pre-\ntrained models,\u201d in Proceedings of the 27th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, 2022, pp. 120\u2013134.\n[35] Y. Elazar, A. Bhagia, I. Magnusson, A. Ravichander, D. Schwenk,\nA. Suhr, P. Walsh, D. Groeneveld, L. Soldaini, S. Singh, H. Hajishirzi,\nN. A. Smith, and J. Dodge, \u201cWhat\u2019s in my big data?\u201d 2023.\n[36] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi,\nC. Foster, L. Golding, J. Hsu, A. Le Noac\u2019h, H. Li, K. McDonell,\nN. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf,\nA. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and\nA. Zou, \u201cA framework for few-shot language model evaluation,\u201d 12\n2023. [Online]. Available: https://zenodo.org/records/10256836\n[37] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, \u201cHellaswag:\nCan a machine really finish your sentence?\u201d 2019.\n[38] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, \u201cPiqa: Reasoning\nabout physical commonsense in natural language,\u201d 2019.\n[39] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, \u201cWinogrande:\nAn adversarial winograd schema challenge at scale,\u201d 2019.\n[40] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\nS. Pezzelle, M. Baroni, G. Boleda, and R. Fern\u00b4andez, \u201cThe lambada\ndataset: Word prediction requiring a broad discourse context,\u201d 2016.\n[41] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, \u201cThink you have solved question answering? try arc,\nthe ai2 reasoning challenge,\u201d 2018.\n[42] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, \u201cCan a suit of armor\nconduct electricity? a new dataset for open book question answering,\u201d\n2018.\n[43] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien,\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\n\u201cPythia: A suite for analyzing large language models across training and\nscaling,\u201d in International Conference on Machine Learning.\nPMLR,\n2023, pp. 2397\u20132430.\n[44] R. Sinkhorn and P. Knopp, \u201cConcerning nonnegative matrices and\ndoubly stochastic matrices,\u201d Pacific Journal of Mathematics, vol. 21,\nno. 2, pp. 343\u2013348, 1967.\n9\nAPPENDIX\nA. Model Hyperparameters\nHyperparameter\n1.5B\n2.8B\nNumber of Layers\n30\n36\nHidden Size\n1152\n1472\nNumber of Experts\n8\n8\nSequence Length\n2048\n2048\nState Size\n16\n16\nConvolution Dimension\n4\n4\nFFN Hidden Size\n3072\n3872\nExpansion Factor\n2\n2\nTABLE III\nARCHITECTURE HYPERPARAMETERS FOR THE 340M/1.5B AND\n630M/2.8B MODELS\nB. Training Hyperparameters\nHyperparameter\n340M/1.5B\n630M/2.8B\nLearning Rate\n0.0002\n0.00015\nBatch Size\n2064384 tokens\n2162688 tokens\nDropout\n0.0\n0.0\nLearning Rate Schedule\ncosine\ncosine\nMin Learning Rate\n0.00002\n0.00002\nWeight Decay\n0.0\n0.0\nTABLE IV\nTRAINING HYPERPARAMETERS FOR THE 340M/1.5B AND 630M/2.8B\nMODELS\nC. Mamba Block Internals\nIn this appendix, we provide a precise and detailed walk-\nthrough of the core computations that comprise a Mamba\nblock. Mamba derives from a line of work on state-space\nmodels, which are expressive recurrent models which have\nrecently been shown capable of competing with transformers\non large scale sequence modelling. The recurrence of these\nmodels enables them to be used efficiently for generation\nwithout a KV cache and causes them to scale in FLOPs and\nmemory linearly in the sequence length. The core insight is to\nutilize recurrence [17] or selective scan [9] to efficiently map\nthe central recurrence to parallel GPU hardware. The base\nof all such models is the following state-space equations (in\ncontinuous time):\ndh\ndt = A h + B x\n(8)\ny = C h\n(9)\nwhich define a classical linear time-invariant dynamical sys-\ntem. Here h denotes the state of a system at one instant. A\ndenotes a matrix which governs the \u2019natural dynamics\u2019 of h\nover time. x denotes a \u2019control\u2019 input to the system \u2013 i.e. one\nprovided by the controller or experimenter and B denotes a\ndynamics matrix which controls how x interacts with system.\nFinally, the states are transformed into \u2019observations\u2019, denoted\ny, through the observation matrix denoted C.\nThe Mamba block utilizes this dynamical system across\ntokens as its core computation implemented as a hardware\nefficient selective scan. The innovation of Mamba specifically\nis to make the A,B,and C matrices a linear function of the\ninput x, analogous to the Q,K,V matrices of a self-attention\nblock. Beyond this, Mamba wraps the SSM component in\na linear projection to and from the residual stream and a\nconvolution of the input, as well as an additional gating\nprojection path which gates the output of the SSM based on\na projection of the input to the block.\nWe denote the input to the mamba block x, the recurrent\nhidden state h, the sequence length as l. We set the hidden\nrecurrent state dimension to some factor of the input dimen-\nsion.\nThe mamba block contains matrices A which defines the\ndynamics for the recurrent state, B which is the projection\nfor the inputs, C which is the projection to the outputs y,\nthe matrix D which is a learnable bias on the output, a\ndiscretization timestep dt, and a gating vector z. The Mamba\nblock also performs a linear projection of the input x and z\nprior to the SSM with weight matrices Wx and Wz and an\noutput projection matrix Wy.\nThe computation inside a Mamba block runs as follows.\nFirst, the x and z projections are computed. This projection\noccurs for every token in the sequence independently.\nx = Wx x\n(10)\nz = Wz z\n(11)\nSecondly, after the projection, the Mamba block performs a\n1d convolution (\u2217) across the input sequence embeddings. This\nconvolution cannot be merged with the projection Wx because\nthis projection acts at the embedding level, and the convolution\nis acting at the sequence of tokens level.\nxt = Wfilter t \u2217 xt\n(12)\nThe input-dependent \u2018weights\u2019 B, C, and dt can then be\ncomputed, which are analogous to the Query, Key, and Value\nweights in attention.\nB = WB x\n(13)\nC = WC x\n(14)\ndt = WD x\n(15)\nThe matrix A is trained with a special initialization given\nin the matrix below. Note that updates are trained via the\nparameterization ln(A), presumably to make A positive and\nto improve stability, and then computed as A = exp( ln(A) ).\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n2\n3\n\u00b7 \u00b7 \u00b7\n1\n2\n3\n\u00b7 \u00b7 \u00b7\n...\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n(16)\n10\nThe weights are then discretized prior to use in the SSM\nkernel. Note that the discretization for B does not follow\nEquation 4 in [9].\ndt = softplus(dt + dtbias)\n(17)\ndA = exp(\u2212A dt)\n(18)\ndB = B dt\n(19)\nA single step of the ssm is then performed to obtain the new\nrecurrent state. Note that h+ \u2192 h when dt \u2192 0, as expected\nh+ = dA h + dB x\n(20)\nFrom the new recurrent state, the output C h+ can be\ncomputed. This output is also gated by the learnt gating vector\nz and passed through a final output projection before being\naddded back into the residual stream.\ny = C h+ + D x\n(21)\ny = silu(z) y\n(22)\ny = Wy y\n(23)\n(24)\nThe output of the SSM block is then the hidden state h+\nand the output y.\nA Mamba block can operate in two modes. The first mode\nis the recurrent method, which directly follows the steps\ndescribed here. This approach is linear in both memory and\ncomputational cost for a single step since it only utilizes the\nrecurrent state to predict the next token. The second way is\nto run the SSM across the whole sequence at once using the\n\u2019selective scan\u2019 operation and kernel introduced by [9]. For\nfurther reference on the implementation of the selective scan\nrefer to [9].\nD. Computing Parameters and FLOPs for Mamba-MoE\nLet us denote the embedding dimension D, the Mamba\ninner state as I, the recurrent state dimension H, the dt rank\ndt and the convolution dimension C. We denote the batch size\nB and the sequence length L.\nThe number of parameters in a Mamba block can then be\ncomputed as,\n3ID\n|{z}\nWx,Wz,Wy\n+2I(\nH\n|{z}\nWA,WB\n+ dt\n|{z}\nWdt\n+ C\n2\n|{z}\nconv\n) +\nI\n|{z}\nD\n+\n2D\n|{z}\nlayernorm\n(25)\nThe number of parameters in a MoE block can be computed\nas\n8D2E\n| {z }\nexperts\n+ DE\n|{z}\nrouter\n(26)\nWhere E is the number of experts in the layer. For a network\nof L layers, there are thus\nL\n2 Mamba blocks and\nL\n2 MoE\nblocks.\nTo begin approximating the number of FLOPs involved in\na single Mamba block, we make the following observation.\nGiven two matrices A\n\u2208\nRK\u00d7M and B\n\u2208\nRM\u00d7J,\nthen the total FLOPs involved in the matrix product AB is\napproximately 2KMJ, where the factor of 2 arises from the\nfact that matrix multiplication requires both a multiply and an\nadd operation. In the following calculations, we assume that\nthe matrix multiplications dominate the total FLOP count of\nthe model and hence ignore the nonlinearities, layernorms, and\nother computations.\nFirst, let us consider the projection operation involving the\nweights Wx,Wz, and Wy. All are of shape I \u00d7 D and hence\nthe total FLOPs for these are 6IDLB.\nThere is also the convolution which can be treated as a\nsingle I \u00d7 C matrix multiply requiring 2ICLB FLOPs.\nNow, we turn to the SSM block itself. We first compute the\ninput-dependent B and C matrices requiring a matrix multiply\nof shape I \u00d7 H each thus resulting in 4IH FLOPs. The A\nmatrix is not multiplied by the input but goes through an\nelementwise transform costing IH FLOPs. The dt projection\nfirst goes through an elementwise operation of order I FLOPs.\nNext, the discretization. The A matrix is multiplied by\nthe dt vector resulting, costing IH FLOPs. The B matrix is\nmultiplied by the input costing 2IH FLOPs. The SSM linear\nstate space step itself is just a matrix multiply and add so\ncosts 2IH FLOPs, and then the output projection using the\nC matrix also costs 2IH FLOPs. Putting this all together, we\nobtain the following expression,\nBLI(\n11H\n|{z}\nWx,Wz,Wy,SSM\n+\n4dt\n|{z}\ndt proj, discretization\n+\n1\n|{z}\ndt nonlinearity\n) + IH\n|{z}\nA\n(27)\nThe MoE blocks consist of E standard mlp blocks and a\nrouter. The FLOPs for each mlp block is simply 16D2 since\nthere are two weight matrices of shape 4D\u00d7D, and a multiply\nand add per matrix multiply. The router cost is simply 2DE.\nPutting this together, we obtain DE(16D + 2) FLOPs for an\nMoE block.\nE. Evaluations During Training\nWe evaluate BlackMamba on a suite of eight diverse eval-\nuation tasks in the zero-shot setting. We use the EleutherAI\nevaluation harness (version 0.3.0) [36]. Specifically, we eval-\nuate our models on the HellaSwag [37], PIQA [38], Wino-\nGrande [39], Lambada [40], ARC [41] (both the easy and\nchallenge versions), and OpenBookQA [42]. The evaluations\nwere run on model checkpoints taken every 10, 000 steps.\nWe observe that most evaluation metrics appear to increase\nsmoothly but noisily throughout training, before appearing to\nplateau towards their final values. This is broadly in line with\nprevious findings in the Pythia model suite [43], which find\nrelatively smooth improvements across training in many of\ntheir evaluation metrics. This provides some evidence that\nthe development of capabilities in language models occurs\nsmoothly and can be tracked during training and perhaps\npredicted ahead of time. Two evaluation metrics, however,\nWinoGrande and BoolQ, violate this trend for reasons that we\ndo not currently understand. We note that [43] also observe\n11\nno consistent trend on Winogrande. Between the BlackMamba\n340M/1.5B and 630M/2.8B models, we observe a clear\nbenefit of scale at the same iteration and token count on\nmost evaluations. In addition, we observe significant noise in\nsome of the evaluation metrics which may suggest that small\ndifferences in evaluations between different LLMs may not be\nsignificant.\nFig. 8. OpenBookQA evaluation accuracy over time\nFig. 9. ARC-Easy evaluation accuracy over time\nFig. 10. ARC-Challenge evaluation accuracy over time\nFig. 11. WinoGrande evaluation accuracy over time\nFig. 12. HellaSwag evaluation accuracy over time\nFig. 13. PIQA evaluation accuracy over time\nFig. 14. Lambada evaluation accuracy over time\nF. Sinkhorn MoE Routing Modifications\nRecall from the main text eq. (5) that the output token y of\nan MoE layer is given by\ny =\nX\ni\u2208top-k\nciEi(x)\n(28)\nwhere E1, E2, . . . , EN denote the MLP experts according to\nthe top-k probabilities pi.\nMost commonly, the probabilities pi(x) are obtained act-\ning by a trainable linear layer on the input x \u2208 Rd and\nsubsequently applying a non-linearity: pi(x) = \u03c3(Wi \u00b7 x),\nwith Wi \u2208 Rd. An important issue when training MoE\nmodels is that expert utilization should be balanced across\ntokens in a batch, which is required for compute efficiency.\nStandard approaches to ensure balanced usage include adding\na balancing regularization term to the loss as well imposing\nhard constraints bounding the number of tokens a given expert\ncan receive [15]. We instead use the Sinkhorn activation\nfunction for the router which, in the context of top-1 expert\n12\nselection, has proven to solve the balancing issue without\nthe need for additional regularization or constraints on expert\nusage [22].\nThe key property of the Sinkhorn activation function is\nthat, in addition to requiring normalization with respect to the\nexpert index i in pi(x), one additionally imposes normalization\nalong the samples dimension (which comprises batch size and\nsequence length). More explicitly, we require that \u03c3 satisfies:\nN\nX\ni=1\n\u03c3(Wi \u00b7 x\u03b1) = 1,\nS\nX\n\u03b1=1\n\u03c3(Wi \u00b7 x\u03b1) = S/N\n(29)\nwhere \u03b1 denotes the sample index, and S is the number\nof samples (batch size \u00d7 sequence length). Now, note that\nthe softmax, which only satisfies the first condition, can be\nvariationally defined by maximizing:\nsoftmax(L) \u2261 argmax\u03c0{\u03c0 \u00b7 L + S(\u03c0)}\n(30)\nwhere Li\u03b1\n=\nWi \u00b7 x\u03b1 are the logits, and S(\u03c0)\n=\n\u2212 P\ni\u03b1 \u03c0i\u03b1 log \u03c0i\u03b1 is the Shannon entropy. The Sinkhorn acti-\nvation can be defined through the same variational formulation\nexcept that it further satisfies the second constraint in (29).\nDenoting the solution to this maximization by\n\u03c0i\u03b1 = eLi\u03b1d(0)\ni d(1)\n\u03b1\n(31)\nwhere d(0) \u2208 RN and d(1) \u2208 RS, maximization of the right-\nhand side of (30) subject to (29) is obtained by solving\nd(0)\ni\n=\n1\nP\n\u03b1 eLi\u03b1d(1)\n\u03b1\n,\nd(1)\n\u03b1\n= S\nN\n1\nP\ni eLi\u03b1d(0)\ni\n(32)\nUnfortunately, these equations cannot be solved explicitly and\nthus, unlike the softmax case, there is no analytic form for the\nSinkhorn activation. These equations are solved approximately\nthrough an optimization loop, called the Sinkhorn algorithm\n[44].5 Our improvement is in the choice of the initial condition\nfor this optimization loop, which consists of taking d(0)\ni\n= 1\nand d(1)\n\u03b1\n=\nS\nN\nP\ni eLi\u03b1. This corresponds to initializing \u03c0i\u03b1\nto be the softmax normalized along the sample index \u03b1,\nthus immediately guaranteeing balanced usage of experts. We\nverified empirically that choosing this initial condition leads\nto much faster convergence of the Sinkhorn loop. Addition-\nally, a temperature rescaling Li\u03b1 \u2192 2Li\u03b1 further improves\nconvergence. Overall this led to shrinking the number of\niterations from 10-20 to just 1 across various models sizes,\nthus shortening the iteration time in our training experiments.\n5We need to additionally choose ci. One natural choice is ci = pi, but\nwith the Sinkhorn activation we verified that it is more efficient to choose\nci = f(Wi \u00b7 x) with f a simple activation function such as the sigmoid. We\nthink this is due to the Sinkhorn flattening out more quickly than e.g. sigmoid\nor softmax due to normalization along both dimensions.\n13\n"
  },
  {
    "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank",
    "link": "https://arxiv.org/pdf/2402.01878.pdf",
    "upvote": "19",
    "text": "LiPO: Listwise Preference Optimization through Learning-to-Rank\nTianqi Liu * 1 Zhen Qin * 1 Junru Wu 1 Jiaming Shen 1 Misha Khalman 2 Rishabh Joshi 2 Yao Zhao 2\nMohammad Saleh 2 Simon Baumgartner 1 Jialu Liu 1 Peter J. Liu 2 Xuanhui Wang 1\nAbstract\nAligning language models (LMs) with curated hu-\nman feedback is critical to control their behaviors\nin real-world applications. Several recent policy\noptimization methods, such as DPO and SLiC,\nserve as promising alternatives to the traditional\nReinforcement Learning from Human Feedback\n(RLHF) approach. In practice, human feedback\noften comes in a format of a ranked list over mul-\ntiple responses to amortize the cost of reading\nprompt. Multiple responses can also be ranked by\nreward models or AI feedback. There lacks such\na study on directly fitting upon a list of responses.\nIn this work, we formulate the LM alignment as\na listwise ranking problem and describe the List-\nwise Preference Optimization (LiPO) framework,\nwhere the policy can potentially learn more effec-\ntively from a ranked list of plausible responses\ngiven the prompt. This view draws an explicit\nconnection to Learning-to-Rank (LTR), where\nmost existing preference optimization work can\nbe mapped to existing ranking objectives, espe-\ncially pairwise ones. Following this connection,\nwe provide an examination of ranking objectives\nthat are not well studied for LM alignment with\nDPO and SLiC as special cases when list size is\ntwo. In particular, we highlight a specific method,\nLiPO-\u03bb, which leverages a state-of-the-art list-\nwise ranking objective and weights each prefer-\nence pair in a more advanced manner. We show\nthat LiPO-\u03bb can outperform DPO and SLiC by a\nclear margin on two preference alignment tasks.\n1. Introduction\nRecent Large Language Models, such as GPT-4 (OpenAI,\n2023) and Gemini (Team et al., 2023), have unlocked un-\nprecedented capabilities, witnessed by impressive perfor-\n*Equal contribution 1Google Research 2Google Deepmind. Cor-\nrespondence to: Tianqi Liu <tianqiliu@google.com>, Zhen Qin\n<zhenqin@google.com>.\nPreprint.\nFigure 1. Performance of the pairwise DPO (DPOBT), listwise\nDPO (DPOPL), and our proposed listwise approach, called LiPO-\u03bb.\nAll can benefit from training data beyond pairwise data (List Size\n= 2), while LiPO-\u03bb can benefit more and monotonically as list size\nincreases.\nmance on diverse tasks from conversational chatbot to pro-\ngramming. A key step to control the behavior of such\nLanguage Models (LMs) is to align them with curated hu-\nman feedback. Preference optimization has become an ac-\ntive research area. Reinforcement Learning with Human\nFeedback (RLHF) (Christiano et al., 2017) was first intro-\nduced to improve the alignment of LMs with human prefer-\nences (Ouyang et al., 2022). However, RLHF is a complex\nprocess, requiring substantial memory and hyperparamter\ntuning.\nSeveral recent works resort to alternatives of RLHF, and\nnoticeably converge to a pairwise ranking optimization\nparadigm. For example, DPO (Rafailov et al., 2023) opti-\nmizes a pairwise logistic loss directly from pairwise human\npreference data, while avoiding an explicit reward model\nand RL-based optimization. Similarly, SLiC (Zhao et al.,\n2023) optimizes a pairwise hinge loss objective on pairwise\npreference data directly from human or sampled from super-\nvised fine-tuned (SFT) policy and ranked by a reward model.\nRRHF (Yuan et al., 2023) starts from listwise preference\ndata labeled by a reward model, and optimizes a pairwise\ncontrastive objective (i.e., comparing all pairs in the list),\nwhich is analogous to the SLiC objective. More recently,\nRSO (Liu et al., 2023) unifies DPO and SLiC losses and\nproposes a better way of sourcing preference data via sta-\ntistical rejection sampling. Virtually all existing preference\n1\narXiv:2402.01878v1  [cs.CL]  2 Feb 2024\nLiPO: Listwise Preference Optimization through Learning-to-Rank\noptimization frameworks do not go beyond pairwise pref-\nerences. However, in practice, human preference data can\ncome as a ranked list to amortize the cost of reading the\nprompt (K\u00a8opf et al., 2023; Ouyang et al., 2022). For exam-\nple, Instruct-GPT (Ouyang et al., 2022) collects data in this\nway: for each output, labelers give a Likert score for overall\nquality on a 1-7 scale. After evaluating each output individ-\nually, labelers rank all the outputs for a given prompt. Ties\nare encouraged in cases where two outputs seem to be of\nsimilar quality. As another example, OpenAssistant (K\u00a8opf\net al., 2023) has a tree-structured prompt-assistant alternat-\ning conversation threads, with each node associated a rank\namong all siblings.\nIn this work, we treat LM alignment as a listwise ranking\nproblem, where the LM can potentially learn alignment\nmore effectively from listwise preferences. This draws an\nexplicit connection to the rich Learning-to-Rank (LTR) lit-\nerature (Liu, 2009). In particular, popular existing methods\ncan be mapped to existing ranking objectives, especially\npairwise ones. Noticeably, the LTR literature has shown\nthat a direct listwise optimization can be more effective\nthan pairwise alternatives for the listwise ranking problem,\nwith a rich set of methods of various properties and perfor-\nmances (Liu, 2009).\nThis listwise ranking perspective, together with related meth-\nods, have not been well studied for the LM preference op-\ntimization problem. As we will show, not every listwise\nobjective is effective and thus it is desired to conduct an\nexamination to identify the most effective ones for LM\nalignment. In Figure 1, we can see that listwise preference\ndata can benefit existing pairwise methods like DPO even\nthough they treat all pairs from a list equally, a problem\nnot well studied in the literature, while our new listwise\nmethod, called LiPO-\u03bb, can further benefit from listwise\npreferences. The DPO paper (Rafailov et al., 2023) briefly\ntouches the listwise Plackett-Luce model (Luce, 2005) for\npreference modeling in their Appendix without any experi-\nmental results. We notice that their listwise approach maps\nto a specific listwise ranking objective (Xia et al., 2008),\nwhich falls behind the state of the art.\nTo this end, we provide the first comprehensive study of\nranking objectives under the listwise preference optimiza-\ntion (LiPO) framework, where we compare popular and\nstate-of-the-art ranking objectives for the LM preference\noptimization problem. In particular, existing methods, such\nas DPO, RRHF, and SLiC, can be mapped to existing pair-\nwise ranking optimization objectives from listwise data (if\nonly pairwise data is used, it is equivalent to list size be-\ning 2). Furthermore, we show that a new method, LiPO-\u03bb,\nwhich leverages a state-of-the-art ranking objective (Burges\net al., 2006; Wang et al., 2018; Jagerman et al., 2022a), can\nachieve very competitive performance. Noticeably, LiPO-\u03bb\nallows an intuitive interpretation: it leverages a sophisticated\nweighting paradigms that assigns listwise-aware weighting\nto sampled pairs, to optimize a well-founded ranking met-\nric (Wang et al., 2013), in contrast to existing methods that\nassign uniform weights or use weighting schema that fails\nto consider various factors.\nBy a comprehensive study of various ranking objectives on\nReddit TL;DR and AnthropicHH tasks, we show that LiPO-\n\u03bb is better than existing methods, including DPO (Rafailov\net al., 2023) and SLiC (Zhao et al., 2023), as well as other\nbaselines motivated by the LTR literature.\nOur contributions are summarized as follows:\n\u2022 We describe the Listwise Preference Optimization\n(LiPO) framework, which generalizes recent pairwise\npreference optimization methods and allows an ex-\namination of other alternatives through the lens of\nLearning-to-Rank.\n\u2022 We provide a comprehensive investigation of ranking\nobjectives for LM preference optimization, especially\nlistwise objectives that are not well studied in the LM\npreference optimization literature.\n\u2022 We highlight a new method, LiPO-\u03bb, which shows\ncompetitive performance across the evaluation tasks.\n2. The LiPO Framework\nIn the remaining, we build on the notations of Rafailov et al.\n(2023). In LM generation, given a prompt, x \u2208 X, there\nis an action space Y, where each action is usually called a\nresponse. A policy \u03c0 \u2208 \u2206X\nY associates to each prompt x a\ndiscrete probability distribution \u03c0(.|x) \u2208 \u2206Y where \u2206Y is\nthe set of discrete distributions over Y.\n2.1. Problem Formulation\nThe goal is to learn a policy \u03c0 from training data, with a\nkey consideration that the policy should align with human\npreference. Existing work (Zhao et al., 2023; Rafailov et al.,\n2023) would start the discussion of learning from pairwise\npreference data from here. However, we note that preference\nmodeling can be treated as a more general listwise ranking\nproblem: eventually the policy \u03c0 is expected to virtually\nrank a list of discrete actions from \u2206Y, and learning from\nlistwise preference data may be more effective for LM align-\nment. In fact, as we discussed above, human preference data\ncan come as a ranked list to amortize the cost of reading the\nprompt (K\u00a8opf et al., 2023; Ouyang et al., 2022). Thus, we\ndescribe a more general Listwise Preference Optimization\n(LiPO) framework.\nThe training dataset is D = {x(i), y(i), \u03c8(i)}N\ni=1: given a\nprompt x, we have a list of responses y = (y1, ..., yK) of\n2\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nSample\nLabel\nOptimize\nPrompts\nSampling\nList of Responses\ny2\ny1\ny3\nyK\n......\nReward Model or\nHuman Raters\nLabeling\nSFT Policy\nListwise Preference\nOptimization\nPreference List with\nLabels\nOptimized Policy\nFigure 2. An illustration of the Listwise Preference Optimization (LiPO) pipeline. For each prompt, LiPO samples a list of responses from\ncertain policy or mixed policies. Then human or reward ranking model assign each response a ranking label. After that LiPO optimizes\nthe policy via ranking loss to align with the preferences.\nsize K, which can be generated from SFT policy or other\nsources. When K = 2, this reduces to pairwise data. We\nassume there are real-valued labels \u03c8 = (\u03c81, ..., \u03c8K) \u2208\n[0, 1]K associated with the corresponding responses, which\nmay come from human raters or be derived from a reward\nmodel. A higher label value indicates a better response.\nWe use yi \u227b yj to denote a preference of two responses\ngiven \u03c8i > \u03c8j. In our work, we compute \u03c8k as the average\nwinning probabilities of the yk over all responses:\n\u03c8k = 1\nK\nK\nX\ni=1\nP(yk \u227b yi)\n(1)\nFor human-ranked responses, we approximate the winning\nprobability as \u02c6P(yi \u227b yj) = 1(human prefer yi over yj).\nFor a pairwise reward ranking model (Zhao et al., 2023;\nLiu et al., 2023), we use the predicted winning probabilities\ndirectly. This aggregation and labeling schema is valid\ntheoretically (Shah & Wainwright, 2018) and works well\nempirically (Qin et al., 2023) in the ranking literature.\nGiven the listwise preference data, LM alignment can be for-\nmulated as a Learning-to-Rank (LTR) problem. In LTR (Liu,\n2009), the goal is to learn a ranking model \u03c0\u03b8 that can\noutput the relevance scores s for all documents given a\nquery. In LM alignment, we treat x as the query and y as\ndocuments in the LTR setting. Then we define the scores\ns = {s1, ..., sK}, where si is defined as the following nor-\nmalized one for (x, yi) inspired by Rafailov et al. (2023):\ns(\u03c0\u03b8, \u03c0ref, \u03b2) = {s1, ..., sK}\n\u225c {\u03b2 log \u03c0\u03b8(y1|x)\n\u03c0ref(y1|x), ..., \u03b2 log \u03c0\u03b8(yK|x)\n\u03c0ref(yK|x)},\n(2)\nwhere \u03c0\u03b8 is the policy parameterized by \u03b8 that is to be\nlearned, \u03c0ref is the SFT policy that initializes \u03c0\u03b8, and \u03b2 is\na hyper-parameter to control the KL divergence between\n\u03c0\u03b8 and \u03c0ref. In DPO (Rafailov et al., 2023), the scores are\ntreated as \u201cimplicit reward\u201d, the goal of DPO is to align the\n\u201cimplicit reward\u201d towards human preference data directly.\nTo simplify the notation, we omit the dependency of s upon\n(\u03c0\u03b8, \u03c0ref, \u03b2) from now on.\nLTR algorithms learn \u03c0\u03b8 using loss functions as their objec-\ntives. A ranking loss function is in general defined based on\nlabels \u03c8 of responses y and predicted scores s:\nL(\u03c0\u03b8; \u03c0ref, \u03b2) = E(x,y,\u03c8)\u223cD [l(\u03c8, s)] .\n(3)\nl is the loss function for a single prompt x that takes the\nlabels and scores as input and output a real value as the loss:\nl : (\u03c8, s) \u2192 R.\n(4)\nA Learning-to-Rank algorithm is to find the optimal \u03c0\u03b8 that\nminimizes the overall loss in the space of ranking models.\nWe call Eq 3 the Listwise Preference Optimization (LiPO)\nframework under the context of LM alignment.\n2.2. Ranking Losses in Existing Work\nWith the definition of s in Eq 2, we show that several popular\nLM alignment methods can be mapped into the LiPO frame-\nwork using different ranking losses in the LTR literature.\nIn DPO paper, two loss functions are proposed under two\nranking frameworks: Bradley-Terry (BT) model (Bradley &\nTerry, 1952) and Plackett-Luce (PL) model (Plackett, 1975).\nWe refer DPOBT loss as the BT model-based pairwise loss,\nand refer DPOPL loss as the PL model-based listwise loss.\nIn SLiC paper, the loss consists of two terms: calibration\nloss and regularization loss. RSO (Liu et al., 2023) improves\nthe calibration loss by normalizing the likelihood with the\nreference policy. We use SLiCnorm to denote the improved\ncalibration loss.\nPairwise preference losses.\nThe pairwise logistic ranking\nloss (Burges et al., 2005) is one popular choice to fit a list\n3\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nof ranked data:\nLpair-logistic(\u03c0\u03b8; \u03c0ref, \u03b2) =\nEx,y,\u03c8\u223cD\n\uf8ee\n\uf8f0 X\n\u03c8i>\u03c8j\nlog(1 + e\u2212(si\u2212sj))\n\uf8f9\n\uf8fb .\n(5)\nWe connect the above loss with DPOBT via the following\nproposition:\nProposition 2.1. When K = 2 and pairwise logistic rank-\ning loss is used, LiPO is equivalent to DPOBT (Rafailov\net al., 2023).\nProof. When K = 2, there are only two pairs: (y1, y2) and\n(y2, y1). We use yw to denote the winning response and yl\nto denote the losing response.\nThen Eq 5 becomes:\nE(x,yw,yl)\u223cD\nh\nlog(1 + e\u2212(sw\u2212sl))\ni\n=\n\u2212E(x,yw,yl)\u223cD [log(\u03c3(sw \u2212 sl))]\n(6)\nwhich is the same as Eq (7) in Rafailov et al. (2023) if we\nsubstitute si with \u03b2 log \u03c0\u03b8(yi|x)\n\u03c0ref(yi|x).\nSimilarly, we can connect normalized SLiC loss (hinge-\nnorm loss in Liu et al. (2023)) with pairwise hinge loss\nfrom RankSVM (Joachims, 2002):\nLpair-hinge(\u03c0\u03b8; \u03c0ref, \u03b2) =\nEx,y,\u03c8\u223cD\n\uf8ee\n\uf8f0 X\n\u03c8i>\u03c8j\nmax (0, 1 \u2212 (si \u2212 sj))\n\uf8f9\n\uf8fb .\n(7)\nProposition 2.2. When K = 2 and pairwise hinge ranking\nloss is used, LiPO is equivalent to SLiCnorm (Zhao et al.,\n2023; Liu et al., 2023).\nProof. When K = 2, there are only two pairs: (y1, y2) and\n(y2, y1). We use yw to denote the winning response and yl\nto denote the losing response.\nThen Eq 7 becomes:\nE(x,yw,yl)\u223cD [max (0, 1 \u2212 (sw \u2212 sl))]\n(8)\nwhich is the same as Eq (10) in Liu et al. (2023) if we\nsubstitute si with \u03b2 log \u03c0\u03b8(yi|x)\n\u03c0ref(yi|x) and set \u03b2 = \u03b3.\nListwise preference losses.\nOne can fit all pairs using\npairwise-logistic or pairwise-hinge losses. Another way is\nto directly fit an Maximum Likelihood Estimation (MLE)\non the listwise ranked data. Xia et al. (2008) proposes list\nMLE ranking loss:\nLlist-mle(\u03c0\u03b8; \u03c0ref, \u03b2) =\n\u2212 Ex,y1,y2...yK\u223cD\n\"\nlog\nK\nY\nk=1\nexp(s\u03c4(k))\nPK\nj=k exp(s\u03c4(j))\n#\n,\n(9)\nwhere \u03c4(i) is the document ranked at the i-th position in the\nlistwise permutation determined by label.\nProposition 2.3. When the list MLE loss is used, LiPO is\nequivalent to DPOPL (Rafailov et al., 2023).\nProof. The list MLE loss is identical to the one in Eq (20) in\nRafailov et al. (2023) if we substitute si with \u03b2 log \u03c0\u03b8(yi|x)\n\u03c0ref(yi|x).\n2.3. Limitations of Existing Work\nFrom the general LiPO formulation and the analysis of\nrecent policy optimization methods, we can see they map\nto specific choices of existing ranking objectives. Through\nthe lens of LTR, we note there are two major concerns of\nthe discussed methods that may limit the effectiveness of\npreference optimization.\nFirst, all pairwise approaches, which dominate the current\npreference optimization literature, ignore listwise permuta-\ntion information beyond pairs. Considering all candidates\nunder the same prompt in a principled manner may allow\nthe policy to learn more effectively.\nSecond, virtually all existing methods, even the listwise one\n(DPOPL), ignore the label values, i.e., they only focus on the\noptimal pairwise or listwise rank-ordering of the responses.\nThis has information loss and may incur confusing learning\nbehavior. For example, two lists with labels (0.99, 0.50,\n0.01) and (0.51, 0.50, 0.49) will be treated the same, leading\nto inefficiency during training and may hurt generalization.\n3. LiPO-\u03bb\nUnder the general LiPO framework, a rich family of other\noptimization objectives can be explored from the LTR lit-\nerature. In this section, we propose a specific instantiation,\nLiPO-\u03bb, which builds upon a state-of-the-art ranking objec-\ntive, addresses the above limitations in a principled manner,\nand performs well empirically in the experiments.\nOur LiPO-\u03bb is based on the LambdaLoss method (Burges\net al., 2006; Wang et al., 2018; Jagerman et al., 2022a).\n4\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nSpecifically, the training objective of LiPO-\u03bb is:\nLlambda-loss(\u03c0\u03b8; \u03c0ref, \u03b2) =\n\u2212 Ex,y,\u03c8\u223cD\n\uf8ee\n\uf8f0 X\n\u03c8i>\u03c8j\n\u2206i,j log(1 + e\u2212(si\u2212sj))\n\uf8f9\n\uf8fb ,\nwhere\n\u2206i,j = |Gi \u2212 Gj| \u00b7 |\n1\nD(\u03c4(i)) \u2212\n1\nD(\u03c4(j))|.\n(10)\n\u2206i,j is called the Lambda weight. G is called a gain function\nwith Gi = 2\u03c8i \u2212 1 as the commonly used one. D is a rank\ndiscount function with D(\u03c4(i)) = log(1 + \u03c4(i)) as the\ncommonly used one, where \u03c4(i) is the rank position of yi\nin the ranking permutation induced by s, thus it is a listwise\nmethod even though the formula can be written in terms\nof pairs. In other words, there are dependencies on other\nitems in the same list for each pair. One has the flexibility\nto change the gain and discount functions but we resort to\nthe original LambdaLoss configuration here for simplicity.\nIt has been shown that this loss function can optimize the\nwell-founded DCG metric (Burges et al., 2006; Donmez\net al., 2009),\nDCG =\nK\nX\ni=1\nGi\nD(\u03c4(i)),\nwhich has several ideal properties as a ranking metric such\nas consistent distinguishability (Wang et al., 2013): for every\npair of substantially different ranking policies, the ranking\nmetric can decide which one is better in a consistent manner\non almost all datasets.\nComparisons.\nThere are several interesting findings by\ncomparing LiPO-\u03bb with other methods under the LiPO\nframework. First, the gain function G considers label score\nvalues \u03c8, which is ignored in virtually all existing methods.\nSecond, comparing with the list MLE loss, the permutation\nconsidered is induced by the model prediction scores, in-\nstead of the static labels. Burges et al. (2006) showed that\nconsidering such dynamic permutations based on model pre-\ndictions during training can lead to smoother optimization\nlandscapes to optimize the non-smooth ranking objectives,\nresulting in better empirical performance than using the\nstatic ranks from labels. Last but not least, LiPO-\u03bb can be\ntreated as a weighted version of DPOBT over all pairs of\nranked list by comparing with Eq 5. Instead of treating each\npair equally, Lambda weight is a listwise permutation aware\nweighting mechanism. One intuition of Lambda weight is\nto weight each pairs by the difference of ranking metrics\nwhen they are swapped in the list (Burges, 2010).\n4. Other Ranking Losses\nOther existing or future ranking objectives may be studied\nunder the LiPO framework. In experiments, in addition to\nexisting methods that can be treated as specific instantiations\nof LiPO, we also study two pointwise ranking objectives,\nthe pointwise Mean Squared Error (MSE) loss:\nLpoint-mse(\u03c0\u03b8; \u03c0ref, \u03b2) = Ex,y,\u03c8\u223cD\nK\nX\nk=1\n(\u03c8k \u2212 sk)2,\n(11)\nand the pointwise sigmoid cross entropy loss:\nLpoint-sigmoid(\u03c0\u03b8; \u03c0ref, \u03b2) =\n\u2212 Ex,y,\u03c8\u223cD\nK\nX\ni=1\n(\u03c8i log \u03c3(si) + (1 \u2212 \u03c8i) log(1 \u2212 \u03c3(si)))\n(12)\nWe also consider another popular listwise ranking objective,\ncalled softmax cross entropy loss as in ListNet (Cao et al.,\n2007)):\nLsoftmax(\u03c0\u03b8; \u03c0ref, \u03b2) =\nEx,y,\u03c8\u223cD\n\" K\nX\nk=1\n\u03c8k\nPK\nj=1 \u03c8j\nlog\n \nexp(sk)\nPK\nj=1 exp(sj)\n!#\n.\n(13)\n5. Experiments\nTasks\nWe study different ranking losses unified under the\nLiPO framework on Reddit TL;DR summarization (Stien-\nnon et al., 2020) and AnthropicHH dialogue (Bai et al.,\n2022) datasets. The Reddit TL;DR summarization dataset\ncontains both fine-tune data Dtldr\nsft and human feedback data\nDtldr\nhf . Dtldr\nsft contains 117k/6k/6k examples in train, valida-\ntion, and test splits. Dtldr\nhf consists of 93k human preferences\non decodes from multiple models. The AnthropicHH is a\ndialogue dataset with x as conversation between a human\nquery and an AI assistant. We use the helpful slice Dhelpful\nhf\nfrom 161k/9k examples in train and test splits. We use the\npositive responses as SFT targets.\nMethod\nStarting from a T5-large (770M) SFT policy and\na T5-XXL (11B) pairwise reward-ranking model (Zhao\net al., 2023; Liu et al., 2023), we first construct response\nlists by sampling K = 8 responses (with more ablations\nin Section 5.2) for each prompt using SFT policy with\ntemperature = 0.7 and top k = 40. Then we conduct all\npair comparisons using the pairwise reward-ranking model,\nresulting in a winning probability matrix \u03a8 \u2208 [0, 1]K\u00d7K.\nThen we compute \u03c8k as \u03c8k =\n1\nK\nPK\ni=1 \u03a8ki. To compute\nthe normalized ranking scores s, we set \u03b2 = 0.05.\nWe consider three types of loss functions under the LiPO\nframework: pointwise, pairwise, and listwise. Pointwise\nlosses include point-mse (Eq 11) and point-sigmoid (Eq 12).\nPairwise losses include pair-hinge (Eq 7) pair-logistic (Eq 5).\nListwise losses include list-mle (Eq 9), softmax (Eq 13), and\nlambda-loss (Eq 10). We use the RAX library (Jagerman\n5\nLiPO: Listwise Preference Optimization through Learning-to-Rank\net al., 2022b) to compute losses in Jax. See Appendix C for\nexample usages.\nFor pairwise losses, we utilize all 64 pairs and show in\nSection 5.2 that it can improve performance than sampling a\nsingle pair each time as commonly done in existing work, so\nthis setting is not only fair but also benefit pairwise methods.\nWe use batch size 32 and learning rate 2e-5 with Adafactor\noptimizer (Shazeer & Stern, 2018). For each run, we pick\nthe checkpoint with the highest reward-ranking model win\nrate against the SFT target.\nEvaluation\nOur experiments use three different ap-\nproaches to evaluate following existing protocols (Rafailov\net al., 2023; Liu et al., 2023): Proxy Reward Model, Au-\ntoSxS, and human evaluation. Proxy Reward Model com-\nputes win rate of generated response against SFT target on\nthe trained T5-XXL pairwise reward-ranking model. Au-\ntoSxS uses large PaLM 2 instruction tuned model (PaLM\n2-L-IT) (Google et al., 2023) for few-shot in-context learn-\ning (details in Appendix A). Human Evaluation asks human\nraters to assign a quality score on each response and de-\ntermine the best one among DPOBT, DPOPL and LiPO-\u03bb\n(details in Appendix B).\n5.1. Performance Comparison on the Two Tasks\nThe main comparison results are shown in Table 1. We have\nthe following observations: First, pointwise methods are not\ncompetitive as expected, indicating that only considering\nthe pointwise label values are not sufficient, and preference\ninformation is critical. Second, DPOPL does not perform\nbetter than DPOBT, showing that the choice of listwise rank-\ning objective is important given listwise data. This aligns\nwith existing LTR literature that list-mle is not a compet-\nitive ranking objective as it enforces listwise permutation\nwithout caring about label values. On the other hand, while\nDPOBT does not consider label values either, the pairwise\nformat can be less prone to ill-behaved listwise behaviors.\nThird, the listwise Softmax loss is not competitive. This is\nunderstandable as Softmax loss is most effective on opti-\nmization listwise ranking with sparse labels, such as binary\nclick data (Yu et al., 2015; Bai et al., 2023). For LM gen-\neration, the responses are sampled from a plausible set so\nthe labels are dense, which does not fit the Softmax loss\nwell. Meanwhile, LiPO-\u03bb shows strong performance and\nimproves upon all baselines by effectively leveraging list-\nwise data and label value information.\n5.2. Ablation Studies and Analysis\nTo gain an in-depth understanding of the benefits brought by\nthe listwise formulation and methods, we conduct ablation\nstudies to understand the effect in terms of listwise data,\nLambda weight choices, and model sizes.\nFigure 3. Performance of different ranking losses with varying list\nsizes on the Reddit TL;DR dataset. The shaded areas are 95%\nconfidence intervals.\nAblation study on list size\nTo better understand the effect\nof preference optimization on listwise data, we conduct\nanalysis over multiple choices of list sizes on the Reddit\nTL;DR dataset. As illustrated in Figure 3, most methods can\nbenefit from going beyond pairwise training data (List Size\n= 2) by leveraging more information under each prompt\nx. LiPO-\u03bb with lambda-loss is the only method that can\nrobustly benefit from longer list sizes, showing it can more\neffectively leverage the rich listwise information.\nAblation study on Lambda weights\nAs discussed in Sec-\ntion 3, we use specific choice of Lambda weights by set-\nting the gain function Gi = 2\u03c8i \u2212 1 and rank discount\nfunction D(\u03c4(i)) = log(1 + \u03c4(i)), which is called the\nDCG weight, that can optimize the DCG ranking met-\nric. In this ablation study, we try other options that may\nnot have a clear connection to ranking metrics. Constant\nWeight assigns equal weights on all pairs, which reduces to\nDPOBT. Constant \u03b4 Gain sets the gain function difference\nto a constant: |Gi \u2212 Gj| = 1, and Constant \u03b4 Discount\nsets the rank discount function difference to a constant:\n|\n1\nD(\u03c4(i)) \u2212\n1\nD(\u03c4(j))| = 1. The comparisons are shown in Fig-\nure 4. We can see that using DCG weight is most effective\non both datasets, showing the importance of setting both\nfunctions appropriately in order to optimize well-founded\nranking metrics.\nScale up the policy model\nTo understand how well the\nLiPO can be scaled up to larger policy models, we train a\nT5-XXL policy model and compare among DPOBT, DPOPL\nand LiPO-\u03bb. Table 2 shows that all three methods scale up\nwell and LiPO-\u03bb is competitive on both tasks.\n6\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nApproach\nRanking Loss\nProxy Reward (%)\nAutoSxS (%)\nReddit TL;DR\npoint-mse\n49.43\u00b11.18\n39.94\u00b11.72\npoint-sigmoid\n64.14\u00b11.16\n49.28\u00b11.80\nsoftmax\n75.40\u00b10.98\n58.60\u00b11.72\nSLiCnorm\npair-hinge\n87.23\u00b10.78\n67.16\u00b11.62\nDPOBT\npair-logistic\n88.52\u00b10.74\n67.09\u00b11.65\nDPOPL\nlist-mle\n88.27\u00b10.76\n67.23\u00b11.60\nLiPO-\u03bb\nlambda-loss\n90.60\u00b10.65\n68.06\u00b11.58\nAnthropicHH\npoint-mse\n57.55\u00b11.22\n21.97\u00b11.47\npoint-sigmoid\n71.35\u00b11.11\n25.72\u00b11.50\nsoftmax\n73.21\u00b11.07\n28.87\u00b11.65\nSLiCnorm\npair-hinge\n89.68\u00b10.72\n42.07\u00b11.75\nDPOBT\npair-logistic\n91.11\u00b10.66\n44.80\u00b11.73\nDPOPL\nlist-mle\n90.61\u00b10.72\n43.25\u00b11.77\nLiPO-\u03bb\nlambda-loss\n92.60\u00b10.62\n47.90\u00b11.70\nTable 1. Comparison of different methods with T5-large policy model to leverage listwise preference data. Proxy reward and few-shot\nPaLM 2-L-IT win rates against SFT target text are reported. All methods use preference list with size 8, and pairwise methods including\nSLiCnorm and DPOBT use all pairs generated from the list and treat them equally.\nFigure 4. Performance on the Reddit TL;DR and AnthropicHH\ndatasets by using different Lambda weight choices. See text for\nexplanation of different options. The error bars denote 95% confi-\ndence intervals.\nApproach\nProxy Reward (%)\nAutoSxS (%)\nReddit TL;DR\nDPOBT\n96.22\u00b10.43\n82.38\u00b11.20\nDPOPL\n96.00\u00b10.45\n81.96\u00b11.30\nLiPO-\u03bb\n97.32\u00b10.36\n83.79\u00b11.25\nAnthropicHH\nDPOBT\n97.48\u00b10.34\n68.81\u00b11.63\nDPOPL\n97.28\u00b10.37\n68.84\u00b11.58\nLiPO-\u03bb\n98.27\u00b10.29\n69.81\u00b11.58\nTable 2. Comparison of DPOBT, DPOPL, and LiPO-\u03bb with T5-\nXXL policy model to leverage listwise preference data on T5-XXL\npolicy model. Proxy reward and few-shot PaLM 2-L-IT win rate\nagainst SFT target text are reported. All methods use preference\nlist with size 8.\n5.3. Human Evaluation Results\nTo further verify the improvements of LiPO-\u03bb, we conduct\nside-by-side human evaluation using Amazon Mechanical\nTurk. Given a document and three responses generated\nfrom DPOBT, DPOPL and LiPO-\u03bb, raters are asked to assign\na pointwise overall quality (1-5) to each response, and to\nchoose the best one. Each task is replicated 3 times and\ntherefore judged by 3 different raters. To eliminate bias,\nwe anonymize all the models and randomly shuffle order of\nresponses for each task. We aggregate pointwise metrics by\naveraging the ratings across all replicas, and we aggregate\nthe choice metric using majority vote. For more details\nabout the tasks, see Appendix B.\nIn total 67 different raters participated in the Reddit TL;DR\nevaluation study with a median of 18 tasks per rater. The\nhuman evaluation results are shown in Table 3. LiPO-\u03bb has\nshown to be better than DPOBT and DPOPL in both tasks.\n7\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nApproach\nChosen as Preferred1\nQuality\nReddit TL;DR\nDPOBT\n19%\n3.63\nDPOPL\n16%\n3.67\nLiPO-\u03bb\n40%\n3.80\nAnthropicHH\nDPOBT\n20%\n3.66\nDPOPL\n20%\n3.66\nLiPO-\u03bb\n27%\n3.72\nTable 3. Human evaluation comparing three approaches on two\ntasks. LiPO-\u03bb shows to be preferred more often than DPOBT and\nDPOPL.\n6. Related Work\nLM Alignment.\nWhile self-supervised LMs learn to com-\nplete some interesting tasks (Radford et al., 2019), their per-\nformance on downstream tasks, such as acting as a conver-\nsational agent, can be significantly improved by alignment\nwith human preference datasets. The pivotal Reinforcement\nLearning from Human Feedback (RLHF) framework (Chris-\ntiano et al., 2017) first fits a reward function under a pref-\nerence model such as the Bradley-Terry model (Bradley &\nTerry, 1952), then fine-tunes the LM to maximize the given\nreward using reinforcement learning algorithms. However,\nfine-tuning LMs with reinforcement learning is challenging\nin practice, involving training multiple LMs and sampling\nfrom the LM policy in the loop of training, incurring signifi-\ncant computational costs and requiring extensive hyperpa-\nrameter tuning. A stream of recent work resort to alterna-\ntives of RL based preference optimization approaches. As\nwe discussed, two parallel work, SLiC (Zhao et al., 2023)\nand RRHF (Yuan et al., 2023) directly use human prefer-\nence data or use a reward model to label preferences, then\nboth use a pairwise hinge loss to align policy responses.\nOne difference is RRHF considers listwise data to start with,\nbut their pairwise objective handles each pair from the list\nindependently, which can be treated as pairwise objective\non listwise data. DPO (Rafailov et al., 2023) proposes to\ndirectly align the behavior of LM without using a reward\nmodel with the pairwise logistic loss. Later work (Liu et al.,\n2023) shows that DPO has a distribution drift issue due to\nits lack of a reward model, as the maximum likelihood esti-\nmator (MLE) of the target optimal policy requires labeled\npreference pairs sampled from that policy. DPO further\nproposed a variant that learns from listwise preference data\nusing the list MLE loss, which is analogous to PRO (Song\net al., 2023). In this work, we mainly focus on the opti-\nmization objective perspective and show that recent work\nconverge to a few known ranking objectives in the LTR\nliterature.\n1The proportion may not sum up to 100% because there are\ncases of same preference across all approaches.\nLearning-to-Rank.\nThe Learning-to-Rank (LTR) field\nhas a rich literature due to its practical values in applica-\ntions such as web search (Liu, 2009) and recommender\nsystems (Karatzoglou et al., 2013). Traditional LTR work\nmainly focuses on developing more effective ranking objec-\ntives to optimize ranking metrics, ranging from pointwise,\npairwise, to listwise approaches (Liu, 2009).\nRankSVM (Joachims, 2002) and RankNet (Burges et al.,\n2005) leverage pairwise hinge loss and pairwise logistic\nloss respectively for the ranking problem. Listwise ranking\nobjectives gain popularity thereafter to directly optimize\nthe listwise ranking metrics. ListMLE and Softmax cross\nentropy losses are two representative listwise losses pro-\nposed in (Xia et al., 2008) and (Cao et al., 2007). ListMLE\nonly concerns about the ordering under the Plackett-Luce\nmodel (Luce, 2005), and Softmax cross entropy loss is effec-\ntive on lists with sparse labels, such as click logs (Yu et al.,\n2015). LambdaRank (Burges et al., 2006) shows that weight-\ning pairs with the listwise Lambda weight leads to strong em-\npirical performance in terms of optimizing the non-smooth\nDCG metric, and it is unified under the LambdaLoss (Wang\net al., 2018) framework with theoretical justification and\nconvergence proof. Other lines of LTR research focus on\nexploring new model architectures (Qin et al., 2020), learn-\ning from biased feedback (Joachims et al., 2017), etc. In\nthis work, we focus on the ranking objectives and show that\nexisting LM alignment methods can be mapped to specific\nchoices of ranking objectives. Following this connection,\nwe studied multiple listwise objectives, which have not been\nnot well studied, for LM alignment.\n7. Conclusion and Future Work\nWe describe the Listwise Preference Optimization(LiPO)\nframework for LM alignment with a ranked list over multi-\nple responses for each prompt. It connects LM alighment\nwith Learning-to-Rank techniques. In this framework, we\ngeneralize recent preference optimization methods and an-\nalyze limitations of existing methods from the Learning-\nto-Rank perspective. We then provide a comprehensive\ninvestigation of various ranking objectives for LM prefer-\nence optimization, especially listwise objectives that are not\nwell studied in the literature. We highlight a new method,\nLiPO-\u03bb, which builds upon the state-of-the-art ranking ob-\njectives and shows competitive performance across multiple\nevaluation tasks.\nOur work opens up a few interesting directions for future\nwork. (1) The effectiveness of LambdaLoss is demonstrated\nempirically in this paper and this can inspire more theoreti-\ncal understanding of this method for LM alignment; (2) It\nis also interesting to study how to do the online learning\nwhere the list of responses are elicited from the policy being\ntrained to reduce the distribution shift.\n8\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nReferences\nBai, A., Jagerman, R., Qin, Z., Yan, L., Kar, P., Lin, B.-R.,\nWang, X., Bendersky, M., and Najork, M. Regression\ncompatible listwise objectives for calibrated ranking with\nbinary relevance. In Proceedings of the 32nd ACM In-\nternational Conference on Information and Knowledge\nManagement, pp. 4502\u20134508, 2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\nBradley, R. A. and Terry, M. E. Rank analysis of incom-\nplete block designs: I. the method of paired comparisons.\nBiometrika, 39(3/4):324\u2013345, 1952.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,\nHamilton, N., and Hullender, G. Learning to rank using\ngradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pp. 89\u201396, 2005.\nBurges, C., Ragno, R., and Le, Q. Learning to rank with\nnonsmooth cost functions. In Sch\u00a8olkopf, B., Platt, J.,\nand Hoffman, T. (eds.), Advances in Neural Information\nProcessing Systems, volume 19. MIT Press, 2006.\nBurges, C. J. From ranknet to lambdarank to lambdamart:\nAn overview. Learning, 11(23-581):81, 2010.\nCao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, H. Learning\nto rank: from pairwise approach to listwise approach.\nIn Proceedings of the 24th international conference on\nMachine learning, pp. 129\u2013136, 2007.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nDonmez, P., Svore, K. M., and Burges, C. J. On the local\noptimality of lambdarank. In Proceedings of the 32nd\nInternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR \u201909, pp.\n460\u2013467, New York, NY, USA, 2009. Association for\nComputing Machinery. ISBN 9781605584836. doi: 10.\n1145/1571941.1572021. URL https://doi.org/\n10.1145/1571941.1572021.\nGoogle, Anil, R., Dai, A. M., Firat, O., Johnson, M., Lep-\nikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P.,\nChen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang,\nY., Meier-Hellstern, K., Mishra, G., Moreira, E., Omer-\nnick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K.,\nXu, Y., Zhang, Y., Abrego, G. H., Ahn, J., Austin, J.,\nBarham, P., Botha, J., Bradbury, J., Brahma, S., Brooks,\nK., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo,\nC. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani,\nM., Dev, S., Devlin, J., D\u00b4\u0131az, M., Du, N., Dyer, E.,\nFeinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia,\nX., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S.,\nHashemi, H., Hou, L., Howland, J., Hu, A., Hui, J., Hur-\nwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W.,\nKenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee,\nK., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H.,\nLin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru, A.,\nMaynez, J., Misra, V., Moussalem, M., Nado, Z., Nham,\nJ., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek,\nM., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter,\nB., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R.,\nShelby, R., Slone, A., Smilkov, D., So, D. R., Sohn, D.,\nTokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K.,\nWang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu,\nY., Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q.,\nZheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and\nWu, Y. PaLM 2 technical report, 2023.\nJagerman, R., Qin, Z., Wang, X., Bendersky, M., and Najork,\nM. On optimizing top-k metrics for neural ranking mod-\nels. In Proceedings of the 45th International ACM SIGIR\nConference on Research and Development in Information\nRetrieval, pp. 2303\u20132307, 2022a.\nJagerman, R., Wang, X., Zhuang, H., Qin, Z., Bendersky,\nM., and Najork, M. Rax: Composable learning-to-rank\nusing jax. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining,\npp. 3051\u20133060, 2022b.\nJoachims, T. Optimizing search engines using clickthrough\ndata. In Proceedings of the eighth ACM SIGKDD inter-\nnational conference on Knowledge discovery and data\nmining, pp. 133\u2013142, 2002.\nJoachims, T., Swaminathan, A., and Schnabel, T. Unbiased\nlearning-to-rank with biased feedback. In Proceedings of\nthe tenth ACM international conference on web search\nand data mining, pp. 781\u2013789, 2017.\nKaratzoglou, A., Baltrunas, L., and Shi, Y. Learning to\nrank for recommender systems. In Proceedings of the 7th\nACM Conference on Recommender Systems, pp. 493\u2013494,\n2013.\nK\u00a8opf, A., Kilcher, Y., von R\u00a8utte, D., Anagnostidis, S.,\nTam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations\u2013\ndemocratizing large language model alignment. arXiv\npreprint arXiv:2304.07327, 2023.\nLee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T.,\nBishop, C., Carbune, V., and Rastogi, A. Rlaif: Scal-\n9\nLiPO: Listwise Preference Optimization through Learning-to-Rank\ning reinforcement learning from human feedback with ai\nfeedback. arXiv preprint arXiv:2309.00267, 2023.\nLiu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M.,\nLiu, P. J., and Liu, J.\nStatistical rejection sam-\npling improves preference optimization. arXiv preprint\narXiv:2309.06657, 2023.\nLiu, T.-Y. Learning to rank for information retrieval. Found.\nTrends Inf. Retr., 2009.\nLuce, R. D.\nIndividual choice behavior: A theoretical\nanalysis. Courier Corporation, 2005.\nOpenAI. Gpt-4 technical report, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nPlackett, R. L. The analysis of permutations. Journal of the\nRoyal Statistical Society Series C: Applied Statistics, 24\n(2):193\u2013202, 1975.\nQin, Z., Yan, L., Zhuang, H., Tay, Y., Pasumarthi, R. K.,\nWang, X., Bendersky, M., and Najork, M. Are neural\nrankers still outperformed by gradient boosted decision\ntrees? In International Conference on Learning Repre-\nsentations, 2020.\nQin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen,\nJ., Liu, T., Liu, J., Metzler, D., Wang, X., et al. Large\nlanguage models are effective text rankers with pairwise\nranking prompting. arXiv preprint arXiv:2306.17563,\n2023.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,\nErmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023.\nShah, N. B. and Wainwright, M. J. Simple, robust and\noptimal ranking from pairwise comparisons. Journal of\nmachine learning research, 18(199):1\u201338, 2018.\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\nrates with sublinear memory cost. In Proceedings of the\n35th International Conference on Machine Learning, pp.\n4596\u20134604, 2018.\nShu, L., Luo, L., Hoskere, J., Zhu, Y., Liu, C., Tong, S.,\nChen, J., and Meng, L. Rewritelm: An instruction-tuned\nlarge language model for text rewriting. arXiv preprint\narXiv:2305.15685, 2023.\nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang,\nH. Preference ranking optimization for human alignment.\narXiv preprint arXiv:2306.17492, 2023.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano,\nP. F. Learning to summarize with human feedback. Ad-\nvances in Neural Information Processing Systems, 33:\n3008\u20133021, 2020.\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu,\nJ., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al.\nGemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805, 2023.\nWang, X., Li, C., Golbandi, N., Bendersky, M., and Najork,\nM. The lambdaloss framework for ranking metric opti-\nmization. In Proceedings of the 27th ACM international\nconference on information and knowledge management,\npp. 1313\u20131322, 2018.\nWang, Y., Wang, L., Li, Y., He, D., and Liu, T.-Y. A theoreti-\ncal analysis of ndcg type ranking measures. In Conference\non learning theory, pp. 25\u201354. PMLR, 2013.\nXia, F., Liu, T.-Y., Wang, J., Zhang, W., and Li, H. List-\nwise approach to learning to rank: theory and algorithm.\nIn Proceedings of the 25th international conference on\nMachine learning, pp. 1192\u20131199, 2008.\nYu, J., Tao, D., Wang, M., and Rui, Y. Learning to rank\nusing user clicks and visual features for image retrieval.\nIEEE Transactions on Cybernetics, 45(4):767\u2013779, 2015.\ndoi: 10.1109/TCYB.2014.2336697.\nYuan, H., Yuan, Z., Tan, C., Wang, W., Huang, S., and\nHuang, F. RRHF: Rank responses to align language mod-\nels with human feedback. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\nZhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and\nLiu, P. J. Slic-hf: Sequence likelihood calibration with\nhuman feedback. arXiv preprint arXiv:2305.10425, 2023.\n10\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nA. AutoSxS Details\nA.0.1. DETAILS\nThe purpose of the AutoSxS is to prevent the artificially high reward scores by Reward Model due to reward hacking on\nlearned policies. Since the policy is trained using the information in the pairwise reward-ranking model, it is not necessary\nthe higher the win rate on reward-ranking model, the better the policy. AutoSxS uses PaLM 2-L-IT few-shot in-context\nlearning to infer 8 decoded samples with 4 flipped order of response A and B. The label contains three choices: A, B, and tie\nwith score 1, 0, and 0.5, respectively. To ensure the robustness, we use average score to determine the win or loss if the\nmagnitude exceeds 0.35. The AutoSxS has been demonstrated as effective and consistent in DPO using GPT-4 as zero-shot\nrater. In this work, we replace GPT-4 with PaLM 2-L-IT for our evaluation using few-shot prompts. The quality of PaLM\n2-L-IT on similar tasks has been shown to be close to human raters (Lee et al., 2023; Shu et al., 2023). The systematic study\non consistency and quality of AutoSxS is beyond the scope of this work.\nA.0.2. REDDIT TL;DR FEW-SHOT PROMPTS\ntask: Judge the quality of two TLDRs, choose the options among (A), (B) or same.\ncontext: I\u2019ve (M[21]) been in a relationship for a year and a half with F[22] and it really has never gone well. I think we\nwant different things and we are not overly compatible. I broke up with her about a year ago and she tried to kill herself so\nwe got back together. This week I met an F[19] who I think I\u2019m really compatible with. She and I talked for a few hours and\nwe have a lot in common. I like her a lot, but she is currently a freshman and I am currently a senior so I will be graduating\nin May and going on to a prestigious PhD program starting next fall.\nSo here are my questions: * What should I do in regards to my current relationship? I know I need to end it, but I just don\u2019t\nknow how. * What should I do in regards to the other girl? * Do you think my feelings for the other girl stem from my\ndistaste for my current relationship?\nI appreciate any help you give me.\ntldr (A): I\u2019m unhappy in my current relationship with a girl I just met, but don\u2019t know how to end it. I have no idea what\nI\u2019m doing or what to do.\ntldr (B): M[21] unhappy in relationship with F[22]. Met an F[19] in town with similar interests and I really like her. What\nshould I do in regards to current relationship/other girl?\nexplanation: tldr (A)\u2019s second and third sentences convey similar idea and are redundant. tldr (B) mentions an important\npiece of information of the new girl, contains more details than tldr (A) and is concise at the same time.\nchoose among (A), (B) or same: (B)\ncontext: Before anything, not a sad story or anything, I don\u2019t think she\u2019s cheating or anything of the sorts. My country\u2019s\nequivalent to Valentine\u2019s Day is coming and I had this pretty simple idea to surprise my girlfriend and it would involve\ngiving her some roses. The thing is, although I know she would appreciate my intention in and of itself, I don\u2019t know if she\nwould like the actual flowers and such, so I wanted to find out if she likes roses and if she would like getting some, but\nwithout her realizing it so as not to spoil the surprise. Any ideas on how to get that information out of her? tldr (A): How do\nI find out if my girlfriend likes roses without her realizing it?\ntldr (B): I want to surprise my girlfriend with some flowers when Valentine\u2019s Day is around the corner, but I don\u2019t know if\nshe would like the flowers or flowers themselves without her knowing.\nexplanation: tldr (A) is a concise that captures the main idea. tldr (B) also captures the main point with more details, but the\nlanguage \u2019flowers or flowers themselves\u2019 is not fluent.\nchoose among (A), (B) or same: (A)\ncontext: Okay, so my younger brothers were out and about when they passed some teenagers who yelled obscenities at\nthem. My father then went over and told them to knock it off, when they started yelling obscenities at him. My dad, with a\nsmall amount of temper, got angry and yelled at them. They started recording it and made a video on YouTube where it\nlooked like he was just screaming at them. After that, we were able to get it taken down only to have it reuploaded with\nblurred faces. We have in no way given consent to be in this video. Is there any way we can get them to take it doen?\ntldr (A): my dad got angry at teenagers for yelling obscenities at him, they got a video on youtube and blurred faces, what\ncan we do to get it taken down?\ntldr (B): My brothers were being verbally harassed by kids, father yelled at them, they made a video of it to get the video\ntaken down, it was like a blur with blurred faces.\n11\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nexplanation: tldr (A) mentions most main points of story while skipping some details like younger brothers being yelled at\nand original videos taken down. tldr (B) has a major factual error, they didn\u2019t make a video to get the video taken down.\nchoose among (A), (B) or same: (A)\ncontext: Apologize for the throw away account.\nMy friend is interested in in making his way into the mainstream music industry as an A&R representative. He has a 4 year\ndegree in English but did some extra curricular activities around music business in college. He also has had 2 internships out\nof college with small time record labels in the Chicago area. He has been trying for a few years now with no success. He\nlives in Chicago but has moved to LA for a year. He returned recently to Chicago. It has been about 3 years since his last\ninternship, but he has done some online collaboration stuff. Still far and few in between.\nMy question is what path have any of you taken to make your way into the record industry? Is networking the only important\nthing? Is there a set of skills that are specifically looked for?\ntldr (A): My friend wants to make his way into the music industry as an A&R representative. What does he need to know to\nmake it happen?\ntldr (B): friend wants to break into the music industry as an A&R rep. what is the best way to do this?\nexplanation: tldr (A) and (B) are basically paraphrase of each other. With (A) slightly more verbose and (B) slightly more\nconcise.\nchoose among (A), (B) or same: same\ncontext: I was at a concert this weekend supporting my favorite artist. Most of the amphitheater was standing, however\n*some* people in my section were not (including some people behind me and to my right). All of a sudden at the end of\nthe set I got basically shoved in the back and told to sit down. I told them absolutely not, I want to support my favorite\nartist (who kept saying STAND UP, btw). I paid over $100 for these seats and I want to have a great time.. after all this IS a\nconcert. Not to mention, I heard them calling me names before they confronted me so now I was certainly not going to sit\ndown. Then a few songs later the woman sitting next to me told me what a bitch I was for not letting the people behind me\nsee. Was I in the wrong here?\ntldr (A): I stood up at a concert, people in front of me didn\u2019t and confronted me. Was I in the wrong?\ntldr (B): I stood up at a concert even though I was told not to. I got yelled at by a woman who was sitting next to me. Was I\nin the wrong?\nexplanation: tldr (A) doesn\u2019t say who confronted the author or wrongly implied it was the people in front. tldr (B) is correct\nbut could mention that more people confronted the author than the woman sitting next.\nchoose among (A), (B) or same: (B)\nA.0.3. ANTHROPICHH FEW-SHOT PROMPTS\ntask: For the following query to a chatbot, which response is more helpful? Choose among (A), (B) and same.\ncontext: Human: Why can\u2019t humans build a space station on the moon? Assistant: That\u2019s a really good question. We don\u2019t\nknow yet if we\u2019ll be able to build a space station on the moon, though it seems quite feasible. But we can also look at the\nscience for something more ambitious, like a spaceship that\u2019s large enough to make a round trip to Mars. That might be hard\nto do! Even with the technology we have now, it\u2019s not clear to scientists how you\u2019d build a spaceship that would travel for\nmonths in space. Human: I think we should start on the Moon first.\nresponse (A): Assistant: I think you\u2019re right that the Moon is a good place to start, since it\u2019s a natural satellite of Earth.\nThere are a lot of interesting places on the Moon to study, but we have to go to Mars to check out the atmosphere, the\nconditions on the ground, and the survival potential of humans, and to see if there are living things.\nresponse (B): Assistant: The problem is that we don\u2019t know yet if it\u2019s even possible to build a space station on the moon.\nBut let\u2019s assume we could do it, and see how far we could get with a spaceship to Mars.\nexplanation: Response (A) offers a more expansive and engaged exploration of the user\u2019s sentiment about starting with the\nMoon, while Response (B) reiterates previous information without addressing the user\u2019s perspective or adding new insights.\nchoose among (A), (B) and same: (A)\ncontext: Human: What is garam masala?\nAssistant: Garam masala is a mixture of spices, typically consisting of cumin, pepper, garlic, and either coriander, ginger, or\nboth. It\u2019s frequently used in curries or vegetable dishes, but it\u2019s also added to some vegetable or meat sides, or used as a\nspread.\n12\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nHuman: What other things can I add to it to make it unique?\nAssistant: Try adding some of the following ingredients, in any combination you like: turmeric cinnamon cardamom celery\nseeds fennel fresh chilies, like jalape\u02dcnos or Thai chiles grated ginger green onions basil mint\nHuman: Is there a sweet garam masala?\nAssistant: Yes, some garam masasalas are sweetened. They might contain sugar or honey, or they might be just made with\nground cinnamon or cardamom. You might also want to mix some sort of mango or pineapple relish into your garam masala.\nHuman: What is a good use for sweet garam masala?\nresponse (A): Assistant: You might use it to flavor vegetable or meat stews, or chilies, or as a spread on toast. You might\nmix some into yogurt or fruit salad, or serve it with nuts and chutney on crackers.\nresponse (B): Assistant: Sweet garam masala is typically used in desserts like ice cream or fudge, or in sweet curries like\nrisotto or k\u00a8ofte. You can also use it in curries to add a sweet flavor. You can also use it as a spice rub for meats, like chicken\nor fish. You can also use it as a spice mix in a curry, or you can use it to make a sweet sauce for a curry.\nexplanation: Response (B) offers specific and diverse uses of sweet garam masala, allowing the user to get a clearer idea of\nits potential applications in cooking.\nchoose among (A), (B) and same: (B)\ncontext: Human: What is the customary amount to tip a taxi driver?\nresponse (A): Assistant: The standard tip for taxi drivers in the United States is typically a 20% tip. This is a fairly standard\ntip that is given to taxi drivers for a job well done, and it\u2019s also a nice gesture to show your appreciation for their service. The\ntip can be more or less depending on the driver\u2019s service and the level of professionalism they showed. If you\u2019re considering\ntipping a taxi driver, I would recommend giving them a tip of at least 20%.\nresponse (B): Assistant: A standard 10% tip is recommended.\nexplanation: Response (A) provides a clear recommendation, contextualizes it to a specific country (the U.S.), and offers\nadditional guidance based on the quality of service. It\u2019s more comprehensive than response (B).\nchoose among (A), (B) and same: (A)\nB. HumanSxS Details\nFor task examples see Figure 5 and Figure 6 for Reddit TL;DR and AnthropicHH respectively.\nC. Python codes for loss computation\nAlgorithm 1 illustrates how to compute each ranking loss in Python using Jax framework.\n13\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nFigure 5. Example of human evaluation task on Reddit TL;DR dataset.\nAlgorithm 1 Computing different ranking losses in Python\nimport rax\nimport jax.numpy as jnp\nscores = jnp.array([[2.0, 1.0, 3.0]])\nlabels = jnp.array([[1.0, 0.0, 0.0]])\npoint_mse_loss = rax.pointwise_mse_loss(scores, labels)\npoint_sigmoid_loss = rax.pointwise_sigmoid_loss(scores, labels)\npair_hinge_loss = rax.pairwise_hinge_loss(scores, labels)\npair_logistic_loss = rax.pairwise_logistic_loss(scores, labels)\nlist_mle_loss = rax.listmle_loss(scores, labels)\nsoftmax_loss = rax.softmax_loss(scores, labels)\nlambda_loss = rax.pairwise_logistic_loss(\nscores, labels, lambdaweight_fn=rax.dcg_lambdaweight\n)\n14\nLiPO: Listwise Preference Optimization through Learning-to-Rank\nFigure 6. Example of human evaluation task on AnthropicHH dialogue dataset.\n15\n"
  },
  {
    "title": "Rethinking Interpretability in the Era of Large Language Models",
    "link": "https://arxiv.org/pdf/2402.01761.pdf",
    "upvote": "17",
    "text": "Rethinking Interpretability in the Era of Large Language Models\nChandan Singh 1 Jeevana Priya Inala 1 Michel Galley 1 Rich Caruana 1 Jianfeng Gao 1\nAbstract\nInterpretable machine learning has exploded as\nan area of interest over the last decade, sparked\nby the rise of increasingly large datasets and deep\nneural networks. Simultaneously, large language\nmodels (LLMs) have demonstrated remarkable\ncapabilities across a wide array of tasks, offering\na chance to rethink opportunities in interpretable\nmachine learning. Notably, the capability to ex-\nplain in natural language allows LLMs to expand\nthe scale and complexity of patterns that can be\ngiven to a human. However, these new capabil-\nities raise new challenges, such as hallucinated\nexplanations and immense computational costs.\nIn this position paper, we start by reviewing ex-\nisting methods to evaluate the emerging field of\nLLM interpretation (both interpreting LLMs and\nusing LLMs for explanation). We contend that,\ndespite their limitations, LLMs hold the opportu-\nnity to redefine interpretability with a more ambi-\ntious scope across many applications, including\nin auditing LLMs themselves. We highlight two\nemerging research priorities for LLM interpreta-\ntion: using LLMs to directly analyze new datasets\nand to generate interactive explanations.\n1. Introduction\nMachine learning (ML) and natural language processing\n(NLP) have seen a rapid expansion in recent years, due\nto the availability of increasingly large datasets and pow-\nerful neural network models. In response, the field of in-\nterpretable ML* has grown to incorporate a diverse array\nof techniques and methods for understanding these models\nand datasets1\u20133. One part of this expansion has focused on\nthe development and use of inherently interpretable mod-\nels 4, such as sparse linear models, generalized additive\nmodels, and decision trees. Alongside these models, post-\n1Microsoft Research. Correspondence to: Chandan Singh\n<chansingh@microsoft.com>.\n*We use the terms interpretable, explainable, and transparent\ninterchangeably.\nhoc interpretability techniques have become increasingly\nprominent, offering insights into predictions after a model\nhas been trained. Notable examples include methods for\nassessing feature importance5, 6, and broader post-hoc tech-\nniques, e.g., model visualizations7, 8, or interpretable distil-\nlation9, 10.\nMeanwhile, pre-trained large language models (LLMs) have\nshown impressive proficiency in a range of complex NLP\ntasks, significantly advancing the field and opening new\nfrontiers for applications11\u201313. However, the inability to\neffectively interpret these models has debilitated their use in\nhigh-stakes applications such as medicine and raised issues\nrelated to regulatory pressure, safety, and alignment14\u201316.\nMoreover, this lack of interpretability has limited the use of\nLLMs (and other neural-network models) in fields such as\nscience and data analysis17\u201319. In these settings, the end\ngoal is often to elicit a trustworthy interpretation, rather than\nto deploy an LLM.\nIn this work, we contend that LLMs hold the opportunity to\nrethink interpretability with a more ambitious scope. LLMs\ncan elicit more elaborate explanations than the previous\ngeneration of interpretable ML techniques. While previous\nmethods have often relied on restricted interfaces such as\nsaliency maps, LLMs can communicate directly in expres-\nsive natural language. This allows users to make targeted\nqueries, such as Can you explain your logic?, Why didn\u2019t\nyou answer with (A)?, or Explain this data to me., and get\nimmediate, relevant responses. We believe simple questions\nsuch as these, coupled with techniques for grounding and\nprocessing data, will allow LLMs to articulate previously in-\ncomprehensible model behaviors and data patterns directly\nto humans in understandable text. However, unlocking these\nopportunities requires tackling new challenges, including\nhallucinated (i.e. incorrect or baseless) explanations, along\nwith the immense size, cost, and inherent opaqueness of\nmodern LLMs.\nContributions and overview\nWe evaluate LLM interpre-\ntation and highlight emerging research priorities, taking a\nbroader scope than recent works, e.g., those focused on\nexplaining LLM predictions20, mechanistic interpretabil-\nity21, social science19, or science more generally17, 22, 23.\nRather than providing an exhaustive overview of methods,\nwe highlight the aspects of interpretability that are unique to\n1\narXiv:2402.01761v1  [cs.CL]  30 Jan 2024\nRethinking Interpretability in the Era of LLMs\n(C) Explain a dataset\nInterpretable model\n  e.g. Linear model of ngrams\nChain of LLMs\n  e.g. Tree of prompts\nAiding data analysis\n  e.g. Find problematic samples\nInteractive NL explanation\n  e.g. Short string\n(Explain data insights)\n(B) Explain an LLM\nLocal explanation\n(Explain a generation)\nGlobal/mechanistic explanation\n(Explain entire model)\nData grounding\n  e.g. RAG\nData influence\n   e.g. Influence function\nFeature attribution\n  e.g. SHAP\nAttribution for LLM internals\n   e.g. Attention head importance\nPrediction decomposition\n  e.g. Chain of thought\nNL explanation\n  e.g. LLM explanations\nExplanations for LLM internals\n  e.g. Attention heads summaries\nAlgorithmic understanding\n  e.g. Circuit analysis\nHallucination\nSize, opacity, and cost\nNatural-language interface\nInteractive explanations\nChallenges\nOpportunities\n(A) \n(D) Themes\nAttribution\nNatural \nlanguage (NL)\nDecomposing \nreasoning\nData \ngrounding\nFigure 1: Categorization of LLM interpretation research. (A) LLMs raise unique opportunities and challenges for\ninterpretation (Sec. 3). (B) Explaining an LLM can be categorized into methods that seek to explain a single generation\nfrom an LLM (i.e. local explanation, Sec. 4.1) or the LLM in its entirety (i.e. global/mechanistic explanation, Sec. 4.2).\nLocal explanation methods build on many techniques that were originally developed for interpreting non-LLM models, such\nas feature attribution methods. More recent local explanation techniques use LLMs themselves to yield interpretations, e.g.,\nthrough post-hoc natural language (NL) explanations, asking an LLM to build explanations into its generation process, or\nthrough data grounding. Similar techniques have been developed and applied to global explanation, although it also includes\nunique types of explanations, e.g., analyzing individual attention heads or circuits inside an LLM. (C) Sec. 5 analyzes the\nemerging area that uses an LLM to aid in directly explaining a dataset. In this setting, an LLM is given a new dataset (which\ncan consist of either text or tabular features) and is used to help analyze it. LLM-based techniques for dataset explanation are\nquite diverse, including helping to build interpretable models, generate NL explanations, generate chains of NL explanations,\nor construct data visualizations. (D) Common themes emerge among methods for local explanation, global explanation, and\ndataset explanation.\nLLMs and showcase them with practically useful methods.\nSpecifically, we begin with a background and defini-\ntions (Sec. 2) before proceeding to analyze the unique op-\nportunities and challenges that LLMs present for interpre-\ntation (Sec. 3). We then ground these opportunities in two\ncomplementary categories for LLM-based interpretation\n(see Fig. 1). The first is generating explanations for an ex-\nisting LLM (Sec. 4), which is useful for auditing a model\u2019s\nperformance, alignment, fairness, etc. The second is ex-\nplaining a dataset (Sec. 5); in this setting, an LLM is used\nto help analyze a new dataset (which can consist of either\ntext or tabular features).\nThroughout the paper, we highlight dataset explanation and\ninteractive explanation as emerging research priorities. To-\ngether, these two areas have great potential real-world signif-\nicance in domains from science to statistics, where they can\nfacilitate the process of scientific discovery, data analysis,\nand model building. Throughout, we focus on pre-trained\nLLMs, mostly applied to text data, but also applied to tabular\ndata.\n2. Background: definitions and evaluation\nDefinitions\nWithout context, interpretability is a poorly\ndefined concept. Precisely defining interpretability requires\nunderstanding the problem and audience an interpretation is\nintended to serve. In light of this imprecision, interpretable\nML has largely become associated with a narrow set of\ntechniques, including feature attribution, saliency maps, and\ntransparent models. However, LLM interpretation is broader\nin scope and more expressive than these methods. Here, we\nparaphrase the definition of interpretable ML from a prior\nwork2 to define LLM interpretation as the extraction of\nrelevant knowledge from an LLM concerning relationships\neither contained in data or learned by the model. We empha-\n2\nRethinking Interpretability in the Era of LLMs\nsize that this definition applies to both interpreting an LLM\nand to using an LLM to generate explanations. Moreover,\nthe definition relies on the extraction of relevant knowledge,\ni.e., knowledge that is useful for a particular problem and\naudience. For example, in a code generation context, a rel-\nevant interpretation may help a user quickly integrate an\nLLM-generated code snippet. In contrast, a relevant inter-\npretation in a medical diagnosis setting may inform a user\nwhether or not a prediction is trustworthy.\nThe term large language model (LLM) is often used impre-\ncisely. Here, we use it to refer to transformer-based neural\nlanguage models that contain tens to hundreds of billions of\nparameters, and which are pre-trained on massive text data,\ne.g., PaLM24, LLaMA12, and GPT-413. Compared to early\npre-trained language models, such as BERT, LLMs are not\nonly much larger, but also exhibit stronger language under-\nstanding, generation abilities, and explanation capabilities.\nAfter an initial computationally intensive pre-training stage,\nLLMs often undergo instruction finetuning and further align-\nment with human preferences to improve instruction follow-\ning25 or to improve interactive chat capabilities, e.g., the\nLLaMA-2 chat model12. They are sometimes also further\nadapted via supervised finetuning to improve performance\nin a specific domain, such as medicine26.\nAfter undergoing these steps, LLMs are often used with\nprompting, the most common interface for applying LLMs\n(and our main focus in this paper). In prompting, a text\nprompt is directly fed to an LLM and used to generate\nsubsequent output text. Few-shot prompting is a type of\nprompting that involves providing an LLM with a small\nnumber of examples to allow it to better understand the task\nit is being asked to perform.\nEvaluating LLM interpretations\nSince different inter-\npretations are relevant to different contexts, the ideal way\nto evaluate an interpretation is by studying whether its us-\nage in a real-world setting with humans improves a desired\noutcome 27. In contrast, simply measuring human judg-\nment of explanations is not particularly useful, as it may\nnot translate into improvements in practice28. A recent\nmeta-analysis finds that introducing NLP explanations into\nsettings with humans yields widely varying utilities, ranging\nfrom completely unhelpful to very useful29. An important\npiece of this evaluation is the notion of complementarity30,\ni.e., that explanations should help LLMs complement hu-\nman performance in a team setting, rather than improve their\nperformance in isolation.\nWhile human studies provide the most realistic evaluation,\nautomated metrics (that can be computed without involv-\ning humans) are desirable to ease and scale evaluation, es-\npecially in mechanistic interpretability. An increasingly\npopular approach is to use LLMs themselves in evaluation,\nalthough great care must be taken to avoid introducing bi-\nases, e.g., an LLM systematically scoring its own outputs\ntoo positively31. One way to reduce bias is to use LLMs\nas part of a structured evaluation process tailored to a par-\nticular problem, rather than directly querying LLMs for\nevaluation scores. For example, one common setting is eval-\nuating a natural-language interpretation of a given function\n(which may be any component of a pre-trained LLM). In\nthis setting, one can evaluate an explanation\u2019s ability to sim-\nulate the function\u2019s behavior32, the function\u2019s output on\nLLM-generated synthetic data33, or its ability to recover a\ngroundtruth function34, 35. In a question-answering setting,\nmany automated metrics have been proposed for measuring\nthe faithfulness of a natural-language explanation for an\nindividual answer to a question36\u201338.\nA final avenue for evaluating interpretations is through their\nability to alter/improve model performance in useful ways.\nThis approach provides strong evidence for the utility of\nan explanation, although it does not encompass all criti-\ncal use cases of interpretability (particularly those directly\ninvolving human interaction). Model improvements can\ntake various forms, the simplest of which is simply improv-\ning accuracy at downstream tasks. For example, few-shot\naccuracy was seen to improve when aligning an LLM\u2019s\nrationales with explanations generated using post-hoc ex-\nplanation methods39 or explanations distilled from large\nmodels 40. Moreover, employing few-shot explanations\nduring inference (not training) can significantly improve\nfew-shot LLM accuracy, especially when these explanations\nare further optimized41, 42. Beyond general performance,\nexplanations can be used to overcome specific shortcom-\nings of a model. For example, one line of work identifies\nand addresses shortcuts/spurious correlations learned by an\nLLM43\u201345. Model editing, a related line of work, enables\nprecise modifications to certain model behaviors, enhancing\noverall performance46\u201348.\n3. Unique opportunities and challenges of\nLLM interpretation\nUnique opportunities of LLM interpretation\nFirst\namong LLM interpretation opportunities is the ability to\nprovide a natural-language interface to explain complex\npatterns. This interface is very familiar to humans, poten-\ntially ameliorating the difficulties that practitioners often\nface when using explainability techniques49, 50. Addition-\nally, natural language can be used to build a bridge between\nhumans and a range of other modalities, e.g., DNA, chem-\nical compounds, or images51\u201353, that may be difficult for\nhumans to interpret on their own. In these cases, natural\nlanguage allows for expressing complex concepts through\nexplanations at different levels of granularity, potentially\ngrounded in evidence or discussions of counterfactuals.\n3\nRethinking Interpretability in the Era of LLMs\nA second major opportunity is the ability for LLMs to gen-\nerate interactive explanations. Interactivity allows users to\ntailor explanations to their unique needs, e.g., by asking\nfollow-up questions and performing analysis on related ex-\namples. Interviews with decision-makers, including physi-\ncians and policymakers, indicate that they strongly prefer\ninteractive explanations, particularly in the form of natural-\nlanguage dialogues54. Interactivity further allows LLM\nexplanations to be decomposed into many different LLM\ncalls, each of which can be audited independently. This can\nbe enabled in different ways, e.g., having a user repeatedly\nchat with an LLM using prompting, or providing a user a\nsequence of LLM calls and evidence to analyze.\nUnique challenges of LLM interpretation\nThese oppor-\ntunities bring new challenges. First and foremost is the\nissue of hallucination, i.e. incorrect or baseless explana-\ntions. Flexible explanations provided in natural language\ncan quickly become less grounded in evidence, whether\nthe evidence is present in a given input or presumed to\nbe present in the knowledge an LLM has learned from its\ntraining data. Hallucinated explanations are unhelpful or\neven misleading, and thus techniques for identifying and\ncombating hallucination are critical to the success of LLM\ninterpretation.\nA second challenge is the immensity and opaqueness of\nLLMs. Models have grown to contain tens or hundreds of\nbillions of parameters11, 12, and continue to grow in size.\nThis makes it infeasible for a human to inspect or even\ncomprehend the units of an LLM. Moreover, it necessitates\nefficient algorithms for interpretation, as even generating a\nsingle token from an LLM often incurs a non-trivial com-\nputational cost. In fact, LLMs are often too large to be run\nlocally or can be accessed only through a proprietary text\nAPI, necessitating the need for interpretation algorithms that\ndo not have full access to the model (e.g., no access to the\nmodel weights or the model gradients).\n4. Explaining an LLM\nIn this section, we study techniques for explaining an LLM,\nincluding explaining a single generation from an LLM\n(Sec. 4.1) or an LLM in its entirety (Sec. 4.2). We eval-\nuate both traditional interpretable ML techniques and LLM-\nbased techniques for explaining an LLM.\n4.1. Local explanation\nLocal explanation, i.e., explaining a single generation from\nan LLM, has been a major focus in the recent interpretability\nliterature. It allows for understanding and using LLMs in\nhigh-stakes scenarios, e.g., healthcare.\nThe simplest approach for providing local explanations in\nLLMs provides feature attributions for input tokens. These\nfeature attributions assign a relevance score to each input\nfeature, reflecting its impact on the model\u2019s generated out-\nput. Various attribution methods have been developed, in-\ncluding perturbation-based methods6, gradient-based meth-\nods55, 56, and linear approximations5. Recently, these meth-\nods have been specifically adapted for transformer models,\naddressing unique challenges such as discrete token embed-\ndings57, 58 and computational costs59. Moreover, the con-\nditional distribution learned by an LLM can be used to en-\nhance existing attribution methods, e.g., by performing input\nmarginalization60. Besides feature attributions, attention\nmechanisms within an LLM offer another avenue for visual-\nizing token contributions to an LLM generation61, though\ntheir faithfulness/effectiveness remains unclear62. Inter-\nestingly, recent work suggests that LLMs themselves can\ngenerate post-hoc attributions of important features through\nprompting63. This approach could be extended to enable\neliciting different feature attributions that are relevant in\ndifferent contexts.\nBeyond token-level attributions, LLMs can also generate\nlocal explanations directly in natural language. While the\ngeneration of natural-language explanations predates the cur-\nrent era of LLMs (e.g., in text classification64, 65 or image\nclassification66), the advent of more powerful models has\nsignificantly enhanced their effectiveness. Natural-language\nexplanations generated by LLMs have shown the ability to\nelucidate model predictions, even simulating counterfactual\nscenarios67, and expressing nuances like uncertainty68\u201370.\nDespite their potential benefits, natural language explana-\ntions remain extremely susceptible to hallucination or inac-\ncuracies, especially when generated post-hoc71, 72.\nOne starting point for combating these hallucinations is\nintegrating an explanation within the answer-generation pro-\ncess itself. Chain-of-thought prompting exemplifies this\napproach73, where an LLM is prompted to articulate its rea-\nsoning step-by-step before arriving at an answer. This rea-\nsoning chain generally results in more accurate and faithful\noutcomes, as the final answer is more aligned with the pre-\nceding logical steps. The robustness of this method can be\ntested by introducing perturbations in the reasoning process\nand observing the effects on the final output74\u201376. Alterna-\ntive methods for generating this reasoning chain exist, such\nas tree-of-thoughts77, which extends chain-of-thought to\ninstead generate a tree of thoughts used in conjunction with\nbacktracking, graph-of-thoughts78, and others79\u201381. All of\nthese methods not only help convey an LLM\u2019s intermediate\nreasoning to a user, but also help the LLM to follow the rea-\nsoning through prompting, often enhancing the reliability\nof the output. However, like all LLM-based generations, the\nfidelity of these explanations can vary76, 82.\nAn alternative path to reducing hallucinations during gener-\n4\nRethinking Interpretability in the Era of LLMs\nation is to employ retrieval-augmented generation (RAG).\nIn RAG, an LLM incorporates a retrieval step in its decision-\nmaking process, usually by searching a reference corpus or\nknowledge base using text embeddings83, 84 (see review85).\nThis allows the information that is used to generate an output\nto be specified and examined explicitly, making it easier to\nexplain the evidence an LLM uses during decision-making.\n4.2. Global and mechanistic explanation\nRather than studying individual generations, global / mech-\nanistic explanations aim to understand an LLM as a whole.\nThese explanations can help to audit a model for concerns\nbeyond generalization, e.g., bias, privacy, and safety, help-\ning to build LLMs that are more efficient / trustworthy, They\ncan also yield mechanistic understanding about how LLMs\nfunction. To do so, researchers have focused on summariz-\ning the behaviors and mechanisms of LLMs through various\nlenses. Generally, these works require access to model\nweights and do not work for explaining models that are only\naccessible through a text API, e.g., GPT-413.\nOne popular method for understanding neural-network rep-\nresentations is probing.\nProbing techniques analyze a\nmodel\u2019s representation either by decoding embedded infor-\nmation, e.g., syntax86, or by testing the model\u2019s capabilities\nthrough precisely designed tasks, e.g., subject-verb agree-\nment87, 88. In the context of LLMs, probing has evolved\nto include the analysis of attention heads89, embeddings90,\nand different controllable aspects of representations91. It\nalso includes methods that directly decode an output token\nto understand what is represented at different positions and\nlayers92, 93. These methods can provide a deeper under-\nstanding of the nuanced ways in which LLMs process and\nrepresent information.\nIn addition to probing, many works study LLM representa-\ntions at a more granular level. This includes categorizing\nor decoding concepts from individual neurons94, 95 or di-\nrectly explaining the function of attention heads in natural\nlanguage 32, 33, 96. Beyond individual neurons, there is\ngrowing interest in understanding how groups of neurons\ncombine to perform specific tasks, e.g., finding a circuit\nfor indirect object identification97, for entity binding98, or\nfor multiple shared purposes99. More broadly, this type of\nanalysis can be applied to localize functionalities rather than\nfully explain a circuit, e.g., localizing factual knowledge\nwithin an LLM46, 100. A persistent problem with these\nmethods is that they are difficult to scale to immense LLMs,\nleading to research in (semi)-automated methods that can\nscale to today\u2019s largest LLMs101, 102.\nA complementary approach to mechanistic understanding\nuses miniature LLMs as a test bed for investigating complex\nphenomena. For example, examining a 2-layer transformer\nmodel reveals information about what patterns are learned\nby attention heads as a function of input statistics103 or\nhelps identify key components, such as induction heads or\nngram heads that copy and utilize relevant tokens104, 105.\nThis line of mechanistic understanding places a particular\nfocus on studying the important capability of in-context\nlearning, i.e., given a few input-output examples in a prompt,\nan LLM can learn to correctly generate an output for a new\ninput106, 107.\nA related area of research seeks to interpret an LLM by\nunderstanding the influence of its training data distribution.\nUnlike other methods we have discussed, this requires ac-\ncess to an LLM\u2019s training dataset, which is often unknown or\ninaccessible. In the case that the data is known, researchers\ncan employ techniques such as influence functions to iden-\ntify important elements in the training data108. They can\nalso study how model behaviors arise from patterns in train-\ning data, such as hallucination in the presence of long-tail\ndata109, in the presence of repeated training data110, or\nstatistical patterns that contradict proper reasoning111.\nAll these interpretation techniques can be improved via\nLLM-based interactivity, allowing a user to investigate dif-\nferent model components via follow-up queries and altered\nprompts from a user. For example, one recent work in-\ntroduces an end-to-end framework for explanation-based\ndebugging and improvement of text models, showing that\nit can quickly yield improvements in text-classification per-\nformance 112. Another work, Talk2Model, introduces a\nnatural-language interface that allows users to interrogate a\ntabular prediction model through a dialog, implicitly calling\nmany different model explainability tools, such as calcu-\nlating feature importance113.\u2020 More recent work extends\nTalk2Model to a setting interrogating an LLM about its\nbehavior114.\nFinally, the insights gained from mechanistic understanding\nare beginning to inform practical applications, with current\nareas of focus including model editing 46, improving in-\nstruction following115, and model compression116. These\nareas simultaneously serve as a sanity check on many mech-\nanistic interpretations and as a useful path to enhancing the\nreliability of LLMs.\n5. Explaining a dataset\nAs LLMs improve their context length and capabilities, they\ncan be leveraged to explain an entire dataset, rather than\nexplaining an LLM or its generations. This can aid with data\nanalysis, knowledge discovery, and scientific applications.\nFig. 2 shows an overview of dataset explanations at differ-\nent levels of granularity, which we cover in detail below.\nWe distinguish between tabular and text data, but note that\n\u2020Note that Talk2Model focuses on interpreting prediction mod-\nels rather than LLMs.\n5\nRethinking Interpretability in the Era of LLMs\nmost methods can be successfully applied to either, or both\nsimultaneously in a multimodal setting.\nTabular data\nOne way LLMs can aid in dataset expla-\nnation is by making it easier to interactively visualize and\nanalyze tabular data. This is made possible by the fact\nthat LLMs can simultaneously understand code, text, and\nnumbers by treating them all as input tokens. Perhaps the\nmost popular method in this category is ChatGPT Code\nInterpreter\u2021, which enables uploading datasets and building\nvisualizations on top of them through an interactive text\ninterface. This capability is part of a broader trend of LLM-\naided visualization, e.g., suggesting automatic visualizations\nfor dataframes117, helping to automate data wrangling118,\nor even conducting full-fledged data analysis119. These ca-\npabilities benefit from a growing line of work that analyzes\nhow to effectively represent and process tabular data with\nLLMs120\u2013122.\nLLMs can also help explaining datasets by directly analyz-\ning models that have been fit to tabular data Unlike mech-\nanistic interpretability, where the goal is to understand the\nmodel, in dataset explanation, the goal is to understand\npatterns in the data through the model (although similar\ntechniques can be used for both problems). For example,\none recent work uses LLMs to analyze generalized additive\nmodels (GAMs) that are fit to tabular data123. GAMs are in-\nterpretable models that can be represented as a set of curves,\neach representing the contribution of a feature to the output\nprediction as a function of the feature\u2019s value. An LLM can\nanalyze the fitted model (and thereby the underlying dataset)\nby processing each curve as a set of numerical tokens and\nthen detecting and describing patterns in each curve. The\nauthors find that LLMs can identify surprising character-\nistics in the curves and the underlying data, largely based\non their prior knowledge of a domain. Rather than using\nan interpretable GAM model, another approach is to distill\ndataset insights by analyzing classifier predictions. For ex-\nample, MaNtLE generates natural-language descriptions of\na classifier\u2019s rationale based on the classifier\u2019s predictions,\nand these explanations are found to identify explainable\nsubgroups that contain similar feature patterns124.\nText data\nText data poses different challenges for dataset\nexplanation than tabular data because it is sparse, high-\ndimensional, and modeling it requires many high-order\ninteractions. As a result, interpretable models that have\nbeen successful in the tabular domain (e.g., sparse linear\nmodels125, 126, GAMs127\u2013129, decision trees130\u2013132, and\nothers 133, 134), have struggled to accurately model text.\nOne recent line of work addresses this issue by using LLMs\nto help build fully interpretable text models, such as linear\n\u2021https://openai.com/blog/chatgpt-plugins#\ncode-interpreter\nmodels or decision trees135; the resulting models are sur-\nprisingly accurate, often outperforming even much larger\nLLM models. These interpretable models can help explain\na dataset by showing which features (i.e. words or ngrams)\nare important for predicting different outcomes. Similar\nmethods, e.g., CHiLL136 use LLMs to build interpretable\nrepresentations for text classification tasks.\nGoing beyond fully interpretable models, LLMs also help\nin building partially interpretable text models. Partially\ninterpretable text models often employ chains of prompts;\nthese chains allow for decomposing an LLM\u2019s decision-\nmaking process to analyze which dataset patterns a model\nlearns. Prompt chains are usually constructed by humans\nor by querying a model to generate a chain of calls on-the-\nfly137. For dataset explanation, the most relevant chains are\nsequences of explanations that are generated by an LLM. For\nexample, a model can generate a single tree of explanations\nthat is shared across all examples in a dataset, a process\nthat enables understanding hierarchical structures stored\nwithin a dataset138. Rather than a tree, a single chain of\nprompts can often help an LLM employ self-verification,\ni.e. the model itself checks its previous generations using a\nchain of prompts, a popular technique that often improves\nreliability139\u2013141. As in local explanation, an LLM can\nincorporate a retrieval step in its decision-making process85,\nand access to different tools can help make different steps\n(e.g., arithmetic) more reliable and transparent142.\nNatural-language explanations hold the potential to produce\nrich, concise descriptions of patterns present in a dataset,\nbut are prone to hallucination. One method, iPrompt143,\naims to avoid hallucination by searching for a dataset ex-\nplanation in the form of a single prompt, and verifying that\nthe prompt induces an LLM to accurately predict a pattern\nin the underlying dataset. Related methods use LLMs to\nprovide descriptions that differentiate between groups in\na dataset, followed by an LLM that verifies the credibility\nof the description35, 144, 145. In addition to a raw natural-\nlanguage explanation, LLMs can aid in summarizing textual\ninformation, e.g., through explainable clustering of a text\ndataset146 or creating prompt-based topic models147.\n6. Future research priorities\nWe now highlight research priorities surrounding LLM in-\nterpretation in three areas: explanation reliability, dataset\nexplanation, and interactive explanations.\nExplanation reliability\nAll LLM explanations are bottle-\nnecked by reliability issues. This includes hallucinations148,\nbut encompasses a broader set of issues. For example, LLMs\ncontinue to be very sensitive to the nuances of prompt phras-\ning; minor variations in prompts can completely change the\nsubstance of an LLM output149, 150. Additionally, LLMs\n6\nRethinking Interpretability in the Era of LLMs\nDataset\nNatural-language \nexplanation\nTo get the output from the \ninput, identify if it\u2019s a \npositive movie review\nChain of \nexplanations\nLow-level\nHigh-level\nInterpretable \ntext models\nTabular data \nvisualizations\nSummarized \ntabular models\nngram1\nngram3\nngram4\nngram2\nprompt1\nprompt3\nprompt4\nprompt2\n(Tabular or text)\nThis should be \ndecreasing\nFigure 2: Dataset explanations at different levels of granularity. Dataset explanation involves understanding a new\ndataset (consisting of either text or tabular features) using a pre-trained LLM. Low-level explanations are more faithful to\nthe dataset but involve more human effort to extract meaningful insights. Many dataset interpretations use prediction models\n(classification or regression) as a means to identify and explain patterns between features.\nmay ignore parts of their context, e.g., the middle of long\ncontexts151 or instructions that are difficult to parse115.\nThese reliability issues are particularly critical in interpre-\ntation, which often uses explanations to mitigate risk in\nhigh-stakes settings. One work analyzing explanation re-\nliably finds that LLMs often generate seemingly correct\nexplanations that are actually inconsistent with their own\noutputs on related questions71, preventing a human prac-\ntitioner from trusting an LLM or understanding how its\nexplanations apply to new scenarios. Another study finds\nthat explanations generated by an LLM may not entail the\nmodel\u2019s predictions or be factually grounded in the input,\neven on simple tasks with extractive explanations72. Future\nwork will be required to improve the grounding of explana-\ntions and develop stronger methods to test their reliability,\nperhaps through methods such as self-verification139, it-\nerative prompting143, or automatically improving model\nself-consistency152\u2013154.\nDataset explanation for knowledge discovery\nDataset\nexplanation using LLMs (Sec. 5) holds the potential to help\nwith the generation and discovery of new knowledge from\ndata17, 22, 23, rather than simply helping to speed up data\nanalysis or visualization. Dataset explanation could initially\nhelp at the level of brainstorming scientific hypotheses that\ncan then be screened or tested by human researchers155.\nDuring and after this process, LLM explanations can help\nwith using natural language to understand data from other-\nwise opaque domains, such as chemical compounds156 or\nDNA sequences51. In the algorithms domain, LLMs have\nbeen used to uncover new algorithms, translating them to hu-\nmans as readable computer programs157. These approaches\ncould be combined with data from experiments to help yield\nnew data-driven insights.\nLLM explanations can also be used to help humans better\nperform a task. Explanations from transformers have already\nbegun to be applied to domains such as Chess, where their\nexplanations can help improve even expert players158. Ad-\nditionally, LLMs can provide explanations of expert human\nbehavior, e.g. \u201cWhy did the doctor prescribe this medication\ngiven this information about the patient?\u201d, that are helpful in\nunderstanding, auditing, and improving human behavior159.\nInteractive explanations\nFinally, advancements in LLMs\nare poised to allow for the development of more user-centric,\ninteractive explanations. LLM explanations and follow-up\nquestions are already being integrated into a variety of LLM\napplications, such as interactive task specification160, rec-\nommendation161, and a wide set of tasks involving dialog.\nFurthermore, works like Talk2Model113 enable users to\ninteractively audit models in a conversational manner. This\ndialog interface could be used in conjunction with many of\nthe methods covered in this work to help with new applica-\ntions, e.g., interactive dataset explanation.\n7. Conclusions\nIn this paper, we have explored the vast and dynamic land-\nscape of interpretable ML, particularly focusing on the\nunique opportunities and challenges presented by LLMs.\nLLMs\u2019 advanced natural language generation capabilities\nhave opened new avenues for generating more elaborate\nand nuanced explanations, allowing for a deeper and more\naccessible understanding of complex patterns in data and\nmodel behaviors. As we navigate this terrain, we assert\nthat the integration of LLMs into interpretative processes is\nnot merely an enhancement of existing methodologies but a\ntransformative shift that promises to redefine the boundaries\nof machine learning interpretability.\nOur position is anchored in the belief that the future of\ninterpretable ML hinges on our ability to harness the full\npotential of LLMs. To this end, we outlined several key\nstances and directions for future research, such as enhancing\nexplanation reliability and advancing dataset interpretation\nfor knowledge discovery. As LLMs continue to improve\nrapidly, these explanations (and all the methods discussed\n7\nRethinking Interpretability in the Era of LLMs\nin this work) will advance correspondingly to enable new\napplications and insights. In the near future, LLMs may be\nable to offer the holy grail of interpretability: explanations\nthat can reliably aggregate and convey extremely complex\ninformation to us all.\nReferences\n[1] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of\ninterpretability. arXiv preprint arXiv:1702.08608, 2017. ,\u21921.\n[2] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin\nYu. Definitions, methods, and applications in interpretable machine learning.\nProceedings of the National Academy of Sciences of the United States of\nAmerica, 116(44):22071\u201322080, 2019. ,\u21922.\n[3] Christoph Molnar. Interpretable machine learning. Lulu. com, 2019. ,\u21921.\n[4] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova,\nand Chudi Zhong. Interpretable machine learning: Fundamental principles\nand 10 grand challenges. arXiv preprint arXiv:2103.11251, 2021. ,\u21921.\n[5] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust\nyou?: Explaining the predictions of any classifier. In Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 1135\u20131144. ACM, 2016. ,\u21921 and 4\n[6] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model\npredictions. In Advances in Neural Information Processing Systems, pages\n4768\u20134777, 2017. ,\u21921 and 4\n[7] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.\nUnderstanding neural networks through deep visualization. arXiv preprint\narXiv:1506.06579, 2015. ,\u21921.\n[8] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenen-\nbaum, William T Freeman, and Antonio Torralba. GAN dissection: Visu-\nalizing and understanding generative adversarial networks. arXiv preprint\narXiv:1811.10597, 2018. ,\u21921.\n[9] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare:\nAuditing black-box models using transparent model distillation. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages\n303\u2013310, 2018. ,\u21921.\n[10] Wooseok Ha, Chandan Singh, Francois Lanusse, Srigokul Upadhyayula, and\nBin Yu. Adaptive wavelet distillation from neural networks through interpreta-\ntions. Advances in Neural Information Processing Systems, 34:20669\u201320682,\n2021. ,\u21921.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020. ,\u21921 and 4\n[12] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. ,\u21923 and 4\n[13] OpenAI. GPT-4 technical report, 2023. ,\u21921, 3, and 5\n[14] Bryce Goodman and Seth Flaxman.\nEuropean union regulations on al-\ngorithmic decision-making and a\u201d right to explanation\u201d.\narXiv preprint\narXiv:1606.08813, 2016. ,\u21921.\n[15] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schul-\nman, and Dan Man\u00b4e.\nConcrete problems in AI safety.\narXiv preprint\narXiv:1606.06565, 2016. (Not cited.)\n[16] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and\nmachines, 30(3):411\u2013437, 2020. ,\u21921.\n[17] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming\nLiu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al.\nScientific discovery in the age of artificial intelligence. Nature, 620(7972):47\u2013\n60, 2023. ,\u21921 and 7\n[18] Enkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00a8uchemann, Maria Bannert, Daryna\nDementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00a8unnemann,\nEyke H\u00a8ullermeier, et al. ChatGPT for good? on opportunities and challenges\nof large language models for education. Learning and individual differences,\n103:102274, 2023. (Not cited.)\n[19] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and\nDiyi Yang. Can large language models transform computational social science?\narXiv preprint arXiv:2305.03514, 2023. ,\u21921.\n[20] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large\nlanguage models: A survey. arXiv preprint arXiv:2309.01029, 2023. ,\u21921.\n[21] Tilman R\u00a8auker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. To-\nward transparent AI: A survey on interpreting the inner structures of deep\nneural networks. In 2023 IEEE Conference on Secure and Trustworthy Ma-\nchine Learning (SaTML), pages 464\u2013483. IEEE, 2023. ,\u21921.\n[22] Abeba Birhane, Atoosa Kasirzadeh, David Leslie, and Sandra Wachter. Sci-\nence in the age of large language models. Nature Reviews Physics, pages 1\u20134,\n2023. ,\u21921 and 7\n[23] Luca Pion-Tonachini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert,\nW Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Haruko Wainwright, Ghan-\nshyam Pilania, Benjamin Nachman, et al. Learning from learning machines: a\nnew generation of AI technology to meet the needs of science. arXiv preprint\narXiv:2111.13786, 2021. ,\u21921 and 7\n[24] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gau-\nrav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.\nJournal of Machine Learning Research, 24(240):1\u2013113, 2023. ,\u21923.\n[25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\net al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n,\u21923.\n[26] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,\nKevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards\nexpert-level medical question answering with large language models. arXiv\npreprint arXiv:2305.09617, 2023. ,\u21923.\n[27] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,\nFernanda Viegas, and Rory Sayres. Interpretability beyond feature attribution:\nQuantitative testing with concept activation vectors (tcav). arXiv preprint\narXiv:1711.11279, 2017. ,\u21923.\n[28] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz\nHardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural\nInformation Processing Systems, pages 9505\u20139515, 2018. ,\u21923.\n[29] Fateme Hashemi Chaleshtori, Atreya Ghosal, and Ana Marasovi\u00b4c. On evalu-\nating explanation utility for Human-AI decision-making in NLP. In XAI in\nAction: Past, Present, and Future Applications, 2023. ,\u21923.\n[30] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,\nEce Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed\nits parts? the effect of AI explanations on complementary team performance.\nIn Proceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems, pages 1\u201316, 2021. ,\u21923.\n[31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-\nbench and chatbot arena. In Thirty-seventh Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2023. ,\u21923.\n[32] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel\nGoh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language\nmodels can explain neurons in language models, 2023. ,\u21923 and 5\n[33] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G\nHuth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural\nlanguage with language models. arXiv preprint arXiv:2305.09863, 2023. ,\u21923\nand 5\n8\nRethinking Interpretability in the Era of LLMs\n[34] Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowd-\nhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba. FIND: A\nfunction description benchmark for evaluating interpretability methods. arXiv\ne-prints, pages arXiv\u20132309, 2023. ,\u21923.\n[35] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob\nSteinhardt. Goal driven discovery of distributional differences via language\ndescriptions. ArXiv, abs/2302.14233, 2023. ,\u21923 and 6\n[36] Pepa Atanasova,\nOana-Maria Camburu,\nChristina Lioma,\nThomas\nLukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. Faithfulness\ntests for natural language explanations. arXiv preprint arXiv:2305.18029,\n2023. ,\u21923.\n[37] Letitia Parcalabescu and Anette Frank. On measuring faithfulness of natural\nlanguage explanations. arXiv preprint arXiv:2311.07466, 2023. (Not cited.)\n[38] Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and\nSwabha Swayamdipta. Rev: information-theoretic evaluation of free-text\nrationales. arXiv preprint arXiv:2210.04982, 2022. ,\u21923.\n[39] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer\nSingh, and Himabindu Lakkaraju. Post hoc explanations of language models\ncan improve language models. arXiv preprint arXiv:2305.11426, 2023. ,\u21923.\n[40] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal,\nHamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from\ncomplex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707, 2023.\n,\u21923.\n[41] Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson,\nMichael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang,\nand Felix Hill. Can language models learn from explanations in context?\narXiv preprint arXiv:2204.02329, 2022. ,\u21923.\n[42] Xi Ye and Greg Durrett. Explanation selection using unlabeled data for chain-\nof-thought prompting. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 619\u2013637, 2023. ,\u21923.\n[43] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. Shortcut\nlearning of large language models in natural language understanding. Commu-\nnications of the ACM (CACM), 2023. ,\u21923.\n[44] Cheongwoong Kang and Jaesik Choi. Impact of co-occurrence on factual\nknowledge of large language models. arXiv preprint arXiv:2310.08256, 2023.\n(Not cited.)\n[45] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and\nKatja Filippova. \u201dWill you find these shortcuts?\u201d a protocol for evaluating the\nfaithfulness of input salience methods for text classification. arXiv preprint\narXiv:2111.07367, 2021. ,\u21923.\n[46] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and\nediting factual knowledge in GPT. arXiv preprint arXiv:2202.05262, 2022.\n,\u21923 and 5\n[47] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D.\nManning. Fast model editing at scale, 2022. (Not cited.)\n[48] Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing\nknowledge representations in language models, 2023. ,\u21923.\n[49] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wal-\nlach, and Jennifer Wortman Vaughan. Interpreting interpretability: under-\nstanding data scientists\u2019 use of interpretability tools for machine learning.\nIn Proceedings of the 2020 CHI conference on human factors in computing\nsystems, pages 1\u201314, 2020. ,\u21923.\n[50] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible\nintelligence. Communications of the ACM, 62(6):70\u201379, 2019. ,\u21923.\n[51] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony\nHartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Sto-\njnic.\nGalactica: A large language model for science.\narXiv preprint\narXiv:2211.09085, 2022. ,\u21923 and 7\n[52] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao,\nLing Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal\nmolecule structure-text model for text-based retrieval and editing. ArXiv,\nabs/2212.10789, 2022. (Not cited.)\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning, pages 8748\u20138763. PMLR,\n2021. ,\u21923.\n[54] Himabindu Lakkaraju, Dylan Slack, Yuxin Chen, Chenhao Tan, and Sameer\nSingh. Rethinking explainability as a dialogue: A practitioner\u2019s perspective.\narXiv preprint arXiv:2202.01875, 2022. ,\u21924.\n[55] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for\ndeep networks. ICML, 2017. ,\u21924.\n[56] Gr\u00b4egoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech\nSamek, and Klaus-Robert M\u00a8uller. Explaining nonlinear classification de-\ncisions with deep taylor decomposition. Pattern Recognition, 65:211\u2013222,\n2017. ,\u21924.\n[57] Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese.\nIntegrated\ndirectional gradients: Feature interaction attribution for neural nlp models. In\nProceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 865\u2013878, 2021. ,\u21924.\n[58] Joseph Enguehard. Sequential integrated gradients: a simple but effective\nmethod for explaining language models. arXiv preprint arXiv:2305.15853,\n2023. ,\u21924.\n[59] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. Algorithms\nto estimate shapley value feature attributions. Nature Machine Intelligence,\npages 1\u201312, 2023. ,\u21924.\n[60] Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon. Interpretation of nlp\nmodels through input marginalization. arXiv preprint arXiv:2010.13984, 2020.\n,\u21924.\n[61] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv\npreprint arXiv:1908.04626, 2019. ,\u21924.\n[62] Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv preprint\narXiv:1902.10186, 2019. ,\u21924.\n[63] Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and\nHimabindu Lakkaraju. Are large language models post hoc explainers? arXiv\npreprint arXiv:2310.05797, 2023. ,\u21924.\n[64] Oana-Maria Camburu, Tim Rockt\u00a8aschel, Thomas Lukasiewicz, and Phil Blun-\nsom. e-snli: Natural language inference with natural language explanations.\nAdvances in Neural Information Processing Systems, 31, 2018. ,\u21924.\n[65] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.\nExplain yourself! leveraging language models for commonsense reasoning.\narXiv preprint arXiv:1906.02361, 2019. ,\u21924.\n[66] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt\nSchiele, and Trevor Darrell. Generating visual explanations. In European\nconference on computer vision, pages 3\u201319. Springer, 2016. ,\u21924.\n[67] Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. LLMs\nas counterfactual explanation modules: Can ChatGPT explain black-box text\nclassifiers? arXiv preprint arXiv:2309.13340, 2023. ,\u21924.\n[68] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and\nBryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of\nconfidence elicitation in LLMs. arXiv preprint arXiv:2306.13063, 2023. ,\u21924.\n[69] Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. Quantifying\nuncertainty in natural language explanations of large language models. arXiv\npreprint arXiv:2311.03533, 2023. (Not cited.)\n[70] Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. Relying on the\nunreliable: The impact of language models\u2019 reluctance to express uncertainty,\n2024. ,\u21924.\n[71] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Stein-\nhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves?\nCounterfactual simulatability of natural language explanations. arXiv preprint\narXiv:2307.08678, 2023. ,\u21924 and 7\n[72] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompt-\ning for textual reasoning. Advances in neural information processing systems,\n35:30378\u201330392, 2022. ,\u21924 and 7\n9\nRethinking Interpretability in the Era of LLMs\n[73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei\nXia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting\nelicits reasoning in large language models. In Alice H. Oh, Alekh Agar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural\nInformation Processing Systems, 2022. ,\u21924.\n[74] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain\nof thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.\n,\u21924.\n[75] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettle-\nmoyer, and Huan Sun. Towards understanding chain-of-thought prompting:\nAn empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.\n(Not cited.)\n[76] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson E.\nDenison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, John\nKernion, Kamil.e Lukovsiut.e, Karina Nguyen, Newton Cheng, Nicholas\nJoseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Samuel McCandlish,\nSandipan Kundu, Saurav Kadavath, Shannon Yang, T. J. Henighan, Timo-\nthy D. Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds,\nJared Kaplan, Janina Brauner, Sam Bowman, and Ethan Perez. Measuring\nfaithfulness in chain-of-thought reasoning. ArXiv, abs/2307.13702, 2023.\n,\u21924.\n[77] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan\nCao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving\nwith large language models. arXiv preprint arXiv:2305.10601, 2023. ,\u21924.\n[78] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gi-\naninazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert\nNiewiadomski, Piotr Nyczyk, et al. Graph of Thoughts: Solving elaborate\nproblems with large language models. arXiv preprint arXiv:2308.09687, 2023.\n,\u21924.\n[79] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob\nAustin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David\nLuan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads\nfor intermediate computation with language models. ArXiv, abs/2112.00114,\n2021. ,\u21924.\n[80] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and\nMike Lewis. Measuring and narrowing the compositionality gap in language\nmodels, 2022. (Not cited.)\n[81] Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi\nWang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-\nto-most prompting enables complex reasoning in large language models. arXiv\npreprint arXiv:2205.10625, 2022. ,\u21924.\n[82] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun\nChen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do\nin-context learning differently. arXiv preprint arXiv:2303.03846, 2023. ,\u21924.\n[83] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei\nChang. REALM: Retrieval-augmented language model pre-training. ArXiv,\nabs/2002.08909, 2020. ,\u21925.\n[84] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu,\nQiuyuan Huang, Lars Lid\u00b4en, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check\nyour facts and try again: Improving large language models with external\nknowledge and automated feedback. ArXiv, abs/2302.12813, 2023. ,\u21925.\n[85] Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and\nCarlos Guestrin. Unifying corroborative and contributive attributions in large\nlanguage models. arXiv preprint arXiv:2311.12233, 2023. ,\u21925 and 6\n[86] Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00a8\u0131c Barrault, and\nMarco Baroni. What you can cram into a single vector: Probing sentence\nembeddings for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.\n,\u21925.\n[87] Frederick Liu and Besim Avci. Incorporating priors with feature attribution\non text classification. arXiv preprint arXiv:1906.08286, 2019. ,\u21925.\n[88] Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language\nmodels. arXiv preprint arXiv:1808.09031, 2018. ,\u21925.\n[89] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.\nWhat does BERT look at? An analysis of bert\u2019s attention. arXiv preprint\narXiv:1906.04341, 2019. ,\u21925.\n[90] John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M\nRush. Text embeddings reveal (almost) as much as text. arXiv preprint\narXiv:2310.06816, 2023. ,\u21925.\n[91] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard\nRen, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dom-\nbrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex\nMallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico\nKolter, and Dan Hendrycks. Representation engineering: A top-down ap-\nproach to AI transparency. ArXiv, abs/2310.01405, 2023. ,\u21925.\n[92] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev\nMcKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions\nfrom transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.\n,\u21925.\n[93] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor\nGeva. PatchScope: A unifying framework for inspecting hidden representa-\ntions of language models, 2024. ,\u21925.\n[94] Jesse Mu and Jacob Andreas. Compositional explanations of neurons. Ad-\nvances in Neural Information Processing Systems, 33:17153\u201317163, 2020.\n,\u21925.\n[95] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii,\nand Dimitris Bertsimas. Finding neurons in a haystack: Case studies with\nsparse probing. arXiv preprint arXiv:2305.01610, 2023. ,\u21925.\n[96] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio\nTorralba, and Jacob Andreas. Natural language descriptions of deep visual\nfeatures. In International Conference on Learning Representations, 2022.\n,\u21925.\n[97] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and\nJacob Steinhardt. Interpretability in the wild: a circuit for indirect object\nidentification in GPT-2 small. arXiv preprint arXiv:2211.00593, 2022. ,\u21925.\n[98] Jiahai Feng and Jacob Steinhardt. How do language models bind entities in\ncontext? arXiv preprint arXiv:2310.17191, 2023. ,\u21925.\n[99] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse\nacross tasks in transformer language models. arXiv preprint arXiv:2310.08744,\n2023. ,\u21925.\n[100] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu\nWei.\nKnowledge neurons in pretrained transformers.\narXiv preprint\narXiv:2104.08696, 2021. ,\u21925.\n[101] Tom Lieberum, Matthew Rahtz, J\u00b4anos Kram\u00b4ar, Geoffrey Irving, Rohin\nShah, and Vladimir Mikulik. Does circuit analysis interpretability scale?\nEvidence from multiple choice capabilities in chinchilla.\narXiv preprint\narXiv:2307.09458, 2023. ,\u21925.\n[102] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman.\nInterpretability at scale: Identifying causal mechanisms in Alpaca. ArXiv,\nabs/2305.08809, 2023. ,\u21925.\n[103] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas\nJoseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,\net al. A mathematical framework for transformer circuits. Transformer Circuits\nThread, 1, 2021. ,\u21925.\n[104] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-\nSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna\nChen, et al.\nIn-context learning and induction heads.\narXiv preprint\narXiv:2209.11895, 2022. ,\u21925.\n[105] Ekin Aky\u00a8urek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context\nlanguage learning: Arhitectures and algorithms, 2024. ,\u21925.\n[106] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What\ncan transformers learn in-context? A case study of simple function classes.\nAdvances in Neural Information Processing Systems, 35:30583\u201330598, 2022.\n,\u21925.\n[107] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi,\nJosh Susskind, Samy Bengio, and Preetum Nakkiran.\nWhat algorithms\ncan transformers learn? a study in length generalization. arXiv preprint\narXiv:2310.16028, 2023. ,\u21925.\n10\nRethinking Interpretability in the Era of LLMs\n[108] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhos-\nsein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al.\nStudying large language model generalization with influence functions. arXiv\npreprint arXiv:2308.03296, 2023. ,\u21925.\n[109] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.\nLarge language models struggle to learn long-tail knowledge. In International\nConference on Machine Learning, pages 15696\u201315707. PMLR, 2023. ,\u21925.\n[110] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain,\nSheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan\nHume, et al. Scaling laws and interpretability of learning from repeated data.\narXiv preprint arXiv:2205.10487, 2022. ,\u21925.\n[111] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark\nJohnson, and Mark Steedman. Sources of hallucination by large language\nmodels on inference tasks. arXiv preprint arXiv:2305.14552, 2023. ,\u21925.\n[112] Dong-Ho Lee, Akshen Kadakia, Brihi Joshi, Aaron Chan, Ziyi Liu, Kiran\nNarahari, Takashi Shibuya, Ryosuke Mitani, Toshiyuki Sekiya, Jay Pujara,\net al. XMD: An end-to-end framework for interactive explanation-based\ndebugging of nlp models. arXiv preprint arXiv:2210.16978, 2022. ,\u21925.\n[113] Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, and Sameer Singh.\nTalktomodel: Understanding machine learning models with open ended dia-\nlogues. arXiv preprint arXiv:2207.04154, 2022. ,\u21925 and 7\n[114] Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leon-\nhard Hennig, and Sebastian M\u00a8oller. LLMCheckup: Conversational exam-\nination of large language models via interpretability tools. arXiv preprint\narXiv:2401.12576, 2024. ,\u21925.\n[115] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng\nGao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention\nsteering for LLMs. arXiv preprint arXiv:2311.02262, 2023. ,\u21925 and 7\n[116] Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. The truth is in there:\nImproving reasoning in language models with layer-selective rank reduction.\narXiv preprint arXiv:2312.13558, 2023. ,\u21925.\n[117] Victor Dibia. Lida: A tool for automatic generation of grammar-agnostic\nvisualizations and infographics using large language models. arXiv preprint\narXiv:2303.02927, 2023. ,\u21926.\n[118] Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, and Christopher R\u00b4e.\nCan foundation models wrangle your data? arXiv preprint arXiv:2205.09911,\n2022. ,\u21926.\n[119] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Benchmarking large\nlanguage models as AI research agents. arXiv preprint arXiv:2310.03302,\n2023. ,\u21926.\n[120] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-\nGPT: Table-tuned gpt for diverse table tasks. arXiv preprint arXiv:2310.09263,\n2023. ,\u21926.\n[121] Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, and Jiang Bian.\nTo-\nwards foundation models for learning on tabular data.\narXiv preprint\narXiv:2310.07338, 2023. (Not cited.)\n[122] Tianping Zhang, Shaowen Wang, Shuicheng Yan, Jian Li, and Qian Liu.\nGenerative table pre-training empowers models for tabular prediction. arXiv\npreprint arXiv:2305.09696, 2023. ,\u21926.\n[123] Benjamin J Lengerich, Sebastian Bordt, Harsha Nori, Mark E Nunnally, Yin\nAphinyanaphongs, Manolis Kellis, and Rich Caruana. LLMs understand\nglass-box models, discover surprises, and suggest repairs. arXiv preprint\narXiv:2308.01157, 2023. ,\u21926.\n[124] Rakesh R Menon, Kerem Zaman, and Shashank Srivastava. MaNtLE: Model-\nagnostic natural language explainer. arXiv preprint arXiv:2305.12995, 2023.\n,\u21926.\n[125] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal\nof the Royal Statistical Society. Series B (Methodological), pages 267\u2013288,\n1996. ,\u21926.\n[126] Berk Ustun and Cynthia Rudin. Supersparse linear integer models for op-\ntimized medical scoring systems. Machine Learning, 102:349\u2013391, 2016.\n,\u21926.\n[127] Trevor Hastie and Robert Tibshirani. Generalized additive models. Statistical\nScience, 1(3):297\u2013318, 1986. ,\u21926.\n[128] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intel-\nligible models with pairwise interactions. In Proceedings of the 19th ACM\nSIGKDD international conference on Knowledge discovery and data mining,\npages 623\u2013631, 2013. (Not cited.)\n[129] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and\nNoemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk\nand hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD\ninternational conference on knowledge discovery and data mining, pages\n1721\u20131730, 2015. ,\u21926.\n[130] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and\nRegression Trees. Wadsworth and Brooks, Monterey, CA, 1984. ,\u21926.\n[131] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81\u2013106,\n1986. (Not cited.)\n[132] Abhineet Agarwal, Yan Shuo Tan, Omer Ronen, Chandan Singh, and Bin\nYu. Hierarchical shrinkage: improving the accuracy and interpretability of\ntree-based methods. arXiv:2202.00858, 2 2022. arXiv: 2202.00858. ,\u21926.\n[133] Chandan Singh, Keyan Nasseri, Yan Shuo Tan, Tiffany Tang, and Bin Yu.\nimodels: a python package for fitting interpretable models. Journal of Open\nSource Software, 6(61):3192, 2021. ,\u21926.\n[134] Yan Shuo Tan, Chandan Singh, Keyan Nasseri, Abhineet Agarwal, and Bin\nYu. Fast interpretable greedy-tree sums (figs). arXiv:2201.11931 [cs, stat], 1\n2022. arXiv: 2201.11931. ,\u21926.\n[135] Chandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting\ninterpretable models with large language models during training. Nature\nCommunications, 14(1):7913, 2023. ,\u21926.\n[136] Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, and By-\nron C Wallace. Chill: Zero-shot custom interpretable feature extraction from\nclinical notes with large language models. arXiv preprint arXiv:2302.12343,\n2023. ,\u21926.\n[137] Madeleine Grunde-McLaughlin, Michelle S Lam, Ranjay Krishna, Daniel S\nWeld, and Jeffrey Heer. Designing LLM chains by adapting techniques from\ncrowdsourcing workflows. arXiv preprint arXiv:2312.11681, 2023. ,\u21926.\n[138] John X Morris, Chandan Singh, Alexander M Rush, Jianfeng Gao, and Yuntian\nDeng. Tree prompting: efficient task adaptation without fine-tuning. arXiv\npreprint arXiv:2310.14034, 2023. ,\u21926.\n[139] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang,\nand William Yang Wang. Automatically correcting large language models:\nSurveying the landscape of diverse self-correction strategies. arXiv preprint\narXiv:2308.03188, 2023. ,\u21926 and 7\n[140] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean\nWelleck, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative refine-\nment with self-feedback, 2023. (Not cited.)\n[141] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley,\nJianfeng Gao, and Hoifung Poon. Self-verification improves few-shot clinical\ninformation extraction. arXiv preprint arXiv:2306.00024, 2023. ,\u21926.\n[142] Gr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram\nPasunuru, Roberta Raileanu, Baptiste Rozi`ere, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint\narXiv:2302.07842, 2023. ,\u21926.\n[143] Chandan Singh, John X. Morris, Jyoti Aneja, Alexander M. Rush, and Jianfeng\nGao. Explaining patterns in data with language models via interpretable\nautoprompting, 2023. ,\u21926 and 7\n[144] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing\ndifferences between text distributions with natural language. In Kamalika\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan\nSabato, editors, Proceedings of the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine Learning Research, pages\n27099\u201327116. PMLR, 17\u201323 Jul 2022. ,\u21926.\n11\nRethinking Interpretability in the Era of LLMs\n[145] Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: A framework for explain-\ning distribution shifts in natural language. arXiv preprint arXiv:2206.15007,\n2022. ,\u21926.\n[146] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clus-\ntering via language descriptions. arXiv preprint arXiv:2305.13749, 2023.\n,\u21926.\n[147] Chau Minh Pham, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. TopicGPT:\nA prompt-based topic modeling framework. arXiv preprint arXiv:2311.01449,\n2023. ,\u21926.\n[148] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman\nChadha, and Amitava Das. A comprehensive survey of hallucination mitiga-\ntion techniques in large language models. arXiv preprint arXiv:2401.01313,\n2024. ,\u21926.\n[149] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.\nQuantify-\ning language models\u2019 sensitivity to spurious features in prompt design or:\nHow i learned to start worrying about prompt formatting. arXiv preprint\narXiv:2310.11324, 2023. ,\u21926.\n[150] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language\nmodels don\u2019t always say what they think: Unfaithful explanations in chain-of-\nthought prompting. arXiv preprint arXiv:2305.04388, 2023. ,\u21926.\n[151] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. Lost in the middle: How language models\nuse long contexts. ArXiv, abs/2307.03172, 2023. ,\u21927.\n[152] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He,\nand Jianfeng Gao. Towards consistent natural-language explanations via\nexplanation-consistency finetuning, 2024. ,\u21927.\n[153] Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, and\nPercy Liang. Benchmarking and improving generator-validator consistency of\nlanguage models. arXiv preprint arXiv:2310.01846, 2023. (Not cited.)\n[154] Afra Feyza Aky\u00a8urek, Ekin Aky\u00a8urek, Leshem Choshen, Derry Wijaya, and\nJacob Andreas. Deductive closure training of language models for coherence,\naccuracy, and updatability. arXiv preprint arXiv:2401.08574, 2024. ,\u21927.\n[155] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik\nCambria. Large language models for automated open-domain scientific hy-\npotheses discovery. arXiv preprint arXiv:2309.02726, 2023. ,\u21927.\n[156] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling\nLiu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal\nmolecule structure\u2013text model for text-based retrieval and editing. Nature\nMachine Intelligence, 5(12):1447\u20131457, 2023. ,\u21927.\n[157] Bernardino Romera-Paredes,\nMohammadamin Barekatain,\nAlexander\nNovikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R.\nRuiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and\nAlhussein Fawzi. Mathematical discoveries from program search with large\nlanguage models. Nature, 2023. ,\u21927.\n[158] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet,\nand Been Kim. Bridging the Human-AI knowledge gap: Concept discovery\nand transfer in alphazero. arXiv preprint arXiv:2310.16410, 2023. ,\u21927.\n[159] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryu-\ntaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al.\nTowards conversational diagnostic ai. arXiv preprint arXiv:2401.05654, 2024.\n,\u21927.\n[160] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting\nhuman preferences with language models. arXiv preprint arXiv:2310.11589,\n2023. ,\u21927.\n[161] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie.\nRecommender AI agent: Integrating large language models for interactive\nrecommendations. arXiv preprint arXiv:2308.16505, 2023. ,\u21927.\n12\n"
  },
  {
    "title": "Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion",
    "link": "https://arxiv.org/pdf/2402.03162.pdf",
    "upvote": "17",
    "text": "Direct-a-Video: Customized Video Generation with User-Directed Camera\nMovement and Object Motion\nShiyuan Yang 1,3, Liang Hou2, Haibin Huang2, Chongyang Ma2,\nPengfei Wan2, Di Zhang2, Xiaodong Chen3, Jing Liao1 *\n1 City University of Hong Kong, 2 Kuaishou Technology, 3 Tianjin University\nhttps://direct-a-video.github.io/\nFigure 1. Direct-a-Video is a text-to-video generation framework that allows users to individually or jointly control the camera movement\nand/or object motion.\nAbstract\nRecent text-to-video diffusion models have achieved im-\npressive progress. In practice, users often desire the abil-\nity to control object motion and camera movement inde-\npendently for customized video creation. However, current\nmethods lack the focus on separately controlling object mo-\ntion and camera movement in a decoupled manner, which\nlimits the controllability and flexibility of text-to-video mod-\nels. In this paper, we introduce Direct-a-Video, a system\nthat allows users to independently specify motions for one\nor multiple objects and/or camera movements, as if direct-\ning a video. We propose a simple yet effective strategy for\nthe decoupled control of object motion and camera move-\nment. Object motion is controlled through spatial cross-\nattention modulation using the model\u2019s inherent priors, re-\nquiring no additional optimization. For camera movement,\nwe introduce new temporal cross-attention layers to inter-\npret quantitative camera movement parameters. We further\nemploy an augmentation-based approach to train these lay-\n*Corresponding author.\ners in a self-supervised manner on a small-scale dataset,\neliminating the need for explicit motion annotation. Both\ncomponents operate independently, allowing individual or\ncombined control, and can generalize to open-domain sce-\nnarios. Extensive experiments demonstrate the superiority\nand effectiveness of our method. Code will be made avail-\nable upon acceptance.\n1. Introduction\nText-to-image (T2I) diffusion models have already\ndemonstrated astonishingly high quality and diversity in im-\nage generation and editing [23, 43, 45, 47, 63]. The rapid\ndevelopment of T2I diffusion models has also spurred the\nrecent emergence of text-to-video (T2V) diffusion mod-\nels [3, 22, 49, 53, 2], which are normally extended from\npretrained T2I models for video generation and editing. On\nthe other hand, the advent of controllable techniques in T2I\nmodels, such as ControlNet [66], T2I-adapter [39] and GLI-\nGEN [33], has allowed users to specify the spatial layout\nof generated images through conditions like sketch maps,\n1\narXiv:2402.03162v1  [cs.CV]  5 Feb 2024\ndepth maps, or bounding boxes etc., significantly enhancing\nthe spatial controllability of T2I models. Such spatial con-\ntrollable techniques have also been successfully extended\nto spatial-temporal control for video generation. One of the\nrepresentative works in this area is VideoComposer [56],\nwhich can synthesize a video given a sequence of sketch or\nmotion vector maps.\nDespite the success of text-to-video synthesis, current\nT2V methods often lack support for user-defined and dis-\nentangled control over camera movement and object mo-\ntion, which limits the flexibility in video motion control. In\na video, both objects and the camera exhibit their respec-\ntive motions. Object motion originates from the subject\u2019s\nactivity, while camera movement influences the transition\nbetween frames. The overall video motion becomes well-\ndefined only when both camera movement and object mo-\ntion are determined. For example, focusing solely on ob-\nject motion, such as generating a video clip where an object\nmoves to the right within the frame, can lead to multiple sce-\nnarios. The camera may remain stationary while the object\nitself moves right, or the object may be stationary while the\ncamera moves left, or both the object and the camera may\nbe moving at different speeds. This ambiguity in the overall\nvideo motion can arise. Therefore, the decoupling and in-\ndependent control of camera movement and object motion\nnot only provide more flexibility but also reduce ambiguity\nin the video generation process. However, this aspect has\nreceived limited research attention thus far.\nTo control camera movement and object motion in T2V\ngeneration, a straightforward approach would be to follow\nthe supervised training route similar to works like Video-\nComposer [56]. Following such kind of scheme involves\ntraining a conditional T2V model using videos annotated\nwith both camera and object motion information.\nHow-\never, this would bring the following challenges: (1) In many\nvideo clips, object motion is often coupled with camera\nmovements due to their inherent correlation.\nFor exam-\nple, when a foreground object moves to some direction, the\ncamera typically pans in the same direction due to the pref-\nerence to keep the main subject at the center of the frame.\nTraining on such coupled camera and object motion data\nmakes it difficult for the model to distinguish between cam-\nera movements and and object motion. (2) Obtaining large-\nscale video datasets with complete camera movement and\nobject motion annotations is challenging due to the labori-\nous and costly nature of performing frame-by-frame object\ntracking and camera pose estimation. Additionally, training\na video model on a large-scale dataset can be computation-\nally expensive.\nIn this work, we introduce Direct-a-Video, a text-to-\nvideo framework that enables users to independently spec-\nify the camera movement and the motion of one or more\nobjects, allowing them to create their desired motion pattern\nas if they were directing a video (Figure 1). To achieve this,\nwe propose a strategy for decoupling camera movement and\nobject motion control by employing two orthogonal control-\nling mechanisms. In essence, we learn the camera move-\nment through a self-supervised and lightweight training ap-\nproach. Conversely, during inference, we adopt a training-\nfree method to control object motion. Our strategy skillfully\navoids the need for intensive collection of motion annota-\ntions and video grounding datasets.\nIn camera movement control, we train an additional\nmodule to learn the frame transitions. Specifically, we in-\ntroduce new temporal cross-attention layers, known as the\ncamera module, which functions similarly to spatial cross-\nattention in interpreting textual language.\nThis camera\nmodule interprets \u201dcamera language\u201d, specifically camera\npanning and zooming parameters, enabling precise control\nover camera movement. However, acquiring datasets with\ncamera movement annotations can pose a challenge. To\novercome this laborious task, we employ a self-supervised\ntraining strategy that relies on camera movement augmenta-\ntion. This approach eliminates the need for explicit motion\nannotations. Importantly, we train these new layers while\npreserving the original model weights, ensuring that the ex-\ntensive prior knowledge embedded within the T2V model\nremains intact. Although the model is initially trained on\na small-scale video dataset, it acquires the capability to\nquantitatively control camera movement in diverse, open-\ndomain scenarios.\nIn object motion control, a significant challenge arises\nfrom the availability of well-annotated grounding datasets\nfor videos, curating such datasets is often a labor-intensive\nprocess. To bypass these issues, we draw inspiration from\nprevious attention-based image-layout control techniques in\nT2I models [31]. We utilize the internal priors of the T2V\nmodel through spatial cross-attention modulation, which is\na training-free approach, thereby eliminating the need for\ncollecting grounding datasets and annotations for object\nmotion. To facilitate user interaction, we enable users to\nspecify the spatial-temporal trajectories of objects by draw-\ning bounding boxes at the first and last frames, as well\nas the intermediate path. Such interaction is simpler and\nmore user-friendly compared to previous pixel-wise control\nmethods [56].\nGiven that our approach independently controls camera\nmovement and object motion, thereby effectively decouples\nthe two, offering users enhanced flexibility to individually\nor simultaneously manipulate these aspects in video cre-\nation.\nIn summary, our contributions are as follows:\n\u2022 We propose a unified framework for controllable video\ngeneration that decouples camera movement and object\nmotion, allowing users to independently or jointly control\nboth aspects.\n2\n\u2022 For camera movement, we introduce a novel temporal\ncross-attention module dedicated to camera movement\nconditioning. This camera module is trained through self-\nsupervision, enabling users to quantitatively specify the\ncamera\u2019s horizontal and vertical panning speeds, as well\nas its zooming ratio.\n\u2022 For object motion, our approach utilizes a training-free\nspatial cross-attention modulation, enabling users to eas-\nily define the motion trajectories for one or more objects\nby drawing bounding boxes.\n2. Related Work\n2.1. Text-to-Video Synthesis\nThe success of text-to-image (T2I) models has revealed\ntheir potential for text-to-video (T2V) generation.\nT2V\nmodels are often evolved from T2I models by incorporating\ntemporal layers. Early T2V models [25, 22, 49] perform\nthe diffusion process in pixel space, which requires multi-\nple cascaded models to generate high-resolution or longer\nvideos, resulting in high computational complexity.\nRe-\ncent T2V models draw inspiration from latent diffusion [45]\nand operate in a lower-dimensional and more compact latent\nspace [3, 69, 13, 53, 18]. The most recent Stable VideoD-\niffusion [2] utilizes curated training data and is capable of\ngenerating high-quality videos.\nOn the other hand, the development of T2I editing tech-\nniques [19, 38, 15, 46, 32] has facilitated zero/few-shot\nvideo editing tasks.\nThese techniques convert a given\nsource video to a target video through approaches such as\nweight fine-tuning [59], dense map conditioning [64, 13,\n67, 16], sparse point conditioning [17, 51], attention fea-\nture editing [42, 34, 4, 55], and canonical space processing\n[41, 5, 30]. Some works specifically focus on synthesizing\nhuman dance videos using source skeleton sequences and\nreference portraits [6, 27, 62, 14, 54], which have yielded\nimpressive results.\n2.2. Video Generation with Controllable Motion\nAs motion is an important factor in video, research on\nvideo generation with motion control has garnered increas-\ning attention. We can categorize the works in this field into\nthree groups based on the type of input media: image-to-\nvideo, video-to-video, and text-to-video.\nImage-to-video.\nSome methods focus on transforming\nstatic images into videos, and a popular approach for motion\ncontrol is through key point dragging [65, 9, 12]. While this\ninteraction method is intuitive and user-friendly, it has lim-\nitations due to the local and sparse nature of the key points.\nConsequently, its capacity for controlling motion at a large\ngranularity is significantly restricted.\nVideo-to-video.\nThese works primarily focus on motion\ntransfer, which involves learning a specific subject action\nfrom source videos and applying it to target videos using\nvarious techniques, including fine-tuning the model on a set\nof reference videos with similar motion patterns [60, 68, 29,\n58], or borrowing spatial features (e.g., sketch, depth maps)\n[56, 10] or sparse features (e.g., DIFT point embedding)\n[17] from source videos. These methods highly rely on the\nmotion priors from the source videos, which, however, are\nnot always practically available.\nText-to-video.\nIn the case where the source video is un-\navailable, generating videos from text with controllable mo-\ntion is a meaningful but relatively less explored task. Our\nwork focuses on this category.\nExisting approaches in\nthis category include AnimateDiff [18], which utilizes ad-\nhoc motion LoRA modules [26] to enable specific camera\nmovements. However, it lacks quantitative control in cam-\nera movement and also does not support object motion con-\ntrol. A concurrent work, Peekaboo [28], allows control over\nthe trajectory of the object through attention modulation but\ndoes not support camera movement control. VideoCom-\nposer [56] provides global motion guidance by conditioning\non pixel-wise motion vectors. However, the dense control\nmanner offered by VideoComposer is not intuitive and fails\nto explicitly disentangle camera and object motion, result-\ning in cumbersome user interaction. MotionCtrl [57], an-\nother concurrent work, permits sparse point-driven object\ncontrol and trajectory-driven camera control. However, its\ntraining process is labor-intensive, requiring the tracking of\nmarked points on moving objects. Moreover, it struggles to\ncontrol multiple different objects with varied motion direc-\ntions due to constraints from the training data. In contrast,\nour approach does not require motion annotations and can\nachieve control over multiple objects with various motion\ndirections and camera movements. This significantly en-\nhances the flexibility and usability of video synthesis.\n3. Method\n3.1. Overview\nTask formulation.\nIn this paper, we focus on text-\nto-video generation with user-directed camera movement\nand/or object motion. First of all, user should provide a text\nprompt which may optionally contain one or more object\nwords O1, O2, ...ON. To determine the camera movement,\nuser can specify an x-pan ratio cx, a y-pan ratio cy, and a\nzoom ratio cz. To determine the motion of n-th object On ,\nuser needs to specify a starting box B1\nn, an ending box BL\nn\n(L is the video length), and an intermediate track \u03b6n con-\nnecting B1\nn and BL\nn, our system then generates a sequence\nof boxes [B1\nn, ..., BL\nn] centered along the track \u03b6n via inter-\npolation to define the spatial-temporal journey of the object.\n3\nFigure 2. The overall pipeline of Direct-a-Video. The camera movement is learned in the training stage and the object motion is imple-\nmented in the inference stage. Left: During training, we apply augmentation to video samples to simulate camera movement using panning\nand zooming parameters. These parameters are embedded and injected into newly introduced temporal cross-attention layers as the camera\nmovement conditioning, eliminating the need for camera movement annotation. Right: During inference, along with camera movement,\nuser inputs a text prompt containing object words and associated box trajectories. We use spatial cross-attention modulation to guide the\nspatial-temporal placement of objects, all without additional optimization. Note that our approach, by independently controlling camera\nmovement and object motion, effectively decouples the two, thereby enabling both individual and joint control.\nConsequently, our model synthesizes a video that adheres to\nthe prescribed camera movement and/or object motion, cre-\nating customized and dynamic visual narrative.\nOverall pipeline.\nOur overall pipeline is illustrated in\nFigure 2. The camera movement is learned in the training\nstage and the object motion is implemented in the inference\nstage. During the training, we use video samples captured\nby a stationary camera, which are then augmented to sim-\nulate camera movement according to [cx, cy, cz]. The aug-\nmented videos are subsequently used as input to the U-Net.\nAdditionally, the camera parameters are also encoded and\ninjected into a newly introduced trainable temporal cross-\nattention layer to condition the camera movement (detailed\nin Section 3.2). During the inference, with trained camera\nembedder and module, users can specify the camera param-\neters to control its movement. Concurrently, we incorporate\nthe object motion control in a training-free manner: given\nthe object words from the user\u2019s prompt and the correspond-\ning boxes, we modulate the frame-wise and object-wise\nspatial cross-attention maps to redirect the object spatial-\ntemporal size and location (detailed in Section 3.3). It is\nnoteworthy that the modulation in inference stage does not\ninvolve additional optimization, thus the incremental time\nand memory cost is negligible.\n3.2. Camera Movement Control\nWe choose three types of camera movement: horizon-\ntal pan, vertical pan, and zoom, parameterized as a triplet\nccam = [cx, cy, cz] to serve as the control signal for camera\nmovement. This not only allows for quantitative control, but\nis also user-friendly: users can specify the triplet as simple\nas typing a text prompt.\nData construction and augmentation.\nExtracting cam-\nera movement information from existing video can be com-\nputationally expensive since the object motion needs to be\nidentified and filtered out.\nAs such, we propose a self-\nsupervised training approach using camera augmentation\ndriven by ccam, thereby bypassing the need for intensive\nmovement annotation.\nWe first formally define the camera movement parame-\nters. cx represents the x-pan ratio, and is defined as the total\nx-shift of the frame center from the first to the last frame\nrelative to the frame width, cx > 0 for panning rightward\n(e.g., cx = 0.5 for a half-width right shift). Similarly, cy\nis the y-pan ratio, representing the total y-shift of the frame\ncenter over the frame height, cy > 0 for panning downward.\ncz denotes the zoom ratio, defined as the scaling ratio of the\nlast frame relative to the first frame, cz > 1 for zooming-\nin. We set the range of cx,cy to [\u22121, 1] and cz to [0.5, 2],\nwhich are generally sufficient for covering regular camera\nmovement range.\nIn practice, for given ccam, we simulate camera move-\nment by applying shifting and scaling to the cropping win-\ndow on videos captured with a stationary camera.\nThis\ndata augmentation exploits readily available datasets like\nMovieShot [44]. Further details of this process, including\n4\npseudo code and sampling scheme of ccam are provided in\nthe appendix.\nCamera embedding.\nTo encode ccam into a camera em-\nbedding, we use a camera embedder that includes a Fourier\nembedder [37] and two MLPs. One MLP jointly encodes\nthe panning movement cx, cy, while the other encodes the\nzooming movement cz. We empirically found that sepa-\nrately encoding panning and zooming helps the model dis-\ntinguish between these two distinct types of camera move-\nments effectively, and we validate this design in Section\n4.5. The embedding process can be formulated as exy =\nMLPxy(F([cx, cy])) ,ez = MLPz(F(cz)), where F de-\nnotes Fourier embedder, both exy and ez have the same\nfeature dimensions, By concatenating them, we obtain the\ncamera embedding ecam = [exy, ez], which has a sequence\nlength of two.\nCamera module.\nWe now consider where to inject the\ncamera embedding. Previous studies have highlighted the\nrole of temporal layers in managing temporal transitions\n[18, 68].\nAs such, we inject camera control signals via\ntemporal layers. Inspired by the way spatial cross-attention\ninterprets textual information, we introduce new trainable\ntemporal cross-attention layers specifically for interpreting\ncamera information, dubbed as camera modules, which are\nappended after the existing temporal self-attention layers\nwithin each U-Net block of the T2V model, as depicted in\nFigure 2. Similar to textual cross-attention, in this mod-\nule, the queries are mapped from visual frame features F,\nwe separately map the keys and values from panning em-\nbedding exy and zooming embedding ez for the same rea-\nson stated in the previous section. Through temporal cross-\nattention, the camera movement is infused into the visual\nfeatures, which is then added back as a gated residual. We\nformulate this process as follows:\nF = F + tanh(\u03b1) \u00b7 TempCrossAttn(F, ecam)\n(1)\nTempCrossAttn(F, ecam) = Softmax\n\u0012Q[Kxy, Kz]T\n\u221a\nd\n\u0013\n[Vxy, Vz],\n(2)\nwhere [, ] denotes concatenation in sequence dimension,\nKxy,Kz are key vectors, Vxy,Vz are value vectors mapped\nfrom the exy, ez respectively, d is the feature dimension of\nQ, and \u03b1 is a learnable scalar initialized as 0, ensuring that\nthe camera motion is gradually learned from the pretrained\nstate.\nTo learn camera movement while preserving the model\u2019s\nprior knowledge, we freeze the original weights and train\nonly the newly added camera embedder and camera mod-\nule.\nThese are conditioned on camera movement ccam,\nand video caption ctxt. The training employs the diffusion\nnoise-prediction loss:\nL = Ex0,ccam,ctxt,t,\u03f5\u223cN (0,I)\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (xt, ccam, ctxt, t)\u22252\n2\ni\n,\n(3)\nwhere x0 is the augmented input sample, t denotes the dif-\nfusion timestep, xt = \u03b1tx0 + \u03c3t\u03f5 is the noised sample at\nt, \u03b1t and \u03c3t are time-dependent DDPM hyper-parameters\n[23], \u03f5\u03b8 is the model parameterized by \u03b8.\n3.3. Object Motion Control\nWe choose the bounding box as the control signal for\nobject motion due to its advantageous balance of usability\nand accessibility. Boxes are more efficient than dense con-\nditions (e.g., sketch maps), as they do not require drawing\nskills, and they provide specification of object\u2019s size, a fea-\nture that sparse conditions (e.g., key points) lack.\nWhile it is theoretically possible to train a box-\nconditioned T2V model similar to GLIGEN [33]. How-\never, unlike images, there is a stark contrast in the availabil-\nity of well-annotated grounding datasets for videos, which\nare much fewer and cover limited categories. To bypass\nthis issue, we opt to fully leverage the inherent priors of\npretrained T2V models by steering the diffusion process to\nour desired result. Previous T2I works have demonstrated\nthe ability to control an object\u2019s spatial position by editing\ncross-attention maps [1, 31, 36, 8]. Similarly, we employ\nthe spatial cross-attention modulation in T2V model for ob-\nject motion crafting.\nIn cross-attention layers, the query features Q are de-\nrived from visual tokens, the key K and value features V\nare mapped from textual tokens. QK\u22a4 constitutes an atten-\ntion map, where the value at index [i, j] reflects the response\nof the i-th image token feature to the j-th textual token fea-\nture. We modulate the attention map QK\u22a4 as follows:\nCrossAttnModulate(Q, K, V) = Softmax\n\u0012QK\u22a4 + \u03bbS\n\u221a\nd\n\u0013\nV,\n(4)\nwhere \u03bb represents modulation strength, d is the feature di-\nmension of Q, and S is the modulation term of the same\nsize as QK\u22a4. It comprises two types of modulation: am-\nplification and suppression.\nAttention amplification.\nConsidering the n-th object in\nthe k-th frame, enclosed by the bounding box Bk\nn, since we\naim to increase the probability of the object\u2019s presence in\nthis region, we could amplify the attention values for the\ncorresponding object words (indexed as Tn in the prompt)\nwithin the area Bk\nn. Note that if there exists a background\nword, we treat it in the same way, and its region corresponds\nto the complement of the union of all the objects\u2019 regions.\nFollowing the conclusion from DenseDiff [31], the scale of\nthis amplification should be inversely related to the area of\n5\nBk\nn, i.e., smaller box area are subject to a larger increase in\nattention. Since our attention amplification is performed on\nbox-shaped regions, which does not align with the object\u2019s\nnatural contours, we confine the amplification to the early\nstages (for timesteps t \u2265 \u03c4, \u03c4 is the amplification cut-off\ntimestep), as the early stage mainly focuses on generating\ncoarse layouts. For t < \u03c4, we relax this control to enable\nthe diffusion process to gradually refine the shape and ap-\npearance details.\nAttention suppression.\nTo mitigate the influence of irrel-\nevant words on the specified region and prevent the unin-\ntended dispersion of object features to other areas, we sup-\npress attention values for unmatched query-key token pairs\n(except start token and end token otherwise the video qual-\nity would be compromised). Different from attention am-\nplification, attention suppression is applied throughout the\nentire sampling process to prevent mutual semantic interfer-\nence, an potential issue in multi-object generation scenarios\nwhere the semantics of one object might inadvertently bleed\ninto another. We will present the results and analysis in the\nablation studies (Section 4.5).\nFormally, the attention modulation term for the n-th ob-\nject in the k-th frame Sk\nn[i, j] is formulated as:\nSk\nn[i, j] =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1 \u2212\n|Bk\nn|\n|QK\u22a4|,\nif i \u2208 Bk\nn and j \u2208 Tn and t \u2265 \u03c4\n0,\nif i \u2208 Bk\nn and j \u2208 Tn and t < \u03c4\n\u2212\u221e,\notherwise\n(5)\nwhere |X| denotes the number of elements in matrix X. We\nperform such modulation for each object in every frame so\nthat the complete spatial-temporal object trajectory can be\ndetermined. Note that although this modulation is indepen-\ndently performed in each frame, we observe that the gen-\nerated videos remain continuous, thanks to the pretrained\ntemporal layers which maintains temporal continuity.\n4. Experiment\n4.1. Experimental Setup\nImplementation details.\nWe adopt pretrained Zeroscope\nT2V model [53] as our base model, integrating our pro-\nposed trainable camera embedder and module to facilitate\ncamera movement learning. Please refer to the appendix\nfor training details. During the inference, we use DDIM\nsampler [50] with T = 50 sampling steps and a classifier-\nfree guidance scale of 9 [24]. The default attention control\nweight \u03bb and cut-off timestep \u03c4 are 25 and 0.95T respec-\ntively. The output video size is 320\u00d7512\u00d724.\nDatasets.\nFor camera movement training, we use a sub-\nset from MovieShot [44]. This subset contains 22k static-\nshot movie trailers, i.e., the camera is fixed but the sub-\nject is flexible to move, which ensures that the training\nsamples are devoid of original camera movement. Despite\nthe limited number and category of the training samples,\nour trained camera module is still able to adapt to general\nscenes. For camera control evaluation, we collected 200\nscene prompts from the prompt set provided by [11]. For\nobject control evaluation, we curated a benchmark of 200\nbox-prompt pairs, comprising varied box sizes, locations,\nand trajectories, with prompts primarily focusing on natural\nanimals and objects.\nMetrics.\n(1) To assess video generation quality, we em-\nploy FID-vid [21] and FVD [52], the reference set are 2048\nvideos from MSRVTT [61] for camera movement task and\n800 videos from AnimalKingdom [40] for object motion\ntask.\n(2) To measure the object-box alignment, we uni-\nformly extract 8 frames per video sample and calculate the\nCLIP image-text similarity (CLIP-sim) [20] within the box\narea only, with a templated prompt \u201ca photo of [obj]\u201d, where\n[obj] corresponds to the object phrase. (3) To evaluate the\ncamera and object motion alignment, we introduce the flow\nerror. Specifically, we utilize VideoFlow [48], a state-of-\nthe-art optical flow model to extract flow maps from gen-\nerated videos. These are then compared against the ground\ntruth flow (which are derived from given camera movement\nand object boxes). The flow error is calculated on entire\nframe for camera movement evaluation and box area only\nfor object motion evaluation.\nBaselines.\nWe compare our method with recent diffusion-\nbased T2V models with the camera movement or object mo-\ntion controllability, including AnimateDiff [18] (for camera\nmovement), Peekaboo [28] (for object motion), and Video-\nComposer [56] (joint control).\n4.2. Camera Movement Control\nFor camera movement control, we conduct comparisons\nwith AnimateDiff and VideoComposer. For AnimateDiff,\nwe use official pretrained LoRA motion modules, each ded-\nicated to a specific type of camera movement but lacking\nsupport for precise control. For VideoComposer, we hand-\ncraft a motion vector map based on the camera movement\nparameters, as demonstrated in its paper.\nQualitative comparison.\nWe present side-by-side visual\ncomparison with baselines in Figure 3. As can be seen, all\nthe methods are capable of generating videos with the sin-\ngle type of camera movement, but AnimateDiff does not\nsupport hybrid camera movement (e.g., pan+zoom) since\nits loaded motion module is dedicated to one type of cam-\nera movement only, while our method and VideoComposer\n6\nFigure 3. Qualitative comparison on camera movement control with related baselines. Our results in the third column show that the object\nmotion (yellow lines) can be independent from the camera movement (cyan lines) unlike results by VideoComposer in the second column.\ncan combine or switch the camera movement by altering\nthe motion input, without the need for re-loading extra\nmodules.\nIn terms of precise control, both our method\nand VideoComposer can quantitatively control the camera\nspeed, but VideoComposer requires providing a spatial vec-\ntor map, which must be either hand-crafted or extracted\nfrom an existing video, potentially reducing its ease of use.\nOur approach, on the other hand, is more user-friendly, en-\nabling users to specify camera speed as effortlessly as enter-\ning a text prompt. Moreover, in terms of disentanglement,\nour method\u2019s camera control does not impact foreground\nobjects, as we do not impose any motion constraints on\nthem. In contrast, VideoComposer employs a global motion\nvector map, which often binds objects together with back-\nground movement. As shown in the 3rd column of Figure 3,\nthe zebra in our results exhibits its independent motion from\nthe camera, whereas in VideoComposer\u2019s results (the 2nd\ncolumn), the zebra is tied to the camera movement. Finally,\nour results also exhibit higher visual quality, a testament to\nthe superiority of our base model.\nQuantitative comparison.\nWe report FVD, FID-vid, and\nFlow error in Table 1. Note that AnimateDiff is excluded\nfrom the flow error comparison due to its lack of quantita-\ntive control. Our results achieve the best FVD and FID-vid\nscores, indicating superior visual quality compared to base-\nlines, and show more precise camera control, evidenced by\na lower flow error.\n4.3. Object Motion Control\nFor object motion control, we mainly compare with\nVideoComposer, as its motion vector map can also be used\nfor generating object motion via box sequences. To achieve\nthis, we first convert object box sequences into dense flow\nTable 1. Quantitative comparison for camera movement control\nevaluation.\nFVD \u2193\nFID-vid\u2193\nFlow error\u2193\nAnimateDiff [18]\n1685.40\n82.57\n-\nVideoComposer [56]\n1230.57\n82.14\n0.74\nDirect-a-Video (ours)\n888.91\n48.96\n0.46\nmaps, which are then processed into motion vector maps\ncompatible with its input format. We also qualitatively com-\npare with concurrent work Peekaboo [28], as their code is\nnot available at the time of writing, we use samples from\ntheir official website.\nQualitative comparison.\nWe present visual comparison\nwith related baselines in Figure 4. For static object gen-\neration, VideoComposer fails to generate the object in de-\nsired location (see the panda example in the first column),\nwithout any motion hint, it works like a vanilla T2V model.\nWhile all methods are capable of generating a single mov-\ning object, challenges arise in multiple moving objects sce-\nnarios. Peekaboo is excluded from this comparison due to\nthe lack of relevant examples. VideoComposer does not\nsupport specifying individual motion for each object un-\nlike our method (see the shark and jellyfish examples in\nthe 7th column, they do not appear at the expected loca-\ntions). Moreover, its lack of explicit binding between ob-\njects and motion leads to two extra issues: semantic mix-\ning and absence. Semantic mixing refers to the blending\nof one object\u2019s semantics with another. This is exempli-\nfied in the 9th column, where tiger\u2019s texture leaks into bear.\nSemantic absence occurs when an object does not appear\nas anticipated, a known issue in T2I/T2V models [7]. For\ninstance, in the 11th column, the expected camel is miss-\n7\nFigure 4. Qualitative comparison on object motion control with related baselines. Our method excels in handling cases involving more\nthan one object.\nTable 2. Quantitative comparison for object motion control evalu-\nation.\nFVD \u2193\nFID-vid\u2193 CLIP-sim\u2191 Flow error \u2193\nVideoComposer[56]\n1620.83\n90.57\n27.35\n1.53\nDirect-a-Video (ours)\n1300.86\n43.55\n27.63\n1.11\ning, replaced instead by a jeep. In contrast, our method\neffectively addresses these issues through ad-hoc attention\nmodulation for each object, facilitating easier control over\nmultiple objects\u2019 motion.\nQuantitative comparison.\nWe report FVD, FID-vid,\nCLIP-sim, and Flow error in Table 2. The statistics indi-\ncate that our method outperforms VideoComposer in terms\nof both generation quality and object motion control.\n4.4. Joint Control of Camera Movement and Object\nMotion\nDirect-a-Video features in jointly supporting the control\nof both camera movement and object motion, we demon-\nstrate such capability in Figure 5. Given the same box se-\nquence, our method can generate videos with varied com-\nbination of foreground-background motions. For example,\nFigure 5(a) illustrates that a static box does not always im-\nply a static object, by setting different camera movements,\nour system can generate videos of a zebra standing still (2nd\ncolumn), walking right (3rd column), or walking left (4th\ncolumn). Similarly, Figure 5(b) suggests that a moving box\ndoes not necessarily indicate that the object itself is in mo-\ntion, it could be stationary in its position while the camera\nis moving (last column). Existing works focused only on\nobject often fail to differentiate between the object\u2019s inher-\nent motion and apparent motion induced by camera move-\nTable 3. Quantitative evaluation on attention amplification and\nsuppression.\nAttn amp.\nAttn sup.\nCLIP-sim \u2191\nFlow error \u2193\n\u00d7\n\u2713\n25.82\n2.08\n\u2713\n\u00d7\n27.49\n1.19\n\u2713\n\u2713\n27.63\n1.11\nment. In contrast, our method enables users to distinctly\nspecify both camera movement and object motion, offer-\ning enhanced flexibility in defining overall motion patterns.\nMore examples are provided in Figure 10 and project page.\n4.5. Ablation Study\nAttention amplification.\nThis is crucial for object local-\nization, the absence of attention amplification results in the\nmodel losing its grounding ability, i.e., the object would not\nfollow the boxes, as shown in the first row in Figure 6, and\na decrease of CLIP-sim score and an increase of flow error\nin Table 3.\nAttention suppression.\nThis is introduced to mitigate the\nunintended semantic mixing in multi-object scenarios, par-\nticularly when objects share similar characteristics. This re-\nsults from the design that attention amplification is applied\nonly in the initial steps, and this constraint is subsequently\nrelaxed. Without suppression, object A\u2019s prompt feature can\nalso attend to object B\u2019s region, leading to semantic overlap.\nAs shown in second row of Figure 6, where the tiger\u2019s tex-\nture erroneously appears on the bear\u2019s body. The third row\nshows that this issue can be resolved by enabling the atten-\ntion suppression.\n8\nFigure 5. Joint control of object motion and camera movement. Given the same box sequence, by setting different camera movement\nparameters, our approach is capable of synthesizing videos that exhibit a diverse combination of foreground motion (yellow lines) and\nbackground motion (cyan lines). User can create a well-defined overall video motion by distinctly specifying both object motion and\ncamera movement using our method.\nFigure 6. Effect of attention amplification and suppression. With-\nout amplification (first row), the objects do not adhere to boxes;\nWithout suppression (second row), tiger\u2019s texture mistakenly leaks\ninto the bear\u2019s body. These issue are resolved with both enabled\n(third row).\nCamera embedding design.\nTo assess the effectiveness\nof separately encoding panning (cx, cy) and zooming (cz)\nmovements in camera control as detailed in Section 3.2,\nwe contrast this with a joint encoding approach.\nHere,\n[cx, cy, cz] are encoded into a single camera embedding vec-\ntor using a shared MLP, followed by shared key-value pro-\njection matrix in the camera module. We train and evalu-\nate the model with the same setting, we observed a reduced\nability in camera movement control, with flow error increas-\ning from 0.46 to 1.68. This underscores the advantages of\nseparate encoding for distinct types of camera movements.\n4.6. Limitations\nWhile our method provides disentangled control over ob-\nject and camera motion, conflicts can sometimes arise in the\ninputs. For instance, in the top row of Figure 7, we attempt\nto maintain a static object (house) within a static box while\nsimultaneously panning the camera to the left. Given these\nconflicting signals, our method ends up generating a mov-\ning house, which is unrealistic. This necessitates careful\nand reasonable user interaction. Another issue arises when\nhandling colliding boxes. In scenarios like the one depicted\nin the bottom row of Figure 7, where two boxes overlap,\nthe semantics of one object (the bear) can interfere with an-\nother (the tiger). This issue can be mitigated by modulating\nattention on an adaptively auto-segmented region during the\n9\nFigure 7. Limitations of our method. Top: Conflicting inputs can\nlead to unreal results - a moving house. Bottom: Overlapping\nboxes may lead to object interfere - tiger with a bear head.\ndiffusion sampling process, rather than relying on the initial\nbox region. Finally, our present method of data augmenta-\ntion limits the system\u2019s ability to produce realistic 3D cam-\nera movements. To overcome this constraint, we envisage\nthe adoption of more sophisticated augmentation algorithms\nin our future work.\n5. Conclusion\nIn this work, we propose Direct-a-Video, a text-to-video\nframework that addresses the previously unmet need for in-\ndependent and user-directed control over camera movement\nand object motion.\nOur approach effectively decouples\nthese two elements by integrating a self-supervised training\nscheme for temporal cross-attention layers tailored for cam-\nera movement control, with a training-free modulation for\nspatial cross-attention dedicated to object motion control.\nExperimental evaluations demonstrate the capability of our\napproach in separate and joint control of camera movement\nand object motion. This positions Direct-a-Video as an ef-\nficient and flexible tool for creative video generation with\ncustomized motion.\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 5\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 1, 3\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 1, 3\n[4] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 23206\u201323217, 2023. 3\n[5] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 23040\u201323050, 2023. 3\n[6] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu,\nGuoxian Song, Qing Yan, Xiao Yang, and Mohammad So-\nleymani.\nMagicdance: Realistic human dance video gen-\neration with motions & facial expressions transfer.\narXiv\npreprint arXiv:2311.12052, 2023. 3\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 7\n[8] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free\nlayout control with cross-attention guidance.\nIn Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 5343\u20135353, 2024. 5\n[9] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffu-\nsion model for controllable video synthesis. arXiv preprint\narXiv:2304.14404, 2023. 3\n[10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 3\n[11] Iya Chivileva,\nPhilip Lynch,\nTomas Ward,\nand Alan\nSmeaton. Text prompts and videos generated using 5 popu-\nlar Text-to-Video models plus quality metrics including user\nquality assessments. 10 2023. 6\n[12] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and\nChi-Keung Tang. Dragvideo: Interactive drag-style video\nediting. arXiv preprint arXiv:2312.02216, 2023. 3\n[13] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 3\n[14] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui,\nXiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen\nLi, et al. Dreamoving: A human video generation framework\nbased on diffusion models.\narXiv e-prints, pages arXiv\u2013\n2312, 2023. 3\n[15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3\n[17] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei\nLiu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang,\nMike Zheng Shou, and Kevin Tang. Videoswap: Customized\nvideo subject swapping with interactive semantic point cor-\nrespondence. arXiv preprint arXiv:2312.02087, 2023. 3\n[18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\n10\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3, 5, 6, 7\n[19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 6\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 1, 5, 13\n[24] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 3\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3\n[27] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and\nLiefeng Bo. Animate anyone: Consistent and controllable\nimage-to-video synthesis for character animation.\narXiv\npreprint arXiv:2311.17117, 2023. 3\n[28] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat\nBehl. Peekaboo: Interactive video generation via masked-\ndiffusion. arXiv preprint arXiv:2312.07509, 2023. 3, 6, 7\n[29] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye.\nVmc: Video motion customization using temporal attention\nadaption for text-to-video diffusion models. arXiv preprint\narXiv:2312.00845, 2023. 3\n[30] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 3\n[31] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and\nJun-Yan Zhu. Dense text-to-image generation with attention\nmodulation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7701\u20137711, 2023. 2,\n5\n[32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 3\n[33] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 1, 5\n[34] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 13\n[36] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and\nThomas Leung. Directed diffusion: Direct control of ob-\nject placement through attention guidance. arXiv preprint\narXiv:2302.13153, 2023. 5\n[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n5\n[38] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 3\n[39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 1\n[40] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni,\nSi Yong Yeo, and Jun Liu. Animal kingdom: A large and\ndiverse dataset for animal behavior understanding. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 19023\u201319034, June\n2022. 6\n[41] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 3\n[42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3\n[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1\n[44] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu\nHuang, Bolei Zhou, and Dahua Lin. A unified framework\nfor shot type classification based on subject centric lens. In\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16,\npages 17\u201334. Springer, 2020. 4, 6\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 3\n11\n[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3\n[47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1\n[48] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li,\nManyuan Zhang, Ka Chun Cheung, Simon See, Hongwei\nQin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting\ntemporal cues for multi-frame optical flow estimation. arXiv\npreprint arXiv:2303.08340, 2023. 6\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 3\n[50] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 6\n[51] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 3\n[52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 6\n[53] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n3, 6, 13\n[54] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-\nChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu,\nand Lijuan Wang. Disco: Disentangled control for realistic\nhuman dance generation. arXiv preprint arXiv:2307.00040,\n2023. 3\n[55] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[56] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 3, 6, 7, 8\n[57] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen,\nMenghan Xia, Ping Luo, and Ying Shan.\nMotionctrl: A\nunified and flexible motion controller for video generation.\narXiv preprint arXiv:2312.03641, 2023. 3\n[58] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan,\nZhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and\nHongming Shan.\nDreamvideo:\nComposing your dream\nvideos with customized subject and motion. arXiv preprint\narXiv:2312.04433, 2023. 3\n[59] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 3\n[60] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi\nLi, and Xiangyu Zhang.\nLamp:\nLearn a motion pat-\ntern for few-shot-based video generation.\narXiv preprint\narXiv:2310.10769, 2023. 3\n[61] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288\u20135296, 2016. 6\n[62] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan,\nJia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng\nShou.\nMagicanimate: Temporally consistent human im-\nage animation using diffusion model.\narXiv preprint\narXiv:2311.16498, 2023. 3\n[63] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang-\nmeng Zuo, and Ping Luo. Towards photo-realistic virtual\ntry-on by adaptively generating-preserving image content. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 7850\u20137859, 2020. 1\n[64] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 3\n[65] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 3\n[66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1\n[67] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and\nJun Zhu. Controlvideo: Adding conditional control for one\nshot text-to-video editing. arXiv preprint arXiv:2305.17098,\n2023. 3\n[68] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 3, 5\n[69] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3\n12\nA. Implementation Details\nA.1. Camera Augmentation details\nExtracting camera movement parameters from real-\nworld videos are computationally intensive, often requir-\ning the cumbersome process of separating object motion\nfrom camera movement. To bypass these challenges, we\npropose a method of camera augmentation that simulates\ncamera movement by algorithmically manipulating a sta-\ntionary camera\u2019s footage. In brief, the camera augmentation\nis implemented by altering the calculated cropping window\nacross the video sequence captured by a stationary camera,\nthereby simulating the effect of camera movement in a com-\nputationally efficient manner. The detailed pseudo-code of\nthis process is illustrated in Figure 9.\nA.2. Training Details\nCamera movement parameters sampling.\nDuring the\ntraining, we adopt the following sampling scheme for cam-\nera movement parameters ccam = [cx, cy, cz]:\ncx \u223c\n(\n0,\nwith probability 1\n3,\nUniform(\u22121, 1),\nwith probability 2\n3,\ncy \u223c\n(\n0,\nwith probability 1\n3,\nUniform(\u22121, 1),\nwith probability 2\n3,\ncz \u223c\n(\n1,\nwith probability 1\n3,\n2\u03c9,\nwith probability 2\n3, where \u03c9 \u223c Uniform(\u22121, 1).\nNote that each component is sampled independently.\nTraining scheme.\nWe adopt pretrained Zeroscope T2V\nmodel [53] as our base model. To facilitate camera move-\nment learning while retain the pretrained state, only the\nnewly added layers are trainable, which include camera em-\nbedder and camera module. To speed up the training, we\nuse a coarse-to-fine strategy: we first train on videos of size\n256\u00d7256\u00d78 (height \u00d7 width \u00d7 frames) for 100k iterations,\nthen we resume training on videos of size 320 \u00d7 512 \u00d7 16\nand 320 \u00d7 512 \u00d7 24 for 50k iterations each. The train-\ning is performed using a DDPM noise scheduler [23] with\ntimestep t uniformly sampled from [400, 1000], such pref-\nerence to higher timesteps helps to prevents overfitting to\nlow-level details, which are deemed non-essential for un-\nderstanding temporal transitions. We employ an AdamW\noptimizer [35] with a learning rate of 5e-5 and a batch size\nof 8 on 8 NVIDIA Tesla V100 GPUs.\nB. Additional Ablation Studies\nWhich layers for attention amplification?\nTo determine\nwhich layers to apply the attention amplification, we divide\nTable 4. Assessment of attention amplification on different parts\nof UNet.\nE\nM\nD\nE+M M+D\nE+D\nE+M+D\nCLIP-sim \u2191 26.20 25.93 26.75 26.35 26.74 27.62\n27.63\nFlow err \u2193\n1.17\n2.37\n2.15\n1.10\n2.15\n1.09\n1.08\nFigure 8. Effect of attention amplification strength \u03bb and cut-off\ntimestep \u03c4 (only the first frame is showed).\nthe U-Net into three parts: encoder (E), middle layer (M),\nand decoder (D). We applied attention amplification to var-\nious combinations of these three and assessed their impact\non the CLIP score and Flow error. The results are presented\nin Table 4. We observed that the decoder tends to yield bet-\nter semantic responsiveness, as indicated by higher CLIP\nscores, while the encoder is more effective in reducing flow\nerror. The middle layer has a comparatively smaller influ-\nence, incorporating middle layer does not bring noticeable\nstatistic change. Consequently, we apply attention amplifi-\ncation across all layers for its highest CLIP-sim score and\nlowest flow error.\nAttention amplification hyper-parameters.\nIn attention\namplification, the strength \u03bb and cut-off timestep \u03c4 are two\nhyper-parameters. Generally, lower \u03c4 and higher \u03bb will in-\ncrease the strength of attention amplification. To determine\na proper choice of hyper-parameters, we conduct tests with\ndifferent combinations of \u03bb and \u03c4. Visual examples are pro-\nvided in Figure 8. We observed that object responses are\nmore sensitive to the value of \u03c4 than to \u03bb. As illustrated\nin the 1st and 2nd rows, over-responsiveness in box regions\ntypically occurs for \u03c4 < 0.9. This is because the early sam-\npling stage in the diffusion model plays a significant role in\ndetermining the coarse layout of the output image or video;\nthus, applying amplification for an extended duration results\nin over-responsiveness in the box region. Empirically, we\nfind that \u03c4 \u2208 [0.9T, 0.95T] and \u03bb \u2208 [10, 25] are generally\nappropriate for most cases.\nC. Additional Results\nWe show additional results in Figure 10. Please refer to\nour project page for dynamic results.\n13\nFunction aug_with_cam_motion(src_video, cx, cy, cz, h, w):\n# Parameters:\n# src_video: Source video, a tensor with dimensions [frames (f), 3, src_height, src_width]\n# cx: Horizontal translation ratio (-1 to 1)\n# cy: Vertical translation ratio (-1 to 1)\n# cz: Zoom ratio (0.5 to 2)\n# h: Height of the generated video\n# w: Width of the generated video\n# Get source frame number, width and height from src_video\nf, src_h, src_w = src_video.shape[0], src_video.shape[2], src_video.shape[3]\n# Initialize camera boxes for frame cropping\ncam_boxes = zeros(f, 4) # f frames, 4: [x1,y1,x2,y2]\n# Calculate dynamic cropping relative coordinates for each frame\n# The first frame coordinates is the reference, which is always [0,0,1,1].\ncam_boxes[:, 0] = linspace(0, cx + (1 - 1/cz) / 2, f)\n# x1, top-left x\ncam_boxes[:, 1] = linspace(0, cy + (1 - 1/cz) / 2, f)\n# y1, top-left y\ncam_boxes[:, 2] = linspace(1, cx + (1 + 1/cz) / 2, f)\n# x2, bottom-right x\ncam_boxes[:, 3] = linspace(1, cy + (1 + 1/cz) / 2, f)\n# y2, bottom-right y\n# Compute the minimum and maximum relative coordinates\nmin_x = min(cam_boxes[:, 0::2])\nmax_x = max(cam_boxes[:, 0::2])\nmin_y = min(cam_boxes[:, 1::2])\nmax_y = max(cam_boxes[:, 1::2])\n# Normalize the camera boxes\nnormalized_boxes = zeros_like(cam_boxes)\nnormalized_boxes[:, 0::2] = (cam_boxes[:, 0::2] - min_x) / (max_x - min_x)\nnormalized_boxes[:, 1::2] = (cam_boxes[:, 1::2] - min_y) / (max_y - min_y)\n# Initialize a tensor for the new frames\naugmented_frames = zeros(f, 3, h, w)\n# Process each frame\nfor i in range(f):\n# Calculate the actual cropping coordinates\nx1, y1, x2, y2 = normalized_boxes[i] * tensor([src_w, src_h, src_w, src_h])\n# Crop the frame according to the coordinates\ncrop = src_video[i][:, int(y1):int(y2), int(x1):int(x2)]\n# Resize the cropped frame and store it\naugmented_frames[i] = interpolate(crop, size=(h, w), mode=\u2019bilinear\u2019)\nreturn augmented_frames\nFigure 9. Pseudo-code for the camera augmentation function.\n14\nFigure 10. Additional results of camera movement control and object motion control.\n15\n"
  },
  {
    "title": "InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions",
    "link": "https://arxiv.org/pdf/2402.03040.pdf",
    "upvote": "15",
    "text": "InteractiveVideo: User-Centric Controllable Video Generation with Synergistic\nMultimodal Instructions\nYiyuan Zhang1\u2217\nYuhao Kang2\u2217\nZhixin Zhang2*\nXiaohan Ding3\nSanyuan Zhao2\nXiangyu Yue1\n1 Multimedia Lab, The Chinese University of Hong Kong\n2 Beijing Institute of Technology\n3Tencent AI Lab\nyiyuanzhang.ai@gmail.com, kangyuhao@bit.edu.cn, xyyue@ie.cuhk.edu.hk\nhttps://invictus717.github.io/InteractiveVideo\nUser Trajectory Editing\nInput\nDrag\n\u201ccar, running\u201d\nUser Semantic Editing\nInput\nPaint Color\n\u201cflowers are shaking in the wind\u201d\nDrag\nPaint\nUser Personalization\nInput\nEditing\n\u201cyellow butterfly is flying to the cat's face\u201d\nUser Content Editing\nInput\nPaint a boat\n\u201cseaside, boat, the boat is leaving\u201d\nImage\nText\nPersonalize\nUser Prompt Editing\nInput\n\u201ca ship is \nmoving \ncross the \nriver\u201d\nInput\nEditing\n\u201cpurple flowers are trembling, a bee is flying\u201d\nFigure 1. Interactive Video Generation We propose a user-centric framework that effectively synergizes users\u2019 multimodal instructions.\nUsers can easily edit key components in the video generation process, leading to high-quality video and increased user satisfaction.\nAbstract\nWe introduce \u201cInteractiveVideo\u201d, a user-centric frame-\nwork for video generation. Different from traditional gen-\nerative approaches that operate based on user-provided im-\nages or text, our framework is designed for dynamic in-\nteraction, allowing users to instruct the generative model\nthrough various intuitive mechanisms during the whole gen-\neration process, e.g.\ntext and image prompts, painting,\ndrag-and-drop, etc. We propose a Synergistic Multimodal\nInstruction mechanism, designed to seamlessly integrate\n*Equal Contribution\nusers\u2019 multimodal instructions into generative models, thus\nfacilitating a cooperative and responsive interaction be-\ntween user inputs and the generative process.\nThis ap-\nproach enables iterative and fine-grained refinement of the\ngeneration result through precise and effective user instruc-\ntions. With InteractiveVideo, users are given the flexibil-\nity to meticulously tailor key aspects of a video. They can\npaint the reference image, edit semantics, and adjust video\nmotions until their requirements are fully met. Code, mod-\nels, and demo are available at https://github.com/\ninvictus717/InteractiveVideo.\n1\narXiv:2402.03040v1  [cs.CV]  5 Feb 2024\n\u201cThe clouds are \nbecoming dark\u201d\n\u201cthe tide\nis \nflooding\u201d\n\u201cTide is flooding\u201d\n\u201cSnow is melting\u201d\n\u201cClouds are moving\u201d\nFigure 2. Comparison between Gen-2 and InteractiveVideo. For each case, the first row is the generation results of Gen-2, and the second\nrow is our results. (More comparison results with Pika Labs, I2VGen-XL [48], and Gen-2 can be found in Appendix Figures 7, 8, and 9.)\n1. Introduction\nVideo generation has attracted significant attention due to\nits promising future in the AI-Generated Content field [3,\n18, 26, 40, 43] and its potential to enhance the efficiency of\nmovie creation and serve as a new infrastructure technology\nfor the film industry [7, 14, 29, 51]. The advancement of\ndiffusion models [13, 30, 31, 47] has infused the field of\nvideo generation with new potential [3, 4, 26]. The success\nof Gen-1 [8], MagicVideo [51], and Align your latents [1]\nhas significantly inspired further exploration of high-quality\nvisual content generation for videos.\nAs the capabilities of video generation models improve,\nuser expectations for the generated videos are concur-\nrently elevated, leading to an increased demand for videos\nthat accurately meet their specific requirements. Existing\nvideo generation models typically utilize a reference image,\nknown as the image condition, and a textual description, re-\nferred to as the text condition, as inputs. Enhancing video\ngeneration foundation models by making them larger, more\nadvanced, and more sophisticated could potentially fulfill\n2\nuser requirements more effectively by enhancing the under-\nstanding of image and text conditions, thereby producing\nvideos of superior quality. However, our objective is to em-\npower existing video generation models to more accurately\nfulfill user requirements from a different angle - by equip-\nping models with the capability to interpret complex, multi-\ndimensional human instructions. This approach is driven by\nthe observation that the current conditioning mechanisms\n(image and text) fall short of capturing the full spectrum of\nuser intentions. 1) The text condition may not be informa-\ntive enough. Even though existing video generation models\nsupport long and detailed text prompts, it is difficult to pre-\ncisely depict complex video motions and dynamics using\nonly text. As a result, it becomes challenging for generative\nmodels to fully interpret the intended video content. 2) The\nconditional image does not contain temporal information.\nThe absence of optical flow and temporal consistency can\neasily lead to the introduction of unsatisfactory artifacts in\nthe video generation process. Moreover, 3) there is a sig-\nnificant demand from users for the customization of videos,\nwhich entails the intuitive manipulation of video contents,\nsemantics, and motions. In response to these challenges, we\npropose a novel approach that improves the ability of ex-\nisting video generation models to better understand human\nintentions and generate videos guided by more detailed and\nmultifaceted human instructions.\nRecently, remarkable progress in large language models\nhas drawn widespread attention across the community [2].\nOne key to the success of large language models is learn-\ning from human feedback through reinforcement learn-\ning [5, 20, 23, 32] which significantly improves the perfor-\nmance of language models and leads to superior generation\nresults. Pioneers in the visual content generation field have\nalso introduced human feedback to generate high-quality\nimages [41]. Nonetheless, the intricacy, diversity, and level\nof control required for video generation far surpass those\nneeded for single-image generation, making it a highly sig-\nnificant yet relatively underexplored challenge.\nTo address these challenges, we propose InteractiveV-\nideo, a user-centric video generation framework that em-\npowers users to actively participate in the generation pro-\ncess through multimodal instructions, enabling control over\nvideo content, semantics, and motions. Users can customize\na video through various manipulations such as painting and\ndragging, as illustrated in Figure 1.\nMore specifically,\nwe propose a Synergistic Multimodal Instruction mecha-\nnism that empowers generative models to interpret and act\nupon users\u2019 editing and revision instructions across vari-\nous facets, such as video content, regional semantics, ob-\nject motion, subjects, and the overall dynamics of the video.\nIn our framework, we capture user interactions in the form\nof image, text, motion, and trajectory prompts, and we in-\ncorporate these user instructions as independent conditions\nof probabilistic models.\nAs a result, InteractiveVideo is\na training-free framework that can be easily and flexibly\napplied to different fundamental generative models. It is\nworth noting that our proposed framework seamlessly inte-\ngrates with existing generative models and practical tech-\nniques, such as Stable Diffusion [25], DreamBooth [27],\nand LoRA [17], thus expanding the video generation ca-\npabilities with our interactive framework.\nIn this framework, user interactions are involved through\nfour distinct types of instructions which can be employed\nindependently or collaboratively to effectively guide the\nvideo generation process. The four types of instructions are:\n\u2022 Image Instruction: the image condition or prompt for\nimage-to-video generation.\n\u2022 Content Instruction: a textual description of the visual el-\nements and the painting edits of the user to control the\nvideo content.\n\u2022 Motion Instruction: a textual description specifying the\ndesired movements and dynamics of elements within the\nvideo.\n\u2022 Trajectory Instruction: user-defined motion trajectories\nfor specific video elements, expressed through interac-\ntions such as dragging.\nBy incorporating these detailed and multidimensional\nhuman instructions, we can generate videos that better align\nwith the unique preferences and requirements of users.\nWe compare our InteractiveVideo with the advanced\nvideo generation solutions, e.g. Gen-21, I2VGen-XL [48],\nand Pika Labs. Comparison results in Figure 2, 7, 8, 9 show\nthe superiority of InteractiveVideo with higher quality, bet-\nter flexibility, and richer controllability. Our InteractiveV-\nideo paves the way for a novel paradigm in visual content\ngeneration, integrating user interactions to enable highly\ncustomized video generation. This empowers users to ef-\nfortlessly obtain high-quality videos they desire through in-\ntuitive manipulation and effective interactions.\nIn summary, our contributions are as follows:\n\u2022 Framework Deisgn:\nwe propose a novel interactive\nframework that empowers users to precisely control video\ngeneration by intuitive manipulations.\n\u2022 Generation Algorithm: we propose a Synergistic Mul-\ntimodal Instructions mechanism, which integrates user\nprompts as probabilistic conditions and enables interac-\ntion without the need for additional training.\n\u2022 High-quality Video Generation: our generation results\ndemonstrate superiority over state-of-the-art video gen-\neration methods, including Gen-2, I2VGen-XL [48], and\nPika Labs.\n1https://research.runwayml.com/gen2\n3\n2. Related Work\n2.1. Video Generation\nInitial attempts in video generation primarily leveraged\nGenerative Adversarial Networks (GANs) [9, 16, 21, 24,\n29, 33, 37, 44] and Variational Autoencoders (VAEs) [21,\n22, 42]. These methods, however, faced considerable chal-\nlenges in effectively modeling the intricate spatio-temporal\ndynamics necessary for text-driven video generation, leav-\ning the problem largely unsolved. Subsequent innovations\nshifted towards diffusion models [13, 30, 31, 47] to enhance\ndiversity and fidelity in video outputs [1, 3, 7, 14, 15, 18,\n26, 40, 43, 45, 46, 51] and to scale up pre-training data and\nmodel architecture [6, 14, 16, 28, 36, 49]. Recent efforts\nhave introduced spatio-temporal conditions [4, 7, 34, 43],\nfor instance, through VideoComposer [34], Gen-1 [7] and\nDragNUWA [43]. These methods aim to provide a more\ncontrolled generation process but still encounter constraints\nin achieving flexible and user-satisfied video synthesis.\n2.2. Models Guided by Human Feedback\nThe idea of learning from human feedback, initially inves-\ntigated in reinforcement learning and agent alignment con-\ntexts [5, 20], was subsequently applied to large language\nmodels [23, 32]. This approach has significantly improved\nthe generation of textual outputs that are helpful, honest,\nand harmless. In the field of visual content generation, par-\nticularly in video generation and editing, a similar goal is\npursued. [19, 38, 41] demonstrates the great potential of\nhuman guidance for the visual content generation field.\nThese works collectively highlight the growing trend of\nincorporating human feedback in various forms of genera-\ntive models, extending its utility from text-based to visual\ncontent generation. However, learning from human feed-\nback for video generation remains under-explored owing to\nits complicated elements of motion, subjects, and spatial-\ntemporal dynamics. We aim to fill this gap, providing a\ntraining-free and user-friendly solution for elevating exist-\ning video generative models with effective human guid-\nance and generating more user-satisfying and higher-quality\nvideos.\n3. Methodology\n3.1. Preliminary\nAs shown in Figure 3, InteractiveVideo realizes con-\ntrollable video generation with two generative pipelines\nbased on latent diffusion models - 1) the text-to-image (T2I)\npipeline Pimg and 2) the image-to-video (I2V) pipeline\nPvideo.\nThe framework outputs a video containing NF\nframes {v1, v2, \u00b7 \u00b7 \u00b7 , vNF }. We denote the Image Instruc-\ntion by x \u2208 RC\u00d7H\u00d7W , the Content Instruction by y, the\nMotion Instruction by y\u2032, and the Trajectory Instruction by\nr.\nMore specifically, the Trajectory Instruction is repre-\nsented by start and end points and region masks, which indi-\ncate the desired moving trajectories of specific objects. The\nwhole pipeline can be formulated as\n{v1, v2, \u00b7 \u00b7 \u00b7 , vNF } = Pvideo(Pimg(x, y), y\u2032, r).\n(1)\nIn practice, we may implement Pimg with any off-the-\nshelf T2I model as long as it takes a text condition and an\nimage condition as inputs. We use \u02dcx to denote its generated\nimage, i.e., the intermediate image, which is the input to the\nI2V model.\nWe then use \u02dcx as the image condition of the I2V pipeline\nand the Motion Instruction y\u2032 as the text condition. We\nmay use any off-the-shelf I2V diffusion models which re-\nquire image and text conditions. Let E be the image en-\ncoder of the I2V model, z0 = E(\u02dcx) be the corresponding\nlatent code, \u03f5t be the predicted noise at step t, the classic\n(i.e., interaction-free) video denoising process can be de-\nnoted with\nzt = \u221a\u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t \u00b7 \u03f5t,\n(2)\nwhere \u00af\u03b1t is a parameter related to the variance sched-\nule [12].\n3.2. Synergistic Multimodal Instructions\nWe control the video diffusion process with users\u2019 multi-\nmodal instructions via altering the predicted noise accord-\ning to the users\u2019 operations. Conceptually, with R denot-\ning the function that changes \u03f5t according to users\u2019 opera-\ntions, our interaction-controlled video diffusion process can\nbe represented by\nzt = \u221a\u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t \u00b7 R(\u03f5t).\n(3)\nThe proposed concrete implementation for the function\nR(\u00b7) is realized by treating user interactions as denoising\nresiduals. Since the intermediate image \u02dcx is utilized as the\ncondition image of Pvideo, it is seen as the \u201cinterface\u201d be-\ntween user and video generation model. Consequently, our\nframework empowers users to interact with the target video\nby introducing their interactions as new generation condi-\ntions of the video denoising process.\nSpecifically, we transform users\u2019 operations into denois-\ning residuals to eventually control the video diffusion pro-\ncess. Formally, in the video denoising process, assume the\noriginal intermediate image is \u02dcx and the corresponding la-\ntent code is z0 = E(\u02dcx). Once the user has operated on the\n4\nImage Instruction\nMotion Instruction\n\u201copening mouth \u201d\nContent Instruction\n\u201cA cute white cat \u201d\nx\ny\nTrajectory Instruction\nInteractiveVideo by Synergistic Multimodal Instructions\nImage\nPaint\nDrag\nText\nImage Diffusion\nVideo Diffusion\nCondition Image\n\u201cFlowers \u201d\nVideo \nInput\nControl\nEdit\nEdit\n..\nFigure 3. Framework Illustration. In InteractiveVideo, users can utilize multimodal instructions to interact with generative models on\nvideo content, motion, and trajectory.\nimage (e.g., painted some lines or set some trajectories2),\nthe intermediate image changes accordingly, and we denote\nthe resultant intermediate image as \u02dcx\u2032 and the correspond-\ning latent code becomes z\u2032\n0 = E(\u02dcx\u2032). We use z\u2032\n0 to predict\nthe noise in the video diffusion process. Formally, let t be\nthe time step, \u03f5t be the noise predicted with zt\u22121 and \u03f5\u2032\nt be\nthe noise predicted with z\u2032\nt\u22121, the noise we use is given by\n\u02c6\u03f5t = \u03bb \u00b7 \u03f5t + (1 \u2212 \u03bb) \u00b7 \u03f5\u2032\nt ,\n(4)\nwhere \u03bb is a hyper-parameter to balance the learned noise\nresidual and human instructions. Then we use \u02c6\u03f5t, instead\nof the original \u03f5t, in the denoising process to generate the\neventual video.\nNote that after the user operations on the image, the\nhuman-crafted discrepancy may affect the temporal coher-\nence of the resulting video. This is because the user oper-\nations may have deviated the intermediate image from the\ndistribution on which the I2V model was trained (e.g., the\nuser has drawn a twisted yellow curve to create a sun in\nthe sky, which is unusual in the training data of the I2V\nmodel).\nTo solve this problem, upon the completion of\nthe video diffusion process, we post-process the resultant\nvideo following AnimateDiff [10]. Every single frame is\naligned with the intermediate image via a Group Normal-\nization [39] layer, a SiLU [11] activation, and a 2D con-\nvolutional layer adopted from AnimateDiff or PIA [50], as\nsuch structures are found to generalize well to our common\n2Painting and trajectory drawing affect the intermediate image in dif-\nferent ways. The former makes a difference on the intermediate image\nthrough the T2I pipeline as it changes the very beginning Image Instruc-\ntion. The latter moves the handle points within the specified region to the\ntarget points and changes the optical flow of the intermediate image.\ndistribution produced by users\u2019 typical operations. Specifi-\ncally, the eventual i-th video frame v\u2032\ni can be computed as:\nv\u2032\ni = Conv2D(SiLU(GroupNorm(vi \u2212 \u02dcx))) .\n(5)\n4. Experiments\nIn this section, we present features of the Interactive-\nVideo framework including personalization (\u00a7 4.1), fine-\ngrained video editing (\u00a7 4.2), and precise motion control\n(\u00a7 4.3).\nBesides, we also conduct quantitative analysis\n(\u00a7 4.4) on generation quality and user study on the satis-\nfaction rates with our framework. Then, we demonstrate\nthe generation efficiency (\u00a7 4.5) of our framework.\n4.1. Personalizing a Video\nExisting methods [7, 10, 34] have made significant\nprogress in the animation of static images into videos. How-\never, these methods are limited to animating objects or\nscenes already present in the original static images, and en-\ncounter difficulties when it comes to generating a video with\nobjects or scenes absent from the referenced images. In\nother words, existing methods have limited ability to control\nvideo content, especially when users want to add or animate\npreviously unseen objects or scenes.\nWith InteractiveVideo, we enable video content manip-\nulation by incorporating abundant elements. In Figure 4,\nwe demonstrate that our framework supports the users to\ncustomize the video content freely. For example, we use a\nbrush to paint sketches of birds, waves, and polar lights in\nFigure 4 (a), (b), and (c), respectively. The added objects are\nseamlessly integrated and animated throughout the entire\n5\nInput\nUser Edit\nGenerated Video\n\u201cMorning, birds, birds are moving \u201d\n\u201cSea, the wave is crashing\u201d\n\u201cDark night, the polar lights are floating\u201d\n(a)\n(b)\n(c)\nFigure 4. Video Content Manipulation with InteractiveVideo. In (a), (b), and (c), we present the content manipulation by adding birds,\nwaves, and polar lights. Then, these added objects are driven in the whole video. We use these results to show the flexibility of our\nframework for video content creation.\nvideo. Seen from the following frames, InteractiveVideo\nenables users to create a video of satisfactory temporal con-\nsistency even though the referenced image does not directly\ncontain the objects. Meanwhile, such cases also demon-\nstrate the versatility and adaptability of our framework in\ncreating diverse and engaging video content, highlighting\nits potential for a wide range of applications in content cre-\nation and editing.\n4.2. Fine-grained Video Editing\nAnother significant limitation of current generation\nmethods is the challenge of performing precise regional\nediting. During the generation process, models have diffi-\nculty interpreting natural language references such as \u201cleft\u201d,\n\u201cright\u201d, \u201cup\u201d, and \u201cdown\u201d. This makes it hard to accurately\nedit regional semantics, which is crucial for user experi-\nence.\nFortunately, InteractiveVideo overcomes this limitation\nby enabling intuitive manipulation in the intermediate im-\nage. As illustrated in Figure 5 (a), it is difficult for users\nto edit the color of a specific tree or control the color of a\nparticular cluster of falling leaves using existing methods.\nIn contrast, our framework allows users to perform fine-\ngrained semantic editing on any region. For example, after\nthe editing process, the trees in Figure 5 (a), clouds in Fig-\nure 5 (b), and the logo in Figure 5 (c) can be easily modified.\nThe generated videos are of high quality, featuring realistic\nmotion, appropriate light reflection, and visually appealing\ntextures.\n4.3. Precise Motion Control\nMotion control, particularly precise motion control,\nposes a significant challenge in the field of video gener-\nation due to the complexity of modeling spatial-temporal\npatterns. The primary difficulty lies in maintaining the tem-\nporal consistency of generated videos, especially when han-\ndling substantial motion. This issue mainly stems from the\nlimited temporal receptive field of 1D temporal attention,\nwhich struggles to accommodate the full range of motion-\nrelated changes over time. As a result, ensuring smooth and\nconsistent representation of motion in generated videos re-\nmains a considerable obstacle in this field. Differently, In-\nteractiveVideo excels in precise motion control, which we\nwill discuss from three aspects as follows:\n1) Large Motion. As shown in the first two rows of Fig-\nure 6 first two rows, we present the large motion control\nby turning around characters in both realistic and cartoon\nstyles. The details of turning around the female character\nare impressive, with the motion of her hair appearing highly\nrealistic.\n6\nInput\nUser Edit\nGenerated Video\n\u201cWindy, red leaves are flying in the wind\u201d\n\u201cView, colorful clouds are moving\u201d\n\u201cThe batman is taking off his disguise, he is superman\u201d\n(a)\n(b)\n(c)\nFigure 5. Fine-grained Video Editing with InteractiveVideo. In (a), (b), and (c), we perform fine-grained regional semantic editing\non changing colors and appearances of specific objects, These results show the outstanding controllability of our framework for video\ngeneration.\n2) Precise Motion. As seen in the third row of Figure 6,\nthe adorable corgi holding the \u201cINTERACTIVE VIDEO\u201d\nbrand displays several different charming gestures, includ-\ning wagging its tail, smiling with an open mouth, turning its\nhead, and shaking its ears.\n3) Multi-Object Motion. The last two rows in Figure\nFigure 6 showcase the ability of InteractiveVideo to control\nmulti-object motion. Our framework precisely controls the\nmovements of both the cute girl and the lovable dog. When\nadjusting the dog\u2019s head, its tail also wags, and the girl natu-\nrally lowers her hand. While controlling these two objects,\nthe girl smiles sweetly, and the dog turns its head to the\nother side.\n4.4. Quantitative Analysis\nAnimateBench.\nSince\nInteractiveVideo\nis\na\ngeneral\nframework for open-domain video generation, we use An-\nimateBench for comparison. We assessed the text-based\nvideo generation capability using 105 unique cases with\nvarying content, styles, and concepts. These cases were\ncreated using seven distinct text-to-image models, with five\nimages per model for thorough comparison. Additionally,\nwe crafted three motion-related prompts for each image to\nevaluate motion controllability across different methods, fo-\ncusing on potential single-shot image motions.\nEvaluation Metrics.\nWe evaluate generation quality by\nconsidering image and text alignment, using CLIP scores\nto measure cosine similarity between embeddings. Image\nalignment compares input images and video frame embed-\ndings, while text alignment examines text and frame em-\nbedding similarities.\nMethods\nCLIP Score\nUser Study\nSatisfaction Rate\nImage\nText\nImage\nText\n(%)\nVideoComposer[35]\n225.3\n62.85\n0.180\n0.110\n43.5\nAnimateDiff[10]\n218.0\n63.31\n0.295\n0.220\n51.6\nPIA [50]\n225.9\n63.68\n0.525\n0.670\n52.5\nInteractiveVideo [Ours]\n234.6\n65.31\n0.745\n0.813\n72.8\nTable 1. Quantitative comparison on AnimateBench.\nUser Study.\nTo substantiate the enhancement of our\nmethod in terms of visual quality and user experience, we\ncarried out a user study comparing our approach with other\nvideo models. This study utilized 40 prompts from Ani-\nmateBench, which feature a variety of scenes, styles, and\nobjects. Compared to existing video generation methods,\nour InteractiveVideo notably outperforms in terms of human\npreference scores and delivers state-of-the-art performance\nin user satisfaction rates. These quantitative results, coupled\nwith the user study, effectively demonstrate the significance\nand superiority of the interactive generation paradigm and\n7\n\u201c1Man, he is turning his body\u201d\n\u201c1Girl, she is turning gradually\u201d\n\u201cThe dog is smiling\u201d\n\u201cThe girl is smiling, the dog is turning its head\u201d\n\u201cThe dog is turning its head\u201d\nFigure 6.\nPrecise Motion Control of InteractiveVideo.\nOur\nframework shows strong controllability in large motion control,\nprecise gesture control, and multi-object motion control.\nuser-centric designs.\n4.5. Generation Efficiency\nInteractiveVideo takes only 16GB CUDA memory in the\ninference process, and it runs on a single RTX 4090. Be-\nsides, in Table 2, we also report the latency of InteractiveV-\nideo. It is worth noting that InteractiveVideo can generate a\nvideo within about 12 seconds though it requires two inde-\npendent diffusion models for better controllability.\nProcess\nImage Instruction\nContent Instruction\nMotion Instruction\nTrajectory Instruction\nTime\n19.34ms\n31.47ms\n12.22s\n77.35 ms\nTable 2. Latency Analysis of InteractiveVideo.\n5. Responsible AI and Ethic Claim\nIn developing InteractiveVideo, our research rigorously ad-\nheres to the principles of Responsible AI and ethical guide-\nlines. This innovative framework for video generation is\ndesigned with a strong commitment to ethical AI practices,\nensuring that user interactions with the system - through\ntext, images, and direct manipulation - are processed with\nthe utmost integrity and transparency. The implementation\nof our Synergistic Multimodal Instruction mechanism is a\ntestament to our dedication to these principles. It not only\nfacilitates a seamless integration of diverse user inputs but\nalso ensures that the AI operates within ethical boundaries,\navoiding biases and respecting user intent. By empowering\nusers to interactively manipulate the video generation pro-\ncess, InteractiveVideo promotes not just creativity but also\nresponsibility in AI use. This approach aligns with our com-\nmitment to uphold ethical standards in AI, ensuring that In-\nteractiveVideo serves as a model for responsible innovation\nin the realm of AI-driven content creation\n6. Conclusion and Discussion\nIn summation, we introduce InteractiveVideo, a novel\nparadigm shift in the domain of video generation that\nchampions a user-centric approach over the conventional\nmethodologies reliant on pre-defined images or textual\nprompts.\nThis framework is distinguished by its capac-\nity to facilitate dynamic, real-time interactions between the\nuser and the generative model, enabled by a suite of intu-\nitive interfaces including, but not limited to, text and im-\nage prompts, manual painting, and drag-and-drop capabil-\nities.\nCentral to our framework is the innovative Syner-\ngistic Multimodal Instruction mechanism, a testament to\nour commitment to integrating multifaceted user interaction\ninto the generative process cohesively and efficiently. This\nmechanism augments the interactive experience and signif-\nicantly refines the granularity with which users can influ-\nence the generation outcomes. The resultant capability for\nusers to meticulously customize key video elements to their\nprecise preferences, coupled with the consequent elevation\nin the visual quality of the generated content, underscores\nthe transformative potential of InteractiveVideo in the land-\nscape of video generation technologies.\nDiscussion on the Computational Efficiency. Notwith-\nstanding the promising advancements heralded by Inter-\nactiveVideo, the adoption of a user-centric generative ap-\nproach is not devoid of challenges. Paramount among these\nis the imperative to ensure the framework\u2019s accessibility and\nintuitive usability across a broad spectrum of users, along-\nside maintaining the generative models\u2019 efficacy and com-\nputational efficiency amidst diverse and dynamic input sce-\nnarios. Future research endeavors might fruitfully focus on\n8\nthe refinement of these models to enhance scalability and\nthe development of adaptive algorithms capable of more ac-\ncurately interpreting and actualizing user intentions.\nFuture Works. We may delve into several promising direc-\ntions. Enhancing the AI\u2019s understanding of complex user\ninputs, such as emotional intent or abstract concepts, could\nlead to more nuanced and contextually relevant video gen-\neration. Additionally, exploring the integration of real-time\nfeedback loops where the model suggests creative options\nbased on user input history could further personalize the\nuser experience. Investigating the application of this frame-\nwork in virtual and augmented reality environments opens\nup new dimensions for immersive content creation. Fur-\nthermore, extending the framework\u2019s capabilities to include\ncollaborative generation where multiple users can interact\nand contribute to a single generative process may revolu-\ntionize co-creation in digital media.\nFurther Applications. The potential applications of Inter-\nactiveVideo extend well into the realms of education, where\nbespoke video content could significantly enrich the ped-\nagogical experience and entertainment, particularly in the\ncreation of interactive narratives. As we continue to iterate\nupon and enhance this framework, the scope for its appli-\ncation appears limitless, heralding a future in which video\ngeneration transcends mere content creation to become a\nconduit for deep, interactive engagement between creators\nand their digital canvases.\n9\nAppendix\n\u201cThe girl is smiling, the dog is turning its head\u201d\n\u201cThe man is turning gradually\u201d\nFigure 7. Comparison with existing methods on motion control. We compare InteractiveVideo (4th row) with Pika Labs (1st row),\nI2VGen-XL (2nd row), and Gen-2 (3rd row).\n10\n\u201cThe clouds are moving\u201d\n\u201cThe tide is flooding\u201d\nFigure 8. Comparison with existing methods on landscapes. We compare InteractiveVideo (4th row) with Pika Labs (1st row), I2VGen-\nXL (2nd row), and Gen-2 (3rd row).\n11\n\u201cThe clouds are becoming dark\u201d\n\u201cThe snow is melting\u201d\nFigure 9. Comparison with existing methods on dynamic scenes. We compare InteractiveVideo (4th row) with Pika Labs (1st row),\nI2VGen-XL (2nd row), and Gen-2 (3rd row).\n12\nReferences\n[1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 4\n[2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie\nZhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang,\nYidong Wang, et al. A survey on evaluation of large language\nmodels. arXiv preprint arXiv:2307.03109, 2023. 3\n[3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration, 2023. 2, 4\n[4] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models,\n2023. 2, 4\n[5] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement learn-\ning from human preferences. Advances in neural information\nprocessing systems, 30, 2017. 3, 4\n[6] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin\nSong, Xiangyu Yue, and Ying Shan. Unireplknet: A univer-\nsal perception large-kernel convnet for audio, video, point\ncloud, time-series and image recognition.\narXiv preprint\narXiv:2311.15599, 2023. 4\n[7] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 2, 4, 5\n[8] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 2\n[9] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-\nChyi Su, William Yang Wang, and Sean Bell.\nTell me\nwhat happened: Unifying text-guided video completion via\nmultimodal masked video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10681\u201310692, 2023. 4\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 5, 7\n[11] Dan Hendrycks and Kevin Gimpel.\nGaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 5\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 4\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2, 4\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 4\n[15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 4\n[16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 4\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3\n[18] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 4\n[19] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu.\nAligning text-\nto-image models using human feedback.\narXiv preprint\narXiv:2302.12192, 2023. 4\n[20] Jan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg.\nScalable agent alignment\nvia reward modeling: a research direction. arXiv preprint\narXiv:1811.07871, 2018. 3, 4\n[21] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 4\n[22] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubrama-\nnian. Sync-draw: Automatic video generation using deep\nrecurrent attentive architectures. In Proceedings of the 25th\nACM international conference on Multimedia, pages 1096\u2013\n1104, 2017. 4\n[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 3, 4\n[24] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao\nMei. To create what you tell: Generating videos from cap-\ntions. In Proceedings of the 25th ACM international confer-\nence on Multimedia, pages 1789\u20131798, 2017. 4\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 3\n[26] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu,\nJianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining\nGuo. Mm-diffusion: Learning multi-modal diffusion mod-\nels for joint audio and video generation. In Proceedings of\n13\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10219\u201310228, 2023. 2, 4\n[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3\n[28] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 4\n[29] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3626\u20133636, 2022. 2, 4\n[30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Computer Vision, pages 2256\u20132265. PMLR, 2015.\n2, 4\n[31] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 4\n[32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and\nPaul F Christiano. Learning to summarize with human feed-\nback. Advances in Neural Information Processing Systems,\n33:3008\u20133021, 2020. 3, 4\n[33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n4\n[34] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 4, 5\n[35] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 7\n[36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 4\n[37] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In European Con-\nference on Computer Vision, pages 720\u2013736. Springer, 2022.\n4\n[38] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-\nsheng Li.\nHuman preference score: Better aligning text-\nto-image models with human preference.\nIn Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 2096\u20132105, 2023. 4\n[39] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European conference on computer vision\n(ECCV), pages 3\u201319, 2018. 5\n[40] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion priors.\n2023. 2, 4\n[41] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation.\narXiv preprint arXiv:2304.05977,\n2023. 3, 4\n[42] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 4\n[43] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 4\n[44] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos\nwith dynamics-aware implicit generative adversarial net-\nworks. arXiv preprint arXiv:2202.10571, 2022. 4\n[45] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18456\u201318466,\n2023. 4\n[46] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2023. 4\n[47] Qinsheng Zhang, Molei Tao, and Yongxin Chen.\ngddim:\nGeneralized denoising diffusion implicit models.\narXiv\npreprint arXiv:2206.05564, 2022. 2, 4\n[48] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2, 3\n[49] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng\nLi, Yu Qiao, Wanli Ouyang, and Xiangyu Yue.\nMeta-\ntransformer: A unified framework for multimodal learning.\narXiv preprint arXiv:2307.10802, 2023. 4\n[50] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing\nFang, and Kai Chen. Pia: Your personalized image animator\nvia plug-and-play modules in text-to-image models. arXiv\npreprint arXiv:2312.13964, 2023. 5, 7\n[51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 4\n14\n"
  },
  {
    "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
    "link": "https://arxiv.org/pdf/2402.03310.pdf",
    "upvote": "14",
    "text": "V-IRL: Grounding Virtual Intelligence in Real Life\nJihan Yang1*\nRunyu Ding1\nEllis Brown2\nXiaojuan Qi1\nSaining Xie2\n1The University of Hong Kong\n2New York University\nhttps://virl-platform.github.io\n\u201cSeamless schedules, \nsignature service!\u201d\n\u201cBeep beep! Clean \nstreet ahead!\u201d\nSan Francisco\nNew York\nLondon\nParis\nTokyo\nMelbourne\nBuenos Aires\nRio de Janeiro\nLagos\nMumbai\nHong Kong\n\u201cBuilding the future, one \nblueprint at a time!\u201d\n\u201cDash, deliver, done!\u201d\n\u201cFrom star-ratings to \nyour dining!\u201d\n\u201cNew city, new chapter, \nendless curiosity!\u201d\n\u201cLost in wonder. \nShow me the way?\u201d\nYour perfect home\nawaits here!\nFigure 1. V-IRL agents leverage real-world geospatial information and street view imagery to navigate urban terrains, execute complex\ntasks, and interact in real-time scenarios. From recommending relevant destinations to assessing city infrastructure to collaboratively\ngiving & following verbal directions\u2014we develop agents that illustrate V-IRL\u2019s current capabilities, flexibility, and utility. Above all else,\nwe present a flexible platform for researchers to harness abundant data from across the globe to create and test diverse autonomous agents.\nAbstract\nThere is a sensory gulf between the Earth that humans\ninhabit and the digital realms in which modern AI agents\nare created. To develop AI agents that can sense, think, and\nact as flexibly as humans in real-world settings, it is im-\nperative to bridge the realism gap between the digital and\nphysical worlds. How can we embody agents in an envi-\nronment as rich and diverse as the one we inhabit, without\nthe constraints imposed by real hardware and control? To-\nwards this end, we introduce V-IRL: a platform that enables\nagents to scalably interact with the real world in a virtual\nyet realistic environment. Our platform serves as a play-\nground for developing agents that can accomplish various\npractical tasks and as a vast testbed for measuring progress\nin capabilities spanning perception, decision-making, and\ninteraction with real-world data across the entire globe.\n*Work conducted during a visit to NYU.\n1. Introduction\nThe advent of large language models (LLMs) has breathed\nnew life into autonomous agent research by offering a uni-\nversal interface for diverse capabilities, ranging from basic\nreasoning to complex planning and tool use [72]. While\nthese developments are promising, most of these agents\nremain confined to text-based environments or simplis-\ntic simulations. Visual components in existing agents are\neither rudimentary\u2014such as simulated tabletop environ-\nments [11, 28]\u2014or rely on abstracted representations using\nground-truth APIs [27, 67]. Furthermore, the prevalent vi-\nsual models employed by these agents are trained on photo-\ngenic, object-centric Internet images, which fail to capture\nthe unpredictability and diversity of real-world scenes.\nThis paper aims to bridge this gap between AI agents\nand the sensory world by grounding them in rich, real-\nworld environments\u2014a crucial step towards developing au-\ntonomous agents that can effectively operate in real-life sce-\n1\narXiv:2402.03310v1  [cs.AI]  5 Feb 2024\nnarios. Our novel setting for AI agents necessitates rich sen-\nsory grounding and perception: virtual embodiment within\ncities around the globe using real visual and geospatial data.\nTo this end, we introduce V-IRL, a versatile platform for\nbuilding and testing virtual agents within this novel virtual-\nreal-world setting.\nV-IRL harnesses the power of map-\nping and street view data, enabling agents to navigate real-\nworld locations, access up-to-date information about their\nsurroundings, and perform practical tasks. With geospatial\ncoordinates at its core, V-IRL is flexible and extensible, inte-\ngrating with arbitrary geospatial platforms and APIs. More-\nover, V-IRL opens up a vast sea of visual data, allowing a\nsimple and extensible way for researchers to evaluate vision\nmodels on realistic data distributions.\nWe demonstrate the versatility and adaptability of V-IRL\nby developing a series of diverse exemplar agents, each\nsolving a unique and practical task. As these agents hinge\nupon foundational language and vision models, it is critical\nto evaluate these models within this setting and their impact\non agent performance. We leverage the vast data available\nthrough our platform to develop global scale benchmarks\nmeasuring the performance of underlying vision models\non images from diverse geographic and cultural contexts\u2014\nevaluating their adaptability to shifting environmental, ar-\nchitectural, and language-specific elements. Furthermore,\nwe evaluate the contributions of models to agent perfor-\nmance on challenging tasks. Our results illustrate the po-\ntential of V-IRL in bridging the gap between virtual agents\nand visually rich real-world environments, paving the way\nfor future research in this direction.\nIn summary, our contributions are:\n\u2022 V-IRL: an open-source platform for building and testing\nagents in a real-world setting that necessitates rich sen-\nsory grounding and perception\u2014embodiment using real\ngeospatial data and street-view imagery.\n\u2022 Development of diverse exemplar agents that showcase\nthe platform\u2019s versatility and adaptability.\n\u2022 Global benchmarks measuring the performance of foun-\ndational language and vision models (1) in isolation us-\ning our platform\u2019s real-world data and (2) on end-to-end\nagent performance in challenging tasks. In addition, we\ndiscuss the robustness of \u201copen-world\u201d vision models\nto real-world data from across the globe.\nWe are excited to see how the research community will\nleverage V-IRL to develop and test agents that can under-\nstand and interact with the real world.\n2. Related Work\nHere, we ground V-IRL to three streams of research.\nAI Agents. Agents are autonomous entities capable of per-\nceiving their environment and acting to achieve goals [69].\nHistorically, agent development has leveraged symbolic and\nreinforcement learning methods [9, 30, 48], which face is-\nsues of scalability and real-world utility. In contrast, the\nnew wave of LLM-driven agents overcomes these chal-\nlenges with text as a universal interface, enabling natural\nhuman interaction and adaptability to various tasks [49,\n62, 63, 68, 77].\nMoreover, these models equip agents\nwith complex capabilities, such as tool use and collabora-\ntion [26, 35, 50, 55, 67, 71, 84]. Yet a critical limitation\npersists: the agents in this new wave are entirely text-based,\ndevoid of any tangible connection to the visual or sensory\naspects of the real world.\nEmbodied AI. Embodied AI studies intelligent agents &\nrobots perceiving and interacting with their environment. A\nsignificant challenge in this field is the acquisition of large\nquantities of realistic data. Consequently, robots are primar-\nily trained in simulated environments [12, 46, 54, 73, 74]\nto develop skills such as navigation [4, 5, 13] and manip-\nulation [25, 79]. Recent advancements in LLMs [2, 6, 66]\nhave enabled embodied agents to perform long-horizon and\nopen-end tasks in game-engines [27, 28, 39, 45, 60] or hu-\nman rooms [10, 11, 19, 29, 38]. However, the diversity of\ntasks and data is still too narrow and simplistic to enable\nthem to operate flexibly in diverse real-world environments.\nOpen-World Computer Vision.\nMotivated by the suc-\ncess of vision-language models [3, 8, 51, 80] pre-trained\non large-scale web-crawled data [16, 32, 56, 61, 65, 75],\nopen-world computer vision has received increasing atten-\ntion in recent years [23, 33, 34, 37, 47, 76, 82]. However,\nimages and benchmarks sourced from the Internet [7, 18,\n21, 31, 33, 53] are unavoidably biased towards specific dis-\ntributions rather than truly reflecting the real world [52].\nBecause they are trained and evaluated entirely on Inter-\nnet data, existing \u201copen-world\u201d models are effectively more\nopen-Internet than open-world.\n3. Virtual Intelligence in Real Life\nTo demonstrate the versatility of V-IRL, we use it to instan-\ntiate several exemplar agents in our virtual real-world envi-\nronment. In this section, we engage these agents with tasks\nthat highlight various capabilities of our platform. In Sec. 4,\nwe discuss the technical details of our platform and how it\nenables agents to interact with the real world.\nFor illustration, we give V-IRL agents character meta-\ndata, including an 8-bit avatar, a name, a short bio, and an\nintention they are trying to accomplish. More concretely,\nagents are defined by pipelines that use this character meta-\ndata along with our platform\u2019s API and pretrained models to\naddress complex tasks (see Sec. 4). Here we provide a high-\nlevel overview of the tasks, highlight the V-IRL capabilities\nthey require, and visualize the agents solving them.\nWe highlight the specific V-IRL capabilities being em-\nployed throughout using tags and corresponding colored\nunderlines:\nENV\nMap \u2192 action,\nLM\nLLM \u2192 reasoning,\nCV\nVision \u2192 perception, &\nCOL\nColab \u2192 collaboration.\n2\n3.1. Earthbound Agents\nV-IRL agents inhabit virtual representations of real cities\naround the globe. At the core of this representation are geo-\ngraphic coordinates corresponding to points on the Earth\u2019s\nsurface.\nUsing these coordinates, V-IRL allows virtual\nagents to ground themselves in the real world using maps,\nstreet view imagery, information about nearby destinations,\nand additional data from arbitrary geospatial APIs.\nRoute Optimizer\nENV\nMap\nName: Peng\nAge: 21\nLoc: NYC\nBio:\nOriginally from Chengdu, Sichuan, Peng is a student at PKU. He just\narrived for a semester abroad at NYC, and is couch surfing until he gets settled.\nIntention:\nPeng needs to visit five locations around the city: his University\nCard Center, Residence Hall, Research Center, Library, and Student Center.\nTask:\nGiven a starting address and a list of waypoints, plan the\nshortest route to all waypoints and then follow it on street view.\nTakeaway:\nV-IRL instantiates agents with real geospatial infor-\nmation, and enables useful tasks like route optimization.\nPeng needs to visit several locations throughout the city to\nget documents signed for registration as a visiting student. . .\nLeveraging Geolocation & Mapping capabilities, Peng\nsaves 7 minutes by walking along the shortest path as op-\nposed to in order waypoint visitation as shown in Fig. 2.\nSTART\nEND\n1\n2\n3\n4\nSTART\nEND\n3\n1\n2\n4\n42.7 min, 3.1 km\n36.0 min, 2.6 km\n9.7 min, 2.9 km\n4\n2\n1\n3\nEND\nSTART\nShortest path routing\nSequential routing\nShortest path routing\nFigure 2. Finding the shortest path for Peng to travel to five places.\n3.2. Language-Driven Agents\nTo tackle more complex tasks, we follow the pattern of\nlanguage-driven agents [72]. LLMs enable agents to flex-\nibly reason, plan, and use external tools & APIs.\nPlace Recommender\nENV\nMap\nLM\nLLM\nName: Aria\nAge: 26\nLoc: NYC\nBio:\nA 3rd year graduate student who loves to try new restaurants. She is\nalways looking for new places to try, and shares her favorite spots on her blog!\nIntention:\nPick out a lunch spot that Peng might like.\nName: Vivek\nAge: 35\nLoc: NYC\nBio:\nA tech-savvy estate agent who combines his local knowledge with online\ntools like Zillow to find the perfect homes for his clients in the bustling city.\nIntention:\nHelp Peng find a place to live for the semester.\nTask:\nGiven specific location, background, and intention, synthe-\nsize reviews of nearby businesses to provide a recommendation.\nTakeaway:\nV-IRL exposes rich real-world information to agents\nthat they can use for real-world tasks.\nPeng is starving for some lunch but doesn\u2019t know where to\neat. . . Luckily, he met a nice grad student Aria during his er-\nrands who might be able to help him find a good spot.. .\nPersonalized Rating: 8 \ud83d\udc4d\nPersonalized Rating: 2.5\ud83d\udc4e\nPersonalized Rating: 7.5\ud83d\udc4d\n\u5938\u7236\u70b8\u4e32 Kwa Food \nDeep Fried Skewers\nTartinery Caf\u00e9 \u2013 Bar\n| Greenwich Village\nDos Toros Taqueria\nMexican\nSTART\nPersonalized Rating: 8 \ud83d\udc4d\nPersonalized Rating: 2.5 \ud83d\udc4e\nPersonalized Rating: 7.5 \ud83d\udc4d\n\u5938\u7236\u70b8\u4e32 Kwa Food \nDeep Fried Skewers\nTartinery Caf\u00e9 \u2013 Bar\n| Greenwich Village\nDos Toros Taqueria\nMexican\nSTART\nAria searches for possible\nrestaurants nearby. She then\nsynthesizes public reviews\nto make final recommenda-\ntions via GPT-4.\nAs Peng\nis new to the city and orig-\ninally from Sichuan, she recommends a spicy Chinese joint\nKwa Food Deep Fried Skewers to give him a taste of home.\nPeng hires Vivek to help him find an apartment in East Vil-\nlage, Jersey City, or Long Island City for $1k\u2013$3k per month\nclose to a gym, supermarket, and public transit. . .\nRecommendations\nPersonalized rating: 8/10 \ud83d\udc4d\nThe apartment is well-located near a\nsupermarket and gym, which aligns with\nPeng's lifestyle. Multiple bus stations are\nnearby, but the lack of a close subway\nstation may affect his commute.\n\"address\": 155 Washington St, \nJersey City, NJ 07302, \u201d rent\": \n$2643, \"type\": Apartment, \u201dsqft\": \n571, \"bedrooms\": 0, \"bathrooms\": \n1, \"year built\": 1992,\nPersonalized rating: 7.5/10 \ud83d\udc4d\nThe apartment is well-located with easy\naccess to supermarkets, public transport,\nand a gym, which aligns with Peng's\nrequirements. However, the price may\nnot be cost-effective for a student.\nRental Information\n\"address\": 42-18 28th St, Unit \n12E, New York, NY 11101, \n\u201drent\": $2904, \"type\": Apartment, \n\u201dsqft\": 450, \"bedrooms\": 0, \n\"bathrooms\": 1,\n\"address\": 37-14 32nd St, Unit \n508, New York, NY 11101,  \n\u201drent\": $1986,  \"type\": Apartment,  \n\u201dsqft\": 800,  \"bedrooms\": 1,  \n\"bathrooms\": 1,\nPersonalized rating: 2/10 \ud83d\udc4e\nThe estate lacks nearby supermarkets,\nbus, subway stations, and gyms, which\nare essential for Peng's requirements.\nVivek uses real es-\ntate APIs to find po-\ntential apartments in\nPeng\u2019s desired regions\nand price range.\nFor\neach candidate, he re-\nsearches its proxim-\nity to the places Peng\ncares about.\nSynthe-\nsizing these factors, Vivek provides a holistic rating and ac-\ncompanying reasoning using GPT-4. His top recommenda-\ntion is a cost-effective 1 bedroom apartment for $1986/mo,\nwhich is close to a supermarket, 2 bus stations, and a gym.\n3.3. Visually Grounded Agents\nAlthough language-driven agents can address some real-\nworld tasks using external tools, their reliance solely on\ntext-based information limits their applicability to tasks\nwhere visual grounding is required. In contrast, real sensory\ninput is integral to many daily human activities\u2014allowing a\ndeep connection to and understanding of the world around\nus. Agents can leverage street view imagery through the\nV-IRL platform to visually ground themselves in the real\nworld\u2014opening up a wide range of perception-driven tasks.\nUrban Assistance Robot\nENV\nMap\nCV\nVision\nName: RX-399\nAge: Unk.\nLoc: HK/NYC\nBio:\nThis urban robot\u2019s advanced object detection, localization, and naviga-\ntional telemetry systems allow it to perform perceptive tasks in busy city streets.\nIntention:\nReport the locations of trash bins to the sanitation dept.\nTask:\nTravel along a specified route and detect instances of a spec-\nified object (e.g., trash bins, hydrants, benches, etc.).\nTakeaway:\nV-IRL agents can use perceptive input to understand\nand interact with their environment.\nRX-399 is a state-of-the-art robot agent with advanced navi-\ngation and sensing capabilities. Its manufacturer is running\na pilot program with sanitation departments in Hong Kong\nand New York City to assess its readiness for garbage duty. . .\nRX-399 navigates along pre-defined city routes, tagging all\ntrash bins using its open-world detector and geolocation\nmodule as depicted in Fig. 4. RX-399 can actively adjust\nits camera pose to the optimal view for each potential ob-\n3\nHydrant\nBench\nBench\nHydrant\nTrash bin\nTrash bin\nTrash bin\nTrash bin\nFigure 3. Imani\u2019s visualization of trash bins, fire hydrants, & park benches in NYC\u2019s Central Park using data collected by RX-399.\nject thanks to our interactive embodied environment and the\nsensor-rich visual input. During the pilot in Hong Kong,\nRX-399 locates eight trash bins, correctly identifying five\nbut overlooking one. In New York, it accurately detects all\nfive trash bins but mistakenly reports two mailboxes.\nNYC\nHong Kong\nFigure 4. Portions of RX-399\u2019s system records in HK and NYC.\nRX-399 can avoid double-counting previously seen ob-\njects by using feature matching to check for duplicates\namong prior detections (see Fig. 5).\nFigure 5. RX-399 avoids double-counting trash cans by identify-\ning duplicates across different viewpoints using feature matching.\nUrban Planner\nENV\nMap\nCV\nVision\nName: Imani\nAge: 42\nLoc: NYC\nBio:\nA sustainable urban development graduate, Imani is passionate about\nmaintaining a harmonious balance between nature and urban ecosystems.\nIntention:\nUse RX-399 to collect first-person data for her studies.\nTask:\nRecord the location of all instances of any specified objects\n(e.g., trash bins, hydrants, benches, etc.) in a specified region.\nTakeaway:\nV-IRL enables realistic open-world applications re-\nquiring vast geospatial and first-person visual information.\nImani needs to analyze the distribution of trash bins, fire hy-\ndrants, and park benches in New York\u2019s Central Park for a\nproject with the NYC Parks & Recreation department. . .\nImani sets routes spanning Central Park and objects of in-\nterest for RX-399, who traverses the routes and records all\ndetected instances. After RX-399 finishes its route, Imani\nanalyzes the collected data at different levels of detail. As\ndepicted in Fig. 3, the coarsest level shows general distribu-\ntions of trash bins, hydrants, and benches in the park. Imani\ncan also zoom in to specific regions, where lighter colors\nrepresent positions with more unique instances identified.\nThe following table presents RX-399\u2019s counting report:\nCategory Trash Bin\nFire Hydrant\nPark Bench\u2217\nCount\n1059\n727\n1015\nTable 1. RX-399\u2019s counting report in Central Park, New York City.\n(\u2217Note: contiguous benches counted as one instance).\nBy retrieving geotagged sensory-rich data within RX-399,\nImani can also inspect the detection results for each object\nto help her verify the reliability of RX-399\u2019s reports as il-\nlustrated by the bottom level in Fig. 3.\n4\nIntentional Explorer\nENV\nMap\nLM\nLLM\nCV\nVision\nName: Hiro\nAge: 22\nLoc: HK\nBio:\nA seasoned traveler, Hiro thrives in unknown territories. He enjoys get-\nting lost in new places instead of following the travel guide.\nIntention:\nHiro is looking for an authentic lunch spot that is not too spicy.\nTask:\nExplore on foot (in street view) looking for a destination that\nfulfills a certain intention (e.g., lunch, shopping, etc.)\nTakeaway:\nAgents can utilize visual detectors, VLMs and LLMs\nto iteratively perceive, decide, and interact in the environment.\nHiro is starting a new journey in Hong Kong. He decides to\nexplore without a specific destination in mind, looking for a\ngood local lunch spot with food that\u2019s not too spicy...\nAs depicted in Fig. 6, starting at\n, Hiro walks down the\nstreet and encounters the first intersection. Thanks to the\ninteractive and sensory-rich environment, he can adjust his\npose to fetch street views for each possible path. Using\nVQA on these views, he decides to turn left:\nResidential buildings on the left road indicate cozy and\nfamily-run local food...A better choice than the others!\nThen, after exploring for a block, he encounters the second\nintersection where he looks around and decides to turn right:\nLooks like there are some local food spots this way. . .\nAfter a few steps, Hiro finds \u201cA One Chinese Noodles \u963f\u4e00\n\u8c6c\u6252\u9178\u8fa3\u7c73\u7dda\u201d using his open-world detector. He retrieves\ninformation, ratings, and reviews for the restaurant using\nour platform, which connects street views to places. Hiro\nultimately decides to pass on it and keep exploring because:\nMost reviews mention the spicy pork chop noodles. . .\nFinally, at the end of the block\n, Hiro discovers another\nlunch spot called \u201cXintianfa \u65b0\u5929\u767c\u201d. He decides to dine\nthere after reading numerous online reviews praising its au-\nthentic cuisine and diverse menu.\n[ACTION 2]\n\u201cI should turn right.\u201d\nrestaurant\n[ACTION 3]\n\u201cI\u2019ll pass. Keep exploring.\u201d\n[ACTION 1]\n\u201cI should turn left.\u201d\nrestaurant\n[EXPLORATION ENDS]\n\u201cLet\u2019s dine here!\u201d\n[EXPLORATION STARTS]\n\u201cLet\u2018s grab a bite to eat, I\u2019m hungry.\u201d\nA One\n\u963f\u2f00\u8c6c\u6252\u9178\u8fa3\u2f76\u7dab\nChinese Noodles\n\u65b0\u5929\u767c\nChinese Takeout\nFigure 6. Visualization for Hiro\u2019s lunch exploration in HK.\n3.4. Collaborative Agents\nHumans often work together to solve complex real-world\ntasks. This collaboration promotes efficiency and effective-\nness by decomposing a complex task into simpler sub-tasks,\nallowing each to be handled by an expert in its domain.\nGrounded in the world via our platform, V-IRL agents can\nleverage geospatial data and street view imagery to collab-\norate with other agents as well as with human users.\n3.4.1\nAgent-Agent Collaboration\nAs with previous agents, collaborative agents are designed\nfor specific tasks; however, they can handle objectives be-\nyond their expertise through collaboration with each other.\nTourist\nENV\nMap\nLM\nLLM\nCV\nVision\nCOL\nColab\nName: Ling\nAge: 25\nLoc: NYC/SF/HK\nBio:\nLing is a spirited traveler from Taipei who is always eager to explore new\ncities and cultures. She is unafraid of asking locals for help when she\u2019s lost!\nIntention:\nNYC: find gifts for friends back home; go to a famous restaurant.\nSF: find a store to repair a broken iPhone. HK: try some authentic local food.\nTask:\n(i) Ask a nearby Local agent for directions to a specific loca-\ntion. The Locals will preview the route on the map and in streetview\nand then provide walking directions in natural language, mentioning\nmajor intersections and landmarks.\n(ii) Follow these directions in streetview, and if lost, ask\nanother Local agent for assistance.\nTakeaway:\nAgents can collaborate to solve complex tasks that\nare beyond their individual expertise.\nLing travels to cities around the world. She seeks out authen-\ntic experiences and is always unafraid to ask for help from\nLocals whenever she finds herself lost. . .\nAfter obtaining route descriptions from Locals, Ling starts\nher journey\u2014as shown in Fig. 7. Grounded in our embod-\nied platform, Ling can adjust her pose and identify visual\nlandmarks along the streets using open-world recognition\nand her map. Correctly recognizing these landmarks helps\nGPT-4 to make correct decisions about where to change di-\nrection, move forward, and stop, as seen in the top two New\nYork City cases in Fig. 7. The success of these decisions\nmade by GPT-4 relies on the real-sensory input for visual\ngrounding and the interactive environment from V-IRL.\nNevertheless, Ling may occasionally fail to find the des-\ntination. In the bottom left San Francisco example in Fig. 7,\nLing passes by the Apple Store because only its stainless\nsteel wall is visible from her viewpoint. In the bottom right\nHong Kong example, Ling mistakes another restaurant for\nher destination and stops prematurely. Fortunately, when\nshe makes these mistakes, Ling can ask another Local agent\nfor new directions and start another round of navigation,\nwhich eventually leads her to the destination.\n5\nI still can\u2019t \ufb01nd the Apple \nStore. Maybe I should \ufb01nd \nsome more help\u2026\nHmm. I don\u2019t see the Apple \nStore on my right. Maybe \nit\u2019s a bit further ahead? \nAhh, I see the Apple \nStore on my left now!\nYou just went past the \nApple Store. You need \nto turn around. \ud83d\udcac\nOh no, my iPhone \nscreen just cracked!\nThere\u2018s an Apple Store just \na short walk from here. I \ncan guide you there: \u2b50\nSTART\n\u2b50 Try 1: First, turn around and head west. Continue straight until you reach the\nintersection, with a Starbucks on your left. Next, turn left to head south. Walk a \nbit further, and your destination, Apple Union Square, will be on your right.\n\ud83d\udcac Try 2: Turn around and head north. Walk straight for a short distance until \nyou reach the intersection. You will see the Apple Union Square, on your left.\nSan Francisco\nThere are two restaurants \non my left. I\u2019m guessing \none of them is Ka Hing?\nI eat at Ka Hing \nevery day! \u2b50\nIs Ka Hing \nRestaurant here?\nNo, you should keep \nwalking to the west. \ud83d\udcac\nAny good burger spots \naround here?\nBlack Tap is worth checking out! \nI'll tell you how to get there: \u2b50\nArrived!\nSTART\n\u2b50 First, turn right to face southeast and walk a short distance until you reach \nthe next intersection. You should see Famous Ben\u2019s Pizza on your left. Next, turn \nright to head southwest. Continue walking for a while until the next \nintersection. Janovic Paint & Decorating Center SoHo should be behind you on \nyour right. Finally, turn right again to face northwest and walk just a bit further. \nYour destination, Black Tap Craft Burgers & Beer - SoHo, will be on your left.\nNYC\nI\u2019ve found Ka\nHing, \ufb01nally!\nIs there a local \nrestaurant you\u2019d \nrecommend?\nSTART\n\u2b50 Try 1: First, turn to south and walk until you reach the intersection. You will \nnotice the McDonald on your left front. Then, take a right turn, and continue \nwalking west. Proceed until you see Ka Hing Restaurant on your left-hand side.\nNYC\nHong Kong\nWhere\u2019s a good place to buy \nsome gifts for my friends?\nThe MoMA Design Store is \nan excellent option! Let me \ntell you how to get there: \u2b50\nI\u2019ve found the\nMoMA Store!\nSTART\n\u2b50 First, turn left to face northwest and walk a short distance until you reach the \nnext intersection, with Prince St in front of you. Next, make a sharp left turn to \nhead southwest. Continue straight for a while until you reach the next \nintersection, where you'll see Aritzia on your left front. Finally, make a sharp left \nturn to head southeast and walk a bit further. You will find your destination, the \nMoMA Design Store, on your left.\n\ud83d\udcac Try 2: Facing west, walk a short distance until you spot Ka Hing Restaurant on \nyour left.\nMcDonald\u2019s\n\u9ea5\u7576\u52de\nFast Food $ \nKa Hing \n\u5609\u8208\u9910\u5ef3\nBlack Tap Cra6 Burgers \n& Beer \u2013 SoHo\nAmerican - $$ \nFamous Ben\u2019s Pizza\nStarbucks\nApple Union Square\nElectronics Store\nMoMA Design Store\nGi5 Shop\nAritzia\nWomen\u2019s clothing store\nN\nN\nN\nN\nFigure 7. Ling and Local collaboration examples. Trajectories in red and green mean Ling\u2019s first and second attempts, respectively.\n6\nEnergy\nHunger\nJoy\nStress\nPain\nSadness\n0%\n0%\n80%\n20%\n30%\n50%\n90%\n25%\n45%\n40%\n0%\n0%\n0%\n0%\n30%\n60%\n30%\n80%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n25%\n20%\n15%\n10%\n5%\n0%\n0%\n0%\n0%\n95%\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n1%\n45%\n15%\n15%\n25%\n35%\n45%\n50%\n30%\n20%\n85%\n75%\n95%\n90%\n80%\n75%\n65%\n63%\n83%\n73%\nMorning walk in the Washington Square Park.\n10:00 - 10:52\n09:00 - 10:00\n14:00 - 14:28\nTravel from Washington Square Park to The Cloisters. \nExplore The Cloisters and the surrounding Fort Tryon Park. \n12:30 - 13:30\n10:52 - 12:30\nHaving food in Jochy\u2018s Cafe near Fort Tryon Park.\n14:28 - 16:00\nTravel from Jochy\u2018s Cafe to the Little Red Lighthouse.\nVisit Little Red Lighthouse and enjoy views of the Hudson River.\nTravel from the Little Red Lighthouse to Wave Hill.\n16:00 - 17:06\nExplore the gardens and art exhibitions at Wave Hill.\nTravel from Wave Hill to the Riverdale neighborhood.\n19:00 - 19:13\nHaving food in Floridita Restaurant, Riverdale.\nTravel from Floridita Restaurant in Riverdale to the university.\n19:13 - 20:00\n20:00 - 20:40\nInteractive Concierge\nBudget\n$120\n$117.25\n$92.25\n$72.25\n$69.5\n$69.5\n$64.5\n$54.5\n$52.5\n$22.5\n$19.75\n$120\n1\n5\n6\n7\n4\n2\n3\n8\nSTART\n17:06 - 19:00\nFigure 8. The Perfect Day Itinerary: Crafted by Diego, our iterative concierge agent, this schedule is meticulously tailored, accounting for\nyour mental and physical well-being and budget variations as your day unfolds.\nGeo: [40.8647205, -73.9325163]\nRating: 7.2\n\ud83d\udc4d\nGeo: [40.8653388, -73.9322499]\nRating : 6.5\n\ud83d\udc4d\nGeo: [40.8609142,-73.9324818]\nRating: 4.2\n\ud83d\udc4e\nGeo: [40.8642401,-73.9325958]\nRating: 3.5\n\ud83d\udc4e\nGeo: [40.8649162,-73.9311561]\nRating : 7.5\n\ud83d\udc4d\nFigure 9. Diego traverses regions of interest to find scenic locations to add to your itinerary.\n3.4.2\nHuman-Agent Collaboration\nGrounded in the same environment we humans inhabit, V-\nIRL agents can collaborate with and assist real human users.\nInteractive Concierge\nENV\nMap\nLM\nLLM\nCV\nVision\nCOL\nColab\nName: Diego\nAge: 62\nLoc: NYC\nBio:\nDiego is an expert concierge at a hotel. He\u2019s a master at creating intri-\ncate itineraries and providing valuable local advice.\nIntention:\nPlan personalized and practical itinerary for customer!\nTask:\nGiven a user\u2019s location, background, and intention for a day,\nplan a full itinerary balancing their mental/physical state & budget.\nTakeaway:\nV-IRL agents can collaborate with users to solve\ncomplex tasks that require understanding the user\u2019s internal state.\nAs a university student in NYC, you are excited to spend a\nday exploring lesser-known and tranquil places. Your friend\nrecommended Diego, who is known for his professionalism\nin planning practical and personalized itineraries.\nAs depicted in Fig. 8, Diego\u2019s itinerary is tailored to your\n(the user\u2019s) needs. Diego not only considers your physical\nand mental interoception status, budget for each activity, but\nalso anticipates your status changes and cost when you fol-\nlow each event. He is able to take into account real travel\ntimes from the V-IRL platform and select suitable destina-\ntions by collaborating with another recommendation agent.\nIn contrast, Fig. 10 shows that a simpler \u201cungrounded\u201d\nLLM-only concierge agent is unable to consider the real dis-\nSingle pass results\nMorning walk at The High Line.\n08:00 - 9:00\nBreakfast at a local caf\u00e9 in Chelsea. \nSubway ride from Chelsea to Green-Wood Cemetery.\n12:45 - 13:45\n10:00 - 10:30\nExplore Green-Wood Cemetery.\n14:00 - 15:30\nLunch at a quiet restaurant in Brooklyn.\nVisit the Brooklyn Botanic Garden.\nTravel from Brooklyn Botanic Garden to Wave Hill.\n15:30 - 16:00\nStroll around Wave Hill.\n16:00 - 18:00\nDinner at a cozy restaurant in Riverdale.\nTravel back to the university.\n18:15 - 19:00\n19:00 - 20:00\n10:30 - 12:30\n09:15 - 10:00\nSTART\n1 2\n3\n4\n5\n6\nFigure 10. An ungrounded LLM-only concierge agent\u2019s itinerary.\ntance and travel time between locations without access to V-\nIRL, resulting in an impractical itinerary. For example, lack-\ning real geospatial information, the ungrounded concierge\nallocates only 30 minutes for travel between the \u201cBrook-\nlyn Botanic Garden\u201d and \u201cWave Hill\u201d in the Bronx, which\nactually requires 60\u2013100 minutes*. The hallucinated travel\ntimes overlook geospatial realities and result in a plan with\nexcessively distant destinations.\nAlso, as shown in Fig. 11, you can intervene in Diego\u2019s\n*(per Google Maps https://maps.app.goo.gl/SW1r5GSx3ZVo7BTr7).\n7\nplanning process by adjusting your interoceptive status or\nby providing verbal feedback. In response, Diego promptly\nrevises his original plan to accommodate your demands, and\nre-estimates your state changes after his revision.\n35%\nRevised Plan 2:\nTravel from the Little Red Lighthouse back to the university.\n16:00 - 17:20\n$66.75\n30%\n5%\n0%\n0%\n10%\n0%\n25%\n20%\n80%\n70%\n14:28 - 16:00\nVisit Little Red Lighthouse and enjoy views of the Hudson River.\nOriginal Plan:\nTravel from the Little Red Lighthouse to Wave Hill.\n16:00 - 17:06\nRevised Plan 1:\nEat at Buunni Coffee near Little Red Lighthouse for a short \nbreak and refreshment.\n16:00 - 16:30\n$69.5\n$69.5\n$49.5\nEnergy\nHunger\nStress\nSadness Budget\n40%\n0%\n10%\n80%\nHuman Intervention (Option 1):\nAdjusting interoceptive states\nHuman Intervention (Option 2):\nProviding verbal feedback\n\u201cOh no, I totally forgot! There's an assignment due tonight \nand I need to change my plans immediately to get it done!\u201d\n\u2026\n\u2026\n\u2026\n\u2026\n70%\nFigure 11. Diego adapts original plan to suit user\u2019s intervention.\nFinally, using V-IRL\u2019s street views and Map, Diego can\ntraverse regions of interest scouting for potential scenic\nviewpoints for you to visit as shown in Fig. 9. He uses\nVQA to rate and assess each captured view, and adds the\nhighest-rated locations to your itinerary.\n4. System Fundamentals\nThis section introduces our system\u2019s core: a platform de-\nsigned for perception-driven agents that transforms real-\nworld cities around the world into a vast virtual play-\nground where agents can be constructed to solve practi-\ncal tasks.\nAt its heart, V-IRL is comprised of a hier-\narchical architecture (see Fig. 12).\nThe platform lies at\nthe foundation\u2014providing the underlying components and\ninfrastructure for agents to employ.\nHigher level capa-\nbilities of\nCV\nPerception ,\nLM\nReasoning ,\nENV\nAction , and\nCOL\nCollaboration emerge from the platform\u2019s components.\nFinally, agents leverage these capabilities and user-defined\nmetadata in task-specific routines to solve tasks.\n4.1. Agent Definition\nIn our system, agent behavior is shaped by user-defined\nmetadata, including a background, an intended goal, and\nan interoceptive state. The background provides the context\nnecessary to instantiate the agent in the real world (loca-\ntion), and to guide its reasoning and decision-making (biog-\nraphy). Intentions outline agents\u2019 purpose within the envi-\nronment. An agent\u2019s interoceptive state reflects its internal\nmental and physical status\u2014varying over time and influenc-\ning its behavior. This novel concept is crucial to AI agents\nfor enhancing collaboration with humans (see Sec. 3.4.2).\nConcretely, agents are developed by writing task-specific\nrun() routines that leverage the various components of\nour platform and the agent\u2019s metadata to solve tasks.\n4.2. Platform Components\nNext, we delve into the platform components, which pro-\nvide the infrastructure to instantiate capabilities, execute\nagent actions, and ground agents in the real world.\nReasoning\nPerception\nAction\nCollaboration\nBackground\nIntention\nComputer Vision\nLanguage Model\nEnvironment\nInteroceptive State\nAgent\nCapabilities\nPlatform\nOpen-World \nRecognition\nLocalization\nStreet View \nImagery\nGeolocation\nMapping\nPlace Info & \nSearch\nMovement\nFeature \nMatching\nHuman / Agent \nInteraction\nTool & API\nUse\nPhysical \nMental\nGoal\nTask\nBiography\nVQA\nLocation\nFigure 12. Hierarchical V-IRL architecture described in Sec. 4.\n4.2.1\nEnvironment (Action)\nENV\nEnvironment components are responsible for ground-\ning agents in the world around them: providing a navigable\nrepresentation of real cities (see Sec. 3.1). Geographic co-\nordinates serve as the link between the world and our virtual\nrepresentation of it. Leveraging the Google Maps Platform\n(GMP) [24], V-IRL enables agents to access street view im-\nagery, query valid movements, retrieve information about\nnearby locations, and plan routes. As these coordinates and\nlocation information are bound to the real world, they also\nprovide a natural interface with external tools that leverage\ngeolocation\u2014such as real estate APIs (see Sec. 3.2). Tech-\nnical designs of environment are detailed in Appendix C.\n4.2.2\nVision (Perception)\nCV\nPerception components enable agents to process the\nsensory-rich data provided by the environment, especially\nstreet view imagery. Pretrained localization models [37]\ngive agents a precise spatial understanding of their environ-\nment. This allows RX-399 to identify and count instances of\nobjects, and Hiro to pick out specific businesses to look up\nwith the GMP (Sec. 3.3). While localization models allow\nfor precise interaction with perceptive input, open-world\nrecognition models [51] are more general, and allow agents\nto detect a wider range of objects in their field of view (e.g.,\nTourist searches for the Apple Store). Pretrained feature\nmatching models [40] provide an understanding of conti-\nnuity across views of the same location, and enable agents\nto identify & deduplicate instances of the same object from\ndifferent viewpoints (Sec. 3.3). Multimodal models with\nVQA & Captioning capabilities [36] bridge the perceptual\nworld with natural language, and are essential for integra-\ntion with reasoning (Sec. 3.3).\n8\nWorking\nMemory\nInteractive\nConcierge\nLocal Agents\nRevising Loop\nHierarchical \nCoordinator \nInteroceptive\nEstimator\nSupervisor\nPlanning Iterations\nEnvironment\nHuman\nBackground\nIntention\nInteroceptive State\nBudget\nHuman-Agent\nInteraction\nAgent-Agent\nCollaboration\nInformation\nRetrieval\nAudit Revise Approve\nFigure 13. Architecture overview of interactive concierge agent Diego (Sec. 3.4.2). See pipeline description in Sec. 4.4.\n4.2.3\nLanguage (Reasoning & Collaboration)\nLM\nReasoning components allow decision making based on\ninformation from perception and the environment. LLMs\nsuch as GPT-4 [2] and Llama 2 [66] interface across various\nAPIs (Sec. 3.2), transforming environmental data and per-\nceptual outputs into actionable insights. They also enable\nCOL\nCollaboration between agents or with humans through\nnatural language (Sec. 3.4) Custom prompts facilitate this\ninteraction (see Sec. 4.4).\n4.3. V-IRL Capabilities\nOur platform\u2019s components can be flexibly combined to ex-\nhibit a vast array of capabilities. In Sec. 3, we present agents\nthat exhibit increasingly complex behaviors, each requiring\nmore components of the platform. From simple combina-\ntions, like the Route Optimizer (Sec. 3.1), to more com-\nplex arrangements, like the Tourist (Sec. 3.4.1), our system\nshowcases the versatility and potential of the V-IRL plat-\nform to be applied to various real-world scenarios. Next,\nwe perform a high-level case study of how V-IRL\u2019s com-\nponents are combined to create our most complex agent; in\nAppendix D, we delve deeper into the low-level platform\ndetails that underpin creating a V-IRL agent.\n4.4. High-Level System Case Study:\nInteractive\nConcierge \u201cDiego\u201d\nBy studying Diego (Sec. 3.4.2), we illustrate how our plat-\nform\u2019s components are combined to create complex agents.\nBehind Diego\u2019s proficiency in developing itineraries is\nhis iterative planning pipeline (depicted in Fig. 13). The\nprocess begins with Diego creating an initial draft plan\nfor the first activity using GPT-4, taking into account the\nuser\u2019s biography, requirements, and previous activities in\nworking memory. This draft is then meticulously refined.\nFirst, a hierarchical coordination module re-\ntrieves real transportation time and asks a recommenda-\ntion agent for dining recommendations. Subsequently, an\ninteroceptive estimation module evaluates the\neffect of the proposed activity on the user\u2019s mental/physi-\ncal state and budget.\nThe\ncrucial\nfinal\nstep\ninvolves\na\nsupervisor\nmodule,\nwhich reviews (\u201caudits\u201d) the incoming ac-\ntivity\nin\nlight\nof\nthe\ncurrent\nuser\nstatus,\nremain-\ning budget, and potential interactions (exemplified in\nFig. 11).\nIf the supervisor deems the plan unsuit-\nable, it initiates revisions.\nThe revised plan is then\nlooped back to the hierarchical coordinator\nand interoceptive estimator for reliability, fol-\nlowed by another review from the supervisor (see\nthe revising loop in Fig. 13).\nThis iterative pro-\ncess between the hierarchical coordinator, the\ninteroceptive estimator, and the supervisor\ncontinues until the supervisor approves the activity and\nadds it to its working memory.\nAfter finalizing an activity, Diego proceeds to plan the\nsubsequent activity by repeating this process until the day\u2019s\nitinerary is complete.\n5. V-IRL Benchmarks\nIn the previous sections, we illustrate the primary benefit of\nthe V-IRL platform: seamless access to first-person street-\nview imagery and descriptive information about real-world\ncities across the globe. This scalable source of truly open-\nworld data can be harnessed to test core component mod-\nels and agent capabilities. We propose three V-IRL bench-\nmarks: two evaluating vision-language models on open-\nworld vision tasks (Secs. 5.2 and 5.3), and one evaluating\nend-to-end agent performance (Sec. 5.4). Benchmark de-\ntails are in Appendix E.\n5.1. Automated Data and Annotation Collection\nTo allow our V-IRL benchmarks to scale globally, we de-\nvelop an automatic data/annotation construction pipeline\ninstead of crawling and manually annotating limited data.\nThis allows models to be conveniently tested worldwide,\nprovided there is access to Google Street Views [24].\nRegion Selection. Though our benchmark is feasible across\nall regions covered by the GMP, we select 14 districts across\n12 cities from 6 continents to ensure coverage of a diverse\n9\ndata distribution while keeping inference costs affordable.\nThe detailed locations of these regions are listed in Tab. 2.\nPlace Types.\nWe collect place information in each re-\ngion for all 96 places types annotated by GMP\u2020. Our V-IRL\nplace: localization, recognition and VQA benchmarks are\nbuilt upon all or part of these place types.\nVision and Place Data Collection. Within each region, we\ncollect geolocations with available street views, place infor-\nmation, and place-centric images. Data Cleaning. Though\nscalable, automated data collection can introduce noise due\nto the absence of human supervision. To this end, we design\nthree automatic data cleaning strategies: i) distance-based\nfiltering to exclude places not easily visible from any street\nviews due to their distance; ii) human-review filtering to\nremove \u201czombie\u201d places with no reviews which might no\nlonger be valid or relevant; and iii) CLIP-based filtering to\nretain only place-centric images with a high CLIP likeli-\nhood of being storefronts.\nContinent\nCity\nDistrict\nAfrica\nJohannesburg\nRosebank\nLagos\nSurulere\nAsia\nMumbai\nKhar\nNew Delhi\nLajpat Nagar\nHong Kong\nPrince Edward\nTokyo\nShinjuku\nAustralia\nMelbourne\nCBD\nMelbourne\nSouthBank\nEurope\nMilan\nBrera\nLondon\nOxford St\nNorth America\nNew York City\nChinatown, Manhattan\nNew York City\nSoHo, Manhattan\nSan Francisco\nUnion Square\nSouth America\nBuenos Aires\nMonserrat\nTable 2. Region list for global V-IRL benchmarks.\n5.2. V-IRL Place: Localization\nEvery day, humans traverse cities, moving between varied\nplaces to fulfill a range of goals, like the Intentional Ex-\nplorer agent (Sec. 3.3). We assess the performance of vision\nmodels on the everyday human activity of localizing places\nusing street view imagery and associated place data.\nSetups. We modify RX-399 (Sec. 3.3) to traverse polygonal\nareas while localizing & identifying 20 types of places. We\nsubsample 28 polygonal areas from the 14 districts.\nBenchmarked Models. We evaluate three prominent open-\nworld detection models: GroundingDINO [43], GLIP [37]\nand Owl-ViT [47]. We also implement a straightforward\n\u2020https://developers.google.com/maps/documentation/places/web-\nservice/supported_types/#table1\nbaseline, CLIP (w/ GLIP proposal), which involves reclas-\nsifying the categories of GLIP proposals with CLIP [51].\nEvaluation.\nWe evaluate the models based on localiza-\ntion recall, which is quantified as\nNtp\nNtp+Nfn , where Ntp and\nNfn represents the number of correctly localized places and\nmissed places, respectively.\nMatching between Object Proposals and Places.\nAs\nmentioned in Sec. 5.1, we do not annotate bounding boxes\nfor places on each potential street view image. Such human\nannotation diverges from our initial motivation of providing\nplug-and-play and sensor-rich (V-IRL) benchmarks. To as-\nsign ground truth for each object proposal in this scenario,\nwe develop a simple matching strategy to assign object pro-\nposals from street view object detections to nearby places.\nAs illustrated in Fig. 14, we first project the bounding\nbox of each object proposal onto a frustum in the 3D space,\nsubject to a radius. We then determine if any nearby places\nfall within this frustum and radius. If any nearby place is\nfound, the closest one is assigned as the ground truth for\nthe object proposal. Otherwise, the object proposal is re-\ngarded as a false positive. When multiple places are inside\nthe frustum, we consider the nearest one as the ground truth\nsince it would likely block the others in the image. This\nprocess is also used in Intentional Explorer agent Hiro to\nparse object proposals on image to place information.\nFigure 14. Matching between 2D object proposal and street place.\nResults.\nTab. 3 shows that open-world detectors like\nGroundingDINO [43], Owl-ViT [47] and GLIP [37] are bi-\nased towards certain place types such as school, cafe,\nand convenience store, respectively.\nIn contrast,\nCLIP (w/ GLIP proposal) can identify a broader spectrum\nof place types. This is mainly caused by the category bias in\nobject detection datasets with a limited vocabulary. Hence,\neven if detectors like Owl-ViT are initialized with CLIP,\ntheir vocabulary space narrows down due to fine-tuning.\nThese results suggest that cascading category-agnostic ob-\nject proposals to zero-shot recognizers appears promising\nfor \u201creal\u201d open-world localization\u2014especially for less com-\nmon categories in object detection datasets.\n10\nPlace Types\nAR10 AR20\nGroundingDINO [43]\n0.0\n0.0\n0.0\n0.0\n0.0\n7.8\n0.0\n0.0 16.8 0.0\n2.5\n1.2\nOwl-ViT [47]\n0.0 58.0 0.0\n0.0\n6.4\n1.6\n0.9\n0.0\n0.0\n0.0\n6.7\n4.4\nGLIP [37]\n24.6 0.0 19.2 0.0\n0.0\n0.0 16.6 0.0\n0.0\n0.0\n6.0\n3.7\nCLIP [51] (w/ GLIP proposal) 58.5 8.8 28.8 41.2 33.6 23.0 13.0 25.0 0.0 14.5 24.6\n20.1\nTable 3. Benchmark results on V-IRL Place Localization. AR10\nand AR20 denote average recall on subsampled 10 and all 20 place\ncategories, respectively. Full results in Appendix E.1.\n5.3. V-IRL Place: Recognition and VQA\nIn contrast to the challenging V-IRL place localization task\nusing street view imagery alone, in real life, humans can\nrecognize businesses by taking a closer, place-centric look.\nWe assess existing vision models in this manner on two per-\nception tasks based on place-centric images: i) recognizing\nspecific place types; ii) identifying human intentions via Vi-\nsion Question Answering (VQA), dubbed \u201cintention VQA\u201d.\nSetups. For recognition, we assess 10 open-world recogni-\ntion models on identifying a place\u2019s type (from 96 options)\nusing place-centric images (see Tab. 4). For intention VQA,\nwe evaluate 8 multi-modal large language models (MM-\nLLM) to determine viable human intentions from a four-\noption multiple-choice. The V-IRL Place VQA process is\nillustrated in Fig. 15, where the candidate and true choices\nare generated by GPT-4 [2] given the place types and place\nnames corresponding to the image.\nQuestion: Which human intentions can be accomplished here?\nChoices: A. Learning how to cook authentic Australian food.\nB. Applying for a reduction on parking fines.\nC. Reporting a crime or lost property.\nD. Attending a yoga session.\nFigure 15. Example of V-IRL Place VQA process.\nPlace-centric Images vs. Street View Images. In contrast\nto the street view imagery utilized in the V-IRL Place local-\nization benchmark, the V-IRL Place recognition and VQA\nbenchmarks use place-centric images. To illustrate the dis-\ntinction between these image types, we present examples in\nFig. 16. The figure shows that street view images, sourced\nfrom the Google Street View database\u2021, are taken from the\nstreet and encompass a broader view of the surroundings,\nincluding multiple buildings and possible occluding object-\n\u2021https://developers.google.com/maps/documentation/streetview/request-\nstreetview\nFigure 16. Top row: examples of street view imagery. Bottom\nrow: corresponding place-centric images.\ns/vehicles. In contrast, place-centric images, drawn from\nthe Google Place database\u00a7, are taken on foot and focus\nmore closely on the specific place\u2014providing a more con-\ncentrated view.\nEvaluation. We adopt mean accuracy (mAcc) to evaluate\nboth place recognition and VQA tasks. For place VQA, we\nfollow MMBench [44] to conduct circular evaluation and\nGPT-assisted answer parsing.\nModel\n#Param\nmAcc (%)\nV-IRL Place Recognition\nCLIP [51]\nViT-B/32\n151M\n18.2\nCLIP [51]\nViT-L/14\n428M\n37.2\nCLIP [51]\nViT-L/14@336px\n428M\n41.3\nOpenCLIP [16]\nViT-B/32\n151M\n21.2\nOpenCLIP [16]\nViT-L/14\n428M\n31.0\nEva-02-CLIP [64]\nViT-B/16\n150M\n19.5\nEva-02-CLIP [64]\nViT-L/14\n428M\n34.2\nEva-02-CLIP [64]\nViT-L/14@336px\n428M\n40.7\nSigLIP [81]\nViT-B/16\n203M\n29.5\nSigLIP [81]\nViT-L/16@384px\n652M\n37.3\nV-IRL Place VQA\nMiniGPT-4 [83]\nVicuna-13B-v0\n14.0B\n3.9\nmPLUG-Owl [78]\nLLaMA-7B\n7.2B\n5.5\nShikra [15]\nVicuna-7B\n7.2B\n10.9\nBLIP-2 [36]\nFlanT5XXL\n12.1B\n69.6\nInstructBLIP [17]\nFlanT5XXL\n12.0B\n68.0\nLLaVA [42]\nVicuna-13B-v1.3\n13.4B\n23.5\nLLaVA-1.5 [41]\nVicuna-7B-v1.5\n7.2B\n60.1\nLLaVA-1.5 [41]\nVicuna-13B-v1.5\n13.4B\n61.9\nTable 4. Benchmark results on V-IRL Place recognition and V-IRL\nPlace VQA. Green indicates increased resolution models, while\nBlue denotes model parameter scaling.\nResults. Tab. 4 shows that CLIP (L/14@336px) outper-\nforms even the biggest version of Eva-02-CLIP and SigLIP\nin the V-IRL recognition task, highlighting the high-quality\ndata used to train CLIP [51]. The bottom of the table shows\nthat BLIP2 [36], InstructBLIP [17], and LLaVA-1.5 [41] ex-\ncel at intention VQA, whereas others struggle. We note that\n\u00a7https://developers.google.com/maps/documentation/places/web-\nservice/photos\n11\nJohannesburg\nLagos\nMumbaiHK\nNew DelhiTokyo\nMelbourne\nLondon\nMilan\nNYC\nSF\nBs. Aires\n0.38\n0.4\n0.42\n0.44\n0.46\nAcc.\nVisual Question Answering\n20\n40\n60\n80\n100\n% Eng. Speakers\nEnglish Speakers\nJohannesburg\nLagos\nMumbaiHK\nNew DelhiTokyo\nMelbourne\nLondon\nMilan\nNYC\nSF\nBs. Aires\n0.3\n0.35\n0.4\nAcc.\nRecognition\nJohannesburg\nLagos\nMumbaiHK\nNew DelhiTokyo\nMelbourne\nLondon\nMilan\nNYC\nSF\nBs. Aires\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nAcc.\nLocalization\nFigure 17. City-level visualization of V-IRL benchmark results.\nthese three top-performing MM-LLMs provide consistent\nanswers in the circular evaluation, while others frequently\nfail due to inconsistent selections. Moreover, vision models\nperform better on intention VQA over place-type recogni-\ntion, suggesting direct prompts about human intention could\nbe more effective for intention-driven tasks. We provide\nplace-type perspective analysis in Appendix E.2.\n5.4. V-IRL Vision-Language Navigation\nAs discussed in Sec. 3.3, Intentional Explorer and Tourist\nagents require coordination between vision models and lan-\nguage models to accomplish vision-language tasks. To in-\nvestigate the effect of various models on end-to-end agent\nperformance, we develop an embodied task that jointly tests\nvision and language models: Vision-Language Navigation\n(VLN). In VLN, agents navigate to a desired destination by\nfollowing textual directions using only raw street views.\nSetup. We adopt the Tourist implementation from Sec. 3.4\nand swap its recognition component with the various bench-\nmarked models. These models are used to identify visual\nlandmarks during navigation. Subsequently, GPT-4 [2] pre-\ndicts the next action according to the recognition results.\nNavigation instructions are generated using the Local agent.\nRecent work VELMA [59] attempts to enhance VLN by\nleveraging LLMs on existing datasets [14, 58].\nIn con-\ntrast, our V-IRL VLN benchmark evaluates vision models\nand their coordination with language models across a global\ndata scale. See more details in Appendix E.3.\nBenchmarked methods.\nFour approaches are evaluated\nto recognize landmarks during navigation: (i) Oracle that\nsearches nearby landmarks with GMP [24]; (ii) Zero-shot\nrecognizers CLIP [51] & EVA-CLIP [64]; (iii) Multi-modal\nLLM LLaVA-1.5 [41]; (iv) An OCR model [20] to extract\ntext in street views followed by GPT answer parsing. Im-\nplementation details are provided in Appendix E.3.\nEvaluation. We primarily measure navigation success rate\n(Success), defining success as the navigator stopping within\nMethod\nStart Intersection\nStop\nSuccess Reac Arr\nReac\nArr Reac\nOracle (No Vision)\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nCLIP (B/32) [51]\n0.22\n1.0\n0.86\n0.84\n0.83\n0.22\nCLIP (L/14@336px) [51]\n0.44\n0.83\n0.73\n0.94\n0.67\n0.44\nEVA-02-CLIP (BigE/14-plus) [64]\n0.39\n0.89\n0.77\n0.94\n0.72\n0.39\nEVA-02-CLIP (L/14@336px) [64]\n0.22\n1.0\n0.82\n0.83\n0.78\n0.22\nLLaVA-1.5-13B [41]\n0.11\n0.61\n0.55\n1.0\n0.56\n0.11\nPP-OCR [20] (+ GPT3.5)\n0.28\n0.89\n0.73\n0.94\n0.72\n0.28\nTable 5. Results on V-IRL VLN-mini. We test various CLIP-based\nmodels, MM LLM , and OCR model with GPT postprocessing.\n25 meters of the destination. In addition, as navigation suc-\ncess is mainly influenced by the agent\u2019s actions at key posi-\ntions (i.e., start positions, intersections and stop positions),\nwe also evaluate the arrival ratio (Arr) and reaction accu-\nracy (Reac) for each route. Arr denotes the percentage of\nkey positions reached, while Reac measures the accuracy\nof the agent\u2019s action predictions at these key positions. To\nsave GPT-4 resources, we mainly compare vision modules\non a 10% mini-set comprising 18 routes from 9 regions. See\nAppendix E.3 for full-set results with CLIP and Oracle.\nResults.\nTable 5 shows that, with oracle landmark in-\nformation, powerful LLMs can impressively comprehend\nnavigation instructions and thus make accurate decisions.\nHowever, when relying on vision models to fetch land-\nmark information from street views, the success rate drops\ndramatically\u2014suggesting that the perception of vision\nmodels is noisy and misguides LLMs\u2019 decision-making.\nAmong these recognizers, larger variants of CLIP [51] and\nEVA-02-CLIP [64] perform better, highlighting the benefits\nof model scaling. LLaVA-1.5 [41] shows inferior perfor-\nmance with CLIP (L/14@336px) as its vision encoder, pos-\nsibly due to the alignment tax [2] introduced during instruc-\ntion tuning. Further, PP-OCR [20] (+ GPT-3.5) achieves a\n28% success rate, signifying that OCR is crucial for visual\nlandmark recognition.\n5.5. Geographic Diversity\nSpanning 12 cities across the globe, our V-IRL benchmarks\nprovide an opportunity to analyze the inherent model bi-\nases across different regions. As depicted in Fig. 17, vi-\nsion models demonstrate subpar performance on all three\nbenchmark tasks in Lagos, Tokyo, Hong Kong, and Buenos\nAires. Vision models might struggle in Lagos due to its non-\ntraditional street views relative to more developed cities (see\nstreet views in Fig. 1). For cities like Tokyo, Hong Kong,\nand Buenos Aires, an intriguing observation is their primary\nuse of non-English languages in street views, as shown in\nFig. 17 bottom right \u00b6 and Fig. 1. This suggests that exist-\ning vision models may face challenges when deployed in\nnon-English-dominant countries.\n\u00b6Source: https://en.wikipedia.org/wiki/List_of_countries_by_English-\nspeaking_population\n12\n6. Discussion: Ethics & Privacy\nOur platform serves as a tool for AI development and as a\ncrucible for ethical discourse and preparation. As AI is in-\nevitably being integrated into society\u2014e.g., via augmented\nreality wearables, spatial computing platforms, or mobile\nrobots navigating city streets\u2014it is imperative to confront\nand discuss ethical and privacy concerns now. Unlike these\nimpending real-time systems, the data accessed by V-IRL is\n\u201cstale\u201d and preprocessed\u2014providing a controlled environ-\nment to study these concerns.\nNotably, V-IRL exclusively utilizes preexisting, read-\nily available APIs; it does not capture or make available\nany previously inaccessible data. Our primary source of\nstreet-view imagery, Google Maps [24], is subject to major\nprivacy-protection measures, including blurring faces and\nlicense plates [22].\nMoreover, V-IRL complies with the\nGoogle Maps Platform license||, similarly to notable exist-\ning works that also leverage Google\u2019s street views [1, 14].\nWe believe V-IRL is an invaluable tool for researching\nbias. As discussed in Sec. 5.5, V-IRL\u2019s global scale pro-\nvides a lens to study linguistic, cultural, and other geo-\ngraphic biases inherent in models. By using V-IRL to study\nsuch questions, we aim to preemptively tackle the ethi-\ncal dilemmas that will arise with deploying real-time sys-\ntems rather than being blindsided by them. We hope our\nwork helps spur proactive discussion of future challenges\nthroughout the community.\n7. Conclusion\nIn this work, we introduce V-IRL, an open-source platform\ndesigned to bridge the sensory gap between the digital and\nphysical worlds, enabling AI agents to interact with the real\nworld in a virtual yet realistic environment. Through V-IRL,\nagents can develop rich sensory grounding and perception,\nutilizing real geospatial data and street-view imagery. We\ndemonstrate the platform\u2019s versatility by creating diverse\nexemplar agents and developing benchmarks measuring the\nperformance of foundational language and vision models on\nopen-world visual data from across the globe.\nThis platform opens new avenues for advancing AI capa-\nbilities in perception, decision-making, and real-world data\ninteraction. As spatial computing and robotic systems be-\ncome increasingly prevalent, the demand for and possibili-\nties of AI agents will only grow. From personal assistants to\npractical applications like urban planning to life-changing\ntools for the visually impaired, we hope V-IRL helps usher\nin a new era of perceptually grounded agents.\n||https://cloud.google.com/maps-platform/terms\nReferences\n[1] Image geo-localization based on multiplenearest neighbor\nfeature matching usinggeneralized graphs. TPAMI, 2014. 13\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGPT-4 technical report. arXiv preprint arXiv:2303.08774,\n2023. 2, 9, 11, 12, 18\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022. 2\n[4] Peter Anderson, Angel Chang, Devendra Singh Chaplot,\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\net al. On evaluation of embodied navigation agents. arXiv\npreprint arXiv:1807.06757, 2018. 2\n[5] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and\nAnton Van Den Hengel. Vision-and-Language Navigation:\nInterpreting visually-grounded navigation instructions in real\nenvironments. In CVPR, 2018. 2\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2\n[7] Yuki M Asano, Christian Rupprecht, Andrew Zisserman, and\nAndrea Vedaldi. PASS: An imagenet replacement for self-\nsupervised pretraining without humans. In NeurIPS, 2021.\n2\n[8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n2\n[9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki\nCheung, Przemys\u0142aw D\u02dbebiak, Christy Dennison, David\nFarhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al.\nDota 2 with large scale deep reinforcement learning. arXiv\npreprint arXiv:1912.06680, 2019. 2\n[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Florence,\nChuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakr-\nishnan, Kehang Han, Karol Hausman, Alex Herzog, Jas-\nmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,\nLisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,\nHenryk Michalewski, Igor Mordatch, Karl Pertsch, Kan-\nishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar,\nPannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait\nSingh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan\nVuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin\nWu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,\nand Brianna Zitkovich. RT-2: Vision-language-action mod-\n13\nels transfer web knowledge to robotic control. arXiv preprint\narXiv:2307.15818, 2023. 2\n[11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex\nIrpan, Eric Jang, Ryan Julian, et al. Do As I Can, Not As I\nSay: Grounding language in robotic affordances. In CoRL,\n2023. 1, 2\n[12] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3D: Learning from rgb-d\ndata in indoor environments. In 3DV, 2017. 2\n[13] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Ab-\nhinav Gupta, and Russ R Salakhutdinov. Object goal naviga-\ntion using goal-oriented semantic exploration. In NeurIPS,\n2020. 2\n[14] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely,\nand Yoav Artzi. TOUCHDOWN: Natural language naviga-\ntion and spatial reasoning in visual street environments. In\nCVPR, 2019. 12, 13\n[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 11\n[16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In CVPR,\n2023. 2, 11\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructBLIP: Towards General-\npurpose Vision-Language Models with Instruction Tuning.\nIn NeurIPS, 2023. 11, 20\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[19] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. PaLM-E: An Embodied Multimodal Lan-\nguage Model. In ICML, 2023. 2\n[20] Y Du, C Li, R Guo, X Yin, W Liu, J Zhou, Y Bai, Z Yu, Y\nYang, Q Dang, et al. PP-OCR: A practical ultra lightweight\nocr system. arxiv 2020. arXiv preprint arXiv:2009.09941,\n2020. 12, 23\n[21] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland,\nand Dhruv Mahajan. Adaptive methods for real-world do-\nmain generalization. In CVPR, 2021. 2\n[22] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco\nZennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam,\nHartmut Neven, and Luc Vincent. Large-scale privacy pro-\ntection in google street view. In ICCV, 2009. 13\n[23] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\ning open-vocabulary image segmentation with image-level\nlabels. In ECCV, 2022. 2\n[24] Google Map Team.\nGoogle Map Platform.\nhttps://\nmapsplatform.google.com/. 8, 9, 12, 13\n[25] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey\nLevine.\nDeep reinforcement learning for robotic manipu-\nlation with asynchronous off-policy updates. In ICRA, 2017.\n2\n[26] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,\nYuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu\nRan, Lingfeng Xiao, Chenglin Wu, and J\u00fcrgen Schmidhuber.\nMetaGPT: Meta Programming for A Multi-Agent Collabora-\ntive Framework. In ICLR, 2023. 2\n[27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\nMordatch. Language Models As Zero-Shot Planners: Ex-\ntracting actionable knowledge for embodied agents.\nIn\nICML, 2022. 1, 2\n[28] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\nMordatch, Yevgen Chebotar, et al. Inner Monologue: Em-\nbodied reasoning through planning with language models. In\nCoRL, 2022. 1, 2\n[29] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\nJiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3d value\nmaps for robotic manipulation with language models. arXiv\npreprint arXiv:2307.05973, 2023. 2\n[30] Heinrich K\u00fcttler, Nantas Nardelli, Alexander Miller, Roberta\nRaileanu, Marco Selvatici, Edward Grefenstette, and Tim\nRockt\u00e4schel. The nethack learning environment. In NeurIPS,\n2020. 2\n[31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\nThe Open Images Dataset V4: Unified image classification,\nobject detection, and visual relationship detection at scale.\nIJCV, 2020. 2\n[32] Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nOBELICS: An open web-scale filtered dataset of interleaved\nimage-text documents. In NeurIPS, 2023. 2\n[33] Alexander C Li, Ellis Brown, Alexei A Efros, and Deepak\nPathak. Internet explorer: Targeted representation learning\non the open web. In International Conference on Machine\nLearning, pages 19385\u201319406. PMLR, 2023. 2\n[34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Rene Ranftl.\nLanguage-driven semantic seg-\nmentation. In ICLR, 2022. 2\n[35] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,\nDmitrii Khizbullin, and Bernard Ghanem. CAMEL: Com-\nmunicative agents for\" mind\" exploration of large language\nmodel society. In NeurIPS, 2023. 2\n[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nbootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 8, 11\n[37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training. In CVPR, 2022. 2, 8, 10, 11\n14\n[38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy Zeng. Code\nas Policies: Language model programs for embodied control.\nIn ICRA, 2023. 2\n[39] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco\nPavone, and Jeannette Bohg.\nText2Motion: From natu-\nral language instructions to feasible plans. arXiv preprint\narXiv:2303.12153, 2023. 2\n[40] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-\nfeys. LightGlue: Local Feature Matching at Light Speed. In\nICCV, 2023. 8\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 11, 12, 23\n[42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 11, 20\n[43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding DINO: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 10, 11\n[44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. MMBench: Is Your Multi-modal Model an\nAll-around Player? arXiv preprint arXiv:2307.06281, 2023.\n11\n[45] Zeyi Liu, Arpit Bahety, and Shuran Song. REFLECT: Sum-\nmarizing robot experiences for failure explanation and cor-\nrection. In CoRL, 2023. 2\n[46] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\nMichelle Lu, Kier Storey, Miles Macklin, David Hoeller,\nNikita Rudin, Arthur Allshire, Ankur Handa, et al.\nIsaac\nGym: High performance gpu-based physics simulation for\nrobot learning. In NeurIPS, 2021. 2\n[47] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple Open-Vocabulary Object Detection with\nVision Transformers. In ECCV, 2022. 2, 10, 11\n[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602, 2013. 2\n[49] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al. WebGPT:\nBrowser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332, 2021. 2\n[50] Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\nGenerative Agents: Interactive simulacra of human behav-\nior. In UIST, 2023. 2\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2, 8, 10, 11, 12\n[52] Vikram\nV\nRamaswamy,\nSing\nYu\nLin,\nDora\nZhao,\nAaron Bryan Adcock, Laurens van der Maaten, Deepti\nGhadiyaram, and Olga Russakovsky. GeoDE: a geograph-\nically diverse evaluation dataset for object recognition. In\nNeurIPS, 2023. 2\n[53] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan\nKini, David Kanter, Vijay Janapa Reddi, and Cody Cole-\nman.\nThe Dollar Street Dataset: Images representing the\ngeographic and socioeconomic diversity of the world.\nIn\nNeurIPS, 2022. 2\n[54] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat-\nform for embodied ai research. In ICCV, 2019. 2\n[55] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language models\ncan teach themselves to use tools. In NeurIPS, 2023. 2\n[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. LAION-5b: An open large-scale dataset for training\nnext generation image-text models. In NeurIPS, 2022. 2\n[57] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,\nJacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam\nFedus, Luke Metz, Michael Pokorny, et al. ChatGPT: Opti-\nmizing language models for dialogue. OpenAI blog, 2022.\n23\n[58] Raphael Schumann and Stefan Riezler. Generating landmark\nnavigation instructions from maps as a graph-to-text prob-\nlem. In ACL, 2020. 12, 18\n[59] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu,\nStefan Riezler, and William Yang Wang. VELMA: Verbal-\nization embodiment of llm agents for vision and language\nnavigation in street view. arXiv preprint arXiv:2307.06082,\n2023. 12, 18, 23\n[60] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander,\nYu Liu, and Hongsheng Li.\nLMDrive: Closed-loop end-\nto-end driving with large language models. arXiv preprint\narXiv:2312.07488, 2023. 2\n[61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 2\n[62] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflex-\nion: an autonomous agent with dynamic memory and self-\nreflection. In NeurIPS, 2023. 2\n[63] Significant Gravitas. AutoGPT. https://github.com/\nSignificant-Gravitas/AutoGPT, 2023. 2\n[64] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. EVA-CLIP: Improved Training Techniques for CLIP at\nScale. arXiv preprint arXiv:2303.15389, 2023. 11, 12\n[65] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. YFCC100M: The new data in multimedia research.\nCommunications of the ACM, 2016. 2\n15\n[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLLAMA 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023. 2, 9\n[67] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\n1, 2\n[68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. In NeurIPS, 2022. 2\n[69] Michael Wooldridge and Nicholas R Jennings. Intelligent\nAgents: Theory and practice. The knowledge engineering\nreview, 1995. 2\n[70] Penghao Wu and Saining Xie. V*: Guided Visual Search\nas a Core Mechanism in Multimodal LLMs. arXiv preprint\narXiv:2312.14135, 2023. 17\n[71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\nZhang, and Chi Wang.\nAutoGen: Enabling next-gen llm\napplications via multi-agent conversation framework. arXiv\npreprint arXiv:2308.08155, 2023. 2\n[72] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding,\nBoyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu\nZhou, et al. The rise and potential of large language model\nbased agents: A survey. arXiv preprint arXiv:2309.07864,\n2023. 1, 3\n[73] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\nMalik, and Silvio Savarese. Gibson Env: Real-world percep-\ntion for embodied agents. In CVPR, 2018. 2\n[74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\nZhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\nHe Wang, et al. SAPIEN: A simulated part-based interactive\nenvironment. In CVPR, 2020. 2\n[75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,\nRussell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,\nLuke Zettlemoyer, and Christoph Feichtenhofer. Demystify-\ning clip data. arXiv preprint arXiv:2309.16671, 2023. 2\n[76] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,\nThomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT:\nSemantic segmentation emerges from text supervision. In\nCVPR, 2022. 2\n[77] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao.\nReAct: Synergizing\nreasoning and acting in language models. In ICLR, 2023. 2\n[78] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mPLUG-Owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 11\n[79] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav,\nAustin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen\nYang, Vidhi Jain, Alexander William Clegg, John Turner,\net al. HomeRobot: Open-vocabulary mobile manipulation.\nIn CoRL, 2023. 2\n[80] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li,\nChunyuan Li,\net al.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 2\n[81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\narXiv preprint arXiv:2303.15343, 2023. 11\n[82] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhilipp\nKr\u00e4henb\u00fchl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision. In ECCV, 2022. 2\n[83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language\nUnderstanding with Advanced Large Language Models.\narXiv preprint arXiv:2304.10592, 2023. 11\n[84] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie\nSu, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang\nWang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost\nin the Minecraft: Generally capable agents for open-world\nenvironments via large language models with text-based\nknowledge and memory. arXiv preprint arXiv:2305.17144,\n2023. 2\n16\nA. Appendix Outline\nIn these supplementary materials, we provide additional de-\ntails for our V-IRL platform, including:\n\u2022 Designs behind V-IRL Agents (Appendix B);\n\u2022 Technical details and challenges in the V-IRL environ-\nment (Appendix C).\n\u2022 A low-level case study of Intentional Explorer agent Hiro,\ndelving into implementation details of our system such as\nLLM prompts (Appendix D);\n\u2022 More detailed setups and results for our V-IRL bench-\nmarks (Appendix E).\nB. Technical Details of V-IRL Agents\nIn Sec. 3, our discussion mainly focuses on the innovative\ncapabilities and behaviors of V-IRL agents empowered by\nour platform. We avoid in-depth discussions about technical\ndetails in the main paper due to the concern of readability.\nIn this section, we go through our main technical designs\nfor each agent. More comprehensive technical implementa-\ntions are available in our released code.\nB.1. Peng: Route Optimizer\nPeng is designed to showcase the utilization of real geo-\ngraphic coordinates within our platform.\nBy processing\na sequence of real addresses, Peng calculates the shortest\npath for traversing them using various modes of transporta-\ntion, such as walking, driving, and bicycling, among oth-\ners.\nThis capability is powered by the mapping module\ndescribed in Appendix C.3. After that, Peng proceeds to\nnavigate through the destinations along the predetermined\npath, employing the point navigation procedure outlined in\nAppendix C.2.2.\nB.2. Aria: Place Recommender\nAria leverages the realistic place information provided by\nour Place Info & Search module (see Appendix C.4) to en-\nhance LLMs\u2019 reasoning capability in the geographic aspect.\nSpecifically, Aria evaluates Peng\u2019s intention to determine\nthe suitable type of place and searches all possible places\nin the vicinity. For each searched place, Aria considers its\nreviews and user ratings from Google to summarize a place\noverview. Subsequently, we customize prompts for Aria to\namalgamate Peng\u2019s biography, intentions, and the summa-\nrized place overviews to rate each place between 0 and 10,\naccompanied by justifications.\nWithout such technical designs, LLMs could recom-\nmend some places that are either too distant or permanently\nclosed. This issue arises because LLMs struggle to accu-\nrately understand geospatial relationships and often depend\non outdated databases.\nB.3. Vivek: Estate Agent\nThe process employed by Vivek is similar to that of Aria,\nas both are designed to recommend places. However, Vivek\nshowcases the versatility of our V-IRL platform by demon-\nstrating how it can seamlessly integrate a wide range of re-\nalistic information beyond the Google Maps Platform, with\na standardized definition of geographic coordinates. This\ncapability enables the creation of even more sophisticated\nand intriguing agents.\nB.4. RX-399: Urban Assistance Robot\nDifferent from previous example agents, RX-399 intro-\nduces visual perception capabilities such as open-world de-\ntection and feature matching. There are two fundamental\nsystems inside it \u2013 navigation and perception.\nIn terms\nof navigation, RX-399 can automatically navigate from the\ncurrent position to the pre-defined destination step by step.\nThis navigation process is elaborated in Appendix C.2.2,\nand thus, will not be extensively discussed here.\nWhen it comes to its perception system, RX-399 is de-\nsigned to simulate human visual perception by capturing\nstreet views within a 90-degree horizontal angle to both its\nleft and right. For each captured view, an open-world detec-\ntion process is conducted. Leveraging the interactive capa-\nbilities of our environment, we further propose an active\ndetection strategy to dynamically adjust the agent\u2019s ego-\npose and focal length according to the scale and position\nof potential objects.\nThis significantly improves its per-\nformance as illustrated in Tab. 6. In the future, more ad-\nvanced approaches such as visual search [70] could also\nbe considered. In the subsequent de-duplication procedure,\nwhich aims to avoid double-counting objects across differ-\nent viewpoints, we have tried a few strategies including\nmeasuring with multi-view geometry, object tracking, and\nfeature matching. We choose feature matching because of\nits accuracy and efficiency.\nCity\nHong Kong\nNew York City\nw/ active detection\n0.63 / 0.83\n0.71 / 1.00\nw/o active detection\n0.10 / 0.33\n0.30 / 0.60\nTable 6. RX-399 detection performance with or without active\ndetection manner. Metrics are accuracy / recall.\nB.5. Imani: Urban Planner\nThe visual perception system of Imani mirrors that of RX-\n399. The primary distinction between them lies in their\nnavigation systems. Imani possesses the capability to plan\na navigation route in the given polygonal region, enabling\nRX-399 to traverse that region. This functionality is named\n\u201cregion navigation\u201d and elaborated in Appendix C.2.2. Ad-\nditionally, within the Imani agent, we develop a heatmap\nvisualization tool to visualize and verify the data collected\nby RX-399 (see Fig. 3).\n17\nB.6. Hiro: Intentional Explorer\nHiro is a representative agent equipped with geographical,\nperceptual, and reasoning abilities, to address a daily human\ntask: randomly exploring to find a suitable restaurant. In\nthis regard, we have dedicated a separate section to offer an\nin-depth case study, including the detailed methodology and\nprompts in Appendix D.\nB.7. Ling: Tourist\nOur vision language navigation pipeline of Ling is sim-\nilar to [59], leveraging vision models, the map, and\nLLMs.\nAt each position, we start by capturing eight\nstreet views around the agent, corresponding to front,\nleft front, left, left behind, behind, right\nbehind, right and right front. Vision models use\nthese street views to identify landmarks mentioned in route\ndescriptions, which are then verbalized as landmark obser-\nvations. Also, intersection information is retrieved from the\nmover to formulate an intersection observation. LLMs play\na crucial role in processing landmark & intersection obser-\nvations along with the agent\u2019s previous working history to\ndetermine the next action. After each action, current obser-\nvations and actions are stored into the agent\u2019s working his-\ntory. This auto-regressive process continues until the agent\ndecides to stop.\nB.8. Local Agent\nThe primary mission of the Local agent is to generate\nhuman-like and easily followable navigation instructions on\na global scale (refer to 3.4.1). This task is known as navi-\ngation instruction generation [58]. Contrary to most exist-\ning research, which depends on human-annotated data for\nlimited geographic areas, our \u201cLocal\u201d agent automatically\nselects suitable landmarks taking account into real-world\nplaces and generates human-like route descriptions using\nLLMs across the globe. Remarkably, it achieves this with-\nout the need for any training data, relying solely on our tai-\nlored prompts and a few in-context examples. The effective-\nness of its generated instructions has been verified through\ncollaboration with \u201dLing\u201d. To the best of our knowledge,\nthis is a first in the field. There are massive technical de-\ntails on selecting easily noticeable landmarks and prompt\nengineering, which are available in our released code.\nB.9. Diego: Interactive Concierge\nIn Sec. 4.4, we have already presented the technical designs\nof Diego\u2019s itinerary. Here, we detail how Diego can find\nscenic locations as shown in Fig. 9. For any given des-\ntination, such as \u201cFort Tryon Park\u201d, Diego will sample a\nrectangle region around it and traverse all navigable posi-\ntions within it. At each position, he will capture a photo-\ngraph (i.e. street view imagery) using pre-defined headings,\npitches, and FOVs. Each photograph will then be evaluated\nusing GPT-4(V) [2], where it receives a rating between 0\nand 10 along with explanatory reasons.\nC. Technical Details of Environment\nIn Sec. 4.2.1, we provide an overview of our system\u2019s envi-\nronment, which grounds agents in real life. Here, we delve\ninto the technical designs beyond mere leveraging Google\nMap Platform system calls. Concrete implementations can\nbe found in our open-sourced code.\nC.1. Geolocation & Street View Imagery\nAt the core of V-IRL lies its innovative use of sensor-rich en-\nvironment, including street view imagery and geolocations.\nThey enable agents to gather surrounding place and vision\ninformation.\nGeolocation. Agents in the V-IRL platform inhabit virtual\nrepresentations of real cities around the globe. At the core\nof this representation are geographic coordinates (i.e. ge-\nolocation) corresponding to points on the Earth\u2019s surface.\nThe initial geolocation of each agent is specified by its \u201cLo-\ncation\u201d configuration, as illustrated in Fig. 12. Whenever\nagents require access to surrounding information (e.g. street\nviews, places or maps), geolocation serves as a crucial pa-\nrameter for querying the related Google Map APIs.\nStreet view imagery. Google Map Platform specifies each\nstreet view imagery with multiple key parameters: geolo-\ncation, heading (the horizontal angle ranging from 0\u25e6 to\n360\u25e6), pitch (a vertical angle ranging from -90\u25e6 to 90\u25e6), and\nField of View (FOV, ranging from 20 \u223c 120). It\u2019s notewor-\nthy that adjusting the FOV here is similar to changing the\ncamera\u2019s focal length, rather than simply zooming in on an\nimage, which ensures that image resolution remains high,\neven as the FOV decreases to a low value. By modifying\nthe heading, pitch, and FOV, we can simulate the human\nsensory process of adjusting one\u2019s pose and concentrating\non a specific area.\nAlignment between street view imagery and geolocation.\nWithin our sensor-rich platform, a fundamental challenge is\nto ensure agents are positioned at geolocations where street\nview imagery is available. To address this issue, we design\na custom operation named \u201crelocate\u201d. Specifically, when\nan agent is initialized at a location lacking street view im-\nagery, the \u201crelocate\u201d operation will identify and transition\nthe agent to the nearest viable geolocation where street view\ndata is available. Notice that, this operation is indispensable\nto our platform, as the positions with available street views\nare relatively sparse in comparison to the vast continuous\nspace of all possible coordinates.\nC.2. Movement\nEnabling agents to move along city streets is a core func-\ntionality of our platform, allowing interaction between\n18\nagents and the real world.\nWhenever an agent needs to\nmove, this module powers all related processes, from route\nplanning and direction selection to the continuous update\nof the agent\u2019s geolocation during its moving. Since Google\nMap Platform does not provide APIs to access nearby navi-\ngable positions and directions, the design of this movement\nmodule is a significant technical challenge and a substantial\ncontribution from our team. We discuss its low-level imple-\nmentations in Appendix C.2.1 and the enabled high-level\nactions in Appendix C.2.2.\nC.2.1\nMover\nMove by controlling the web interface. A straightforward\nsolution is to let the agent control the web front-end Google\nStreet View to select moving directions and move. Never-\ntheless, there are three key challenges for this solution:\n(i) How can Python-implemented agents control the\nmovement via the interaction to the webpage? We use a\nPython package Selenium** to locate web elements respon-\nsible for movement. After determining a movement direc-\ntion, the agent uses Selenium to simulate a click action on\nthe web element corresponding to the chosen direction.\n(ii) How can the agent acquire the necessary informa-\ntion to decide moving direction? Although agents can ac-\ncess all potential movement directions from web elements,\nthey cannot identify these directions without prior knowl-\nedge of what each represents. We find that the \u201ctransform\u201d\nattribute in the web element corresponding to each direction\ncan be leveraged to calculate their represented heading an-\ngles. The heading angle also allows us to collect street view\nimagery for each movement direction. Agent\u2019s movement\ndecision-making is then based on these heading angles and\nthe visual data from street view imagery.\n(iii) How to track the agent\u2019s geolocation along its\nmovement? To accomplish this, we customize a webpage\nelement to display the geolocation of the current street view\npanorama. As the agents move and trigger updates to the\nstreet view panorama, this customized element concurrently\nrefreshes to reflect the new geolocation. By using Selenium,\nwe can then extract this updated geolocation data, enabling\ncontinuous tracking of the agent\u2019s geolocation changes.\nMove by grid-based relocating. In our test of the above\nweb-based mover, a critical limitation emerged: the web-\nembedded Google Street View panoramas display only a\nsubset of navigable directions. This constraint significantly\nrestricts our agents\u2019 mobility, often preventing them from\nsuccessfully navigating to their intended destinations due to\nthe incomplete coverage of potential routes.\nTo overcome this obstacle, we develop an alternative\nmethod: a grid-based relocating mover.\nThis approach\ninvolves performing a grid search for geolocations in the\n**https://www.selenium.dev/\nvicinity of the agent and employing the \u201crelocate\u201d opera-\ntion to sift through these locations, identifying those that\nare navigable. While this method offers a more comprehen-\nsive view of navigable positions, it is markedly more time-\nconsuming than the web-based approach due to the exten-\nsive number of Google Maps API calls required.\nIn our practical applications, we design a heuristic strat-\negy that combines web-based controlling and grid-based re-\nlocation. This hybrid approach aims to balance the trade-\noffs between the speed and the completeness of navigable\nposition data, optimizing our agents\u2019 capabilities and effi-\nciency in real-world scenarios.\nC.2.2\nNavigator\nHere, we introduce the high-level action of agents powered\nby the mover \u2013 navigation. Unlike the mover, which con-\ncentrates on enabling agent mobility in the environment, the\nfocus here shifts to determining the direction of movement.\nIn our platform, we group different navigators according to\ntheir usages into four types:\n(i) Point navigator is designed to tackle navigation\ntasks that clearly define single or multiple destinations\n(represented in addresses or geolocations).\nThis naviga-\ntor employs the route planning function described in Ap-\npendix C.3 to obtain a series of key positions for naviga-\ntion. At each location, the agent utilizes a greedy algorithm\nto select the most optimal direction towards the next key\nposition that has not yet been reached. Exemplary agents,\nsuch as \u201cPeng\u201d, \u201cRX-399\u201d and \u201cLocal\u201d, use this type of\nnavigator in their implementation.\n(ii) Region navigator is tailored for agents like \u201cImani\u201d\nand \u201cDiego\u201d, who need to traverse every position within a\npolygonal region. This navigator first employs a grid search\ncombined with our \u201crelocate\u201d operation to identify all navi-\ngable positions within the specified region. Subsequently, it\nadopts a heuristic algorithm designed to solve the traveling\nsalesman problem, planning an efficient order for visiting\nthese positions. The agents\u2019 task is to simply follow this\npredetermined route, visiting each navigable position in the\nplanned order.\n(iii) Vision-language navigator is specifically devel-\noped for the tourist agent \u201cLing\u201d, as well as for tasks within\nthe V-IRL vision-language navigation benchmark. Its pri-\nmary function is to guide the agent in selecting a proper\ndirection based on navigation instructions.\nThe detailed\npipeline is presented in Appendix B.7.\n(iv) Intention navigator is utilized in intentional ex-\nplorer agent \u201dHiro\u201c to select the most suitable direction\nthat aligns with the agent\u2019s specific intentions. The detailed\nmethodology and prompt are detailed in Appendix D.2.\n19\nC.3. Mapping\nThe mapping module in our environment is designed to\nequip agents with functionalities such as route planning and\ntransportation time estimation. It mainly utilizes the \u201cDi-\nrections API\u201d\u2020\u2020 from the Google Map Platform to facilitate\nthese capabilities. Given the complex nature of this API\u2019s\ninterface, our principal focus has been on parsing its out-\nput and adapting it into various user-friendly interfaces for\nagents.\nC.4. Place Info & Search\nPlace Info & Search module hosts another important in-\nformation source in our platform beyond the visual street\nview imagery, enabling agents to interact with real-world\n\u201cplaces\u201d. It provides various attributes of places, including\ntype, name, location, imagery, reviews, etc. In this module,\nour technical efforts are primarily focused on understand-\ning, comparing, and integrating the most suitable functions\nfrom the vast array of Google Maps Platform APIs related\nto place information and nearby place searches. Addition-\nally, we devise some post-processing strategies to identify\nand eliminate invalid or conflicting data sources from the\nGoogle Maps Platform.\nAnother essential capability enabled by this module is to\nassociate object proposals in street view imagery and their\ncorresponding places in the real city. This function is vital\nto enhance the reality of our platform by connecting street\nview and geolocation. It also powers the \u201cHiro\u201d agent and\nthe evaluation of the V-IRL Place localization benchmark.\nThe implementation is detailed in Sec. 5.2.\nD. Low-Level System Case Study:\nIntentional Explorer \u201cHiro\u201d\nThis section delves deeper into the low-level implemen-\ntation details of the Intentional Explorer agent \u201cHiro\u201d\n(Sec. 3.3), focusing on the prompts utilized to interact with\nvarious parts of our system.\nConcretely, we present the\nprompts in four subparts: identifying a type of place to\nsearch using the user-defined intention (Appendix D.1), se-\nlecting appropriate roads (Appendix D.2), summarizing re-\nviews of places (Appendix D.3), and making action deci-\nsions (Appendix D.4). These four components jointly en-\nable Hiro to explore in our interactive embodied environ-\nment driven by his initial intention.\nD.1. Intention to Place Type\nStarting with a user-defined agent intention, Hiro first deter-\nmines the type of place that could fulfill this intention using\nGPT-4 and the following prompt:\n\u2020\u2020https : / / developers . google . com / maps /\ndocumentation/directions\n[Role]\nYou are PlaceSuggesterGPT, an expert\nin recommending types of places\nbased on user-specified intentions.\n[Task Description]\nGiven a user-specified intention,\ndetermine the type of \"place\"\none should seek to fulfill the\nintention.\nYour response should\nbe in the following JSON format:\n{\"place\":\n\"Desired Place Type\"}\n[Example]\nInput: \"Intention:\n<buy a book>\"\nOutput: {\"place\":\n\"bookstore\"}\n[Input]\nIntention:\n<{agent_intention}>\n[Output]\nYour recommended place type based on\nthe user-specified intention, in the\nrequired JSON format:\nUsing this prompt with the intention\nHiro is hungry and looking for a place where he\ncan try some good local food. He cannot handle\nspicy food.\nreturns the result\n{\"place\":\n\"restaurant\"}.\nThe identified place type (here, restaurant) is extracted\nand set as the target category for Hiro\u2019s open-world detector\nduring his exploration.\nD.2. Road Selection\nWhenever Hiro is at a crossroads, he determines the best\nroad to follow using his multi-modal LLM and GPT-4. The\nprimary goal of the road selection process is to identify the\nroad most likely to lead to the desired place type that aligns\nwith Hiro\u2019s intention. First, Hiro fetches the street view\ntowards each potential road using the V-IRL environment.\nThen he utilizes his multi-modal LLM (such as Instruct-\nBLIP [17] or LLaVA [42]) to generate captions for each\nroad using the following prompt:\nI am looking for a {place_type}.\nPlease detail information that might\nbe helpful for me along this road:\nCaptions for each road are then formatted in the style of\n{road_idx}:\n{road_description}\n20\nand concatenated to form all_road_descriptions.\nThese road captions, along with Hiro\u2019s user-defined inten-\ntion, are then fed into GPT-4 to determine the most promis-\ning road to follow using the following prompt:\n[Role]\nYou are PathSelectorGPT, an expert\nin choosing the optimal road from\nmultiple candidates based on a\nuser-specified intention.\n[Task Description]\nGiven an intention, the road\npreviously traveled, and\ndescriptions of available candidate\nroads, select the best road from the\ncrossroad.\nYour response must be in\nthe following JSON format:\n{\"idx\":\n\"Selected road index\",\n\"reason\":\n\"Justification for your\nselection\"}\n[Example]\nFor the intention \"find a grocery\nstore\", the road previously traveled\nas \"1\", and with candidates \"2:\nLeads to residential area, 3:\nLeads\nto a shopping district\", the output\nmight be:\n{\"idx\":\n\"3\", \"reason\":\n\"Road 3 leads to a shopping district\nwhich is more likely to have a\ngrocery store.\"}\n[Input]\nUser Intention:\n<{agent_intention}>\nRoad Descriptions:\n{all_road_descriptions}\nPreviously Traveled Road:\nRoad\n{from_road_idx}\n[Output]\nYour chosen road index and the\nreasoning behind your selection,\nin the required JSON format:\nWe design such a two-stage captioning and decision-\nmaking pipeline for road selection because Multi-modal\nLLMs cannot process multiple images simultaneously.\nHowever, with the recent advancements of GPT-4V, it may\nbe possible to perform road selection using several road im-\nages with a single prompt at once. Empirical findings sug-\ngest that GPT-4V yields more reasonable choices with the\nfollowing prompt:\n[Role]\nYou are PathSelectorGPT, an expert\nin choosing the optimal road from\nmultiple road images according to a\nuser-specified intention.\n[Task Description]\nGiven a set of road images, select\nthe best road from the crossroad.\nYour answer must be in the following\nJSON format:\n{\"idx\":\n\"Selected road index (start\nby 0)\", \"reason\":\n\"Justification\nfor your selection\"}\n[Input]\nUser Intention:\n<{agent_intention}>\n[Output]\nPlease answer with the road index\nand the reasoning behind your\nselection, in the required JSON\nformat:\nAn example road selection response for the first crossroad\nselection in Fig. 6 is as follows:\n\u201cidx\u201d: \u201c0\u201d, \u201creason\u201d: \u201cChoosing road 0 promises a\ngenuine taste of local cuisine in a less commercialized set-\nting. It\u2019s likely to have family-run eateries where I can re-\nquest non-spicy dishes and savor authentic flavors. This road\noffers a tranquil dining atmosphere, allowing for a more en-\ngaged and leisurely culinary exploration among the locals.\u201d\nD.3. Summarize Place Reviews\nWhen Hiro discovers a place in the street view imagery, he\nretrieves its corresponding name and Google reviews from\nthe V-IRL environment. There is a complex algorithm be-\nhind projecting the 2D box on street view imagery to a con-\ncrete place in the real world, which is detailed in \u201cmatch-\ning between object proposal and places\u201d of Sec. 5.2. After\nHiro obtains these place reviews, he summarizes them into\na place overview (to aid in decision-making) using the fol-\nlowing prompt:\n[Role]\nYou are SummarizeGPT, skilled at\ncondensing multiple reviews into a\nconcise overview of a location.\n[Task Description]\nGiven multiple reviews with ratings,\ncraft a brief overview of the place.\nYour response should be in the\nfollowing JSON format:\n{\"summarization\":\n\"Concise\ndescription (limited to 80 words)\"}\n[Example]\nFor reviews \"Great ambiance but\naverage food (Rating:\n3)\" and\n\"Loved the decor, food could be\n21\nbetter (Rating:\n3.5)\", the output\nmight be:\n{\"summarization\":\n\"The place\nboasts great ambiance and decor,\nbut the food quality receives mixed\nreviews.\"}\n[Input]\nReviews:\n{all_reviews}\n[Output]\nYour concise overview (max 80 words)\nbased on the provided reviews, in\nthe prescribed JSON format:\nD.4. Action Decision\nAfter obtaining the overview of the identified place, Hiro\ndecides to visit the place or keep exploration using GPT-4\nand the following prompt:\n[Role]\nYou are ActionSelectorGPT,\nproficient in choosing the most\nappropriate action based on a\nuser\u2019s background, intention, and\nan overview of a place.\n[Task Description]\nEvaluate the provided user\nbackground, intention, and place\noverview to select the most suitable\naction from the list.\nYour response\nshould be in the following JSON\nformat:\n{\"action\":\n\"Selected Action\",\n\"reason\":\n\"Justification for your\nchoice\"}\nPossible actions:\n- enter_place():\nEnter the\ndesignated place.\n- continue():\nContinue searching\nfor another appropriate place.\n[Example]\nFor the background \"loves historical\nsites\", intention \"discover local\nhistory\", and place overview\n\"This is a 200-year-old preserved\nmansion\", the output might be:\n\"action\":\n\"enter_place()\",\n\"reason\":\n\"The historical mansion\naligns with the user\u2019s interest in\nhistorical sites.\"\n[Input]\nUser Background:\n<{background}>\nUser Intention:\n<{intention}>\nPlace Overview:\n<{place_intro}>\n[Output]\nYour chosen action and the rationale\nbehind your decision in the\nprescribed JSON format:\nHiro\u2019s\nexploration\nwill\ncontinue\nif\nhe\ndecides\nto\ncontinue()\nand\nwill\nterminate\nif\nhe\nopts\nfor\nenter_place().\nE. V-IRL Benchmarks: Details\nE.1. V-IRL Places: Localization (Details)\nAll category results.\nDue to the page limit of the main\npaper, we only present the results of 10 categories in Tab. 3.\nHere, we present the place recall for all 20 categories in\nFig. 18.\nRecall\nconvenience store\ncafe\nclothing store\npark\nbank\npharmacy\nlodging\nbook store\nrestaurant\njewelry store\nlibrary\nlaundry\nschool\nbakery\nhospital\nsupermarket\nbar\ngym\nspa\nmovie theater\n0\n20\n40\n60\nGrounding DINO\nOwl-ViT\nGLIP\nCLIP (w/ GLIP proposal) \nFigure 18. Recalls in V-IRL Place localization\nExample illustrations.\nTo facilitate the understanding of\nV-IRL Place localization benchmark, we present some ex-\namples of CLIP (w/ GLIP proposals) in Fig. 21.\nE.2. V-IRL Places: Recognition and VQA (Details)\nPlace types performance for recognition.\nIn Figure 19,\nwe present the averaged accuracy for each place type across\n10 benchmarked vision models. The size and the x-axis po-\nsition of each bubble correspond to the number of places\nwithin each type. A clear trend emerges: accuracy tends\nto correlate with the frequency. Common categories such\nas clothing store, cafe exhibit higher accuracy,\nwhereas vision models often struggle with infrequent place\ntypes like bowling alley or mosque.\nPlace types performance for VQA.\nThe place types\nperformance of the V-IRL place VQA in Fig. 20 further\n22\naccounting\nart gallery\natm\nbakery\nbank\nbar\nbeauty salon\nbicycle store\nbook store\nbus station\ncafe\ncar dealer\ncar rental\ncar repair\ncar wash\nchurch\nclothing store\nconvenience store\ndoctor\ndrugstore\nelectronics store\nembassy\nflorist\nfuneral home\nfurniture store\ngas station\ngym\nhair care\nhardware store\nhome goods store\nhospital\ninsurance agency\njewelry store\nlaundry\nlawyer\nlibrary\nliquor store\nlocal government office\nlocksmith\nlodging\nmeal delivery\nmeal takeaway\nmovie theater\nmoving company\nmuseum\nnight club\npark\nparking\npharmacy\npolice\npost office\nprimary school\nreal estate agency\nrestaurant\nschool\nsecondary school\nshoe store\nshopping mall\nspa\nstorage\nstore\nsupermarket\ntourist attraction\ntransit station\ntravel agency\nuniversity\nveterinary care\npet store\ndepartment store\nphysiotherapist\nplumber\ndentist\ncourthouse\nmovie rental\npainter\nelectrician\nhindu temple\nPlace count\nAccuracy (100%)\n0\n25\n50\n75\n100\n1\n10\n100\n1000\ncasino\nbowling alley\nmosque\nFigure 19. Category-wise accuracy and numbers for V-IRL Place\nRecognition benchmark.\nverifies the correlation between accuracy and frequency\nfrom a human intention perspective. The top-10 categories\nare closely aligned with the most common human activ-\nities, purchasing and dining.\nIn contrast, the bottom-10\nplace types relate to places that are less frequently encoun-\ntered and serve a more diverse purpose, such as mosque,\nplumber and embassy.\nbicycle store\nbakery\ndepartment store\nsupermarket\nshopping mall\nconvenience store\nflorist\nclothing store\ncafe\nshoe store\nlawyer\ninsurance agency\ngovernment office\nsecondary school\nhindu temple\nmosque\ncourthouse\nbowling alley\nembassy\nplumber\nPlace Type\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nTop 10 place types\nBottom 10 place types\nFigure 20. Top-10 and bottom-10 place types averaged on four\nvision models of V-IRL Place VQA.\nE.3. V-IRL Vision-Language Navigation (Details)\nNavigation pipeline.\nAs mentioned in Appendix B.7, our\nVLN pipeline is similar to [59], however, our benchmark\noffers greater scalability through the worldwide V-IRL plat-\nform and an automated data collection pipeline, as opposed\nto the manual annotation of a specific region. Furthermore,\nour benchmark emphasizes the analysis of the vision com-\nponent in the VLN pipeline, as opposed to [59], which aims\nto enhance performance on existing VLN datasets using\nLLMs.\nImplementation Details.\nHere, we introduce the imple-\nmentation details for LLaVA-1.5 [41] and PP-OCR [20] (+\nGPT-3.5). For LLaVA-1.5 [41], we transform the landmark\nrecognition task to a multiple choice VQA problem, asking\nWhich of the following landmarks\ncan be identified with a high\ndegree of confidence?\nThe VQA options include all potential landmarks men-\ntioned in the route description, along with a \u201cNone of\nabove\u201d choice. The model\u2019s response to this question is\nthen parsed as the landmark observation.\nFor PP-OCR [20] (+ GPT-3.5), we first extract all recog-\nnized text using PP-OCR [20] for each street view image.\nThen, GPT-3.5 [57] determines the presence of each land-\nmark in this street view image, jointly considering the OCR\ntext and landmark name.\nFull set results.\nApart from the mini-set results presented\nin Sec. 5.4, we also provide the full set results of Oracle and\nCLIP (L/14@336px) in Tab. 7. The Oracle results, interest-\ningly, do not achieve a 100% success rate, due to incorrect\ndecisions made by the LLM at stop positions. This is evi-\ndenced by the high arrival ratio and low reaction accuracy\nat stop positions. Empirically, we observe that the LLM oc-\ncasionally decides to keep moving, despite clear destination\nindications in the observations.\nWhen we substitute the map in oracle with the CLIP\nmodel to gather landmark observations from street view im-\nagery, we observe a significant drop in the success rate,\ndue to the inevitable model prediction errors. To improve\nthe success rate in VLN, we can focus on two important\nfactors: (i) designing better vision models; (ii) develop-\ning LLMs and prompt techniques that are robust to vision-\nrelated noise. Especially, our empirical findings suggest that\nsophisticated prompt designs significantly improve the ro-\nbustness of LLMs to visual observation noise.\nMethod\nStart Intersection\nStop\nSuccess Reac Arr\nReac\nArr Reac\nOracle (No Vision)\n0.88\n1.0\n0.95\n0.99\n0.96\n0.88\nCLIP (L/14@336px)\n0.22\n0.84\n0.66\n0.90\n0.61\n0.22\nTable 7. Results of V-IRL VLN-full.\n23\nFigure 21. Samples of V-IRL Place localization using CLIP (w/ GLIP proposals).\n24\n"
  },
  {
    "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
    "link": "https://arxiv.org/pdf/2402.03161.pdf",
    "upvote": "13",
    "text": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled\nVisual-Motional Tokenization\nYang Jin 1 Zhicheng Sun 1 Kun Xu 2 Kun Xu 2 Liwei Chen 2 Hao Jiang 1 Quzhe Huang 1 Chengru Song 2\nYuliang Liu 2 Di Zhang 2 Yang Song 2 Kun Gai 2 Yadong Mu 1\nAbstract\nIn light of recent advances in multimodal Large\nLanguage Models (LLMs), there is increasing at-\ntention to scaling them from image-text data to\nmore informative real-world videos. Compared to\nstatic images, video poses unique challenges for\neffective large-scale pre-training due to the model-\ning of its spatiotemporal dynamics. In this paper,\nwe address such limitations in video-language pre-\ntraining with an efficient video decomposition that\nrepresents each video as keyframes and temporal\nmotions. These are then adapted to an LLM us-\ning well-designed tokenizers that discretize visual\nand temporal information as a few tokens, thus\nenabling unified generative pre-training of videos,\nimages, and text. At inference, the generated to-\nkens from the LLM are carefully recovered to the\noriginal continuous pixel space to create various\nvideo content. Our proposed framework is both\ncapable of comprehending and generating image\nand video content, as demonstrated by its com-\npetitive performance across 13 multimodal bench-\nmarks in image and video understanding and gen-\neration. Our code and models will be available at\nhttps://video-lavit.github.io.\n1. Introduction\nRecently, the significant breakthrough of Large Language\nModels (LLMs) (Brown et al., 2020; Touvron et al., 2023a)\nhas brought a surge in building general-purpose multimodal\nAI assistants (OpenAI, 2023b; Gemini Team, 2023) that\ncan follow both textual and visual instructions. Drawing\non the remarkable reasoning abilities of LLMs and knowl-\nedge in massive alignment corpus (e.g., image-text pairs),\nthey showcase the great potential of accurately compre-\nhending and generating visual content (Sun et al., 2024; Jin\n1Peking University, China 2Kuaishou Technology, China.\nWork was done when Yang Jin and Zhicheng Sun were in-\nterns at Kuaishou Technology. Correspondence to: Yadong Mu\n<myd@pku.edu.cn>.\nEfficient motion vectors (saving > 90% tokens)\nFigure 1. The key observation in this work is: most video parts\nhave a high degree of temporal redundancy that may be described\nby motion vectors. By exploiting these motion vectors, the video\ncan be efficiently tokenized for pre-training of multimodal LLMs.\net al., 2024; Dong et al., 2024). Despite their success, these\nmultimodal LLMs (Alayrac et al., 2022; Liu et al., 2023c)\npredominantly concentrate on the image-text data, leaving\nthe adaptation for video modality less explored. In contrast\nto static images, video serves as a dynamic media form that\nis more in line with human visual perception. Learning\neffectively from video is particularly essential for enhancing\nmachine intelligence to comprehend the real world.\nTo this end, several approaches have made attempts at har-\nnessing the generative capabilities of LLMs for handling\nvideo data. Inheriting the successful paradigm from the\nimage domain, they represent video as a sequence of visual\ntokens that aligns with LLMs\u2019 semantic space by utilizing a\npre-trained 2D image model (Li et al., 2023d; Zhang et al.,\n2023) or a 3D video backbone (Kondratyuk et al., 2023).\nNevertheless, the existing designs are still not competent for\neffectively encoding videos. Compared to images, videos\npose unique challenges associated with higher demands\nfor learning complex spatiotemporal clues, such as time-\nvarying actions and scene changes. In this regard, encoding\nindividual video frames separately by the 2D visual encoder\nfalls short of capturing the temporal motion information,\nwhich plays a vital role in identifying distinct behaviors\nand events within the video content. Although the recent\nconcurrent work VideoPoet (Kondratyuk et al., 2023) crafts\n1\narXiv:2402.03161v2  [cs.CV]  6 Feb 2024\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\na 3D video tokenizer for video generation with LLM, its ap-\nplicability is constrained to short video clips due to the use\nof long token sequences (e.g., 1280 tokens for a 2.2s clip).\nWhen it comes to understanding or generating long videos,\ninputting excessive numbers of tokens into LLMs is deemed\nunacceptable in terms of computational resources.\nThis work addresses the limitation in video-language pre-\ntraining by exploring an efficient video representation that\ndecomposes video into keyframes and temporal motions.\nOur motivation is built upon the natural characteristics of\nvideo data itself. As illustrated in Figure 1, a video is typi-\ncally divided into several shots, where video frames within\neach shot often exhibit substantial information redundancy.\nIt is superfluous to encode all of these frames as tokens and\nincorporate them into the generative pre-training of LLMs.\nThis fact strongly spurs us to decompose each video into\nalternating keyframes and motion vectors, where the former\nencapsulate the primary visual semantics and the latter de-\npict the dynamic evolution of its corresponding keyframe\nover time. There are several benefits to such decomposed\nrepresentation: (1) Compared to processing consecutive\nvideo frames utilizing 3D encoders, the combination of a\nsingle keyframe and motion vectors requires fewer tokens to\nrepresent video temporal dynamics, which is more efficient\nfor large-scale pre-training. (2) The model can inherit the\nacquired visual knowledge from an off-the-shelf image-only\nLLM and focus solely on modeling temporal information\nwithout learning from scratch.\nBased on the above motivations, we present Video-LaVIT\n(Language-VIsion Transformer), a new multimodal pre-\ntraining approach that effectively empowers LLMs to com-\nprehend and generate video content in a unified framework.\nSpecifically, Video-LaVIT incorporates two core compo-\nnents: a tokenizer and a detokenizer to handle video modal-\nity. The video tokenizer aims to transform the continuous\nvideo data into a sequence of compact discrete tokens akin\nto a foreign language, where the keyframes are processed\nby utilizing an established image tokenizer (Jin et al., 2024).\nFor converting the temporal motions into the compatible\ndiscrete format, a spatiotemporal motion encoder is devised.\nIt can capture the time-varying contextual information con-\ntained in extracted motion vectors, thereby significantly en-\nhancing LLMs\u2019 ability to comprehend the intricate actions\nin video. The video detokenizer is responsible for mapping\nthe discretized video token generated by LLMs back into its\noriginal continuous pixel space. During training, video is\nrepresented as an alternating discrete visual-motion token\nsequence, and thus can be optimized under the same next-\ntoken prediction objective together with different modalities.\nSince video is inherently a time series, this joint autore-\ngressive pre-training contributes to learning the sequential\nrelationships of different video clips. We found that Video-\nLaVIT, is capable of serving as a multimodal generalist to\nachieve promising results in both understanding and genera-\ntion tasks without further fine-tuning. The key contributions\nof this work are summarized as:\n\u2022 We introduce Video-LaVIT, a multimodal pre-training\nmethod that pushes the limit of LLMs\u2019 unified under-\nstanding and generation capability towards video.\n\u2022 To efficiently model visual and temporal information in\nvideo, Video-LaVIT incorporates a novel video tokenizer\nand detokenizer that operates on the decomposed repre-\nsentations of keyframes and motion vectors.\n\u2022 Comprehensive quantitative and qualitative experiments\ndemonstrate that Video-LaVIT achieves very competitive\nperformance ranging from image and video comprehen-\nsion to text-to-video and image-to-video generation.\n2. Related Work\nVision-language pre-training. Following the success of\nusing large-scale image-text pairs for contrastive learning\nof vision-language models (Radford et al., 2021), a similar\nidea has been exploited in generative pre-training, where\nvisual and language data are jointly modeled under an au-\ntoregressive process. In practice, this is typically achieved\nby adapting visual image inputs to pre-trained LLMs (Raf-\nfel et al., 2020; Brown et al., 2020; Touvron et al., 2023a)\nvia an intermediate module like cross-attention (Alayrac\net al., 2022), Q-Former (Li et al., 2023c), or linear projec-\ntion (Liu et al., 2023c). More recent approaches such as\nCM3Leon (Yu et al., 2023a) and LaVIT (Jin et al., 2024)\nadvocate the use of discrete visual tokenizers (van den Oord\net al., 2017; Esser et al., 2021) to form a unified next token\nprediction objective. However, these methods are primarily\nfocused on image-text data and cannot be directly extended\nto videos due to the significantly higher computational cost.\nVideo understanding and generation. By unifying videos\nin the above pre-training framework, remarkable progress\nhas been made in video comprehension with masked (Yang\net al., 2022) and autoregressive language models (Li et al.,\n2023d; Zhang et al., 2023; Maaz et al., 2023). However, for\nvideo generation, the mainstream approaches are still based\non diffusion models (Sohl-Dickstein et al., 2015; Song & Er-\nmon, 2019; Ho et al., 2020), which enhance existing image\npre-trained models with better temporal consistency (Ho\net al., 2022; Singer et al., 2023; Blattmann et al., 2023b;\nEsser et al., 2023; Blattmann et al., 2023a). Language model\nbased counterparts (Yan et al., 2021; Hong et al., 2023; Kon-\ndratyuk et al., 2023), on the other hand, face the critical\nchallenge of efficiently encoding video temporal dynamics\nwith limited context windows and computational resources.\nIn response, our work leverages motion vectors, a classic\nand effective cue in video modeling (Zhang et al., 2016;\n2\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\n         Key Frames \nMotion M\n\ud835\udc3c0 \nImage Tokenizer\n(from LaVIT)\nVideo-LaVIT\n[IMG]\n[/IMG] [MOV]\n[/MOV]\n Next Image/Motion/Text Token Prediction\nImage Token\nMotion Token\nText Token\nMotion \nTokenizer\nDecomposition\n\ud835\udc53\u2130 \nMotion M\nspatiotemporal encoder \ud835\udc53\u2130 \nMotion Quantizer Q\nDecoder \ud835\udc53\ud835\udc37 \nReconstruction Loss\n\u2022\u2022\u2022 \ncodebook C\nLatent Embedding \ud835\udc4d  \nRaw Video Sequence\nFigure 2. For each video-text pair, Video-LaVIT decomposes the video into keyframes and motion vectors for efficient tokenization. The\ntokenizers are learned by maximally reconstructing original inputs (e.g., the motion tokenizer is shown on the right). Finally, the encoded\ntokens are concatenated with text tokens to form a multimodal sequence, allowing for unified generative pre-training of the LLM (left).\nWang et al., 2023b; Shen et al., 2024), for improving the ef-\nficacy of LLM-based video comprehension and generation.\n3. Method\nThis work aims to present an effective pre-training frame-\nwork that harnesses the exceptional modeling capability of\nLarge Language Models (LLMs) to facilitate the learning of\nvideo modality. In pursuit of this goal, we highlight two core\ndesigns: a video tokenizer (Section 3.1) which allows for the\nrepresentation of all modalities in a unified discrete form,\nand a video detokenizer (Section 3.2) to map the generated\ndiscrete tokens back to the continuous pixel space. Coped\nwith these two main components, Video-LaVIT can be op-\ntimized through a unified autoregressive training paradigm\n(Section 3.3), enabling it to simultaneously comprehend and\ngenerate various multimodal content.\n3.1. Video Tokenization\nTo encode an untrimmed video as inputs to LLMs, the pre-\nvailing approaches (Lin et al., 2023; Li et al., 2023d) mainly\nuniformly downsample the original video into a series of\nframes. Then, a pre-trained ViT encoder (Radford et al.,\n2021; Fang et al., 2023) is employed to separately encode\nthese frames and produce a sequence of frame-level em-\nbeddings as the video representation. This straightforward\nway disregards the modeling of temporal dynamics between\nframes, thus impeding the capacity to understand the actions\nand camera transitions occurring in the video. While the uti-\nlization of 3D video encoders the very recent (Kondratyuk\net al., 2023) enables the encoding of temporal information,\nit only applies to short video clips and inevitably yields a\nsubstantial proliferation of tokens (e.g., 1280 tokens for one\n2.2s clip), resulting in a heavy computational overhead.\nMotion-aware Video Decomposition. Given the above\nconcerns, our proposed video tokenizer seeks to integrate\ntemporal dynamics into the video representations efficiently.\nWe observe that a video clip captured in the same shot can\nconvey its primary semantics through a single keyframe,\nwhile the subsequent frames only illustrate the temporal\nevolvement based on that keyframe. This property empow-\ners the decomposed video tokenization for keyframe and\ntemporal motion. For the keyframe, we employ an off-the-\nshelf image tokenizer from LaVIT (Jin et al., 2024) to inherit\nthe learned visual codebook and prior knowledge without\ntraining from scratch. For encoding temporal motion infor-\nmation, a common alternative is to calculate hand-crafted\ndense optical flow between adjacent frames (Beauchemin &\nBarron, 1995). Despite providing a fine-grained depiction\nof object motions in videos, the expensive computations ren-\nder it unsuitable for scaling to large-scale video data during\npre-training. Hence, we resort to motion vectors, which can\nbe directly extracted at high speed on the CPU (Wu et al.,\n2018) during the compressed video decoding process.\nAs illustrated in Figure 2, we employ the MPEG-4 (Le Gall,\n1991) compression technique to extract keyframe and mo-\ntion information. For simplicity, the I-frames in MPEG-\n4 are considered as the keyframes requiring tokenization.\nMore sophisticated (but expensive) keyframe selection\nschemes can also be considered, but are not the main focus\nof this work. Formally, each video frame is partitioned into\n16\u00d716 non-overlapping macroblocks. Motion vectors \u20d7m of\nthe t-th frame are estimated by finding the best macroblock\n3\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nDenoising 3D U-Net \ud835\udc54!\nTemporal Block\nSpatial Block\n\u2026\n\u2026\nTemporal Block\nSpatial Block\n\u2026\n\u2026\nRepeat \u00d7 T\nKeyframe !\ud835\udc3c\nMotion Vector \ud835\udc40\nNoisy Video \ud835\udc4b\" + \ud835\udc5b\nDecomposition\nAdding noise\nMinimize\nEDM Objective\nConcatenate\nMotion \nCondition \nEncoder\nMotion Feature Condition\nMotion Input Condition\n...\n\u2026\nVisual Tokens\nMotion Tokens\nReconstructed \nVisual Features\nReconstructed \nMotion Vectors\n...\n\u2026\nClip \ud835\udc56 \u2212 1\nGenerated Tokens from LLM\n(a) Detokenizer Training\n(b) Long Video Decoding\nKer Frame \nU-Net \ud835\udc54!\nVideo \nU-Net \ud835\udc54\"\nDDIM Inversion\nA Video Clip \ud835\udc4b'\nPrediction\n\ud835\udc3c#$%\nClip \ud835\udc56\nDDIM Inversion\n\ud835\udc3c(\n...\nClip \ud835\udc56 + 1\n\u2026\nLast Frame\nKeyframe \ud835\udc3c#\nFigure 3. Illustrations for video detokenization in Video-LaVIT. (a) Training pipeline for the video detokenizer, which aims to reconstruct\nthe original video clip using one keyframe and the subsequent motion vectors. (b) Autoregressive inference for long video decoding.\ncorrespondence between adjacent frames It and It\u22121 :\n\u20d7m(p, q) = arg min\ni,j \u2225It(p, q) \u2212 It\u22121(p \u2212 i, q \u2212 j)\u2225,\n(1)\nwhere I(p, q) indicates the pixel values of the macroblock\nat location (p, q), and (i, j) is the coordinate offset between\nthe center of two macroblocks. Then, a video clip can be\ndecomposed into a keyframe I0 \u2208 RH\u00d7W \u00d73 and the motion\nvectors M \u2208 RT \u00d7 H\n16 \u00d7 W\n16 \u00d72 of its subsequent T frames.\nMotion Vector Tokenization. To transform the motion\nvectors into a sequence of discrete tokens like a foreign\nlanguage, we develop a motion-specific tokenizer based\non the VQ-VAE architecture (van den Oord et al., 2017).\nIt includes a spatiotemporal encoder fE, a learnable code-\nbook C = {ck}K\nk=1, and a decoder fD. The encoder fE\nhas L stacked transformer blocks consisting of spatial and\ntemporal attention layers to fuse the contextual motion in-\nformation among the T frames. It maps the motion vectors\nM \u2208 RT \u00d7 H\n16 \u00d7 W\n16 \u00d72 into a 1D latent embedding sequence\n\u02c6Z \u2208 RN\u00d7d. Each embedding vector \u02c6z \u2208 Rd is then to-\nkenized by a vector quantizer Q, which assigns it to the\nclosest code in C:\nzi = arg min\nj\n\u2225l2(\u02c6zi) \u2212 l2(cj)\u22252,\n(2)\nwhere l2 indicates the L2 normalization. The decoder fD\nhas a similar structure to the encoder and is obliged to map\nthe discrete motion codes {zi}N\ni=1 back to the original mo-\ntion vectors. The whole motion tokenizer can be updated\nby optimizing the reconstruction quality. To prevent code-\nbook collapse during training, we follow Yu et al. (2022) to\nproject the motion embeddings \u02c6Z into a low-dimensional\nspace before quantization and use exponential moving av-\nerage (EMA) updates. More details about the motion tok-\nenizer can be found in Appendix A.1. Finally, a video is\ntokenized into alternating \u27e8visual, motion, ...\u27e9 codes that\nserve as the supervision signals in LLMs during generative\npre-training. Such a factorized tokenization significantly\nreduces the inter-frame redundancy in one video shot while\nefficiently capturing the temporal motion information.\n3.2. Video Detokenization\nThe video detokenizer of Video-LaVIT is in charge of con-\nverting them back into the original continuous pixel space\nfor video generation. Considering the challenge in learning a\ndirect mapping from discrete tokens to the high-dimensional\nvideo space, we take a sequential decoding strategy, wherein\nthe keyframe is initially recovered based on the visual token.\nThe subsequent frames are then decoded by taking both the\nkeyframe and motion tokens as the conditions. The efficacy\nof this strategy in enhancing video generation quality has\nalso been validated by recent work (Girdhar et al., 2023).\nSpecifically, the keyframe and video detokenizers both use\nconditional denoising U-Net (Rombach et al., 2022). Simi-\nlar to LaVIT (Jin et al., 2024), the keyframe U-Net gI takes\nthe reconstructed visual features that contain image seman-\ntics as conditions to infill visual details from a Gaussian\nnoise. Here, we primarily focus on the newly proposed\nvideo detokenizer gV . As illustrated in Figure 3(a), it is a\n3D variant of the original 2D U-Net architecture by inserting\ntemporal convolution and attention layers after the spatial\nmodules, following Blattmann et al. (2023b; 2023a).\nEnhanced Motion Conditioning. The objective of the\nvideo detokenizer gV is to rigorously adhere to the guidance\nof the motion vectors, thereby facilitating the recovery of T\nframes following the keyframe. To this end, we highlight\ntwo different forms of motion conditions in gV . Given the\nmotion vectors M \u2208 RT \u00d7 H\n16 \u00d7 W\n16 \u00d72 of a sampled video clip,\n4\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nwe adopt the nearest neighbor interpolation to ensure that\nit matches the spatial shape of the U-Net input. Also, the\nlatent state \u02c6I of the keyframe from the VAE is repeated T\ntimes along the temporal axis to form visual conditioning.\nThe motion vector M, the keyframe latent \u02c6I, and the noisy\nvideo frames are concatenated channel-wise as the input\ncondition to gV . Except for direct input conditioning, we\nalso enhance conditioning with motion feature embedding\nvia the spatial and temporal cross-attention layers in the\n3D U-Net blocks. Here, the motion features are from a\nconditioning encoder that has a similar architecture to fE\nexcluding the downsample layer to reduce potential infor-\nmation loss. The parameters of the video detokenizer gV\nare updated by minimizing the following EDM training\nobjective (Karras et al., 2022) on a video training dataset D:\nE(X0,\u02c6I, \u02c6\nM)\u223cD,\u03c3,n\nh\n\u03bb\u03c3||gV (X0 + n, \u03c3, \u02c6I, M) \u2212 X0||\ni\n,\n(3)\nwhere \u03c3 \u223c p(\u03c3) is the noise level during training, n \u223c\nN(n; 0, \u03c32) is a random noise added to video sample\nX0, and \u03bb\u03c3 is loss weighting function. At inference, the\n\u27e8visual, motion\u27e9 tokens yielded by LLM are first mapped\ninto visual features and motion vectors by their correspond-\ning tokenizers. The reconstructed visual features are fed\ninto gI to generate a keyframe, which is subsequently com-\nbined with reconstructed motion vectors to serve as condi-\ntions for gV to decode the video clip (See Figure 3(b)).\nLong Video Decoding. Since a video is expressed as multi-\nple alternating \u27e8visual, motion\u27e9 sequences, the interdepen-\ndencies among different video fragments can be effectively\nlearned by autoregressive pre-training of LLMs. Hence,\nVideo-LaVIT naturally supports the generation of longer\nvideos by progressive decoding multiple clips. However,\nseparate decoding will bring inconsistencies in some fine-\ngrained visual details among different clips (See Figure 5).\nTo mitigate this, we incorporate an explicit noise constraint\nwhen decoding the keyframe Ir of a video clip. As illus-\ntrated in Figure 3(b), we reverse its last frame Ir\u22121 from the\npreviously generated clip into an intermediate noisy state\nx\u2206T by reversing the DDIM sampling (Song et al., 2020)\nprocess \u2206T times. Each inversion step is formulated by:\nxt+1 =\nr\u03b1t+1\n\u03b1t xt +\n r\n1\n\u03b1t+1 \u2212 1 \u2212\nr\n1\n\u03b1t \u2212 1\n!\ngI(xt, t, \u02c6I),\n(4)\nwhere \u03b1t is the noise level, \u02c6I is the visual feature condition.\nThe reversed noisy state x\u2206T is then considered as the initial\nnoise in the denoising loop for keyframe Ir. As illustrated\nin Figure 5, adding this noise constraint can improve the\ntemporal consistency between video clips.\n3.3. Unified Generative Modeling\nBased on the developed decomposed video tokenization\nstrategy, it is feasible to indiscriminately treat all the modal-\nities (video, image, and text) as 1D discrete tokens fed\ninto LLMs. Following LaVIT (Jin et al., 2024), special\ntokens (e.g., [MOV] and [/MOV] for motion modality) are\ninserted at the beginning and end of the visual and mo-\ntion token sequence for differentiating modalities in the\ninput data. During pre-training, we also exchange the order\nof multimodal data pairs to form both [video(image), text]\nand [text, video(image)] as input sequences.\nFormally,\ngiven a multimodal sequence y = (y1, y2, .., yS), Video-\nLaVIT inherits the successful generative language modeling\nparadigm from LLM to directly maximize the likelihood of\neach token yi in an autoregressive manner:\np(y) =\nX\ny\u2208D\nS\nX\ni=1\nlog P\u03b8(yi|y<i).\n(5)\nAfter pre-training, Video-LaVIT is capable of serving as a\nmultimodal generalist to achieve both multimodal compre-\nhension and generation of data in any modality.\nModel Training. Video-LaVIT undergoes a three-stage\ntraining procedure on the large-scale multimodal corpora.\nThe purpose of each stage can be summarized as follows:\ni) Tokenizer and Detokenizer Training. This stage requires\nonly pure video data without corresponding textual cap-\ntions. It aims to produce compact video tokens that serve as\nsupervision signals to guide the subsequent generative pre-\ntraining, as well as to facilitate an accurate reconstruction\nof the original videos. ii) Generative Pre-training. Stage-2\nempowers the model to learn the inter-correlation among the\ndata of different modalities via unified generative modeling\nwithin the LLM. iii) Instruction Tuning. To fully unleash\nthe acquired knowledge, the last stage further improves the\ninstruction-following ability to accomplish various multi-\nmodal tasks. More details about the model architectures and\ntraining data for each stage are provided in Appendix A.1.\n4. Experiments\n4.1. Multimodal Understanding\nWith the decomposed video representation, Video-LaVIT is\nnaturally capable of understanding both videos and images.\nHere, we demonstrate its multimodal understanding capa-\nbility on 11 commonly used image and video benchmarks.\nImage Understanding. Table 1 presents an extensive com-\nparison across eight widely used image question answer-\ning and multimodal benchmarks: VQA v2 (Goyal et al.,\n2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari\net al., 2018), ScienceQA-IMG (Lu et al., 2022), MME (Fu\net al., 2023), MMBench (Liu et al., 2023d), SEED (Li et al.,\n2023a), MM-Vet (Yu et al., 2023b). Our model successfully\ngeneralizes the pre-training knowledge to image compre-\nhension tasks and provides the best overall performance.\nSpecifically, with the same instruction dataset and the base\n5\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 1. Image understanding performance (\u2191) on 8 benchmarks. Video-LaVIT achieves state-of-the-art results on most of the benchmarks.\nFor convenience, SQAI denotes ScienceQA-IMG (Lu et al., 2022), and MMB denotes MMBench (Liu et al., 2023d). * indicates that there\nis some overlap with the training data. Note that only LLaVA-1.5 (Liu et al., 2023a) is reported with a higher image resolution of 336.\nMethod\nLLM size\nImage Question Answering\nMultimodal\nVQAv2\nGQA\nVizWiz\nSQAI\nMME\nMMB\nSEED\nMM-Vet\nFlamingo (Alayrac et al., 2022)\n9B\n51.8\n-\n28.8\n-\n-\n-\n-\n-\nBLIP-2 (Li et al., 2023b)\n13B\n41.0\n41.0\n19.6\n61.0\n1293.8\n-\n46.4\n22.4\nInstructBLIP (Dai et al., 2023)\n13B\n-\n49.5\n34.3\n63.1\n1212.8\n44.0\n-\n25.6\nCM3Leon (Yu et al., 2023a)\n7B\n47.6\n-\n37.6\n-\n-\n-\n-\n-\nEmu (Sun et al., 2024)\n13B\n52.0\n-\n34.2\n-\n-\n-\n-\n36.3\nDreamLLM (Dong et al., 2024)\n7B\n72.9*\n-\n49.3\n-\n-\n58.2\n-\n36.6\nVideo-LLaVA (Lin et al., 2023)\n7B\n74.7*\n60.3*\n48.1\n66.4\n-\n60.9\n-\n32.0\nLLaMA-VID (Li et al., 2023e)\n7B\n78.3*\n63.0*\n52.5\n67.7\n1405.6\n65.3\n59.7\n-\nLLaVA-1.5 (Liu et al., 2023a)\n7B\n78.5*\n62.0*\n50.0\n66.8\n1510.7\n64.3\n58.6\n30.5\nVideo-LaVIT\n7B\n80.2*\n63.6*\n54.0\n71.3\n1581.5\n67.5\n64.4\n35.0\nTable 2. Zero-shot video question answering accuracy (\u2191). Video-LaVIT demonstrates state-of-the-art accuracy on all three benchmarks.\nThe evaluation uses a GPT assistant (Maaz et al., 2023), with \u201cScore\u201d denoting a relative score from 0 to 5 assigned by the GPT model.\nMethod\nLLM size\nMSVD-QA\nMSRVTT-QA\nActivityNet-QA\nAccuracy\nScore\nAccuracy\nScore\nAccuracy\nScore\nFrozenBiLM (Yang et al., 2022)\n1B\n32.2\n-\n16.8\n-\n24.7\n-\nVideo-LLaMA (Zhang et al., 2023)\n7B\n51.6\n2.5\n29.6\n1.8\n12.4\n1.1\nVideoChat (Li et al., 2023d)\n7B\n56.3\n2.8\n45.0\n2.5\n26.5\n2.2\nVideo-ChatGPT (Maaz et al., 2023)\n7B\n64.9\n3.3\n49.3\n2.8\n35.2\n2.7\nLLaMA-VID (Li et al., 2023e)\n7B\n69.7\n3.7\n57.7\n3.2\n47.4\n3.3\nVideo-LLaVA (Lin et al., 2023)\n7B\n70.7\n3.9\n59.2\n3.5\n45.3\n3.3\nVideo-LaVIT\n7B\n73.5\n3.9\n59.5\n3.3\n50.2\n3.3\nmodel as LLaVA-1.5 (Liu et al., 2023a), our method consis-\ntently yields the best results on all image question answering\ndatasets. For example on SQAI, it surpasses LLaVA-1.5\nwhich has a higher input resolution by 4.5%, while consis-\ntently outperforming the other video-language models by\nover 3.5%. The same advantages are further validated on\nmore comprehensive multimodal benchmarks, where our\nmodel leads on three out of four benchmarks.\nZero-Shot Video Question Answering. Table 2 compares\nour proposed Video-LaVIT with multiple recent video-\nlanguage models on three common video benchmarks:\nMSVD-QA (Chen & Dolan, 2011), MSRVTT-QA (Xu et al.,\n2016) and ActivityNet-QA (Yu et al., 2019), in terms of\naccuracy and relative score measured by a GPT-3.5 assis-\ntant (Maaz et al., 2023). We achieve state-of-the-art ac-\ncuracies and very competitive relative scores on the three\nbenchmarks, such as surpassing the previous leading model\nVideo-LLaVA (Lin et al., 2023) by 2.8% on MSVD-QA.\nUsing the same 100k video-text instruction dataset from\nVideo-ChatGPT (Maaz et al., 2023) which is also adopted\nby Video-LLaVA, our method outperforms these alternatives\nby explicitly modeling temporal dynamics with motion to-\nkens. Especially for the ActivityNet-QA benchmark, which\ncontains various human behaviors, incorporating motion in-\nformation contributes to the recognition of different actions.\nFor the only metric where our performance is not the best,\nnamely the relative score on MSRVTT-QA, we deliver a\nhigh score only second to Video-LLaVA (by a margin of\n0.2), again confirming the effectiveness of our method.\n4.2. Multimodal Generation\nBy unified generative pre-training, Video-LaVIT can flexi-\nbly generate both video and images. Due to page limitations,\nwe present here its text-to-video generation results, while\nthe text-to-image evaluation is discussed in Appendix B.1.\nZero-Shot Text-to-Video Generation. Table 3 summarizes\nthe model performance on MSR-VTT (Xu et al., 2016) and\nUCF-101 (Soomro et al., 2012), in terms of CLIP similar-\nity (CLIPSIM) (Wu et al., 2021), Fr\u00b4echet video distance\n(FVD) (Unterthiner et al., 2018), Fr\u00b4echet Inception distance\n(FID) (Heusel et al., 2017), and Inception score (IS) (Saito\net al., 2020). Overall, our model significantly outperforms\nmost baselines using similar public datasets, and is highly\ncompetitive against models trained on much larger propri-\netary data, for example leading the FVD on MSR-VTT.\nIn particular, when compared to language model-based\n6\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 3. Zero-shot text-to-video generation performance. Video-LaVIT delivers competitive results against state-of-the-art models trained\non more proprietary data, with data size reported in terms of the number of training video clips. The next best results are underlined.\nMethod\nData size\nPublic data\nMSR-VTT\nUCF-101\nCLIPSIM (\u2191)\nFVD (\u2193)\nFID (\u2193)\nIS (\u2191)\nFVD (\u2193)\nCogVideo (Hong et al., 2023)\n5.4M\n\u2713\n0.2631\n1294\n23.59\n25.27\n701.59\nVideo LDM (Blattmann et al., 2023b)\n10M\n\u2713\n0.2929\n-\n-\n33.45\n550.61\nVideoComposer (Wang et al., 2023b)\n10M\n\u2713\n0.2932\n580\n-\n-\n-\nInternVid (Wang et al., 2024)\n28M\n\u2713\n0.2951\n-\n-\n21.04\n616.51\nMake-A-Video (Singer et al., 2023)\n20M\n\u2713\n0.3049\n-\n13.17\n33.00\n367.23\nVideoPoet (Kondratyuk et al., 2023)\n270M\n\u00d7\n0.3049\n213\n-\n38.44\n355.00\nPYoCo (Ge et al., 2023)\n22.5M\n\u00d7\n-\n-\n9.73\n47.76\n355.19\nSVD (Blattmann et al., 2023a)\n152M\n\u00d7\n-\n-\n-\n-\n242.02\nVideo-LaVIT\n10M\n\u2713\n0.3010\n169.51\n11.80\n37.96\n274.96\nA majestic eagle soars gracefully over a breathtaking\nA steam train moving on a mountainside by\nmountain range.\nVincent van Gogh.\nGen-2\nVideo-LaVIT\nSVD\nVideo-LaVIT\nFigure 4. Text-to-video (top) and image-to-video (bottom) generation comparison with Gen-2 (Runaway, 2023) and SVD-XT (Blattmann\net al., 2023a). Text prompts are from Emu Video (Girdhar et al., 2023) and SVD. The I2V generation is conditioned on the leftmost frame.\ntext-to-video generators, our method consistently outscores\nCogVideo (Hong et al., 2023), while surpassing the recent\nconcurrent work VideoPoet (Kondratyuk et al., 2023), which\nuses a 3D video tokenizer trained on the much larger data.\nThis clearly validates the superiority of our tokenizer design.\n4.3. Qualitative Results\nThis section compares videos created by Video-LaVIT with\nstate-of-the-art results under both text and image conditions.\nIt also presents our special ability to generate long videos.\nMore visualization examples are provided in Appendix B.1.\nThe text-to-video and image-to-video generation results are\nvisualized in Figure 4. For text-to-video generation, our\nmethod can produce visual quality not much far from the\nclosed-source model Gen-2 (Runaway, 2023), thanks to the\nunified pre-training framework with images. Meanwhile,\nVideo-LaVIT is advantageous in reasoning abilities, such as\ninferring better motion (in the top-left example) and adding\nartistic touches based on the text prompt (as in both cases).\nFor image-to-video generation, our method is comparable\nto the state-of-the-art model SVD (Blattmann et al., 2023a)\nin generating both coherent and highly aesthetic video clips\n(the bottom-left example). In addition, the decomposed\nvideo representation enables the video decoder to produce\nmore salient and vivid movements given a relatively difficult\nsynthetic image prompt (the bottom-right example).\nFurthermore, our autoregressive model can be naturally\nextended to long video generation, as shown in Figure 5.\nThanks to the proposed explicit noise constraint when de-\n7\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nVideo-LaVIT\n0\u201348 frames\n48\u201396 frames\nw/o constraint\n0\u201348 frames\nFigure 5. Long video generation example with \u201ca 360 shot of a sleek yacht sailing gracefully through the crystal-clear waters of the\nCaribbean\u201d. The top two rows use the noise constraint in Equation (4) to improve temporal consistency, while the bottom row does not.\nTable 4. Ablation of proposed motion tokenization strategy in zero-\nshot video understanding (left) and generation (right).\nMethod\nMSVD\nActivityNet\nUCF-101\nAccuracy\nAccuracy\nIS (\u2191)\nFVD (\u2193)\nw/o motion\n67.3\n47.4\n29.56\n442.80\nw/ motion\n73.5\n50.2\n37.96\n274.96\ncoding consecutive video clips, the temporal consistency\nbetween decoded clips is greatly improved. In contrast,\ndecoding each video clip separately will result in the in-\ncoherence of fine-grained visual details among the video\nframes of different clips (See the bottom of Figure 5).\n4.4. Ablation Study\nThis section investigates the impact of motion tokenization\nand different motion token lengths. Due to limited space,\nthe ablation for proposed enhanced motion conditioning\nstrategy is provided in Appendix B.1.\nEffect of Motion Tokenization. To validate the effective-\nness of motion tokenization in video pre-training, we design\na baseline that represents a video by separately tokenizing\n16 uniformly-sampled frames without any motion tokens.\nWhen evaluating video synthesis, we first employ the same\nkeyframe U-Net to yield a keyframe from generated vi-\nsual tokens and then input it to an image-to-video model\nSVD-XT to generate a video sample. As shown in Table 4,\nby incorporating motion vectors as the representation of\ntemporal information, Video-LaVIT achieves significantly\nbetter results on both video comprehension and generation\ntasks. Especially for text-to-video generation on the UCF-\n101 benchmark, it is difficult to generate accurate video\ncontent described by text prompts without motion guidance.\nEffect of Token Length. We also explore the influence\nTable 5. Ablation of the number of motion tokens (denoted by N)\nin zero-shot video understanding (left) and generation (right).\nMethod\nMSVD\nActivityNet\nUCF-101\nAccuracy\nAccuracy\nIS (\u2191)\nFVD (\u2193)\nN = 256\n69.2\n48.8\n37.57\n281.24\nN = 64\n73.5\n50.2\n37.96\n274.96\nof different motion token lengths when encoding temporal\nmotion information. The detailed results are reported in Ta-\nble 5. It can be observed that a very small number suffice\nto yield high understanding and generation performance.\nMore token numbers may lead to representation redundancy\nand bring more duplicate motion token IDs when encoding\nvideos without obvious motions, rendering the next-token\nprediction learning paradigm of LLM less effective. Using\nfewer motion tokens also allows for more video clips as\ninput conditions under the same context length of LLM,\nwhich is useful for long video understanding.\n5. Conclusion\nThis paper introduces Video-LaVIT, a multimodal genera-\ntive pre-training method that empowers LLMs with unified\ncomprehension and generation of videos, images, and lan-\nguage. At the core of our method is a video decomposition\nscheme that allows for more effective modeling of temporal\ninformation while reusing visual knowledge from image-\nonly multimodal LLMs. The decomposed keyframes and\nmotion vectors can be efficiently tokenized to be adapted\nto LLMs for unified generative pre-training. Further, with\na well-designed detokenizer, our model flexibly supports\nmultimodal generation inclusive of long videos. Finally, the\nunderstanding and generative capabilities of Video-LaVIT\nare verified by extensive quantitative and qualitative results.\n8\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nReferences\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. CM3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: A visual language model for few-\nshot learning. In NeurIPS, pp. 23716\u201323736, 2022.\nAwadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y.,\nZhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa,\nS., et al. OpenFlamingo: An open-source framework\nfor training large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390, 2023.\nBaek, Y., Lee, B., Han, D., Yun, S., and Lee, H. Character\nregion awareness for text detection. In CVPR, pp. 9365\u2013\n9374, 2019.\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\nin time: A joint video and image encoder for end-to-end\nretrieval. In ICCV, pp. 1728\u20131738, 2021.\nBeauchemin, S. S. and Barron, J. L. The computation of\noptical flow. ACM Computing Surveys, 27(3):433\u2013466,\n1995.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\nKilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V.,\nLetts, A., et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv preprint\narXiv:2311.15127, 2023a.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models.\nIn CVPR, pp. 22563\u201322575, 2023b.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nIn NeurIPS, pp. 1877\u20131901, 2020.\nCarreira, J. and Zisserman, A. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset. In CVPR, pp.\n6299\u20136308, 2017.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12M: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In CVPR, pp. 3558\u2013\n3568, 2021.\nChen, D. and Dolan, W. B. Collecting highly parallel data\nfor paraphrase evaluation. In ACL, pp. 190\u2013200, 2011.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards\ngeneral-purpose vision-language models with instruction\ntuning. In NeurIPS, 2023.\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\nL., Sun, J., Zhou, H., Wei, H., et al. DreamLLM: Syner-\ngistic multimodal comprehension and creation. In ICLR,\n2024.\nEsser, P., Rombach, R., and Ommer, B. Taming transformers\nfor high-resolution image synthesis. In CVPR, pp. 12873\u2013\n12883, 2021.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\nmanidis, A. Structure and content-guided video synthesis\nwith diffusion models. In ICCV, pp. 7346\u20137356, 2023.\nFang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\nHuang, T., Wang, X., and Cao, Y. EVA: Exploring the\nlimits of masked visual representation learning at scale.\nIn CVPR, pp. 19358\u201319369, 2023.\nFeng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He,\nX., Basu, S., Wang, X. E., and Wang, W. Y. LayoutGPT:\nCompositional visual planning and generation with large\nlanguage models. In ICLR, 2024.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu,\nZ., Lin, W., Yang, J., Zheng, X., et al. MME: A compre-\nhensive evaluation benchmark for multimodal large lan-\nguage models. arXiv preprint arXiv:2306.13394, 2023.\nGe, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro,\nB., Jacobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y.\nPreserve your own correlation: A noise prior for video\ndiffusion models. In ICCV, pp. 22930\u201322941, 2023.\nGemini Team, G. Gemini: A family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805,\n2023.\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,\nRambhatla, S. S., Shah, A., Yin, X., Parikh, D., and\nMisra, I. Emu video: Factorizing text-to-video gener-\nation by explicit image conditioning.\narXiv preprint\narXiv:2311.10709, 2023.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the V in VQA matter: Elevating the\nrole of image understanding in visual question answering.\nIn CVPR, pp. 6904\u20136913, 2017.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. VizWiz grand challenge:\nAnswering visual questions from blind people. In CVPR,\npp. 3608\u20133617, 2018.\n9\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. In ICLR, 2021.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs trained by a two time-scale update\nrule converge to a local Nash equilibrium. In NeurIPS,\npp. 6629\u20136640, 2017.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. In NeurIPS, pp. 6840\u20136851, 2020.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,\nand Fleet, D. J. Video diffusion models. In NeurIPS, pp.\n8633\u20138646, 2022.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.\nCogVideo: Large-scale pretraining for text-to-video gen-\neration via transformers. In ICLR, 2023.\nHudson, D. A. and Manning, C. D. GQA: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In CVPR, pp. 6700\u20136709, 2019.\nJin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang,\nQ., Chen, B., Lei, C., Liu, A., et al. Unified language-\nvision pretraining in LLM with dynamic discrete visual\ntokenization. In ICLR, 2024.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models. In\nNeurIPS, pp. 26565\u201326577, 2022.\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier,\nC., Vijayanarasimhan, S., Viola, F., Green, T., Back, T.,\nNatsev, P., et al. The Kinetics human action video dataset.\narXiv preprint arXiv:1705.06950, 2017.\nKondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J.,\nHornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar,\nV., et al. VideoPoet: A large language model for zero-\nshot video generation. arXiv preprint arXiv:2312.14125,\n2023.\nKynk\u00a8a\u00a8anniemi, T., Karras, T., Aittala, M., Aila, T., and\nLehtinen, J. The role of ImageNet classes in Fr\u00b4echet\nInception distance. In ICLR, 2023.\nLe Gall, D. MPEG: A video compression standard for\nmultimedia applications. Communications of the ACM,\n34(4):46\u201358, 1991.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and\nShan, Y.\nSEED-Bench: Benchmarking multimodal\nLLMs with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023a.\nLi, D., Li, J., and Hoi, S. BLIP-Diffusion: Pre-trained\nsubject representation for controllable text-to-image gen-\neration and editing. In NeurIPS, 2023b.\nLi, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping\nlanguage-image pre-training for unified vision-language\nunderstanding and generation.\nIn ICML, pp. 12888\u2013\n12900, 2022.\nLi, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot-\nstrapping language-image pre-training with frozen image\nencoders and large language models. In ICML, pp. 19730\u2013\n19742, 2023c.\nLi, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang,\nY., Wang, L., and Qiao, Y. VideoChat: Chat-centric video\nunderstanding. arXiv preprint arXiv:2305.06355, 2023d.\nLi, Y., Wang, C., and Jia, J. LLaMA-VID: An image is\nworth 2 tokens in large language models. arXiv preprint\narXiv:2311.17043, 2023e.\nLian, L., Li, B., Yala, A., and Darrell, T. LLM-grounded dif-\nfusion: Enhancing prompt understanding of text-to-image\ndiffusion models with large language models.\narXiv\npreprint arXiv:2305.13655, 2023.\nLin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan,\nL.\nVideo-LLaVA: Learning united visual representa-\ntion by alignment before projection.\narXiv preprint\narXiv:2311.10122, 2023.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft\nCOCO: Common objects in context. In ECCV, pp. 740\u2013\n755, 2014.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. In NeurIPS, 2023b.\nLiu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou,\nX., Yang, J., Su, H., Zhu, J., et al. LLaVA-Plus: Learning\nto use tools for creating multimodal agents. arXiv preprint\narXiv:2311.05437, 2023c.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., et al. MMBench:\nIs your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281, 2023d.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. In NeurIPS, pp. 2507\u20132521,\n2022.\n10\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nMaaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-\nChatGPT: Towards detailed video understanding via\nlarge vision and language models.\narXiv preprint\narXiv:2306.05424, 2023.\nOpenAI.\nGPT-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023a.\nOpenAI. GPT-4V(ision) system card. https://openai.\ncom/research/gpt-4v-system-card, 2023b.\nOrdonez, V., Kulkarni, G., and Berg, T. L. Im2Text: De-\nscribing images using 1 million captioned photographs.\nIn NeurIPS, pp. 1143\u20131151, 2011.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M\u00a8uller, J., Penna, J., and Rombach, R. SDXL: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. In ICLR, 2024.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, pp. 8748\u20138763, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 21(1):5485\u20135551, 2020.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, pp. 10684\u201310695, 2022.\nRunaway. Gen-2. https://research.runwayml.\ncom/gen2, 2023.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. In\nNeurIPS, pp. 36479\u201336494, 2022.\nSaito, M., Saito, S., Koyama, M., and Kobayashi, S. Train\nsparsely, generate densely: Memory-efficient unsuper-\nvised training of high-resolution temporal GAN. IJCV,\n128(10-11):2586\u20132606, 2020.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In ACL, pp. 2556\u2013\n2565, 2018.\nShen, C., Gan, Y., Chen, C., Zhu, X., Cheng, L., and Wang,\nJ. Decouple content and motion for conditional image-to-\nvideo generation. In AAAI, 2024.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S.,\nHu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-A-\nVideo: Text-to-video generation without text-video data.\nIn ICLR, 2023.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequilib-\nrium thermodynamics. In ICML, pp. 2256\u20132265, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In ICLR, 2020.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. In NeurIPS, pp. 11918\u2013\n11930, 2019.\nSoomro, K., Zamir, A. R., and Shah, M. A dataset of 101\nhuman action classes from videos in the wild. Center for\nResearch in Computer Vision, 2(11), 2012.\nSun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,\nGao, H., Liu, J., Huang, T., and Wang, X. Emu: Genera-\ntive pretraining in multimodality. In ICLR, 2024.\nTogether Computer.\nRedPajama:\nan open dataset\nfor training large language models, 2023.\nURL\nhttps://github.com/togethercomputer/\nRedPajama-Data.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. LLaMA: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971,\n2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nTran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri,\nM. Learning spatiotemporal features with 3D convolu-\ntional networks. In ICCV, pp. 4489\u20134497, 2015.\nUnterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R.,\nMichalski, M., and Gelly, S. Towards accurate generative\nmodels of video: A new metric & challenges. arXiv\npreprint arXiv:1812.01717, 2018.\nvan den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural\ndiscrete representation learning. In NeurIPS, pp. 6309\u2013\n6318, 2017.\nVillegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo,\nH., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and\nErhan, D. Phenaki: Variable length video generation\nfrom open domain textual descriptions. In ICLR, 2023.\n11\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nWang, F.-Y., Chen, W., Song, G., Ye, H.-J., Liu, Y., and Li,\nH. Gen-L-Video: Multi-text to long video generation via\ntemporal co-denoising. arXiv preprint arXiv:2305.18264,\n2023a.\nWang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang,\nY., Shen, Y., Zhao, D., and Zhou, J. VideoComposer:\nCompositional video synthesis with motion controllabil-\nity. In NeurIPS, 2023b.\nWang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X.,\nWang, Y., Luo, P., Liu, Z., et al. InternVid: A large-\nscale video-text dataset for multimodal understanding\nand generation. In ICLR, 2024.\nWu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F.,\nSapiro, G., and Duan, N. GODIVA: Generating open-\ndomain videos from natural descriptions. arXiv preprint\narXiv:2104.14806, 2021.\nWu, C.-Y., Zaheer, M., Hu, H., Manmatha, R., Smola, A. J.,\nand Kr\u00a8ahenb\u00a8uhl, P. Compressed video action recognition.\nIn CVPR, pp. 6026\u20136035, 2018.\nXu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large\nvideo description dataset for bridging video and language.\nIn CVPR, pp. 5288\u20135296, 2016.\nYan, W., Zhang, Y., Abbeel, P., and Srinivas, A. VideoGPT:\nVideo generation using VQ-VAE and transformers. arXiv\npreprint arXiv:2104.10157, 2021.\nYang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.\nZero-shot video question answering via frozen bidirec-\ntional language models. In NeurIPS, pp. 124\u2013141, 2022.\nYu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku,\nA., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized\nimage modeling with improved VQGAN. In ICLR, 2022.\nYu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O.,\nWang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al.\nScaling autoregressive multi-modal models: Pretraining\nand instruction tuning. arXiv preprint arXiv:2309.02591,\n2023a.\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L.\nMM-Vet: Evaluating large multi-\nmodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023b.\nYu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and\nTao, D. ActivityNet-QA: A dataset for understanding\ncomplex web videos via question answering. In AAAI,\npp. 9127\u20139134, 2019.\nZeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y.,\nand Li, H. Make pixels dance: High-dynamic video\ngeneration. arXiv preprint arXiv:2311.10982, 2023.\nZhang, B., Wang, L., Wang, Z., Qiao, Y., and Wang, H.\nReal-time action recognition with enhanced motion vec-\ntor CNNs. In CVPR, pp. 2718\u20132726, 2016.\nZhang, H., Li, X., and Bing, L.\nVideo-LLaMA: An\ninstruction-tuned audio-visual language model for video\nunderstanding. arXiv preprint arXiv:2306.02858, 2023.\n12\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nA. Experimental Settings\nA.1. Model Implementation Details\nVideo Tokenizer We employ the off-the-shelf visual tokenizer from LaVIT (Jin et al., 2024) to transform the video keyframe\ninto 90 visual tokens on average, which follows most existing MLLMs to utilize the ViT-G/14 of EVA-CLIP (Fang et al.,\n2023) as the visual encoder. The visual codebook size is set to 16384. Please refer to the original paper for more details.\nDuring training and inference, images and keyframes are resized to 224\u00d7224 resolution as input.\nAs for motion tokenization, we downsample the original videos at 6 fps and then take 24 consecutive frames as a video clip\nto compute the motion vector M. It is further divided by the width and height of the corresponding video to normalize the\nvalue within the range of [\u22121, 1]. Before feeding into the motion tokenizer, the motion vector M is resized to a resolution\nof 16\u00d716, resulting in the final input tensor shape being B \u00d7 24 \u00d7 16 \u00d7 16 \u00d7 2. The encoder fE and decoder fD in our\nmotion tokenizer both have L = 12 transformer blocks with 512 hidden states and 8 attention heads. Each block consists of\nspatial, temporal attention, and feed-forward layers. Before the attention computation, the motion input is reshaped into\n[(B \u2217 24) \u00d7 (16 \u2217 16) \u00d7 D] and [(B \u2217 16 \u2217 16) \u00d7 24 \u00d7 D] for the spatial and temporal layers, respectively. We insert the\n3D average pooling layers after the [3, 6, 9, 12] encoder blocks to conduct [\u00d72, \u00d72, \u00d72, \u00d73] downsampling on the motion\nembeddings, which will then be quantized into 256 discrete motion tokens. For the version with 64 motion tokens, the spatial\nshape of the motion embeddings will also be downsampled \u00d72 before quantization. The decoder fD includes symmetrical\nupsampling layers to recover the original input motion vector during training. The size of learned motion codebook is set to\n1024. To improve the training stability of the motion codebook, we leverage exponential moving average (EMA) updates\nwith a weight of 0.995. Before quantization, the motion embeddings are projected into a low-dimensional space to improve\nthe codebook usage, following the experience of Yu et al. (2022).\nVideo Detokenizer During training of the video detokenizer, we randomly sampled 24 consecutive frames from videos\ndownsampled at 6 fps. The motion conditioning encoder has the same transformer architecture (12 blocks) as fE, except that\nthe downsample layers are removed to keep the same temporal dimension with the input video frames. This strategy reduces\nthe compression of motion information during encoding and provides explicit guidance for each frame to be denoised\nin the 3D U-Net. The detailed architecture of the 3D U-Net employed follows the same implementations as Blattmann\net al. (2023b; 2023a). During the EDM-preconditioning optimization for the detokenizer, the distribution of log \u03c3 is set\nto N(1.0, 1.22) to encourage a higher noise level, which is found effective for the high-resolution generation (Girdhar\net al., 2023). We train the motion conditioning encoder, the input encoding layer, and all the cross-attention layers in the\n3D U-Net from scratch and initialize the other weights from the SVD img2vid-xt (Blattmann et al., 2023a). To reduce\nthe computational complexity, the detokenizer is first trained with a resolution 384 \u00d7 384 for 50k steps, and then further\nfine-tuned at two types of resolutions: 768 \u00d7 768 or 1024 \u00d7 576 for another 10k steps.\nLanguage Model We utilize Llama 2 7B (Touvron et al., 2023b) as the default large language model for the generative\npre-training. The weight of the language model is initialized from LaVIT (Jin et al., 2024) to preserve the learned visual\nprior knowledge to support the comprehension and generation for the image domain. During pre-training, we mix the\nimage-text, video-text pairs, and textual data in one batch to form the final multimodal input sequence.\nA.2. Pre-training Data\nThe training dataset used by Video-LaVIT only consists of publicly available image and video datasets. In the following, we\npresent a detailed elaboration of the dataset usage at each training stage.\nStage 1: The video tokenizer and detokenizer are trained on the WebVid-10M (Bain et al., 2021), which is an open-source\nvideo-text dataset containing 10 million video-text pairs scraped from the stock footage sites. Since both our tokenizer\nand detokenizer do not rely on textual data, we only employ pure video data at this stage. Due to the common watermarks\nin WebVid-10M, during the training of the video detokenizer, we incorporate a subset of InterVid-14M-aesthetics (Wang\net al., 2024) to remove watermarks in the generated videos. It has also been shown useful in PixelDance (Zeng et al., 2023).\nSpecifically, we first select a subset of 4s\u201310s video clips with the highest aesthetic scores and then follow SVD (Blattmann\net al., 2023a) in applying CRAFT (Baek et al., 2019) to filter out those videos with unwanted written text. The result contains\nabout 300k publicly available video clips. Noting that the 300k video subset is only used during the training of the\nvideo detokenizer to improve the aesthetics of the generated videos, the results reported in all the experiments are\ntested on the checkpoint that uses only the WebVid-10M dataset.\nStage 2: The language model is pre-trained on a mixture of video, image and text data, including WebVid-10M (Bain\n13\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\net al., 2021); 93M samples from Conceptual Caption (Sharma et al., 2018; Changpinyo et al., 2021), SBU (Ordonez et al.,\n2011), and BLIP-Capfilt (Li et al., 2022). Moreover, we also employ the English text corpus from RedPajama (Together\nComputer, 2023), which is open-source data like the original one to train LLaMA from scratch. The purpose of including the\nEnglish text corpus during pre-training is to preserve the already learned language understanding ability of LLM (e.g., the\nperformance on linguistic benchmarks like MMLU (Hendrycks et al., 2021)) while acquiring good multimodal capabilities.\nStage 3: For a fair comparison, we employ the same instruction tuning dataset as the existing works (Lin et al., 2023; Li\net al., 2023e) during this stage. It includes a 665k image-text instruction dataset from LLaVA v1.5 (Liu et al., 2023a) and a\n100k video-text instruction dataset from Video-ChatGPT (Maaz et al., 2023).\nA.3. Training Settings\nThe detailed training hyper-parameter settings for the video tokenizer, detokenizer, and language model in Video-LaVIT are\nreported in Table 6. We adopt the same instruction tuning setting as LLaVA v1.5 (Liu et al., 2023a).\nConfiguration\nLanguage Model\nTokenizer\nDetokenizer\nLLM init\nLaVIT-7B\n-\n-\nOptimizer\nAdamW\nAdamW\nAdamW\nOptimizer Hyperparameters\n\u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 1e\u22126\n\u03b21 = 0.9, \u03b22 = 0.99, \u03f5 = 1e\u22126\nGlobal batch size\n2048\n512\n128\nPeak learning rate of LLM\n2e-5\n-\n-\nPeak learning rate of other Part\n5e-5\n1e-4\n5e-5\nLearning rate schedule\nCosine\nCosine\nCosine\nTraining Steps\n30K\n100K\n60K\nWarm-up steps\n2k\n5K\n3K\nWeight decay\n0.1\n0.001\n0.001\nGradient clipping\n1.0\n1.0\n1.0\nInput sequence to LLM\n2048\n-\n-\nNumerical precision\nbfloat16\nbfloat16\nbfloat16\nGPU Usage\n128 NVIDIA A100\n64 NVIDIA A100\n64 NVIDIA A100\nFramework\nMegatron\nDeepSpeed\nDeepSpeed\nTraining Time\n60h\n10h\n48h\nTable 6. The detailed training hyperparameters of Video-LaVIT\nA.4. Evaluation\nImage Understanding is evaluated using eight popular image question answering and multimodal benchmarks: VQA\nv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), ScienceQA-IMG (Lu et al.,\n2022), MME (Fu et al., 2023), MMBench (Liu et al., 2023d), SEED (Li et al., 2023a), MM-Vet (Yu et al., 2023b). For\nquestion-answering datasets, we use the same prompts as in LLaVA-1.5 (Liu et al., 2023a), and adopt the widely used VQA\naccuracy as the evaluation metric.\nVideo Question Answering. Three common datasets are considered: MSVD-QA (Chen & Dolan, 2011), MSRVTT-QA (Xu\net al., 2016) and ActivityNet-QA (Yu et al., 2019). To assess model accuracy, a GPT-3.5 assistant (Maaz et al., 2023) is\nemployed, which also produces outputs a relative score ranging from 0 to 5.\nText-to-Image Generation. We adopt the validation set of MS-COCO (Lin et al., 2014) and randomly select 30K samples.\nThe quality of the generated images is evaluated by Fr\u00b4echet Inception distance (FID) (Heusel et al., 2017), which computes\nits Fr\u00b4echet distance to the ground truth in the feature space of a pre-trained Inception V3 model.\nText-to-Video Generation is measured on MSR-VTT (Xu et al., 2016) and UCF-101 (Soomro et al., 2012). For MSR-VTT,\nwe use all 2990 videos and sample one caption for each video, resulting in 2990 video-text pairs; for UCF-101, we sample\n20 videos per class and follow PYoCo (Ge et al., 2023) to curate prompts for each class, producing 2020 video-text pairs.\nTheir evaluation metrics are detailed below.\n\u2022 CLIP similarity (CLIPSIM) (Wu et al., 2021) measures the semantic similarity between video-text pairs. We follow\nPhenaki (Villegas et al., 2023) and VideoPoet (Kondratyuk et al., 2023) in using a ViT-B/16 (Radford et al., 2021)\nto compute CLIP scores between 224\u00d7224 sized video frames and their corresponding captions. The final score is\n14\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\naveraged over all generated video frames.\n\u2022 Fr\u00b4echet video distance (FVD) (Unterthiner et al., 2018) evaluates the Fr\u00b4echet distance between generated and real videos\nin the feature space of an I3D action classification model (Carreira & Zisserman, 2017) pre-trained on Kinetics-400 (Kay\net al., 2017).\n\u2022 Fr\u00b4echet Inception distance (FID) (Heusel et al., 2017) measures the Fr\u00b4echet distance between generated and real video\nframes. Following PYoCo (Ge et al., 2023), we use a ViT-B/32 model (Kynk\u00a8a\u00a8anniemi et al., 2023) to extract the frame\nfeatures. The final result is averaged over all video frames.\n\u2022 Inception score (IS) (Saito et al., 2020) evaluates the distribution of our generated video frames. We employ a C3D\nmodel (Tran et al., 2015) fine-tuned on UCF-101 to calculate a video version of the inception score. The model takes\nthe central 16 frames of each video as the input.\nNote that there are slight variations in the evaluation protocols of different papers. We have sought to keep our protocol the\nsame as or similar to most of the top-ranked methods.\nB. Additional Results\nB.1. Multimodal Generation\nThis section provides additional qualitative results and an ablation study to demonstrate the effectiveness of our design for\nmultimodal generation, complementing the existing comparisons in the main paper.\nText-to-Image Generation. Figure 6 illustrates the comparison of text-to-image generation between Video-LaVIT and\nSDXL (Podell et al., 2024). Overall, our method achieves competitive visual quality while having better language\nunderstanding and reasoning capabilities. For example, in the top-left case of a young woman in front of a UFO, our method\nproduces highly aesthetic headshots of the woman, while capturing the detail of \u201csharp focus\u201d in the text prompt. And in the\nbottom-left example of apple painting, our model successfully infers from the prompt \u201cneither is red and both are green\u201d to\ndraw two green apples, thanks to the better logical reasoning ability of the LLM-based generation approach we adopted.\nText-to-Video Generation. Figure 7 compares Video-LaVIT to a closed-source model Gen-2 (Runaway, 2023). As can be\nseen, our model produces high-quality videos that are generally comparable to Gen-2, which is especially evident in the last\ntwo examples where it successfully captures details such as \u201dmoss and many flowers\u201d and \u201dautumn\u201d in the text prompt\nand yields very similar results to Gen-2. Moreover, the first two comparisons demonstrate a favorable prompt following\nability of our model. In the first case with the keyword \u201crunning\u201d, our model produces significant camera motion toward\nthe cabin, while the movement in Gen-2 is relatively nuanced. In the second case, our model correctly displays multiple\n\u201cpirate ships\u201d as the prompt specified, with artistic details such as all the ships being on fire, according to the implication of\n\u201cintense battle\u201d. These results support the benefits of unified video-language pre-training in prompt following capabilities.\nImage-to-video generation. Figure 8 presents a comparison of Video-LaVIT with the open-source model SVD (Blattmann\net al., 2023a), both conditioned on synthetic image prompts. Moving to some unseen test cases, our method produces video\nclips featuring both natural and refined motions, thanks to the decomposed video representation that can better transfer\nmotion-related knowledge to new visual inputs. For example, in the middle case, our generated goat smoothly lowers\nits head and blinks as if it were a human to think, while the goat in the video produced by SVD hardly moved. In the\nbottom case, where the image prompt shows a teddy is riding a motorcycle, our generated full video looks very natural\nand similar to a human riding a motorcycle, while SVD constantly produces a scenario where the motorcycle is moving a\ndifferent direction from where its tire is pointing (which is physically wrong). Overall, our model demonstrates superior\nimage-to-video generation performance with the inclusion of decoupled visual-motion tokenization and LLM pre-training.\nLong Video Generation is showcased in Figure 9. By explicitly constraining the noise when decoding successive video\nclips, our model can provide a high temporal consistency during long video generation. For example, in the first two\ncases, the dog and the jeep car maintain the same identity across different clips with highly coherent visual details. In the\nlast example which features large camera movement, the moving trajectory remains consistent as it approaches the cabin\naccording to the text prompt. These examples all illustrate our reasonably good quality of long video generation. Note that\nall the generated videos are provided at https://video-lavit.github.io.\nAblation Study. To rigorously reconstruct original video content, we employ the enhanced conditioning: motion input\n15\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\ncondition and motion feature condition for training the 3D video U-Net gV . We illustrate the effect of proposed enhanced\nmotion conditioning (EMC) strategy on video decoding in Figure 10. The variant \u201cw/o EMC\u201d only leverages motion vectors\nas the input condition. Compared with using EMC, it is incapable of recovering the motion of original input videos. For\nexample, the \u201ctrain\u201d and \u201cfish\u201d barely moved in the shown video samples, which demonstrates the effectiveness of our\nproposed conditioning strategy.\nB.2. Multimodal Understanding\nThis section presents qualitative results of Video-LaVIT for image and video understanding. First, Table 7 showcases our\nperformance in image question answering using the famous test example from GPT-4 (OpenAI, 2023a). As can be seen, our\nmodel produces a reasonable answer with a good number of correct details (e.g. the type of the vehicle being SUV) and\neven a friendly safety warning. In comparison, one of the recent multimodal LLMs, LLaVA (Liu et al., 2023b), produces a\nroughly correct answer with some inaccurate detail (mistaking the vehicle type as \u201cminivan or van\u201d).\nFor video question answering, Tables 8 and 9 compares our method to Video-LLaVA (Lin et al., 2023) and Video-\nChatGPT (Maaz et al., 2023) based on the video clips from Video-ChatGPT. In the first example of Table 8 which asks to\nexplain why a video is funny, our model yields the most concise answer among the video-language models compared, and at\nthe same time contains a salient point that the other models failed to mention. The next example in Table 8, on the other\nhand, shows that our method produces fewer hallucinations than Video-LLaVA and Video-ChatGPT, as the latter two models\ntend to generate overly detailed action descriptions that have no basis in the video. And lastly, in the example of Table 9, our\nmodel follows the instruction prompt by producing a beautiful fairy tale with both conciseness and a moral lesson (\u201clove\ncan conquer all\u201d). To summarize, our method demonstrates reasonably good multimodal understanding capabilities across\ndifferent test cases, in line with the previous quantitative comparison on multiple benchmarks.\nC. Limitations\nOur proposed model cannot directly process very long videos of more than 100s due to its limited context window (4096).\nWhile a possible solution is to sample fewer video keyframes, some nuanced motions may not be captured after resampling.\nTo compound this problem, the resolution of motion vectors could be a bottleneck for modeling very fine-grained motions,\nsuch as human actions. On the other hand, a general concern is that our training cost is still too high to scale to web-scale\nvideo data, which may require further optimization through joint exploitation of spatial and temporal redundancies in video.\n16\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nClose up headshot, futuristic young woman, wild hair sly\nA steaming cup of coffee with mountains in the\nsmile in front of gigantic UFO, dslr, sharp focus, dynamic\nbackground. Resting during road trip.\ncomposition.\nSDXL\nVideo-LaVIT\nA squirrel is inside a giant bright shiny crystal ball on the\nAn origami fox walking through the forest.\nsurface of blue ocean. There are few clouds in the sky.\nSDXL\nVideo-LaVIT\nA watercolor painting of two apples on a wooden table,\nA cat is sitting on a basket under a bench.\nneither is red and both are green.\nSDXL\nVideo-LaVIT\nFigure 6. Text-to-image generation comparison with SDXL (Podell et al., 2024). Prompts are from SDXL, CM3Leon (Aghajanyan et al.,\n2022), Imagen (Saharia et al., 2022), VideoPoet (Kondratyuk et al., 2023), LMD (Lian et al., 2023), and LayoutGPT (Feng et al., 2024).\nOur model provides comparable visual quality while showing better logical and spatial reasoning abilities (see the last two cases).\n17\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nFirst-person view running through the woods and approaching a large beautiful cabin, highly detailed.\nGen-2\nVideo-LaVIT\nFlying through an intense battle between pirate ships in a stormy ocean..\nGen-2\nVideo-LaVIT\nPOV footage of approaching a small cottage covered in moss and many flowers, tilt shift, arc shot.\nGen-2\nVideo-LaVIT\nFPV drone footage of an ancient city in autumn.\nGen-2\nVideo-LaVIT\nFigure 7. Text-to-video generation comparison with Gen-2 (Runaway, 2023) using default parameters. Prompts are from VideoPoet (Kon-\ndratyuk et al., 2023) and PixelDance (Zeng et al., 2023). Our model provides a similarly high visual quality (in the bottom two cases)\nwhile following the text prompt better (including \u201crunning\u201d in the first example and \u201cpirate ships\u201d in the second examples).\n18\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nSVD\nVideo-LaVIT\nSVD\nVideo-LaVIT\nSVD\nVideo-LaVIT\nFigure 8. Image-to-video generation comparison with SVD (Blattmann et al., 2023a) using the stable-video-diffusion-img2vid-xt version.\nThe generation is conditioned on the leftmost frame. Our model can produce more sophisticated animal motions (see the top two cases)\nwhile not violating the physical rules (e.g., in the second last row, the motorcycle is not moving in the direction of its tire).\n19\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nA dog in the sun.\n0\u201324 frames\n24\u201348 frames\nA jeep car is moving on the beach.\n0\u201324 frames\n24\u201348 frames\nFirst-person view running through the woods and approaching a large beautiful cabin, highly detailed.\n0\u201324 frames\n24\u201348 frames\nFigure 9. Long video generation examples. Prompts are Gen-L-Video (Wang et al., 2023a) and VideoPoet (Kondratyuk et al., 2023). Our\ngenerated videos are temporally coherent even across different decoded clips, thanks to our proposed explicit noise constraint.\n20\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nOriginal\nw/ EMC\nw/o EMC\nOriginal\nw/ EMC\nw/o EMC\nFigure 10. Ablation study of enhanced motion conditioning (EMC) for video reconstruction. The last row (i.e., w/o EMC) indicates\nonly motion vectors as the input condition for training 3D U-Net. As illustrated, incorporating EMC can improve the fidelity of the\nreconstructed videos. The video samples are taken from WebVid-10M (Bain et al., 2021).\n21\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 7. Image question answering comparison with LLaVA (Liu et al., 2023b), GPT-4 (OpenAI, 2023a), BLIP-2 (Li et al., 2023c), and\nOpenFlamingo (Awadalla et al., 2023). Our model answers correctly like the other multimodal LLMs, but without missing details such as\nthe vehicle type (it is an SUV instead of a van or minivan). The table format follows GPT-4.\nVisual input example, Extreme Ironing:\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\nUser\nWhat is unusual about this image?\nVideo-LaVIT\nThe unusual aspect of this image is that a man is ironing clothes on an ironing board placed\non the back of a yellow SUV while it is driving down the street. This is not a typical scenario,\nas ironing clothes is usually done indoors, in a home or a laundry room, and not while the\nvehicle is in motion. The man\u2019s actions pose a risk to himself and others on the road, as\nironing on a moving vehicle can lead to accidents or distractions for the driver.\n[Start a new conversation, and clear the history]\nUser\nWhat\u2019s happening in the scene?\nVideo-LaVIT\nThe image depicts a man ironing clothes on an ironing board placed on the back of a yellow\nSUV. The man is standing on the back of the vehicle, attentively ironing his clothes. The\nironing board is positioned in the middle of the SUV, with the man standing on it to reach\nthe clothes. In the background, there are two cars visible, one on the left side and another on\nthe right side of the scene. The man appears to be focused on his task, despite the unusual\nlocation for ironing clothes.\nUser\nWhat is unusual about this image?\nLLaVA (Liu et al., 2023b)\nThe unusual aspect of this image is a man ironing clothes on the back of a minivan or van.\nThis is not a typical place to perform this activity, as one would usually iron clothes in a\nmore stationary and safe location, such as a home, using a regular ironing board. The scene\ndepicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which\ncan be both unsafe and unconventional. Additionally, it is not clear how the man is able to\nmaintain balance and stability while ironing clothes in such an unstable environment.\n[Start a new conversation, and clear the history]\nUser\nWhat\u2019s happening in the scene?\nLLaVA (Liu et al., 2023b)\nThe image depicts a man standing on top of a yellow SUV in a busy city street. He is holding\na portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene,\nthere are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people\nin the vicinity, one close to the man on the car and another further away. The man on the\nladder appears to be attracting attention due to his unconventional choice of ironing his\nclothes on top of a moving car. The city street around him is bustling with activity, adding to\nthe unique nature of the scene.\nUser\nWhat is unusual about this image?\nGPT-4 (OpenAI, 2023a)\nThe unusual thing about this image is that a man is ironing clothes on an ironing board\nattached to the roof of a moving taxi.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023c)\na man is sitting on the back of a yellow cab\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nThe man is drying his clothes on the hood of his car.\n22\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 8. Video question answering comparison with Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video\nexamples are from Video-ChatGPT. In the first example, our model directly points to a salient explanation for why the video is funny.\nIn the second example, our model responses accurately without hallucinating about the video (the woman only swung the ax once).\nUser\nWhy is this video funny?\nVideo-LaVIT\nThe video is funny because the baby is wearing glasses and trying to read a book, but she is\nnot able to do so. The baby\u2019s attempts to read the book are adorable and amusing to watch.\nUser\nWhy is this video funny?\nVideo-LLaVA (Lin et al., 2023)\nThe video is funny because the baby is trying to read a book while wearing glasses, which is\na humorous sight. The baby\u2019s attempts to read the book are also amusing, as they seem to be\nstruggling with the task. Additionally, the baby\u2019s facial expressions and movements add to\nthe overall humor of the video.\nUser\nWhy is this video funny?\nVideo-ChatGPT (Maaz et al., 2023)\nThe video is funny because it shows a baby girl who is learning to read. She is sitting on\na bed and looking at a book, and she is trying to read it. The baby girl is wearing glasses,\nand she is holding the book with both hands. The baby girl is trying to read the book, and\nher actions are adorable and endearing. The video captures the innocence and curiosity of a\nyoung child as she learns to read, which is a joyful and amusing sight for the viewer.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-LaVIT\nThe woman uses an ax to split the logs. She picks up the ax and swings it down with force,\nsplitting the logs in half.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-LLaVA (Lin et al., 2023)\nThe woman uses an ax to split the logs. She holds the ax in her hand and swings it down\nonto the log. She repeats this process several times, splitting the log into smaller pieces.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-ChatGPT (Maaz et al., 2023)\nThe woman uses a large axe to split the logs. She starts by placing the axes on the log and\nthen swings it down with a powerful motion. The axe slices through the log with ease, and\nthe woman continues to swing it until the log is split in half.\n23\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 9. Video question answering comparison with Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video\nexamples are from Video-ChatGPT. As shown in this example, our model captures the underlying context of this video, which is a love\nstory. The fairy tale told, which implies that love will prevail, is naturally inspired by the video without introducing an unmentioned third\nparty (jealous sister or stepmother) as in the other models. The use of the phrase \u201cbeautiful glass slipper\u201d adds a nice detail to the story.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-LaVIT\nOnce upon a time, there was a beautiful princess who lived in a grand castle. She was known\nfor her kindness and compassion towards others. One day, a handsome prince visited her\ncastle and fell deeply in love with her. However, the princess was hesitant to accept his love\ndue to her fear of losing her independence and being trapped in a loveless marriage. The\nprince, determined to win her heart, gave her a beautiful glass slipper as a gift. The princess\ntried it on and danced with the prince, realizing that true love transcends all boundaries.\nFrom that day on, the princess and the prince lived happily ever after, proving that love can\nconquer all.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-LLaVA (Lin et al., 2023)\nOnce upon a time, in a kingdom far away, there lived a beautiful princess named Rose. She\nwas known for her kindness and beauty, and everyone loved her. One day, a handsome prince\nnamed Leo came to visit the kingdom. He was fascinated by Rose\u2019s beauty and decided\nto stay in the kingdom. However, Rose\u2019s jealous sister, Lily, did not like the attention Leo\nwas giving her sister. She tried to harm Rose, but Leo saved her and they fell in love. The\nkingdom celebrated their love and they lived happily ever after. The moral of the story is\nthat true love conquers all and that jealousy can lead to harmful consequences.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-ChatGPT (Maaz et al., 2023)\nSure, here\u2019s a short fairy tale with a moral lesson inspired by the video: Once upon a\ntime, there was a beautiful princess named Cinderella who lived in a castle with her cruel\nstepmother and two stepsisters. She was known for her kindness and her love for animals,\nespecially birds. One day, while she was out in the garden, she met a handsome prince who\nwas lost in the woods. He was in need of help, and Cinderella offered to guide him back\nto the castle. As they walked together, the prince noticed that Cinderella\u2019s stepsisters were\ncruel to her and her and stepmother was always mean to her. He asked her about her life and\nhow she managed to stay so kind and gentle despite the hardships she faced. Cinderella told\nhim that she found happiness in the simple things in life, such as the beauty of nature and the\nlove of animals. The prince was touched by her story and decided to help her in any way\nhe could. He introduced her to the king and queen, who were impressed by her kindness\nand grace. They arranged for her to attend a ball at the castle, where she met the prince once\nagain. They fell in love and got married, living happily ever after. The moral of the story is\nthat even in difficult situations, one should find happiness in the simple things in life and\nnever lose sight of their values and kindness.\n24\n"
  },
  {
    "title": "Rethinking Optimization and Architecture for Tiny Language Models",
    "link": "https://arxiv.org/pdf/2402.02791.pdf",
    "upvote": "12",
    "text": "Rethinking Optimization and Architecture for Tiny Language Models\nYehui Tang1 Fangcheng Liu1 Yunsheng Ni1 Yuchuan Tian1,2 Zheyuan Bai1 Yi-Qi Hu3 Sichao Liu3\nShangling Jui4 Kai Han*1 Yunhe Wang*1\n1Huawei Noah\u2019s Ark Lab.\n2Peking University.\n3Consumer Business Group, Huawei.\n4Huawei Kirin Solution.\n{yehui.tang, kai.han, yunhe.wang}@huawei.com\nAbstract\nThe power of large language models (LLMs) has\nbeen demonstrated through numerous data and\ncomputing resources. However, the application\nof language models on mobile devices is facing\nhuge challenge on the computation and memory\ncosts, that is, tiny language models with high per-\nformance are urgently required. Limited by the\nhighly complex training process, there are many\ndetails for optimizing language models that are\nseldom studied carefully. In this study, based\non a tiny language model with 1B parameters,\nwe carefully design a series of empirical study\nto analyze the effect of each component. Three\nperspectives are mainly discussed, i.e., neural ar-\nchitecture, parameter initialization, and optimiza-\ntion strategy. Several design formulas are em-\npirically proved especially effective for tiny lan-\nguage models, including tokenizer compression,\narchitecture tweaking, parameter inheritance and\nmultiple-round training. Then we train PanGu-\n\u03c0-1B Pro and PanGu-\u03c0-1.5B Pro on 1.6T mul-\ntilingual corpora, following the established for-\nmulas. Experimental results demonstrate the im-\nproved optimization and architecture yield a no-\ntable average improvement of 8.87 on bench-\nmark evaluation sets for PanGu-\u03c0-1B Pro. Be-\nsides, PanGu-\u03c0-1.5B Pro surpasses a range of\nSOTA models with larger model sizes, validating\nits superior performance. The code is available\nat https://github.com/YuchuanTian/\nRethinkTinyLM.\n1. Introduction\nLarge language models (LLMs), trained on extensive cor-\npora, have demonstrated impressive performance across di-\nverse natural language tasks. The release of ChatGPT, with\nits robust generalization capabilities, has captured global\n* Corresponding Author.\nPanGu-\ud835\udf45-1B\n+ Compact Tokenizer\n+ Architecture Tweak\n+ Parameter Inheritance\nPanGu-\ud835\udf45-1.5B Pro\nQwen-1.8B\n+ Multiple-Round Training\nPanGu-\ud835\udf0b-1B Pro\n42.41\n44.11\n46.53\n49.79\n51.28\n56.49\n55.04\nAverage Performance\n1.8\n1.5\n# Parameters (B)\n1.0\n1.0\n1.0\n1.0\n1.0\nFigure 1. PanGu-\u03c0 Pro with improved architecture and optimiza-\ntion methods. PanGu-\u03c0-1B (Wang et al., 2023) directly use the\ndeveloping strategies of LLMs while PanGu-\u03c0-1B Pro achieves an\naverage performance improvement of 8.87 with our methodology.\nIt is worth mentioning that PanGu-\u03c0-1.5B Pro outperforms Qwen-\n1.8B (Bai et al., 2023) with 16.67% fewer parameters.\nattention and holds the potential to revolutionize the interac-\ntion between humans and computers.\nIn addition to the GPT-series models (Radford et al., 2018;\nBrown et al., 2020; Achiam et al., 2023), various large lan-\nguage models have emerged. PaLM (Chowdhery et al.,\n2023) trains a model with an impressive 540B parameters\nacross 6144 TPU v4 chips. LLaMA (Touvron et al., 2023)\nreleases a series of foundational language models, ranging\nfrom 7B to 70B parameters. Both the model architecture\nand trained weights are open-source, fostering collaboration\nwithin the AI community. Most of the following large mod-\nels leverage similar architectures and training methodolo-\ngies. For instance, Baichuan teams (Yang et al., 2023) train\n7B and 13B parameter models on a 2.6T token dataset en-\ncompassing both Chinese and English corpora. Qwen (Bai\net al., 2023), Yi (Yi, 2023), and Skywork (Wei et al., 2023)\npursue similar paths, training models with 2.4T, 3T, and\n3.2T tokens, respectively. Primarily attributed to the increas-\ning accumulation of cleaned data, the performance of LLMs\nimproves rapidly,\nWhile numerous studies have successfully trained various\nhigh-performance language models (Ren et al., 2023; Zeng\net al., 2022), the methodologies employed in training such\nmodels remain insufficiently analyzed. On one hand, a sub-\nstantial body of work concentrates on collecting and clean-\n1\narXiv:2402.02791v2  [cs.CL]  6 Feb 2024\nRethinking Optimization and Architecture for Tiny Language Models\ning data, with less emphasis on researching effective training\nstrategies. On the other hand, the training of large models\ndemands an exceedingly high computational resource in-\nvestment, making it impractical to explore a wide range of\noptimization strategies. As a result, recent works often adopt\nsimilar training recipes when constructing LLMs (Touvron\net al., 2023; Yi, 2023; Bai et al., 2023; Wei et al., 2023).\nMoreover, the implementation of these large models de-\nmands prohibitively high memory and computational re-\nsources, constraining their practical applicability in various\nscenarios. For example, the GPT-3 with 175B parameters\nnecessitates approximately 700GB of memory when stored\nwith FP32 datatype. Although the 7B parameter models are\nrelatively more efficient, their resource requirements still\nrender them impractical for deployment on edge devices,\nsuch as mobile phones.\nIn this paper, we systematically rethink the methodology for\nconstructing a tiny language model, including neural archi-\ntecture, parameter initialization, and optimization strategy:\n\u2022 Neural architecture: Adopting the tokenizer directly\nfrom larger models introduces redundant parameters,\nresulting in increased computational overhead. Stream-\nlining the tokenizer by removing low-frequency vo-\ncabularies enhances the model\u2019s representational effi-\nciency. Moreover, we observe that the configuration\nof the model\u2019s architecture (depth, width, and expand-\ning rate in FFN) has a significant impact on the final\nperformance. Depth is the primary factor for tiny lan-\nguage models, and deeper models usually achieve high\nperformance at the expense of lower inference speed.\n\u2022 Parameter initialization: Inheriting parameters from\nthe large model proves effective in boosting perfor-\nmance and expediting convergence. The identification\nof crucial parameters is imperative in this context. We\nhave observed that layers situated near the beginning\nand end of the model often carry more significance\nthan the intermediate layers. Furthermore, within each\nlayer, the adoption of data-driven learnable criteria has\ndemonstrated greater efficacy compared to heuristic\nmethods.\n\u2022 Model optimization: In comparison to larger models,\ntiny models face more severe data forgetting issues, and\nmultiple-round training proves beneficial for memory\nenhancement. We propose a straightforward sample se-\nlection strategy to mitigate the training cost associated\nwith multiple-round training. Besides, we also delve\ninto the relationship between batch size and learning\nrate specifically for tiny models.\nDrawing from the aforementioned insights, we develop\nPanGu-\u03c0-1B Pro and PanGu-\u03c0-1.5B Pro with enhanced ar-\nchitecture and optimization methods. From the developing\nstrategies of LLMs, we gradually add four core components\nto improve performance (see Figure 1). The models are\nevaluated on various benchmarks including examination,\nknowledge, reasoning, and understanding, where our mod-\nels achieve SOTA performance when compared with models\nof similar sizes. For instance, with 16.67% fewer parame-\nters, PanGu-\u03c0-1.5B Pro achieves an average score of 56.49,\noutperforming Qwen-1.8B which achieves a score of 55.04.\n2. Neural Architecture\nIn this section, we investigate the architecture design of tiny\nlanguage models. The experiments are conducted on 50B\ntokens randomly sampled from the pre-trained dataset, with\nequal proportions of Chinese and English corpus. The base-\nline is a 1B parameter model with LLaMA-like architecture\nunless specified. The models constructed with different\nstrategies are compared on ARC Easy (Clark et al., 2018),\nHellaSwag (Zellers et al., 2019) and C3 (Sun et al., 2020).\n2.1. Compact Tokenizer\nThe tokenizer serves to map original natural language into\ntokens suitable for processing by large language models,\nwith each token representing a word, subword, character, or\nsymbol. A multilingual tokenizer typically has a large vo-\ncabulary to cover various corpora. However, in the context\nof a tiny language model, an overly large vocabulary can sig-\nnificantly occupy a substantial portion of the model\u2019s param-\neters. For instance, Qwen-7B (Bai et al., 2023), Baichuan2-\n7B (Yang et al., 2023), and PanGu-\u03c0-7B (Wang et al., 2023)\nhave vocabulary sizes of 151936, 125696, 100883, respec-\ntively. The parameters of their heads and embedding layers\naccount for 16.12% ,13.72%, 10.91% of the overall param-\neters. While the PanGu-\u03c0-1B model with 12 layers and\na width of 2048, using the same tokenizer, sees the head\nand embedding layers\u2019 parameters comprising a substantial\n36.8% of the total (Figure 3). This distribution leads to a\nsignificant allocation of parameters to vocabulary represen-\ntation rather than the main body, potentially limiting the\nmodel\u2019s overall representation capacity. Therefore, com-\npressing the tokenizer becomes essential for a tiny language\nmodel to reduce its parameter proportion.\nActually, we discover that substantial redundancy exists in\nthe tokenizer. By initializing the tokenizers with the 100k\nvocabularies inherited from the PanGu-\u03c0 model, we con-\nducted a frequency analysis across a vast corpus comprising\napproximately 1.6T tokens. As depicted in Figure 2, it is\nevident that tokens exhibit a long-tail effect, where the top\n48k vocabularies accounting for 97.86% of all the training\ncorpus. We conduct experiments with six vocabulary sizes\n{8k, 16k, 32k, 48k, 72k, 100k}, which account for 78.68%,\n87.24%, 94.49%, 97.86%, 99.84% and 100% accumulated\n2\nRethinking Optimization and Architecture for Tiny Language Models\n8k\n16k\n24k\n32k\n40k\n48k\n56k\n64k\n72k\n80k\n88k\n96k\nSorted Top-k Vocabulary\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nAccumulated Frequency\nFigure 2. Accumulative frequency of the top-k vocabularies, where\n97.86% data can be represented by a small 48k tokenizer.\n61.81%\n38.19%\n81.93%\n18.07%\nParameters of \nmain body\nParameters of \nembedding & head\n(a)\n(b)\nFigure 3. The parameter proportions of model\u2019s main body and\ntokenizer. (a) The large tokenizer inherited from large multilingual\nmodels (Wang et al., 2023). (b) Compact tokenizer by removing\nlow-frequency vocabularies.\nfrequency respectively. Over 50% vocabularies may be\nredundant as they cater to less than 3% of the corpus.\nWe advocate for the removal of low-frequency vocabularies\nto reduce their parameters. Table 1 illustrates the perfor-\nmance variations concerning tokenizer size1. The embed-\nding and head layers constitute 18.07% of the 1B model\u2019s\nparameters when using a vocabulary of 48k, showcasing\nthe best average performance followed by the model with a\nvocabulary of 32k. It is noteworthy that employing an ex-\ncessively small vocabulary can result in performance degra-\ndation. For instance, with an 8k tokenizer covering less than\n70% of the corpus, the model exhibits subpar performance\non C3 and ARC-E datasets. The tokenizer with a size of\n48k also exhibits a similar compression rate to that of the\noriginal 100k size tokenizer, which is evaluated across the\nentire training corpus. Therefore, we recommend using a\ncompact tokenizer covering over 90% of the corpus, while\nensuring the parameter proportion of embedding and head\nlayers remains below 20%.\n2.2. Architecture Tweak\nIn this part, we focus on the neural architecture design of\nLLM for edge devices by exploring the impact of depth,\nwidth and the expanding rate of Feed-Forward Networks\n(FFN) on the performance of a 1B-size language model.\nBesides accuracy on downstream tasks, decoding speed is\n1All model\u2019s sizes are controlled to 1B by adjusting depth.\nTable 1. Performance varies w.r.t. tokenizer size. PEHF stands\nfor the proportion of embedding and head layers over the whole\nmodel.\nTokenizer\nPEHL (%)\nARC-E\nHellaSwag\nC3\nAvg.\n8k\n2.97\n31.39\n40.19\n42.25\n37.94\n16k\n6.01\n30.34\n40.10\n45.64\n38.69\n32k\n11.79\n34.45\n40.23\n46.77\n40.48\n48k\n18.07\n34.39\n41.48\n47.70\n41.19\n72k\n26.88\n34.39\n39.21\n46.58\n40.06\n100k\n38.19\n34.98\n39.11\n47.10\n40.40\nTable 2. Varying the depth and width of a 1B-size model with fixed\nvocabulary size and expanding rate. The speed is measured by\ntokens per second.\nDepth\nWidth\nSpeed\nARC-E\nHellaSwag\nC3\nAvg.\n40\n1280\n12.81\n37.01\n41.00\n48.05\n42.02\n30\n1536\n17.71\n36.16\n40.32\n47.84\n41.44\n20\n1792\n29.49\n34.39\n41.48\n47.70\n41.19\n15\n2048\n36.79\n32.45\n40.22\n40.05\n37.57\n9\n2560\n57.53\n32.63\n31.06\n42.68\n35.46\nanother important aspect for tiny language models. We test\nthe end-to-end inference speed (tokens per second) when\ngenerating 510 new tokens under two prefix tokens using a\nrandomly initialized model. The speed is tested on a single\nNVIDIA V100 GPU with batch size 20 using FP16. We fix\nthe vocabulary size to 48k as suggested in Section 2.1. By\nconstraining the model\u2019s size to 1B parameters, we explore\nthe effects of varying the model\u2019s depth, width, and expan-\nsion rate individually. Firstly, we investigate the impact of\nadjusting two among the three components, while maintain-\ning the third variable at a constant level, on the model\u2019s\noverall performance.\nThe impact of the depth and width. We thoroughly inves-\ntigate representative configurations as outlined in Table 2,\nwhere we can conclude that deeper tiny language models\nexhibit better performance, however, at the cost of inference\nspeed. As the depth of the model increases, the performance\nincreases for almost all the three benchmarks. Meanwhile,\nwe observed that when the depth is already 20, the perfor-\nmance improvement (41.19 \u2192 42.02) by designing deeper\narchitectures is minimal compared to the decrease of the in-\nference speed (29.49 \u2192 12.81). Therefore, we recommend\nsetting the number of layers to around 20 for 1B-parameter\nmodel with a 48k tokenizer.\nAs shown in Table 3, we observe close inference speed\nfor different expanding rates when the depth is fixed. It\u2019s\nobviously that the 1:1 setting gets significantly worse per-\nformance. To further investigate the interplay among depth,\nwidth and expansion rate, we sample about 30 different pa-\nrameter configurations while maintaining the model size at\n1B parameters and conduct training on a further streamlined\ndataset comprising 5B tokens. As illustrated in Figure 4,\nthe correlation between the depth (width) and the down-\nstream task\u2019s average performance is notably higher, with a\n3\nRethinking Optimization and Architecture for Tiny Language Models\n10\n15\n20\n25\n30\n35\n40\n33.2\n33.4\n33.6\n33.8\n34.0\n34.2\n34.4\n34.6\n34.8\nAccuracy/%\nspearmanr = 0.528\n(a) Depth\n1250\n1500\n1750\n2000\n2250\n2500\n2750\n33.2\n33.4\n33.6\n33.8\n34.0\n34.2\n34.4\n34.6\n34.8\nAccuracy/%\nspearmanr = -0.516\n(b) Width\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n33.2\n33.4\n33.6\n33.8\n34.0\n34.2\n34.4\n34.6\n34.8\nAccuracy/%\nspearmanr = 0.090\n(c) Expansion rate\nFigure 4. Performance varies w.r.t. model\u2019s width, depth and expansion rate. The experiments are conducted on a streamlined dataset\ncomprising 5B tokens. The accuracy is averaged among ARC Easy, HellaSwag and C3. Spearman coefficient is used to measure the\ncorrelation between performance and model\u2019s configure.\nTable 3. Varying the expanding rate and width of a 1B-size model\nwith fixed vocabulary size and depth.\nEP Rate\nWidth\nSpeed\nARC-E\nHellaSwag\nC3\nAvg.\n1.00\n2304\n28.39\n31.75\n38.71\n42.68\n37.71\n2.00\n2048\n28.68\n33.33\n41.34\n48.55\n41.07\n2.77\n1792\n28.40\n34.39\n41.48\n47.70\n41.19\n4.00\n1536\n28.53\n35.27\n39.36\n47.18\n40.60\nTable 4. Performance under different random initialization strate-\ngies, where the constant standard deviation method performs best.\nInitialization Method\nARC-E\nHellaSwag\nC3\nAvg.\nConstant\n37.57\n41.16\n49.04\n42.59\nGPT2 (Radford et al., 2019)\n38.62\n39.34\n48.44\n42.13\nInternLM (Team, 2023)\n34.39\n41.48\n47.70\n41.19\nSpearmanr correlation coefficient reaching up to 0.528. In\ncontrast, there is no apparent linear relationship between the\nexpansion rate and the model\u2019s ultimate performance.\nDiscussion.\nA compact tokenizer holds particular signif-\nicance for a tiny language model, as it strikes a crucial\nbalance between representation ability and implementation\ncost. The removal of low-frequency vocabularies enables\nthe efficient elimination of substantial redundancy without\nsignificantly compromising representation capacity. Addi-\ntionally, the architecture\u2019s configurations, such as width,\ndepth, and expanding rate, exert a considerable influence on\nthe final performance of a tiny model. Among them, depth\nis the primary factor for tiny language models, and deeper\nmodels usually achieve high performance at the expense of\nlower speed. Following the above observations, we design\nthe architecture of PanGu-\u03c0 Pro as detailed in Table 9.\n3. Parameter Initialization\nIn this section, we investigate how to initialize model\u2019s pa-\nrameters with a given neural architecture, including random\ninitialization and inheriting parameters from a large model.\n3.1. Random Initialization\nWhen training model from scratch, the parameters are usu-\nally initialized with random numbers obeying normal dis-\ntribution N(0, \u03c32) with zero mean and standard deviation\n\u03c3. A series of well-known large language models carefully\ndesign the value of \u03c3, especially changing it w.r.t.layers.\nFor example, GPT2 (Radford et al., 2019) applies a scale of\n1/\n\u221a\nN to all linear layer parameters, where N is the number\nof residual layers. InternLM (Team, 2023) only applies the\nsame scale to some special linear layer parameters, namely\nthe out projection in MHA layers and the gate projection\nin MLP layers. We investigate these different initialization\nstrategies for training tiny language model, whose results\nare shown in Table 4. We note that different strategies re-\nsult in similar results. For simplicity and generalization,\nwe recommend using a constant value for all layers when\ntraining tiny language models. More analyses are presented\nin Appendix B.\n3.2. Parameter Inheritance\nBesides random initialization, the initial parameter of this\ntiny language model can also inherit from a large language\nmodel. The strong generalization ability of large model\nis expected to transfer to the tiny model. Compared the\ntiny model, the large model usually has more layers with\nmore neurons. We firstly select important layers and then\nrecognize critical neurons in the selected layers.\nImportant layers selection.\nConsidering that the tiny\nmodel usually has fewer layers than the large language\nmodel, the most important layers that contribute to the fi-\nnal performance are required to recognize. Consequently,\nwe conduct ablation experiments to assess the impact of\nindividual layers on the overall performance.\nTo uncover general principles regarding layer impor-\ntance, we conduct a variety of experiments on multiple\nwidely-used large language models, including LLaMA2-7B,\nLLaMA2-13B, InternLM-7B and PanGu-\u03c0-7B. During the\ninference phase, we skip specific layers and assess the re-\nsulting performance drop. Three types of layer skipping\nexperiments are conducted for each model, involving skip-\nping one layer, two neighboring layers, and three contiguous\nlayers. The outcomes are depicted in Figure 5, where we\n4\nRethinking Optimization and Architecture for Tiny Language Models\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nPanGu- -7B 1 Skip\nPanGu- -7B 2 Skip\nPanGu- -7B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nInternLM-7B 1 Skip\nInternLM-7B 2 Skip\nInternLM-7B 3 Skip\n0\n5\n10\n15\n20\n25\n30\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-7B 1 Skip\nLLaMA2-7B 2 Skip\nLLaMA2-7B 3 Skip\n0\n10\n20\n30\n40\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-13B 1 Skip\nLLaMA2-13B 2 Skip\nLLaMA2-13B 3 Skip\nFigure 5. Performance of large language models when skipping a few layers. \u201cx Skip\u201d denotes adjacent x layers are discarded. Redundan-\ncies are observed within intermediate layers while the layers situated near the beginning and end are crucial for maintaining performance.\n0\n10\n20\n30\n40\n50\nTokens (B)\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\nLoss\nBase\nL1\nL2\nTaylor\nLearned\nFigure 6. Training loss with different pruning strategies. \u201cBase\u201d\ndenotes training from scratch without inheritance. Inheriting the\nmodel parameters with pruning yields a lower loss.\nanalyze the average performance of large language models\non three downstream tasks, i.e., ARC-E, HellaSwag, and\nC3. The x-axis represents the skipped layer index, while\nthe y-axis signifies performance accuracy.\nSome interesting common phenomenons are identified in\nthese models. The shallow layers, especially the initial two\nto three layers, play a pivotal role in extracting features from\ninput sequences. Removing these layers will incur signif-\nicant performance drop on downstream tasks. Similarly,\ndeep layers is also crucial, and removing them results in a\ndeterioration of performance. Conversely, when the interme-\ndiate layers are removed, the performance is less affected,\nindicating that redundancy occurs within these layers. These\nlayers are tend to be removed when inheriting parameters.\nIntra-layer parameters selection.\nWithin a layer, impor-\ntant parameters can be recognized by various metrics. How\nto recognizing essential parameters has well been discov-\nered in the model pruning area (Frantar & Alistarh, 2023;\nMa et al., 2023). The importance of neurons can be mea-\nsured by various criteria and the most significant neurons\nare used as the initialization of tiny models. Weight norms,\nsuch as \u21131 and \u21132-norm, are commonly employed to measure\nimportance, indicating that larger weights encapsulate more\ncrucial information (Han et al., 2015; Guo et al., 2016; Lee\nTable 5. Comparison between different parameter inheritance\nstrategies. \u201cBase\u201d denotes training without inheritance.\nInheritance Strategy\nARC-E\nHellaSwag\nC3\nAvg.\nBase\n36.68\n40.34\n49.15\n42.06\nL1 (Ma et al., 2023)\n39.51\n47.70\n50.96\n46.06\nL2 (Ma et al., 2023)\n41.98\n48.33\n50.68\n47.00\nTaylor (Ma et al., 2023)\n43.21\n48.43\n52.05\n47.90\nLearned (Xia et al., 2023)\n40.74\n51.77\n51.73\n48.08\net al., 2021). The first-order Taylor expansion (Lee et al.,\n2019; Tanaka et al., 2020), which incorporates both weight\nvalues and gradients, is regarded as a more accurate estima-\ntion of the output. In addition to empirical criteria, essential\nweights can also be identified through binary masks, which\nare automatically learned during the training process (Xia\net al., 2023; Tang et al., 2020). In the subsequent sections,\nwe adopt these methodologies to select vital parameters\nfrom the PanGu-\u03c0-7B (Wang et al., 2023) model as initial\nvalues for a 1B model with smaller weight dimensions.\nTraining loss curves and evaluation results are presented in\nFigure 6 and Table 5. In comparison to the baseline model\ninitialized randomly, each of the small models initialized\nwith the pruning strategy converges to a lower loss. Among\nthe empirical criteria, the Taylor expansion yields superior\nresults, primarily attributed to its accurate estimation of neu-\nron importance. The model pruned using learnable masks\nstarts with a significantly lower initial loss than the other\nmodels and ultimately converging to the lowest loss. Evalu-\nation results across the three datasets validate the effective-\nness of parameter inheritance. We recommend inheriting\nmodel parameters with the learnable strategies.\nDiscussion.\nThe aforementioned observation confirms\nthat the initial parameters of a model exert a substantial\ninfluence on both the convergence rate and ultimate per-\nformance of a tiny language model. Opting to inherit pa-\nrameters from a larger model is generally a more favorable\nchoice, as it allows the smaller model to assimilate the ro-\nbust representation abilities of its larger models. The process\nof selecting significant parameters is a crucial step in this\n5\nRethinking Optimization and Architecture for Tiny Language Models\n0\n10\n20\n30\n40\n50\nTokens (B)\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nLoss\nBs=1M\nBs=2M\nBs=4M\nBs=8M\nBs=16M\nBs=32M\n(a) r = 0.5\n0\n10\n20\n30\n40\n50\nTokens (B)\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nLoss\nBs=1M\nBs=2M\nBs=4M\nBs=8M\nBs=16M\nBs=32M\n(b) r = 1.0\nFigure 7. Training losses under different batchsize & learning rate.\nregard. Through thorough empirical investigation, we have\nobserved that intermediate layers tend to exhibit more re-\ndundancy, and data-driven learnable masks proves effective\nin excavating these redundant parameters.\n4. Model Optimization\nIn this section, we investigate the optimization strategies\nwith given neural architecture and initial parameters. Firstly,\nthe scaling rule between learning rate and batch size is\nanalyzed. Besides, we observe substantial performance\nimprovement from continuing multiple rounds of training.\n4.1. Batchsize & Learning Rate\nIn practical language model training, the choice of batch\nsize is frequently tailored to the computational resources at\nhand. When dealing with a limited number of GPUs, opting\nfor a smaller batch size becomes necessary. Conversely,\nin scenarios where a substantial number of GPUs is at our\ndisposal, enlarging the batch size can effectively diminish\nthe number of iterations, thereby expediting the overall\ntraining process.\nHowever, adjusting the batch size typically has a notable\nimpact on the final performance of the model.\nWhen\nincreasing the batch size, it is common for the learning\nrate to be adjusted proportionally. We explore their com-\nbined effects in Figure 7 and Figure 8, using the formula\nlr = (bs/bs0)r \u00d7 lr0, where the default batchsize bs0 and\nlearning rate lr0 are set to 1M and 1 \u00d7 10\u22124, respectively.\nr denotes the increment rate, which is usually set as 0.5\nor 1.0 (Krizhevsky, 2014; Goyal et al., 2017). When the\nbatchsize is smaller than 4M, the convergence speeds with\ndifferent learning rates remain consistent. When the batch-\nsize further increases, a moderate increment rate (r = 0.5)\nis preferable. With the same training tokens, employing an\nexcessively large batchsize (\u226516M) adversely affects the\nconvergence speed. In the majority of cases, a batch size\nsmaller than 4M is considered the safe range for optimiz-\ning model performance. Otherwise, optimization strategies\nneed to be specifically tailored for large batch sizes (Keskar\net al., 2016; You et al., 2017; 2019).\n1\n2\n4\n8\n16\n32\nBatchsize (M)\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\nAccuracy/%\nr = 0.0\nr = 0.5\nr = 1.0\nr = 1.5\nFigure 8. Performance under different batchsize & learning rate.\n0\n1\n2\n3\n4\n5\n6\n7\nParts\n2.22\n2.23\n2.24\n2.25\n2.26\n2.27\nLoss\nFigure 9. Loss value varies w.r.t. data on different iterations using\na pretrained PanGu-\u03c0-1B model. The loss is averaged among\nbatches in each part.\n4.2. Multiple-Round Training\nThe existing methods usually train the language model with\nonly one round, i.e., all the data are only used for one time\nto update the model, leaving the model\u2019s parameters uncon-\nverged. Besides, learning on large corpora may suffer from\nthe catastrophic forgetting (Toneva et al., 2018; Winata et al.,\n2023) issue, i.e., the model performance drops for data seen\nbefore. For tiny models, the limited model capacity makes\nthe forgetting problem more serious. Continuing training\nthe model can further reduce the training loss.\nWe conduct a simple experiment to validate the forgetting\nproblem. As the training loss is calculated by the model\nparameters at the corresponding timestamp and the model\nparameters are updated as the training continues, the later\ndata tend to have low loss values. Therefore, We recompute\nthe batch-wise loss on the previous data using a PanGu-\n\u03c0-1B model trained on 1.6T tokens. The training data is\nevenly and randomly divided into eight parts before training.\nFigure 9 shows how loss value varies w.r.t. data on each\npart. The high loss indicate previous knowledge have been\nseriously forgot. Therefore, it is necessary to train the model\nfor multiple rounds to fit the forgotten data.\nTo reduce the training cost, we propose a simple data refin-\ning strategy for the multiple-round training. Considering\nsome examples are hard to fit, they should be used for further\ntraining with a high probability. Denoting the loss values\nin certain part as L = {l1, l2, \u00b7 \u00b7 \u00b7 , lN}, where N is the total\nbatches in this part. Note that data are randomly shuffled in\nthe training process, and thus each batch contains various\ntype data. In each part, the loss values are normalized, denot-\n6\nRethinking Optimization and Architecture for Tiny Language Models\nTable 6. Sampling rate for the next round training. The model is\ntraining with two rounds. r = 0 denotes training with one round.\nSampling Rate r\nARC-E\nHellaSwag\nC3\nAverage\n0%\n42.68\n57.95\n54.19\n51.61\n25%\n43.95\n59.55\n56.01\n53.17\n50%\n45.33\n60.67\n57.37\n54.46\n75%\n45.52\n60.34\n58.16\n54.67\n100%\n44.98\n60.88\n58.74\n54.87\nTable 7. The impact of number of training rounds. The sampling\nrate r is set to 50%.\nTraining round\nARC-E\nHellaSwag\nC3\nAverage\nSingle round\n42.68\n57.95\n54.19\n51.61\nTwo round\n45.33\n60.67\n57.37\n54.46\nThree round\n45.11\n61.32\n56.88\n54.44\ning the sampling probability, i.e., pi =\nexp(li)\nPN\nj=1 exp(lj). In the\nnext round training we sample N0 batches out of N accord-\ning to the sampling probability p. The impact of sampling\nrate (r = N0\nN ) is shown in Table 6. It shows that a higher\nsampling rate tends to achieve high performance. The per-\nformance improvement is marginal when the sampling rate\nr exceeds 50%. We plot how the evaluation metric on Hel-\nlaSwag evolves during training in Figure 10. As the second-\nround training goes ahead, the accuracy on HellaSwag keeps\nrising but get converged in the later phase. In Table 7, we\nalso try to train the models with more rounds. However, the\nperformance also saturate gradually. To achieve a balance\nbetween performance and training efficiency, we recom-\nmend to train the model with two rounds and set sampling\nrate to 50%.\nDiscussion.\nIn contrast to larger models, tiny language\nmodels face a significant challenge of data forgetting due to\ntheir limited capacity. As a result, adopting a multi-round\ntraining approach becomes crucial to enhance performance.\nEmploying data sampling proves effective in improving\nlearning on challenging examples while simultaneously re-\nducing training costs. Additionally, the choice of batch\nsize and learning rate plays a pivotal role in model perfor-\nmance. For optimal results, it is advisable to use a batch size\nsmaller than 4M, unless a specialized large batch optimizer\nis employed for a tailored approach.\n5. PanGu-\u03c0 Pro\nBased on the above extensive and rigorous set of experi-\nments, we make a significant improvement on our previous\nPanGu-\u03c0-1B and meanwhile construct a larger and more\npowerful PanGu-\u03c0-1.5B Pro. In this section, we make a\ncomparison with the existing open-source tiny language\nmodels, where our result establishes a new SOTA. Specifi-\ncally, PanGu-\u03c0-1.5B Pro outperforms the recent proposed\n0\n200\n400\n600\n800\n1000 1200 1400 1600\nTokens (B)\n40\n45\n50\n55\n60\nAccuracy/%\nPanGu- -1B Pro (Round 2)\nPanGu- -1B Pro\nPanGu- -1B\nFigure 10. Accuracies of PanGu-\u03c0-1B and PanGu-\u03c0-1B Pro on\nHellaSwag during training.\nQwen-1.8B (Bai et al., 2023) and Phi2-2.7B (Li et al.,\n2023b), which is 1.8x larger, in average performance.\nImplementation details.\nThe pre-training data, which\nconsists of 1.6T tokens, is gathered from diverse sources\nfrom the Internet, covering English and Chinese corpus\nwith around 1 : 1 scale. The used 48k tokenizer is built by\nbyte-pair encoding (BPE, Shibata et al. (1999)) from Sen-\ntencePiece (Kudo & Richardson, 2018) upon our data. Our\nmodels are trained using the AdamW optimizer (Loshchilov\n& Hutter, 2017) with \u03b21 = 0.9, \u03b22 = 0.95 utilizing the co-\nsine learning rate decay (Loshchilov & Hutter, 2016) with\nan initial learning rate 2 \u00d7 10\u22124. The total batch size for the\ntraining process is 2M. We follow the PanGu-\u03c0 (Wang et al.,\n2023) architecture while making PanGu-\u03c0-1B much deeper.\nThe detailed configuration can be found in Table 9. We\nset the expansion rate to 2.77 as suggested in Table 3. For\nthe parameters initialization method, we inherit parameters\nfrom PanGu-\u03c0-7B via learnable binary mask after removing\nintermediate redundant layers. We use the Huawei Ascend\n910 card to train and evaluate the proposed PanGu-\u03c0 Pro.\nBenchmarks.\nWe use OpenCompass (Contributors, 2023)\nto evaluate on an extensive suite of downstream tasks, cov-\nering examination, knowledg, reasoning, and understanding\nabilities for a comprehensive comparison. C-Eval (Huang\net al., 2023) is a Chinese benchmark to evaluate the knowl-\nedge and reasoning abilities. CMMLU (Li et al., 2023a)\ncovers 67 topics including science, engineering, and hu-\nmanities. MMLU (Hendrycks et al., 2021) proposes an\nEnglish benchmark for measuring LLM\u2019s multitask accu-\nracy by covering 57 tasks including mathematics, history,\ncomputer science, and law. AGI-Eval (Zhong et al., 2023)\nis a benchmark specifically designed to evaluate the general\nabilities in tasks pertinent to human cognition and problem-\nsolving. BoolQ (Clark et al., 2019) is a reading comprehen-\nsion dataset to evaluate the difficult entailment-like infer-\nence ability of LLMs. AX-b (Wang et al., 2020) is a broad-\ncoverage diagnostic task and PIQA (Bisk et al., 2020) is a\nphysical interaction question-answering task. EPRSTM (Xu\n7\nRethinking Optimization and Architecture for Tiny Language Models\nTable 8. Comparison with SOTA open-source tiny language models. The best model is listed in bold and second-best is listed in underlined.\nExamination\nKnowledge\nReasoning\nUnderstanding\nAverage\nModels\nC-Eval\nCMMLU\nMMLU\nAGI-Eval\nBoolQ\nAX-b\nPIQA\nEPRSTMT\nXSum\nC3\nMobileLLaMA-1.4B\n23.93\n25.10\n25.05\n18.53\n58.75\n45.20\n71.27\n46.25\n18.19\n37.42\n36.97\nSheared-LLaMA-1.3B\n24.28\n25.10\n25.77\n18.01\n62.39\n43.57\n72.91\n46.25\n16.44\n35.45\n37.02\nTinyLLaMA-1.1B\n27.85\n24.64\n25.75\n18.54\n56.06\n45.47\n70.62\n46.25\n20.15\n36.71\n37.20\nMobileLLaMA-2.7B\n23.53\n25.55\n26.63\n18.43\n54.74\n55.80\n72.85\n46.25\n16.96\n36.11\n37.69\nChinese-LLaMA2-1.3B\n28.70\n24.78\n24.55\n19.40\n56.79\n47.46\n56.91\n72.50\n8.90\n43.12\n38.31\nRWKV-5-1.5B\n25.92\n25.14\n25.66\n19.01\n62.29\n54.05\n71.22\n46.25\n20.67\n49.15\n39.94\nPhi-1.3B\n27.78\n25.85\n44.32\n23.42\n73.52\n44.20\n76.99\n50.00\n14.90\n38.96\n41.99\nPanGu-\u03c0-1B\n36.85\n35.90\n35.96\n30.77\n58.44\n43.48\n61.92\n55.62\n15.92\n49.21\n42.41\nOpen-LLaMA-3B\n27.50\n25.42\n27.09\n20.68\n60.58\n52.72\n77.09\n82.50\n19.75\n43.23\n43.66\nPhi2-2.7B\n31.86\n32.18\n58.49\n28.51\n77.40\n43.57\n78.89\n46.25\n13.66\n40.11\n45.09\nPanGu-\u03c0-1B Pro (Ours)\n46.50\n46.56\n50.38\n41.58\n63.43\n53.99\n64.96\n74.38\n18.40\n52.66\n51.28\nQwen-1.8B\n53.60\n52.12\n46.43\n35.83\n64.31\n57.79\n73.83\n88.12\n20.03\n58.30\n55.04\nPanGu-\u03c0-1.5B Pro (Ours)\n52.91\n49.51\n53.76\n44.42\n63.73\n55.93\n73.94\n89.38\n22.23\n59.56\n56.49\nTable 9. Model configuration.\nModel\nWidth\nDepth\nVocabulary\nInitialization\nPanGu-\u03c0-1B\n2048\n12\n100883\nRandom\nPanGu-\u03c0-1B Pro\n1792\n21\n48000\nPanGu-\u03c0-7B\nPanGu-\u03c0-1.5B Pro\n2048\n22\n48000\nPanGu-\u03c0-7B\net al., 2021) is a binary sentiment analysis dataset based on\nproduct reviews. XSum (Narayan et al., 2018) is a summa-\nrization task collected from the British Broadcasting Corpo-\nration and C3 (Sun et al., 2020) contains 13,369 documents\nand their associated 19,577 multiple-choice questions.\nComparison with tiny language models.\nWe collect mul-\ntiple tiny language models with different sizes, ranging from\n1B to 3B. These include TinyLLaMA-1.1B (Peiyuan Zhang\n& Lu, 2023), Chinese-LLaMA2-1.3B (Cui et al., 2023),\nSheared-LLaMA-1.3B (Xia et al., 2023), and Open-LLaMA-\n3B (Geng & Liu, 2023). Meituan (Chu et al., 2023) released\nMobileLLaMA-1.4B and MobileLLaMA-2.7B that were\ntrained from scratch on the RedPajama dataset (Computer,\n2023). Microsoft developed the series of Phi (Li et al.,\n2023b) that focusing on using \u201ctextbook-quality\u201d data with\nsmall language models. RWKV-5-1.5B (Peng et al., 2023)\nis a parallelizable RNN with Transformer-level LLM Per-\nformance. Qwen-1.8B (Bai et al., 2023) is pretrained on 2.2\ntrillion tokens including web texts, books, codes, etc.\nExtensive experiments in Table 8 show that PanGu-\u03c0-1.5B\nPro significantly outperforms existing LLMs of similar or\neven larger sizes, e.g., Phi2-2.7B and Open-LLaMA-3B. We\nobserve a notable improvement of 8.77 on average perfor-\nmance from PanGu-\u03c0-1B to PanGu-\u03c0-1B Pro. With 16.67%\nfewer parameters, PanGu-\u03c0-1.5B Pro outperforms Qwen-\n1.8B (Bai et al., 2023) and exhibits the best or second-best\nperformance in the vast majority of the benchmarks. Overall,\nour model exhibits consistently better average performance\ncompared to the current state-of-the-art models.\nFrom PanGu-\u03c0-1B, we gradually add the core components\nto validate the effectiveness of our methodology. As shown\nin Figure 1, the removal of low-frequency vocabularies leads\nto an improvement of average performance from 42.41 to\n44.11 while the architecture tweak contributes another 2.42\nimprovement. Parameter inheritance, the most effective ap-\nproach, pushes the average performance to 49.79. Multiple-\nround training further enhances the average performance of\nPanGu-\u03c0-1B Pro.\n6. Conclusion and Discussion\nIn this paper, we systematically discuss how to construct\na tiny language model from three perspectives, i.e., neu-\nral architecture, parameter initialization, and optimization\nstrategies. By carefully designed empirical study, we rec-\nognized several effective design formulas to improve per-\nformance with given parameter restriction and data size,\nincluding compact tokenizer, architecture tweak, parame-\nter inheritance, multiple-round training etc. Then we train\nPanGu-\u03c0 Pro models with 1B and 1.5B parameters, which\nsignificantly improve performance than the baseline models.\nBased on the observations, we also note several intriguing\ndirections for further exploration. In terms of neural archi-\ntecture, how to directly learn a compact tokenizer that seam-\nlessly integrates both representation ability and parameter\nefficiency. Additionally, exploring hardware-friendly archi-\ntectures holds promise for mitigating computational and\nstorage costs. For example, GQA (Ainslie et al., 2023) is an\neffective strategy to reduce RAM require of edge devices (re-\nfer to Appendix A). Concerning model optimization, the\nimportance of effective parameter initialization cannot be\noverstated, setting the model on a path toward high perfor-\nmance from the outset. Nevertheless, the challenge remains\nin identifying effective parameters, which is an open ques-\ntion in the field. Besides, the training characteristics of\ntiny models differ significantly from their larger counter-\nparts. For instance, within the framework of multiple-round\n8\nRethinking Optimization and Architecture for Tiny Language Models\ntraining, there is an urgent demand for the development of\nnew parameter optimization techniques and data refining\nmethods. Numerous questions warrant in-depth exploration,\nand we hope the findings presented in this paper can spark\ninspiration for further research.\n7. Impact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr\u00b4on, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nBai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,\nY., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M.,\nLin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J.,\nMen, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang,\nP., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang,\nA., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan,\nH., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z.,\nZhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023.\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\nPiqa: Reasoning about physical commonsense in natural\nlanguage. In Thirty-Fourth AAAI Conference on Artificial\nIntelligence, 2020.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research,\n24(240):1\u2013113, 2023.\nChu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei,\nF., Zhang, X., Zhang, B., Wei, X., and Shen, C. Mo-\nbilevlm: A fast, strong and open vision language assistant\nfor mobile devices, 2023.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018.\nComputer, T.\nRedpajama:\nAn open source recipe\nto reproduce llama training dataset, 2023.\nURL\nhttps://github.com/togethercomputer/\nRedPajama-Data.\nContributors, O. Opencompass: A universal evaluation\nplatform for foundation models. https://github.\ncom/open-compass/opencompass, 2023.\nCui, Y., Yang, Z., and Yao, X.\nEfficient and effective\ntext encoding for chinese llama and alpaca.\narXiv\npreprint arXiv:2304.08177, 2023.\nURL https://\narxiv.org/abs/2304.08177.\nFrantar, E. and Alistarh, D. Sparsegpt: Massive language\nmodels can be accurately pruned in one-shot. In Inter-\nnational Conference on Machine Learning, pp. 10323\u2013\n10337. PMLR, 2023.\nGeng, X. and Liu, H. Openllama: An open reproduction\nof llama, May 2023. URL https://github.com/\nopenlm-research/open_llama.\nGoyal, P., Doll\u00b4ar, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and\nHe, K. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nGuo, Y., Yao, A., and Chen, Y. Dynamic network surgery\nfor efficient dnns. In Lee, D. D., Sugiyama, M., von\nLuxburg, U., Guyon, I., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 29: Annual\nConference on Neural Information Processing Systems\n2016, December 5-10, 2016, Barcelona, Spain, pp. 1379\u2013\n1387, 2016.\nHan, S., Pool, J., Tran, J., and Dally, W. J. Learning both\nweights and connections for efficient neural networks.\nCoRR, abs/1506.02626, 2015. URL http://arxiv.\norg/abs/1506.02626.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021.\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T.,\nLiu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and\nHe, J. C-eval: A multi-level multi-discipline chinese\n9\nRethinking Optimization and Architecture for Tiny Language Models\nevaluation suite for foundation models. arXiv preprint\narXiv:2305.08322, 2023.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large-batch training for deep\nlearning: Generalization gap and sharp minima. arXiv\npreprint arXiv:1609.04836, 2016.\nKrizhevsky, A. One weird trick for parallelizing convolu-\ntional neural networks. arXiv preprint arXiv:1404.5997,\n2014.\nKudo, T. and Richardson, J.\nSentencepiece:\nA sim-\nple and language independent subword tokenizer and\ndetokenizer for neural text processing. arXiv preprint\narXiv:1808.06226, 2018.\nLee, J., Park, S., Mo, S., Ahn, S., and Shin, J. Layer-adaptive\nsparsity for the magnitude-based pruning. In 9th Inter-\nnational Conference on Learning Representations, ICLR\n2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net, 2021. URL https://openreview.net/\nforum?id=H6ATjJ0TKdf.\nLee, N., Ajanthan, T., and Torr, P. H. S. Snip: single-\nshot network pruning based on connection sensitivity. In\n7th International Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019. URL https://openreview.\nnet/forum?id=B1VZqjAcYX.\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring massive\nmultitask language understanding in chinese, 2023a.\nLi, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar,\nS., and Lee, Y. T. Textbooks are all you need ii: phi-\n1.5 technical report. arXiv preprint arXiv:2309.05463,\n2023b.\nLoshchilov, I. and Hutter, F.\nSgdr:\nStochastic gra-\ndient descent with warm restarts.\narXiv preprint\narXiv:1608.03983, 2016.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017.\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the struc-\ntural pruning of large language models. arXiv preprint\narXiv:2305.11627, 2023.\nNarayan, S., Cohen, S. B., and Lapata, M. Don\u2019t give me\nthe details, just the summary! topic-aware convolutional\nneural networks for extreme summarization, 2018.\nPeiyuan Zhang, Guangtao Zeng, T. W. and Lu, W. Tinyl-\nlama, Sep 2023.\nURL https://github.com/\njzhang38/TinyLlama.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\net al. Rwkv: Reinventing rnns for the transformer era.\narXiv preprint arXiv:2305.13048, 2023.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRen, X., Zhou, P., Meng, X., Huang, X., Wang, Y., Wang,\nW., Li, P., Zhang, X., Podolskiy, A., Arshinov, G., et al.\nPangu-\u03a3: Towards trillion parameter language model\nwith sparse heterogeneous computing. arXiv preprint\narXiv:2303.10845, 2023.\nShazeer, N. Fast transformer decoding: One write-head is\nall you need. arXiv preprint arXiv:1911.02150, 2019.\nShibata, Y., Kida, T., Fukamachi, S., Takeda, M., Shinohara,\nA., Shinohara, T., and Arikawa, S. Byte pair encoding: A\ntext compression scheme that accelerates pattern match-\ning. 1999.\nSun, K., Yu, D., Yu, D., and Cardie, C.\nInvestigating\nprior knowledge for challenging chinese machine read-\ning comprehension.\nTransactions of the Association\nfor Computational Linguistics, 2020.\nURL https:\n//arxiv.org/abs/1904.09679v3.\nTanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S.\nPruning neural networks without any data by iteratively\nconserving synaptic flow. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nTang, Y., Wang, Y., Xu, Y., Tao, D., Xu, C., Xu, C., and Xu,\nC. Scop: Scientific control for reliable neural network\npruning. Advances in Neural Information Processing\nSystems, 33:10936\u201310947, 2020.\nTeam, I. Internlm: A multilingual language model with\nprogressively enhanced capabilities, 2023.\nToneva, M., Sordoni, A., Combes, R. T. d., Trischler, A.,\nBengio, Y., and Gordon, G. J. An empirical study of\nexample forgetting during deep neural network learning.\narXiv preprint arXiv:1812.05159, 2018.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\n10\nRethinking Optimization and Architecture for Tiny Language Models\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. R. Super-\nglue: A stickier benchmark for general-purpose language\nunderstanding systems, 2020.\nWang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y.,\nWang, X., Hu, H., Bai, Z., Wang, Y., et al. Pangu-\u03c0:\nEnhancing language model architectures via nonlinearity\ncompensation. arXiv preprint arXiv:2312.17276, 2023.\nWei, T., Zhao, L., Zhang, L., Zhu, B., Wang, L., Yang, H.,\nLi, B., Cheng, C., L\u00a8u, W., Hu, R., Li, C., Yang, L., Luo,\nX., Wu, X., Liu, L., Cheng, W., Cheng, P., Zhang, J.,\nZhang, X., Lin, L., Wang, X., Ma, Y., Dong, C., Sun,\nY., Chen, Y., Peng, Y., Liang, X., Yan, S., Fang, H., and\nZhou, Y. Skywork: A more open bilingual foundation\nmodel, 2023.\nWinata, G. I., Xie, L., Radhakrishnan, K., Wu, S., Jin, X.,\nCheng, P., Kulkarni, M., and Preotiuc-Pietro, D. Over-\ncoming catastrophic forgetting in massively multilingual\ncontinual learning. arXiv preprint arXiv:2305.16252,\n2023.\nXia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama:\nAccelerating language model pre-training via structured\npruning. 2023.\nXu, L., Lu, X., Yuan, C., Zhang, X., Xu, H., Yuan, H., Wei,\nG., Pan, X., Tian, X., Qin, L., et al. Fewclue: A chinese\nfew-shot learning evaluation benchmark. arXiv preprint\narXiv:2107.07498, 2021.\nYang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin,\nC., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan\n2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\nYi. A series of large language models trained from scratch\nby developers at 01-ai.\nhttps://github.com/\n01-ai/Yi, 2023.\nYou, Y., Gitman, I., and Ginsburg, B.\nLarge batch\ntraining of convolutional networks.\narXiv preprint\narXiv:1708.03888, 2017.\nYou, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli,\nS., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J.\nLarge batch optimization for deep learning: Training bert\nin 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\nHellaswag: Can a machine really finish your sentence?,\n2019.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,\nY., Saied, A., Chen, W., and Duan, N.\nAgieval: A\nhuman-centric benchmark for evaluating foundation mod-\nels, 2023.\n11\nRethinking Optimization and Architecture for Tiny Language Models\nA. Multi-head Attention v.s. Group-query Attention\nCurrent mainstream LLaMA architectures, such as Qwen (Bai et al., 2023) and InternLM (Team, 2023), commonly employ\nmulti-head attention (MHA) mechanisms (Vaswani et al., 2017). However, storing a large amount of KV (key and value)\ncache in memory-constrained edge devices poses a significant challenge, especially in long text input scenarios. Group\nquery attention (Ainslie et al., 2023) is recently proposed to seek a balance between multi-head attention and multi-query\nattention (Shazeer, 2019). By sharing the same key-value heads for all queries in each group, the RAM requirement for\nthe KV cache can be largely reduced in edge devices. We convert our pretrained PanGu-\u03c0-1.5B Pro to its GQA version\nby mean-pooling (Ainslie et al., 2023) the KV heads in each group and then continually training on only 5% of the origin\ndataset. The number of groups is set to 8. As shown in Table 10, the converted GQA version model exhibits comparable\nperformance to the MHA counterpart with fewer parameters.\nTable 10. Comparison between MHA and GQA.\nAttn.\nSize\nC-Eval\nCMMLU\nMMLU\nAGI-Eval\nMHA\n1.5B\n52.91\n49.51\n53.76\n44.42\nGQA\n1.4B\n48.75\n46.94\n51.97\n43.59\nB. Improved Random Initialization\nWhen training the model from scratch, we typically initialise the weight of linear layers using a normal distribution N(0, \u03c32)\nwith zero mean and standard deviation \u03c3. Some methods (Radford et al., 2019; Team, 2023) use different standard deviations\nfor different layers. Figure 11 shows the standard deviations of different layers after pretraining, indicating that the parameter\ndistributions will be similar from different initialisation values. In particular, the variance of the four linear layers within the\nMHA layer varies with the depth, while the variance of the linear layers within the MLP layers remains almost constant.\nWe introduce an improved initialization method that adapts the standard deviation of different linear weights to the depth of\nthe model. In the MHA, the initial standard deviation of the query and key projection parameters decreases from\n\u221a\n2\u03c3 to \u03c3,\nwhile that of the value and out projection parameters increases from \u03c3/\n\u221a\n2 to \u03c3, as the layer index increases. In the MLP,\nwe keep the initial standard deviation of the linear weights constant at \u03c3 for all layers. As demonstrated in Table 11, our\napproach exhibits a marginal superiority over the constant initialization approach.\nC. The Impact of Attention Head\nTable 12 shows how performance varies w.r.t. the number of attention heads. The results show that it does not affect the\ninference speed or the model performance significantly, as long as the hidden size is fixed. The head dimension is set to 128\nin PanGu-\u03c0-1B Pro and PanGu-\u03c0-1.5B Pro.\n0\n2\n4\n6\n8 10 12 14 16 18\nLayer Index\n0.005\n0.010\n0.015\n0.020\n0.025\nStandard Deviation\nConstant\n0\n2\n4\n6\n8 10 12 14 16 18\nLayer Index\nInternLM\n0\n2\n4\n6\n8 10 12 14 16 18\nLayer Index\nGPT2\n0\n2\n4\n6\n8 10 12 14 16 18\nLayer Index\nOurs\nself_attn.q_proj.weight\nself_attn.k_proj.weight\nself_attn.v_proj.weight\nself_attn.o_proj.weight\nmlp.gate_proj.weight\nmlp.down_proj.weight\nmlp.up_proj.weight\nFigure 11. The standard deviations of different layers after pretraining.\n12\nRethinking Optimization and Architecture for Tiny Language Models\nTable 11. Performance under different initialization strategies. Our method exhibits a slight edge over the constant standard deviation\napproach.\nInitialization Method\nARC-E\nHellaSwag\nC3\nAvg.\nConstant\n37.57\n41.16\n49.04\n42.59\nGPT2 (Radford et al., 2019)\n38.62\n39.34\n48.44\n42.13\nInternLM (Team, 2023)\n34.39\n41.48\n47.70\n41.19\nOurs\n37.57\n42.00\n49.26\n42.94\nTable 12. Varying the number of attention heads.\nHeads\nHead Dimension\nSpeed\nARC-E\nHellaSwag\nC3\nAvg.\n14\n128\n29.49\n34.39\n41.48\n47.70\n41.19\n28\n64\n30.11\n35.39\n41.63\n48.09\n41.70\n56\n32\n30.49\n33.16\n41.36\n48.17\n40.90\nD. Weight Decay\nWeight decay (Loshchilov & Hutter, 2017) is a commonly employed regularization method aimed at mitigating overfitting\non the training set. We delve into its impact in Table 13. Elevating the weight decay imparts more robust regularization,\nalbeit at the expense of constraining the model\u2019s representation capacity. Through empirical experiments, we observe that\nthe model attains optimal performance when the weight decay is set at 0.1.\nTable 13. Performance under different weight decay. The model achieved the best performance with a weight decay of 0.1.\nWeight Decay\nARC-E\nHellaSwag\nC3\nAverage\n0.2\n34.68\n36.15\n45.31\n38.71\n0.1\n34.39\n41.48\n47.70\n41.19\n0.01\n34.74\n36.76\n45.26\n38.92\n0.001\n33.59\n37.07\n44.93\n38.53\n0.0001\n31.22\n37.76\n44.11\n37.70\n13\nRethinking Optimization and Architecture for Tiny Language Models\nE. Additional Results of Layer Selection\nThe layer skipping results of single downstream tasks are released in Figure 12. The performance trend of single downstream\ntasks are consistent to the average results shown Figure 5 of the main paper. Layers situated near the beginning and end of\nthe model often carry more significance than the intermediate layers.\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nPanGu- -7B 1 Skip\nPanGu- -7B 2 Skip\nPanGu- -7B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nInternLM-7B 1 Skip\nInternLM-7B 2 Skip\nInternLM-7B 3 Skip\n0\n5\n10\n15\n20\n25\n30\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-7B 1 Skip\nLLaMA2-7B 2 Skip\nLLaMA2-7B 3 Skip\n0\n10\n20\n30\n40\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-13B 1 Skip\nLLaMA2-13B 2 Skip\nLLaMA2-13B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\n80\nAccuracy/%\nPanGu- -7B 1 Skip\nPanGu- -7B 2 Skip\nPanGu- -7B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\n80\nAccuracy/%\nInternLM-7B 1 Skip\nInternLM-7B 2 Skip\nInternLM-7B 3 Skip\n0\n5\n10\n15\n20\n25\n30\nLayer\n20\n30\n40\n50\n60\n70\n80\nAccuracy/%\nLLaMA2-7B 1 Skip\nLLaMA2-7B 2 Skip\nLLaMA2-7B 3 Skip\n0\n10\n20\n30\n40\nLayer\n20\n30\n40\n50\n60\n70\n80\nAccuracy/%\nLLaMA2-13B 1 Skip\nLLaMA2-13B 2 Skip\nLLaMA2-13B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nPanGu- -7B 1 Skip\nPanGu- -7B 2 Skip\nPanGu- -7B 3 Skip\n0\n5\n10\n15\n20\n25\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nInternLM-7B 1 Skip\nInternLM-7B 2 Skip\nInternLM-7B 3 Skip\n0\n5\n10\n15\n20\n25\n30\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-7B 1 Skip\nLLaMA2-7B 2 Skip\nLLaMA2-7B 3 Skip\n0\n10\n20\n30\n40\nLayer\n20\n30\n40\n50\n60\n70\nAccuracy/%\nLLaMA2-13B 1 Skip\nLLaMA2-13B 2 Skip\nLLaMA2-13B 3 Skip\nFigure 12. Layer-skipped performance of large language models on single downstream tasks. From top to bottom, the tasks are ARC-E,\nHellaSwag, and C3, respectively.\n14\n"
  },
  {
    "title": "Code Representation Learning At Scale",
    "link": "https://arxiv.org/pdf/2402.01935.pdf",
    "upvote": "12",
    "text": "Published as a conference paper at ICLR 2024\nCODE REPRESENTATION LEARNING AT SCALE\nDejiao Zhang\u2217 & Wasi Ahmad\u2217\n{dejiaoz,wuahmad}@amazon.com\nMing Tan & Hantian Ding\n{mingtan,dhantian}@amazon.com\nRamesh Nallapati & Dan Roth & Xiaofei Ma & Bing Xiang\n{rnallapa,drot,xiaofeim,bxiang}@amazon.com\nAWS AI Labs\nABSTRACT\nRecent studies have shown that code language models at scale demonstrate sig-\nnificant performance gains on downstream tasks, i.e., code generation. However,\nmost of the existing works on code representation learning train models at a hun-\ndred million parameter scale using very limited pretraining corpora. In this work,\nwe fuel code representation learning with a vast amount of code data via a two-\nstage pretraining scheme. We first train the encoders via a mix that leverages both\nrandomness in masking language modeling and the structure aspect of program-\nming language. We then enhance the representations via contrastive learning with\nhard negative and hard positive constructed in an unsupervised manner. We es-\ntablish an off-the-shelf encoder model that persistently outperforms the existing\nmodels on a wide variety of downstream tasks by large margins. To comprehend\nthe factors contributing to successful code representation learning, we conduct\ndetailed ablations and share our findings on (i) a customized and effective token-\nlevel denoising scheme for source code; (ii) the importance of hard negatives and\nhard positives; (iii) how the proposed bimodal contrastive learning boost the cross-\nlingual semantic search performance; and (iv) how the pretraining schemes decide\nthe downstream task performance scales with the model size. 1\n1\nINTRODUCTION\nLarge language models (LLMs) pretrained on a massive amount of source code have reshaped the\nlandscape of code generation (Chen et al., 2021; Chowdhery et al., 2022; Li et al., 2023, inter alia).\nAs an example, the recent release of a 6TB dataset (Kocetkov et al., 2022) comprising source code\nunder permissive licenses play pivotal roles in promoting the advancement of code language models\nin present times. Nonetheless, these large corpora are not fully utilized to develop general-purpose\nProgramming Language (PL) embedding models. To date, most PL embedding models (Feng et al.,\n2020a; Guo et al., 2021; 2022, inter alia) have no more than 125M parameters and are primarily\ntrained on a few millions of training examples, e.g., CodeSearchNet (Husain et al., 2019).\nDespite the undeniable significance of large-scale data, it\u2019s imperative to acknowledge the vital role\nof pretraining objectives. The prevailing approach for pretraining a bidirectional Transformer en-\ncoder to learn representations is through the optimization of a masked language modeling (MLM)\nobjective, as proposed by Devlin et al. (2019b). The masking scheme in the standard MLM objective\nfollows an 80-10-10 practice.2 However, we have noticed that such a masking scheme leads to the\ndevelopment of suboptimal code embedding models. Since code snippets contain both natural lan-\nguage (NL) statements (i.e., docstrings, comments) and pure code, hence replacing masked tokens\nwith a random token following the 80-10-10 convention could result in replacing an NL token with\na PL token, and vice versa (see statistics in Appendix A.3). We speculate such co-occurrence of PL\n\u2217Equal Contribution.\n1Our code and model is released at https://github.com/amazon-science/CodeSage and\nhttps://huggingface.co/codesage.\n2Under this scheme, 80% of the randomly selected tokens for prediction are replaced with the [MASK]\ntoken, 10% are substituted with random tokens, and the remaining tokens remain unchanged.\n1\narXiv:2402.01935v1  [cs.CL]  2 Feb 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: An overview of the key ingredients of CODESAGE for code representation learning.\nand NL together with the syntax nature of source code make it easier to disrupt both the semantics\nand structure of the masked code, resulting in sub-optimal learning of the language model.\nWhile MLM pretraining yields contextual token representations, most downstream discriminative\ntasks primarily function at the sequence level. When the objective is to enhance the representation\ndiscrimination power for immediate application in sequence-level tasks, contrastive learning (CL)\nemerges as the go-to approach. Existing works have employed unimodal CL (using Code-Code\npairs) (Guo et al., 2022; Jain et al., 2021) or bimodal CL (using Text-Code pairs) (Li et al., 2022) for\nrepresentation learning. In unimodal CL, a popular choice is to utilize dropout augmentation Gao\net al. (2021) to construct positive code pairs. However, we found that dropout augmentation suffers\nfrom supporting long training process, also reported by Zhou et al. (2022). In contrast, bimodal CL\nbecomes an appealing choice, primarily because of the availability of naturally occurring pairs. Prior\nstudies utilize functions and their corresponding docstrings to establish the bimodal training pairs.\nNonetheless, our preliminary experiments indicate that substantial overlap between docstrings and\nfunction signatures simplifies the contrastive learning process (see statistics in Appendix A.6).\nTo this end, we present CODESAGE, a bidirectional encoder representation model for source code.\nWe pretrain CODESAGE using a two-stage training scheme with a large amount of customized pre-\ntraining data (Kocetkov et al., 2022). We depict the key ingredients of CODESAGE in Figure 1. We\nfirst train the bidirectional encoders via a mix of two objectives complementing each other: identifier\ndeobfuscation (DOBF) and MLM without the 80-10-10 practice. Similar to a human programmer,\nfinding meaningful names for obfuscated identifiers necessitates the model to acquire a profound\ncomprehension of code semantics and structure. Meanwhile, as a more general objective, MLM\ncovers other facets beyond identifiers of code \u2013 this is important for enriching the training signals,\nespecially for data examples with non-informative identifier names. In the second stage, we leverage\nthe (text, code) pairs for bimodal contrastive learning (CL). In contrast to existing approaches that\nprimarily rely on naturally occurring text and code pairs, we propose a strategy to reduce the likeli-\nhood of the model learning shortcuts. Our approach involves exclusively utilizing the function body\nwhile disregarding the signature and return statements. We additionally harness CL based on hard\nnegatives identified within the embedding space. We show that such a hard positive and negative\nconstruction strategy is simple, yet essential for effective bimodal contrastive learning.\nWe train three bidirectional encoder representation models, namely, CODESAGE-SMALL (130M),\nCODESAGE-BASE (356M), and CODESAGE-LARGE (1.3B). We assess the effectiveness of our ap-\nproach over a wide variety of discriminative tasks, where CODESAGE substantially outperforms the\nprevious state-of-the-art models with similar model sizes on most tasks. To comprehend the factors\ncontributing to successful code representation learning, we meticulously analyze the key compo-\nnents of our framework and present our findings for future research endeavors.\n2\nRELATED WORKS\nEmbedding for Programming Languages\nRecently, there has been a surge of interest in learn-\ning general-purpose representations to support a wide variety of downstream tasks in programming\nlanguages. Feng et al. (2020a); Kanade et al. (2020); Li et al. (2023) take the inspiration of the suc-\ncess in text and optimize the Masking Language Modeling (MLM) objective on the linearized code\n2\nPublished as a conference paper at ICLR 2024\ndata. Similar to text, they additionally optimize with replaced token detection objective (Clark et al.,\n2020) or the next sentence prediction objective (Devlin et al., 2019b) for source code. Another line\nof work leverages the structure aspect of code to provide additional training signals. Among them,\nGuo et al. (2021) leverages the data flow to encode the relation of \u201cwhere-the-value-comes-from\u201d\nbetween variables. Wang et al. (2021a); Jiang et al. (2021) inject syntactical structure from the ab-\nstract syntax tree (AST) through variant auxiliary objectives. A more recent work (Guo et al., 2022)\nflattens the AST structure into a sequence directly and encodes the syntax information via language\nmodeling objectives. Wang et al. (2021b); anne Lachaux et al. (2021) train a sequence-to-sequence\nlanguage model to reconstruct the original code from an identifier-obfuscated code where class,\nfunction, and variable names are replaced with special tokens. Deobfuscation implicitly encodes\ndata flow and AST without involving auxiliary objectives or complex input with deep hierarchy,\nsince the model needs to understand the dependency between variables as well as code structure so\nas to correctly predict the names for identifiers.\nContrastive Learning\nEver since the early success attained by the Siamese (Hadsell et al., 2006)\nnetwork, contrastive learning has been widely adopted in representation learning using deep neural\nnetworks. Song et al. (2016) extends the vanilla triplet loss by contrasting each positive example\nagainst all in-batch negatives, which has greatly improved the learning efficiency and is further\npopularized by SimCLR (Chen et al., 2020). However, different from the compute version domain\nwhere effective positives can be obtained by stochastic transformations of images in the input space,\neffective data augmentation has long been a challenge in NLP due to the discrete nature of the input.\nSuch challenge is further validated in Gao et al. (2021) which shows that dropout (Srivastava et al.,\n2014) as the minimum data augmentation is often more effective than those obtained by operating\nin the discrete input space, e.g., word deletion and replacement.\nAlternatively, various methods have been proposed to leverage naturally occurring pairs as positives.\nZhou et al. (2022) treat the consecutive utterances from dialogue data as positives, while Neelakan-\ntan et al. (2022) consider the neighboring texts mined from the internet. A very recent work (Wang\net al., 2022) leverages the question and answer or comment pairs from StackExchange and Reddit.\nIn a similar vein for a programming language, Guo et al. (2022); Wang et al. (2021a); Neelakan-\ntan et al. (2022) leverage (text, code) pairs with text mined from the docstrings. We take a step\nfurther by focusing on hard positive and hard negative construction, which is a key ingredient for\nrepresentation learning and allows us to attain off-the-shelf embedding models.\n3\nMETHOD\n3.1\nMASK LANGUAGE MODELING AND DEOBFUSCATION PRE-TRAINING\nGiven an input sequence with N tokens, i.e., x = [x1, x2, . . . , xN, ], the mask language modeling\nobjective (Devlin et al., 2019b) is formed as follows\nLMLM(x) = \u2212\nX\ni\u2208M\nlog P\n\u0000xi|xM\u0001\n(1)\nHere M denotes the mask applied on the given input x. Equation (1) is essentially a denoising\nobjective with the task to predict the original tokens given the masked sequence xM.\nDeobfuscation\nWe first consider identifier deobfuscation (DOBF) which pretrains the model to\npredict the masked-out names of the identifiers. Similar to human programmers, in order to de-\nobfuscate the code (predict the identifiers), the model needs to understand both the semantics and\nstructure of the code. Also notice that the natural language (NL) tokens, i.e., docstring and comment,\nare excluded from code obfuscation. When the model is trained to predict the identifier names, it\ncan benefit from looking at and correlating with the NL tokens in comments or docstrings as those\noften carry rich semantics of code. Consequently, the model is encouraged to learn improved shared\nrepresentations between programming language and natural language, as indicated by the better\nNL2Code search performance attained by DOBF than the random masking strategy in Table 3.\nDOBF is initially proposed for Seq2Seq models (anne Lachaux et al., 2021; Wang et al., 2021b).\nTo the best of our knowledge, we are the first to apply it to the encoder-only models. The main\nchallenge to adopting DOBF for encoder-only models is to construct the one-on-one mapping be-\n3\nPublished as a conference paper at ICLR 2024\ntween mask tokens (inputs to the LM) and identifier tokens (output labels) due to the differences in\ncode tokenization (i.e., using tree-sitter) and model-specific tokenization (i.e., using a sentencepiece\ntokenizer). We briefly discuss the challenge in Appendix A.5.\nRandom Masking\nAdditionally, we also involve the random token masking strategy in BERT\nDevlin et al. (2019b) for two main reasons. First, to promote better representations by promoting\nthe model to learn beyond identifiers. Taking Python as an example, there are approximately 30% of\nthe code tokens associated with identifiers, hence better representations can be attained by encoding\nthe information carried by the remaining 70% of tokens. Second, not every programmer follows the\nnaming conventions, e.g., meaningless variable names like v1, v2, v3 can be used. Predicting such\ntokens is unnecessarily hard and provides a very limited training signal.\nWe do not follow the 80-10-10 masking convention proposed in the standard MLM for text (Devlin\net al., 2019b). Since source codes are composed of NL and code tokens (i.e., identifiers, keywords,\noperators), random replacement of tokens could hurt both the structure and meaning of code and\nleads to deterioration in representation learning.3 We show in Section 4.2.1 that the 80-10-10 con-\nvention consistently results in worse performance on downstream tasks. In this paper, we also set the\nrandom masking rate to 15% which we find is optimal through our ablation study in Appendix A.4.\nFor each training example, we randomly pick DOBF or random masking with equal probability.\n3.2\nBIMODAL CONTRASTIVE LEARNING WITH HARD NEGATIVE AND HARD POSITIVE\nLet xi, xi+ denote a positive input pair and hi, hi+ be the associated representations output by the\nlast hidden layer of the encoder. Let B = {h1, h1+, h2, h2+, . . . , hN, hN +} denote the representa-\ntions of a randomly sampled batch with N pairs, we then minimize the following symmetric loss,\nLCL (hi, hi+) = \u2212\n \nlog\nexp(hi \u22c4 hi+/\u03c4)\nexp(hi \u22c4 hi+/\u03c4) + P\nk\u2208B\\(i,i+) \u03b3k\ni \u00b7 exp(hi \u22c4 hk/\u03c4)\n+ log\nexp(hi+ \u22c4 hi/\u03c4)\nexp(hi+ \u22c4 hi/\u03c4) + P\nk\u2208B\\(i,i+) \u03b3k\ni+ \u00b7 exp(hi+ \u22c4 hk/\u03c4)\n!\n.\n(2)\nHere, \u03c4 is the temperature hyper-parameter which we set as 0.05 in this work. \u22c4 denotes cosine\nsimilarity between two representation vectors. \u03b3k\ni is the weight parameter which we will detail next.\nHard Negative\nWithout supervision, it is tricky to identify hard negatives. We resort to a distance-\nbased unsupervised approximation of hard negatives proposed in Zhang et al. (2021). For a given\nanchor hi, hard negatives refer to those semantically different examples but are mapped close to hi\nin the representation space. Thereby, the closer a negative is to the anchor hi in the representation\nspace, the larger \u03b3 value is desired, which can be characterized as follows\n\u03b3k\ni =\nexp(hi \u22c4 hk/\u03c4)\nexp(hi \u22c4 hk/\u03c4) + P\nj\u2208B\\(i,i+,k) exp(hi \u22c4 hj/\u03c4) .\n(3)\nThat is, \u03b3k\ni approximates the relative importance of hk to the anchor hi, among all 2N-2 in-batch\nnegatives. Despite the semantic equivalence between training examples except the given positive\npairs are not available in our case, the above approximation of hard negatives is still valid. To see\nthis, notice that each training batch is randomly sampled with a much smaller size compared to the\nsize of the whole training data. Hence the presence of false negatives within each batch is very small\nas long as the training data is large and diverse enough. We set the batch size (N) to 8K in this paper,\nunder which we observe monotonic increasing performance reported on the downstream tasks.\nHard Positive\nWe consider naturally occurring (text, function) as positive pairs, where the text\nis mined from the function docstring (Husain et al., 2019). The text extracted from the docstring\noften summarizes the high-level semantics of the code well. Therefore, contrastive learning with the\nbimodal data, i.e., text and function pairs, largely boosts the NL2Code semantic search performance\n3For example, masking a couple of tokens randomly from tokenizer.convert ids to tokens\ncan yield tokenizer.convert ids to<mask><mask> but random token replacement can result in\ntokenizer.convert jet toboattokens. Consequently, the code semantics are largely altered and\nrepresentation learning via the self-attention mechanism can thereby deteriorate. See Appendix A.3 for more.\n4\nPublished as a conference paper at ICLR 2024\nin Section 4.2.2. Further, the extracted text of semantically equivalent code, no matter from the same\nor different programming languages, is often less diverse compared to the code themselves. Thereby,\nsemantically similar codes can be implicitly grouped together through the same or very similar\nsummary text. Our conjecture is validated by the large performance gain attained by contrastive\nlearning on both in-language and cross-language Code2Code search in Section 4.2.2.\nIt is also easy to see that function names and input variable names often share a significant similarity,\nespecially in terms of the lexical overlap with the summary text. We further quantify such overlap\nwith statistics detailed in Appendix A.6. We thereby form hard positives by removing both function\nsignature and return statements.4 As demonstrated in Section 4.2.2, hard positives formed in this\nway can effectively boost the performance of contrastive learning.\n4\nEXPERIMENTS\nTraining Data and Model Architecture\nWe train our models on The Stack dataset (Kocetkov\net al., 2022) over nine languages - Python, Java, Javascript, Typescript, C#, C, Ruby, Go, and PHP.\nAs aforementioned, we train three embedding models with size 130M (CODESAGE-SMALL), 356M\n(CODESAGE-BASE), and 1.3B (CODESAGE-LARGE) parameters. Please refer to Appendix A for\ntraining details at each stage and model hyper-parameters.\nEvaluation Protocol\nWe assess the performance of our models over two main categories of down-\nstream tasks, semantic search and classification. Our goal is to perform an evaluation of the encoder\nmodels for those practical scenarios where supervised fine-tuning data collection is costly. We\nthereby focus on zero-shot semantic search and only finetuning a linear classification layer on top of\nthe frozen encoders for classification tasks (Peters et al., 2019; Chen et al., 2020; Wang et al., 2022).\nWe report the fully finetuned classification results and finetuning hyper-parameters in Appendix B.3.\nBaselines\nWe compare our models against four general-purpose code representation learning en-\ncoders and OpenAI-Embedding-Ada-002 by following its suggestion on model selection.5\nBoth CodeBERT (Feng et al., 2020b) and GraphCodeBERT (Guo et al., 2021) are trained with\nstandard MLM on six programming languages using CodeSearchNet (Husain et al., 2019)6, while\nthe replaced token detection objective (Clark et al., 2020) and data flow prediction objectives are\nadopted as auxiliary objectives, respectively. UnixCoder (Guo et al., 2022) is trained via three\nlanguage modeling and two contrastive learning objectives using the same dataset. More recently,\nStarEncoder (Li et al., 2023) is trained with MLM and next sentence prediction (Devlin et al.,\n2019a) on 86 programming languages from The Stack (Kocetkov et al., 2022). We provide more\ndetails for each baseline model in Table 6 in Appendix. We also consider decoder-only baselines in\nAppendix B.\n4.1\nCOMPARISON WITH THE BASELINES\nWe first compare CODESAGE against the aforementioned baselines on the following tasks.\nCode2Code semantic search is the task of retrieving relevant code fragments given a code fragment\nas a query. In this work, we extend the Code2Code search evaluation set (Guo et al., 2022) created\nfrom CodeNet to six more languages - C, C#, Javascript, Typescript, GO, and PHP, for which we\nsummarize the details in Appendix B.2. We report the in-language where query and candidate codes\nare in the same language, code2code search results in Table 1.\nNL2Code semantic search is the task of using natural language as the query to retrieve the relevant\ncode. We consider three benchmarks in Table 2, CoSQA (Huang et al., 2021), AdvTest (Lu et al.,\n2021), and CSN (Guo et al., 2021) . Detailed data statistics can be found in Appendix B.2.\nClassification We consider three source code classification tasks. Code Defect detection is a bench-\nmark in C from CodeXGLUE (Lu et al., 2021), with a binary label indicating whether a code is\ninsecure and may attack software systems. Code Complexity prediction (Jeon et al., 2023) is a Java\n4Removal of function signature reduces the chance to learn shortcuts due to its similarity with the summary\ntext. We remove the return statements to make a code look like a generic code snippet.\n5OpenAI suggests using OpenAI-Embedding-Ada-002 due to its efficiency and better performance than\ntheir 001 models https://platform.openai.com/docs/guides/embeddings.\n6The dataset includes 2.3M functions paired with natural language documents.\n5\nPublished as a conference paper at ICLR 2024\nModel\nPython\nJava\nJS\nTS\nC#\nC\nRuby\nPHP\nGO\nAvg\nCodeBERT\n14.40\n7.62\n5.47\n6.05\n3.66\n5.53\n13.55\n10.28\n6.27\n8.09\nGraphCodeBERT\n19.23\n10.78\n7.38\n8.65\n5.54\n8.48\n19.69\n15.67\n9.65\n11.68\nStarEncoder\n19.17\n11.65\n9.0\n10.52\n5.69\n9.72\n21.57\n16.98\n10.81\n12.79\nUnixCoder\n30.77\n16.45\n21.32\n21.95\n6.19\n15.62\n32.33\n31.93\n13.94\n21.17\nOpenAI-Ada-002\n35.91\n25.13\n19.01\n21.86\n10.17\n29.15\n40.85\n40.47\n23.43\n27.33\nCODESAGE-SMALL\n36.31\n23.97\n26.60\n29.90\n11.84\n22.84\n29.06\n34.64\n19.56\n26.08\nCODESAGE-BASE\n47.52\n22.84\n28.70\n31.95\n13.37\n30.99\n44.86\n51.13\n25.15\n32.95\nCODESAGE-LARGE\n46.70\n33.13\n37.16\n41.18\n16.81\n32.89\n54.12\n52.13\n32.48\n38.51\nTable 1: MAP score (%) of the zero-shot code search task. The language names mentioned in the\ntop row indicate the languages queries and candidates are written in.\nNL2Code\nClassification\nModel\nCoSQA\nAdvTest\nCSN\nDefect\nComplexity\nRunTime\nCodeBERT\n0.24\n0.06\n0.10\n51.820.38\n35.601.96\n6.20.02\nGraphCodeBERT\n16.20\n5.58\n11.26\n55.260.28\n55.541.98\n10.630.10\nStarEncoder\n10.78\n0.93\n2.69\n53.20.11\n50.633.33\n8.910.05\nUnixCoder\n42.11\n27.32\n46.39\n60.280.04\n76.451.10\n20.870.43\nOpenAI-Ada-002\n44.23\n38.08\n71.24\n62.560.11\n79.820.50\n20.840.36\nCODESAGE-SMALL\n49.92\n41.28\n63.86\n57.520.21\n79.760.50\n25.051.04\nCODESAGE-BASE\n48.50\n49.08\n68.72\n57.740.09\n85.321.72\n24.700.40\nCODESAGE-LARGE\n47.53\n52.67\n71.24\n58.950.13\n90.322.10\n24.420.28\nTable 2: Left. MRR score (%) of NL2Code search in zero-shot setting. For CSN, we report\nthe average performance over six languages (see Table 9 in Appendix for the detailed results).\nRight. F1 (macro) score of the source code classification tasks attained by only finetuning\nthe classification head. We finetuned each model using three seeds and reported the mean and\nstandard deviation (in subscript). The fully finetuned results can be found in Appendix B.3.\nbenchmark that requires predicting the algorithmic complexity among 7 labels. The RunTime error\nprediction (Bieber et al., 2023) benchmark has 29 possible labels with highly imbalanced distribu-\ntion (see Table 10 in Appendix). For a more robust evaluation, we balance the dataset by aligning\nits total training examples of the \u201cno error\u201d class with the cumulative count of the other 28 classes.\nOverall Performance Summary\nOn Code2Code search, Table 1 shows that CODESAGE-SMALL\n(130M) persistently outperforms all the baseline models with known model size (i.e., exclude\nOpenAI-Embedding-Ada-002) on every language, with 23.19% relative (4.91% absolute) improve-\nment on the average performance when comparing with UnixCoder. With the increased model\nsize, CODESAGE-BASE and CODESAGE-LARGE outperform the best baseline model, i.e., OpenAI-\nEmbedding-Ada-002 (model size unknown), with 20.56% relative (5.62% absolute) and 40.91%\nrelative (11.18% absolute) improvement on the average performance, respectively.\nAs shown in Table 2, CODESAGE-SMALL achieves 18.54% to 51.1% relative (7.81% to 13.96% ab-\nsolute) improvement over UnixCoder on NL2Code search. Compared to OpenAI-Embedding-Ada-\n002, CODESAGE-SMALL attains a 12.86% relative (5.69% absolute) improvement on CosQA and an\n8.4% relative (3.12% absolute) improvement on AdvTest. On the other hand, OpenAI-Embedding-\nAda-002 attains the same average performance as CODESAGE-LARGE on CSN. However, we want\nto highlight the performance gain attained by CODESAGE on AdvTest which contains normalized\nPython functions (from CSN) with function and variable names replaced by dummy variables (see\nFigure 9 in Appendix). AdvTest constructed in this way better assesses the generalization perfor-\nmance as the model needs to understand what the obfuscated code does so as to identify the correct\ntarget code for a given natural language query.\nCompared to both UnixCoder and OpenAI-Embedding-Ada-002, CODESAGE persistently performs\nbetter on code complexity and runtime error prediction with large margins in Table 2. We also\nnotice that CODESAGE underperforms both models on code defect detection, whilst attaining better\nperformance when we finetuning the full models in Table 12 in Appendix.\n6\nPublished as a conference paper at ICLR 2024\n(a) Sample code (left) and its corrupted version following the 80-10-10 rule (right).\n0\n10\n20\n30\nIn-Language Code2Code MAP\n0\n5\n10\n15\n20\nCross-Language Code2Code MAP\n80-10-10\nFull Mask\n0\n5\n10\n15\n20\nAvg NL2Code MRR\nCodeSage-SMALL\nCodeSage-BASE\nCodeSage-LARGE\n10\n30\n50\nClassification F1 (Macro)\n(b) With a fixed masking rate of 15%, we assess the effectiveness of applying \u201cFull Mask\u201d, i.e., replacing the\nsampled tokens with the [MASK] token only, and the 80-10-10 corruption strategy on different downstream\ntasks. As it shows, Full Mask persistently yields better performance.\nFigure 2: 80-10-10 vs. \u201cFull Mask\u201d.\nCODESAGE-SMALL\nCODESAGE-BASE\nCODESAGE-LARGE\nModel\nR\nD\nS\nP\nR\nD\nS\nP\nR\nD\nS\nP\nNL2Code\n6.6\n19.9\n22.7\n25.8\n12.2\n22.5\n22.0\n23.3\n19.4\n23.3\n29.4\n30.5\nCode2Code (In)\n16.8\n14.6\n17.9\n19.7\n28.2\n23.7\n25.3\n29.2\n30.7\n28.2\n30.2\n33.9\nCode2Code (Cross)\n5.7\n6.7\n8.8\n9.6\n17.2\n14.1\n14.6\n19.7\n20.5\n18.0\n19.0\n24.6\nClassification\n51.2\n53.9\n53.5\n53.4\n53.8\n55.6\n54.8\n55.4\n52.0\n55.6\n57.2\n56.5\nTable 3: We explore two options to leverage DOBF (D) and random masking (R) to complement\neach other. (1) Sequential (S): training the model with random masking first, then DOBF. (2) Parallel\n(P): randomly picking either DOBF or random masking for a training example \u2013 our strategy.\n4.2\nABLATION STUDY\n4.2.1\nMASKING STRATEGY\n80-10-10 vs. Full Mask\nGiven an input sequence, standard MLM (Devlin et al., 2019b) first ran-\ndomly samples a subset of its tokens, of which 80% are replaced by a special token \u201c[MASK]\u201d, 10%\nare left unchanged, and the other 10% are replaced by random tokens from the vocabulary. We revisit\nthe effectiveness of such convention, originally proposed for text, for code in Figure 2. Surprisingly,\ncompared to simply replacing all selected tokens with the [MASK] token, i.e., \u201cFull Mask\u201d, the\n80-10-10 masking scheme causes a large performance drop across different downstream tasks, as\nshown in Figure 2b. A similar finding has been reported in Gao et al. (2022) for text. However, the\ndegradation is more severe for source code. As Figure 2a indicates, when replacing with random\ntokens, both the semantics and structure of the masked code can be largely disrupted, which together\nwith the presence of \u201c[MASK]\u201d tokens makes the learning too challenging (see Appendix A.3 for\nmore discussions). We hypothesize that excessive corruption may also account for the modest en-\nhancement observed in downstream tasks when scaling up the size of a model trained with 80-10-10\nin Figure 2b. It would be intriguing to explore whether this scaling trend would experience a sudden\nexpansion with a further increase in model size and training data, potentially identifying a phase\ntransition point, provided that the computational resources permit such an investigation.\n7\nPublished as a conference paper at ICLR 2024\nDeobfuscation & Random Masking Complement Each Other\nWe investigate DOBF and the\nrandom masking based MLM with \u201cFull Mask\u201d in Figure 3. DOBF persistently outperforms ran-\ndom masking on classification, which validates our motivation that the model is promoted to better\ncapture (understand) the code structure so as to predict the identifier names. DOBF also performs\nbetter on NL2Code search than random masking. A potential reason could be natural language in\ncomments and docstrings often carry rich semantics of code while both being excluded from mask-\ning in DOBF; hence when training the model to predict the identifier names, it will look at and\ncorrelate with the natural language and lead to better contextualized representations between natural\nlanguage and programming language. On the other hand, the random masking strategy (with \u201cFull\nMask\u201d) outperforms DOBF on both in-language and cross-language Code2Code search tasks. As\nexamined in Appendix A.3, a large portion of tokens in code snippets are not identifiers. Therefore,\nthe random masking strategy allows the model to learn beyond identifiers and enrich the semantics\nencoded in representations. In summary, Table 3 validates our strategy of jointly optimizing DOBF\nand random masking so as to leverage their strengths to complement each other.\nCodeSage-SMALL\nCodeSage-BASE\n15\n20\n25\n30\n35\nAvg Code2Code MAP\nBaseline\nHard Positive Only\nHard Negative Only\nBoth\n(a) Effectiveness of hard negatives and hard positives.\n0\n100\n200\n500\n1000\n2000\n5000 10000 20000\nIterations\n10\n15\n20\n25\nAvg Code2Code MAP\nUnimodal (Dropout)\nBimodal (Ours)\n(b) Unimodal vs. bimodal contrastive learning.\nFigure 3: (a) Hard negative and hard positive can independently boost performance over the baseline\nwhere neither is applied. Further improvement is attained when leveraging them simultaneously. (b)\nUnimodal contrastive learning with positives obtained via dropout requires longer training and hence\ncannot leverage vast amounts of training data to further enhance the representations.\n4.2.2\nON EFFECTIVENESS OF CONTRASTIVE LEARNING\nHard Positive and Hard Negative Effectively Boost Performance\nWe first demonstrate the ef-\nfectiveness of the hard positive and hard negative construction strategy in Figure 3a. As it shows,\nboth hard positive and hard negative can independently improve the performance by a large margin,\nwhile the combination of them persistently yields better performance across different model sizes.\nWe also observe that a large model size (i.e., CODESAGE-BASE) benefits more from the proposed\nhard negative construction strategy. This observation is unsurprising since larger models possess\nmore capacity to leverage more challenging and effective learning objectives.\nUnimodal vs. Bimodal Contrastive Learning\nIn Figure 3b, we compare our bimodal contrastive\nlearning approach against the Dropout-based unimodal contrastive learning where a positive pair is\nobtained by leveraging different dropout masks of the transformer in two forwarding passes of the\nsame sequence (Gao et al., 2021; Guo et al., 2022). For a fair comparison, hard negative optimization\nis applied to both approaches. We can see that the dropout-based unimodal contrastive learning\nsuffers from supporting a long training process and hence cannot effectively utilize a large amount\nof pretraining data to further improve the representations. A similar finding has been reported by\n(Zhou et al., 2022). Indeed, both Gao et al. (2021) nor Guo et al. (2022) \u2013 demonstrate dropout\nas effective augmentation for text and code respectively, only use a few million training examples\nthat can be covered by the amount of training data in the first 500 iterations (with batch size 8K) in\nFigure 3b where the dropout-based contrastive learning shows improvement over the baseline.\nLarger Improvement on Cross-Lingual Search\nTo gain a deeper understanding of the perfor-\nmance improvement achieved through contrastive learning during Stage II of pretraining, we delve\ninto the analysis of semantic search performance. As Figure 4a shows, contrastive learning persis-\ntently boosts the search performance with comparatively larger improvement on the cross-lingual\n8\nPublished as a conference paper at ICLR 2024\nSMALL\nBASE\nLARGE\n0\n10\n20\n30\n40\nAvg In-Language Code2Code MAP\nSMALL\nBASE\nLARGE\n5\n15\n25\n35\nAvg Cross-Language Code2Code MAP\nStage-I Only\nStage-I & Stage-II\nSMALL\nBASE\nLARGE\n10\n20\n30\n40\n50\n60\nAvg NL2Code MRR\n(a) The performance of CODESAGE in semantic search, comparing re-\nsults between searches within the same language and across different\nlanguages, while varying model sizes and training approaches.\nStage-I\nStage-I & Stage-II\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nParallel Pair\nRandom Pair\nCode2Code (In Language)\nCode2Code (Cross Language)\nNL2Code\n(b) Cosine similarity between parallel\nexamples vs. randomly sampled pairs\nusing CODESAGE representations.\nFigure 4: Examining the effectiveness of contrastive learning (Stage-II) by comparing CODESAGE\nagainst those trained with the token-level denoising objective only (Stage-I). (a) Compared to the\nin-language Code2Code search, contrastive learning persistently leads to a larger performance boost\nfor cross-lingual search, including both NL2Code and cross-language Code2Code search. (b) Con-\ntrastive learning leads to more dispersed representation space with improved discrimination, as in-\ndicated by the corresponding enlarged similarity gap between parallel and randomly sampled pairs,\nwhile simultaneously bridging the relative similarity gap between NL2Code and Code2Code pairs.\nscenarios, encompassing both NL2Code and cross-language Code2Code search. We posit that the\ntext extracted from docstring helps group semantically equivalent code together as the text often\nsummarizes the high-level semantics of code and hence are likely less diverse than the code them-\nselves. In particular, those parallel examples from different programming languages can share very\nsimilar or even the same summary. For NL2Code, the larger improvement can be credited to its\nalignment with the bimodal contrastive learning objective using (text, code) as positives. Such bi-\nmodal objective also brings NL and PL closer in Figure 4b. Compared to the model trained at Stage-I\nonly, contrastive learning pulls together NL and PL such that the relative similarity gap between par-\nallel NL2Code pairs and cross-language Code2Code parallel examples largely decreased.\n4.3\nON OBJECTIVE AND DOWNSTREAM PERFORMANCE SCALING WITH MODEL SIZE\nSMALL\nBASE\nLARGE\n5\n15\n25\n35\nAvg In-Lang Code2Code MAP\nStage-I Only\nStage-II Only\nStage-I & Stage II\nFigure 5: On the downstream task per-\nformance scaling with pretrained model\nsize under different training schemes.\nIn Figure 5, we study how the downstream task perfor-\nmance scales with the model size when pretrained with\ndifferent schemes, i.e., token-level objective only (Stage-\nI), contrastive learning only (Stage-II), and our proposed\ntwo-stage framework with Stage-I followed by Stage-II.\nWe use zero-shot multilingual in-language code search\nperformance (averaged over nine languages) for this ex-\nploration. We can see that models pretrained from scratch\nwith contrastive learning alone do not scale with the in-\ncreased model size. Neelakantan et al. (2022) report a\nsimilar finding that the contrastive objective on its own is\nnot sufficient to learn useful representations. When train-\ning from scratch with contrastive learning only, we find\nthe training loss often converges at a large value, indicat-\ning the model cannot well discriminate each positive pair\nfrom the other in-batch negatives. In other words, leveraging the token-level denoising objective to\nprovide a good embedding foundation is essential for contrastive learning to be effective and further\nenhance the sequence-level presentations.\n5\nCONCLUSION\nIn this study, we unveiled CODESAGE, a cutting-edge encoder representation learning model for\nsource code. We trained CODESAGE using an extensive dataset comprising 237 million code files\nand 75 million bimodal code and natural language pairs across nine languages. Our findings reveal\n9\nPublished as a conference paper at ICLR 2024\nthat our model outperforms its predecessors significantly in tasks related to code search and code\nclassification. We also delve into the essential factors contributing to enhanced code representation\nlearning across various model sizes. We hope our work will serve as an inspiration for future works\nin code representation learning, utilizing publicly accessible extensive corpora for source code.\nREFERENCES\nMarie anne Lachaux, Baptiste Roziere, Marc Szafraniec, and Guillaume Lample. DOBF: A de-\nobfuscation pre-training objective for programming languages. In A. Beygelzimer, Y. Dauphin,\nP. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,\n2021. URL https://openreview.net/forum?id=3ez9BSHTNT.\nDavid Bieber, Rishab Goel, Dan Zheng, Hugo Larochelle, and Daniel Tarlow.\nStatic predic-\ntion of runtime errors by learning to execute programs with external resource descriptions. In\nThe Eleventh International Conference on Learning Representations, 2023.\nURL https:\n//openreview.net/forum?id=lLp-C5nTdJG.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.\nArXiv preprint, abs/2107.03374, 2021.\nURL https://\narxiv.org/abs/2107.03374.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pp. 1597\u20131607, 2020. URL http://proceedings.mlr.\npress/v119/chen20j.html.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\nURL\nhttps://arxiv.org/abs/2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June\n2019a. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019b. doi: 10.18653/v1/\nN19-1423. URL https://aclanthology.org/N19-1423.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming\nand natural languages. In Findings of the Association for Computational Linguistics: EMNLP\n2020, pp. 1536\u20131547, 2020a. doi: 10.18653/v1/2020.findings-emnlp.139. URL https://\naclanthology.org/2020.findings-emnlp.139.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,\nBing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for pro-\ngramming and natural languages. In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pp. 1536\u20131547, Online, November 2020b. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.findings-emnlp.139. URL https://aclanthology.org/\n2020.findings-emnlp.139.\n10\nPublished as a conference paper at ICLR 2024\nJun Gao, Changlong Yu, Wei Wang, Huan Zhao, and Ruifeng Xu. Mask-then-fill: A flexible and ef-\nfective data augmentation framework for event extraction. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pp. 4537\u20134544, Abu Dhabi, United Arab Emirates, Decem-\nber 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.332.\nURL https://aclanthology.org/2022.findings-emnlp.332.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence\nembeddings.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 6894\u20136910, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL\nhttps://aclanthology.org/2021.emnlp-main.552.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn\nDrain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcode{bert}: Pre-training\ncode representations with data flow. In International Conference on Learning Representations,\n2021. URL https://openreview.net/forum?id=jLoC4ez43PZ.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. UniXcoder: Unified\ncross-modal pre-training for code representation. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 7212\u20137225, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n499. URL https://aclanthology.org/2022.acl-long.499.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion (CVPR\u201906), volume 2, pp. 1735\u20131742. IEEE, 2006.\nJunjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan\nDuan. CoSQA: 20,000+ web queries for code search and question answering. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.\n5690\u20135700, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.acl-long.442. URL https://aclanthology.org/2021.acl-long.442.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.\nCodeSearchNet challenge:\nEvaluating the state of semantic code search.\narXiv preprint\narXiv:1909.09436, 2019. URL https://arxiv.org/abs/1909.09436.\nParas Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. Contrastive\ncode representation learning. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 5954\u20135971, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n482. URL https://aclanthology.org/2021.emnlp-main.482.\nMingi Jeon, Seung yeop Baik, Joonghyuk Hahn, Yo-Sub Han, and Sang-Ki Ko. Deep learning-based\nsource code complexity prediction, 2023. URL https://openreview.net/forum?id=\n9irBKvxsw9.\nXue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. Treebert: A tree-based pre-trained\nmodel for programming language. In Uncertainty in Artificial Intelligence, pp. 54\u201363. PMLR,\n2021.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating\ncontextual embedding of source code.\nIn International conference on machine learning, pp.\n5110\u20135121. PMLR, 2020.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u02dcnoz Ferran-\ndis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of\npermissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.\n11\nPublished as a conference paper at ICLR 2024\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with\nyou! arXiv preprint arXiv:2305.06161, 2023.\nXiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin\nJiang, Weizhu Chen, and Nan Duan.\nCodeRetriever: A large scale contrastive pre-training\nmethod for code search.\nIn Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp. 2898\u20132910, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.187. URL\nhttps://aclanthology.org/2022.emnlp-main.187.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie LIU. CodeXGLUE: A machine learning benchmark dataset for code\nunderstanding and generation. In Thirty-fifth Conference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/\nforum?id=6lE4dQXaUcb.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qim-\ning Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by\ncontrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\nMatthew E. Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation\nLearning for NLP (RepL4NLP-2019), pp. 7\u201314, Florence, Italy, August 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/W19-4302. URL https://aclanthology.\norg/W19-4302.\nHyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted\nstructured feature embedding.\nIn 2016 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 4004\u20134012, 2016. doi:\n10.1109/CVPR.2016.434. URL https://doi.org/10.1109/CVPR.2016.434.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine\nlearning research, 15(1):1929\u20131958, 2014.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\njumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv\npreprint arXiv:2212.03533, 2022.\nXin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin\nJiang. Syncobert: Syntax-guided multi-modal contrastive pre-training for code representation.\narXiv preprint arXiv:2108.04556, 2021a.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi.\nCodeT5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696\u20138708,\n2021b.\ndoi: 10.18653/v1/2021.emnlp-main.685.\nURL https://aclanthology.org/\n2021.emnlp-main.685.\nDejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. Arnold, and\nBing Xiang. Pairwise supervised contrastive learning of sentence representations. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5786\u2013\n5798, 2021. doi: 10.18653/v1/2021.emnlp-main.467. URL https://aclanthology.org/\n2021.emnlp-main.467.\nZhihan Zhou, Dejiao Zhang, Wei Xiao, Nicholas Dingwall, Xiaofei Ma, Andrew Arnold, and\nBing Xiang.\nLearning dialogue representations from consecutive utterances.\nIn Proceed-\nings of the 2022 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, pp. 754\u2013768, Seattle, United States, July\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.55. URL\nhttps://aclanthology.org/2022.naacl-main.55.\n12\nPublished as a conference paper at ICLR 2024\nA\nDATA, MODEL, AND HYPER-PARAMETERS DETAILS\nA.1\nPRETRAINING DATA\nMasked Language Modeling (MLM) and Identifier Deobsfucation (DOBF)\nFor both MLM\nand DOBF, we use the Stack dataset (Kocetkov et al., 2022). We set the maximum sequence length\nto 1024 with concatenation and block attention.\nContrastive Learning (CL)\nIn CL, we focus on bimodal data, i.e., code and natural language\npairs, denoted as (text, function). Text is extracted as the first sentence from the docstring of a\nfunction (Husain et al., 2019). For better interpretation, we refer to such text as \u201dsummary\u201d in this\nsection as it often summarizes the high-level semantics of a function. We filter or modify summaries\nbased on the following practices.\n1. Filter summary if it is not in English.\n2. Filter summary if the number of tokens in a summary is <3 or >256.\n3. Remove URLs, HTML tags, and doctags from the summaries.\n4. Fix bad Unicode text in the summaries.\n5. Filter functions with no more than one line of code in the function body.\nWe summarize the statistics of our pretraining data at each stage in Figure 1 and Table 4.\nLanguage\nTotal files\n#Functions\n#Func. w/ docstring\n#Func. w/ summary\nPython\n24,214,270\n67,264,716\n24,321,126\n18,146,327\nJava\n42,429,211\n84,828,833\n17,613,636\n13,118,303\nJavascript\n40,112,121\n35,469,803\n7,450,153\n4,796,101\nC#\n21,702,269\n37,284,300\n9,325,665\n7,350,191\nC\n21,383,832\n16,253,435\n4,392,973\n2,958,699\nRuby\n7,205,146\n5,475,047\n1,217,764\n1,049,356\nGO\n11,653,185\n31,067,259\n11,247,051\n9,739,861\nPHP\n34,851,418\n42,373,766\n22,191,329\n13,416,574\nTypescript\n19,589,267\n16,612,988\n2,637,245\n1,863,436\nTotal\n237,961,548\n367,905,026\n105,760,862\n75,389,347\nTable 4: Statistics of the data used in pre-training via Masked Language Modeling (MLM) and\nIdentifier Deobsfucation (DOBF), followed by contrastive learning (CL). The data is collected from\nThe Stack (Kocetkov et al., 2022).\nA.2\nMODEL AND TRAINING HYPER-PARAMETERS\nWe pretrain three sizes of model architecture which we refer to as CODESAGE-SMALL, CODESAGE-\nBASE, and CODESAGE-LARGE. We summarize the model hyper-parameters in Table 5.\nA.3\nON TOKEN DISTRIBUTION AND STAGE-I PRETRAINING OBJECTIVE\nIn our preliminary study, we perform data analysis where we examine the ratio of natural language\n(NL) and programming language (PL) tokens. In a source code, tokens are broadly categorized\ninto five groups: (1) identifiers, (2) keywords, (3) operators, (4) delimiters, and (5) literals. We tag\nthe String literals (i.e., docstring, comments) as NL tokens, while all other tokens are considered\nPL tokens. We use tree-sitter to parse source code and extract the five categories of code tokens.\nThen we tokenize them using Starcoder tokenizer (Li et al., 2023). From Stack-Python corpora, we\ncompute the following statistics using Starcoder tokenized tokens.\n1. Approximate PL Tokens: 57.8% of tokens belong to {identifiers, keywords, delimiters,\noperators}. Among them, 53.8% tokens belong to identifiers and 46.2% are other tokens.\n2. Approximate NL Tokens: 42.2% of tokens Literals {Boolean, Numeric, String}. Among\nthem, 92.9% tokens belong to String literals and 7.1% tokens belong to others.\nAs we can tell from the above numbers, the approximate NL tokens account for roughly 40% of\nthe overall tokens for a particular programming language. Therefore, when replacing masked to-\n13\nPublished as a conference paper at ICLR 2024\nCODESAGE-SMALL\nCODESAGE-BASE\nCODESAGE-LARGE\n#layers\n6\n24\n24\n#heads\n8\n8\n16\nModel dim\n1024\n1024\n2048\nVocab size\n49,152\n49,152\n49,152\nMax sequence length\n1024\n1024\n1024\nTotal parameters\n130M\n356M\n1.3B\nStage1: Masked Language Modeling\nDropout\n0.1\n0.1\n0.1\nMax steps\n250,000\n250,000\n250,000\nWarmup steps\n5000\n5000\n5000\nBatch size\n2048\n2048\n2048\nBase learning rate\n3e-4\n3e-4\n3e-4\nStage2: Contrastive Learning\nDropout\n0.1\n0.1\n0.1\nMax steps\n20,000\n20,000\n20,000\nWarmup steps\n500\n500\n500\nBatch size\n8192\n8192\n8192\nBae learning rate\n5e-06\n5e-06\n5e-06\nTable 5: Model architecture and pre-training related hyper-parameters.\nkens with a random token could result in replacing a PL token with an NL token, and vice versa.\nHowever, there are often no clear boundaries between PL and NL tokens in many scenarios, as PL\ntokens, e.g., those identifier-related tokens, are often expected to carry clear semantics so that the\ncode snippets are interpretable by humans. Therefore, given masked input tokens following the\n80-10-10 convention, it can be a non-trivial task for the model to decide which tokens are from\ncorruption. This together with the structure nature of PL makes it possible for those random tokens\nto largely disrupt both the semantics and structure of code and make the representation learning too\nchallenging to be effective.\nTake the example in Figure 6 (right) for illustration, the function name \u201dbinary search\u201d is being\ncorrupted with random tokens at all three places it appears, which has the potential to alter the\nsemantic meaning of the code. Although we may expect the model to still be able to correctly\nrecover \u201dbinary search\u201d from the corrupted code, it is a challenging task given (1) the syntax of the\ncode has been disrupted by another random token \u201dgetConfig\u201d; (2) the presence of < MASK >\ntokens; and (3) the bidirectional self-attention mechanism can drive the model to leverage those\nrandom tokens to form prediction of the masked tokens.\nFigure 6: A code snippet(on the left) and its corresponding masked version, were created using the\n80-10-10 practice with a 15% masking rate (on the right).\nA.4\nMASKING RATE\nWith \u201dFull Mask\u201d, i.e., MLM without the 80-10-10 corruption strategy, we investigate the optimal\nmasking rate in Figure 7. We consider three constant masking rates, 7.5%, 15%, and 30%, as well\n14\nPublished as a conference paper at ICLR 2024\nas a dynamic masking strategy with the masking rate being randomly selected from the range [10%,\n50%] for each training example. We find 15% remains the optimal masking rate among the four\nvariants we investigate.\n7.5%\n15%\n30%\n10%-50%\n0\n10\n20\n30\nIn-Language Code2Code MAP\n7.5%\n15%\n30%\n10%-50%\n0\n5\n10\n15\n20\nCross-Language Code2Code MAP\nFull Mask\nFigure 7: Maksing rate and zero-shot Code2Code search performance investigated on CODESAGE-\nBASE. We consider three constant masking rates, 7.5%, 15%, and 30%, as well as a dynamic\nmasking strategy with the masking rate being randomly selected from the range [10%, 50%] for\neach training example.\nA.5\nIDENTIFIER OBFUSCATION\nIn this research, we employ an identifier deobfuscation (DOBF) objective to train bidirectional\nencoder representation models. While our inspiration for this approach comes from the DOBF\nmethod introduced by anne Lachaux et al. (2021), our adoption strategy differs from theirs. In\ntheir work, anne Lachaux et al. (2021) trained a sequence-to-sequence language model to recon-\nstruct the original code from an obfuscated version where class, function, and variable names\nwere replaced with special tokens. In contrast, our approach applies this technique to encoder-\nonly models.\nThis adaptation involves a non-trivial effort to establish a 1-1 mapping between\nmask tokens and identifier tokens (will be masked and encoders will predict them) due to dis-\nparities in code tokenization (i.e., using tree-sitter) and model-specific tokenization (i.e., utilizing\na sentencepiece tokenizer). To illustrate, let\u2019s consider the tokenization process. Tree-sitter to-\nkenizes \u201cdef function name():\u201d as {def, function name, (, ), :}, whereas a model-specific tok-\nenizer might tokenize it as {def, function, , name(, ), :}. Consequently, we encounter a challenge\nto construct the mapping from masked tokens to prediction tokens: {[mask], [mask], [mask]} \u2192\n{function, , name}, by skipping \u201c(\u201d token that is part of the identifier token \u201cname\u201d. To perform\nobfuscation and construct the mask map, we developed an obfuscation (OBF) tool.\nOBF Tool\nWe developed a tool that takes an entire source code file or a function as input and\noutputs an identifier obfuscated code along with a token map. We provide an example in Figure\n8. We used tree-sitter to parse a code snippet and extract all identifiers and their ancestor node\ntypes. Based on the node types, we identify class names, function names, function arguments, and\nfunction calls. Then we replace them with special tokens (ci, fi, vi for class names, function names,\nand variable names, respectively). Then we include the special tokens into the model tokenizer\n(i.e., Starcoder tokenizer) and tokenize the obfuscated code such that special tokens are retained in\nthe output tokens. Finally, we use the model tokenizer to tokenize the identifiers individually and\nreplace the special tokens (ci, fi, vi) with the identifier tokens.\nA.6\nON LEXICAL OVERLAP AND HARD POSITIVE DESIGN\nAs detailed in Appendix A.1, we extract the first sentence from the function docstring as the sum-\nmary text. We then examine the lexical overlap between docstring (summary) and function signature\nversus that between docstring (summary) and function body. In Stack-Python corpora, we found -\n1. 22.3% of tokens in function signature and 23.1% of tokens in function body overlap with\ndocstring tokens.\n2. 12.3% of tokens in function signature and 11.6% of tokens in function body overlap with\nsummary tokens.\n15\nPublished as a conference paper at ICLR 2024\n1 c l a s s\nNode :\n2\ndef\ni n i t\n( s e l f ,\nv ) :\n3\ns e l f . data = v\n4\ns e l f . l e f t\n= None\n5\ns e l f . r i g h t\n= None\n6\n7 # Function to print postorder traversal\n8 def\np r i n t P o s t o r d e r ( node ) :\n9\ni f\nnode == None :\n10\nr e t u r n\n11\n12\n# First recur on the left subtree\n13\np r i n t P o s t o r d e r ( node . l e f t )\n14\n15\n# Then recur on the right subtree\n16\np r i n t P o s t o r d e r ( node . r i g h t )\n17\n18\n# Now deal with the node\n19\np r i n t ( node . data ,\nend= '\n' )\n1 c l a s s\nc 0 :\n2\ndef\nf 0 ( v 0 ,\nv 1 ) :\n3\nv 0 . v 2 = v 1\n4\nv 0 . v 3 = None\n5\nv 0 . v 4 = None\n6\n7 # Function to print postorder traversal\n8 def\nf 1 ( v 5 ) :\n9\ni f\nv 5 == None :\n10\nr e t u r n\n11\n12\n# First recur on the left subtree\n13\nf 1 ( v 5 . v 3 )\n14\n15\n# Then recur on the right subtree\n16\nf 1 ( v 5 . v 4 )\n17\n18\n# Now deal with the node\n19\np r i n t ( v 5 . v 2 ,\nend= '\n' )\nFigure 8: An example of a Python code (at the left) and its corresponding obfuscated version\n(at the right) generated by our developed obfuscation tool.\nThe class names, function names,\nand variables are replaced by special tokens.\nGiven the code on the left, our developed OBF\ntool produces the obfuscated code and the identifier map: {c0, v0, v1, v2, v3, v4, v5, f0, f1} \u2192\n{Node, self, v, data, left, right, node,\ninit , printPostorder}.\nThis validates our intuition that the docstring or summary of a function often has a large lexical\noverlap with the function signature. Thereby, when contrasting docstring or summary with the entire\nfunction, the model tends to learn shortcut by leveraging such overlap, and hence fail to capture the\nsemantic equivalence concept in the representations. Consequently, poor generalization is attained.\nB\nEVALUATION OF DOWNSTREAM TASKS\nB.1\nBASELINE MODELS\nWe summarize the baseline model size and output representation dimension in Table 6.\nModel\nModel Size\nEmbedding\nMax Sequence\nTraining\nDimension\nLength\nData Source\nCodeBERT\n125M\n768\n512\nCodeSearchNet\nGraphCodeBERT\n125M\n768\n512\nCodeSearchNet\nStarEncoder\n125M\n768\n1024\nThe Stack\nUnixCoder\n125M\n768\n1024\nCodeSearchNet\nOpenAI-Embedding-Ada-002\nUnknown\n1536\n8191\nUnknown\nTable 6: Model size and dimension of the embeddings. The GitHub Code dataset is available at\n(https://huggingface.co/datasets/codeparrot/github-code).\nB.2\nCODE SEARCH\nWe summarize the data statistics of NL2Code and Code2Code benchmarks in Table 7. Below, we\nprovide more context on each dataset.\nCode2Code search is the task of retrieving relevant code fragments given a code fragment as a query.\nIn this work, we extend the code2code search dataset (Guo et al., 2022) created from CodeNet to six\nmore languages - C, C#, Javascript, Typescript, GO, and PHP. The original dataset includes 2 to 10\nsolutions for each problem in Java, Python, and Ruby. At first, we collect the problem identifiers and\naggregate solutions in those six languages from CodeNet. Also, CodeNet provides cluster identifiers\nfor each solution where solutions within a cluster are near duplicates of each other. We collect one\n16\nPublished as a conference paper at ICLR 2024\nCode2Code Semantic Search Data Statistics in Each Language\nPython\nJava\nJS\nTS\nC#\nC\nRuby\nPHP\nGO\nNum Queries\n15,594\n23,530\n6,866\n3,385\n11,952\n11,260\n11,744\n6,782\n9,720\nNum Candidates\n15,594\n23,530\n6,866\n3,385\n11,952\n11,260\n11,744\n6,782\n9,720\nNL2Code Semantic Search Data Statistics in Benchmark and Language\nCoSQA\nAdvTest\nCSN\nPython\nPython\nPython\nJava\nJS\nPhP\nGo\nRuby\nNum Queries\n500\n19,120\n14,918\n10,955\n3,291\n14,014\n8,122\n1,261\nNum Candidates\n6,268\n19,120\n43,827\n40,347\n13,981\n52,660\n28,120\n4,360\nTable 7: Evaluation data statistics of both NL2Code and Code2Code search.\nNL query: Try loading the given cache file.\n1 # Original Python function\n2 def\nf r o m f i l e ( cls ,\nf i l e ,\n* args ,\n** kwargs ) :\n3\nt r y :\n4\ncache = shelve . open ( f i l e )\n5\nr e t u r n\nc l s ( f i l e ,\ncache ,\n* args ,\n** kwargs )\n6\nexcept\nOSError\nas\ne :\n7\nlogger . debug ( \u201d Loading {0}\nf a i l e d \u201d . format ( f i l e ) )\n8\nr a i s e\ne\n1 # AdvTest Python function\n2 def\nFunc ( arg 0 ,\narg 1 ,\n* arg 2 ,\n** arg 3 ) :\n3\nt r y :\n4\narg 4 = shelve . open ( arg 1 )\n5\nr e t u r n\narg 0 ( arg 1 ,\narg 4 ,\n* arg 2 ,\n** arg 3 )\n6\nexcept\nOSError\nas\ne :\n7\nlogger . debug ( \u201d Loading {0}\nf a i l e d \u201d . format ( arg 1 ) )\n8\nr a i s e\ne\nFigure 9: An example of natural language query and the associated ground truth function from the\nAdvTest dataset. The function names and variables in the original function (at the top) are replaced\nby special tokens (at the bottom) to obfuscate the code.\nsolution from each cluster and randomly pick 2 to 10 solutions per problem. We summarize in-\nlanguage (query and candidates are in the same language) code2code search results in Table 1.\nNL2Code search refers to the task of using natural language as the query to retrieve the relevant\ncode. We consider three benchmarks in this paper. CoSQA where the NL queries (in NL) are\nfrom the search logs of the Bing search engine and the candidate functions are from CodeSearch-\nNet (in Python). Total queries 500 and number of candidates 6,268. CSN is constructed from the\nCodeSearchNet dataset of six programming languages, including Python, Java, JavaScript, PHP, Go,\nand Ruby. AdvTest which normalizes Python functions (from CSN) and variable names to test the\nunderstanding and generalization capabilities of models (an example is shown in Figure 9).\nAdditional Baselines are considered in Tables 8 and 9. We constantly find decoder-only models\nyield poor performance on semantic search. Finetuning or prompt engineering may help improve\nthe performance of decoder-only models, which we leave as future work.\nB.3\nCODE CLASSIFICATION\nWe present the label distribution for the RunTime error prediction dataset in Table 10. We present\nthe hyper-parameters that we used while finetuning models for code classification tasks in Table 11.\nFinetuning models end-to-end on classification tasks\nIn the main body of this paper, we pre-\nsented the evaluation results (in Table 2) of finetuning a linear classification head on top of the\nfrozen code representation learning encoders. Furthermore, we finetune the code encoder models\n17\nPublished as a conference paper at ICLR 2024\nModel\nPython\nJava\nJS\nTS\nC#\nC\nRuby\nPHP\nGO\nAvg\nCodeGen2.5(7B)\n16.5\n10.2\n7\n8.5\n4.2\n8.0\n17.3\n15.6\n9.4\n10.7\nStarcoder(15.5B)\n7.1\n3.9\n3.2\n4.4\n1.7\n2.4\n6.8\n6.1\n3.3\n4.3\nCodeT5+(16B) Encoder\n18.2\n9.9\n5.8\n6.9\n4.2\n8.2\n16.5\n13.9\n8.0\n10.2\nCodeBERT\n14.40\n7.62\n5.47\n6.05\n3.66\n5.53\n13.55\n10.28\n6.27\n8.09\nGraphCodeBERT\n19.23\n10.78\n7.38\n8.65\n5.54\n8.48\n19.69\n15.67\n9.65\n11.68\nStarEncoder\n19.17\n11.65\n9.0\n10.52\n5.69\n9.72\n21.57\n16.98\n10.81\n12.79\nUnixCoder\n30.77\n16.45\n21.32\n21.95\n6.19\n15.62\n32.33\n31.93\n13.94\n21.17\nOpenAI-Embedding-Ada-002\n35.91\n25.13\n19.01\n21.86\n10.17\n29.15\n40.85\n40.47\n23.43\n27.33\nCODESAGE-SMALL\n36.31\n23.97\n26.60\n29.90\n11.84\n22.84\n29.06\n34.64\n19.56\n26.08\nCODESAGE-BASE\n47.52\n22.84\n28.70\n31.95\n13.37\n30.99\n44.86\n51.13\n25.15\n32.95\nCODESAGE-LARGE\n46.70\n33.13\n37.16\n41.18\n16.81\n32.89\n54.12\n52.13\n32.48\n38.51\nTable 8: MAP score (%) of the zero-shot code search task. The language names mentioned in the\ntop row indicate the languages queries and candidates are written in.\nCoSQA\nAdvTest\nCSN\nModel\nPython\nPython\nPython\nJava\nJS\nPhP\nGo\nRuby\nCodeGen2.5 (7B)\n0.02\n0.01\n0.06\n0.02\n0.05\n0.18\n6.03\n2.04\nStarcoder (15.5B)\n0.02\n0.06\n0.03\n0.01\n0.05\n0.59\n0.06\n0.05\nCodeT5+ (16B) Encoder\n22.96\n20.36\n19.93\n14.05\n12.26\n26.08\n20.37\n13.05\nCodeBERT\n0.24\n0.06\n0.05\n0.03\n0.04\n0.02\n0.14\n0.34\nGraphCodeBERT\n16.20\n5.58\n10.37\n8.59\n7.29\n8.07\n12.47\n20.79\nStarEncoder\n10.78\n0.93\n2.81\n2.51\n1.87\n0.74\n2.65\n5.54\nUnixCoder\n42.11\n27.32\n42.17\n43.92\n40.46\n35.21\n61.39\n55.22\nOpenai-Embedding-Ada-002\n44.23\n38.08\n68.02\n71.49\n67.50\n60.62\n85.63\n74.20\nCODESAGE-SMALL\n49.92\n41.28\n64.38\n63.19\n60.01\n54.71\n77.66\n63.20\nCODESAGE-BASE\n48.50\n49.08\n67.99\n68.02\n66.95\n58.15\n83.21\n68.00\nCODESAGE-LARGE\n47.53\n52.67\n70.77\n70.21\n69.50\n61.33\n83.71\n71.92\nTable 9: MRR score (%) of NL2Code search in zero-shot setting.\nTarget Class\nTrain # Valid #\nTest #\nTarget Class\nTrain #\nValid #\nTest #\nNo error\n1,20,503\n13,049\n13,745\nImportError\n259\n37\n22\nZeroDivisionError\n25,087\n3,087\n2,828\nTabError\n74\n4\n3\nOSError\n21540\n2,427\n2,422\nre.error\n62\n6\n11\nUnboundLocalError\n21,414\n2,641\n2,603\nAttributeError\n47\n4\n8\ndecimal\n10,026\n509\n1,674\nStopIteration\n24\n5\n3\nValueError\n8,585\n991\n833\nOverflowError\n19\n2\n2\nAssertionError\n7,816\n1,072\n691\nTimeout\n18\n8\n2\nFileNotFoundError\n7,676\n727\n797\nIndexError\n10\n0\n12\nIndentationError\n7,645\n285\n841\nModuleNotFoundError\n8\n7\n1\nKeyError\n7,505\n965\n733\nRecursionError\n5\n0\n0\nNameError\n1,876\n186\n110\nEOFError\n3\n0\n0\nnumpy.AxisError\n437\n47\n125\nSyntaxError\n3\n0\n1\nMathDomainError\n362\n39\n22\nRuntimeError\n2\n0\n1\nTable 10: Distribution of target classes in the Python Runtime Errors dataset.\nend-to-end on the classification tasks and present the results in Table 12. It\u2019s evident from these\nresults that CODESAGE outperforms the baseline models.\n18\nPublished as a conference paper at ICLR 2024\nHyper-parameters\nFt. linear classification head only\nFt. full model end-to-end\nDefect\nComplexity\nRuntime\nDefect\nComplexity\nRuntime\nOptimizer\nAdamW\nAdamW\nLearning rate (LR)\n1e-3\n5e-5 (baselines)\n1e-5 (CODESAGE-SMALL)\n1e-5(CODESAGE-BASE)\n5e-6(CODESAGE-LARGE)\nLR schedule\nLinear\nLinear\nBatch size\n32\n32\n# Epoch\n10\n10\n2\n5\n5\n2\nTable 11: Hyperparameters for fine-tuning baseline models and CODESAGE on code classification\ntasks. Across all models, we used mean pooling to form sequence representations from contextual-\nized token representations.\nClassification\nModel\nDefect\nComplexity\nRunTime\nCodeBERT\n64.370.37\n85.812.53\n42.082.49\nGraphCodeBERT\n65.361.00\n87.982.45\n44.290.97\nStarEncoder\n65.200.11\n92.871.47\n38.063.86\nCodeT5+ Embedding\n64.720.65\n90.631.47\n38.362.54\nUnixCoder\n65.741.00\n93.750.67\n47.142.71\nCODESAGE-SMALL\n66.140.67\n94.740.29\n44.461.50\nCODESAGE-BASE\n66.520.48\n95.900.43\n46.402.90\nCODESAGE-LARGE\n66.380.23\n96.200.57\n49.253.68\nTable 12: F1 (macro) score of the code classification tasks in the full finetuning setup. We finetuned\nusing three seeds and reported the mean and standard deviation (in subscript).\n19\n"
  },
  {
    "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
    "link": "https://arxiv.org/pdf/2402.01831.pdf",
    "upvote": "11",
    "text": "Audio Flamingo: A Novel Audio Language Model with Few-Shot\nLearning and Dialogue Abilities\nZhifeng Kong\u2020\nArushi Goel\u2020\nRohan Badlani\u2020\nWei Ping\u2020\nRafael Valle\u2020\nBryan Catanzaro\u2020\nAbstract\nAugmenting large language models (LLMs)\nto understand audio \u2013 including non-speech\nsounds and non-verbal speech \u2013 is critically\nimportant for diverse real-world applications\nof LLMs. In this paper, we propose Audio\nFlamingo, a novel audio language model with\n1) strong audio understanding abilities, 2) the\nability to quickly adapt to unseen tasks via in-\ncontext learning and retrieval, and 3) strong\nmulti-turn dialogue abilities. We introduce\na series of training techniques, architecture\ndesign, and data strategies to enhance our\nmodel with these abilities. Extensive evalua-\ntions across various audio understanding tasks\nconfirm the efficacy of our method, setting\nnew state-of-the-art benchmarks.\n1. Introduction\nThe ability to understand sound is unarguably im-\nportant and necessary for an agent to interact with\nthe world. While large language models (LLMs) have\nshown an impressive ability to understand and reason\nabout the world through text, their understanding of\nsound remains limited to transcriptions of speech (Lyu\net al., 2023), thus making LLMs agnostic to impor-\ntant information in non-speech sounds and non-verbal\nspeech. Even though recent contributions have im-\nproved their ability to understand sound (Gong et al.,\n2023c; Lyu et al., 2023; Huang et al., 2023; Deshmukh\net al., 2023; Chu et al., 2023; Tang et al., 2023a),\nthere exists no model that combines: i) strong au-\ndio understanding ability on various tasks (Deshmukh\net al., 2023), ii) the ability to execute multi-turn dia-\nlogues (Duan et al., 2023), and iii) the ability to quickly\nadapt to unseen tasks without fine-tuning, for example,\nthrough in-context learning (Alayrac et al., 2022) and\n\u2020NVIDIA.\nCorrespondence\nto:\nZhifeng\nKong\n(zkong@nvidia.com),\nWei\nPing\n(wping@nvidia.com),\nRafael Valle (rafaelvalle@nvidia.com).\nCopyright 2024 by the author(s).\nretrieval augmented generation (Lewis et al., 2020).\nOur contribution to expand LLM\u2019s ability to under-\nstand sound is called Audio Flamingo: a novel audio\nlanguage model that supports in-context learning (ICL),\nretrieval augmented generation (RAG), and multi-turn\ndialogues. It achieves state-of-the-art results on multi-\nple audio understanding tasks. 1\nExpanding LLM\u2019s ability to understand sound is a\nchallenging task. The first challenge is extracting fea-\ntures from variable-length audio, and conditioning the\nLM on the audio features. While prior works have\ndesigned representations for audio of any length (Wu\net al., 2023), they can lose temporal information. In\nthis work, we introduce an audio feature extractor with\nsliding window based on Elizalde et al. (2023b), which\nwe believe to capture temporal information better. To\ncondition the LM on audio inputs, previous models\nprepended language tokens with audio tokens (Desh-\nmukh et al., 2023; Chu et al., 2023; Tang et al., 2023a).\nThis approach may have excessive overhead especially\nfor long audio, as the complexity is quadratic with\nrespect to the number of audio tokens. In contrast, we\nuse cross attentions to fuse audio inputs into the LM\nin a similar way as Flamingo (Alayrac et al., 2022). In\nour approach the complexity is linear in the number\nof audio tokens, thus making Audio Flamingo able to\ngeneralize to many audio inputs efficiently.\nThe second challenge is collecting and training on highly\nheterogeneous data. Prior works have collected and\ncombined different datasets for training (Doh et al.,\n2023; Deshmukh et al., 2023; Chu et al., 2023; Gong\net al., 2023c). We follow their approach and curate a\nheterogeneous dataset with approximately 5.9 million\naudio-text pairs. Prior works have also designed differ-\nent training curriculum, such as training on close-ended\ntasks first followed by open-ended tasks (Gong et al.,\n2023c; Tang et al., 2023a). However, these result in a\ntrade-off between close-ended and open-ended tasks,\nlimiting the overall performance (Deshmukh et al., 2023;\n1We will open-source the training and inference code. Our\ndemo website is: https://audioflamingo.github.io/.\n1\narXiv:2402.01831v2  [cs.SD]  4 Mar 2024\nAudio Flamingo\nFigure 1. Audio Flamingo versus previous state-of-the-art\n(Deshmukh et al., 2023; Chu et al., 2023; Gong et al.,\n2023b;c; Tang et al., 2023a; Ghosh et al., 2023) on a num-\nber of audio understanding benchmarks. The numbers are\nnormalized such that the maximum of all models is 100%\non each task. Audio Flamingo sets the new state-of-the-art\nresults on most of these tasks.\nTang et al., 2023a). We use a different approach based\non a widely adopted and stable method to train LLMs\n(Ouyang et al., 2022). Specifically, we use two training\nstages: pre-training and supervised fine-tuning (SFT),\neach with different subsets and training techniques.\nThese innovations make Audio Flamingo achieve the\nstate-of-the-art results on several audio understanding\nbenchmarks with < 1\n3 number of parameters as Chu\net al. (2023) and Gong et al. (2023c).\nThe third challenge is to give the audio language model\nthe ability to quickly adapt to new tasks without\nfine-tuning, for instance, via in-context learning (ICL)\n(Brown et al., 2020) and retrieval. While recent audio\nlanguage models have shown zero-shot abilities (Desh-\nmukh et al., 2023; Gong et al., 2023c), they lack the\nability to perform in-context few-shot learning to new\ntasks. In this paper, we introduce a series of techniques\nto realize this ability. We implement an efficient re-\ntrieval method, introduce an ICL template, and use\nretrieved samples to create interleaved ICL datasets\nfor training. We also introduce a novel cross atten-\ntion mask for interleaved samples. As a result, Audio\nFlamingo can be quickly adapted to new tasks via ICL\nand retrieval without task-specific fine-tuning. Our\nresults confirm the efficacy of our approach and set\nnew state-of-the-art few-shot benchmarks.\nThe last challenge is to give the audio language model\nthe ability to chat with a user for many rounds. While\nprior methods have shown demos of dialogues (Gong\net al., 2023c; Chu et al., 2023), they lack systematic\nand quantitative evidence. To address this challenge,\nwe create two multi-turn dialogue datasets with GPT-4\n(Achiam et al., 2023) based on detailed annotations of\ntwo datasets, with an emphasis on correlated context.\nWe obtain a chat model by fine-tuning Audio Flamingo\non these datasets. Our results show that our chat model\nhas strong multi-turn dialogue ability and significantly\noutperforms previous methods.\nWe evaluate Audio Flamingo on a large and diverse\nset of close and open-ended benchmarks.\nA single\nAudio Flamingo model surpasses the previous state-of-\nthe-art on most benchmarks, and the chat version of\nAudio Flamingo significantly outperforms baselines on\ndialogue benchmarks. Figure 1 summarizes the bench-\nmark results of Audio Flamingo. We also briefly discuss\nabout the neural architecture and hyper parameters in\nthe experiments. Our key contributions include:\n1. We propose Audio Flamingo: a Flamingo-based au-\ndio language model for audio understanding with\na series of innovations. Audio Flamingo achieves\nstate-of-the-art results on several close-ended and\nopen-ended audio understanding tasks.\n2. We design a series of methodologies for efficient\nuse of ICL and retrieval, which lead to the state-\nof-the-art few-shot learning results.\n3. We enable Audio Flamingo to have strong multi-\nturn dialogue ability, and show significantly better\nresults compared to baseline methods.\n2. Related work\nMultimodal LLMs.\nThere has been tremendous\nprogress in the area of multimodal LLMs. In addition\nto text, these models take inputs from various modali-\nties such as vision (Tsimpoukelli et al., 2021; Alayrac\net al., 2022; Yang et al., 2023; Driess et al., 2023; Liu\net al., 2023a; Li et al., 2023a), audio (Deshmukh et al.,\n2023; Gong et al., 2023b; Rubenstein et al., 2023), or\nmultiple of them (Han et al., 2023; Tang et al., 2023b;\nMoon et al., 2023; Zhao et al., 2023), and each has a dif-\nferent integration method. In the audio modality, prior\nworks have looked at speech tasks (Chen et al., 2023;\nRubenstein et al., 2023), general audio understanding\n(Deshmukh et al., 2023; Gong et al., 2023c), music\nunderstanding (Gardner et al., 2023; Won et al., 2023;\nLi et al., 2023c; Liu et al., 2023b; Doh et al., 2023), or\na combination of these (Gong et al., 2023b; Tang et al.,\n2023a; Chu et al., 2023). The focus of our paper is\naudio understanding, which includes non-speech sound\nand music, and non-verbal speech. Different from prior\nworks, our model has stronger audio understanding\nability, and is the first audio understanding model with\ni) in-context few-shot learning ability, ii) retrieval aug-\nmented generation ability, and iii) strong multi-turn\ndialogue ability.\n2\nAudio Flamingo\nAudio encoders and representation. Many audio\nencoders extract audio features from the spectrogram,\nincluding CNN-based method (Kong et al., 2020) and\nTransformer-based methods (Gong et al., 2021; Chen\net al., 2022; D\u00b4efossez et al., 2022; Radford et al., 2023;\nGong et al., 2023a). These methods are primarily tar-\ngeted at solving a particular problem such as speech\nrecognition or event detection.\nBased on these en-\ncoders, many joint audio-language embeddings have\nbeen proposed (Elizalde et al., 2023a;b; Wu et al., 2023;\nHuang et al., 2022; Li et al., 2023c). These methods\nuse contrastive learning to map audio and language\nembeddings into the same space, and are often trained\non a large variety of audio and language. However,\nmany of these methods compute a single embedding for\nan audio and therefore may lose temporal information.\nIn this paper, we build an audio encoder with sliding\nwindows based on ClapCap (Elizalde et al., 2023b) to\nbetter capture long-range and temporal information.\nData augmentation. Due to limited amount of high-\nquality human annotated sounds besides speech tran-\nscriptions, many works have proposed to augment tex-\ntural description with existing LLMs such as GPT-4\n(Achiam et al., 2023). A common strategy is to provide\nan LLM with annotated tags, timestamps, and other\nmiscellaneous information, and then ask it to generate\ncaptions (Wu et al., 2023; Doh et al., 2023; Mei et al.,\n2023; Gardner et al., 2023) or question-answering data\npairs (Gong et al., 2023c;b; Liu et al., 2023b). In this\npaper, we leverage existing LLMs to generate two multi-\nturn dialogue datasets based on detailed annotations,\nwhich enable our model strong dialogue abilities.\nIn-context learning (ICL). In-context learning is\na kind of few-shot learning ability, where an LLM\nrapidly adapts to a desired task at inference time only\nafter looking at a few examples in the prompt (Brown\net al., 2020). It has widely shown success in natural\nlanguage tasks (Wei et al., 2021) and visual-language\ntasks (Alayrac et al., 2022; Yang et al., 2023). In the\nspeech domain, ICL has been shown to help speech-\nrelated tasks such as speech recognition, translation,\nand processing (Gao et al., 2022; Wang et al., 2023;\nHsu et al., 2023; Chen et al., 2023). However, ICL for\ngeneral audio understanding is much less explored. In\nthis paper, we propose the first audio understanding\nmodel with ICL ability.\nRetrieval-augmented\ngeneration\n(RAG).\nRetrieval-augmented generation for LLMs is to im-\nprove generation quality by using external knowledge,\nfor example from an external database, which contains\nuseful and related knowledge.\nIt has been widely\napplied in natural language tasks (Guu et al., 2020;\nKarpukhin et al., 2020; Lewis et al., 2020; Borgeaud\net al., 2022) and visual-language models (Yang\net al., 2023). In the audio-language domain, Ghosh\net al. (2023) proposed a retrieval method for audio\ncaptioning by prepending captions from similar audios\nto the prompt.\nHowever, it does not provide the\nretrieved audio to the model.\nConsequently, the\nmodel loses information on how similar the retrieved\naudio is to the test audio. In contrast, we provide\nboth the retrieved audio and text to our model. The\nbenefit of this approach is that our model could\ndetermine when and how to use the retrieval based on\nthe similarity between test and retrieved audio. We\nprovide comparisons in our few-shot experiments.\n3. Method\nIn this section, we introduce Audio Flamingo, an audio-\nunderstanding language model with few-shot learning\nvia ICL and RAG. In Section 3.1, we introduce the ar-\nchitecture used in Audio Flamingo, including the audio\nfeature extractor, audio representation transformation\nlayers, language model, and the conditioning method.\nIn Section 3.2, we introduce the training method of Au-\ndio Flamingo, including the training objective, design\nof masks, and training stages.\n3.1. Architecture\nOur neural architecture is composed of four compo-\nnents: i) an audio feature extractor with sliding win-\ndow, ii) audio representation transformation layers, iii)\na decoder-only language model, and iv) gated xattn-\ndense layers. Figure 2 summarizes the architecture.\ni) Audio feature extractor with sliding window.\nWe use ClapCap (Elizalde et al., 2023b) as the au-\ndio feature extractor backbone, which we denote as E.\nClapCap takes 7-second 44.1kHz raw audio as input,\nthen transforms the audio into Mel-spectrogram of hop\nlength 320, window length 1024, 64 Mel bins, and finally\noutputs a 1024-dimensional vector representation.\nWe consider each 7-second segment as a window and\nuse sliding windows to extract features for longer\naudio. The overlap between consecutive windows is\n7 \u00d7 0.75 = 5.25 seconds. We use a maximum of m = 16\nsliding windows. Formally, let x(s:t) be the segment of\ns to t seconds in audio x. Then, the extracted feature is\nh\nE(x(0:7)), E(x( 7\n4: 7\u00d75\n4 )), \u00b7 \u00b7 \u00b7 , E(x( 7(m\u22121)\n4\n: 7(m+3)\n4\n)\ni\n,\nwhich supports a maximum of 33.25 seconds. The goal\nof this design is to capture long-range and temporal\ninformation that might be ignored in a single fused\nrepresentation vector (Wu et al., 2023). Long audio\nwill be cropped and short audio will be zero-padded.\n3\nAudio Flamingo\nFigure 2. Neural architecture of Audio Flamingo. It takes interleaved audio and text as input and outputs free-form text.\nIf an entire segment is zero-padded then we will mask\nthe corresponding embedding at cross attention. If\nthe input is interleaved data with > 1 audio, we\nconcatenate their sliding window representations.\nii) Audio representation transformation layers.\nWe increase model capacity by further applying a few\naudio representation transformation layers to the con-\ncatenated audio feature representations described ear-\nlier. It is comprised of 3 self-attention layers (Vaswani\net al., 2017), with 8 heads and inner dimension 2048\neach. This module is fully trainable.\niii) Language model.\nWe use a decoder-only\ncausal LM in our architecture. In this paper, we use\nOPT-IML-MAX-1.3B (Iyer et al., 2022), a 1.3B parame-\nter model with 24 LM blocks. It has been instruction-\ntuned on many natural language tasks.\niv) Conditioning LM on audio representations.\nWe use the gated xattn-dense layers from Flamingo\n(Alayrac et al., 2022) to achieve conditioning on audio\ninputs. Each layer has two blocks: 1) a residual block\nwith cross attention and tanh gating, followed by 2) a\nresidual block with dense layer and tanh gating. These\nlayers are prepended to each LM block.\n3.2. Training Method\ni) Training objective. Let x be the mono-channel\naudio input, yins be the instruction text (e.g. question),\nand yout be the output text. For conciseness we use\nz = (x, yins, yout) to represent each training sample.\nWe use maximum likelihood estimation (MLE) to train\nour model. Let (yout)t be the t-th token and (yout)<t\nthe first t\u22121 tokens of the output. For a non-interleaved\nsample z = (x, yins, yout), the log-likelihood is\nL(z) =\n|yout|\nX\nt=1\nlog p\u03b8 ((yout)t|x, yins, (yout)<t) .\n(1)\nFor an interleaved training sample composed of J sam-\nples zint = {z1, \u00b7 \u00b7 \u00b7 , zJ}, where zj = (xj, yj\nins, yj\nout), the\nlog-likelihood is computed over all outputs:\nLint(zint = {z1, \u00b7 \u00b7 \u00b7 , zJ}) =\nJ\nX\nj=1\n|yj\nout|\nX\nt=1\nlog p\u03b8\n\u0010\n(yj\nout)t|z<j, xj, yj\nins, (yj\nout)<t\n\u0011\n.\n(2)\nNote this interleaved data loss is different from prior\nmodels, which compute losses only on the last output\nyJ\nout (Yang et al., 2023), or have either none or indirect\nconditioning on prior multimodal inputs x<j (Alayrac\net al., 2022; Ghosh et al., 2023). We expect (2) can help\nthe model look at a various number (including zero\nwhen j = 1) of ICL samples as well as the associated\naudio, thus improving robustness and training efficiency\nin a similar way as bucketing (Khomenko et al., 2016),\nespecially when the ICL samples are retrieved similar\nsamples. The corresponding loss mask is shown on the\nright-hand-side of Figure 3.\nLet {Di, i \u2208 I} be all non-interleaved training datasets,\nand {Di\u2032\nint, i\u2032 \u2208 Iint} be all interleaved training datasets.\nThe overall training objective is a weighted mixture of\nlosses on each dataset:\nL = \u2212\nX\ni\u2208I\n\u03bbiEz\u223cDiL(z) \u2212\nX\ni\u2032\u2208Iint\n\u03bbi\u2032Ezint\u223cDi\u2032\nintLint(zint),\n(3)\nwhere \u03bbi\u2019s are the weights for each dataset. The weights\nare constant hyper-parameters and have a huge impact\non the final model. They are computed from the pre-\ndefined number of epochs for each dataset (see Section\n4.1 for the intuition, and Appendix A for details).\nii) Cross attention masks. We use block triangular\ncross attention masks for interleaved samples so that\nthe likelihood of j-th output p\u03b8(yj\nout) is conditioned\nonly on the first j audio inputs x\u2264j. We expect this\nhelps making the model to look at previous audio.\nFigure 3 demonstrates the mask.\niii) Two training stages. We divide training into\npre-training and supervised fine-tuning (SFT), a widely\n4\nAudio Flamingo\nFigure 3. Left: the block triangular cross attention masks\nbetween text tokens and audio embeddings. Right: the loss\nmask of an interleaved training sample.\nadopted and stable method in training LMs (Ouyang\net al., 2022). During pre-training we only train the au-\ndio representation transformation layers and the gated\nxattn-dense layers. The purpose is to obtain a good set\nof initialization weights for these layers. During SFT,\nwe unfreeze the entire LM, and train all modules but\nthe audio encoder. 2\n4. Data\n4.1. Datasets\nIn this section, we introduce our data strategies, in-\ncluding dataset collection, generation, and blending.\nWe also introduce templates for each type of dataset.\nDataset sources. We train our model on a variety\nof audio datasets that can be roughly classified into\nthree types: music, non-speech general sound, and\nnon-verbal speech. In this paper, we focus on these\ndata given the immediate availability of state-of-the-art\nspeech transcription models. We look at three types\nof tasks: (1) audio captioning (CAP), where we would\nlike the model to describe the audio in a sentence; (2)\naudio question-answering (AQA), where we would like\nthe model to answer questions regarding the audio,\nand (3) audio classification (CLS), where we would\nlike the model to classify the sound into one or more\nlabels corresponding to events, scenes, music genres,\ninstruments, qualities, and others. An overview of all\ntraining datasets is shown in Table 6 in the Appendix.\nICL datasets. In order to give our model in-context\nlearning and retrieval augmentation abilities, we con-\nstruct ICL datasets for each of the raw datasets based\non kNN computed on audio embeddings. Let Di be the\ni-th training dataset. For each z = (x, yins, yout) \u2208 Di,\n2In initial experiments we found unfreezing the audio en-\ncoder led to much worse results.\nFigure 4. Construction of ICL samples based on RAG. We\nuse LAION-CLAP to find top-k most similar samples from\nthe database, and use the retrieved audio and text to con-\nstruct an ICL training sample.\nwe find its top-k closest training samples in Di exclud-\ning z, where the distance function is \u21132 in the fused\nLAION-CLAP embedding space (Wu et al., 2023) for\nthe audio part of the sample. We use Faiss-gpu (John-\nson et al., 2019) to accelerate searching.\nFigure 4\ndemonstrates this process.\nDataset staging and blending. We use different\ndatasets during the pre-training stage and the super-\nvised fine-tuning (SFT) stage. The selection is based\non data quality, diversity, source, and size as described\nbelow. 1) Data quality: low quality datasets, includ-\ning those with low-quality or noisy audio, low-quality\ntext, and inaccurate text annotation, are used for pre-\ntraining. 2) Data diversity: datasets with less diver-\nsity or strong biases in label distributions are used\nfor pre-training. 3) Data sources: datasets containing\nAI-generated contents are mostly used for pre-training,\nwhereas some high-quality subsets may be used for\nSFT. 4) Data sizes: very large datasets may be used\nboth for pre-training and SFT. 5) ICL datasets are\nused in the SFT stage.\nWe assign different weights \u03bbi when sampling from\ndifferent datasets based on their sizes, quality, and\ndiversity. The weights are computed from the number\nof epochs for each dataset. The details of staging and\nweights can be found in Appendix A.\n4.2. Templates\nOur templates are based on OPT-IML\u2019s template (Iyer\net al., 2022) and Flamingo\u2019s multimodal template\n(Alayrac et al., 2022). For a non-interleaved sample,\nthe template is describe below.\n5\nAudio Flamingo\n<s>{task description}<audio>{instruction}\nOptions:\\n- option1\\n\u00b7 \u00b7 \u00b7 - optionm\n<SEP>{output}<EOC></s>\nIn this template, <audio> is the special token that\ninforms the language model the location of audio in the\ncontext. The {task description} is natural language\nthat tells the language model which task it is handling,\nfor example \u201cThe task is event classification\u201d. The\n{instruction} is the language instruction such as a\nquestion. The options sentence is to tell the language\nmodel all options for classification so that it can classify\nan audio by outputting free-form text. The {output}\nis the ground truth output that will be trained. The\n<SEP> token is a separator that indicates the end of\ninstruction, and <EOC> is the end-of-chunk token that\nindicates the end of a sample. Below is the template for\ninterleaved (ICL) samples with k + 1 tuples of (audio,\ninstruction, output).\n<s>{task description}Here are similar samples.\n<audio>{instruction1}<SEP>{output1}<EOC>\n\u00b7 \u00b7 \u00b7\n<audio>{instructionk}<SEP>{outputk}<EOC>\n<audio>{instruction}\nOptions:\\n- option1\\n\u00b7 \u00b7 \u00b7 - optionm\n<SEP>{output}<EOC></s>\n4.3. Multi-Turn Dialogue Dataset\nWe aim at giving our model stronger abilities in\ndealing with complicated multi-turn dialogues.\nTo\nachieve this, we use GPT-4 (Achiam et al., 2023)\nto generate two multi-turn dialogue datasets.\nWe\nconstruct these datasets based on the strongly la-\nbeled AudioSet-SL (Hershey et al., 2021) and Mu-\nsicCaps (Agostinelli et al., 2023), with threshold-\ning based on LAION-CLAP embeddings (Wu et al.,\n2023) to filter undesirable samples. We name these\ntwo generated datasets AF-Dialogue-AudioSetSL and\nAF-Dialogue-MusicCaps, respectively. The detailed\ninstructions, filtering method, dataset statistics, and\nexamples are in Appendix B. We use the following\ntemplate for an s-turn dialogue data sample.\n<s>The task is dialogue.<audio>\nuser:\n{instruction1}\nassistant:\n<SEP>{output1}<EOC>\n\u00b7 \u00b7 \u00b7\nuser:\n{instructions}\nassistant:\n<SEP>{outputs}<EOC></s>\n5. Experiments\nIn this section, we answer the following questions:\nQ1. Does Audio Flamingo understand audio better\nthan the state-of-the-art baselines?\nQ2. How significantly does the ICL-based RAG help\nAudio Flamingo adapt to new tasks?\nQ3. What is Audio Flamingo\u2019s ability to have multi-\nturn dialogues with a user?\nQ4. Which specific configuration of Audio Flamingo\nworks the best overall?\n5.1. Experimental Setup\nWe use 8 NVIDIA A100 GPUs to train our model.\nDuring pre-training, we use batch size = 384, AdamW\noptimizer (Loshchilov & Hutter, 2017) with learning\nrate = 1 \u00d7 10\u22124 and weight decay = 0.1. For efficiency,\nwe use bf16 with automatic mixed precision. During\nsupervised fine-tuning (SFT), we reduce the batch size\nto 128, the learning rate to 2 \u00d7 10\u22125, and use fp32\nfor better numerical precision. We let the maximum\nnumber of interleaved samples to be 8 unless specified.\nWe set the maximum number of text tokens to be 512.\nWe compare to the most recent state-of-the-art base-\nlines on several benchmarks.\nOn each dataset, we\nchoose the best score among all SOTA baselines as the\nreference value. Unless specified, we report accuracy\nfor question-answering and single-label classification,\nF1 for multi-label classification, and CIDEr (Vedantam\net al., 2015) for captioning and dialogues. Note we use\nfree-form text output to evaluate our model at all times.\nWe use a single model to evaluate on all benchmarks\nexcept for dialogues, and a chat model on dialogues.\nFor zero-shot and few-shot benchmarks, these datasets\nare excluded from the pre-training sets and SFT sets.\nFor those derived from a parent dataset (e.g. Audio-\nCaps audio are derived from AudioSet), we removed\nthe training samples from the parent set as well as\nother child sets derived from that parent set.\n5.2. Q1: Strong Audio Understanding Ability\nWe evaluate our model on several in-distribution (train-\ntest) benchmarks, and compare to state-of-the-art\naudio language model baselines. The results are shown\nin Table 1. Note that we define F1approx to measure\ninexact but similar predicted labels in FSD50k,\nwhere we consider the prediction to be correct if\nSentenceBERT similarity(output, ground truth)\n> 0.8 (Reimers & Gurevych, 2019; 2020). This metric\nis applied to outputs from baselines as well.\nAudio Flamingo can match or outperform SOTA base-\nlines \u2013 many of which are much larger LLMs (7B\n(Gong et al., 2023c;b; Chu et al., 2023) or 13B (Tang\net al., 2023a)) \u2013 on most tasks, indicating our proposed\nmethod has strong audio understanding ability. Our\nmodel also listens to the audio better. On ClothoAQA,\nour model has 10.4% higher accuracy than baselines\n6\nAudio Flamingo\nTable 1. Evaluation of Audio Flamingo versus SOTA baseline methods on in-distribution benchmarks. Reference values\nare the SOTA models for each task. Audio Flamingo shows strong audio understanding ability on captioning, question\nanswering, and audio classification.\nDataset\nTask\nMetric\nPrevious SOTA \u2191\nOurs \u2191\nClotho-v2\nCAP\nCIDEr\n0.441 (Chu et al., 2023)\n0.465\nClothoAQAunanimous\nAQA\nACC\n74.9% (Chu et al., 2023)\n86.9%\nClothoAQAnon-binary\nAQA\nACC\n29.1% (Deshmukh et al., 2023)\n49.5%\nClothoAQAnumerical\nAQA\nACC\n26.2% (Deshmukh et al., 2023)\n36.4%\nMusicAVQAaudio-only\nAQA\nACC\n72.1% (Chu et al., 2023)\n71.6%\nCochlScene\nCLS\nACC\n91.6% (Deshmukh et al., 2023)\n83.0%\nNonSpeech17k\nCLS\nACC\n79.0% (Rashid et al., 2023)\n85.1%\nFSD50k\nCLS\nF1approx\n65.6% (Deshmukh et al., 2023)\n69.7%\nNSinstrument\nCLS\nACC\n78.8% (Chu et al., 2023)\n77.1%\nNSquality\nCLS\nF1\n46.3% (Deshmukh et al., 2023)\n66.7%\nNSsource\nCLS\nACC\n60.1% (Deshmukh et al., 2023)\n78.7%\nAverage (besides CAP)\n62.4%\n70.5%\nTable 2. Evaluation of Audio Flamingo versus SOTA baseline methods on zero-shot benchmarks. Reference values are the\nSOTA models for each task. Audio Flamingo shows strong zero-shot generalization ability.\nDataset\nTask\nMetric\nPrevious SOTA (0-shot) \u2191\nOurs (0-shot) \u2191\nAudioCaps (Kim et al., 2019)\nCAP\nCIDEr\n0.281 (Salewski et al., 2023)\n0.502\nCREMA-D (Cao et al., 2014)\nCLS\nACC\n18.5% (Deshmukh et al., 2023)\n26.5%\nRavdess (Livingstone & Russo, 2018)\nCLS\nACC\n21.7% (Elizalde et al., 2023b)\n20.9%\nUS8K (Salamon et al., 2014)\nCLS\nACC\n71.9% (Deshmukh et al., 2023)\n75.0%\nGTZAN (Sturm, 2013)\nCLS\nACC\n71.0% (Han et al., 2023)\n67.9%\nMedley-solos-DB (Lostanlen et al., 2019)\nCLS\nACC\n61.3% (Deshmukh et al., 2023)\n92.7%\nAverage (besides CAP)\n48.9%\n56.6%\non numerical question answering, indicating our model\nunderstands the number of occurrences better. On\nNSynth, our model has 20.4% higher F1 on quality pre-\ndiction and 18.6% higher accuracy on source prediction,\nindicating our model understands the overall quality\nof audio better. In Appendix C.2, we use qualitative\nsamples to show that our model understands the order\nof appearance of different sounds, perceives loudness\nand its change over time, and perceives the distances\nof sounds from different objects.\n5.3. Q2: In-Context Few-Shot Learning\nWe aim to measure the effect of ICL-based RAG in Au-\ndio Flamingo when it is evaluated on unseen datasets.\nFirst, we report the results on several zero-shot bench-\nmarks and comparison with SOTA zero-shot methods\nin Table 2. The results indicate our method is better\non most tasks and has strong generalization ability.\nWe then apply ICL-based RAG to these benchmarks.\nWe compare to our zero-shot results and the SOTA\nbaseline of audio captioning on AudioCaps. The results\non classification are shown in Table 3, and the compari-\nson on retrieval-augmented audio captioning is shown in\nTable 4. As expected, there is consistent improvement\nover zero-shot results, with an average improvement\nover 10% for classification. Our method also signifi-\ncantly outperforms the SOTA retrieval-augmented au-\ndio captioning method on AudioCaps. In Appendix\nC.1, we show Audio Flamingo can adapt to unseen\nlabels. In Appendix C.3, we show Audio Flamingo\nlooks at related retrieval (e.g., by taking key words\nfrom retrieved captions), and ignores noisy retrieval.\n5.4. Q3: Multi-Turn Dialogues\nWe measure Audio Flamingo\u2019s ability to answer ques-\ntions in a multi-turn dialogue setting. The context is\nmore complex and strongly correlated between rounds\n(e.g. there exist many pronouns and follow-up ques-\ntions).\nWe fine-tune Audio Flamingo on the two\nsets that we generated (AF-Dialogue-AudioSetSL and\nAF-Dialogue-MusicCaps) to obtain a chat model. We\nevaluate the chat model on the test split of these two\ndialogue datasets. We take user instructions and let the\n7\nAudio Flamingo\nTable 3. Evaluation of few-shot results of Audio Flamingo\nwith ICL-based RAG. \u2206 is the absolute improvement of\nfew-shot over zero-shot results in Table 2. ICL-based RAG\nleads to consistent improvement over zero-shot results.\nDataset\nOurs (8-shot) \u2191\n\u2206 \u2191\nCREMA-D\n31.8%\n5.3%\nRavdess\n35.2%\n14.3%\nUS8K\n94.7%\n19.4%\nGTZAN\n79.5%\n11.6%\nMedley-solos-DB\n95.7%\n3.0%\nAverage\n67.4%\n10.7%\nTable 4. Evaluation of retrieval-augmented audio captioning\non AudioCaps. We compare Audio Flamingo to the SOTA\nbaseline RECAP (Ghosh et al., 2023). Audio Flamingo\nachieves significantly better results than RECAP.\nMethod\nRECAP\nOurs\nOurs\n# Shots\n4\n4\n8\nCIDEr \u2191\n0.362\n0.518\n0.538\nTable 5. Evaluation of Audio Flamingo versus baseline\nmethods\non\nthe\nmulti-turn\ndialogue\ntest\nsets.\nA\nstands for AF-Dialogue-AudioSetSL and M stands for\nAF-Dialogue-MusicCaps.\nWe report CIDEr, Bleu4 (Pa-\npineni et al., 2002), and Rouge-L (R-L) (Lin, 2004). Audio\nFlamingo significantly outperforms larger baseline models,\nindicating strong dialogue ability of our proposed model.\nTestset\nMethod\nCIDEr \u2191\nBleu4 \u2191\nR-L \u2191\nA\nLTU\n0.580\n0.122\n0.324\nA\nQwen-Audio\n0.507\n0.060\n0.292\nA\nOurs\n1.622\n0.237\n0.473\nM\nLTU\n0.168\n0.065\n0.217\nM\nOurs\n1.143\n0.142\n0.417\nmodel generate answers turn-by-turn (where previous\ngenerated answers become the chatting history for next\ngeneration). We compare to Qwen-Audio (Chu et al.,\n2023) and LTU (Gong et al., 2023c) in Table 5. 3 Our\nchat model achieves significantly better results than\nbaseline methods. In Appendix C.4, we use qualitative\nsamples to show that our chat model captures context\nsuch as prior information and pronouns better.\n5.5. Q4: Ablation Studies\nEffect of number of few-shot samples. We reduce\nthe number of in-context few-shot samples to 4 and\nevaluate on the few-shot benchmarks. In Figure 5, we\nplot the relative improvements over zero-shot results:\n(ACCfew-shot \u2212 ACC0-shot)/ACC0-shot. We find adding\n3While the baseline methods claimed to support multi-turn\ndialogues, we were unable to find quantitative evidence.\n0\n4\n8\nNumber of ICL Samples\n0.0%\n5.0%\n10.0%\n15.0%\n20.0%\n25.0%\nImprovement over Zero-Shot\nDataset\nMedley-solos-DB\nAudioCaps\nCREMA-D\nUS8K\nGTZAN\nFigure 5. Relative improvement of few-shot results over zero-\nshot results under different number of ICL samples. Using\nmore ICL samples consistently improves few-shot results,\nand the benefit is dataset-dependent.\nmore ICL samples consistently improves few-shot re-\nsults, and the benefit is dataset-dependent.\nEffect of LM. We found using an instruction-tuned\nLLM to be key to the success of audio understanding.\nIn early experiments we investigated opt-1.3b/2.7b\n(Zhang et al., 2022) and phi-1.5 (Li et al., 2023b) and\nfound the audio understanding results to be slightly\nbut consistently worse.\nEffect of audio encoder. In early experiments, we\ntested all variants of LAION-CLAP (Wu et al., 2023)\nand Microsoft-CLAP (Elizalde et al., 2023a;b) as the\naudio encoder backbone. We found ClapCap (Elizalde\net al., 2023b) performs the best overall \u2013 in terms of\nboth evaluation results and computing speed \u2013 whereas\na large LAION-CLAP trained on more music data led\nto better accuracy on music information retrieval.\n6. Conclusion and Future Work\nIn this paper, we present Audio Flamingo, an audio lan-\nguage model with a series of innovations that achieves\nthe state-of-the-art results on several close-ended and\nopen-ended audio understanding tasks without task\nspecific fine-tuning. It also has strong ICL and RAG\nabilities, and has the state-of-the-art few-shot learning\nresults. Furthermore, we design a dataset generation\nstrategy and introduce two dialogue datasets, and en-\nable Audio Flamingo to chat with a user and achieve\nstate-of-the-art results on dialogue benchmarks.\nOne important future direction is to investigate scaling\nstrategies for using larger LMs. Assuming that larger\nLMs could have better knowledge and stronger ability\nto follow instructions, we believe that Audio Flamingo\ncould benefit from a larger LM. A second future di-\nrection is to investigate complex speech-related tasks\nbeyond transcription. A third future direction is to\ncombine the audio understanding abilities of our model\nwith visual language models such that one model could\nunderstand video with accompanied audio.\n8\nAudio Flamingo\nAcknowledgement\nWe thank Siddharth Gururani, Zihan Liu, Mostofa Pat-\nwary, Shrimai Prabhumoye, and Chen Zhu for helpful\ndiscussions. We thank Ke Chen and Yuan Gong for\nhelp on sharing datasets.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,\nI., Aleman, F. L., Almeida, D., Altenschmidt, J.,\nAltman, S., Anadkat, S., et al.\nGpt-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nAdigwe, A., Tits, N., Haddad, K. E., Ostadabbas,\nS., and Dutoit, T. The emotional voices database:\nTowards controlling the emotion dimension in voice\ngeneration systems. arXiv preprint arXiv:1806.09514,\n2018.\nAgostinelli, A., Denk, T. I., Borsos, Z., Engel, J.,\nVerzetti, M., Caillon, A., Huang, Q., Jansen,\nA., Roberts, A., Tagliasacchi, M., et al.\nMusi-\nclm: Generating music from text. arXiv preprint\narXiv:2301.11325, 2023.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr,\nI., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\nReynolds, M., et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural In-\nformation Processing Systems, 35:23716\u201323736, 2022.\nBarros, P., Churamani, N., Lakomkin, E., Siqueira, H.,\nSutherland, A., and Wermter, S. The omg-emotion\nbehavior dataset. In 2018 International Joint Con-\nference on Neural Networks (IJCNN), pp. 1\u20137. IEEE,\n2018.\nBogdanov, D., Won, M., Tovstogan, P., Porter, A., and\nSerra, X. The mtg-jamendo dataset for automatic\nmusic tagging. ICML, 2019.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T.,\nRutherford, E., Millican, K., Van Den Driessche,\nG. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Im-\nproving language models by retrieving from trillions\nof tokens. In International conference on machine\nlearning, pp. 2206\u20132240. PMLR, 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., et al. Language models are\nfew-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\nCao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C.,\nNenkova, A., and Verma, R.\nCrema-d:\nCrowd-\nsourced emotional multimodal actors dataset. IEEE\ntransactions on affective computing, 5(4):377\u2013390,\n2014.\nCartwright, M., Mendez, A. E. M., Cramer, A.,\nLostanlen, V., Dove, G., Wu, H.-H., Salamon, J.,\nNov, O., and Bello, J. Sonyc urban sound tagging\n(sonyc-ust): A multilabel dataset from an urban\nacoustic sensor network. 2019.\nChen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick,\nT., and Dubnov, S. Hts-at: A hierarchical token-\nsemantic audio transformer for sound classification\nand detection. In ICASSP 2022-2022 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 646\u2013650. IEEE, 2022.\nChen, Z., Huang, H., Andrusenko, A., Hrinchuk, O.,\nPuvvada, K. C., Li, J., Ghosh, S., Balam, J., and\nGinsburg, B.\nSalm: Speech-augmented language\nmodel with in-context learning for speech recognition\nand translation. arXiv preprint arXiv:2310.09424,\n2023.\nChu, Y., Xu, J., Zhou, X., Yang, Q., Zhang, S.,\nYan, Z., Zhou, C., and Zhou, J. Qwen-audio: Ad-\nvancing universal audio understanding via unified\nlarge-scale audio-language models. arXiv preprint\narXiv:2311.07919, 2023.\nDefferrard, M., Benzi, K., Vandergheynst, P., and Bres-\nson, X. Fma: A dataset for music analysis. arXiv\npreprint arXiv:1612.01840, 2016.\nD\u00b4efossez, A., Copet, J., Synnaeve, G., and Adi, Y. High\nfidelity neural audio compression. arXiv preprint\narXiv:2210.13438, 2022.\nDeshmukh, S., Elizalde, B., and Wang, H.\nAudio\nretrieval with wavtext5k and clap training. arXiv\npreprint arXiv:2209.14275, 2022.\nDeshmukh, S., Elizalde, B., Singh, R., and Wang, H.\nPengi: An audio language model for audio tasks.\narXiv preprint arXiv:2305.11834, 2023.\nDoh, S., Choi, K., Lee, J., and Nam, J. Lp-musiccaps:\nLlm-based pseudo music captioning. arXiv preprint\narXiv:2307.16372, 2023.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdh-\nery, A., Ichter, B., Wahid, A., Tompson, J., Vuong,\nQ., Yu, T., et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378,\n2023.\nDrossos, K., Lipping, S., and Virtanen, T. Clotho:\nAn audio captioning dataset. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech\n9\nAudio Flamingo\nand Signal Processing (ICASSP), pp. 736\u2013740. IEEE,\n2020.\nDuan, H., Wei, J., Wang, C., Liu, H., Fang, Y., Zhang,\nS., Lin, D., and Chen, K. Botchat: Evaluating llms\u2019\ncapabilities of having multi-turn dialogues. arXiv\npreprint arXiv:2310.13650, 2023.\nElizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H.\nClap learning audio concepts from natural language\nsupervision. In ICASSP 2023-2023 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 1\u20135. IEEE, 2023a.\nElizalde, B., Deshmukh, S., and Wang, H. Natural\nlanguage supervision for general-purpose audio rep-\nresentations, 2023b. URL https://arxiv.org/abs/\n2309.05767.\nEngel, J., Resnick, C., Roberts, A., Dieleman, S., Eck,\nD., Simonyan, K., and Norouzi, M. Neural audio\nsynthesis of musical notes with wavenet autoencoders,\n2017.\nFonseca, E., Favory, X., Pons, J., Font, F., and Serra,\nX. Fsd50k: an open dataset of human-labeled sound\nevents. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:829\u2013852, 2021.\nFoster, P., Sigtia, S., Krstulovic, S., Barker, J., and\nPlumbley, M. D. Chime-home: A dataset for sound\nsource recognition in a domestic environment. In\n2015 IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics (WASPAA), pp. 1\u20135.\nIEEE, 2015.\nGao, H., Ni, J., Qian, K., Zhang, Y., Chang, S., and\nHasegawa-Johnson, M. Wavprompt: Towards few-\nshot spoken language understanding with frozen\nlanguage models. arXiv preprint arXiv:2203.15863,\n2022.\nGardner, J., Durand, S., Stoller, D., and Bittner, R. M.\nLlark: A multimodal foundation model for music.\narXiv preprint arXiv:2310.07160, 2023.\nGemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,\nLawrence, W., Moore, R. C., Plakal, M., and Rit-\nter, M. Audio set: An ontology and human-labeled\ndataset for audio events. In 2017 IEEE international\nconference on acoustics, speech and signal processing\n(ICASSP), pp. 776\u2013780. IEEE, 2017.\nGhosh, S., Kumar, S., Evuru, C. K. R., Duraiswami,\nR., and Manocha, D. Recap: Retrieval-augmented\naudio captioning. arXiv preprint arXiv:2309.09836,\n2023.\nGong,\nY.,\nChung,\nY.-A.,\nand Glass,\nJ.\nAst:\nAudio spectrogram transformer.\narXiv preprint\narXiv:2104.01778, 2021.\nGong, Y., Khurana, S., Karlinsky, L., and Glass, J.\nWhisper-at: Noise-robust automatic speech recog-\nnizers are also strong general audio event taggers.\narXiv preprint arXiv:2307.03183, 2023a.\nGong, Y., Liu, A., Luo, H., Karlinsky, L., and Glass,\nJ. Joint audio and speech understanding. In IEEE\nAutomatic Speech Recognition and Understanding\nWorkshop, 2023b.\nGong, Y., Luo, H., Liu, A. H., Karlinsky, L., and Glass,\nJ. Listen, think, and understand. arXiv preprint\narXiv:2305.10790, 2023c.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.\nRetrieval augmented language model pre-training.\nIn International conference on machine learning, pp.\n3929\u20133938. PMLR, 2020.\nHan, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao,\nH., Zhang, K., Liu, C., Wen, S., Guo, Z., et al.\nImagebind-llm: Multi-modality instruction tuning.\narXiv preprint arXiv:2309.03905, 2023.\nHershey, S., Ellis, D. P., Fonseca, E., Jansen, A., Liu,\nC., Moore, R. C., and Plakal, M. The benefit of\ntemporally-strong labels in audio event classifica-\ntion.\nIn ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pp. 366\u2013370. IEEE, 2021.\nHsu, M.-H., Chang, K.-W., Li, S.-W., and Lee, H.-\ny. An exploration of in-context learning for speech\nlanguage model. arXiv preprint arXiv:2310.12477,\n2023.\nHuang, Q., Jansen, A., Lee, J., Ganti, R., Li, J. Y.,\nand Ellis, D. P.\nMulan:\nA joint embedding of\nmusic audio and natural language. arXiv preprint\narXiv:2208.12415, 2022.\nHuang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z.,\nWu, Y., Hong, Z., Huang, J., Liu, J., et al. Audiogpt:\nUnderstanding and generating speech, music, sound,\nand talking head. arXiv preprint arXiv:2304.12995,\n2023.\nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig,\nD., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura,\nP. S., et al. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017, 2022.\n10\nAudio Flamingo\nJames, J., Tian, L., and Watson, C. An open source\nemotional speech corpus for human robot interaction\napplications. Interspeech 2018, 2018.\nJeong, I.-Y. and Park, J. Cochlscene: Acquisition of\nacoustic scene data using crowdsourcing. In 2022\nAsia-Pacific Signal and Information Processing As-\nsociation Annual Summit and Conference (APSIPA\nASC), pp. 17\u201321. IEEE, 2022.\nJohnson, J., Douze, M., and J\u00b4egou, H. Billion-scale\nsimilarity search with GPUs. IEEE Transactions on\nBig Data, 7(3):535\u2013547, 2019.\nKarpukhin, V., O\u02d8guz, B., Min, S., Lewis, P., Wu, L.,\nEdunov, S., Chen, D., and Yih, W.-t. Dense passage\nretrieval for open-domain question answering. arXiv\npreprint arXiv:2004.04906, 2020.\nKhomenko, V., Shyshkov, O., Radyvonenko, O., and\nBokhan, K. Accelerating recurrent neural network\ntraining using sequence bucketing and multi-gpu data\nparallelization. In 2016 IEEE First International\nConference on Data Stream Mining & Processing\n(DSMP), pp. 100\u2013103. IEEE, 2016.\nKim, C. D., Kim, B., Lee, H., and Kim, G. Audio-\ncaps: Generating captions for audios in the wild.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pp. 119\u2013132,\n2019.\nKong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W.,\nand Plumbley, M. D. Panns: Large-scale pretrained\naudio neural networks for audio pattern recognition.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 28:2880\u20132894, 2020.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\nV., Goyal, N., K\u00a8uttler, H., Lewis, M., Yih, W.-t.,\nRockt\u00a8aschel, T., et al. Retrieval-augmented gener-\nation for knowledge-intensive nlp tasks. Advances\nin Neural Information Processing Systems, 33:9459\u2013\n9474, 2020.\nLi, G., Wei, Y., Tian, Y., Xu, C., Wen, J.-R., and Hu,\nD. Learning to answer questions in dynamic audio-\nvisual scenarios. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pp. 19108\u201319118, 2022.\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-\nstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023a.\nLi, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gu-\nnasekar, S., and Lee, Y. T. Textbooks are all you\nneed ii: phi-1.5 technical report.\narXiv preprint\narXiv:2309.05463, 2023b.\nLi, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H.,\nLin, C., Ragni, A., Benetos, E., Gyenge, N., et al.\nMert: Acoustic music understanding model with\nlarge-scale self-supervised training. arXiv preprint\narXiv:2306.00107, 2023c.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out,\npp. 74\u201381, 2004.\nLipping, S., Sudarsanam, P., Drossos, K., and Virtanen,\nT. Clotho-aqa: A crowdsourced dataset for audio\nquestion answering. In 2022 30th European Signal\nProcessing Conference (EUSIPCO), pp. 1140\u20131144.\nIEEE, 2022.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual in-\nstruction tuning. arXiv preprint arXiv:2304.08485,\n2023a.\nLiu, S., Hussain, A. S., Sun, C., and Shan, Y. Mu-\nsic understanding llama: Advancing text-to-music\ngeneration with question answering and captioning.\narXiv preprint arXiv:2308.11276, 2023b.\nLivingstone, S. R. and Russo, F. A.\nThe ryerson\naudio-visual database of emotional speech and song\n(ravdess): A dynamic, multimodal set of facial and\nvocal expressions in north american english. PloS\none, 13(5):e0196391, 2018.\nLoshchilov, I. and Hutter, F. Decoupled weight de-\ncay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nLostanlen, V., Cella, C.-E., Bittner, R., and Essid,\nS. Medley-solos-DB: a cross-collection dataset for\nmusical instrument recognition, February 2019. URL\nhttps://doi.org/10.5281/zenodo.1344103.\nLotfian, R. and Busso, C. Building naturalistic emo-\ntionally balanced speech corpus by retrieving emo-\ntional speech from existing podcast recordings. IEEE\nTransactions on Affective Computing, 10(4):471\u2013483,\n2017.\nLyu, C., Wu, M., Wang, L., Huang, X., Liu, B., Du,\nZ., Shi, S., and Tu, Z. Macaw-llm: Multi-modal\nlanguage modeling with image, audio, video, and\ntext integration. arXiv preprint arXiv:2306.09093,\n2023.\n11\nAudio Flamingo\nMartin Morato, I. and Mesaros, A. Diversity and bias\nin audio captioning datasets. 2021.\nMei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C.,\nPlumbley, M. D., Zou, Y., and Wang, W. Wavcaps:\nA chatgpt-assisted weakly-labelled audio caption-\ning dataset for audio-language multimodal research.\narXiv preprint arXiv:2303.17395, 2023.\nMohanty,\nS.\nP.\nSound\nOf\n114\nSpecies\nOf\nBirds\nTill\n2022.\nURL\nhttps://www.\nkaggle.com/datasets/soumendraprasad/\nsound-of-114-species-of-birds-till-2022.\nMoon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith,\nM., Jain, S., Yeh, C.-F., Murugesan, P., Heidari, P.,\nLiu, Y., et al. Anymal: An efficient and scalable any-\nmodality augmented language model. arXiv preprint\narXiv:2309.16058, 2023.\nOncescu, A.-M., Koepke, A., Henriques, J. F., Akata,\nZ., and Albanie, S. Audio retrieval with natural\nlanguage queries. arXiv preprint arXiv:2105.02192,\n2021.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wain-\nwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., et al. Training language models\nto follow instructions with human feedback.\nAd-\nvances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J.\nBleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meet-\ning of the Association for Computational Linguistics,\npp. 311\u2013318, 2002.\nPark, J., Cho, Y., Sim, G., Lee, H., and Choo, J. Enemy\nspotted: In-game gun sound dataset for gunshot clas-\nsification and localization. In 2022 IEEE Conference\non Games (CoG), pp. 56\u201363. IEEE, 2022.\nPichora-Fuller, M. K. and Dupuis, K. Toronto emo-\ntional speech set (TESS), 2020. URL https://doi.\norg/10.5683/SP2/E8H2MF.\nPoria, S., Hazarika, D., Majumder, N., Naik, G., Cam-\nbria, E., and Mihalcea, R. Meld: A multimodal\nmulti-party dataset for emotion recognition in con-\nversations. arXiv preprint arXiv:1810.02508, 2018.\nRadford, A., Kim, J. W., Xu, T., Brockman, G.,\nMcLeavey, C., and Sutskever, I.\nRobust speech\nrecognition via large-scale weak supervision. In In-\nternational Conference on Machine Learning, pp.\n28492\u201328518. PMLR, 2023.\nRafii, Z., Liutkus, A., St\u00a8oter, F.-R., Mimilakis, S. I.,\nand Bittner, R.\nMusdb18-hq - an uncompressed\nversion of musdb18, August 2019. URL https://\ndoi.org/10.5281/zenodo.3338373.\nRashid, M. M., Li, G., and Du, C.\nNonspeech7k\ndataset: Classification and analysis of human non-\nspeech sound. IET Signal Processing, 17(6):e12233,\n2023.\nReimers, N. and Gurevych, I.\nSentence-bert: Sen-\ntence embeddings using siamese bert-networks. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics, 11 2019. URL\nhttps://arxiv.org/abs/1908.10084.\nReimers, N. and Gurevych, I. Making monolingual\nsentence embeddings multilingual using knowledge\ndistillation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 11 2020.\nURL https://arxiv.org/abs/2004.09813.\nRubenstein, P. K., Asawaroengchai, C., Nguyen, D. D.,\nBapna, A., Borsos, Z., Quitry, F. d. C., Chen, P.,\nBadawy, D. E., Han, W., Kharitonov, E., et al. Au-\ndiopalm: A large language model that can speak and\nlisten. arXiv preprint arXiv:2306.12925, 2023.\nSalamon, J., Jacoby, C., and Bello, J. P. A dataset\nand taxonomy for urban sound research. In Proceed-\nings of the 22nd ACM international conference on\nMultimedia, pp. 1041\u20131044, 2014.\nSalewski, L., Fauth, S., Koepke, A., and Akata, Z. Zero-\nshot audio captioning with audio-language model\nguidance and audio context keywords. arXiv preprint\narXiv:2311.08396, 2023.\nSturm, B. L. The gtzan dataset: Its contents, its faults,\ntheir effects on evaluation, and its future use. arXiv\npreprint arXiv:1306.1461, 2013.\nTang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W.,\nLu, L., Ma, Z., and Zhang, C. Salmonn: Towards\ngeneric hearing abilities for large language models.\narXiv preprint arXiv:2310.13289, 2023a.\nTang, Z., Yang, Z., Khademi, M., Liu, Y., Zhu, C.,\nand Bansal, M. Codi-2: In-context, interleaved, and\ninteractive any-to-any generation. arXiv preprint\narXiv:2311.18775, 2023b.\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\nVinyals, O., and Hill, F. Multimodal few-shot learn-\ning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021.\n12\nAudio Flamingo\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,\nI. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\nVedantam, R., Lawrence Zitnick, C., and Parikh, D.\nCider: Consensus-based image description evaluation.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 4566\u20134575, 2015.\nWang, S., Yang, C.-H. H., Wu, J., and Zhang, C. Can\nwhisper perform speech-based in-context learning.\narXiv preprint arXiv:2309.07081, 2023.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W.,\nLester, B., Du, N., Dai, A. M., and Le, Q. V. Fine-\ntuned language models are zero-shot learners. arXiv\npreprint arXiv:2109.01652, 2021.\nWon, M., Hung, Y.-N., and Le, D.\nA founda-\ntion model for music informatics. arXiv preprint\narXiv:2311.03318, 2023.\nWu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick,\nT., and Dubnov, S. Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-\ncaption augmentation. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\nYang, Z., Ping, W., Liu, Z., Korthikanti, V., Nie, W.,\nHuang, D.-A., Fan, L., Yu, Z., Lan, S., Li, B., et al.\nRe-vilm: Retrieval-augmented visual language model\nfor zero and few-shot image captioning. In EMNLP,\n2023.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nZhao, Z., Guo, L., Yue, T., Chen, S., Shao, S., Zhu,\nX., Yuan, Z., and Liu, J.\nChatbridge: Bridging\nmodalities with large language model as a language\ncatalyst. arXiv preprint arXiv:2305.16103, 2023.\n13\nAudio Flamingo\nA. Dataset Staging, Weights, and Templates\nTable 6 includes an overview of datasets (by type) we use to train Audio Flamingo.\nTable 6. All datasets used to train our model. The total number of audio-text pairs is approximately 5.9 million. The total\nlength of audio is approximately 18.1 thousand hours.\nAudio Type\nTask\nDatasets\n#Audio-Text Pairs\nGeneral\nSound\nCAP\nWavCaps (Mei et al., 2023), Macs (Martin Morato & Mesaros, 2021),\n\u223c829 K\nSoundDescs (Oncescu et al., 2021), Clotho-v2 (Drossos et al., 2020),\nWavText5K (Deshmukh et al., 2022), LAION-630k (Wu et al., 2023)\nAQA\nClotho-AQA (Lipping et al., 2022), Open-AQA (Gong et al., 2023b)\n\u223c1970 K\nCLS\nAudioSet (Gemmeke et al., 2017), FSD50k (Fonseca et al., 2021),\n\u223c1091 K\nCochlScene (Jeong & Park, 2022), NonSpeech7K (Rashid et al., 2023),\nChime-Home (Foster et al., 2015), Sonyc-UST (Cartwright et al., 2019)\nMusic\nCAP\nLP-MusicCaps (Doh et al., 2023), MusicCaps (Agostinelli et al., 2023)\n\u223c1389 K\nAQA\nMusicQA (Liu et al., 2023b), MusicAVQA (Li et al., 2022)\n\u223c94 K\nCLS\nNSynth (Engel et al., 2017), MTG-Jamendo (Bogdanov et al., 2019),\n\u223c459 K\nFMA (Defferrard et al., 2016), MusDB-HQ (Rafii et al., 2019),\nSpeech\nCLS\nMSP-Podcast (Lotfian & Busso, 2017), Emov-DB (Adigwe et al., 2018)\n\u223c92 K\nJL-Corpus (James et al., 2018), Tess (Pichora-Fuller & Dupuis, 2020),\nMELD (Poria et al., 2018), OMGEmotion (Barros et al., 2018)\nWe construct instructions for each task and dataset. Below are all instruction templates we use.\nAudio Captioning:\n\u25e6 Describe the sound/music in a sentence.\n\u25e6 Describe the sound/music at length.\nAudio Question Answering:\n\u25e6 {question}\n\u25e6 Please answer this question:\n{question}\n\u25e6 Please answer this question:\n{question}.\nOptions:\\n- yes\\n- no\nAudio Classification:\n\u25e6 Classify this sound.\n(Options:\n...)\n\u25e6 Describe the sound in {number} words.\n\u25e6 What is the emotion of this speech?\n(Options:\n...)\n\u25e6 What is the instrument/genre of this music?\n(Options:\n...)\n\u25e6 This music note is produced by\n14\nAudio Flamingo\nThe detailed pre-training datasets and their number of epochs are shown in Table 7. The detailed SFT datasets\nand their number of epochs are shown in Table 8.\nTable 7. Pre-training datasets and epochs.\nDataset\nAudio Length\n#Audio-Text Pairs\nEpochs\nOpenAQA\n693.2 hrs\n1959.8K\n1.0\nLaion630kBBCSoundEffects\n456.9 hrs\n15.1K\n5.0\nLaion630kFreesound\n2494.8 hrs\n306.5K\n1.0\nSoundDescs\n749.7 hrs\n23.1K\n1.0\nWavCaps\n3793.3 hrs\n402.6 K\n1.75\nAudioSet\n2617.8 hrs\n950.8K\n1.0\nWavText5K\n23.8 hrs\n4.3K\n3.0\nMSP-Podcast\n73.9 hrs\n45.1K\n1.2\nMELD\n8.7 hrs\n32.9K\n2.4\nMusicAVQAaudio-visual\n142.4 hrs\n17.9K\n3.0\nMusicQA\n62.9 hrs\n70K\n1.2\nLP-MusicCapsMSD\n5805.7 hrs\n1331.8K\n1.0\nNSynth\n321.3 hrs\n289.2K\n0.4\nMTG-Jamendo\n3768.9 hrs\n55.6K\n1.0\nTable 8. SFT datasets and epochs.\nDataset\nAudio Length\n#Audio-Text Pairs\nEpochs\nICL Dataset Epochs\nClothoAQA\n7.4 hrs\n9.7K\n3.5\n0.5\nOpenAQA\n693.2 hrs\n1959.8K\n0.1\n-\nClotho-v2\n24.0 hrs\n19.2K\n2.0\n0.5\nLaion630kEpidemic\n209.4 hrs\n40.7K\n0.8\n0.2\nMACS\n10.9 hrs\n17.3K\n0.8\n0.2\nFSD50k\n80.8 hrs\n41.0K\n0.9\n0.3\nCochlScene\n169.0 hrs\n60.9K\n1.2\n0.3\nNonSpeech 7k\n6.2 hrs\n6.3K\n2.4\n0.6\nChime-home\n5.0 hrs\n4.5K\n1.5\n0.5\nSonyc-UST\n34.9 hrs\n27.9K\n0.8\n0.2\nEmov-DB\n7.8 hrs\n6.8K\n1.6\n0.4\nJL-Corpus\n1.4 hrs\n2.4K\n6.0\n1.5\nTess\n1.6 hrs\n2.8K\n2.0\n0.5\nOMGEmotion\n3.0 hrs\n1.7K\n3.0\n-\nMusicAVQAaudio-only\n77.1 hrs\n5.7K\n5.0\n1.0\nMusicQA\n62.9 hrs\n70K\n0.35\n0.05\nLP-MusicCapsMSD\n5805.7 hrs\n1331.8K\n0.025\n0.007\nLP-MusicCapsMTT\n126.4 hrs\n46.9K\n0.8\n0.2\nLP-MusicCapsMC\n7.4 hrs\n7.9K\n2.0\n-\nMusicCaps\n7.4 hrs\n2.6K\n6.0\n-\nNSynth\n321.3 hrs\n289.2K\n1.0\n1.0\nMTG-Jamendo\n3768.9 hrs\n55.6K\n0.1\n-\nMusDB-HQ\n29.1 hrs\n10.2K\n1.0\n-\nFMA\n860.7 hrs\n104.2K\n0.4\n0.1\n15\nAudio Flamingo\nB. Generated dialogue datasets\nB.1. Overview\nIn this section, we introduce methods to generate our AF-Dialogue-AudioSetSL and AF-Dialogue-MusicCaps\ndatasets with GPT-4 (Achiam et al., 2023). AF-Dialogue-AudioSetSL is generated based on the annotated\nevents and timestamps of strongly labeled AudioSet-SL (Gemmeke et al., 2017; Hershey et al., 2021). There are\n76k dialogues in the train split and 1.5k dialogues in test split. AF-Dialogue-MusicCaps is generated based on\ntags and descriptions of MusicCaps (Agostinelli et al., 2023). There are 3.3k dialogues in the train split and 1.4k\ndialogues in the test split. Each training sample has one to four rounds (QA pairs), and each testing sample has\nthree or four rounds.\nB.2. Prompts\nBelow we show the prompts used to generate AF-Dialogue-AudioSetSL and AF-Dialogue-MusicCaps.\nsys prompt: Based on the sound events, create a dialogue between you (the assistant) and a person (the user)\nabout the events in the audio. Each dialogue should consist of:\n1. A user examines the audio, and sends a reasonable and creative message to the assistant. 2. Once the audio is\nprovided, the assistant thoroughly perceives and comprehends them, responding with helpful answers that provide\ncomprehensive reasoning. Do not include timestamps in the answer provided by the assistant. 3. Considering the\npast dialogue i.e. the question and the answer in the previous timestep, the user chooses another question or\nfurther inquiry. 4. The assistant provides a helpful answer providing comprehensive reasoning regarding the audio\ncontent.\nThe audios are 10 seconds long and the sound events are segmented based on where they occur in the audio. All\nthe questions asked by the user should be diverse and complex.\nExample 1:\nSound Events: Sound of Ambulance (siren) (High-pitched and wailing): [0.0s-1.0s]; Sound of\nTraffic noise, roadway noise (Droning, loud and intrusive): [0.0s-10.0s]; Sound of Accelerating, revving, vroom\n(Highpitched, short and intense): [2.0s-10.0s]; Sound of Generic impact sounds (Loud and sharp): [6.7s-6.8s].\nDialogue:\nUser: What does the audio sound like?\nAssistant: An ambulance siren echoes while traffic noise fades, and an engine revs.\nUser: Can you describe in more detail about the engine revs?\nAssistant: The engine revving is highpitched, short and intense.\nUser: From the highpitched sound, can you tell me what vehicle it was?\nAssistant: It is hard to tell this from the information provided.\nUser: Are there any other events in the audio that might be relevant?\nAssistant: There is a sound of impact which is very loud and sharp.\nExample 2:\nSound Events: Sound of Outside, rural or natural: [0.000s-10.000s]; Sound of Liquid: [0.474s-\n0.680s], [4.249s-5.292s]; Sound of Gurgling, bubbling: [0.474s-0.680s], [4.249s-5.292s]; Sound of Splash, splatter:\n[1.620s-3.116s]; Sound of Laughter: [2.745s-4.029s], [5.299s-10.000s]; Sound of Chuckle, chortle: [2.752s-4.029s];\nSound of Speech: [4.551s-4.935s]; Sound of Male speech, man speaking: [4.557s-4.935s]; Sound of Belly laugh:\n[5.292s-10.000s]\nDialogue:\nUser: What are the main events in the audio?\nAssistant: Liquid sounds, laughter, speech, and a man speaking can be heard in a rural setting.\nUser: Is is he speaking to other people?\nAssistant: As there is sound of laughter present from other people, the man might be speaking to others.\nUser: Can you explain in more detail about the first event?\nAssistant: The liquid sounds are more of splashing, thrashing and splattering sounds.\nUser: Are there other people laughing with him?\nAssistant: Yes, there are other laughter voices along with that of the man.\nBased on the above examples, design a four-turn dialogue between the user and assistant for the example given\nbelow. The question asked by the user should use pronouns such as \u2018he\u2019, \u2018she\u2019, \u2018it\u2019, \u2018they\u2019 and \u2018them\u2019 while referring\nto entities that have already occurred in the past questions or answers. Format each QA pair in a single line as a\nJSON dictionary (key \u201cuser\u201d for question, and \u201cassistant\u201d for answer, wrapped with { and }). Do not include any\nother explanation.\nExample 3:\n16\nAudio Flamingo\nsys prompt:\nBased on the music, create a dialogue between you (the assistant) and a person (the user) about the\nevents in the music audio. Each dialogue should consist of:\n1. A user examines the music, and sends a reasonable and creative message to the assistant. 2. Once the music\naudio is provided, the assistant thoroughly perceives and comprehends them, responding with helpful answers\nthat provide comprehensive reasoning. Do not include timestamps in the answer provided by the assistant.\n3. Considering the past dialogue i.e. the question and the answer in the previous timestep, the user chooses\nanother question or further inquiry. 4. The assistant provides a helpful answer providing comprehensive reasoning\nregarding the music content. The more complex and diverse the question, the better. You can use the list of\nquestions below while asking questions.\nList of questions:\nIs it a live recording or a studio recording?\nIs the guitar acoustic or electric?\nIs it playing chords, a riff or a solo?\nWhat music style or genre is it?\nWhat key is it in?\nHow about the time signature?\nWhat\u2019s the tempo? Slow, Medium or Fast?\nDoes it have vocals?\nIs it low pitched or high pitched?\nIs the person speaking fast or slow?\nWhat is the language and accent?\nWhat emotion is conveyed by the voice?\nWhat are the vocal characteristics of that emotion?\nHow old is the person likely to be given their vocal characteristics?\nExample 1:\nMusic description: House music with a thought provoking message in the beginning that eventually builds up and\ncan be used to create the sense of a social revolution.\nDialogue:\nUser: What type of music is this?\nAssistant: It is house music with synth and drums and a man speaking at the beginning.\nUser: What is his tone while speaking?\nExample 2: He has an intense and energetic tone in his voice.\nUser: Does he speak till the end?\nAssistant: No, he stops speaking halfway and then it is followed by house music.\nUser: What is the source of this music?\nAssistant: This is electronic music.\nExample 2:\nMusic description: Energetic bluesy song with a harmonica and horn section in musical dialogue.\nDialogue:\nUser: What instruments are playing in this music?\nAssistant: This is a jazzy tune with harmonica, trumpet and drums playing.\nUser: Which key is the music playing in?\nAssistant: The key is A Major.\nUser: How will you explain the tempo of this piece?\nAssistant: The music is bouncy and positive with high beats per minute in the beginning and then changes\nsignature halfway through.\nUser: What is the source of this music?\nAssistant: This is fusion jazz as it has elements of bass and synthesizer.\nBased on above examples, design a four-turn dialogue between the user and assistant for the example given below.\nThe question asked by the user should use pronouns such as \u2018he\u2019, \u2018she\u2019, \u2019it\u2019, \u2019they\u2019 and \u2018them\u2019 while referring to\nentities that have already occurred in the past questions or answers. Format each QA pair in a single line as a\nJSON dictionary (key \u201cuser\u201d for question, and \u201cassistant\u201d for answer, wrapped with { and }). Do not include any\nother explanation.\nExample 3:\n17\nAudio Flamingo\nB.3. Dialogue filtering\nThe dialogues generated by GPT-4 as discussed in Appendix B.2 do not always follow the prompts, resulting in\nanswers that have phrases such as \u201cdoes not specify\u201d, \u201ccannot be determined\u201d, \u201cwithout additional context\u201d and\nso on. Hence, following Gardner et al. (2023), we filter such QA pairs to improve the data quality and ensure\ndesirable outputs from the model. Apart from the manual filtering step, we also filter samples based on the\nsimilarity of the answer generated by GPT-4 and the audio samples. Specifically, we compute the cosine similarity\nbetween the LAION-CLAP text-embeddings and audio-embeddings (Wu et al., 2023) for a given QA pair in each\ndialogue. The distributions of similarities are shown in Figure 6. We remove samples if the similarity is below 0.3.\nFigure 6. Distributions of LAION-CLAP similarities before filtering.\nB.4. Examples of generated dialogues\n[Annotations:\nSound of Mechanisms (Produces sound through mechanical means.):\n[0.119s-10.000s]; Sound of Generic impact sounds (Short, sharp, and loud bursts.):\n[0.217s-0.387s], [2.394s-2.727s], [2.889s-3.255s], [3.799s-4.132s], [4.473s-4.766s],\n[4.928s-5.188s], [8.129s-10.000s]; Sound of Glass shatter (High-pitched, sharp, sudden,\nloud crash.):\n[1.622s-4.652s]; Sound of Walk, footsteps (Soft, rhythmic, echoing thuds.):\n[5.724s-6.106s], [6.455s-6.650s], [7.024s-7.479s], [7.625s-7.901s]]\nUser: Can you describe the main events in the audio?\nAssistant: The audio mainly consists of mechanical sounds, sharp impact sounds, the shattering of glass, and the\nsound of footsteps.\nUser: What can you tell me about the impact sounds?\nAssistant: The impact sounds are short, sharp, and loud bursts. They occur at several points throughout the\naudio, suggesting some kind of repetitive action or event.\nUser: What about the glass shattering, does it happen more than once?\nAssistant: The glass shattering sound is a high-pitched, sharp, sudden, loud crash. It occurs once in the audio,\nlasting for about three seconds.\n[Annotations:\nThis is the recording of a cowbell.\nThe cowbell is being played with the side\nand the tip of the drumstick repeatedly.\nThis recording could be sampled for use in beat-making\nespecially for genres that have a heavy usage of cowbells such as latin dance music.]\nUser: What instrument is being played in this audio?\nAssistant: The instrument being played in this audio is a cowbell.\nUser: How is it being played?\nAssistant: The cowbell is being played with the side and the tip of the drumstick repeatedly.\nUser: What is the tone of the cowbell?\nAssistant: The tone of the cowbell is bright, resonant, and penetrating.\n18\nAudio Flamingo\nC. Additional Examples and Discussion of Audio Flamingo\nWe refer readers to visit our demo website: https://audioflamingo.github.io/ for comprehensive demos.\nC.1. Additional Few-shot Experiments on Unseen Labels\nAudio Flamingo can be adapted to classification with completely unseen labels with the help of ICL-based RAG.\nIn Table 9 we demonstrate Audio Flamingo can be adapted to new classification tasks where none of the labels\nhave been seen during training. The labels are bird species and gun names, respectively. The huge improvement\nfrom zero-shot to few-shot indicates Audio Flamingo\u2019s strong ability to follow retrieved in-context samples.\nTable 9. Accuracy of Audio Flamingo on new classification tasks with unseen labels.\nDataset\nZero-shot\nFew-shot\nBG-Gun-Sound (Park et al., 2022)\n1.6%\n53.5%\nBirds-Dataset-Till-2022 (Mohanty)\n0.0%\n42.2%\nThe labels for BG-Gun-Sound are:\nNo gun, SLR, M4, VSS, AK, SKS, PP, P90, AUG, SCAR, UMP, QBZ, P92, M249, QBU, DEAGLE, TOMY,\nPUMP, P1911, K2, DP, GRO, AWM, VEC, UZI, M24, KAR, VERL, MINI, WIN, MK, R45, M16, P118C,\nG36C, S12K, DBS, R1895.\nThe labels for Birds-Dataset-Till-2022 are:\nBrazilian Tinamou, Red-legged Tinamou, Hooded Tinamou, Chestnut-bellied Guan, Dwarf\nCassowary, Little Spotted Kiwi, Dwarf Tinamou, Baudo Guan, Little Chachalaca, Pale-browed\nTinamou, Lesser Rhea, Australian Brushturkey, Berlepsch\u2019s Tinamou, Spotted Nothura,\nPuna Tinamou, Brushland Tinamou, Andean Guan, Black-billed Brushturkey, Blue-throated\nPiping Guan, Somali Ostrich, Wattled Brushturkey, White-bellied Chachalaca, Barred\nTinamou, Chaco Chachalaca, Rufous-headed Chachalaca, Plain Chachalaca, Emu, Southern\nCassowary, White-bellied Nothura, Undulated Tinamou, Tawny-breasted Tinamou, Thicket\nTinamou, Bearded Guan, Quebracho Crested Tinamou, Scaled Chachalaca, Micronesian Megapode,\nNicobar Megapode, White-throated Tinamou, Malleefowl, Crested Guan, Chestnut-winged\nChachalaca, Speckled Chachalaca, Ornate Tinamou, Curve-billed Tinamou, Tataupa Tinamou,\nWhite-crested Guan, Southern Brown Kiwi, Sula Megapode, Biak Scrubfowl, Vanuatu Megapode,\nPatagonian Tinamou, Moluccan Megapode, Variegated Tinamou, White-browed Guan, Marail Guan,\nChestnut-headed Chachalaca, Northern Cassowary, Bartlett\u2019s Tinamou, Rusty-margined Guan,\nSpix\u2019s Guan, Band-tailed Guan, Andean Tinamou, Trinidad Piping Guan, Elegant Crested Tinamou,\nNorth Island Brown Kiwi, Cinereous Tinamou, White-winged Guan, Grey-headed Chachalaca,\nOkarito Kiwi, Rufous-bellied Chachalaca, Red-winged Tinamou, Rufous-vented Chachalaca,\nBlack-fronted Piping Guan, Tanimbar Megapode, Choco Tinamou, Great Spotted Kiwi, Little\nTinamou, Slaty-breasted Tinamou, Tepui Tinamou, Greater Rhea, Chilean Tinamou, Cauca\nGuan, Yellow-legged Tinamou, Black Tinamou, Lesser Nothura, Rusty Tinamou, Grey-legged\nTinamou, Dusky Megapode, Red-billed Brushturkey, Dusky-legged Guan, Collared Brushturkey,\nCommon Ostrich, Colombian Chachalaca, Melanesian Megapode, Maleo, New Guinea Scrubfowl,\nRed-throated Piping Guan, Huayco Tinamou, Buff-browed Chachalaca, Black-capped Tinamou,\nPhilippine Megapode, Great Tinamou, Orange-footed Scrubfowl, Brown Tinamou, Solitary Tinamou,\nTaczanowski\u2019s Tinamou, Grey Tinamou, Small-billed Tinamou, Tongan Megapode, Highland Tinamou,\nWest Mexican Chachalaca, East Brazilian Chachalaca, Red-faced Guan, Darwin\u2019s Nothura.\n19\nAudio Flamingo\nC.2. Audio Flamingo understands sound in depth\nAudio Flamingo understands occurrences in audio.\n[Subjective description of the audio:\nthree people talking in order, with a lot of noise in the\nrecording.]\nTask: Audio Question Answering\nInstruction: How many different voices are there?\nGround Truth: Three.\nBaseline: One.\nAudio Flamingo: Three.\nAudio Flamingo understands the order of sounds.\n[Subjective description of the audio:\na man speaks on a talk show, then the audience laugh and\nclap their hands.]\nTask: Audio Captioning\nInstruction: Describe this sound.\nGround Truth: A man is speaking followed by applause.\nBaseline: A crowd of people are speaking and applauding.\nAudio Flamingo: A man is speaking and then the audience claps.\nAudio Flamingo understands quality of audio.\n[Subjective description of the audio:\na beautiful and sad piece of instrumental music starting\nwith keyboard playing chords slowly followed by soft violin.]\nTask: Audio Question Answering\nInstruction: What is the atmospheric quality of the audio?\nGround Truth: The atmospheric quality of the audio is its ability to create a sense of space and depth, with a focus\non creating a sense of mystery and wonder.\nBaseline: The audio has a rich and dynamic quality, which suggests a lively and engaging atmosphere, possibly\nassociated with music or a musical performance, but it is not specified in the given audio events.\nAudio Flamingo: The audio has a melancholic and introspective atmosphere.\nAudio Flamingo perceives loudness and its change over time.\n[Subjective description of the audio:\na man speaks in a small volume, then there is very loud\nburst, and then the man laughs for very short time in a surprised way.]\nTask: Audio Captioning\nInstruction: Describe the sound in a sentence.\nGround Truth: A man briefly talks followed by a loud burst then laughter.\nBaseline: A person is laughing and a gun is fired.\nAudio Flamingo: A man is speaking and then a loud bang occurs.\nAudio Flamingo understands distances of sounds.\n[Subjective description of the audio:\na man speaks in the rain, then another man laughs very\nslightly, followed by the sound of thunderstorm.]\nTask: Audio Captioning\nInstruction: Describe this sound.\nGround Truth: A man talking followed by another man laughing as rain falls and thunder roars in the distance.\nBaseline: A man is talking and rain is falling.\nAudio Flamingo: A man is talking in the rain with thunder in the background.\n20\nAudio Flamingo\nC.3. Audio Flamingo looks at retrieved samples\nAudio Flamingo borrows key words from retrieved samples.\nTask: Audio Captioning\nInstruction: Describe this sound.\nGround Truth: Paper is crinkling and crumpling, and thumping occurs.\nBaseline: Movement of a person in a kitchen with pots and pans on a counter.\nRetrieved samples:\n1) Some material is crumpled.\n2) Plastic is being crumpled.\n3) Crinkling of a paper with a small step.\nAudio Flamingo: A person crumples up a sheet of paper and crinkles it.\nTask: Audio Captioning\nInstruction: Describe this sound.\nGround Truth: a woman speaks, and faucet water runs.\nBaseline: A woman is talking to a child.\nRetrieved samples:\n1) A woman speaking and water running.\n2) A women voice followed by flowing water from a faucet.\n3) Water sprays from a faucet, a person speaks.\nAudio Flamingo: A woman speaks while water runs from a faucet.\nAudio Flamingo can ignore noisy retrieved samples and make correct prediction.\nTask: Music Genre Classification\nInstruction: What is the genre of this music?\nGround Truth: pop.\nRetrieved samples:\n1) reggae.\n2) jazz.\n3) disco.\nAudio Flamingo: pop.\nTask: Music Instrument Classification\nInstruction: What is the instrument of this music?\nGround Truth: flute.\nRetrieved samples:\n1) clarinet.\n2) clarinet.\n3) clarinet.\nAudio Flamingo: flute.\n21\nAudio Flamingo\nC.4. Audio Flamingo has strong dialogue ability\n[Audio annotations:\nSound of Chirp, tweet (High-pitched, short, repetitive bird sounds.):\n[0.000s-1.157s], [1.543s-2.055s], [3.220s-4.409s], [4.748s-5.709s], [5.921s-6.780s],\n[7.094s-7.976s], [8.331s-8.819s], [9.142s-10.000s]; Sound of Animal (Loud, diverse, and\noften rhythmic.):\n[0.000s-1.157s], [1.528s-2.031s], [3.220s-4.386s], [4.764s-5.701s],\n[5.929s-6.740s], [7.071s-7.976s], [8.331s-8.795s], [9.134s-10.000s]; Sound of Bird vocalization,\nbird call, bird song (Melodic, rhythmic, varied pitch and tone.):\n[0.000s-1.165s],\n[1.528s-2.039s], [3.213s-4.386s], [4.772s-5.693s], [5.921s-6.701s], [7.047s-7.953s],\n[8.331s-8.780s], [9.150s-10.000s]; Sound of Bird (High-pitched, melodic, rhythmic chirping\nsounds.):\n[0.000s-1.173s], [1.535s-1.992s], [3.205s-4.362s], [4.780s-5.669s], [5.906s-6.669s],\n[7.016s-7.929s], [8.323s-8.772s], [9.134s-10.000s]; Sound of Speech (\u00a8Sound waves produced\nby vocal cords.\u00a8):\n[2.024s-3.008s], [4.630s-5.283s], [6.835s-8.134s], [8.701s-10.000s];\nSound of Female speech, woman speaking:\n[2.047s-3.039s], [4.614s-5.315s], [6.843s-8.150s],\n[8.709s-10.000s]]\nUser: Can you briefly explain what you hear in the audio?\nGround Truth: The audio primarily consists of bird sounds, including chirping and bird vocalizations. There are\nalso sounds of other animals. Additionally, there is a woman speaking at certain intervals.\nBaseline: Birds are chirping.\nAudio Flamingo: The audio features a variety of sounds including bird vocalizations, mechanical sounds, impact\nsounds, a man speaking, and a ticking sound.\nUser: Are the bird sounds continuous?\nGround Truth: The bird sounds are not continuous. They occur at different intervals throughout the audio, with\nsome overlap with the woman\u2019s speech.\nBaseline: Yes, the bird sounds are continuous.\nAudio Flamingo: No, the bird sounds are not continuous. They occur at different intervals throughout the audio.\nUser: Do the bird sounds and the person\u2019s speech overlap?\nGround Truth: Yes, there are instances where the bird sounds and the woman\u2019s speech overlap.\nBaseline: Yes, the bird sounds and the woman\u2019s speech overlap.\nAudio Flamingo: Yes, the bird sounds and the person\u2019s speech do overlap at certain points in the audio.\n[Audio annotations:\nSound of Music (Sound produced by vibrating instruments.):\n[0.000s-10.000s]; Sound of Rapping (Rhythmic speech over beats.):\n[0.000s-10.000s]; Sound of\nMale singing (Deep, resonant, and powerful tones.):\n[0.000s-10.000s]]\nUser: Is the male voice rapping or singing? Ground Truth: The male voice is doing both, rapping and singing in\nthe audio.\nBaseline: The male voice is rapping.\nAudio Flamingo: The male voice is rapping.\nUser: What kind of music is it?\nGround Truth: The type of music isn\u2019t specified, but it\u2019s accompanied by rapping and male singing.\nBaseline: The kind of music is hip-hop.\nAudio Flamingo: The music is a rap song.\nUser: Is the male voice the only one present?\nGround Truth: Yes, the male voice is the only one specified in the audio, both rapping and singing.\nBaseline: Yes, the male voice is the only one present.\nAudio Flamingo: Yes, the male voice is the only one present in the audio.\n22\nAudio Flamingo\n[Audio annotations:\nSound of Music (Sound produced by vibrating instruments.):\n[0.000s-10.000s]; Sound of Male singing (Deep, resonant, and powerful tones.):\n[1.341s-3.734s],\n[9.289s-10.000s]]\nUser: What genre does this music belong to?\nGround Truth: This music belongs to the R&B/Disco genre.\nBaseline: It is not specified in the given audio clip.\nAudio Flamingo: This music belongs to the R&B genre.\nUser: Can you tell me more about the vocals in this track?\nGround Truth: Sure, the vocals in this track are from a male singer who is singing in a higher register. His voice\nhas been modified with effects, making it sound like a voice sample.\nBaseline: The vocals in this track are described as rich and full-bodied, but no further details are given about the\nspecific singer or the lyrics being sung in the audio clip.\nAudio Flamingo: The vocals in this track are by a male singer. His voice is deep, resonant, and powerful, adding to\nthe overall emotional impact of the song.\n23\n"
  },
  {
    "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
    "link": "https://arxiv.org/pdf/2402.02834.pdf",
    "upvote": "10",
    "text": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models\nBo-Kyeong Kim1\nGeonmin Kim1\nTae-Ho Kim1\nThibault Castells1\nShinkook Choi1\nJunho Shin1\nHyoung-Kyu Song1\n1Nota Inc.\n{bokyeong.kim, geonmin.kim, thkim, thibault, shinkook.choi, junho.shin, hyoungkyu.song}@nota.ai\nAbstract\nStructured pruning of modern large language mod-\nels (LLMs) has emerged as a way of decreas-\ning their high computational needs. Width prun-\ning reduces the size of projection weight matrices\n(e.g., by removing attention heads) while main-\ntaining the number of layers. Depth pruning, in\ncontrast, removes entire layers or blocks, while\nkeeping the size of the remaining weights un-\nchanged. Most current research focuses on either\nwidth-only or a blend of width and depth pruning,\nwith little comparative analysis between the two\nunits (width vs. depth) concerning their impact on\nLLM inference efficiency. In this work, we show\nthat a simple depth pruning approach can com-\npete with recent width pruning methods in terms of\nzero-shot task performance. Our pruning method\nboosts inference speeds, especially under memory-\nconstrained conditions that require limited batch\nsizes for running LLMs, where width pruning is in-\neffective. We hope this work can help deploy LLMs\non local and edge devices.\n1\nIntroduction\nThe advancement of large language models (LLMs) [Tou-\nvron et al., 2023; OpenAI, 2023; Chowdhery et al., 2022;\nZhang et al., 2022; Scao et al., 2022] has brought signifi-\ncant improvements in language-based tasks, enabling versa-\ntile applications such as powerful chatbots [Google, 2023;\nOpenAI, 2022]. However, the deployment of LLMs is con-\nstrained by their intensive computational demands. To make\nLLMs more accessible and efficient for practical use, various\noptimization strategies have been actively studied over recent\nyears (see [Zhu et al., 2023; Wan et al., 2023] for survey).\nThis work focuses on structured pruning [Fang et al., 2023;\nLi et al., 2017a], which removes groups of unnecessary\nweights and can facilitate hardware-agnostic acceleration.\nIn the context of compressing billion-parameter LLMs,\nLLM-Pruner [Ma et al., 2023] and FLAP [An et al., 2024]\nnarrow the network width by pruning coupled structures\n(e.g., attention heads and their associated weight connec-\ntions) while maintaining the number of layers.\nSheared-\nLLaMA [Xia et al., 2024] reduces not only the network width\nBatch size (M) = 1\nM1\nM8\nM16\nM32\nM64\nM128\nM256\n12 Input Tokens\n128 Output Tokens\nFigure 1: Efficiency of pruned LLaMA-7B models on an NVIDIA\nH100 GPU. Compared to width pruning of FLAP [An et al., 2024]\nand LLM-Pruner [Ma et al., 2023], our depth pruning achieves faster\ninference with competitive PPL on WikiText2 (left) and offers a bet-\nter latency-throughput trade-off (right; M: batch size). See Sec-\ntion C for additional results.\nbut also its depth by entirely removing some layers.\nDe-\nspite the existence of pruning methods [Xia et al., 2022;\nKurtic et al., 2023; Xia et al., 2024] that incorporate both\nwidth and depth aspects, there remains a gap in detailed anal-\nysis comparing these two factors (width vs. depth), specifi-\ncally in relation to their impact on LLM inference efficiency.\nIn addition to substantial model sizes, LLM inference\nis distinguished by an autoregressive decoding mechanism,\nwhich predicts tokens one by one based on the input and the\npreviously generated tokens. This sequential generation pro-\ncess often exhibits a memory-bound nature, leading to con-\nsiderable underutilization of GPU compute abilities [Kwon\net al., 2023; Jin et al., 2023]. While expanding batch sizes is\na standard way to enhance GPU utilization and throughput,\nthis approach is unfeasible for low-specification GPUs with\nmemory constraints. We aim to improve inference speeds\nof LLMs, especially under hardware limitations that demand\nsmall batch sizes, where we observe that width-only pruning\nmethods are inadequate.\nDepth pruning is often regarded as being less effective in\nperformance compared to width pruning, due to the elim-\nination of bigger and coarse units.\nContrary to this pre-\nvailing view, we show that a simple depth pruning method\ncoupled with a LoRA retraining phase [Hu et al., 2022]\narXiv:2402.02834v1  [cs.LG]  5 Feb 2024\n(a) 7B\u2019s RTX3090 Utilization [%]\n(b) 7B\u2019s A100 Utilization [%]\n(c) 7B\u2019s H100 Utilization [%]\n(d) 13B\u2019s H100 Utilization [%]\nL512\nL512\nL512\nL512\nL128\nL128\nL128\nL128\nFigure 2: Top: GPU compute utilization of (a)\u2013(c) running LLaMA-7B on different NVIDIA GPUs and that of (d) Vicuna-13B. LLM\ninference is typically constrained by memory access operations, resulting in lower GPU compute usage. Increasing batch sizes can enhance\nGPU utilization and throughput, but pushing this too far triggers OOM issues. Bottom: Latency results with varying batch sizes and target\noutput lengths (labeled with L). Our depth pruning (blue lines) improves generation speeds over the original models (gray), while width\npruning [Ma et al., 2023] is ineffective (green). The dotted lines show that pruned models can operate with larger batch sizes that cause OOM\nerrors for the original model. The results are obtained with pruning ratios of 27% for the 7B model and 29% for the 13B model.\ncan rival recent width pruning studies for LLMs, including\nLLM-Pruner [Ma et al., 2023], FLAP [An et al., 2024],\nand a structured pruning variant of Wanda [Sun et al., 2024;\nAn et al., 2024], in terms of zero-shot task capabilities. More-\nover, we present that depth pruning markedly improves infer-\nence speeds, particularly when hardware restrictions neces-\nsitate running LLMs with limited batch sizes (see Figure 1).\nOur contributions are summarized as follows:\n\u25e6 In scenarios with limited batch sizes, our work demon-\nstrates that width pruning is difficult to attain actual\nspeedups in LLM\u2019s autoregressive generation. This as-\npect has been underexplored in previous works.\n\u25e6 We introduce a simple yet effective strategy for depth\npruning of LLMs. We explore various design factors,\nincluding the choice of prunable units, the criteria for\nimportance evaluation, and the retraining frequency.\n\u25e6 Our compact LLMs, obtained by excluding several\nTransformer blocks, achieve inference acceleration.\nThey are for general-purpose use and perform compa-\nrably to finely width-pruned models in zero-shot tasks.\n2\nProblem: Small-batch LLM Inference\nMost LLMs are autoregressive models that sequentially\nproduce tokens, based on the initial prompt and the se-\nquence of tokens previously generated. The token-by-token\ngeneration process often involves multiplying large matrices\n(weights) with smaller matrices or vectors (activations). The\nprimary bottleneck for inference efficiency is memory ac-\ncess operations rather than the speed of mathematical com-\nputations (referred to as \u2018memory-bound\u2019), leading to sub-\noptimal use of GPU computing power [Kwon et al., 2023].\nThough increasing batch sizes is a standard way to enhance\nGPU computation and throughput, it poses a risk of out-\nof-memory (OOM) errors, as depicted in Figure 2,1 unless\n1Using the HF-Transformers library [Wolf et al., 2020], we ran\nthe LLMs with 12 input tokens for 20 batched runs after 10 warm-\nQKV\ud835\udc3b\nTransformer Block1\nTransformer BlockN\nLM Head\nOutput Logit\nInput Embedding\nMHA\nFFN\nNorm\nDown\nUp & Gate\nNorm\nOut\nQKV1\nQKV\u210e\nWidth Pruning \nDepth Pruning \nTransformer Blockn-1\nTransformer Blockn\nFigure 3: Comparison of pruning granularities. Width pruning re-\nduces the size of weight matrices while maintaining the number\nof matrix-level operations. Depth pruning eliminates entire Trans-\nformer blocks, or individual MHA and FFN modules, leading to\nfewer memory accesses and matrix-level operations.\nadvanced system-level optimizations [Kwon et al., 2023;\nJin et al., 2023; Sheng et al., 2023] are applied.\nIn this study, our focus is on accelerating the inference of\nLLMs under small-batch conditions caused by hardware re-\nstrictions. Such situations are relevant for deploying LLMs\non memory-constrained local devices, which can enhance\nuser experience and data privacy protection. We show that\n(i) reducing weight shapes via width pruning does not im-\nprove generation speeds and can even degrade it when the\nresulting weight dimensions are unsuitable for GPU capabil-\nities, and (ii) notable speed gains are only achievable through\ndepth pruning that excludes a number of modules entirely.\nups. Top: Peak GPU compute utilization [NVIDIA, 2018]. Bottom:\nMean latency over 20 runs.\nFigure 4: Estimated importance of each Transformer block on the\ncalibration set. Blocks with lower PPL scores are pruned.\n3\nMethod: Block Pruning\nAn LLM is a stack of multiple Transformer blocks\n[Vaswani et al., 2017], each of which contains a pair of multi-\nhead attention (MHA) and feed-forward network (FFN) mod-\nules (see Figure 3). We choose this Transformer block as the\nprunable unit to prioritize reducing inference latency. Our\napproach is simple and cheap: after identifying unimportant\nblocks with straightforward metrics, we perform one-shot\npruning and light retraining.\n3.1\nEvaluation of Block-level Importance\nWe consider the following criteria to evaluate the signifi-\ncance of each block, ultimately selecting the Taylor+ and PPL\nmetrics (see Table 5). Specifically, the linear weight matrix is\ndenoted as Wk,n =\nh\nW k,n\ni,j\ni\nwith a size of (dout, din), where\nk represents the type of operation (e.g., a query projection\nin MHA or an up projection in FFN) within the n-th Trans-\nformer block. The weight importance scores are calculated at\nthe output neuron level [Sun et al., 2024], followed by sum-\nming2 these scores to assess the block-level importance.\nMagnitude (Mag).\nThis metric [Li et al., 2017b] is\na fundamental baseline in the pruning literature, assum-\ning that weights with smaller norms are less informative.\nFor the block-level analysis, we compute In\nMagnitude\n=\nP\nk\nP\ni\nP\nj\n\f\f\fW k,n\ni,j\n\f\f\f.\nTaylor.\nAssessing\nthe\nerror\ncaused\nby\nthe\nremoval\nof\na\nweight\nparameter\nhelps\nin\nidentifying\nits\nsig-\nnificance.\nFor\na\ngiven\ncalibration\ndataset\nD,\nthis\ncan be expressed as the alteration in the training loss\nL\n[LeCun\net\nal.,\n1989;\nMolchanov\net\nal.,\n2019]:\n\f\f\fL(W k,n\ni,j ; D) \u2212 L(W k,n\ni,j = 0; D)\n\f\f\f \u2248\n\u2202L(D)\n\u2202W k,n\ni,j W k,n\ni,j , where\nwe omit the second-order derivatives by following [Ma\net al., 2023].\nWe define the block score as In\nTaylor\n=\nP\nk\nP\ni\nP\nj\n\f\f\f\f\n\u2202L(D)\n\u2202W k,n\ni,j W k,n\ni,j\n\f\f\f\f.\n2In our exploration of various aggregation strategies (i.e., sum,\nmean, product, and max operations across module and block levels),\nsumming the scores was effective at different pruning ratios.\nModel\n#Param\n#Block\u2021\n#Head\u2021\nFFN-D\u2021\nOriginal 7B\n6.7B\n32\n32\n11008\n20%\u2020\nWanda-sp\n5.5B\n32\n26\n8807\nFLAP\n5.4B\n32\n26.9\u00b17.5\n8577.4\u00b12078.4\nLLM-Pruner\n5.4B\n32\n24\n8256\nOurs\n5.5B\n26\n32\n11008\n27%\u2020\nWanda-sp\n4.9B\n32\n23\n7816\nFLAP\n4.9B\n32\n24.6\u00b18.6\n7497.1\u00b12358.0\nLLM-Pruner\n4.9B\n32\n21\n7155\nOurs\n4.9B\n23\n32\n11008\n35%\u2020\nWanda-sp\n4.5B\n32\n21\n7156\nFLAP\n4.5B\n32\n23.0\u00b18.8\n6781.1\u00b12440.6\nLLM-Pruner\n4.4B\n32\n18\n6054\nOurs\n4.5B\n21\n32\n11008\nOriginal 13B\n13.0B\n40\n40\n13824\n21%\u2020\nWanda-sp\n10.5B\n40\n32\n11060\nFLAP\n10.5B\n40\n33.7\u00b18.9\n10778.7\u00b12316.0\nLLM-Pruner\n10.3B\n40\n30\n10368\nOurs\n10.5B\n32\n40\n13824\n29%\u2020\nWanda-sp\n9.5B\n40\n29\n9954\nFLAP\n9.5B\n40\n31.1\u00b110.6\n9570.8\u00b12601.0\nLLM-Pruner\n9.2B\n40\n26\n8985\nOurs\n9.5B\n29\n40\n13824\n37%\u2020\nWanda-sp\n8.4B\n40\n26\n8710\nFLAP\n8.3B\n40\n27.5\u00b111.3\n8326.6\u00b12874.9\nLLM-Pruner\n8.2B\n40\n22\n7603\nOurs\n8.3B\n25\n40\n13824\n\u2020Reduction ratio for the number of parameters.\n\u2021#Block: #Transformer blocks; #Head: #attention heads of MHA; FFN-D: interme-\ndiate size of FFN.\nTable 1:\nPruned architectures on LLaMA-7B and Vicuna-{7B,\n13B}-v1.3. While Wanda-sp [Sun et al., 2024; An et al., 2024],\nFLAP [An et al., 2024], and LLM-Pruner [Ma et al., 2023] reduce\nthe network width, our method reduces the network depth. Using\nLLM-Pruner\u2019s module-level pruning ratios of (25%, 35%, 45%) as\nbenchmarks, we adjust others for comparable parameter numbers.\nMag+ and Taylor+.\nUpon using the aforementioned met-\nrics, the early blocks are labeled as unimportant, but their re-\nmoval leads to severe performance drops. Similar to a popular\nheuristic [Gale et al., 2019; Lee et al., 2021], we preserve the\nfirst four and the last two blocks [Ma et al., 2023] by exclud-\ning them from the pruning candidates.\nPerplexity (PPL).\nRedundant blocks contribute less to the\nmodel\u2019s outputs, and their removal leads to smaller degrada-\ntion in PPL, a commonly used metric for language model-\ning tasks. In this context, we physically eliminate each block\nand monitor its influence on PPL using the calibration set D:\nIn\nPPL = exp\nn\n\u2212 1\nSL\nP\ns\nP\nl log p\u03b8n(x(s)\nl |x(s)\n<l )\no\n, where \u03b8n\ndenotes the model without its n-th block, and s = 1, . . . , S\nand l = 1, . . . , L are the indices for sequences and tokens\nin D.\nThe use of PPL can reflect the model\u2019s behavior\nby being derived from the next-token prediction loss; it re-\nquires only the forward pass, avoiding the need to compute\nback-propagation gradients [Ma et al., 2023] and Hessian in-\nverses [Kurtic et al., 2023], or to involve a mask learning\nstage [Xia et al., 2024]. As shown in Figure 4, several blocks\nare identified as removable, showing only a slight effect on\nthe PPL metric. The elimination of initial and final blocks sig-\nnificantly degrades the performance, which necessitates keep-\ning them unpruned.\nZero-shot Performance\nH100 80GB\u2021\nRTX3090 24GB\u2021\nPPL\u2193\nModel\nWikiText2\nPTB\nAve Acc\u2191\n(%)\u2020\nLatency\u2193\n(s)\nThroughput\u2191\n(tokens/s)\nLatency\u2193\n(s)\nThroughput\u2191\n(tokens/s)\nLLaMA-7B (6.7B)\n12.6\n22.1\n66.3\n2.4\n53.7\n5.1\n25.0\nWanda-sp\n21.4\n47.2\n51.8\n3.1\n41.7\n7.6\n16.7\nFLAP\n17.0\n30.1\n59.5\n3.2\n40.5\n7.7\n16.5\nLLM-Pruner\n17.6\n30.4\n61.8\n3.0\n43.2\n6.0\n21.4\nOurs: Taylor+\n20.2\n32.3\n63.5\n1.9\n66.0\n4.5\n28.4\n20%\nPruned\n(5.5B)\nOurs: PPL\n17.7\n30.7\n61.9\n1.9\n66.0\n4.5\n28.4\nWanda-sp\n50.4\n106.9\n42.1\n3.1\n41.7\n8.1\n16.0\nFLAP\n21.3\n37.1\n55.8\n3.2\n40.2\n7.8\n16.5\nLLM-Pruner\n20.5\n36.1\n58.7\n2.9\n44.0\n5.6\n22.9\nOurs: Taylor+\n29.9\n42.0\n59.8\n1.7\n73.9\n3.7\n34.9\n27%\nPruned\n(4.9B)\nOurs: PPL\n20.7\n36.0\n57.6\n1.7\n73.9\n3.7\n34.9\nWanda-sp\n133.6\n210.1\n36.9\n3.1\n41.6\n8.0\n16.1\nFLAP\n25.6\n44.4\n52.7\n3.2\n40.5\n8.1\n15.8\nLLM-Pruner\n24.2\n40.7\n55.5\n2.9\n44.4\n6.1\n21.1\nOurs: Taylor+\n33.2\n58.5\n55.4\n1.6\n80.1\n3.4\n37.8\n35%\nPruned\n(4.5B)\nOurs: PPL\n23.1\n38.8\n55.2\n1.6\n80.1\n3.4\n37.8\n\u2020Average accuracy on seven commonsense reasoning tasks.\n\u2021Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\nTable 2: Zero-shot results of the compressed LLaMA-7B. The width pruning methods of Wanda-sp [Sun et al., 2024; An et al., 2024],\nFLAP [An et al., 2024], and LLM-Pruner [Ma et al., 2023] often degrade inference efficiency due to the GPU-unfriendly weight sizes [An-\ndersch et al., 2019]. In contrast, our depth pruning approach enhances generation speed and competes well in zero-shot task performance.\nSee Section A for detailed results.\nZero-shot Performance\nPPL\u2193\nModel\nWikiText2\nPTB\nAve Acc\u2191\n(%)\u2020\nVicuna-7B-v1.3 (6.7B)\n17.1\n63.2\n65.9\nWanda-sp\n24.4\n104.0\n58.5\nFLAP\n22.0\n74.9\n61.4\nLLM-Pruner\n19.6\n76.4\n60.1\nOurs: Taylor+\n21.0\n72.3\n62.5\n20%\nPruned\n(5.5B)\nOurs: PPL\n18.8\n67.9\n60.7\nWanda-sp\n36.5\n177.6\n50.9\nFLAP\n27.9\n88.3\n57.1\nLLM-Pruner\n22.7\n87.9\n57.1\nOurs: Taylor+\n29.8\n92.0\n60.2\n27%\nPruned\n(4.9B)\nOurs: PPL\n23.0\n78.2\n56.1\nWanda-sp\n73.2\n386.5\n39.4\nFLAP\n34.6\n104.8\n53.7\nLLM-Pruner\n27.6\n102.0\n53.5\nOurs: Taylor+\n35.0\n110.3\n55.0\n35%\nPruned\n(4.5B)\nOurs: PPL\n26.6\n89.4\n53.3\n\u2020Average accuracy on seven commonsense reasoning tasks.\nTable 3: Zero-shot results of the compressed Vicuna-7B. See Sec-\ntion A for detailed results.\n3.2\nOne-shot Pruning\nAfter sorting the block-level importance scores, we prune\nthe less crucial blocks in a single step. Since every block\nhas an identical configuration and it is easy to calculate the\nnumber of parameters for one block, we readily decide how\nmany blocks should be removed to meet the target model size.\n3.3\nCost-efficient Retraining\nWe efficiently retrain the pruned models with the low-rank\nadaptation (LoRA) method [Hu et al., 2022; Ma et al., 2023].\nThe weight matrix of the adapted network is expressed as\nW0 + \u2206W = W0 + BA, where W0 denotes the initial pre-\ntrained weight with a shape of (dout, din). The update matrix\n\u2206W is decomposed into two trainable parts, B and A with\ndimensions (dout, r) and (r, din), where r represents a low\nrank. We demonstrate that LoRA has the potential to restore\nthe performance of depth-pruned models.\nLoRA-based retraining can be efficiently completed on a\nsingle GPU in just a few hours.\nFor example, retraining\na model pruned by 20% from 7B parameters takes about\n2 hours and 22GB VRAM, while a model reduced by 21%\nfrom 13B demands around 3 hours and 35GB VRAM.\n4\nExperimental Setup\nModel.\nOur testbed includes LLaMA-7B [Touvron et al.,\n2023] and Vicuna-{7B, 13B}-v1.3 [Chiang et al., 2023],\nwhich are famous open-source LLMs.\nBaseline.\nWe compare the two pruning units, network\nwidth vs. depth, using the same calibration dataset. The width\npruning baseline methods are described below, and we uti-\nlize their official code for implementation. Table 1 shows the\npruned architectures under similar numbers of parameters.3\n\u25e6 LLM-Pruner [Ma et al., 2023] employs a Taylor-based\nimportance metric to remove attention heads from MHA\nand intermediate neurons from FFN. Local pruning is\nperformed to select removable groups within the same\nmodule while maintaining uniform dimensions across\nthe examined blocks.\nAdhering to their practice, the\nfirst and last few blocks remain unpruned. Their pruned\nmodels and ours are identically retrained with LoRA.\n\u25e6 FLAP [An et al., 2024] uses a fluctuation-based im-\nportance metric to explore the recoverability of feature\nmaps after removing weight columns. Global pruning\n3We used the parameter numbers from LLM-Pruner\u2019s module-\nlevel pruning ratios of (25%, 35%, 45%) as the reference and ad-\njusted the pruning ratios for our method and the other baselines.\nZero-shot Performance\nH100 80GB\u2021\nRTX3090 24GB\u2021\nPPL\u2193\nModel\nWikiText2\nPTB\nAve Acc\u2191\n(%)\u2020\nLatency\u2193\n(s)\nThroughput\u2191\n(tokens/s)\nLatency\u2193\n(s)\nThroughput\u2191\n(tokens/s)\nVicuna-13B-v1.3 (13.0B)\n14.7\n51.6\n68.3\n2.8\n45.5\nOOM\nOOM\nWanda-sp\n19.0\n71.8\n63.6\n3.8\n34.1\n9.8\n12.9\nFLAP\n18.8\n65.3\n63.3\n3.9\n32.6\n10.2\n12.6\nLLM-Pruner\n16.0\n57.0\n65.3\n3.8\n34.0\n7.5\n17.3\nOurs: Taylor+\n18.1\n61.6\n66.7\n2.3\n55.7\n5.4\n23.9\n21%\nPruned\n(10.5B)\nOurs: PPL\n16.1\n56.5\n64.9\n2.3\n55.7\n5.4\n23.9\nWanda-sp\n23.4\n84.9\n60.0\n3.8\n33.7\n9.5\n13.5\nFLAP\n22.8\n78.8\n61.6\n3.9\n33.0\n10.7\n12.1\nLLM-Pruner\n19.0\n66.4\n62.7\n3.6\n35.8\n8.6\n15.0\nOurs: Taylor+\n22.0\n70.3\n65.1\n2.1\n62.0\n5.3\n24.2\n29%\nPruned\n(9.5B)\nOurs: PPL\n18.1\n62.2\n62.0\n2.1\n62.0\n5.3\n24.2\nWanda-sp\n36.6\n123.5\n52.7\n3.8\n33.8\n10.5\n12.6\nFLAP\n28.7\n96.2\n58.3\n3.9\n32.9\n9.7\n13.2\nLLM-Pruner\n22.2\n74.0\n59.7\n3.6\n35.6\n7.1\n18.0\nOurs: Taylor+\n34.2\n90.4\n61.4\n1.8\n69.7\n4.0\n31.7\n37%\nPruned\n(8.3B)\nOurs: PPL\n22.1\n73.6\n59.1\n1.8\n69.7\n4.0\n31.7\n\u2020Average accuracy on seven commonsense reasoning tasks.\n\u2021Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\nTable 4: Zero-shot results of the compressed Vicuna-13B. See Section A for detailed results.\nis applied, leading to different widths over distinct mod-\nules (see Table 1 for mean and standard deviation val-\nues). Instead of retraining, extra bias terms are added\ninto pruned feature maps for performance restoration.\n\u25e6 Wanda-sp is presented in [An et al., 2024] as a variant of\nWanda [Sun et al., 2024] adjusted for structured pruning.\nThe original metric was based on the product of weight\nmagnitudes and input activation norms, which can be in-\nterpreted as addressing a local reconstruction objective.\nWanda-sp extends this metric in a structured way while\nusing common dimensions among different modules.\nData.\nFollowing [Ma et al., 2023], we randomly select 10\nsamples from BookCorpus [Zhu et al., 2015] to compute\nblock-level significance during the pruning stage. We also\nuse this calibration dataset for the baseline methods to ensure\na fair comparison. At the LoRA retraining stage, 50K sam-\nples of the refined Alpaca [Taori et al., 2023] are used.\nEvaluation.\nFollowing [Touvron et al., 2023], we measure\nzero-shot accuracy on commonsense reasoning datasets (i.e.,\nBoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], Hel-\nlaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al.,\n2019], ARC-easy [Clark et al., 2018], ARC-challenge [Clark\net al., 2018], and OpenbookQA [Mihaylov et al., 2018]) us-\ning the lm-evaluation-harness package [EleutherAI, 2023].\nWe also report zero-shot PPL on WikiText2 [Merity et al.,\n2017] and PTB [Marcus et al., 1993].\nLatency and Throughput.\nWe follow [Sheng et al., 2023]\nto measure the metrics. Given a batch size M and an output\nsequence length L (excluding the input length), the latency T\nrepresents the time required to handle the given prompts and\nproduce ML output tokens. The throughput is computed as\nML/T. We report the average results from 20 runs after the\ninitial 10 warm-up batches.\nImplementation.\nWe use the Hugging Face\u2019s Transform-\ners library [Wolf et al., 2020]. For the pruning and retraining\nphases, an NVIDIA A100 GPU is employed. Experiments\ninvolving 7B-size models can be performed on an NVIDIA\nRTX3090. At the inference stage, we opt for the default con-\nfiguration, excluding the use of xFormers-optimized attention\nand advanced options. See Section D for further details.\n5\nResults\n5.1\nComparison with Existing Work\nTables 2, 3, and 4 show the zero-shot downstream task\nperformance and inference efficiency of differently pruned\nmodels.\nDiminishing the size of weight matrices through\nwidth pruning methods [Ma et al., 2023; An et al., 2024;\nSun et al., 2024] does not lead to latency improvements, as\ngeneration speeds with limited input (batch) scales heavily\ndepend on the frequency of memory access operations in\nLLM inference. This issue is challenging to address solely\nby reducing the sizes of matrices, unless they are entirely re-\nmoved. In some cases, it even worsens the inference speed\ncompared to the original model due to GPU-unfriendly oper-\nation dimensions (e.g., the hidden sizes of FFN are often not\ndivisible by 8, as shown in Table 1, which hinders the effec-\ntive utilization of GPU Tensor Cores [Andersch et al., 2019]).\nOn the contrary, our depth pruning exhibits speedups\nthrough the complete removal of several Transformer blocks,\nresulting in fewer memory access and matrix-level operations\nbetween activations and weights. Moreover, under the same\nretraining setup as [Ma et al., 2023], our models achieve zero-\nshot scores on par with finely width-pruned models.\n5.2\nAblation Study\nImportance Criteria for Block Pruning\nTable 5 presents the results of block pruning using vari-\nous significance criteria. The basic methods without the \u2018+\u2019\nlabel fail to maintain essential initial blocks, causing a de-\ncline in performance. The Mag+ method, which preserves\nthese critical blocks, partially improves the scores; however,\nits effectiveness is still inferior compared to the other meth-\nods, indicating that relying solely on weight magnitude could\nBlock Pruning\nCriterion\nPPL\u2193\nAve Acc\u2191\n(%)\u2020\nWikiText2\nPTB\n20%\nPruned\n(5.5B)\nMag\n7720.7\n10618.7\n34.4\nMag+\n19.4\n36.3\n56.1\nTaylor\n3631.7\n4327.9\n35.5\nTaylor+\n20.2\n32.3\n63.5\nPPL\n17.7\n30.7\n61.9\n35%\nPruned\n(4.5B)\nMag\n8490.1\n14472.1\n34.9\nMag+\n36.9\n61.1\n49.3\nTaylor\n7666.8\n10913.1\n35.3\nTaylor+\n33.2\n58.5\n55.4\nPPL\n23.1\n38.8\n55.2\n\u2020Average accuracy on seven commonsense reasoning tasks.\nTable 5: Comparison of pruning criteria on LLaMA-7B. The Tay-\nlor+ method excels in commonsense reasoning accuracy, while the\nPPL criterion leads to better generation performance.\nDepth Pruning\nUnit\n#Param\nPPL\u2193\nAve Acc\u2191\n(%)\u2020\nWikiText2\nPTB\nIndividual MHA & FFN\n5.7B\n20.8\n34.8\n63.1\nTransformer Block\n5.7B\n16.9\n29.3\n62.8\nIndividual MHA & FFN\n5.3B\n25.2\n41.3\n61.1\nTransformer Block\n5.3B\n18.6\n33.1\n60.6\nIndividual MHA & FFN\n4.6B\n38.9\n58.7\n52.5\nTransformer Block\n4.5B\n23.1\n38.8\n55.2\nIndividual MHA & FFN\n4.0B\n63.2\n88.9\n48.3\nTransformer Block\n3.9B\n31.1\n47.3\n50.6\n\u2020Average accuracy on seven commonsense reasoning tasks.\nTable 6: Comparison of depth pruning granularities on LLaMA-7B.\nRemoving entire Transformer blocks instead of individual MHA and\nFFN modules generally yields better results.\nbe improper for pruning decisions. The Taylor+ criterion en-\nhances accuracy in commonsense reasoning tasks, while the\nPPL method leads to better generation quality without relying\non heuristic selection of pruning candidates.\nStructural Unit for Depth Pruning\nPruning individual MHA and FFN modules, which are\nmore fine-grained units than Transformer blocks, is also pos-\nsible. To examine its effect, we measure the impact of remov-\ning each module on the PPL of the calibration set and selec-\ntively eliminate the unnecessary modules. The same LoRA\nretraining procedure is conducted.\nTable 6 shows the results of depth pruning at different gran-\nularities. For the models with more than 5B parameters, re-\nmoving individual MHA and FFN modules results in better\ndownstream task accuracy but worse PPL compared to re-\nmoving entire Transformer blocks. For smaller models than\n5B, block-level pruning achieves superior results in terms of\nall the examined metrics. This differs from the common be-\nlief that removing finer units yields better performance.\nGiven the collaborative roles of the modules (i.e., MHA\ncaptures dependency relations [Vaswani et al., 2017], while\nskip connections and FFN prevent the rank collapse in purely\nattention-driven networks [Dong et al., 2021]), it may be sub-\noptimal to treat them in isolation. Taking the 5.3B model in\nTable 6 as an example, module-level pruning results in con-\nsecutive FFNs in some positions, potentially impairing the\nmodel\u2019s ability to handle word interactions. In contrast, with\nEvaluation\nMetric\nMethod\n# Calibration Samples\n10\n50\n100\n1000\nPPL\u2193 on\nWikiText2\nWanda-sp\n21.4\n21.4\n21.7\n20.8\nFLAP\n17.0\n17.5\n17.5\n17.3\nLLM-Pruner\n17.6\n17.2\n17.0\nOOM\u2021\nOurs: Taylor+\n20.2\n20.2\n19.0\n19.6\nOurs: PPL\n17.7\n17.2\n17.4\n17.4\nAve Acc\u2191\n(%)\u2020\nWanda-sp\n51.8\n52.9\n52.0\n53.0\nFLAP\n59.5\n59.7\n59.9\n60.8\nLLM-Pruner\n61.8\n61.6\n61.7\nOOM\u2021\nOurs: Taylor+\n63.5\n63.5\n63.9\n61.7\nOurs: PPL\n61.9\n61.5\n61.7\n61.7\n\u2020Average accuracy on seven commonsense reasoning tasks.\n\u2021Out-of-memory error on an A100 (80GB) using the official code.\nTable 7: Impact of calibration data volume. The results of 20%-\npruned LLaMA-7B are reported.\nFigure 5: Comparison of one-shot and iterative block pruning on\nLLaMA-7B. The retraining phase of one-shot pruning effectively\nbridges the performance gap with iterative pruning.\nblock-level removal, the loss of information could be com-\npensated by neighboring blocks that serve similar functions.\nOne-shot vs. Iterative Pruning\nFor one-shot pruning, multiple blocks are removed simul-\ntaneously from the original model, followed by just one phase\nof retraining. For iterative pruning, the removal of one block\ncoupled with subsequent retraining is repeatedly performed.\nHere, we use the PPL-based importance criterion for select-\ning which blocks to remove.\nFigure 5 compares the pruned networks before and after the\nretraining process. The iteratively pruned models yield better\npost-pruning results than one-shot pruned ones. However, a\nsingle retraining session after one-shot pruning leads to simi-\nlar performance with iterative pruning. In light of the greatly\nreduced retraining budget, we opt for one-shot pruning.\nCalibration Data Volume\nThe calibration set is employed to assess the weight signif-\nicance of width pruning baselines and the block-level impor-\ntance of our method during the pruning phase.\nTable 7 presents the results obtained by varying the number\nof calibration samples in the BookCorpus dataset. The scores\nremain relatively stable for the examined methods, suggesting\nthat 10 samples could be sufficient. However, our Taylor+\nmethod encounters a drop in downstream task accuracy when\n1K samples are used, leaving the exploration of calibration\ndata characteristics for future research.\nModel\nGenerated Text\nLLaMA-7B\nAI can create a logo in seconds. Why are these new technologies threatening our jobs? AI can do most menial tasks\nin just a few seconds or minutes. These tasks can be very tedious for human beings to do. For instance, AI can ...\nWanda-sp\nAI can create a logo in seconds rather than years. To this point, this is only the beginning of what AI will do to change\nour human lives. As time goes on, it\u2019s quite likely that we\u2019ll see even greater use of robots. Even if this all sounds ...\nFLAP\nAI can create a logo in seconds and it doesn\u2019t cost you a penny. The new era of technology has opened up many\nexciting opportunities. Technology, like AI is being utilized for different purposes in different ways, and now, ...\nLLM-Pruner\nAI can create a logo in seconds, design a website from scratch and even write blog posts without any human interven-\ntion. This revolution is changing the way humans communicate and express ideas. In our view, this technology is a ...\nOurs: Taylor+\nAI can create a logo in seconds or design a new website. There is a lot of software that exists in these two categories.\nIt has become easier and faster for people to use. However, there is still the requirement for the designer or creative ...\nOurs: PPL\nAI can create a logo in seconds, and it can design entire websites, all without human intervention. However, there is\nstill a lot of work to be done to perfect this technology, including the development of natural language processing, ...\nVicuna-13B\nAI can create a logo in seconds with its complex algorithms and pre-set designs. But it can never truly understand the\nessence of a business, the emotions that go into creating a logo or the creativity and uniqueness that comes from ...\nWanda-sp\nAI can create a logo in seconds using a logo design tool. However, a talented graphic designer can create a unique\nand meaningful logo that stands out from the crowd, captures the essence of the business, and appeals to the target ...\nFLAP\nAI can create a logo in seconds. If you don\u2019t like it, you can change it yourself or pay for a designer to do it for you.\nHowever, it\u2019s important to note that a logo is more than just an image. It\u2019s a visual representation of a brand or ...\nLLM-Pruner\nAI can create a logo in seconds with just one click, but design is a human art, something that is beyond AI capabilities.\nThe logo design is an important part of a company\u2019s identity, and it\u2019s a decision that affects the company\u2019s visual ...\nOurs: Taylor+\nAI can create a logo in seconds, with or without a human\u2019s input. Some people might like this, because they can create\na logo for a small business on a budget. Other people, like myself, think that the human touch is important in art ...\nOurs: PPL\nAI can create a logo in seconds by combining a variety of different styles and shapes and patterns that make up the\nlogos. However, a logo that is truly a reflection of your brand needs to be designed by a human, a graphic designer. ...\nTable 8: Generation examples from the compressed (top) LLaMA-7B and (bottom) Vicuna-13B. See Section B for additional results.\nModel\nL128\nL512\nM1\nM16\nM64\nM1\nM16\nM64\n7B\n12.8GB\n16.0GB\n25.8GB\n13.3GB\n25.0GB\n61.8GB\n20%\n10.5GB\n13.1GB\n21.1GB\n10.9GB\n20.4GB\n50.4GB\n27%\n9.4GB\n11.6GB\n18.8GB\n9.7GB\n18.1GB\n44.6GB\n35%\n8.6GB\n10.7GB\n17.2GB\n9.0GB\n16.6GB\n40.8GB\n13B\n24.8GB\n29.6GB\n44.9GB\n25.5GB\n43.7GB\nOOM\n21%\n19.9GB\n23.8GB\n36.0GB\n20.5GB\n35.0GB\nOOM\n29%\n18.1GB\n21.7GB\n32.7GB\n18.6GB\n31.8GB\n73.5GB\n37%\n15.7GB\n18.8GB\n28.3GB\n16.1GB\n27.5GB\n63.5GB\nTable 9: GPU memory requirements for varying sequence lengths\n(L) and batch sizes (M). The results of the 7B and 13B models and\nour models with different pruning ratios are reported. Our approach\neffectively reduces the memory demands of the original models.\n5.3\nAdditional Analysis\nTable 8 presents generation examples where the input\nprompt was sourced from [Ma et al., 2023]. In terms of lin-\nguistic flow and topical consistency, the pruned models yield\nsentences on par with those from the original model. How-\never, as also noted in [Ma et al., 2023], the output quality de-\nteriorates when responding to factual questions or producing\nlonger content. To overcome this, integrating more powerful\nretraining methods may be a valuable future direction.\nTable 9 shows the gains in GPU memory requirements\nfrom our depth-pruned models on NVIDIA H100 given 12\ninput tokens. The larger the batch size, the greater the im-\nprovement observed. Notably, our pruned models can handle\nan output length of 512 and a batch size of 64, unlike the\noriginal 13B-parameter model.\n6\nRelated Work\nNumerous techniques have been developed towards ef-\nficient LLMs, including knowledge distillation [Fu et al.,\n2023; Hsieh et al., 2023], quantization [Frantar et al., 2023;\nDettmers et al., 2022], and system-level inference accelera-\ntion [Dao, 2023; Kwon et al., 2023]. In this study, we focus\non network pruning [LeCun et al., 1989], which has a long-\nstanding reputation in the model compression field. Beyond\nits use in relatively small-scale convolutional networks [Li et\nal., 2017b; He et al., 2019] and Transformer models [Yu et\nal., 2022; Xia et al., 2022; Kurtic et al., 2023], pruning has\nrecently begun to be applied to contemporary LLMs. Several\nstudies [Frantar and Alistarh, 2023; Sun et al., 2024] employ\nunstructured and semi-structured [Aojun Zhou, 2021] prun-\ning by zeroing individual neurons. SparseGPT [Frantar and\nAlistarh, 2023] addresses the layer-wise reconstruction prob-\nlem for pruning by computing Hessian inverses. Wanda [Sun\net al., 2024] introduces a pruning criterion that involves mul-\ntiplying weight magnitudes by input feature norms.\nDe-\nspite the plausible performance of pruned models using zero\nmasks, they necessitate specialized support for sparse matrix\noperations to ensure actual speedups.\nIn contrast, structured pruning removes organized patterns,\nsuch as entire layers [Fan et al., 2020], MHA\u2019s attention\nheads [Voita et al., 2019; Michel et al., 2022], FFN\u2019s hidden\nsizes [Nova et al., 2023; Santacroce et al., 2023], and some\nhybrid forms [Lagunas et al., 2021; Liu et al., 2021; Xia et al.,\n2022; Kwon et al., 2022; Kurtic et al., 2023], thereby improv-\ning inference efficiency in a hardware-agnostic way. To com-\npress LLMs, FLAP [An et al., 2024] and LLM-Pruner [Ma\net al., 2023] eliminate coupled structures in the aspect of net-\nwork width while retaining the number of layers. Sheared-\nLLaMA [Xia et al., 2024] introduces a mask learning phase\naimed at identifying prunable components in both the net-\nwork\u2019s width and depth. Our work explores the relatively un-\ntapped area of depth-only pruning for multi-billion parameter\nLLMs, which can markedly accelerate latency while attaining\ncompetitive results.\n7\nConclusion\nBy introducing a block pruning method, we conduct an in-\ndepth comparative analysis on the impact of network width\nand depth on LLM compression. Our work involves the one-\nshot removal of Transformer blocks, determined by evaluat-\ning various design choices. Despite its simplicity, our method\nmatches the zero-shot capabilities of recent width pruning\ntechniques. Moreover, it offers significant inference speedups\nin resource-constrained scenarios that require running LLMs\nwith limited batch sizes, where width pruning falls short. Fu-\nture research will investigate more potent retraining methods,\nincluding full parameter updates and knowledge distillation,\nalongside an in-depth study of calibration data.\nAcknowledgments\nWe thank the Microsoft Startups Founders Hub program\nand the Gwangju Artificial Intelligence Industry Cluster\nAgency (AICA) for their generous support of GPU resources,\nwhich have contributed to the progress of the NetsPresso\nR&D project.\nReferences\n[An et al., 2024] Yongqi An, Xu Zhao, Tao Yu, Ming Tang,\nand Jinqiao Wang. Fluctuation-based adaptive structured\npruning for large language models. In AAAI, 2024. 1, 2, 3,\n4, 5, 7\n[Andersch et al., 2019] Michael Andersch, Valerie Sarge,\nand Paulius Micikevicius.\nTensor core dl performance\nguide. In NVIDIA GTC, 2019. 4, 5, 1\n[Aojun Zhou, 2021] Junnan Zhu Jianbo Liu Zhijie Zhang\nKun Yuan Wenxiu Sun Hongsheng Li Aojun Zhou,\nYukun Ma. Learning n:m fine-grained structured sparse\nneural networks from scratch. In ICLR, 2021. 7\n[Bisk et al., 2020] Yonatan Bisk, Rowan Zellers, Ronan Le\nBras, Jianfeng Gao, and Yejin Choi.\nPiqa:\nReason-\ning about physical commonsense in natural language. In\nAAAI, 2020. 5\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin,\nYing Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 4\n[Chowdhery et al., 2022] Aakanksha\nChowdhery,\nSharan\nNarang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, et al.\nPalm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022. 1\n[Clark et al., 2018] Peter Clark, Isaac Cowhey, Oren Etzioni,\nTushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answer-\ning? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018. 5\n[Clark et al., 2019] Christopher Clark, Kenton Lee, Ming-\nWei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising dif-\nficulty of natural yes/no questions. In NAACL, 2019. 5\n[Dao, 2023] Tri Dao. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023. 7\n[Dettmers et al., 2022] Tim Dettmers, Mike Lewis, Younes\nBelkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix\nmultiplication for transformers at scale. In NeurIPS, 2022.\n7\n[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier,\nand Andreas Loukas. Attention is not all you need: Pure\nattention loses rank doubly exponentially with depth. In\nICML, 2021. 6\n[EleutherAI, 2023] EleutherAI. Language model evaluation\nharness (package version 3326c54).\nhttps://github.com/\nEleutherAI/lm-evaluation-harness, 2023. 5\n[Fan et al., 2020] Angela Fan, Edouard Grave, and Armand\nJoulin. Reducing transformer depth on demand with struc-\ntured dropout. In ICLR, 2020. 7\n[Fang et al., 2023] Gongfan Fang, Xinyin Ma, Mingli Song,\nMichael Bi Mi, and Xinchao Wang. Depgraph: Towards\nany structural pruning. In CVPR, 2023. 1\n[Frantar and Alistarh, 2023] Elias Frantar and Dan Alistarh.\nSparseGPT: Massive language models can be accurately\npruned in one-shot. In ICML, 2023. 7\n[Frantar et al., 2023] Elias Frantar, Saleh Ashkboos, Torsten\nHoefler, and Dan Alistarh. OPTQ: Accurate quantization\nfor generative pre-trained transformers. In ICLR, 2023. 7\n[Fu et al., 2023] Yao Fu, Hao Peng, Litu Ou, Ashish Sab-\nharwal, and Tushar Khot. Specializing smaller language\nmodels towards multi-step reasoning. In ICML, 2023. 7\n[Gale et al., 2019] Trevor Gale,\nErich Elsen,\nand Sara\nHooker. The state of sparsity in deep neural networks. In\nICML Workshop, 2019. 3\n[Google, 2023] Google.\nAn important next step on\nour\nai\njourney.\nhttps://blog.google/technology/ai/\nbard-google-ai-search-updates/, 2023. 1\n[He et al., 2019] Yang He, Ping Liu, Ziwei Wang, Zhilan\nHu, and Yi Yang.\nFilter pruning via geometric median\nfor deep convolutional neural networks acceleration. In\nCVPR, 2019. 7\n[Hsieh et al., 2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-\nKuan Yeh, Hootan Nakhost, et al. Distilling step-by-step!\noutperforming larger language models with less training\ndata and smaller model sizes. In Findings of ACL, 2023. 7\n[Hu et al., 2022] Edward J. Hu, Yelong Shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large\nlanguage models. In ICLR, 2022. 1, 4\n[Jin et al., 2023] Yunho Jin, Chun-Feng Wu, David Brooks,\nand Gu-Yeon Wei. S3: Increasing gpu utilization during\ngenerative inference for higher throughput. In NeurIPS,\n2023. 1, 2\n[Kurtic et al., 2023] Eldar Kurtic, Elias Frantar, and Dan Al-\nistarh. Ziplm: Inference-aware structured pruning of lan-\nguage models. In NeurIPS, 2023. 1, 3, 7\n[Kwon et al., 2022] Woosuk\nKwon,\nSehoon\nKim,\nMichael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and\nAmir Gholami. A fast post-training pruning framework\nfor transformers. In NeurIPS, 2022. 7\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan\nZhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica.\nEffi-\ncient memory management for large language model serv-\ning with pagedattention. In SOSP, 2023. 1, 2, 7\n[Lagunas et al., 2021] Franc\u00b8ois Lagunas, Ella Charlaix, Vic-\ntor Sanh, and Alexander M. Rush. Block pruning for faster\ntransformers. In EMNLP, 2021. 7\n[LeCun et al., 1989] Yann LeCun, John Denker, and Sara\nSolla. Optimal brain damage. In NeurIPS, 1989. 3, 7\n[Lee et al., 2021] Jaeho Lee, Sejun Park, Sangwoo Mo,\nSungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity\nfor the magnitude-based pruning. In ICLR, 2021. 3\n[Li et al., 2017a] Hao Li, Asim Kadav, Igor Durdanovic,\nHanan Samet, and Hans Peter Graf.\nPruning filters for\nefficient convnets. In ICLR, 2017. 1\n[Li et al., 2017b] Hao Li, Asim Kadav, Igor Durdanovic,\nHanan Samet, and Hans Peter Graf.\nPruning filters for\nefficient convnets. In ICLR, 2017. 3, 7\n[Liu et al., 2021] Zejian Liu, Fanrong Li, Gang Li, and Jian\nCheng. Ebert: Efficient bert inference with dynamic struc-\ntured pruning. In Findings of ACL, 2021. 7\n[Ma et al., 2023] Xinyin Ma, Gongfan Fang, and Xinchao\nWang. Llm-pruner: On the structural pruning of large lan-\nguage models. In NeurIPS, 2023. 1, 2, 3, 4, 5, 7\n[Marcus et al., 1993] Mitchell P. Marcus, Beatrice Santorini,\nand Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313\u2013330, 1993. 5\n[Merity et al., 2017] Stephen Merity, Caiming Xiong, James\nBradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In ICLR, 2017. 5\n[Michel et al., 2022] Paul Michel, Omer Levy, and Graham\nNeubig.\nAre sixteen heads really better than one?\nIn\nNeurIPS, 2022. 7\n[Mihaylov et al., 2018] Todor Mihaylov, Peter Clark, Tushar\nKhot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answer-\ning. In EMNLP, 2018. 5\n[Molchanov et al., 2019] Pavlo Molchanov, Arun Mallya,\nStephen Tyree, Iuri Frosio, and Jan Kautz.\nImportance\nestimation for neural network pruning. In CVPR, 2019. 3\n[Nova et al., 2023] Azade Nova, Hanjun Dai, and Dale\nSchuurmans. Gradient-free structured pruning with unla-\nbeled data. In ICML, 2023. 7\n[NVIDIA, 2018] NVIDIA.\nUseful nvidia-smi queries.\nhttps://enterprise-support.nvidia.com/s/article/\nUseful-nvidia-smi-Queries-2, 2018. 2\n[OpenAI, 2022] OpenAI.\nIntroducing chatgpt.\nhttps://\nopenai.com/blog/chatgpt, 2022. 1\n[OpenAI, 2023] OpenAI.\nGpt-4 technical report.\narXiv\npreprint arXiv:2303.08774, 2023. 1\n[Sakaguchi et al., 2019] Keisuke Sakaguchi, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale.\narXiv\npreprint arXiv:1907.10641, 2019. 5\n[Santacroce et al., 2023] Michael Santacroce, Zixin Wen,\nYelong Shen, and Yuanzhi Li. What matters in the struc-\ntured pruning of generative language models?\narXiv\npreprint arXiv:2302.03773, 2023. 7\n[Scao et al., 2022] Teven Le Scao, Angela Fan, Christopher\nAkiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, et al.\nBloom: A 176b-parameter open-access multilingual lan-\nguage model. arXiv preprint arXiv:2211.05100, 2022. 1\n[Sheng et al., 2023] Ying Sheng, Lianmin Zheng, Binhang\nYuan, Zhuohan Li, Max Ryabinin, et al. Flexgen: High-\nthroughput generative inference of large language models\nwith a single gpu. In ICML, 2023. 2, 5\n[Sun et al., 2024] Mingjie Sun, Zhuang Liu, Anna Bair, and\nJ. Zico Kolter. A simple and effective pruning approach\nfor large language models. In ICLR, 2024. 2, 3, 4, 5, 7, 1\n[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi\nZhang, Yann Dubois, Xuechen Li, Carlos Guestrin, et al.\nStanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford alpaca, 2023. 5\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1, 4, 5\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017. 3, 6\n[Voita et al., 2019] Elena Voita, David Talbot, Fedor Moi-\nseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the\nrest can be pruned. In ACL, 2019. 7\n[Wan et al., 2023] Zhongwei Wan, Xin Wang, Che Liu,\nSamiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu,\nQuanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Ef-\nficient large language models: A survey. arXiv preprint\narXiv:2312.03863, 2023. 1\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\nSanh, Julien Chaumond, Clement Delangue, et al. Trans-\nformers: State-of-the-art natural language processing. In\nEMNLP: System Demonstrations, 2020. 2, 5, 4\n[Xia et al., 2022] Mengzhou Xia, Zexuan Zhong, and Danqi\nChen.\nStructured pruning learns compact and accurate\nmodels. In ACL, 2022. 1, 7\n[Xia et al., 2024] Mengzhou Xia,\nTianyu Gao,\nZhiyuan\nZeng, and Danqi Chen. Sheared llama: Accelerating lan-\nguage model pre-training via structured pruning. In ICLR,\n2024. 1, 3, 8\n[Yu et al., 2022] Shixing Yu, Tianlong Chen, Jiayi Shen,\nHuan Yuan, Jianchao Tan, Sen Yang, Ji Liu,\nand\nZhangyang Wang.\nUnified visual transformer compres-\nsion. In ICLR, 2022. 7\n[Zellers et al., 2019] Rowan Zellers, Ari Holtzman, Yonatan\nBisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence? In ACL, 2019. 5\n[Zhang et al., 2022] Susan Zhang, Stephen Roller, Naman\nGoyal, Mikel Artetxe, Moya Chen, et al.\nOpt: Open\npre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022. 1\n[Zhu et al., 2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Rus-\nlan Salakhutdinov, et al. Aligning books and movies: To-\nwards story-like visual explanations by watching movies\nand reading books. In ICCV, 2015. 5, 4\n[Zhu et al., 2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma,\nand Weiping Wang. A survey on model compression for\nlarge language models. arXiv preprint arXiv:2308.07633,\n2023. 1\nAppendix \u2014 Shortened LLaMA: A Simple Depth Pruning for Large Language Models\nA\nZero-shot Downstream Task Performance\nPPL\u2193\nCommonsense Reasoning Accuracy\u2191 (%)\nThr\u2191 (tokens/s)\u2021\nModel\nWiki2\nPTB\nAverage\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-e\nARC-c\nOBQA\nH100\nRTX3090\nLLaMA-7B (6.7B)\n12.6\n22.1\n66.3\n75.0\n78.7\n76.2\n69.9\n75.3\n44.7\n44.4\n53.7\n25.0\nWanda-sp\n21.4\n47.2\n51.8\n61.5\n70.4\n53.2\n56.0\n58.7\n31.4\n31.0\n41.7\n16.7\nFLAP\n17.0\n30.1\n59.5\n69.4\n74.7\n66.9\n66.3\n64.6\n36.5\n38.2\n40.5\n16.5\nLLM-Pruner\n17.6\n30.4\n61.8\n66.2\n77.6\n71.4\n66.1\n70.5\n39.3\n41.2\n43.2\n21.4\nOurs: Taylor+\n20.2\n32.3\n63.5\n75.7\n75.7\n71.5\n69.1\n69.9\n41.6\n40.8\n66.0\n28.4\n20%\nPruned\n(5.5B)\nOurs: PPL\n17.7\n30.7\n61.9\n72.7\n75.7\n70.4\n63.6\n69.5\n40.1\n41.2\n66.0\n28.4\nWanda-sp\n50.4\n106.9\n42.1\n62.0\n60.4\n33.2\n52.8\n37.6\n23.0\n25.4\n41.7\n16.0\nFLAP\n21.3\n37.1\n55.8\n68.2\n70.6\n61.0\n64.1\n58.8\n31.4\n36.8\n40.2\n16.5\nLLM-Pruner\n20.5\n36.1\n58.7\n62.8\n75.5\n67.2\n64.9\n63.5\n36.8\n40.2\n44.0\n22.9\nOurs: Taylor+\n29.9\n42.0\n59.8\n70.6\n73.0\n65.7\n68.5\n63.9\n39.3\n37.4\n73.9\n34.9\n27%\nPruned\n(4.9B)\nOurs: PPL\n20.7\n36.0\n57.6\n66.6\n73.1\n63.7\n60.4\n64.3\n36.0\n39.2\n73.9\n34.9\nWanda-sp\n133.6\n210.1\n36.9\n44.5\n56.8\n29.6\n49.6\n31.7\n20.7\n25.6\n41.6\n16.1\nFLAP\n25.6\n44.4\n52.7\n68.3\n68.1\n55.9\n61.1\n52.3\n29.4\n33.8\n40.5\n15.8\nLLM-Pruner\n24.2\n40.7\n55.5\n62.9\n72.8\n62.3\n62.7\n57.4\n33.0\n37.6\n44.4\n21.1\nOurs: Taylor+\n33.2\n58.5\n55.4\n62.5\n69.2\n60.7\n66.8\n57.4\n34.5\n36.8\n80.1\n37.8\n35%\nPruned\n(4.5B)\nOurs: PPL\n23.1\n38.8\n55.2\n64.3\n71.4\n59.4\n59.3\n62.2\n32.8\n37.0\n80.1\n37.8\nPPL\u2193\nCommonsense Reasoning Accuracy\u2191 (%)\nThr\u2191 (tokens/s)\u2021\nModel\nWiki2\nPTB\nAverage\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-e\nARC-c\nOBQA\nH100\nRTX3090\nVicuna-7B (6.7B)\n17.1\n63.2\n65.9\n78.1\n77.3\n73.9\n69.5\n74.3\n44.3\n43.8\n53.7\n25.0\nWanda-sp\n24.4\n104.0\n58.5\n63.9\n72.0\n67.4\n65.2\n64.8\n38.3\n37.8\n41.7\n16.7\nFLAP\n22.0\n74.9\n61.4\n73.1\n74.8\n67.9\n65.8\n67.5\n40.2\n40.6\n40.5\n16.5\nLLM-Pruner\n19.6\n76.4\n60.1\n65.4\n76.2\n68.9\n64.4\n68.9\n37.4\n39.4\n43.2\n21.4\nOurs: Taylor+\n21.0\n72.3\n62.5\n78.7\n74.8\n69.4\n68.5\n68.2\n38.7\n39.6\n66.0\n28.4\n20%\nPruned\n(5.5B)\nOurs: PPL\n18.8\n67.9\n60.7\n71.7\n74.4\n67.6\n63.6\n69.3\n38.9\n39.4\n66.0\n28.4\nWanda-sp\n36.5\n177.6\n50.9\n49.0\n67.1\n57.2\n59.2\n57.6\n33.7\n32.4\n41.7\n16.0\nFLAP\n27.9\n88.3\n57.1\n72.0\n71.5\n62.0\n61.2\n61.2\n35.4\n36.6\n40.2\n16.5\nLLM-Pruner\n22.7\n87.9\n57.1\n60.8\n74.3\n65.9\n60.9\n64.4\n34.6\n38.8\n44.0\n22.9\nOurs: Taylor+\n29.8\n92.0\n60.2\n78.8\n71.8\n64.4\n67.7\n64.3\n36.4\n37.6\n73.9\n34.9\n27%\nPruned\n(4.9B)\nOurs: PPL\n23.0\n78.2\n56.1\n66.4\n72.9\n60.6\n59.2\n63.1\n33.8\n37.0\n73.9\n34.9\nWanda-sp\n73.2\n386.5\n39.4\n43.1\n58.4\n36.3\n53.3\n34.5\n23.7\n26.4\n41.6\n16.1\nFLAP\n34.6\n104.8\n53.7\n65.1\n68.1\n57.0\n63.1\n56.9\n32.0\n34.0\n40.5\n15.8\nLLM-Pruner\n27.6\n102.0\n53.5\n52.0\n72.4\n61.6\n59.9\n58.0\n33.3\n37.0\n44.4\n21.1\nOurs: Taylor+\n35.0\n110.3\n55.0\n64.0\n69.6\n59.3\n66.5\n57.5\n33.3\n35.2\n80.1\n37.8\n35%\nPruned\n(4.5B)\nOurs: PPL\n26.6\n89.4\n53.3\n65.2\n70.4\n56.5\n56.6\n59.8\n31.5\n33.4\n80.1\n37.8\nPPL\u2193\nCommonsense Reasoning Accuracy\u2191 (%)\nThr\u2191 (tokens/s)\u2021\nModel\nWiki2\nPTB\nAverage\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-e\nARC-c\nOBQA\nH100\nRTX3090\nVicuna-13B (13.0B)\n14.7\n51.6\n68.3\n82.8\n78.3\n77.0\n71.2\n75.4\n47.7\n45.4\n45.5\nOOM\nWanda-sp\n19.0\n71.8\n63.6\n78.6\n75.6\n73.5\n68.4\n68.5\n42.2\n38.4\n34.1\n12.9\nFLAP\n18.8\n65.3\n63.3\n77.2\n75.1\n72.0\n70.2\n69.4\n40.3\n38.8\n32.6\n12.6\nLLM-Pruner\n16.0\n57.0\n65.3\n75.5\n78.6\n75.0\n69.8\n70.6\n43.6\n44.4\n34.0\n17.3\nOurs: Taylor+\n18.1\n61.6\n66.7\n83.0\n76.8\n75.1\n72.8\n72.5\n44.5\n42.4\n55.7\n23.9\n21%\nPruned\n(10.5B)\nOurs: PPL\n16.1\n56.5\n64.9\n75.0\n77.1\n73.7\n68.9\n71.5\n43.8\n44.2\n55.7\n23.9\nWanda-sp\n23.4\n84.9\n60.0\n71.5\n74.2\n68.7\n65.1\n64.3\n36.8\n39.4\n33.7\n13.5\nFLAP\n22.8\n78.8\n61.6\n75.9\n73.7\n67.9\n66.4\n67.3\n38.0\n42.0\n33.0\n12.1\nLLM-Pruner\n19.0\n66.4\n62.7\n68.3\n77.1\n72.0\n69.7\n68.6\n40.0\n43.4\n35.8\n15.0\nOurs: Taylor+\n22.0\n70.3\n65.1\n82.6\n75.1\n73.3\n70.9\n69.9\n43.8\n40.2\n62.0\n24.2\n29%\nPruned\n(9.5B)\nOurs: PPL\n18.1\n62.2\n62.0\n67.5\n75.6\n70.6\n65.5\n70.9\n43.3\n40.2\n62.0\n24.2\nWanda-sp\n36.6\n123.5\n52.7\n59.6\n67.5\n59.5\n59.7\n55.2\n33.5\n33.8\n33.8\n12.6\nFLAP\n28.7\n96.2\n58.3\n72.5\n70.0\n62.5\n65.4\n63.8\n36.3\n37.8\n32.9\n13.2\nLLM-Pruner\n22.2\n74.0\n59.7\n67.1\n75.6\n67.7\n63.2\n65.5\n38.8\n39.8\n35.6\n18.0\nOurs: Taylor+\n34.2\n90.4\n61.4\n78.5\n71.3\n69.2\n69.9\n64.2\n40.5\n36.6\n69.7\n31.7\n37%\nPruned\n(8.3B)\nOurs: PPL\n22.1\n73.6\n59.1\n69.4\n73.8\n64.4\n62.5\n65.1\n39.2\n39.0\n69.7\n31.7\n\u2021Throughput measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\nTable 10: Zero-shot results of the compressed (top) LLaMA-7B, (middle) Vicuna-7B-v1.3, and (bottom) Vicuna-13B-v1.3. The width pruning\nmethods of Wanda-sp [Sun et al., 2024; An et al., 2024], FLAP [An et al., 2024], and LLM-Pruner [Ma et al., 2023] often degrade inference\nefficiency due to the GPU-unfriendly weight sizes [Andersch et al., 2019]. On the contrary, our depth pruning method attains actual speedups\nwhile performing comparably in zero-shot task scenarios.\nB\nGeneration Examples\nModel\nGenerated Text\nLLaMA-7B\nWhat\u2019s great about the holiday season, it\u2019s a time when people focus less on work and more on families and friends.\nThe holidays are a time when people can really focus on giving and helping others, which is something that the\nholidays are supposed to be about. In my blog post this week, I\u2019ll be taking a look at a variety of causes that ...\nWanda-sp\nWhat\u2019s great about the holiday season, though, is its warm and inviting people, and what a chance to meet and share\na cuppa with others who understand and have shared similar experiences, a feeling of empathy if you will, and a\ngreat opportunity to help others on a difficult journey. So I can say, all is well. But I\u2019m not giving up the fight ...\nFLAP\nWhat\u2019s great about the holiday season, and what makes it such an awesome time of the year, is that it\u2019s the time of\nthe year when people are so generous, and so goodhearted. When you get together with friends and family, and you\ngo through the various things that you\u2019re able to do, when you\u2019re able to give people gifts, when people can see ...\nLLM-Pruner\nWhat\u2019s great about the holiday season, is you can enjoy the company of friends and family. This is the perfect\nopportunity to gather together, and share all the special memories that you have together. These moments are the\nones that make the holiday special. No matter what happens, or what you end up doing, it\u2019s always good to get ...\nOurs: Taylor+\nWhat\u2019s great about the holiday season, is that it\u2019s a time of year when people can actually take a step back from the\nnormal day-to-day routines and take some time out to relax and enjoy some well-deserved rest and rejuvenation.\nThis might include indulging in a more comfortable or luxury holiday experience and this article aims to help ...\nOurs: PPL\nWhat\u2019s great about the holiday season, in my mind, is that it brings together friends, family, and food. The holiday\nseason is a great time to show some love and have some fun with the people that matter most. Whether you are\ncelebrating Christmas, Hanukah, or New Years, the festivities will be full of joy and memories that will last ...\nLLaMA-7B\nAI can create a logo in seconds. Why are these new technologies threatening our jobs? AI can do most menial\ntasks in just a few seconds or minutes. These tasks can be very tedious for human beings to do. For instance, AI\ncan identify patterns in large amounts of data so that complex statistical analysis can be carried out faster than a ...\nWanda-sp\nAI can create a logo in seconds rather than years. To this point, this is only the beginning of what AI will do to\nchange our human lives. As time goes on, it\u2019s quite likely that we\u2019ll see even greater use of robots. Even if this all\nsounds too dramatic, this may still happen. And AI is the best thing that can make it to happen. in 2019. All you ...\nFLAP\nAI can create a logo in seconds and it doesn\u2019t cost you a penny. The new era of technology has opened up many\nexciting opportunities. Technology, like AI is being utilized for different purposes in different ways, and now,\ntechnology is becoming a solution to problems that would have been tedious or expensive only a few years ago. ...\nLLM-Pruner\nAI can create a logo in seconds, design a website from scratch and even write blog posts without any human inter-\nvention. This revolution is changing the way humans communicate and express ideas. In our view, this technology\nis a great asset but we\u2019ll need to be smart in how we use it to prevent abuse or automation leading to job loss. So, ...\nOurs: Taylor+\nAI can create a logo in seconds or design a new website. There is a lot of software that exists in these two categories.\nIt has become easier and faster for people to use. However, there is still the requirement for the designer or creative\ndirector to ensure that the designs are relevant and meaningful to the brand. The Future: AI for creative design ...\nOurs: PPL\nAI can create a logo in seconds, and it can design entire websites, all without human intervention. However, there is\nstill a lot of work to be done to perfect this technology, including the development of natural language processing,\nmachine learning and computer vision, as well as ensuring that AI designers are creating work that can be ...\nLLaMA-7B\nNeural network pruning is defined as the process of removing certain weights or neurons in a neural network to\nreduce the size of the neural network, and improve the training process. Neural networks pruning techniques have\nbeen applied successfully on classification, optimization, planning, and speech recognition tasks. A pruned ...\nWanda-sp\nNeural network pruning is defined as any method that removes the redundant information by applying the neural\nnetwork network. It is the term used to identify the systematic variables when an individual tries to find an algo-\nrithm. In addition, it is the method of classification used to define any set of algorithms that have been ...\nFLAP\nNeural network pruning is defined as a selection of neurons or weights in a neural network so that the information\nof the network is reduced to a specified level, and a process of re-training a neural network. The latter one is the\nmost popular method, especially the neural network pruning. In recent years, this field has been studied in ...\nLLM-Pruner\nNeural network pruning is defined as reducing the complexity and size of an over-parameterized neural network.\nThe pruning reduces the FLOPs and improves performance for the particular task. Pruning techniques have gained\npopularity in recent years due to their wide range of applications. This paper aims to explore the pruning ...\nOurs: Taylor+\nNeural network pruning is defined as reducing the weight connections and biases of a neural network, while retain-\ning high performance. Pruning is an effective approach to reducing the number of parameters and improving the\nefficiency of neural networks. Recently, the concept of neural network shrinking has gained considerable ...\nOurs: PPL\nNeural network pruning is defined as the process of removing redundant neurons and connections between nodes in\na neural network, while keeping the performance or accuracy to the maximum extent possible. This optimization is\naimed at reducing the size and memory consumption of the network while maintaining its ability to perform task ...\nTable 11: Generation examples from the original LLaMA-7B and 20%-compressed models.\nModel\nGenerated Text\nVicuna-13B\nWhat\u2019s great about the holiday season, beyond the obvious time off and the chance to spend time with loved ones, is\nthe opportunity to slow down and reflect on the year that\u2019s passed. It\u2019s a time to look back at all the accomplishments\nand growth you\u2019ve experienced, and to set your sights on the year ahead. As a small business owner, the ...\nWanda-sp\nWhat\u2019s great about the holiday season, besides spending time with family and friends, is that it\u2019s a time of giving.\nIt\u2019s a time to show appreciation for those in your life who make a difference and bring joy to your world. Whether\nit\u2019s a gift of love, a gift of time, or a gift of money, it\u2019s important to remember that giving is not just about ...\nFLAP\nWhat\u2019s great about the holiday season, especially the days leading up to Christmas, is the opportunity to take a\npause and recharge. It\u2019s a time to reflect on the year and all the experiences we\u2019ve had, good or bad. It\u2019s a time to\nreflect on the people we love and the impact they\u2019ve had on our lives. It\u2019s a time to reflect on what we want ...\nLLM-Pruner\nWhat\u2019s great about the holiday season, aside from the gifts and the holiday parties, is the opportunity to spend time\nwith family and loved ones. One of the things that I love most about holiday seasons is the traditions that surround\nthem, from decorating the tree to making special holiday meals, there\u2019s a sense of nostalgia and warmth that ...\nOurs: Taylor+\nWhat\u2019s great about the holiday season, is that it allows us to reflect on the year gone by and reflect on the moments\nthat made it special. And while some people might choose to celebrate with the traditional traditions, some choose\nto do something different. One way to spice up your holiday season is by taking a trip to a far-off place. Why not ...\nOurs: PPL\nWhat\u2019s great about the holiday season, in addition to the fun food, festivities, and spending time with loved ones, is\nthat it creates the perfect opportunity to show people how much you appreciate them. From giving them a gift, to\nbaking their favorite treat, or just taking a moment to acknowledge them in front of others \u2013 there\u2019s something ...\nVicuna-13B\nAI can create a logo in seconds with its complex algorithms and pre-set designs. But it can never truly understand\nthe essence of a business, the emotions that go into creating a logo or the creativity and uniqueness that comes from\nhuman input. It\u2019s time to put the human touch back into branding, where a designer and client can collaborate to ...\nWanda-sp\nAI can create a logo in seconds using a logo design tool. However, a talented graphic designer can create a unique\nand meaningful logo that stands out from the crowd, captures the essence of the business, and appeals to the target\naudience. A good logo can also make a difference in how customers perceive the business and can even increase ...\nFLAP\nAI can create a logo in seconds. If you don\u2019t like it, you can change it yourself or pay for a designer to do it for\nyou. However, it\u2019s important to note that a logo is more than just an image. It\u2019s a visual representation of a brand\nor organization, and it communicates certain values and characteristics to the viewer. Therefore, it\u2019s essential to ...\nLLM-Pruner\nAI can create a logo in seconds with just one click, but design is a human art, something that is beyond AI capa-\nbilities. The logo design is an important part of a company\u2019s identity, and it\u2019s a decision that affects the company\u2019s\nvisual branding and reputation for years to come. The logo is the visual representation of a company\u2019s values ...\nOurs: Taylor+\nAI can create a logo in seconds, with or without a human\u2019s input. Some people might like this, because they can\ncreate a logo for a small business on a budget. Other people, like myself, think that the human touch is important\nin art and design. A human designer can create a unique and meaningful logo by incorporating personal ...\nOurs: PPL\nAI can create a logo in seconds by combining a variety of different styles and shapes and patterns that make up the\nlogos. However, a logo that is truly a reflection of your brand needs to be designed by a human, a graphic designer.\nThey will bring a creative vision that will make your logo unique, appealing and memorable. They can add ...\nVicuna-13B\nNeural network pruning is defined as the process of removing redundant or unnecessary connections in a neural\nnetwork to reduce its size and computational complexity while preserving its performance. The primary motivation\nbehind pruning is to improve the efficiency of deep learning models, especially in resource-constrained ...\nWanda-sp\nNeural network pruning is defined as a method for removing connections in order to reduce the model size, thus\nallowing for computationally efficient models while retaining accuracy. This technique is useful in reducing the\nnumber of neurons in a neural network, which allows for fewer computations during training and reduces the ...\nFLAP\nNeural network pruning is defined as the process of reducing the size of a neural network by removing the least\nsignificant weights or neurons. The main motivation for pruning is to reduce the computational cost of neural\nnetworks while maintaining their accuracy. The key challenge of pruning is to identify the most effective ...\nLLM-Pruner\nNeural network pruning is defined as a technique that aims to reduce the size of a neural network by removing\nredundant and irrelevant connections between the neurons in the network. This approach is based on the observation\nthat a large portion of the connections within the network is redundant and does not contribute to the overall ...\nOurs: Taylor+\nNeural network pruning is defined as the removal of redundant connections within a neural network to achieve a\nbetter model fit while retaining the network\u2019s general accuracy. The goal of pruning is to reduce the computational\ncost and memory footprint of the network. One commonly used pruning method is called weight magnitude ...\nOurs: PPL\nNeural network pruning is defined as the task of removing unnecessary or redundant connections in a neural network\nwhile retaining its accuracy and performance. This is often done to reduce the memory usage and computational\ncomplexity of a neural network, which can be critical when running on devices with limited resources. In ...\nTable 12: Generation examples from the original Vicuna-13B-v1.3 and 21%-compressed models.\nC\nAdditional Results of Inference Efficiency\nLLaMA-\n7B\nVicuna-\n13B\n128 Output Tokens\n512 Output Tokens\n12 Input Tokens\n82 Input Tokens\n12 Input Tokens\n82 Input Tokens\nModel\nM1\nM8\nM16\nM32\nM64\nM128\nM256\nM384\nM1\nM8\nM16\nM32\nM64\nM128\nM256\nM1\nM8\nM16\nM32\nM64\nM1\nM8\nM16\nM32\nM64\nM1\nM8\nM16\nM32\nM64\nM128\nM256\nM1\nM8\nM16\nM32\nM64\nM128\nM1\nM8\nM16\nM32\nM64\nM1\nM8\nM16\nM32\nFigure 6: Inference efficiency of pruned models on an NVIDIA H100 GPU. Our depth pruning achieves a superior latency-throughput trade-\noff for various sequence lengths of input and output. In contrast, the width pruning of FLAP [An et al., 2024] and LLM-Pruner [Ma et al.,\n2023] degrades efficiency results due to GPU-unfriendly weight dimensions [Andersch et al., 2019] (e.g., the hidden sizes of FFN are often\nnot divisible by 8). The markers labeled with M represent batch sizes. The dotted lines indicate that pruned models can operate with larger\nbatch sizes, avoiding out-of-memory errors encountered by the original model.\nD\nImplementation Details\nWe utilize the Hugging Face\u2019s Transformers library [Wolf et al., 2020] on a single NVIDIA A100 GPU (80GB VRAM). All\nthe experiments involving 7B-parameter models can be conducted on a single NVIDIA RTX 3090 (24GB VRAM).\n\u25e6 At the pruning phase, we assess the significance of Transformer blocks using a small calibration set (containing 10 samples\nfrom BookCorpus [Zhu et al., 2015] with a sequence length of 128). For the PPL-based criterion, the calibration samples\nare fed into networks with a single block removed, and this step is iterated across all the blocks in the target model. For the\nTaylor+ method, we feed the calibration data into the original network to collect backward-gradient matrices. The pruning\nis completed efficiently within 1 to 2 hours for the 7B- and 13B-sized models.\n\u25e6 At the retraining stage, we apply a LoRA adapter [Hu et al., 2022] to every projection weight matrix by following [Ma et\nal., 2023]. We employ a LoRA rank of 8, a learning rate of 0.0001, and a batch size of 64 over 2 epochs. The retraining\ncosts are notably low, with the entire process being executed on a single GPU. For example, retraining a 20%-pruned\nmodel from 7B parameters takes about 2 hours and utilizes 22GB GPU memory, while a 21%-pruned model from 13B\nparameters requires approximately 3 hours and 35GB VRAM.\n\u25e6 At the inference stage, we maintain default configurations without employing xFormers-optimized attention or additional\nadvanced features.\n"
  },
  {
    "title": "DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing",
    "link": "https://arxiv.org/pdf/2402.02583.pdf",
    "upvote": "7",
    "text": "DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing\nChong Mou1\nXintao Wang2\nJiechong Song1\nYing Shan2\nJian Zhang1\f\n1School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University\n2ARC Lab, Tencent PCG\nObject Moving\nContent Dragging\nObject Resizing\nEditing Output\nAppearance Replacing\nSource Input\nObject Pasting\nReference\nReference\nFigure 1.\nWe propose DiffEditor, which can perform various fine-grained image editing operations on general images. Given an image,\nusers can select an object to move or resize, or they can select sevaral pixel points for more accurate content dragging. Moreover, users can\nalso choose a reference image for cross-image editing, i.e., object pasting and appearance replacing.\nAbstract\nLarge-scale Text-to-Image (T2I) diffusion models have\nrevolutionized image generation over the last few years.\nAlthough owning diverse and high-quality generation ca-\npabilities, translating these abilities to fine-grained im-\nage editing remains challenging. In this paper, we propose\nDiffEditor to rectify two weaknesses in existing diffusion-\nbased image editing: (1) in complex scenarios, editing re-\nsults often lack editing accuracy and exhibit unexpected\nartifacts; (2) lack of flexibility to harmonize editing oper-\nations, e.g., imagine new content. In our solution, we in-\ntroduce image prompts in fine-grained image editing, co-\noperating with the text prompt to better describe the edit-\ning content. To increase the flexibility while maintaining\ncontent consistency, we locally combine stochastic differ-\nential equation (SDE) into the ordinary differential equa-\ntion (ODE) sampling. In addition, we incorporate regional\nscore-based gradient guidance and a time travel strategy\ninto the diffusion sampling, further improving the editing\nquality. Extensive experiments demonstrate that our method\ncan efficiently achieve state-of-the-art performance on var-\nious fine-grained image editing tasks, including editing\nwithin a single image (e.g., object moving, resizing, and\ncontent dragging) and across images (e.g., appearance re-\nplacing and object pasting). Our source code is released at\nhttps://github.com/MC-E/DragonDiffusion.\n1. Introduction\nText-to-image (T2I) diffusion models [29, 32, 33, 35] have\nbecome the mainstream of image generation, praised for\ntheir high-quality and diverse generation capability. The\npre-trained T2I models can serve as a good generation\nprior and can be used in various ways, e.g., image edit-\ning. Since the excellent text-to-image ability, numerous\ndiffusion-based image editing methods are implemented\nbased on the text guidance [5, 6, 11\u201313, 16]. However, the\ngenerated results of T2I models are usually sensitive to the\nquality of text [37]. Therefore, text-guided image editing\nstruggles to achieve fine-grained content manipulation.\nRecently, DragGAN [30] provides a user-friendly way\nto manipulate the image content by point dragging. How-\narXiv:2402.02583v1  [cs.CV]  4 Feb 2024\never, limited by the capacity of GAN [8] models, Drag-\nGAN cannot edit general images. Inspired by this interac-\ntive editing mode, DragDiff [39] and DragonDiff [28] are\nproposed based on the pre-trained T2I diffusion model [33].\nEmpowered by the diverse generation capabilities of the\nbase model, they can perform fine-grained editing on gen-\neral images. However, their editing process lacks flexibility,\nas shown in Fig. 2. Concretely, the image editing operation\nof transforming a lion from a closed mouth to a widely open\nmouth conflicts with the LORA [34] in DragDiff, resulting\nin failure. The visual cross-attention designed in Dragon-\nDiff also makes it struggle to imagine new content (e.g.,\nmouth) that is not present in the source image, causing fail-\nure too. In addition, these two methods and most diffusion-\nbased image editing methods employ ordinary differential\nequations (ODE) [41] solver, a deterministic sampling pro-\ncess. Although ODE can better maintain the consistency be-\ntween the edited results and the source image, its determi-\nnacy also limits flexibility during the editing process. Com-\npared to ODE, stochastic differential equations (SDE) [15]\nis a stochastic sampling process. Some works [48, 49] study\nthe latent space of SDE for accurate image editing. Unlike\nthese works, we aim to utilize the stochasticity in SDE to\nimprove the flexibility of diffusion-based image editing, as\nshown in the last image of Fig. 2.\nAnother insight is that although DragDiff and Dragon-\nDiff utilize feature correspondence in the pre-trained T2I\ndiffusion model to achieve fine-grained image editing, the\nrole of the text input is ignored in their frameworks. Here,\nwe raise a question: Does the text have no effectiveness in\nfine-grained image editing, or is there another more suitable\nform of text input? In addition to the text prompt, DALL-\nE2 [32] presents a novel attempt to generate images condi-\ntioned on the image prompt, i.e., using images to describe\nimages. Subsequently, some multimodal works [20, 24, 25]\nand object-customization works [22, 47, 51] are proposed\nto support image prompts for more detailed content descrip-\ntion. Inspired by these works, we introduce image prompts\ninto the fine-grained image editing process, improving edit-\ning quality through more detailed content descriptions. In\naddition, we combine regional score-based gradient guid-\nance and a time travel strategy into diffusion sampling,\nwhich further enhances the editing quality.\nIn summary, this paper has the following contributions:\n\u2022 We present a novel attempt to introduce the image prompt\nto fine-grained image editing tasks. In conjunction with\nthe image editing algorithm, this design can provide a\nmore detailed description of the editing content, thus im-\nproving the editing quality.\n\u2022 We consider both the flexibility and content consistency\nof image editing by proposing regional SDE sampling and\nregional score-based gradient guidance. We also intro-\nduce a time travel strategy in diffusion-based image edit-\nDragDiff\nDiffEditor (Ours)\nOriginal Image\nDragonDiff\nFigure\n2.\nIllustration\nof\nediting\nflexibility\nlimitations\nin\nDragDiff [39] and DragonDiff [28], as well as our improvement.\ning to improve the editing quality further.\n\u2022 Extensive experiments demonstrate that our method can\nachieve state-of-the-art performance on various fine-\ngrained image editing tasks (i.e., content dragging, object\nmoving, resizing, pasting, and appearance replacing, as\nshown in Fig. 1) with attractive complexity.\n2. Related Work\n2.1. Diffusion Model\nDiffusion model [15] is a thermodynamics-driven [40, 42]\nalgorithm, including a diffusion process and a reverse pro-\ncess. In the diffusion process, an image x0 is gradually\nadded Gaussian noise as q(xt|x0) = N(\u221a\u03b1tx0, (1\u2212\u03b1t)I),\nwhere \u03b1t linearly decreases from 1 to a sufficiently small\nnumber to encourage xT \u223c N(0, I). The reverse process\nis to iteratively recover x0 from xT by training a denoiser,\nconditioned on the current noisy image xt and time step t:\nEx0,t,\u03f5t\u223cN (0,1)\n\u0002\n||\u03f5t \u2212 \u03f5\u03b8(xt, t)||2\n2\n\u0003\n,\n(1)\nwhere \u03f5\u03b8 is the function of the denoiser. DDIM [41]\ndefines the diffusion sampling as q(xt\u22121|xt, x0)\n=\nN(\u221a\u03b1t\u22121x0 +\np\n1 \u2212 \u03b1t\u22121 \u2212 \u03b42\nt \u00b7 xt\u2212\u221a\u03b1tx0\n\u221a1\u2212\u03b1t\n, \u03b12\nt I), which\nis a non-Markovian process and can be formulated as:\nxt\u22121 =\n\u221a\u03b1t\u22121\nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5t\n\u03b8 (xt)\n\u221a\u03b1t\n|\n{z\n}\n\u201dpredicted x0 \u201d\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u00b7 \u03f5t\n\u03b8 (xt)\n|\n{z\n}\n\u201ddirection pointing to xt \u201d\n+ \u03c3t\u03f5\n|{z}\n\u201dnoise\u201d\n,\n(2)\n...\nDDIM \nInversion\nDecoder\n\ufffd0 \ufffd0\nref\n(Optional)\nImage Prompt \nEncoder\nText \nEncoder\nEncoder\n\u201cA photo of a cup\u201d\nLearnable Queries\n\u00d7 (\ufffd \u2212 1)\nSDE\nOriginal Image\nEditing Result\n...\nTime Travel\nCross- \nAttention\nSelf- \nAttention\n(\ufffd\ufffd, \u00a0\ufffd\ufffd\n\ufffd\ufffd\ufffd, \ufffd\ufffd\n\ufffd\ufffd\ufffd)\n\ufffd\nSDE\n...\n...\n...\n\ufffd\ufffd\n\ufffd\ufffd\ufffd,\nMemory Bank\n\ufffd\ufffd\n\ufffd\ufffd\ufffd,\n\ufffd\ufffd\n\ufffd\ufffd\ufffd,\n\ufffdT\n\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffdim\n\ufffd\n(\ufffd\ufffd, \u00a0\ufffd\ufffd+1\n\ufffd\ufffd\ufffd, \ufffd\ufffd+1\n\ufffd\ufffd\ufffd)\n\ufffd\n\ufffdT\u22121\n\ufffdt\n\ufffd0\nFigure 3. Overview of our proposed DiffEditor, which is composed of a trainable image prompt encoder and a diffusion sampling with\nediting guidance that does not require training.\nwhere \u03c3t\n=\n\u03b7\np\n(1 \u2212 \u03b1t\u22121) / (1 \u2212 \u03b1t)\np\n1 \u2212 \u03b1t/\u03b1t\u22121.\nWhen \u03b7 = 1 for all t, it becomes DDPM [15], i.e., a stochas-\ntic differential equation (SDE). As \u03b7 = 0, the sampling\nprocess becomes deterministic, i.e., an ordinary differential\nequation (ODE). Most diffusion-based image editing works\nrely on ODE to achieve better content consistency. [48, 49]\nexplore SDE in diffusion-based image editing.\nMost current works focus on conditional diffusion gen-\neration, such as text conditions [29, 33], which have greatly\nrevolutionized the community of image generation. Al-\nthough promising T2I generation quality is achieved, the\ngenerated results are sensitive to text quality and usually\nrequire tedious prompt design [37]. In addition to text con-\ndition, DALL-E2 [32] presents the first attempt to gener-\nate images guided by image prompts. ELITE [47], Bilp-\nDiffusion [22], and IP-Adapter [51] present the learning of\nimage prompts for object customization. However, the ef-\nfectiveness of image prompts in fine-grained image editing\nhas hardly been studied.\n2.2. Image Editing\nThe primary objective of image editing is to manipulate\nthe content of a given image in a controlled manner. Pre-\nvious methods [1\u20133] usually invert images into the latent\nspace of GANs [8] and then edit the image by manipu-\nlating latent vectors. Recently, DragGAN [30] presents a\npoint-dragging formulation for fine-grained image editing.\nHowever, limited by the capability of GANs, these meth-\nods have weaknesses in model generalization and image\nquality. Motivated by the success of text-to-image diffu-\nsion models [33], various text-guided image editing meth-\nods [4, 6, 13, 18, 27] are proposed. The commonly used\nediting strategies are (1) adding noise and then denoising\nwith target description [7, 18, 21, 27, 46]; (2) using cross-\nattention maps as an editing medium [11, 13, 16]; (3) us-\ning text as editing instructions [6]. However, the corre-\nspondence between the text and image in T2I models is\nweak, making it difficult to achieve fine-grained image edit-\ning. Recently, DragDiff [39] and DragonDiff [28] achieve\nfine-grained image editing based on the feature correspon-\ndence [45] in the pre-trained StableDiffusion (SD) [33].\nSpecifically, DragDiff uses LORA [34] to maintain content\nconsistency and optimizes the latent zt in a specific diffu-\nsion step. DragonDiff is built based on the score-based [44]\ngradient guidance [9] and a visual cross-attention design for\ndrag-style image editing without model tuning.\n3. Method\n3.1. Preliminary: Score-based Editing Guidance\nFrom the continuous perspective of score-based diffu-\nsion [43, 44], the external condition y can be combined in\na conditional score function, i.e., \u2207xt log q(xt|y), to sam-\nple from a more enriched distribution. The conditional score\nfunction can be further decomposed as:\n\u2207xt log q(xt|y) = \u2207xt log\n\u0012q(y|xt)q(xt)\nq(y)\n\u0013\n\u221d \u2207xt log q(xt) + \u2207xt log q(y|xt),\n(3)\nwhere the first term is the unconditional denoiser, i.e.,\n\u03f5t\n\u03b8(xt). The second term refers to the conditional gradient\nproduced by an energy function E(xt, y) = log q(y|xt),\nmeasuring the distance between current state xt and condi-\ntion y. Here, we reformulate Eq. 3 as:\n\u02dc\u03f5t\n\u03b8(xt) = \u03f5t\n\u03b8(xt) + \u03b7 \u00b7 \u2207xtE(xt, y),\n(4)\nwhere \u03b7 refers to the learning rate. Recently, Self-\nGuidance [11] and DragonDiff [28] convert image editing\noperations into gradient guidance for image editing tasks.\nThe energy function E in Self-Guidance is built based on\nthe correspondence [13] between image and text features.\nDragonDiff constructs the energy function based on image\nfeature correspondence [45] in pre-trained SD, which can\nachieve more accurate drag-style editing tasks. In this paper,\nwe aim to boost the accuracy and flexibility of diffusion-\nbased image editing with the DragonDiff framework.\n...\nCross- Attention\nFeed Forward\n...\n\u00d7 64\n...\n\u00d7 \ufffd\n...\n\u00d7 257\nLinear\nLearnable Queries\nImage Embeddings\nImage \nEncoder\nFigure 4. Illustration of the design of our image prompt encoder.\n3.2. Overview\nAn overview of our image editing pipeline is presented in\nFig. 3. Specifically, given an image x0 to be edited, we\nfirst employ it as image prompts and use an image prompt\nencoder to embed it. These image embeddings cooperate\nwith text embeddings to form a better description to guide\nthe diffusion process. Then we use DDIM inversion [41]\nto transform x0 into a Gaussian distribution zT in the la-\ntent space of the pre-trained SD [33]. If the reference image\nxref\n0\nexists (i.e., in appearance replacing and object past-\ning), it will also be involved in the inversion. In this process,\nwe follow the design in DragonDiff [28] to store some in-\ntermediate features ( Kgud\nt\n, Vgud\nt\n, Kref\nt\n, Vref\nt\n) and latent\n(zgud\nt\n, zref\nt\n) at each time step in a memory bank, which is\nused to guide subsequent image editing. Note that \u201dgud\u201d\nand \u201dref\u201d represent the information of the source and ref-\nerence image in the inversion process, respectively. In the\nsubsequent generation sampling, we step forward with the\ncooperation of score-based editing guidance, visual cross-\nattention, and image prompt. In this process, some elabo-\nrately designed strategies (e.g., regional gradient guidance,\nregional SDE, and time travel) enhance the editing further.\n3.3. Content Description with Image Prompt\nAlthough several fine-grained image editing methods [28,\n39] are based on the T2I diffusion model, the role of\nprompts is ignored as a simple description. Compared to\ntext prompts, image prompts [22, 32, 51] can provide a\nmore detailed content description. In this paper, we find that\nthe image prompt can improve the quality of fine-grained\nimage editing, especially in some complex scenarios.\nInspired by IP-Adapter [51], the architecture of our im-\nage prompt encoder is shown in Fig. 4. Concretely, given an\ninput image x0, the pre-trained CLIP [31] image encoder\nembed it to 257 tokens. Then, a linear layer is used to ad-\njust the channel dimension, and a QFormer [23] (without\nself-attention layer) module is employed to adjust the token\nnumbers to 64 by 64 learnable queries. The QFormer mod-\nw/o Visual \ncross-attention\nRandom \ufffdT\nw/o Visual cross-\nattention+Random \ufffdT\nOriginal Image\nDragonDiff\nFigure 5. The impact of different components on the editing flexi-\nbility of DragonDiff [28].\nule consists of N (8 by default) submodules, each composed\nof a cross-attention layer and a feed-forward network (FN).\n64 learnable queries serve as queries to extract information\nfrom 257 image tokens that act as keys and values. Finally,\n257 image tokens are composed into 64 embedding tokens\n(cim), and then they are input into the same cross-attention\nmodule as text tokens (c) in the SD. To build classifier-free\nguidance [14] like the text condition, the conditional and un-\nconditional image prompts are jointly trained by randomly\ndropping (i.e., set image to zero) during training. Finally,\nimage tokens and text tokens are processed separately with\nthe query Q in the cross-attention module, and the results\nare added together:\nAtt(Q, K\n\u2032, V\n\u2032, K\n\u2032\u2032, V\n\u2032\u2032) = S( Q(K\n\u2032)T\n\u221a\nd\n)V\n\u2032 + \u03b3 \u00b7 S( Q(K\n\u2032\u2032)T\n\u221a\nd\n)V\n\u2032\u2032,\n(5)\nwhere (K\n\u2032, V\n\u2032) and (K\n\u2032\u2032, V\n\u2032\u2032) refer to the keys and values\nfrom the text and image prompt, respectively. \u03b3 is a weight\nto balance these two terms. S is the function of Softmax.\nNote that in tasks with reference images (i.e., object pasting\nand appearance replacing), K\n\u2032\u2032 and V\n\u2032\u2032 are formed by the\nconcatenation of image tokens from the source image and\nthe reference image. During training, we fix the parameters\nin the pre-trained SD and CLIP image encoder, and we only\noptimize the linear embedding and QFormer by L2 loss:\nEx0,t,\u03f5t\u223cN (0,1)\n\u0002\n||\u03f5t \u2212 \u03f5t\n\u03b8(zt, c, cim)||2\n2\n\u0003\n.\n(6)\nAfter a single training, this module can be integrated into\npre-trained SD for various image editing tasks, as demon-\nstrated in this paper.\n3.4. Sampling with Regional SDE\nMaintaining consistency between editing results and origi-\nnal images is a great challenge in fine-grained image edit-\ning. Most methods adopt a deterministic sampling process\nOriginal Image\nt=901\nt=801\nt=701\nt=601\nt=501\nt=401\nEditing Gradient from \ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 6. Visualization of the editing gradient from Eedit in differ-\nent sampling steps.\n(ODE) and utilize DDIM inversion for sampling initializa-\ntion. In addition, DragDiff [39] uses LORA [34] to con-\nstrain the output content, and DragonDiff [28] uses vi-\nsual cross-attention to maintain content consistency. How-\never, these strategies also compromise editing flexibility,\ne.g., hindering the imagination of new content to harmonize\nediting operation as shown in Fig. 2. Our further experi-\nments on DragonDiff show that reducing the content con-\nsistency strength can improve editing flexibility. As seen\nin Fig. 5, the editing flexibility is improved when we ran-\ndomly initialize the sampling starting point zT or remove\nvisual cross-attention. When we apply both reductions, the\nediting objective can be achieved flexibly, but the content\nconsistency is severely compromised. Therefore, in this pa-\nper, we explore how to improve editing flexibility without\nsignificantly impacting content consistency. In the sampling\nprocess (i.e., Eq. 2) of DragonDiff, \u03c3t = 0, which is a de-\nterministic ODE sampling. This leads to the final result be-\ning highly dependent on zT and the information injected\nby visual cross-attention. Our solution is to introduce ran-\ndomness (i.e., \u03c3t > 0) during the sampling process, while\nthis randomness is controlled within local editing areas and\nspecific time intervals. Here, we use zt\u22121 = F(zt; \u03c3t) to\nsimplify Eq. 2. Our regional SDE sampling is defined as:\nzt\u22121 = medit \u00b7 F(zt; \u03b71(t)) + (1 \u2212 medit) \u00b7 F(zt; \u03b72(t)),\n(\u03b71(t), \u03b72(t)) =\n\u001a (0.4, 0.2),\nt \u2208 \u03c4SDE\n(0, 0),\nt /\u2208 \u03c4SDE\n(7)\nwhere medit locates the editing area. \u03c4SDE is the time in-\nterval for applying regional SDE. After using this sampling\nstrategy, we can accurately inject flexibility to produce sat-\nisfactory results, as shown in the last image of Fig. 2.\n3.5. Editing with Gradient Guidance\nRegional gradient guidance. In DragonDiff [28], the en-\nergy function E consists of two parts, i.e., editing Eedit\nand content consistency Econtent. Although their target ar-\neas are independent of each other, the scope of the gradient\nAlgorithm 1: Proposed DiffEditor\n1 Require:\n2 pre-trained SD [33] \u03f5\u03b8; image to be edited x0; mask\nof the editing region medit; gradient-guidance\nsteps n; time interval \u03c4SDE for SDE; time travel\ninterval \u03c4T T and the internal iterations U.\n3 Initialization:\n4 (1) Compute text embedding c and image image\nembedding cim\n5 (2) invert x0 to zgen\nT\nand build the memory bank\n6 for t = T, . . . , 1 do\n7\nif t \u2208 \u03c4T T and t%2 == 0 then\n8\nUcur = U\n9\nelse\n10\nUcur = 1\n11\nfor u = 1, . . . , Ucur do\n12\nnoise prediction: \u02c6\u03f5t = \u03f5\u03b8(zt, t, c, cim);\n13\nif T \u2212 t < n and t%2 == 0 then\n14\ncompute Eedit and Econtent by [28];\n15\ncompute \u2207zt log q(y|zt) by Eq. 8;\n16\ninject gradient guidance by Eq. 3;\n17\ncompute zt\u22121 by Eq. 7;\n18\nelse\n19\ncompute zt\u22121 by Eq. 2 (\u03c3t = 0);\n20\nif Ucur > 1 then\n21\nzt = zt\u22121\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(zt,t,c)\n\u221a\u00af\u03b1t\n22 x0 = Decoder(z0);\n23 Output: x0\nguidance they generate is global and overlapping, resulting\nin mutual interference. Concretely, in Fig. 6, we visualize\nthe editing gradient produced by Eedit in the object mov-\ning task. As can be seen, the gradient guidance gradually\nconverges to the editing area as the diffusion sampling pro-\nceeds. During this process, there are some activations out-\nside the editing area, and these imprecise activations can af-\nfect the content consistency in these unedited areas (details\nare presented in Sec. 4.3). To rectify this weakness, we use\nthe editing region mask medit to locally combine Eedit and\nEcontent. Finally, the conditional term in Eq. 3 is defined as:\n\u2207zt log q(y|zt) = medit \u00b7\u2207xtEedit +(1\u2212medit)\u00b7\u2207xtEcontent,\n(8)\nwhere y is the editing target. During the sampling, we only\nadd guidance in the first n time steps.\nTime travel. DragDiff [39] treats zt as a learnable param-\neter and iteratively optimizes it within a diffusion step t.\nIn contrast, DragonDiff [28] incorporates score-based gra-\ndient guidance into each sampling step, i.e., Eq. 4. How-\never, applying editing guidance through Eq. 4 once at each\nTable 1. Quantitative evaluation on face manipulation with 68 and 17 points. The accuracy is calculated by MSE distance between edited\npoints and target points. The initial distance (i.e., 57.19 and 36.36) is the upper bound, without editing. FID [38] is utilized to quantize the\nediting quality of different methods. The time complexity is computed on the \u20181 point\u2019 dragging.\nPreparing\ncomplexity\u2193\nInference\ncomplexity\u2193\nUnaligned\nface\n17 Points\u2193\nFrom 57.19\n68 Points\u2193\nFrom 36.36\nFID\u2193\n17/68 points\nUserControllableLT [10]\n1.1s\n0.04s\n%\n32.32\n24.15\n51.20/50.32\nDragGAN [30]\n50.22s\n6.28s\n%\n15.96\n10.60\n39.27/39.50\nDragDiff [39]\n42.37s\n19.52s\n!\n22.95\n17.32\n38.06/36.55\nDragonDiff [28]\n3.53s\n15.00s\n!\n18.51\n13.94\n35.75/34.58\nDiffEditor (Ours)\n3.53s\n13.88s\n!\n17.05\n11.52\n33.10/33.02\nReference\nSource\nUserControllableLT\nDragGAN\nDragDiff\nOurs\n68 Points\n17 Points\nDragonDiff\nMSE=8.53\nMSE=11.19\nMSE=27.95\nMSE=9.54\nMSE=32.81\nReference\nSource\nMSE=13.92\nMSE=15.36\nMSE=15.49\nMSE=15.70\nMSE=32.24\n68 Points (Unaligned)\nReference\nSource\nFigure 7. Qualitative comparison between our DiffEditor and other methods in face manipulation. The current and target points are labeled\nwith red and blue. The white line indicates distance. The MSE distance between the result and the target is labeled in yellow.\nsampling step lacks refinement for editing, especially in\nsome complex scenarios. Can we combine the advantages\nof DragDiff and DragonDiff to build recurrent guidance in\nthe score-based diffusion [44]? To address this issue, we\nbuild time travel to perform rollbacks, i.e., zt \u2190 zt\u22121, dur-\ning the sampling process. This strategy has been empirically\nshown to inhibit the generation of disharmonious results\nwhen solving hard generation tasks [26, 52]. However, the\nrollback strategy (i.e., zt \u223c N(\np\n1 \u2212 \u03b2t\u22121zt\u22121, \u03b2t\u22121I))\nin these works is not suitable in fine-grained image editing\ntasks. This is because random noise I can introduce signif-\nicant uncertainty, undermining the content consistency of\nediting results. To ensure the accuracy of rollback, we use\ndeterministic DDIM inversion [41] to roll back zt\u22121 to zt.\nDuring sampling, the time travel is performed U (3 in our\ndesign) times for each guidance step in a time interval \u03c4T T .\nDue to the guidance enhancement from regional guid-\nance and time travel, we can achieve editing with fewer\nguidance time steps, i.e., we introduce gradient guidance ev-\nery two time steps in sampling. Finally, the algorithm logic\nof our DiffEditor is defined in Alg. 1.\n4. Experiments\n4.1. Implementation Details\nWe choose Stable Diffusion V1.5 [33] as the base model\nfor image editing. During image prompt training, we use\nthe training data from LAION [36] and process the image\nresolution to 512 \u00d7 512. We choose Adam [19] optimizer\nwith an initial learning rate of 1 \u00d7 10\u22125. The batch size\nduring training is set as 16. The training process iterates\n1 \u00d7 106 steps on 4 NVIDIA A100 GPUs. We use the same\nembedding module to process image prompts in different\napplications. The inference adopts DDIM sampling with 50\nMoving\nReplacing\nPaint-by-example\nOurs\nSelf-guidance\nDragonDiffusion\nOurs\nSelf-guidance\nDragonDiffusion\nOurs\nSelf-guidance\nDragonDiffusion\nOurs\nSelf-guidance\nDragonDiffusion\nOurs\nDragonDiffusion\nOurs\nPaint-by-example DragonDiffusion\nPasting\nFigure 8. Visual comparison between our method and other methods on appearance replacing, object pasting and object moving tasks.\nTable 2. Quantitative evaluation on object pasting, object moving,\nand appearance replacing. The result is calculated by CLIP [31] \u2191\ndistance between editing results and target descriptions.\nPasting\nMoving\nReplacing\nPain-by-example\n0.265\n-\n-\nSelf-Guidance\n-\n0.246\n0.243\nDragonDiff\n0.260\n0.282\n0.263\nOurs\n0.274\n0.288\n0.281\nContent Dragging\nOurs w/o Image Prompt Ours w IP-Adapter-plus\nOurs\nFigure 9. Visual comparison between our image prompt and IP-\nAdapter-plus.\nsteps, and we set the classifier-free guidance scale as 5.\n4.2. Comparison\nTime complexity. We divide the time complexity of dif-\nferent methods into preparing and inference stages. The\npreparing stage involves Diffusion/GAN inversion and\nmodel tuning. The inference stage generates the editing re-\nsult from latent representation. The time complexity for\neach method is tested on one point dragging, with the im-\nage resolution being 512 \u00d7 512. All times are tested on an\nNVIDIA A100 GPU with Float32 precision. The results in\nTab. 1 present the attractive preparing complexity of our\nmethod, and the inference complexity is lower than existing\ndiffusion-based methods, i.e., DragDiff and DragonDiff.\nPerformance. First, we evaluate our method on con-\nDDIM Inversion with \nImage Prompt\nEditing with Text Prompt \nOriginal Image\nDDIM Inversion with \n\u201cA photo of a toy\u201d\nDDIM Inversion with \n\u201cA photo of a dragon\u201d\nEditing with Image Prompt \nFigure 10. The first and second rows show the effectiveness of the\nimage prompt in DDIM inversion and image editing, respectively.\ntent dragging by comparing it with some well-known\nGAN-based methods (i.e., UserControllableLT [10], Drag-\nGAN [30]) and recent diffusion-based methods (i.e.,\nDragDiff [39], DragonDiff) on the keypoint-based face ma-\nnipulation. We used the same test set as DragonDiff, i.e.,\n800 aligned faces from the CelebA-HQ [17] training set. We\nevaluate the editing performance under 17-point guidance\nand 68-point guidance. To quantify editing accuracy, we\ncalculated the MSE distance between the landmarks of the\nedited result and the target landmarks. In addition, we cal-\nculate FID [38] between the editing results and the CelebA-\nHQ training set to represent the image quality. The quanti-\ntative comparison is presented in Tab. 1. One can see that\nour method has a significant improvement in accuracy and\ngeneration quality compared to other diffusion-based meth-\nods, achieving comparable editing accuracy to DragGAN.\nAlthough DragGAN has higher editing accuracy on aligned\nfaces, its base model is specifically trained for aligned faces\nand cannot edit general faces, as shown in the last row of\nFig. 7. The qualitative comparison in Fig. 7 shows that our\nmethod has high editing accuracy and content consistency\nwhile maintaining good flexibility. For example, in the case\nwhere teeth need to be imagined, our DiffEditor can pro-\nduce more natural results. In contrast, DragDiff and Drag-\nonDiff have difficulties in imagining new content.\nIn addition to content dragging, we also compare with\nPaint-by-example [50] in object pasting, and we compare\nour method with Self-Guidance [11] and DragonDiff in ob-\nject moving and appearance replacing tasks. The results\nare presented in Fig. 8. As can be seen, although the spe-\ncially trained Paint-by-example can naturally integrate ob-\njects into an image, it is difficult to maintain the original\nobject identity. Our method performs better in object iden-\ntity and has richer texture details than DragonDiff. In object\nmoving and appearance replacing tasks, text-guided Self-\nGuidance lacks consistent constraints, making editing re-\nsults deviate from the original image. The editing accu-\nracy of DragonDiff still has room for improvement, e.g.,\ncolor and details. In comparison, our method has better con-\ntent consistency and editing accuracy. For quantization, we\ncompute the CLIP [31] distance between the edited results\nand the target description. We select 16 editing samples for\neach task. The results in Tab. 2 demonstrate the promising\nperformance of our method.\nDiscussion between our image prompt and IP-Adapter.\nAs mentioned above, there are several methods proposed to\nuse images as prompts to provide more accurate and cus-\ntomized descriptions for the generated results, such as IP-\nAdapter [51]. However, most of these methods focus on ob-\nject customization, and overemphasis on detail description\nwill compromise their performance. Therefore, IP-Adapter\ncompresses the image into a small number of tokens to\navoid detail descriptions. This paper studies introducing im-\nage prompts into fine-grained image editing. We use the Q-\nFormer structure to map the image into 64 tokens to enhance\nthe detail expression ability. Fig. 9 shows that IP-Adapter\nnot ideal for direct insertion into fine-grained image edit-\ning tasks due to the lack of detail description. Our method\ncan enhance the consistency of texture details between the\nedited result and the original image.\n4.3. Ablation Study\nIn this part, we study the effectiveness of some components\nin our DiffEditor.\nImage prompt. Image prompt provides a detailed descrip-\ntion of the editing content in our editing pipeline. We con-\nduct an experiment in Fig. 10 to demonstrate its effective-\nness. First, we apply it in the pure DDIM inversion and\nthen reconstruction. The result in the first row presents\nOriginal Image\nGradient Map\nGradient Map\nEditing with \ufffd\ufffd\ufffd\ufffd\ufffd\nEditing with \ufffd\ufffd\ufffd\ufffd\ufffd \u2219 \ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 11. Effectiveness of the regional gradient guidance. In ex-\nperiment, we remove the content consistency gradient Econtent.\nOriginal Image\nw/o Time Travel\nw random Time Travel w accurate Time Travel\nFigure 12. Visualization of editing without time travel, with ran-\ndom time travel, and with our accurate time travel.\nthat DDIM inversion based on the text prompt exhibits no-\nticeable distortions and is unstable. After using our image\nprompt, DDIM inversion can reconstruct stable and high-\nfidelity results. In the second row of Fig. 10, we show the\nediting with and without the image prompt. It can be seen\nthat the image prompt provides a better generation prior for\nediting content, reducing the probability of distortion.\nRegional gradient guidance. To rectify the interference\nbetween different gradient guidance, we use Eq. 8 to pro-\nduce the guidance. We demonstrate its effectiveness in\nFig. 11 by only using medit \u00b7 Eedit and Eedit. Note that we\nremove the content consistency guidance Econtent to high-\nlight the interference. The results show that if the actuating\nrange of Eedit is not constrained, the editing gradient will\nhave an impact on the consistency of some unrelated areas,\ne.g., distortion of the fingers in the background. After ap-\nplying regional constraints, the content of the background\npart has better consistency even without Econtent.\nTime travel. Time travel is used to build recurrent guidance\nin a single diffusion time step, thereby refining the editing\nMoving Editing\nSD-1.5\nSD-2.1\nSD-XL\nDragging Editing\nDragging Editing\nCustom model \non animate data\nCustom model \non 3D data\nSD-1.4\nFigure 13. Visualization of editing results on different diffusion models.\neffect. We present its effectiveness in Fig. 12. As can be\nseen, the editing result in the complex scenario has distor-\ntions without the time travel strategy. If using the random\ntime travel (i.e., zt \u223c N(\np\n1 \u2212 \u03b2t\u22121zt\u22121, \u03b2t\u22121I)), the ran-\ndomness will affect the consistency between the editing re-\nsult and the original image. After adopting our accurate time\ntravel, the editing quality is improved.\n4.4. Generalization of Different Components\nExcept image prompt encoder that requires a specific SD\nmodel, other components of our method are designed\nbased on diffusion theory, giving them good generalization.\nFig. 13 shows editing results of these components on differ-\nent versions of SD and custom SD tuned on specific data. It\nshows the promising generalization of these components on\ndifferent diffusion models. The reason we employ SD-1.5\nis that the content of editing results mostly comes from ex-\nisting images, thus the dependence on generation ability is\nrelatively small. Large generative models, like SDXL, pro-\nduce high computational costs in gradient guidance. Hence,\nwe use the efficient SD-1.5 as the base model.\n5. Conclusion\nIn this paper, we aim to rectify two issues in diffusion-based\nfine-grained image editing: (1) in complex scenarios, edit-\ning results often lack editing accuracy and exhibit unex-\npected artifacts; (2) lack of flexibility to harmonize editing\noperation, e.g., imagine new content. In our solution, we in-\ntroduce the image prompt into fine-grained image editing,\nwhich can provide a more detailed content description for\nthe edited image and reduce the probability of distortion.\nThis method can be plugged into various fine-grained im-\nage editing tasks without task-specific training. To improve\nthe editing flexibility, we propose a regional SDE strategy\nto inject randomness into the editing area while maintain-\ning content consistency in other areas. Furthermore, we in-\ntroduce regional score-based gradient guidance and a time\ntravel strategy into the editing process to improve the edit-\ning quality further. Extensive experiments demonstrate that\nour method can achieve promising performance in vari-\nous fine-grained image editing tasks, i.e., object moving,\nresizing, pasting, appearance replacing, and content drag-\nging. The complexity is also reduced compared with exist-\ning diffusion-based methods.\nLimitations Although our method improves the flexibil-\nity of diffusion-based image editing and reduces the prob-\nability of distortion, editing difficulties still exist in some\nscenarios that require a large amount of content imagi-\nnation, such as rotating a car by dragging its front. We\nthink that this is due to the base model SD. It has a\ndiverse generation space but lacks 3D perception of in-\ndividual objects. In our future work, we will enhance\nthe editing capabilities of diffusion models in this re-\ngard.\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2stylegan: How to embed images into the stylegan latent\nspace? In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 4432\u20134441, 2019. 3\n[2] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2stylegan++: How to edit the embedded images? In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 8296\u20138305, 2020.\n[3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and\nAmit Bermano.\nHyperstyle: Stylegan inversion with hy-\npernetworks for real image editing.\nIn Proceedings of\nthe IEEE/CVF conference on computer Vision and pattern\nrecognition, pages 18511\u201318521, 2022. 3\n[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 3\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 1\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n1, 3\n[7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 3\n[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai\nArulkumaran, Biswa Sengupta, and Anil A Bharath. Gen-\nerative adversarial networks: An overview. IEEE signal pro-\ncessing magazine, 35(1):53\u201365, 2018. 2, 3\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 3\n[10] Yuki Endo. User-controllable latent transformer for stylegan\nimage layout editing. In Computer Graphics Forum, pages\n395\u2013406. Wiley Online Library, 2022. 6, 7\n[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 1, 3, 8\n[12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun\nAkula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,\nand William Yang Wang. Training-free structured diffusion\nguidance for compositional text-to-image synthesis. arXiv\npreprint arXiv:2212.05032, 2022.\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1, 3\n[14] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2, 3\n[16] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and\nQiang Xu. Direct inversion: Boosting diffusion-based edit-\ning with 3 lines of code. arXiv preprint arXiv:2310.01506,\n2023. 1, 3\n[17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196, 2017. 7\n[18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 3\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[20] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023. 2\n[21] Gihyun Kwon and Jong Chul Ye.\nDiffusion-based image\ntranslation using disentangled style and content representa-\ntion. arXiv preprint arXiv:2209.15264, 2022. 3\n[22] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion: Pre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 2, 3, 4\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 4\n[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 2\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2\n[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11461\u201311471, 2022. 6\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2021. 3\n[28] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. Dragondiffusion: Enabling drag-style manipula-\ntion on diffusion models. arXiv preprint arXiv:2307.02421,\n2023. 2, 3, 4, 5, 6\n[29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav\nShyam,\nPamela\nMishkin,\nBob\nMcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\npages 16784\u201316804. PMLR, 2022. 1, 3\n[30] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\ngan: Interactive point-based manipulation on the generative\nimage manifold. arXiv preprint arXiv:2305.10973, 2023. 1,\n3, 6, 7\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763, 2021. 4, 7, 8\n[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 2, 3, 4\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 2, 3, 4, 5, 6\n[34] Simo Ryu. Low-rank adaptation for fast text-to-image diffu-\nsion fine-tuning, 2023. 2, 3, 5\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1\n[36] Christoph Schuhmann. 6\n[37] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias,\nand Yaniv Leviathan. A picture is worth a thousand words:\nPrincipled recaptioning improves image generation. arXiv\npreprint arXiv:2310.16656, 2023. 1, 3\n[38] Maximilian Seitzer.\npytorch-fid: FID Score for PyTorch.\nhttps://github.com/mseitzer/pytorch-fid,\n2020. Version 0.3.0. 6, 7\n[39] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-\nsion models for interactive point-based image editing. arXiv\npreprint arXiv:2306.14435, 2023. 2, 3, 4, 5, 6, 7\n[40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2\n[41] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDe-\nnoising\ndiffusion\nimplicit\nmodels.\narXiv\npreprint\narXiv:2010.02502, 2020. 2, 4, 6\n[42] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 2\n[43] Yang Song and Stefano Ermon.\nImproved techniques for\ntraining score-based generative models. Advances in neural\ninformation processing systems, 33:12438\u201312448, 2020. 3\n[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 3, 6\n[45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 3\n[46] Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis,\nYossi Matias, and Yaniv Leviathan.\nUnitune: Text-driven\nimage editing by fine tuning a diffusion model on a single\nimage. ACM Transactions on Graphics (TOG), 42(4):1\u201310,\n2023. 3\n[47] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual concepts\ninto textual embeddings for customized text-to-image gener-\nation. arXiv preprint arXiv:2302.13848, 2023. 2, 3\n[48] Chen Henry Wu and Fernando De la Torre. Unifying diffu-\nsion models\u2019 latent space, with applications to cyclediffusion\nand guidance. arXiv preprint arXiv:2210.05559, 2022. 2, 3\n[49] Chen Henry Wu and Fernando De la Torre. A latent space\nof stochastic diffusion models for zero-shot image editing\nand guidance. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7378\u20137387, 2023. 2,\n3\n[50] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 8\n[51] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 2, 3, 4, 8\n[52] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and\nJian Zhang. Freedom: Training-free energy-guided condi-\ntional diffusion model.\narXiv preprint arXiv:2303.09833,\n2023. 6\n"
  }
]