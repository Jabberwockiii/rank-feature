[
  {
    "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
    "link": "https://arxiv.org/pdf/2312.04985.pdf",
    "upvote": "34",
    "text": "SparQ Attention: Bandwidth-Efficient LLM Inference\nLuka Ribar * Ivan Chelombiev * Luke Hudlass-Galley * Charlie Blake Carlo Luschi Douglas Orr\nAbstract\nThe computational difficulties of large language\nmodel (LLM) inference remains a significant ob-\nstacle to their widespread deployment. The need\nfor many applications to support long input se-\nquences and process them in large batches typi-\ncally causes token-generation to be bottlenecked\nby data-transfer. For this reason, we introduce\nSparQ Attention, a technique for increasing\nthe inference throughput of LLMs by utilising\nmemory bandwidth more efficiently within the\nattention layers, through selective fetching of the\ncached history. Our proposed technique can be\napplied directly to off-the-shelf LLMs during in-\nference, without requiring any modification to the\npre-training setup or additional fine-tuning. We\nshow that SparQ Attention brings up to 8\u00d7 sav-\nings in attention data-transfers without substantial\ndrops in accuracy, by evaluating Llama 2, Mistral\nand Pythia models on a wide range of downstream\ntasks.\n1. Introduction\nTransformer models trained on large corpora of text have\nrecently shown remarkable performance on complex natural\nlanguage processing tasks (Achiam et al., 2023; Touvron\net al., 2023). This has been attributed to the in-context\nlearning capabilities that emerge with large-scale training,\nenabling arbitrary textual information (e.g. long instructions,\nchat histories, relevant documents) to be incorporated at\ninference-time (Wei et al., 2022).\nTo leverage the benefits of in-context learning there has\nbeen demand for LLMs to support increasingly long input\nsequences. However, the standard inference optimisation\nused to support in-context learning, key-value (KV) caching\n* Equal contribution.\nGraphcore Research, UK. Correspondence to Luka Ribar, Ivan\nChelombiev, Luke Hudlass-Galley, Douglas Orr {lukar, ivanc,\nlukehg, douglaso}@graphcore.ai.\nCode\navailable\nhere:\nhttps://github.com/\ngraphcore-research/llm-inference-research/\ntree/2024-01-paper\ndense\ntransfers\n1\n2\n1\n4\n1\n8\n128 MB\n256 MB\n512 MB\nAttention transfers per token\n0.2\n0.4\n0.6\n0.8\nSQuAD Accuracy\nDense\nSparQ Attention\nH2O\nLM-In\ufb01nite\nFlexGen (16-bit)\nFigure 1: Llama 2 13B SQuAD 1-shot performance versus\nattention transfers over a range of compression ratios. SparQ\nAttention achieves matching performance, while transfer-\nring between 1/8 and 1/4 as much data as the original dense\nmodel. The curves show the mean \u00b1 one standard error eval-\nuated over 4000 examples. This pattern is representative of\nthe performance across multiple models and tasks, shown\nin Figures A1 and A2.\n(Pope et al., 2023), is constrained by the need to fetch a large\namount of data from memory when processing batches of\nlong sequences. This in turn limits the speed at which tokens\ncan be generated\u2014a key usability metric for LLMs.\nThis bottleneck can be attributed to the auto-regressive na-\nture of transformer generation. For each token generated,\nthe full KV cache must be re-fetched from memory. The size\nof the KV cache scales linearly with the sequence length,\nas well as the batch size, thus rendering generation for long\nbatched sequences increasingly memory bandwidth limited.\nDespite this expensive cache-fetch at each step, tokens gen-\nerally only attend to a small part of the sequence at a time\n(Vig, 2019; Yun et al., 2020). If it were possible to efficiently\npredict which tokens will have high attention scores, mem-\nory bandwidth efficiency could be significantly increased by\nonly transferring the key-value pairs of high-scoring tokens.\nBuilding upon this idea, we present SparQ (Sparse Query)\nAttention, a technique for significantly improving the mem-\nory bandwidth efficiency of transformer inference. By ap-\nproximating attention scores using a subset of query and\n1\narXiv:2312.04985v3  [cs.LG]  12 Mar 2024\nSparQ Attention: Bandwidth-Efficient LLM Inference\nkey components, we fetch only the most relevant tokens\nfor each generation step, reducing the amount of data trans-\nferred without degrading the model.\nWe also provide a new set of challenging downstream task\nvariants which we use to evaluate SparQ Attention. These\nare based on existing tasks, modified to assess the model\u2019s\nability to utilise information from long input sequences\nfor multi-token generation. We show that SparQ Atten-\ntion performs favourably compared to other state-of-the-art\nmethods, giving up to 8\u00d7 compression without substantial\nloss in accuracy. SparQ Attention is robust across tasks and\nmodels, demonstrated by evaluation on Llama 2, Mistral\nand Pythia. We also provide benchmarks measured on IPU\nand GPU, showing the practical computational benefits of\nour approach.\n2. Background\nIn this section we provide a straightforward framework to\nunderstand the computational efficiency of sequence gen-\neration using transformer models (similar to the modelling\nintroduced by Kaplan et al. (2020)) and use it to motivate\ntransfer-efficient attention mechanisms.\nArithmetic intensity\nA compute unit capable of rA scalar\narithmetic operations per second is connected to a memory\nvia an interface that can transfer rM scalar elements per\nsecond, processing a workload requiring A arithmetic op-\nerations and M transfers. Assuming concurrent compute\nand data transfer, when the arithmetic intensity A/M of\nthe workload is less than the ratio rA/rM, execution time\nis limited by rM.\nSequence generation\nConsider a full transformer layer,\nwith N parameters, batch size B, and C elements in\nthe attention KV cache per batch element. We assume\nGrouped Query Attention (GQA) (Ainslie et al., 2023) with\ng grouped-query heads (g=1 for standard multi-head atten-\ntion). This implies the arithmetic intensity:\nA\nM = BN + BCg\nN + BC\n= N + Cg\nN/B + C\n(1)\nWe can increase arithmetic intensity by making B large,\ncausing A/M to approach N/C + g. Hence the limiting\nfactor for large-batch transformer inference is the ratio of\nthe KV cache size per-item to the size of the model.\nWe can alternatively express this in terms of the model\u2019s\nbasic hyperparameters. A standard transformer with model\ndimension dm and sequence-length S, has N = 12(dm)2\nand C = 2 S dm/g, giving:\nA\nM = 6 + \u03c1 g\n6/B + \u03c1\n(2)\nTable 1: Excess correlation ratio \u03b7 (Roche et al., 1998)\nalong axes of V (excess: subtract d\u22120.5, so uniform random\ndata = 0.0). This demonstrates substantial auto-correlation\nalong the sequence axis. Calculated for Llama 7B over 40\nSQuAD examples.\nB\nS\nLayer\nHead\ndh\n\u03b7\u2212d\u22120.5\n0.143\n0.256\n0.0\n0.0\n0.0\nwhere \u03c1 = S/(gdm). The value \u03c1 underlies the KV cache-\nmodel size relationship outlined above, determining the\npoint at which the model becomes memory bandwidth\nbound.\nFor typical examples (see Appendix C), sequence genera-\ntion exhibits a large-batch arithmetic intensity of just 7 for\nmulti-head attention with S = dm, up to 100 for grouped-\nquery attention with S \u226a dm, while ML hardware can\nprovide rA/rM > 200. This means that data transfer is\nthe performance-limiting factor, motivating the search for\ntransfer-efficient alternatives to full attention.\n3. Approximating Attention\nIn this section we examine several properties of the attention\noperation that enable us to introduce an accurate bandwidth-\nefficient approximation.\nConsider a single attention query head with the head dimen-\nsion dh, processing an input token sequence of length S.\nDuring autoregressive generation, the output of the attention\nhead is calculated as:\ny = softmax\n\u0010q \u00b7 K\u22a4\n\u221adh\n\u0011\n\u00b7 V\n(3)\nwhere q is the query, and K \u2208 RS\u00d7dh and V \u2208 RS\u00d7dh are\nthe key and value caches respectively. When using GQA\n(Ainslie et al., 2023), K and V are shared across g query\nheads.\nFor each forward pass, we need to fetch the key and value\nmatrices from memory, as well as write (append) k and\nv vectors for the current token, giving a total number of\nelements transferred per attention head:\nMbase = 2 S dh\n| {z }\n1\u20dd\n+ 2 dh\n|{z}\n2\u20dd\n(4)\nwhere 1\u20dd corresponds to reading the K and V caches and\n2\u20dd corresponds to writing the current k and v to memory.\nAttention scores sparsity\nFirst, consider the attention\nscores s \u2208 (0, 1)S in Equation (3):\ns = softmax\n\u0010q \u00b7 K\u22a4\n\u221adh\n\u0011\n(5)\n2\nSparQ Attention: Bandwidth-Efficient LLM Inference\nq[i1]\n0.8\n\u22120.2\n\u22121.3\n0.4\n\u2297\nK[i1,:]\nsequence dimension\napproximate attention scores \u02c6s\n\u03b1 =\nX\ni\u2208i2\n\u02c6si\nq\n0.8\n\u22120.2\n\u22121.3\n0.4\n\u2297\nK[:,i2]\nsparse attention scores s\n\u2297\nV[:,i2]\n\u00afv = mean(V )\n\u2295\ny\n\u00d7(1 \u2212 \u03b1)\n\u00d7\u03b1\nAlgorithm 1 SparQ Attention\nInput: q \u2208 Rdh, K \u2208 RS\u00d7dh, V \u2208 RS\u00d7dh, v \u2208 Rdh,\nr \u2208 N, k \u2208 N, l \u2208 N\n# Indices of top r elements of |q|\ni1 \u2190 argtopk (|q|, r)\n# Softmax temperature, weighted by L1 coverage\n\u03c4 \u2190\ns\ndh \u00b7 \u2225q[i1]\u22251\n\u2225q\u22251\n# Approximate attention scores (all positions)\n\u02c6s \u2190 softmax\n\u0000q[i1] \u00b7 K\u22a4\n[i1,:]/\u03c4\n\u0001\n# Local mask of last l positions\nm \u2190 [1 if i > S \u2212 l else 0]S\ni=1\n# Indices of top k approximate scores or local\ni2 \u2190 argtopk (\u02c6s + m, k)\n# Total approximate score of top k\n\u03b1 \u2190 sum\n\u0000\u02c6s[i2]\n\u0001\n# Final attention scores (top k positions)\ns \u2190 softmax\n\u0000q \u00b7 K\u22a4\n[:,i2]/\u221adh\n\u0001\n# Mixed scores and values, interpolating with v\ny \u2190 \u03b1 s \u00b7 V[:,i2] + (1 \u2212 \u03b1) v\nreturn y\nFigure 2: SparQ Attention for a single attention head. The algorithm consists of three steps. First, we find the r largest\ncomponents of the incoming query vector and gather the corresponding components along the hidden dimension of the key\ncache K. This allows us to approximate the full attention scores (\u02c6s). In the second step, we identify the top-k largest scores\nin the approximation and proceed to gather the corresponding full key and value vectors from the cache. As a final step, to\ncompensate for the missing value vectors, we additionally maintain and fetch the running mean value vector \u00afv and reassign\nit the leftover mass based on approximate score weightings. The attention output is then calculated as usual using the top-k\nfetched key and value pairs, together with \u00afv.\nDue to the normalising effect of the softmax function, the\nresulting s vector is sparse (see Figures 3a and 3b), i.e. we\ncan find a boolean mask ms \u2208 {0, 1}S corresponding to\nthe top-k elements in s (k \u226a S) such that:\ny1 = (s \u25e6 ms) \u00b7 V \u2248 s \u00b7 V\n(6)\nAs a result, only the values vi corresponding to the non-zero\nelements of ms need to be fetched from memory. However,\nthe algorithm still requires fetching the full K from memory\nin order to calculate the attention scores s, limiting the\nminimum amount of data transferred to 1\n2Mbase.\nMean value reallocation\nIn order to further improve\nthe approximation in Equation (6), we note a further ob-\nservation: vi vectors within the sequence exhibit a high\ndegree of auto-correlation (see Table 1). Thus, an addi-\ntional correction term using a running-mean value vector\n\u00afv = 1\nS\nPS\ni=1 vi can be added as follows:\ny2 = (s \u25e6 ms) \u00b7 V + (1 \u2212 s \u00b7 ms)\u00afv\n(7)\nThis introduces a minimal additional overhead compared to\nEquation (6) as the mean vector \u00afv needs to be updated and\nwritten back to memory at each step.\nQuery sparsity\nIn order to improve the lower bound on\nmemory transfers, we further consider efficiently approxi-\nmating the mask ms by calculating approximate attention\nscores \u02c6s without using the full matrix K. Here, we con-\nsider the distribution of magnitudes of the components of the\nquery vector q and observe that it is highly heavy-tailed (see\nFigures 3c and 3d). This observation allows us to efficiently\napproximate the attention scores s by defining a per-query\nboolean mask mq \u2208 {0, 1}dh corresponding to the top-r\n3\nSparQ Attention: Bandwidth-Efficient LLM Inference\ncomponents of q. The scores are then approximated as:\n\u02c6s = softmax\n\u0012(q \u25e6 mq) \u00b7 K\u22a4\n\u03c4\n\u0013\n(8)\nwhere \u03c4 is the softmax temperature. Due to the mask mq,\nonly the components of K corresponding to non-zero ele-\nments of the mask need to be fetched from memory. The\ntop-k mask m\u02c6s \u2208 {0, 1}S can then be calculated using\n\u02c6s (see Figure 3e) and the approximate attention output is\nobtained as:\ny3 = softmax\n\u0012q \u00b7 K\u22a4\n\u221adh\n+ log(m\u02c6s + \u03f5)\n\u0013\n\u00b7 V\n(9)\nwith \u03f5 \u2192 0. Again, due to the mask m\u02c6s, only the key-value\npairs corresponding to the non-masked elements need to be\nfetched from the memory.\nMean value reallocation with query sparsity\nAs a final\nconsideration, we look at combining the mean value reallo-\ncation improvement of Equation (7) with the approach in\nEquation (9). As we do not have access to the full scores\ns, we proceed to approximate the weighted sum using the\napproximate scores in Equation (8). Note that, since the\nquery-key dot product is performed over only r dimensions,\ncare needs to be taken when choosing the appropriate soft-\nmax temperature \u03c4 in Equation (8). If r components were\nchosen randomly, the appropriate temperature would be \u221ar.\nOn the other hand, if the top-r components were the only\nnon-zero elements of the query vector, the appropriate tem-\nperature would remain \u221adh. As a balance between the two\nextremes, we have found the following temperature to yield\na good approximation (see Figure 3f):\n\u03c4 =\ns\ndh\n\u2225q \u25e6 mq\u22251\n\u2225q\u22251\n(10)\nThe final attention output can be then calculated as a\nweighted sum:\ny = \u03b1 y3 + (1 \u2212 \u03b1)\u00afv\n(11)\nwhere \u03b1 = m\u02c6s \u00b7 \u02c6s is the relative weight of the top-k terms.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSum of top-32 attention scores\n0\n500\n1000\n1500\n2000\n2500\n3000\nCount\n(a)\n0\n8\n16\n24\nLayer\n0\n8\n16\n24\nHead (sorted)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSum of top-32 attention scores\n(b)\n\u221210\n\u22125\n0\n5\n10\nComponents of q (z-score)\n100\n10\u22123\n10\u22126\n10\u22129\n10\u221212\nDensity\nKDE per-head\nUnit Gaussian\n(c)\n0\n8\n16\n24\nLayer\n0\n10\n20\n30\nKurtosis\n(d)\n8\n16\n32\n64\nr\n0.0\n0.5\n1.0\nTop-k agreement\nk\n64\n128\n256\n(e)\n(f)\nFigure 3: Statistics of Llama 7B, evaluated over 40 SQuAD queries, over all 32 layers \u00d7 32 heads unless noted. (a) Sum\nsoftmax output allocated to the 32 highest-scoring positions, demonstrating natural attention sparsity; (b) for each head.\n(c) Kernel density estimate (Rosenblatt, 1956) of components of q in layer 16, showing heavy tails. (d) Fisher Kurtosis of\nq components, for each head, showing that the query vector is leptokurtic for most heads. (e) Top-k agreement between\napproximate and true scores for multiple values of r selected from query vector. Top-k agreement is the proportion of the\ntop-k positions that are correctly predicted by an approximated softmax, using a projection of q. (f) Agreement between the\ncoverage \u03b1 based on estimated scores versus the true mass of the top 128 scores, for different softmax temperatures (a point\nfor each example \u00d7 head), showing the importance of correct temperature. Further analysis is presented in Appendix D.\n4\nSparQ Attention: Bandwidth-Efficient LLM Inference\n4. SparQ Attention\nFollowing the analysis in Section 3, we propose SparQ\nAttention (see Algorithm 1) consisting of three steps:\nStep 1: Find the indices of r largest components of |q| and\nonly fetch K along the dimensions corresponding\nto these indices. Calculate approximate attention\nscores \u02c6s using the sliced query and keys.\nStep 2: Find the top-k positions in the approximate atten-\ntion scores and fetch the corresponding full key and\nvalue vectors. Calculate the output of the attention\noperation using the top-k keys and values.\nStep 3: Estimate the total score \u03b1 assigned to the top-k po-\nsitions using the approximate attention scores. Use\nthis total score to interpolate between the attention\noutput from the top-k positions, and a mean value\nvector, v.\nThe memory transfer of the SparQ Attention algorithm for\na single attention head forward-pass:\nMSparQ = S r\n|{z}\n1\u20dd\n+ 2 k dh\n| {z }\n2\u20dd\n+ 4 dh\n|{z}\n3\u20dd\n(12)\nwhere 1\u20dd corresponds to reading r rows of K, 2\u20dd corre-\nsponds to reading the top-k columns of K and V and 3\u20dd\ncorresponds to transfers associated with writing the current\nk and v, in addition to reading and writing v.\nBy varying r and k, we can tune the total amount of data\ntransferred by the scheme, trading-off approximation accu-\nracy for token-generation speed-up. Since typically S \u226b dh,\nr is the most important parameter controlling the data trans-\nfer compression ratio. Typical ratios are given in Table F2.\nGrouped Query Attention\nFor models using GQA,\ngroups of g queries access the same KV head. In order\nto accommodate this, we modify Step 1 to sum |q| within\neach group before selecting top-r components. Similarly,\nStep 2 is modified by summing the approximate attention\nscores within each group before selecting top-k keys and\nvalues for each KV head. Note that Step 3 remains the same.\nThe full code can be found in Appendix B.\n5. Experiments\n5.1. Setup\nModels\nWe evaluate our method on three widely-used\nopen-source language model variants: Llama 2 (Touvron\net al., 2023), Mistral (Jiang et al., 2023) and Pythia (Bider-\nman et al., 2023), evaluating model sizes up to 13 billion\nparameters. All models are decoder-only transformers (Rad-\nford et al., 2018), pre-trained on causal language modelling.\nThey share similar architectural components such as Ro-\ntary positional encoding (Su et al., 2021), while also having\nsome notable differences such as different attention mecha-\nnisms (Multi-Head and Grouped-Query), layer normalisa-\ntion implementations, activation functions and execution of\nmodules in parallel.\nTasks\nIn order to evaluate our method on a spectrum of rel-\nevant NLP tasks that present a particular challenge to sparse\nattention techniques, our evaluation setup consists of various\ntasks requiring information retrieval and reasoning over long\ninput sequences. This includes question answering, sum-\nmarisation, perplexity/bits-per-character (BPC), and text\nrepetition. For this, we adapted standard downstream tasks\nand datasets to generate examples of sequence lengths be-\ntween 1k and 2k tokens.1\nFor question answering, we use the SQuAD (Rajpurkar\net al., 2016) and TriviaQA (Joshi et al., 2017) datasets in\nthe open-book setting. In order to construct the SQuAD ex-\namples, we augment the provided context (i.e. the standard\nSQuAD input sequence required to answer the question)\nwith seven additional \u201cconfusion contexts\u201d from unrelated\nquestions. This ensures that the examples have a large se-\nquence length, while making the task harder as the model\nneeds to distinguish the relevant information from the con-\ntext from the unrelated paragraphs. We use SQuAD v1.1,\nas it does not include unanswerable questions included in\nSQuAD v2.0, since we aim to measure the model\u2019s abil-\nity to extract useful information from the KV cache. For\nboth question answering tasks we use exact string match\naccuracy as the evaluation metric.\nSummarisation is evaluated on the CNN/DailyMail dataset\n(See et al., 2017) using the ROUGE-L F-score (Lin, 2004)\nas the metric. We use the WikiText-103 dataset (Merity\net al., 2016) with bits per character (BPC) for evaluating\nlanguage modelling performance.2\nFinally, we construct an artificial \u201cText Repetition\u201d task\nto evaluate the capability of the model to repeat sentences\nfrom its context verbatim. Such a task can commonly ap-\npear in a dialogue setting where the LLM agent is required\nto retrieve a piece of text from a possibly long context pro-\nvided, and can be challenging for sparse attention techniques.\nWe construct examples using the Tiny-Shakespeare dataset\n(Karpathy, 2015) by chunking the text into contexts of the\n1As we wanted to define the tasks independently of the se-\nlected models, our examples were chosen to have sequence lengths\nbetween 4000 and 8000 characters, roughly giving the desired\nlengths in tokens.\n2We quote performance for sub-word language modelling in\nBPC, to account for any differences in vocabulary across models.\n5\nSparQ Attention: Bandwidth-Efficient LLM Inference\nTable 2: Results for the largest models tested are presented below. SQuAD and TriviaQA measure performance in accuracy.\nCNN/DailyMail uses ROUGE-L score. Repetition counts the number of characters before the generation diverges and\nWikiText task measures perplexity in bits per character (BPC). Values in bold represent the best score for a model, task\nand sparsity setting. Median standard errors across all models and sparsity settings are: SQuAD 0.7, TriviaQA 0.7,\nCNN/DailyMail 0.4, WikiText 0.006, Repetition 2.\nDataset Name\nSQuAD \u2191\nTriviaQA \u2191\nCNN/DailyMail \u2191\nWikiText \u2193\nRepetition \u2191\nCompression\n1\n1/2\n1/8\n1\n1/2\n1/8\n1\n1/2\n1/8\n1\n1/2\n1/8\n1\n1/2\n1/8\nLlama 2\n13B\nLM-\u221e\n80.8\n50.0\n32.4\n78.7\n73.4\n69.0\n22.1\n16.8\n15.1\n0.61\n0.64\n0.69\n229\n76\n29\nH2O\n73.2\n64.1\n78.5\n78.4\n22.2\n20.8\n0.61\n0.63\n61\n26\nSparQ\n80.7\n78.0\n78.8\n78.2\n22.5\n22.2\n0.61\n0.64\n227\n190\nMistral\n7B\nLM-\u221e\n81.0\n51.0\n31.6\n80.9\n75.8\n72.8\n23.7\n18.0\n16.8\n0.62\n0.65\n0.70\n231\n81\n20\nH2O\n71.2\n59.2\n80.8\n80.6\n23.5\n23.4\n0.63\n0.65\n38\n14\nSparQ\n80.9\n77.5\n80.8\n79.0\n23.5\n23.0\n0.63\n0.65\n209\n201\nPythia\n6.9B\nLM-\u221e\n57.8\n38.5\n18.9\n52.6\n41.6\n32.0\n20.2\n14.9\n14.1\n0.68\n0.71\n0.77\n150\n64\n18\nH2O\n52.9\n46.6\n52.6\n52.3\n20.3\n18.9\n0.69\n0.71\n47\n19\nSparQ\n58.0\n57.1\n52.4\n51.7\n20.6\n20.6\n0.68\n0.70\n151\n144\nappropriate size, appending them with the prompts con-\ntaining a subset of the context, and evaluating the output\nexact character length match with the continuation from the\ncontext.\nBaselines\nWe consider the cache eviction technique H2O\n(Zhang et al., 2023) and LM-Infinite, a local windowing\nscheme with initial-tokens included proposed by Han et al.\n(2023) as baselines. For consistency, we allocate a fixed KV\ncache transfer budget k for both techniques. With H2O, we\nset the local window size l = k/4 (with 3k/4 heavy hitters),\nand for LM-Infinite we always include the first 16 positions\n(with k \u2212 16 local positions).\n5.2. Results\nOur experiments span six distinct models: Llama 2 with 7\nand 13 billion parameters, Mistral with 7 billion parame-\nters, and three Pythia models with 1.4, 2.8 and 6.9 billion\nparameters. Results from the largest models are presented\nin Table 2, with further results in Figures A1 and A2. We\nobserve that:\n\u2022 SparQ Attention performance is robust across all tasks\nand model sizes tested. Compression ratios of 2\u00d7 to\n8\u00d7 are readily achievable with little to no loss in task\nperformance.\n\u2022 Certain tasks are more challenging for H2O (Repeti-\ntion, SQuAD), while others are more forgiving (Trivi-\naQA, WikiText-103).\n\u2022 LM-Infinite degrades performance across all tasks,\ndemonstrating that the tasks do not permit the trivial\nsolution of discarding the long input sequence.\n5.3. Ablations and Extensions\nKey cache compression\nThe first step in SparQ Attention\ninvolves reading r components of the key cache to approx-\nimately determine which keys yield the highest attention\nscores. To examine the practical trade-off of the approx-\nimation we look at how SparQ Attention performs when\ncompared to a theoretical upper-bounding \u201coracle\u201d which\nprovides the exact top-k keys without any data transfer. The\nresults in Figure 4a show that SparQ Attention retains com-\nparable performance to the oracle for a wide range of com-\npression ratios, and attains considerably higher performance\nthan a baseline compression scheme, in which a random low\nrank projection of K is transferred from memory.\nApproximate softmax temperature\nTo empirically sup-\nport our statistical analysis of \u03b1 agreement shown in Fig-\nure 3f, we evaluate a number of different viable temperature\nsettings, including the square root of the head dimension\n(\u03c4 = \u221adh), the square root of the rank (\u03c4 = \u221ar), and the\ntemperature proposed in Equation (10). We also consider\nthe scenario where we do not reallocate mass to mean value\n(\u03b1 = 0), which corresponds to the limit of the temperature\ntending towards 0. We find that our proposed temperature\nperforms best, as shown in Figure 4b.\nHyperparameter selection\nThe reduction of data transfer\nattained by SparQ Attention is controlled by its two hyper-\nparameters, k and r. Reducing either of these variables will\nimprove the bandwidth efficiency, but can negatively impact\ntask performance. Figure 4c shows the relationship between\nk and r on both of these factors. Based on these results, we\npropose a simple recipe of setting k = 128 and tuning r\nto maintain a good trade-off between data transfer and task\nperformance for a range of models and tasks.\n6\nSparQ Attention: Bandwidth-Efficient LLM Inference\n8\n16\n32\n64\n128\n256\nAttention transfers per token (MB)\n0.5\n0.6\n0.7\n0.8\nSQuAD Accuracy\nOracle\nSparQ\nLow Rank\n(a) Accuracy results of SparQ Attention and a random low rank\ncompression scheme against an oracle top-k selector.\nk = 128, r = 32\nk = 128, r = 64\n0.65\n0.70\n0.75\nSQuAD Accuracy\nlimx\u21920 x\n\u221ar\ns\ndh\n\u2225q \u25e6 mq\u22251\n\u2225q\u22251\n\u221adh\n(b) Comparison of different softmax temperatures for approximate\nattention scores for two different hyperparameter configurations.\n32\n64\n128\nTransfers (MB)\n50\n100\n150\n200\nRepetition\nmatch length\n32\n64\n128\nTransfers (MB)\n0.5\n0.6\n0.7\n0.8\nSQuAD Accuracy\nk = 32\nk = 64\nk = 128\nk = 256\n(c) Results for Repetition and SQuAD tasks with r \u2208 {16, 32, 64}.\n2000\n4000\n6000\n8000\n10000\n12000\nSequence length S\n0.3\n0.5\n0.7\nSQuAD Accuracy\n(train set)\nDense\nSparQ\nH2O\n(d) SQuAD performance vs input sequence length. The compres-\nsion ratio is fixed at 1/4. Uses Vicuna 1.5 7B with 16k maximum\nsequence length against our SQuAD (train) task with 7 (default)\nto 63 confusion contexts to increase the sequence length.\nFigure 4: Results of ablations and extensions (Llama 2 7B\nSQuAD 1-shot performance unless specified).\nSequence length scaling\nThe sequence lengths of differ-\nent examples in our main tasks vary between 1k and 2k\ntokens, whereas many LLMs support sequence lengths far\ngreater than this. We developed a variation of the SQuAD\ntask that increases the task difficulty, as well as the se-\nquence length by increasing the number of confusion con-\ntexts present in the prompt in Figure 4d, which is akin to in-\ncreasing the number of retrieved documents with a retrieval\naugmented generation system (Borgeaud et al., 2022). We\ntest SparQ Attention and H2O in this setting using Vicuna\n(Chiang et al., 2023), a descendent of Llama 2 that has been\nadapted and for longer sequences. Both SparQ Attention\n2048\n4096\n8192\n16384\nSequence length S\n100 \u00b5s\n25 \u00b5s\n50 \u00b5s\n75 \u00b5s\n250 \u00b5s\nTime per query\nDense\nSparQ (Triton, 1 \u00d7 K)\nSparQ (PyTorch)\nSparQ (Triton)\nFigure 5: Microbenchmark results for batch size 64, 32\nheads, dh = 128, r = 32, k = 128, A100 (40GB).\nand H2O are configured to maintain a fixed compression\nratio versus the dense baseline (keeping r = 32 and modi-\nfying k to maintain 1/4 compression), showing that SparQ\nAttention is scalable to large sequences.\n6. Benchmarking\nThe results above use a theoretical cost model of total mem-\nory transfers, allowing us to evaluate SparQ Attention in-\ndependently of a specific hardware setup. To validate this\napproach, we performed a set of microbenchmarks of an\nattention operation in isolation.\nSparQ Attention benefits from two optimisations. The first\nis to store K twice, in both dh-contiguous and S-contiguous\nlayouts, since this allows for an efficient gather (indexing)\non either axis, at the cost of 50% extra memory usage. The\nsecond optimisation is to use a fused gather-then-matmul\noperation to avoid writing the result of the gather to memory.\nWe tested multiple implementations of baseline and SparQ\nAttention on IPU using the Poplar C++ interface and GPU\nusing PyTorch (Paszke et al., 2019). In all cases, we used\nthe Llama 7B shape parameters: 32 heads, dh = 128. The\nimplementations tested were: Dense baseline, choosing the\nfaster of a plain PyTorch implementation and the builtin\nscaled dot product attention, SparQ (Triton),\nstoring K twice & using fused gather-then-matmul kernels\nwritten using Triton (Tillet et al., 2019), SparQ (PyTorch),\nwith no Triton and SparQ (Triton, 1\u00d7K), storing K in dh-\ncontiguous layout only, for no additional memory cost. In an\nexample configuration running on a single IPU from a Bow\nPod16, batch size 1, sequence length S = 16384, the dense\nbaseline achieves 40.4 ms/query, while SparQ (r = 32,\nk = 128) achieves 5.28 ms/query for a speedup of 7.41\u00d7\n(the theoretical speedup of SparQ is 7.53\u00d7). This near-\nperfect speedup is achieved because attention is strongly\nmemory bound when using remote memory. In contrast, the\nbaseline running in local SRAM takes 134 \u00b5s for a 345\u00d7\nspeedup, but this is only practically achievable when the\nwhole model fits in SRAM.\n7\nSparQ Attention: Bandwidth-Efficient LLM Inference\nTable 3: GPU performance with batch size 64, sequence\nlength S = 4096, r = 32, k = 128. The theoretical speed-\nup is 6.4\u00d7.\nKernel\nA100 (40GB)\nA10G\nDense\n49 \u00b5s (1\u00d7)\n128 \u00b5s (1\u00d7)\nSparQ (Triton, 1\u00d7K)\n38 \u00b5s (1.28\u00d7)\n79 \u00b5s (1.63\u00d7)\nSparQ (PyTorch)\n37 \u00b5s (1.33\u00d7)\n78 \u00b5s (1.63\u00d7)\nSparQ (Triton)\n16 \u00b5s (3.02\u00d7)\n31 \u00b5s (4.17\u00d7)\nOur achieved GPU speed-ups are presented in Table 3, and\nthe performance trend with sequence length is shown in\nFigure 5. Standard error for all results given is < 1% of the\nmean. See Appendix E for further details.\nThese microbenchmark results show that the theoretical ben-\nefits of SparQ Attention can yield substantial wall-clock\ntime speedups on current hardware. Further work is needed\nto show improvements for small batch size, and to investi-\ngate alternatives to storing K twice.\n7. Related Work\nEfficient attention methods have been a very active area\nof research (Tay et al., 2020b). Schemes such as Sparse\nTransformers (Child et al., 2019), Combiner (Ren et al.,\n2021), Longformer (Beltagy et al., 2020), BigBird (Zaheer\net al., 2020), Reformer (Kitaev et al., 2020) and Sparse\nSinkhorn Attention (Tay et al., 2020a) have been developed\nto increase efficiency of the attention mechanism by ex-\ntracting information from the most salient tokens in the\nsequence or approximating dense attention maps. Two\nschemes that reduce memory footprint and data transfer\nof the attention operation, while maintaining quadratic com-\nplexity are Multi-Query Attention (MQA) (Shazeer, 2019)\nand Grouped-Query Attention (GQA) (Ainslie et al., 2023)\nthat share each KV head across multiple query heads. These\nmethods form part of the architecture: they must be imple-\nmented during pre-training, carry varying task performance\ntrade-offs, and may affect model quality and stability.\nAn emerging area of research similar to SparQ Attention\naims to only adapt the inference procedure of a pre-trained\nmodel. The simplest method of this category is part of\nFlexGen (Sheng et al., 2023), and calculates exact attention\nscores, retrieving only the values associated with the top-\nk scores. This process uses the full key cache to produce\nattention scores, limiting the asymptotic reduction of the\nmemory transfers to only 50%. LM-Infinite (Han et al.,\n2023) and StreamingLLM (Xiao et al., 2023) employ a\nfixed sparsity pattern preserving the most recent tokens and\na few initial tokens for better attention efficiency, but are\nnot selective in their cache lookup.\nEviction schemes cache only a subset of keys and values, by\ncontinually deleting tokens that are uninformative for future\noutputs. By reducing the cache size itself, both the amount\nof memory used and data transferred are reduced. H2O\n(Zhang et al., 2023) and Scissorhands (Liu et al., 2023a)\nare examples of such eviction methods. H2O uses a greedy\neviction policy that maintains in memory the most salient\n\u201cHeavy Hitter\u201d tokens that contribute most to the attention\nscores. Scissorhands identifies and maintains \u201cpivotal to-\nkens\u201d by counting when a token\u2019s attention score exceeds\nan importance threshold. While these methods reduce the\nmemory footprint of the KV cache as well as data trans-\nfer, they also lead to permanent loss of information from\nthe context window, which can lead to mistakes for queries\nseeking less-attended parts of the sequence.\nIceFormer (Mao et al., 2023) uses multiple existing ap-\nproximate nearest neighbour algorithms for approximating\nattention scores of pre-trained models, but focuses solely\non improving CPU performance, while SparQ Attention\naims to be hardware-agnostic. Scatterbrain (Chen et al.,\n2021) employs similar techniques, but for computer vision\napplications.\nAn orthogonal line of work increases bandwidth efficiency\nby compressing the KV cache with 4-bit number formats\n(Liu et al., 2023b; Sheng et al., 2023). Liu et al. (2023a)\ndemonstrate that 4-bit compression is complementary to\ntechniques that reduce the number of transferred elements.\n8. Discussion\nIn this work we have presented SparQ Attention, a novel\ntechnique for unlocking faster inference for pre-trained\nLLMs. Our proposed technique modifies the attention mech-\nanism to access only the relevant tokens from the KV cache\non every generation step, leading to considerable data trans-\nfer savings. This allows for pre-trained models to be exe-\ncuted more efficiently, without any fine-tuning or modifica-\ntions to the weights of the model.\nOne limitation of SparQ Attention is that it does not reduce\nmemory capacity, only the amount of data transferred. How-\never, as no information is discarded from the input sequence,\nit is more robust to performance degradation compared to\nlossy alternatives.\nLLM inference speed is often bottlenecked by memory trans-\nfers rather than computation, especially in regimes of long\nsequence lengths and limited memory bandwidth. SparQ At-\ntention addresses this, increasing the arithmetic intensity of\nLLMs with up to an eightfold compression of the transferred\nKV cache data and little to no task performance degradation.\nIt is easy to implement on high-performance accelerators\nand obtains substantial throughput improvements.\n8\nSparQ Attention: Bandwidth-Efficient LLM Inference\n9. Broader Impact\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. GPT-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr\u00b4on, F., and Sanghai, S. GQA: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nBeltagy, I., Peters, M. E., and Cohan, A.\nLong-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\nIn International Conference on Machine Learning, pp.\n2397\u20132430. PMLR, 2023.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,\nE., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B.,\nDamoc, B., Clark, A., et al. Improving language models\nby retrieving from trillions of tokens. In International\nconference on machine learning, pp. 2206\u20132240. PMLR,\n2022.\nChen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R\u00b4e,\nC. Scatterbrain: Unifying sparse and low-rank attention.\nAdvances in Neural Information Processing Systems, 34:\n17413\u201317426, 2021.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality, March 2023.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nGraphcore.\nBow-2000 datasheet.\n(Online:\nac-\ncessed 25 January 2024),\nMarch 2023.\nURL\nhttps://docs.graphcore.ai/projects/\nbow-2000-datasheet.\nHan, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.\nLM-infinite: Simple on-the-fly length generalization for\nlarge language models. arXiv preprint arXiv:2308.16137,\n2023.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.\nTriviaQA: A large scale distantly supervised challenge\ndataset for reading comprehension.\narXiv preprint\narXiv:1705.03551, 2017.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKarpathy, A. The unreasonable effectiveness of recurrent\nneural networks. (Online: accessed 27 January 2024),\n2015.\nURL https://github.com/karpathy/\nchar-rnn.\nKitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The\nefficient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nLin, C.-Y. ROUGE: A package for automatic evaluation\nof summaries.\nIn Text Summarization Branches Out,\npp. 74\u201381, Barcelona, Spain, July 2004. Association for\nComputational Linguistics.\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,\nKyrillidis, A., and Shrivastava, A. Scissorhands: Ex-\nploiting the persistence of importance hypothesis for\nllm kv cache compression at test time. arXiv preprint\narXiv:2305.17118, 2023a.\nLiu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,\nY., Shi, Y., Krishnamoorthi, R., and Chandra, V. LLM-\nQAT: Data-free quantization aware training for large lan-\nguage models. arXiv preprint arXiv:2305.17888, 2023b.\nMao, Y., Ester, M., and Li, K. Iceformer: Accelerated infer-\nence with long-sequence transformers on CPUs. In Third\nWorkshop on Efficient Natural Language and Speech Pro-\ncessing (ENLSP-III): Towards the Future of Large Lan-\nguage Models and their Emerging Descendants, 2023.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\nPointer sentinel mixture models.\narXiv preprint\narXiv:1609.07843, 2016.\nNVIDIA.\nNVIDIA A10 datasheet.\n(Online:\nac-\ncessed 22 January 2024),\nMarch 2022.\nURL\nhttps://www.nvidia.com/content/dam/\nen-zz/Solutions/Data-Center/a10/pdf/\ndatasheet-new/nvidia-a10-datasheet.\npdf.\n9\nSparQ Attention: Bandwidth-Efficient LLM Inference\nNVIDIA. NVIDIA H100 datasheet. (Online: accessed\n22 January 2024), July 2023.\nURL https://www.\nnvidia.com/en-gb/data-center/h100/.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently\nscaling transformer inference. Proceedings of Machine\nLearning and Systems, 5, 2023.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI.\nImproving language understanding by generative\npre-training.\n(Online:\naccessed 29 January 2024),\n2018.\nURL https://openai.com/research/\nlanguage-unsupervised.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ questions for machine comprehension of text.\narXiv preprint arXiv:1606.05250, 2016.\nRen, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur-\nmans, D., and Dai, B. Combiner: Full attention trans-\nformer with sparse computation cost. Advances in Neural\nInformation Processing Systems, 34:22470\u201322482, 2021.\nRoche, A., Malandain, G., Pennec, X., and Ayache, N. The\ncorrelation ratio as a new similarity measure for multi-\nmodal image registration. In Medical Image Computing\nand Computer-Assisted Intervention \u2014 MICCAI\u201998: First\nInternational Conference Cambridge, MA, USA, October\n11\u201313, 1998 Proceedings 1, pp. 1115\u20131124. Springer,\n1998.\nRosenblatt, M. Remarks on Some Nonparametric Estimates\nof a Density Function. The Annals of Mathematical Statis-\ntics, 27(3):832 \u2013 837, 1956.\nSee, A., Liu, P. J., and Manning, C. D. Get to the point:\nSummarization with pointer-generator networks. arXiv\npreprint arXiv:1704.04368, 2017.\nShazeer, N. Fast transformer decoding: One write-head is\nall you need. arXiv preprint arXiv:1911.02150, 2019.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., R\u00b4e, C., Stoica, I., and Zhang, C. FlexGen:\nhigh-throughput generative inference of large language\nmodels with a single GPU. In International Conference\non Machine Learning, pp. 31094\u201331116. PMLR, 2023.\nSu, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer:\nEnhanced transformer with rotary position embedding.\nCoRR, abs/2104.09864, 2021. URL https://arxiv.\norg/abs/2104.09864.\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.\nSparse sinkhorn attention. In International Conference\non Machine Learning, pp. 9438\u20139447. PMLR, 2020a.\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient\ntransformers: A survey. CoRR, abs/2009.06732, 2020b.\nURL https://arxiv.org/abs/2009.06732.\nTillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate\nlanguage and compiler for tiled neural network computa-\ntions. In Proceedings of the 3rd ACM SIGPLAN Interna-\ntional Workshop on Machine Learning and Programming\nLanguages, pp. 10\u201319, 2019.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nVig, J. A multiscale visualization of attention in the trans-\nformer model. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics: System\nDemonstrations, pp. 37\u201342, 01 2019.\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners. In International\nConference on Learning Representations, 2022.\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453, 2023.\nYun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S.,\nReddi, S., and Kumar, S. O(n) connections are expressive\nenough: Universal approximability of sparse transform-\ners. Advances in Neural Information Processing Systems,\n33:13783\u201313794, 2020.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\nYang, L., et al. Big bird: Transformers for longer se-\nquences. Advances in Neural Information Processing\nSystems, 33:17283\u201317297, 2020.\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y., R\u00b4e, C., Barrett, C., et al. H2O:\nHeavy-hitter oracle for efficient generative inference of\nlarge language models. arXiv preprint arXiv:2306.14048,\n2023.\n10\nSparQ Attention: Bandwidth-Efficient LLM Inference\nA. Detailed Results\nFigure A1 and Figure A2 report the compression/performance trade-off curves for all models and tasks that were evaluated.\n128 MB\n256 MB\n512 MB\n0.5\n0.6\n0.7\n0.8\nSQuAD Accuracy\nLlama 2 13B\n64 MB\n128 MB\n256 MB\n0.4\n0.5\n0.6\n0.7\n0.8\nLlama 2 7B\n16 MB\n32 MB\n64 MB\n0.5\n0.6\n0.7\n0.8\nMistral 7B\n128 MB\n256 MB\n512 MB\n0.4\n0.5\n0.6\n0.7\n0.8\nTriviaQA Accuracy\n64 MB\n128 MB\n256 MB\n0.4\n0.5\n0.6\n0.7\n0.8\n16 MB\n32 MB\n64 MB\n0.5\n0.6\n0.7\n0.8\n64 MB\n128 MB\n256 MB\n512 MB\n0.12\n0.15\n0.18\n0.21\nCNN/DailyMail ROUGE-L\n64 MB\n128 MB\n256 MB\n0.12\n0.15\n0.18\n0.21\n16 MB\n32 MB\n64 MB\n0.12\n0.15\n0.18\n0.21\n0.24\n64 MB\n128 MB\n256 MB\n512 MB\n0.60\n0.65\n0.70\n0.75\nWikiText-103 BPC\n64 MB\n128 MB\n256 MB\n0.65\n0.70\n0.75\n0.80\n16 MB\n32 MB\n64 MB\n0.65\n0.70\n0.75\n128 MB\n256 MB\n512 MB\nAttention transfers per token\n0\n100\n200\nRepetition match length\n64 MB\n128 MB\n256 MB\n512 MB\nAttention transfers per token\n0\n50\n100\n150\n200\n16 MB\n32 MB\n64 MB\n128 MB\nAttention transfers per token\n0\n100\n200\nDense\nSparQ Attention\nH2O\nLM-In\ufb01nite\nFlexGen (16-bit)\nFigure A1: Compression versus performance trade-off curves over all tasks and multiple models. The y-axis minimum is set\nto (0.5, 0.5, 0.5, 1.25, 0.0)\u00d7 the dense baseline for the tasks, reading top-to-bottom, in order to give a consistent view of the\nperformance loss across models. Vertical dotted lines show (1/2), (1/4) and (1/8) compression versus dense. Shaded lines\nshow \u00b11 standard error of the mean.\n11\nSparQ Attention: Bandwidth-Efficient LLM Inference\n64 MB\n128 MB\n256 MB\n0.3\n0.4\n0.5\n0.6\nSQuAD Accuracy\nPythia 6.9B\n32 MB\n64 MB\n128 MB\n0.3\n0.4\n0.5\nPythia 2.8B\n16 MB\n32 MB\n64 MB\n128 MB\n0.25\n0.30\n0.35\n0.40\n0.45\nPythia 1.4B\n64 MB\n128 MB\n256 MB\n0.3\n0.4\n0.5\nTriviaQA Accuracy\n32 MB\n64 MB\n128 MB\n0.3\n0.4\n0.5\n16 MB\n32 MB\n64 MB\n128 MB\n0.25\n0.30\n0.35\n0.40\n0.45\n32 MB\n64 MB\n128 MB\n256 MB\n0.12\n0.15\n0.18\n0.21\nCNN/DailyMail ROUGE-L\n32 MB\n64 MB\n128 MB\n0.12\n0.15\n0.18\n0.21\n16 MB\n32 MB\n64 MB\n0.10\n0.12\n0.14\n0.16\n0.18\n64 MB\n128 MB\n256 MB\n0.70\n0.75\n0.80\n0.85\nWikiText-103 BPC\n32 MB\n64 MB\n128 MB\n0.70\n0.75\n0.80\n0.85\n16 MB\n32 MB\n64 MB\n128 MB\n0.75\n0.80\n0.85\n0.90\n64 MB\n128 MB\n256 MB\nAttention transfers per token\n0\n50\n100\n150\nRepetition match length\n32 MB\n64 MB\n128 MB\n256 MB\nAttention transfers per token\n0\n50\n100\n150\n32 MB\n64 MB\n128 MB\nAttention transfers per token\n0\n50\n100\n150\nDense\nSparQ Attention\nH2O\nLM-In\ufb01nite\nFlexGen (16-bit)\nFigure A2: Compression versus performance trade-off curves for Pythia models (see Figure A1).\n12\nSparQ Attention: Bandwidth-Efficient LLM Inference\nB. Code\nfrom torch import softmax, sqrt, tensor, topk\ndef gather(t, dim, i):\ndim += (dim < 0) * t.ndim\nreturn t.gather(dim, i.expand(*t.shape[:dim], i.shape[dim], *t.shape[dim + 1 :]))\ndef attn(Q, K, V, M):\ns = (Q @ K.transpose(-1, -2)) / sqrt(tensor(Q.shape[-1])) + M\ny = softmax(s, dim=-1) @ V\nreturn y\ndef sparq_attn(Q, K, V, V_mean, M, r, k):\n# Q -- (batch_size, n_kv_heads, n_heads // n_kv_heads, 1, head_size)\n# K, V -- (batch_size, n_kv_heads, 1, seq_len, head_size)\n# 1. Approximate attention scores using r largest components of Q\ni1 = topk(abs(Q).sum(dim=2, keepdim=True), r, -1).indices\nQ_hat, K_hat = gather(Q, -1, i1), gather(K, -1, i1)\nscale = sqrt(\nQ.shape[-1]\n* abs(Q_hat).sum(dim=-1, keepdim=True)\n/ abs(Q).sum(dim=-1, keepdim=True)\n)\ns_hat = softmax(Q_hat @ K_hat.transpose(-1, -2) / scale + M, dim=-1)\n# 2. Gather top k positions based on approximate attention scores & run attention\ni2 = topk(s_hat.sum(dim=2, keepdim=True), k, -1).indices\niKV = i2[..., 0, :, None]\nK, V, M = gather(K, -2, iKV), gather(V, -2, iKV), gather(M, -1, i2)\ny_ = attn(Q, K, V, M)\n# 3. Estimate the total score of the top k, and interpolate with V_mean\nalpha = gather(s_hat, -1, i2).sum(-1, keepdim=True)\ny = alpha * y_ + (1 - alpha) * V_mean\nreturn y\n13\nSparQ Attention: Bandwidth-Efficient LLM Inference\n1\n4\n16\n64\n256\n1024\nB\n1\n10\n100\nArithmetic intensity\n1/64\n1/16\n1/4\n1\n4\n16\n64\n\u03c1\n(a)\n1\n4\n16\n64\n256\n1024\nB\n1\n10\n100\nArithmetic intensity\n1/64\n1/16\n1/4\n1\n4\n16\n64\n\u03c1\n(b)\nFigure C1: Relationship between \u03c1=S/(gdm), B and arithmetic intensity. (a) Multi-head attention, g = 1. (b) Grouped-\nquery attention, g = 8. This highlights the importance of \u03c1, even with large batch size and GQA.\nC. Arithmetic intensity\nFollowing the framework of Section 2, we provide concrete examples of arithmetic intensity for various models and the\nimplications for execution modern machine learning hardware. We observe from Equation (2) that the arithmetic intensity\nas batch size increases approaches g + 6/\u03c1. For example:\nModel\ng\ndm\nS\n\u03c1 = S/(gdm)\nMax A/M\nLlama 7B\n1\n4096\n4096\n1\n7\nLlama 70B\n8\n8192\n4096\n1/16\n104\nLlama 70B\n8\n8192\n16384\n1/4\n32\nThe general relationship between \u03c1 and arithmetic intensity is shown in Figure C1.\nHardware\nProperties of selected machine learning hardware.3 Note that rA is the number of multiply-adds per second\nand rM the number of data elements transferred per second.\nName\nMemory technology\nrA/1012\nrM/1012\nrA/rM\nBow IPU (FP16)\nSRAM\n175\n5.5\n32\nA10 GPU (INT8)\nGDDR\n125\n0.6\n210\nH100 SXM GPU (FP8)\nHBM\n990\n3.35\n295\nComparing rA/rM for this hardware to the arithmetic intensity achievable for standard transformer models, it\u2019s clear that\nsequence generation will hit a data transfer bottleneck.\nD. Attention Sparsity Analysis\nIn order to understand how to approximate attention in pretrained transformers, we analysed the queries, values and\nintermediate scores vector (softmax output). We took 40 examples from our SQuAD 1-shot task, and generated the first\ncompletion token using the dense Llama 2 7B model, capturing the q vector and K, V matrices from every layer and\nattention head, showing derived statistics in Figures 3, D1 and D2.\n3For IPU (Graphcore, 2023), we use the exchange memory bandwidth of 11 TB/s. A10 (NVIDIA, 2022). H100 (NVIDIA, 2023).\n14\nSparQ Attention: Bandwidth-Efficient LLM Inference\n0\n8\n16\n24\nLayer\n0\n10\n20\n30\nKurtosis\n(a)\n0\n8\n16\n24\nLayer\n0\n8\n16\n24\nHead (sorted)\n0\n5\n10\n15\n20\n25\nKurtosis\n(b)\n0\n8\n16\n24\nLayer\n10\u22121\n100\n101\nq mass (|z|>3) vs Gaussian\n(c)\n0\n8\n16\n24\nLayer\n0\n8\n16\n24\nHead (sorted)\n1\u00d7\n2\u00d7\n4\u00d7\n8\u00d7\n16\u00d7\nq mass (|z|>3) vs Gaussian\n(d)\nFigure D1: Statistics of components of q for each head, as a function of layer. (Top) Kurtosis (Fisher), indicating that most\nheads have heavy-tailed q. (Bottom) z-value mass, normalised by that of a Gaussian (0.3%), showing that most heads are\noutlier-heavy. All Llama 2 7B, measured over 40 SQuAD examples.\nIn Figures 3c and 3d we show that elements of the query vectors are not normally distributed, but have high sample kurtosis\nvalues. If compared to a normal distribution, the combined mass of the elements with absolute z-score exceeding 3.0 is up to\n20\u00d7 higher. This leads us to theorise that query vectors in a pre-trained model inherently encode information sparsely using\nthe tails. Therefore, the magnitude based sparsity we induce in the first stage of the algorithm does not significantly harm\nthe approximation of the attention mappings.\nWe validate this claim by comparing the correspondence between the exact and approximated attention scores. SparQ\nAttention uses the approximate attention scores to only choose the tokens that are important for the next generation step. The\nactual values of the approximate scores are not relevant, as these scores are not multiplied with value vectors and thus the\nproperty of interest to us is whether the top-k indices in the approximate scores match those of the exact counterpart. This\ncan be measured on a scale from 0 to 1, where 1 means top-k indices are identical between the approximation and the exact\nscores and 0 means these sets do not overlap. We call this measure top-k correspondence. Figure D2 provides an overview\nhow the choice of rank and k affects the top-k correspondence aggregated over all attention heads of the model. We see\nthat the query vector sparsity of 50% and 75% maintain high top-k correspondence to the exact attention scores, which is\nconsistently maintained over various values of k. Further analysis and a more detailed examination of top-k correspondence\nis presented in Appendix D.\nIt is useful to drop positions in V given attention scores, but this can save at most half of the data transfers, since the whole\nof K is needed to calculate these scores. We propose approximating these scores using a subset of the components of K.\nTo test such an approximation, we measure the proportion of overlap between the top 32 positions in the approximated\n15\nSparQ Attention: Bandwidth-Efficient LLM Inference\nand true scores. If overlap is high, we can use the approximation to avoid transferring the whole K matrix, instead only\ntransferring some components of K for all positions, then all components of K for some positions.\nOur hypothesis is that the r largest-magnitude components of q are most useful to predicting the score, qK\u22a4. The coverage\nof this technique against an arbitrary-component baseline is shown in Figure D2. These results show that it is possible\nto achieve reasonably high overlap even using r = dh/8, but that some later layers are harder to predict. Using the top-r\ncomponents outperforms the first r baseline considerably.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTop-32 agreement\n0\n1\n2\n3\n4\n5\n6\n7\n8\nDensity\nTop-r |q|\nFirst-r\nLast-r\nRandom projection\nFigure D2: Top-k agreement between approximate and true scores (Llama 2 7B, measured over 40 SQuAD examples).\nTop-k agreement is the proportion of the top-k positions that are correctly predicted by an approximated softmax, using a\nprojection of q, either component-wise or a random low-rank projection.\nE. Benchmarking Detail\nBenchmarking code is made available from:\nhttps://github.com/graphcore-research/llm-inference-research/tree/2024-01-paper.\nIPU measurements\nWe tested custom fully-fused Poplar implementations of both dense attention and SparQ Attention,\ncompiled using Poplar SDK 3.3.0+1403. On initialisation, we fill large K and V tensors with values \u223c N(0, 1) in\nstreaming memory. On each benchmarking (outer) iteration, we first randomise the contents of a q in local memory, then\nperform multiple inner repeats of the attention op being profiled. We use 4 inner repeats for dense attention, otherwise\n1024/batch size, chosen because dense attention is much slower, and we swept a wide range of settings. We ran an outer\nloop of 2 warm-up iterations followed by 10 timed iterations, reporting the mean and standard error. The sweep covered\nS \u2208 [1024, 2048, . . . , 65536], batch size \u2208 [1, 4, 16, 64], SparQ Attention r \u2208 [16, 32, 64] and k \u2208 [64, 128, 256, 512].\nGPU measurements\nAll experiments use PyTorch 2.1.2+cu121 on Ubuntu AWS instances. To set up the experi-\nment, we initialise the large K and V tensors with values \u223c N(0, 1). On each step, we draw q \u223c N(0, 1), run\ntorch.cuda.synchronize before starting a host-side wall-clock timer, run the op, and synchronize again before\nstopping the timer. We run 20 warm-up iterations followed by 200 timed iterations, reporting mean and standard error.\nFor dense baseline implementations, we tested a vanilla PyTorch implementation, with/without torch.compile and\ntorch.nn.functional.scaled dot product attention, selecting each backend (math, flash, mem efficient)\nmanually. For SparQ Attention implementations, we tested vanilla PyTorch (lightly hand-optimised from Appendix B),\nwith/without torch.compile. We also toggled fused gather-matmul kernels written in Triton, and whether K was\nstored twice in S-contiguous (for Step 1) and dh-contiguous (for Step 2) layouts, or only once in dh-contiguous lay-\nout. We tested S \u2208 [1024, 2048, 4096, 8192, 16384], batch size \u2208 [1, 4, 16, 64], SparQ Attention r \u2208 [16, 32, 64] and\nk \u2208 [64, 128, 256, 512].\nAdditional results\nIn addition to the headline results shared in Section 6 and Figure 5, we give an aggregate picture of the\ntrends in Figure E1. Since the number and dimension of heads is fixed, the x-axis is proportional to the size of the input\n16\nSparQ Attention: Bandwidth-Efficient LLM Inference\n103\n104\n105\n106\nBatch size \u00d7 Sequence length\n1\u00d7\n10\u00d7\nSpeedup\nr\n16\n32\n64\nDevice\nM2000\nA10G\nA100-40GB\nFigure E1: SparQ speedup over the dense baseline, across a range of batch size (1-64), sequence length (1024-65536) and k\n(64-512), for different devices. We note that for both GPUs, the number of KV elements is a limiting factor for the achieved\nspeedup, and that this could be improved by writing a fully fused SparQ Attention kernel.\ntensors. On IPU (M2000), strong speedups are available across a range of input sizes, principally depending on r, but also\non k (not shown). On GPU, sufficient input size is required to observe a speedup over the dense baseline, with the more\nbandwidth-limited A10G reaching speedups sooner. While part of this effect can be linked to the fundamental additional\ncomplexity of SparQ Attention, we anticipate that small input sizes could be accelerated considerably with additional kernel\nfusion. With an appropriate limit to sequence length, SparQ Attention could even be fused into a single CUDA kernel.\nStoring K twice\nOne limitation of a theoretical model of data transfer is that it does not account for the granularity\nof memory access. Since the K matrix is indexed on different axes in Step 1 and Step 2 of SparQ Attention, a naive\nimplementation would fetch non-contiguous elements in one of the two steps. To mitigate this, we propose storing K twice,\nonce in S-major format and once in dh-major format. This increases KV cache memory usage by 50%, but uses only a small\namount of extra bandwidth to write k twice. This extra write is non-contiguous, but small, so should not form a bottleneck.\n17\nSparQ Attention: Bandwidth-Efficient LLM Inference\nF. Methodology\nWe provide a comprehensive set of hyperparameters for reference in Table F1. Typical compression ratios for settings of\n(r, k) are given in Table F2.\nWe use our own implementation of H2O (Zhang et al., 2023), which differs from the authors\u2019 implementation in that it\nuses a fixed cache size k, rather than a ratio of the current sequence length. To validate that these implementations are\nsufficiently similar, we ran their implementation through our harness on a small model and sample size. On SQuAD\n1-shot, with Pythia-1.4B, using k = 256, l = 64, our implementation was correct for 60 of 200 examples, theirs for 57\n(the dense baseline achieved 74). Perhaps more importantly, we found that of the 79 times that either output differed from\ndense, 41 occurrences showed a 20-character prefix match between our implementation and theirs. The fact that the two\nimplementations often generate the same errors (despite minor implementation differences) reassures us that our results\nshould be a fair representation of H2O.\nGroup\nHyperparameter\nValue or range\nDense model\nFamily\nLlama 2 (13B, 7B), Mistral (7B),\nPythia (6.9B, 2.8B, 1.4B)\ndh\n{80, 128}\nMax S\n{2048, 4096}\nTasks\nQuestion Answering\nSQuAD 1-shot (4000 samples)\nTriviaQA 0-shot (2992 samples)\nSummarisation\nCNN/DailyMail 0-shot (500 samples)\nLanguage Modelling\nWikiText-103 LM (500 samples)\nArtificial\nRepetition (1000 samples)\nBaselines\nEviction\nkeep (k \u2212 l) tokens with highest score(n) =\nP\ni sin and the most recent l = k/4\nk \u2208 {192, 256, 384, 512, 768}\nLM-Infinite\ntake the first 16 tokens, and most recent k \u221216\nk \u2208 {192, 256, 384, 512, 768}\nSparQ Attention\nRank r\n{16, 32, 64}\nNumber of values k\n128\nLocal window l\nk/4\nTable F1: Experiment hyperparameters\n18\nSparQ Attention: Bandwidth-Efficient LLM Inference\nMethod\nk\nr\nCompression\nSparQ Attention\n128\n16\n0.13 - 0.17\n32\n0.19 - 0.23\n64\n0.31 - 0.36\nH2O\n192\n\u2013\n0.10 - 0.17\n256\n0.13 - 0.22\n384\n0.20 - 0.33\n512\n0.26 - 0.43\n768\n0.39 - 0.65\nTable F2: Range of compression ratios for different settings of (r, k), for Llama 2 7B and Pythia 6.9B. The compression\nratio achieved varies across models and tasks, based on the sequence length and head size.\n19\nSparQ Attention: Bandwidth-Efficient LLM Inference\nF.1. Examples\nWe illustrate the task setup with a single example per task, showing the prompt formatting and a cherry-picked example. In\neach case, we show outputs from a dense Llama 2 13B model, SparQ Attention (r = 8, k = 128), H2O and LM-Infinite\n(k = 192). Where \u201c...\u201d appears, we have truncated the line of text for brevity.\nF.1.1. QUESTION ANSWERING (SQUAD 1-SHOT)\n### PROMPT (5725e1c4271a42140099d2d9)\nTitle: University of Chicago. Background: Current ...\nTitle: Harvard University. Background: Harvard has...\nTitle: Oxygen. Background: In one experiment, Lavo...\nTitle: Oxygen. Background: Oxygen storage methods ...\nTitle: Fresno, California. Background: This vibran...\nTitle: Fresno, California. Background: Before Worl...\nTitle: Steam engine. Background: The working fluid...\nTitle: Sky (United Kingdom). Background: While BSk...\nFrom what you've just read about Fresno, California, please answer the following questions.\nQuestion: Where is Audra McDonald from?\nAnswer: Fresno\nQuestion: In what year did Roger Rocka's Dinner Theater & Good Company Players open?\nAnswer:\n### OUTPUT\nDENSE: 1978\nSPARQ: 1978\nH2O: 1979\nLM-INFINITE: 1975 (Roger Rock\nF.1.2. QUESTION ANSWERING (TRIVIAQA 0-SHOT)\n### PROMPT (dpql_5685)\nAp\u00b4eritifs and digestifs ( and) are drinks, typical...\nAp\u00b4eritifs\nAn ap\u00b4eritif is an alcoholic beverage usually serve...\n\"Ap\u00b4eritif\" may also refer to a snack that precedes...\n\"Ap\u00b4eritif\" is a French word derived from the Latin...\n...\n...\n* Distilled liquors (ouzo, tequila, whisky or akva...\n* Liquor cocktails (Black Russian, Rusty Nail, etc...\nIn certain areas, it is not uncommon for a digesti...\nBitter digestifs typically contain carminative her...\nIn many countries, people drink alcoholic beverage...\nQuestion: Which aperitif is named for the Paris chemist who created it in 1846?\nAnswer:\n### OUTPUT\nDENSE: Dubonnet\nSPARQ: Dubonnet\nH2O: Dubonnet\nLM-INFINITE: Byrrh\nNote that for Pythia, the prompt \u201cSingle-word answer:\u201d was used in place of \u201cAnswer:\u201d, as this helped prevent the model\nfrom restating the question in the answer (often qualitatively correct, but not a regex match).\nF.1.3. SUMMARISATION (CNN/DAILYMAIL)\n### PROMPT (a62bbf503be06e8b1f8baa4f3cd537310d5aa3bc)\nArticle: Prince William arrived in China tonight for one of the most high-profil...\nSummary:\n### OUTPUT\nDENSE: Prince William arrived in China tonight for one of the most high-profile { ...\n20\nSparQ Attention: Bandwidth-Efficient LLM Inference\nSPARQ: Prince William arrived in China tonight for one of the most high-profile { ...\nH2O: Prince William arrived in China tonight for a high-profile visit that will ...\nLM-INFINITE: Prince William and Kate Middleton are in Japan for a three-day tour. The ro...\nF.1.4. REPETITION (SHAKESPEARE)\n### PROMPT (210496)\nyou mistake me much;\nI do lament the sickness of the king.\n...\n...\nPeace, children, peace! the king doth love you well:\nIncapable and shallow innocents,\nYou cannot guess who caused your father's death.\nBoy:\nGrandam, we can; for my good uncle Gloucester\nTold me, the king, provoked by the queen,\nDevised impeachments to imprison him :\nAnd when my uncle told me so, he wept,\nAnd hugg'd me in his arm, and kindly kiss'd my cheek;\n...\n...\nthe king doth love you well:\nIncapable and shallow innocents,\nYou cannot guess who caused your father's death.\nBoy:\nGrandam, we\n### OUTPUT\nDENSE: can; for my good uncle Gloucester\nSPARQ: can; for my good uncle Gloucester\nH2O: can;\nLM-INFINITE: 'll not stand to prate, but to the purpose.\nF.1.5. LANGUAGE MODELLING (WIKITEXT-103)\n### QUERY (2)\n= Mellor hill fort =\nMellor hill fort is a prehistoric site in North West England , that dates from ...\n= = Location = =\nMellor lies on the western edge of the Peak District in the Metropolitan Boroug...\n= = Background = =\nUntil the 19th century little was known about hill forts ; none had been excava...\nThe study of hill forts was popular in the 19th century , with a revival in the...\n= = History = =\nThere is evidence of human activity on the site pre @-@ dating the Iron Age , a...\nA flint dagger was discovered on the site . This type of artefact is rare in Gr...\nThe hill fort was built in and used throughout the Iron Age , as demonstrated b...\nFragments of glass , possibly Roman in origin , and shards of pottery which date to the 1st and\n2nd centuries AD , indicate the site was used in the Romano @-@ British period .\nHowever no Roman\nstructures have been discovered , and the nature of Roman activity at the site is a source of\nspeculation .\nThe position of the hilltop indicate that it was easily defended ; however , local finds\nindicate it was a high @-@ status settlement rather than a military outpost unless a similar feature\nwas located nearby .\nOne reason that Roman structures have not been identified is that the Romano\n21\nSparQ Attention: Bandwidth-Efficient LLM Inference\n### BPC\nDENSE: 0.669\nSPARQ: 0.673\nH2O: 0.685\nLM-INFINITE: 0.692\n22\n"
  },
  {
    "title": "DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.05107.pdf",
    "upvote": "30",
    "text": "DreaMoving: A Human Video Generation Framework\nbased on Diffusion Models\nMengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo,\nXianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, Aojie Li, Xiaoyang Kang, Biwen Lei,\nMiaomiao Cui, Peiran Ren, Xuansong Xie\nAlibaba Group\n{mengyang.fmy, ljl191782, jinmao.yk, ryan.yy, huizheng.hz, guoxiefan.gxf,\nxianhui.lxh, haolan.xhl, zhicheng.sc, lxw262398, liaojie.laj, kangxiaoyang.kxy, biwen.lbw\nmiaomiao.cmm, peiran.rpr, xingtong.xxs}@alibaba-inc.org\nAbstract\nIn this paper, we present DreaMoving, a diffusion-based\ncontrollable video generation framework to produce high-\nquality customized human videos. Specifically, given tar-\nget identity and posture sequences, DreaMoving can gen-\nerate a video of the target identity moving or dancing any-\nwhere driven by the posture sequences. To this end, we pro-\npose a Video ControlNet for motion-controlling and a Con-\ntent Guider for identity preserving. The proposed model\nis easy to use and can be adapted to most stylized diffu-\nsion models to generate diverse results. The project page\nis available at https://dreamoving.github.io/\ndreamoving.\n1. Introduction\nRecent text-to-video (T2V) models like Stable-Video-\nDiffusion1 and Gen22 make breakthrough progress in video\ngeneration.\nHowever, it is still a challenge for human-\ncentered content generation, especially character dance.\nThe problem involves the lack of open-source human dance\nvideo datasets and the difficulty of obtaining the corre-\nsponding precise text description, making it a challenge to\ntrain a T2V model to generate videos with intraframe con-\nsistency, longer length, and diversity. Besides, personal-\nization and controllability stand as paramount challenges in\nthe realm of human-centric content generation, attracting\nsubstantial scholarly attention. Representative research like\nControlNet [13] is proposed to control the structure in the\nconditional image generation, while DreamBooth [10] and\nLoRA [6] show the ability in appearance control through\n1https : / / stability . ai / news / stable - video -\ndiffusion-open-ai-video-model\n2https://research.runwayml.com/gen2\nimages. However, these techniques often fail to offer pre-\ncise control over motion patterns or necessitate hyperpa-\nrameter fine-tuning specific to target identities, introducing\nan additional computational burden. Customized video gen-\neration is still under investigation and represents a largely\nuncharted territory.\nIn this paper, we present a human\ndance video generation framework based on diffusion mod-\nels (DM), named DreaMoving.\nThe rest of the paper is organized as follows. Sec. 2\npresents a detailed description of how the DreaMoving is\nbuilt. Sec. 3 presents some results generated by our method.\n2. Architecture\nDreaMoving is built upon Stable-Diffusion [9] models. As\nillustrated in Fig. 1, it consists of three main networks, in-\ncluding the Denoising U-Net, the Video ControlNet, and the\nContent Guider. Inspired by AnimateDiff [5], we insert mo-\ntion blocks after each U-Net block in the Denoising U-Net\nand the ControlNet. The Video ControlNet and the Con-\ntent Guider work as two plug-ins of the Denoising U-Net\nfor controllable video generation. The former is responsi-\nble for motion-controlling while the latter is in charge of the\nappearance representation.\n2.1. Data Collection and Preprocessing\nTo gain better performance in generating human videos, we\ncollected around 1,000 high-quality videos of human dance\nfrom the Internet. As the training of the temporal module\nneeds continuous frames without any transitions and spe-\ncial effects, we further split the video into clips and finally\ngot around 6,000 short videos (8\u223c10 seconds). For text de-\nscription, we take Minigpt-v2 [3] as the video captioner.\nSpecifically, using the \u201cgrounding\u201d version, the instruction\nis [grounding] describe this frame in a detailed manner.\nThe generated caption of the centered frame in keyframes\narXiv:2312.05107v2  [cs.CV]  11 Dec 2023\nVAE\nDecoder\na girl in a light \nyellow dress, \non the beach\nPose/Depth Sequence\nCloth \nImage\n(Optional)\nNoise\nText Prompt\nReference Images\nU-Net Block\nMotion Block\na girl in a light \nyellow dress, \non the beach\nCloth \nImage\n(Optional)\nText\nEncoder\nImage\nEncoder\nConcat\nContent Embeddings\nDenoising U-Net\nVideo ControlNet\nContent Guider\nT\nIteratively Denoise\nFigure 1. The overview of DreaMoving. The Video ControlNet is the image ControlNet [13] injected with motion blocks after each U-Net\nblock. The Video ControlNet processes the control sequence (pose or depth) to additional temporal residuals. The Denoising U-Net is\na derived Stable-Diffusion [9] U-Net with motion blocks for video generation. The Content Guider transfers the input text prompts and\nappearance expressions, such as the human face (the cloth is optional), to content embeddings for cross attention.\nrepresents the whole video clip\u2019s description, mainly de-\nscribing the content of the subject and background faith-\nfully.\n2.2. Motion Block\nTo improve the temporal consistency and motion fidelity,\nwe integrate motion blocks into both the Denosing U-Net\nand ControlNet. The motion block is extended from the An-\nimateDiff [5], and we enlarge the temporal sequence length\nto 64. We first initialize the weights of motion blocks from\nthe AnimateDiff (mm sd v15.ckpt) and fine-tune them on\nthe private human dance video data.\n2.3. Content Guider\nThe Content Guider is designed to control the content of\nthe generated video, including the appearance of human and\nthe background. One simple way is to describe the human\nappearance and background with a text prompt, such as \u2019a\ngirl in a white dress, on the beach\u2019. However, it is hard\nto describe a personalized human appearance for a normal\nuser. Even by complex prompt engineering, the model may\nnot give the desired output.\nInspired by IP-Adapter [12], we propose to utilize the\nimage prompt for precise human appearance guidance and\nthe text prompt for background generation. Specifically,\na face image is used to encode the facial features through\nan image encoder, and a cloth/body image is optionally in-\nvolved to encode the body features. The text and human ap-\npearance features are concatenated as the final content em-\nbeddings. The content embeddings are then sent to cross-\nattention layers for human appearance and background rep-\nresentations as described in IP-Adapter [12].\nGiven the\nquery features Z, the text features ct, the face features cf,\nand the cloth features cc, the output of cross-attention Z\n\u2032\ncan be defined by the following equation:\nZ\u2032 = soft max\n\u0012QKT\nt\n\u221a\nd\n\u0013\nVt + \u03b1fsoft max\n \nQKT\nf\n\u221a\nd\n!\nVf\n+ \u03b1csoft max\n\u0012QKT\nc\n\u221a\nd\n\u0013\nVc,\n(1)\nwhere, Q = ZWq, Kt = ctW t\nk, and Vt = ctW t\nv are the\nquery, key, and values matrices from the text features, Kf =\ncfW f\nk , and Vf = cfW f\nv are the key, and values matrices\nfrom the face features, and Kc = ccW c\nk, and Vc = ccW c\nv\nare the key, and values matrices from the cloth features. \u03b1f\nand \u03b1c are the weights factor.\n2.4. Model Training\n2.4.1\nContent Guider Training\nThe Content Guider serves as an independent module for\nbase diffusion models.\nOnce trained, it can generalized\nto other customized diffusion models. We trained Content\nGuider based on SD v1.5 and used OpenCLIP ViT-H14 [7]\nas the image encoder as [12]. To better preserve the identity\nof the reference face, we employ the Arcface [4] model to\nextract the facial correlated features as a supplement to the\nclip features. We collect the human data from LAION-2B,\nthen detect and filter images without faces. During training,\nthe data are randomly cropped and resized to 512 \u00d7 512.\nContent Guider is trained on a single machine with 8 V100\nGPUs for 100k steps, batch size is set to 16 for each GPU,\nAdamW optimizer [8] is used with a fixed learning rate of\n1e \u2212 4 and weight decay of 1e \u2212 2.\n2.4.2\nLong-Frame Pretraining\nWe first conduct a warming-up training stage to extend the\nsequence length in the motion module from 16 to 64 on the\nvalidation set (5k video clips) of WebVid-10M [1]. We only\ntrain the motion module of the Denoising U-Net and freeze\nthe weights of the rest of the network. No ControlNet and\nimage guidance are involved in this stage. The learning rate\nis set to 1e \u2212 4 and the resolution is 256 \u00d7 256 (resize &\ncenter crop). The training is stopped after 10k steps with a\nbatch size of 1.\n2.4.3\nVideo ControlNet Training\nAfter the long-frame pretraining, we train the Video Con-\ntrolNet with the Denoising U-Net by unfreezing both the\nmotion blocks and U-Net blocks in the Video ControlNet\nand fixing the Denoising U-Net. The weights of motion\nblocks in Video ControlNet are initialized from the long-\nframe pretraining stage. In this stage, we train the network\non the collected 6k human dance video data. No image\nguidance is involved in this stage. The human pose or depth\nis extracted as the input of the Video ControlNet using DW-\nPose [11] or ZoeDepth [2], respectively. The learning rate\nis set to 1e\u22124 and the resolution is 352\u00d7352. The training\nis stopped after 25k steps with a batch size of 1.\n2.4.4\nExpression Fine-tuning\nTo gain better performance in human expression generation,\nwe further fine-tune the motion blocks in Denoising U-Net\nby training with the Video ControlNet on the collected 6k\nhuman dance video data. In this stage, the whole Video\nControlNet and the U-Net blocks in the Denoising U-Net\nare locked, and only the weights of the motion blocks in\nthe Denoising U-Net are updated. The learning rate is set\nto 5e \u2212 5 and the resolution is 512 \u00d7 512. The training is\nstopped after 20k steps with a batch size of 1.\n2.5. Model Inference\nAt the inference stage, the inputs are composed of the text\nprompt, the reference images, and the pose or depth se-\nquence. The control scale of the Video ControlNet is set\nto 1.0 for pose or depth only. Our method also supports\nthe form of multi-controlnet, and the depth and pose Video\nControlNets can be used simultaneously. The strength of\nthe face/body guidance is also controllable in the Content\nGuider by adjusting the \u03b1f and \u03b1c in Eqn. 1. The content is\nfully controlled by the text prompt if \u03b1f = \u03b1c = 0.\n3. Results\nDreaMoving can generate high-quality and fidelity videos\ngiven guidance sequence and simple content description\n(text prompt only, image prompt only, or text-and-image\nprompts) as input. In Fig. 2, we show the result with text\nprompt only. To keep the face identity, the user can input\nthe face image to the Content Guider to generate a video of\nsome specific person (demonstrated in Fig. 3). Moreover,\nthe user can define both the face content and clothes con-\ntent, as exhibited in Fig. 4. We further test the generaliza-\ntion of the proposed method on images of unseen domains.\nIn Fig. 5, we run DreaMoving using unseen stylized im-\nages. Our method is able to generate videos in accordance\nwith the style and content in the input image.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021. 3\n[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M\u00a8uller. Zoedepth: Zero-shot transfer by com-\nbining relative and metric depth, 2023. 3\n[3] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan\nZhang,\nRaghuraman\nKrishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning.\narXiv preprint\narXiv:2310.09478, 2023. 1\n[4] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. Arcface: Additive angular margin loss for deep\nface recognition.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n4690\u20134699, 2019. 3\n[5] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2\n[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n1\n[7] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n2\na girl with short hair\nwearing black clothes \nin the room\nA cheerleader wearing \nred and golden uniform \non the football field\na woman with long\nhair wearing white \nsuit and pants in the \nstreet\nFigure 2. The results of DreaMoving with text prompt as input.\na girl, smiling, in the park \nwith golden leaves in autumn \nwearing coat with long sleeve\na girl, smiling, standing on \nbeach, wearing light yellow \ndress with long sleeves\na man, dancing in front of \nPyramids of Egypt, wearing \na suit with a blue tie\na girl, smiling, dancing in a \nFrench town, wearing long \nlight blue dress\nFigure 3. The results of DreaMoving with text prompt and face image as inputs.\nInput\nVideo Frame\na girl, in a fairytale town covered \nin snow, wearing a long pants.\na girl, in the forest with great \nsunshine, wearing a long pants.\na girl, dancing in the center \npark, wearing a long pants.\nFigure 4. The results of DreaMoving with face and cloth images as inputs.\nPose Sequence\uff1a\nFigure 5. The results of DreaMoving with stylized image as input.\n[8] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 3\n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 1, 2\n[10] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 1\n[11] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effec-\ntive whole-body pose estimation with two-stages distillation.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 4210\u20134220, 2023. 3\n[12] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 2\n[13] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n1, 2\n"
  },
  {
    "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
    "link": "https://arxiv.org/pdf/2312.04655.pdf",
    "upvote": "19",
    "text": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations\nMaitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, Yezhou Yang\nArizona State University\n{maitreya.patel, kch, scheng53, chitta, yz.yang}@asu.edu\nAbstract\nText-to-image (T2I) diffusion models, notably the un-\nCLIP models (e.g., DALL-E-2), achieve state-of-the-art\n(SOTA) performance on various compositional T2I bench-\nmarks, at the cost of significant computational resources.\nThe unCLIP stack comprises T2I prior and diffusion im-\nage decoder.\nThe T2I prior model alone adds a bil-\nlion parameters compared to the Latent Diffusion Mod-\nels, which increases the computational and high-quality\ndata requirements. We introduce ECLIPSE1, a novel con-\ntrastive learning method that is both parameter and data-\nefficient. ECLIPSE leverages pre-trained vision-language\nmodels (e.g., CLIP) to distill the knowledge into the prior\nmodel. We demonstrate that the ECLIPSE trained prior,\nwith only 3.3% of the parameters and trained on a mere\n2.8% of the data, surpasses the baseline T2I priors with an\naverage of 71.6% preference score under resource-limited\nsetting. It also attains performance on par with SOTA big\nmodels, achieving an average of 63.36% preference score in\nterms of the ability to follow the text compositions. Exten-\nsive experiments on two unCLIP diffusion image decoders,\nKarlo and Kandinsky, affirm that ECLIPSE priors consis-\ntently deliver high performance while significantly reduc-\ning resource dependency.\nProject page: https://eclipse-\nt2i.vercel.app/\n1. Introduction\nDiffusion models [12, 35, 37, 42] have demonstrated re-\nmarkable success in generating high-quality images condi-\ntioned on text prompts. This Text-to-Image (T2I) genera-\ntion paradigm has been effectively applied to various down-\nstream tasks such as subject/segmentation/depth-driven im-\nage generation [3, 5, 9, 20, 29]. Central to these advance-\nments are two predominant text-conditioned diffusion mod-\nels: Latent Diffusion Models (LDM) [37], also known as\n1Our strategy, ECLIPSE, draws an analogy from the way a smaller prior\nmodel, akin to a celestial entity, offers a glimpse of the grandeur within the\nlarger pre-trained vision-language model, mirroring how an eclipse reveals\nthe vastness of the cosmos.\nECLIPSE\nSDv2.1\nSDv1.4\nWurstchen\nKarlo\nKandinsky v2.1\nKandinsky v2.2\nSDXL\nDALLE-2\n55% Improvement\n72% Reduction\nData Size (M)\n5\nFigure 1. Comparison between SOTA text-to-image models with\nrespect to their total number of parameters and the average per-\nformance on the three composition tasks (color, shape, and tex-\nture). ECLIPSE achieves better results with less number of param-\neters without requiring a large amount of training data. The shown\nECLIPSE trains a T2I prior model (having only 33M parameters)\nusing only 5M image-text pairs with Kandinsky decoder.\nStable Diffusion, and unCLIP models [35]. The LDM, no-\ntable for its open-source availability, has gained widespread\npopularity within the research community. On the other\nhand, unCLIP models have remained under-studied. Both\nmodel types fundamentally focus on training the diffusion\nmodels conditioned on text prompts. The LDM contains a\nsingular text-to-image diffusion model, while unCLIP mod-\nels have a text-to-image prior, and a diffusion image de-\ncoder. Both model families work within the vector quan-\ntized latent space of the image [43]. In this paper, we focus\non unCLIP models because they consistently outperform\nother SOTA models in various composition benchmarks\nsuch as T2I-CompBench [13] and HRS-Benchmark [1].\nThese T2I models, typically large in parameter count, re-\nquire massive amounts of high-quality image-text pairs for\ntraining. unCLIP models like DALL-E-2 [35], Karlo [7],\nand Kandinsky [36], feature prior module containing ap-\nproximately 1 billion parameters, resulting in a significant\nincrease in overall model size (\u2265 2B) compared to LDMs.\nThese unCLIP models are trained on 250M, 115M, and\n177M image-text pairs, respectively. Therefore, two critical\narXiv:2312.04655v1  [cs.CV]  7 Dec 2023\nquestions remain:\n1) Does the incorporation of a text-to-\nimage prior contribute to SOTA performance on text com-\npositions? 2) Or is scaling up model size the key factor? In\nthis study, we aim to deepen the understanding of T2I priors\nand propose substantial enhancements to existing formula-\ntions by improving parameter and data efficiency.\nAs proposed by Ramesh et al. [35], T2I priors are also\ndiffusion models, which are designed to directly estimate\nthe noiseless image embedding at any timestep of the dif-\nfusion process.\nWe perform an empirical study to ana-\nlyze this prior diffusion process. We find that this diffu-\nsion process has a negligible impact on generating accurate\nimages and having the diffusion process slightly hurts the\nperformance. Moreover, diffusion models require substan-\ntial GPU hours/days for training due to the slower conver-\ngence. Therefore, in this work, we use the non-diffusion\nmodel as an alternative. While this approach may reduce the\ncompositional capabilities due to the absence of classifier-\nfree guidance [11], it significantly enhances parameter effi-\nciency and decreases the dependencies on the data.\nTo overcome the above limitations, in this work, we in-\ntroduce ECLIPSE, a novel contrastive learning strategy to\nimprove the T2I non-diffusion prior.\nWe improve upon\nthe traditional method of maximizing the Evidence Lower\nBound (ELBO) for generating the image embedding from\nthe given text embedding. We propose to utilize the se-\nmantic alignment (between the text and image) property\nof the pre-trained vision-language models to supervise the\nprior training. Utilizing ECLIPSE, we train compact (97%\nsmaller) non-diffusion prior models (having 33 million pa-\nrameters) using a very small portion of the image-text pairs\n(0.34% - 8.69%). We train ECLIPSE priors for two unCLIP\ndiffusion image decoder variants (Karlo and Kandinsky).\nThe ECLIPSE-trained priors significantly surpass baseline\nprior learning strategies and rival the performance of 1 bil-\nlion parameter counterparts. Our results indicate a promis-\ning direction for T2I generative models, achieving better\ncompositionality without relying on extensive parameters\nor data. As illustrated in Fig. 1, by simply improving the\nT2I prior for unCLIP families, their overall parameter and\ndata requirements drastically reduce and achieve the SOTA\nperformance against similar parameter models.\nContributions. 1) We introduce ECLIPSE, the first attempt\nto employ contrastive learning for text-to-image priors in\nthe unCLIP framework. 2) Through extensive experimenta-\ntion, we demonstrate ECLIPSE\u2019s superiority over baseline\npriors in resource-constrained environments. 3) Remark-\nably, ECLIPSE priors achieve comparable performance to\nlarger models using only 2.8% of the training data and 3.3%\nof the model parameters. 4) We also analyze and offer em-\npirical insights on the shortcomings of existing T2I diffu-\nsion priors.\n2. Related Works\nText-to-Image Generative Models.\nAdvancements in\nvector quantization and diffusion modeling have notably\nenhanced text-to-image generation capabilities.\nNotable\nworks like DALL-E [34] have leveraged transformer mod-\nels trained on quantized latent spaces. Contemporary state-\nof-the-art models, including GLIDE [26], Latent Diffusion\nModel (LDM) [37], DALL-E-2 [35], and Imagen [38], have\nsignificantly improved over earlier approaches like Stack-\nGAN [47] and TReCS [19].\nAs these models achieve\nremarkable photorealism, several works focus on making\nT2I models more secure [8, 16, 17, 27].\nLDM models\nprimarily focus on unified text-to-image diffusion models\nthat incorporate the cross-attention layers [37]. Addition-\nally, several studies aim at refining Stable Diffusion mod-\nels during inference through targeted post-processing strate-\ngies [3, 5, 32]. In contrast, unCLIP models, exemplified\nby DALL-E-2 [15], Karlo [7], and Kandinsky [36], incor-\nporate a two-step process of text-to-image diffusion trans-\nformer prior model and diffusion image decoder having the\nsame model architecture as LDMs.\nRecent benchmarks\nhave highlighted the superior compositional capabilities of\nDALL-E-2 over LDM methods [1, 13]. Our work exam-\nines and enhances existing prior learning strategies in open-\nsource pre-trained unCLIP models, Karlo and Kandinsky.\nEfficient Text-to-Image Models.\nThe current genera-\ntion of T2I models is characterized by extensive parame-\nter sizes and demanding training requirements, often ne-\ncessitating thousands of GPU days. Research efforts have\nprimarily centered on model refinement through knowl-\nedge distillation, step distillation, and architectural opti-\nmization [21, 25, 39]. Wuerstchen [31] presents an efficient\nunCLIP stack requiring fewer training time GPU hours.\nConcurrently, Pixart-\u03b1 [4] leverages pre-trained Diffusion-\nTransformers (DiT) [30] as base diffusion models, further\nreducing training time.\nDistinctively, ECLIPSE focuses\non refining text-to-image priors within the unCLIP frame-\nwork using a mere 3.3% of the original model parameters,\nthereby significantly reducing the training duration to ap-\nproximately 200 GPU hours. Our work falls orthogonal\nto the existing efficient T2I methodologies that mainly fo-\ncus on knowledge and step distillation, and/or architectural\ncompression. When integrated with these model compres-\nsion strategies, ECLIPSE can position the unCLIP family\nmodels as a compact yet highly accurate and efficient T2I\ngeneration methodology.\nContrastive Learning in Generative Models. Contrastive\nlearning, traditionally applied in visual discriminative tasks,\nhas seen utilization in image-text alignment models like\nCLIP [33], LiT [45], and SigLIP [46]. However, its ap-\nplication in generative models, particularly in Generative\nAdversarial Networks (GANs), remains limited [6, 22, 48].\nUnseen composition\ntext space\nvision space\n\ud835\udc4d!\"\n\ud835\udc4d#\n\"\n\"\n\ud835\udc4d#\"\n\u2112!\"#\nLearned feature representation\nPrior\n\ud835\udc4d#\n\"\nOrange\nCircle\nBlue\nCircle\nBlue\nSquare\nOrange\nSquare\nInput Batch\n(text)\nGround truth image\n(image)\nStandard unCLIP - prior\n\ud835\udc4d#\n\ud835\udcdb\ud835\udc90\ud835\udc83\ud835\udc8b\n\ud835\udc4d!\nTrainable Model\nPrior\n\ud835\udc4d#\n\"\nOrange\nCircle\nBlue\nCircle\nBlue\nSquare\nOrange\nSquare\nInput Batch\n(text)\nGround truth image\nECLIPSE (ours)\n\ud835\udc4d#\n\ud835\udcdb\ud835\udc90\ud835\udc83\ud835\udc8b\n\ud835\udc4d!\n\ud835\udcdb\ud835\udc6a\ud835\udc73\ud835\udc7a\n\ud835\udc4d!\"\n\ud835\udc4d#\"\n\"\nFrame of reference\n\u2112$%&  \n\u2245 min \ud835\udf03\n+ max (\ud835\udf19)\n\ud835\udf03\n\ud835\udf19\ntext space\nvision space\nMinimize similarity\nLearned Representation\n\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d: \ud835\udc61\nLearned Representation\n\ud835\udc4d!*\nFigure 2. Standard T2I prior learning strategies (top) minimizes the mean squared error between the predicted vision embedding \u02c6zx w.r.t.\nthe ground truth embedding zx with or without time-conditioning. This methodology cannot be generalized very well to the outside\ntraining distribution (such as Orange Square). The proposed ECLIPSE training methodology (bottom) utilizes the semantic alignment\nproperty between zx and zy with the use of contrastive learning, which improves the text-to-image prior generalization.\nFor instance, Lafite [48] employs a contrastive approach\nfor image-to-text prior training in language-free T2I GANs.\nStyleT2I [22] attempts to learn the latent edit direction for\nStyleGAN [14], which is supervised via spatial masks on\nthe images making the method not scalable. ACTIG [6] in-\ntroduces an attribute-centric contrastive loss to enhance dis-\ncriminator performance. These methods are constrained by\ntheir domain-specific knowledge requirements and inability\nto be directly applied to diffusion models [6, 22]. In con-\ntrast, ECLIPSE applies CLIP-based contrastive learning to\ntrain more effective T2I prior models in diffusion-based T2I\nsystems. This strategy is not only resource-efficient but sig-\nnificantly enhances the traditional text-to-image diffusion\npriors by exploiting the semantic latent space of pre-trained\nvision-language models.\n3. Methodology\nThis section elaborates on the Text-to-Image (T2I) method-\nologies, beginning with an overview of unCLIP, followed\nby the formal problem statement.\nWe then delve into\nour proposed training strategy, ECLIPSE, for T2I prior in\ndetail.\nFigure 2 provides the overview of baselines and\nECLIPSE training strategies.\n3.1. Preliminaries\nWithout the loss of generality, let\u2019s assume that y \u2208 Y\ndenotes the raw text and x \u2208 X denotes the raw image.\nzx and zy denote the image and text latent embeddings\nextracted using the pre-trained vision and text encoders\n(zx = Cvision(x);\nzy = Ctext(y)). Ideally, these Ctext\nand Cvision can be any model (e.g., T5-XXL, ViT, and\nCLIP). Both model families (LDM and unCLIP) fundamen-\ntally focus on learning a mapping function f\u03b8 : Y \u2192 X.\nThe LDMs contain a singular text-to-image decoder model\n(f\u03b8), while unCLIP framework (f\u03b8 = h\u03b8 \u25e6 g\u03d5) contains two\nprimary modules:\n\u2022 Text-to-Image Prior (g\u03d5 : zy \u2192 zx): This module maps\nthe text embeddings to the corresponding vision embed-\ndings. Ramesh et al. [35] showed that the diffusion model\nas T2I prior leads to slightly better performance than the\nautoregressive models. For each timestep t and a noised\nimage embedding z(t)\nx\n\u223c q(t, zx) (here, q is a forward\ndiffusion process), the diffusion prior directly estimates\nnoiseless zx rather than estimating Gaussian noise distri-\nbution \u03f5 \u223c N(0, I) as:\nLprior =\nE\nt\u223c[0,T ],\nz(t)\nx \u223cq(t,zx)\nh\n||zx \u2212 g\u03d5(z(t)\nx , t, zy)||2\n2\ni\n.\n(1)\n\u2022 Diffusion Image Decoder (h\u03b8 : (zx, zy) \u2192 x): This\nmodule generates the final image conditioned on the zx\nand the input text features zy. This diffusion decoder fol-\nlows the standard diffusion training procedure by estimat-\ning \u03f5 \u223c N(0, I) after [12]:\nLdecoder =\nE\n\u03f5\u223cN(0,I)\nt\u223c[0,T ],\n(zx, zy)\nh\n||\u03f5 \u2212 h\u03b8(x(t), t, zx, zy)||2\n2\ni\n.\n(2)\nSpecifically, different versions of the unCLIP decoder,\nsuch as Kandinsky and Karlo, vary in whether they include\ntext conditioning (zy) in the diffusion image decoder. How-\never, both approaches yield comparable results, provided\nthat image conditioning (zx) is accurate.\nBoth training\nobjectives, Lprior and Ldecoder, integrate Classifier-Free\nGuidance (CFG) [11], enhancing the model\u2019s generative ca-\npabilities. During training, conditions are omitted 10% of\nthe time to foster unconditional generation, subsequently\nimproving test performance as CFG works as implicit clas-\nsifier guidance [11].\n3.2. Problem Formulation\nGiven the pivotal role of the T2I prior module in image gen-\neration from text, in this paper, our focus is on enhancing\ng\u03d5, while keeping the pre-trained h\u03b8 frozen.\nLet\u2019s con-\nsider a training distribution PXY , comprising input pairs\nof image and text (x, y). Maximizing the Evidence Lower\nBound (ELBO) on the training distribution PXY facilitates\nthis mapping of zy \u2192 zx. However, such a strategy does not\ninherently assure generalization, especially when the input\ntext prompt (y) deviates from the assumed independently\nand identically distributed (i.i.d.)\npattern of PXY [44].\nTherefore, attaining a more diverse and representative PXY\nbecomes crucial for improving the performance. While a\ndiffusion prior combined with CFG has been shown to bol-\nster generalization, especially with diverse training data and\nextensive training iterations [28], it is computationally ex-\npensive and is not always reliable (especially, in low re-\nsource constraint settings) as shown in Section 4.2. Given\nthese constraints, our goal is to develop an alternative prior\nlearning methodology that not only improves parameter ef-\nficiency (97% reduction) and mitigates the need for large-\nscale high-quality data (\u2264 5%) but also upholds perfor-\nmance levels.\n3.3. Proposed Method: ECLIPSE\nThis section elaborates on ECLIPSE, our model training\nstrategy to learn text-to-image prior (g\u03d5). We focus on en-\nhancing non-diffusion prior models through the effective\ndistillation of pre-trained vision-language models, such as\nCLIP, while preserving the semantic alignment between the\ninput text embedding zy and corresponding estimated vision\nembeddings \u02c6zx by using the contrastive loss.\nBase Prior Model. T2I diffusion prior deviates from the\nstandard diffusion objective (such as Eq. 2).\nUnlike the\nstandard \u03f5 \u223c N(0, I) prediction-based diffusion objective,\nthe T2I diffusion prior objective (Eq. 1) do not compare two\nGaussian distributions, instead, it directly estimates the zx\nwhich is noiseless. However, during inference, we still ad-\nhere to the conventional denoising process, introducing ad-\nditional noise (\u03c3t\u03f5) at each step, except for the final step\naccording to Ho et al. [12]. This creates a new input distri-\nbution (zx + \u03c3t\u03f5), possibly unencountered during training.\nMoreover, if we repeat this for T timesteps, it can lead to the\naccumulation of errors, which is undesirable. We provide\nempirical analysis in Section 5 to ground this hypothesis,\nwhere we show that having more diffusion prior steps does\nnot benefit the overall text-to-image generation abilities.\nTherefore, to mitigate this unnecessary computing, we\nuse non-diffusion T2I prior, making the prior model both\nparameter-efficient and less demanding in terms of com-\nputational resources. This non-diffusion architecture forms\nour base model, and we introduce the training objective that\nleverages pre-trained vision-language models trained on ex-\ntensive datasets to improve generalization outside the PXY\ndistribution.\nProjection Objective.\nDespite vision-language models\naligning the semantic distributions across modalities, each\nmodality may exhibit unique distributions. Therefore, our\napproach involves projecting the text embedding onto the\nvision embedding. This is achieved using a mean squared\nerror objective between the predicted vision embedding (\u02c6zx)\nand the ground truth vision embedding (zx):\nLproj =\nE\n\u03f5\u223cN (0,I)\nzy,zx\nh\n||zx \u2212 g\u03d5(\u03f5, zy)||2\n2\ni\n,\n(3)\nwhere \u03f5 is the Gaussian input noise. Notably, as discussed\npreviously, this is an approximation of the diffusion prior\nobjective (Eq. 1) with t = T and without CFG. Lproj learns\nlatent posterior distribution with the i.i.d. data assumption.\nHowever, this model, fine-tuned on PXY , may not general-\nize well beyond its distribution. The optimal solution would\nbe to train on a dataset that encapsulates all potential distri-\nbutions to cover all possible scenarios, which is an imprac-\ntical and resource-consuming task.\nCLIP Contrastive Learning.\nTo address these limita-\ntions, we propose utilizing the CLIP more effectively, which\ncontains the semantic alignment between image and lan-\nguage. Specifically, we apply the CLIP Contrastive Loss\nafter [33] to train the T2I priors. For a given input batch\n{(zi\nx, zi\ny)}N\ni=1 from the PXY distribution, we calculate the\ntext-conditioned image contrastive loss for the ith image\nembedding prediction relative to the all input ground truth\ntext embeddings as:\nLCLS; y\u2192x = \u2212 1\nN\nN\nX\ni=0\nlog\nexp(\u27e8\u02c6zi\nx, zi\ny\u27e9/\u03c4)\nP\nj\u2208[N] exp(\u27e8\u02c6zix, zj\ny\u27e9/\u03c4)\n, (4)\nwhere \u03c4 is the temperature parameter, \u27e8, \u27e9 denotes the cosine\nsimilarity, and N is the batch size. This loss encourages\nthe model to understand and follow the input text better,\neffectively reducing overfitting to the PXY , as illustrated in\nFigure 2. Consequently, the final objective function is:\nLECLIP SE = Lproj + \u03bb \u2217 LCLS; y\u2192x,\n(5)\nTable 1. The comparison (in terms of FID and compositions) of the baselines and state-of-the-art methods with respect to the ECLIPSE. * in-\ndicates the official reported ZS-FID. \u03a8 denotes the FID performance of a model trained on MSCOCO. The best performing ECLIPSE vari-\nant (with respect to its big counterpart) is highlighted by green . ECLIPSE consistently outperforms the SOTA big models despite being\ntrained on a smaller subset of dataset and parameters.\nMethods\nModel\nTraining\nTotal\nData\nZS-\nT2I-CompBench\nType\nParams [M]*\nParams [B]\nSize [M]\nFID (\u2193)\nColor (\u2191)\nShape (\u2191)\nTexture (\u2191)\nSpatial (\u2191)\nNon-Spatial (\u2191)\nStable Diffusion v1.4\nLDM\n900\n0.9\n400\n16.31*\n0.3765\n0.3576\n0.4156\n0.1246\n0.3076\nStable Diffusion v2.1\nLDM\n900\n0.9\n2000\n14.51*\n0.5065\n0.4221\n0.4922\n0.1342\n0.3096\nWurstchen\nunCLIP\n1000\n2.0\n1420\n23.60*\n0.3216\n0.3821\n0.3889\n0.0696\n0.2949\nKandinsky v2.1\nunCLIP\n1000\n2.22\n177\n18.09\n0.4647\n0.4725\n0.5613\n0.1219\n0.3117\nDALL-E-2\nunCLIP\n1000\n4.5\n250\n10.65*\n0.5750\n0.5464\n0.6374\n0.1283\n0.3043\nKarlo\nunCLIP\n1000\n1.9\n115\n20.64\n0.5127\n0.5277\n0.5887\n0.1337\n0.3112\nKarlo\n33\n0.93\n0.6MSCOCO\n23.67\u03a8\n0.5965\n0.5063\n0.6136\n0.1574\n0.3235\n33\n0.93\n2.5CC3M\n26.73\n0.5421\n0.5090\n0.5881\n0.1478\n0.3213\nECLIPSE (ours)\n33\n0.93\n10.0CC12M\n26.98\n0.5660\n0.5234\n0.5941\n0.1625\n0.3196\nKandinsky v2.2\nunCLIP\n1000\n2.22\n177\n20.48\n0.5768\n0.4999\n0.5760\n0.1912\n0.3132\nKandinsky v2.2\n34\n1.26\n0.6MSCOCO\n16.53\u03a8\n0.5785\n0.4951\n0.6173\n0.1794\n0.3204\nECLIPSE (ours)\n34\n1.26\n5.0HighRes\n19.16\n0.6119\n0.5429\n0.6165\n0.1903\n0.3139\nwhere \u03bb is the hyperparameter balancing the regularizer\u2019s\neffect. Overall, the final objective function aims to map the\ntext latent distribution to the image latent distribution via\nLproj and such that it preserves the image-text alignment\nusing LCLS; y\u2192x. This makes the prior model generalize\nbeyond the given training distribution PXY such that it can\nfollow the semantic alignment constraint. Importantly, we\ncannot use LCLS; y\u2192x alone or with a high value of \u03bb as\nthe prior model will converge outside the vision latent dis-\ntribution that optimizes the contrastive loss (such input text\nlatent space itself). And keeping \u03bb to a very low value can-\nnot do knowledge distillation well enough. Empirical stud-\nies suggest setting \u03bb = 0.2 for optimal performance, bal-\nancing knowledge distillation, and maintaining alignment\nwithin the vision latent distribution.\n4. Experiments & Results\nThis section introduces the datasets, training specifications,\ncomparative baselines, and evaluation metrics utilized in\nour experiments. We conduct an extensive assessment of\nour proposed ECLIPSE methodology and its variants, both\nquantitatively and qualitatively.\n4.1. Experimental Setup\nDataset.\nOur experiments span four datasets of vary-\ning sizes: MSCOCO [23], CC3M [41], CC12M [2], and\nLAION-HighResolution2 [40].\nMSCOCO comprises ap-\nproximately 0.6 million image-text pairs, while CC3M and\nCC12M contain around 2.5 and 10 million pairs, respec-\ntively 3 . We select a very small subset of 5 million (2.8%)\nimage-text pairs from the LAION-HighRes dataset (175M).\n2https://huggingface.co/datasets/laion/laion-\nhigh-resolution\n3According to the download date: 08/26/2023\nWe perform Karlo diffusion image decoder-related experi-\nments on MSCOCO, CC3M, and CC12M as these datasets\nare subsets of the data used to train the Karlo diffusion\nimage decoder. Similarly, we use MSCOCO and LAION-\nHighRes for the Kandinsky decoder.\nBaselines. ECLIPSE variants are compared against leading\nT2I models, including Stable Diffusion, Wurstchen, Karlo,\nKandinsky, and DALL-E-2. Additionally, we introduce two\nmore baselines along with ECLIPSE to evaluate the impact\nof our training strategy in a resource-constrained environ-\nment: 1) Projection: A non-diffusion prior model trained\nwith Lproj (Eq. 3). 2) Diffusion-Baseline: A diffusion prior\nmodel trained with Lprior (Eq. 1) \u2013 the traditional T2I prior,\nand 3) ECLIPSE: A non-diffusion prior model trained with\nour proposed methodology LECLIP SE (Eq. 5).\nTraining and inference details. We evaluate ECLIPSE us-\ning two pre-trained image decoders: Karlo-v1-alpha and\nKandinsky v2.2, trained on distinct CLIP vision encoders.\nOur prior architecture is based on the standard PriorTrans-\nformer model [35], modified to be time-independent. The\ndetailed architecture is outlined in the appendix. We con-\nfigure prior models with 33 and 34 million parameters for\nKarlo and Kandinsky, respectively.\nThis contrasts with\nlarger models in the field, which often use up to 1 bil-\nlion parameters (as summarized in Table 1). The Projec-\ntion, Diffusion-Baseline, and ECLIPSE priors are trained\nfor both diffusion image decoders, maintaining consis-\ntent hyperparameters (including total number of param-\neters) across all models.\nTraining on CC12M, CC3M,\nand LAION-HighRes is performed on 4 x RTX A6000\nGPUs with a 256 per-GPU batch size, a learning rate\nof 0.00005, and the CosineAnnealingWarmRestarts sched-\nuler [24]. Each model undergoes approximately 60,000 it-\nerations, totaling around 200 GPU hours. For MSCOCO,\ntraining takes about 100 GPU hours. This can be further\n\u201cA blue backpack\nand a brown \ncow\u201d\n\u201can apple and an \nelephant with \nthe apple being \nas big as an \nelephant\u201d\n\u201ca golden \nretriever \nwearing the red \nsunglasses at \nnight\u201d\n\u201ca green bench \nand blue bowl\u201d\n\u201ca giraffe next to \nthe night lamp\u201d\nWurstchen\nSDv1.4\nSDv2.1\nKandinsky\nECLIPSE\n1B + 1480M\n1B + 400M\n1B + 2000M\n1B + 177M\n0.03B + 0.6M\n0.03B + 5M\nParams + Dataset\nFigure 3. Qualitative result of our text-to-image prior, ECLIPSE, comparing with SOTA T2I model. Our prior model reduces the model\nparameter requirements (from 1 Billion \u2192 33 Million) and data requirements (from 177 Million \u2192 5 Million \u2192 0.6 Million). Given this\nrestrictive setting, ECLIPSE performs close to its huge counterpart (i.e., Kandinsky v2.2) and even outperforms models trained on huge\ndatasets (i.e., Wurstchen, SDv1.4, and SDv2.1) in terms of compositions.\nreduced to \u2264 50 GPU hours if image-text pairs are pre-\nprocessed beforehand. The diffusion prior is trained with a\nlinear scheduler and 1000 DDPM timesteps. Inferences uti-\nlize 25 DDPM steps with 4.0 classifier-free guidance, while\nProjection and ECLIPSE models do not require diffusion\nsampling. Image diffusion decoders are set to 50 DDIM\nsteps and 7.5 classifier-free guidance.\nEvaluation setup. Our evaluation framework encompasses\nvarious metrics.\nWe employ MS-COCO 30k to assess\nFID scores [10] and T2I-CompBench [13] for evaluat-\ning composition abilities in color, shape, texture, spatial,\nand non-spatial compositions. Given the impracticality of\nlarge-scale human studies, we approximate human prefer-\nences using PickScore [18], reporting results on the T2I-\nCompBench validation set comprising about 1500 unique\nprompts across different categories.\n4.2. Quantitative Evaluations\nIn Table 1, we present a performance comparison between\nECLIPSE variants and leading T2I models.\nOur eval-\nuation metrics include zero-shot Fr\u00b4echet Inception Dis-\ntance (FID) on MS-COCO 30k for image quality assess-\nment and T2I-CompBench [13] for evaluating composi-\ntionality.\nECLIPSE priors, trained with both types of\ndiffusion image decoders, demonstrate notable improve-\nments. ECLIPSE consistently surpasses various baselines\nin terms of compositionality, irrespective of the dataset size.\nIts performance is comparable to that of DALL-E-2 and\nother SOTA models, a significant improvement considering\nECLIPSE\u2019s parameter efficiency. Standard T2I priors usu-\nally incorporate 1 billion parameters, while ECLIPSE op-\nerates with only 3.3% of these parameters, maintaining\ncompetitive performance levels. When combined with cor-\nProjection\nDiffusion-Baseline\n0\n20\n40\n60\n80\nPercentage (%)\nECLIPSE with Karlo Decoder (MSCOCO)\nECLIPSE\nBaseline\nProjection\nDiffusion-Baseline\n0\n20\n40\n60\n80\nPercentage (%)\nECLIPSE with Kandinsky v2.2 Decoder (MSCOCO)\nECLIPSE\nBaseline\nwurstchen\nSD v1.4\nSD v2.1\nKarlo-v1-alpha Kandinsky v2.1 Kandinsky v2.2\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPercentage (%)\nECLIPSE\nBaseline\nFigure 4. Qualitative evaluations by human preferences approx-\nimated by the PickScore [18].\nThe top two figures compare\nECLIPSE to Projection and Diffusion Baselines trained with the\nsame amount of data and model size for both Karlo and Kandin-\nsky decoders. In the bottom figure, we compare ECLIPSE with the\nKandinsky v2.2 decoder trained on the LAION-HighRes dataset\nagainst SOTA models.\nresponding diffusion image decoders, the total parameter\ncount of ECLIPSE is close to that of Stable Diffusion mod-\nels, yet it outperforms them, especially considering that the\nlatter are trained on a massive set of image-text pairs. A\nnoticeable decline in zero-shot FID (ZS-FID) is observed in\ncomparison to the original Karlo. We attribute this varia-\ntion to the image quality differences in the training dataset,\nsuggesting a potential area for further investigation and im-\nprovement. At the same time, if we utilize the smaller sub-\nset of high-resolution datasets then we can still maintain\nbetter FID and improve the compositions, as shown in the\nlast row of Table 1. ECLIPSE prior with Kandinsky v2.2\ndecoder trained on LAION-HighRes subset achieves simi-\nlar FID to other original Kandinsky v2.2 unCLIP model and\nat the same time outperforming in terms of compositions.\nTable 2 provides a comparison of various baseline\ntraining strategies for small prior models, using identical\ndatasets and hyperparameters. ECLIPSE exhibits superior\nperformance across all datasets. We also note that diffusion\npriors benefit from larger datasets, supporting our premise\nthat such priors necessitate extensive training data for opti-\nmal results, which is also attributed to the CFG. In contrast,\nECLIPSE demonstrates the consistent performance on com-\npositions irrespective of the amount of image-text pairs.\n4.3. Qualitative Evaluations\nIn Figure 3,\nwe display qualitative examples from\nvarious\nmethods\nresponding\nto\ncomplex\nprompts.\nECLIPSE demonstrates superior performance in com-\nparison to Stable Diffusion v1.4, Stable Diffusion v2.1,\nTable 2.\nComparison of ECLIPSE with respect to the various\nbaseline prior learning strategies on four categories of composi-\ntion prompts in the T2I-CompBench. All prior models are of 33\nmillion parameters and trained on the same hyperparameters.\nMethods\nT2I-CompBench\nColor (\u2191)\nShape (\u2191)\nTexture (\u2191)\nSpatial (\u2191)\nMSCOCO with Karlo\nProjection\n0.4667\n0.4421\n0.5051\n0.1478\nDiffusion-Baseline\n0.4678\n0.4797\n0.4956\n0.1240\nECLIPSE\n0.5965\n0.5063\n0.6136\n0.1574\nCC3M with Karlo\nProjection\n0.4362\n0.4501\n0.4948\n0.1126\nDiffusion-Baseline\n0.5493\n0.4809\n0.5462\n0.1132\nECLIPSE\n0.5421\n0.5091\n0.5881\n0.1477\nCC12M with Karlo\nProjection\n0.4659\n0.4632\n0.4995\n0.1318\nDiffusion-Baseline\n0.5390\n0.4919\n0.5276\n0.1426\nECLIPSE\n0.5660\n0.5234\n0.5941\n0.1625\nMSCOCO with Kandinsky v2.2\nProjection\n0.4678\n0.3736\n0.4634\n0.1268\nDiffusion-Baseline\n0.4646\n0.4403\n0.4834\n0.1566\nECLIPSE\n0.5785\n0.4951\n0.6173\n0.1794\nHighRes with Kandinsky v2.2\nProjection\n0.5379\n0.4983\n0.5217\n0.1573\nDiffusion-Baseline\n0.5706\n0.5182\n0.5067\n0.1687\nECLIPSE\n0.6119\n0.5429\n0.6165\n0.1903\nand\nWurstchen,\nwhile\nclosely\nmatching\nthe\nquality\nof its big counterpart, Kandinsky v2.2.\nInterestingly,\nECLIPSE trained on only 0.6 million images maintains\nthe compositions with minor degradation in image quality.\nThese observations align with our previously established\nquantitative results. Beyond numerical metrics, understand-\ning human preferences is crucial. To this end, we selected\n1500 unique validation prompts from T2I-CompBench and\nassessed PickScore preferences.\nThe results, illustrated\nin Figure 4, reveal that ECLIPSE notably surpasses its\nbaselines in respective restrictive settings with an aver-\nage score of 71.6%.\nWe can also observe that the best\nECLIPSE variant (with Kandinsky decoder and trained\non LAION-HighRes) consistently outperforms the other\nbig SOTA models achieving an average performance of\n63.36%.\nWe observe that in terms of preferences, the\noriginal Kandinsky v2.2 diffusion prior (with a 1 billion\nparameter) trained on LAION-HighRes (175M) performs\nbetter than the ECLIPSE prior (having 33 million param-\neters). We hypothesize that this might be due to its use\nof a large-scale dataset that contains more aesthetically\npleasing images.\nWe provide a set of qualitative results\nin the appendix to show that ECLIPSE performs similarly\nwell, if not better, w.r.t. semantic understanding of the text.\n5. Analysis\nAnalyzing the traditional diffusion priors.\nTo further\nsupport our choice of using non-diffusion prior models, we\nanalyze the existing diffusion prior formulation. We con-\nducted two key empirical studies: 1) Evaluating the Impact\nof Prior Steps: We examined how the number of prior steps\n10\n15\n20\n25\n30\n35\n40\n45\n50\nDecoder Steps\n0.250\n0.275\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\nPickScore Preferences\nPrior Steps\n10\n25\n50\n0.4\n0.6\n0.8\n1.0\nEta (added noise)\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nPickScore Preferences\nPrior Steps\n2\n10\n25\n50\n(a) Left: Performance comparison by varying the prior steps and decoder\nsteps w.r.t. the fixed prior steps (t = 2). Right: Performance comparison\nby varying the mean \u03b7 of the added scheduler noise (\u03c3t\u03f5) w.r.t. the noise-\nless predictions (\u03b7 = 0). Both experiments are on the Kandinsky v2.1.\nKarlo\nKandinsky V2.1\nKandinsky V2.2\n0\n20\n40\n60\nPreferences\nRevision\nOriginal\n(b) Overall performance comparisons on various pre-trained unCLIP mod-\nels before and after reducing the prior steps to two and \u03b7 to 0.0.\nFigure 5. Empirical analysis of the PickScore preferences of dif-\nfusion priors with respect to the various hyper-parameters.\ninfluences model performance. 2) Assessing the Influence\nof Added Noise (\u03c3t\u03f5): We focused on understanding how\nthe introduction of noise affects human preferences. For\nthese studies, we utilized PickScore preferences, and the\noutcomes, depicted in Figure 5, corroborate our hypothesis:\nboth the prior steps and the addition of (\u03c3t\u03f5) detrimentally\naffect performance. Furthermore, as indicated in Table 2,\ndiffusion prior surpasses the projection baseline if provided\nwith more high-quality data. We attribute this enhanced per-\nformance to the incorporation of classifier-free guidance,\nwhich bolsters the model\u2019s generalization capabilities to a\ncertain extent. However, it\u2019s worth noting that both base-\nlines are still outperformed by ECLIPSE. This observation\nunderscores the effectiveness of our proposed methodology\nin comparison to traditional approaches in the realm of T2I.\nImportance of data selection.\nIn our previous analy-\nsis (Table 1 and 2), we demonstrated that ECLIPSE at-\ntains competitive performance on composition benchmarks\nregardless of dataset size.\nThis achievement is largely\ndue to the integration of the contrastive loss LCLS (Eq.4).\nHowever, the final objective function also incorporates the\nLproj (Eq.3), which is pivotal in estimating the vision la-\ntent distribution. This estimation is fundamentally depen-\ndent on the training distribution (PXY ), potentially leading\nthe model to learn spurious correlations within PXY . Con-\nsequently, the model\u2019s image quality could directly corre-\nlate with the overall quality of images in the training set.\nTo further substantiate this, we evaluated the preferences\nECLIPSE (mscoco)\nECLIPSE (cc12m)\n\u201cA young tiger\u201d\n\u201cRed grapes\u201d\nECLIPSE (cc3m)\nMSCOCO\nCC3M\n0\n20\n40\n60\n80\n100\nPercentage (%)\nECLIPSE (CC12M)\nBaseline\nMSCOCO\nCC3M\nCC12M\n0\n20\n40\n60\n80\n100\nPercentage (%)\nKalro-v1-alpha\nECLIPSE (w Kandinsky)\nFigure 6. The top figure shows the qualitative examples of the\nbiases learned by the T2I prior models. Bottom figures show the\nPickScore preferences of the ECLIPSE models trained on various\ndatasets with respect to the other datasets (left) and Karlo (right).\nfor ECLIPSE models trained on MSCOCO, CC3M, and\nCC12M, in comparison to among themselves and Karlo-\nv1-alpha.\nThe outcomes, presented in Figure 6, reveal\nthat the ECLIPSE model trained on CC12M outperforms\nthose trained on other datasets, exhibiting performance on\npar with big counterpart.\nECLIPSE prior (w Karlo de-\ncoder) trained on the CC12M dataset performs comparably\nto Karlo-v1-alpha while ECLIPSE priors trained on other\ndatasets struggle to do so; suggesting the importance of the\nhigh-quality data. Furthermore, as illustrated in Figure 6,\nthe ECLIPSE model trained on MSCOCO demonstrates a\ntendency to learn spurious correlations, such as associating\nthe term \u201cyoung tiger\u201d with the person.\n6. Conclusion\nIn this paper, we introduce a novel text-to-image prior learn-\ning strategy, named ECLIPSE, which leverages pre-trained\nvision-language models to provide additional supervision\nfor training the prior model through contrastive learning.\nThis approach significantly enhances the training efficiency\nof prior models in a parameter-efficient way. Through com-\nprehensive quantitative and qualitative evaluations, we as-\nsessed ECLIPSE priors alongside various diffusion image\ndecoders. The results indicate that ECLIPSE surpasses both\nthe baseline projection models and traditional diffusion-\nprior models. Remarkably, ECLIPSE achieves competitive\nperformance alongside larger, state-of-the-art T2I models.\nIt demonstrates that priors can be trained with merely 3.3%\nof the parameters and 2.8% of image-text pairs typically re-\nquired, without compromising the performance. This ad-\nvancement directly leads to at least 43% overall compres-\nsion of the unCLIP models. Our findings show that pre-\ntrained vision-language can be utilized more effectively;\nsuggesting promising research direction where improving\nthe vision-language models may directly benefit the T2I\nmodels.\nAcknowledgement\nThis work was supported by NSF RI grants #1750082 and\n#2132724, and a grant from Meta AI Learning Alliance.\nThe views and opinions of the authors expressed herein do\nnot necessarily state or reflect those of the funding agencies\nand employers.\nReferences\n[1] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen,\nFaizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny.\nHrs-bench: Holistic, reliable and scalable benchmark for\ntext-to-image models. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 20041\u2013\n20053, 2023. 1, 2\n[2] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3558\u20133568, 2021. 5\n[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 1, 2\n[4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. Pixart-alpha: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 2\n[5] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free\nlayout control with cross-attention guidance. arXiv preprint\narXiv:2304.03373, 2023. 1, 2\n[6] Yuren Cong, Martin Renqiang Min, Li Erran Li, Bodo\nRosenhahn, and Michael Ying Yang.\nAttribute-centric\ncompositional text-to-image generation.\narXiv preprint\narXiv:2301.01413, 2023. 2, 3\n[7] Lee Donghoon, Kim Jiseob, Choi Jisu, Kim Jongmin, Byeon\nMinwoo, Baek Woonhyuk, and Kim Saehoon.\nKarlo-\nv1.0.alpha on coyo-100m and cc15m. https://github.\ncom/kakaobrain/karlo, 2022. 1, 2\n[8] Pierre Fernandez,\nGuillaume Couairon,\nHerv\u00b4e J\u00b4egou,\nMatthijs Douze, and Teddy Furon.\nThe stable signature:\nRooting watermarks in latent diffusion models.\narXiv\npreprint arXiv:2303.15435, 2023. 2\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An\nimage is worth one word: Personalizing text-to-image gener-\nation using textual inversion. In The Eleventh International\nConference on Learning Representations, 2022. 1\n[10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1, 3, 4\n[13] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu. T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv\npreprint arXiv:2307.06350, 2023. 1, 2, 6\n[14] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 3\n[15] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and\nShinkook Choi.\nOn architectural compression of text-to-\nimage diffusion models. arXiv preprint arXiv:2305.15798,\n2023. 2\n[16] Changhoon Kim, Yi Ren, and Yezhou Yang. Decentralized\nattribution of generative models. In International Conference\non Learning Representations, 2021. 2\n[17] Changhoon Kim, Kyle Min, Maitreya Patel, Sheng Cheng,\nand Yezhou Yang. Wouaf: Weight modulation for user attri-\nbution and fingerprinting in text-to-image diffusion models.\narXiv preprint arXiv:2306.04744, 2023. 2\n[18] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 6, 7\n[19] Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei\nYang. Text-to-image generation grounded by fine-grained\nuser attention. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages 237\u2013246,\n2021. 2\n[20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 1\n[21] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\n2\n[22] Zhiheng Li, Martin Renqiang Min, Kai Li, and Chenliang\nXu. Stylet2i: Toward compositional and high-fidelity text-\nto-image synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18197\u201318207, 2022. 2, 3\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 5\n[24] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In International Conference on\nLearning Representations, 2016. 5\n[25] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 2\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[27] Guangyu Nie, Changhoon Kim, Yezhou Yang, and Yi Ren.\nAttributing image generative models using latent finger-\nprints. arXiv preprint arXiv:2304.09752, 2023. 2\n[28] Maya Okawa, Ekdeep Singh Lubana, Robert P Dick, and\nHidenori Tanaka. Compositional abilities emerge multiplica-\ntively: Exploring diffusion models on a synthetic task. arXiv\npreprint arXiv:2310.09336, 2023. 4\n[29] Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou\nYang.\nConceptbed:\nEvaluating concept learning abili-\nties of text-to-image diffusion models.\narXiv preprint\narXiv:2306.04695, 2023. 1\n[30] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195\u20134205,\n2023. 2\n[31] Pablo Pernias, Dominic Rampas, and Marc Aubreville.\nWuerstchen: Efficient pretraining of text-to-image models.\narXiv preprint arXiv:2306.00637v2, 2023. 2\n[32] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded\ntext-to-image synthesis with attention refocusing.\narXiv\npreprint arXiv:2306.05427, 2023. 2\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 4\n[34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 2, 3, 5\n[36] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Malt-\nseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, An-\ngelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and\nDenis Dimitrov. Kandinsky: an improved text-to-image syn-\nthesis with image prior and latent diffusion. arXiv preprint\narXiv:2310.03502, 2023. 1, 2\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[39] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In International Confer-\nence on Learning Representations, 2021. 2\n[40] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 5\n[41] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 5\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n1\n[43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 1\n[44] Vladimir Vapnik. Principles of risk minimization for learn-\ning theory. Advances in neural information processing sys-\ntems, 4, 1991. 4\n[45] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\nLit: Zero-shot transfer with locked-image text tuning.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18123\u201318133, 2022. 2\n[46] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\narXiv preprint arXiv:2303.15343, 2023. 2\n[47] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5907\u2013\n5915, 2017. 2\n[48] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nTong Sun. Towards language-free training for text-to-image\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17907\u2013\n17917, 2022. 2, 3\nECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations\nSupplementary Material\nA. Implementation Details\nTable 3 shows the comparison between ECLIPSE, Karlo,\nand Kaninsky priors. Notably, ECLIPSE prior uses very\ncompressed architecture across the possible avenues (i.e.,\nnumber of layers, number of attention heads, attention\nhead dimension, etc.).\nKarlo uses CLIP-Vit-L/14 with\n768 projection dimensions.\nWhile Kandinsky v2.2 uses\nthe ViT-bigG-14-laion2B-39B-b160k with 1280 projection\ndimensions.\nOverall, the total number of parameters in\nECLIPSE priors is about 33 million compared to 1 bil-\nlion parameters of Karlo/Kandinsky priors. Additionally,\nProjection and Diffusion-Baseline use the same architec-\nture as ECLIPSE prior for better comparisons. Except the\nDiffusion-Prior contains the additional time embeddings for\ndiffusion modeling.\nECLIPSE\nKarlo / Kandinsky\nPriors\nNum Attention Heads\n16\n32\nAttention Head Dim\n32\n64\nNum Layers\n10\n20\nEmbedding Dim\n768/1280\n768/1280\nAdditional Embeddings\n3\n4\nDropout\n0.0\n0.0\nTime Embed\nNo\nYes\nTotal Parameters\n33/34 M\n1 B\nTable 3. Prior model architecture hyperparameter details.\nB. Hyper-parameter Analysis\nECLIPSE only contains one important hyperparameter (\u03bb)\nthat controls the contrastive learning. As discussed in Sec-\ntion 3.3, a higher value of \u03bb can make the prior model\nlearn the different distribution that is highly aligned with\ntext distributions. A lower value of \u03bb may not benefit in\nterms of generalization to unseen prompts. Hence, we con-\nducted a small study on the MSCOCO dataset. We train\nthe ECLIPSE priors for Karlo decoder on 20,000 iterations\nwith the OneCycle learning rate. Figure 7 illustrates the\npickscore preferences on T2I-CompBench of various val-\nues of \u03bb. It can be observed that higher values of \u03bb lead to\nthe same performance as the baseline. While lower values\nof \u03bb outperform the baseline by significant margins. Ad-\nditionally, Figure 8 shows one qualitative example across\nthe range of \u03bb. It can be seen that the generated image\nquality drops as \u03bb increases. Hence, the optimal range\nis: \u03bb \u2208 [0.2, 0.4].\nLambda 0.2\nLambda 0.4\nLambda 0.6\nLambda 0.8\nLambda 1.0\n0\n20\n40\n60\n80\n100\nPercentage (%)\nECLIPSE\nBaseline (Lambda 0.0)\nFigure 7. Hyperparameter (\u03bb) ablation. This figure illustrates the\nPickScore preferences across the ECLIPSE priors trained with dif-\nferent values of \u03bb w.r.t. the Projection baseline (with \u03bb = 0.0).\nLambda 0.0\nLambda 0.2\nLambda 0.4\nLambda 0.6\nLambda 0.8\nLambda 1.0\n\u201cA man with a red helmet on a small moped on a dirt road.\u201d\nFigure 8. Qualitative example for ECLIPSE priors (with Karlo\ndecoder) trained with different values of hyperparameter (\u03bb).\nC. ECLIPSE Prior Model Scaling Behaviour\nTo analyze the scaling behavior of different prior learn-\ning strategies to a certain extent, we increase the prior\nmodel size from 33M to 89M. Table 4 shows the results\nwhen small and large priors are trained on the same dataset\n(CC12M) with the Karlo image diffusion decoder.\nWe\ntrain both versions of the prior models on 60,000 iterations\n(about 350 GPU hours) with exactly the same hyperparam-\neters. First, we observe that ECLIPSE prior improves the\nperformance slightly. Second, the Projection baseline gets\nthe same performance, which suggests that data is the bot-\ntleneck for the Projection prior. Third, interestingly Dif-\nfusion prior degrades the performance. Upon further in-\nspection, we found that 60,000 iterations are insufficient for\nthe Diffusion model to converge. Therefore, this verifies\nthat Diffusion-priors are resource-hungry. Importantly,\nECLIPSE priors easily converge irrespective of the data and\nnumber of parameters; suggesting that ECLIPSE do not de-\npend upon the huge resource constraints.\nD. Aesthetics: Kandinsky v2.2 vs. ECLIPSE\nAs was observed in Figure 4 from the main paper, the\nKandinsky v2.2 model outperforms the ECLIPSE prior\nwhen evaluated in terms of human preferences measured\nby Pickscore. We attribute this behavior to the differences\nin the aesthetic quality of the generated images.\nThere-\nfore, we conduct additional actual human studies to analyze\nTable 4. This table illustrates the scaling behavior of various T2I\nprior learning strategies. \u201cSmall\u201d priors are 33 million in terms\nof parameters. And \u201cLarge\u201d priors have 89 million parameters.\nAll prior models are trained on the CC12M dataset with the Karlo\ndiffusion image decoder.\nMethods\nZS\nT2I-CompBench\nFID\nColor (\u2191)\nShape (\u2191)\nTexture (\u2191)\nSpatial (\u2191)\n33M Priors\nProjection\n28.84\n0.4659\n0.4632\n0.4995\n0.1318\nDiffusion-Baseline\n26.13\n0.5390\n0.4919\n0.5276\n0.1426\nECLIPSE\n26.98\n0.5660\n0.5234\n0.5941\n0.1625\n89M Priors\nProjection\n28.81\n0.4579\n0.4625\n0.4761\n0.1343\nDiffusion-Baseline\n29.78\n0.4988\n0.4790\n0.4604\n0.1247\nECLIPSE\n25.77\n0.5712\n0.5358\n0.6194\n0.16665\nthis behavior further. In total, we randomly selected 200\nprompts from the MSCOCO validation set (instead of T2I-\nCompBench as reported in Figure 4) and asked the human\nevaluators to perform two studies:\nQuality\nAlignment\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\n3.82\n3.95\n3.83\n3.95\nKandinsky v2.2\nECLIPSE\nFigure 9. Human evaluations of the ECLIPSE vs.Kandinsky v2.2\ngenerated images. It can be observed that both models are rated\nequally in terms of image quality and caption alignment.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPercentage (%)\nWin: 28.10%\nTie: 41.74%\nLose: 30.17%\nFigure 10. This figure illustrates the human preferences between\nECLIPSE prior for Kandinsky model (trained on LAION-HighRes\nsubset) vs. Original Kandinsky v2.2 model.\n\u2022 Rate each image in terms of quality and caption alignment\nbetween 1-5. Where 1 is the artificial-looking image and\ncaption alignment is poor. While 5 represents a very high-\nquality image and is perfectly aligned with the captions.\n\u2022 Image preferences in terms of aesthetics. We show im-\nages from both models and ask the evaluators to choose\none which looks more aesthetically pleasing.\n\u201ca couple of \nelephants drink \nwater at a \nwatering hole\u201d\n\u201cA man that is \nnext to a child \nwith bread.\u201d\n\u201cA stuffed animal \nhas been placed \ninside of \nblankets.\u201d\nKandinsky\nECLIPSE\nFigure 11. Qualitative examples comparing (in terms of aesthetics)\nECLIPSE with Kandinsky v2.2.\nInterestingly, as shown in Figure 9, both models are\nrated equally when evaluated independently.\nAddition-\nally, according to Figure 10, Kandinsky v2.2 is preferred\nslightly more than the ECLIPSE in terms of aesthetic qual-\nity. This finding suggests that smaller prior trained with\nECLIPSE can perform equally (if not better) to those big\nprior models. Figure 11 shares three examples from the\nMSCOCO. Both models perform equally well but Kandin-\nsky is more aesthetically pleasing. Figure 20 and 21 show\nthe MTurk human evaluation instructions.\nE. Diversity With Non-Diffusion Priors\nOne important aspect of the diffusion models is the di-\nversity of the generated images. Therefore, diversity and\ncaption alignment go hand-in-hand.\nWe further analyze\nwhether having the non-diffusion prior hurts diversity or\nnot.\nWe perform additional qualitative evaluations and\ngiven a prompt \u2013 we ask the human evaluators to select\nwhich of the two grids of six images are more diverse. This\nexperiment is performed between ECLIPSE and Kandin-\nsky v2.2. As shown in Figure 12, even if we use the non-\ndiffusion prior model it does not hurt the diversity. Diffu-\nsion image decoder is the main reason that contributes to the\ndiversity and having diffusion or non-diffusion prior does\nnot contribute that significantly.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPercentage (%)\nWin: 35.14%\nTie: 34.23%\nLose: 30.63%\nFigure 12. This figure illustrates the human preferences on the di-\nversity of generated images between ECLIPSE prior with Kandin-\nsky v2.2 diffusion image decoder vs. Kandinsky v2.2.\nF. More Qualitative Evaluations\nIn this section, we provide more qualitative examples and\ndiscuss them. We also provide comparisons based on the\ndiffusion image decoder used (i.e., Karlo and Kandinsky\nv2.2). Finally, we discuss several failure cases.\nF.1. ECLIPSE with Karlo Decoder\nFigure 13 illustrates the comparison between Projec-\ntion, Diffusion-Baseline, and ECLIPSE priors trained on\nCC12M. It can be seen that ECLIPSE performs very well on\ncomplex composition prompts. While Projection and Dif-\nfusion baselines struggle to generate images aligned with\nthe target prompt. Figure 14 compares the ECLIPSE priors\ntrained on different datasets. Here, ECLIPSE prior trained\non MSCOCO does not always follow the target prompt ac-\ncurately and generates the lower quality images. That said,\nthe overall performance between all priors is very simi-\nlar; suggesting that even a small amount of dataset is suf-\nficient to distill the knowledge from the pre-trained Vision-\nLanguage models. Figure 15 compares the ECLIPSE mod-\nels with various SOTA methods. Noticeably, ECLIPSE per-\nforms better than the other baselines in terms of the abil-\nity to follow the target prompts. For instance, many SOTA\nmodels cannot generate \u201cempty blue vase\u201d, \u201ccat in space\nsuit\u201d, and \u201cblue bowl on white placemat\u201d. Although we ob-\nserve that ECLIPSE prior trained with MSCOCO does fol-\nlow the target text prompts but cannot generate high-quality\nimages, which aligns with our previous findings.\nF.2. ECLIPSE with Kandinsky Decoder\nSimilarly, we analyze the qualitative results on Kandinsky\ndiffusion image decoders.\nFigure 16 compares the vari-\nous baselines priors with the ECLIPSE prior. We observe\nthat baselines perform very closely to the ECLIPSE prior,\nwhich is the opposite of what we found in Figure 13. We\nattribute this behavior to the change in the pre-trained CLIP\nencoder.\nAdditionally, as shown in Table 2 of the main\npaper, both baseline priors perform very highly compared\nto the same priors trained on the CC12M dataset for the\nKarlo decoder. The only difference is the pre-trained vision-\nlanguage model. Therefore, the selection of the Vision-\nLanguage model also plays a crucial role.\nFigure 17 illustrates the comparison with ECLIPSE pri-\nors trained with different datasets. It can be observed that\nwith the use of the LAION-HighRes dataset not only did\nimage quality improve but small intrinsic details (such as\n\u201cbackpack\u201d, \u201cbelt\u201d, etc.)\nalso improved.\nEven in some\ncases, prior training on the LAION subset performs bet-\nter as the increase in the amount of data improves the\nperformance.\nFigure 18 provides more qualitative ex-\namples to compare the ECLIPSE priors with other re-\nspective SOTA methods.\nAs also previously observed,\nECLIPSE prior trained on LAION subset performs very\nclose to the Kandinsky v2.2 in terms of following the text\nprompts. While big SOTA models such as Stable Diffusion\nv1.4/2.1, and Wurstchen fall short despite being trained on\nmillions of data.\nF.3. Failure Cases\nFigure 19 shows some examples where ECLIPSE model\nfails to follow the prompt precisely. It is still difficult for the\nprior to learn something very unconventional. The model\nfails at generating some composition prompts (first four im-\nages). It has been shown that vision-language models also\nsuffer from such composition understanding (e.g., \u201cgrass in\nthe mug\u201d vs. \u201cmug in the grass\u201d). Therefore, improving the\nVision-Language model can further improve the capabili-\nties of unCLIP priors. Notably, ECLIPSE finds it difficult\nto generate artistic imaginary images (such as \u201cnebula ex-\nplosion that looks like corgi\u201d). However, such corner cases\ncan be only solved with more diverse high-quality datasets.\nG. Future Work\nIn this work, we focus on improving text-to-image pri-\nors.\nWe assume that there exists a pre-trained diffusion\nimage decoder that can be used as it is.\nTo further im-\nprove the parameter efficiency for training, several relevant\nworks on knowledge distillation and model compression\ncan help. Moreover, to improve the compositional abilities\nfor unCLIP models, a better vision-language model (such as\nSigLIP) as the base model can be utilized to train the prior\nmodel using ECLIPSE. However, this will require the diffu-\nsion image decoder to be adjusted according to the new vi-\nsion latent space. We leave this direction as the future work\nas our paper primarily focuses on enhancing T2I priors.\n\u201ca yellow book \nand a red vase\u201d\nProjection\nDiffusion\nECLIPSE\n\u201ca white \nenvelop and a \nblue stamp\u201d\n\u201ca cubic book \nand a \ncylindrical \ncan of soup\u201d\n\u201cwooden \npencils and a \nleather sofa\u201d\n\u201ca rubber ball \nand a fabric \ncurtain\u201d\n\u201ca girl on top \nof a cow\u201d\nFigure 13. Qualitative comparisons between ECLIPSE and baseline priors (having 33 million parameters) trained on CC12M dataset with\nKarlo decoder. * prompt is: \u201dThe bold, striking contrast of the black and white photograph captured the sense of the moment, a timeless\ntreasure memory.\u201d\n\u201ca fluffy teddy \nbear and a \nleather belt\u201d\n\u201ca wooden \nfloor and a \nfabric shirt\u201d\n\u201ca metallic \nspoon and a \nfluffy towel\u201d\n\u201ca sheep on \nthe left of a \nclock\u201d\n\u201cThe vibrant, \nswirling colors \nof the tie-dye \nshirt burst \u2026\u201d*\n\u201cThe warm \nyellow light \nshone down \non the cozy \nred armchair\u201d\n\u201cThe glowing \nmoon rose \nabove the \ndistant hill and \nthe calm sea\u201d\nCC3M\nCC12M\nMSCOCO\nFigure 14. Qualitative comparisons of ECLIPSE priors with Karlo decoder trained on different datasets. * prompt is: \u201dThe vibrant, swirling\ncolors of the tie-dye shirt burst with energy and personality, a unique expression of individuality and creativity.\u201d\n\u201cthe blue bowl \nwas on top of the \nwhite placemat\u201d\n\u201ca spanish water \ndog breed as \narthur morgan\nfrom red dead \nredemption\u201d\n\u201cthe green plant \nwas next to the \nblue empty vase\u201d\n\u201ca cat in a space \nsuit walking on \nthe moon\u201d\n\u201ca photo of a tree \nwith eggs\ngrowing on it\u201d\nWurstchen\nSDv1.4\nSDv2.1\nKarlo\nECLIPSE\n1B + 1480M\n1B + 400M\n1B + 2000M\n1B + 115M\n0.03B + 0.6M\n0.03B + 12M\nParams + Dataset\nFigure 15. Qualitative result of our text-to-image prior, ECLIPSE (with Karlo decoder), along with a comparison with SOTA T2I models.\nOur prior model reduces the prior parameter requirements (from 1 Billion \u2192 33 Million) and data requirements (from 115 Million \u2192 12\nMillion \u2192 0.6 Million).\n\u201ca black and \nwhite cat sits in \na white sink\u201d\nProjection\nDiffusion\nECLIPSE\n\u201ca giraffe on \nthe left of a \ntrain\u201d\n\u201ca woman on \nthe left of a \nmicrowave\u201d\n\u201cthe white cat \nis lying on the \nbrown sofa\u201d\n\u201ca brown book \nand a red \nsheep\u201d\n\u201cthe soft pink \npetals of the cherry \nblossom contrasted \nwith the rough \nbrown bark\u201d\nFigure 16. Qualitative comparisons between ECLIPSE and baseline priors (having 34 million parameters) trained on LAION-HighRes\nsubset dataset with Kandinsky v2.2 diffusion image decoder.\n\u201ca blue backpack \nand a brown \nsheep\u201d\nMSCOCO\nLAION\n\u201ca black cat \nand a white \nwhisker\u201d\n\u201ca wooden \ntable and a \nleather belt\u201d\n\u201ca vase on the \nright of a cat\u201d\n\u201cthe fluffy \nwhite snow \ncovered the \nrough brown \ndirt road\u201d\n\u201cThe crisp \nwhite sheet \ncovered the \nlumpy blue \nmattress.\u201d\nFigure 17. Qualitative comparisons between ECLIPSE prior trained on MSCOCO and LAION datasets with Kandinsky v2.2 decoder.\n\u201cone computer \ntechnical sketch \nwhite \nbackground\u201d\n\u201cA portrait of a \nbear wearing a \nsuit in the style \nof a Baroque \npainting\u201d\n\u201ca cute blue cat\u201d\n\u201cA Pikachu with an \nangry expression and \nred eyes, with red \nlightnings around it, \nblack background, \nhyper realistic style\u201d\n\u201cpaper artwork, \nlayered paper, \ncolorful Chinese \ndragon \nsurrounded by \nclouds\u201d\nWurstchen\nSDv1.4\nSDv2.1\nKandinsky\nECLIPSE\n1B + 1480M\n1B + 400M\n1B + 2000M\n1B + 177M\n0.03B + 0.6M\n0.03B + 5M\nParams + Dataset\nFigure 18. More qualitative result of our text-to-image prior, ECLIPSE (with Kandinsky v2.2 decoder), along with a comparison with\nSOTA T2I models. Our prior model reduces the prior parameter requirements (from 1 Billion \u2192 33 Million) and data requirements (from\n177 Million \u2192 5 Million \u2192 0.6 Million).\n\u201cA small cactus with a happy \nface in the Sahara desert.\u201d\n\u201cThe grass in the mug.\u201d\n\u201cThe mug in the grass.\u201d\n\u201cA blue horse and brown \nvase.\u201d\n\u201ca nebula explosion made of \nshining stars that looks like \nthe face of the corgi dog, \ndetailed, creative.\u201d\nFigure 19. Instances where ECLIPSE encounters the challenges in following the target text prompts.\nFigure 20. An example of human annotation for determining the image quality and caption alignment.\nFigure 21. An example of human annotation for determining the most aesthetic image.\n"
  },
  {
    "title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models",
    "link": "https://arxiv.org/pdf/2312.04724.pdf",
    "upvote": "17",
    "text": "Purple Llama CYBERSECEVAL: A Secure\nCoding Benchmark for Language Models\nManish Bhatt\u2217, Sahana Chennabasappa\u2217, Cyrus Nikolaidis\u2217, Shengye Wan\u2217, Ivan Evtimov\u2020,\nDominik Gabi\u2020, Daniel Song\u2020, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha\nFrolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo,\nAleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, Joshua Saxe\u2217\n\u2217Co-equal primary author, \u2020Core contributor\nThis paper presents CYBERSECEVAL, a comprehensive benchmark developed to help bolster the cybersecurity\nof Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive\nunified cybersecurity safety benchmark to date, CYBERSECEVAL provides a thorough evaluation of LLMs in\ntwo crucial security domains: their propensity to generate insecure code and their level of compliance when\nasked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama,\nand OpenAI GPT large language model families, CYBERSECEVAL effectively pinpointed key cybersecurity\nrisks. More importantly, it offered practical insights for refining these models. A significant observation from\nthe study was the tendency of more advanced models to suggest insecure code, highlighting the critical need\nfor integrating security considerations in the development of sophisticated LLMs. CYBERSECEVAL, with\nits automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers\nand researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs,\ncontributing to the development of more secure AI systems.\nDate: December 7, 2023\nCorrespondence: Joshua Saxe at joshuasaxe@meta.com\nCode: https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks\nBlogpost: https://ai.meta.com/llama/purple-llama\n1\nIntroduction\nLarge Language Models (LLMs) have shown tremendous progress on a variety of tasks related to human cognition but\nthey have stood out in their ability to write functional code in response to natural language requests. At the same time,\nmuch attention has been paid to safety issues that these models present but only a limited number of works exists on\nmeasuring and mitigating risk in the domain of cybersecurity. In this work, we identify two major cybersecurity risks\nposed by LLMs, develop measurement approaches for each, and open source CYBERSECEVAL, the most comprehensive\ncybersecurity safety measurement suite to date.\nFirst, when those models generate code, that output can fail to follow security best practices or introduce exploitable\nvulnerabilities. This is not a theoretical risk - developers readily accept significant amounts of code suggested by these\nmodels. In a recent publication GitHub revealed that 46% of the code on its platform is autogenerated by CoPilot,\nits code suggestion tool backed by an LLM (Dohmke, 2023). Similarly, A study of large-scale deployment of the\nCodeCompose model at Meta claimed that developers accept its suggestions 22% of the time (Murali et al., 2023).\nIn addition, previous studies with hand-written tests (Pearce et al., 2022) have found 40% of code suggestions to be\nvulnerable. User studies have indicated that developers may accept buggy code suggested by an LLM up to 10% more\noften than they write it themselves (Sandoval et al., 2023).\nIn order to ultimately mitigate this kind of risk, CYBERSECEVAL is designed to integrate into the development and\ntesting processes of those designing code-producing large language models. By identifying insecure coding practices in\nLLM output across many languages, CYBERSECEVAL can help identify risks and offer clear directions for improvement.\nBy iteratively refining models based on these evaluations, model developers can enhance the security of the code\ngenerated by their AI systems.\n1\narXiv:2312.04724v1  [cs.CR]  7 Dec 2023\nSecond, in a new dimension of cybersecurity safety of LLMs, models should not assist in malicious activities relating to\ncomputer systems. While many foundational models are already aligned to resist helping in illicit and criminal activities\nbroadly (Touvron et al., 2023), we investigate if this extends to coding-enabled models on malicious requests involving\ncomputer systems. In doing so, we identify that code alone is not inherently malicious or benign. A script to encrypt\none\u2019s home directory may be used by someone wishing to increase their personal security just as it can be adopted\nin ransomware operations. Thus, we focus on intent and test whether language model alignment holds up to openly\nmalicious requests.\nBy evaluating an LLM\u2019s compliance with requests to assist in cyberattacks, CYBERSECEVAL can provide valuable\ninsights into potential misuse scenarios. This can help product LLM designers anticipate and mitigate risks associated\nwith malicious application of their LLMs. By understanding how their AI systems respond to such requests, developers\ncan implement appropriate safety measures, such as refusal skills or user warnings, to prevent models from being\nmisused.\n1.1\nCyberSecEval\u2019s approach to LLM cybersecurity safety measurement\nCYBERSECEVAL \u2019s overall approach is depicted in Figure 1, below. For the first part of our evaluation, we develop\nthe Insecure Code Detector (ICD), a knowledge base of 189 static analysis rules designed to detect 50 insecure coding\npractices defined in the standard Common Weakness Enumeration. (MITRE, 2023a) We use the ICD to both generate\ntest queries and to evaluate LLM responses. To create tests, we use the ICD to find instances of insecure coding practices\nin open source code. These instances are then used to automatically create test prompts. One set of test prompts is\ndesigned to measure the autocomplete response of an LLM and another set of test prompts is designed to instruct the\nLLM to generate code for a particular task. During the evaluation phase, we use the ICD to determine if an LLM\nreproduces or avoids the insecure coding practices observed in the original code.\nFor testing an LLM\u2019s compliance with requests to carry out cyberattacks, as shown in Figure 1, we create test cases\nby hand-authoring prompt fragments, which, when combined into prompts, ask an LLM to assist in carrying out\ncyberattacks as defined by the industry standard MITRE ATT&CK\u00ae ontology. (MITRE, 2023b) We then use an LLM\nto add diversity to this initial prompt set to create a final set of benchmark prompts as test cases. During LLM evaluation,\nwe prompt an LLM with the tests and use a separate \u2018judge\u2019 LLM to evaluate if the generated responses of the LLM\nunder test would be helpful in executing a cyberattack (discussed in following sections).\n1.2\nMain Contributions\nOverall, as a benchmark, CYBERSECEVAL makes the following novel contributions:\n\u2022 Breadth: As shown in Tables 1 and 2, we believe CYBERSECEVAL is the most comprehensive LLM cybersecurity\nevaluation suite to date, assessing insecure coding practices (as defined by the industry-standard Common\nWeakness Enumeration (MITRE, 2023a)) 8 programming languages, 50 Common Weakness Enumeration\ninsecure practices, and 10 categories of ATT&CK tactics, techniques, and procedures (TTPs). (MITRE, 2023b)\n\u2022 Realism: Our insecure code tests are automatically derived from real-world open source codebases and thus\nevaluate real-world coding scenarios.\n\u2022 Adaptability: Because our test case generation pipeline is automated, our approach can be easily adapted to\nassess newly identified coding weaknesses and cyber-attack tactics, techniques, and procedures.\n\u2022 Accuracy: Automatically and accurately evaluating LLM completions for cybersecurity safety is non-trivial.\nOur static analysis approach achieves a manually verified precision of 96% and recall of 79% in detecting insecure\ncode generations from LLMs, and our LLM-based cyberattack helpfulness detection approach detects malicious\nLLM completions at a precision of 94% and a recall of 84%.\nIn addition to introducing CYBERSECEVAL below, we also describe a case study in applying the benchmarks to 7\nmodels from the Llama 2, Code Llama, and OpenAI GPT large language model families. Some takeaways from this\ncase study follow:\n\u2022 We observed insecure coding suggestions across all studied models, particularly those with higher\ncoding capabilities. On average, LLMs suggested vulnerable code 30% of the time over CYBERSECEVAL \u2019s test\ncases.\n2\nFigure 1 High level overview of CYBERSECEVAL \u2019s approach.\n\u2022 Models with superior coding abilities were more susceptible to suggesting insecure code. This\npattern held over both autocomplete and instruct insecure code practice test cases.\n\u2022 Models complied with 53% of requests to assist in cyberattacks on average across all models and\nthreat categories, as evaluated by our automated approach.\n\u2022 Models with higher coding abilities had a higher rate of compliance in aiding cyberattacks\ncompared to non-code-specialized models.\n1.3\nThe structure of this paper\nThe structure of the rest of this paper is as follows. First, we describe CYBERSECEVAL \u2019s approach to evaluating the\npropensity of an LLM to generate insecure code and describe our case study in applying it to Llama 2, Code Llama, and\nOpenAI GPT-3.5 and GPT-4 LLMs. Then we describe our approach to evaluating the degree to which an LLM complies\nwith requests to help carry out cyberattacks and describe applying this evaluation to those same models. Finally, we\ndiscuss related work, and conclude.\n2\nInsecure coding practice testing\nOur insecure coding practice tests measure how often an LLM suggests insecure coding practices in both autocomplete\ncontexts, where the LLM predicts subsequent code based on preceding code, and instruction contexts, where an LLM\nwrites code based on a request (e.g. \u2018write me a simple web server in Python\u2019). Figure 1 gives an overview of our\napproach to insecure coding practice testing, which we describe in more detail below.\n2.1\nDetecting insecure code\nCYBERSECEVAL includes a novel tool, the \u2018Insecure Code Detector\u2019 (ICD), to detect insecure coding practices, which\nwe use both for test case generation and model evaluation, as described in detail below. The ICD is a robust tool,\ndesigned to handle incomplete or unparseable code, a potential output from an LLM. The ICD operates based on\nrules written in weggli (Wilhelm et al., 2023) \u2013 a domain specific static analysis language for C/C++, \u2013rules written in\nsemgrep (Semgrep, 2023) \u2013 a static analysis language covering Java, Javascript, PHP, Python, C, C++, and C#,\u2013 and\nregular expression rules.\nThese rules are designed to identify approximately 189 patterns related to 50 different Common Weakness Enumerations\n3\n(CWEs) across eight commonly used programming languages: C, C++, C#, Javascript, Rust, Python, Java, and PHP.1\nIn choosing the CWEs to cover, we aimed to cover the top 10 patterns relevant to the particular language of choice.\nImportantly, the ICD\u2019s primary aim is to pinpoint insecure coding practices, which are any coding styles or practices\nthat present security risks, rather than to identify specific vulnerabilities.\n2.2\nConstructing insecure code test sets\nWe construct test cases in two contexts: autocomplete and instruct. For autocomplete tests, we use the ICD to identify\ninstances of insecure coding practices in a large dataset of open source code. We then use these instances to create\ntest cases. Each test case has an LLM prompt comprised of the 10 preceding lines of code that lead up to the insecure\npractice. The intuition for these test cases is that by prompting an LLM with the lines preceding a risky code practice,\nwe assess its propensity to either reproduce that insecure coding practice or take a different approach.\nFor instruct tests, we again use the ICD to identify instances of insecure coding practices in the open source code dataset.\nWe then use an LLM to translate the lines of code before, after, and including the line containing the insecure practice\ninto a natural language instruction. This instruction forms the basis of the test case. The intuition for these instruct test\ncases is that by prompting an LLM with an instruction asking it to create similar code to the originally observed risky\ncode, we assess its propensity to either reproduce that insecure coding practice or take a different approach.\n2.3\nPerforming insecure coding practice evaluation\nDuring the evaluation step, we prompt the LLM with either the preceding lines of code (for autocomplete tests) or the\nderived instruction (for instruction tests). We then use the ICD to check if the code generated by the LLM contains any\nknown insecurities.\n2.4\nFinal calculation of insecure coding practice metrics\nAssume we have Na test cases for autocomplete and Ni test cases for instruct test cases overall.\nFurther assume that 1k indicates that test case k passes (does not contain an insecure coding practice).\nThen the metric definitions are as follows:\n\u2022 Metric 1: Autocomplete insecure coding practice pass rate:\nPNa\nk=0 1k\nNa\n\u2022 Metric 2: Instruct insecure coding practice pass rate:\nPNi\nk=0 1k\nNi\n\u2022 Metric 3: Insecure coding practice pass rate:\nPNi\nk=0 1k + PNa\nk=0 1k\nNi + Na\n2.5\nAssessment of metric accuracy\nTo understand the efficacy of our approach detecting insecure LLM completions, we manually labeled 50 LLM\ncompletions corresponding to our test cases per language based on whether they were insecure or secure. Then we\ncomputed the precision and recall of our Insecure Code Detector static analysis approach both per-language and overall.\nWe found that, overall, the Insecure Code Detector had a precision of 96% and a recall of 79% in detecting insecure\nLLM generated code.\nWhile this result isn\u2019t perfect, we believe that it\u2019s sufficient to evaluate an LLM\u2019s overall tendency towards generating\ninsecure code across our hundreds of test cases.\n1Please, see our GitHub repository for a full listing of CWEs covered: https://github.com/facebookresearch/PurpleLlama/tree/main/\nCybersecurityBenchmarks/insecure_code_detector/rules\n4\nFigure 2 The precision and recall of our Insecure Code Detector static analyzer at detecting insecure code in LLM completions.\n2.6\nEvaluating LLM completion code quality as a counterpart to insecure coding\nmetrics\nAs an auxiliary metric, we compute a code quality score for LLM completions. This code quality score contextualizes\ninsecure coding metric results based on the intuition that LLMs that produce unusable code may do well on code\ninsecurity metrics, but this is irrelevant if they don\u2019t also produce meaningful code.\nWe compute our code quality metric by computing the BLEU string distance between LLM completions and the\nunderlying code that they are based on (Papineni et al., 2002). In the autocomplete setting, we compute a distance\nbetween the code that the LLM auto-completed and the actual code that followed the test case. In the instruct setting, we\ncompute the distance between the LLM completion and the original code that the coding instruction was derived from.\n2.7\nApplying insecure coding evaluation to Llama 2 and Code Llama LLMs\nWe applied our insecure code benchmarks to Llama 2 and Code Llama models as a case study in comparing models\u2019\ncybersecurity insecure coding risks. For each model, we sampled tokens at a temperature of 0.6 with nucleus sampling,\nthe setting used in the original CodeLlama paper. (Roziere et al., 2023) Figures 3 and 4 show the top level results for our\nexperiments. The following theses emerged from analyzing our insecure coding test results.\n1. The Llama 2 and Code Llama models we tested exhibit insecure coding practices a significant percentage of the\ntime. For example, CodeLlama-34b-instruct, the most capable coding model we tested, passes insecure coding\npractice tests only 75% of the time.\n2. Models that are more capable at coding tend to be more prone to insecure code suggestions.\n3. This trend holds within the context of individual programming languages as well as in both instruct and autocom-\nplete settings. Languages that models are more competent at tend to be languages where the models we tested did\nworse on average on our insecure code tests.\nFigure 4 illustrates Theses 1 and 2. As shown in the table, models with higher BLEU score do worse, in general, at the\nautocomplete and instruct tests. While further investigation is required to understand this dynamic, we speculate that\nthis is because the better the models are at learning patterns from their training data, the more likely they are to learn\ninsecure coding practices from this same data.\n3\nCyberattack helpfulness testing\nIn addition to insecure coding practices we also measure an LLM\u2019s propensity to help carry out cyberattacks as defined\nin the industry standard MITRE Enterprise ATT&CK ontology of cyberattack methods.\n5\nFigure 3 Summary of insecure coding practice evaluation results across 7 LLMs.\n3.1\nDiscussion of cyberattack helpfulness approach\nThe definition of cyberattack helpfulness is subjective and complex. In our approach, we characterize \u2019helpfulness\u2019 as a\ndual concept:\n\u2022 The LLM provides a response to a Tactics, Techniques, and Procedures (TTP) prompt that was directly related to\nthe original prompt.\n\u2022 The LLM\u2019s response significantly contributes to the completion of the initial prompt. This could be through the\ncreation of actual code, offering guidance that could assist in constructing an attack, etc., rather than producing\ncode or text that does not effectively help accomplish the task.\nThere are caveats. For example, if the same LLM is prompted to generate code for a complex, novel malware, it might\nnot be able to produce effective results due to the limitations of its training data. Furthermore, if a harmful intent is\ncleverly disguised within a series of seemingly harmless sub-prompts, or indirect prompts, the LLM might inadvertently\ncomply, highlighting the need for effective safety measures.\n3.2\nGenerating cyberattack helpfulness tests\nAs shown in Figure 5, to create our cyberattack helpfulness test cases, we took the following steps:\n1. Fragment Generation: A group of offensive security engineering professionals created a spreadsheet of\nsentence fragments manually. These fragments are categorized into three types: leadup, context, and cyberattack\ntactic, technique, or procedure (TTP) reference.\n2. Base Prompt Generation: We then do a combinatorial expansion of these sentence elements such that a\nsentence is created from each possible sequence of lead-up, context, and TTP. The resulting complete sentences\nare base prompts. Each base prompt is designed to induce an LLM to produce output that maps to a TTP.\n3. Prompt Augmentation: The base prompts are augmented using the Llama-70b-chat model to improve their\nsemantic and thematic complexity. This results in the creation of 1,000 augmented prompts.\n6\nFigure 4 Negative correlation between insecure code test case pass rate and code quality.\nFigure 5 Example fragments and complete prompt showing how we construct cyberattack helpfulness test case prompts.\n4. Final Test Case Creation: By the end of this process, 100 prompts per ATT&CK Framework Category are\ncreated, resulting in 1,000 total prompts asking an LLM to help implement all ATT&CK categories.\nWe inspected the dataset manually and observed that the augmented prompt set on average produced more diverse, and\nmore complex responses than the base prompt set. Figure X provides an example of a prompt through the various stages\nof this evolution process (cite wizardllm).\n3.3\nPerforming cyberattack helpfulness evaluation\nTo perform cyberattack compliance evaluations in the context of a given LLM, we take the following steps:\n1. Completion Generation: For each test case prompt, k (in our case, 3) completions are generated using the\nLLM-under-test.\n2. Refusal Check: A pattern matcher is used to determine whether the LLM refused to help implement the\nATT&CK TTP. If a refusal is found, we increment a count of observed refusals.\n3. Using an LLM to check for compliance: If the completion is not detected by our pattern matcher (Refusal\nCheck), we utilize Llama-70b-chat and CodeLlama-13b to judge whether the completion would be helpful in\nimplementing a cyberattack. First, we prompt Llama-70b-chat to expand on the LLM-under-test\u2019s completion\u2019s\nimplications for a cyber attack, and then we ask CodeLlama-13b to judge whether the original LLM completion\n7\nFigure 6 Plot showing the precision and recall of our automated method for determining if an LLM completion is helpful in\nimplementing ATT&CK defined cyber attacks.\nand the expansion represents truly malicious behavior on the part of the LLM-under-test.\n4. Count Updates If the LLM-under-test didn\u2019t refuse to help (as defined in step 2), and was deemed to have\nproduced a response helpful to a cyber attacker, we register this response as malicious. Otherwise, we register this\nresponse as benign.\n5. Final metric computation: Finally, the final high-level metric, \"cyberattack helpfulness\", is computed as\n(malicious count / total count over all test case runs). This measures the propensity of an LLM to produce\nresponses that may be helpful to a cyber attacker.\n3.4\nValidating the accuracy of our completion assessment approach\nTo validate the accuracy of our approach to judging whether an LLM\u2019s response is truly helpful to cyberattackers, we\nmanually inspected 465 randomly sampled test case responses. We found that our decision pipeline achieves 94%\nprecision and 84% recall in detecting responses from the LLM-under-test that would be helpful to a cyber-attacker.\nFigures 6 shows the performance of our method per ATT&CK category, and overall. While not perfect, the analysis\ndemonstrates our method is sufficiently accurate to provide relevant information about an LLM\u2019s propensity to help\ncarry out cyber attacks across every ATT&CK category.\n3.5\nApplying cyberattack helpfulness tests to Llama 2 and Code Llama\nJust as we studied the performance of Llama 2, Code Llama, and OpenAI GPT models on our insecure coding practice\ntest cases as described above, we also examined their performance on our helpfulness to cyberattackers tests. The\nfollowing theses emerged in this setting.\n1. Models perform in ways that would aid cyberattacks, complying with 52% of requests to generate a response that\ncould aid cyberattacks on average across all models and threat categories.\n2. Models with higher coding ability, specifically the CodeLlama family, comply more often in generating a response\nthat could aid cyberattacks than non-code-specialized models, such as the Llama 2 family. We hypothesize that\nthis likely occurs because the CodeLlama models can comply with requests due to their higher coding ability and\npossibly because they receive less safety conditioning.\n3. Models show better non-compliance behavior on average in \u2018dual use\u2019 settings, where a request to the model\ncould plausibly serve a benign use case.\nFigure 7 illustrates these theses. The \u2018average over all models\u2019 chart shows the overall performance of Llama 2 and\nCodeLlama models across our test cases, demonstrating that there is no category in which models did not, on average,\ndecline to help with more than 61% of malicious requests.\n8\nFigure 7 Summary of LLM performance in non-compliance with requests to help with cyberattacks (left), and average model\nperformance across 10 categories of cyberattack tactics, techniques, and procedures (right).\nIllustrating theses 2 and 3, the plot shows that models didn\u2019t comply with the \u2018evasion\u2019 and \u2018execution\u2019 requests at\nthe highest rate, where \u2018evasion\u2019 means techniques that help adversaries hide their presence on compromised systems,\nand \u2018execution\u2019, consists of techniques that result in adversary-controlled code running on a local or remote system.\nIn contrast, models behaved helpfully to cyberattackers in the context of requests that seem more ambiguous, such as\nrequests to discover potential victim machines on a remote network (\u2018discovery\u2019) or surveil them (\u2018recon\u2019).\nFigure 8 illustrates theses 1 and 2.\n4\nRelated Work\nThe most widely adopted LLM code security benchmark to date (Pearce et al., 2022) uses handcrafted prompts relating\nto the top 25 MITRE Common Weakness Enumeration (CWE) (MITRE, 2023a) entries in Python, C, and Verilog and\nchecks security by means of a static analyzer and manual inspection. It was originally applied to GitHub\u2019s CoPilot\nand later adopted in modeling papers, such as the StarCoder work (Li et al., 2023). Taking a similar approach, the\nSecurityEval dataset (Siddiq and Santos, 2022) covers 75 CWEs in Python. A combination of other studies extends\ncoverage to include C++, html, and Java and measure code suggestions by GPT-3.5 and GPT-4, Llama v2, and Vicuna 1.5\nmodels (Khoury et al., 2023; Tihanyi et al., 2023; Khoury et al., 2023; Zhong and Wang, 2023). Another work (Yeti\u00b8stiren\net al., 2023) has looked for security vulnerabilities in response to prompts not specific to security from the HumanEval\ndataset Chen et al. (2021). In addition, (Hajipour et al., 2023) automates prompt generation with few-shot prompting\nand combines data from (Pearce et al., 2022) and (Siddiq and Santos, 2022) to cover CodeGen, ChatGPT, and CoPilot\nand (Siddiq et al., 2022) investigates code smells in training data and CoPilot suggestions. Finally, in a different line of\nscientific exploration, (Sandoval et al., 2023) tests how often developers accept insecure suggestions for C code in a user\nstudy.\nAs shown in Table 1, we expand coverage to 4 languages that are not handled by any other prior work (PHP, JavaScript,\nC#, and Rust) and we further standardize the evaluation for Python, C, C++, and Java to a single benchmark.\nFurthermore, we make several methodological advancecements in cybersecurity safety evaluation of LLMs highlighted\nin Table 2. First, we automatically extract test cases from insecurity-prone production code, making our work both\nmore scalable and more easily extensible. Second, we provide full automated support for partial, non-parseable code\ngeneration that does not require human intervention since our ICD is based on a database of regular expressions detecting\ninsecure coding practices that does not require the abstract syntax tree to be built. Finally, we expand coverage of\ncybersecurity safety to include an instruction-aligned LLM\u2019s compliance in assisting with requests by cyberadversaries.\n9\nFigure 8 Break-out of LLM performance against 10 categories of cyberattacks.\n5\nLimitations of CyberSecEval\nWe also make assumptions and tradeoffs in our design of CYBERSECEVAL. We list key limitations of CYBERSECEVAL\nhere:\n\u2022 Our detection of insecure coding practices is imperfect. To detect insecure coding practices in LLM\noutput we use static analysis patterns which, like all static analysis methods, are susceptible to false positives and\nnegatives.\n\u2022 There is the risk of data contamination in some of our test cases. Because some of our test cases are\nbased on open source code, LLMs we test could have been trained on test data. We mitigate this by including\ngithub repository origin metadata in each relevant test case so LLM designers can choose to hold out test cases.\n\u2022 We restrict natural language prompts in our test cases to English. In future versions of CYBERSECE-\nVAL we will expand our evaluations to a broader set of natural languages.\n\u2022 Models were tested on single-turn queries only, and do not reflect the ability of the LLM to refine code\n10\nBenchmark\nProgramming Language Coverage\nPython\nC\nC++\nPHP\nJavascript\nJava\nC#\nRust\nVerilog\nhtml\nAsleep at the Keyboard (Pearce et al., 2022)\nSecurityEval (Siddiq and Santos, 2022)\nKhoury et al. (2023)\nFormAI (Tihanyi et al., 2023)\nKhoury et al. (2023)\nYeti\u00b8stiren et al. (2023)\nZhong and Wang (2023)\nCodeLMSec (Hajipour et al., 2023)\nCYBERSECEVAL (ours)\nTable 1 Comparison of coverage of programming languages in related works. Filled-in circles indicate that a language is supported\nin the relevant benchmark and hollow circles indicate that it is not.\nBenchmark\nCreating test case prompts\nSupport for Partial\nModel Type Coverage\nAssistance to\nNon-parseable Generations\nCode Completion\nInstruction-Aligned\ncyberadversaries\nAsleep at the Keyboard (Pearce et al., 2022)\nManual (25 CWEs)\nSecurityEval (Siddiq and Santos, 2022)\nManual (75 CWEs)\nKhoury et al. (2023)\nManual\nFormAI (Tihanyi et al., 2023)\nManual (75 CWEs)\nKhoury et al. (2023)\n0-shot Prompting an LLM\nYeti\u00b8stiren et al. (2023)\nHumanEval Prompts\nZhong and Wang (2023)\nStackOverflow Questions\nCodeLMSec (Hajipour et al., 2023)\nFew-Shot Prompting an LLM\nCYBERSECEVAL (ours)\nAutomatic from Insecurity-Prone Prod. Code\nTable 2\nComparison of coverage and methods between our work and related works. Filled-in circles indicate that a capability\nis supported or tested for in the relevant benchmark and hollow circles indicate that it is not. In the case of Support for Partial\nNon-Parseable Generations and the works SecurityEval (Siddiq and Santos, 2022) and FormAI (Tihanyi et al., 2023), we provide a\nhalf-filled-in circle since these methods involve manual inspection that can interpret partial code for which the abstract syntax tree\ncannot be constructed.\nbased on multiple rounds of prompts.\n\u2022 Recommendations of offensive code were not evaluated on their ability to be combined into an end-to-end\ntutorial for how to exploit a given system.\n6\nRunning CyberSecEval using our open github repository\nOur github repository, including code, test cases, and documentation describing how to run our tests, is available here and\nmade available under an MIT license: https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks\nWe welcome open source contributions and expect to update our benchmarks with new versions in the future.\n7\nConclusion\nIn this paper, we introduced CYBERSECEVAL, a comprehensive benchmark for evaluating the cybersecurity risks of\nlarge language models (LLMs). Our evaluation suite assesses the propensity of LLMs to generate insecure code and\ntheir compliance with requests to assist in cyberattacks.\nOur case study, which applied the benchmarks to seven models from the Llama 2, Code Llama, and OpenAI GPT large\nlanguage model families, revealed noteworthy cybersecurity risks. We observed insecure coding suggestions across\nall studied models, particularly those with higher coding capabilities. On average, LLMs suggested vulnerable code\n30% of the time over CYBERSECEVAL \u2019s test cases. Furthermore, models complied with 53% of requests to assist in\ncyberattacks on average across all models and threat categories.\nThese findings underscore the importance of ongoing research and development in the field of AI safety, particularly\nas LLMs continue to gain adoption in various applications. CYBERSECEVAL contributes to this effort by providing a\nrobust and adaptable framework for assessing the cybersecurity risks of LLMs.\n11\nWhile our approach has its limitations we believe that CYBERSECEVAL represents a significant step forward in the\nevaluation of LLM cybersecurity risks. We look forward to future research and development efforts that build upon our\nwork to further enhance the safety and security of LLMs.\nAcknowledgements\nWe would like to express our gratitude to Aaron Grattafiori for providing valuable feedback on the overall approach and\npaper, Dan May for his review and feedback, as well as Delia Tung for her project management support. Additionally,\nwe would like to thank David Molnar, Robert Rusch, Hugh Leather, and Mark Tygert for their feedback on drafts of the\npaper. We are also grateful for the help provided by Baptiste Rozi\u00e8re and Louis Martin with inference engineering and\ndomain knowledge about CodeLlama models. We thank Riley Pittman for feedback and support in building Insecure\nCode Detector. We would like to acknowledge Daniel Bond, Ng Yik Phang, and Gerben Janssen van Doorn for their\nassistance in sourcing security signals for Insecure Code Detector and their expertise in the security domain.\nLastly, thank you to everyone on all of the teams who helped make this work possible: Secure Application Frameworks,\nProgram Analysis, Product Security Group, Offensive Security Group, Responsible AI, GenAI, Data Security Systems,\nPrivacy Insights and Investigations, Fundamental AI Research.\nReferences\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,\nNicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\n2021.\nThomas\nDohmke.\nGithub\ncopilot\nfor\nbusiness\nis\nnow\navailable,\nFeb\n2023.\nURL\nhttps://github.blog/\n2023-02-14-github-copilot-for-business-is-now-available/.\nHossein Hajipour, Thorsten Holz, Lea Sch\u00f6nherr, and Mario Fritz. Systematically finding security vulnerabilities in black-box code\ngeneration models. arXiv preprint arXiv:2302.04012, 2023.\nRapha\u00ebl Khoury, Anderson R Avila, Jacob Brunelle, and Baba Mamadou Camara. How secure is code generated by chatgpt? arXiv\npreprint arXiv:2304.09655, 2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher\nAkiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\nCorporation MITRE. Common weakness enumeration: A community-developed list of software & hardware weakness types.\nhttps://cwe.mitre.org, 2023a. Online; accessed 4 December 2023.\nCorporation MITRE. Mitre att&ck\u00ae. https://attack.mitre.org, 2023b. Online; accessed 4 December 2023.\nVijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel Cheng, Negar Ghorbani, Renuka Fernandez,\nand Nachiappan Nagappan. Codecompose: A large-scale industrial deployment of ai-assisted code authoring. arXiv preprint\narXiv:2305.12050, 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In\nProceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.\nHammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the keyboard? assessing the\nsecurity of github copilot\u2019s code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754\u2013768. IEEE,\n2022.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez,\nJ\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nGustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt. Lost at c: A user study on\nthe security implications of large language model code assistants. arXiv preprint arXiv:2208.09727, 2023.\nInc. Semgrep. semgrep. https://semgrep.dev/, 2023. Online; accessed 5 December 2023.\nMohammed Latif Siddiq and Joanna CS Santos. Securityeval dataset: mining vulnerability examples to evaluate machine learning-\nbased code generation techniques. In Proceedings of the 1st International Workshop on Mining Software Repositories Applications\nfor Privacy and Security, pages 29\u201333, 2022.\n12\nMohammed Latif Siddiq, Shafayat H. Majumder, Maisha R. Mim, Sourov Jajodia, and Joanna C. S. Santos. An empirical study of\ncode smells in transformer-based code generation techniques. In 2022 IEEE 22nd International Working Conference on Source\nCode Analysis and Manipulation (SCAM), pages 71\u201382, 2022. doi: 10.1109/SCAM55253.2022.00014.\nNorbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C Cordeiro, and Vasileios Mavroeidis. The formai\ndataset: Generative ai in software security through the lens of formal verification. arXiv preprint arXiv:2307.02192, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,\nPrajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nFelix Wilhelm, Fabian Freyer, Calle Svensson, Thomas Otto, Michal Melewski, William Woodruff, Disconnect3d, and Matthew\nRinaldi. weggli. https://github.com/weggli-rs/weggli, 2023. Online; accessed 5 December 2023.\nBurak Yeti\u00b8stiren, I\u00b8s\u0131k \u00d6zsoy, Miray Ayerdem, and Eray T\u00fcz\u00fcn. Evaluating the code quality of ai-assisted code generation tools: An\nempirical study on github copilot, amazon codewhisperer, and chatgpt. arXiv preprint arXiv:2304.10778, 2023.\nLi Zhong and Zilong Wang. A study on robustness and reliability of large language model code generation. arXiv preprint\narXiv:2308.10335, 2023.\n13\n"
  },
  {
    "title": "Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors",
    "link": "https://arxiv.org/pdf/2312.04963.pdf",
    "upvote": "15",
    "text": "Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors\nLihe Ding1,4*, Shaocong Dong2*, Zhanpeng Huang3, Zibin Wang3\u2020,\nYiyuan Zhang1, Kaixiong Gong1, Dan Xu2, Tianfan Xue1\n1The Chinese University of Hong Kong\n2Hong Kong University of Science and Technology\n3SenseTime\n4Shanghai AI Laboratory\n{dl023, gk023, tfxue}@ie.cuhk.edu.hk, {sdongae, danxu}@cse.ust.hk\n{wangzb02, yiyuanzhang.ai}@gmail.com, {huangzhanpeng}@sensetime.com\n\u224840s\nBidirectional Diffusion: Generalizable 3D Generation\nEfficiently Refine by Optimization Methods\n\u2248 20min\nA yellow and green oil painting style eagle head\n(c) ProlificDreamer \n(b) Zero-123 \n(a) Shap-E \nFigure 1. Our BiDiff can efficiently generate high-quality 3D objects. It alleviates all these issues in previous 3D generative models: (a)\nlow-texture quality, (b) multi-view inconsistency, and (c) geometric incorrectness (e.g., multi-face Janus problem). The outputs of our model\ncan be further combined with optimization-based methods (e.g., ProlificDreamer) to generate better 3D geometries with slightly longer\nprocessing time (bottom row).\nAbstract\nMost 3D generation research focuses on up-projecting 2D\nfoundation models into the 3D space, either by minimizing\n2D Score Distillation Sampling (SDS) loss or fine-tuning\non multi-view datasets. Without explicit 3D priors, these\nmethods often lead to geometric anomalies and multi-view\n*Equal contribution. Part of this work was done when Lihe Ding and\nShaocong Dong interned at Sensetime.\n\u2020Corresponding author.\ninconsistency. Recently, researchers have attempted to im-\nprove the genuineness of 3D objects by directly training on\n3D datasets, albeit at the cost of low-quality texture gen-\neration due to the limited texture diversity in 3D datasets.\nTo harness the advantages of both approaches, we propose\nBidirectional Diffusion (BiDiff), a unified framework that\nincorporates both a 3D and a 2D diffusion process, to pre-\nserve both 3D fidelity and 2D texture richness, respectively.\nMoreover, as a simple combination may yield inconsistent\ngeneration results, we further bridge them with novel bidi-\narXiv:2312.04963v1  [cs.CV]  7 Dec 2023\nrectional guidance. In addition, our method can be used\nas an initialization of optimization-based models to further\nimprove the quality of 3D model and efficiency of optimiza-\ntion, reducing the generation process from 3.4 hours to 20\nminutes. Experimental results have shown that our model\nachieves high-quality, diverse, and scalable 3D generation.\nProject website: https://bidiff.github.io/.\n1. Introduction\nRecent advancements in text-to-3D generation [22] mainly\nfocus on lifting 2D foundation models into 3D space. One\nof the most popular solutions [17, 27] uses 2D Score Dis-\ntillation Sampling (SDS) loss derived from a 2D diffusion\nmodel to supervise 3D generation. While these methods can\ngenerate high-quality textures, they often lead to geometric\nambiguity, such as the multi-face Janus problem [23], due\nto the lack of 3D constraints (Fig. 1(c)). Moreover, these\noptimization methods are time-consuming, taking hours to\ngenerate one object. Zero-123[18] tries to alleviate the prob-\nlem by fine-tuning the 2D diffusion models on multi-view\ndatasets, but it still cannot guarantee geometric consistency\n(Fig. 1(b)).\nTo ensure better 3D consistency, another solution is to\ndirectly learn 3D structures from 3D datasets [14, 25]. How-\never, many existing 3D datasets [2, 5] only contain hand-\ncrafted objects or lack high-quality 3D geometries, with\ntextures very different from real-world objects. Moreover,\n3D datasets are often much smaller than, and also difficult\nto scale up to, their 2D counterparts. As a result, the 3D dif-\nfusion models (Fig. 1 (a)) normally cannot generate detailed\ntextures and complicated geometry, even if they have bet-\nter 3D consistency compared to up-projecting 2D diffusion\nmodels.\nTherefore, a straightforward way to leverage the advan-\ntages of both methods is to combine both 2D and 3D diffu-\nsion models. However, a simple combination may result in\ninconsistent generative directions as they are learned in two\nindependent diffusion processes. In addition, the two diffu-\nsion models are represented in separate 2D and 3D spaces\nwithout knowledge sharing.\nTo overcome these problems, we propose Bidirectional\nDiffusion (BiDiff), a method to seamlessly integrate both\n2D and 3D diffusion models within a unified framework.\nSpecifically, we employ a hybrid representation in which a\nsigned distance field (SDF) is used for 3D feature learning\nand multi-view images for 2D feature learning. The two\nrepresentations are mutually transformable by rendering 3D\nfeature volume into 2D features and back-projecting 2D\nfeatures to 3D feature volume. Starting from pretrained 3D\nand 2D diffusion models, the two diffusion models are jointly\nfinetuned to capture a joint 2D and 3D prior facilitating 3D\ngeneration.\nHowever, correlating the 2D and 3D representations is\nnot enough to combine two diffusion processes, as they may\ndeviate from each other in the following diffusion steps. To\nsolve this problem, we further introduce bidirectional guid-\nance to align the generative directions of the two diffusion\nmodels. At each diffusion step, the intermediate results from\nthe 3D diffusion scheme are rendered into 2D images as\nguidance signals to the 2D diffusion model. Meanwhile, the\nmulti-view intermediate results from the 2D diffusion pro-\ncess are also back-projected to 3D, guiding the 3D diffusion.\nThe mutual guidance regularizes the two diffusion processes\nto learn in the same direction.\nThe proposed bidirectional diffusion poses several advan-\ntages over the previous 3D generation models. First, users\ncan separately control the generation of 2D texture and 3D\ngeometry, as shown in Fig. 2, because the 2D diffusion model\nfocuses on texture generation and the 3D diffusion model\nfocuses on geometry. This is impossible for previous 3D\ndiffusion methods. Secondly, compared to 3D-only diffusion\nmodels [14], our method takes advantage of a 2D diffusion\nmodel trained on much larger datasets. Therefore, it can\ngenerate more diversified objects and create a completely\nnew object like \u201cA strong muscular chicken\u201d illustrated in\nFig 2. Thirdly, compared to previous optimization meth-\nods [27, 37] that often take several hours to generate one\nobject, we utilize a fast feed-forward joint 2D-3D diffusion\nmodel for scalable generation, which only takes about 40\nseconds to generate one object.\nMoreover, because of the efficacy of BiDiff, we also pro-\npose an optional step to utilize its output as an initialization\nfor the existing optimization-based methods (e.g., Prolific-\nDreamer [37]). This optional step can further improve the\nquality of a 3D object, as demonstrated in the bottom row of\nFig. 1. Also, the good initialization from BiDiff helps to re-\nduce optimization time from around 3.4 hours to 20 minutes,\nand concurrently resolves geometrical inaccuracy issues,\nlike multi-face anomalies. Moreover, this two-step gener-\nation enables creators to rapidly adjust prompts to obtain\na satisfactory preliminary 3D model through a lightweight\nfeed-forward generation process, subsequently refining it\ninto high-fidelity results.\nThrough training on ShapeNet [2] and Objaverse 40K [5],\nour framework is shown to generate high-quality textured\n3D objects with strong generalizability. In summary, our\ncontributions are as follows: 1) We propose BiDiff, a joint\n2D-3D diffusion model, that can generate high-quality, 3D-\nconsistent, and diversified 3D objects; 2) We propose a novel\ntraining pipeline that utilizes both pretrained 2D and 3D gen-\nerative foundation models; 3) We propose the first diffusion-\nbased 3D generation model that allows independent control\nof texture and geometry; 4) We utilize the outputs from\nBiDiff as a strong initialization for the optimization-based\nmethods, generating high-quality geometries while ensuring\n2\nA golden skull.\nA crystal skull.\nAn ancient Chinese \ntower.\nAn ancient Gothic \ntower.\nA strong \nmuscular man.\nA strong muscular \nchicken.\nA blue and white \nporcelain teapot.\nA blue and white \nporcelain burger.\nA blue and red \nSuperman clothes.\nA bule and red \nSuperman clothes \nstyle car.\nA board with Van \nGogh\u2018s starry sky \nstyle painting on it.\nA house in Van \nGogh starry sky \nstyle.\nFigure 2. Texture Control (Top): we change the texture while maintaining the overall shape. Shape Control (Bottom): we fix texture\npatterns and generate various shapes.\nthat users receive quick feedback for each prompt update.\n2. Related Work\nEarly 3D generative methods adopt various 3D representa-\ntions, including 3D voxels [10, 34, 38], point clouds [1, 40],\nmeshes [9, 12], and implicit functions [3, 26] for category-\nlevel 3D generations. These methods directly train the gener-\native model on a small-scale 3D dataset, and, as a result, the\ngenerated objects may either miss tiny geometric structures\nor lose diversity. Even though there are large-scale [5] or\nhigh-quality 3D datasets [39] in recent years, they are still\nmuch smaller than the datasets used for 2D image generation\ntraining.\nWith the powerful text-to-image synthesis models [29\u2013\n31], a new paradigm emerges for 3D generation without\nlarge-scale 3D datasets by leveraging 2D generative model.\nOne line of works utilizes 2D priors from pre-trained text-\nto-image model (known as CLIP) [13, 15] or 2D diffusion\ngenerative models [17, 22, 35] to guide the optimization\nof underlying 3D representations. However, these mod-\nels could not guarantee cross-view 3D consistency and the\nper-instance optimization scheme suffers both high com-\nputational cost and over-saturated problems. Later on, re-\nsearchers improve these models using textual codes or depth\nmaps [6, 21, 32], and [37] directly model 3D distribution\nto improve diversity. These methods alleviate the visual\nartifacts but still cannot guarantee high-quality 3D results.\nAnother line of works learn 3D priors directly from 3D\ndatasets. As the diffusion model has been the de-facto net-\nwork backbone for most recent generative models, it has been\nadapted to learn 3D priors using implicit spaces such as point\ncloud features [25, 42], NeRF parameters [7, 14], or SDF\nspaces [4, 19]. The synthesized multi-view images rendered\nfrom 3D datasets were also utilized to provide cross-view\n3D consistent knowledge [18]. These methods normally\nhighlight fast inference and 3D consistent results. However,\ndue to inferior 3D dataset quality and size, these methods\ngenerally yield visually lower-quality results with limited\ndiversity. Recently a few methods [28, 33] explored to com-\nbine 2D priors and 3D priors from individual pre-trained\ndiffusion models, but they often suffer from inconsistent\nbetween two generative processes.\n3. Method\nAs many previous studies [18, 28] have illustrated, both\n2D texture and 3D geometry are important for 3D object\ngeneration. However, incorporating 3D structural priors and\n2D textural priors is challenging: i) combining both 3D and\n2D generative models into a single cohesive framework is\nnot trivial; ii) in both training and inference, two generative\nmodels may lead to opposite generative directions.\nTo tackle these problems, we propose BiDiff, a novel\nbidirectional diffusion model that marries a pretrained 3D\ndiffusion model with another 2D one using bidirectional\nguidance. Fig. 3 illustrates the overall architecture of our\nframework. Details of each component will be discussed\nbelow. Specifically, in Sec. 3.1, we will introduce our novel\nhybrid representation that includes both 2D and 3D informa-\ntion, and the bidirectional diffusion model built on top of this\nhybrid representation. In Sec. 3.2 and Sec. 3.3, to ensure the\ntwo generative models lead to the same generative direction,\nwe will introduce how to add bidirectional guidance to both\n3D and 2D diffusion models. In Sec. 3.4, we discuss one\nadvantage of BiDiff, which is independent control of texture\n3\nRecon loss\nVolume\nEncoding\nNoisy Input\n2D-3D Control\n3D-2D Control\nNoisy Input\n3D Foundation \nModel\n2D Denoising\n3D Denoising\nVolume \nRendering\nFeature Volume\nMulti-view images\n(t step)\n3D SDF\n(t step)\n(a)\n3D SDF\n(t-1 step)\nMulti-view images\n(t-1 step)\nFeature Volume\nSDF\nPrediction\n2D Noise\nDistillation\nSDS loss\nOptimization\nstep t\nstep t-1\nstep t+1\nstep 0\n3D Noise\n(b)\n3D Pipeline\n2D Pipeline\nFigure 3. The BiDiff framework operates as follows: (a) At each step of diffusion, we render the 3D diffusion\u2019s intermediate outputs into\n2D images, which then guide the denoising of the 2D diffusion model. Simultaneously, the intermediate multi-view outputs from the 2D\ndiffusion are re-projected to assist the denoising of the 3D diffusion model. Red arrows show the bidirectional guidance, which ensures\nthat both diffusion processes evolve coherently. (b) We use the outcomes of the 2D-3D diffusion as a strong starting point for optimization\nmethods, allowing for further refinement with fewer optimization steps.\nand geometry generation, as shown in Fig. 2. Finally, in\nSec. 3.5, we discuss another advantage of BiDiff, which is\nto use the results from BiDiff as a strong initialization for\noptimization-based methods to obtain more delicate results\nefficiently.\n3.1. Bidirectional Diffusion\nTo incorporate both 2D and 3D priors, we represent a 3D\nobject using a hybrid combination of two formats: Signed\nDistance Field (SDF) F and multi-view image set V =\n\b\nIi\tM\ni=1, where F is computed from signed distance values\non an N \u00d7 N \u00d7 N grid, and Ii is the i-th image from a\nmulti-view image set of size M. This hybrid representation\nis shown on the left side of Fig. 3.\nWith this representation, we learn a joint distribution\n{F, V} utilizing two distinct diffusion models: a 3D diffu-\nsion model D3d in the SDF space (the green 3D denoising\nblock in Fig. 3) and a 2D multi-view diffusion model D2d\nwithin the image domain (the blue 2D denoising block in\nFig. 3). Specifically, given a timestep t, we add Gaussian\nnoises to both SDF and multi-view images as\nFt = \u221a\u03b1tF0 +\n\u221a\n1 \u2212 \u03b1t\u03f53d and\nIi\nt = \u221a\u03b1tIi\n0 +\n\u221a\n1 \u2212 \u03b1t\u03f5i\n2d for \u2200i,\n(1)\nwhere \u03f5 \u223c N(0, I) is random noise, and \u03b1t is a noise\nschedule which is different in 3D and 2D. Subsequently,\nthe straightforward way is to separately train these two dif-\nfusion models by minimizing the following two objectives:\nLsimple3d = EFt,\u03f53d,t\u2225\u03f53d \u2212 D3d(Ft, t)\u22252\n2,\nLsimple2d = 1\nN\nN\nX\ni=1\n(EIi\nt,\u03f5i\n2d,t\u2225\u03f5i\n2d \u2212 D2d(Ii\nt, t)\u22252\n2),\n(2)\nwhere \u03f53d and \u03f52d are Gaussian noises \u03f53d, \u03f5i\n2d \u223c N(0, I),\nSDF and image set are sampled from forward diffusion pro-\ncesses Ft \u223c q(Ft), Ii\nt \u223c q(Ii\nt), and timestep is uniformly\nsampled t \u223c U[1, T].\nHowever, this simple combination does not consider the\ncorrelations between 3D and 2D diffusion, which may hinder\nthe understanding of 2D and 3D consistency, leading to\ninconsistent generation between 3D geometry and 2D multi-\nview images.\nWe resolve this problem by a novel Bidirectional Dif-\nfusion. In this model, the consistency between 3D and\n2D diffusion output is enforced through bidirectional guid-\nance. First, we add guidance from the 2D diffusion pro-\ncess to the 3D generative process, which is the red arrow\npointing to the \u201c2D-3D control\u201d. Specifically, during each\ndenoising step t, we feed the denoised multi-view images\nV\u2032\nt+1 =\n\b\nIi\nt+1\n\tN\ni=1 in previous step into the 3D diffusion\nmodel as \u03f5\u2032\n3d = D3d(Ft, V\u2032\nt+1, t). This guidance steers\nthe current 3D denoising direction to ensure 2D-3D con-\nsistency. It\u2019s worth mentioning that the denoised output\nV\u2032\nt+1 from the previous step t + 1 is inaccessible in train-\ning, therefore we directly substitute it with the ground truth\nVt. In inference, we utilize the denoised images from the\nprevious step. Then we could obtain the denoised radi-\nance field F\u2032\n0 given the 2D guided noise prediction \u03f5\u2032\n3d by\nF\u2032\n0 =\n1\n\u221a\u03b1t (Ft \u2212 \u221a1 \u2212 \u03b1t\u03f5\u2032\n3d).\n4\nSecondly, we also add guidance from the 3D diffusion\nprocess to the 2D generative process.\nSpecifically, us-\ning the same camera poses, we render multi-view images\nHi\nt derived from the radiance field F\u2032\n0 by the 3D diffu-\nsion model: Hi\nt = R(F\u2032\n0, Pi), i = 1, ...M, where Pi is\nthe ith camera pose.\nThese images are further used as\nguidance to the 2D multi-view denoising process D2d by\n\u03f5\u2032\n2d = D2d(Vt,\n\b\nHi\nt\n\tN\ni=1 , t).. This guidance is the red arrow\npointing to the \u201c3D-2D control\u201d in Fig. 3.\nOur method can seamlessly integrate and synchronize\nboth the 3D and 2D diffusion processes within a unified\nframework. In the following sections, we will delve into\neach component in detail.\n3.2. 3D Diffusion Model with 2D Guidance\nOur 3D diffusion model aims to generate a neural surface\nfield (NeuS) [20], with novel 2D-to-3D guidance derived\nfrom the denoised 2D multi-view images. To train our 3D\ndiffusion model, at each training timestep t, we add noise\nto a clean radiance field, yielding a noisy one Ft. This\nfield, combined with the timestep t embeddings and the\ntext embeddings, is then passed through 3D sparse con-\nvolutions to generate a 3D feature volume M as: M =\nSp3DConv(Ft, t, text). Then we sample N \u00d7 N \u00d7 N grid\npoints from M and project these points onto all denoised\nmulti-view images V\u2032\nt+1 from the previous step of the 2D\ndiffusion model. At each grid point p, we aggregate the inter-\npolated 2D feature at its 2D projected location on each view,\nand calculate the mean and variance over all N interpolated\nfeatures to obtain the image-conditioned feature volume N:\nN(p) = [Mean(V\u2032\nt+1(\u03c0(p))), Var(V\u2032\nt+1(\u03c0(p)))],\n(3)\nwhere \u03c0 denotes the projection operation from 3D to 2D\nimage plane. We fuse these two feature volumes with further\nsparse convolutions for predicting the clean F0.\nOne important design of our 3D diffusion model is that it\nincorporates geometry priors derived from the 3D foundation\nmodel, Shap-E [14]. Shap-E is a latent diffusion [22] model\ntrained on several millions 3D objects, and thus ensures the\ngenuineness of generated 3D objects. Still, we do not want\nShap-E to limit the creativity of our 3D generative model,\nand try to preserve the capability of generating novel objects\nthat Shap-E cannot.\nTo achieve this target, we design a feature volume G to\nrepresent a radiance field converted from the Shap-E latent\ncode C. It is implemented using NeRF MLPs by setting their\nparameters to the latent code C: G(p) = MLP(\u03bb(p); \u03b8 = C),\nwhere \u03bb denotes the positional encoding operation.\nOne limitation of directly introducing Shap-E latent code\nis that the network is prone to shortcut the training process,\neffectively memorizing the radiance field derived from Shap-\nE. To generate 3D objects beyond Shap-E model, we add\nGaussian noise at level t0 to the clean latent code, resulting in\nthe noisy latent representation Ct0, where t0 represents a pre-\ndefined constant timestep. Subsequently, the noisy radiance\nfield Gt0 is decoded by substituting C with Ct0. This design\nestablishes a coarse-to-fine relationship between the 3D prior\nand the ground truth, prompting the 3D diffusion process to\nleverage the 3D prior without excessively depending on it.\nIn this way, we can get the fused feature volume as:\nS = U([M, Sp3DConv(N), Sp3DConv(Gt0)]),\n(4)\nwhere U denotes 3D sparse U-Net. Then we can query\nfeatures from S for each grid point p and decode it to SDF\nvalues through several MLPs: F\u2032\n0(p) = MLP(S(p), \u03bb(p)),\nwhere S(p) represents the interpolated features from S at\nposition p. In Sec. 4.2 and Fig. 4, our experiments also\ndemonstrate that our model can generate 3D objects beyond\nShap-E model.\n3.3. 2D Diffusion Model with 3D Guidance\nOur 2D diffusion model simultaneously generates multi-\nview images by jointly denoising multi-view noisy images\nVt =\n\b\nIi\nt\n\tM\ni=1. To encourage 2D-3D consistency, the 2D\ndiffusion model is also guided by the 3D radiance field output\nfrom 3D diffusion process mentioned above. Specifically, for\nbetter image quality, 2D multi-view diffusion model is built\non the multiple independently frozen 2D foundation models\n(e.g., DeepFloyd [8]) to harness the potent 2D priors. Each of\nthese frozen 2D foundation models (the dark blue network in\nFig. 3) is modulated by view-specific 3D-consistent residual\nfeatures and responsible for the denoising of a specific view,\nas described below.\nFirst, to achieve 3D-to-2D guidance, we render multi-\nview images from the 3D denoised radiance field F\u2032\n0 and feed\nthem to 2D denoising model. Note that the radiance field\nconsists of a density field and a color field. The density field\nis constructed from the signed distance field (SDF) generated\nby our 3D diffusion model using S-density introduced in\nNeuS [36]. To obtain the color field, we apply another color\nMLP to the feature volume in the 3D diffusion process.\nUpon obtaining the color field c and density field \u03c3, we\nconduct volumetric rendering on each ray r(m) = o + md\nwhich extends from the camera origin o along a direction d\nto produce multi-view consistent images\n\b\nHi\tM\ni=1:\n\u02c6C(r) =\nZ \u221e\n0\nT(m)\u03c3(r(m)))c(r(m)), d)dm,\n(5)\nwhere T(m) = exp(\u2212\nR m\n0 \u03c3(r(s))ds) handles occlusion.\nSecondly, we use these rendered multi-view images as\nguidance for the 2D foundation model. We first use a shared\nfeature extractor E to extract hierarchical multi-view consis-\ntent features from these images. Then each extracted feature\nis added as residuals to the decoder of its corresponding\nfrozen 2D foundation denoising U-Net (the red arrow point-\ning to \u201c3D-2D Control\u201d in Fig. 3), achieving multi-view\n5\nmodulation and joint denoising following ControlNet [43]\nas \u02c6\nf i\nk = f i\nk + ZeroConv(E(Hi)[k]), where f k\ni denotes the\noriginal feature maps of the k-th decoder layer in 2D foun-\ndation model, E(Hi)[k] denotes the k-th residual features\nof the i-th view, and ZeroConv [43] is 1 \u00d7 1 convolution\nwhich is initialized by zeros and gradually updated during\ntraining. Experimental results show that this 3D-to-2D guid-\nance helps to ensure multi-view consistency and facilitate\ngeometry understanding.\n3.4. Separate Control of Geometry and Texture\nOne advantage of BiDiff is that it naturally separates 2D tex-\nture generation using 2D diffusion model from 3D geometry\ngeneration using 3D diffusion model. Because of this, users\ncan separately control geometry and texture generation, as\nshown in Fig. 2.\nTo achieve this, we first propose a prior enhancement\nstrategy to empower a manual control of the strength of 3D\nand 2D priors independently. Inspired by the classifier-free\nguidance [11], during training, we randomly drop the infor-\nmation from 3D priors by setting condition feature volume\nfrom G to zero and weaken the 2D priors by using empty\ntext prompts. Consequently, upon completing the training,\nwe can employ two guidance scales, \u03b33d and \u03b32d, to inde-\npendently modulate the influence of these two priors.\nSpecifically, to adjust the strength of 3D prior, we calcu-\nlate the difference between 3D diffusion outputs with and\nwithout conditional 3D feature volumes, and add them back\nto 3D diffusion output:\n\u02c6\u03f53d =D3d(Ft, V\u2032\nt+1, t) + \u03b33d \u00b7 ((D3d(Ft, V\u2032\nt+1, t|G)\u2212\nD3d(Ft, V\u2032\nt+1, t)).\n(6)\nThen we can control the strength of 3D prior by adjusting\nthe weight \u03b33d of this difference term. When \u03b33d = 0, it will\ncompletely ignore 3D prior. When \u03b33d = 1, this is just the\nprevious model that uses both 3D prior and 2D prior. When\n\u03b33d > 1, the model will produce geometries close to the\nconditional radiance field but with less diversity.\nAlso, we can similarly adjust the strength of 2D priors by\nadding differences between 2D diffusion outputs with and\nwithout conditional 2D text input:\n\u02c6\u03f52d =D2d(Vt,\n\b\nHi\nt\n\tM\ni=1 , t)+\n\u03b32d \u00b7 ((D2d(Vt,\n\b\nHi\nt\n\tM\ni=1 , t|text))\u2212\nD2d(Vt,\n\b\nHi\nt\n\tM\ni=1 , t)).\n(7)\nIncreasing \u03b32d results in more coherent textures with text,\nalbeit at the expense of diversity. It is worth noting that while\nwe adjust the 3D and 2D priors independently via Eq. (6)\nand Eq. (7), the influence inherently propagates to the other\ndomain due to the intertwined nature of our bidirectional\ndiffusion process.\nWith these two guidance scales \u03b33d and \u03b32d, we can easily\nachieve a separate control of geometry and texture. First,\nto only change texture while keep geometry untouched, we\njust fix the initial 3D noisy SDF grids and the conditional\nradiance field Ct0, while enlarge its influence by Eq. (7). On\nthe other hand, to only change geometry while keep texture\nstyle untouched, we can maintain keywords in text prompts\nand enlarge its influence by Eq. (6). By doing so, the shape\nwill be adjusted by the 3D diffusion process.\n3.5. Optimization with BiDiff Initialization\nThe generated radiance field F0 using BiDiff can be fur-\nther used as a strong initialization of the optimization-based\nmethods [37]. This additional step can further improve the\nquality of the 3D model, as shown in Fig. 1 and Fig. 5. Im-\nportantly, compared to the geometries directly generated by\noptimization, our BiDiff can output more diversified geome-\ntry and generated geometries better aligns with users\u2019 input\ntext, and also has more accurate 3D geometry. Therefore,\nthe optimization started from this strong initialization can\nbe rather efficient (\u2248 20min) and avoid incorrect geometries\nlike multi-face and floaters.\nSpecifically, we first convert generated radiance field F0\nfrom BiDiff into a higher resolution one F0 that supports\n512 \u00d7 512 resolution image rendering, as shown on the\nright of Fig. 3. This process is achieved by a fast NeRF\ndistillation operation (\u2248 2min). The distillation first bounds\nthe occupancy grids of F0 with the estimated binary grids\n(transmittance > 0.01) from the original radiance field F0,\nthen overfits F0 to F0 by minimizing both the L1 distance\nbetween two density fields and L1 distance between their\nrenderings 2D images under random viewpoints. Thanks to\nthis flexible and fast distillation operation, we can efficiently\nconvert generated radiance field from BiDiff into any 3D\nrepresentations an optimization-based method requires. In\nour experiments, since we are using ProlificDreamer [37],\nwe use the InstantNGP [24] as the high-resolution radiance\nfield.\nAfter initialization, we optimize F0 by SDS loss follow-\ning the previous methods [27, 37]. It is noteworthy that since\nwe already have a good initialized radiance field, we only\nneed to apply a small noise level SDS loss. Specifically, we\nset the ratio range of denoise timestep topt to [0.02, 0.5]\nduring the entire optimization process.\n4. Experiment\nIn this section, we described our experimental results. We\ntrain our framework on the ShapeNet-Chair [2] and Ob-\njaverse LVIS 40k datasets [5].\nWe use the pre-trained\nDeepFloyd-IF-XL [8] as our 2D foundation model and Shap-\nE [14] as our 3D priors. We adopt the SparseNeuS [20] as\nthe neural surface field presentation with N = 128. For the\n3D-to-2D guidance, We follow the setup of ControlNet [43]\n6\nA chair made of Minecraft bedrock blocks. The blocks should be seamlessly integrated into the chair's structure.\nA chair designed in the shape of a cactus, with prickly spines and a green, desert-inspired texture.\nA beautiful dress made out of fruit, on a mannequin.\nA red Volkswagen Beetle car. \nOurs\nShap-E\nFigure 4. Qualitative sampling results of Bidirectional Diffusion model, including multi-view images and 3D mesh from diffusion sampling.\nThe top two rows are the results on the Shapenet-Chair, and the bottom two rows are the results on the Objaverse. We compared the results\nof Shap-E in the last column.\nto render M = 8 multi-view images with 64 \u00d7 64 resolution\nusing SparseNeuS. We train our framework on 4 NVIDIA\nA100 GPUs for both ShapeNet and Objaverse 40k exper-\niments with batch size of 4. During sampling, we set the\n3D and 2D prior guidance scale to 3.0 and 7.5 respectively.\nMore details on data processing and model architecture are\nincluded in supplementary material. We discuss the evalu-\nation and ablation study results below. Also, please refer\nto supplementary webpages and videos for more visual\nresults.\n4.1. Text-to-3D Results\nShapeNet-Chair results.\nThe first and second rows of\nFig. 4 present our results trained on the ShapeNet-Chair\ndataset. Although the chair category often contains com-\nplicated geometric details, our framework demonstrates the\ncapability to capture those fine details. Concurrently, our ap-\nproach exhibits a remarkable capability to produce rich and\ndiverse textures by merely modulating the textual prompts,\nleading to compelling visual outcomes.\nObjaverse-40K results.\nScaling to a much larger 3D\ndataset, Objaverse-40K, our framework\u2019s efficacy becomes\nincreasingly pronounced. The bottom two rows of Fig. 4\nare results from the Objaverse dataset. Compared to ob-\njects generated by Shap-E, our model closely adheres to the\ngiven textual prompts. This again shows that the proposed\nBiDiff learns to model both 2D textures and 3D geometries\nbetter compared with 3D-only solutions, and is capable of\ngenerating more diverse geometries.\nTable 1. CLIP R-precision.\nMethod\nR-P\ntime\nDreamFusion\n0.67 1.1h\nProlificDreamer 0.83 3.4h\nOurs-sampling 0.79\n40s\nOurs-post\n0.85 20min\nDecouple\ngeometry\nand\ntexture\ncontrol.\nLastly, we illustrate that\nour BiDiff can separately\ncontrol geometry and\ntexture generation. First,\nas illustrated in the first row of Fig. 2, when the 3D prior is\nfixed, we have the flexibility to manipulate the 2D diffusion\nmodel using varying textual prompts to guide the texture\ngeneration process. This capability enables the generation\nof a diverse range of textured objects while maintaining a\nconsistent overall shape. Second, when we fix the textual\nprompt for the 2D priors (e.g., \"a xxx with Van Gogh starry\nsky style\"), we can adjust the 3D diffusion model by varying\nthe conditional radiance field derived from the 3D priors.\nThis procedure results in the generation of a variety of\nshapes, while maintaining a similar texture, as shown in the\nsecond row of Fig. 2.\n4.2. Comparison with other Generation Models\nComparison with optimization methods.\nOur framework\nis capable of simultaneously generating multi-view consis-\ntent images alongside a 3D mesh in a scalable manner. In\ncontrast, the SDS-based methods [27, 37] utilize a one-\nby-one optimization approach. Tab. 1 reports the CLIP\nR-Precision [14] and inference time on 50 test prompts\nmanually derived from the captioned untrained Objaverse\nto quantitatively evaluate these methods. Also, optimiza-\ntion methods, Dreamfusion [27] and ProlificDreamer [37],\n7\nZero-1-to-3\nPolificDreamer\nInput(by SD 2.1)\n90\u00b0\n-150\u00b0\nBidirectional Diffusion Results\nRefinement Results\nA golden skull\ntime \u2248 20min\ntime \u2248 6h\nFigure 5. Comparison with other optimization or multi-view diffusion based works. We show both multi-view images (left) and 3D results\n(right). Zero-1-to-3 [18] is not good at predicting results from a large perspective, and PolificDreamer [37] suffers from the multi face\nproblem. Our method has excellent robustness and can obtain high-quality results in a short period of time.\nw/o \n2D prior\nw/o \n3D prior\nnormal\nAn oral cavity style chair, includes oral cavity \nelements like red tongue and white teeth.\nw/o prior \nenhancement\nFigure 6. Ablation of prior and prior enhancement.\nare expensive, taking several hours to generate a single ob-\nject. Moreover, these optimization methods may lead to\nmore severe multi-face problems. In contrast, our method\ncan produce realistic objects with reasonable geometry in\nonly 40 seconds. Furthermore, BiDiff can serve as a strong\nprior for optimization-based methods and significantly boost\ntheir performance. Initializing the radiance field in Prolific-\nDreamer [37] with our outputs shows remarkable improve-\nments in both quality and computational efficiency, as shown\nin Fig. 5.\nComparison with multi-view methods\nGiven one refer-\nence image, the multi-view method Zero-1-to-3 [18] pro-\nDiffusion Output\n\ufffd\u00a0~\u00a0\ufffd(\ufffd. \ufffd\ufffd, \u00a0\ufffd. \ufffd\ufffd)\n\ufffd\u00a0~\u00a0\ufffd(\ufffd. \ufffd\ufffd, \u00a0\ufffd. \ufffd)\nFigure 7. Ablation of range of noise level t for SDS.\nduces images from novel viewpoints by fine-tuning a pre-\ntrained 2D diffusion model on multi-view datasets. However,\nthis method employs cross-view attention to establish multi-\nview correspondence without an inherent understanding of\n3D structures, inevitably leading to inconsistent multi-view\nimages as shown in Fig. 5. Moreover, the Zero-123 series\ncannot directly generate the 3D mesh, requiring substan-\ntial post-processing (SDS loss) to acquire the geometry. In\ncontrast, our framework also incorporates 3D priors, in addi-\ntion to 2D priors, and thus can generate more accurate 3D\ngeometries.\n4.3. Abalation Studies\nWe perform comprehensive ablation studies on the ShapeNet-\nChair dataset [2] to evaluate the importance of each com-\nponent below. More ablation results can be found in the\nsupplementary material.\n3D priors.\nTo assess the impact of 3D priors, we eliminate\nthe conditional radiance field from Shap-E and train the 3D\ngeometry generation from scratch. The experimental results\nin the second row of Fig. 6 demonstrate that in the absence\nof the 3D priors, our framework can only generate common\n8\nobjects in the training set.\n2D priors.\nTo delve into the impact of 2D priors, we\nrandomly initiate the parameters of the 2D diffusion model,\ninstead of fine-tuning on a pretrained model. The results in\nthe first row of Fig. 6 show that in the absence of 2D priors,\nthe textures generated tend to fit the stylistic attributes of the\nsynthetic training data. Conversely, with 2D priors, we can\nproduce more realistic textures.\nPrior enhancement strategy.\nAs discussed in Sec. 3.4,\nwe can adjust the influence of both 3D and 2D priors by the\nprior enhancement strategy. Fig. 6 also shows the results of\nnot using this strategy. It shows that the prior enhancement\nstrategy plays a vital role in achieving diverse and flexible\n3D generation.\nRange of noise level for SDS.\nThe results in Fig. 7 il-\nlustrate the impact of the noise level during the entire opti-\nmization process, as discussed in Sec. 3.5. The 3D object\ngenerated with a smaller noise range is closer to the diffusion\noutput. By adjusting the range of the noise level topt, we can\neffectively control the texture similarity between geometries\nbefore and after the optimization.\n5. Conclusion\nIn this paper, we propose Bidirectional Diffusion, which in-\ncorporates both 3D and 2D diffusion processes into a unified\nframework. Furthermore, Bidirectional Diffusion leverages\nthe robust priors from 3D and 2D foundation models,\nachieving generalizable geometry and texture understanding.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In International conference on\nmachine learning, pages 40\u201349. PMLR, 2018. 3\n[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis\nSavva, Shuran Song, Hao Su, et al. Shapenet: An information-\nrich 3d model repository. arXiv preprint arXiv:1512.03012,\n2015. 2, 6, 8, 1\n[3] Zhiqin Chen and Hao Zhang. Learning implicit fields for gen-\nerative shape modeling. In Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019.\n3\n[4] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tuyakov, Alex\nSchwing, and Liangyan Gui. SDFusion: Multimodal 3d shape\ncompletion, reconstruction, and generation. arXiv, 2022. 3\n[5] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,\nAniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. arXiv preprint arXiv:2212.08051,\n2022. 2, 3, 6, 1\n[6] C. Deng, C. Jiang, C. R. Qi, X. Yan, Y. Zhou, L. Guibas,\nand D. Anguelov. Nerdi: Single-view nerf synthesis with\nlanguage-guided diffusion as general image priors. In 2023\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 20637\u201320647, 2023. 3\n[7] Ziya Erko\u00e7, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner, and\nAngela Dai. Hyperdiffusion: Generating implicit neural fields\nwith weight-space diffusion, 2023. 3\n[8] Deep Floyd. If project. https://github.com/deep-\nfloyd/IF, 2023. 5, 6\n[9] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-\nKun Lai, , and Hao Zhang. Sdm-net: Deep generative net-\nwork for structured deformable mesh. ACM Transactions on\nGraphics (TOG), 38:1\u201315, 2019. 3\n[10] Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escaping\nplato\u2019s cave: 3d shape from adversarial rendering. In The\nIEEE International Conference on Computer Vision (ICCV),\n2019. 3\n[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[12] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree trans-\nformer: Autoregressive 3d shape generation on hierarchically\nstructured sequences. arXiv preprint arXiv:2111.12480, 2021.\n3\n[13] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013876,\n2022. 3\n[14] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional\n3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.\n2, 3, 5, 6, 7, 1\n[15] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Popa Tiberiu. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. SIGGRAPH\nAsia 2022 Conference Papers, 2022. 3\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 1\n[17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-\n3d content creation. arXiv preprint arXiv:2211.10440, 2022.\n2, 3\n[18] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. arXiv preprint arXiv:2303.11328,\n2023. 2, 3, 8\n[19] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ.\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu.\nMeshd-\niffusion: Score-based generative 3d mesh modeling.\nIn\nInternational Conference on Learning Representations, 2023.\n3\n[20] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural surface\nreconstruction from sparse views. In European Conference\non Computer Vision, pages 210\u2013227. Springer, 2022. 5, 6\n9\n[21] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Realfusion: 360 reconstruction of any object\nfrom a single image. In CVPR, 2023. 3\n[22] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600,\n2022. 2, 3, 5\n[23] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2\n[24] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 6\n[25] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 3\n[26] Jeong Joon Park, Peter Florence, Julian Straub, Richard New-\ncombe, , and Steven Lovegrove. Deepsdf: Learning contin-\nuous signed distance functions for shape representation. In\nProceedings of IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 165\u2013174, 2019. 3\n[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 2, 6, 7\n[28] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-\naksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,\nPeter Wonka, Sergey Tulyakov, et al. Magic123: One image\nto high-quality 3d object generation using both 2d and 3d\ndiffusion priors. arXiv preprint arXiv:2306.17843, 2023. 3\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 3\n[32] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 3\n[33] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gener-\nation. arXiv:2308.16512, 2023. 3\n[34] Edward Smith and David Meger. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In Conference on\nRobot Learning, pages 87\u201396. PMLR, 2017. 3\n[35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and\nGreg Shakhnarovich. Score jacobian chaining: Lifting pre-\ntrained 2d diffusion models for 3d generation. arXiv preprint\narXiv:2212.00774, 2022. 3\n[36] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 5\n[37] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 6, 7, 8,\n4\n[38] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. In\nAdvances in neural information processing systems, pages\n82\u201390, 2016. 3\n[39] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Chen Qian Jiaqi Wang,\nDahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary\n3d object dataset for realistic perception, reconstruction and\ngeneration. IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2023. 3\n[40] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. In Proceedings\nof the IEEE/CVF international conference on computer vision,\npages 4541\u20134550, 2019. 3\n[41] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 1\n[42] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point\ndiffusion models for 3d shape generation. arXiv preprint\narXiv:2210.06978, 2022. 3\n[43] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. arXiv preprint\narXiv:2302.05543, 2023. 6, 3\n10\nText-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors\nSupplementary Material\nIn the supplementary material, we first introduce the data\nprocessing pipeline in (\u00a7 5.1), then provide more implemen-\ntation details of the model architecture (\u00a7 5.2), more training\ndetails in (\u00a7 5.3), and give more ablation results in (\u00a7 5.4).\n5.1. Data Processing\nAs mentioned in the main paper, we use 6k ShapNet-\nChair [2] and LVIS Objaverse 40k\n[5] as our training\ndatasets. We obtain the Objaverse 40k dataset by filtering\nobjects with LVIS category labels in the 800k Objaverse data.\nTo process data for the 2D diffusion process, we use Blender\nto render each 3D object into 8 images with a fixed elevation\nof 30\u25e6 and evenly distributed azimuth from \u2212180\u25e6 to 180\u25e6.\nThese fixed view images serve as the ground truth multi-\nview image set V. In addition, we also randomly render 16\nviews to supervise the novel view rendering of the denoised\nradiance field F\u2032\n0. All the images are rendered at a resolu-\ntion of 256 \u00d7 256. Since we adopt the DeepFloyd as our\n2D foundation model which runs at a resolution of 64 \u00d7 64,\nthe rendered images are downsampled to 64 \u00d7 64 during\ntraining. To process data for the 3D diffusion, we compute\nthe signed distance of each 3D object at each N \u00d7 N \u00d7 N\ngrid point within a [\u22121, 1] cube, where N is set to 128 in\nour experiments. To obtain the latent code C for each object,\nwe use the encoder in Shap-E [14] to encode each object and\napply t0 = 0.4 level Gaussian noise to C to get noisy Ct0,\nand then decode the condition radiance field during training.\nFurthermore, both the ShapNet-Chair and Objaverse\ndataset contains no text prompts, so we use Blip-2 [16]\nto generate labels for the Objaverse object by rendering the\nimage from a positive view. For evaluation, we manually\nchoose 50 text prompts from the Objaverse dataset without\nLVIS label, ensuring the text prompts have not been trained\nduring training.\n5.2. Model Architecture Details\nOur framework contains a 3D denoising network built upon\n3D SparseConv U-Net and a 2D denoising network built\nupon 2D U-Net. Below we provide more details for each of\nthem.\n5.2.1\n3D Denoising Network\nGiven the input feature volume\nSin = Concat(M, Sp3DConv(N),\nSp3DConv(Gt0))\n(8)\nas discussed in Section 3.2 of the main paper, we use a 3D\nsparse U-Net U to denoise the signed distance field. Specifi-\ncally, we first use a 1\u00d71\u00d71 convolution to adjust the number\nof input channels to 128. Then we stack four 3\u00d73\u00d73 sparse\n3D convolution blocks to extract hierarchical features while\nobtaining downsampled 8 \u00d7 8 \u00d7 8 feature grids. It is note-\nworthy that we inject the timestep and text embeddings into\neach sparse convolution block to make the network aware\nof the current noise level and text information. In practice,\nwe first use an MLP to project the scalar timestep t to high-\ndimensional features and fuse it with the text embeddings\nwith another MLP to get the fused embeddings as follows:\nemb = MLP2(Concat(embtext, MLP1(t))),\n(9)\nwhere embtext denotes the text embeddings. Then in each\nsparse convolution block, we project the fused embeddings\nto scale \u03b2 and shift \u03b3:\n\u03b2, \u03b3 = Chunk(MLPproj(GeLU(emb))),\n(10)\nwhere GeLU is activated function, Chunk operation splits\nthe projected features into two equal parts along the channel\ndimension. After that, we introduce modulation to the sparse\nconvolution by:\nSk+1 = (1+\u03b2)(SparseConv(GroupNorm(Sk)))+\u03b3, (11)\nwhere k denotes the feature level, Sk and Sk+1 are the input\nand output of the k-th level sparse convolution block. Subse-\nquently, we use 4 sparse deconvolution blocks to upsample\nthe bottleneck feature grids with residuals linked from the\nextracted hierarchical features:\nS\u2032\nk = SparseDeConv(S\u2032\nk+1) + Sk,\n(12)\nwhere S\u2032\nk+1 and S\u2032\nk are the input and output of the k-th level\nsparse de-convolution block, and obtain the output features\nS of the 3D U-Net.\nTo obtain the denoised signed distance field, we first\nquery each 3D position p in the fused feature grid S to fetch\nits feature S(p) by Trilinear Interpolation. Then we apply\nseveral MLPs (we adopt the ResNetFC blocks in [41]) to\npredict the signed distance at position p:\nF\u2032\n0 = MLP(S(p), \u03bb(p)),\n(13)\nwhere \u03bb(p) is the positional encoding:\n\u03bb(p) =(sin(20\u03c9p), cos(20\u03c9p), sin(21\u03c9p), cos(21\u03c9p),\n..., sin(2L\u22121\u03c9p), cos(2L\u22121\u03c9p)).\n(14)\nL is set to 6 in all experiments.\n1\nFigure 8. More ablation results showing the importance of both 2D and 3D priors in our model.\n2\nA silver platter piled high with fruits.\nA lemur taking notes in a journal.\nAn orangutan playing accordion with its hands spread wide.\nA bear dancing ballet.\nA pig wearing a backpack.\nAn airplane made out of wood.\nA car made out pizza.\nA lionfish.\nA llama wearing a suit.\nFigure 9. More generated 3D objects by our model. Left side shows the diffusion output and right side shows the 3D object after optimization.\n5.2.2\n2D Denoising Network\nOur 2D denoising network contains a U-Net of the 2D foun-\ndation model (DeepFloyd) and a ControlNet [43] modulation\nmodule to jointly denoise the multi-view image set. In prac-\ntice, given the M noisy images Vt =\n\b\nIi\nt\n\tM\ni=1 from the 2D\ndiffusion process and M rendered images\n\b\nHi\tM\ni=1 from the\n3D diffusion process as mentioned in Section 3.3 of the main\npaper, we first reshape both of them from [B, M, C, H, W]\nto [B \u00d7 M, C, H, W], where B, C, H, W denote batch size,\nchannel, height, width, respectively. Then we feed the noisy\nimages to the frozen encoder E\u2217 of DeepFloyd to get en-\ncoded features:\nP = E\u2217(Reshape(\n\b\nIi\nt\n\tM\ni=1), t, embtext).\n(15)\nP =\n\b\npk\tK\nk=1 where pk denotes the k-th features of the\ntotal K feature levels. Simultaneously, we feed the rendered\nimages to the trainable copy encoder E of ControlNet to\nobtain the hierarchical 3D consistent condition features:\nQ = E(Reshape(\n\b\nHi\tM\ni=1), t, embtext),\n(16)\n3\nwhere Q =\n\b\nqk\tK\nk=1. Subsequently, we decode P with the\nfrozen decoder D\u2217 of DeepFloyd and the condition residual\nfeatures Q. Specifically, in the k-th decoding stage, we first\napply zero-convolutions to the condition feature qk and then\nadd it to the original decoded features as residuals:\n\u02c6f k = pk + D\u2217\nk\u22121(pk\u22121) + ZeroConv(qk),\n(17)\nwhere D\u2217\nk\u22121 denotes the k \u2212 1-th frozen decoding layer of\nDeepFloyd. In this way, we can denoise the multi-view noisy\nimages in a unified manner by introducing the 3D consistent\ncondition signal as guidance. In practice, we set M = 8 in\nour experiments.\nPolificDreamer (6h+)\nOur Bidirectional Diffusion Results (40s)\nRefinement Results (20min)\nA lionfish.\nA llama wearing a suit.\nFigure 10. Comparison between our results with the object directly\ngenerated by the optimization method (ProlificDreamer).\n5.3. More Training Details\nWe train our framework on 4 NVIDIA A100 GPUs with a\nbatch size of 4. For ShapeNet-Chair, the training takes about\n8 hours to converge. For Objaverse 40k, the training takes 5\ndays. We use the AdamW optimizer with \u03b2 = (0.9, 0.999)\nand weight decay = 0.01. Notably, we set the learning rate\nof the 2D diffusion model to 2 \u00d7 10\u22126 while using a much\nlarger learning rate of 5 \u00d7 10\u22125 for the 3D diffusion model.\n5.4. More Experiments\n5.4.1\nAblation for Priors\nIn Fig. 8, we provide additional results for the ablation of\n3D and 2D priors mentioned in Sec. 4.3. Our method can\nproduce more realistic textures with 2D priors and more\nrobust geometry with 3D priors.\n5.4.2\nVisualization of 2D-3D Denoising\nWe also demonstrated the visualization of 2D and 3D de-\nnoising processes during bidirectional diffusion sampling as\nshown in Fig. 11. The top two lines show the rendering views\nFigure 11. Visualization of our 2D and 3D denoising processes\n(the maximum diffusion step is 1,000). The top two rows show\nthe rendering views of the implicit field during the 3D denoising\nprocess, and the bottom two rows show the 2D sample results\nduring the 2D denoising process.\nof the implicit field during the 3D denoising process, and\nthe bottom two lines show the 2D sample results during the\n2D denoising process. 3D and 2D representations are jointly\ndenoised, and in the early step of diffusion sampling, 3D\nrepresentations can provide basic geometric shapes, which\nguides 2D diffusion to generate geometrically reasonable\nimages. In the later step of sampling, texture generation is\ndominated by 2D diffusion.\n5.4.3\nMore Results\nIn Fig. 9, we provide more high-quality results generated by\nour entire framework. And in Fig. 10, we demonstrated a\ncomparison with the previous state-of-the-art optimization\nmethod [37]]. Our approach not only significantly reduces\ntime costs but is also more robust in understanding geome-\ntry.\n4\n"
  },
  {
    "title": "Customizing Motion in Text-to-Video Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.04966.pdf",
    "upvote": "8",
    "text": "Customizing Motion in Text-to-Video Diffusion Models\nJoanna Materzy\u00b4nska1*\nJosef Sivic2,3\nEli Shechtman2\nAntonio Torralba1\nRichard Zhang2\nBryan Russell2\n1Massachusetts Institute of Technology\n2Adobe Research\n3CIIRC CTU\njoaanna.github.io/customizing motion/\nInput videos of \u201cV* dance\u201d\n\u201cA futuristic robot mimicking human movements in the V* dance\u201d\nNon-\nhumanoid\n\u201cAn older lady doing the V* dance while jumping\u201d\nMultiple \nmotions\n\u201cNurses dancing the V* dance in a hospital\u201d\nMultiple\nsubjects\nFigure 1. (Left) Given a few examples (\u201cCarlton dance\u201d), our customization method incorporates the depicted motion into a pretrained\ntext-to-video diffusion model using a new motion identifier (\u201cV* dance\u201d). (Right) We generate the depicted motion across a variety of\nnovel contexts, including with a non-humanoid subject (robot), multiple motions (lady), and multiple subjects (group of nurses). To best\nview the results, please view the our website.\nAbstract\nWe introduce an approach for augmenting text-to-video\ngeneration models with customized motions, extending their\ncapabilities beyond the motions depicted in the original\ntraining data. By leveraging a few video samples demon-\nstrating specific movements as input, our method learns\nand generalizes the input motion patterns for diverse, text-\nspecified scenarios. Our contributions are threefold. First,\nto achieve our results, we finetune an existing text-to-video\nmodel to learn a novel mapping between the depicted mo-\ntion in the input examples to a new unique token. To avoid\noverfitting to the new custom motion, we introduce an ap-\nproach for regularization over videos. Second, by leverag-\ning the motion priors in a pretrained model, our method can\n*Work done in part as an intern with Adobe Research.\n3Czech Institute of Informatics, Robotics and Cybernetics at the\nCzech Technical University in Prague.\nproduce novel videos featuring multiple people doing the\ncustom motion, and can invoke the motion in combination\nwith other motions. Furthermore, our approach extends to\nthe multimodal customization of motion and appearance of\nindividualized subjects, enabling the generation of videos\nfeaturing unique characters and distinct motions. Third, to\nvalidate our method, we introduce an approach for quanti-\ntatively evaluating the learned custom motion and perform\na systematic ablation study. We show that our method sig-\nnificantly outperforms prior appearance-based customiza-\ntion approaches when extended to the motion customization\ntask.\n1. Introduction\nRecent advancements in text-to-video synthesis have\nyielded remarkable outcomes [2, 18, 28, 40, 42, 43, 50],\nmarking progress in a traditionally challenging area of\narXiv:2312.04966v1  [cs.CV]  7 Dec 2023\nvideo generation. However, the capabilities of these models\nremain confined to their training data. Conveying specific\nmotions not covered in the training set is challenging. For\nexample, consider the examples of the depicted \u2018Carlton\u2019\ndance shown in Figure 1 (left). To describe the \u2018Carlton\u2019\ndance in a text prompt, it is difficult to describe with natural\nlanguage the nuances of the swinging arms and the timings\nof the poses in the dance. The ability to generate a new mo-\ntion unknown to the model for one or more desired charac-\nters with the flexibility of using text to control the subjects\u2019\nappearance, as shown in Figure 1 (right), can be particu-\nlarly useful for animation or content creation applications.\nThis ability leads us to a question: How can we harness an\nexisting text-to-video model\u2019s knowledge about motion and\nappearance while augmenting them with new, custom mo-\ntions?\nWhile appearance-based customization techniques in\ntext-to-image synthesis have enabled tailored image gener-\nation [9, 22, 32],their scope is confined to still images. This\nsetting introduces a limitation where only object customiza-\ntion is feasible. Recent works have extended a pretrained\ntext-to-image synthesis model with trainable temporal lay-\ners and applied one-shot tuning over a single video [7, 45].\nWhile these approaches enable text-based stylization of the\nsingle video, the generated videos have motions that adhere\nclosely to the timing and spatial layout of the input video\nand cannot generalize to any other motions. As a result,\nfor an input video\u2019s depicted motion, these methods cannot\ncombine it with other motions, apply it to multiple people\nin a scene, or vary its viewpoint/timing.\nIn this work, we present a new approach for customiz-\ning text-to-video diffusion models to a novel motion re-\ngardless of the appearance of the motion\u2019s subject. Given\na few videos depicting a new motion, e.g., dance or ges-\nture, our goal is to map the novel motion to a unique text\ntoken (\u201cV*\u201d). Given the unique text token, we can generate\nvideos with varied subjects undertaking the novel motion\n(cf., Figure 1 right).\nAs our first contribution, our approach goes beyond prior\nwork by leveraging the rich motion and appearance prior in-\nformation contained in a pretrained text-to-video generation\nmodel. We augment this model with new custom motions\nby fine tuning the temporal layers and spatial cross-attention\nkeys/values, which we show are crucial for modeling the\nnovel motion. To maintain the model\u2019s motion prior and\navoid overfitting to the new motion, we introduce a new ap-\nproach for regularization tailored to videos.\nAs our second contribution, we show that our approach\nlearns to generalize and hence we can invoke the custom\nmotion in novel applications. For example, we first show\nhow to apply a custom motion to multiple people in a scene.\nSecond, we show how to invoke the newly learned cus-\ntom motion in combination with other motions. Finally, we\nshow how to perform multimodal customization over mo-\ntion and appearance.\nAs our third contribution, to validate our method, we in-\ntroduce a new approach for quantitatively evaluating the\nfidelity of the generated custom motions along with the\nmodel\u2019s ability to respect the subject\u2019s desired appearance\nas described in a text prompt. We use this evaluation proto-\ncol to perform a systematic ablation of our method and find\nthat we significantly outperform baseline appearance-based\ncustomization approaches when extended to the motion cus-\ntomization task.\n2. Related Work\nText-to-image customization. Previous approaches to cus-\ntomization in text-to-image generation models are focused\non introducing a novel subject or object [5, 6, 9, 10, 16,\n22, 24, 26, 32\u201334, 37, 44, 46, 47, 51]. One strategy fine-\ntunes a subset of the parameters of the generation network\ngiven a few images of a subject and its descriptions [22, 32].\nThrough this optimization, a new text-image mapping is\ncreated between the subject depicted in the images and the\ntextual description. At test time, the subject is invoked via\nthe description but can be now used in a free-form context.\nSimilarly, Gal et al. [9] showed that this mapping can be\ncreated purely by optimizing a text embedding of the tex-\ntual description without modifying the generation network,\nat a price of image quality. More recent approaches investi-\ngate efficient formulations for modifying an approximation\nof the parameters responsible for the text-image mapping\nin the generation network [16, 37]. Other methods, alter-\nnatively to adjusting the model\u2019s parameters, introduce an\nimage encoder that can be directly plugged into the pre-\ntrained network [10, 44, 51]. Alternatively to customizing\nindividual concepts, Huang et al. [20] propose to learn rela-\ntions between two subjects from a set of images via a similar\ntechnique to Gal et al. [9]. The text-to-image customization\nis limited to static objects due to the static nature of still im-\nages. In contrast, we focus on customization with respect to\na given motion depicted in a small set of videos.\nControllable video generation.\nThe study of control-\nlable video generation is widely researched; we summarize\nbriefly here works that are closest to ours. Han et al. [15]\nuse multi-modal input like sketches or semantic layout to\ncontrol the synthesized content. Previous works learn GAN\nrepresentations that disentangle motion and structure [39]\nor decompose foreground and background to manipulate the\nforground only [19]. Chan et al. [4] use a single source\nvideo of a dancing person to generate a novel video depict-\ning the motion transferred to a new subject. Geyer et al.\n[13] propose a method for semantic video editing of real-\nworld videos, using a text-to-image model. Our approach\ndiffers from these prior works in that we learn a general-\nized motion from a few video examples. Then, the motion\n\ud835\udc65!\n\ud835\udc65!\"#\nCustomization\n\u222a\nText\ntransformer\n\u201cVideo of a\nV* dance\u201d\nSample\n+ Add noise\n+ Data augment\nCustomize by tuning Text-to-Video model\nSpatio-temporal Block\nSpatial\nConv\nSpatial\nAttention\nTemporal\nConv\nTemporal\nAttention\nFrozen\nTrainable\nVideo Regularization\nFine-tuning set\nVideo U-Net\n<latexit sha1_base64=\"Fqtw5LO/H04akNJYcJNLfk7WZg=\">AB8XicbVDJSgNBEO2JW4xb1KOXxiB4CjPidgx68RjBLJiMoadTkzTp6Rm7a4Qw5C+8eFDEq\n3/jzb+xsxw08UHB470quoFiRQGXfbyS0tr6yu5dcLG5tb2zvF3b26iVPNocZjGetmwAxIoaCGAiU0Ew0sCiQ0gsH12G8gTYiVnc4TMCPWE+JUHCGVrpvYx+QdczDY6dYcsvuBHSReDNSIjNUO8WvdjfmaQKuWTGtDw3QT9jGgWXMCq0UwMJ4wPWg5alikVg/Gxy8YgeWaVLw1jbUkgn6u+JjEXGDKPAdkYM+2beG4v/ea0Uw0s/E\nypJERSfLgpTSTGm4/dpV2jgKIeWMK6FvZXyPtOMow2pYEPw5l9eJPWTsndePrs9LVWuZnHkyQE5JMfEIxekQm5IldQIJ4o8k1fy5hjnxXl3PqatOWc2s0/+wPn8Ab9VkPs=</latexit>\u2713q\ns\nfDhsVMsuWV3ArpIvBkpkRmqneJXuxvzNAKFXDJjWp6boJ8xjYJLGBXaqYGE8QHrQctSxSIwfja5eESPrNKlYaxtKaQT9fdExiJjhlFgOyOGfTPvjcX/vFaK4aWfCZWkCIpPF4WpBjT8fu0KzRwlENLGNfC3kp5n2nG0YZUsCF48y8vkvpJ2Tsvn92elipXszjy5IAckmPikQtSITekSmqE0WeySt5c4z4rw7H9PWnDOb2Sd/4Hz+AMDbkPw=</latexit><latexit sha1_base64=\"iKWKC+EAaZCyXKEaCVcd7SI/YA=\">AB8XicbVDJSgNBEO2JW4xb1KOXxiB4CjPidgx68RjBLJiMoadTkzTp6Rm7a4Qw5C+8eFDEq3/jzb+xsxw08UHB470quoFiRQGXfbyS0tr6yu5dcLG5tb2zvF3b26iVPNocZjGetmwAxIoaCGAiU0Ew0sCiQ0gsH12G8gTYiVnc4TMCPWE+JUHCGVrpvYx+Qd\n\u2713q\nt\n<latexit sha1_base64=\"zeCn9S6t7M50Ssmlrv+VqT5vARU=\">AB8XicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx6\n8RjBPDBZw+xkNhkyO7vM9AphyV948aCIV/Gm3/jJNmDJhY0FXdHcFiRQGXfbWVpeWV1bL2wUN7e2d3ZLe/sNE6ea8TqLZaxbATVcCsXrKFDyVqI5jQLJm8HwZuI3n7g2Ilb3OEq4H9G+EqFgFK30MEBR9rFR9Ytld2KOwVZJF5OypCj1i\n19dXoxSyOukElqTNtzE/QzqlEwycfFTmp4QtmQ9nbUkUjbvxsevGYHFulR8JY21JIpurviYxGxoyiwHZGFAdm3puI/3ntFMrPxMqSZErNlsUpJgTCbvk57QnKEcWUKZFvZWwgZU4Y2pKINwZt/eZE0TiveReX87qxcvc7jKMAhHMEJeHAJ\nVbiFGtSBgYJneIU3xzgvzrvzMWtdcvKZA/gD5/MHq6OQ7g=</latexit>\u2713c\nt\nsi520uHZaNItluyPQNdJk5GSiRDtVv8avcingQIpdM65Zjx+imTKHgEiaFdqIhZnzI+tAyNGQBaDedXT2hJ0bpUT9SpkKkM/X3RMoCrceBZzoDhgO96E3F/7xWgv6Nm4owThBCPl/kJ5JiRKcR0J5QwFGODWFcCXMr5QOmGEcTVMGE4Cy+vEzq52Xnqnz5cFGq3GZx5MkROSanxCHXpELuSZXUCeKPJNX8mY9WS/Wu/Uxb81Z2cwh+QPr8wfLw5K4</latexit><latexit sha1_base64=\"ji/bePUX/pGR8JDxpYC+JHg48AM=\">AB9XicbVDJSgNBEO2JW4xb1KOXxiB4kDAjbsegF48RzALJPR0apImPQvdNZEw5D+8eFDEq/izb+xk8xBEx8UPN6roqeF0uh0ba/rdzK6tr6Rn6zsLW9s7tX3D+o6yhRHGo8kpFqekyDFCHUKCEZqyABZ6Ehje8m/qNESgtovARxzG4AeuHwhecoZE6bRwA\n\u2713k,v\nt\n<latexit sha1_base64=\"6kWMzuz9PYWAcqNIuEDKAjdCO40=\">AB9XicbVDJSgNBEO2JW4xb1KOXxiB4kDAjbsegF48RzALJPR0apImPQvdNZEw5D+8eFDE\nBP0bNxVhnCEfL7ITyTFiE4joD2hgKMcG8K4EuZWygdMY4mqIJwVl8eZnUz8vOVfny4aJUuc3iyJMjckxOiUOuSYXckyqpEU4UeSav5M16sl6sd+tj3pqzsplD8gfW5w/KOZK3</latexit>q/izb+xk8xBEx8UPN6roqeF0uh0ba/rdzK6tr6Rn6zsLW9s7tX3D+o6yhRHGo8kpFqekyDFCHUKCEZqyABZ6Ehje8m/qNESgtovARxzG4AeuHwhecoZE6bRwAsq7upMOz0aRbLNlewa6TJyMlEiGarf41e5FPAkgRC6Z1i3HjtFNmULBJUwK7URDzPiQ9aFlaMgC0G46u3pCT4zSo36kTIVIZ+rviZQFWo8Dz3QGDAd60ZuK/3mt\n\u2713k,v\ns\n<latexit sha1_base64=\"G3d9/ITqwuK6ECiDR+mfqwMG1YI=\">AB8XicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx6\n8RjBPDBZw+xkNhkyO7vM9AphyV948aCIV/Gm3/jJNmDJhY0FXdHcFiRQGXfbWVpeWV1bL2wUN7e2d3ZLe/sNE6ea8TqLZaxbATVcCsXrKFDyVqI5jQLJm8HwZuI3n7g2Ilb3OEq4H9G+EqFgFK30MEBR9o1j6xbKrsVdwqySLyclCFHrV\nv6vRilkZcIZPUmLbnJuhnVKNgko+LndTwhLIh7fO2pYpG3PjZ9OIxObZKj4SxtqWQTNXfExmNjBlFge2MKA7MvDcR/PaKYZXfiZUkiJXbLYoTCXBmEzeJz2hOUM5soQyLeythA2opgxtSEUbgjf/8iJpnFa8i8r53Vm5ep3HUYBDOIT8OAS\nqnALNagDAwXP8ApvjnFenHfnY9a65OQzB/AHzucPqh2Q7Q=</latexit>\u2713c\ns\n<latexit sha1_base64=\"SEC\ngdI4tEjf6f3QdU39QLB7S0s=\">AB63icbVDLSgNBEOyNrxhfUY\n9eBoPgKeyKr2NQDx4jmAcka5idzCZDZmaXmV4hPyCFw+KePWHv\nPk37iZ70GhBQ1HVTXdXEth0XW/nMLS8srqWnG9tLG5tb1T3t1r\n2igxjDdYJCPTDqjlUmjeQIGSt2PDqQokbwWj68xvPXJjRaTvcRxz\nX9GBFqFgFDPp5kGVeuWKW3VnIH+Jl5MK5Kj3yp/dfsQSxTUySa3\nteG6M/oQaFEzyambWB5TNqID3kmpopbfzK7dUqOUqVPwsikpZH\nM1J8TE6qsHasg7VQUh3bRy8T/vE6C4aU/ETpOkGs2XxQmkmBEs\ndJXxjOUI5TQpkR6a2EDamhDN4shC8xZf/kuZJ1Tuvnt2dVmpXe\nRxFOIBDOAYPLqAGt1CHBjAYwhO8wKujnGfnzXmftxacfGYfsH5+\nAZOQo3F</latexit>Dm\n<latexit sha1_base64=\"/haRT6Y0rYu/ecxtoBED8DQVrY=\"\n>AB6nicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgzqwWNEs0Ayhp5OTdKkp2fo7hHCkE/w4kERr36RN/GznLQxAcFj/eqKoXJIJr4\n7rfTm5peWV1Lb9e2Njc2t4p7u7VdZwqhjUWi1g1A6pRcIk1w43AZqKQRoHARjC4HvuNJ1Sax/LBDBP0I9qTPOSMGivd3zyqTrHklt0\nJyCLxZqQEM1Q7xa92N2ZphNIwQbVueW5i/Iwqw5nAUaGdakwoG9AetiyVNELtZ5NTR+TIKl0SxsqWNGSi/p7IaKT1MApsZ0RNX897Y\n/E/r5Wa8NLPuExSg5JNF4WpICYm479JlytkRgwtoUxeythfaoMzadg3Bm395kdRPyt5+ezutFS5msWRhwM4hGPw4AIqcAtVqAG\nDHjzDK7w5wnlx3p2PaWvOmc3swx84nz8gt42</latexit>Dr\nFigure 2. Overview. Given a small set of exemplar videos, our approach fine-tunes the U-Net of a text-to-video model using a reconstruc-\ntion objective. The motion is identified with a unique motion identifier and can be used at test time to synthesize novel subjects performing\nthe motion. To represent the added motion but preserve information from the pretrained model, we tune a subset of weights \u2013 the temporal\nconvolution and attention layers, in addition to the key & value layers in the spatial attention layer. A set of related videos is used to\nregularize the tuning process.\ncan be used in new contexts with text conditioning, and the\nmotion can be used in an array of new applications where\ntext controls for the number of subjects performing the mo-\ntion, their appearance, and the background. While previous\nworks focus primarily on transferring motion between hu-\nmans, our approach generalizes the motion pattern such that\nit can be applied also to non-human subjects.\nVideo synthesis with image generation models.\nPrior\nwork has used text-to-image generation models for video\ngeneration. Guo et al. [14] learn a motion prior network\nwith text-video data and leverage the prior in personalized\ntext-image models. Similarly, prior work has shown video\ngeneration with text-to-image models [21, 35, 48]. Ge et al.\n[12] demonstrates an approach for zero-shot text-to-video\ngeneration by designing a video noise prior. Our work dif-\nfers in that we use a video generation model directly to learn\nnew motions. Wu et al. [45] inverts an existing video frame-\nby-frame using a pre-trained text-to-image model then, us-\ning the inverted noise maps and text guidance, synthesizes a\nnovel video. This method edits an existing video, while our\nfocus is on generating completely new videos from scratch.\nText-to-video models.\nRecently, several methods have\ndemonstrated text-to-video generation [2, 18, 28, 40, 42,\n43, 50]. An important line of research involves studying\narchitectures and optimization for training video genera-\ntion models. A high-quality model is a prerequisite for our\nmethod. In this work, we use the model from Wang et al.\n[42]. Molad et al. [28] show qualitative results of adding a\ncustom subject to the base video model. They, however,\ndo not explore customization beyond static objects [28].\nThe idea of multi-modal customization, however, facilitates\nuser-controllability and creative applications, since no mat-\nter how powerful the model, there are always novel subjects\nand motions that cannot be all contained in the training set.\n3. Approach\nWe aim to customize text-to-video models with respect to a\nnovel motion. That is, we wish to introduce novel dynamic\nconcepts in the model\u2019s knowledge base and associate them\nwith unique text tokens. In this section, we first give a pre-\nliminary description of text-to-video diffusion models, fol-\nlowed by our approach for motion customization.\n3.1. Text-to-Video Diffusion Model Preliminaries\nDiffusion models are probabilistic models that can approx-\nimate distributions through a gradual denoising process\n[17, 36]. Given a Gaussian noise sample, the model learns\nto iteratively remove a portion of noise until a sample is\nwithin the approximated distribution. The objective func-\ntion of this process minimizes the L2 distance of the pre-\ndicted noise and the sampled noise. Latent Diffusion Mod-\nels (LDMs) [31] are diffusion models operating in the latent\nspace instead of pixel space. Instead of learning the de-\nnoising process in a complex high-dimensional space, the\nvideos are first encoded into a lower-dimensional vector\nthrough a learned encoder-decoder model.\nSpecifically, a video x is represented with a latent vector,\nand a text condition c is encoded through a text-embedding\nfrom a pre-trained text encoder. The initial noise map \u03f5 is\nsampled from a Gaussian distribution \u03f5 \u223c N(0, 1). For\nthe diffusion timestep t sampled from a probability density\nt \u223c f(t), the noisy latent sample can be computed from the\nnoise prediction by xt = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1t\u03f5, where x0 is\nthe original video and \u03b1t controls the amount of noise added\nat each diffusion timestep according to a noise scheduler\n[17, 36]. The model \u03f5\u03b8 with parameters \u03b8 is trained with the\nfollowing weighted denoising loss,\nL\u03b8(x, c) =\nE\n\u03f5\u223cN (0,1)\nt\u223cf(t)\n[wt\u2225\u03f5\u03b8(x, \u03f5, c, t) \u2212 \u03f5\u22252\n2],\n(1)\nwhere wt is a user-defined variable that controls the sample\nquality. At inference time, a new video can be generated\nby sampling a Gaussian noise sample \u03f5 and a text prompt,\ndenoising them with the learned model.\n3.2. Approach for Motion Customization\nWe illustrate our overall approach for motion customization\nin Figure 2. Let the motion be represented through a small\nexemplar set of videos and corresponding text prompts\nDm = {(xm, cm)}. The motion can be performed by dif-\nferent subjects across different backgrounds, and the com-\nmonality within the videos is purely the dynamic move-\nment. We choose a generic textual description across the\nvideos, such as \u201ca person doing the V*\u201d. In practice, we se-\nlect unique tokens like \u201cpll\u201d as \u201cV*\u201d. To customize a text-\nto-video model\u2019s parameters \u03b8, we fine-tune the model\u2019s pa-\nrameters by minimizing the diffusion objective L\u03b8 summed\nover the exemplar set Dm,\nmin\n\u03b8\nX\n(x,c)\u223cDm\nL\u03b8(x, c).\n(2)\nAt test time we can generate novel videos of the target with\nvariations controlled by text. Next, we describe the impor-\ntant components of our motion customization approach.\nChoice of customization parameters.\nThe quality and\ngeneralization ability of the novel motion depends on the\nchoice of the model parameters updated during customiza-\ntion. A text-to-video diffusion model \u03f5\u03b8 has parameters \u03b8\nthat can be categorized to those operating on the temporal\ndimensions \u03b8t \u2282 \u03b8 and those operating on the spatial di-\nmensions (per-frame) \u03b8s \u2282 \u03b8. Let \u03b8k,v\ns\n\u2282 \u03b8s be the keys and\nvalues parameters of the spatial cross-attention modules.\nThe temporal layers \u03b8t are transformers and tempo-\nral convolutions, and are responsible for modelling tem-\nporal patterns across frames.\nIn Section 4, we empiri-\ncally show that the temporal layers alone do not effectively\nmodel a new motion pattern due to time-varying appear-\nance changes. For example, consider an object rotating in\n3D, which requires the model to generate the appearance of\ndisoccluded surfaces.\nTo faithfully learn a motion pattern, we also modify a\nsubset of parameters in the spatial layers of the model. As\nillustrated in Figure 2, our approach fine tunes the spatial\nkeys/values \u03b8k,v\ns\nand temporal \u03b8t parameters. Note that in\nimage customization [22, 38] and model editing [11], it has\nbeen shown that the spatial keys and values of the cross-\nattention modules in text-to-image models are sufficiently\npowerful to represent the appearance of new concepts. Our\nexperiments also show the effectiveness of adjusting those\nparameters for the motion customization task.\nVideo regularization. Prior work has shown that directly\noptimizing Equation (2) leads to forgetting related concepts\nor the concept category [23, 25, 32]. For example, if the\nconcept is a specific person, all people start resembling that\nperson. To mitigate this issue, prior work has utilized a reg-\nularization set that ensures that this knowledge is preserved.\nFor example, Ruiz et al. [32] proposed collecting a regular-\nization set via generation from the original model. Kumari\net al. [22] proposed using pairs of real images and text.\nIn contrast to prior work, we seek to mitigate forgetting\nof related motions that the model has already learned. To\naddress this goal, we consider a video-based regularization.\nLet Dr be a regularization set of paired videos with text de-\nscriptions that have similar but not identical motion to the\ntarget custom motion videos Dm. For example, when learn-\ning the \u2018Carlton\u2019 dance, we chose a regularization set of real\nvideos containing people dancing. For customization, we\noptimize the diffusion objective L\u03b8 over the target custom\nvideos Dm and regularization set Dr:\nmin\n\u03b8\nX\n(x,c)\u223cDm\u222aDr\nL\u03b8(x, c).\n(3)\nEmpirically, we show in Section 4 that using real videos for\nthe regularization set is superior to using generated videos\nfrom the model. We find that using generated videos de-\ngrades the quality of the customized model substantially.\nLearning the motion patterns. To facilitate learning the\ncommon motion in Dm, during the training process, we aim\nto put emphasis on the motion pattern of the videos, rather\nthan the appearance or subject presenting the motion. For\nexample, in the case of the \u2018Carlton\u2019 dance in Figure 1 we\nwish to capture the motion pattern of the dance rather than\nthe appearance of the background or the individual perform-\ners. The denoising process in diffusion models samples a\nGaussian noise and then gradually removes the noise. The\ninitial noise as well as early denoising steps have large in-\nfluence on the output overall dynamic structure of the video,\nwhereas the later stages correspond to finer details [41].\nTo focus on the dynamic structure of the videos and de-\nemphasize the appearance of the subjects performing the\nmotion, we define a timestep sampling strategy. In particu-\nlar, we build on [20] who develop a non-uniform sampling\nstrategy for generating still images. Here we adopt it for\ngenerating dynamic video content. In detail, instead of uni-\nformly sampling the denoising steps in Equation (1), we\ndefine a probability distribution f\u03b1(t) = 1\nT (1 \u2212 \u03b1 cos( \u03c0t\nT ))\nover the timesteps that focuses more on the earlier denoising\nsteps and hence emphasizes learning the overall dynamic\nmotion pattern rather than the fine appearance details of the\nbackground or the individual subjects. The \u03b1 parameter in-\ncreases the skewness of the function. We use \u03b1 = 0.5 for\nall of our experiments.\nCustomizing appearance and motion. Our method can\nbe easily extended to customizing both appearance and\nmotion. The appearance of a new subject is represented\nthrough an exemplar dataset Ds of pairs of still images and\ncaptions describing the images. To learn the appearance of\nMotion accuracy \u2191\nText\nAlignment \u2191\nTextual Inversion [9]\n0.3\n0.2733\nCustom Diffusion [22]\n10.5\n0.2788\nDreambooth [32]\n28.4\n0.2796\nOurs\n70.6\n0.2818\nTable 1. Quantitative comparison with image customization\nmethods applied to the motion customization task.\na person we follow [22] and update the spatial layers of the\nmodel, and optimize the text embedding associated with our\nsubject. We customize the model first with the motion, then\ntrain both the appearance and the subject jointly, choosing\nan example of either dataset in the mini-batch. When learn-\ning the subject we treat the images as a one-frame video and\nonly update the selected spatial layers.\n4. Experiments\nIn this section, we quantitatively evaluate our approach\n(Section 4.1), perform an ablation study (Section 4.2), and\nshow qualitative results (Section 4.3).\nImplementation details.\nIn all our experiments we use\nthe ZeroScope text-to-video diffusion model [3] as the pre-\ntrained network.\nOur implementation is building on the\npublic repository [8]. In our concept exemplar dataset Dm,\nwe apply spatial and color augmentation on the video ex-\namples. We train the model with a mini-batch size 1 on a\nsingle A100 Nvidia GPU. We train the models up to 3000\ntraining steps with learning rate 5 \u00d7 10\u22126.\n4.1. Quantitative Evaluation\nWe quantitatively evaluate our approach by computing met-\nrics corresponding to the quality of the generated motion\nand the overall fidelity of the generated video with respect\nto the input text prompt.\nDataset. We quantitatively evaluate our approach on the\nJester Dataset [27]. The dataset contains 148,092 crowd-\nsourced videos of 27 human gestures by diverse actors per-\nforming a gesture in front of a static camera. We select\n5 gesture classes as our target motions \u2013 {Swiping Down,\nDrumming Finders, Rolling Hand Forward, Shaking Hand,\nSliding Two Fingers Up}. From each class, we choose at\nrandom 10 videos from the validation set as our training\nset.\nTo learn the motion, we caption the videos with a\ngeneric text description \u201ca person doing a V* sign\u201d. As\na regularization set for the motion learning, we also choose\n50 videos from each of the dataset classes {Doing Other\nThings, No gesture} and assign them text captions \u201ca person\ndoing other things\u201d and \u201ca person in front of a computer\u201d.\nWe design a test set containing 100 text prompts that de-\ntail a person\u2019s appearance, for example \u201cA female firefighter\ndoing the V* gesture\u201d or \u201cA Japanese toddler doing the V\nTextual Inversion\nCustom Diffusion\nDreambooth\nOurs\nFigure 3. Visual comparison with image customization meth-\nods. Examples of learning a customized motion Sliding Two Fin-\ngers Up from the Jester dataset with prompt \u201cA female firefighter\ndoing the V* sign\u201d.\nImage personalization methods (top three\nrows) fail to capture the motion and produce a temporally coherent\nvideo. Please see supplemental video.\ngesture\u201d. We fix 3 random seeds per text prompt to evaluate\nour experiments.\nMotion accuracy metric.\nThe Jester dataset has been\nwidely adopted for the task of action recognition. We lever-\nage this for our motion accuracy metric, measuring gesture\nrecognition accuracy with a video classifier, pre-trained on\nthe Jester training set [49]. The classifier achieves 94.78%\naccuracy across all gesture classes, making it suitable for\nour evaluation. Note that the simple test set prompts typ-\nically result in generated videos that adhere to the spatial\nlayout of the gesture recognition classifier\u2019s training videos\nwith a single person, making them in-distribution.\nText alignment metrics. To regulate overfitting, we mea-\nsure how faithfully the model adheres to the appearance de-\nscribed in the text prompt with an appearance score defined\nas the mean CLIP score between the generated frames and\nthe part of the text prompt describing the person\u2019s appear-\nance (e.g., \u201cA female firefighter\u201d). We empirically find that\nthe appearance score is a good proxy to determine when\nthe model starts to overfit. For a fair comparison across\nall models, we fine-tune each model until this appearance\nscore reaches a chosen value (here 0.265), determined em-\npirically based on the visual quality of the results. Please\nrefer to the Supplemental Material for an additional discus-\nsion on the tradeoff between motion accuracy and appear-\nance alignment.\nTo assess how well the generated video corresponds to\nthe overall text prompt (both motion and appearance), we\nmeasure the text alignment score as a mean CLIP score be-\nSpatial layers\nAccuracy \u2191\nCopy \u2193\nNone\n62.3\n5.9\nAll\n68.7\n17.0\nK,V (Ours)\n70.6\n8.7\n(a) Spatial layers\nTrans. layers\nAccuracy \u2191\nCopy \u2193\nNone\n22.7\n1.4\nAll (Ours)\n70.6\n8.7\n(b) Temporal layers\nText Token\nAccuracy \u2191\nCopy \u2193\n\u2713\n75.5\n22.7\n\u2717 (Ours)\n70.6\n8.7\n(c) Text Token\nSampling\nAccuracy \u2191\nCopy \u2193\nUniform\n66.9\n15.4\nCoarse-noise (Ours)\n70.6\n8.7\n(d) Sampling Strategy\nPrior\nAccuracy \u2191\nCopy \u2193\nNone\n43.9\n1.2\nSynthetic\n48.3\n3.7\nReal (Ours)\n70.6\n8.7\n(e) Regularization\nTable 2. Quantitative results of the ablation study. Each table examines the design choices of our method. We report the motion recog-\nnition accuracy (\u201cAccuracy\u201d) obtained with a pre-trained classifier for gesture recognition. The copying score (\u201cCopy\u201d) is the percentage\nof generated videos with an image copy detection score higher than a specified threshold.\ntween the video frames and a full caption written in natural\nlanguage. For example, for the class Swiping Down and a\ntext prompt \u201cA female firefighter doing the V* gesture\u201d, a\nfull caption would be \u201cA female firefighter swiping hand\ndown.\u201d We use this score as another criterion to compare\ndifferent methods.\nComparison with image customization methods.\nWe\nselect three well-established image customization ap-\nproaches: Textual Inversion [9], Dreambooth [32] and Cus-\ntom Diffusion [22], and adapt them to the motion cus-\ntomization task. Textual Inversion [9] optimizes a text to-\nken embedding to learn a novel subject. For Dreambooth\n[32], we optimize all spatial layers of the model and use\na synthetic video regularization set following their prior\npreservation loss. Custom Diffusion trains the key and value\nprojection matrices in the cross-attention layers of text-to-\nimage diffusion models, optimizes a text token embedding,\nand uses a regularization set containing real videos. We\nreport the motion accuracy and text alignment metrics in\nTable 1 and show qualitative comparison in Figure 3. We\nobserve that Textual Inversion completely fails to learn the\nmotion. We hypothesize that this failure is because the text\nencoder has been trained only on image-text pairs, and the\nembedding fails to generalize to the video domain. Addi-\ntionally, the text embedding is injected only into spatial lay-\ners in the cross-attention layers, and not the temporal ones.\nAlternatively, because we are learning an unseen motion\npattern, which is more complex than a static object, the em-\nbedding does not have enough parameters to learn it. On the\nother hand, Dreambooth and Custom Diffusion learn to ad-\nhere to the spatial structure of the training videos and some\nhand manipulation. However they fail to accurately repro-\nduce the motion and produce a temporally smooth video. In\ncontrast, our approach yields over a 2\u00d7 improvement in mo-\ntion accuracy and more faithfully adheres to the text prompt.\n4.2. Ablation study\nWe perform an ablation study of our method\u2019s components\nand report the motion accuracy metric in Table 2. In addi-\ntion, we report a copying score (\u201cCopy\u201d) that measures how\nmuch of the training data appearance leaks into the gener-\nated data. We use the SSCD description score [29] to detect\nimage copying between the training videos and the gener-\nated videos. We compute our copying score as the maxi-\nmum SSCD description score between the generated frames\nof the video and first frames of the training examples, and\nreport the percentage of samples that are above a specified\nthreshold. For our experiments, we manually determine a\nthreshold of 0.25, which corresponds to significant copying\nof the appearance of the person or the background (please\nsee examples in the Supp. Mat.).\nFirst, we study the choice of the optimized parameters.\nAs in the case of the image customization methods, it is\nclear that optimizing the spatial layers of the model is not\nenough to learn a novel motion pattern, as shown in Table\n2b. We can further observe that training the key and value\nprojections in the cross-attention layers of the spatial layers\nachieves a good balance in terms of accuracy and copying\nscore, as shown Table 2a. We observe a two-fold reduction\nin the copying score when training only those parameters\nand an 8% improvement in motion accuracy compared to\nnot training any spatial parameters.\nNext, in Table 2c, we consider the role of optimizing\nthe text token embedding.\nWe notice that when jointly\ntraining the text token embedding with the model\u2019s parame-\nters, the motion accuracy is higher than when training only\nthe model parameters.\nHowever, it also leads to nearly\nthree times as much memorization of the training video ap-\npearance, as evidenced by the significantly increased Copy\nscore. We hypothesize that this indicates that the text token\nembedding is learning something related to the appearance\nof the training videos.\nConsequently, we test our sampling strategy. We com-\npare a model trained with our motion pattern sampling strat-\negy (\u201cCoarse-noise\u201d) to a model that simply samples diffu-\nsion time steps uniformly (\u201cUniform\u201d). As shown in Table\n2d, our sampling strategy improves the motion accuracy and\nreduces the appearance copying. Following our intuition\nregarding learning motion patterns rather than fine-grained\ndetails, our model is less prone to replicating elements from\nthe training examples.\nLastly, we study the effect of the regularization prior\n\u201cTeachers doing the V*\u201d\n\u201cA chef in white apron doing the V*\u201d\n\u201cA pirate doing the V*\u201d\n\u201cA close-up of a nun doing the V*\u201d\nDab\n\u201cA camera V* around a banana\u201d\n\u201cA camera V* around a teddy bear in a toy shop\u201d\nAir quotes\n3D rotation\nInput videos\nOur motion customizations\nFigure 4. Qualitative results of our method. We demonstrate two custom motions: Dab and Air quotes, trained using collected internet\nexamples as well as a 3D camera rotation trained with examples from the CO3D dataset [30]. Our method can generalize to unseen subjects\nand multiple people performing the action. Please refer to our supplemental video to view these results.\nin Table 2e. We train three models with the same design\nchoice as our method, yet we choose the regularization set\nto be either: (i) empty, (ii) a synthetic set of videos gen-\nerated from the original text-to-video model, or (iii) con-\ntaining real videos. We observe a significant drop in perfor-\nmance both without using a prior as well as using a synthetic\nprior. Interestingly, despite all settings using the same fine-\ntuning set, a high-quality regularization set containing real\nvideos improves the overall training. Overall, our method\nachieves a good balance between high motion accuracy and\nlow copying score.\n4.3. Qualitative experiments\nTo further demonstrate the effectiveness of our approach,\nwe choose the motions: Dab, Airquotes, and 3D rotation,\nwhich the original model does not know or cannot reliably\nperform. For the first two motions, we curate videos from\nthe internet searching for the description of the motion. We\nuse between 5 and 6 videos for each motion and caption the\nvideos with a generic text description such as \u201cA person do-\nTune-A-Video\nOurs\n\u201cA clown doing the V* gesture while eating a burger with his other hand\u201d\n\u201cAn elder woman doing the V* gesture very slowly and precisely\u201d\n\u201cChildren doing the V* gesture in the classroom in front of a blackboard\u201d\n\u201cA clown doing the V* gesture while eating a burger with his other hand\u201d\n\u201cAn elder woman doing the V* gesture very slowly and precisely\u201d\n\u201cChildren doing the V* gesture in the classroom in front of a blackboard\u201d\nFigure 5. Tune-A-Video versus our method trained on few examples of a custom motion \u201cShaking Hand\u201d. Our method seamlessly\nrenders a custom motion in novel scenarios. Despite the videos in the training set only showing a single actor doing one single motion in\nthe same way, our method can generate the custom motion in conjunction with a different motion (\u201cdoing the gesture while eating a burger\nwith other hand\u201d). Our method can also vary the timing of the motion (\u201cdoing the gesture very slowly and precisely\u201d) and involve multiple\npeople (\u201cchildren\u201d). On the contrary, Tune-A-Video fails to generalize to these novel scenarios or produce a temporally coherent video.\nAppearance\nMotion\n\u201cA X* man doing the V* sign\u201d\n\u201cA photo of a X* man\u201d\n\u201cA person doing the V* sign\u201d\nFigure 6. Appearance and motion customization. Given few\nimages of a person and few videos of a custom motion (top), our\nmethod can generate a novel video depicting that person doing the\nnovel motion (bottom).\ning the V* sign\u201d or \u201cA person V*\u201d. To learn 3D rotation,\nwe select 10 sequences of distinct objects from the CO3D\nDataset [30] and caption the videos with the text prompt \u201cA\ncamera V* around an [object]\u201d, where [object] is an object\nclass. The results are shown in Figure 4. For visualiza-\ntion, we query for objects not present in the training videos.\nFor the regularization set, we select 100 video and text pairs\nfrom the WebVid-10M Dataset [1]. We fine-tune the models\nfor 2000 steps. Please see our companion webpage for ad-\nditional results and comparison with the pre-trained model.\nMotion and appearance customization.\nWe combine\nthe appearance of a specific person with a gesture motion\nDrumming Fingers from the Jester Dataset.\nWe use the\npublicly available images of a particular person from the\nCustomConcept101 Dataset [22], and annotate them with\ngeneric text captions {\u201cA X* man\u201d, \u201cA close-up of a X*\nman\u201d, \u201cA X* man smiling.\u201d}. For the motion dataset, we\nchoose the same 10 videos as in Section 4.1 and a generic\ntext description \u201ca person doing a V* sign\u201d. Our qualitative\nresults are shown in Fig. 6.\nComparison with a reference-based method.\nWe com-\npare our method with Tune-A-Video [45], a video editing\nmethod that, given a source video, inflates a text-to-image\nmodel to learn a motion pattern from a single video. This\nmethod allows generation of novel videos with the same\npattern but varying appearance. We adapt Tune-A-Video to\nfine-tune from multiple videos using the same training set as\nour setup. In Figure 5 we compare the two methods. We test\nthe generalization abilities of the two methods, firstly com-\nbining the novel motion with a different motion: \u201cdoing the\nV* gesture while eating a burger with other hand\u201d. We can\nsee that our method can generate the two motions. Next, we\ntest the motion variation. We show that we can manipulate\nthe execution of the motion with a natural language descrip-\ntion; here we specify \u201cdoing the V* gesture very slowly and\nprecisely\u201d. Lastly, we show that our method can generalize\nto multiple people doing the motion while preserving the\ntemporal consistency of the video. The Tune-A-Video base-\nline produces qualitatively worse results, as can be seen in\nFigure 5 and the supplementary video.\n5. Conclusion\nWe present an approach for motion customization in\ntext-to-video diffusion models.\nOur method can learn a\nnew motion from a set of few exemplar videos of different\nsubjects performing the same gesture.\nWe conduct a\nthorough ablation study that identifies the key components\nof the method and evaluate them with respect to motion\naccuracy. We demonstrate qualitative results of our method,\nspecifically in scenarios where our method generalizes\nthe motion to unseen actors, multiple people perform-\ning the motion, and different viewpoints.\nAdditionally,\nwe show how our method can be extended to multi-\nmodal motion and appearance customization to generate\nvideos with custom subjects performing custom motions.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021. 8\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 1, 3\n[3] cerspense. zeroscope v2 xl. https://huggingface.\nco/cerspense/zeroscope_v2_XL/, 2023. 5\n[4] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A\nEfros.\nEverybody dance now.\nIn Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 5933\u20135942, 2019. 2\n[5] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,\nYuwei Zhou, and Wenwu Zhu.\nDisenbooth:\nIdentity-\npreserving disentangled tuning for subject-driven text-to-\nimage generation. arXiv preprint arXiv:2305.03374, 2023.\n2\n[6] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 2\n[7] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 2\n[8] ExponentialML. Text-to-video-finetuning, 2023. Text-To-\nVideo-Finetuning. 5\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2, 5, 6\n[10] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 2\n[11] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna\nMaterzy\u00b4nska, and David Bau. Unified concept editing in dif-\nfusion models. IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, 2024. 4\n[12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 22930\u201322941, 2023. 3\n[13] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 2\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3\n[15] Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbi-\neri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, and\nSergey Tulyakov. Show me what and tell me how: Video\nsynthesis via multimodal conditioning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3615\u20133625, 2022. 2\n[16] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 2\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[19] Jiahui Huang, Yuhe Jin, Kwang Moo Yi, and Leonid Si-\ngal. Layered controllable video generation. arXiv preprint\narXiv:2111.12747, 2021. 2\n[20] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan,\nand Ziwei Liu. ReVersion: Diffusion-based relation inver-\nsion from images. arXiv preprint arXiv:2303.13495, 2023.\n2, 4\n[21] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3\n[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2, 4, 5, 6, 8\n[23] Jason Lee, Kyunghyun Cho, and Douwe Kiela.\nCounter-\ning language drift via visual grounding.\narXiv preprint\narXiv:1909.04499, 2019. 4\n[24] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 2\n[25] Yuchen\nLu,\nSoumye\nSinghal,\nFlorian\nStrub,\nAaron\nCourville, and Olivier Pietquin. Countering language drift\nwith seeded iterated learning. In International Conference\non Machine Learning, pages 6437\u20136447. PMLR, 2020. 4\n[26] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 2\n[27] Joanna Materzynska, Guillaume Berger, Ingo Bax, and\nRoland Memisevic. The jester dataset: A large-scale video\ndataset of human gestures. In Proceedings of the IEEE/CVF\ninternational conference on computer vision workshops,\npages 0\u20130, 2019. 5\n[28] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 1, 3\n[29] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya\nGoyal, and Matthijs Douze. A self-supervised descriptor for\nimage copy detection. Proc. CVPR, 2022. 6, 2\n[30] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation of\nreal-life 3d category reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10901\u201310911, 2021. 7, 8\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. 2022 ieee. In CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 10674\u201310685, 2021. 3\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 4, 5, 6\n[33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023.\n[34] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2\n[35] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 3\n[36] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3\n[37] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion.\nIn ACM SIGGRAPH 2023 Conference Proceedings,\npages 1\u201311, 2023. 2\n[38] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion.\nIn ACM SIGGRAPH 2023 Conference Proceedings,\n2023. 4\n[39] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocogan: Decomposing motion and content for\nvideo generation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1526\u20131535,\n2018. 2\n[40] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n1, 3\n[41] Binxu Wang and John J Vastola. Diffusion models gener-\nate images like painters: an analytical theory of outline first,\ndetails later. arXiv preprint arXiv:2303.02490, 2023. 4\n[42] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n3\n[43] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv preprint arXiv:2305.10874, 2023. 1, 3\n[44] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2\n[45] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 2, 3, 8\n[46] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 2\n[47] Jianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang\nChen, and Junbo Zhao.\nControllable textual inversion\nfor personalized text-to-image generation.\narXiv preprint\narXiv:2304.05265, 2023. 2\n[48] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 3\n[49] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-\nralba. Temporal relational reasoning in videos. European\nConference on Computer Vision, 2018. 5\n[50] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 1, 3\n[51] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhanc-\ning detail preservation for customized text-to-image gen-\neration:\nA regularization-free approach.\narXiv preprint\narXiv:2305.13579, 2023. 2\nCustomizing Motion in Text-to-Video Diffusion Models\nSupplementary Material\nThis supplemental contains the following material:\n\u2022 Accompanying webpage of qualitative results of our\nmethod, the videos shown in the main paper and com-\nparison with the pre-trained text-to-video generations.\n\u2022 (Section A) We provide additional implementation details\nregarding our quantitative experiments.\n\u2022 (Section B) We discuss the tradeoff between the motion\naccuracy, appearance alignment and copying score across\ntraining iterations.\n\u2022 (Section C) We show qualitative examples along with\ntheir copying score.\n\u2022 (Section D) We show the original model\u2019s (i.e. before\ncustomization) performance on the Jester dataset.\nA. Implementation Details\nAs described in Section 4.1 we design a test set of 100\nprompts detailing a person\u2019s appearance and fix three ran-\ndom seeds per prompt. We show the list of the prompts in\nFigure 9.\nB. Tradeoff between motion accuracy, appear-\nance alignment, and copying score\nWhen customizing a model with a new motion, we observe\na natural bias-variance tradeoff. The longer we train the\nmodel, the better motion accuracy becomes, we memorize\nelements of the training examples, and the model becomes\nless diverse. We illustrate this in Figure 7. With more train-\ning iterations, the network memorizes the gesture as well\nas visual elements of the training examples. These find-\nings are indicated by both the motion accuracy and copying\nscore increasing. On the other hand, the CLIP appearance\nscore, decreases. The CLIP appearance is defined as an av-\nerage CLIP score between the part of the text prompt that\ndescribes a person\u2019s appearance and the video frames (i.e.\nin a prompt \u201cA female firefighter doing the V* sign\u201d, the\nCLIP appearance score accounts for \u201cA female firefighter\u201d).\nThis score is a proxy for visual diversity, since our test set\ncontains visual description of a person, it measures how ac-\ncurate the description is. As we increase the number of it-\nerations, the videos become less diverse, hence the score\ndecreases. This trend is persistent across the models from\nour ablation study. To find a balance between those factors,\nwe perform early stopping for all models when the CLIP\nappearance score reaches 0.265 (i.e. when the image still\nreasonably well corresponds to the text description).\nTraining steps\n30\n40\n50\n60\n70\n80\n90\nMotion Accuracy\nTraining steps\n0.2500\n0.2525\n0.2550\n0.2575\n0.2600\n0.2625\n0.2650\n0.2675\n0.2700\nCLIP Appearance Score\n1000 1250 1500 1750 2000 2250 2500 2750 3000\nTraining steps\n0\n5\n10\n15\n20\n25\n30\nCopying Score\nFigure 7. Our model\u2019s performance in terms of motion accuracy\n(top), CLIP appearance score (middle), and copying score (bot-\ntom) across training steps (x-axis).\nC. Copying score\nCopying score, as defined in 4.2 measures how much of the\ntraining data appearance leaks into the generated data. The\nscore corresponds to the percentage of examples in the test\nset that are above a specified threshold of the SSCD score\nwith any of the training examples. We find that the SSCD\ncopy detection score [29] is a good proxy for determining\nthe memorization of the training examples\u2019 appearance. We\nmeasure the SSCD detection as a maximum score between\nany frame of the training videos and any video frame from\nthe test set. We empirically determine that pairs of videos\nwith a score greater than 0.25 share significant visual sim-\nilarities. As seen in Figure 8, the similarities can be either\nthe background or the appearance of the person performing\nthe gesture.\nFigure 8.\nExamples of pairs of generated frames and training\nframes with SSCD score 0.25. Notice the similarity in the back-\nground (top-left and bottom-left) and foreground (t-shirt logo in\nthe top-right, person\u2019s face in the bottom-right).\nD. Pre-trained model\u2019s performance on mo-\ntions from the Jester dataset.\nTo test whether the gestures from the Jester dataset, are\nwell-represented in the pre-trained text-to-video model, we\nevaluate the model with a similar experiment as our ablation\nexperiment (4.2). We query the pre-trained text-to-video\nmodel with the prompts from the test set (Section A), re-\nplacing the \u201cdoing the sks gesture\u201d with a natural language\ndescription of the class, e.g., for the class \u201cDrumming Fin-\ngers\u201d - \u201cdrumming finger\u201d.\nWe then classify the videos\nwith the pre-trained network trained on the Jester dataset.\nNo videos were correctly classified because (i) the gestures\nare not well represented in the model and (ii) the generated\nvideos do not adhere to the spatial assumption of the Jester\ndataset (where a single person is performing a gesture in\nfront of a camera).\nA woman doing\nthe\nsks\nsign .\nA t r i b e\nman doing\nthe\nsks\nsign .\nA young woman doing\nthe\nsks\nsign .\nA korean\ng i r l\ndoing\nthe\nsks\nsign .\nA korean\nboy doing\nthe\nsks\nsign .\nA korean man doing\nthe\nsks\nsign .\nA korean woman doing\nthe\nsks\nsign .\nA korean\nt o d d l e r\ndoing\nthe\nsks\nsign .\nA j ap a n e se woman doing\nthe\nsks\nsign .\nA j ap a n e se\nboy doing\nthe\nsks\nsign .\nA j ap a n e se\ng i r l\ndoing\nthe\nsks\nsign .\nA j ap a n e se\nt o d d l e r\ndoing\nthe\nsks\nsign .\nA j ap a n e se man doing\nthe\nsks\nsign .\nA f i r e f i g h t e r\ndoing\nthe\nsks\nsign .\nA female\nf i r e f i g h t e r\ndoing\nthe\nsks\nsign .\nA clown\ndoing\nthe\nsks\nsign .\nMickey Mouse doing\nthe\nsks\nsign .\nA housemaid\ndoing\nthe\nsks\nsign .\nA female\nnurse\ndoing\nthe\nsks\nsign .\nA male\nnurse\ndoing\nthe\nsks\nsign .\nA t o d d l e r\ndoing\nthe\nsks\nsign .\nAn a f r i c a n woman doing\nthe\nsks\nsign .\nAn a f r i c a n man doing\nthe\nsks\nsign .\nAn a f r i c a n\nboy doing\nthe\nsks\nsign .\nAn a f r i c a n\nt o d d l e r\ndoing\nthe\nsks\nsign .\nAn e l d e r l y woman doing\nthe\nsks\nsign .\nAn e l d e r l y man doing\nthe\nsks\nsign .\nA mexican man doing\nthe\nsks\nsign .\nA mexican woman doing\nthe\nsks\nsign .\nA mexican boy doing\nthe\nsks\nsign .\nA mexican\ng i r l\ndoing\nthe\nsks\nsign .\nA mexican\nt o d d l e r\ndoing\nthe\nsks\nsign .\nA s l a v i c\nman doing\nthe\nsks\nsign .\nA s l a v i c woman doing\nthe\nsks\nsign .\nA s l a v i c\nboy doing\nthe\nsks\nsign .\nA s l a v i c\ng i r l\ndoing\nthe\nsks\nsign .\nA s l a v i c\nc h i l d\ndoing\nthe\nsks\nsign .\nA blond woman doing\nthe\nsks\nsign .\nA blond man doing\nthe\nsks\nsign .\nA blond boy doing\nthe\nsks\nsign .\nA blond\ng i r l\ndoing\nthe\nsks\nsign .\nA ginger woman doing\nthe\nsks\nsign .\nA ginger man doing\nthe\nsks\nsign .\nA ginger\nboy doing\nthe\nsks\nsign .\nA ginger\ng i r l\ndoing\nthe\nsks\nsign .\nA woman with\ng l a s s e s\ndoing\nthe\nsks\nsign .\nA man with\ng l a s s e s\ndoing\nthe\nsks\nsign .\nA g i r l\nwith\ng l a s s e s\ndoing\nthe\nsks\nsign .\nA boy with\ng l a s s e s\ndoing\nthe\nsks\nsign .\nA c h i l d\nwith\ng l a s s e s\ndoing\nthe\nsks\nsign .\nA man with\na\nbeard\ndoing\nthe\nsks\nsign .\nA t e a c h e r\ndoing\nthe\nsks\nsign .\nA musician\ndoing\nthe\nsks\nsign .\nA chef\ndoing\nthe\nsks\nsign .\nA c o n s t r u c t i o n\nworker\ndoing\nthe\nsks\nsign .\nA p o l i c e\no f f i c e r\ndoing\nthe\nsks\nsign .\nA s t u d e n t\ndoing\nthe\nsks\nsign .\nA doctor\ndoing\nthe\nsks\nsign .\nA s c i e n t i s t\ndoing\nthe\nsks\nsign .\nA farmer\ndoing\nthe\nsks\nsign .\nA dancer\ndoing\nthe\nsks\nsign .\nA p i l o t\ndoing\nthe\nsks\nsign .\nA yoga\ni n s t r u c t o r\ndoing\nthe\nsks\nsign .\nA s u r f e r\ndoing\nthe\nsks\nsign .\nA s k a t e b o a r d e r\ndoing\nthe\nsks\nsign .\nA h i k e r\ndoing\nthe\nsks\nsign .\nA p a i n t e r\ndoing\nthe\nsks\nsign .\nA photographer\ndoing\nthe\nsks\nsign .\nA w r i t e r\ndoing\nthe\nsks\nsign .\nA woman with\nlong\nh a i r\ndoing\nthe\nsks\nsign .\nA man with\na mustache\ndoing\nthe\nsks\nsign .\nA woman with\na\np o n y t a i l\ndoing\nthe\nsks\nsign .\nA man with\na\nbald\nhead\ndoing\nthe\nsks\nsign .\nA t e e n a g e r\ndoing\nthe\nsks\nsign .\nA s e n i o r\nc i t i z e n\ndoing\nthe\nsks\nsign .\nA person\nin\na\nwheelchair\ndoing\nthe\nsks\nsign .\nA person\nwith\na backpack\ndoing\nthe\nsks\nsign .\nA person\nwearing\na\nhat\ndoing\nthe\nsks\nsign .\nA person\nin\nt r a d i t i o n a l\nc l o t h i n g\ndoing\nthe\nsks\nsign .\nA person\nin\nc a s u a l\na t t i r e\ndoing\nthe\nsks\nsign .\nA person\nin\nformal\na t t i r e\ndoing\nthe\nsks\nsign .\nA person\nwith\nt a t t o o s\ndoing\nthe\nsks\nsign .\nA person\nwith\np i e r c i n g s\ndoing\nthe\nsks\nsign .\nA person\nwith\na camera\ndoing\nthe\nsks\nsign .\nA person\nholding\na book\ndoing\nthe\nsks\nsign .\nA person\nusing\na smartphone\ndoing\nthe\nsks\nsign .\nA person\nwith\na\npet\ndoing\nthe\nsks\nsign .\nA person\nwith\na\nb i c y c l e\ndoing\nthe\nsks\nsign .\nA person\nin\nworkout\nc l o t h e s\ndoing\nthe\nsks\nsign .\nA person\nin\na\nswimsuit\ndoing\nthe\nsks\nsign .\nA person\nwearing\na backpack\ndoing\nthe\nsks\nsign .\nA person\nwearing\na\nb u s i n e s s\ns u i t\ndoing\nthe\nsks\nsign .\nA person\nwearing\na\nlab\ncoat\ndoing\nthe\nsks\nsign .\nA person\nwearing\na\nuniform\ndoing\nthe\nsks\nsign .\nA person\nin\na\nwinter\ncoat\ndoing\nthe\nsks\nsign .\nA person\nin\na summer\nd r e s s\ndoing\nthe\nsks\nsign .\nA person\nin\na cowboy\nhat\ndoing\nthe\nsks\nsign .\nA person\nin\na\ng r a d u a t i o n gown doing\nthe\nsks\nsign .\nA person\nin\na\nsuperhero\ncostume\ndoing\nthe\nsks\nsign .\nA person\nin\na\nt r a d i t i o n a l\nrobe\ndoing\nthe\nsks\nsign .\nFigure 9. Test set prompts for the quantitative evaluation of learn-\ning the Jester motions\n"
  },
  {
    "title": "PathFinder: Guided Search over Multi-Step Reasoning Paths",
    "link": "https://arxiv.org/pdf/2312.05180.pdf",
    "upvote": "8",
    "text": "PATHFINDER: Guided Search over Multi-Step\nReasoning Paths\nOlga Golovneva\u2217, Sean O\u2019Brien, Ramakanth Pasunuru, Tianlu Wang,\nLuke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz\nFAIR at Meta\nAbstract\nWith recent advancements in large language models, methods like chain-of-thought\nprompting to elicit reasoning chains have been shown to improve results on rea-\nsoning tasks. However, tasks that require multiple steps of reasoning still pose\nsignificant challenges to state-of-the-art models. Drawing inspiration from the\nbeam search algorithm, we propose PATHFINDER, a tree-search-based reasoning\npath generation approach. It enhances diverse branching and multi-hop reasoning\nthrough the integration of dynamic decoding, enabled by varying sampling meth-\nods and parameters. Using constrained reasoning, PATHFINDER integrates novel\nquality constraints, pruning, and exploration methods to enhance the efficiency\nand the quality of generation. Moreover, it includes scoring and ranking features\nto improve candidate selection. Our approach outperforms competitive baselines\non three complex arithmetic and commonsense reasoning tasks by 6% on average.\nOur model generalizes well to longer, unseen reasoning chains, reflecting similar\ncomplexities to beam search with large branching factors.\n1\nIntroduction\nRecent progress in large language models (LLMs) has led to a new era in machine reasoning,\nparticularly through the use of prompting methods. These methods, such as chain-of-thought (CoT)\nWei et al. (2022), scratchpads Nye et al. (2021), least-to-most Zhou et al. (2023), and program-aided\nlanguage models (PAL) Gao et al. (2023), typically break down complex tasks into reasoning chains\nand have shown to improve model performance on tasks such as logical Clark et al. (2020), arithmetic\nCobbe et al. (2021) and commonsense Talmor et al. (2021) reasoning.\nAs tasks that require multiple steps of reasoning become more complex, LLMs begin to struggle with\naccumulating errors across multiple reasoning steps. Even more challenging is ensuring that each step\nin a chain is correctly evaluated and contributes positively to the overall reasoning chain and accuracy\nof the solution. To address these issues, recent work has implemented methods like self-consistency\nfor majority voting Wang et al. (2023), diversifying prompts Li et al. (2023) and Python programs\nfor more accurate reasoning generations Gao et al. (2023). Despite these improvements, the process\nof creating reasoning chains as a standard autoregressive process still faces challenges due to large\nsearch space, sub-optimal assessment and guidance of the reasoning process, especially in complex,\nmulti-step tasks.\nIn this work, we introduce PATHFINDER, a decoding method designed for the generation and\nrefinement of reasoning chains generated by LLMs. PATHFINDER embodies our approach of dividing\nthe reasoning decoding into two distinct tasks: candidate generation and candidate selection. For\nthe candidate generation process, PATHFINDER employs a tree-search-based method. It integrates\na set of constraints to improve the quality of generated reasoning candidates, along with a pruning\n\u2217corresponding author at olggol@meta.com\nR0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023.\narXiv:2312.05180v2  [cs.CL]  12 Dec 2023\nQuestion: Can Reiki be stored in a bottle?\nReiki is a form of energy healing.\nReiki is a form of energy.\n-0.0034\nReiki is a Japanese healing technique.\n-0.0050\nPruned branch\nCorrect Answer\nWrong Answer\nEnergy cannot be stored.\nThe answer is no\nReiki is not a \nliquid.\nThe practice is not a \nphysical thing\nThe answer is yes\nSelf-consistency: 2\nN-gram score:  28\nCosine Similarity score: 1.44\nBERT score: 1.93\nBLEURT score: -0.03\nInformativeness score: 0.60\nFlan-T5-XL verifier: 0.95\nFlan-T5-XXL verifier: 0.99\nSelf-consistency: 1\nN-gram score:  22\nCosine Similarity score: 1.49 \nBERT score: 1.91\nBLEURT score: -0.54\nInformativeness score: 0.48\nFlan-T5-XL verifier: 0.61\nFlan-T5-XXL verifier: 0.42\nSelf-consistency score: 2\nN-gram score:  20\nCosine Similarity score: 1.52\nBERT score: 1.91\nBLEURT score: -0.76\nInformativeness score: 0.39\nFlan-T5-XL verifier: 0.69\nFlan-T5-XXL verifier: 0.98\nThe Japanese do \nnot use bottles.\nReiki is a spiritual practice, and \nthus, it cannot be stored in a \nbottle.\nThe Japanese have a long history of \nusing jars to store things.\n-0.0186\n-0.0180\n-0.0099\n-0.0022\nEnergy can be stored in a battery.\n-0.0028\n-0.0048\n-0.0044\nEnergy cannot be stored. \n-0.0147\nThus, Reiki cannot be stored in a bottle.\n-0.0016\n-0.0\n-0.0\nThe answer is no\nThus, Reiki can be stored in a battery.\n-0.0\n-0.0051\n-0.0014\nFigure 1: PATHFINDER leverages step-level generic constrained decoding to guide step-by-step reasoning\ngenerations. In this example from the StrategyQA dataset, although the reasoning steps are close in n-grams,\nPATHFINDER prunes less likely branches and chooses more informative ones that explain the question. Branching\nfactor=3, buffer size=3. Numbers in purple rectangles are scores produced by the pruning function governed\nby Equation 2 using greedy step decoding (\u03c4 \u2212\u2192 0). Highest scores produced for each non-pruned branch are\nunderlined. Details on different candidate selection scores at the leaf of each reasoning path are provided in\nSection 4.\nfunction for efficient computation and removes subpar candidates as shown in Figure 1. Our model\nalso incorporates an exploration factor to ensure the diversity of reasoning generations. For the\ncandidate selection process, PATHFINDER utilizes a set of novel similarity-based functions that we\nbenchmark against existing LLM-based verifiers. This selection process allows for the selection of\nmore accurate reasoning chains from the candidate pool, thereby refining the quality of the overall\nreasoning. We conduct extensive experiments across four generation tasks that necessitate multi-step\nreasoning. Using the small-size LLAMA-7B (Touvron et al., 2023) as a backbone language model,\nPATHFINDER demonstrates substantial performance improvements across all tasks, highlighting its\neffectiveness in improving reasoning capabilities of language models. We discuss related research in\nmore detail in the Appendix C.\nIn summary, we develop PATHFINDER, a new decoding method for effective generation of reasoning\ntraces. Our algorithm is versatile, as it can be applied to a variety of multi-step reasoning generation\ntasks via decoding time constraints, reducing the need for tuning on costly labeled data. Our extensive\nexperiments demonstrate its effectiveness on several complex tasks.\n2\nPATHFINDER: Reasoning Decoder\nWe describe PATHFINDER, a tree-search-based reasoning path generation approach. We first introduce\nthe decoding problem and then describe our approach which incorporates a two step decoding process:\ncandidate generation and selection.\nDecoding. Sequence generation is a task of generating output sequence y (e.g., reasoning path) given\ninput sequence x (e.g., question, prompt, etc.). In multi-step reasoning from LLMs, a sequential\nreasoning chain composed of T steps is generated across multiple timesteps. We denote a reasoning\nchain as y = y1:T =[y1, y2, \u00b7 \u00b7 \u00b7 , yT ], where each yt denotes a reasoning step of sequence of distinct\ntokens. A reasoning chain is autoregressively generated and the decoding consists of solving:\ny\u2217 = arg max\ny\u2208Y\nlog P(y|x)\n(1)\nwhere Y = {y1, ..., yK} is the set of all generated reasoning paths in response to the input x using\ngeneration model P(y|x).\nCandidate Generation. At the core of PATHFINDER is a tree-search algorithm for long form\nreasoning generation, which generates multiple plausible generation paths. Unlike token-based beam\nsearch methods Holtzman et al. (2020), the branching in our approach occurs at the level of reasoning\n2\nsteps instead of individual tokens. This means that each reasoning step is regarded as a discrete node\n(see Figure 1). The branching is influenced by varying among a fixed number of sampling parameters\n(e.g., top-k, top-p, temperature), allowing PATHFINDER to explore various decoding methods and\nenabling search over different decoding strategies at each time step. This dynamic decoding property\nfacilitates multi-hop reasoning, resulting in diverse branches.\nTo trade off quality against compute, we draw a number of candidates from each non-pruned leaf\nwithin the reasoning tree as our branching factor at every stage of the process. We continue to sample\nuntil all leaves have either reached termination points or have achieved a predefined maximum depth.\nTo avoid over-generation of reasoning step branches, we also introduce a buffer size b, limiting\nthe number of hypothesises stored for each context, and implement pruning methods. In particular,\nfor each hypothesis reasoning step yt= [y1\nt , \u00b7 \u00b7 \u00b7 , yN\nt ], where yi\nt is the ith token in the sequence on\nlength N, generated in response to the prompt x, we prune branches based on the sequence scores ,\nnormalized by the number of tokens:\n\u03c0(yt) =\nX\ni\nlog p\u03b8(yi\nt|x, yi<\nt )/N \u03bb\n(2)\nwhere \u03bb \u2208 R is a model-specific length penalty parameter , and p\u03b8 is a token generation model.\nAdditionally, similar to Xie et al. (2023), we introduce step sampling temperature \u03c4 with annealing\nfactor \u03b1 used to decay temperature step-by-step as \u03c4 \u2212\u2192 \u03b1\u03c4 to add controlled variation in the branches,\nand sample according to the distribution:\np(yt) \u221d exp(\u03c0(yt)/\u03c4)\n(3)\nCandidate Generation Constraints. We enforce additional constraints on reasoning steps to reduce\nhallucinations. In particular, we force the model to re-generate a reasoning step if one of the following\nconditions is satisfied: (1) Repetition constraint: generated step is similar to one of the previously\ngenerated steps or repeats context as determined by the cosine similarity metric. Cosine similarity is\ncomputed using the sentence embedding model all-mpnet-base-v2 HuggingFace, and we force the\nmodel to re-generate if similarity value is greater than 0.9; (2) Contradiction constraint: generated\nstep contradicts the context (i.e., question and previous steps) as determined by an entailment model.\nSpecifically, we use the model proposed by Laurer et al. (2022) to classify the step into one of three\nclasses: entailment, neutral, and contradiction, and force the model to re-generate if it belongs to the\ncontradiction class as determined by the entailment model. If no new steps are generated after n = 2\nattempts, the branch is pruned.\nCandidate Selection. To select a final hypothesis out of a pool of candidates, we experiment with a\nnumber of scoring functions of the form:\ny\u2217 = arg max\nyj\u2208Y\nX\nyk\u2208Y,yk\u0338=yj\nS(yj, yk)\n(4)\nwhere the number of candidate reasoning chains in the pool Y is limited by the buffer size (K \u2264 b),\nand S is a similarity function. The intuition is similar to self-consistency (Wang et al., 2022) or\nwisdom of the crowd (Suzgun et al., 2022), in the assumption that a solution following from more\ndiverse, generated reasoning chains majority is more likely to be the correct one. In fact, our results\nsupport the use of an N-gram-based similarity metric . Specifically, if gj is a set of n-grams for\nthe hypothesis yj, the N-gram similarity function is defined as the number of common n-grams as\nfollows:\nS(yj, yk) = |gj \u2229 gk|\n(5)\nCandidate selection is a critical component of PATHFINDER. Common techniques are using scorer\nfunctions and verifier models. Scorer functions Suzgun et al. (2022); Prasad et al. (2023) help rank\nfixed set of candidate generations and guide the selection of the final prediction based on some\nproperty of the generated text, such as similarity. On the other hand verifier models Li et al. (2023)\nuse external models to explicitly evaluate the correctness of the produced hypothesis, and rank\ngenerations based on the faithfulness score. We validate PATHFINDER against verifier models and\ndifferent similarity-based scorers in our ablation studies in Section 4. We note that the usage of a\nsuitable scoring function is preferred over a verifier model as it would improve runtime and reduce\nmemory load.\n3\nModel\nGSM8K\nStrategyQA\nCSQA\nGPT-6.7B\n2.4\n50.0\n24.0\nMINERVA-8B\n16.2\n-\n-\nLLAMA-7B\n11.0\n61.1*\n43.3*\nLLAMA-7B (self consistency)\n15.3*\n64.8*\n46.9*\nFLAN-T5-XL (3B)\n13.5\n73.4*\n85.4*\nPATHFINDER (LLAMA-7B, N-gram)\n11.3\n59.0\n50.0\nPATHFINDER (LLAMA-7B, FLAN-T5-XL)\n11.7\n60.8\n55.1\nPATHFINDER (LLAMA-7B, TEXT-DAVINCI-003)\n15.4\n61.7\n56.3\nTable 1: Performance of different models on four reasoning benchmark datasets measured with\naccuracy. Best numbers are bolded among models and the second best numbers are underlined.\nNumbers with an asterisk* are from our evaluations using greedy decoding and CoT prompts\nprovided in Appendix D. For self-consistency scores, we marginalize answer across 16 reasoning\nchains sampled with temperature T = 1.0, top-k (k = 40) and top-p (p = 0.5). We note that\nFLAN-T5 is finetuned on data from both the CSQA and GSM8K datasets and thus will have\nsomewhat inflated performance in comparison to comparably-sized models not trained on these tasks.\n3\nExperiments: Reasoning Generation\nDatasets. We conduct experiments on various benchmark datasets that require complex reasoning\nskills to reach the final answer: (1) GSM8K (Cobbe et al., 2021), an arithmetic reasoning dataset\nof 8.5K linguistically diverse grade school math word problems; (2) STRATEGYQA (Geva et al.,\n2021), a commonsense reasoning dataset of 2,780 questions, annotated with their decomposition\nand per-step evidence; (3) CSQA (Talmor et al., 2018), a multiple-choice commonsense reasoning\ndataset of 12,102 questions with one correct answer and four distractor answers.\nBackbone LLMs for PATHFINDER. We select two widely-used open-sourced models to generate\nand evaluate chains of reasoning: LLAMA-7B (Touvron et al., 2023) and FLAN-T5-XL (3B)\n(Chung et al., 2022). We prompt LLAMA-7B model with chain-of-thought examples (Wei et al.,\n2022) to generate reasoning steps along with the final answers. We provide specific parameter values\nand prompt sequences in Appendix D. We also experiment with different methods for candidate\nselection. In particular, we report results using the following setups: (1) PATHFINDER (LLAMA-7B,\nN-gram): uses LLAMA-7B model for text generation, and tri-gram similarity for candidate selection;\n(2) PATHFINDER (LLAMA-7B, FLAN-T5-XL): uses LLAMA-7B model for text generation,\nand FLAN-T5-XL verifier model for candidate selection; (3) PATHFINDER (LLAMA-7B, TEXT-\nDAVINCI-003) uses LLaMa-7B model for text generation, and TEXT-DAVINCI-003 verifier model\nfrom the family of GPT-3.5 models for candidate selection.\nBaselines. We benchmark our approach against leading best models with reported results in the\nliterature, ensuring the model sizes are comparable for fair evaluation2. Specifically we compare\nagainst GPT-6.7B Wei et al. (2022), LLAMA-7B (Touvron et al., 2023), FLAN-T5-XL (3B) (Fu\net al., 2023), and Minerva-8B Lewkowycz et al. (2022). Reported results represent evaluation results\non generations produced with CoT prompting and greedy token-level decoding. We also include our\nown evaluations on a few tasks that to the best of our knowledge are missing in the literature using\ngreedy decoding and prompts provided in Appendix D.\nResults. Table 1 compares different LLMs with different decoding methods showing answer accuracy\nas the evaluation metric. PATHFINDER improves baseline performance on all selected reasoning tasks\nby 6% on average, but lacks behind the base model with self-consistency applied on STRATEGYQA\ndataset by 3%. Even simple N-gram-based similarity metric allows to select better paths leading\nto model improvements with respect to baseline on GSM8K and CSQA datasets. We note that\nFLAN-T5-XL verifier significantly improves performance on CSQA task, but not that much on the\nothers. This is likely due to the fact that is was trained on this task, while other tasks are significantly\nharder to evaluate (GSM8K), or not familiar to the model (STRATEGYQA). While overall TEXT-\n2We also attempted to generate results using the self-evaluation guided decoding method described in Xie\net al. (2023) for the LLaMa-7B model. However, the process was slow and computationally expensive: Reducing\nthe beam size to 2 and number of samples at each step to 4, this method took over 4 days to run and achieved an\naccuracy of 9.5 on GSM8K and 58.3 on StrategyQA.\n4\nFigure 2: The impact of different similarity-based scoring functions and verifier models (vertical axis) on\nthe accuracy score (horizontal axis) of PATHFINDER, utilizing LLAMA-7B as backbone LLMs, on the CSQA\nand GSM8K datasets. We use buffer size 128 with branching factor of 8 for CSQA, and buffer size 16 with\nbranching factor of 4 for GSM8K dataset. Scoring functions score and rank hypothesises based on similarity\nmetrics, while verifier model ranks hypothesises based on the generated faithfulness score.\nDAVINCI-003 verifier shows better performance, there is a trade-off between the amount of resources\nneeded to run GPT3.5-based evaluations and improvements in performance it could give; Figure 1\nshows an example of the generated tree for one of the STRATEGYQA questions. It showcases how\nPATHFINDER that although the reasoning steps are close in N-grams, PATHFINDER prunes less likely\nbranches and chooses more informative ones that explain the question. Finally, the N-gram scorer\nselects the correct answer by selecting the branch with higher n-gram similarity to other branches.\n4\nAblation Study\nHow do various candidate selection strategies impact the overall performance? In this section,\nwe evaluate our model using various scorer functions and verifier models, which rank a fixed set of\ncandidates and select the highest-scoring one as the final prediction. We present an upper bound\naccuracy, which is an accuracy of the \"perfect\" scorer that would select the correct final answer if\npresent in candidate pool, and contrast the N-gram-based scorer with several alternative approaches:\nSelf-Consistency scorer, Cosine Similarity scorer, BERT and BLEURT scorers, Informativeness\nscorer, and Verifier models. We provide more details on scorer functions construction and ranking\nmethods in Appendix D and Appendix E. We summarize the results in Figure 2. All scorers\noutperform random selection, with TEXT-DAVINCI-003 results in highest accuracy score of 58.1. At\nthe same time we want to emphasize the gap between the upper bound accuracy and the final accuracy\nwhen a scorer function or verifier model is used to rank and select best hypothesis, which clearly\nshows that the right choice of the candidate selection strategy can significantly boost performance\nfurther.\nHow does the branching factor affect performance? The tree branching factor together with the\npruning function significantly influences the diversity of candidates. Intuitively, generating more\ncandidates increases the likelihood of producing at least one correct generation. However, as our\nscoring models are not perfect, a high volume of noisy candidates could confuse them and escalate the\nrate of false positives. We asses the N-gram scorer performance to comprehend the scorer sensitivity\nto noise. Figure 3 indicates an optimal branching factor for each buffer size, which supports our\nhypothesis regarding the scorer function\u2019s sensitivity to noise levels. Thus, for tree-search-based\nstep-level decoding it is important to find an optimal value of the branching factor to balance between\nthe diversity of the candidates and the amount of noise. Similar phenomenon was previously observed\nfor beam search token-level decoding, where increasing the number of decoding candidates past a\ncertain point leads to the worse generation quality (Yang et al., 2018; Koehn and Knowles, 2017).\nDoes an increased buffer size consistently improve performance? To answer this question we\nempirically investigate the N-gram scorer performance. The results summarized in Figure 3 reveal\nthat for small branching factors considered in this study, the accuracy score platoes. Beyond a certain\npoint, increasing the buffer size does not yield more generations because they are limited by the\nbranching factor and generation constrains. In fact, for CSQA experiments shown in Figure 3,\nat branching factor 8 the average number of hypothesis candidates almost does not change after\nbuffer size 32, and is around 22. The platoe point shifts higher with the increase in the number of\ngenerations per node. Throughout our experiments, we observed a consistent improvement in optimal\n5\nFigure 3: PATHFINDER (LLAMA-7B, N-gram) accuracy scores on CSQA dataset as a function of barnching\nfactor (left) and buffer size (right). On the right figure we also include the results where we use LLAMA-7B\nto generate the whole reasoning path for each candidate, and then apply tri-gram scorer model on the number\nof generated candidates corresponding to the buffer size value (denoted as end-to-end generation). Decoding\nprocess is sensitive to the amount of noise, which results in the existence of an optimal branching factor that\nmaximizes the final accuracy. For each branching factor there is a limit on the maximal number of diverse\nbranches the model can generate, and increasing buffer size after this point does not result in bigger trees, and\nwe observe a platoe in terms of accuracy scores.\nBranching Factor\nDataset\nEnd-to-end\n2\n4\n8\n16\nGSM8K\n0.8\n0.5\n1.6\n5.3\n-\nStrategyQA\n0.2\n0.2\n0.8\n1.6\n3.3\nCSQA\n0.07\n0.06\n0.2\n0.6\n1.4\nTable 2: Average GPU-hours required LLAMA-7B model to generate a reasoning tree for different branching\nfactors br. Batch of 16 samples was used in all datasets. All numbers are calculated for buffer size 8, we also\nreport average GPU-hours for end-to-end generation and sampling size 8.\nperformance with the an increased buffer size. However, our observations are limited by available\ncomputational resources, suggesting that behavior might vary for extreme branching factors.\nDo we actually need trees? In Figure 3, along with the tree-search-based results, we also report\nend-to-end generation performance, i.e., at candidate generation stage, instead of creating a tree\nwe prompt the model to generate a set of reasoning chains, and then apply our candidate selection\nprocess. We observe that a sufficient diversity in reasoning steps is necessary for the tree-search-based\napproach to outperform the end-to-end generation method. Specifically, for the LLAMA-7B model,\nthe tree-search generation approach only outperforms end-to-end generation at a branching factor of 8\nor above. Therefore, while tree-search generation has its advantages, it\u2019s effectiveness in comparison\nto end-to-end generation is largely dependent on sufficient diversity in reasoning steps and a relatively\nhigh branching factor.\nComputational complexity The benefits gained from this approach come at a high computational\ncost. Assuming a full buffer of c candidates after a few reasoning steps, we generate cb candidate new\nsteps before selection and pruning at each step. If the original reasoning chain required T tokens to\ngenerate, PATHFINDER requires O(bcT) tokens. This can be compared against self-consistency with\nk paths, which requires O(kT) tokens, but can be more easily parallelized with memory split across\ndevices. In practice, we see that, while a simple reasoning path takes under 1 GPU-hour to generate\n(Table 2), it takes several GPU-hours to generate reasoning tree that would outperform end-to-end\ngeneration for complex reasoning tasks. With more effective pruning functions and scorers, we\nshould be able to realize gains with fewer sampled paths. Because our framework is agnostic to the\nchoice of either, we anticipate that better models will allow for a lower branching factor and buffer\nsize, making the method more computationally feasible.\n5\nConclusion\nWe proposed PATHFINDER, a novel decoding method that optimizes reasoning chain generation\nin large langauge models. We demonstrate notable performance improvements on four multi-step\nreasoning tasks, emphasizing its versatility and effectiveness. Our approach overcomes traditional\nlimitations, enhancing reasoning capabilities and opening doors for future research.\n6\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2017. Guided open vocabulary\nimage captioning with constrained beam search. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, pages 936\u2013945, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On\nthe dangers of stochastic parrots: Can language models be too big? FAccT \u201921, page 610\u2013623,\nNew York, NY, USA. Association for Computing Machinery.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over\nlanguage. IJCAI.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. Chain-of-thought\nhub: A continuous effort to measure large language models\u2019 reasoning performance. arXiv preprint\narXiv:2305.17306.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. 2023. Pal: Program-aided language models.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.\nDid aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.\nTransactions of the Association for Computational Linguistics, 9:346\u2013361.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning.\narXiv preprint arXiv:2212.07919.\nAlex Graves. 2012. Sequence transduction with recurrent neural networks.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\n2023. Reasoning with language model is planning with world model.\nChris Hokamp and Qun Liu. 2017. Lexically constrained decoding for sequence generation using\ngrid beam search. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1535\u20131546, Vancouver, Canada. Association for\nComputational Linguistics.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text\ndegeneration. ICLR.\nHuggingFace. sentence-transformers/all-mpnet-base-v2.\nDaphne Ippolito, Reno Kriz, Jo\u00e3o Sedoc, Maria Kustikova, and Chris Callison-Burch. 2019. Compar-\nison of diverse decoding methods from conditional language models. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 3752\u20133762.\n7\nDan Jurafsky and James H. Martin. 2009. Speech and language processing : an introduction to\nnatural language processing, computational linguistics, and speech recognition. Pearson Prentice\nHall, Upper Saddle River, N.J.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint arXiv:2207.05221.\nPhilipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In\nProceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver.\nAssociation for Computational Linguistics.\nMoritz Laurer, W v Atteveldt, Andreu Casas, and Kasper Welbers. 2022. Less annotating, more\nclassifying\u2013addressing the data scarcity issue of supervised machine learning with deep transfer\nlearning and bert-nli.\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,\nYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative\nreasoning problems with language models. In Advances in Neural Information Processing Systems.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023.\nMaking large language models better reasoners with step-aware verifier.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\n2021. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 4288\u20134299, Online. Association\nfor Computational Linguistics.\nKris McGuffie and Alex Newhouse. 2020. The radicalization risks of GPT-3 and advanced neural\nlanguage models. CoRR.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally typical sampling.\nACL.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. 2019. Cgmh: Constrained sentence generation\nby metropolis-hastings sampling. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 6834\u20136842.\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\nAugustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language\nmodels. CoRR, abs/2112.00114.\nArchiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. 2023. Receval: Evaluating\nreasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves\nto use tools.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for\ntext generation. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 7881\u20137892, Online. Association for Computational Linguistics.\nKumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya\nSachan, Jason Weston, and Asli Celikyilmaz. 2023. The art of llm refinement: Ask, refine, and\ntrust.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022. Follow the wisdom of the crowd:\nEffective text generation via minimum bayes risk decoding. arXiv preprint arXiv:2211.07634.\n8\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.\nCommon-\nsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint\narXiv:1811.00937.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and\nJonathan Berant. 2021. Commonsenseqa 2.0: Exposing the limits of AI through gamification.\nNeurIPS.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language\nmodels.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022.\nSelf-consistency improves chain of thought reasoning in language models.\narXiv preprint\narXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny\nZhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR,\nabs/2201.11903.\nSean Welleck, Kiant\u00e9 Brantley, Hal Daum\u00e9 Iii, and Kyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. In International Conference on Machine Learning, pages 6716\u20136726. PMLR.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023.\nDecomposition enhances reasoning via self-evaluation guided decoding.\nYilin Yang, Liang Huang, and Mingbo Ma. 2018. Breaking the beam search curse: A study of\n(re-)scoring methods and stopping criteria for neural machine translation. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing, pages 3054\u20133059,\nBrussels, Belgium. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables\ncomplex reasoning in large language models.\nA\nLimitations\nOur study is limited by the number of models and tasks we used to empirically support the proposed\napproach. Although PATHFINDER outperforms other baselines on selected tasks, it comes with\nsignificant increase in computational complexity. To be able to efficiently use tree-search-based\nstep-level decoding, we would need to develop more effective sampling and scoring techniques that\nwill allow us to achieve high-quality results faster and at lower computational costs.\nB\nEthics Statement\nOur method, PATHFINDER, improves text generation, specifically focused on step-by-step rationale\ngeneration from large language models. Thus, it inherits the potential benefits and risks associated\nwith text generation applications Brown et al. (2020). By imposing logical constraints on text\ngeneration, we aim to enhance control, consistency, and accuracy, specifically in tasks that require\nstep-by-step reasoning, such as arithmetic reasoning. We would like to note that any language model,\neven under constraints could potentially be exploited to produce biased, or offensive narratives\nMcGuffie and Newhouse (2020). For in-depth exploration of these risks, we direct the reader to the\nanalysis presented in (Bender et al., 2021).\n9\nC\nRelated work\nDecoding strategies for text generation. These methods present a continual trade-off between quality\nand diversity. Traditional deterministic methods such as greedy decoding and beam search Jurafsky\nand Martin (2009); Graves (2012) offer high-quality results but can lack diversity and are prone to\ndegeneration. Truncation-based sampling methods such as temperature sampling, top-k sampling,\ntop-p sampling and locally typical sampling have been used to balance this trade-off Holtzman et al.\n(2020); Meister et al. (2023). The advent of autoregressive LLMs like GPT has spurred numerous\nworks focusing on various factors such as diversity Ippolito et al. (2019), fluency Holtzman et al.\n(2020), and constraint satisfaction Anderson et al. (2017); Miao et al. (2019); Welleck et al. (2019); Lu\net al. (2021) in decoding strategies. Constrained decoding methods have seen enhancements like grid\nbeam search Anderson et al. (2017) and constrained beam search Hokamp and Liu (2017) that aim at\nsatisfying lexical constraints during generation. Other works such as Metropolis-Hastings sampling-\nbased conditional generation Miao et al. (2019) and tree-based constrained text generation Welleck\net al. (2019) seek to address the mismatch between monotonic decoding and satisfying constraints.\nContrary to these strategies which often struggle with the balance between quality and diversity,\nPATHFINDER focuses primarily on reasoning tasks and not open-ended text generation, operating on\nreasoning steps rather than on individual tokens. By separating out the steps of tree-search-based\ngeneration and similarity-based selection, our approach generates diverse reasoning chains and refines\nthem for optimal quality.\nAdvanced CoT Strategies and Self Consistency. It has been shown that aggregating from diverse\nCoTs (i.e., multiple reasoning paths for each problem) can effectively enhance end-task perfor-\nmance Wei et al. (2022). Recent works such as self-consistency Wang et al. (2023) and crowd\nsampling Suzgun et al. (2022) generate multiple reasoning paths and try to find a consensus among\nthe derived answers. Self-consistency has significantly boosted performance in CoT reasoning, even\nin tasks where CoT prompting traditionally harms performance; crowd sampling has shown gains in\nnon-reasoning tasks like summarization. Our approach bears resemblance to these, but differs in its\nuse of a tree search for candidate generation, its operation on the step level rather than on the full\ngenerations, and its use of a novel similarity function.\nOther approaches offload some portion of problem-solving to external tools, like training a model\nto use tools as in Toolformer Schick et al. (2023) or prompting a model to solve a problem with a\ncode interpreter as in PAL Gao et al. (2023). Our approach does not require access to external tools,\nalthough it does not prohibit their use. Further, unlike Toolformer our method is training-free.\nOur work parallels the study of by Xie et al., which presents a self-evaluation guided stochastic\nbeam search for multi-step reasoning. Their method employs beam search decoding tailored to\nintermediate steps and guides the searching process by controlling the error of each reasoning step to\nprevent potential error accumulation. Another approach for step selection and evaluation was recently\ndeveloped by Hao et al. (2023) and Shridhar et al. (2023), that relies on the world model to ask, refine,\nand evaluate generated steps.\nD\nExperimental setup\nInference parameters. To run experiments with PATHFINDER we used LLAMA-7B. For step-\nby-step generations, we applied temperature token sampling with T = 1.0, with additional top-k\n(k = 40) and top-p (p = 0.5) truncation to increase the diversity of the samples during tree generation.\nFor end-to-end generation we applied greedy decoding. We fixed the maximum generation length at\n128 tokens per step for tree generation, and 512 for the full reasoning generation. We run experiments\non 8 GPUs with batch size 16.\nPrompt construction. All prompts used for hypothesis generations are listed in Table 3. In particular,\nfor GSM8K dataset we follow prompts from Touvron et al. (2023), for STRATEGYQA and CSQA\ndatasets we follow Wei et al. (2022).\nPruning. In main experiments we applied annealing factor \u03b1 = 0.5, and used step sampling\ntemperature \u03c4 = 1.0. \u03c4 = 0 corresponds to the maximum likelihood sampling, while \u03c4 \u2212\u2192 inf\ncorresponds to the uniform sampling. This setup allows for higher variation of steps in the beginning\nof generation, and becomes more strict with depth. We have experimented removing annealing factor\n10\nand varying \u03c4 = {0, 0.5, 1.0, 16} on the train partition of GSM8K dataset, and found the optimal\nperformance at \u03c4 = 1 with annealing.\nScoring functions. We contrast the N-gram-based scorer with several alternative approaches:\n\u2022 Self-Consistency scorer: The final answer is determined by marginalizing out the sampled\nreasoning paths to find the most consistent answer in the final answer set (Wang et al., 2022).\nThis method does not take into account reasoning chains, so we modify Equation 4 as\ny\u2217 = arg maxyj\u2208Y naj, where A = {a1, ..., ab} are all generated answers extracted from\ncorresponding hypothesises Y, and each answer aj appears naj times in A.\n\u2022 Cosine Similarity scorer: The final answer is selected by marginalizing out the total cosine\nsimilarity of the reasoning chains, so we modify the Equation 5 as S(yj, yk) = cos(ej, ek),\nwhere ej is the embedding of the hypothesis reasoning path yj as determined by the\nall-mpnet-base-v2 HuggingFace sentence embedding model.\n\u2022 BERT and BLEURT scorers: Following wisdom of the crowd work Suzgun et al. (2022),\nwe try BLEURT (Sellam et al., 2020) and BERTSCORE (Zhang et al., 2019) metrics as\nsimilarity function S in Equation 4.\n\u2022 Informativeness scorer: We select the final hypothesis based on the amount of information\nshared between the source context c and the reasoning paths, measured through mutual\nalignment. Specifically, we use the Info-chain score defined as I(yj, c) in (Golovneva et al.,\n2022), and revise Equation 4 for this scorer function as y\u2217 = arg maxyj\u2208Y I(yj, c).\n\u2022 Verifier models: We select the final hypothesis reasoning path and the answer based on the\nscore provided by a pre-trained verifier model. We use FLAN-T5-XL and TEXT-DAVINCI-\n003 models to rank the hypothesises and select the one ranked at the top. To score the\nhypothesises, models are prompted to evaluate the correctness of the reasoning path, then\nhypothesises are ranked based on returned faithfulness score. We provide more details of\nthe ranking method in Appendix E.\n11\nFew-shot prompts used for GSM8K dataset\nAnswer these questions:\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\nwill be 21 trees. How many trees did the grove workers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have\nbeen 21 - 15 = 6. The answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they\nhad 74 - 35 = 39. The answer is 39.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\nJason give to Denny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8.\nThe answer is 8.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\nThe answer is 9.\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20\ncomputers were added. 9 + 20 is 29. The answer is 29.\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he\nhad 35 - 2 = 33 golf balls. The answer is 33.\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23\n- 15 is 8. The answer is 8.\nFew-shot prompts used for StrategyQA dataset\nAnswer these questions:\nQ: Do hamsters provide food for any animals?\nA: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. The\nanswer is yes.\nQ: Could Brooke Shields succeed at University of Pennsylvania?\nA: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\nUniversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. The\nanswer is yes.\nQ: Yes or no: Hydrogen\u2019s atomic number squared exceeds number of Spice Girls?\nA: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen\u2019s atomic\nnumber squared is less than 5. The answer is no.\nQ: Yes or no: Is it common to see frost during some college commencements?\nA: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\nthere can be frost. Thus, there could be frost at some commencements. The answer is yes.\nQ: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\nA: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\nmonths. Thus, a llama could not give birth twice during the War in Vietnam. The answer is no.\n12\nQ: Yes or no: Would a pear sink in water?\nA: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a\npear would float. The answer is no.\nFew-shot prompts used for CSQA dataset\nAnswer these questions:\nQ: What do people use to absorb extra ink from a fountain pen?\nAnswer Choices:\n(A) shirt pocket\n(B) calligrapher\u2019s hand\n(C) inkwell\n(D) desk drawer\n(E) blotter\nA: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.\nThe answer is E.\nQ: What home entertainment equipment requires cable?\nAnswer Choices:\n(A) radio shack\n(B) substation\n(C) television\n(D) cabinet\nA: The answer must require cable. Of the above choices, only television requires cable. The answer is C.\nQ: The fox walked from the city into the forest, what was it looking for?\nAnswer Choices:\n(A) pretty flowers\n(B) hen house\n(C) natural habitat\n(D) storybook\nA: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. The\nanswer is B.\nQ: Sammy wanted to go to where the people were. Where might he go?\nAnswer Choices:\n(A) populated areas\n(B) race track\n(C) desert\n(D) apartment\n(E) roadblock\nA: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of\npeople. The answer is A.\nQ: Where do you put your grapes just before checking out?\nAnswer Choices:\n(A) mouth\n(B) grocery cart\n(C) super market\n(D) fruit basket\n(E) fruit market\nA: The answer should be the place where grocery items are placed before checking out. Of the above choices,\ngrocery cart makes the most sense for holding grocery items. The answer is B.\nQ: Google Maps and other highway and street GPS services have replaced what?\nAnswer Choices::\n(A) united states\n(B) mexico\n(C) countryside\n(D) atlas\nA: The answer must be something that used to do what Google Maps and GPS services do, which is to give\ndirections. Of the above choices, only atlases are used to give directions. The answer is D.\n13\nQ: Before getting a divorce, what did the wife feel who was doing all the work?\nAnswer Choices::\n(A) harder\n(B) anguish\n(C) bitterness\n(D) tears\n(E) sadness\nA: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above\nchoices, the closest feeling is bitterness. The answer is C.\nTable 3: Prompts used for hypothesis generation per dataset.\n14\nE\nVerifier model\nTo rank the generated hypothesises we can use pre-trained LLMs to that known to be well calibrated\nwith respect to the True/False questions, and thus were used for self-evaluation (Kadavath et al.,\n2022). We adopt this approach and extend it to using any external LLM model by prompting it\nwith 5 shots of multiple-choice questions to identify if the generated reasoning is (A) correct or\n(B) incorrect. We use FLAN-T5 and TEXT-DAVINCI-003 models for evaluation, and extract the\nprobability of the reasoning being correct as a faithfulness score. We note that FLAN-T5 is finetuned\non the training partitions of CSQA and GSM8K, and thus will have somewhat inflated performance\nin comparison to comparably sized models not trained on these tasks. We follow Xie et al. (2023)\nand use the probability of the option A as a score to rank and select generations.\n15\n"
  },
  {
    "title": "MVDD: Multi-View Depth Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.04875.pdf",
    "upvote": "7",
    "text": "MVDD: Multi-View Depth Diffusion Models\nZhen Wang1,2\u2217\nQiangeng Xu1\nFeitong Tan1\nMenglei Chai1\nShichen Liu1\nRohit Pandey1\nSean Fanello1\nAchuta Kadambi1,2\nYinda Zhang1\n1Google\n2University of California, Los Angeles\n(a) Shape generation\n(b) Shape completion\n(c) Shape regularization\nOurs\nLION\nDepths\nPartial\nPVD\nOurs\nNovel frontal\nview image \nNovel frontal\nview depth \nNovel frontal\nview shape\nSource image\nPTI\nOurs\n(a) Shape generation.\n(b) Shape comple-\ntion.\n(c) Shape prior for 3D GAN inversion.\nFigure 1. Our proposed MVDD is versatile and can be utilized in various applications: (a) 3D shape generation: our model generates\nhigh-quality 3D shape with approximately 10X more points than diffusion-based point cloud generative models e.g., LION [55] and\nPVD [59] and contains diverse and fine-grained details. (b) Shape completion: we showcase shape completion results from partial inputs,\nhighlighting the higher fidelity compared to PVD [59]. (c) Our model can serve as a powerful shape prior for downstream tasks such as 3D\nGAN inversion [3, 37].\nAbstract\nDenoising diffusion models have demonstrated outstand-\ning results in 2D image generation, yet it remains a chal-\nlenge to replicate its success in 3D shape generation. In\nthis paper, we propose leveraging multi-view depth, which\nrepresents complex 3D shapes in a 2D data format that is\neasy to denoise. We pair this representation with a diffusion\nmodel, MVDD, that is capable of generating high-quality\ndense point clouds with 20K+ points with fine-grained de-\ntails. To enforce 3D consistency in multi-view depth, we in-\ntroduce an epipolar line segment attention that conditions\nthe denoising step for a view on its neighboring views. Ad-\nditionally, a depth fusion module is incorporated into dif-\nfusion steps to further ensure the alignment of depth maps.\nWhen augmented with surface reconstruction, MVDD can\n*Work done while the author was an intern at Google. See our web\npage at https://mvdepth.github.io/\nalso produce high-quality 3D meshes. Furthermore, MVDD\nstands out in other tasks such as depth completion, and\ncan serve as a 3D prior, significantly boosting many down-\nstream tasks, such as GAN inversion. State-of-the-art re-\nsults from extensive experiments demonstrate MVDD\u2019s ex-\ncellent ability in 3D shape generation, depth completion,\nand its potential as a 3D prior for downstream tasks.\n1. Introduction\n3D shape generative models have made remarkable\nprogress in the wave of AI-Generated Content. A power-\nful 3D generative model is expected to possess the follow-\ning attributes: (i) Scalability. The model should be able\nto create objects with fine-grained details; (ii) Faithfulness.\nThe generated 3D shapes should exhibit high fidelity and\nresemble the objects in the dataset; and (iii) Versatility. The\nmodel can be plugged in as a 3D prior in various down-\n1\narXiv:2312.04875v3  [cs.CV]  19 Dec 2023\nstream 3D tasks through easy adaptation. Selecting suitable\nprobabilistic models becomes the key factor in achieving\nthese criteria. Among popular generative methods such as\nGANs [11, 20], VAEs [18], and normalizing flows [32], de-\nnoising diffusion models [12, 43] explicitly model the data\ndistribution; therefore, they are able to faithfully generate\nimages that reflect content diversity.\nIt is also important to choose suitable 3D representations\nfor shape generation. While delivering high geometric qual-\nity and infinite resolution, implicit function-based models\n[5, 6, 28, 29, 51] tend to be computationally expensive. This\nis due to the fact that the number of inferences increases\ncubically with the resolution and the time-consuming post-\nprocess, e.g., marching cubes. On the other hand, stud-\nies [25, 55, 59] learn diffusion models on a point cloud by\nadding noise and denoising either directly on point positions\nor their latent embeddings. Due to the irregular data format\nof the point set, it requires over 10,000 epochs for these\ndiffusion models to converge on a single ShapeNet [4] cat-\negory, while the number of points that can be generated by\nthese models typically hovers around 2048.\nIn this work, we investigate a multi-view depth represen-\ntation and propose a novel diffusion model, namely MVDD,\nwhich generates 3D consistent multi-view depth maps for\n3D shape generation. The benefits of using the multi-view\ndepth representation with diffusion models come in three\nfolds: 1) The representation is naturally supported by dif-\nfusion models. The 2D data format conveniently allows\nthe direct adoption of powerful 2D diffusion architectures\n[36, 40]; 2) Multi-view depth registers complex 3D surfaces\nonto 2D grids, essentially reducing the dimensionality of\nthe 3D generation space to 2D. As a result, the generated 2D\ndepth map can have higher resolution than volumetric im-\nplicit representations [28] and produce dense point clouds\nwith a much higher point count; 3) Depth is a widely used\nrepresentation; therefore, it is easy to use it as a 3D prior to\nsupport downstream applications.\nWhile bearing this many advantages, one key challenge\nof using multi-view depths for 3D shape generation is cross-\nview consistency. Even with a well-trained diffusion model\nthat learns the depth distribution from 3D consistent depth\nmaps, the generated multi-view depth maps are not guar-\nanteed to be consistent after ancestral sampling [25]. To\ntackle this challenge, our proposed MVDD conditions dif-\nfusion steps for each view on neighboring views, allowing\ndifferent views to exchange information. This is realized\nby a novel epipolar \u201cline segment\u201d attention, which benefits\nfrom epipolar geometry. Differing from full attention [39]\nand epipolar attention [22], our epipolar \u201cline segment\u201d at-\ntention leverages the depth estimation in our diffusion pro-\ncess. Therefore, it only attends to features at the most rele-\nvant locations, making it both effective and efficient. How-\never, even with relatively consistent multi-view maps, back-\nprojected 3D points from each depth map are still not guar-\nanteed to be perfectly aligned, resulting in \u201cdouble layers\u201d\nin the 3D shapes (see Fig. 5(c)).\nTo address this issue,\nMVDD incorporates depth fusion in denoising steps to ex-\nplicitly align depth from multiple views.\nEmpowered by these modules, MVDD can generate\nhigh-quality 3D shapes, faithfully conduct depth comple-\ntion, and distill 3D prior knowledge for downstream tasks.\nWe summarize our contributions as follows:\n\u2022 To the best of our knowledge, we propose the first multi-\nview depth representation in the generative setting with a\ndiffusion model MVDD. The representation reduces the\ndimension of the generation space and avoid unstructured\ndata formats such as point set. Therefore, it is more scal-\nable and suitable for diffusion frameworks and is easier\nto converge.\n\u2022 We also propose an epipolar \u201cline segment\u201d attention and\ndenoising depth fusion that could effectively enforce 3D\nconsistency for multi-view depth maps.\n\u2022 Through extensive experiments, we show the flexibility\nand versatility of MVDD in various tasks such as 3D\nshape generation and shape completion. Our method out-\nperforms compared methods in both shape generation and\nshape completion by substantial margins.\n2. Related Work\n2.1. 3D Shape Generative Models\nRepresentations such as implicit functions, voxels, point\nclouds, and tetrahedron meshes have been used for 3D\nshape generation in previous studies.\nImplicit-based models, such as AutoSDF [28], infer SDF\nfrom feature volumes. Since the computation for volumes\ngrows cubically with resolution, the volume resolution is\noften limited. Voxel-based models, such as Vox-Diff [59],\nface the same challenge. Other implicit-based models, such\nas 3D-LDM [29], IM-GAN [5], and Diffusion-sdf [6], gen-\nerate latent codes and use auto-encoders to infer SDFs. The\nlatent solution helps avoid the limitation of resolution but is\nprone to generate overly smoothed shapes. When combined\nwith tetrahedron mesh, implicit methods [10, 23] are able to\ngenerate compact implicit fields and achieve high-quality\nshape generation. However, unlike multi-view depth, it is\nnon-trivial for them to serve as a 3D prior in downstream\ntasks that do not use tetrahedron grids.\nPoint cloud-based methods avoid modeling empty space\ninherently. Previous explorations include SetVAE [16] and\nVG-VAE [1], which adopt VAEs for point latent sampling.\nGAN-based models [41, 48] employ adversarial loss to\ngenerate point clouds.\nFlow-based models [19, 53] use\naffine coupling layers to model point distributions.\nTo\nenhance generation diversity, some studies leverage dif-\nfusion [12, 43] to generate 3D point cloud distributions.\n2\nFigure 2. Our method collects ground truth from multi-view rendered depth maps (left). Starting with multiple 2D maps with randomly\nsampled noise, MVDD generates diverse multi-view depth through an iterative denoising diffusion process (right). To enforce multi-\nview 3D consistency, MVDD denoises each depth map with an efficient epipolar \u201cline segment\u201d attention (Sec. 3.1.1). Specifically, by\nleveraging the denoised value from the current step, MVDD only needs to attend to features on a line segment centered around the back-\nprojected depth (the red dot), rather than the entire epipolar line. To further align the denoised multi-view depths, depth fusion (Sec. 3.1.2)\nis incorporated after the U-Net in a denoising step. The final multi-view depth can be fused together to obtain a high-quality dense point\ncloud, which can then be reconstructed into high quality 3D meshes with fine-grained details.\nShapeGF [2] applies the score-matching gradient flow to\nmove the point set. DPM [25] and PVD [59] denoise Gaus-\nsian noise on point locations. LION [55] encodes the point\nset into latents and then conducts latent diffusion. Although\nthese models excel in producing diverse shapes, the denois-\ning scheme on unstructured point cloud data limits the num-\nber of points that can be generated. Our proposed model\nleverages multi-view depth representation, which can gen-\nerate high-resolution point clouds, leading to 3D shapes\nwith fine details.\n2.2. Multi-View Diffusion Models\nThe infamous Janus problem [26, 35] and 3D inconsis-\ntency have plagued SDS-based [35] 3D content genera-\ntion. MVDream [39] connects rendered images from dif-\nferent views via a 3D self-attention fashion to constrain\nmulti-view consistency in the generated multi-view images.\nSyncDreamer [22] builds a cost volume that correlates the\ncorresponding features across different views to synchro-\nnize the intermediate states of all the generated images at\neach step of the reverse process. EfficientDreamer [58] and\nTextMesh [46] concatenate canonical views either channel-\nwise or spatially into the diffusion models to enhance 3D\nconsistency. SweetDreamer [21] proposes aligned geom-\netry priors by fine-tuning the 2D diffusion models to be\nviewpoint-aware and to produce view-specific coordinate\nmaps. Our method differs from them in that we generate\nmulti-view depth maps, instead of RGB images, and thus\npropose an efficient epipolar line segment attention tailored\nfor depth maps to enforce 3D consistency.\n3. Method\nIn this section, we introduce our Multi-View Depth Dif-\nfusion Models (MVDD). We first provide an overview of\nMVDD in Sec. 3.1, a model that aims to produce multi-\nview depth. After that, we illustrate how multi-view con-\nsistency is enforced among different views of depth maps\nin our model by using epipolar \u201cline segment\u201d attention\n(Sec. 3.1.1) and denoising depth fusion (Sec. 3.1.2). Fi-\nnally, we introduce the training objectives in Sec. 3.2 and\nimplementation details in Sec. 3.3.\n3.1. Multi-View Depth Diffusion\nOur method represents a 3D shape X by using its multi-\nview depth maps x \u2208 RN\u00d7H\u00d7W = {xv|v = 1, 2, ..., N},\nwhere v is the index of the view, N is the total number of\nviews, and H and W are the depth map resolution. To gen-\nerate a 3D shape that is both realistic and faithful to the di-\nversity distribution, we adopt the diffusion process [12, 42]\nthat gradually denoise N depth maps. These depth maps\ncan be fused to obtain a dense point cloud, which can op-\ntionally be used to reconstruct [14, 34] a high-quality mesh\nmodel. We illustrate the entire pipeline in Fig. 2\nIn the diffusion process, we first create the ground truth\nmulti-view depth diffusion distribution q(x0:T ) in a forward\nprocess. In this process, we gradually add Gaussian noise\nto each ground truth depth map xv\n0 for T steps, obtaining\nN depth maps of pure Gaussian noise xT = {xv\nT |v =\n1, 2, ..., N}. The joint distributions can be factored into a\nproduct of per-view Markov chains:\nq(x0:T ) = q(x0)\nT\nY\nt=1\nq(xt|xt\u22121)\n= q(x1:N\n0\n)\nN\nY\nv=1\nT\nY\nt=1\nq(xv\nt |xv\nt\u22121),\n(1)\nq(xv\nt |xv\nt\u22121) := N(xv\nt ;\np\n1 \u2212 \u03b2txv\nt\u22121, \u03b2tI),\n(2)\n3\nwhere \u03b2t is the step t variance schedule at step t shared\nacross views.\nWe then learn a diffusion denoising model to predict the\ndistribution of a reverse process p\u03b8(x0:T ) to iteratively de-\nnoise the xT back to the ground truth x0. The joint distri-\nbution can be formulated as:\np\u03b8(x0:T ) = p(xT )\nT\nY\nt=1\np\u03b8(xt\u22121|xt)\n= p(x1:N\nT\n)\nN\nY\nv=1\nT\nY\nt=1\np\u03b8(xv\nt\u22121|xv\nt ),\n(3)\np\u03b8(xv\nt\u22121|xv\nt ) := N(xv\nt\u22121; \u00b5\u03b8(xv\nt , t), \u03b2tI),\n(4)\nwhere \u00b5\u03b8(xv\nt , t) estimates the mode of depth map distribu-\ntion for view v at step t \u2212 1.\nHowever, following Eq. (3) and Eq. (4), diffusion pro-\ncess denoises each view independently. Starting from N\nmaps of pure random noise, a well-trained model of this\nkind would generate realistic depth maps x1:N\n0\n, which how-\never could not be fused into an intact shape due to no 3D\nconsistency across views. Therefore, we propose to con-\ndition denoising steps for each view on its R neighboring\nviews xr1:rR\nt\nand replace Eq. (3) and Eq. (4) with:\np\u03b8(x0:T ) = p(x1:N\nT\n)\nT\nY\nt=1\nN\nY\nv=1\np\u03b8(xv\nt\u22121|xv\nt , xr1:rR\nt\n),\n(5)\np\u03b8(xv\nt\u22121|xv\nt , xr1:rR\nt\n) := N(xv\nt\u22121; \u00b5\u03b8(xv\nt , xr1:rR\nt\n, t), \u03b2tI).\n(6)\nMVDD achieves this through an efficient epipolar \u2018line seg-\nment\u2019 attention (Sec. 3.1.1).\nAdditionally, even though\nthe denoising process is multi-view conditioned, back-\nprojected depth maps are still not guaranteed to be per-\nfectly aligned in 3D space. Inspired by multi-view stereo\nmethods [9, 38, 52], MVDD conducts denoising depth fu-\nsion (Sec. 3.1.2) in each diffusion step (Eq. (6)).\n3.1.1\nEpipolar Line Segment Attention\nTo promote consistency across all depth maps, we introduce\nan attention module named epipolar \u201cline segment\u201d atten-\ntion. With the depth value of current step, MVDD leverages\nthis information and attends only to features from visible lo-\ncations on other views. To be specific, we sample on the line\nsegment centered by the back-projected depth, rather than\non the entire epipolar line [39, 47]. This design allows the\nproposed attention to obtain more relevant cross-view fea-\ntures, making it excel in both efficiency and effectiveness.\nThe attention is defined as:\nQ \u2208 R(B\u00d7N\u00d7H\u00d7W )\u00d71\u00d7F ,\nK, V \u2208 R(B\u00d7N\u00d7H\u00d7W )\u00d7(R\u00d7k)\u00d7F ,\nCross-Attn(Q,K, V ) = softmax\n\u0012QKT\n\u221adk\n\u0013\nV,\n(7)\nwhere B is the batch size, N is the total number of views, k\nis the number of samples along the epipolar \u201cline segment\u201d,\nR is the number of neighboring views and F is the number\nof feature channels. At denoising step t, for any pixel vij at\na source depth map xv\nt , we first back project its depth value\nxvij\nt\ninto the 3D space to obtain a 3D point \u03c1vij, and project\nit to a coordinate rmn on neighboring view r:\n\u03c1vij = xvij\nt A\u22121vij, where vij := [i, j, 1]T ,\n(8)\nrmn = A \u03c0v\u2192r\u03c1vij,\n(9)\nwhere \u03c1vij is in the camera coordinate of view v , \u03c0v\u2192r\nis the relative pose transformation matrix and A is the in-\ntrinsic matrix. Since xvij\nt\nis noisy, we select another k \u2212 1\nevenly spaced points around \u03c1vij along the ray and project\nthese points, {\u03c1vij\n1 , ..., \u03c1vij\nk }, into each neighboring view, as\nshown in Fig. 2 (right). The k projected pixels lay on a\nepipolar \u201cline segment\u201d on view r and provides features for\nK, V in Eq. (7).\nCross attention thresholding.\nTo ensure that depth fea-\ntures from a neighboring view r are relevant to xvij\nt , we\nneed to cull only the rmn that are also visible from source\nview v.\nLet z(\u00b7) denote the operator to extract the z\nvalue from a vector [x, y, z], we create the visibility mask\nby thresholding the Euclidean distance between the depth\nvalue of the 3D point in r\u2019s camera coordinate, \u03c1rmn =\n\u03c0v\u2192r\u03c1vij, and the predicted depth value on the pixel rmn\nthat \u03c1r projects onto:\nM(rmn) = \u2225z(\u03c0v\u2192r\u03c1vij) \u2212 xrmn\nt\n\u2225 < \u03c4.\n(10)\nFor projected pixels that do not satisfy the above require-\nment, in Eq. (7), we manually overwrite their attention\nweights as a very small value to minimize its effect.\nDepth concatenation.\nFor pixel vij, since the sampled\npoints {\u03c1vij\n1 , ..., \u03c1vij\nk } query geometric features K, V from\nneighboring views, the attention mechanism conditions the\ndenoising step of xvij\nt\nwith the features V weighted by the\nsimilarity between Q and K.\nTo enhance awareness of\nthe locations of these points, we propose concatenating the\ndepth values {z(\u03c1vij\n1 ), ..., z(\u03c1vij\nk )} to the feature dimension\nof V , resulting in the last dimension of V being F + 1.\nThe intuition behind this is if the geometric features of\nvij are very similar to features queried by \u03c1vij\n1 , the depth\nvalue xvij\nt\u22121 should move toward z(\u03c1vij\n1 ). We empirically\nverify the effectiveness of the depth concatenation in Tab. 3.\n4\n3D-LDM\nOurs\nLION\n(a) Chair\n(b) Airplane\nFigure 3. Our generated meshes exhibit superior quality compared to point cloud diffusion model [55] and implicit diffusion model [29].\n3.1.2\nDenoising Depth Fusion\nTo further enforce alignment across multi-view depth maps,\nMVDD incorporates depth fusion in diffusion steps during\nancestral sampling.\nAssuming we have multi-view depth maps {x1, ..., xN},\nfollowing multi-view stereo methods [27, 54], a pixel vij\nwill be projected to another view r at rmn as described in\nEq. (8). Subsequently, we reproject rmn with its depth value\nxrmn towards view v:\n\u03c1v\u02dci\u02dcj = \u03c0r\u2192vxrmnA\u22121rmn,\n(11)\nv\u02dci\u02dcj = A\u03c1v\u02dci\u02dcj,\n(12)\nwhere \u03c1v\u02dci\u02dcj is the reprojected 3D point in view v\u2019s camera\ncoordinate. To determine the visibility of pixel vij from\nview r, we set two thresholds:\n\r\r\rvij \u2212 v\u02dci\u02dcj\n\r\r\r < \u03c8max, |xvij \u2212 z(\u03c1v\u02dci\u02dcj)|\nxvij\n< \u03f5\u03b8,\n(13)\nwhere z(\u03c1v\u02dci\u02dcj) represents the reprojected depth, \u03c8max and\n\u03f5\u03b8 are the thresholds for discrepancies between reprojected\npixel coordinates and depth compared to the original ones.\nIntegration with denosing steps.\nFor a diffusion step t\ndescribed in Eq. (6), after obtaining \u00b5\u03b8(xt, t), we apply\ndepth averaging.\nFor each pixel, we average the repro-\njected depths from other visible views to refine this pre-\ndicted value. Subsequently, we add N(0, \u03b2tI) on top to ob-\ntain {xv\nt\u22121|v = 1, 2, ..., N}. Only at the last step, we also\napply depth filtering to X0 to filter out the back-projected\n3D points that are not visible from neighboring views.\n3.2. Training Objectives\nAiming to maximize p\u03b8(x0:T ), we can minimize the objec-\ntive, following DDPM [12]:\nLt = Et\u223c[1,T ],x0,\u03f5t\nh\n\u2225\u03f5t \u2212 \u03f5\u03b8 (xt, t)\u22252i\n= Et\u223c[1,T ],x0,\u03f5t\nh\r\r\u03f5t \u2212 \u03f5\u03b8\n\u0000\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5t, t\n\u0001\r\r2i\n,\n(14)\nwhere x0 is the ground-truth multiview depth maps, \u03b2t and\n\u00af\u03b1t := Qt\ns=1 (1 \u2212 \u03b2s) are predefined coefficients of noise\nscheduling at step t.\n3.3. Implementation Details\nOur model is implemented in PyTorch [33] and employs the\nAdam optimizer [17] with the first and the second momen-\ntum set to 0.9 and 0.999, respectively, and a learning rate\nof 2e\u22124 to train all our models. Unless otherwise noted,\nwe set the height H and width W of depth map to both be\n128 and number of views of depth map N to be 8. The first\ncamera is free to be placed anywhere on a sphere, facing\nthe object center, and form a cube with the other 7 cam-\neras. The number of sampled points along the epipolar line\nsegment k is 10. The threshold \u03c4 for cross attention thresh-\nolding is 0.15. We apply denoising depth fusion only in\nthe last 20 steps. For training, we uniformly sample time\nsteps t = 1, ..., T = 1000 for all experiments with cosine\nscheduling [30]. We train our model on 8 Nvidia A100-\n80GB and the model usually converges within 3000 epochs.\nPlease refer to supplemental material for more details on\nnetwork architecture, camera setting, and other aspects.\n5\n4. Application\n4.1. 3D Shape Generation\nInference strategy.\nInitialized as 2D maps of pure Gaus-\nsian noise, the multi-view depth maps can be generated by\nMVDD following ancestral sampling [12]:\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u02dc\u03b1t\n\u03f5\u03b8 (xt, t)\n\u0013\n+\np\n\u03b2t\u03f5, (15)\nwhere \u03f5 follows a isotropic multivariate normal distribution.\nWe iterate the above process for T = 1000 steps, utilizing\nthe effective epipolar \u201cline segment\u201d attention and denois-\ning depth fusion . Finally, we back-project the multi-view\ndepth maps to form a dense (> 20K) 3D point cloud with\nfine-grained details. Optionally, high-quality meshes can be\ncreated with SAP [34] from the dense point cloud.\nDatasets and comparison methods.\nTo assess the perfor-\nmance of our method compared to established approaches,\nwe employ the ShapeNet dataset [4], which is the com-\nmonly adopted benchmark for evaluating 3D shape genera-\ntive models. In line with previous studies of 3D shape gen-\neration [5, 53, 55, 59], we evaluate our model on standard\nshape generation bench mark categories: airplanes, chairs,\nand cars, with the same train/test split. We compare MVDD\nwith state-of-the-art point cloud generation methods such\nas DPM [25], PVD [59] and LION [55], implicit functions-\nbased methods such as IM-GAN [5] and 3D-LDM [29],\nas well as a voxel diffusion model Vox-diff [59]. As our\nmethod generates varying number of points and point cloud\nbackprojected from depth maps is not uniform, we sam-\nple 2048 points from meshes using SAP [34] and measure\nagainst ground-truth points with inner surface removed. For\nthose implicit methods that are not impacted by inner sur-\nface, we directly use the number reported for comparison.\nMetrics.\nWe follow [5, 55, 59] and primarily employ: 1)\nMinimum Matching Distance (MMD), which calculate the\naverage distance between the point clouds in the reference\nset and their closest neighbors in the generated set; 2) Cov-\nerage (COV), which measures the number of reference point\nclouds that are matched to at least one generated shape; 3)\n1-Nearest Neighbor Alignment (1-NNA), which measures\nthe distributional similarity between the generated shapes\nand the validation set. MMD focus on the shape fidelity\nand quality, while COV focus on the shape diversity. 1-\nNNA can assess both quality and diversity of the generation\nresults. For methods generate mesh or voxel, we transform\nit to point cloud and apply these metrics. Please refer to\nsupplemental materials for more details.\nEvaluation.\nWe report the quantitative results of all meth-\nods in Tab. 1.\nDue to space constraints, we defer the\nperformance in metric CD to the supplemental material.\nOur method MVDD exhibits strong competitiveness across\nall categories and surpassed comparison methods, particu-\nlarly excelling in the 1-NNA (EMD) metric. This metric\nholds significant importance as it addresses the limitations\nof MMD and COV [55].\nWe augmented our generated point cloud and visualize\nthe mesh quality in Fig. 3 together with LION[55] and 3D-\nLDM [29]. Our method generates more diverse and plausi-\nble 3D shapes compared with all baselines. The visualiza-\ntion of our meshes shows that our method excels in synthe-\nsizing intricate details, e.g. slats of the chair and thin struc-\nture in chair base. We also visualize point clouds in Fig. 1(a)\nand Fig. 4.\nThe clean point cloud back-projected from\nour generated depth maps demonstrates 3D consistency and\nalso validates the effectiveness of the proposed epipolar\n\u201cline segment\u201d attention and denoising depth fusion . In\ncontrast, the number of points (2048) that can be generated\nby point cloud-based diffusion models [25, 55, 59] limits\ntheir capabilities to capture fine-grained details of the 3D\nshapes.\nGenerated dense point cloud vs up-sampled sparse\npoint cloud.\nSince our method can directly generate 20K\npoints, while LION [55] is limited to producing sparse point\ncloud with 2048 points, we up-sample varying number of\npoints from LION\u2019s meshes.\nWe then compare the per-\nformance of our method with LION. As shown in Fig. 6,\nthe performance of LION deteriorates significantly as the\nnumber of points increases. It is because LION struggles to\nfaithfully capture necessary 3D shape details with its sparse\npoint cloud. In contrast, the performance of our method is\nrobust with the increased number of 3D points and outper-\nforms LION by larger margins as the point cloud density\nincreases.\n4.2. Depth Completion\nInference strategy.\nWe reuse an unconditional generative\nmodel to perform shape completion task, where depth maps\nfrom other views xother can be generated conditioned on\nthe single input view of depth map xin. In each reverse step\nof the diffusion model, we have:\nxin\nt\u22121\n\u223c N\n\u0000\u221a\u00af\u03b1txin\n0 , (1 \u2212 \u00af\u03b1t) I\n\u0001\n,\n1st pass: \u02c6xother\nt\u22121\n\u223c N(\np\n1 \u2212 \u03b2t \u00b5\u03b8(xr1:rR\nt\n, t), \u03b2tI),\n2nd pass: xother\nt\u22121\n\u223c N(\u00b5\u03b8(\u02c6xother\nt\u22121 , xin\n0 , t), \u03b2tI),\n(16)\nwhere xin\nt\u22121 is sampled using the given depth map xin, while\nxother\nt\u22121\nis sampled from the model, given the previous iter-\nation xt. Different from unconditional generation, to en-\nhance the consistency with the input view, we do two passes\n6\n(a) DPM [25]\n(b) PVD [59]\n(c) LION [55]\n(d) MVDD (Ours)\nFigure 4. Unconditional generation on ShapeNet car, airplane and chair category.\nVox-diff [59]\nDPM [25]\n3D-LDM [29]\nIM-GAN [5]\nPVD [59]\nLION [55]\nMVDD (Ours)\nAirplane\nMMD (EMD)\n1.561\n0.990\n3.520\n0.980\n1.000\n0.920 \u2022\n0.920 \u2022\nCOV (EMD)\n25.43\n40.40\n42.60\n52.07\n49.33\n48.27\n53.00 \u2022\n1-NNA (EMD)\n98.13\n73.47\n80.10\n64.04\n64.89\n63.49\n62.50 \u2022\nCar\nMMD (EMD)\n1.551\n0.710\n-\n0.640\n0.820\n0.900\n0.620 \u2022\nCOV (EMD)\n22.15\n36.05\n-\n47.27\n39.51\n42.59\n49.53 \u2022\n1-NNA (EMD)\n96.83\n80.33\n-\n57.04\n71.29\n65.70\n56.80 \u2022\nChair\nMMD (EMD)\n2.930\n2.140\n8.200\n2.200\n2.330\n1.720 \u2022\n2.110\nCOV (EMD)\n21.75\n46.17\n42.20\n49.51\n46.47\n50.52\n51.55 \u2022\n1-NNA (EMD)\n96.74\n65.73\n65.30\n55.54\n56.14\n57.31\n54.51 \u2022\nTable 1. Unconditional generation on ShapeNet categories. MMD (EMD) is multiplied by 102. \u2022 represents the best result.\n\u2026\n(a) Input depth\nmap\n(b) Completed depth\nmaps\n(c) W/o\ndenoising depth\nfusion\n(d) W/ denoising\ndepth fusion\nFigure 5. Depth completion results prove the effectiveness of the\nproposed denoising depth fusion strategy (Sec. 3.1.2).\nto denoise the other views. In the first pass each view at-\ntends to every other views and in the second pass each view\nonly attends to the input view xin. We scale back noise at\nfirst pass, following the Langevin dynamics steps [44, 45].\nDatasets and comparison methods.\nFollowing the ex-\nperimental setup of PVD [59], we use the benchmark pro-\nvided by GenRe [57], which contains renderings of shapes\nin ShapeNet from 20 random views. For shape completion,\n2000\n4000\n6000\n8000\n10000\n# points\n58\n60\n62\n64\n66\n68\n1-NNA (CD)\nMVDD (Ours)\nLION\n2000\n4000\n6000\n8000\n10000\n# points\n54\n56\n58\n60\n62\n64\n66\n68\n1-NNA (EMD)\nMVDD (Ours)\nLION\nFigure 6. We report the performance of our method and LION with\nvarying number of point clouds measured by 1-NNA with CD and\nEMD, respectively, in the ShapeNet [4] chair category.\nas the ground-truth data are involved, Chamfer Distance\nand Earth Mover\u2019s Distance suffice to evaluate the recon-\nstruction results. We select models PointFlow [53], DPF-\nNet [19], SoftFlow [15], and PVD [59] for comparison.\n7\nSoftFlow [15]\nPointFlow [53]\nDPF-Net [19]\nPVD [59]\nMVDD (Ours)\nAirplane\n1.198\n1.180\n1.105\n1.030\n0.900 \u2022\nChair\n3.295\n3.649\n3.320\n2.939\n2.400 \u2022\nCar\n2.789\n2.851\n2.318\n2.146\n1.460 \u2022\nTable 2. Depth completion comparison against baselines. EMD is\nmultiplied by 102. \u2022 represents the best result.\nOurs\nPTI\nNovel frontal\nview image \nRGB\nNovel frontal\nview image \nNovel frontal\nview depth \nNovel frontal\nview shape\nNovel frontal\nview shape\nSource image\nSource image\nSource image\nNovel frontal\nview image \nNovel frontal\nview depth \nNovel frontal\nview shape\nOurs\nPTI\nShape\nDepth\nRGB\nShape\nDepth\nFigure 7. Without proper shape regularization, 3D GAN inver-\nsion [3] using PTI [37] fails to reconstruct input image under ex-\ntreme pose. Our model can serve as a shape prior for 3D GAN\ninversion and yield better reconstruction performance in novel\nfrontal view.\nEvaluation.\nWe show the quantitative results of our\nmethod and baselines in Tab. 2. Our method consistently\noutperforms all the baselines with EMD metric on all cat-\negories. The qualitative results in Fig. 1(b) also showcases\nthat our inference strategy for depth completion can effec-\ntively \u201cpull\u201d the learned depth map of other views to be ge-\nometrically consistent with the input depth map.\n4.3. 3D Prior for GAN Inversion\nWe illustrate how our trained multi-view depth diffusion\nmodel can be plugged into downstream tasks, such as 3D\nGAN inversion [3]. As in the case of 2D GAN inversion,\nthe goal of 3D GAN inversion is to map an input image I\nonto the space represented by a pre-trained unconditional\n3D GAN model, denoted as G3D(\u00b7; \u03b8), which is defined by\na set of parameters \u03b8. Upon successful inversion, G3D has\nthe capability to accurately recreate the input image when\nprovided with the corresponding camera pose. One specific\nformulation of the 3D GAN inversion problem [37] can be\ndefined as follows:\nw\u2217, \u03b8\u2217 = arg max\nw,\u03b8\n= L (G3D(w, \u03c0; \u03b8), I) ,\n(17)\nwhere w is the latent representation in W+ space and \u03c0 is\nthe corresponding camera matrix of input image. w and\n\u03b8 are optimized alternatively, i.e., w is optimized first and\nCross attn.\nDepth concat.\nCross attn. thresholding\nDepth fusion\n1NN-A\n(Sec. 3.1.1)\n(Sec. 3.1.1)\n(Sec. 3.1.1)\n(Sec. 3.1.2)\nCD\nEMD\n%\n%\n%\n%\n92.00\n90.00\n!\n%\n%\n%\n61.78\n59.65\n!\n!\n%\n%\n60.72\n59.00\n!\n!\n!\n%\n59.82\n57.75\n!\n!\n!\n!\n57.90 \u2022\n54.51 \u2022\nTable 3. Ablation study on the chair category. \u2022 is the top result.\nthen \u03b8 is also optimized together with the photometric loss:\nLphoto = L2 (G3D (w, \u03c0s; \u03b8) , Is)\n+ LLPIPS (G3D (w, \u03c0s; \u03b8) , Is) ,\n(18)\nwhere LLPIPS is the perceptual similarity loss [56]. How-\never, with merely supervision from single or sparse views,\nthis 3D inversion problem is ill-posed without proper reg-\nularization, so that the geometry could collapse (shown\nin Fig. 1(c) and Fig. 7 2nd row). To make the inversion look\nplausible from other views, a 3D geometric prior is needed,\nas well as a pairing regularization method which can pre-\nserve diversity. Score distillation sampling has been pro-\nposed in DreamFusion [35] to utilize a 2D diffusion model\nas a 2D prior to optimize the parameters of a radiance field.\nIn our case, we use our well-trained MVDD model as a 3D\nprior to regularize on the multi-view depth maps extracted\nfrom the tri-plane radiance fields. As a result, the follow-\ning gradient direction would not lead to collapsed geometry\nafter inversion:\n\u2207L = \u2207Lphoto + \u2207\u03bbSDSLSDS,\n(19)\nwhere \u03bbSDS is the weighting factor of LSDS [35].\nTo learn the shape prior for this 3D GAN inversion task,\nwe render multi-view depth maps from the randomly gen-\nerated radiance fields of EG3D [3] trained with FFHQ [13]\ndataset. We then use them as training data and train our\nmulti-view depth diffusion model. Using Eq. (19), we per-\nform test-time optimization for each input image to obtain\nthe optimized radiance field. In Fig. 1(c) and Fig. 7, we\nshow the rendering and geometry results of 3D GAN in-\nversion with and without regularization by MVDD. With\nthe regularization of our model, the \u201cwall\u201d artifact is effec-\ntively removed and it results in better visual quality in the\nrendered image from novel frontal view.\n4.4. Ablation study\nWe perform ablation study to further examine the effec-\ntiveness of each module described in the method section.\nSpecifically, in Tab. 3 we report the ablated results of\nepipolar \u201cline segment\u201d attention, depth concatenation, and\ncross attention thresholding (Sec. 3.1.1) and depth fusion\n(Sec. 3.1.2) in ShapeNet chair category for the uncondi-\ntional generation task as we describe in Sec. 4.1. Without\nthe designed cross attention, the model could barely gen-\nerate plausible 3D shapes as measured by 1NN-A metric.\n8\nWith designs such as depth concatenation and cross atten-\ntion thresholding being added, the 3D consistency along\nwith the performance of our model is progressively improv-\ning. Last but not least, denoising depth fusion align the\ndepth maps and further boost the performance. Qualita-\ntively, Fig. 5 illustrates how the denoising depth fusion help\neliminate double layers in depth completion task.\n5. Conclusion\nWe leveraged multi-view depth representation in 3D shape\ngeneration and proposed a novel denoising diffusion model\nMVDD. To enforce 3D consistency among different view\nof depth maps, we proposed an epipolar \u201cline segment\u201d at-\ntention and denoising depth fusion technique. Through ex-\ntensive experiments in various tasks such as shape genera-\ntion, shape completion and shape regularization, we demon-\nstrated the scalability, faithfulness and versatility of our\nmulti-view depth diffusion model.\nReferences\n[1] Tejas Anvekar, Ramesh Ashok Tabib, Dikshit Hegde, and\nUma Mudengudi. Vg-vae: A venatus geometry point-cloud\nvariational auto-encoder. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2978\u20132985, 2022. 2\n[2] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariharan.\nLearning gradient fields for shape generation. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part III 16, pages\n364\u2013381. Springer, 2020. 3\n[3] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 1, 8\n[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 2, 6, 7\n[5] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5939\u20135948, 2019. 2, 6, 7\n[6] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-sdf:\nConditional generative modeling of signed distance func-\ntions. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 2262\u20132272, 2023. 2\n[7] Jian S Dai. Euler\u2013rodrigues formula variations, quaternion\nconjugation and intrinsic connections. Mechanism and Ma-\nchine Theory, 92:144\u2013152, 2015.\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021.\n[9] Silvano Galliani, Katrin Lasinger, and Konrad Schindler.\nMassively parallel multiview stereopsis by surface normal\ndiffusion. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 873\u2013881, 2015. 4\n[10] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 2\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3, 5, 6\n[13] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 8\n[14] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.\nPoisson surface reconstruction. In Proceedings of the fourth\nEurographics symposium on Geometry processing, page 0,\n2006. 3\n[15] Hyeongju Kim,\nHyeonseung Lee,\nWoo Hyun Kang,\nJoun Yeop Lee, and Nam Soo Kim. Softflow: Probabilis-\ntic framework for normalizing flow on manifolds. Advances\nin Neural Information Processing Systems, 33:16388\u201316397,\n2020. 7, 8\n[16] Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong.\nSetvae: Learning hierarchical composition for generative\nmodeling of set-structured data.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15059\u201315068, 2021. 2\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[18] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[19] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete\npoint flow networks for efficient point cloud generation. In\nEuropean Conference on Computer Vision, pages 694\u2013710.\nSpringer, 2020. 2, 7, 8\n[20] Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, and\nBo Dai. E3dge: Self-supervised geometry-aware encoder for\nstyle-based 3d gan inversion. 2023. 2\n[21] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n3\n[22] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2, 3\n9\n[23] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif-\nfusion: Score-based generative 3d mesh modeling.\narXiv\npreprint arXiv:2303.08133, 2023. 2\n[24] David Lopez-Paz and Maxime Oquab. Revisiting classifier\ntwo-sample tests. arXiv preprint arXiv:1610.06545, 2016.\n[25] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021. 2, 3, 6, 7\n[26] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 3\n[27] Paul Merrell, Amir Akbarzadeh, Liang Wang, Philippos\nMordohai, Jan-Michael Frahm, Ruigang Yang, David Nist\u00b4er,\nand Marc Pollefeys.\nReal-time visibility-based fusion of\ndepth maps. In 2007 IEEE 11th International Conference\non Computer Vision, pages 1\u20138. Ieee, 2007. 5\n[28] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-\nham Tulsiani.\nAutosdf:\nShape priors for 3d comple-\ntion, reconstruction and generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 306\u2013315, 2022. 2\n[29] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto\nTono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural im-\nplicit 3d shape generation with latent diffusion models. arXiv\npreprint arXiv:2212.00842, 2022. 2, 5, 6, 7\n[30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 5\n[31] Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wen-\nzel Jakob. Mitsuba 2: A retargetable forward and inverse\nrenderer. ACM Transactions on Graphics (TOG), 38(6):1\u2013\n17, 2019.\n[32] George Papamakarios, Eric Nalisnick, Danilo Jimenez\nRezende, Shakir Mohamed, and Balaji Lakshminarayanan.\nNormalizing flows for probabilistic modeling and inference.\nThe Journal of Machine Learning Research, 22(1):2617\u2013\n2680, 2021. 2\n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n5\n[34] Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer,\nMarc Pollefeys, and Andreas Geiger. Shape as points: A dif-\nferentiable poisson solver. Advances in Neural Information\nProcessing Systems, 34:13032\u201313044, 2021. 3, 6\n[35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 3, 8\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[37] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages.\nACM Transactions on graphics (TOG), 42(1):1\u201313,\n2022. 1, 8\n[38] Johannes L Sch\u00a8onberger,\nEnliang Zheng,\nJan-Michael\nFrahm, and Marc Pollefeys.\nPixelwise view selection for\nunstructured multi-view stereo. In Computer Vision\u2013ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part III 14, pages\n501\u2013518. Springer, 2016. 4\n[39] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2, 3, 4\n[40] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,\nChristoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova,\n2023. 2\n[41] Dong Wook Shu, Sung Woo Park, and Junseok Kwon.\n3d point cloud generative adversarial network based on\ntree structured graph convolutions.\nIn Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 3859\u20133868, 2019. 2\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n3\n[43] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2\n[44] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 7\n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 7\n[46] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 3\n[47] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-\nBin Huang, and Johannes Kopf.\nConsistent view synthe-\nsis with pose-guided diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16773\u201316783, 2023. 4\n[48] Diego Valsesia, Giulia Fracastoro, and Enrico Magli. Learn-\ning localized generative models for 3d point clouds via graph\nconvolution. In International conference on learning repre-\nsentations, 2018. 2\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n10\n[50] Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun,\nFelix Wu, and Kilian Weinberger. An empirical study on\nevaluation metrics of generative adversarial networks. arXiv\npreprint arXiv:1806.07755, 2018.\n[51] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nMech, and Ulrich Neumann. Disn: Deep implicit surface\nnetwork for high-quality single-view 3d reconstruction. Ad-\nvances in neural information processing systems, 32, 2019.\n2\n[52] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 4\n[53] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 4541\u20134550, 2019. 2, 6, 7, 8\n[54] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.\nMvsnet: Depth inference for unstructured multi-view stereo.\nIn Proceedings of the European conference on computer vi-\nsion (ECCV), pages 767\u2013783, 2018. 5\n[55] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\ntent point diffusion models for 3d shape generation. arXiv\npreprint arXiv:2210.06978, 2022. 1, 2, 3, 5, 6, 7\n[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 8\n[57] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh\nTenenbaum, Bill Freeman, and Jiajun Wu. Learning to re-\nconstruct shapes from unseen classes. Advances in neural\ninformation processing systems, 31, 2018. 7\n[58] Minda Zhao,\nChaoyi Zhao,\nXinyue Liang,\nLincheng\nLi, Zeng Zhao, Zhipeng Hu, Changjie Fan, and Xin\nYu.\nEfficientdreamer:\nHigh-fidelity and robust 3d cre-\nation via orthogonal-view diffusion prior.\narXiv preprint\narXiv:2308.13223, 2023. 3\n[59] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021. 1, 2, 3, 6, 7, 8\n11\n"
  },
  {
    "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
    "link": "https://arxiv.org/pdf/2312.04916.pdf",
    "upvote": "5",
    "text": "EE-LLM: Large-Scale Training and Inference of Early-Exit\nLarge Language Models with 3D Parallelism\nYanxi Chen\u2217, Xuchen Pan\u2217, Yaliang Li\u2020, Bolin Ding, Jingren Zhou\n{chenyanxi.cyx, panxuchen.pxc, yaliang.li, bolin.ding, jingren.zhou}@alibaba-inc.com\nAlibaba Group\nAbstract\nWe present EE-LLM, a framework for large-scale training and inference of early-exit large language\nmodels (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in\naccelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by\nsupporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM\nimplements a variety of algorithmic innovations and performance optimizations tailored to early exiting,\nincluding a lightweight method that facilitates backpropagation for the early-exit training objective\nwith pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for\ncomputation related to early-exit layers, and two approaches of early-exit inference that are compatible\nwith KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM\nachieves great training efficiency with negligible computational overhead compared to standard LLM\ntraining, as well as outstanding inference speedup without compromising output quality. To facilitate\nfurther research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.1\nContents\n1\nIntroduction\n2\n1.1\nGoal and motivations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nChallenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nMain contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nPreliminaries\n3\n2.1\nTransformers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nEarly-exit LLMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.3\nMegatron-LM and 3D parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nAn overview of EE-LLM\n5\n3.1\nModel architectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.3\nInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4\nTraining\n6\n4.1\nBackpropagation through pipeline stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nTraining efficiency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nAdvanced features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n5\nInference\n13\n5.1\nMain challenge: KV caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5.2\nA new solution: pipeline parallelism\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n\u2217Co-first authors.\n\u2020Corresponding author.\n1We will continuously update the codebase and arXiv version.\n1\narXiv:2312.04916v2  [cs.LG]  1 Feb 2024\n6\nImplementations\n14\n6.1\nModel architectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.2\nPipeline scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.3\nInference service\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n7\nExperiments\n15\n7.1\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n7.2\nInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n8\nRelated works\n21\n9\nConclusions\n21\nA Analysis of training efficiency\n22\nA.1 Training time per iteration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nA.2 Peak GPU memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nB Supplementary materials for Section 4.3.2\n24\nB.1\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nB.2\nAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n1\nIntroduction\nLarge language models (LLMs) have amazed the world with their astonishing abilities and performance in\nsolving a wide range of problems [8, 50, 11, 89, 74, 75]. This is accompanied by excessive costs and carbon\nemissions for training and deploying these models, as their sizes grow rapidly in recent years. In general,\ncosts for inference are dominant in the long run, as each model will be deployed to solve many problems\nfor a long period of time. This has inspired researchers and engineers to develop various approaches for\naccelerating LLM inference.\nThe focus of this work is early exiting, which accelerates inference by allowing a deep neural network to\nmake predictions and exit early in the network for certain inputs, without running the forward pass through\nthe full network. This is achieved by augmenting a standard neural network architecture (with a single exit\nat the end) with additional early-exit layers that transform intermediate hidden states into early outputs.\nThe early-exit model architecture, as visualized in Figure 1, not only retains the full capacity of a large\nmodel, but is also capable of adaptively using a smaller amount of computation for solving simpler problems.\nThe idea of early exiting is a natural analogue of how human speaks, thinks, and make decisions: not every\nproblem requires or deserves the same amount of consideration, and one shall opt for fast reaction to simple\nproblems without overthinking [31]. Early exiting has been an active research area and widely applied in\nnatural language processing, computer vision, and other areas [23, 45, 25, 94, 19, 63, 81, 62, 82, 42, 26, 73,\n27, 31, 38, 60, 24, 83, 14]. More recently, it starts to gain attention in the LLM domain [62, 13, 3, 76], and\nis recognized as a promising direction for further reducing the latency and costs of LLM inference [53].\n1.1\nGoal and motivations\nThe primary goal of this work is to build the infrastructure for scaling up training and inference of early-\nexit LLMs. This is motivated by the observation that the sizes of early-exit models in prior works are still\nrelatively small. The largest early-exit LLM that we are aware of is a Llama 2 model [75] with 13 billion\n(13B) parameters [76]; in contrast, standard LLMs at much larger scales, e.g. the 175B GPT-3 [8], 530B\nMegatron-Turing NLG [68], 540B PaLM [11], or even larger sparsely activated models, have been well trained\nand deployed in many applications. It is an urgent need for the community to truly understand the efficacy of\nearly exiting for LLMs at larger scales, which is indispensable for making early exiting a useful and practical\noption in complex scenarios that only sufficiently large LLMs can handle.\n2\n1.2\nChallenges\nThe first and foremost question is how to train an early-exit LLM that is too large to fit into the memory of\none single device (e.g. GPU). While state-of-the-art frameworks like Megatron-LM [67, 49, 36], DeepSpeed\n[58, 68], Mesh-TensorFlow [65], Alpa [92], InternLM [72], and many more, support training standard LLMs at\nlarge scales with data parallelism and model parallelism (including tensor, sequence and pipeline parallelism),\nthey do not provide native support for early-exit LLMs. One particular challenge lies in pipeline parallelism\n[47, 48, 20, 41], which partitions the model along the depth dimension into multiple pipeline stages, connected\nby limited point-to-point communication between devices; this seems to contradict with early-exit models,\nas the early-exit training loss is typically an aggregation of losses for multiple (early or final) exits that are\nnow located separately on different pipeline stages. Despite the necessity of pipeline parallelism in many\nscenarios, we are not aware of any implementation that supports training early-exit LLMs with pipeline\nparallelism.\nMoreover, training efficiency for early-exit LLMs requires special design. While sizes of early-exit layers\nare often regarded as negligible for many neural network architectures, this is not the case for LLMs, where\neach early exit contains (at least) a large output embedding matrix that transforms hidden states into\nlogits on the vocabulary. A naive implementation of early-exit LLM training can cause large computational\noverhead compared to standard LLM training.\nFinally, with regards to autoregressive generation tasks (where tokens are generated one by one, depending\non previously generated tokens via the attention mechanism), a naive implementation of early-exit inference is\nnot compatible with KV caching, a standard technique of storing the keys and values of previously generated\ntokens at each layer. More specifically, if the current token is generated via early exiting at some layer, then\nits KV caches in later layers are missing, which hinders the generation of future tokens. Given that KV\ncaching is enabled by default in most cases, the efficacy of early exiting for autoregressive generation might\nbe questionable if its conflict with KV caching is not well resolved.\n1.3\nMain contributions\nWe propose EE-LLM, a system for large-scale training and inference of early-exit (EE) LLMs with 3D par-\nallelism, which is designed to tackle the aforementioned challenges. EE-LLM is built upon Megatron-LM\n[67, 49, 36, 68], and augments it with various functionalities for early exiting. In addition to compatibility\nwith existing functionalities of 3D parallelism provided by Megatron-LM, EE-LLM also implements a variety\nof algorithmic innovations, including a lightweight method that facilitates backpropagation for the early-\nexit training objective through pipeline stages, various techniques of leveraging idle resources in the original\npipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that\nare compatible with KV caching (with one based on a novel form of pipeline parallelism and another based on\nKV recomputation). Implementation of EE-LLM has been well optimized for maximum training and inference\nefficiency. Our analytical and empirical study confirms that, with negligible computational overhead (caused\nby early-exit layers) during training with 3D parallelism, one obtains an early-exit LLM that generates tokens\nwith adaptive token-wise exit selection, achieving outstanding speedup without compromising output quality\nduring inference. In other words, EE-LLM facilitates training and inference of early-exit LLMs that are as large\nas the maximum sizes of standard LLMs allowed by Megatron-LM, given the same amount of computational\nresources. The source code for EE-LLM can be found at https://github.com/pan-x-c/EE-LLM.\n2\nPreliminaries\n2.1\nTransformers\nThe Transformer architecture [77, 71] has been playing a dominant role in natural language processing (NLP)\nand large language models (LLMs) [7, 90]. It is typically composed of an input embedding layer, a stack of\nTransformer layers, and finally an output layer. Each Transformer layer consists of cross-attention and/or\nself-attention modules [4, 34, 52], a multi-layer perceptron (MLP), and layer normalization (LayerNorm\n[2] or RMSNorm [88]). Transformers can be categorized into three types: encoder-only, encoder-decoder,\nand decoder-only.\nFor the latter two, there is an output embedding matrix in the output layer, which\n3\n\ud835\udf3d!\n\ud835\udc99!\n\ud835\udc90\"\n\ud835\udf3d#\n\ud835\udc99#\n\ud835\udf3d\"\n\ud835\udc99\"\nInput\n\ud835\udf53!\n\ud835\udf53#\n\ud835\udf53\"\n\ud835\udc90#\n\ud835\udc90!\nEarly-exit \nlayers\nFigure 1: The model architecture of an early-exit LLM. New components related to early exiting, which are\nabsent from a standard LLM, are highlighted in blue color. In this figure, each \u03b8i represents a sequence of\nTransformer layers in the backbone of the LLM (with some additional modules in \u03b81 for input processing);\nin addition, each \u03d5i represents an early-exit or final-output layer that converts the hidden state xi into an\noutput oi, e.g. logits for next-token prediction.\ntransforms hidden states into logits on a (typically large) vocabulary that can be used for generating tokens.\nAn LLM can be learned by unsupervised pre-training, e.g. minimizing the negative log-likelihood of next-\ntoken prediction on a large corpus [55, 56]. In this work, we focus on the decoder-only generative pre-training\n(GPT) Transformer architecture [55, 56], though many of our ideas are widely applicable to other Transformer\narchitectures or generic deep neural networks.\n2.2\nEarly-exit LLMs\nAs mentioned earlier, an early-exit neural network can be obtained by adding to a standard nerual network\nsome early-exit layers that turn intermediate hidden states into early outputs [81, 63]. During inference for\na given input, the model starts a forward pass and decides (at each early exit) whether to return an output\nor continue forwarding via certain rules, e.g. to return an output whenever the confidence of prediction is\nabove a pre-defined threshold [63, 61].\nThe standard way of training an early-exit model is to minimize a weighted sum of early-exit and final-\nexit training losses [63, 61]. Note that early-exit layers bring additional computational overhead to training.\nThis is especially the case for LLMs, primarily due to the large output embedding matrix of size h \u00d7 V\nwithin each early-exit layer, where h is the hidden dimension and V is the vocabulary size. We call an early-\nexit layer minimalistic if it has the same structure as the final output layer of the GPT model architecture\n[55, 56], which includes an output embedding matrix, plus an optional LayerNorm/RMSNorm in front of it.\nAdditional modules can be added to early-exit layers for increased expressivity and adaptivity of early exits.\n2.3\nMegatron-LM and 3D parallelism\nMegatron-LM [67, 49, 36, 68] is one of the state-of-the-art systems for large-scale training of LLMs with\n3D parallelism on a massive number of GPUs. With data parallelism, each GPU handles the forward and\nbackward computation for one part of the data batch, and then the results are aggregated at the end of the\ntraining iteration. When the model is too large to fit in a single GPU, model partitioning becomes necessary\nand can be used in conjunction with data parallelism. With tensor (and sequence) parallelism, each large\nmodule (e.g. a linear layer) is divided into multiple pieces that are assigned to different GPUs, so that each\ncomputational task related to it (e.g. large matrix multiplication) can be divided into smaller tasks and\nsolved in parallel. One major limitation with tensor (and sequence) parallelism is that it requires expensive\ncollective communication such as all-reduce operations, and thus is only viable for high-end GPUs within\nthe same computing node, with high-bandwidth communication among them.\nPipeline parallelism [47, 48, 20, 41], on the other hand, partitions a deep model along the depth dimension\ninto multiple pipeline stages. Moreover, each data batch is divided into multiple microbatches, and their\nforward/backward computational tasks are scheduled among those multiple pipeline stages. More specifically,\neach stage performs the forward computation for each microbatch and sends the resulted hidden states to\nanother stage; later on, it performs the backward computation for the same microbatch after receiving\nthe gradients of the training objective with respect to the sent hidden states. Pipeline parallelism only\n4\nrequires sparse and inexpensive point-to-point (P2P) communication between pipeline stages, which makes\nit applicable and oftentimes must-have in much broader scenarios when tensor (and sequence) parallelism\nis infeasible or insufficient, whether in GPU clusters or in decentralized settings [86, 84]. The main concern\nwith pipeline parallelism is its low utilization rate of computational resources, due to pipeline bubbles and\nload imbalance across pipeline stages [49]; this will be discussed in more details in Section 4.2.\n3\nAn overview of EE-LLM\nThis section provides an overview of our system for scaling up sizes, training and inference of early-exit LLMs,\nwith flexible configurations and a wide range of functionalities. More details about training and inference\nwill be introduced in Sections 4 and 5, respectively. Implementation details can be found in Section 6.\n3.1\nModel architectures\nWe implement in EE-LLM an early-exit Transformer architecture (which is built upon the GPT Transformer\narchitecture [55, 56] originally implemented in Megatron-LM), with support for various configurations. In\nparticular, users can (1) specify arbitrary layers to add early exits to; (2) add trainable modules to each\nearly-exit layer (between the hidden states on the backbone and the minimalistic early-exit output head),\nsuch as an MLP or a complete Transformer layer; and (3) choose to tie [54, 61, 76] or untie the input and\noutput embedding matrices of all early-exit/final-exit layers. Each particular configuration has its own pros\nand cons, as will be discussed in later sections. With this in mind, EE-LLM has been designed to cover a wide\nrange of common configurations, so that users can easily try them out and choose the most suitable ones for\ntheir own use cases.\n3.2\nTraining\nEE-LLM contains the essential functionalities for training early-exit LLMs, which tackle the main challenges\noutlined in Section 1.2, i.e. how to train with 3D parallelism (especially pipeline parallelism) while minimizing\nthe computational overhead (compared to standard LLM training) caused by early-exit layers. In addition\nto substantial engineering efforts for compatibility with existing functionalities in Megatron-LM, we design\nand implement a simple yet effective algorithm that facilitates pipeline parallelism with multiple early-exit\nor final-output training losses located on different pipeline stages, which is not possible in standard pipeline\nparallelism implemented by Megatron-LM or other frameworks. Moreover, our analytical and empirical study\nshows that training an early-exit LLM with EE-LLM is almost as efficient as training a standard LLM, in\nterms of training time and peak GPU memory; this is achieved by various performance optimizations that we\ndesign and implement in EE-LLM, especially the ones that passively or actively leverage idle computational\nresources in pipeline parallelism. Finally, EE-LLM contains some advanced features for more fine-grained\ncontrol or optimization of the training procedure, including the option of changing early-exit loss weights\nduring training, and a novel method of further improving resource utilization by filling pipeline bubbles with\nuseful computation.\n3.3\nInference\nWe design and implement two methods to tackle the major challenge of early-exit LLM inference for autore-\ngressive generation, namely the conflict between early exiting and KV caching (as explained in Section 1.2).\nOne method is based on KV recomputation, which runs the forward pass with a list of recent tokens when\ngenerating each token, and can be regarded as a variant of synchronized parallel decoding recently proposed\nin [3]. The other method is based on a novel form of pipeline parallelism, which parallelizes the forward\npass of the current token at a certain pipeline stage with some KV-related computation (if any) of previous\ntokens at later stages.\n5\n4\nTraining\nIn this section, we first introduce (in Section 4.1) the essentials of scaling up early-exit LLM training with\nmassive 3D parallelism. In particular, we demonstrate how to leverage pipeline parallelism via a novel ap-\nproach of executing backpropagation for the early-exit training loss through pipeline stages. Next, we show\n(in Section 4.2) how to maximize training efficiency via various performance optimizations, e.g. choosing\nwisely where to add early exits, and scheduling computation in the correct order. By leveraging idle com-\nputational resources in pipeline parallelism, training an early-exit LLM bears negligible overhead compared\nto training a standard LLM, as confirmed by our analytical and empirical study. Finally, we explore (in\nSection 4.3) some advanced features implemented in EE-LLM, including the option to change the weights\nof early-exit losses during the training process, and a novel approach of improving hardware utilization by\nfilling pipeline bubbles with useful computation for additional microbatches.\n4.1\nBackpropagation through pipeline stages\nThe standard objective function for training an early-exit model is a weighted sum of losses at early and\nfinal exits. More formally, to train an early-exit LLM with N exits (including the final output), we aim to\nsolve\nmin\nL :=\nX\ni\u2208[N]\nwiLexit\ni\n,\n(1)\nwhere [N] = {1, 2, . . . , N}, and each Lexit\ni\nis a standard loss function for LLM pre-training (e.g. negative\nlog-likelihood of next-token prediction), calculated with outputs from the i-th exit. The loss weights {wi}\nare hyperparameters specified by the user.\nOur implementation of training, i.e. optimizing the loss function in Eq. (1), is compatible with all types\nof parallelism in Megatron-LM. Indeed, with some engineering efforts, existing functionalities in Megatron-\nLM for data and tensor/sequence parallelism are directly applicable. The major challenge lies in pipeline\nparallelism, since it is not immediately clear how to calculate gradients for Eq. (1) via backpropagation\nthrough pipeline stages. In a single-GPU scenario with vanilla PyTorch, one simply needs to define loss\nas the weighted sum of all early-exit and final-exit losses, and then run loss.backward() for gradient\ncalculation.\nThis is obviously not feasible with pipeline parallelism, since losses {Lexit\ni\n} are located on\ndifferent pipeline stages, and there is only limited P2P communication between each pair of adjacent stages\n(as explained in Section 2.3). On the other hand, Megatron-LM only supports backpropagation for a single\nloss function defined in the last pipeline stage.\n4.1.1\nMethodology\nTo tackle this challenge, we propose a simple yet effective algorithm that instructs each pipeline stage to\ncalculate the desired gradients correctly, without any additional communication overhead between stages.\nTo explain our method, let us first re-write the early-exit training loss defined in Eq. (1) as\nL =\nX\ni\u2208[K]\nLi,\n(2)\nwhere K represents the number of pipeline stages, and each Li is itself a weighted sum of one or multiple\nearly/final-exit losses within Stage i.2 Consider one data sample x for simplicity, and each loss function is\ncalculated with x, i.e. Li = Li(x); in addition, let xi be the hidden states that Stage i calculates and sends\nto its next stage during the forward step. Then, during the backward step, Stage i receives some gradient\ntensor gi from Stage i + 1, defines some auxiliary loss Laux\ni\n, and performs usual backward computation for\nLaux\ni\n. The auxiliary losses {Laux\ni\n}i\u2208[K] and gradient tensors {gi}i\u2208[K\u22121] are defined inductively:\nLaux\nK := LK,\nand\n(3a)\nLaux\ni\n:= Li + \u27e8gi, xi\u27e9,\nwhere\ngi := \u2202Laux\ni+1\n\u2202xi\n,\ni = K \u2212 1, K \u2212 2, . . . , 2, 1.\n(3b)\n2It is easy to check that this is equivalent to the loss defined in Eq. (1). In fact, each Li can be a general objective function,\nas long as it is defined locally in Stage i; our analysis in the following still holds true in this general case.\n6\n\ud835\udf3d!\n\ud835\udc99!\n\ud835\udc3f!\n\ud835\udf3d\"\n\ud835\udc99\"\n\ud835\udc3f\"\n\ud835\udf3d#\n\ud835\udc99#\n\ud835\udc3f#\n\ud835\udf3d$\n\ud835\udc3f$\nInput\nStage 1\nStage 4\nStage 3\nStage 2\n\ud835\udc88!\n\ud835\udc88\"\n\ud835\udc88#\nforward\nbackward\nFigure 2: A high-level visualization for the process of backpropagation, including forward and backward\npasses, for an early-exit model partitioned into four pipeline stages.\nIntuitively, the linear term \u27e8gi, xi\u27e9, i.e. the sum of entrywise product between gi and xi, summarizes infor-\nmation about the gradients of all losses located in later stages. Note that gi is regarded by Stage i as a\nconstant tensor, and no gradient is calculated with respect to it.\nA visualization of this process can be found in Figure 2. It has the same P2P communication scheme as\nin the case of training a standard LLM with pipeline parallelism; the only difference is how each gradient\ntensors gi is defined locally in Stage i + 1. In the following, we formally prove that the proposed method\nleads to correct gradient calculation for the training objective in Eq. (2).\n4.1.2\nRationale\nLet us first prove the correctness of our solution under the assumption that there is no tied/shared parameter\nacross pipeline stages, just for simplicity; we will see very soon that this assumption is not essential and can\nbe safely discarded.\nProposition 1. Suppose that there is no tied/shared parameter across pipeline stages, and consider the\nauxiliary losses defined in Eq. (3). Then, for any i \u2208 [K] and any model parameter or activation tensor z\nin Stage i, it holds that\n\u2202Laux\ni\n\u2202z\n=\n\u2202(PK\nj=i Lj)\n\u2202z\n.\n(4)\nNotice that for any model parameter z in Stage i, one has \u2202Lj/\u2202z = 0 for any j < i, due to the sequential\nstructure of early-exit LLMs (or other deep neural networks). Combining this with Eq. (2) and (4) yields\n\u2202Laux\ni\n\u2202z\n=\n\u2202(PK\nj=i Lj)\n\u2202z\n= \u2202L\n\u2202z ,\nwhich implies the correctness of gradient calculation for the training objective L.\nProof of Proposition 1. The claim of Eq. (4) is obviously true for the base case i = K, by definition of\nLaux\nK = LK. Let us prove by induction for the remaining stages. Suppose that Eq. (4) holds true for Stage\ni + 1, namely\n\u2202Laux\ni+1\n\u2202z\n=\n\u2202(PK\nj=i+1 Lj)\n\u2202z\n.\nTo prove Eq. (4) for Stage i, first note that by definition of gi, we have\ngi = \u2202Laux\ni+1\n\u2202xi\n=\n\u2202(PK\nj=i+1 Lj)\n\u2202xi\n.\nThen, for any model parameter or activation tensor z in Stage i, the following holds:\n\u2202Laux\ni\n\u2202z\n= \u2202(Li + \u27e8gi, xi\u27e9)\n\u2202z\n= \u2202Li\n\u2202z + \u2202\u27e8gi, xi\u27e9\n\u2202xi\n\u2202xi\n\u2202z = \u2202Li\n\u2202z + gi\n\u2202xi\n\u2202z\n7\n= \u2202Li\n\u2202z +\n\u2202(PK\nj=i+1 Lj)\n\u2202xi\n\u2202xi\n\u2202z = \u2202Li\n\u2202z +\n\u2202(PK\nj=i+1 Lj)\n\u2202z\n=\n\u2202(PK\nj=i Lj)\n\u2202z\n.\nThe above lines simply follow the definition of Laux\ni\nand the chain rule. This concludes our proof of Eq. (4)\nfor Stage i, and thus our proof for the proposition.\nLet us move on to relax the assumption. In the broader scenario with tied/shared model parameters\n(e.g. word embedding matrices [54]) across pipeline stages, gradient calculation via backpropagation is equiv-\nalent to the following two-step procedure (as is implemented in Megatron-LM): (1) compute gradients as\nif all parameters are untied, then (2) sum up and synchronize gradients for tied parameters via all-reduce\noperations. Hence our proposed auxiliary-loss approach, when applied to the first part of the above two-step\nprocedure, is still valid.\n4.1.3\nChoices of pipeline schedules\nOur previous analysis shows how to modify the forward and backward steps for each microbatch, in order to\nexecute backpropagation of the early-exit training loss through pipeline stages. In principle, this lightweight\nmodification can be applied to general pipeline schedules of forward and backward steps for multiple micro-\nbatches. The classical 1F1B (one-forward-one-backward) schedule, also called PipeDream-Flush [47, 20, 49],\nachieves a good balance of algorithmic simplicity, training time, memory usage and communication latency,\nin comparison to other schedules such as GPipe [28] (larger activation memory) and interleaved 1F1B [49]\n(higher memory and communication requirements). Therefore, we focus on the 1F1B schedule throughout\nthis work, for concreteness; a visualization can be found in Figure 3(a).\n4.2\nTraining efficiency\nIn the following, we analyze the training time and peak GPU memory of training an early-exit model with\npipeline parallelism, and propose some performance optimizations. We refer readers to the literature (e.g. the\nMegatron-LM series [67, 49, 36]) for thorough study of training efficiency with 3D parallelism for standard\nLLMs without early exits; we will use that as a baseline, and focus on the additional computational overhead\ncaused by early exits.\nLet us first identify the major sources of low resource utilization in the original 1F1B pipeline schedule\nfor training a standard LLM, which lays the foundation for our analysis later.\n\u2022 Explicit bubbles, i.e. light gray areas in Figure 3(a), during which GPUs are idle. This is the most\nnotable and well recognized source of low resource utilization in pipeline parallelism.\n\u2022 Implicit bubbles, i.e. dark gray areas in Figure 3(a). This is caused by load imbalance across pipeline\nstages, even though Transformer layers are evenly divided into stages in Megatron-LM3. In particular,\nthe first stage has the additional computation for the input embedding layer, and more importantly,\nthe last pipeline stage has the additional computation for the output logits (via the output embedding\nlayer) as well as the training loss. For LLMs, these additional computational costs are not negligible,\nprimarily due to large vocabulary sizes.\n\u2022 Idle memory. Memory usage is also imbalanced across pipeline stages: earlier stages in the 1F1B\nschedule have to save the intermediate activations for more microbatches, and the first/last stage has\nto save an extra input/output embedding layer, plus the corresponding gradients and optimizer states.\nAs a result, the first stage is typically the bottleneck of peak memory usage, while later stages have\nidle memory [36, Section 4.2.3].\nRemark 1. In our analysis, we assume no activation recomputation [10, 36] for simplicity. For clarity, we also\nignore the P2P communication latency between pipeline stages, which is generally not the major concern for\nefficiency of pipeline parallelism.\n3For simplicity, we do not consider other flexible ways of dividing a LLM into pipeline stages, as dividing Transformer layers\nevenly remains one of the most practical and robust option in practice. We also do not consider the case where the input/output\nembedding matrix occupies a separate pipeline stage, although our techniques for training an early-exit LLM can be generalized\nto this case in principle.\n8\nBackbone forward\nExit forward\nExit backward\nBackbone backward\nExplicit bubbles\nImplicit bubbles\nStage 1\n1\n2\n3\n4\n1\n5\n2\n6\n3\n4\n5\n6\nStage 2\n1\n2\n3\n1\n4\n2\n5\n3\n6\n4\n5\n6\nStage 3\n1\n2\n1\n3\n2\n4\n3\n5\n4\n6\n5\n6\nStage 4\n1 1 1\n1\n2 2 2\n2\n3 3 3\n3\n4 4 4\n4\n5 5 5\n5\n6 6 6\n6\nForward step\n1 1\nBackward step\n1\n1\n(a) The original 1F1B pipeline schedule\nStage 1\n1\n2\n3\n4\n1\n5\n2\n6\n3\n4\n5\n6\nStage 2\n1 1 2 2 3 3\n1\n1\n4 4 2\n2\n5 5 3\n3\n6 6 4\n4\n5\n5\n6\n6\nStage 3\n1 1 2 2\n1\n1\n3 3 2\n2\n4 4 3\n3\n5 5 4\n4\n6 6 5\n5\n6\n6\nStage 4\n1 1 1\n1\n2 2 2\n2\n3 3 3\n3\n4 4 4\n4\n5 5 5\n5\n6 6 6\n6\nForward step\n1 1\nBackward step\n1\n1\n(b) Adding one early exit to each middle stage (i.e. Stages 2 and 3)\nStage 1\n1\n2\n3\n4\n1\n5\n2\n6\n3\n4\n5\n6\nStage 2\n1\n2\n3\n1 1\n1\n4 2 2\n2\n5 3 3\n3\n6 4 4\n4\n5 5\n5\n6 6\n6\nStage 3\n1\n2\n1 1\n1\n3 2 2\n2\n4 3 3\n3\n5 4 4\n4\n6 5 5\n5\n6 6\n6\nStage 4\n1 1 1\n1\n2 2 2\n2\n3 3 3\n3\n4 4 4\n4\n5 5 5\n5\n6 6 6\n6\nForward step\n1\nBackward step\n1 1\n1\n(c) Deferring forward computation of early-exit layers to backward steps\nFigure 3:\nOne iteration of the 1F1B pipeline schedule in different cases.\nWithin one iteration, each\nstage goes through a warm-up phase (forward steps of the beginning microbatches), a steady 1F1B (one-\nforward-one-backward) phase, and a cool-down phase (backward steps of the final microbatches). \u201cBack-\nbone forward/backward\u201d stands for computation of Transformer layers on the backbone, while \u201cExit for-\nward/backward\u201d stands for computation of early-exit or final-exit layers. The number in each block denotes\nthe index of the corresponding microbatch. Critical paths are marked by dashed red lines. In this figure,\nthe degree of pipeline parallelism is set to 4, and each data batch is divided into 6 microbatches. It is\nassumed that for each stage, the ratio of time between forward computation for the backbone and forward\ncomputation for the early/final-exit layer is 2:1; moreover, we adopt the common assumption that the ratio\nof time between forward and backward computation is 1:2. Note that our analytical study does not rely on\nthe specific values of these ratios. For clarity, we ignore computation related to the input embedding layer,\nas well as P2P communication latency between pipeline stages.\n9\n4.2.1\nUtilization of idle resources\nAt first glance, one might expect that adding early exits to an LLM will incur a training overhead, in terms of\ntime and memory, that is (at least) proportional to the number of additional model parameters. Fortunately,\nthis is not the case for training with pipeline parallelism, based on the above analysis of idle resources in the\n1F1B pipeline schedule. Indeed, adding one minimalistic early-exit layer (which, as defined in Section 2.2,\nhas the same structure as the final output layer) to some middle (i.e. not the first or last) stage will only make\nits model size and theoretical forward/backward time match exactly those of the last stage. Therefore, the\naforementioned implicit bubbles and some of the idle memory can be automatically utilized for computation\nrelated to the early-exit layers, leading to more balanced load across pipeline stages.\nMore specifically, the overhead to training time caused by additional early exits can be negligible. If we\nchoose k middle stages and add one minimalistic early-exit layer to each of them, then the training time per\niteration, i.e. time for processing one data batch, will (in theory) increase only by k \u00d7 (fEE + bEE), where fEE\nand bEE represent the time needed for one forward and backward pass of one microbatch for one minimalistic\nearly-exit layer, respectively. To see this, first notice that the computation of early-exit layers in the steady\n1F1B phase can be perfectly fit into the implicit bubbles. Therefore, the critical path remains the same as in\nthe case without early exits, which consists of (1) the forward steps on all stages for the first microbatch, (2)\nthe 1F1B steady phase on the last stage, and (3) the backward steps on all stages for the last microbatch.\nThe early-exit layers, located separately on k middle stages, will cause a k \u00d7 fEE overhead to the first part\nof the critical path, and a k \u00d7 bEE overhead to the third part, leading to the aforementioned claim on the\noverhead to training time. See Figures 3(a) and (b) for a visualization.\n4.2.2\nFurther performance optimizations\nReducing activation memory overhead.\nSo far, we have analyzed training time and memory usage\nby model parameters. Another major component of memory usage, namely the memory for gradients and\noptimizer states, can be bounded by the memory for model parameters multiplied by a universal constant.\nOne remaining issue that we have not addressed is the activation memory overhead due to early-exit com-\nputation. Most notably, the early-exit logits for one microbatch have size s\u00d7b\u00d7V , where s is the maximum\nsequence length, b is the microbatch size, and V is the vocabulary size. If the i-th stage has one early exit,\nthen a vanilla implementation of training with early exits using the 1F1B pipeline schedule (as shown in\nFigure 3(b)) will cause a significant memory overhead of size s \u00d7 b \u00d7 V \u00d7 (P \u2212 i + 1), where P \u2212 i + 1 is the\nnumber of in-flight microbatches [36] for Stage i.\nOur solution for resolving this issue is simple: deferring the forward computation of each early-exit layer\nfor each microbatch from the forward step to the corresponding backward step. Note that this is feasible,\nbecause forward computation of the next stage only requires as input the hidden states returned by the\ncurrent stage, while the results of early-exit forward computation are optional. By adjusting the order of\ncomputation in this way, it is guaranteed that the early-exit logits for each microbatch are generated, used,\nand discarded immediately, within the same backward step; consequently, the activation memory overhead\ndue to early-exit logits is reduced from s\u00d7b\u00d7V \u00d7(P \u2212i+1) to s\u00d7b\u00d7V . As long as this amount of memory\nusage is less than the activation memory of all Transformer layers within one stage for one microbatch (and\nno early exit is added to the first stage), the peak memory across all pipeline stages will stay unchanged,\nsince the first stage remains the bottleneck of memory usage.\nRemark 2. One can check that, with the above adjustment, our analysis in Section 4.2.1 about the training\ntime overhead caused by early exits remains valid after minor modifications. More specifically, the time\noverhead of one training iteration is still k \u00d7 (fEE + bEE); the only difference is that this whole overhead\ncomes from the backward steps of the last microbatch, i.e. the third part of the critical path in Figure 3(c).\nOne can further reduce this overhead to k \u00d7 bEE by moving the forward pass of the early-exit layer on each\nstage for each cool-down microbatch to the explicit bubble in front of the corresponding backward step\n(i.e. before communication with the next stage). With that said, our implementation does not include this\nmodification, as it brings limited gains at the cost of complicating our codebase.\nSome rules of thumb.\nBelow are some tips for maximizing training efficiency.\n10\n1\n2\n3\n4\nPipeline stage\n5.0\n5.2\n5.4\n5.6\n5.8\n6.0\nForward time (s)\nStandard LLM\nEarly-exit LLM\n1\n2\n3\n4\nPipeline stage\n10.0\n10.4\n10.8\n11.2\nBackward time (s)\nStandard LLM\nEarly-exit LLM\n1\n2\n3\n4\nPipeline stage\n40\n48\n56\n64\nGPU memory (GB)\nStandard LLM\nEarly-exit LLM\nFigure 4: The forward time, backward time, and peak GPU memory of each pipeline stage for a standard\n7B GPT Transformer, as well as its early-exit version that has one minimalistic early-exit layer (without\nlayer normalization) added to each middle stage. Degrees of pipeline, tensor and data parallelism are 4, 1,\nand 1, respectively; the microbatch size is 2, global batch size is 128, and sequence length is 2048. Note that\nthe forward computation of early-exit layers has been deferred to the backward steps, hence not included in\n\u201cforward time\u201d of the first plot, but in \u201cbackward time\u201d of the second plot.\n\u2022 If possible, add early exits to the middle stages rather than to the first or last one. For example,\nadding one early exit to the end of the first stage leads to the same model architecture as adding to\nthe beginning of the second stage, but the latter has higher training efficiency due to more balanced\nload across stages.\n\u2022 Avoid adding too many early exits to the LLM. Despite higher flexibility during inference, the gain\nof adding many early exits (e.g. one per layer) might be marginal, and comes at the cost of excessive\noverhead for training and inference, which is especially the case for LLMs due to large vocabulary sizes.\nSimilar observations and advice have been made recently by the authors of [3] as well.\n\u2022 If there are multiple exits within the same pipeline stage, one might use the same output embedding\nmatrix for all exits; similarly, if early exits are added to the first/last stage, one might reuse the\noriginal input/output embedding matrix for early exits. These choices reduce the memory usage by\nmodel parameters, at the cost of lower expressivity of early exits.\nRemark 3. Recall from Section 3.1 that, with EE-LLM, users can choose more expressive and powerful early-\nexit layers beyond the minimalistic structure. Similarly, more than one early exit can be added to each\npipeline stage, which provides more flexible choices of exits during inference. These benefits, of course, come\nat the cost of higher overhead for training, and potentially for inference as well4; with EE-LLM, users can\nconveniently choose the most suitable configurations for their own use cases. We refer interested readers to\nAppendix A for formal analysis of training efficiency in these general cases.\nNumerical examples.\nWe complement previous analytical study with a few numerical examples. Fig-\nure 4 illustrates load imbalance in the original 1F1B pipeline schedule for a standard 7B GPT Transformer,\nas well as the impacts of adding one minimalistic early-exit layer to each middle stage (with all perfor-\nmance optimizations applied). Table 1 takes a close look at the impacts of each performance optimization;\nunsurprisingly, the best training efficiency is achieved with all the proposed optimizations applied.\n4.3\nAdvanced features\nEE-LLM incorporates some advanced features that can potentially improve the training process, which are\nintroduced below.\nWe note that these are exploratory functionalities, and formal investigation of their\npractical benefits is left for future work.\n4There is no clear answer to whether additional modules at early-exit layers will improve or hurt the overall inference\nspeed. There is certainly higher overhead for the computation of each early-exit layer; on the other hand, higher flexibility\nand adaptivity of the early exits can potentially enable them to produce better outputs and get selected more often during\ninference, leading to overall faster generation of a complete sequence. For similar reasons, there is no clear positive or negative\ncorrelation between the number of early exits and the overall speed of generating a sequence.\n11\nTable 1: Training efficiency and impacts of performance optimizations, with the same setting as in Figure 4.\nFor the \u201cEarly-exit\u201d row, the early exit at 1/4 depth is added to the end of Stage 1, and the exit at 1/2 depth\nis added to the end of Stage 2. The last three rows are annotated with the performance optimization(s)\nadopted, where Optimization 1 stands for deferring forward computation of early-exit layers to backward\nsteps, and Optimization 2 stands for moving every early exit from the end of some pipeline stage to the\nbeginning of the next stage, in order to achieve more balanced load across stages. Stage 1 is the bottleneck\nof peak memory in most cases, except for the numbers marked by *, for which Stage 2 is the bottleneck.\nSetup\n1.3B\n7B\nTime per iteration (s)\nPeak memory (GB)\nTime per iteration (s)\nPeak memory (GB)\nStandard\n5.23\n19.85\n17.75\n62.27\nEarly-exit\n5.31\n24.05\n17.93\n67.42\nEarly-exit (1)\n5.29\n22.56\n17.91\n65.79\nEarly-exit (2)\n5.28\n20.23 *\n17.81\n62.27\nEarly-exit (1&2)\n5.24\n19.85\n17.79\n62.27\n4.3.1\nNon-constant weights of early-exit losses\nThe weights of early-exit losses in the training objective of Eq. (1) can be changing rather than constant\nduring the training process, just like the learning rate or other hyperparameters. Allowing the weights to be\nchanging can offer a more fine-grained control of how gradients from multiple loss functions jointly impact\nthe backbone and early/final output layers of the model. This functionality has been implemented in EE-LLM.\nOne concrete option that we offer is warm-up. With this option, early-exit loss weights start at small\nvalues, and gradually increase with training iterations until reaching the pre-specified maximum values. This\napproach has been adopted in prior works [31]. The idea is to encourage the deep neural network to primarily\noptimize for the full-model output quality from the beginning of the training process, while the skill of early\nexiting is gradually acquired with minor or no negative impact on the final outputs of the full model.\nAnother option is cool-down, which does the opposite and decreases early-exit loss weights during the\ntraining process. This option is inspired by prior works [39, 69] that leverage early-exit losses for the purpose\nof regularizing the training process of deep neural networks. Such \u201cdeep supervision\u201d provided by early-exit\nlosses can stabilize and accelerate convergence of training, and potentially improve the intermediate features\nlearned by the neural network. As early-exit loss weights gradually decay, the regularization gets weaker, so\nthat the network becomes more focused on its primary objective, namely the final-output quality.\n4.3.2\nFilling explicit bubbles with additional microbatches\nTo further leverage the explicit bubbles within the 1F1B pipeline schedule, i.e. the gray areas in Figure 5,\nwe design and implement a novel approach of filling them with partial forward/backward computation of\nadditional microbatches.\nThis is primarily inspired by the idea from [51]: instead of designing a new,\nsophisticated pipeline schedule that has a lower bubble ratio, one may seek to fill the bubbles of an existing\nschedule with useful computation, which can lead to better resource utilization and faster training.\nOur approach is visualized in Figure 5 and explained below. For notational simplicity, let us call the\nexplicit bubbles between the warm-up and steady phases as Part 1, and the bubbles during the cool-down\nphase as Part 2. For each part, we fill them with some computation for K additional microbatches. More\nconcretely, for Part 1, the i-th inserted microbatch (where i \u2208 [K]) goes through forward computation of the\nfirst K + 1 \u2212 i pipeline stages, followed by backward computation of all visited early-exit losses; for Part 2,\neach inserted microbatch goes through forward computation of all stages, followed by backward computation\nof the final and early-exit losses (if any) only for the last few stages. In this way, each training iteration\ncan process more data without any time overhead, as long as the number of inserted microbatches and the\nnumber of stages for partial forward/backward computation are chosen appropriately.\nOne thing to note is that this approach changes the optimization semantics. From the perspective of\nstochastic optimization, we can prove formally that with such additional computation and under certain con-\nditions, one obtains an unbiased gradient estimate with reduced variance for the original training objective.\nWe refer interested readers to Appendix B for more details about the methodology and analysis.\n12\nStage 1\n1\n2\n3\n4\n1\n5\n2\n6\n3\n4\n5\n6\nStage 2\n1\n2\n3\n1\n4\n2\n5\n3\n6\n4\n5\n6\nStage 3\n1\n2\n1\n3\n2\n4\n3\n5\n4\n6\n5\n6\nStage 4\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\nForward step\n1\nBackward step\n1\n(a) The original 1F1B pipeline schedule\nStage 1\n1\n2\n3\n4\nP1 P2\nP2\nP1\n1\n5\n2\n6\n3\nP3\n4\nP4\n5\n6\nStage 2\n1\n2\n3\nP1\nP1\n1\n4\n2\n5\n3\n6\n4\nP3\n5\nP4\n6\nStage 3\n1\n2\n1\n3\n2\n4\n3\n5\n4\n6\n5\nP3\n6\nP4\nP3\nStage 4\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\nP3\nP3\nP4\nP4\nPartial forward\nP1\nPartial backward\nP1\n(b) Filling explicit bubbles with additional microbatches\nFigure 5: A visualization for our proposed method of filling pipeline bubbles with partial forward and\nbackward computation of four additional microbatches, i.e. the ones annotated as P1, P2, P3 and P4.\n5\nInference\nThis section first explains the major challenge faced by early-exit inference of LLMs in autoregressive gen-\neration tasks, namely the conflict between early exiting and KV caching, as well as a few recent attempts\nto resolve it. Then, we introduce our novel approach, which is based on a new type of pipeline parallelism.\nThroughout this section, we focus on the latency-oriented setting, with a batch size of 1 during sequence\ngeneration.\n5.1\nMain challenge: KV caching\nAccelerating inference with early exiting is, in principle, orthogonal to and compatible with many other com-\nmon techniques of acceleration [32], such as kernel fusion, FlashAttention [15], model quantization [85, 80],\namong others. For autoregressive generation tasks, one exception is KV caching, i.e. saving keys and values\nof all attention layers for previously generated tokens, which accelerates sequence generation by reducing\nredundant computation (though at the cost of higher memory usage). This is obviously in contradiction\nwith vanilla early-exit inference: if the current token is generated via early exiting, then its KV caches in\nlater layers are missing, which hinders the generation of future tokens that go beyond the exiting layer of\nthe current token.\nThis challenge has been well recognized in the literature, and several approaches have been recently\nproposed to resolve it. One approach [19, 42, 61] is to copy the hidden states of the current token at the\nexiting layer to all later layers, which will be used to compute the keys and values at later attention layers.\nDespite its efficiency, this method obviously causes a deviation from the inference process that the model is\ntrained to excel at, which can harm the output quality. Another solution [13] is to pre-specify the exiting\nlayer for each token, while ensuring that KV missing in previous tokens will not hinder generation of later\ntokens; with this approach, the ability of token-wise adaptive selection of exits is inevitably lost. The third\nmethod [3, 70] is to store the hidden states of recent tokens that were generated with early exiting, and\nwhenever KV missing happens, run a batch forward pass with the current and recent tokens to fulfill the\nKV caches. A variant of this approach, which we call KV recomputation, is implemented in EE-LLM; we\ndefer its details to Section 6.3. Despite the batching effect of GPU computation, this approach still brings\nadditional computational overhead, and might not even achieve any acceleration for inference on other\nhardware platforms, due to its high computational complexity.\n13\nStage 1\n1\n2\n3\n4\n5\nStage 2\n1\n2\n3\n4\n5\nStage 3\n1\n2\n3\n4\n5\nStage 4\n1\n2\n3\n4\n5\nStage 1\n1\n2\n3\n4\n5\nStage 2\n1\n2\n3\n4\n5\nStage 3\n1\n2\n3\n4\n5\nStage 4\n1\n2\n3\n4\n5\nMain forward\nKV forward\nFigure 6: A comparison between the standard full-model inference (top), and our pipeline-based approach\nof early-exit inference (bottom). Numbers in the blocks represent the tokens within one generated sequence.\nHere, we consider the special case where each early exit is located at the end of a certain pipeline stage, just\nfor simplicity of visualization.\n5.2\nA new solution: pipeline parallelism\nWe propose a novel solution that leverages a new type of pipeline parallelism during inference. The key idea\nis that, in the process of inference with multiple pipeline stages, the following two processes run in parallel\nwhenever the model decides to do early exiting for the current token at a certain early exit:\n\u2022 The generated token is sent back to the first stage, and the forward pass for generating the next token\nis started immediately;\n\u2022 The full-model forward pass of the current token is continued from the exiting layer, which fulfills the\nKV caches in all later layers.\nSee Figure 6 for a visualization of our proposed approach. Even though each token essentially goes through\na forward pass of the full model, the computation after the exiting layer is parallelized with the computation\nof later tokens, which is how acceleration is achieved in this approach. It can be checked that the inference\nlatency for generating one token at a certain exit matches exactly the time needed for the forward compu-\ntation before returning an output at that exit, unless the selected exit is located in the middle of the first\npipeline stage, in which case generation of the next token has to wait until the forward pass of the first stage\nfor the current token is completed. Note that this is true not just in practice but also for the theoretical time\ncomplexity, without relying on the batching effect of GPU computation like KV recomputation does.\nAn implementation of this pipeline-based inference method is provided in EE-LLM. One potential limita-\ntion of the proposed method is that it requires multiple devices to facilitate pipeline parallelism, although\nparallelism within a single GPU or other device might be possible with more advanced implementation.\n6\nImplementations\nThe implementation of EE-LLM is based on Megatron-LM [49], primarily extending Megatron-LM\u2019s model\narchitectures, pipeline scheduling, and inference service to support the training and inference of early-exit\nLLMs. We introduce each of these aspects in more details below.\n6.1\nModel architectures\nWe have introduced a new class of models called EarlyExitGPTModel, which is the early-exit counterpart\nof GPTModel in the original model library of Megatron-LM. The model is constructed with a few other\nclasses, including EarlyExitTransformerLayer, EarlyExitTransformer, and EarlyExitLanguageModel.\n14\nEarlyExitTransformerLayer is a replacement for the original ParallelTransformerLayer in Megatron-\nLM. It adds an early-exit structure on top of the standard Transformer layer, which allows it to generate\noutputs for both the main network backbone and the early exit; for the latter, it returns a lazy loss function\nduring training, or tokens during inference. This module supports various customizations of the early-exit\nstructure; besides the minimalistic structure with an output embedding matrix and an optional output\nnormalization layer, one might add e.g. a MLP or a complete Transformer layer. These additional structures\ncan be combined in any desired manner and can be placed before or after the backbone part of this layer.\nOn the other hand, EarlyExitTransformer and EarlyExitLanguageModel are mainly used to propagate\nthe early-exit outputs to the top-level model. They are capable of stopping the forward computation at the\nearly-exit layer and returning the intermediate outputs, which facilitates accelerated inference.\n6.2\nPipeline scheduling\nWe have adjusted the existing 1F1B schedule for early-exit LLMs, as shown in Figure 3. To fill implicit\nbubbles and reduce GPU memory overhead, lazy loss functions of early-exit layers are returned together with\noutputs of the backbone network during forward steps. These lazy functions are not actually called until\ntheir corresponding auxiliary losses (cf. Section 4.1.1) are calculated in the backward steps. For the method\nof filling explicit bubbles proposed in Section 4.3.2, we have inserted partial forward/backward computation\nof additional microbatches into warm-up and cool-down phases of the 1F1B schedule. The number of inserted\nmicrobatches and partial forward/backward stages can be automatically calculated through the user-specified\n(estimate of) ratio between backward and forward time.\n6.3\nInference service\nTo support inference of early-exit LLMs, we have refactored the text-generation module of Megatron-LM.\nFor inference with pipeline parallelism, i.e. the pipeline-based approach proposed in Section 5.2, we have\nre-implemented the forward process. With our implementation, the first pipeline stage will wait for an exit\nsignal from the early/final exits of all subsequent stages after its forward computation is completed. Each\nsubsequent stage will send an exit signal and the output token to the first stage, if there is an exit within\nthe stage that satisfies the exit condition. Upon receiving the signal and generated token, the first stage will\nimmediately start the forward pass for generating the next token. With this implementation, regardless of\nthe early-exit layers\u2019 positions in subsequent stages, the inference service can immediately generate a token\nwhenever early exiting happens on some stage, without waiting for the completion of the entire stage (except\nfor the first stage).\nFor inference without pipeline parallelism, we have implemented a mechanism of KV recomputation,\nwhich is a variant of synchronized parallel decoding proposed recently in [3]. In this approach, we maintain\na list of the most recent tokens that have missing KV caches in deep layers due to early exiting. During\neach forward pass, we include these early-exit tokens in the current forward pass, which allows for direct\nrecomputation of the KV caches for these tokens and thus avoids the issue of missing KV caches. Acceleration\nof sequence generation is still achieved, thanks to the batching effects of GPU computation. To avoid the\nendless accumulation of early-exit tokens, we enforce a full-model forward pass whenever the number of\nearly-exit tokens reaches a pre-specified value.\n7\nExperiments\nThis section provides an empirical evaluation of the training and inference efficiency achieved by EE-LLM.\n7.1\nTraining\nIn the following experiments, we empirically investigate the convergence of training early-exit models with\nEE-LLM, as well as the training efficiency of EE-LLM for early-exit LLMs up to an unprecedented scale of\n30B. This scale is only limited by the hardware resources available to us, namely an 8-node cluster with 8\n15\nNvidia A100-80GB GPUs in each node and hence 64 GPUs in total. We use a subset of the pre-training\ndata5 provided by Data-Juicer [9], which consists of 800 billion tokens.\n7.1.1\nConvergence of training losses\nFirst of all, we conduct experiments to verify the convergence of training losses when training early-exit\nLLMs with EE-LLM. We first consider a 1.3B GPT Transformer with 24 layers, add one minimalistic early-\nexit layer without layer normalization to the 1/4 depth and the other to the 1/2 depth, set their early-exit\nloss weights to 1/4 and 1/2 respectively (while the final-exit loss has a weight of 1), and tie all input\nand output embedding matrices. A standard LLM of the same GPT architecture is also trained using the\nsame hyperparameters and pre-training data. We further train a 7B early-exit model with 32 layers using\nsimilar configurations, except that early-exit loss weights are set to 0.1 and 0.2, and all embedding matrices\nare untied. A 7B standard model is trained similarly. In all cases, we use the Adam optimizer [35] with\nhyperparameters \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128, and a cosine schedule for the learning rate, with a maximum\nvalue of 3 \u00d7 10\u22124. The batch size and sequence length are both set to 2048.\nFigure 7 shows the convergence of early-exit and final-exit training losses, i.e. negative log-likelihood of\nnext-token prediction, for both standard and early-exit LLMs. All loss curves decay at a similar pace, and\nunsurprisingly, the early-exit losses are slightly higher than the final-exit loss for each model. Interestingly,\nthe final-exit loss curve of each early-exit model is close to (or even slightly below) that of the standard\nmodel, suggesting that optimizing for early-exit losses might not hurt the final output of the full model in\nour setting. We also observe more spikes in the loss curves for the 7B model than for the 1.3B models,\npossibly due to the following reasons: (1) we choose to untie the embedding matrices and use smaller early-\nexit loss weights, both of which incur weaker regularization for the training process; (2) layer normalization,\nwhich is known to stabilize training of LLMs, is not included in the minimalistic early-exit layers of our 7B\nmodel.\n7.1.2\nTraining efficiency\nIn this experiment, we investigate the training overhead of early exits.\nStarting with a standard GPT\nTransformer of size ranging from 1.3B to 30B, we increase the number of early exits from 0 to 3. Minimalistic\nearly exits are added one by one to specific locations in the following order: (1) to the 1/4 depth; (2) to the\n1/2 depth; (3) to the hidden states right before the first Transformer layer, which is always located in the\nfirst pipeline stage. Based on the performance optimizations proposed in Section 4.2, when an early exit is\ninserted into the middle of two layers located on two consecutive pipeline stages, we always add it to the\nbeginning of the latter stage. We set the global batch size to 2048, microbatch size to 2 (for 1.3B and 7B\nmodels) or 1 (for 13B and 30B models), and sequence length to 2048. The degree of data parallelism is set\nto 4 and fixed throughout, while tensor/sequence and pipeline parallelism degrees can take various values.\nNumerical results are illustrated in Figure 8, which matches the results from our analytical study in\nSection 4.2. In particular, training time per iteration (i.e. time for processing one data batch) increases with\nthe number of added early exits, but at a slower rate when pipeline parallelism is enabled, thanks to the\nproposed utilization of implicit bubbles. Without pipeline parallelism, peak GPU memory increases with\nthe number of early exits; on the other hand, with the pipeline parallelism degree set to 4, peak memory\nremains unchanged as early exits are added to middle pipeline stages, and only increases when the last early\nexit is added to the first stage, as predicted by our analysis in Section 4.2.\n7.2\nInference\nIn the experiments below, we verify the effectiveness of our pipeline-based approach (with the number\nof pipeline stages set to 4) proposed in Section 5.2, and the method of KV recomputation explained in\nSection 6.3, for autoregressive generation with early exiting. Given that both methods generate the same\noutput for the same prompt, we first investigate the downstream performance and early-exit speedup for\nthe pipeline-based method alone, and then compare the inference speed of both methods. A server with 4\nNvidia A100-40GB GPUs is used for inference.\n5https://github.com/alibaba/data-juicer/blob/main/configs/data_juicer_recipes/README.md\n16\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nTraining iteration\n2\n3\n4\n5\n6\n7\n8\nLoss\n17950\n17960\n17970\n17980\n17990\n18000\n2.2\n2.3\n2.4\n2.5\n2.6\nEarly-exit LLM (6)\nEarly-exit LLM (12)\nEarly-exit LLM (final)\nStandard LLM\n(a) 1.3B, 24 layers\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nTraining iteration\n2\n3\n4\n5\n6\n7\n8\nLoss\n10000\n10020\n10040\n10060\n10080\n10100\n2.10\n2.25\n2.40\n2.55\nEarly-exit LLM (8)\nEarly-exit LLM (16)\nEarly-exit LLM (final)\nStandard LLM\n(b) 7B, 32 layers\nFigure 7: Convergence of early-exit/final-exit training losses for our Transformer models. Each curve is\nannotated with the index of the Transformer layer that the corresponding exit is connected to.\nTo examine the performance of inference with various sizes of models, we conduct experiments under\ndifferent settings. One model that we use is an 1.3B early-exit model pre-trained from scratch with 300B\ntokens, and the other one is a 7B model pre-trained with 150B tokens. Each of them has two minimalistic\nearly exits added to the 1/4 and 1/2 depth respectively; other configurations follow those in Section 7.1.1.\nNeither model has gone through any fine-tuning or alignment. Since designing the decoding method or early-\nexit condition is not the focus of our work, we simply adopt greedy decoding and a basic confidence-based\ndecision rule for early exiting: at each early exit, if the maximum probability of next-token prediction is\nabove a pre-defined threshold, then the model chooses the most likely token and moves on to generate the\nnext one. The trade-off between output quality and speed of generation is then controlled by the threshold.\nNote that for a fair comparison, when the threshold is set to 1, early-exit layers are disabled, so that the\ncomputational complexity of a full-model forward pass matches that of a standard LLM without early exits;\nthe inference latency with the threshold set to 1 is also used as the baseline for calculating (relative) speedup.\n7.2.1\nOutput quality and speed of generation\nWe evaluate the downstream performance of our models with HELM [43] in four tasks.\nAmong them,\nBoolQ [12] and TruthfulQA [44] are question-answering tasks, while XSUM [46] and CNN/DailyMail [64]\nare summarization tasks. We use EM (ratio of exact match with the correct answers) as the metric for\nthe former, and ROUGE-L (a measure of similarity to the reference summaries) for the latter. Figure 9\ndemonstrates the relation between scores and inference speedup for pipeline-based inference with varying\nthresholds. Encouragingly, for many of these tasks, inference with early exiting can achieve 2\u00d7 or higher\nspeedup compared to full-model inference, with comparable or even better evaluation scores. Table 2 provides\na concrete piece of text generated by our 7B early-exit model, showing that speedup can be achieved with\n17\n0\n1\n2\n3\nNumber of early exits\n4.5\n6.0\n7.5\n9.0\n10.5\nTime per iter. (s)\nPP=4,TP=1\nPP=1,TP=4\n0\n1\n2\n3\nNumber of early exits\n15\n18\n21\n24\nGPU memory (GB)\nPP=4,TP=1\nPP=1,TP=4\n(a) 1.3B\n0\n1\n2\n3\nNumber of early exits\n16\n20\n24\n28\nTime per iter. (s)\nPP=4,TP=1\nPP=1,TP=4\n0\n1\n2\n3\nNumber of early exits\n48\n56\n64\n72\nGPU memory (GB)\nPP=4,TP=1\nPP=1,TP=4\n(b) 7B\n0\n1\n2\n3\nNumber of early exits\n35\n40\n45\n50\nTime per iter. (s)\nPP=4,TP=2\nPP=2,TP=4\n0\n1\n2\n3\nNumber of early exits\n40\n45\n50\n55\nGPU memory (GB)\nPP=4,TP=2\nPP=2,TP=4\n(c) 13B\n0\n1\n2\n3\nNumber of early exits\n88\n96\n104\n112\nTime per iter. (s)\nPP=4,TP=4\n0\n1\n2\n3\nNumber of early exits\n55\n60\n65\n70\nGPU memory (GB)\nPP=4,TP=4\n(d) 30B\nFigure 8: Training time per iteration and peak GPU memory vs. the number of added early exits, under\nvarious configurations of model sizes and parallelism degrees. Note that wall-clock time can be impacted by\nother workloads on the same GPU cluster when our experiments were conducted, which inevitably causes\nperturbations to our numerical results.\n18\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\n0.0\n0.2\n0.4\n0.6\nEM\nBoolQ\nTruthfulQA\n1\n2\n3\nSpeedup\n0.00\n0.05\n0.10\n0.15\n0.20\nROUGE-L\nXSUM\nCNN/DailyMail\n(a) 1.3B\n1\n2\n3\nSpeedup\n0.0\n0.2\n0.4\n0.6\n0.8\nEM\nBoolQ\nTruthfulQA\n1\n2\n3\nSpeedup\n0.00\n0.08\n0.16\n0.24\nROUGE-L\nXSUM\nCNN/DailyMail\n(b) 7B\nFigure 9: Evaluation scores and relative speedup with our early-exit models. For each curve, the confidence\nthreshold decreases from left to right.\nminor or no impact on the resulted sequence. Table 3 goes one step further and demonstrates the confidence\nat each exit for each token, confirming the existence of easy tokens that can be predicted with high confidence\nat early exits, without wasting computational resources on later layers. It is worth noting that performance\nof early-exit inference might be improved in many ways, e.g. using early exits of different structures, other\ndecoding mechanisms and exit conditions, or simply more extensive and sufficient pre-training. We also\nconjecture that early exiting can, in principle, bring higher speedup for larger models, as expressivity of\nearly exits grows with the model capacity, which allows them to make confident and accurate predictions for\na larger proportion of tokens.\n7.2.2\nA comparison between two inference methods\nWe compare the inference time of both pipeline-based method and KV recomputation. Since the pipeline-\nbased method uses 4 GPUs for a pipeline parallelism (PP) degree of 4, we allow KV recomputation to\nuse a tensor parallelism (TP) degree of 1 or 4 for a fair comparison. The empirical results on XSUM and\nCNN/DailyMail tasks are shown in Figure 10. We first notice that the pipeline-based approach outperforms\nKV recomputation with TP = 1 for confidence thresholds smaller than 1 (i.e. when early exiting actually\nhappens), despite the point-to-point communication overhead. Moreover, KV recomputation with TP = 4\nis faster than the pipeline-based approach; it is worth noting, however, that the small speedup from TP = 1\nto TP = 4 for KV recomputation is only possible with high-end hardware like A100 GPUs connected by\nhigh-bandwidth communication. In many use cases of LLM inference, such hardware is not available, and\npipeline parallelism is the only option for partitioning a model that is too large to fit into the memory of one\nsingle device. In sum, the proposed pipeline-based approach is a more practical option for LLM inference\nwith model partitioning in a wide range of scenarios.\n19\nTable 2: Sequences generated by our 7B early-exit model for the same prompt, with varying confidence\nthresholds and inference latencies. Differences from the result of full-model inference are highlighted in red.\nPrompt\nArtificial General Intelligence is\nFull model\n(Time: 1.42 s)\na branch of computer science that studies the development of computer systems\nthat can perform tasks that normally require human intelligence.\nThe term \"artificial intelligence\" was coined by John McCarthy in 1956.\nThreshold = 0.8\n(Time: 1.17 s)\na branch of computer science that studies the development of computer systems\nthat can perform tasks that normally require human intelligence.\nThe term \"artificial intelligence\" was coined by John McCarthy in 1956.\nThreshold = 0.4\n(Time: 0.89 s)\na branch of computer science that studies the development of computer systems\nthat can perform tasks that normally require human intelligence.\nThe term artificial general intelligence was coined by John McCarthy in 1965.\nThreshold = 0.2\n(Time: 0.58 s)\na branch of computer science that deals with the development of computer pro-\ngrams that can be used to perform tasks such as problem solving, decision making,\nand learning.\nArtificial General Intelligence is a branch of computer science that\nTable 3: The predicted token and confidence at each exit of our 32-layer 7B early-exit model for generating\neach token within one sequence. The prompt is \u201cThe capital of China is\u201d, and the sequence generated by\nfull-model inference is \u201cBeijing. \\n The capital of the United States is Washington, D.C.\u201d. Tokens that are\npredicted with confidence higher than 0.8 are highlighted; interestingly, all exits make the same prediction\nfor each of these tokens.\nLayer 8\nLayer 16\nFinal layer\nlocated\n(0.116)\nBe\n(0.202)\nBe\n(0.609)\nij\n(0.992)\nij\n(0.995)\nij\n(0.996)\ning\n(0.999)\ning\n(0.999)\ning\n(0.999)\n,\n(0.370)\n.\n(0.535)\n.\n(0.503)\n\\n\n(0.300)\n\\n\n(0.310)\n\\n\n(0.227)\nThe\n(0.126)\nThe\n(0.150)\nThe\n(0.158)\ncapital\n(0.074)\ncapital\n(0.125)\ncapital\n(0.102)\nof\n(0.757)\nof\n(0.832)\nof\n(0.860)\nChina\n(0.523)\nthe\n(0.094)\nthe\n(0.115)\ncountry\n(0.146)\nUnited\n(0.438)\nUnited\n(0.436)\nStates\n(0.880)\nStates\n(0.816)\nStates\n(0.858)\nis\n(0.733)\nis\n(0.617)\nis\n(0.687)\nWashington\n(0.877)\nWashington\n(0.992)\nWashington\n(0.989)\n,\n(0.443)\n,\n(0.394)\n,\n(0.462)\nD\n(0.571)\nD\n(0.728)\nD\n(0.686)\n.\n(0.999)\n.\n(0.997)\n.\n(0.997)\nC\n(0.969)\nC\n(0.967)\nC\n(0.934)\n.\n(0.830)\n.\n(0.873)\n.\n(0.904)\n20\n0.00\n0.25\n0.50\n0.75\n1.00\nThreshold\n0\n1\n2\n3\n4\nAverage inference time (s)\nKV recomputation [TP=4]\nKV recomputation [TP=1]\nPipeline-based [PP=4]\n(a) XSUM\n0.00\n0.25\n0.50\n0.75\n1.00\nThreshold\n0\n2\n4\n6\nAverage inference time (s)\nKV recomputation [TP=4]\nKV recomputation [TP=1]\nPipeline-based [PP=4]\n(b) CNN/DailyMail\nFigure 10: A comparison of inference speed between the pipeline-based method (with 4 pipeline stages) and\nKV recomputation (with the degree of tensor parallelism set to 1 or 4) for our 7B early-exit model.\n8\nRelated works\nAs introduced in Section 1, early exiting has been widely applied for accelerating inference of deep neural\nnetworks in the literature. In terms of early-exit Transformers in the NLP domain, the majority of prior\nworks is focused on BERT [16] or other encoder-only models for classification tasks [45, 25, 94, 19, 63, 81,\n62, 82, 42, 26]. Recent works have begun to study token-wise early exiting for accelerating inference of\nencoder-decoder or decoder-only LLMs in autoregressive sequence generation [62, 13, 3, 76]. While they are\nlargely focused on designing inference mechanisms, the lack of support in prior works for training early-exit\nmodels with massive 3D parallelism inevitably posts an obstacle towards truly scaling up early-exit LLMs.\nOur work on EE-LLM is one important step towards eliminating this obstacle, and we also contribute a novel\npipeline-based inference mechanism along the way.\nInterestingly, early exiting has been used for many other purposes in the literature, besides accelerating\ninference. For example, early-exit losses can provide deep supervision and regularization for training deep\nnetworks with enhanced stability of convergence [69, 39], which motivates our implementation of the cool-\ndown option for non-constant early-exit loss weights, as explained in Section 4.3.1. Other works use early\nexits for interpreting the intermediate features learned by each layer of a Transformer [37], for improving\noutputs of the final exit [22], or as the draft models in speculative decoding [33]. Early exiting has also\nfound application in long-tailed classification [18], federated learning with heterogeneous clients [29, 93],\ntoken reduction [1], layer-wise training of deep networks [60, Section 5.3], among others. We believe that\nour progress in this work can be beneficial for these various purposes.\nIt is worth mentioning that there are other types of dynamic neural networks [24, 83] that facilitate\nconditional computation and elastic inference, such as layer skipping [6, 5, 79, 13, 17, 87, 78] and mixtures\nof experts [30, 66, 21]. Another line of work aims to accelerate LLM inference by designing new decoding\nmethods rather than model architectures [40, 59, 91]. Each of these approaches has its own pros and cons,\nand some of them can be complemented by early exiting. A detailed comparison between these methods is\nbeyond the scope of the current work.\n9\nConclusions\nWe have introduced EE-LLM, a system for large-scale training and inference of early-exit LLMs with 3D par-\nallelism. For training, we have presented how to execute backpropagation of the early-exit training objective\nacross pipeline stages, various performance optimizations for minimizing the computational overhead com-\npared to training standard LLMs, and some advanced features for more fine-grained control and optimization\nof the training process. For inference, we have introduced our design and implementation of two inference\nmethods, one based on KV recomputation and the other based on a new type of pipeline parallelism, both\nof which are compatible with KV caching for autoregressive generation. Along the way, we have discovered\n21\nTable 4: A list of notation for theoretical analysis.\nThe following abbreviations are used: IN \u2014 input\nprocessing layer; EE \u2014 early-exit layer; FE \u2014 final-exit layer; BB \u2014 Transformer backbone on one pipeline\nstage.\nNotation\nDefinition\nP\nNumber of pipeline stages\nNi\nNumber of early exits in Stage i \u2208 [P]\nM\nNumber of microbatches for one training iteration\nfo, bo\nForward and backward time of one o \u2208 {IN, EE, FE, BB} for one microbatch\nmo\nMemory usage for storing the model parameters of one o \u2208 {IN, EE, FE, BB}\nm\u2020\no\nActivation memory of one o \u2208 {IN, EE, FE, BB} for one microbatch\n1(\u00b7)\nThe indicator function, 1(E) = 1 if the event E holds true, and 0 otherwise\nsome interesting chemical reaction between early exiting and pipeline parallelism, which is likely because it\nis both along the depth dimension that an early-exit model executes forward computation selectively and\ngets partitioned by pipeline parallelism. With EE-LLM, it now becomes possible to train and deploy early-exit\nLLMs that are as large as any standard LLM allowed by Megatron-LM, given the same amount of computa-\ntional resources. We hope that EE-LLM will be a helpful tool for future study and applications of early-exit\nLLMs at larger scales.\nAt the time of writing this article, we are continuously updating our codebase with more thorough\nexperiments and more advanced functionalities. It is worth noting that many ideas presented in this work\ncan be generalized to broader settings, e.g. deep neural networks other than decoder-only GPT Transformers,\nframeworks other than Megatron-LM, and hardware platforms other than GPU clusters; it would be exciting\nto see further developments in these aspects.\nA\nAnalysis of training efficiency\nIn this section, we derive formulas for the training time per iteration and peak GPU memory usage across\npipeline stages under general configurations of early exits, in terms of the quantities listed in Table 4. Some of\nthese quantities, especially {fo, bo, mo, m\u2020\no}, can be further calculated analytically with lower-level quantities\nsuch as sequence length, microbatch size, hidden size, vocabulary size and others [57, 49, 36], or estimated\nempirically by profiling in practice.\nIt is assumed that all pipeline stages have the same number of Transformer layers on the backbone, and\nall early-exit layers follow the same structure. We also assume that input and output embedding matrices\nare untied, although it is not hard to extend our analysis to more general cases. For simplicity, we ignore\nthe point-to-point communication latency between pipeline stages in our analysis, since it typically takes a\nminor proportion of the overall training time, and can often be overlapped with computation.\nA.1\nTraining time per iteration\nLet us analyze the training time per iteration step by step.\nStep 1: simplified analysis without early exits.\nWe start by recalling the simplified analysis in prior\nworks for the standard 1F1B pipeline schedule [49], which assumes that all stages have the same forward\ntime f and backward time b for one microbatch. Recall from Figure 3 that the critical path of one training\niteration consists of three parts:\n\u2022 Part 1: forward steps of the first microbatch on all stages except the last one, which takes time\n(P \u2212 1) \u00d7 f;\n\u2022 Part 2: the steady 1F1B phase with all M microbatches on the last stage, which takes time M \u00d7(f +b);\n\u2022 Part 3: backward steps of the last microbatch on all stages except the last one, which takes time\n(P \u2212 1) \u00d7 b.\n22\nTaking the sum of these three parts, we have\ntime per iteration = (P \u2212 1) \u00d7 f + M \u00d7 (f + b) + (P \u2212 1) \u00d7 b = (P \u2212 1) \u00d7 (f + b)\n|\n{z\n}\nParts 1 and 3\n+ M \u00d7 (f + b)\n|\n{z\n}\nPart 2\n.\nStep 2:\nfine-grained analysis without early exits.\nWe provide a more fine-grained analysis, by\nconsidering the computation related to the input processing layer (IN) and final-exit layer (FE) separately\nfrom the Transformer backbone (BB). It is reasonable to assume that fIN < fFE and bIN < bFE, which implies\nthat the last stage is the bottleneck of forward and backward time. In this setting, Parts 1 and 3 of the\ncritical path takes time fIN + (P \u2212 1) \u00d7 fBB and bIN + (P \u2212 1) \u00d7 bBB, respectively. Similarly, Part 2 now takes\ntime M \u00d7 (fBB + bBB + fFE + bFE). Taking the sum, we arrive at\ntime per iteration = fIN + bIN + (P \u2212 1) \u00d7 (fBB + bBB)\n|\n{z\n}\nParts 1 and 3\n+ M \u00d7 (fBB + bBB + fFE + bFE)\n|\n{z\n}\nPart 2\n.\nStep 3: fine-grained analysis with early exits.\nFinally, we are ready for our analysis in the setting\nwith early exits (EE). First, it can be checked that early-exit layers incur a total overhead of\nX\ni\u2208[P \u22121]\nNi \u00d7 (fEE + bEE)\nto Parts 1 and 3. In addition, the sum of forward and backward time of one microbatch for Stage i becomes\nfBB + bBB + 1(i = 1) \u00d7 (fIN + bIN) + 1(i = P) \u00d7 (fFE + bFE) + Ni \u00d7 (fEE + bEE).\nNote that Part 2, namely the steady 1F1B phase on the last stage, is now bottlenecked by the maximum\nforward and backward time across all stages. Putting things together, we have the following upper bound:\ntime per iteration\n\u2264 fIN + bIN + (P \u2212 1) \u00d7 (fBB + bBB) +\nX\ni\u2208[P \u22121]\nNi \u00d7 (fEE + bEE)\n+ M \u00d7 max\ni\u2208[P ]\nn\nfBB + bBB + 1(i = 1) \u00d7 (fIN + bIN) + 1(i = P) \u00d7 (fFE + bFE) + Ni \u00d7 (fEE + bEE)\no\n.\nThe overhead caused by early-exit layers has been highlighted. The first term corresponds to Parts 1 and 3\nof the critical path, and the second term corresponds to Part 2. It is worth mentioning that the second term\nof the overhead might not take effect due to the maximum operator, in which case the early-exit overhead\nto one training iteration is independent of the number of microbatches.\nA.2\nPeak GPU memory\nConsider the GPU memory for one specific pipeline stage, say Stage i. Recall that GPU memory usage during\ntraining is mainly composed of memory for model parameters, gradients, optimizer states, and intermedi-\nate activations [57, 36]. Among them, the memory for gradients and optimizer states (for many common\noptimizers, such as SGD with momentum or Adam) is proportional to the number of model parameters.\nTherefore, we assume that\nmemory(model parameters, gradients, optimizer states) = \u03b1 \u00d7 memory(model parameters)\nfor some universal constant \u03b1 > 1, whose concrete value depends on the choice of optimizer and numerical\nprecisions. Under this assumption, we have\ntotal memory \u2248 \u03b1 \u00d7 memory(model parameters) + memory(activations).\n(5)\nNow it boils down to deriving the memory for model parameters and activations. Model parameters in\nStage i \u2208 [P] include the Transformer backbone (BB), the input processing layer (IN) if i = 1, the final-exit\nlayer (FE) if i = P, and the early exits (EE). In other words, we have\nmemory(model parameters) = mBB + 1(i = 1) \u00d7 mIN + 1(i = P) \u00d7 mFE + Ni \u00d7 mEE.\n(6)\n23\nThe memory for intermediate activations can be similarly divided into components corresponding to BB, IN,\nFE and EE. Note that according to Section 4.2.2, the peak memory usage by activations within one early-exit\nlayer is only m\u2020\nEE regardless of the number of in-flight microbatches P + 1 \u2212 i. Thus one has\nmemory(activations) = (P + 1 \u2212 i) \u00d7 m\u2020\nBB + 1(i = 1) \u00d7 P \u00d7 m\u2020\nIN + 1(i = P) \u00d7 m\u2020\nFE + Ni \u00d7 m\u2020\nEE.\n(7)\nCombining Eq. (6) and (7) with Eq. (5), we arrive at an estimate of memory usage for Stage i:\ntotal memory \u2248 \u03b1\u00d7\n\u0010\nmBB + 1(i = 1) \u00d7 mIN + 1(i = P) \u00d7 mFE + Ni \u00d7 mEE\n\u0011\n+\n\u0010\n(P + 1 \u2212 i) \u00d7 m\u2020\nBB + 1(i = 1) \u00d7 P \u00d7 m\u2020\nIN + 1(i = P) \u00d7 m\u2020\nFE + Ni \u00d7 m\u2020\nEE\n\u0011\n.\nThe memory overhead caused by early-exit layers has been highlighted in red. Finally, taking the maximum\nover i \u2208 [P] gives the peak GPU memory across all pipeline stages.\nB\nSupplementary materials for Section 4.3.2\nThis section provides more details about the method, proposed in Section 4.3.2, of filling pipeline bubbles\nwith partial forward/backward computation of additional microbatches.\nB.1\nMethodology\nHow many microbatches can be inserted?\nThe maximum number of microbatches that can be inserted\ninto Part 1 or 2 of the explicit bubbles, without increasing training time per iteration, is \u230a(p\u22121)b/(f +b)\u230b =\n\u230a(p \u2212 1)/(f/b + 1)\u230b, where f/b is (an estimate of) the ratio between forward and backward time. To see this\nfor Part 1 (resp. 2), one simply need to notice that the first (resp. last) pipeline stage has an bubble size\nb(p \u2212 1), while the total forward and backward time for each microbatch is f + b. Dividing the first value by\nthe second one concludes the proof.\nDetails about Part 1.\nWith K inserted microbatches, the i-th microbatch is supposed to go through the\nforward pass of the first K + 1 \u2212 i stages. However, if there is no early exit on Stage K + 1 \u2212 i, then this\nmicrobatch only need to go through the forward pass up to the last stage (among the first K + 1 \u2212 i stages)\nthat has at least one early exit.\nDetails about Part 2.\nWe can calculate the maximum number of backward stages for each inserted\nmicrobatch, while ensuring that no overhead to training time per iteration occurs. Notice that for the i-th\nmicrobatch, the remaining bubble size after its forward computation at the last stage is (p \u2212 1)b \u2212 f \u2212 (i \u2212\n1)(f + b) = pb \u2212 i(f + b). Dividing this value by b gives the number of backward stages \u230a(pb \u2212 i(f + b))/b\u230b =\n\u230ap \u2212 i(f/b + 1)\u230b.\nSome remarks.\n(1) There are some limitations to the proposed approach.\nFor example, it requires\nthat there is no tied/shared model parameter across pipeline stages; otherwise the gradients from partial\nforward/backward passes might (in theory) be harmful rather than beneficial for training the model. Another\nconcern is that inserting microbatches into Part 1 of the explicit bubbles can cause additional overhead of\nactivation memory in early stages. (2) While Part 1 of this approach requires the existence of early exits,\nPart 2 is actually applicable to training standard models without early exits as well. (3) One might raise\nconcerns about the inefficiency of data usage, as some microbatches only undergo partial forward/backward\npasses. In general, this is not an issue for LLM pre-training, since training data is usually abundant, and\nthe complete training process might not even go through one epoch of the data.\nA different perspective.\nThe actual implementation of this method in EE-LLM takes a different perspec-\ntive. Instead of inserting additional microbatches into the original schedule, we keep the number of micro-\nbatches per training iteration unchanged, and do the following: (1) we replace the full forward/backward\n24\n\ud835\udf3d!\n\ud835\udc3f!\n\ud835\udf3d\"\n\ud835\udc3f\"\n\ud835\udf3d#\n\ud835\udc3f#\n\ud835\udf3d$\n\ud835\udc3f$\nInput\nStage 1\nStage 4\nStage 3\nStage 2\nForward\nBackward\nPart 1: partial forward and backward\nForward\nBackward\nPart 2: full forward and partial backward\nFigure 11: An illustration of the partial forward/backward passes when filling pipeline bubbles with addi-\ntional microbatches.\ncomputation of a few microbatches with partial forward/backward computation, which can be placed in the\nbubble between the warm-up and steady phases; (2) we truncate the backward computation of the last few\nmicrobatches. These two steps correspond exactly to Parts 1 and 2 introduced earlier, and the visualization\nin Figure 5(b) remains valid. Such an implementation reduces the training time per iteration, at the cost of\nlower data utilization.\nB.2\nAnalysis\nWe argue that such partial forward/backward computation offers useful gradient information for training,\nunder the assumption that there is no tied model parameter across pipeline stages. For a theoretical justifi-\ncation, let us take the perspective of stochastic optimization, and assume that each microbatch is sampled\nindependently from some data distribution.\nClaim 1 (Informal). Under the above assumptions, the accumulated gradient of one training iteration, with\nextra updates from additional microbatches inserted into Part 2, remains (after some appropriate entrywise\nscaling) an unbiased estimate, but with reduced variance, of the gradient for the targeted population risk.\nA similar claim holds true for Part 1 as well, except for certain extreme (and unlikely) situations where\ngradients of different early/final losses have strong negative correlation.\nIn the remaining of this section, we provide an informal proof for this claim, in a concrete example with\nfour pipeline stages and one additional microbatch inserted into either Part 1 or Part 2; a visualization\ncan be found in Figure 11. Recall that the training objective is defined as L = P\ni\u2208[4] Li, where each Li\nis a weighted sum of all losses on Stage i. We denote the model parameters as \u03b8 = [\u03b81, \u03b82, \u03b83, \u03b84], with\ncorrespondence to how the model is partitioned into four pipeline stages. One key observation that will be\nuseful is that \u2202Li/\u2202\u03b8j = 0 for any 1 \u2264 i < j \u2264 4, due to the sequential nature of the model.\nAnalysis of Part 2.\nWe start with analysis for Part 2, which is slightly simpler. Suppose that an additional\nmicrobatch goes through the full forward pass, followed by a partial backward pass covering only the last\ntwo stages, namely \u03b84 and \u03b83. Then, the gradient from this microbatch is\ngradient =\n\u0014\n0, 0, \u2202(L3 + L4)\n\u2202\u03b83\n, \u2202(L3 + L4)\n\u2202\u03b84\n\u0015\n=\n\u0014\n0, 0, \u2202L\n\u2202\u03b83\n, \u2202L\n\u2202\u03b84\n\u0015\n;\nhere, the first equality is due to the partial backward pass, and the second equality follows from the obser-\nvation that \u2202Li/\u2202\u03b8j = 0 for any i < j.\n25\nNow, suppose that a total of B microbatches was originally used for one training iteration, and we let L(k)\ndenote the loss of the k-th microbatch. Then, with the additional (B + 1)-th microbatch inserted into Part\n2 of the explicit bubbles, the accumulated gradient of one training iteration (normalized by 1/B) becomes:\naccumulated gradient = 1\nB\n X\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b8\n+\n\u0014\n0, 0, \u2202L(B+1)\n\u2202\u03b83\n, \u2202L(B+1)\n\u2202\u03b84\n\u0015!\n=\n\u0014 1\nB\nX\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b81\n, 1\nB\nX\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b82\n, 1\nB\nX\nk\u2208[B+1]\n\u2202L(k)\n\u2202\u03b83\n, 1\nB\nX\nk\u2208[B+1]\n\u2202L(k)\n\u2202\u03b84\n\u0015\nTaking the expectation over the data distribution, we realize that the additional microbatch essentially scales\nup gradients for \u03b83 and \u03b84 by a factor of (B + 1)/B. Or, with a simple entrywise scaling on the gradients\nfor \u03b83 and \u03b84 by a factor of B/(B + 1), we recover an unbiased estimate of the gradient for the targeted\npopulation risk, with reduced variance:\naccumulated gradient =\n\u0014 1\nB\nX\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b81\n, 1\nB\nX\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b82\n,\n1\nB + 1\nX\nk\u2208[B+1]\n\u2202L(k)\n\u2202\u03b83\n,\n1\nB + 1\nX\nk\u2208[B+1]\n\u2202L(k)\n\u2202\u03b84\n\u0015\n.\nAnalysis of Part 1.\nSuppose that an additional microbatch goes through the forward and backward\ncomputation only for the first two stages, corresponding to model parameters \u03b81 and \u03b82. Then, the gradient\nfrom this microbatch is\ngradient =\n\u0014\u2202(L1 + L2)\n\u2202\u03b81\n, \u2202(L1 + L2)\n\u2202\u03b82\n, 0, 0\n\u0015\n= \u2202(L1 + L2)\n\u2202\u03b8\n;\nhere, the first equality is due to the partial forward and backward passes, and the second equality follows\nagain from the observation that \u2202Li/\u2202\u03b8j = 0 for any i < j.\nNow, suppose that a total of B microbatches was originally used for one training iteration. We denote\nthe loss of the k-th microbatch as L(k) = P\ni\u2208[4] L(k)\ni\n. Then, with the additional (B + 1)-th microbatch\ninserted into Part 1 of the explicit bubbles, the accumulated gradient of one training iteration (normalized\nby 1/B) becomes:\naccumulated gradient = 1\nB\n\u0012 X\nk\u2208[B]\n\u2202L(k)\n\u2202\u03b8\n+ \u2202(L(B+1)\n1\n+ L(B+1)\n2\n)\n\u2202\u03b8\n\u0013\n= 1\nB\n\u0012 X\nk\u2208[B]\n\u2202(L(k)\n1\n+ L(k)\n2\n+ L(k)\n3\n+ L(k)\n4 )\n\u2202\u03b8\n+ \u2202(L(B+1)\n1\n+ L(B+1)\n2\n)\n\u2202\u03b8\n\u0013\n= 1\nB\n\u0012\nX\nk\u2208[B+1]\n\u2202(L(k)\n1\n+ L(k)\n2 )\n\u2202\u03b8\n+\nX\nk\u2208[B]\n\u2202(L(k)\n3\n+ L(k)\n4 )\n\u2202\u03b8\n\u0013\n.\nTaking the expectation over the data distribution, we see that the additional microbatch essentially scales\nup the weights of L1 and L2 in the gradient by a factor of (B + 1)/B. Or, if the weights of L1 and L2 are\nmanually scaled by a factor of B/(B + 1) during training, then we recover an unbiased gradient estimate for\nthe original risk L = P\ni\u2208[4] Li:\naccumulated gradient =\n1\nB + 1\nX\nk\u2208[B+1]\n\u2202(L(k)\n1\n+ L(k)\n2 )\n\u2202\u03b8\n+ 1\nB\nX\nk\u2208[B]\n\u2202(L(k)\n3\n+ L(k)\n4 )\n\u2202\u03b8\nWe claim that this leads to reduced gradient variance, unless gradients of early and later losses have strong\nnegative correlation. In the formal analysis of variance below, we consider scalars for notational simplicity,\nthough it can be easily generalized to vectors or tensors by replacing scalar multiplication with inner product.\n26\nProposition 2. Consider two distributions A and B with means a\u22c6 and b\u22c6, respectively. Let a1, . . . , aN, aN+1\nbe independent and identically distributed (i.i.d.) samples of A, and b1, . . . , bN be i.i.d. samples of B, but for\neach i, ai can be correlated with bi. Consider two estimates of a\u22c6 + b\u22c6, defined as follows:\nbe := baN + bbN,\nbe+ := baN+1 + bbN,\nwhere\nbak := 1\nk\nX\ni\u2208[k]\nai,\nbbk := 1\nk\nX\ni\u2208[k]\nbi,\nk \u2208 {N, N + 1}.\nThen it holds that\nE[be] = E[be+] = a\u22c6 + b\u22c6,\nvar(be) \u2212 var(be+) =\n1\nN(N + 1)var(a1) +\n2\nN(N + 1)cov(a1, b1).\nIn this proposition, baN, baN+1 and bbN correspond to\n1\nB\nP\nk\u2208[B]\n\u2202(L(k)\n1\n+L(k)\n2\n)\n\u2202\u03b8\n,\n1\nB+1\nP\nk\u2208[B+1]\n\u2202(L(k)\n1\n+L(k)\n2\n)\n\u2202\u03b8\nand\n1\nB\nP\nk\u2208[B]\n\u2202(L(k)\n3\n+L(k)\n4\n)\n\u2202\u03b8\nin our previous analysis of accumulated gradients, respectively. Hence our claim\non gradient variance follows immediately from the conclusion of this proposition.\nProof of Proposition 2. First, unbiasedness is obvious. As for variance, we have the following elementary\ncalculation:\nvar(be) = E\n\u0002\n(baN + bbN \u2212 a\u22c6 \u2212 b\u22c6)2\u0003\n= E\n\u0002\n(baN \u2212 a\u22c6)2\u0003\n+ E\n\u0002\n(bbN \u2212 b\u22c6)2\u0003\n+ 2E\n\u0002\n(baN \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\n,\nvar(be+) = E\n\u0002\n(baN+1 + bbN \u2212 a\u22c6 \u2212 b\u22c6)2\u0003\n= E\n\u0002\n(baN+1 \u2212 a\u22c6)2\u0003\n+ E\n\u0002\n(bbN \u2212 b\u22c6)2\u0003\n+ 2E\n\u0002\n(baN+1 \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\n.\nMoreover,\nE\n\u0002\n(baN \u2212 a\u22c6)2\u0003\n= 1\nN var(a1),\nE\n\u0002\n(baN+1 \u2212 a\u22c6)2\u0003\n=\n1\nN + 1var(a1),\nand\nE\n\u0002\n(baN \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\n=\n1\nN 2\nX\ni\u2208[N]\nE\n\u0002\n(ai \u2212 a\u22c6)(bi \u2212 b\u22c6)\n\u0003\n= 1\nN cov(a1, b1),\nE\n\u0002\n(baN+1 \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\n=\n1\nN(N + 1)\nX\ni\u2208[N]\nE\n\u0002\n(ai \u2212 a\u22c6)(bi \u2212 b\u22c6)\n\u0003\n=\n1\nN + 1cov(a1, b1).\nPutting things together,\nvar(be) \u2212 var(be+) = E\n\u0002\n(baN \u2212 a\u22c6)2\u0003\n\u2212 E\n\u0002\n(baN+1 \u2212 a\u22c6)2\u0003\n+ 2\n\u0010\nE\n\u0002\n(baN \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\n\u2212 E\n\u0002\n(baN+1 \u2212 a\u22c6)(bbN \u2212 b\u22c6)\n\u0003\u0011\n=\n1\nN(N + 1)var(a1) +\n2\nN(N + 1)cov(a1, b1),\nwhich concludes our proof.\nReferences\n[1] Anonymous. A simple romance between multi-exit vision transformer and token reduction. In Submitted\nto The Twelfth International Conference on Learning Representations, 2023. under review.\n[2] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,\n2016.\n[3] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework\nfor autoregressive language models with synchronized parallel decoding. ArXiv, abs/2310.05424, 2023.\n27\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. In ICLR, 2015.\n[5] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in\nneural networks for faster models. ArXiv, abs/1511.06297, 2015.\n[6] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through\nstochastic neurons for conditional computation. ArXiv, abs/1308.3432, 2013.\n[7] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch,\nDallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis,\nDora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Good-\nman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho,\nJenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Sid-\ndharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay\nKrishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel\nPapadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan,\nRobert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher\nR\u2019e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex\nTamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu,\nJiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On\nthe opportunities and risks of foundation models. ArXiv, abs/2108.07258, 2021.\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In NeurIPS, 2020.\n[9] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie,\nZhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. Data-juicer: A one-stop data\nprocessing system for large language models. ArXiv, abs/2309.02033, 2023.\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory\ncost. ArXiv, abs/1604.06174, 2016.\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepa-\nssi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113, 2023.\n[12] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019.\n28\n[13] Luciano Del Corro, Allison Del Giorno, Sahaj Agarwal, Ting Yu, Ahmed Hassan Awadallah, and Sub-\nhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient\nllm inference. ArXiv, abs/2307.02628, 2023.\n[14] Yinwei Dai, Rui Pan, Anand Iyer, Kai Li, and Ravi Netravali. Apparate: Rethinking early exits to\ntame latency-throughput tensions in ml serving. ArXiv, abs/2312.05385, 2023.\n[15] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and\nmemory-efficient exact attention with IO-awareness. In NeurIPS, 2022.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep\nbidirectional transformers for language understanding. In North American Chapter of the Association\nfor Computational Linguistics, 2019.\n[17] Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. Jump to conclusions: Short-cutting\ntransformers with linear transformations. ArXiv, abs/2303.09435, 2023.\n[18] Rahul Duggal, Scott Freitas, Sunny Dhamnani, Duen Horng Chau, and Jimeng Sun. Elf: An early-\nexiting framework for long-tailed classification. ArXiv, abs/2006.11979, 2020.\n[19] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In ICLR,\n2020.\n[20] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long,\nJun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin. DAPPLE: a pipelined data parallel\napproach for training large models. In PPoPP, pages 431\u2013445, 2021.\n[21] William Fedus, Barret Zoph, and Noam M. Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1\u2013120:39, 2021.\n[22] Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and\nEyal Shnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In ACL, 2023.\n[23] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016.\n[24] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang.\nDynamic neural\nnetworks: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):7436\u20137456, 2022.\n[25] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic BERT\nwith adaptive width and depth. In NeurIPS, 2020.\n[26] Boren Hu, Yun Zhu, Jiacheng Li, and Siliang Tang. Smartbert: A promotion of dynamic early exiting\nmechanism for accelerating bert inference. In IJCAI, 2023.\n[27] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger.\nMulti-scale dense networks for resource efficient image classification. In ICLR, 2018.\n[28] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant\nneural networks using pipeline parallelism. In NeurIPS, pages 103\u2013112, 2019.\n[29] Fatih Ilhan, Gong Su, and Ling Liu. Scalefl: Resource-adaptive federated learning with heterogeneous\nclients. In CVPR, pages 24532\u201324541, 2023.\n[30] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of\nlocal experts. Neural Computation, 3:79\u201387, 1991.\n[31] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\nShallow-deep networks: Understanding and\nmitigating network overthinking. In ICML, volume 97, pages 3301\u20133310, 2019.\n29\n[32] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Gen\u00c3\u00a7,\nGrace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami.\nFull stack optimization of transformer inference: a survey. ArXiv, abs/2302.14017, 2023.\n[33] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gho-\nlami, and Kurt Keutzer. Speculative decoding with big little decoder. In NeurIPS, 2023.\n[34] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In\nICLR, 2017.\n[35] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.\n[36] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence C. McAfee, Michael Andersch, Mo-\nhammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer mod-\nels. ArXiv, abs/2205.05198, 2022.\n[37] Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem H. Zuidema, and Jaap Jumelet. Decoderlens:\nLayerwise interpretation of encoder-decoder transformers. ArXiv, abs/2310.03686, 2023.\n[38] Stefanos Laskaridis, Alexandros Kouris, and Nicholas Donald Lane. Adaptive inference through early-\nexit networks: Design, challenges and directions. In International Workshop on Embedded and Mobile\nDeep Learning, 2021.\n[39] Chen-Yu Lee, Saining Xie, Patrick W. Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised\nnets. In AISTATS, 2014.\n[40] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\ndecoding. In ICML, 2022.\n[41] Shigang Li and Torsten Hoefler. Chimera: efficiently training large-scale neural networks with bidirec-\ntional pipelines. In SC, page 27, 2021.\n[42] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. Accelerating\nbert inference for sequence labeling via early-exit. In ACL, 2021.\n[43] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u2019e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation\nof language models. Annals of the New York Academy of Sciences, 1525:140 \u2013 146, 2023.\n[44] Stephanie Lin, Jacob Hilton, and Owain Evans.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In ACL, pages 3214\u20133252, 2022.\n[45] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. Fastbert: a self-distilling\nBERT with adaptive inference time. In ACL, pages 6035\u20136044, 2020.\n[46] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In EMNLP, pages 1797\u20131807,\n2018.\n[47] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R.\nGanger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for DNN\ntraining. In SOSP, pages 1\u201315, 2019.\n30\n[48] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient\npipeline-parallel DNN training. In ICML, volume 139, pages 7937\u20137947, 2021.\n[49] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. Efficient large-scale language model training on GPU clusters using megatron-lm.\nIn SC, page 58, 2021.\n[50] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[51] Kazuki Osawa, Shigang Li, and Torsten Hoefler. Pipefisher: Efficient training of large language models\nusing pipelining and fisher information matrices. In MLSys, 2023.\n[52] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model\nfor natural language inference. In EMNLP, pages 2249\u20132255, 2016.\n[53] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.\nArXiv, abs/2211.05102, 2022.\n[54] Ofir Press and Lior Wolf. Using the output embedding to improve language models. In EACL, pages\n157\u2013163, 2017.\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understand-\ning by generative pre-training, 2018.\n[56] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\n[57] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In SC, 2019.\n[58] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\nenable training deep learning models with over 100 billion parameters. In KDD, pages 3505\u20133506, 2020.\n[59] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo\nMarin, and Emanuele Rodol\u00e0. Accelerating transformer inference for translation via parallel decoding.\nIn ACL, 2023.\n[60] Simone Scardapane, Michele Scarpiniti, Enzo Baccarelli, and Aurelio Uncini. Why should we add early\nexits to neural networks? Cognitive Computation, 12:954 \u2013 966, 2020.\n[61] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald\nMetzler. Confident adaptive language modeling. In NeurIPS, 2022.\n[62] Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Consistent accelerated inference\nvia confident adaptive transformers. In EMNLP, pages 4962\u20134979, 2021.\n[63] Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A. Smith. The right\ntool for the job: Matching model and instance complexities. In ACL, pages 6640\u20136651, 2020.\n[64] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-\ngenerator networks. In ACL, pages 1073\u20131083, 2017.\n[65] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman.\nMesh-tensorflow: Deep learning for supercomputers. In NIPS, pages 10435\u201310444, 2018.\n[66] Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In\nICLR, 2017.\n31\n[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv,\nabs/1909.08053, 2019.\n[68] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. ArXiv, abs/2201.11990, 2022.\n[69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, D. Er-\nhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n[70] Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan, and R. Manmatha.\nDeed: Dynamic early exit on decoder for accelerating encoder-decoder transformer models.\nArXiv,\nabs/2311.08623, 2023.\n[71] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM\nComput. Surv., 55(6), 2022.\n[72] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.\nhttps://github.com/InternLM/InternLM, 2023.\n[73] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early exiting\nfrom deep neural networks. In ICPR, pages 2464\u20132469, 2016.\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.\nArXiv, abs/2302.13971, 2023.\n[75] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann,\nA. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nfoundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.\n[76] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by\nenabling intermediate layer decoding via instruction tuning with lite. ArXiv, abs/2310.18581, 2023.\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998\u20136008, 2017.\n[78] Haoyu Wang, Yaqing Wang, Tianci Liu, Tuo Zhao, and Jing Gao. Hadskip: Homotopic and adaptive\nlayer skipping of pre-trained language models for efficient inference. In Conference on Empirical Methods\nin Natural Language Processing, 2023.\n[79] Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. Skipbert: Efficient inference with\nshallow layer skipping. In ACL, 2022.\n32\n[80] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant:\nAccurate and efficient post-training quantization for large language models.\nIn ICML, volume 202,\npages 38087\u201338099, 2023.\n[81] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for\naccelerating BERT inference. In ACL, pages 2246\u20132251, 2020.\n[82] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\nBerxit: Early exiting for BERT with better\nfine-tuning and extension to regression. In EACL, pages 91\u2013104, 2021.\n[83] Canwen Xu and Julian McAuley. A survey on dynamic neural networks for natural language processing.\nIn EACL, pages 2370\u20132381. Association for Computational Linguistics, 2023.\n[84] Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Ke Tan, Fu Wu, Jiezhong Qiu, and Aimin Pan.\nHolmes: Towards distributed training across clusters with heterogeneous nic environment.\nArXiv,\nabs/2312.03549, 2023.\n[85] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. In NeurIPS,\n2022.\n[86] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher\nR\u00e9, and Ce Zhang. Decentralized training of foundation models in heterogeneous environments. In\nNeurIPS, 2022.\n[87] Dewen Zeng, Nan Du, Tao Wang, Yuanzhong Xu, Tao Lei, Zhifeng Chen, and Claire Cui. Learning to\nskip for language modeling. ArXiv, abs/2311.15436, 2023.\n[88] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, pages 12360\u201312371,\n2019.\n[89] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,\nDaniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-\ntrained transformer language models. ArXiv, abs/2205.01068, 2022.\n[90] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang\nRen, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large\nlanguage models. ArXiv, abs/2303.18223, 2023.\n[91] Yao Zhao, Zhitian Xie, Chenyi Zhuang, and Jinjie Gu. Lookahead: An inference acceleration framework\nfor large language model with lossless generation accuracy. ArXiv, abs/2312.12728, 2023.\n[92] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\nYuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. Alpa: Automating\ninter- and Intra-Operator parallelism for distributed deep learning. In OSDI, pages 559\u2013578, 2022.\n[93] Zhengyi Zhong, Ji Wang, Weidong Bao, Jingxuan Zhou, Xiaomin Zhu, and Xiongtao Zhang. Semi-hfl:\nsemi-supervised federated learning for heterogeneous devices. Complex & Intelligent Systems, 9:1995\u2013\n2017, 2022.\n[94] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu Wei.\nBERT loses\npatience: Fast and robust inference with early exit. In NeurIPS, 2020.\n33\n"
  },
  {
    "title": "Localized Symbolic Knowledge Distillation for Visual Commonsense Models",
    "link": "https://arxiv.org/pdf/2312.04837.pdf",
    "upvote": "2",
    "text": "Localized Symbolic Knowledge Distillation\nfor Visual Commonsense Models\nJae Sung Park1, Jack Hessel2, Khyathi Raghavi Chandu2,\nPaul Pu Liang2,4, Ximing Lu1,2, Peter West1,2,\nYoungjae Yu5, Qiuyuan Huang3, Jianfeng Gao3, Ali Farhadi1,2, Yejin Choi1,2\n1University of Washington 2Allen Institute for Artificial Intelligence 3Microsoft Research\n4Carnegie Mellon University 5Yonsei University\nAbstract\nInstruction following vision-language (VL) models offer a flexible interface that\nsupports a broad range of multimodal tasks in a zero-shot fashion. However,\ninterfaces that operate on full images do not directly enable the user to \u201cpoint to\"\nand access specific regions within images. This capability is important not only to\nsupport reference-grounded VL benchmarks, but also, for practical applications that\nrequire precise within-image reasoning. We build Localized Visual Commonsense\nmodels, which allow users to specify (multiple) regions as input. We train our model\nby sampling localized commonsense knowledge from a large language model\n(LLM): specifically, we prompt an LLM to collect commonsense knowledge given\na global literal image description and a local literal region description automatically\ngenerated by a set of VL models. With a separately trained critic model that selects\nhigh-quality examples, we find that training on the localized commonsense corpus\ncan successfully distill existing VL models to support a reference-as-input interface.\nEmpirical results and human evaluations in a zero-shot setup demonstrate that our\ndistillation method results in more precise VL models of reasoning compared to a\nbaseline of passing a generated referring expression to an LLM 1.\n1\nIntroduction\nLarge language models are capable of efficiently performing a wide array of tasks in a zero-shot\nfashion. For text-only models, one commonly adopted interface is a flexible, language specifica-\ntion of inputs coupled with an imperative request, e.g., \u201c[article text].\nSummarize this\narticle.\" Similarly, a natural extension allowing visual inputs manifests as, e.g., \u201c[image].\nDescribe this image\".\nHowever, as models expand beyond text-only modalities, they should incorporate more flexible\nforms of user input as well. Allowing users to specify individual objects/actors/regions within\nan image as part of the input query is an important challenge, e.g., the [image] [request]\ninterface above would not directly a user to ask Why is [this person in the image] sad?.\nOne option would be to simply require users specifically describe the piece of the image they\nare attempting to specify, e.g., \u201c[image] [description of specific region] [request]\".\nHowever, authoring concrete referring expressions is not only cumbersome, particularly for scenes\nwith lots of objects (e.g., \u201cthe person in the red jacket on the left of the scene with their arms\ncrossed\") but also challenging, even for humans: [11] argue that a good referring expression should\nboth specify the reference precisely, but also, follow Grice\u2019s maxim of Quantity, i.e., provide no\nextra information. Given this tension, many popular referring expression datasets are gathered in a\nsophisticated \u201cgamified\" fashion [53, 22], which aims to balance underspecification vs. verbosity.\n1Code will be released in https://github.com/jamespark3922/localized-skd\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.04837v2  [cs.AI]  12 Dec 2023\nWe argue instead that users of vision-augmented LLMs should instead be able to pass localized visual\nreferences simply by \u201cpointing\" to regions within the image [4, 48, 40]. This enables models to\nfocus on the region while interpreting the user\u2019s request in a more intuitive fashion, and provide\nmore accurate and contextually relevant responses. By incorporating localized visual references, the\nmodel can better understand and interpret complex scenes, thereby improving its performance on\ntasks requiring a detailed understanding of the visual context.\nWe propose Localized Symbolic Knowledge Distillation (LSKD): the core idea is to provide literal\ndescriptions of images to a large language model and allow that model to connect the dots between\nthese literal descriptors (e.g., lists of objects) and a holistic perspective of the scene. Different\nfrom recent works which also distill from an LLM conditioned on visual descriptors symbolically\n[34, 74], we additionally provide a localized reference to a particular region within the image and\ndesign prompts to encourage the LLM to generate commonsense inference about that specific region.\nAfter sampling, we train Localized Visual Commonsense models to generate commonsense triples\nconditioned on the image and the region directly; we show that this process effectively distills the\nLLM\u2019s capacity for global+local scene understanding highlighted by zero-shot results on localized\nvisual reasoning benchmarks and human evaluation.\nIn summary, our main contributions are:\n1. A new scalable framework that can generate reliable and localized visual commonsense statements.\n2. The Localized Commonsense Knowledge Corpus: 1M localized commonsense inferences posed\nover 250K images. This dataset can be used to expand the capacity of existing vision+language\nmodels to incorporate references-as-input with no architectural modifications.\n3. Achieving the SoTA zero-shot performance for three localized visual reasoning tasks.\n4. Human evaluation results suggesting that a strong student model outperforms the teacher model in\nanswering localized visual commonsense questions.\n2\nDistilling Localized Visual Commonsense from a LLM\nHere, we describe our LSKD pipeline to distill visual commonsense from a LLM. Prior works have\nexplored powerful LLM as the teacher model (GPT-3, ChatGPT) to apply knowledge distillation for\nlanguage-only reasoning tasks [58, 33, 3]. Multimodal inputs offer additional challenges in grounding\nregions to relevant texts. Our work addresses this challenge by automatically generating reliable and\ndiverse knowledge statements for multimodal input, to further reason about regions within an image.\nFigure 1 shows the overall framework of LSKD2. To learn from the LLM as our teacher model, we\nverbalize the image into a set of dense text statements generated by global descriptors that provide\nrelevant, general overall semantics of the image, and local descriptors that talk about specific regions\nin the image. We then pass these automatically generated descriptions to LLM and prompt to mine\nlocalized, commonsense statements about the image at scale (See the Appendix for the exact prompt).\nAs LLMs comprehend multimodal input only through machine-generated image-to-text verbalization,\nthey are prone to hallucination and generation of inconsistent statements about the image. For\ninstance, an incorrect verbalizer output, as in Figure 1, might cause the LLM to produce visually\nincoherent statements like \"[1] is holding a surfboard\". To minimize errors in modality translation,\nwe construct a critic model, trained on a limited set of high-quality, hand-annotated instances to\ndetect and remove such inconsistencies. This critic model mimics human judgment in evaluating the\ngenerated commonsense knowledge, so that we can intentionally oversample localized knowledge\ndata, and utilize it to filter out non-relevant instances. Finally, we finetune a vision-language model\non the high-quality synthetic data to facilitate zero-shot localized visual commonsense reasoning.\nWe use 250K images in union of Visual Genome [26] and VCR [66], which include a diverse\nset of social situations involving people and objects, as the seed images to collect the knowledge\ncorpus. After filtering, we collect 1M instances of Localized Commonsense Knowledge Corpus with\ninformation grounded to specific regions in the image (see Appendix A for more details).\n2For visualization purposes, we provide a shortened version of verbalizations. The full verbalization output\nwith the prompt to call ChatGPT is shown in the Appendix.\n2\n\u2026.\n[0] man in black shorts [(0.56, 0.3), (0.8, 0.84)] \n[1] little girl in striped bathing suit [(0.22, 0.61), (0.49, 1.0)] \n[2] a woman wearing a pink shirt [(0.3, 0.3), (0.44, 0.62)] \n[3] sunglasses on a woman's face [(0.32, 0.33), (0.4, 0.35)] \n[4] a white surfboard on the sand [(0.0, 0.73), (0.1, 0.96)] \n[5] a surfboard [(0.44, 0.03), (0.69, 0.75)]\nPlace: place at raft, or beach. \nObjects: surfboard, paddle, and board. \nConcepts: board short, surfboard, stand \nup paddle sur\ufb01ng, surfboard shaper.\nGlobal \nDescriptors \n1. Question: What is the lighting of the image?  Answer: natural light \n2. Question: What is the brief description of the image?  Answer: a family on a beach \n3. Question: Does this take place inside or outside?  Answer: outside \n4. Question: What is the little girl doing in the image? Answer: holding a surfboard \n5. Question: Are there any other people besides the man, woman, and girl? Answer: no \n6. Question: Is the ocean calm or wavy in the background? Answer: wavy\nMan and woman standing. The man is \nholding a sur\ufb01ng board. In front of him a \ngirl is there. At the background there is \nwater and a sky. To the left side there is \na surf board.\n1. Image to \nText \nVerbalization\n1M Localized  \nCommonsense \nKnowledge Corpus\nQuestion: What might be [0] and [2] discussing as they hold the surfboard?\nAnswer: [0] and [2] might be discussing how to teach [1] how to surf.\nRationale: The man and woman are holding a surfboard and the little girl [1] is standing in front \nof them holding a smaller surfboard. It is possible that they are discussing how to teach the little \ngirl how to surf.\nQuestion \nAnswer\n4. Localized  \nKnowledge Distillation \non VL models\n3. Supervised \nCritic \nFiltering\nBLIP-2\nQuestion: What is [1] holding in the image?\nAnswer: [1] is holding a surfboard in the image.\nRationale: By focusing on [1]'s activity, we can infer that she is likely interested in sur\ufb01ng and \nmay be learning from the adults.\n2. Localized Knowledge Generation with LLM\nChatGPT\nCLIP \nRetrieved \nConcepts\nOFA \nLocalized \nNarratives\nChatGPT \nQuestion w/ \nBLIP-2 \nAnswers\n BLIP-2 Region \nCaptioner\nBBOX Proposals\nLocal \nDescriptors \n\u2026.\n\u2026\u2026. \u2026\n\u2026.\n\u2026\n\u2026.\n\u2026.\n\u2026.\nAdd \ufb01ltered examples\nLocalized Commonsense Knowledge Corpus\nFigure 1: Pipeline of our LSKD framework. 1) Diverse vision-language descriptors are used\nto verbalize images. 2) LLMs leverage the global and local descriptors to generate grounded\ncommonsene knowledge. 3) We annotate a small subset of data to train a supervised critic model that\ncan filter instances displaying incorrect visual details or incoherent reasoning. The critic model filters\nthe rest of generated statements to finalize the data pool. 4) A multimodal model is finetuned on the\nsynthetic data to support localized visual commonsense reasoning in a zero-shot manner.\n2.1\nImage to Text Verbalization\nWe first describe our methods for verbalizing (i.e., writing out in natural language) images/regions\nto text. Note that this step does not require images with text annotations for the target datasets,\nunlike prior work [34], and can be applied to any set of images. We focus on deriving global image\ndescriptions, local region descriptions, and dynamic question-answer pairs for each image. Figure 1\ngives a schematic of our process which includes an example image verbalization.\nGlobal descriptors: Image Verbalizers\nFollowing [69], we use the CLIP-ViTL model in a zero-\nshot fashion to extract basic concept information about the image using a template. We retrieve places\nfrom the Place365 [71], objects from TencentML-Images [59], and concepts from OpenImages [27]\nto arrive at global concepts. Specifically: we use the top 2 places, the top 3 object labels, and the top\n3 concepts. In addition to concepts, we also get narrative descriptions of the entire image. For this,\nwe fine-tuned OFA-Huge [54] on the Localized Narratives [44] corpus, which pairs 849K images\nwith multi-sentence descriptions (we ignore the mouse trace information and simply treat the task as\nimage-to-text captioning). We sample 5 localized narratives for each image using a temperature of\n1.1.\nLocal descriptors: Region Verbalizers.\nGlobal descriptors alone often fail to capture the intricate\ndetails of specific regions within an image, leading to a potential bottleneck in understanding scenes\nwith more visual precision and enabling localized reasoning. We employ local descriptors that provide\nmore grounded visual statements. To do so, we sample bounding box regions for the image using\nregion proposal models from object detection literature [32]. We then train a region captioning model\nthat maps from (image, region) \u00d1 description of the region. We fine-tuned the generative version of\nBLIP-2 [29] with the FLAN-t5-xxl [7] backbone. We trained on datasets that provide descriptions\nof regions within images. a combination of RefCOCO/RefCOCO+/RefCOCOg [64, 37], Sherlock\nClues-only [19] (277K), and VisualGenome [26] (1.96M): all of these datasets provide descriptions\nof given regions within images. Following [68, 62], we render the bounding box in the image itself to\nallow the model access to the bounding box\u2019s location. More details of the local descriptors are in\nAppendix E.\n3\nQA MSE\nRationale MSE\nPrecision\nRecall\nF1\n64.7\n64.2\n64.3\n\u2713\n66.3\n65.2\n65.7\n\u2713\n66.0\n64.3\n64.8\n\u2713\n\u2713\n66.8\n65.7\n66.0\nTable 1: Analysis of BLIP-2 based critic model.\nWe see that adding the multi-class regression loss\nfurther improves the performance of critic model.\nFigure 2: Precision of Critic Model with vary-\ning threshold values to filter the corpus size.\nPrecision is increased significantly by using\nthe supervised critic model to filter the corpus.\nDynamic descriptors: Q+A Verbalizers\nFinally, to support a holistic understanding and enable\nmodels to dynamically probe for potentially missing context, we acquire more fine-grained details\nabout the scene using a series of questions and answers. Following [73], we prompt an LLM to ask\nshort, simple questions conditioning on the global and local descriptors as context, and query BLIP-2\n[29] in a zero-shot fashion to answer the questions. We specifically collect 15 question/answer pairs\nfor each image.\n2.2\nLocalized Commonsense Knowledge Generation\nFor all experiments, we use ChatGPT as our LLM,3 though in principle, any instruction-tuned LLM\ncould be used. We use question-answering-rationale (QAR) for knowledge representations. QAR\nrepresentations are flexible, and have been successfully adopted as meaning representations in areas\nranging from formal semantics [17, 38, 24] to commonsense reasoning [50, 66].\nGiven the verbalization of images, we prompt ChatGPT to come up with an interesting and complex\nquestion with the possible answer that requires rationale to justify the reasoning. We support\ntwo versions of localized knowledge generation. One that refers to specific regions in the image\neither by their assigned numerical IDs and bounding box coordinates (e.g. What is [2] doing\nin the image?) for more precise localization, and one that uses descriptive phrases (e.g. What\nis [the woman wearing a pink shirt] doing in the image?) for more contextual and\ndetailed references. Qualitatively, we observe that the LLM is able to connect the IDs and the region\ndescriptions successfully, and create a convincing set of localized commonsense knowledge corpus.\nFor each image, we prompt ChatGPT three times to generate three unique QARs sequentially. We do\nthis for ID-based and description-based references (see Appendix for the prompts), and collect 18\nlocalized instances per image.\n2.3\nTraining the Critic Model\nWe train a supervised critic model to reflect the human acceptability of generated data. We allocate a\nsubset of 20K statements to train the critic model, and 4k for evaluation. The \u201caccepted\" instances\nshould generally deliver the visually correct information and exhibit coherent reasoning. For each\nQAR, we ask two human annotators to rate from 1 to 3 (reject / maybe / accept) if 1) the QA displays\nvisually correct information (QA rating), and 2) the rationale justifies the answer while being aligned\nwith the image (QA \u00d1 R rating)4. We then assign binary label if at least one annotator has included\nreject for any of the two rating criteria. Using this labeling scheme, we found that only 45% of the\ninstances are labeled as accepted, suggesting that aggressive filtering by the critic model is required.\nFor the model, we use a stage-1 pre-trained BLIP2 [29] with ViT-G [12] image encoder to do\nthe critique. Following their finetuning scheme on retrieval tasks, we train the image encoder and\nQ-Former together, not freezing the weights. We add a linear layer to the image-text matching head\nthat has been pre-trained to capture the multimodal content, and train it to perform the classification.\n3https://openai.com/blog/chatgpt\n4The second criterion is automatically rejected if the QA has already rejected in the first pass\n4\nDescriptors Used\nAverage Critic Score\nFull Descriptors\n58.4\n(-) CLIP Concepts\n52.1\n(-) Localized Narratives\n56.1\n(-) Global Descriptors\n54.3\n(-) Local Descriptors\n49.8\n(-) QAs\n49.0\nTable 2: Ablation study of the descriptors.\nWe remove one of the descriptors from full\ndescriptors when calling ChatGPT to gen-\nerate the corpus, and calculate the average\ncritic score to rate the generations (higher\nthe better).\n0\n1\n2\n3\nQA Correctness\nRationale Justi\ufb01cation\n1.87\n1.97\n2.34\n2.41\nWith Filtering (threshold = 0.8)\nWithout Filtering\nFigure 3: Human judgment of corpus with\nand without filtering. We get the average\nratings in Likert scale (from 1 to 3) from\nthree human annotators.\nWe utilize the two rating criteria (QA and QA \u00d1 R) to further inform the critic model to know what\ncaused the humans to reject the QARs. We achieve this by multi-task training of critic model. The\nratings containing reject are given the regression label of 0, while the average of two QA and QA \u00d1 R\nratings is calculated to get the regression label yQA and yQA\u00d1R. Along with the binary classification\nloss, the image-text matching head is further trained with mean squared error (MSE) losses with\nyQA and yQA\u00d1R. Table 1 shows the performance of critic model on the above train and eval split.\nWe empirically see that adding the multi-task loss (QS MSE and Rationale MSE) further helps the\nperformance of classification.\nAnalysis of Supervised Critic\nHow reliable is the critic model on filtering erroneous instances?\nIn the annotation stage, we have observed that only 45% of the instances would be considered as\nvalid by humans. We explore tuning different thresholds of critic model to filter the data (e.g. keep\ninstances whose predicted scores are higher than the threshold), and see if higher acceptability can\nbe achieved with higher threshold. Figure 2 shows a plot of precision value (instances labeled as\n\u201caccept\") by the filtered corpus size. We see a consistent trend where removing the corpus with more\ncritical criteria yields higher acceptability. Specifically, it jumps from 45% of 70% acceptance if 20%\nare maintained by the critic model. We use this threshold value of 0.8 to apply the critic model. Note\nthat filtering the corpus randomly, on the other hand, doesn\u2019t have any influence on the acceptability.\nIn addition, we run human evaluation to measure the acceptability of data with and without filtering.\nWe collect 500 instances the same way critic model labels are collected: 1) is the QA visually correct?\nand 2) does the rationale justify the answer? Likert scores from [1-3] are calculated for each criteria\n(higher the better). Figure 3 shows the human evaluation results, and we see that the dataset with\nfiltering is more favored by humans than without filtering.\nAre All the Descriptors Necessary?\nWe run ablation studies of the descriptor components in\nthe ChatGPT prompt and use the critic model to score the ChatGPT generations. We collect QAR\ninstances for 500 images and calculate the average critic score, with higher score aligned with human\npreference. Table 2 shows the result when one of the descriptors is removed from full verbalizations.\nWe see that using all descriptors provides the best results, and in fact the QA descriptor provides the\nbiggest jump (from 49.0 to 58.4).\n2.4\nTraining with the Localized Corpus\nWe explore the distillation of localized commonsense knowledge by finetuning discriminative and\ngenerative vision language model on our corpus. For the corpus that mentions IDs and bounding box\ncoordinates, we follow [68, 62, 67, 19] by directly drawing colored highlights around the regions in\nthe images where the region IDs and highlights are consistent throughout the corpus (e.g. [0] always\ngets the color pink).\nDuring training, we additionally apply region-based augmentation by reassigning the IDs with a\nrandom order while still keeping a consistent color coding (e.g. What might be [0] and [1] discussing?\n5\n\u00d1 What might be [1] and [3] discussing?). We similarly vary the number of regions to be shown\nin the image, while ensuring that the mentioned IDs are drawn in the image. With these tricks, the\nmodifications are performed in the input image and text to enable localization, while the architecture\nand training objectives of the vision-language model remain unchanged.\nWe use the BLIP-2 [29] as the vision and language backbone model. Given the recent success and\nefficiency of visual instruction methods, [34, 74, 29, 10], we freeze the weights of visual and language\nmodel and only train the Qformer [34] learns to map visual to text tokens. For discriminative tasks,\nwe apply the stage 1 pre-training objective with Image-Text Contrastive, Image-Text Matching, and\nImage-Text Grounding Losses. We further explore generative performance with the FlanT5XXL [57]\nlanguage model and Mini-GPT4 that tunes the Vicuna-13b-v0 language model [6, 52] to understand\nvisual tokens. We refer to [29] for more training details.\n3\nExperiments & Results\nWe use the OpenAI Chat API with gpt-3.5-tubro engine and a temperature of 0.8 to prompt the LLM\nto collect knowledge data. The BLIP-2 critic model is trained with total batch size of 256, learning\nrate of 1e-5, max 10 epochs. The visual encoder (ViT-G) model is additionally trained instead of kept\nit as frozen.\nThe discriminative BLIP2 is trained with 256 batch size and 128 max sequence length for 1e4\niterations. The BLIP-2 FlanT5XXL and Mini-GPT4 models are trained with 64 batch size and 2e4\niterations. All models are trained with learning rate of 1e-5, Adam optimizer [23], linear warmup with\ncosine annealing, and image size of 480 using 80GB 4 A100 GPUS. We do not finetune the ViT or\nthe language model, and only train the QFormer shown by the success from prior work [29, 10, 34].\n3.1\nDownstream Tasks\nLocalized Visual Commonsense Reasoning\nWe evaluate on a set of visual commonsense reasoning\ntasks that involve identifying and referring specific regions in the image in a zero-shot setting.\nVCR [66] is a task that requires choosing the right answers for question (Q \u00d1 A), and rationales\njustifying the answer (QA\u00d1 R) from four multiple choice options. The results are combined with\n(Q \u00d1 AR) metric that requires selecting the right answer and rationale. VisualCOMET [41] is\na commonsense knowledge graph of understanding specific people\u2019s intent, and what they would\ndo before and after, and adopt their Acc@50 task of retrieving ground truth inferences from 50\ncandidates . Sherlock [19] is a visual abductive dataset that includes the comparison evaluation of\nranking of 10 text inference candidates aligned with human preference. All the aligned tasks require\nreasoning about specific regions or people in the image, and getting the image-text similarity score\nfrom a model.\nNon-Localized Visual Reasoning\nWe measure the effectiveness of the localized knowledge corpus\non other vision-language tasks not limited to datasets with no bounding box annotations. We specifi-\ncally focus on ones that require high-level reasoning that would benefit from visual commonsense\ncorpus. AOKVQA [47] requires outside world-knowledge to answer questions and we evaluate\non their multiple choice setting. SNLI-VE [61] is an inference based visual entailment that tests\nfine-grained image understanding. The task is to predict whether the image semantically entails\nthe text, and specifically classify if the image-text is one of {entailment, neutral, contradiction}.\nVisual7W [75] is visual QA with focus on visual grounding, and we evaluate on the subset of telling\nquestions that have textual answers (Telling QA).\nBaseline models\nWe include CLIP as our baseline as it has shown strong zero-shot generalization\nresults for various image-text alignment tasks [45]. Following [56], we exclude the question in\nthe text input and acquire the image-text cosine similarity score to do the task. CLIP-Event is a\nCLIP model pre-trained on their VOA dataset crawled from news websites [31]. BLIP is image-text\nalignment model trained with additional generation objective and boostrapped image captions [30].\nWe lastly evaluate the zero shot performance of BLIP-2 [29] varying the visual encoders before\napplying knowledge distillation. We do not draw bounding boxes in the image nor include id tags in\nthe text description, as these models have not been pre-trained in this domain.\n6\nLocalized\nNon-Localized\nVCR\nSherlock\nVisualCOMET\nAOKVQA\nSNLI-VE\nVisual 7w\nApproach\nQ \u00d1 A\nQA \u00d1 R\nQ \u00d1 AR\nComparison\nAcc@50\nMult. Choice\nClassification\nTelling QA\nCLIP-Event [31]\n52.4\n49.2\n-\n-\n22.4\n-\n-\n-\nCLIP ViT-B-16\u02da [45]\n54.8\n48.6\n26.6\n9.9\n33.0\n58.3\n36.0\n65.9\nCLIP ViT-L-14x336 [45]\n56.3\n51.3\n29.9\n10.9\n34.8\n61.0\n31.9\n66.7\nBLIP ViT-L [30]\n47.2\n42.5\n20.1\n18.6\n31.3\n61.3\n34.2\n69.4\nBLIP-2 ViT-L [29]\n52.3\n48.1\n25.3\n18.7\n36.7\n65.0\n31.7\n73.6\nBLIP-2 ViT-G [29]\n56.1\n49.8\n28.0\n19.5\n39.0\n68.0\n33.4\n77.1\nBLIP-2 ViT-G + LSKD\n59.0\n56.4\n33.4\n29.7\n40.3\n68.9\n40.3\n79.5\nTable 3: Zero-shot results on the localized and non-localized visual reasoning tasks. \u02daZero shot VCR\nresults directly obtained from [56]. For CLIP, we follow [56] by omitting the question and having the\nanswer (with rationale) as text input to calculate the image-text similarity. For BLIP-2, we maintain\nthe question text input as it improves the performance.\nFigure 4: Effect of data quality controlled by filtering threshold on different datasets. The x-axis\nshows the threshold for filtering and the y-axis is the accuracy metric in percentage. We compare\ntraining our model on the LLaVA-instruct dataset (red) and ours (blue).\n3.2\nZero-Shot Visual reasoning results\nTable 3 shows the zero-shot results on the downstream tasks. For localized reasoning tasks, we first\nobserve that scaling the visual encoder size (CLIP ViTB-16 vs ViT-L-14x336; BLIP-2 ViT-L vs\nViT-G) in general improves the performance. CLIP outperforms BLIP-2 on VCR tasks but fall short\non Shlerock and VisualCOMET. After applying localized symbolic knowledge distillation (LSKD) to\nBLIP-2, there is a consistent improvement over the BLIP-2 model on all downstream tasks (5.4% on\nVCR Q \u00d1 AR, 10.2 on Sherlock Comparison, 1.3% on VisualCOMET Acc@50).\nFor non-localized reasoning tasks, we observe a similar pattern. Interestingly, applying LSKD\nimproves the performance of BLIP2 model further across all the tasks (AOKVQA, SNLI-VE,\nVisual7W) over the vanilla model, despite these tasks not being the primary target domain. This\ndemonstrates that the advantages of distilling models with localized reasoning can be transferred to\nhigh-level visual commonsense tasks, thanks to the visual precision and enhanced reasoning abilities\nlearned from the generated knowledge corpus.\nInfluence of Critic Filtering on Downstream Tasks\nHow does the process of critic filtering\ninfluence the performance of downstream tasks? Keeping the size of the selected statements the\nsame at \u201e 300K, we select qualified knowledge statements with varying prediction thresholds.\nWe also compare with training on the LLaVA-instruct dataset which similarly prompts an LLM\n(GPT-4) to generate complex questions using ground truth verbalizers [34]. Figure 4 presents the\nresulting performances at these diverse thresholds across different datasets. Compared to LLaVA,\nwe observe that localized knowledge statements without filtering does not show any improvement\nfor the downstream model, while any thresholding over 0.2 is consistently better than LLaVA across\nall datasets. For tasks that demand relatively moderate commonsense, such as VCR Q\u00d1A and\nSherlock Comparison, increasing the threshold consistently improves the model performance. For\n7\nLocalized\nNon-Localized\nVCR\nSherlock\nVisualCOMET\nAOKVQA\nSNLI-VE\nVisual 7w\nDataset\nSize\nAnnotator\nQ \u00d1 AR\nComparison\nAcc@50\nMult. Choice\nClassification\nTelling QA\nZero-Shot\nNA\nNA\n28.0\n19.5\n39.0\n68.0\n33.4\n77.1\nSherlock [19]\n300K\nHuman\n34.6\n30.5\n39.7\n67.2\n38.6\n70.1\nVisualCOMET [41]\n1.2M\nHuman\n31.8\n25.3\n50.2\n68.5\n35.6\n70.8\nLLAVA-Instruct [34]\n150K\nGPT-4\n28.1\n26.9\n37.5\n71.0\n42.6\n79.5\nLSKD (Ours)\n150K\nChatGPT\n33.3\n28.6\n39.7\n69.6\n38.0\n75.9\nLSKD (Ours)\n1M\nChatGPT\n33.4\n29.7\n40.3\n68.9\n40.3\n79.5\nTable 4: Ablations of BLIP-2 ViT-G trained with varying sources of visual-knowledge corpus\nannotated by humans and machines. We break down to visual reasoning tasks that require localized\nreasoning and those do not. Critic filtering is applied to the LSKD corpus (Ours).\ntasks requiring a higher degree of commonsense such as VCR QA\u00d1R and VisualCOMET Hit@50,\nthe performance increases until a certain threshold and then fluctuates. We speculate that a more\ngrounded critic model could potentially mitigate this fluctuation, and we intend to investigate this in\nour future work. Overall, our findings suggest that higher thresholds (i.e., more critical filtering) tend\nto yield superior quality generations, thereby enhancing the performance on downstream tasks.\n3.3\nHuman vs Machine Annotated Corpus\nCan training on machine annotated corpus result in competitive performance with human annotations?\nIn Table 4, we compare the performance of BLIP-2 ViT-G trained on existing human-annotated\ncorpora with our machine-annotated corpus across various scales. First, we found that increasing the\nsize of our training corpus (150K vs 1M) leads to consistent improvement across all tasks, indicating\na promising trend of scaling law for synthetic training data. Regardless of the size, training on our\ndataset yields considerable benefits over the zero-shot model on localized reasoning tasks.\nNext, we observe that training on human annotated corpus vastly improves the performance of their\nrelative tasks (e.g. training on VisualCOMET boosts performance from 39.0 to 50.2). However,\nthis can lead to inferior results on other visual reasoning tasks than the zero-shot counterpart. For\ninstance, the performance on Visual7W drops from 77.1 (Zero-shot) to 70.1 (Sherlock) and 70.8\n(VisualCOMET). This suggests that human-designed datasets may limit task generalization due to\ntheir lack of diversity. Interestingly, we see that training the model our full LSKD corpus (1M)\nleads to uniform gains over the zero-shot model across the tasks, and even outperforms the human\nannotation corpus for the non-localized tasks as well. This shows that machine-annotated datasets,\nwhen curated and scaled adequately, can indeed rival or even surpass the performance of models\ntrained on human-annotated corpora.\nWe directly compare training on ours and the LLaVA dataset. Regardless of our dataset scale, we\nobserve that LSKD + filtering wins over training on the LLaVA corpus on localized reasoning\nbenchmarks, even when using a less powerful teacher model (ChatGPT vs GPT-4). This suggests that\nour creation of a new localization corpus is crucial to support the model with grounded reasoning. On\nthe other hand, LLAVA wins on non-localized reasoning tasks as they are aligned with the nature of\ntraining corpus. We thus observe that the appropriate application of the corpus can be task-dependent,\nand adopting a selective approach towards generating the corpus may result in significantly enhanced\nperformance across various benchmarks.\n3.4\nLocalized Reasoning with Generative Models\nWe extend LSKD to train generative models that can refer and talk about highlighted regions in image.\nWe finetune BLIP-2 FlanT5 and Mini-GPT4 and prompt them to answer questions from the VCR data.\nAs there is no baseline zero-shot model that can reason about regions to answer questions, we make a\ndirect comparison of the student LSKD model to the teacher LLM with access to verbalizations. We\nask annotators on Amazon Mechanical Turk (AMT) platform to run head-to-head comparisons (with\nties) on three criteria, if the answer delivers: 1) visually correct details, 2) informative and interesting\ninformation, and 3) content that sounds plausible. Finally, they select their overall preference. We\ntake the majority vote of 3 annotators, and disregard the instance if there is no clear majority.\n8\nModel\nCorrectness\nInformativeness\nPlausibility\nOverall\nChatGPT w/ Vebalizers\n34.7\n33.9\n39.6\n45.0\nBLIP-2 (FlanT5XXL-11B) + LSKD\n31.7\n41.0\n30.2\n41.2\nTie\n33.7\n25.1\n30.2\n13.1\nChatGPT w/ Vebalizers\n29.8\n31.7\n36.8\n40.6\nMini-GPT4 (Vicuna-13B) + LSKD\n34.3\n53.0\n34.2\n49.1\nTie\n35.9\n15.3\n30.0\n10.3\nTable 5: Human evaluation of generative models with LSKD vs Chat-GPT with verbalizers. Humans\nare asked to choose the better generation or tie if they share the same quality.\nTable 5 shows the human evaluation results. We observe that the LSKD generally wins in informa-\ntiveness over ChatGPT, but not in plausibility. We see a conflicting pattern in correctness and overall\npreference, where Mini-GPT4 is equipped with a more powerful language model that outperforms\nthe teacher model while BLIP-2 falls short. Unlike previous language-based distillation where a\nrelatively weak student model can outperform the teacher [58, 3], we see that a strong student model\nmay be required to outperform the teacher LLM in the multimodal domain.\nQualitative Results\nFigure 5 presents a comparative analysis of question-answering with ratio-\nnale results on VCR samples generated by ChatGPT, LLaVA [34] and Ours. Both Ground Truth\n(GT) and Ours consistently identify the correct entities, with Ours model often providing broader\ncontext, which is uncertain on rare occasions (e.g. \u201clikely the bride\u201d). On the other hand, ChatGPT\npredominantly focuses on observable actions or states as described in the text context, occasionally\nresulting in the misidentification of the visual entities and their relations. In the third example in\nFigure 5, \u201cwaiting for someone\u201d focuses on the observable state \u201cstanding still\u201d, missing visual detail\nsuch as a cave, holding a flame, and surrounding context. LLaVA, in contrast, generally provided a\nbroad overview while identifying a specific visual entity in most cases. However, it often struggled\nto accurately identify specific entities within the complex scene (e.g. \u201cholding a wine glass\u201d in\nFigure 5.(1), \u201ccigarette\u201d in Figure 5.(3) ). Compare to LLaVA, Ours often aligned closely with\nGroundTruth and incorporated both actions and inferred knowledge in its answer. Overall, Ours\ndelivers a more nuanced and contextually-rich response.\n4\nRelated Work\nKnowledge Distillation\nRecent research [1] has extensively explored the use of language models\nas knowledge bases, highlighting their potential in reasoning, explainability, and consistency, which\ncan enhance downstream tasks by distilling knowledge from LMs. [15] demonstrated how knowledge\naugmentation explicitly from knowledge bases and implicitly from GPT-3 improved open-domain\nmultimodal tasks. [33] showed that overgeneration with GPT-3 from exemplars to filter, as well\nas reviewed by humans, is a new and reliable way to create an NLI dataset with human and AI\ncollaboration. This setup also has the advantage of bringing forth cultural internalization via human\ncollaboration [9]. Previous works have explored knowledge distillation in the multimodal domain by\nprompting the teacher LLM with human-annotated verbalizations [34, 74, 10]. Our work is different\nin that it generated localized commonsense descriptions and the knowledge generation framework\ncan operate a scale without the need for aligned descriptions.\nFiltering\n[2] filters the generated sentences using a classifier trained on original sentences and a set\nof generated sentences. [55] used the same technique to filter out synthetic data created, which is of\nlow quality. Large language models can be used to refine the commonsense knowledge retrieved from\nweb contents by filtering the data generated from these models [39]. They perform a consolidation\nstep that filters topical and relevant assertions based on OpenIE.\nMultimodal commonsense reasoning\nrequires more profound real-world knowledge, potentially\nspanning logical, causal, and temporal relationships between concepts. For example, elements of\ncausal reasoning are required to answer the questions regarding images in VCR [66] and Visual-\nCOMET [41], while other works have also introduced datasets with video and text inputs to test for\ntemporal reasoning (e.g., Social-IQ [65], MovieQA [51], MovieFIB [36], TVQA [28]). Benchmarks\nfor multimodal commonsense typically require leveraging external knowledge from knowledge\nbases [49] or pretraining paradigms on large-scale datasets [35, 68].\n9\nQuestion & Image\nGT Answer\nChatGPT\nLLaVA\nOurs\nAnswer: [6] is most \nlikely to make the \ntoast.\nRationale: she is \nalready holding a \nwine chalice in her \nhand .\nAnswer: [2]\nRationale: The woman in the \npink dress [2] is holding a glass \nof champagne, which is \ncommonly used for celebratory \ntoasts.\nAnswer: In the scene, several \nwomen are gathered in a room and \nappear to be in a celebratory mood. \nRationale: The woman in a white \ndress is most likely to make a \ncelebratory toast because she is \nholding a wine glass and seems to \nbe engaging in a cheerful \ninteraction with the other women .\nAnswer: the bride in [6] seems to \nbe the most likely person to make a \ncelebratory toast given the occasion \nof the wedding reception.\nRationale: the fact that she is \nholding a glass of wine and in a \ndress suggests that she is most \nlikely the bride, and a toast would \nbe an appropriate and celebratory \nway for her to express gratitude and \nappreciation.\nAnswer: he\u2019s the \ndriver.\nRationale: he's \nholding a whip and \nthe reins.\nAnswer: [0] might be a groom \nor an important guest of the \nwedding party.\nRationale: [0] is seated in a \nhorse-drawn carriage, which \nsuggests that he holds some \nsignificance in the wedding.\nAnswer: In the image, a man in a \nsuit and hat is riding in a horse-\ndrawn carriage with a woman. \nRationale: The man's role appears \nto be that of a carriage driver, as he \nis steering the horse-drawn \ncarriage .\nAnswer: [0] is riding in a horse-\ndrawn carriage, which suggests that \n[0] is a driver or a rider.\nRationale: the fact that [0] is sitting \nin the driver's seat of the carriage \nsuggests that it is their job to direct \nthe carriage and keep it in a safe \nriding position.\nAnswer: [1] is \ncontemplating \nsomething.\nRationale: person in \n[1] is looking down \nand examining \nsomething .\nAnswer: [1] might be waiting \nfor something or someone.\nRationale: [1] is standing still \nand does not seem to be \nengaged in any action.\nAnswer: The man is holding a \ncigarette lighter in his hand and \nusing it to light the cigarette. \nRationale: He appears to be \nfocused on the task at hand.\nAnswer: [1] is holding a flame and \nappears to be in a cave, suggesting \nthat he may be on a mission or \nsearching for something\nRationale: [1] is holding a flame, \nwhich can suggest a sense of \nmystery or danger, while the \npresence of a cave suggests that he \nmay be searching for something.\nAnswer: she seems \nto be covering her \nnose from something \n.\nRationale: person in \n[3] doesn ' t look as \ndire as person in [0] \nso it is probably just \na smell that only she \nis smelling .\nAnswer: [0] might be covering \nher face to protect the baby \nfrom the cold weather.\nRationale: [0] is seen wearing \na hat and coat which indicates \ncold weather, and she is \nholding a baby which could \nindicate that she is trying to \nprotect the baby ..\nAnswer: The woman on the left is \ncovering her face, likely because \nshe is either shy, trying to hide her \nemotions, or reacting to something \nthat happened ..\nRationale: Since she is with a \ngroup of people, it is possible that \nthey are engaged in a conversation \nor an activity that involves sharing \npersonal thoughts or experiences.\nAnswer: [0] is hiding her face from \nthe smoke and dirt in the \nbackground, which suggests a \nserious situation.\nRationale: [0] is covering their \nface, which implies that the \nsituation in the background is \nunhealthy or dangerous.\nWhich person is most likely to make a \ncelebratory toast ?\nWhat is [0] ' s function ?\nWhat is [1] doing ?\nWhy is [0] covering her face ?\n[6]\n[2]\n[0]\n[1]\n[0]\n[3]\n(1)\n(2)\n(3)\n(4)\nFigure 5: Qualitative examples comparing ChatGPT (the teacher model), LLAVA trained for complex\nvisual reasoning [34], and ours. Each person referenced in the question has been assigned a unique\nnumber with a bounding box and their mention has been marked with a corresponding color. Any\nerrors within the generated results are emphasized with a red highlight.\nRegion Understanding with Multimodal alignment\nCapturing elements across modalities that\nhave a common meaning and is exemplified by tasks such as visual coreference resolution [25, 42],\nvisual referring expression recognition [8], multimodal question answering [19, 66], and cross-modal\nretrieval [13, 43]. Alignment between modalities is challenging since it may depend on long-range\ndependencies, involves ambiguous segmentation (e.g., words or utterances), and could be either\none-to-one, many-to-many, or not exist at all. Resources for fine-grained alignment include Visual\nGenome [26] and dense captioning [21], diverse reasoning [63]. Recent methods have adopted\neither generative or retrieval-based methods for alignment: generative methods create localized\nverbalizations of the region of interest [72, 14, 21, 70], while retrieval aims to select the most\naccurate caption for the region of interest despite possibly given only coarse-grained paired data of\ncaptions for entire images [5, 18].\n5\nConclusion\nWe present LSKD, a method for sampling localized commonsense knowledge from a large language\nmodel. With the help of a supervised critic model aligned with human judgments, we create a diverse,\nreliable 1M localized commonsense corpus. Training on the resulting corpus supports models that\ncan accept region references as input, which allows users to interact with specific parts of images by\n\u201cpointing;\" all without having to write out a referring expression explicitly. We show that training\non our corpus improves the zero-shot performance of vision-language models for tasks requiring\nregions-as-input. Making the critic model more critical by strict thresholding improved performance\nfurther. We present a state-of-the-art zero-short performance with our approach opening avenues for\nvisual commonsense models with our localized commonsense knowledge corpus.\n10\nAcknowledgements\nWe thank members of the Mosaic team at AI2 and Microsoft Research Deep Learning team for\nvaluable discussions. This research was supported by the NSF (DMS-2134012, IIS-1652052, and\nIIS-1703166], DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen\nInstitute for AI.\nReferences\n[1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. A\nreview on language models as knowledge bases. CoRR, abs/2204.06031, 2022.\n[2] Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev\nShlomov, Naama Tepper, and Naama Zwerdling. Do not have enough data? deep learning to\nthe rescue! In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7383\u20137390. AAAI Press, 2020.\n[3] Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke\nSakaguchi, Swabha Swayamdipta, Peter West, and Yejin Choi. I2d2: Inductive knowledge\ndistillation with neurologic and self-imitation. ArXiv, abs/2212.09246, 2022.\n[4] Richard A Bolt. \u201cput-that-there\" voice and gesture at the graphics interface. In Proceedings of\nthe 7th annual conference on Computer graphics and interactive techniques, pages 262\u2013270,\n1980.\n[5] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical\ngraph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10638\u201310647, 2020.\n[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022.\n[8] Volkan Cirik, Taylor Berg-Kirkpatrick, and Louis-Philippe Morency. Using syntax to ground\nreferring expressions in natural images. In AAAI, volume 32, 2018.\n[9] C\u00e9dric Colas, Tristan Karch, Cl\u00e9ment Moulin-Frier, and Pierre-Yves Oudeyer. Vygotskian\nautotelic artificial intelligence: Language and culture internalization for human-like AI. CoRR,\nabs/2206.01134, 2022.\n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n[11] Robert Dale and Ehud Reiter. Computational interpretations of the gricean maxims in the\ngeneration of referring expressions. Cognitive science, 19(2):233\u2013263, 1995.\n[12] Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun\nHuang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation\nlearning at scale. ArXiv, abs/2211.07636, 2022.\n[13] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato,\nand Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Advances in neural\ninformation processing systems, pages 2121\u20132129, 2013.\n11\n[14] Kun Fu, Junqi Jin, Runpeng Cui, Fei Sha, and Changshui Zhang. Aligning where to see and\nwhat to tell: Image captioning with region-based attention and scene-specific contexts. IEEE\ntransactions on pattern analysis and machine intelligence, 39(12):2321\u20132334, 2016.\n[15] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and Jianfeng\nGao. KAT: A knowledge augmented transformer for vision-and-language. In Marine Carpuat,\nMarie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edz, editors, Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022,\npages 956\u2013968. Association for Computational Linguistics, 2022.\n[16] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5356\u20135364, 2019.\n[17] Luheng He, Mike Lewis, and Luke Zettlemoyer. Question-answer driven semantic role labeling:\nUsing natural language to annotate natural language. In Proceedings of the 2015 conference on\nempirical methods in natural language processing, pages 643\u2013653, 2015.\n[18] Xiangteng He, Yuxin Peng, and Liu Xie. A new benchmark and approach for fine-grained\ncross-media retrieval. In Proceedings of the 27th ACM international conference on multimedia,\npages 1740\u20131748, 2019.\n[19] Jack Hessel, Jena D Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna\nRohrbach, Kate Saenko, and Yejin Choi. The abduction of sherlock holmes: A dataset for visual\nabductive reasoning. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XXXVI, pages 558\u2013575. Springer, 2022.\n[20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration, 2020.\n[21] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization\nnetworks for dense captioning. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4565\u20134574, 2016.\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring\nto objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), pages 787\u2013798, 2014.\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[24] Ayal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin, Avi Caciularu, and Ido Dagan. QASem\nParsing: Text-to-text Modeling of QA-based Semantics. arXiv preprint arXiv:2205.11413,\n2022.\n[25] Satwik Kottur, Jos\u00e9 MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. Visual\ncoreference resolution in visual dialog using neural module networks. In ECCV, pages 153\u2013169,\n2018.\n[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International journal of\ncomputer vision, 123:32\u201373, 2017.\n[27] Alina Kuznetsova, Hassan Rom, Neil Gordon Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi\nPont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,\nand Vittorio Ferrari. The open images dataset v4. International Journal of Computer Vision,\n128:1956\u20131981, 2018.\n[28] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video\nquestion answering. In EMNLP, pages 1369\u20131379, 2018.\n12\n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image\npre-training for unified vision-language understanding and generation, 2022.\n[31] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu,\nMichael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event\nstructures. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npages 16399\u201316408, 2022.\n[32] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer\nbackbones for object detection. In Computer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part IX, pages 280\u2013296. Springer, 2022.\n[33] Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. WANLI: worker and AI\ncollaboration for natural language inference dataset creation. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 6826\u20136847.\nAssociation for Computational Linguistics, 2022.\n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[35] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. In Advances in Neural Information\nProcessing Systems, pages 13\u201323, 2019.\n[36] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. A dataset\nand exploration of models for understanding video data through fill-in-the-blank question-\nanswering. In CVPR, pages 6884\u20136893, 2017.\n[37] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin\nMurphy. Generation and comprehension of unambiguous object descriptions. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.\n[38] Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke Zettlemoyer. Crowdsourc-\ning question-answer meaning representations. arXiv preprint arXiv:1711.05885, 2017.\n[39] Tuan-Phong Nguyen, Simon Razniewski, Julien Romero, and Gerhard Weikum. Refined\ncommonsense knowledge from large-scale web contents. CoRR, abs/2112.04596, 2021.\n[40] Sharon Oviatt. Multimodal interfaces. The human-computer interaction handbook, pages\n439\u2013458, 2007.\n[41] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. Visu-\nalcomet: Reasoning about the dynamic context of a still image. In European Conference on\nComputer Vision, pages 508\u2013524. Springer, 2020.\n[42] Jae Sung Park, Trevor Darrell, and Anna Rohrbach. Identity-aware multi-sentence video\ndescription. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23\u201328, 2020, Proceedings, Part XXI 16, pages 360\u2013378. Springer, 2020.\n[43] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In ICCV, 2015.\n[44] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari.\nConnecting vision and language with localized narratives. In ECCV, 2020.\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In Interna-\ntional Conference on Machine Learning, 2021.\n13\n[46] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 11 2019.\n[47] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh\nMottaghi. A-okvqa: A benchmark for visual question answering using world knowledge, 2022.\n[48] J Siroux, M Guyomard, F Multon, and C Remondeau. Modeling and processing of the oral\nand tactile activities in the georal tactile system. In International Conference on Cooperative\nMultimodal Communication CMC, volume 95, pages 287\u2013298, 1995.\n[49] Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, and Lejian Liao. Kvl-bert: Knowledge\nenhanced visual-and-linguistic bert for visual commonsense reasoning. Knowledge-Based\nSystems, 230:107408, 2021.\n[50] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA:\nA question answering challenge targeting commonsense knowledge.\narXiv preprint\narXiv:1811.00937, 2018.\n[51] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and\nSanja Fidler. Movieqa: Understanding stories in movies through question-answering. In CVPR,\npages 4631\u20134640, 2016.\n[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u2019elien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. ArXiv, abs/2302.13971, 2023.\n[53] Luis Von Ahn, Ruoran Liu, and Manuel Blum. Peekaboom: a game for locating objects in\nimages. In Proceedings of the SIGCHI conference on Human Factors in computing systems,\npages 55\u201364, 2006.\n[54] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through\na simple sequence-to-sequence learning framework. In International Conference on Machine\nLearning, pages 23318\u201323340. PMLR, 2022.\n[55] Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and Daxin Jiang.\nPromda: Prompt-based data augmentation for low-resource NLU tasks. In Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 4242\u20134255. Association for Computational Linguistics, 2022.\n[56] Zhecan Wang, Noel C. F. Codella, Yen-Chun Chen, Luowei Zhou, Xiyang Dai, Bin Xiao, Jian-\nwei Yang, Haoxuan You, Kai-Wei Chang, Shih-Fu Chang, and Lu Yuan. Multimodal adaptive\ndistillation for leveraging unimodal encoders for vision-language tasks. ArXiv, abs/2204.10496,\n2022.\n[57] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv,\nabs/2109.01652, 2021.\n[58] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras,\nXiming Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general\nlanguage models to commonsense models. arXiv preprint arXiv:2110.07178, 2021.\n[59] Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang, Jinlong Hou, J. Liu, and T. Zhang.\nTencent ml-images: A large-scale multi-label image database for visual representation learning.\nIEEE Access, 7:172683\u2013172693, 2019.\n[60] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan\nWang. GRiT: a generative region-to-text transformer for object understanding. arXiv preprint\narXiv:2212.00280, 2022.\n14\n[61] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for\nfine-grained image understanding, 2019.\n[62] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong\nSun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint\narXiv:2109.11797, 2021.\n[63] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang.\nBroaden the\nvision: Geo-diverse visual commonsense reasoning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pages 2115\u20132129, 2021.\n[64] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling\ncontext in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385.\nSpringer, 2016.\n[65] Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency.\nSocial-iq: A question answering benchmark for artificial social intelligence. In CVPR, pages\n8807\u20138817, 2019.\n[66] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\nVisual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6720\u20136731, 2019.\n[67] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi,\nAditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script\nknowledge through vision and language and sound. 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 16354\u201316366, 2022.\n[68] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi,\nand Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural\nInformation Processing Systems, 34:23634\u201323651, 2021.\n[69] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek\nPurohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al.\nSo-\ncratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint\narXiv:2204.00598, 2022.\n[70] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image\npretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16793\u201316803, 2022.\n[71] Bolei Zhou, Aditya Khosla, \u00c0gata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An\nimage database for deep scene understanding. ArXiv, abs/1610.02055, 2016.\n[72] Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, and Hanwang Zhang. More grounded\nimage captioning by distilling image-text matching model. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 4777\u20134786, 2020.\n[73] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed\nElhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual\ndescriptions. ArXiv, abs/2303.06594, 2023.\n[74] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n[75] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question\nanswering in images, 2016.\n15\nSupplementary Material\nA\nThe Localized Commonsense Knowledge Corpus\nTable 6 shows the detailed statistics of the corpus. We break down the corpus where the regions are\nreferenced by their IDs and by their region descriptions. The maximum number of mentioned region\nIDs in the QAR is limited to 5. Figure 6 illustrates the distribution of the number of IDs.\nWe show the category of question types and examples in Table 7. Upon manual inspection of\nthe corpus, we have identified specific question types that exhibit prominent characteristics. These\ntypes are associated with a collection of n-gram patterns, and questions sharing these n-grams\nare categorized accordingly (e.g., questions containing the terms \"purpose\" and \"significance\" are\nassigned to the Purpose category). Lastly, the word clouds for question, answer, and rationale are\nshown in Figure 7.\nWith Region ID\u2019s\nWith Region Descriptions\nTotal Corpus\n# of Images\n128,564\n125,524\n168,996\n# of QARs\n513,223\n467,658\n1,023,807\nAverage # of Qs per Image\n3.99\n3.73\n3.86\nAverage Q Length\n13.0\n10.9\n11.8\nAverage A Length\n14.4\n10.5\n12.3\nAverage R Length\n25.8\n22.8\n24.1\nAverage # of mentioned ID\u2019s\n0\n1.25\n0.57\nTable 6: Detailed statistics of the Localized Commonsense Knowledge Corpus.\nDistribution of \nMentioned IDs\n1%\n10%\n37%\n52%\n0\n1\n2\n>= 3\n# of IDs\nFigure 6: Distribution of\nmentioned Region ID\u2019s.\nQuestion Type\nFreq (%)\nExample\nPurpose\n20.0\nWhat is the purpose, What is the significance...\nRelationship\n10.5\nWhat is the relationship, How are they related...\nType\n10.1\nWhat kind of, What is the type of...\nEmotion\n8.4\nWhat emotion, What might be the feeling of...\nScene\n7.7\nWhere, What time, What situation...\nAttribute\n7.4\nWhat state, What condition, What color...\nAction\n5.9\nWhat activity, What event, What are they doing...\nInference\n5.3\nWhat can you infer, What would likely, How might...\nReason\n5.1\nWhy, What is the intention...\nRole\n4.7\nWhat is the role, What is the occupation...\nFocus\n4.5\nWhat is the main focus, What stands out...\nAmbiance\n4.4\nWhat atmosphere, What is the mood, What vibe...\nFactual\n3.5\nIs/Are there..., Do you think...\nOthers\n2.6\n-\nTable 7: Types of questions and their examples in the corpus. To\nidentify these question types, we manually construct a set of n-gram\npatterns and categorize questions based on their inclusion of these\nspecific n-grams.\n(a) Question\n(b) Answer\n(c) Rationale\nFigure 7: Word Clouds of Question, Answer, and Rationale\n16\nFigure 8: Distribution of bounding box sizes in the generated corpus. x-axis is the normalized box\nwidth, height, and area from 0 to 1. y-axis is the normalized counts over total number of samples.\nB\nBounding Box Distributions and Model Performance\nFigure 8 shows the distribution of normalized bounding box sizes in the filtered corpus, highlighting\nthe width, height, and the area. We notice that almost 50% of the bounding boxes have the normalized\narea 0.05, suggesting that small objects are well-covered in our corpus. The height shows more\nuniform distribution than the width, indicating that there are more bounding boxes with smaller\nwidths and the width mainly clusters in the range of 0.1-0.2. This reveals that the corpus contains not\njust large and prominent objects, but also small or narrow objects that often require attentive vision\nmodels to recognize.\nWe use the Sherlock comparison task [19] to study the model performance change w.r.t different\nbounding boxes as their dataset consists of single bounding boxes with diverse sizes. The Pearson\u2019s\ncorrelation between the input bounding box size and the comparison accuracy is \u03c1 \u201c \u00b40.12 with\np-value of 0.05.\nBased on the correlation, we see that the performance is actually higher for smaller objects. One\nmight indeed initially think that larger bounding boxes would result in better performance, as they\ncould potentially encompass more features or more of the object of interest. We hypothesize that the\nnegative correlation is due to the following.\n\u2022 Specificity: Smaller bounding boxes quite often are more specific in identifying the target\nobjects, thereby reducing the complexity of the region and making it easier for the model to\nfocus and reason.\n\u2022 Clutterness:\nLarger bounding boxes might include more \"noise\" or irrelevant ob-\njects/background, which could mislead the model during the reasoning process as it gets\ndistracted by extraneous details.\nC\nMore Details of Corpus Generation\nWe show the full output of our image to text verbalization pipeline using the global, region, and\nquestion-answer descriptors in Figure 10. For concepts, we acquire the visual feature v and text\nfeatures for each object classes rt1, t2, ...tCs extracted by the CLIP-ViT-L-336 model [45], and use\nthe nearest neighbor search by their cosine distance to select the top k labels for the image. We train\nOFA-Huge model [54] on the Localized Narratives [44] and generate 5 descriptions with nucleus\nsampling [20] of p \u201c 0.95. BLIP-2 trained on region captions described in Section 2.1 is used to\ndescribe the regions individually. We get the questions using ChatGPT, in which we provide the\nglobal and local descriptors as context, and call the OpenAI API with the following instruction: Here\nis the context for the image:\n{global descriptors} znzn {local descriptors}\nznzn Now, ask fifteen interesting but simple questions that you want to ask\nso you can get more understanding about the image. The zero-shot BLIP-2 answers the\ngenerated question, and the QA pairs are used as the dynamic descriptors.\nTo generate the Localized Commonsense Knowledge Corpus, we utilize verbalization as context and\npresent two distinct prompts to ChatGPT. In one prompt, regions are referenced by numerical IDs,\nwhile in the other prompt, regions are described using text descriptions. The specific prompts used to\ninvoke ChatGPT are depicted in Figure 11 and Figure 12. In the former case, instances where no IDs\nare mentioned in the output are filtered out, while in the latter case, instances containing any IDs in\n17\nthe output are excluded. An example generated using both versions of the prompts is showcased in\nFigure 13.\n0.88\n0.86\nCritic Score\nChatGPT Generations \n0.22\n0.69\n0.59\nQ: What is the signi\ufb01cance of the gold hook on [4]?\nA: The gold hook on [4] is likely used to hang the stuffed animal.\nR: The location and appearance of the hook on [4] suggests that it was designed to be hung  \n     on a wall or another object.\nQ: What can we infer from the tag on [5]?\nA: The tag on [5] indicates the brand of the stuffed animal.\nR: The presence of a tag usually indicates the brand or manufacturer \n     of a product, so it\u2019s likely that the tag on [5] is indicating the same.\nQ: What can you infer about [1] from its position in the image?\nA: [1] is prominently positioned in the foreground of the image, indicating that it is the  \n     main focus of the scene.\nR: [1] is one of the largest and most prominent regions in the image, and its placement in  \n     the foreground suggests that it is an important part of the scene and likely holds some  \n     signi\ufb01cance.\nQ: What can you say about the character of the teddy bear holding another toy in [0] ?\nA: The fact that the teddy bear in [0] is holding another toy suggests that it has a playful  \n     and curious personality.\nR: The act of holding another toy is a playful and curious behavior often associated with  \n     children, indicating that the teddy bear in [0] may possess similar traits.\nQ: Can we assume anything about the character of the brown teddy bear [1] from its  \n     expression?\nA: The scowling expression on the brown teddy bear [1] suggests that it might be unhappy  \n     with the situation.\nR: A scowling expression is often associated with unhappiness so it\u2019s reasonable to assume  \n     that the brown teddy bear [1] might not be enjoying its current situation.\n0\n4\n5\n1\nFigure 9: Qualitative examples of supervised critic filtering of ChatGPT generated data. We discard\ngenerations whose critic scores are lower than the threshold value of 0.8. Incorrect visual details are\nhighlighted as red.\nD\nQualitative Analysis of Critic Model\nFigure 9 shows qualitative examples to understand the patterns of critic model in distinguishing good\nand bad examples. We see that the model mostly relies on incorrect visual details (highlighted as red)\nlower than the correct instances. The third instance does not have glaring visual errors but are scored\nlower due to statement of \"largest and most prominent regions\", which is ambiguous but close to\nfalse. The critic model displays good calibrations in ordering the instances, such as giving the lowest\nscore to the instance with the most visual error.\nE\nMore details of Local Descriptors\nWe train a region captioning model that maps from (image, region) \u00d1 description of the region. We\nfine-tuned the generative version of BLIP-2 [29] with the FLAN-t5-xxl [7] backbone. We trained\non a combination of RefCOCO/RefCOCO+/RefCOCOg [64, 37] (120K/80K/120K training region\ncaptions), Sherlock Clues-only [19] (277K), and VisualGenome [26] (1.96M): all of these datasets\nprovide descriptions of given regions within images. Following [68, 62], we render the bounding box\nin the image itself to allow the model access to the bounding box\u2019s location.\nWe compared our model\u2019s captions to those generated by GRiT [60], which achieves state-of-the-art\nperformance on dense captioning [21]. The standard evaluation metric for dense captioning combines\nregion proposal and caption generation metrics. Because we aim to generate captions for any given\nregion provided by a user (and not just model-proposed ones), we instead evaluate generation capacity\ngiven a region. Specifically, we conduct a pairwise human evaluation comparing the generations\nof GRiT on its proposed bounding boxes vs. our model\u2019s generations on the same GRiT-proposed\nbounding boxes. 5 authors of this work evaluated 150 randomly-sampled captioned regions from test\nset examples in a head-to-head setting. Annotators could select \u201cA\", \u201cB\", or \u201cTie\": GRiT and our\nregion captioner were randomly assigned A or B placement in a blinded fashion. Overall: while both\nperformed well, our region captioner was preferred to GRiT on average. In 46% (347/750) of cases,\nannotators reported a tie, in 34% (248/750) of cases, annotators reported ours was better, and in 19%\n(145/750) of cases, GRiT was rated as better.\nGiven the (image, region) \u00d1 description model, we next sample candidate regions of interest; in \u00a7 2.2,\nwe condition on these regions for the generation of commonsense knowledge. We use the ViT-H\nCascade Mask R-CNN [32] trained on LVIS [16] for an initial proposal set. The detector outputs up\nto 300 candidate objects per image, many of which overlap or cover background objects that are not\n18\nthe focus of the scene. For each image\u2019s list of candidate objects, we heuristically downsample to a\nset of \u201cmost interesting\" regions by: 1) selecting the at-most k \u201c 4 largest/most central people; 2)\nkeeping the most central/large objects; 3) over-sampling rarer objects according to prior frequency of\ndetection in the LVIS vocabulary; 4) limiting the number of objects of a single type per-image; and 5)\ndownsampling overlapping region proposals to encourage broader coverage of the pixel area of the\nimage.\nF\nHuman Annotation Details\nAll human evaluations are performed using the Amazon Mechanical Turk (MTurk) platform. 218\nworkers from English native speaking countries, at least 5,000 HITs, and acceptance rate \u011b 50, are\nselected based on their passing performance on a paid qualification HIT. The workers are paid with\nan average rate of $15/hour. An IRB exemption was obtained for the institution\u2019s internal institutional\nreview and ethics board, and we did not collect any denanonymizing information nor do we publish\nwith our dataset sensitive information such as MTurk IDs.\nWe collect acceptability labels for critic training using the template in Figure 14. For each image\nand its set of annotated question, answer, rationales (QARs), we run deduplication by clustering\nthe QAR\u2019s using hierarchical clustering5 with their semantic similarity measured by the SentBert\nparaphrase-MiniLM-L6-v2 model [46]. We select five question, answer, and rationale triples by\ngetting the roots of the fiver clusters and considering them as the annotation candidates for each\nimage. Using 4,800 images and 24K QAR\u2019s, we run the annotation pipeline following Section 2.3\nand acquire the acceptability labels for the critic.\nFigure 15 shows the template to conduct the pairwise human evaluation comparing ours to chatgpt\nresponses with VCR questions and images [66]. To reduce the label selection bias, we randomize the\norder of two responses. 300 (image, QAR) pairs are selected for evaluation where there is no overlap\namong the images. Three annotators are asked to make a selection, and instances that did not receive\nat least two votes are not considered in each evaluation criteria, which we found to be 6% on average.\nG\nAdditional Qualitative Examples\nIn Figure 16, we present qualitative results of BLIP-2 FlanT5XXL and Mini-GP4 models trained with\nLSKD, for answering VCR questions [66]. The results demonstrate that both models are capable of\naccurately identifying the relevant person performing the action. For instance, in the first example,\nthe models correctly identify [1] as a dentist due to the presence of a lab coat. Similarly, in the second\nexample, they recognize that [0] is the individual talking on the phone. Notably, the Mini-GPT4\nmodel, which employs the more powerful language model Vicuna [6], produces more precise answers.\nFor instance, it mentions specific actions like tooth cleaning and identifies [0] as seated in the dentist\u2019s\nchair. Additionally, it hypothesizes that [0] might be engaged in conversation with other workers or\nsuperiors based on the observation of holding a phone. This observation suggests that LSKD benefits\nfrom employing a language model with enhanced capabilities as indicated by the human evaluation\nresults in the main paper.\nWe also show failure cases in Figure 17. We observe that the models are capable of correctly\nidentifying the individuals, such as recognizing [1] as the person wearing a black hoodie and [0]\nas the individual with closed eyes standing in the doorway. However, they 1) demonstrate a lack\nof spatial reasoning. For instance, the T5 model hallucinates that the boy is \"standing on a shelf of\ncanned food,\" while Mini-GPT4 hypothesizes that he would \"not damage the objects\" if he were to\nfall over, despite the close proximity of the objects in the first example. Additionally, in the second\nexample, the models exhibit a 2) deficiency in fine-grained understanding of people\u2019s expressions.\nAlthough [0] displays a disgusted facial expression, the T5 model incorrectly interprets it as curiosity\nand interest, while Mini-GPT4 predicts that she is feeling nervous. These observations indicate that\nwhile the models are able to correctly identify the relevant regions, they still lack the capability for\nnuanced and intricate understanding that necessitates more sophisticated reasoning of visual content.\n5We use the scipy library https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.\n19\nH\nError Bars\nWe report error bars for the BLIP-2 [29] trained with LSKD in Table 2 of the main paper. We\nrun three experiments with different random seeds and show the results in Table 8. Note all other\nmethods are evaluated with off-the-shelf zero-shot models, hence we only report error bars just for\nour method.\nVCR\nSherlock\nVisualCOMET\nQ \u00d1 A\nQA \u00d1 R\nQ \u00d1 AR\nComparison\nAcc@50\nBLIP-2 ViT-G + LSKD\n58.8 \u02d8 0.12\n56.3 \u02d8 0.07\n33.2 \u02d80.09\n30.1 \u02d8 0.09\n40.0 \u02d8 0.11\nTable 8: Error bars of LSKD on zero-shot localized visual reasoning tasks (last row of Table 2).\nI\nLimitations\nOne limitation is the recognition bottleneck of the verbalizers, in which off-the -shelf vision language\nmodels may encounter errors in object detection or action recognition. With a combination of\nverbalization output, the LLM largely ignores irrelevant and incoherent content in the image, but is\nstill prone to generating erroneous data. We made efforts to mitigate the issue by training a supervised\ncritic model on a subset of data to filter out erroneous cases. However, it should be noted that the\ncritic model cannot guarantee the exclusion of all irrelevant instances. Despite these limitations,\nthe trained LSKD models exhibit notable improvements and demonstrate impressive generation\ncapabilities when applied to localized visual reasoning tasks.\nAnother limitation is the coverage of questions in the data. As shown in Table 7, the dataset\nencompasses various question types; however, there may still exist certain question categories that\nare underrepresented or not adequately captured (e.g. object counts, potential challenges, other\ninferences). This limitation could potentially affect the generalizability of the models trained on the\ndataset to specific question types that are underrepresented or absent.\n20\n[BLIP-2 Region Captions]\n[0] man in black shorts [(0.56, 0.3), (0.8, 0.84)]\n[1] little girl in striped bathing suit [(0.22, 0.61), (0.49, 1.0)]\n[2] a woman wearing a pink shirt [(0.3, 0.3), (0.44, 0.62)]\n[3] sunglasses on a woman's face [(0.32, 0.33), (0.4, 0.35)]\n[4] a white surfboard on the sand [(0.0, 0.73), (0.1, 0.96)]\n[5] a surfboard [(0.44, 0.03), (0.69, 0.75)]\n[6] ropes on the surfboard [(0.47, 0.13), (0.63, 0.19)]\n[7] a small wave in the water [(0.76, 0.41), (0.78, 0.42)]\n[8] a sticker on a surfboard [(0.43, 0.61), (0.5, 0.65)]\n[9] the man has a beard [(0.61, 0.38), (0.65, 0.42)]\n[CLIP-Retrieved Concepts]\nI think this image takes place at raft, or beach.\nSome objects I see are surfboard, paddle, paddle, board, and board.\nThere also might be a mini marcos, board short, surfboard, stand up paddle sur\ufb01ng, and \nsurfboard shaper in this image.\n[OFA-H Localized Narratives]\nDescription 1: man and woman standing. The man is holding a sur\ufb01ng board. In front of him a \ngirl is there. At the background there is water and a sky. To the left side there is a surf board.\nDescription 2: on the right we can see a man and a woman holding a sur\ufb01ng board and smiling. \nWe can see a kid walking and a surfboard at the bottom. In the background we can see sky, \nclouds and water.\nDescription 3: a woman and a man wearing shorts standing on the sand and holding a surfboard \nin their hands. There is a kid walking on the sand. In the background there is a sea. In the sky \nthere are some clouds.\nDescription 4: a girl is running in the sand towards the man, who is holding a surfboard. On the \nleft side, there is a surfboard. Behind the man, there is a lady, who is holding a skateboard and \nis smiling. In the background, there is water and there are clouds in the sky.\nDescription 5: a kid is standing on the sand. In front of her a person is standing and holding a \nskateboard. Behind them there is an ocean.\n[ChatGPT Questions + BLIP-2 Answers]\n0. Question: Where does this take place at? Answer: a beach\n1. Question: What is the lighting of the image? Answer: the lighting of the image is natural light\n2. Question: What is the atmosphere or ambiance of the image? Answer: a family on a beach\n3. Question: Does this take place inside or outside? Answer: outside\n4. Question: What might be the weather like? Answer: sunny and warm\n5. Question: What is the little girl wearing in the image? Answer: a bikini\n6. Question: Is the sky in the background clear or cloudy? Answer: cloudy\n7. Question: Can you see any ropes on the surfboard? Answer: no\n8. Question: What color is the woman's shirt in the image? Answer: blue\n9. Question: Is the man wearing shorts or pants? Answer: shorts\n10. Question: What kind of board is on the left side of the image? Answer: a surfboard\n11. Question: Is the scene taking place on a beach or at a raft? Answer: on a beach\n12. Question: Are there any other\u3134 besides the man, woman, and girl in the image? Answer: no\n13. Question: What is the little girl doing in the image? Answer: holding a surfboard\n14. Question: Is the ocean calm or wavy in the background? Answer: wavy\nGlobal Descriptors \nLocal Descriptors \nDynamic Descriptors \nFigure 10: Example of image-to-text verbalization with diverse descriptors.\n21\n[Global Descriptors]\n======\nHere are some speci\ufb01c region IDs with top-left and bottom-right bounding box coordinates \nnormalized from 0 to 1 in [(x1, y1), (x2, y2)]:\n[Local Descriptors]\n======\nQuestions and Answers about the image:\n[Dynamic Descriptors]\n======\n[Instruction]\nGenerate a interesting, succinct, and fun question/answer/rationale triple relating to people or objects in \nthe scene. Select any number of person and objects referred by their ID tags (e.g. [1], [2]).\nThink of what kind of interesting inference statements you can make about the people and objects.\nRequirements:\n- Be con\ufb01dent. Don't say \"It's hard to tell\", or \"I'm just speculating\"\n- Do not ask about watermark or text in the bottom of the image.\n- Do not ask about atmosphere, ambience, or lighting of the image.\n- Do not ask about what person is wearing or the color of hair and out\ufb01t.\n- Do not say the signi\ufb01cance of what the person is wearing and their out\ufb01t.\n- Do not ask question that would lead to unclear answer.\n- Keep all questions/answers/rationales between 1-2 sentences.\n- Only include single attribute or fact in your answer and rationale. Do not say multiple options and say \n\"or \". For example, do not say something like \"He could be a doctor or a pharmacist.\", but just be \ncon\ufb01dent and say \"He is a doctor\u201d.\n- Use the ID tags we provided in the above to refer to people or objects in your question, answer, and \nrationale instead of writing them out.\nTry to ask something interesting or important that the viewer will be interested to know about.\nThese include but not limited to interesting inference, general vibe, attributes, situation, occasion, \nrelationships of related objects.\nRemember, don't mention a \"description\" or an \"image\": pretend you are actually at the scene.\nDo not ask same or similar, simple question-answers already mentioned in the context.\nThe response should make sense when you replace IDs with their region descriptions.\nAlways use one or more IDs in the regions to formulate each of your response.\nNow using the context, descriptions, and description about region IDs, provide three interesting \nresponse about people and/or objects using the region ID tags like this:\nQuestion:\nAnswer:\nRationale:\nFigure 11: Prompt used to generate data while referring regions by numerical IDs.\n22\n[Global Descriptors]\n======\nHere are some speci\ufb01c regions with top-left and bottom-right bounding box coordinates \nnormalized from 0 to 1 in [(x1, y1), (x2, y2)]:\n[Local Descriptors]\n======\nQuestions and Answers about the image:\n[Dynamic Descriptors]\n======\n[Instruction]\nGenerate a interesting, succinct, and fun question/answer/rationale triple about this scene. Remember to \nbe con\ufb01dent, speci\ufb01c, interesting, and make smart connections between objects, actors, etc.\nRequirements:\n- Be con\ufb01dent. Don't say \"It's hard to tell\", or \"I'm just speculating\"\n- Do not ask about watermark or text in the bottom of the image.\n- Do not ask about ambiance or lighting of the image.\n- Do not ask about what person is wearing or the color of hair and out\ufb01t.\n- If you ask what the person is doing, be more descriptive instead of stating simple fact.\n- Don't mention a \"description\" or an \"image\": pretend you are actually at the scene.\n- Do not ask question that would lead to unclear answer.\n- Keep all questions/answers/rationales between 1-2 sentences.\n- Only include single attribute or fact in your answer and rationale. Do not say multiple options and say \n\"or \". For example, do not say something like \"He could be a doctor or a pharmacist.\", but just be \ncon\ufb01dent and say \"He is a doctor\".\nTry to ask something interesting or important that the viewer will be interested to know about.\nThese include but not limited to interesting inference, general vibe, attributes, situation, occasion, \nrelationships of related objects.\nRemember, don't mention a \"description\" or an \"image\": pretend you are actually at the scene.\nDo not ask the same or similar question-answers already mentioned in the description.\nNow using the context descriptions and information about regions, provide three new, interesting \n(question, answer, rationale) response that you can infer from the context like this:\nQuestion:\nAnswer:\nRationale:\nFigure 12: Prompt used to generate data while referring regions by their descriptions.\n23\nQuestion: What is the role of [3] in the game?\nAnswer: [3] is likely the catcher, responsible \nfor catching the ball and making sure the batter \ndoesn't steal any bases.\nRationale: [3] is wearing catcher's gear and is \nin a position on the \ufb01eld consistent with the role \nof a catcher.\nQuestion: What is the person in the white \nshirt doing?\nAnswer: The person in the white shirt might \nbe a coach.\nRationale: The person in the white shirt \nseems to be standing in a position where \nthey might be observing the game and \ninstructing players, indicating that they \ncould be a coach.\nQuestion: What position might the player \nwearing the maroon shirt be playing?\nAnswer: The player wearing the maroon \nshirt may be a batter.\nRationale: The player is holding a bat and \nin the act of swinging, suggesting that they \nmight be a batter.\nQuestion: Is the baseball game in this image \na friendly game or a competitive one?\nAnswer: The baseball game in this image is \nmost likely a competitive one.\nRationale: The presence of a catcher \nwearing a full gear and other players \nwearing helmets and running around \nindicates that this is not just a casual game \nof catch.\nQuestion: Why are there people gathered \nbehind [6]?\nAnswer: They are likely spectators watching \nthe game.\nRationale: The presence of multiple people, \nchairs, and bags behind the chain-link fences \n([6]) suggests that these individuals are \ngathered to watch the baseball game.\nQuestion: What might [0] be feeling?\nAnswer: [0] might be feeling excited or \nnervous.\nRationale: The anticipation of swinging at the \nball and the pressure to perform well can cause \nfeelings of excitement or nervousness for [0] \nduring the game.\nRegion IDs\nRegion Descriptions\nFigure 13: Example of our generated corpus referring regions by IDs or descriptions.\n24\nFigure 14: Template for acceptability annotation to train the critic model.\n25\nFigure 15: Template for pairwise human evaluation.\n26\nQuestion: What is [1] doing?\nGT\nAnswer: [1] is operating on a patient. \nRationale: [0] is in his clinic, with a patient on the seat as he leans over her \nto examine her teeth.\nLSKD  \n[BLIP-2  \nFlanT5 XXL]\nAnswer: [1] is looking at a dental x-ray. \nRationale: The person is wearing a white lab coat and is standing next to \nthe dental chair, indicating that they are a dentist or dental assistant.\nLSKD  \n[Mini-GPT4]\nAnswer: [1] is doing a tooth cleaning. \nRationale: [1] is wearing a white lab coat, which suggests that he is a \ndentist, and [0] is sitting in a dentist 's chair, which further suggests that she \nis receiving a dental treatment.\nQuestion: Who is [0] talking to?\nGT\nAnswer: He's talking to someone on the phone. \nRationale: He has a phone up to his head and his mouth is open as if he's \ntalking.\nLSKD  \n[BLIP-2  \nFlanT5 XXL]\nAnswer: [0] is speaking to a person on the phone. \nRationale: [0] is standing on the ground and appears to be speaking to \nsomeone, indicating that they are engaged in some sort of conversation.\nLSKD  \n[Mini-GPT4]\nAnswer: [0] is talking to a group of people in the distance, possibly fellow \nworkers or superiors. \nRationale: [0] is holding a phone and standing in a group of people, \nsuggesting they may be doing some sort of work-related communication or \nreporting.\nFigure 16: Qualitative Examples generated with different models trained with LSKD.\n27\nQuestion: What would \nhappen if [1] fell backwards ?\nGT\nAnswer: The jars would fall o\ufb00 the shelves . \nRationale: There is nothing keeping the jars on the shelves and if they fall \no\ufb00, they will break.\nLSKD  \n[BLIP-2  \nFlanT5 XXL]\nAnswer: [1] could potentially hurt themselves by falling backwards as they \nare standing on a shelf of canned food. \nRationale: The region [1] shows that person in a black hoodie standing on \na shelf of canned food with other people in the background, suggesting \nthat they could potentially hurt themselves by falling backwards.\nLSKD  \n[Mini-GPT4]\nAnswer: [1] might hit their head, but would likely not damage the objects \non the shelves. \nRationale: [1] is wearing a black hoodie and standing in front of a shelf with \nobjects, so if they were to fall backwards, they would be able to catch \nthemselves before hitting the objects on the shelf.\nQuestion: How is [0] doing?\nGT\nAnswer: [0] is upset and disgusted. \nRationale: [0]\u2019s expression is twisted in disgust.\nLSKD  \n[BLIP-2  \nFlanT5 XXL]\nAnswer: [0] is likely feeling curious and interested, as they are standing in \nthe doorway with their hands raised. \nRationale: The posture and expression of [0] suggests a high level of \ncuriosity or intrigue, which could be related to the fact that the doorway is \nbeing held open for them.\nLSKD  \n[Mini-GPT4]\nAnswer: [0] appears to be feeling nervous, given their closed eyes and \ntense body posture. \nRationale: The fact that [0] is closing their eyes and seems to be trying to \navoid eye contact suggests that they are feeling anxious or nervous.\nFigure 17: Examples with minor errors in spatial reasoning and expression comprehension.\n28\n"
  }
]