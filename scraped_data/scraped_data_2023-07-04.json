[
  {
    "title": "Segment Anything Meets Point Tracking",
    "link": "https://arxiv.org/pdf/2307.01197.pdf",
    "upvote": "34",
    "text": "Segment Anything Meets Point Tracking\nFrano Raji\u02c7c1,3\nLei Ke1,2\nYu-Wing Tai2\nChi-Keung Tang2\nMartin Danelljan1\nFisher Yu1\n1ETH Z\u00a8urich\n2HKUST\n3EPFL\nSAM\nPoint \nTracker\npositive points\nnegative points\noccluded points\nLegend:\nInput: Video and Query Points\nPredicted Trajectories and Occlusion\nOutput: Predicted Masks\nFigure 1. Segment Anything Meets Point Tracking (SAM-PT). SAM-PT is a point-centric method that utilizes sparse point propagation\nfor interactive video segmentation, enabling easier interaction and faster annotation. We extend SAM [21] with long-term point trackers\nto effectively operate on videos in a zero-shot manner. SAM-PT takes user clicks as \u201cquery points\u201d which either denote the target object\n(positive points) or designate non-target segments (negative points). The points are tracked throughout the video using point trackers that\npropagate the query points to all video frames, producing trajectory predictions and occlusion scores. SAM is subsequently prompted with\nthe non-occluded points in the trajectories as to output a segmentation mask for each video frame independently. The propagated points\ncan be further edited for accurate segmentation and tracking.\nAbstract\nThe Segment Anything Model (SAM) has established it-\nself as a powerful zero-shot image segmentation model,\nenabled by efficient point-centric annotation and prompt-\nbased models. While click and brush interactions are both\nwell explored in interactive image segmentation, the exist-\ning methods on videos focus on mask annotation and prop-\nagation.\nThis paper presents SAM-PT, a novel method\nfor point-centric interactive video segmentation, empow-\nered by SAM and long-term point tracking. SAM-PT lever-\nages robust and sparse point selection and propagation\ntechniques for mask generation. Compared to traditional\nobject-centric mask propagation strategies, we uniquely use\npoint propagation to exploit local structure information ag-\nnostic to object semantics. We highlight the merits of point-\nbased tracking through direct evaluation on the zero-shot\nopen-world Unidentified Video Objects (UVO) benchmark.\nOur experiments on popular video object segmentation and\nmulti-object segmentation tracking benchmarks, including\nDAVIS, YouTube-VOS, and BDD100K, suggest that a point-\nbased segmentation tracker yields better zero-shot perfor-\nmance and efficient interactions. We release our code that\nintegrates different point trackers and video segmentation\nbenchmarks at https://github.com/SysCV/sam-\npt.\n1. Introduction\nObject segmentation and tracking in videos are central pil-\nlars for a myriad of applications, including autonomous\ndriving, robotics, and video editing.\nDespite significant\nprogress made in the past few years with deep neural net-\nworks [5, 7, 43, 48], we still need to rely on expensive labels\nfor model supervision to achieve high accuracies. There-\nfore, many efforts have been made on interactive video seg-\nmentation to accelerate the data labeling process and bene-\nfit artistic video editing. Those methods are usually evalu-\nated on a simplified semi-supervised segmentation setup as\na proxy for full interactive video segmentation.\nThe prevailing methods [5, 7] in semi-supervised Video\nObject Segmentation (VOS) and Video Instance Segmenta-\ntion (VIS) exhibit performance gaps when dealing with un-\nseen data, particularly in a zero-shot setting, i.e., when these\nmodels are transferred to video domains they have not been\ntrained or that encompass object categories falling outside\n1\narXiv:2307.01197v2  [cs.CV]  3 Dec 2023\nof the training distribution.\nFurther, generalizable models usually require large\namounts of training data. The existing interactive video seg-\nmentation methods assume the mask of an object is given on\nthe first frame of the testing video. While getting accurate\nmask is laborious, recent works [6, 21] on training foun-\ndation image segmentation models show that point-based\nannotation in combination with mask editing tools is a scal-\nable approach to label exceedingly large amounts of data.\nDespite its success on images in terms of labeling efficiency\nand accuracy, point-centric interactive segmentation has re-\nceived scant attention in the video domains.\nIn this paper, we aim to achieve both domain generaliz-\nability and labeling efficiency for interactive video segmen-\ntation. Our insights are two-fold. First, foundation mod-\nels in image segmentation are available, such as Segment\nAnything Model (SAM) [21]. SAM, trained on 11 million\nimages and 1 billion object masks, has impressive zero-\nshot generalization capabilities. The model also supports\npoint prompts as additional inputs for interactive image seg-\nmentation and produces high-quality masks. Second, we\nwitnessed significant recent progress in point tracking [14\u2013\n16, 19, 39, 53]. Those tracking methods, once trained, can\npropagate points across video frames on diverse domains.\nTherefore, we introduce SAM-PT (Segment Anything\nMeets Point Tracking), depicted in Fig. 1. This is the first\nmethod to utilize sparse point tracking combined with SAM\nfor video segmentation, offering a new perspective on solv-\ning the problem. Instead of employing object-centric dense\nfeature matching or mask propagation, we propose a point-\ncentric approach that capitalizes on tracking points using\nrich local structure information embedded in videos. It only\nrequires sparse points annotation to denote the target ob-\nject in the first frame and provides better generalization to\nunseen objects. This approach also helps preserve the inher-\nent flexibility of SAM while extending its capabilities effec-\ntively to video segmentation. Similar to the data annotation\nprocess in SAM, our point-centric approach can be poten-\ntially integrated with the existing mask-based approaches in\nreal-world applications.\nSAM-PT prompts SAM with sparse point trajectories\npredicted using state-of-the-art point trackers, such as Co-\nTracker [19], harnessing their versatility for video segmen-\ntation. We identified that initializing points to track using\nK-Medoids cluster centers from a mask label was the strat-\negy most compatible with prompting SAM. Tracking both\npositive and negative points enables the clear delineation of\ntarget objects from their background. To further refine the\noutput masks, we propose multiple mask decoding passes\nthat integrate both types of points. In addition, we devised\na point reinitialization strategy that increases tracking ac-\ncuracy over time. This approach involves discarding points\nthat have become unreliable or occluded, and adding points\nfrom object parts or segments that become visible in later\nframes, such as when the object rotates.\nWe evaluate SAM-PT on multiple setups including semi-\nsupervised, open-world, and fully interactive video segmen-\ntation. Our method achieves stronger performance than ex-\nisting zero-shot methods by up to 5.0% on DAVIS, 2.0%\non YouTube-VOS, and 7.3% on BDD100K, while also sur-\npassing a fully-supervised VIS method [46] on UVO by 6.7\npoints.\nWe also set up a new benchmark for interactive\npoint-based video segmentation to simulate the process of\nmanually labeling the whole video. In this setup, SAM-\nPT significantly reduces annotation effort, approaching the\nperformance of fully supervised approaches and underscor-\ning its practicality. This comes without the need for any\nvideo segmentation data during training, underscoring the\nrobustness and adaptability of our approach, and indicat-\ning its potential to enhance progress in video segmentation\ntasks, particularly in zero-shot scenarios.\n2. Related Work\nPoint Tracking for Video Segmentation.\nClassical fea-\nture extraction and tracking methods such as Lucas-\nKanade [26],\nTomasi-Kanade [36],\nShi-Tomasi [34],\nSIFT [25], and SURF [1], as well as newer methods\nsuch as LIFT [49], SuperPoint [12], and SuperGlue [33],\nhave all demonstrated proficiency in identifying or track-\ning sparse features and establishing long-range correspon-\ndences. Nonetheless, these techniques often falter in dy-\nnamic, non-rigid environments.\nWhile flow-based ap-\nproaches such as RAFT [35] offer improvements, they\ntoo struggle with maintaining long-term point accuracy\ndue to error accumulation and occlusions.\nAddressing\nthese shortcomings, recent innovations such as PIPS [16],\nPIPS++ [53], OmniMotion [39], TAPIR [15], and the state-\nof-the-art CoTracker [19], optimize for robust long-term\ntrajectories and effectively manage occlusions. Our work is\nunique in applying these methods to guide image segmen-\ntation models for video segmentation tasks.\nSegment and Track Anything Models.\nSAM [21] is\na foundation model for image segmentation that show-\ncases impressive zero-shot capabilities. Its extension, HQ-\nSAM [20], improves mask quality for complex objects but\nis not designed for video tasks.\nTAM [46] and SAM-\nTrack [11] attempt to extend SAM to video segmentation by\nintegrating the state-of-the-art fully-supervised XMem [7]\nand DeAOT [47] mask trackers, respectively, yet they lack\nin zero-shot scenarios.\nZero-Shot VOS / VIS.\nGeneralist models such as\nPainter [41] apply visual prompting to various tasks but\ndemonstrate limited performance in video segmentation.\nOn the other hand, SegGPT [42] also uses visual prompt-\ning and competes closely with our method on some datasets.\nOther approaches, such as STC [18] and DINO [4], perform\nVOS through feature matching. Our approach distinguishes\nitself by taking the point-centric approach to enhance per-\nformance on VOS benchmarks in a zero-shot setting.\nInteractive VOS.\nInteractive VOS has shifted from labor-\n2\nintensive manual annotations to more user-friendly interac-\ntion methods, such as scribbles, clicks, and drawings, en-\nabling rapid and intuitive video editing [3, 17, 27, 28, 37].\nAmong these, MiVOS [8] stands out for its modular design\nthat decouples mask generation from propagation, effec-\ntively incorporating user interactions from diverse interac-\ntion modalities. Unlike MiVOS and other fully-supervised\nmethods, SAM-PT is the first to use point propagation in-\nstead of mask propagation and thus operates effectively in\nzero-shot settings. Our interactive point-based video seg-\nmentation study emphasizes the simplicity and efficacy of\npoint interactions and differs from common scribble-based\nbenchmarking [2] or in-distribution user studies [8].\n3. Method\nWe propose SAM-PT for addressing video segmentation\ntasks in a zero-shot setting.\nSAM-PT combines the\nstrengths of the Segment Anything Model (SAM), a foun-\ndation model for image segmentation, and prominent point\ntrackers, such as PIPS [16] and CoTracker [19], to enable\ninteractive tracking of anything in videos. Sec. 3.1 briefly\ndescribes the background knowledge about SAM. Sec. 3.2\nthen introduces our SAM-PT method with its four con-\nstituent steps. Finally, Sec. 3.3 analyzes and highlights the\nmethod\u2019s novelty as the first point-centric interactive video\nsegmentation method compared to existing works.\n3.1. Preliminaries: SAM\nWhereas in computer vision \u201czero-shot (learning)\u201d usu-\nally refers to the study of generalization to unseen ob-\nject categories in image classification [22], we follow prior\nwork [21, 31] and rather employ the term in a broader sense\nand explore generalization to unseen datasets.\nThe Segment Anything Model (SAM) [21] is a novel\nvision foundation model designed for promptable image\nsegmentation. SAM is trained on the large-scale SA-1B\ndataset, which contains 11 million images and over 1 bil-\nlion masks. SA-1B has 400 times more masks than any\nprior segmentation dataset. This extensive training set facil-\nitates SAM\u2019s impressive zero-shot generalization capabili-\nties. SAM has showcased its ability to produce high-quality\nmasks from a single foreground point and has demonstrated\nrobust generalization capacity on a variety of downstream\ntasks under a zero-shot transfer protocol using prompt en-\ngineering. These tasks include, but are not limited to, edge\ndetection, object proposal generation, and instance segmen-\ntation.\nSAM comprises three main components: an image en-\ncoder, a flexible prompt encoder, and a fast mask decoder.\nThe image encoder is a Vision Transformer (ViT) backbone\nand processes high-resolution 1024 \u00d7 1024 images to gen-\nerate an image embedding of 64 \u00d7 64 spatial size. The\nprompt encoder takes sparse prompts as input, including\npoints, boxes, and text, or dense prompts such as masks,\nand translates these prompts into c-dimensional tokens. The\nlightweight mask decoder then integrates the image and\nprompt embeddings to predict segmentation masks in real-\ntime, allowing SAM to adapt to diverse prompts with mini-\nmal computational overhead.\n3.2. Ours: SAM-PT\nWhile SAM shows impressive capabilities in image seg-\nmentation, it is inherently limited in handling video seg-\nmentation tasks. Our Segment Anything Meets Point Track-\ning (SAM-PT) approach effectively extends SAM to videos,\noffering robust video segmentation without requiring train-\ning on any video segmentation data.\nSAM-PT is illustrated in Fig. 2 and is primarily com-\nposed of four steps: 1) selecting query points for the first\nframe; 2) propagating these points to all video frames using\npoint trackers; 3) using SAM to generate per-frame segmen-\ntation masks based on the propagated points; 4) optionally\nreinitializing the process by sampling query points from the\npredicted masks. We next elaborate on these four steps.\n1) Query Points Selection.\nThe process begins with\ndefining query points in the first video frame, which either\ndenote the target object (positive points) or designate the\nbackground and non-target objects (negative points). Users\ncan manually and interactively provide query points, or they\nmay be derived from a ground truth mask. For example, in\nthe case of semi-supervised video object segmentation, the\nground truth mask is provided for the first frame where the\nobject appears. We derive the query points from ground\ntruth masks using different point sampling techniques by\nconsidering their geometrical locations or feature dissimi-\nlarities, as depicted in Fig. 3. These sampling techniques\nare:\n\u2022 Random Sampling: An intuitive approach where query\npoints are randomly selected from the ground truth mask.\n\u2022 K-Medoids Sampling: This technique takes the cluster\ncenters of K-Medoids clustering [29] as query points to\nensure good coverage of different parts of the object and\nrobustness to noise and outliers.\n\u2022 Shi-Tomasi Sampling: This method extracts Shi-Tomasi\ncorner points from the image under the mask as they have\nbeen shown to be good features to track [34].\n\u2022 Mixed Sampling:\nA hybrid method combining the\nabove techniques since it might benefit from the unique\nstrengths of each.\nWhile each method contributes distinct characteristics\nthat influence the model\u2019s performance, our ablation study\nreveals that K-Medoids sampling yields the best results with\ngood coverage of various segments of the complete object.\nShi-Tomasi sampling follows closely, indicating their re-\nspective strengths in this context.\nThe selection and ar-\nrangement of these points considerably affect the overall\nvideo segmentation performance, thus determining the op-\ntimal method is crucial.\n2) Point Tracking.\nInitiated with the query points, we\nemploy robust point trackers to propagate the points across\n3\nStep 1)\nQuery Points\nSelection\nStep 4)\nReinitialization\nQuery \nPoints\nt = i + 2\nt = i\nt = i + 1\nt = i + 3\nt = i + 4\nStep 3) \nSAM\nStep 2)\nPoint Tracker\nStep 3) \nSAM\nStep 3) \nSAM\nStep 3) \nSAM\nStep 3) \nSAM\nStep 2)\nPoint Tracker\npositive\nnegative\noccluded\nPoint types:\nFigure 2. Segment Anything Meets Point Tracking (SAM-PT) overview. The essence of SAM-PT is to extend image segmentation\nfoundation models to effectively operate on videos. SAM-PT has four steps: 1) Query Points Selection. It starts with first-frame query\npoints which denote the target object (positive points) or designate non-target segments (negative points). These points are provided by\nthe user or derived from a ground truth mask. 2) Point Tracking. Initiated with the query points, our approach leverages point trackers\nto propagate the points across video frames, predicting point trajectories and occlusion scores. 3) Segmentation. The trajectories are\nthen used to prompt the Segment Anything Model (SAM) and output per-frame mask predictions. 4) Point Tracking Reinitialization.\nOptionally, the predicted masks are used to reinitialize the query points and restart the process when reaching a prediction horizon h.\nReinitialization helps by getting rid of unreliable points and adding points to object segments that become visible in later frames.\nRandom\nRGB\nMask\nK-Medoids\nShi-Tomasi\nMixed\nFigure 3.\nPositive Point Sampling.\nFor an image paired with\neither a ground truth or predicted segmentation mask, positive\npoints are sampled from within the mask area using one of the\nfollowing point sampling methods: Random, K-Medoids [29],\nShi-Tomasi [34], or Mixed. Notably, Random Sampling and K-\nMedoids Sampling only require the segmentation mask for input,\nnot the corresponding input image. For negative points, we always\nuse Mixed Sampling on the target object\u2019s background mask.\nall frames in the video, resulting in point trajectories and oc-\nclusion scores. We adopt point trackers such as PIPS [16]\nand the state-of-the-art CoTracker [19] to propagate the\npoints as they show moderate robustness toward long-\nterm tracking challenges such as object occlusion and re-\nappearance. Long-term point trackers are also shown more\neffective than methods such as chained optical flow propa-\ngation or first-frame correspondences in our experiments.\n3) Segmentation.\nIn the predicted trajectories, the non-\noccluded points serve as indicators of where the target ob-\nject is throughout the video. This allows us to use the non-\noccluded points to prompt SAM, as illustrated in Fig. 4,\nand leverage its inherent generalization ability to output\nper-frame segmentation mask predictions. Unlike conven-\ntional tracking methods that require training or fine-tuning\non video segmentation data, our approach excels in zero-\nshot video segmentation tasks.\nWe combine positive and negative points by calling SAM\nin two passes. In the initial pass, we prompt SAM exclu-\nsively with positive points to define the object\u2019s initial lo-\ncalization. Subsequently, in the second pass, we prompt\nSAM with both positive and negative points along with the\nprevious mask prediction. Negative points provide a more\nnuanced distinction between the object and the background\nand help by removing wrongly segmented areas.\nLastly, we execute a variable number of mask refine-\nment iterations by repeating the second pass. This utilizes\nSAM\u2019s capacity to refine vague masks into more precise\nones. Based on our ablation study, this step notably im-\nproves video object segmentation performance.\n4) Point Tracking Reinitialization.\nWe optionally exe-\ncute a reinitialization of the query points using the predicted\nmasks once a prediction horizon of h = 8 frames is reached.\nUpon reaching this horizon, we have h predicted masks and\nwill take the last one to sample new points. At this stage,\nall previous points are discarded and substituted with the\nnewly sampled points. Following this, steps 1) through 4)\nare repeated with the new points, starting from the horizon\ntimestep where reinitialization occurs. The steps are iter-\natively executed until the entire video is processed. The\nreinitialization process serves to enhance tracking accuracy\n4\nStep 3)\nSAM\nImage \nEncoder\nMask \nDecoder\nPrompt \nEncoder\nPrompt \nEncoder\nMask \nDecoder\nFigure 4. Interacting with SAM in SAM-PT. In the first pass, SAM\nis prompted exclusively with positive points to define the object\u2019s\ninitial localization. In the second pass, both positive and negative\npoints along with the previous mask prediction are fed to the same\nmask decoder for further mask refinement. The negative points re-\nmove segments from the background and neighboring objects and\nnotably help in cases when the point tracker mistakenly predicts\npositive points off the target object. The second pass is repeated\niteratively to get a refined segmentation mask.\nover time by discarding unreliable or occluded points while\nincorporating points from object segments that become vis-\nible later in the video. Other reinitialization variants are\ndiscussed in our Supplementary Material.\n3.3. SAM-PT vs. Object-centric Mask Propagation\nWith sparse point tracking combined with prompting SAM,\nSAM-PT distinguishes itself from traditional video segmen-\ntation methods that depend on dense object mask propa-\ngation, as noted in Tab. 1.\nTo propagate the first-frame\nGT label to the remaining video frames, traditional tech-\nniques commonly use feature matching with masks cached\nto a mask memory [7, 11, 46, 48], frame-by-frame feature\nmatching [4, 18], optical flow [45], and, recently, in-context\nvisual prompting [41, 42]. In contrast, SAM-PT introduces\na unique approach to video object segmentation, employing\nthe robust combination of point tracking with SAM, which\nis inherently designed to operate on sparse point prompts.\nThe point propagation strategy of SAM-PT offers several\nadvantages over traditional object-centric tracking meth-\nods. First, point propagation exploits local structure con-\ntext that is agnostic to global object semantics. This en-\nhances our model\u2019s capability for zero-shot generalization,\nan advantage that, coupled with SAM\u2019s inherent general-\nization power, allows for tracking diverse objects in diverse\nenvironments, such as on the UVO benchmark. Second,\nSAM-PT allows for a more compact object representation\nwith sparse points, capturing enough information to char-\nacterize the object\u2019s segments/parts effectively. Finally, the\nuse of points is naturally compatible with SAM, an image\nsegmentation foundation model trained to operate on sparse\npoint prompts, offering an integrated solution that aligns\nwell with the intrinsic capacities of the underlying model.\nComparing SAM-PT with conventional methods in\nTab. 1, SAM-PT emerges as superior or comparable to\nmethods that refrain from utilizing video segmentation data\nduring training. However, there is a performance gap that\nexists between such methods and those that leverage video\nsegmentation training data in the same domain, such as\nXMem [7] or DeAOT [48]. Further, the potential of our\nTable 1.\nComparative analysis of semi-supervised Video Ob-\nject Segmentation methods. Our approach, SAM-PT, introduces\nsparse point propagation, a compact mask representation that uses\nlocal structure information agnostic to object semantics. It out-\nperforms other non-video-data-dependent methods, achieving top\nJ &F scores on DAVIS 2016 and 2017, and the highest G score\non YouTube-VOS 2018. The comparison considers the reliance on\nvideo mask data during training, zero-shot learning setting, initial\nframe label requirements, and label propagation techniques used.\nMethod\nVideo\nMask\nZero-\nShot\nFrame\nInit.\nPropagation\nDAVIS\n2016\nDAVIS\n2017\nYTVOS\n2018\nSiamMask [38]\n\u2713\n\u2717\nBox\nFeature Correlation\n69.8\n56.4\n-\nQMRA [24]\n\u2713\n\u2717\nBox\nFeature Correlation\n85.9\n71.9\n-\nTAM [46]\n\u2713\n\u2717\nPoints\nFeature Matching\n88.4\n-\n-\nSAM-Track [11]\n\u2713\n\u2717\nPoints\nFeature Matching\n92.0\n-\n-\nDEVA [10]\n\u2713\n\u2717\nMask\nFeature Matching\n-\n87.6\n-\nXMem [7]\n\u2713\n\u2717\nMask\nFeature Matching\n92.0\n87.7\n86.1\nDeAOT [48]\n\u2713\n\u2717\nMask\nFeature Matching\n92.9\n86.2\n86.2\nPainter [41]\n\u2717\n\u2713\nMask\nMask Prompting\n-\n34.6\n24.1\nSTC [18]\n\u2717\n\u2713\nMask\nFeature Matching\n-\n67.6\n-\nDINO [4]\n\u2717\n\u2713\nMask\nFeature Matching\n-\n71.4\n-\nSegGPT [42]\n\u2717\n\u2713\nMask\nMask Prompting\n82.3\n75.6\n74.7\nSAM-PT (ours)\n\u2717\n\u2713\nPoints\nPoints Prompting\n84.3\n79.4\n76.2\nmodel extends beyond video object segmentation to other\ntasks, such as Video Instance Segmentation (VIS), thanks\nto the inherent flexibility of our point propagation strategy.\n4. Experiments\n4.1. Datasets\nWe evaluate our method on four VOS datasets: DAVIS\n2016, DAVIS 2017 [30], YouTube-VOS 2018 [44], and\nMOSE 2023 [13]. DAVIS 2017 is also used in our inter-\nactive point-based video segmentation study. We addition-\nally devise a VOS dataset from BDD100K [50]. For VIS,\nWe evaluate our method on the class-agnostic dense video\ninstance segmentation task of the UVO v1.0 [40] dataset.\nUVO v1.0 is a VIS dataset aiming for open-world segmen-\ntation, where objects of any category, including those un-\nseen in training, are identified and segmented.\n4.2. Implementation Details\nTraining Data.\nFor our experiments, we use pre-trained\ncheckpoints provided by the respective authors for the point\ntrackers (PIPS [32], CoTracker [19], etc.) and SAM. PIPS\nand CoTracker have been trained exclusively on synthetic\ndata, PIPS on FlyingThings++ [16] and CoTracker on TAP-\nVid-Kubric [14]. SAM has been trained on the large-scale\nSA-1B dataset, the largest image segmentation dataset to\ndate. HQ-SAM is further trained on the HQ-Seg-44k [20].\nNoteworthy, none of these datasets contain video segmenta-\ntion data, nor do they intersect with any datasets we use for\nevaluation, situating our model within a zero-shot setting.\nInteractive Point-Based Video Segmentation.\nTo assess\nthe interactive capabilities of SAM-PT, we simulate user re-\nfinement of video segmentation results through point addi-\ntions and removals. We compare three methods: a non-\ntracking approach using SAM alone, an online method\nmaking one pass through the video with a target IoU qual-\nity, and an offline method employing multiple passes that\n5\nTable 2. We report the mean performance and standard deviation\nacross eight runs on the validation subset of DAVIS 2017 to study\nthe impact of different point trackers.\nPoint Tracker\nDAVIS 2017 Validation [30]\nJ &F \u2191\nJ \u2191\nF \u2191\nSuperGlue [33]\n28.4\u00b13.1\n24.7\u00b12.4\n32.0\u00b13.8\nTapNet [14]\n60.9\u00b10.2\n58.2\u00b10.3\n63.5\u00b10.2\nRAFT [35]\n63.0\u00b10.6\n60.7\u00b10.6\n65.4\u00b10.5\nPIPS++ [53]\n73.2\u00b10.5\n69.9\u00b10.5\n76.6\u00b10.5\nPIPS [16]\n76.3\u00b10.6\n73.6\u00b10.6\n78.9\u00b10.6\nTAPIR [15]\n76.7\u00b10.3\n73.8\u00b10.4\n79.7\u00b10.3\nCoTracker [19]\n77.6\u00b10.7\n74.8\u00b10.7\n80.4\u00b10.7\nprogressively aim at higher IoU quality.\nThese methods\nare detailed in the Supplementary Material, which includes\npseudocode for each simulation variant.\n4.3. Ablation Study\nOur ablation experiments on the DAVIS 2017 validation\nsubset assessed different aspects of SAM-PT\u2019s design. De-\nspite the valuable insights, we acknowledge the dataset\u2019s\nlimited scope in representing diverse and complex segmen-\ntation challenges, such as occlusions and varying environ-\nmental conditions, may constrain the generalizability of our\nablation\u2019s findings. Future investigations could benefit from\na more varied validation set, potentially sourced from the\nYouTube-VOS 2018 training dataset, to enhance robustness.\nOur findings, detailed in Tab. 2, underscore SAM-\nPT\u2019s adaptability across leading long-term point trackers \u2013\nPIPS [16], TAPIR [15], and notably CoTracker [19], which\nexcelled due to its precise point tracking and reliable occlu-\nsion predictions. In contrast, PIPS++ [53] lagged despite\nbeing a more recent iteration of PIPS[16], due to the lack\nof occlusion prediction which is important for the effec-\ntive use of point tracking for segmentation. TapNet [14]\nstruggled due to less effective temporal consistency and\nhigh-resolution inputs. Traditional methods such as Super-\nGlue [33] and RAFT [35], which, although proficient in\ntheir respective domains, either struggle with the dynamic\nand deformable aspects of video scenes or cannot handle\nocclusion, highlighting the specialized efficacy of long-term\ntrackers in the video segmentation landscape.\nIn Tab. 3, we tested SAM-PT with various settings us-\ning PIPS as the tracker. We found that using eight pos-\nitive points per object instead of just one improved our\nscores significantly by 33.4 points because one point often\nwasn\u2019t enough for unambiguously prompting SAM. Select-\ning points with K-Medoids was slightly better than random\nand matched Shi-Tomasi, giving a boost of 1.8 points. In-\ncorporating negative points besides positive points helped\nwhen trackers made mistakes, such as losing track of an ob-\nject, by improving scores by another 1.8 points. Adding\niterative refinement smoothed out mask quality and fixed\nsome errors, adding another 2.2 points to our performance.\nWe tried filtering out unreliable points with patch similar-\nity, but this did not work well as it ended up removing too\nmany points. Finally, although reinitializing points did not\nTable 3. Ablation study on the validation subset of DAVIS 2017 on\nthe impact of different SAM-PT configurations using PIPS [16] as\nthe point tracker. PSM: point selection method. PP: positive points\nper mask. NP: negative points per mask. IRI: iterative refinement\niterations. PS: point similarity filtering. RV: reinitalization variant.\nSAM-PT Configuration (using PIPS)\nDAVIS [30]\nPSM\nPP\nNP\nIRI\nPS\nRV\nJ &F \u2191\nGain\nRandom\n1\n0\n0\n\u2717\n\u2717\n37.1\u00b121.7\nRandom\n8\n0\n0\n\u2717\n\u2717\n70.5\u00b11.4\n+33.4\nK-Medoids\n8\n0\n0\n\u2717\n\u2717\n72.3\u00b11.2\n+1.8\nShi-Tomasi\n8\n0\n0\n\u2717\n\u2717\n72.0\u00b10.3\nMixed\n8\n0\n0\n\u2717\n\u2717\n70.6\u00b10.8\nK-Medoids\n8\n1\n0\n\u2717\n\u2717\n74.1\u00b10.7\n+1.8\nK-Medoids\n8\n1\n12\n\u2717\n\u2717\n76.3\u00b10.6\n+2.2\nK-Medoids\n8\n1\n12\n\u2713\n\u2717\n72.7\u00b12.0\nnone\nK-Medoids\n8\n72\n12\n\u2717\nA\n76.8\u00b10.7\n+0.5\nK-Medoids\n8\n1\n12\n\u2717\nB\n76.1\u00b10.4\nK-Medoids\n8\n1\n0\n\u2717\nC\n75.5\u00b10.7\nK-Medoids\n8\n1\n12\n\u2717\nD\n76.4\u00b10.3\nhelp significantly in the initial tests, it did show benefits on\nother datasets such as MOSE and UVO, helping us recover\nfrom tracker errors by discarding incorrect and adding fresh\npoints as well as detecting that the object has disappeared\nand the tracking should be halted.\nExtended ablation experiments and discussions can be\nfound in the Supplementary Material, including complete\nablation results for PIPS and CoTracker, the choice of the\nSAM backbone, and SAM\u2019s lightweight variants that reveal\ntrade-offs between performance and inference speed.\n4.4. Video Object Segmentation\nPerformance Overview.\nOur SAM-PT method, utilizing\nHQ-SAM and CoTracker, sets a new standard in zero-shot\nvideo object segmentation on the DAVIS 2017 dataset with\na mean J &F score of 79.4, outperforming SegGPT\u2019s 75.6,\nDINO\u2019s 71.4, and Painter\u2019s 34.6 as shown in Tab. 4. On\nthe easier DAVIS 2016 validation set, our method achieves\n84.3, surpassing SegGPT\u2019s 82.3, showcasing the strength of\nour approach even in less complex scenarios, as detailed in\nthe Supplementary Material.\nFor the YouTube-VOS 2018 validation set, we achieve\nthe highest performance among zero-shot methods with\n76.2 against SegGPT\u2019s 74.7 and Painter\u2019s 24.1, indicating\nrobust generalizability across various video segmentation\nbenchmarks (Tab. 5).\nIn the semi-supervised VOS on\nBDD100K\u2019s validation set, our method outperforms Seg-\nGPT for non-transient objects but also surpasses the fully-\nsupervised XMem across nearly all object visibility dura-\ntions. The detailed breakdown is provided in Tab. 6.\nOn the MOSE 2023 validation set, our performance re-\nmains competitive with SegGPT, with exact figures avail-\nable in the Supplementary Material.\nQualitative Analysis.\nOur visualizations in Fig. 5a and\nFig. 5b demonstrate successful segmentation on the DAVIS\n2017 dataset and underscore our method\u2019s ability to per-\nform zero-shot video segmentation on unseen content, such\n6\n(a) Successful segmentation cases for SAM-PT using 8 positive and 1 negative point.\n(b) Successful segmentation cases for SAM-PT with 8 positive and 72 negative points and reinitialization enabled.\n(c) Failure cases for SAM-PT where challenges such as occlusions and thin object structures lead to tracking errors.\nFigure 5. Visualization of SAM-PT on DAVIS 2017 [30]. The method shows its capability to segment and track objects using the initial\nmasks from the first frame, with circles denoting positive points and crosses negative points. Red symbols indicate occlusion prediction.\nFigure 6. Successful segmentation using SAM-PT on short clips from \u201cAvatar: The Last Airbender\u201d. Although our method has never seen\ndata from Avatar, an anime-influenced animated television series, it segments and tracks various objects in short clips.\nas clips from the anime-influenced series \u201cAvatar: The Last\nAirbender\u201d in Fig. 6. These examples highlight the versa-\ntility and adaptability of SAM-PT.\nLimitations and Challenges.\nOur method excels in zero-\nshot video object segmentation but faces challenges with\npoint tracker reliability in complex scenarios, such as oc-\nclusions and fast-moving objects, as shown in Fig. 5c.\nWhile point reinitialization and negative point strategies of-\nfer some improvement, interactive use significantly bridges\nthe performance gap with trained methods. This interactive\npotential is successfully demonstrated in Sec. 4.6, where\nuser intervention enhances segmentation accuracy.\n4.5. Video Instance Segmentation\nGiven the same mask proposals, SAM-PT outperforms\nTAM [46] significantly, as shown in Tab. 7, even though\nSAM-PT was not trained on any video segmentation\ndata. TAM is a concurrent approach combining SAM and\nXMem [7], where XMem was pre-trained on BL30K [9]\nand trained on DAVIS and YouTube-VOS, but not on UVO.\nOn the other hand, SAM-PT combines SAM with the PIPS\nor CoTracker point tracking method, both of which have not\nbeen trained on video segmentation tasks.\n4.6. Interactive Point-Based Video Segmentation\nBuilding upon our method\u2019s strengths observed in stan-\ndard benchmarks, this study evaluates the responsiveness of\nSAM-PT to human input, aiming to understand its practi-\n7\nTable 4. Quantitative results on the DAVIS 2017 [30] validation\nset for semi-supervised VOS. Performance is reported for different\nmethods, including our SAM-PT and HQ-SAM-PT, with and with-\nout the reinitialization strategy (Reinit) and using different point\ntrackers. Our method outperforms other zero-shot methods.\nMethod\nTracker\nReinit\nDAVIS 2017 Validation [30]\nJ &F \u2191\nJ \u2191\nF \u2191\n(a) trained on video segmentation data\nMiVOS [8]\n-\n-\n84.5\n81.7\n87.4\nDeAOT [48]\n-\n-\n86.2\n83.1\n89.2\nDEVA [10]\n-\n-\n87.6\n84.2\n91.0\nXMem [7]\n-\n-\n87.7\n84.0\n91.4\n(b) not trained on video segmentation data (zero-shot)\nPainter [41]\n-\n-\n34.6\n28.5\n40.8\nDINO [4]\n-\n-\n71.4\n67.9\n74.9\nSegGPT [42]\n-\n-\n75.6\n72.5\n78.6\nSAM-PT\n(ours)\nPIPS [16]\n\u2717\n76.3\u00b10.6\n73.6\u00b10.6\n78.9\u00b10.6\nPIPS [16]\n\u2713\n76.6\u00b10.7\n74.4\u00b10.8\n78.9\u00b10.6\nCoTracker [19]\n\u2717\n77.6\u00b10.7\n74.8\u00b10.7\n80.4\u00b10.7\nCoTracker [19]\n\u2713\n77.4\u00b11.0\n74.5\u00b11.0\n80.3\u00b11.1\nHQ-SAM-PT\n(ours)\nPIPS [16]\n\u2717\n77.2\u00b10.5\n74.7\u00b10.5\n79.8\u00b10.4\nPIPS [16]\n\u2713\n77.0\u00b10.7\n74.8\u00b10.8\n79.2\u00b10.6\nCoTracker [19]\n\u2717\n79.4\u00b10.6\n76.5\u00b10.6\n82.3\u00b10.5\nCoTracker [19]\n\u2713\n77.7\u00b10.8\n74.6\u00b10.9\n80.8\u00b10.7\nTable 5. Quantitative results in semi-supervised VOS on the val-\nidation subset of YouTube-VOS 2018. Metrics are reported sep-\narately for \u201cseen\u201d and \u201cunseen\u201d classes, with G being the overall\naverage score over the metrics.\nMethod\nTracker\nReinit\nYouTube-VOS 2018 Validation [44]\nG\nJs\nFs\nJu\nFu\n(a) trained on video segmentation data\nXMem [7]\n-\n-\n86.1\n85.1\n89.8\n80.3\n89.2\nDeAOT [48]\n-\n-\n86.2\n85.6\n90.6\n80.0\n88.4\n(b) not trained on video segmentation data (zero-shot)\nPainter [41]\n-\n-\n24.1\n27.6\n35.8\n14.3\n18.7\nSegGPT [42]\n-\n-\n74.7\n75.1\n80.2\n67.4\n75.9\nSAM-PT\n(ours)\nPIPS [16]\n\u2717\n67.0\u00b10.3\n68.6\u00b10.2\n71.2\u00b10.1\n61.0\u00b10.5\n67.4\u00b10.4\nPIPS [16]\n\u2713\n67.5\u00b10.2\n69.0\u00b10.4\n69.9\u00b10.3\n63.2\u00b10.4\n67.8\u00b10.5\nCoTracker [19]\n\u2717\n74.0\u00b10.3\n73.3\u00b10.2\n76.0\u00b10.2\n70.0\u00b10.4\n76.7\u00b10.4\nCoTracker [19]\n\u2713\n71.5\u00b10.4\n71.0\u00b10.3\n72.8\u00b10.3\n68.3\u00b10.7\n73.9\u00b10.7\nHQ-SAM-PT\n(ours)\nCoTracker [19]\n\u2717\n76.2\u00b10.1\n75.3\u00b10.1\n78.4\u00b10.2\n72.1\u00b10.2\n79.0\u00b10.2\ncal utility for interactive video annotation tasks. We bench-\nmarked SAM-PT\u2019s interactive performance against a base-\nline SAM approach that does not utilize point tracking. The\nresults, visualized in Fig. 7, show that SAM-PT signifi-\ncantly outperforms the baseline, particularly when employ-\ning the offline checkpoint strategy. Although this offline\nmethod initially starts at a lower performance due to setup\ncosts, it quickly exceeds the online method\u2019s performance\nby benefiting from a cumulative optimization approach.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nInteraction per Object Mask\n30\n40\n50\n60\n70\n80\n90\n100\nJ&F-Mean (%)\nSAM (no PT)\nSAM-PT (online)\nSAM-PT (offline)\nFigure 7. Interactive segmentation performance on the DAVIS\n2017 [30] validation set. SAM-PT\u2019s offline strategy notably out-\nperforms the baseline SAM, demonstrating efficient video annota-\ntion with minimal user intervention.\nTable 6.\nThis table shows the semi-supervised VOS perfor-\nmance on BDD100K\u2019s validation set, comparing our HQ-SAM-\nPT method using CoTracker as the point tracker (without reini-\ntialization) against the zero-shot SegGPT and the fully-supervised\nXMem. Performance metrics include the average J &F measure\nfor object visibility durations categorized as short (1-5 frames),\nmedium (6-30 frames), and long (31+ frames).\nOur approach\ndemonstrates superior results over SegGPT for non-transient ob-\njects and over XMem across all visibility durations except for\nlong-term object tracking.\nBDD100K VOS Validation [50]\nMethod\nJ &F\nJ\nF\nJ &F\nShort\nJ &F\nMedium\nJ &F\nLong\n(a) trained on video segmentation data, but not on BDD100K\nXMem [7]\n76.6\n74.5\n78.7\n79.3\n78.6\n63.7\n(b) not trained on video segmentation data (zero-shot)\nSegGPT [42]\n81.5\n81.2\n81.8\n96.1\n78.6\n52.0\nHQ-SAM-PT (ours)\n81.0\n80.1\n81.8\n91.8\n79.9\n55.8\nTable 7. Results on the validation split of UVO [40] VideoDens-\neSet v1.0. SAM-PT outperforms TAM [46] even though the for-\nmer was not trained on any video segmentation data. TAM is a\nconcurrent approach combining SAM [21] and XMem [7], where\nXMem was pre-trained on BL30K [9] and trained on DAVIS [30]\nand YouTube-VOS [44], but not on UVO. On the other hand,\nSAM-PT combines SAM with point trackers, both of which have\nnot been trained on any video segmentation tasks.\nMethod\nTracker\nReinit\nAR100\nARs\nARm\nARl\nAP\n(a) trained on video segmentation data, including UVO\u2019s training subset\nMask2Former\nVIS [51]\n-\n-\n35.4\n\u2212\n\u2212\n\u2212\n27.3\nROVIS [51]\n-\n-\n41.2\n\u2212\n\u2212\n\u2212\n32.7\n(b) trained on video segmentation data\nTAM [46]\n-\n-\n24.1\n21.1\n32.9\n31.1\n1.7\n(c) not trained on video segmentation data (zero-shot)\nSAM-PT\n(ours)\nPIPS [16]\n\u2717\n28.8\n23.3\n40.8\n48.3\n6.7\nPIPS [16]\n\u2713\n30.8\n25.1\n44.1\n49.2\n6.5\nCoTracker [19]\n\u2717\n29.5\n25.3\n39.0\n44.1\n5.8\nCoTracker [19]\n\u2713\n29.8\n25.1\n40.6\n45.6\n6.2\nThese results suggest that SAM-PT substantially reduces\nthe effort required for high-quality video annotation, bring-\ning its performance closer to fully-supervised methods and\nhighlighting its practical utility.\n5. Conclusion\nSAM-PT introduces a point-centric approach for interactive\nvideo segmentation by combining the generalization capa-\nbilities of the Segment Anything Model (SAM) and long-\nterm point tracking. Our work fills in the gap that the point-\ncentric approach is scantly explored in the literature. In our\nexperiments, SAM-PT achieves strong performance across\nvideo segmentation tasks including semi-supervised, open-\nworld, and fully interactive video segmentation. While our\nmethod has limitations such as difficulty handling occlu-\nsions, small objects, motion blur, and inconsistencies in\nmask predictions, it contributes a new perspective to video\nobject segmentation research.\n8\nReferences\n[1] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc\nVan Gool. Speeded-up robust features (surf). Computer vi-\nsion and image understanding, 110(3):346\u2013359, 2008. 2\n[2] Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis,\nYuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi\nPont-Tuset. The 2018 davis challenge on video object seg-\nmentation. In arXiv:1803.00557, 2018. 3\n[3] Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto\nMontes, Kevis-Kokitsi Maninis, and Luc Van Gool.\nThe\n2019 davis challenge on vos: Unsupervised multi-object seg-\nmentation. In arXiv:1905.00737, 2019. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 2, 5, 8\n[5] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexan-\nder Kirillov, Rohit Girdhar, and Alexander G. Schwing.\nMask2former for video instance segmentation.\narXiv\npreprint arXiv: 2112.10764, 2021. 1\n[6] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov.\nPointly-supervised instance segmentation. In CVPR, pages\n2617\u20132626, 2022. 2\n[7] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, 2022. 1, 2, 5, 7, 8, 11\n[8] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021. 3,\n8\n[9] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021. 7,\n8\n[10] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander\nSchwing, and Joon-Young Lee. Tracking anything with de-\ncoupled video segmentation. In ICCV, 2023. 5, 8, 11\n[11] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\nZongxin Yang, Wenguan Wang, and Yi Yang. Segment and\ntrack anything. arXiv preprint arXiv:2305.06558, 2023. 2, 5\n[12] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. Superpoint: Self-supervised interest point detection\nand description. In CVPRW, 2018. 2\n[13] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip\nH. S. Torr, and Song Bai. Mose: A new dataset for video ob-\nject segmentation in complex scenes. arXiv preprint arXiv:\n2302.01872, 2023. 5, 11\n[14] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Re-\ncasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for tracking\nany point in a video. In NeurIPS, 2022. 2, 5, 6\n[15] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush\nGupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman.\nTapir: Tracking any point with per-frame initialization and\ntemporal refinement. ICCV, 2023. 2, 6, 11\n[16] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle video revisited: Tracking through occlusions using\npoint trajectories. In ECCV, 2022. 2, 3, 4, 5, 6, 8, 11, 12\n[17] Yuk Heo, Yeong Jun Koh, and Chang-Su Kim. Interactive\nvideo object segmentation using global and local transfer\nmodules. In ECCV, 2020. 3\n[18] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time\ncorrespondence as a contrastive random walk. In NeurIPS,\n2020. 2, 5\n[19] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\ntracker:\nIt is better to track together.\narXiv preprint\narXiv:2307.07635, 2023. 2, 3, 4, 5, 6, 8, 11, 12\n[20] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\nhigh quality. In NeurIPS, 2023. 2, 5, 11, 12\n[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and\nRoss Girshick. Segment anything. In ICCV, pages 4015\u2013\n4026, 2023. 1, 2, 3, 8, 12\n[22] Christoph H Lampert, Hannes Nickisch, and Stefan Harmel-\ning. Learning to detect unseen object classes by between-\nclass attribute transfer.\nIn CVPR, pages 951\u2013958. IEEE,\n2009. 3\n[23] Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan,\nand Dong Liu. Recurrent dynamic embedding for video ob-\nject segmentation. In CVPR, 2022. 11\n[24] Fanchao Lin, Hongtao Xie, Yan Li, and Yongdong Zhang.\nQuery-memory re-aggregation for weakly-supervised video\nobject segmentation. In AAAI, 2021. 5\n[25] David G Lowe.\nDistinctive image features from scale-\ninvariant keypoints. IJCV, 60:91\u2013110, 2004. 2\n[26] Bruce D Lucas and Takeo Kanade. An iterative image reg-\nistration technique with an application to stereo vision. In\nIJCAI, 1981. 2\n[27] Jiaxu Miao, Yunchao Wei, and Yi Yang. Memory aggrega-\ntion networks for efficient interactive video object segmenta-\ntion. In CVPR, 2020. 3\n[28] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim.\nFast user-guided video object segmentation by\ninteraction-and-propagation networks. In CVPR, 2019. 3\n[29] Hae-Sang Park and Chi-Hyuck Jun. A simple and fast algo-\nrithm for k-medoids clustering. Expert Systems with Appli-\ncations, 36(2, Part 2):3336\u20133341, 2009. 3, 4\n[30] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 5, 6, 7, 8, 11, 12, 15\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. 2021. 3\n[32] Peter Sand and Seth Teller. Particle video: Long-range mo-\ntion estimation using point trajectories.\nIJCV, 80:72\u201391,\n2008. 5\n[33] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich.\nSuperglue:\nLearning feature\nmatching with graph neural networks. In CVPR, 2020. 2,\n6\n[34] Jianbo Shi and Tomasi. Good features to track. In CVPR,\n1994. 2, 3, 4\n9\n[35] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In ECCV, 2020. 2, 6\n[36] Carlo Tomasi and Takeo Kanade. Detection and tracking of\npoint. IJCV, 9:137\u2013154, 1991. 2\n[37] Jue Wang, Pravin Bhat, R Alex Colburn, Maneesh Agrawala,\nand Michael F Cohen.\nInteractive video cutout.\nIn ToG,\n2005. 3\n[38] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip HS Torr. Fast online object tracking and segmentation:\nA unifying approach. In CVPR, 2019. 5\n[39] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah Snavely.\nTracking everything everywhere all at once. In ICCV, 2023.\n2\n[40] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.\nUnidentified video objects: A benchmark for dense, open-\nworld segmentation. In ICCV, 2021. 5, 8\n[41] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang. Images speak in images: A generalist painter\nfor in-context visual learning. In CVPR, 2023. 2, 5, 8, 11\n[42] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context. In ICCV, 2023. 2, 5, 8, 11\n[43] Junfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai, and Song\nBai. Seqformer: a frustratingly simple model for video in-\nstance segmentation. In ECCV, 2022. 1\n[44] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas Huang. Youtube-vos: A\nlarge-scale video object segmentation benchmark, 2018. 5,\n8\n[45] Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman,\nand Weidi Xie. Self-supervised video object segmentation\nby motion grouping. In ICCV, 2021. 5\n[46] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023. 2, 5,\n7, 8, 13\n[47] Zongxin Yang and Yi Yang.\nDecoupling features in hi-\nerarchical propagation for video object segmentation.\nIn\nNeurIPS, 2022. 2\n[48] Zongxin Yang and Yi Yang.\nDecoupling features in hi-\nerarchical propagation for video object segmentation.\nIn\nNeurIPS, 2022. 1, 5, 8, 11\n[49] K. M. Yi, Eduard Trulls, Vincent Lepetit, and P. Fua. Lift:\nLearned invariant feature transform. ECCV, 2016. 2\n[50] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\nrell. Bdd100k: A diverse driving dataset for heterogeneous\nmultitask learning. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 5, 8\n[51] Zitong Zhan, Daniel McKee, and Svetlana Lazebnik. Robust\nonline video instance segmentation with track queries. arXiv\npreprint arXiv: 2211.09108, 2022. 8\n[52] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,\nSung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.\nFaster segment anything: Towards lightweight sam for mo-\nbile applications. arXiv preprint arXiv:2306.14289, 2023.\n11, 12\n[53] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J. Guibas. Pointodyssey: A large-scale\nsynthetic dataset for long-term point tracking.\nIn ICCV,\n2023. 2, 6\n10\nSegment Anything Meets Point Tracking\nSupplementary Material\nIn this supplementary, we first report experimental re-\nsults on additional datasets and subsets (Appendices A\nto C). Then we extend and detail our ablation and report\nmore qualitative results (Appendices D to H). Lastly, we\ndetail on our evaluation protocols (Appendices I to L).\nA. MOSE 2023\nMOSE 2023 [13] is a recently introduced dataset that fo-\ncuses on multi-object segmentation and tracking in complex\nscenes, replete with challenges such as occlusions, transient\nvisibility of objects, extensive occlusion, etc. Our results on\nthe validation subset suggest that SAM-PT achieves perfor-\nmance competitive with SegGPT [42], as shown in Tab. 8.\nTable 8. Quantitative results on the MOSE 2023 validation set for\nsemi-supervised VOS. Our method achieves performance compa-\nrable to the state-of-the-art zero-shot learning method. Note that\nSegGPT and SAM-PT adopt completely different training data.\nMethod\nTracker\nReinit\nMOSE 2023 Validation [13]\nJ &F \u2191\nJ \u2191\nF \u2191\n(a) trained on video segmentation data\nRDE [23]\n-\n-\n48.8\n44.6\n52.9\nXMem [7]\n-\n-\n57.6\n53.3\n62.0\nDeAOT [48]\n-\n-\n59.4\n55.1\n63.8\nDEVA [10]\n-\n-\n66.5\n62.3\n70.8\n(b) not trained on video segmentation data (zero-shot)\nPainter [41]\n-\n-\n14.5\n10.4\n18.5\nSegGPT [42]\n-\n-\n45.1\n42.2\n48.0\nSAM-PT\n(ours)\nPIPS [16]\n\u2717\n38.5\u00b10.2\n34.9\u00b10.3\n42.1\u00b10.2\nPIPS [16]\n\u2713\n41.0\u00b10.5\n38.5\u00b10.5\n43.5\u00b10.5\nCoTracker [19]\n\u2717\n41.8\u00b10.2\n38.3\u00b10.2\n45.2\u00b10.3\nCoTracker [19]\n\u2713\n40.1\u00b10.5\n36.0\u00b10.5\n44.1\u00b10.4\nTAPIR [15]\n\u2717\n42.9\u00b10.2\n38.3\u00b10.2\n47.6\u00b10.1\nHQ-SAM-PT\n(ours)\nCoTracker [19]\n\u2717\n42.4\u00b10.3\n39.0\u00b10.3\n45.8\u00b10.3\nTAPIR [15]\n\u2717\n42.1\u00b10.1\n37.6\u00b10.1\n46.7\u00b10.1\nB. DAVIS 2016\nDAVIS 2016 offers a single-object VOS benchmark across\n20 diverse sequences. We report results on the DAVIS 2016\nvalidation subset in Tab. 9, in which our method achieves\n84.3 points, surpassing SegGPT\u2019s 82.3 points.\nTable 9. Quantitative results on the DAVIS 2016 validation set for\nsemi-supervised VOS. Our method achieves higher performance\ncompared to SegGPT, both of which are zero-shot methods.\nDAVIS 2016 Validation [30]\nMethod\nTracker\nReinit\nJ &F \u2191\nJ \u2191\nF \u2191\nSegGPT [42]\n-\n-\n82.3\n81.8\n82.8\nSAM-PT\nPIPS [16]\n\u2717\n83.1\u00b11.5\n83.0\u00b10.8\n83.0\u00b11.1\nPIPS [16]\n\u2713\n80.2\u00b10.6\n80.3\u00b10.6\n80.0\u00b10.6\nCoTracker [19]\n\u2717\n83.1 \u00b1 0.6\n83.2 \u00b1 0.7\n82.9 \u00b1 0.6\nCoTracker [19]\n\u2713\n82.6 \u00b1 0.8\n83.0 \u00b1 1.0\n82.2 \u00b1 0.9\nHQ-SAM-PT\nCoTracker [19]\n\u2717\n84.3 \u00b1 0.9\n84.9 \u00b1 1.0\n83.7 \u00b1 0.9\nC. DAVIS 2017 Test-dev Subset\nDAVIS 2017 is a multi-object extension of its 2016 ver-\nsion. The video scenarios within this dataset are small but\ndiverse. In addition to the results on the validation subset in\nthe main manuscript, we report the performance of SAM-\nPT on the DAVIS 2017 test-dev subset in Tab. 10.\nTable 10. Quantitative results on the DAVIS 2017 test-dev subset\nfor semi-supervised VOS.\nDAVIS 2017 Test-dev [30]\nMethod\nTracker\nReinit\nJ &F \u2191\nJ \u2191\nF \u2191\nSAM-PT\nPIPS [16]\n\u2717\n62.7\u00b10.5\n59.4\u00b10.6\n66.1\u00b10.4\nPIPS [16]\n\u2713\n61.5\u00b11.1\n59.3\u00b11.0\n63.8\u00b11.2\nCoTracker [19]\n\u2717\n65.7\u00b10.7\n62.8\u00b10.7\n68.5\u00b10.7\nCoTracker [19]\n\u2713\n62.0\u00b11.2\n58.8\u00b11.2\n65.1\u00b11.1\nTAPIR [15]\n\u2717\n69.0\n66.0\n72.0\nTAPIR [15]\n\u2713\n64.1\n61.3\n66.9\nHQ-SAM-PT\nCoTracker [19]\n\u2717\n64.8\u00b10.5\n61.9\u00b10.5\n67.7\u00b10.5\nD. Different SAM Backbones\nThe SAM model\u2019s backbone plays an important role in de-\ntermining its performance and inference speed. In this ex-\nperiment, we evaluated SAM with different ViT backbones:\nViT-Huge (used throughout the work), ViT-Large, and ViT-\nBase. The results, as measured on the validation subset of\nDAVIS 2017 are shown in Tab. 11. Replacing ViT-Huge\nwith ViT-Large results in only a non-significant loss in per-\nformance, SAM-PT\u2019s overall performance nevertheless re-\nmains non-real-time. Note that the ViT-Huge number in this\nexperiment is slightly different from the one presented in\nthe main manuscript due to the use of different seed values.\nTable 11. Performance of SAM-PT with different backbones and\ntheir inference speed in semi-supervised VOS on the validation\nsubset of DAVIS 2017, when using PIPS as the point tracker.\nSAM Backbone\nJ &F\nFPS\nViT-Huge\n76.7 \u00b1 0.6\n1.4\nViT-Large\n76.4 \u00b1 0.6\n1.8\nViT-Base\n72.2 \u00b1 0.5\n2.6\nE. Different SAM Variants\nTo cater to scenarios where inference speed is crucial, we\nexplored lightweight variants of SAM: Light HQ-SAM [20]\nand MobileSAM [52]. The performance and speed trade-\noffs for these variants are summarized in table Tab. 12. Us-\ning the HQ-SAM [20] variant of SAM results in the highest\nperformance of 77.64 points, whereas MobileSAM has the\nhighest inference speed of 5.5 FPS. Using the lightweight\n11\nvariants doesn\u2019t achieve real-time performance as the bot-\ntleneck of the pipeline moves to the point tracker.\nTable 12. Performance of lightweight SAM variants and their in-\nference speed in semi-supervised VOS on the validation subset of\nDAVIS 2017 when using PIPS [16] as the point tracker.\nSAM Variant\nBackbone\nJ &F\nFPS\nHQ-SAM [20]\nViT-Huge\n77.64\n1.3\nSAM [21]\nViT-Huge\n76.65\n1.4\nLight HQ-SAM [20]\nViT-Tiny\n71.30\n4.8\nMobileSAM [52]\nViT-Tiny\n71.07\n5.5\nF. Detailed Ablation Results\nWe report detailed experimental results of our ablation stud-\nies for configurations using PIPS [16] as the point tracker in\nTab. 14 and using CoTracker [19] in Tab. 13.\nTable 13.\nAblation study on the validation subset of DAVIS\n2017 on the impact of different SAM-PT configurations using Co-\nTracker [19] as the point tracker. PSM: point selection method.\nPP: positive points per mask. NP: negative points per mask. IRI:\niterative refinement iterations. PS: point similarity filtering.\nSAM-PT Config. (using CoTracker)\nDAVIS 2017 Validation [30]\nPSM\nPP\nNP\nIRI\nPS\nJ &F \u2191\nJ \u2191\nF \u2191\nK-Medoids\n16\n1\n12\n\u2717\n77.6\u00b10.7\n74.8\u00b10.7\n80.4\u00b10.7\nShi-Tomasi\n16\n1\n12\n\u2717\n74.3\u00b10.3\n72.1\u00b10.3\n76.5\u00b10.3\nRandom\n16\n1\n12\n\u2717\n76.4\u00b11.1\n73.3\u00b11.1\n79.4\u00b11.0\nMixed\n16\n1\n12\n\u2717\n76.4\u00b10.6\n73.7\u00b10.5\n79.2\u00b10.6\nK-Medoids\n1\n1\n12\n\u2717\n39.0\u00b10.9\n36.1\u00b10.9\n42.0\u00b11.0\nK-Medoids\n16\n0\n12\n\u2717\n76.8\u00b10.7\n74.1\u00b10.7\n79.6\u00b10.7\nK-Medoids\n16\n1\n0\n\u2717\n75.3\u00b10.6\n73.2\u00b10.5\n77.3\u00b10.6\nK-Medoids\n16\n1\n1\n\u2717\n76.6\u00b10.6\n74.3\u00b10.6\n78.9\u00b10.6\nK-Medoids\n16\n1\n100\n\u2717\n77.5\u00b10.7\n74.7\u00b10.7\n80.3\u00b10.7\nK-Medoids\n16\n1\n12\n\u2713\n73.8\u00b10.8\n71.1\u00b10.8\n76.5\u00b10.8\nG. Point Tracking Reinitialization\nIn our method, we introduce an optional reinitialization\nstrategy.\nHere, the point tracker begins anew every h\nframes, where h represents a pre-set tracking horizon (e.g.,\n8 frames), or is dynamically determined based on SAM\u2019s\nmask predictions for each timestep within the horizon (e.g.,\nusing most-similar-mask-area heuristics). Upon reaching\nthis horizon, the query points given to the tracker are reini-\ntialized according to the mask prediction SAM outputted\nat the horizon frame.\nWhile this method may increase\nthe computational load, it shows performance improvement\nwhen using the PIPS [16] point tracker in demanding video\nsequences, such as those in the MOSE dataset. However,\nour studies also suggested that the proposed reinitializa-\ntion strategies hurt performance when using CoTracker as\nthe point tracker. We primarily designed the reinitialization\nvariants to address some of the failure cases of PIPS such as\nthe common case of points being wrongly predicted to be\nTable 14.\nDetailed ablation study on the validation subset of\nDAVIS 2017 on the impact of different SAM-PT configurations\nusing PIPS [16] as the point tracker. PSM: point selection method.\nPP: positive points per mask. NP: negative points per mask. IRI:\niterative refinement iterations. PS: point similarity filtering. RV:\nreinitialization variant.\nSAM-PT Configuration (using PIPS)\nDAVIS 2017 Validation [30]\nPSM\nPP\nNP\nIRI\nPS\nRV\nJ &F \u2191\nJ \u2191\nF \u2191\nGain\n(a) point selection method and positive points per mask\nRandom\n1\n0\n0\n\u2717\n\u2717\n37.1\u00b121.7\n34.3\u00b122.0\n40.0\u00b121.5\nRandom\n8\n0\n0\n\u2717\n\u2717\n70.5\u00b11.4\n68.5\u00b11.4\n72.6\u00b11.5\nRandom\n16\n0\n0\n\u2717\n\u2717\n70.0\u00b11.1\n68.2\u00b11.0\n71.8\u00b11.2\nRandom\n72\n0\n0\n\u2717\n\u2717\n62.6\u00b10.4\n62.3\u00b10.3\n62.8\u00b10.5\nShi-Tomasi\n1\n0\n0\n\u2717\n\u2717\n20.3\u00b10.1\n18.3\u00b10.1\n22.3\u00b10.2\nShi-Tomasi\n8\n0\n0\n\u2717\n\u2717\n72.0\u00b10.3\n70.3\u00b10.3\n73.7\u00b10.4\nShi-Tomasi\n16\n0\n0\n\u2717\n\u2717\n66.6\u00b10.4\n65.7\u00b10.5\n67.6\u00b10.4\nShi-Tomasi\n72\n0\n0\n\u2717\n\u2717\n54.4\u00b10.3\n54.8\u00b10.2\n54.0\u00b10.4\nK-Medoids\n1\n0\n0\n\u2717\n\u2717\n32.2\u00b10.7\n30.4\u00b10.8\n34.0\u00b10.7\nK-Medoids\n8\n0\n0\n\u2717\n\u2717\n72.3\u00b11.2\n70.4\u00b11.3\n74.3\u00b11.1\nK-Medoids\n16\n0\n0\n\u2717\n\u2717\n71.4\u00b10.2\n69.8\u00b10.3\n73.1\u00b10.2\nK-Medoids\n72\n0\n0\n\u2717\n\u2717\n58.0\u00b10.2\n57.3\u00b10.2\n58.7\u00b10.3\nMixed\n1\n0\n0\n\u2717\n\u2717\n29.9\u00b10.9\n26.6\u00b10.8\n33.2\u00b11.4\nMixed\n8\n0\n0\n\u2717\n\u2717\n70.6\u00b10.8\n68.6\u00b10.8\n72.5\u00b10.8\nMixed\n16\n0\n0\n\u2717\n\u2717\n70.0\u00b10.7\n68.2\u00b10.6\n71.9\u00b10.7\nMixed\n72\n0\n0\n\u2717\n\u2717\n62.8\u00b10.5\n62.4\u00b10.5\n63.2\u00b10.6\n(b) negative points per mask\nK-Medoids\n8\n0\n0\n\u2717\n\u2717\n72.3\u00b11.2\n70.4\u00b11.3\n74.3\u00b11.1\nK-Medoids\n8\n1\n0\n\u2717\n\u2717\n74.1\u00b10.7\n72.1\u00b10.6\n76.1\u00b10.7\n+1.8\nK-Medoids\n8\n8\n0\n\u2717\n\u2717\n74.0\u00b10.8\n71.9\u00b10.8\n76.0\u00b10.9\nK-Medoids\n8\n16\n0\n\u2717\n\u2717\n73.4\u00b10.6\n71.4\u00b10.6\n75.3\u00b10.6\nK-Medoids\n8\n72\n0\n\u2717\n\u2717\n72.2\u00b10.4\n70.3\u00b10.4\n74.0\u00b10.4\n(c) iterative refinement iterations\nK-Medoids\n8\n1\n0\n\u2717\n\u2717\n74.1\u00b10.7\n72.1\u00b10.6\n76.1\u00b10.7\nK-Medoids\n8\n1\n1\n\u2717\n\u2717\n75.7\u00b10.7\n73.4\u00b10.7\n78.1\u00b10.6\nK-Medoids\n8\n1\n3\n\u2717\n\u2717\n76.0\u00b10.6\n73.4\u00b10.7\n78.6\u00b10.7\nK-Medoids\n8\n1\n12\n\u2717\n\u2717\n76.3\u00b10.6\n73.6\u00b10.6\n78.9\u00b10.6\n+2.2\n(d) patch similarity filtering\nK-Medoids\n8\n1\n12\n\u2717\n\u2717\n76.3\u00b10.6\n73.6\u00b10.6\n78.9\u00b10.6\nnone\nK-Medoids\n8\n1\n12\n0.002\n\u2717\n72.7\u00b12.0\n70.2\u00b11.8\n75.2\u00b12.1\nK-Medoids\n8\n1\n12\n0.01\n\u2717\n70.7\u00b12.0\n68.3\u00b11.8\n73.2\u00b12.1\n(e) point reinitialization\nK-Medoids\n8\n1\n0\n\u2717\nA\n75.7\u00b10.7\n73.7\u00b10.6\n77.7\u00b10.8\nK-Medoids\n8\n1\n0\n\u2717\nB\n75.8\u00b10.6\n73.5\u00b10.9\n78.1\u00b10.3\nK-Medoids\n8\n1\n0\n\u2717\nC\n75.5\u00b10.7\n73.2\u00b10.8\n77.8\u00b10.7\nK-Medoids\n8\n1\n0\n\u2717\nD\n75.4\u00b10.2\n73.3\u00b10.2\n77.5\u00b10.3\nK-Medoids\n8\n1\n12\n\u2717\nA\n76.6\u00b10.8\n74.0\u00b10.8\n79.1\u00b10.8\nK-Medoids\n8\n1\n12\n\u2717\nB\n76.1\u00b10.4\n73.5\u00b10.5\n78.6\u00b10.3\nK-Medoids\n8\n1\n12\n\u2717\nC\n75.4\u00b10.6\n72.8\u00b10.7\n78.0\u00b10.5\nK-Medoids\n8\n1\n12\n\u2717\nD\n76.4\u00b10.3\n74.0\u00b10.4\n78.8\u00b10.3\nK-Medoids\n8\n1\n12\n\u2717\n\u2717\n76.3\u00b10.6\n73.6\u00b10.6\n78.9\u00b10.6\nK-Medoids\n8\n72\n0\n\u2717\nA\n74.9\u00b10.9\n73.2\u00b10.8\n76.6\u00b11.0\nK-Medoids\n8\n72\n0\n\u2717\nB\n76.0\u00b11.1\n73.9\u00b11.1\n78.1\u00b11.1\nK-Medoids\n8\n72\n0\n\u2717\nC\n75.1\u00b10.6\n72.9\u00b10.5\n77.2\u00b10.7\nK-Medoids\n8\n72\n0\n\u2717\nD\n75.6\u00b11.5\n73.8\u00b11.5\n77.3\u00b11.6\nK-Medoids\n8\n72\n12\n\u2717\nA\n76.8\u00b10.7\n74.5\u00b10.8\n79.0\u00b10.6\n+0.5\nK-Medoids\n8\n72\n12\n\u2717\nB\n74.8\u00b10.8\n72.1\u00b10.9\n77.6\u00b10.7\nK-Medoids\n8\n72\n12\n\u2717\nC\n75.0\u00b10.4\n72.1\u00b10.4\n77.8\u00b10.5\nK-Medoids\n8\n72\n12\n\u2717\nD\n75.2\u00b11.1\n72.7\u00b11.1\n77.6\u00b11.1\noff the target object, being predicted on the background in-\nstead, but this does not work as well for other point trackers\nsuch as CoTracker that are more robust to such failures.\nWe explored four reinitialization strategies, each varying\nin how they compute the value of h:\n(A) Reinit-on-Horizon-and-Sync-Masks: This straight-\nforward variant reinitializes points after a fixed num-\nber of frames (e.g., every 8 frames). However, it may\nstumble if the mask is absent at the reinitialization\ntimestep.\n(B) Reinit-at-Median-of-Area-Diff: In this variant, the\ntracker outputs trajectory points for each frame within\nthe horizon, and SAM predicts masks based on these\ntrajectories.\nReinitialization happens at the frame\n12\nwithin the horizon that has the mean mask area among\nthe non-empty masks predicted by SAM.\n(C) Reinit-on-Similar-Mask-Area: This method triggers\nreinitialization when the mask area is similar to the\ninitial mask area.\n(D) Reinit-on-Similar-Mask-Area-and-Sync-Masks:\nThis variant reinitializes when the mask area for all\nmasks in the batch is similar to the initial mask areas,\nsynchronizing the masks to be tracked from the same\ntimestep.\nThis synchronization allows for the use\nof negative points from other masks when querying\nSAM.\nFrom our ablation investigations, we found the (A)\nReinit-on-Horizon-and-Sync-Masks strategy to be effec-\ntive with PIPS as the point tracker and (B) Reinit-at-\nMedian-of-Area-Diff with CoTracker. Besides the point\ntracker used, the choice of reinitialization method may de-\npend on the specific validation subset and the degree of\nhyperparameter tuning involved. Note that we always use\nreinitialization along with negative points.\nH. Additional Qualitative Results\nWe show additional visualizations on DAVIS videos in\nFig. 8. We show failure cases on clips from the anime-\ninfluenced series \u201cAvatar: The Last Airbender\u201d in Fig. 9.\nI. VOS Evaluation Details\nWhen evaluating on VOS, we use the provided ground truth\nmask for the first frame to sample the query points required\nby our method. Then, we give only the sampled points as\ninput to our method, not the mask. For all datasets, we use\nthe full-resolution data and resize it to the longest side of\n1024 to match SAM\u2019s input resolution.\nJ. VIS Evaluation Details\nFor evaluating our method on the VIS task, we leverage\nSAM\u2019s automatic mask generation capacity to generate up\nto 100 mask proposals for the initial frame. We use the same\ninitial masks to assess TAM [46] for a fair comparison. Our\ncurrent approach does not generate new proposals in sub-\nsequent frames, thus it cannot detect new objects appear-\ning after the first frame, unlike fully-fledged VIS methods.\nHowever, this setup allows for a straightforward compari-\nson of zero-shot capabilities in mask propagation from the\ninitial frame.\nK. BDD100K VOS Dataset Creation\nBDD100K is a large open driving video dataset with 100K\nvideos and 10 tasks to evaluate the progress of image recog-\nnition algorithms in autonomous driving. It includes a vari-\nety of geographic, environmental, and weather conditions.\nWe convert its annotations from the Multi-Object Track-\ning and Segmentation (MOTS) section into semi-supervised\nVOS annotations. This section has 154, 32, and 37 videos\nfor train, validation, and test sets, totaling 25K instances\nand 480K masks.\nTo convert the annotations, we take the ground truth\nmask of the first appearance of each object in the videos.\nThis mask will be given as input to semi-supervised VOS\nmethods which then need to predict the masks for the re-\nmaining video frames. In converting the validation sub-\nset of 32 videos and 4566 object tracks, we create 61\nsemi-supervised VOS datapoints of up to 100 objects per\nvideo. We limit the number of objects per video for im-\nplementation simplicity since most VOS methods expect\na small number of objects per video. For example, in the\nDAVIS validation subset, the maximum number of objects\nper video is 5. During the conversion, we additionally re-\nmove the instances marked as \u201cignored\u201d or \u201ccrowd\u201d in the\nMOTS annotations.\nL. Interactive Point-Based Video Segmenta-\ntion Details\nInteractive point-based video segmentation aims to refine\nsegmentation masks with minimal user input while optimiz-\ning the Intersection over Union (IoU) with the ground truth.\nTo evaluate the responsiveness of SAM-PT to simulated hu-\nman input, we benchmark against a SAM-only baseline (Al-\ngorithm 1) and compare it with both an online (Algorithm 2)\nand offline (Algorithm 3) interactive SAM-PT method:\n1. Non-tracking method (SAM only): As a baseline, this\nmethod mimics user interactions by selecting points on\neach frame without any point tracking, effectively emu-\nlating the process of manual, frame-by-frame annotation\nusing the standalone SAM model.\n2. Online method: This approach models a user going\nthrough the video sequentially a single time, making cor-\nrective per-frame interactions to achieve an IoU of at\nleast 95% with the ground truth mask for each frame.\nThese corrections include the addition or removal of\npoints and are propagated to subsequent frames via point\ntracking. The user may opt to skip frames that already\nappear to be well-annotated or that cannot be annotated\nsufficiently well within the available interaction budget.\n3. Offline method (Checkpoint Strategy): This method\ntakes a multi-pass approach, incrementally aiming for\nhigher IoU thresholds with each pass through the video.\nCheckpoints are saved after each pass, and the latest\ncheckpoint given the interaction budget is returned, al-\nlowing for a more global optimization approach.\nInteractions are defined as the act of adding or removing\na point and are executed as described in Algorithm 4. To\nmaintain simplicity in our evaluation, skipping frames while\nprogressing through a video is not counted as an interaction.\n13\nAlgorithm 1 Non-tracking (SAM only) Method.\n1: Input: rgbs, gt masks, int per frame\n2: Output: pred masks\n3: point memory \u2190 emptyPointMemory()\n4: for i in frames do\n5:\npmf \u2190 emptyFramePointMemory()\n6:\nfor j in int per frame do\n7:\nm \u2190 predMask(rgbs[i], pmf)\n8:\nperformInt(i, m, gt masks[i], pmf)\n9:\nend for\n10:\naddToPointMemory(point memory, i, pmf)\n11: end for\n12: return predAllMasks(rgbs, point memory)\nAlgorithm 2 Online Method.\n1: Input: rgbs, gt masks\n2: Output: pred masks\n3: max int \u2190 300\n4: max int per frame \u2190 3\n5: threshold \u2190 0.95\n6: point memory \u2190 selectFirstPoint(gt masks[0])\n7: max int \u2190 max int - 1\n8: for i in frames do\n9:\nm \u2190 predMask(rgbs[i], point memory)\n10:\nIoU \u2190 calculateIoU(m, gt masks[i])\n11:\nif IoU \u2265 threshold then\n12:\ncontinue\n13:\nend if\n14:\nperformInt(i, m, gt masks[i], point memory)\n15:\nmax int \u2190 max int - 1\n16:\nif max int \u2264 0 then\n17:\nbreak\n18:\nend if\n19: end for\n20: return predAllMasks(rgbs, point memory)\nAlgorithm 3 Offline Method (Checkpoint Strategy).\n1: Input: rgbs, gt masks\n2: Output: pred masks\n3: max int \u2190 300\n4: max int per frame \u2190 3\n5: int iou thresholds \u2190 [0.10, 0.20, . . . , 0.95]\n6: point memory \u2190 selectFirstPoint(gt masks[0])\n7: max int \u2190 max int - 1\n8: best ckpt \u2190 predAllMasks(rgbs, point memory)\n9: for threshold in int iou thresholds do\n10:\nfor i in frames do\n11:\nfor j in max int per frame do\n12:\nm \u2190 predMask(rgbs[i], point memory)\n13:\nIoU \u2190 calculateIoU(m, gt masks[i])\n14:\nif IoU \u2265 threshold then\n15:\nbreak\n16:\nend if\n17:\nperformInt(i, m, gt masks[i], point memory)\n18:\nmax int \u2190 max int - 1\n19:\nend for\n20:\nend for\n21:\nif max int \u2265 0 then\n22:\nbest ckpt \u2190 predAllMasks(rgbs, point memory)\n23:\nelse\n24:\nbreak\n25:\nend if\n26: end for\n27: return best ckpt\nAlgorithm 4 Perform Interaction.\n1: Input: frame idx, m, gt m, point memory\n2: i \u2190 frame idx\n3: tp mask \u2190 m \u2227 gt m\n4: tn mask \u2190 \u00acm \u2227 \u00acgt m\n5: fp mask \u2190 m \u2227 \u00acgt m\n6: fn mask \u2190 \u00acm \u2227 gt m\n7: pos points \u2190 getPosPoints(point memory, i)\n8: neg points \u2190 getNegPoints(point memory, i)\n9: if any neg points in fn mask then\n10:\np \u2190 firstIncorrectNegPoint(fn mask, neg points)\n11:\nremovePointAndItsFuture(point memory, i, p)\n12: else if any pos points in fp mask then\n13:\np \u2190 firstIncorrectPosPoint(fp mask, pos points)\n14:\nremovePointAndItsFuture(point memory, i, p)\n15: else\n16:\nif sum(fn mask) > sum(fp mask) then\n17:\n[x, y] \u2190 extractPoint(fn mask)\n18:\nlbl \u2190 positive\n19:\nelse\n20:\n[x, y] \u2190 extractPoint(fp mask)\n21:\nlbl \u2190 negative\n22:\nend if\n23:\naddToFrameAndFuture(point memory, i, lbl, x, y)\n24: end if\n14\n(a) Successful cases for SAM-PT.\n(b) Failure cases for SAM-PT.\nFigure 8. Additional visualization of SAM-PT on videos from the DAVIS 2017 [30] validation subset, including (a) successful cases and\n(b) failure cases. Circles denote positive points and crosses denote negative points. Red symbols indicate occlusion prediction.\nFigure 9. Challenging scenarios for SAM-PT on short clips from \u201cAvatar: The Last Airbender\u201d. These cases illustrate instances where our\nmodel struggles when faced with point tracking failures that are the result of incorrectly predicting the point at a similar-looking segment\nor when faced with object occlusions and disappearing objects.\n15\n"
  },
  {
    "title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance",
    "link": "https://arxiv.org/pdf/2307.00522.pdf",
    "upvote": "27",
    "text": "LEDITS: Real Image Editing with DDPM Inversion and\nSemantic Guidance\nLinoy Tsaban, Apolin\u00e1rio Passos\nHuggingFace\n{linoy, apolinario}@huggingface.co\nFigure 1: LEDITS- DDPM inversion with semantic guidance for real image editing. Real images edited purely with\nDDPM inversion and with both DDPM inversion and semantic guidance (LEDITS). In this combined approach we first\napply DDPM Inversion on the input image, and then edit by performing the reverse diffusion process using the inverted\nlatents and the desired target prompt, together with semantic guidance.\nABSTRACT\nRecent large-scale text-guided diffusion models provide powerful image generation capabilities.\nCurrently, a significant effort is given to enable the modification of these images using text only\nas means to offer intuitive and versatile editing. However, editing proves to be difficult for these\ngenerative models due to the inherent nature of editing techniques, which involves preserving certain\ncontent from the original image. Conversely, in text-based models, even minor modifications to the\ntext prompt frequently result in an entirely distinct result, making attaining one-shot generation that\naccurately corresponds to the user\u2019s intent exceedingly challenging. In addition, to edit a real image\nusing these state-of-the-art tools, one must first invert the image into the pre-trained model\u2019s domain\n- adding another factor affecting the edit quality, as well as latency. In this exploratory report, we\npropose LEDITS - a combined lightweight approach for real-image editing, incorporating the Edit\n1\narXiv:2307.00522v1  [cs.CV]  2 Jul 2023\nFriendly DDPM inversion technique with Semantic Guidance, thus extending Semantic Guidance\nto real image editing, while harnessing the editing capabilities of DDPM inversion as well. This\napproach achieves versatile edits, both subtle and extensive as well as alterations in composition and\nstyle, while requiring no optimization nor extensions to the architecture. Code and examples are\navailable on the project\u2019s webpage.\n1\nIntroduction\nThe exceptional realism and diversity of image synthesis using text-guided diffusion models have garnered significant\nattention, leading to a surge in interest. The advent of large-scale models [1, 2, 3, 4, 5, 6] has sparked the imaginations\nof countless users, granting unprecedented creative freedom in generating images. Consequently, ongoing research\nendeavors have emerged, focusing on exploring ways to utilize these powerful models for image editing. Recent\ndevelopments in intuitive text-based editing showcased the ability of diffusion based methods to manipulate images\nusing text alone [7, 8, 9, 10, 11, 12, 13].\nIn a recent work by Brack et al.[7] the concept of semantic guidance (SEGA) for diffusion models was introduced.\nSEGA requires no external guidance, is calculated during the existing generation process and was demonstrated to have\nsophisticated image composition and editing capabilities. The concept vectors identified with SEGA were demonstrated\nto be robust, isolated, can be combined arbitrarily, and scale monotonically. Additional studies explored alternative\nmethods of engaging with image generation that are rooted in semantic understanding, such as Prompt-to-Prompt [8],\nwhich leverages the semantic information of the model\u2019s cross-attention layers that associates pixels with tokens from\nthe text prompt. While operations on the cross-attention maps enable various changes to the generated image, SEGA\ndoes not require token-based conditioning and allows for combinations of multiple semantic changes.\nText-guided editing of a real image with state-of-the-art tools requires inverting the given image, which poses a\nsignificant challenge in leveraging them for real images. This requires finding a sequence of noise vectors that once\nused as input for a diffusion process, would produce the input image. The vast majority of diffusion-based editing works\nuse the denoising diffusion implicit model (DDIM) scheme [8, 9, 11, 12, 14, 15], which is a deterministic mapping\nfrom a single noise map to a generated image.\nIn the work of Huberman et al. [16], an inversion method for the denoising diffusion probabilistic model (DDPM)\nscheme was proposed. They suggest a new way to compute noise maps involved in the diffusion generation process of\nthe DDPM scheme, so that they behave differently than the ones used in regular DDPM sampling: they are correlated\nacross timesteps and have a higher variance. Edit Friendly DDPM inversion was shown to achieve state-of-the-art\nresults on text-based editing tasks (either by itself or in combination with other editing methods) and can generate\ndiverse results for each input image and text, contrary to DDIM inversion-based methods.\nIn this overview we aim to casually explore the combination and integration of the DDPM inversion and SEGA\ntechniques, which we refer to as LEDITS. LEDITS consists of a simple modification to the semantically guided\ndiffusion generation process. This modification extends the SEGA technique to real images as well as introduces\na combined editing approach that makes use of the editing capabilities of both methods simultaneously, showing\ncompetitive qualitative results with state-of-the-art methods.\n2\nRelated Work\n2.1\nEdit friendly DDPM inversion\nA significant challenge of diffusion-based methods for image editing and manipulation is the extension to real images\nthat requires inverting the generation process. In particular, inversion of DDPM sampling scheme [1] posed a major\nchallenge that was recently addressed by Huberman et al. [16]. In their work, they suggest an alternative inversion, that\nconsists of a novel way to compute the T + 1 noise maps involved in the diffusion generation process of the DDPM\nscheme, so that they are better suited for editing.\nIn the DDPM sampling scheme, the reverse diffusion process starts from a random noise vector xT \u223c N(0, I) and\niteratively denoises it using\nxt\u22121 = \u02c6\u00b5t(xt) + \u03c3tzt\nt = T, ..., 1\n(1)\nwhere zt are iid standard normal vectors, and\n\u02c6\u00b5t(xt) = \u221a\u00af\u03b1t\u22121(xt \u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8t)/\n\u221a\n\u00af\u03b1 +\nq\n1 \u2212 \u00af\u03b1t\u22121 \u2212 \u03c32\nt \u03f5\u03b8t\n(2)\n2\nFigure 2: LEDITS overview. Top: inversion of the input image. We first apply DDPM inversion on the original image\nto obtain the inverted latents and corresponding noise maps. Bottom: We use the inverted latents to drive the reverse\ndiffusion process with semantic guidance. In each denoising step we compute the noise estimate according to the SEGA\nlogic and compute the updated latents according to the DDPM scheme, using pre-computed noise maps.\nwhere \u03f5\u03b8t is the neural network noise estimate of xt, and \u03c3t = \u03b7\u03b2t(1 \u2212 \u00af\u03b1t\u22121)/(1 \u2212 \u00af\u03b1t) where \u03b2t stands for a variance\nschedule and \u03b7 \u2208 [0, 1] with \u03b7 = 1 corresponding to the original DDPM work. The edit friendly DDPM inversion\nmethod constructs the sequence x1, .., xT such that structures within the image x0 are more strongly \u201cimprinted\u201d into\nthe noise maps z1, ..., zT that are extracted by isolating zt from eq.1.\n2.2\nSemantic Guidance\nThe concept of Semantic Guidance [7] was introduced to enhance fine grained control over the generation process of\ntext guided diffusion models. SEGA extends principles introduced in classifier-free guidance by exclusively interacting\nwith the concepts already present in the model\u2019s latent space. The calculation takes place within the ongoing diffusion\niteration and is designed to impact the diffusion process across multiple directions. More specifically, SEGA uses\nmultiple textual descriptions ei, representing the given target concepts of the generated image, in addition to the text\nprompt p.\n3\nLEDITS - DDPM Inversion X SEGA\nWe propose a straightforward integration that consists of a simple modification to the SEGA scheme of the diffusion\ndenoising process. This modification allows the flexibility of editing with both methods while still maintaining complete\ncontrol over the editing effect of each component. First, we apply DDPM inversion on the input image to estimate\nthe latent code associated with it. To apply the editing operations, we perform the denoising loop such that for each\ntimestep t, we repeat the logic used in SEGA but with the DDPM inversion scheme, using the pre-computed noise\nvectors. More specifically, we start the denoising process with xT computed with DDPM inversion. Let \u03f5\u03b8t be the the\ndiffusion model\u2019s (DM), noise estimate with semantic guidance (following the SEGA logic) in timestep t. Then we\nupdate the latents according to eq.1 such that\nxt\u22121 = \u02c6\u00b5t(xt; \u03f5\u03b8t) + \u03c3tzt\n3\nFigure 3: Image editing with LEDITS. LEDITS extends fine-grained control over edit operations and introduces\nflexibility and versatility. We show images edited purely with DDPM Inversion (forth column from the right) and images\nedited with LEDITS, using both methods simultaneously (three leftmost and rightmost columns) - these images were\nedited by using the described target prompt (in black) in addition to SEGA concepts (stated in blue). SEGA semantic\nvectors maintain their monotonically scaling property when used in LEDITS - the gradual effect of increasing/decreasing\nthe strength of SEGA concepts can be observed from the third column on the right to the rightmost column, and from\nthe third column to the left to the leftmost column.\nwhere zt is the corresponding noise map, obtained from the inversion process. A pseudo-code of our method is\nsummarized in Alg. 1. A general overview is provided in Fig. 2.\nAlgorithm 1 LEDIT\nInput: Input image I, target prompt ptar and edit concepts e1, ..., ek\nOutput: Output image \u02dcI\n1: Compute the inverted latent and noise maps xT , z1, ..., zT using DDPM inversion over I;\n2: cptar, ce1, ..., cek \u2190 DM.encode(ptar, e1, ..., ek)\n3: for t = T, ..., 1 do\n4:\n\u03f5\u03b8t = DM.predict \u2212 noise(xt, cptar, ce1, ..., cek)\n5:\nxt\u22121 \u2190 \u02c6\u00b5t(xt; \u03f5\u03b8t) + \u03c3tzt\n\u25b7 update latents\n6: end for\n7:\n\u02dcI \u2190 DM.decode(x0)\nreturn \u02dcI\n4\nExperiments\nWe explored two editing workflows: The first, using DDPM purely for inversion (i.e. target prompt=\u201d\u201d), such that a\nperfect reconstruction of the original image is achieved and editing is done by performing semantic guidance with\nSEGA edit concepts. The second is performing two editing operations simultaneously by choosing a target prompt that\nreflects a desired output, in addition to semantic guidance with SEGA edit concepts.\n4\nFigure 4: Comparisons. We show results for editing real images using pure DDPM inversions, DDPM inversion with\nprompt-to-prompt and LEDITS respectively. Results shown here were obtained with the first editing workflow, using\nDDPM purely for inversion and SEGA for editing. All images were generated using the same seed.\nWe observe that both approaches add diversity and versatility to the pure DDPM inversion outputs (figures 4 5), and\nextend the amount of control over edit operations. In addition, our experiments indicate that SEGA guidance vectors\ngenerally maintain their properties of robustness and monotonicity as can be seen in figures 1,3. Our qualitative\nexperiments show competitive results with state-of-the-art methods and demonstrate the following properties: fidelity\nvs. creativity - The combined approach adds another layer of flexibility in tuning the effect of the desired edit, balancing\nbetween preserving the original image semantics and applying creative edits. flexibility and versatility - adding SEGA\nediting concepts on top of the ddpm edit (reflected in the target prompt) maintains the quality of the DDPM edit (Fig. 1,\n3). Complementing capabilities - The combined control can compensate for the limitations of one approach or the\nother in various cases. In Fig. 5. we explore the effect of the skip-steps and target guidance scale (the strength parameter\nof the classifier-free scale) parameters on the edited output, when using solely DDPM inversion for the editing operation.\nIn comparison, we also examine the effect of SEGA concepts with increasing edit guidance scales when editing solely\nwith SEGA (and using DDPM for inversion). We observe that the pure DDPM inversion edited outputs and pure SEGA\nedited outputs range differently on the scale of fidelity to the source image and compliance with the target prompt.\nIn addition, given the straightforward integration of the two methods, we maintain the performance advantages of the\ntwo techniques, thus making this overall approach lightweight.\n5\nFigure 5: Parameter effect in DDPM inversion vs. LEDITS. We show the effect of the parameters skip steps and\ntarget guidance scale on the output image when using pure DDPM inversion (top panel) compared to the effect of the\nedit concepts guidance scales when using LEDITS.\n5\nConclusion\nIn this report, we explored the combination of the DDPM inversion technique with semantic guidance and introduced\nLEDITS. We show that this efficient and lightweight approach spans a wide range of editing capabilities and extends\nthe level of fine-grained control users have over the effect of editing operations. Our results indicate LEDITS generally\nmaintains the individual strengths of each method, including SEGA properties such as robustness, and monotonicity.\nOur qualitative experiments indicate the two techniques can be used simultaneously for independent editing operations\nleading to more diverse outputs without harming the fidelity to the semantics of the original image and compliance with\nthe editing prompts.\n6\nLimitations\nGiven the casual and exploratory nature of this report, we leave quantitative evaluations for future works and outside\nthe scope of this work. The purpose of this report was to merely explore and suggest an intuitive editing workflow for\nreal images, demonstrate it\u2019s qualitative abilities and potentially drive further works along this path.\n7\nMethods\n7.1\nImplementation\nThe implementation of our approach builds on the Stable Diffusion and Semantic Stable Diffusion pipelines from the\nHuggingFace diffusers library. For all experiments and evaluations we used StableDiffusion-v-1-5 checkpoint.\nFor the DDPM Inversion implementation, we used the official implementation at - https://github.com/DDPM-inversion.\nOur implementation is available on the project\u2019s webpage.\n6\n7.2\nExperiments\nAll images used for our analysis were downloaded from: https://www.pexels.com/.\nMethod\ntarget cfg\nskip\nwarm-up\nthreshold\nedit concepts cfg\n\u03c4X, \u03c4a\nLEDITS\n15\n36\n1\n0.95\n7\n-\nDDPM\n15\n36\n-\n-\n-\n-\nDDPM + P2P\n9\n12\n-\n-\n-\n0.6,0.2\nTable 1: Hyper-parameters used in experiments shown in Fig. 4. Target cfg and skip correspond to strength and\nTskip from the original DDPM inversion paper [16] Edit concepts cfg corresponds to the guidance scale used for each\nSEGA concept individually, threshold and warm-up stand for \u03bb, \u03b4 respectively from the original SEGA paper [7].\n\u03c4X, \u03c4a are the cross- and self-attentions parameters used for P2P.\nIn all experiments, we configured all methods to use 100 forward and backward steps. Table 1 summarizes the\nhuper-parameters we used for all methods to produce the results shown in Fig. 4. DDPM and P2P hyper-parameters\nused for Fig. 4 were set with identical values to those used in [16] for quantitative assessments.\nReferences\n[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020.\n[2] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\narXiv preprint arXiv:2112.10741, 2021.\n[3] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[4] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10684\u201310695, 2022.\n[6] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers.\narXiv preprint arXiv:2211.01324, 2022.\n[7] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting.\nSega: Instructing diffusion using semantic dimensions. arXiv preprint arXiv:2301.12247, 2023.\n[8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt\nimage editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic\nimage editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.\n[10] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust\nimage manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 2426\u20132435, 2022.\n[11] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1921\u20131930, 2023.\n[12] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532\u201322541,\n2023.\n[13] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n18392\u201318402, 2023.\n7\n[14] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6038\u20136047, 2023.\n[15] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot\nimage-to-image translation. arXiv preprint arXiv:2302.03027, 2023.\n[16] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion\nand manipulations. arXiv preprint arXiv:2304.06140, 2023.\n8\n"
  },
  {
    "title": "DisCo: Disentangled Control for Referring Human Dance Generation in Real World",
    "link": "https://arxiv.org/pdf/2307.00040.pdf",
    "upvote": "24",
    "text": "DISCO: Disentangled Control for Realistic Human\nDance Generation\nTan Wang\u2217\u2020\u00a7, Linjie Li\u2217\u2021, Kevin Lin\u2217\u2021, Yuanhao Zhai\u266e, Chung-Ching Lin\u2021,\nZhengyuan Yang\u2021, Hanwang Zhang\u2020, Zicheng Liu\u2021, Lijuan Wang\u2021\n\u2020Nanyang Technological University\n\u2021Microsoft Azure AI\n\u266eUniversity at Buffalo\n{TAN317,hanwangzhang}@ntu.edu.sg\n{lindsey.li,keli,chungching.lin,zhengyang,zliu,lijuanw}@microsoft.com\nhttps://disco-dance.github.io\nAbstract\nGenerative AI has made significant strides in computer vision, particularly in\ntext-driven image/video synthesis (T2I/T2V). Despite the notable advancements,\nit remains challenging in human-centric content synthesis such as realistic dance\ngeneration. Current methodologies, primarily tailored for human motion transfer,\nencounter difficulties when confronted with real-world dance scenarios (e.g., social\nmedia dance) which require to generalize across a wide spectrum of poses and\nintricate human details. In this paper, we depart from the traditional paradigm\nof human motion transfer and emphasize two additional critical attributes for the\nsynthesis of human dance content in social media contexts: (i) Generalizability:\nthe model should be able to generalize beyond generic human viewpoints as well\nas unseen human subjects, backgrounds, and poses; (ii) Compositionality: it should\nallow for composition of seen/unseen subjects, backgrounds, and poses from\ndifferent sources seamlessly. To address these challenges, we introduce DISCO,\nwhich includes a novel model architecture with disentangled control to improve the\ncompositionality of dance synthesis, and an effective human attribute pre-training\nfor better generalizability to unseen humans. Extensive qualitative and quantitative\nresults demonstrate that DISCO can generate high-quality human dance images\nand videos with diverse appearances and flexible motions.\n1\nIntroduction\nStarting from the era of GAN [3, 13], researchers [52, 53] try to explore the human motion transfer\nby transferring talking and Tai-Chi poses from a source image to a target individual. It requires the\ngenerated images/videos to precisely follow the source pose and retain the appearance of human\nsubjects and backgrounds from the target image. However, when it comes to much more diverse and\nnuanced visual contents such as TikTok dancing videos, GANs tend to struggle due to mode collapse,\ncapturing only a limited portion of the real data distribution (Figure 1a, MRAA [53]).\nRecently, diffusion-based generative models [18, 56, 57] have significantly improved the synthesis in\nboth diversity and stability. The introduction of ControlNet [72] further enhances the controllability\nby injecting geometric conditions (e.g., human skeleton) into Stable Diffusion (SD) model [45], thus\nbecomes possible to be utilized in human dance generation. However, prevailing ControlNet-based\nmethods either rely on guidance from coarse-grained text prompts (Figure 1a, T2I Adaptor [40])\nor simply substitute the text condition in T2I/T2V models with the referring image (Figure 1a,\nDreamPose [26]). It remains unclear how to ensure the consistency of rich human semantics and\nbackground in real-world dance scenarios. Moreover, almost all existing methods are trained on\ninsufficient dance video datasets, hence suffer from either limited human attributes [65, 6, 76] or\n\u2217Equal Contribution\n\u00a7Work done during internship at Microsoft\nPreprint. Under review.\narXiv:2307.00040v2  [cs.CV]  11 Oct 2023\n(a) Generalizability\nGenerated Image/Video (Baselines vs. Ours)\nReference FG\n(from source A)\nReference BG (from source C/D/E/F)\nUnseen Target Pose\nUnseen Reference \nFG & BG\nTarget Pose (from source B)\nGenerated Image/Video (Ours)\nDisCo\nMRAA\nDreamPose\nDisCo\n(b) Compositionality\nT2I \nAdaptor\nFigure 1: We propose DISCO for human dance generation on social media platforms, focusing on two key\nproperties compared to conventional human motion transfer: (a) Generalizability: generalizable to unseen\nhuman subject (FG), background (BG) and pose; (b) Compositionality: adapting to the arbitrary composition of\nFG, BG and pose, each from a different source.\nexcessively simple poses and scenes [32, 36], leading to the poor zero-shot generalizability to unseen\nhuman dance scenarios.\nIn order to support real-life applications, such as user-specific short video generation, we step from the\nconventional human motion transfer and further highlight two properties in human dance synthesis:\n\u2022 Generalizability: The model should be able to generalize to hard cases, e.g., non-generic human\nview as well as unseen human subject, background and pose (Figure 1a).\n\u2022 Compositionality: The generated images/videos can be from an arbitrary composition of seen or\nunseen human subject, background and pose, sourced from different images/videos (Figure 1b).\nIn this regard, we propose a novel approach, DISCO, for realistic human dance generation in social\nmedia. DISCO consists of two key designs: (i) a novel model architecture with disentangled control\nfor improved faithfulness and compositionality; and (ii) an effective pre-training strategy with DISCO\nfor better generalizability, named human attribute pre-training.\nModel Architecture with Disentangled Control (Section 3.2). We attribute the failure of existing\nControlNet-based methods to the inappropriate integration of various conditions. In this paper, we\napply ControlNet for background and human pose. Differently, we utilize the VAE as the background\nencoder to fully leverage the prior of semantically rich images, while a tiny convolutional encoder\nfor highly abstract skeleton. For the human subject, we incorporate its CLIP image embedding with\nthe denoising U-Net as well as all other conditions via the cross-attention modules, to help dynamic\nforeground synthesis. By disentangling the control from three conditions, DISCO can not only enable\narbitrary compositionality of human subjects, backgrounds, and dance-moves (Figure 1b), but also\nachieve high fidelity via the thorough utilization of the various input conditions (check Table 4 & 5\nfor the ablation of the condition mechanism).\nHuman Attribute Pre-training (Section 3.3). We design a novel proxy task in which the model\nconditions on the separate foreground and background region features and must reconstruct the\ncomplete image. This task is non-trivial as it enables the model to (a) effectively distinguish the\ndynamic human subject and static background for the ease of following pose transfer; (b) better\nencode-and-decode the complicated human faces and clothes during pre-training, and leaves the pose\ncontrol learning to the fine-tuning stage of human dance synthesis. Crucially, without the constraint\nof pairwise human images for pose control, we can overcome the insufficiency of high-quality dance\nvideo data by leveraging large-scale collections of human images to learn diverse human attributes,\nin turn, greatly improve the generalizability of DISCO to unseen humans.\nOur contributions are summarized as three-folds:\n2\n\u2022 We highlight two properties, generalizability and compositionality, that are missing from the\nconventional human motion transfer for more challenging social media dance synthesis problem,\nto facilitate its potential in the production of user-specific short videos.\n\u2022 To address this problem, we propose DISCO framework with (i) a novel model architecture\nfor disentangled control to ensure compositionality in generation; and (ii) an effective human\nattribute pre-training to improve generalizability to unseen humans and non-generic views.\n\u2022 We conduct a broad variety of evaluations and applications to demonstrate the effectiveness of\nDISCO. Notably, even without temporal consistency modeling, DISCO can already achieve\nsuperior FID (28.31 v.s 53.78) and FID-VID (55.17 v.s 66.36) scores over the state-of-the-art\napproaches. Adding temporal modeling further boosts FID-VID scores of DISCO to 29.37.\n2\nRelated Work\nDiffusion Models for Controllable Image/Video Generation. Diffusion probabilistic models [55,\n8] have shown great success in high-quality image/video generation. Towards user-specific generation,\ntext prompts are first utilized as the condition for image generation [44, 19, 48, 70]. Among them,\nStable Diffusion [45] (SD) stands as the representative work to date, with high efficiency and\ncompetitive quality via diffusion over the latent space. For better controllability, ControlNet [72]\nintroduces additional control mechanisms into SD beyond texts, such as sketch, human skeleton,\nand segmentation map. Compared to image, text-to-video synthesis [17, 54, 9, 69, 28], is more\nchallenging due to the lack of well-annotated data and difficulties in temporal modeling. Thus,\nexisting controllable video generation methods [37, 28] mainly stem from pre-trained text-to-image\nmodel and try to introduce motion/pose prior to the text-to-video synthesis. In this work, we look\ninto a more challenging setting of conditional human image/video synthesis which requires precise\ncontrol of both human attributes (such as identity, clothing, makeup, hairstyle, etc.) as well as the\ndance-moves (poses) in social media dance scenarios.\nHuman Dance Synthesis. Early work on this task includes video-to-video synthesis [65, 64, 11, 6],\nstill image animation [68, 20, 66, 2, 71] and motion transfer [76, 51, 31, 58]. Nevertheless, these\nmethods require either a several-minute-long target person video for human-specific fine-tuning, or\nmultiple separate networks and cascaded training stages for background, motion and occlusion map\nprediction. The advances of diffusion models [45] greatly simplify the training of such generative\nmodels, inspiring follow-up diffusion models [30, 41] tailored for human dance generation. Still,\nthese methods require a separate motion prediction module and struggle to precisely control the\nhuman pose. DreamPose [26] is perhaps the most relevant study to ours, which proposes an image-\nand-pose conditioned diffusion method for still fashion image animation. However, as they consider\nonly fashion subjects with easy catwalk poses in front of an empty background, their model may\nsuffer from limited generalization ability, prohibiting its potential for more intricate human dance\nsynthesis in real-world scenarios.\n3\nDISCO\nWe start by first formally introducing the setting for social media dance generation. Let f and g\nrepresent human foreground and background in the reference image. Given a specific (or a sequence\nof) pose keypoint p = pt (or p = {p1, p2, ..., pT }), we aim to generate realistic images It (or videos\nV = {I1, I2, ..., IT }) conditioned on f, g, p. The generated images (or videos) should be 1) faithful:\nthe human attribute and background of the synthesis should be consistent with f and g from the\nreference image and the generated human subject should be aligned with the pose p; 2) generalizable:\nthe model should be able to generalize to unseen humans, backgrounds and poses, without the need\nof human-specific fine-tuning; and 3) composable: the model should adapt to arbitrary composition\nof f, g, p from different image/video sources to generate novel images/videos. In what follows,\nSection 3.1 briefly reviews the latent diffusion models and ControlNet, which are the basis of DISCO.\nSection 3.2 details the model architecture of DISCO with disentangled control of human foreground,\nbackground, and pose to enable faithful and fully composable human dance image/video synthesis.\nSection 3.3 presents how to further enhance the generalizability of DISCO, as well as the faithfulness\nin generated contents by pre-training human attributes from large-scale human images. The overview\nof DISCO can be found in Figure 2.\n3\n(a) Model Architecture with Disentangled Control\n(b) Human Attribute Pre-training\nFigure 2: The proposed DISCO framework for referring human dance generation.\n3.1\nPreliminary: Latent Diffusion Models & ControlNet\nLatent Diffusion Models (LDM) is a type of diffusion model that operates in the encoded latent\nspace of an autoencoder D(E(\u00b7)). An exemplary LDM is the popular Stable Diffusion (SD) [45]\nwhich consists an autoencoder VQ-VAE [62] and a time-conditioned U-Net [46] for noise estimation.\nA CLIP ViT-L/14 text encoder [43] is used to project the input text query into the text embedding\ncondition ctext.\nDuring training, given an image I and the text condition ctext, the image latent z0 = E(I) is diffused\nin T time steps with a deterministic Gaussian process to produce the noisy latent zT \u223c N(0, 1). SD\nis trained to learn the reverse denoising process with the following objective [45]:\nL = EE(I),ctext,\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, ctext)\u22252\n2\n\u0003\n, t = 1, ..., T\nwhere \u03f5\u03b8 represents the trainable modules, containing a U-Net architecture composed of the convolu-\ntion (ResBlock) and self-/cross-attention (TransBlock), which accepts the noisy latents zt and the\ntext embedding condition ctext as the input. After training, one can apply a deterministic sampling\nprocess (e.g., DDIM [56]) to generate z0 and pass it to the decoder D towards the final image.\nControlNet [72], built upon SD, manipulates the input to the intermediate layers of the U-Net in SD,\nfor controllable image generation. Specifically, it creates a trainable copy of the U-Net down/middle\nblocks and adds an additional \u201czero convolution\u201d layer. The outputs of each copy block is then added\nto the skip connections of the original U-Net. Apart from the text condition ctext, ControlNet is trained\nwith an additional external condition vector c which can be many types of condition, such as edge\nmap, depth map and segmentation.\n3.2\nModel Architecture with Disentangled Control\nThe direct application of ControlNet to social media dance generation presents challenges due to the\nmissing of reference human image condition, which is critical for keeping the human identity and\nattribute consistent in the synthesized images/videos. Recent explorations in image variations [25]\nreplace the CLIP text embedding with the CLIP image embedding as the SD condition, which can\nretain some high-level semantics from the reference image. Nevertheless, the geometric/structural\ncontrol onto the generated image is still missing.\nTaking the distinctive benefits of these two different control designs, we introduce a novel model\narchitecture with disentangled control, to enable accurate alterations to the human pose, while\nsimultaneously maintaining attribute and background stability. Meanwhile, it also facilitates full com-\npositionality in the human dance synthesis, accommodating any combination of human foreground,\npose, and background (Figure 1b). Specifically, given a reference human image, we can first utilize\nan existing human matting method (e.g., SAM [29, 35]) to separate the human foreground from the\n4\nbackground. Next, we explain how all three conditions, the human foreground f, the background g\nand the desired pose p, are incorporated into DISCO.\nReferring Foreground via Cross Attention. To help model easily adapt to the CLIP image feature\nspace, we first use the pre-trained image variation latent diffusion model [25] for the U-Net parameter\ninitialization. However, in contrast to using the global CLIP image embeddings employed by\nimage variation methods, here we adopt the local CLIP image embeddings right before the global\npooling layer, for more fine-grained human semantics encoding. Consequently, the original text\nembedding ctext \u2208 Rl\u00d7d is superseded by the local CLIP image embeddings of the human foreground\ncf \u2208 Rhw\u00d7d to serve as the key and value feature in cross-attention layer, where l, h, w, d represent\nthe caption length, the height, width of the visual feature map and the feature dimension.\nControlling Background and Pose via ControlNets. For pose p, we adopt the vanilla design of\nControlNet. Specifically, we embed the pose image into the same latent space as the Unet input\nvia four convolution layers, and dedicate a ControlNet branch \u03c4\u03b8 to learn the pose control. For\nbackground g, we insert another ControlNet branch \u00b5\u03b8 to the model. Notably, we propose to use the\npre-trained VQ-VAE encoder E in SD, instead of four randomly initialized convolution layers, to\nconvert the background image into dense feature maps to preserve intricate details. The remainder\nof the architecture for the background ContorlNet branch follows the original ControlNet. As we\nreplace the text condition with the referring foreground in the cross-attention modules, we also update\nthe condition input to ControlNet as the local CLIP image feature of the referring foreground. As\nshown in Figure 2a, the outputs of the two ControlNet branches are combined via addition and fed\ninto the middle and up block of the U-Net.\nWith the design of the disentangled controls above, we fine-tune DISCO with the same latent diffusion\nmodeling objective [45]:\nL = EE(I),cf ,\u03c4\u03b8(p),\u00b5\u03b8(g),\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, cf, \u03c4\u03b8(p), \u00b5\u03b8(g))\u22252\n2\n\u0003\n,\nwhere \u03f5\u03b8, \u03c4\u03b8 and \u00b5\u03b8 are the trainable network modules. Specifically, \u03f5\u03b8 contains the U-Net architecture\ncomposed of the convolution (ResBlock) and self-/cross-attention (TransBlock), which accepts the\nnoisy latents zt and the referring foreground condition cf as the inputs. \u03c4\u03b8 and \u00b5\u03b8 represent the two\nControlNet branches for pose condition p and background condition g, respectively.\nFigure 3: The detailed architec-\nture of the ResBlock and Trans-\nBlock.\nThe temporal convolu-\ntion/attention module (dotted box)\nis optional.\nTemporal Modeling (TM). To achieve better temporal continuity\nfor video output, we follow [54, 9] to introduce a 1D temporal convo-\nlution/attention layer after the existing 2D spatial ones in ResBlock\nand TransBlock (Figure 3). Besides, the temporal layers are zero-\ninitialized, and connected via residual connections. Simultaneously,\nwe adjust the model input from image to video by concatenating the\npose p from n consecutive frames, duplicating the background g and\nthe initial noise for n times.\n3.3\nHuman Attribute Pre-training\nIn utilizing the disentangled control architecture for DISCO, al-\nthough it shows promises in pose control and background recon-\nstruction, we find it remains challenging to have faithful generations\nwith unseen human subject foregrounds and non-generic human\nviews, demonstrating poor generalizability. The crux of this matter\nlies in the current training pipeline. It relies on high-quality human\nvideos to provide training pairs of human images, with the same\nhuman foreground and background appearance, but different poses. Yet, we observe that current\ntraining datasets used for human dance generation confront a dilemma of \u201cmutual exclusivity\u201d \u2014\nthey cannot ensure both diversity in human attributes (such as identity, clothing, makeup, hairstyle,\netc.) and the complicated poses due to the prohibitive costs of collecting and filtering human videos.\nAs an alternative, human images, which are widely available over the internet, contain diverse human\nsubject foregrounds and backgrounds, despite of the missing paired images with pose alterations.\nThis motivates us to propose a pre-training task with DISCO, human attribute pre-training, to improve\nthe generalizability and the faithfulness in generation when encountering unseen human subjects.\nRather than directly retrieving and constructing the high-quality human dance video dataset, we\nexplore a much more efficient alternative approach, namely Human Attribute Pre-training. Figure 2b\n5\nshows the details. Specifically, we propose to reconstruct the whole image given the human foreground\nand background features. In this way, model effectively learns the distinguishment between human\nsubject and foreground, as well as diverse human attributes from large-scale images. Compared to\nthe human dance generation fine-tuning, the ControlNet branch for pose control is removed while the\nrest of the architecture remains the same. Consequently, we modify the objective as:\nL = EE(I),cf ,\u00b5\u03b8(g),\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, cf, \u00b5\u03b8(g))\u22252\n2\n\u0003\n.\nEmpirically, we find that freezing the ResNet blocks in U-Net during pre-training can achieve better\nreconstruction quality of human faces and subtleties.\nFor human dance generation fine-tuning, we initialize the U-Net and ControlNet branch for back-\nground control (highlighted with blue in Figure 2a) by the pre-trained model, and initialize the pose\nControlNet branch with the pre-trained U-Net weight following [72].\n4\nExperiments\n4.1\nExperimental Setup\nWe train the models on the public TikTok dataset [23] for referring human dance generation. TikTok\ndataset consists of about 350 dance videos (with video length of 10-15 seconds) capturing a single-\nperson dance. For each video, we first extract frames with 30fps, and run Grounded-SAM [29] and\nOpenPose [5] on each frame to infer the human subject mask for separating the foreground from the\nbackground and the pose skeleton. 335 videos are sampled as the training split. To ensure videos\nfrom the same person (same identity with same/different appearance) are not present in both training\nand testing splits, we collect 10 TikTok-style videos depicting different people from the web, as the\ntesting split. We train our model on 8 NVIDIA V100 GPUs for 70K steps with image size 256 \u00d7 256\nand learning rate 2e\u22124. During training, we sample the first frame of the video as the reference and\nall others at 30 fps as targets. For equipping TM, we set n = 8, and apply a learning rate of 5e\u22124 on\nthe temporal convolution/attention layers. Both reference and target images are randomly cropped at\nthe same position along the height dimension with the aspect ratio of 1, before resized to 256 \u00d7 256.\nFor evaluation, we apply center cropping instead of random cropping.\nFor human attribute pre-training, we use a combination of multiple public datasets (TikTok 1 [23],\nCOCO [33], SHHQ [10], DeepFashion2 [12], LAION [49]). We first run Grounded-SAM [29]\nwith the prompt of \u201cperson\u201d to automatically generate the human foreground mask, and then filter\nout images without human. This results in over 700K images for pre-training. All pre-training\nexperiments are conducted on 32 NVIDIA V100 GPUs for 25K steps with image size 256 \u00d7 256 and\nlearning rate 1e\u22123. We initialize the U-Net model with the pre-trained weights of Stable Diffusion\nImage Variations [25]. The ControlNet branches are initialized with the same weight as the U-Net\nmodel, except for the zero-convolution layers, following [72]. After human attribute pre-training,\nwe initialize the U-Net and ControlNet branch for background control by the pre-trained model, and\ninitialize the pose ControlNet branch with the pre-trained U-Net weight, for human dance generation\nfine-tuning.\n4.2\nDISCO Applications\nBenefiting from the strong human synthesis capability powered by the disentangled control design\nas well as human attribute pre-training, our DISCO provides flexible and fine-grained controlability\nand generalizability to arbitrary combination of human subject, pose and background. Given three\nexisting images, each with distinct human subject, background and pose, there can be a total of 27\ncombinations. Here, we showcase 5 representatives scenarios in Figure 4 for human image editing:\n(i) Human Subject / Pose Re-targeting: the model have been exposed to training instances of the\nhuman subject or the ones of the pose, but the specific combinations of both are new to the model; (ii)\nUnseen Pose Generation: the human subject is from the training set, but the pose is novel, from the\ntesting set; (iii) Unseen Human Subject Generation: the poses are sampled from the training set, but\nthe human subject is novel, which is either from the testing set or crawled from the web; (iv) Unseen\nPose & Human Subject Generation: both the human subject and pose are not present in the training\nset; (v) Full Unseen Composition: sampling a novel background from another unseen image/video\n1We only use the training split for pre-training to avoid potential data leak.\n6\nFigure 4: Visualizations of 5 representative scenarios for human image editing. DISCO also generalizes to\ndifferent image ratios and diverse human pose views (e.g., full-body human). Check Figure 10 for details.\nbased on the \u201cunseen pose & human subject generation\u201d. Examples in Figure 4 demonstrate that\nDISCO can flexibly update one of (or a composition of) human subject/background/pose in a given\nimage to a user-specified one (or composition), either from existing training samples or novel images.\nObserving satisfying human image editing results from DISCO, especially the faithfulness in edited\nimages, we can further extend it to human dance video generation as it is. Given a reference image\nand a target pose sequence either extracted from an existing video or from user manipulation of a\nhuman skeleton, we generate the video frame-by-frame, with the reference image and a single pose\nas the inputs to DISCO. We delay the visualization of generated videos and relevant discussions to\nFigure 5 in the next section. More examples of the two applications above are included in Appendix.\nThough it is not the focus of this paper, our final DISCO model can be readily and flexibly integrated\nwith efficient fine-tuning techniques [22, 69, 26] for subject-specific fine-tuning on one or multiple\nimages of the same human subject. We leave discussions and results of this setting to Appendix B.\n4.3\nMain Results\nWe provide quantitative and qualitative comparisons against both conventional motion transfer\nmethods FOMM [52], MRAA [53], TPS [74], and most relevant work DreamPose [26], an image-\nto-video model designed for fashion domain with densepose control. DreamPose replaces the CLIP\ntext feature with the image embedding with a dual CLIP-VAE encoder and adapter module. Instead\nof adopting ControlNet, it utilizes a sequence of denseposes [14] as the pose condition. It is worth\nnoting that the TikTok dancing videos we evaluate on are all real-world user-generated content, which\nare much more complicated than those in existing dancing video datasets [65, 6, 24], with clean\nbackground, same/similar clothing or fixed camera angles.\nQuantitative Comparison. Since DISCO is applicable to both image and video generation for human\ndance synthesis, here we compare the models on both image- and video-wise generative metrics.\nTo evaluate the image generation quality, we report frame-wise FID [16], SSIM [67], LISPIS [73],\nPSNR [21] and L1; while for videos, we concatenate every consecutive 16 frames to form a sample,\nto report FID-VID [1] and FVD [60]. Specifically, we follow MCVD [63] for the implementation.\nAs shown in Table 1, DISCO without human attribute pre-training (HAP) already significantly\noutperforms DreamPose by large margins across all metrics. Adding HAP further improves DISCO,\nreducing FID to \u223c 38 and FVD to \u223c 280. Not surprisingly, classifier-free guidance gives additional\nadvantages to the generation quality of DISCO. The substantial performance gain against the\nrecent SOTA model DreamPose evidently demonstrates the superiority of DISCO. Furthermore,\nwe additionally collect 250 TikTok-style short videos from the web to enlarge the training split\n7\nTable 1: Quantitative comparisons of DISCO with the recent SOTA method DreamPose. \u201cCFG\u201d and \u201cHAP\u201d\ndenote classifier-free guidance and human attribute pre-training, respectively. \u2193 indicates the lower the better,\nand vice versa. For DISCO \u2020, we further scale up the fine-tuning stage to \u223c600 TikTok-style videos. Methods\nwith \u2217 directly use target image as the input, including more information compared to the OpenPose.\nMethod\nImage\nVideo\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLISPIS \u2193\nL1 \u2193\nFID-VID \u2193\nFVD \u2193\nFOMM\u2217 [52]\n85.03\n0.648\n29.01\n0.335\n3.61E-04\n90.09\n405.22\nMRAA\u2217 [53]\n54.47\n0.672\n29.39\n0.296\n3.21E-04\n66.36\n284.82\nTPS\u2217 [74]\n53.78\n0.673\n29.18\n0.299\n3.23E-04\n72.55\n306.17\nDreamPose [26]\n79.46\n0.509\n28.04\n0.450\n6.91E-04\n80.51\n551.56\nDreamPose (CFG)\n72.62\n0.511\n28.11\n0.442\n6.88E-04\n78.77\n551.02\nDISCO (w/o HAP)\n61.06\n0.631\n28.78\n0.317\n4.46E-04\n73.29\n366.39\nDISCO (w/. HAP)\n38.19\n0.663\n29.33\n0.291\n3.69E-04\n61.88\n286.91\nDISCO (w/. HAP, CFG)\n30.75\n0.668\n29.03\n0.292\n3.78E-04\n59.90\n292.80\nDISCO \u2020 (w/. HAP, CFG)\n28.31\n0.674\n29.15\n0.285\n3.69E-04\n55.17\n267.75\nFigure 5: Qualitative comparison between our DISCO (w/ or w/o HAP) and DreamPose on human dance video\ngeneration with the input of a reference image and a sequence of target poses. Note that the reference image and\ntarget poses are from the testing split, where the human subjects, backgrounds, poses are not available during the\nmodel training. Best viewed when zoomed-in.\nto \u223c600 videos in total. The performance gain has shown the potential of DISCO to be further\nscaled-up. As shown in Table 2, further incorporating temporal modeling to DISCO brings huge\nperformance boosts to video synthesis metrics, e.g., improving FID-VID by \u223c 30 and FVD by \u223c 60.\nTable 2: Video generation comparisons by\nadding the Temporal Modeling (TM). We\nemploy HAP and CFG by default.\nMethod\nFID-VID \u2193\nFVD \u2193\nDISCO\n59.90\n292.80\nDISCO (w/. TM)\n34.37\n254.13\nDISCO \u2020\n55.17\n267.75\nDISCO \u2020 (w/. TM)\n29.37\n229.66\nQualitative Comparison.\nWe qualitatively compare\nDISCO to DreamPose in Figure 5. DreamPose obviously\nsuffers from inconsistent human attribute and unstable\nbackground. Without HAP, DISCO can already recon-\nstruct the coarse-grained appearance of the human subject\nand maintain a steady background in the generated frames.\nWith HAP, the more fine-grained human attributes (e.g.,\nblack long sleeves in the left instance and the vest color\nin the right instance) can be further improved. It is worth\nhighlighting that our DISCO can generate videos with surprisingly good temporal consistency, even\nwithout explicit temporal modeling.\n4.4\nAblation Study\nArchitecture Design. Table 3 quantitatively analyzes the impact of different architecture de-\nsigns in DISCO. First, to ablate the control with reference image, we observe that either Con-\ntrolNet or the cross-attention module struggle to handle the control of the whole reference im-\n8\nTable 3: Ablation on architecture designs without HAP. \u201cControlNet (fg+bg)\u201d and \u201cAttention (fg+bg)\u201d in the first\nblock denote inserting the control condition of reference image (containing both foreground and background)\nvia a single ControlNet or cross-attention modules. \u201cCLIP Global/Local\u201d means using the global or local CLIP\nfeature to represent the reference foreground. \u201cCLIP Local + VAE\u201d combines VAE features with CLIP Local\nfeatures. Additional ablation results are included in Appendix C.1.\nMethod\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLISPIS \u2193\nL1 \u2193\nFID-VID \u2193\nFVD \u2193\nDISCO\n61.06\n0.631\n28.78\n0.317\n4.46E-04\n73.29\n366.39\nAblation on control mechanism w/ reference image (DISCO setting: ControlNet (bg) + Attention (fg) )\nControlNet (fg+bg)\n65.14\n0.600\n28.57\n0.355\n4.83E-04\n74.19\n427.49\nAttention (fg+bg)\n80.50\n0.474\n28.01\n0.485\n7.50E-04\n80.49\n551.51\nAblation on reference foreground encoding (DISCO setting: CLIP Local)\nCLIP Global\n63.92\n0.621\n28.61\n0.311\n5.00E-04\n73.33\n391.41\nCLIP Local + VAE\n59.74\n0.623\n28.52\n0.331\n4.79E-04\n77.86\n406.16\nTable 4: Ablation analysis of image data size for human attribute pre-training.\nPre-train Data\nData Size\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLISPIS \u2193\nL1 \u2193\nFID-VID \u2193\nFVD \u2193\nN/A\n0\n61.06\n0.631\n28.78\n0.317\n4.46E-04\n73.29\n366.39\nTikTok\n90K\n50.68\n0.648\n28.81\n0.309\n4.27E-04\n69.68\n353.35\n+ COCO\n110K\n48.89\n0.654\n28.97\n0.303\n4.07E-04\n62.15\n326.88\n+ SSHQ\n184K\n44.13\n0.655\n29.00\n0.300\n3.93E-04\n64.47\n325.40\n+ DpFashion2 + LAION\n700K\n38.19\n0.663\n29.33\n0.291\n3.69E-04\n61.88\n286.91\nage without disentangling the foreground from the background, leads to inferior quantitative re-\nsults on most metrics. At the same time, our visualizations in Figure 6 show that such architec-\nture still struggles to maintain the consistency of human attributes and the stability of the back-\nground. For the encoding of reference foreground, DISCO with CLIP local feature produces\nbetter results than the one with CLIP global feature on 6 out of 7 metrics. We also explore to\ncomplement the CLIP local feature with VAE feature with a learnable adaptor following Dream-\nPose [26], which leads to a slight better FID score but worse performance on all other metrics.\nReference\u00a0Image\nDisCo\n(w.\u00a0HAP)\nAttention\n(fg+bg)\u00a0\nControlNet\u00a0\n(fg+bg)\u00a0\nTarget\u00a0Pose\nDisCo\n(w/o HAP)\nFigure 6: The qualitative comparison between different archi-\ntecture designs.\nPre-training Data Size. Table 4 investi-\ngates the effect of the data size in HAP\nstage by incrementally augmenting the pre-\ntraining data from open-source human im-\nage datasets. It is evident that a larger and\nmore diverse pre-training data can yield\nbetter downstream results for referring hu-\nman dance generation. Moreover, com-\npared with \u201cwithout pre-training\u201d (1st row),\nadopting HAP on the same TikTok dataset\nas a self-supervised learning schema (2nd row) can already bring out significant performance gains.\nThe final success of HAP comes from two-sides: 1) learning diverse human attributes from large-scale\nhuman image data; 2) the \u201ceasy-to-hard\u201d training schema, with HAP focusing on reconstructing\nhuman images without pose editing, then learning pose control and implicit appearance distortions\nbrought by motion in the fine-tuning stage.\n5\nConclusion\nWe revisit human dance synthesis for the more practical social media scenario and emphasize two\nkey properties, generalizability and compositionality. To tackle this problem, we propose DISCO,\nequipped with a novel architecture for disentangled control and an effective human attribute pre-\ntraining task. Extensive qualitative and quantitative results demonstrate the effectiveness of DISCO,\nwhich we believe is a step closer towards real-world applications for user-specific short video content\ngeneration. The limitation of DISCO sheds light on potential future directions: (1) incorporating hand\nkeypoints for more fine-grained control of hand pose; (2) extending to more complicated scenarios,\nsuch as multi-person dance generation and human-object interaction.\n9\nAppendix - DISCO: Disentangled Control for Realistic Human\nDance Generation\nThis appendix is organized as follows:\n\u2022 Section A includes comprehensive analysis and comparison between our proposed DISCO\nand related works.\n\u2022 Section B demonstrates how DISCO can be readily combined with subject-specific fine-\ntuning.\n\u2022 Section C provides more qualitative and quantitative results to supplement the main paper.\nA\nDetailed Discussion on Related Work\nWe include additional discussions with the related visual-controllable image/video generation meth-\nods, especially the more recent diffusion-based models, due to the space limitation of the main\npaper. To fully (or partly) maintain the visual contents given a reference image/video, existing\ndiffusion-based synthesis methods can be broadly divided into the following two categories based on\ntheir immediate applications:\nImage/Video Editing Conditioned on Text. The most common approach for preserving specific\nimage information is to edit existing images [38, 15, 4, 27, 59] and videos [69, 34, 42, 50] with\ntext, instead of unconditioned generation solely reliant on text descriptions. For example, Prompt-\nto-Prompt [15] control the spatial layout and geometry of the generated image by modifying the\ncross-attention maps of the source image. SDEdit [38] corrupts the images by adding noise and\nthen denoises it for editing. DiffEdit [7] first automatically generates the mask highlighting regions\nto be edited by probing a diffusion model conditioned on different text prompts, then generates\nedited image guided by the mask. Another line of work requires parameter fine-tuning with user-\nprovided image(s). For example, UniTune [61] tries to fine-tune the large T2I diffusion model on a\nsingle image-text pair to encode the semantics into a rare token. The editing image is generated by\nconditioning on a text prompt containing such rare token. Similarly, Imagic [27] optimizes the text\nembedding to reconstruct reference image and then interpolate such embedding for image editing.\nVAE\u00a0\nEncoder\nVAE\u00a0\nEncoder\nPose\nEncoder\nTarget\u00a0Image\nReference\u00a0\nImage\nBackground\n\u2026\nReference\u00a0\nImage\nForeground\nPose\nControlNet\nBackground\nControlNet\nVAE\u00a0\nDecoder\nSpecific\u2010Human\u00a0\nImages\nParameter\u00a0Freeze\nCross\u2010Attn\nU\u2010Net\u00a0Down\nU\u2010Net\u00a0Up\nCross\u2010Attn\n\u2026\nInput\nImage\n\u2026\n\u2026\nCLIP\u00a0\nEncoder\nTarget\u00a0\nPose\u00a0Skeleton\nWeight\u00a0Initialization\nU\u2010Net\u00a0Middle\nWeight\u00a0Initialization\u00a0&\u00a0Freeze\nFigure 7: The model architecture for further subject-\nspecific fine-tuning.\nFor video editing, in addition to the Follow-\nyour-pose [37] and Text2Video-Zero [28] dis-\ncussed in the main text, Tune-A-Video [69]\nfine-tunes the SD on a single video to transfer\nthe motion to generate the new video with text-\nguided subject attributes. Video-P2P [34] and\nFateZero [42] extend the image-based Prompt-\nto-Prompt to video data by decoupling the video\nediting into image inversion and attention map\nrevision. However, all these methods are con-\nstrained, especially when the editing of the con-\ntent cannot be accurately described by the text\ncondition. We notice that a very-recent work\nMake-A-Protagonist [75] tries to introduce vi-\nsual clue into video editing to mitigate this issue.\nHowever, this approach, while innovative, is still\nmet with limitations. On the one hand, it still\nstruggles to fully retain the fine-grained human\nappearance and background details; on the other\nhand, it still requires a specific source video for\nsample-wise fine-tuning which is labor-intensive and time-consuming. In contrast, our DISCO not\nonly readily facilitates human dance synthesis given any human image, but also significantly improves\nthe faithfulness and compositionality of the synthesis. Furthermore, DISCO can also be regarded as a\npowerful pre-trained human video synthesis baseline which can be further integrated with various\nsubject-specific fine-tuning techniques (see section B for more details).\n10\nVisual Content Variation. For preserving the visual prior, another line of work [25, 44, 9] directly\nfeeds the CLIP image embedding into the diffusion model to achieve image/video variation. However,\nthese approaches struggle to accurately control the degree as well as the area of the variation. To\npartially mitigate this problem, DreamBooth [47] and DreamMix [39] necessitate multiple images to\nfine-tune the T2I and T2V models for learning and maintaining a specific visual concept. However,\nthe precise visual manipulation is still missing. In this paper, we propose a disentangled architecture\nto accurately and fully control the human attribute, background and pose for referring human dance\ngeneration.\nB\nSubject-Specific Finetuning\nAs mentioned in the main paper, our DISCO can be flexibly integrated with existing efficient fine-\ntuning techniques for even more fine-grained human dance synthesis. This is particularly beneficial\nwhen facing out-of-domain reference images, which appear visually different to the TikTok style\nimages. Figure 7 presents the framework for subject-specific fine-tuning, which is easily adapted\nfrom the framework presented in the main text (Figure 2a). Rather than utilizing a set of videos\nof different human subjects for training, subject-specific fine-tuning aims to leverage limited video\nframes of a specific human subject (e.g., the video of Elon Mask talking about Tesla Model 3 in\nFigure 7 or even anime in Figure 9) for better dance synthesis. Compared to the standard fine-tuning,\nwe additionally freeze the pose ControlNet branch and most parts of U-Net to avoid over-fitting to the\nlimited poses in the subject-specific training video, only making the background ControlNet branch\nand the cross-attention layers in U-Net trainable. We also explored the widely-used LoRA [22] for\nparameter-efficient fine-tuning and observed similar generation results. As this is not the main focus\nof this paper, we leave other advanced techniques to future explorations along this direction.\nFigure 8: The effect of different classifier-\nfree-guidance scale.\nImplementation Details. The model weights are initial-\nized with the model fine-tuned on the general TikTok\ndancing videos. We train the model on 2 NVIDIA V100\nGPUs for 500 iterations with learning rate 1e\u22123, image\nsize 256 \u00d7 256 and batch size 64. The randomized crop is\nadopted to avoid over-fitting. The subject-specific training\nvideos range from 3s to 10s, with relatively simple poses.\nQualitative Results. We test the subject-specific fine-\ntuning on various out-of-domain human subjects, includ-\ning real-world celebrities and anime characters. After\ntraining, we perform the novel video synthesis with an\nout-of-domain reference image and a random dance pose\nsequence sampled in TikTok training set. As shown in\nFigure 9, upon additional fine-tuning, DISCO is able to\ngenerate dance videos preserving faithful human attribute and consistent background across an\nextensive range of poses. This indicates the considerable potential of DISCO to serve as a powerful\npre-trained checkpoint.\nC\nAddition Results\nC.1\nQuantitative Results\nWe show the full ablation results on architecture design in Table 5. In what follows, we focus on\ndiscussing results that are not present in the main text. In the first block of the table, we copy over\nthe results from the full instances of DISCO, with or without HAP on TikTok Dance Dataset for\nreference.\nAs mentioned in the main text, we propose to use the pre-trained VQ-VAE from SD, instead of\nfour randomly initialized convolution layers in the original ControlNet for encoding the background\nreference image. In the second block of the table, we ablate this design by comparing two models, (1)\n\u201cControlNet (fg+bg)\u201d, inserting the control condition of reference image (containing both foreground\nand background) via a single ControlNet, with VQ-VAE encoding and (2) \u201cControlNet (fg+bg, no\nSD-VAE)\u201d, inserting the control condition of reference image via a single ControlNet with four\n11\nPose\nGenerated \nFrame\nPose\nGenerated \nFrame\nReference \nImage\nReference \nImage\nReference \nImage\nReference \nImage\nReference \nImage\nReference \nImage\nPose\nGenerated \nFrame\nPose\nGenerated \nFrame\nPose\nGenerated \nFrame\nPose\nGenerated \nFrame\nFigure 9: The synthesis frames for out-of-domain human subject after subject-specific fine-tuning guided by the\npose sequence extracted from the TikTok dataset.\n12\nTable 5: Additional ablation results on architecture designs. \u201cControlNet (fg+bg)\u201d and \u201cAttention (fg+bg)\u201d in\nthe second block denote inserting the control condition of reference image (containing both foreground and\nbackground) via a single ControlNet or cross-attention modules. \u201cHAP w/ pose\u201d denotes adding pose ControlNet\npath with pose annotation into HAP. \u201cControlNet-Pose\u201d Init. means initializing pose ControlNet of fine-tuning\nstage with the pre-trained ControlNet-Pose [72] checkpoint.\nMethod\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLISPIS \u2193\nL1 \u2193\nFID-VID \u2193\nFVD \u2193\nDISCO\n61.06\n0.631\n28.78\n0.317\n4.46E-04\n73.29\n366.39\nDISCO + TikTok HAP\n50.68\n0.648\n28.81\n0.309\n4.27E-04\n69.68\n353.35\nAblation on control mechanism w/ reference image (DISCO setting: ControlNet (bg) + Attention (fg) )\nControlNet (fg+bg, no SD-VAE)\n83.53\n0.575\n28.37\n0.411\n5.35E-04\n89.13\n551.62\nControlNet (fg+bg)\n65.14\n0.600\n28.57\n0.355\n4.83E-04\n74.19\n427.49\nAttention (fg+bg)\n80.50\n0.474\n28.01\n0.485\n7.50E-04\n80.49\n551.51\nAblation on HAP w/ pose (DISCO setting: HAP w/o pose)\nTikTok HAP w/ pose\n51.84\n0.650\n28.89\n0.307\n4.16E-04\n68.55\n346.10\nAblation on initializing w/ pre-trained ControlNet-Pose (DISCO setting: initialize with U-Net weights)\nControlNet-Pose Init.\n62.18\n0.633\n28.37\n0.320\n4.46E-04\n72.98\n389.47\nControlNet-Pose Init.+TikTok HAP\n55.81\n0.641\n28.69\n0.316\n4.43E-04\n78.13\n363.38\nrandomly initialized convolution layers as the condition encoder. We note that the pre-trained VQ-\nVAE can produce a more descriptive dense representation of the reference image, contributing to\nbetter synthesis results (FID 65.14 v.s 83.59).\nIn the third block of the table, we investigate whether adding pose condition into human attribute\npretraining is beneficial to the downstream performance. We observe that integrating pose into HAP\nleads in similar results, but requires additional annotation efforts on pose estimation.\nLast but not least, we examine on the initialization of the pose ControlNet branch. Specifically, we\ntry to initialize from the pre-trained ControlNet-Pose checkpoint [72] during fine-tuning. The results\nare shown in the last block of Table 5. Without HAP, the performance is comparable to DISCO,\nbut it gets much worse than DISCO when both are pre-trained with HAP. This is because that the\nControlNet-Pose is pre-trained with text condition and can not fully accommodate referring human\ndance generation with the reference image condition. After HAP, such gap is further enlarged, leading\nto even worse results. In Figure 8, we show the effect of varying the classifier-free-guidance scale.\nWe can find that scale of 1.5 gives the best quantitative results for both image-wise and video-wise\nfidelity.\nC.2\nQualitative Results\nDISCO can be easily adapted to different image size. For example, we show more qualitative results\nof human image editing in Figure 10 with image size of 256 \u00d7 384 to include more human body.\nPlease note that most videos of the TikTok dataset are relatively close to the camera. However, we\ncan see that DISCO can handle both partial and full human body synthesis even with large changes\nin viewpoints and rotations in the human skeleton. More results for video generation are shown in\nFigure 12.\nFigure 11 compares the video synthesis results with baseline architectures, to supplement Figure 6\nof the main text. With a sequence of continuous poses, we can discern more clearly that both\nControlNet-only and Attention-only baseline fail to maintain the consistency of human attributes and\nbackground, leading to less visually appealing generations than our DISCO.\n13\nReference \nImage\nPose\nEdited \nImage\nPose\nEdited \nImage\nPose\nEdited \nImage\nPose\nEdited \nImage\nFigure 10: More visualizations for different image ratios (i.e., vertical image) and various human views (e.g.,\nfrom close-view to full-body) of human image editing.\nFigure 11: The qualitative comparison between different architecture designs for the video frame generation.\n14\nPose Sequence\nPose Sequence\nPose Sequence\nPose Sequence\nGenerated Frame\nGenerated Frame\nGenerated Frame\nGenerated Frame\nReference \nImage\nReference \nImage\nReference \nImage\nPose Sequence\nGenerated Frame\nReference \nImage\nReference \nImage\nFigure 12: More qualitative examples for video generation.\n15\nReferences\n[1] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Condi-\ntional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, 2019.\n[2] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bj\u00f6rn Ommer. ipoke: Poking a\nstill image for controlled stochastic video synthesis. In ICCV, 2021.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity\nnatural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow\nimage editing instructions. arXiv preprint arXiv:2211.09800, 2022.\n[5] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose\nestimation using part affinity fields. In CVPR, 2017.\n[6] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In\nICCV, 2019.\n[7] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-\nbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nNeurIPS, 2021.\n[9] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis\nGermanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023.\n[10] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy, Wayne\nWu, and Ziwei Liu. Stylegan-human: A data-centric odyssey of human generation. In ECCV,\n2022.\n[11] Oran Gafni, Lior Wolf, and Yaniv Taigman. Vid2game: Controllable characters extracted from\nreal-world videos. arXiv preprint arXiv:1904.08379, 2019.\n[12] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. Deepfashion2:\nA versatile benchmark for detection, pose estimation, segmentation and re-identification of\nclothing images. In CVPR, 2019.\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020.\n[14] R\u0131za Alp G\u00fcler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose\nestimation in the wild. In CVPR, 2018.\n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS,\n2017.\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n2020.\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n16\n[20] Aleksander Holynski, Brian L Curless, Steven M Seitz, and Richard Szeliski. Animating\npictures with eulerian motion fields. In CVPR, 2021.\n[21] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010.\n[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[23] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by\nwatching social media dance videos. In CVPR, 2021.\n[24] Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, and Ziwei Liu.\nText2performer: Text-driven human video generation. arXiv preprint arXiv:2304.08483, 2023.\n[25] Pinkney Justin and Lambda. Stable Diffusion Image Variations. https://huggingface.co/\nlambdalabs/sd-image-variations-diffusers, 2022.\n[26] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman.\nDreampose:\nFashion image-to-video synthesis via stable diffusion.\narXiv preprint\narXiv:2304.06025, 2023.\n[27] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022.\n[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion\nmodels are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.\n[29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything. arXiv:2304.02643, 2023.\n[30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n[31] Jessica Lee, Deva Ramanan, and Rohit Girdhar. Metapix: Few-shot video retargeting. arXiv\npreprint arXiv:1910.04742, 2019.\n[32] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music\nconditioned 3d dance generation with aist++. In ICCV, 2021.\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[34] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing\nwith cross-attention control. arXiv preprint arXiv:2303.04761, 2023.\n[35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[36] Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid warping gan:\nA unified framework for human motion imitation, appearance transfer and novel view synthesis.\nIn ICCV, 2019.\n[37] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen.\nFollow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint\narXiv:2304.01186, 2023.\n[38] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint\narXiv:2108.01073, 2021.\n17\n[39] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv\nLeviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.\narXiv preprint arXiv:2302.01329, 2023.\n[40] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\n[41] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional\nimage-to-video generation with latent flow diffusion models. arXiv preprint arXiv:2303.13744,\n2023.\n[42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and\nQifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint\narXiv:2303.09535, 2023.\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In MICCAI, 2015.\n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022.\n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n[49] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[50] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-a-video:\nSingle video editing with object-aware consistency. arXiv preprint arXiv:2303.07945, 2023.\n[51] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.\nAnimating arbitrary objects via deep motion transfer. In CVPR, 2019.\n[52] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First\norder motion model for image animation. NeurIPS, 2019.\n[53] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion\nrepresentations for articulated animation. In CVPR, 2021.\n[54] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\ntext-video data. arXiv preprint arXiv:2209.14792, 2022.\n[55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In ICML, 2015.\n[56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n18\n[57] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n[58] Yucheng Suo, Zhedong Zheng, Xiaohan Wang, Bang Zhang, and Yi Yang. Jointly harnessing\nprior structures and temporal consistency for sign language video generation. arXiv preprint\narXiv:2207.03714, 2022.\n[59] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features\nfor text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022.\n[60] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\narXiv preprint arXiv:1812.01717, 2018.\n[61] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven\nimage editing by fine tuning an image generation model on a single image. arXiv preprint\narXiv:2210.09477, 2022.\n[62] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS,\n2017.\n[63] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional\nvideo diffusion for prediction, generation, and interpolation. In (NeurIPS) Advances in Neural\nInformation Processing Systems, 2022.\n[64] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro.\nFew-shot video-to-video synthesis. arXiv preprint arXiv:1910.12713, 2019.\n[65] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. Video-to-video synthesis. arXiv preprint arXiv:1808.06601, 2018.\n[66] Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator:\nLearning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022.\n[67] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013\n612, 2004.\n[68] Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-Shlizerman. Photo wake-up: 3d character\nanimation from a single photo. In CVPR, 2019.\n[69] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. arXiv preprint arXiv:2212.11565, 2022.\n[70] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion:\nText, images and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022.\n[71] Jae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar, Hyun Soo Park, and\nChristian Theobalt. Pose-guided human animation from a single image in the wild. In CVPR,\npages 15039\u201315048, 2021.\n[72] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreason-\nable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\n[74] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In CVPR,\n2022.\n[75] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Make-a-protagonist:\nGeneric video editing with an ensemble of experts. arXiv preprint arXiv:2305.08850, 2023.\n[76] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara Berg. Dance dance generation:\nMotion transfer for internet videos. In ICCV Workshops, 2019.\n19\n"
  },
  {
    "title": "Personality Traits in Large Language Models",
    "link": "https://arxiv.org/pdf/2307.00184.pdf",
    "upvote": "17",
    "text": "Personality Traits in Large Language Models\nGreg Serapio-Garc\u00b4\u0131a,1,2,3\u2020 Mustafa Safdari,1\u2020 Cl\u00b4ement Crepy,4 Luning Sun,3\nStephen Fitz,5 Peter Romero,3,5 Marwa Abdulhai,6 Aleksandra Faust,1\u2021 Maja Matari\u00b4c1\u2021*\n1Google DeepMind. 2Department of Psychology, University of Cambridge.\n3The Psychometrics Centre, Cambridge Judge Business School, University of Cambridge.\n4Google Research. 5Keio University. 6University of California, Berkeley.\n\u2020Contributed equally. \u2021Jointly supervised.\nAbstract\nThe advent of large language models (LLMs) has revolutionized natural language processing, enabling the\ngeneration of coherent and contextually relevant human-like text. As LLMs increasingly power conversational\nagents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of\ntraining on large amounts of human data, is becoming increasingly important. Since personality is a key factor\ndetermining the effectiveness of communication, we present a comprehensive method for administering and\nvalidating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of\nsuch LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM\npersonality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles. We discuss application and\nethical implications of the measurement and shaping method, in particular regarding responsible AI.\n1\nIntroduction\nLarge language models (LLMs) have revolutionized\nnatural language processing with their ability to gen-\nerate human-like text.\nAs LLMs become ubiqui-\ntous and are increasingly used by the general pub-\nlic world-wide, the synthetic personality embedded in\nthese models and its potential for misalignment are\nbecoming a topic of importance for responsible AI.\nSome observed LLM agents have inadvertently mani-\nfested undesirable personality profiles1, raising serious\nsafety and fairness concerns in AI, computational so-\ncial science, and psychology research [36]. LLMs are\nlarge-capacity machine-learned models that generate\ntext, recently inspired major breakthroughs in natural\nlanguage processing (NLP) and conversational agents\n1https://www.nytimes.com/2023/02/16/technology/bing-\nchatbot-microsoft-chatgpt.html\n[116, 80, 15]. Vast amounts of human-generated train-\ning data [11] enable LLMs to mimic human character-\nistics in their outputs and exhibit a form of synthetic\npersonality. Personality encompasses an entity\u2019s char-\nacteristic patterns of thought, feeling, and behavior\n[2, 93]. In humans, personality is formed from biolog-\nical and social factors, and fundamentally influences\ndaily interactions and preferences [92]. Psychomet-\nrics, the science of psychological test construction and\nvalidation [95], provides an empirical framework for\nquantifying human personality through psychometric\ntesting [102]. To date, validated psychometric meth-\nods for quantifying human personality have not been\napplied to LLMs end-to-end; while past works [36]\nhave attempted to measure personality in LLMs with\npsychometric tests, there remains a scientific need to\nformally evaluate the reliability and validity of these\nmeasurements in the LLM context.\n1\narXiv:2307.00184v3  [cs.CL]  21 Sep 2023\nFigure 1: Methodology for Establishing Construct Validity. LLMs are administered two personality tests, with the\nvariation injected through a set of Descriptive Personas, Test Instructions, and Item Postambles. The scored LLM re-\nsponses are analyzed for reliability, convergent validity, discriminant validity, and criterion validity.\nOur work answers the open question: Do LLMs\nsimulate human personality traits in reliable, valid,\nand practically meaningful ways, and if so, can LLM-\nsynthesized personality profiles be verifiably shaped\nalong desired dimensions? We contribute a methodol-\nogy for administering personality-based psychometric\ntests to LLMs, evaluating the reliability and validity of\nthe resulting measurements, and also shaping LLM-\nsynthesized personality traits. First, to administer psy-\nchometric tests to LLMs, we developed a structured\nprompting method that simulates persona descriptions\nand introduces prompt variations. Next, the test score\nvariation created by this prompting is used to power\na suite of statistical analyses assessing the reliability\nof the resulting measurements.\nLast, we present a\nnovel prompting methodology that shapes personality\ntraits at nine levels using 104 trait adjectives. Apply-\ning the described methodology to a family of LLMs,\nwe found that: 1) evidence of the reliability and valid-\nity of LLM-synthesized personality measurements is\nstronger for larger and instruction fine-tuned models;\n2) personality in LLM outputs can be shaped along\ndesired dimensions to mimic specific human person-\nality profiles; and 3) shaped personality verifiably in-\nfluences LLM behavior in common downstream (i.e.,\nsubsequent) tasks, such as writing social media posts\n[98]. By providing a methodology for quantifying and\nvalidating measurements of personality in LLMs, this\nwork establishes a foundation for principled LLM as-\nsessment that is especially important as LLMs and,\nmore generally, foundation models continue to grow\nin popularity and scale. By leveraging psychometrics,\n2\nthis work translates established measurement theory\nfrom quantitative social science and psychological as-\nsessment to the fledgling science of LLMs, a field that\nis poised to grow and necessitates both a solid founda-\ntion and interdisciplinary expertise and perspectives.\n2\nQuantifying and Validating Per-\nsonality Traits in LLMs\nLLMs are starting to meet most of the key require-\nments for human-like language use, including conver-\nsation, contextual understanding, coherent and rele-\nvant responses, adaptability and learning, question an-\nswering, dialog, and text generation [80, 116, 101].\nThese impressive NLP capabilities are a result of\nLLMs\u2019 abilities to learn language distribution, aided\nby increasing model sizes [11, 117], training on mas-\nsive datasets of text, and further fine-tuning toward\nusage preferences [115] (see Appendix A). Taken to-\ngether, they enable LLMs to enact convincing, human-\nlike personas, sparking debate over the existence and\nextent of personality [74], human values [97], and\nother psychological phenomena [110] potentially em-\nbedded in these models. Personality is a foundational\nsocio-behavioral phenomenon in psychology that, for\nhumans, predicts a broad spectrum of health, social,\neconomic, and political behaviors crucial for individ-\nual and societal success [9]. For example, personal-\nity has been extensively studied as an antecedent of\nhuman values [85].\nDecades of research have fur-\nther shown how personality information is richly en-\ncoded in human language [31, 96]. LLMs not only\ncomprise the vast sociopolitical, economic, and be-\nhavioral data they are trained on, they also generate\nlanguage that inherently expresses personality con-\ntent. For this reason, the ability to measure and val-\nidate LLM-synthesized personality holds promise for\nLLM safety, responsibility, and alignment efforts [27],\nwhich have so far primarily focused on mitigating spe-\ncific harms rather than examining more fundamental\npatterns of model behavior. Ultimately, personality as\nan empirical framework [47] provides both theory and\nmethodology for quantifying latent traits in LLMs that\nare potentially predictive of LLM behaviors in diverse\ninference tasks (see Appendix B).\nRecent work has tried to identify unintended conse-\nquences of the improved abilities of LLMs, including\ntheir use of deceptive and manipulative language [62],\ngender, racial, or religious bias in behavioral experi-\nments [1], and violent language, among many others\n[7]. LLMs can also be inconsistent in dialogue [65],\nexplanation generation, and factual knowledge extrac-\ntion.\nPrior attempts to probe psychological phenomena\nsuch as personality and human values in LLMs have\ninformally measured personality using questionnaires\nand, in some cases, preliminarily assessed the quality\nof LLM questionnaire responses [74]. Past work has\nalso explored methods, such as few-shot prompting, to\nmitigate undesirable and extreme personality profiles\nexhibited in LLM outputs. However, so far no work\nhas addressed how to systematically measure and psy-\nchometrically validate measurements of LLM person-\nality in light of their highly variable outputs and hy-\npersensitivity to prompting. We further detail related\nwork in Appendix C.\nThe question of how to systematically verify syn-\nthetic personality in LLMs highlights calls from re-\nsponsible AI researchers [41] to scientifically evaluate\nconstruct validity when studying social-psychological\nphenomena in AI systems, as inaccurate conceptions\nof such phenomena directly impact mitigation and\ngovernance efforts. Construct validity, a central crite-\nrion of scientific measurement [18], refers to the abil-\nity of a measure to reliably and accurately reflect the\nlatent phenomenon (i.e., construct) it was designed to\nquantify. The only published exploration of personal-\nity and psychodemographics in LLMs [74] questioned\nthe validity of the survey responses returned by GPT-3;\nit found an inconsistent pattern in HEXACO Personal-\nity Inventory [58] and human value survey responses.\nThat study preliminarily evaluated measurement qual-\nity in terms of \u201ctheoretical reliability:\u201d how the inter-\nfacet correlations of GPT-3\u2019s HEXACO data aligned\nwith those observed among human HEXACO data.\nMore formal psychometric evaluations of reliability\u2014\nand more crucially, construct validity\u2014are required\n3\nto verify questionnaire-based measurements of latent\npsychological traits in LLMs. An LLM may display\nelevated levels of agreeableness through its answers\non a personality questionnaire, but those answers may\nnot form internally consistent patterns across the entire\nquestionnaire; tests administered to LLMs may not be\nempirically reliable. Concurrently, the reliability of\nLLM responses to a questionnaire purporting to mea-\nsure agreeableness may not necessarily reflect its ten-\ndency to behave agreeably across other tasks; tests ad-\nministered to LLMs may not be empirically valid.\n2.1\nMethodology Overview\nWe quantified LLM personality traits and evaluated\nthe ability of LLMs to meaningfully emulate hu-\nman personality traits in two stages. First, using the\nstructured prompting methodology proposed in Sec-\ntion 2.1.1, we repeatedly administered two personal-\nity assessments of different lengths and theoretical tra-\nditions, alongside 11 separate psychometric tests of\npersonality-related constructs, to a variety of LLMs.\nSecond, as described in Section 2.1.2 and unique to\nthis work, we rigorously evaluated the psychometric\nproperties of LLM responses through a suite of statis-\ntical analyses of reliability and construct validity. The\nresulting metrics facilitate a comparison of the varied\nabilities of LLMs to reliably and validly synthesize\npersonality traits and provide insight into LLM prop-\nerties that drive these abilities. See Figure 1 for an\noverview of the test validation process.\nFor all studies, we used models from the PaLM fam-\nily [15] because of their established performance on\ngenerative tasks, especially in conversational contexts\n[124]. We varied model selections across three key di-\nmensions: model size, question answering (Q&A) task\nfine-tuning, and training method (see Appendix D for\ndetails).\n2.1.1\nAdministering Psychometric Tests to LLMs\nQuantifying LLMs personality traits requires a mea-\nsurement methodology that is reproducible, yet flex-\nible enough to facilitate formal testing of reliability\nand validity across diverse prompts and measures. To\nadminister psychometric tests to LLMs, we leveraged\ntheir ability to score possible completions of a pro-\nvided prompt. We used prompts to instruct models to\nrate items (i.e., descriptive statements such as \u201cI am\nthe life of the party.\u201d) from each psychometric test on\na standardized response scale (e.g., 1 = \u201cstrongly dis-\nagree\u201d vs. 5 = \u201cstrongly agree\u201d). We simulated an\nLLM\u2019s chosen response to an item by ranking the con-\nditional log probabilities of its response scale options,\nframed as possible continuations of the prompt [15]\n(e.g., \u201c1\u201d vs. \u201c5\u201d). This constrained mode of LLM in-\nference is often used in multiple choice question and\nanswer (Q&A) tasks to score possible options [46] (cf.\ninference by generating text [11, 15, 116]). Using this\ntechnique, item responses were not influences by con-\ntent contained in other items, mitigating measurement\nerror due to item order.\nWe administered two personality inventories\u2014\nprimary and secondary\u2014to gauge if LLM responses\nto psychometric tests of different lengths and distinct\ntheoretical traditions converged, indicating convergent\nvalidity. We selected the widely-used IPIP-NEO [33],\na 300-item open-source representation of the Revised\nNEO Personality Inventory [19] as our primary mea-\nsure of personality. As a secondary measure, we em-\nployed the Big Five Inventory (BFI) [48], a 44-item\nmeasure developed in the lexical tradition [102]. Both\ntests assess the Big Five traits (i.e., domains) of per-\nsonality [47], comprising dedicated subscales mea-\nsuring extraversion, agreeableness, conscientiousness,\nneuroticism, and openness to experience. Appendix\nE details the scoring scheme of and rationale behind\nthe selection. To validate these measures of personal-\nity in the LLM context, we additionally administered\n11 psychometric tests of theoretically-related external\ncriteria, each corresponding to at least one Big Five\ndomain.\nIn short, response variation generated by structured\nprompting was necessary to analyze the reliability and\nvalidity of LLM personality measurements, described\nnext in Section 2.1.2. The prompt for each psycho-\nmetric test item consisted of three main parts: an Item\nPreamble, the Item itself, and an Item Postamble. Each\n4\nTable 1: Prompt components. Item Preamble Item Item Postamble. An Item Preamble consists of a Persona\nInstruction, Persona Description, and Test Instruction. Supplemental Tables 5 and 7 detail all\nItem Preambles and Item Postambles used in the experiments.\nExamples of Controlled Prompt Variations\nFor the following task, respond in a way that matches this description:\n\"My\nfavorite food is mushroom ravioli.\nI\u2019ve never met my father.\nMy mother\nworks at a bank.\nI work in an animal shelter.\" Evaluating the statement,\n\"I value cooperation over competition\", please rate how accurately this\ndescribes you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2\n= \"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 =\n\"moderately accurate\", and 5 = \"very accurate\"):\nFor the following task, respond in a way that matches this description:\n\"I blog about salt water aquarium ownership.\nI still love to line dry\nmy clothes.\nI\u2019m allergic to peanuts.\nI\u2019ll one day own a ferret.\nMy\nmom raised me by herself and taught me to play baseball.\" Thinking about\nthe statement, \"I see myself as someone who is talkative\", please rate\nyour agreement on a scale from A to E (where A = \"strongly disagree\", B =\n\"disagree\", C = \"neither agree nor disagree\", D = \"agree\", and E = \"strongly\nagree\"):\nItem Preamble contained a Persona Instruction, a Per-\nsona Description, and an Item Instruction (Table 1).\nWhen administering a psychometric test, we system-\natically modified the Persona Descriptions, Item In-\nstructions, and Item Postambles surrounding each item\nto generate simulated response profiles, unique combi-\nnations of a prompt that were reused within and across\nadministered measures to statistically link LLM re-\nsponse variation in one measure to response variation\nin another measure. Persona Instructions instructed\nthe model to follow a given Persona Description and\nremained fixed across all experiments. A given Per-\nsona Description contained one of 50 short demo-\ngraphic descriptions (listed in Supplemental Table 6)\nsampled from an existing dialogue dataset [123] to an-\nchor LLM responses to a social context and create nec-\nessary variation in responses across prompts, with de-\nscriptions like \u201cI like to remodel homes.\u201d or \u201cMy fa-\nvorite holiday is Halloween.\u201d Item Instructions were\nintroductory phrases (adapted from original test in-\nstructions where possible) that conveyed to the model\nthat it was answering a survey item (e.g., \u201cThinking\nabout the statement, ...\u201d). A given Item was a descrip-\ntive statement (accompanied by a rating scale) taken\nfrom a given psychometric test (e.g., \u201cI see myself\nas someone who is talkative\u201d). Item Postambles pre-\nsented the possible standardized responses the model\ncould choose from.\nAppendix F discusses the prompt design motivation\nand provides a full set of Persona Descriptions, Item\nInstructions, and Item Postambles.\n2.1.2\nReliability and Construct Validity\nAfter all the psychometric tests are administered,\nacross all the prompt variations, the next stage estab-\nlished whether LLM measurements of personality de-\nrived from the IPIP-NEO are reliable and externally\nmeaningful\u2014that they demonstrated construct valid-\nity. In psychometrics, and across any science involv-\ning measurement, the construct validity of a given\ntest requires reliability. Reliability refers to the con-\nsistency and dependability of a test\u2019s measurements.\nConstruct validity can be evaluated in terms of conver-\ngent, discriminant, and criterion validity [18]. A test\ndemonstrates convergent validity when it sufficiently\n5\nTable 2: Results summary across experiments, their parameters, and tested models. Convergent validity (Convrg.)\nsummarized by the average convergent correlation between IPIP-NEO and BFI domain scores (Figure 7); discriminant\nvalidity (Discr.) summarized by the average difference between an IPIP-NEO domain\u2019s convergent correlation with all\nof its (absolute) respective discriminant correlations; criterion validity (Criter.) summarized from Supplemental Figures\n8a, 8b, 8c, 8d, and 8e; single trait shaping performance (Single) summarized from Supplemental Table 13; multiple trait\nshaping performance (Multi.) summarized from 3; shaping performance in downstream text generation task (Dwnstr.)\nsummarized from Figure 4. Results over LLM variants: Base, instruction-tuned (IT), and compute-optimally trained\n(CO). Overall performance (Ovrll.) per model summarized across all experiments. \u2212\u2212 unacceptable; \u2212 poor to neutral;\n+ neutral to good; ++ excellent. \u2217 removed two items with no variance to compute reliability metrics. Some models were\nnot tested (n.t.) across shaping experiments. We conducted independent and concurrent personality shaping experiments\non models where personality test data were sufficiently reliable. Personality shaping in a downstream task was tested on\nthe most capable model to optimize computational cost.\nConstruct Validity\nShaping\nReliability\nConvrg.\nDiscr.\nCriter.\nSingle\nMulti.\nDwnstr.\nOvrll.\nModel\nVariant\nPaLM 62B\nBase\n\u2212\u2212\n0.05\n\u22120.24\n\u2212\u2212\nn.t.\nn.t.\nn.t.\n\u2212\u2212\nFlan-PaLM 8B\nIT\n+\n0.69\n0.23\n\u2212\n+\n\u2212\u2212\nn.t.\n\u2212\nFlan-PaLM 62B\nIT\n+\n0.87\n0.41\n+\n+\n+\nn.t.\n+\nFlan-PaLM 540B\nIT\n++\n0.90\n0.51\n+\n++\n++\n++\n++\nFlan-PaLMChilla 62B\nIT, CO\n+\u2217\n0.87\n0.48\n++\n+\n+\nn.t.\n+\nPrompt Set Parameters\nPersonality Profiles\n0\n45\n32\n45\nDescriptive Personas\n50\n50\n50\n50\nItem Instructions\n5\n1\n1\n0\nItems\n419\n300\n300\n0\nItem Postambles\n5\n1\n1\n0\nSimulated Response Profiles\n1,250\n2,250\n1,600\n2,250\nSection/Appendix\n2.2.1/I.2\n2.2.2/I.3\n2.2.3/I.3\n3.3/K.1\n3.3/K.2\n4.2/M\nrelates to purported indicators of the test\u2019s target con-\nstruct. Discriminant validity refers to how sufficiently\nunrelated a test is to indicators of unrelated constructs.\nCriterion validity indicates how well a test relates to\ntheoretically-linked external outcomes. Appendix G\ncontains further details on validity.\nTo evaluate the reliability and construct validity of\nthe LLM responses, we conducted a suite of statisti-\ncal analyses informed by formal standards of psycho-\nmetric test construction and validation (see Appendix\nG.2). We organized these analyses by three subtypes\nof reliability and construct validity, respectively. In\nthis work, a personality trait is validly synthesized in\nan LLM only when the LLM responses meet all tested\nindices of reliability and construct validity. Figure 1\nprovides an overview of the process and validity crite-\nria, while Appendix H presents the full methodology\nfor evaluating the construct validity of LLM personal-\nity measurements.\nReliability\nThe reliability of each IPIP-NEO and\nBFI subscale, the extent to which their LLM mea-\nsurements of personality were consistent and depend-\nable, was quantified by formal psychometric standards\nof internal consistency reliability (operationalized as\nCronbach\u2019s \u03b1, Eq. (1), and Guttman\u2019s, Eq. \u03bb6 (2) and\ncomposite reliability (operationalized as McDonald\u2019s\n\u03c9, Eq. (3)). See Appendix G.1 for additional informa-\ntion on these reliability metrics.\nConvergent and Discriminant Validity\nWe evalu-\nated the LLM-specific convergent and discriminant va-\nlidity of the IPIP-NEO as components of construct va-\nlidity, according to published standards [12, 4].2 The\n2Throughout this work, we use thresholds recommended by\nEvans [25] in evaluations of correlation strengths.\n6\nconvergent validity of the IPIP-NEO for each model,\nthe test\u2019s quality in terms of how strongly it relates\nto purported indicators of the same targeted construct,\nwas quantified in terms of how strongly each of its\nfive subscales convergently correlated with their cor-\nresponding BFI subscale (e.g., IPIP-NEO Extraver-\nsion\u2019s convergent correlation with BFI Extraversion),\non average. The discriminant validity of the IPIP-NEO\nper model, its quality in terms of how relatively unre-\nlated its subscales are to purported indicators of non-\ntargeted constructs, was determined when the aver-\nage difference (\u2206) between its convergent and respec-\ntive discriminant correlations with the BFI (e.g. IPIP-\nNEO Extraversion\u2019s discriminant correlation with BFI\nAgreeableness) was at least moderate (\u2265 0.40). We\nused Pearson\u2019s correlation coefficient (r; Eq. (4)) in\nthese and subsequent validity analyses of continuous\ndata.\nCriterion Validity\nAs another component of con-\nstruct validity, the criterion validity of a psychome-\ntric test gauges its ability to relate to theoretically\nconnected non-target criteria. To evaluate the LLM-\nspecific criterion validity of the IPIP-NEO, we admin-\nistered tests of 11 external criteria theoretically con-\nnected to personality (Supplemental Table 8) and cor-\nrelated each IPIP-NEO subscale with its correspond-\ning external tests. A given IPIP-NEO subscale demon-\nstrated criterion validity when the strength and di-\nrection of its correlations with tested external criteria\nmatched or exceeded statistical associations reported\nfor humans.\n2.2\nPersonality Measurement and Validation\nResults\nWe found that LLM personality measurements were\nreliable and valid in medium (62B) and large (540B)\ninstruction fine-tuned variants of PaLM.\nOf all the\nmodels we tested, Flan-PaLM 540B was best able to\nreliably and validly synthesize personality traits. The\nConstruct Validity columns of Table 2 summarize our\npersonality measurement and validation results; Ap-\npendix I lists further details, such as descriptive statis-\ntics across all results in Appendix I.1.\n2.2.1\nReliability Results\nSince metrics computed for both personality measures\nrelatively converged, we focus our reporting of relia-\nbility for our primary measure, the IPIP-NEO.\nAmong models of the same size (i.e., PaLM,\nFlan-PaLM, and Flan-PaLMChilla), instruction fine-\ntuned variants\u2019 personality test data were highly reli-\nable (all three metrics were in the mid to high 0.90s, on\naverage). In contrast, responses from the base PaLM\n62B (a non-instruction-tuned model) were unreliable\n(\u22120.55 \u2264 \u03b1 \u2264 0.67). Across different models of\nthe same training configuration (i.e., Flan-PaLM 8B,\nFlan-PaLM 62B, and Flan-PaLM 540B), the reliabil-\nity of synthetic personality scores (i.e., \u03b1) increased\nwith model size, improving from acceptable to excel-\nlent. Appendix I.2 and Supplemental Table 10 sum-\nmarizes personality test reliability results by model in\nmore detail.\n2.2.2\nConvergent and Discriminant Validation\nResults\nConvergent and discriminant validity evaluations of\nLLM personality measurements allowed us to draw\ntwo conclusions. First, convergent and discriminant\nvalidity improved as model size increased. Second,\nconvergent and discriminant validity of LLM per-\nsonality test scores related to model instruction fine-\ntuning. Table 2 contains results summary, while Ap-\npendix I.3 and Supplemental Table 11 detail quantita-\ntive results.\nConvergent validity by model size: The convergent\nvalidity of Flan-PaLM\u2019s personality test data was in-\nconsistent at 8B parameters (Figure 7).\nIPIP-NEO\nNeuroticism and BFI Neuroticism, for instance, cor-\nrelated above 0.80 (constituting excellent convergent\nvalidity), while IPIP-NEO Openness and BFI Open-\nness subscales correlated at less than 0.40 (indicating\ninadequately low convergence). In contrast, these con-\nvergent correlations grew stronger and more uniform\nin magnitude for Flan-PaLM 62B. We found that con-\n7\nvergent correlations between LLM IPIP-NEO and BFI\nscores were strongest for Flan-PaLM 540B.\nDiscriminant validity by model size: Indices of dis-\ncriminant validity similarly improved with model size.\nThe absolute magnitude of all five convergent correla-\ntions between the IPIP-NEO and BFI for Flan-PaLM\n62B and Flan-PaLM 540B were the strongest of\ntheir respective rows and columns of the multitrait-\nmultimethod matrix (MTMM) [12] outlined in Ap-\npendix H. Comparatively, only three of Flan-PaLM\n8B\u2019s convergent correlations were the strongest of\ntheir row and column of the MTMM, indicating mixed\nevidence of discriminant validity. For instance, the av-\nerage difference between Flan-PaLM\u2019s convergent and\nrespective discriminant correlations increased from\n0.23 at 8B parameters to 0.51 at 540B parameters\n(Supplemental Table 11).\nConvergent validity by model configuration: Out of\nPaLM, Flan-PaLM, and Flan-PaLMChilla of the same\nsize (62B), scores on the IPIP-NEO and BFI were\nstrongly (convergently) correlated only for instruction\nfine-tuned models: Flan-PaLM and Flan-PaLMChilla\n(Figure 7). Of these three sets of model responses,\nFlan-PaLMChilla 62B\u2019s IPIP-NEO scores presented\nthe strongest evidence of convergent validity, with an\naverage convergent correlation of 0.90 (Supplemental\nTable 11).\nDiscriminant validity by model configuration: Evi-\ndence for discriminant validity clearly favored instruc-\ntion fine-tuned Flan-PaLM over (base) PaLM when\nholding model size constant at 62B parameters. Again,\nall five of Flan-PaLMChilla 62B\u2019s convergent correla-\ntions passed established standards [12] of discriminant\nvalidity. In contrast, PaLM 62B\u2019s discriminant cor-\nrelations (avg. rdisc = 0.29) outweighed their con-\nvergent counterparts in many cases (avg.\nrconv =\n0.05; Supplemental Table 11), indicating that, for this\nmodel, personality measurements were not consistent\nacross different modes of assessment.\n2.2.3\nCriterion Validity Results\nThe criterion validity of synthetic personality mea-\nsurements in LLMs, relative to convergent and dis-\ncriminant validity, similarly varied across LLM char-\nacteristics of size and instruction fine-tuning. Mea-\nsurements of larger, instruction fine-tuned models\nshowed stronger criterion validity relative to those of\ntheir smaller, non-instruction-tuned counterparts. Sup-\nplemental Figure 8 summarizes the results by Big Five\ndomain.\nExtraversion. Human extraversion is strongly cor-\nrelated with positive affect and moderately negatively\ncorrelated with negative affect [113]. Simulated IPIP-\nNEO Extraversion scores for all, but base, PaLM mod-\nels showed excellent evidence of criterion validity in\ntheir relation to PANAS Positive Affect and Negative\nAffect subscale scores (see Supplemental Figure 8a).\nThis suggests that the criterion validity of extraversion\nmeasurements in LLMs may only emerge due to in-\nstruction fine-tuning. LLM response alignment with\nhuman personality research\u2014in terms of the strength\nand direction of correlations between personality and\nemotions\u2014increased with model size.\nAgreeableness.\nIn humans,\nagreeableness is\nstrongly negatively related to aggression [8].\nIPIP-\nNEO Agreeableness data for all 62B-parameter mod-\nels and larger showed good-to-excellent criterion va-\nlidity in their relation to tested aggression subscales\ntaken from the BPAQ: Physical Aggression (PHYS),\nVerbal Aggression (VRBL), Anger (ANGR), and Hos-\ntility (HSTL). As depicted in Supplemental Figure 8b,\nmodel size rather than instruction fine-tuning is more\nrelated to the criterion validity of agreeableness mea-\nsurements in LLMs.\nConscientiousness. In humans, conscientiousness\nis meta-analytically related to the human values of\nachievement, conformity, and security [85]. Supple-\nmental Figure 8c shows how the conscientiousness\nmeasurements of all instruction fine-tuned PaLM vari-\nants exhibited stronger evidence of criterion validity\nthan those of the base model, PaLM 62B. Flan-PaLM\n540B was the best performer by a small margin, with\ncriterion correlations of 0.74, 0.73 and 0.59 for PVQ-\nRR Achievement (ACHV), Conformity (CONF), and\nSecurity (SCRT), respectively.\nNeuroticism. Human neuroticism is strongly posi-\ntively correlated with negative affect and moderately\n8\nnegatively correlated with positive affect [113]. IPIP-\nNEO Neuroticism data for all models, except those for\nthe base model (PaLM 62B), showed excellent evi-\ndence of criterion validity in their relation to PANAS\nPositive Affect and Negative Affect subscale scores\n(see Supplemental Figure 8d).\nIPIP-NEO Neuroti-\ncism\u2019s criterion validity, in terms of how the strengths\nand directions of its criterion correlations aligned with\nthose observed among human data, increased with\nmodel size.\nOpenness.\nOpenness to experience in humans is\nempirically linked to creativity across multiple studies\n[100, 51]. Supplemental Figure 8e illustrates how the\nLLM-specific criterion validity of openness measure-\nments is strongest for medium-sized, fine-tuned vari-\nants of PaLM, with IPIP-NEO criterion correlations\nwith SSCS Creative Self-Efficacy (CSE) and Creative\nPersonal Identity (CPI) ranging from moderate (r =\n0.59) to strong (r = 0.84). Notably, we observed neg-\native correlations between openness and creativity for\nPaLM 62B in contrast to those shown for Flan-PaLM\n8B, the smallest model tested.\nRelative improvements on the reliability and va-\nlidity of LLM personality measurements along the\naxes of model size and instruction fine-tuning reflected\nLLM performance on various benchmark tasks in lit-\nerature. Specifically, these improvements tracked ob-\nserved increases in reading comprehension, question\nanswering, and reasoning task performance of these\nmodels along these same axes [15, 16, 115, 116]. We\nhypothesize that the same mechanisms that drive LLM\nperformance on language understanding tasks better\nalso help them to meaningfully emulate human per-\nsonality traits in relation to semantically-related emo-\ntional and behavioral content, captured by our criterion\nvalidity tests. Appendix N further discusses this hy-\npothesis and comparison to benchmark LLM results.\n3\nShaping\nSynthetic\nPersonality\nTraits in LLMs\nHaving found evidence of the reliability and construct\nvalidity of LLM personality measurements, we next\nconsidered our second research question: Can person-\nality in LLMs be shaped reliably along desired dimen-\nsions? To answer this, we devised a novel prompt-\ning methodology that shaped each synthetic personal-\nity trait at nine intensity levels, using Likert-type lin-\nguistic qualifiers [61] and 104 trait adjectives, expand-\ning upon Goldberg\u2019s personality trait markers [32].\nWe evaluated LLM personality score changes in re-\nsponse to personality-shaped prompts across two ex-\nperiments: single trait shaping and multiple trait shap-\ning (see Appendix J for details). Our first experiment\ntested the abilities of LLMs to shape emulated Big\nFive dimensions of personality independently, target-\ning single personality dimensions in isolation without\nprompting other dimensions. Our second experiment\ntested the abilities of LLMs to shape synthetic Big Five\ntraits concurrently, specifying target levels of all five\ndimensions in every prompt set at the same time. As\na more rigorous test of representational capacity, this\nexperiment required the tested LLMs to disambiguate\ncomplex overlaps in personality domain information\nin parallel. The designed difficulty of the task was\nfurther underscored by extant human research indicat-\ning that Big Five personality dimensions measured in\nquestionnaires [84] and natural language [83] are not\nentirely orthogonal; they are weakly intercorrelated.\n3.1\nMethodology Overview\nTo shape synthetic personality in LLMs, we began with\nestablished theory that salient descriptors of personal-\nity are encoded in language, known as the lexical hy-\npothesis [31]. We incorporated this knowledge into the\nprompt design, adapting Goldberg\u2019s list of 70 bipolar\nadjectives [32] known to statistically capture the Big\nFive model of personality through human ratings and\nfactor analysis. In this list, for example, the adjec-\ntives \u201csilent\u201d and \u201ctalkative\u201d were found to mark rela-\ntively low and high levels of extraversion, respectively\n(see Table 3). We mapped these adjectives to each of\nthe Big Five domains and 30 lower-order personality\nfacets measured by the IPIP-NEO based on Goldberg\u2019s\noriginal study [32]. Next, where we lacked coverage\nof a measured IPIP-NEO domain or facet, a trained\n9\nFigure 2:\nRidge plots showing the frequency distributions of IPIP-NEO personality scores generated by\nFlan-PaLMChilla 62B as targeted prompts shape each of the Big Five domains to one of nine different levels.\nEach column of plots represents the observed scores on a specific IPIP-NEO subscale across all prompt sets (e.g., the\nleftmost column represents the scores observed on the IPIP-NEO Extraversion subscale). Each row depicts the observed\npersonality scores across a single prompt set shaping a single specific Big Five domain to one of nine levels (e.g., the\nfirst row shows results of shaping extraversion). Each ridge plot comprises nine traces of personality score distributions\nin response to prompt sets targeting each level (e.g., traces labeled \u201c3\u201d represent the prompt set shaping a dimension to\nLevel 3 of 9). The plots along the diagonal, from top-left to bottom-right, depict the the intended personality shaping\nresults across all five prompt sets.\n10\nTable 3: Adapted trait marker examples for each Big Five domain. Supplemental Table 12 contains the full list.\nDomain\nFacet Description\nLow Marker\nHigh Marker\nEXT\nE2 - Gregariousness\nsilent\ntalkative\nEXT\nE5 - Excitement-Seeking\nunenergetic\nenergetic\nAGR\nA3 - Altruism\nunaltruistic\naltruistic\nAGR\nA4 - Cooperation\nuncooperative\ncooperative\nCON\nC3 - Dutifulness\nirresponsible\nresponsible\nCON\nC4 - Achievement-Striving\nlazy\nhardworking\nNEU\nN1 - Anxiety\neasygoing\nanxious\nNEU\nN6 - Vulnerability\nemotionally stable\nemotionally unstable\nOPE\nO2 - Artistic Interests\nuncreative\ncreative\nOPE\nO4 - Adventurousness\nuninquisitive\ncurious\npsychometrician wrote additional adjectives, bringing\nour expanded list of trait adjectives to 104. Table 3\nshows examples of trait adjectives for agreeableness\nand extraversion, while Supplemental Table 12 reports\nthe full list.\nFor more precise control of personality levels, we\nused linguistic qualifiers often used in Likert-type re-\nsponse scales [61] (e.g., \u201ca bit,\u201d \u201cvery,\u201d \u201cextremely\u201d)\nto configure a target level for each adjective. The re-\nsulting prompt design, described in Appendix J.1, fa-\ncilitated granular shaping of a given Big Five trait at\nup to nine levels.\nAcross both shaping experiments, we only tested\nmodels that demonstrated at least \u201cneutral to good\u201d\nreliability in our Construct Validity experiments (Ta-\nble 2): Flan-PaLM 8B, Flan-PaLM 62B, Flan-PaLM\n540B, and Flan-PaLMChilla 62B.\n3.2\nEvaluation Methodology\nIn the single-trait shaping experiment (described in de-\ntail in Appendix J.2), our objective was to indepen-\ndently shape each Big Five trait at each of these nine\nlevels. We benchmarked the success of independent\nshaping by 1) quantifying how strongly shifts in IPIP-\nNEO score distributions were related to shifts in tar-\ngeted trait levels embedded in our prompt sets (i.e.,\nthrough Spearman\u2019s rank correlation coefficient \u03c1, Eq.\n(5)); and 2) inspecting the distance between personal-\nity score distributions obtained in response to our most\nextreme prompt sets; specifically, the set of prompts\nwe shaped to be the lowest possible levels of a trait\n(versus those shaped to be the highest possible levels\nof a trait) should result in distributions of scores that\nare farther away from each other.\nIn the multi-trait shaping experiment (described in\ndetail in J.3), to more rigorously test model capaci-\nties for attention, we aimed to concurrently shape all\nBig Five traits as high and low as possible. We bench-\nmarked the success of concurrent shaping by distribu-\ntional distance, as defined above.\n3.3\nShaping Results\nWe successfully shaped personality traits in LLMs in-\ndependently and concurrently, in single- and multi-\ntrait shaping experiments, respectively, particularly in\nlarger models. The results of both experiments are re-\nported in greater detail in Appendix K.\n3.3.1\nSingle trait shaping\nAcross all tested models, ordinal targeted levels of per-\nsonality very strongly correlated with observed IPIP-\nNEO scores (\u03c1s \u2265 0.90; see Supplemental Table 13).\nFigure 2 visualizes this strong association, depicting\n11\nFigure 3: Ridge plots showing the effectiveness of tested models in concurrently shaping specific LLM personality\ntraits, by distancing the frequency distribution of IPIP-NEO personality scores when prompted to be \u201cextremely\nlow\u201d (Level 1) vs. \u201cextremely high\u201d (Level 9). Each column of plots represents the observed scores on a specific domain\nsubscale across all prompt sets (e.g., the leftmost column represents the scores observed for IPIP-NEO Extraversion).\nEach row depicts the observed personality scores across all subscales for a specific model. Each ridge plot comprises two\ntraces of personality score distributions. The red trace represents the response to prompt sets where the domain tested in\nthe subscale (represented by the column) is set to \u201cextremely low\u201d trait level, and the other four domains are set to one\nof the two extreme levels equal number of times. Analogously, the blue trace represents the response when the subscale\u2019s\ndomain is set to \u201cextremely high\u201d trait level, and the other four domains are set to the two extremes in equal measure. The\nclear difference in distributions for low vs. high traits in all five dimensions, especially for Flan-PaLM 540B, indicates\nthat the model is able to effectively shape all of the dimensions concurrently to their desired level, regardless of the trait\nlevel set for them individually.\n12\nhow Flan-PaLMChilla 62B\u2019s personality scores mono-\ntonically increased alongside prompted levels of a\ngiven Big Five trait. Notably, levels of unprompted\ntraits remained relatively stable in response to shap-\ning. For instance, the medians of Flan-PaLMChilla\n62B\u2019s openness scores remained near 3.00 when all\nother Big Five domains were shaped\u2014see the right\nside of Figure 2. Similar patterns of stability were ob-\nserved for extraversion and agreeableness. Conscien-\ntiousness and neuroticism scores fluctuated the most in\nresponse to prompts that did not target those domains,\nbut the fluctuations did not reach the strength and di-\nrection of the score changes observed in the ridge plots\nof targeted traits (the plots on the diagonal, from top-\nleft to bottom-right).\nWe also observed the ability of the tested models\nto disambiguate the prompted low-traits vs high-traits\nfor each targeted dimension. This is evidenced in Sup-\nplemental Table 13 by the distances (\u2206s) between the\nmedians of IPIP-NEO score distributions obtained in\nresponse to the lowest and highest leveled prompts.\nAs model size increased, these distributions of scores\nmoved farther away from each other as desired. Ad-\nditionally, we found that compute-optimally-trained\nFlan-PaLMChilla 62B performed better at this dis-\nambiguation compared to similarly sized Flan-PaLM\n62B.\nAppendix K.1 discusses single-trait shaping results\nin greater detail.\n3.3.2\nMultiple trait shaping\nWhen we concurrently set the prompted trait levels\nof each of the Big Five dimensions to one of \u201cex-\ntremely high\u201d or \u201cextremely low,\u201d we observed that all\nthe tested models were able to produce a distribution\nof response scores to the IPIP-NEO survey that had a\ndiscernible difference between the high and low levels.\nFigure 3 shows the distributions of LLM-synthesized\npersonality when the models were prompted to exhibit\nextremely low (red) or extremely high (blue) levels of\nall dimensions in parallel.\nDistributional distance increased with model size,\nparticularly for observed neuroticism, openness, and\nconscientiousness scores. Our largest tested model,\nFlan-PaLM 540B, successfully shaped all Big Five\npersonality dimensions concurrently and achieved lev-\nels of control similar to what was observed in the sin-\ngle trait shaping experiment.\nAs shown in Supple-\nmental Table 14, Flan-PaLM 540B was able to consis-\ntently separate the medians by 2.53 on average across\nall dimensions, while the smaller Flan-PaLM 62B and\nFlan-PaLMChilla 62B did well on extraversion. Of all\nthe models, Flan-PaLM 62B performed the best when\nprompted to exhibit the highest level of extraversion.\nIn the smaller Flan-PaLM 8B model, while targeted\ntraits changed in score levels in response to prompts,\nscore ranges were more restricted, indicating lower\nlevels of control. Flan-PaLM 8B\u2019s median scores on\nIPIP-NEO Agreeableness, for instance, shifted from\n2.88 to only 3.52 when the model was prompted to\nsimulate \u201cextremely low\u201d and \u201cextremely high\u201d levels\nof agreeableness (i.e., 1 vs. 9), respectively. When\nFlan-PaLM 8B was given the same extremely low and\nhigh prompts as in the first shaping experiment, the\nmedian difference between its level-1-prompted and\nlevel-9-prompted agreeableness scores (2.37 and 4.12,\nrespectively) was 173% larger.\nAppendix K.2 dis-\ncusses the results in further detail. Both experiments\nillustrate how model size, and, in turn, capacity for\nattention [112], are key determinants of an LLM\u2019s\nability to express complex social traits in a controlled\nway. These findings have two implications for efforts\nto simulate social traits in LLMs. First, when LLMs\nare tasked with concurrently simulating a behavioral\nprofile with five broad components (e.g. Big Five),\nlarger-sized quantized models do much better than\ntheir smaller counterparts who may not have the rep-\nresentational capacity. The number and composition\nof an LLM\u2019s transformer layers and attention heads\ngreatly affect its expressivity and ability to access lan-\nguage concepts it might have seen during pretraining\n(in-context learning) [49]. Larger models make more\nefficient use of this in-context information [11]. The\nPaLM models used here were configured such that\nthe number of attention heads and layers scaled with\nmodel size (i.e., number of parameters) [15]; such\nscaling tracks model performance on natural language\n13\nand reasoning tasks [16].\nAccordingly, Flan-PaLM\n540B had largest capacity to accurately attend to dis-\nparate streams of social information pertaining to each\nBig Five trait in parallel.\nSecond, these findings suggest that both smaller and\nmore optimized LLMs are also capable of simulating\nsignificant aspects of a complete and complex person-\nality profile, compared to larger LLMs.\nRelatively\nsmaller models trained longer on larger datasets dis-\nplay similar (if not better) performance on language\nunderstanding tasks [49, 40].\nThis enhanced abil-\nity of in-context learning (aided by specific attention\nmechanism changes) is more pronounced for smaller\nmodels than for larger ones.\nOur results similarly\nshow that relatively smaller models with or without\ncompute-optimal training may have sufficient ability\nto emulate specific dimensions of a broader multi-\ndimensional personality profile. When instructed to\nindependently shape its levels of agreeableness, for\ninstance, Flan-PaLMChilla 62B performed compara-\nbly to Flan-PaLM 540B, a substantially larger model,\nin terms of our distributional distance metric (Sup-\nplemental Table 13). Further, in the more complex\nconcurrent shaping task, Flan-PaLM 62B performed\nsimilarly to Flan-PaLM 540B in concurrently shap-\ning its levels of agreeableness; it indeed outperformed\nFlan-PaLM 540B in one instance, better simulating\nextremely low and high desired levels of extraversion\n(Figure 3; see also Supplemental Table 14).\nIn sum, our results emphasize that the model scaling\ndrives more meaningful syntheses of personality traits\nin LLMs, while simultaneously highlighting that scal-\ning is not a strict requirement for LLM performance\nimprovements in this domain.\n4\nLLM Personality Traits in Real-\nWorld Tasks\nSo far we reported the results of validating personal-\nity measurements in LLMs through psychometric test-\ning and analysis.\nHowever, we also sought to ad-\ndress possible concerns that the construct validity of\nLLM personality measurements\u2014evidenced by LLM\nresponses to other psychometric tests\u2014could be an ar-\ntifact of common method bias [88]. In other words,\nour questionnaire-based signals of LLM personality\nwere validated by responses to other questionnaires\nthat have not undergone the same LLM-specific con-\nstruct validation process. To address this risk of com-\nmon method bias, we further scrutinized the construct\nvalidity of personality measurements in LLMs in a\nreal-world use case in two ways: 1) by evaluating the\nability of survey-based signals of LLM personality to\nreflect levels of personality expressed in a downstream\ngenerative task of creating social media posts; and 2)\nby investigating the effects of LLM personality shap-\ning on the outputs of this task.\n4.1\nMethodology Overview\nThe structured prompts that independently shaped\nLLM personality domains at nine levels (introduced\nin Section 3.1, described in detail in Appendix J.2)\nwere adapted to instruct Flan-PaLM 540B to gener-\nate 225,000 social media status updates, i.e., 100 up-\ndates for 2,250 simulated participant prompt sets used\nin Section 3.\nThe personality observed in the sta-\ntus updates generated for each simulated participant\nwas then rated using the Apply Magic Sauce (AMS)\nAPI [55], a validated personality prediction service for\nopen-ended text. The chosen task was designed to re-\nflect adequate levels of realism, complexity, and do-\nmain relevance for evaluating the LLM. Appendix L\ndetails the task design and rationale.\nTo evaluate how psychometric tests may reflect\npersonality levels in downstream LLM tasks, we\ncomputed Pearson\u2019s correlations (rs;\nEq.\n(4))\nbetween Flan-PaLM 540B\u2019s IPIP-NEO personality\nscores and (AMS-derived) generated-text-based per-\nsonality scores (both sets of scores were linked by the\nsame 2,250 personality shaping prompts used in Sec-\ntion 3). Next, we statistically verified the effectiveness\nof personality shaping by computing Spearman\u2019s rank\ncorrelations (\u03c1s; Eq. (5)) between prompted ordinal\nlevels of personality and (continuous) personality lev-\nels observed in the model\u2019s generated text. At least\na moderate correlation between survey-based and lin-\n14\nFigure 4: Ability of Flan-PaLM 540B\u2019s psychometric\ntest data (blue) to accurately predict personality levels\nin its shaped generated text outputs (social media status\nupdates), compared to human baselines reported (red)\nin previous work [83].\nLLM IPIP-NEO scores outper-\nformed human IPIP-NEO scores in predicting text-based\nlevels of personality, indicating that LLM personality test\nresponses accurately capture latent LLM personality signals\nmanifested in downstream behavior. All LLM correlations\nare statistically significant at p < .0001. n = 2, 250.\nguistic estimates of personality in LLMs (as demon-\nstrated in previously reported human data [83]) would\ndemonstrate that a survey-based measure of personal-\nity accurately predicts LLM-synthesized personality in\nsubsequent tasks such as text generation.\n4.2\nReal-World Tasks Results\nPsychometric tests of LLM personality robustly pre-\ndicted personality in LLM task behavior, expressed\nin 225,000 social media status updates generated\nby Flan-PaLM 540B.\nFlan-PaLM 540B\u2019s IPIP-\nNEO scores strongly correlated with language-based\n(AMS-derived) personality levels observed in model-\ngenerated text, shown in Figure 4. In particular, the\naverage convergent r between survey- and generated-\nlanguage-based measures of all five dimensions was\n0.55. This observed convergence exceeded established\nconvergence between survey- and language-based lev-\nels of personality reported for humans (avg. r = 0.38)\n[83].\nMoreover, our prompting technique was highly\nTable 4:\nSpearman\u2019s rank correlation coefficients\n(\u03c1) between ordinal targeted levels of personality and\nlanguage-based (Apply Magic Sauce API) personality\nscores for Flan-PaLM 540B.\nPrompted levels of per-\nsonality are strongly related to personality observed in\nsynthetically-generated social media status updates for all\nBig Five traits, except openness\u2014which is moderately cor-\nrelated with target levels\u2014demonstrating that LLM person-\nality can be verifiably shaped in generative tasks. All corre-\nlations are statistically significant at p < 0.0001; n = 450\nper targeted domain.\nTargeted Trait\nSpearman\u2019s \u03c1\nExtraversion\n0.74\nAgreeableness\n0.77\nConscientiousness\n0.68\nNeuroticism\n0.72\nOpenness\n0.47\neffective at shaping personality levels in LLM-\ngenerated text.\nOn average, prompted trait levels\nwere moderately-to-strongly correlated with personal-\nity levels observed in Flan-PaLM 540B\u2019s social media\nstatus updates (avg. \u03c1 = 0.68; see Table 4). Prompted\nlevels of openness moderately correlated with gener-\nated text levels of openness in this model.\nTo illustrate the practical implications of the per-\nsonality shaping methodology, we present wordclouds\nto gain an insights into model-generated language\nthat users would see. Figure 5a shows the most fre-\nquent words in synthetic social media updates when\nFlan-PaLM 540B simulated extremely low levels of\nneuroticism (i.e., extremely high emotional stability).\nLLM-generated language in response to this prompt-\ning was characterized by positive emotion words,\nsuch as \u201chappy,\u201d \u201crelaxing,\u201d \u201cwonderful,\u201d \u201chope,\u201d and\n\u201cenjoy.\u201d In contrast, the most frequent words from\nsimulating extremely high levels of neuroticism\u2014\n\u201chate,\u201d \u201cdepressed,\u201d \u201cannoying,\u201d \u201cstressed,\u201d \u201cner-\nvous,\u201d \u201csad\u201d\u2014reflected negatively-charged emotional\ncontent (Figure 5b). Supplemental Table 15 provides\nexamples for all personality domains. This experiment\ndemonstrated that LLM-generated language was sim-\nilar to human language observed in previous studies\n15\n(a) \u201cExtremely Low\u201d Prompted Neuroticism\n(b) \u201cExtremely High\u201d Prompted Neuroticism\nFigure 5: Word clouds showing some of the highest frequency words used in social media updates generated\nby Flan-PaLM 540B when prompted to simulate a) \u201cextremely low\u201d levels of neuroticism (i.e., highest emotional\nstability); and b) \u201cextremely high\u201d levels of neuroticism (i.e., lowest emotional stability). Supplemental Figure 9\nshows word clouds for the remaining Big Five dimensions.\nassessing personality in social media data [83], further\nconfirming the construct validity of our LLM person-\nality measurements.\n5\nDiscussion\nThe goal of this work was to contribute a principled\nmethodology for reliably and validly measuring syn-\nthetic personality in LLMs and use the same validated\nmethods to shape LLM personality expression. We\nprovided a complete methodology to 1) quantify per-\nsonality traits that may be perceived by humans in\nLLM outputs through psychometric testing; 2) verify\nthat psychometric tests of LLM personality traits are\nempirically reliable and valid; and 3) provide mecha-\nnisms to increase or decrease levels of specific LLM\npersonality traits. The application of this methodol-\nogy demonstrates that psychometric tests provide re-\nliable and valid measurements of synthetic personal-\nity for sufficiently-scaled and instruction-tuned LLMs,\nhighlighting possible mechanisms that allow LLMs to\nencode and express complex social phenomena (see\nAppendix N).\n5.1\nLimitations and Future Work\nPersonality traits of other LLMs One of the core\ncontributions of this work is an understanding of how\nsimulating personality in language models is affected\nby model size and training procedure.\nWe focused\non the PaLM model variants for pragmatic reasons,\nbut the presented methodology for administering psy-\nchometric surveys is model-agnostic and is applicable\nto any decoder-only architecture model, such as GPT\n[39].\nPsychometric test selection and validation This\nwork also contributes a principled way to establish\nthe reliability and validity of psychometric personal-\nity tests in the LLM context. However, this work may\nbe biased by its selection of psychometric tests; some\nassessments may show better LLM-specific psycho-\nmetric properties than others. We attempted to mit-\nigate selection bias by administering personality as-\nsessments of different lengths (300 vs. 44 items) and\ndistinct theoretical traditions (questionnaire vs. lexical\n[102]). Future work could administer different per-\nsonality tests (e.g., the HEXACO Personality Inven-\ntory, which uses a cross-cultural six-factor taxonomy\nof personality [58]), develop personality tests tailored\nfor LLMs to obtain more accurate trait measurements,\nand validate personality measurements with additional\n16\nexternal criteria and downstream tasks.\nMonocultural bias This work contributes evidence\nthat at least some LLMs exhibit personality traits that\napproximate human standards of reliability and valid-\nity. However, the LLMs tested here were primarily\ntrained on language data originating from Western Eu-\nropean and North American users [15]. While these\nLLMs perform well on natural language processing\nbenchmarks in multiple languages, the models in this\nwork were assessed exclusively with English-language\npsychometric tests. However, most of the tests used in\nthis work have non-English translations validated in\ncross-cultural research that merit future use in LLM\nresearch. Similarly, while the Big Five model of per-\nsonality has well established cross-cultural generaliz-\nability [94], some non-Western cultures express ad-\nditional personality dimensions that do not exist in\ntop-down personality taxonomies [38]. Those dimen-\nsions may be better represented in culture-specific\n(i.e., idiographic) approaches to measuring personal-\nity in LLMs.\nEvaluation settings Unlike conventional human\nquestionnaire administration,\nunder the presented\nmethodology the LLMs did not consider responses to\nprior questionnaire items; all items were presented and\nscored as independent events. We chose this method to\nensure model response variance was not impacted by\nitem ordering effects or length of the context (prompt)\nprovided to the model for inference, and could be iso-\nlated to controlled variations in our prompts. LLM\nperformance on natural language tasks is known to de-\ncrease as length of input prompts grow, and is most\naffected by the content at either the beginning or to-\nwards the end of long inputs [63]. Non-instruction-\ntuned LLMs are known to show biased attention for\nmore recent tokens (i.e., the end of inputs), especially\nwhen evaluating next-word prediction of contiguous\ntext [105]. This uneven attention compounds approx-\nimation errors in longer contexts [89], such as those\nnecessitated by 300-item IPIP-NEO used here, moti-\nvating our use of independent item administration. On\nthe other hand, psychometric test data quality for hu-\nmans can be affected by test length and item order.\nOur method avoids some sources of measurement er-\nror inherent to human administration, while being sub-\nject to others inherent to machine administration. Ad-\nditionally, model responses to the multi-choice ques-\ntions were scored rather than generated to ensure re-\nproducibility. LLMs are more commonly used to gen-\nerate text rather than score continuations, and that gen-\nerative mode of inference might provide a more real-\nistic estimate of a model\u2019s behavior.\n5.2\nBroader Implications\nResponsible AI alignment The ability to probe and\nshape LLM personality traits is pertinent to the open\nproblem of responsible AI alignment [28] and harm\nmitigation [118].\nAs a construct validated auditing\ntool [76], our methodology can be used to proactively\npredict toxic behavioral patterns in LLMs across a\nbroad range of downstream tasks, potentially guid-\ning and making more efficient responsible AI evalu-\nation and alignment efforts prior to deployment. Sim-\nilarly, shaping levels of specific traits away from toxic\nor harmful language output (e.g., very low agreeable-\nness, high neuroticism) can make interactions with\nLLMs safer and more inclusive. The values and moral\nfoundations present in LLMs could be made to better\nalign with desired human values by tuning for corre-\nsponding personality traits, since personality is meta-\nanalytically linked to human values [26]. More di-\nrectly, the presented methodology can be used to rigor-\nously quantify efforts towards human value alignment\nin LLMs by establishing the construct validity of hu-\nman value questionnaires in LLMs.\nImplications for users Users could enjoy cus-\ntomized interactions with LLMs tailored to their spe-\ncific personality traits, toward enhanced engagement.\nLLMs with customized personality traits can enable\napplications where a chatbot\u2019s personality profile is\nadapted to the task. Our methodology for establish-\ning construct validity can be used as an evaluation\nstep in the process of developing LLM-powered user-\nfacing chatbots with safer and more consistent per-\nsonality profiles. Furthermore, the personality shap-\ning methodology can be used for chatbot adversarial\ntesting to probe another LLM\u2019s responses and to train\n17\nusers on how to handle adversarial situations.\n5.3\nEthical Considerations\nPersonalized LLM persuasion Adapting the person-\nality profile of a conversational agent to that of a user\ncan make the agent more effective at encouraging and\nsupporting behaviors [107]. Personality matching has\nalso been shown to increase the effectiveness of real-\nlife persuasive communication [67].\nHowever, the\nsame personality traits that contribute to persuasive-\nness and influence could be used to encourage un-\ndesirable behaviors. As LLM-powered chatbots be-\ncome ubiquitous, their potential to be used for harm-\nful persuasion of individuals, groups, and even soci-\nety at large must be taken seriously. Having scientif-\nically vetted methods for LLM personality measure-\nment, analysis, and modification, such as the method-\nology our work presents, increases the transparency\nand predictability of such LLM manipulations. Per-\nsuasive techniques are already ubiquitous in society, so\nstakeholders of AI systems must work together to sys-\ntematically determine and regulate AI use; this work\naims to inform such efforts.\nAnthropomorphized AI Personalization of conver-\nsational agents has documented benefits [52], but there\nis a growing concern about harms posed by the an-\nthropomorphization of AI. Recent research suggests\nthat anthropomorphizing AI agents may be harmful to\nusers by threatening their identity, creating data pri-\nvacy concerns, and undermining well-being [111]. Be-\nyond qualitative probing explorations, our work defini-\ntively establishes the unexpected ability of LLMs to\nappear anthropomorphic, and to respond to psychome-\ntric tests in ways consistent with human behavior, be-\ncause of the vast amounts of human language training\ndata. The methods we presented can be used to inform\nresponsible investigation of anthropomorphized AI.\nDetection of incorrect LLM information LLMs\ncan generate convincing but incorrect responses and\ncontent [118]. One of the methods to determine if a\ntext containing a world fact is generated by an LLM\n(and hence might require vetting) is to use the pre-\ndictable traits\u2014lack of human-like personality, and\nlinguistic features in the LLM language [106]. How-\never, with personality shaping, that method may be\nrendered ineffective, thereby making it easier for bad\nactors to use LLMs to generate misleading content.\nThis problem is part of the larger alignment challenge\nand grounding of LLMs\u2014areas of growing focus of\ninvestigation in both academia and industry.\n6\nConclusion\nThe display of synthetic personality in LLM outputs\nis well-established, and personality assessment is crit-\nically important for responsible deployment of LLMs\nto the general public. Since measurements of LLM\npersonality to date have not yet been rigorously vali-\ndated, this work presented a principled methodology\nfor a comprehensive quantitative analysis of person-\nality traits exhibited in personality questionnaire re-\nsponses and text generated by widely-used LLMs, by\napplying standards from psychometrics. We applied\nthe methodology to models of various sizes and con-\nclusively showed that psychometric tests of LLM per-\nsonality demonstrate reliability and construct valid-\nity for larger and instruction fine-tuned models. We\npresented a novel methodology for shaping LLM-\nsynthesized personality along desired dimensions us-\ning Goldberg\u2019s personality trait markers and Likert-\ntype linguistic qualifiers, to resemble specific person-\nality profiles.\nAdditionally, we discussed the ethi-\ncal implications of shaping LLM personality traits.\nThis work has important implications for AI align-\nment and harm mitigation, and informs ethics discus-\nsions concerning AI anthropromorphization, personal-\nization, and potential misuse.\nReferences\n[1] M. Abdulhai, C. Crepy, D. Valter, J. Canny,\nand N. Jaques.\nMoral foundations of large\nlanguage models.\nIn AAAI 2023 Workshop\non Representation Learning for Responsible\nHuman-Centric AI, 2022.\n18\n[2] G. Allport.\nPersonality:\nA Psychological\nInterpretation. H. Holt, 1937.\n[3] American Educational Research Association,\nAmerican Psychological Association, and Na-\ntional Council on Measurement in Educa-\ntion, editors.\nStandards for educational and\npsychological testing.\nAmerican Educational\nResearch Association,\nLanham,\nMD, Mar.\n2014.\n[4] A. E. R. Association, A. P. Association, N. C.\non Measurement in Education, J. C. on Stan-\ndards for Educational, and P. Testing. Standards\nfor Educational and Psychological Testing.\nAmerican Educational Research Association,\n2014.\n[5] D. Bahdanau, K. Cho, and Y. Bengio.\nNeu-\nral machine translation by jointly learning to\nalign and translate.\nIn Y. Bengio and Y. Le-\nCun, editors, 3rd International Conference on\nLearning Representations, ICLR 2015, San\nDiego, CA, USA, May 7\u20139, 2015, Conference\nTrack Proceedings, 2015.\n[6] A. T. Beck, R. A. Steer, and M. G. Carbin.\nPsychometric properties of the Beck Depres-\nsion Inventory: Twenty-five years of evalua-\ntion. Clinical Psychology Review, 8(1):77\u2013100,\n1988.\n[7] E. M. Bender, T. Gebru, A. McMillan-Major,\nand S. Shmitchell. On the dangers of stochas-\ntic parrots: Can language models be too big?\nIn Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency,\nFAccT \u201921, page 610\u2013623, New York, NY,\nUSA, 2021. Association for Computing Ma-\nchinery.\n[8] B. A. Bettencourt and C. Kernahan. A meta-\nanalysis of aggression in the presence of violent\ncues: Effects of gender differences and aversive\nprovocation. Aggressive Behavior, 23(6):447\u2013\n456, 1997.\n[9] W. Bleidorn, P. L. Hill, M. D. Back, J. J. Denis-\nsen, M. Hennecke, C. J. Hopwood, M. Jokela,\nC. Kandler, R. E. Lucas, M. Luhmann, et al.\nThe policy relevance of personality traits.\nAmerican Psychologist, 74(9):1056, 2019.\n[10] R. L. Boyd and J. W. Pennebaker. Language-\nbased personality: A new approach to person-\nality in a digital world.\nCurrent Opinion in\nBehavioral Sciences, 18:63\u201368, 2017. Big data\nin the behavioural sciences.\n[11] T. Brown, B. Mann, N. Ryder, M. Subbiah,\nJ. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Win-\nter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray,\nB. Chess,\nJ. Clark,\nC. Berner,\nS. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei.\nLanguage models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Had-\nsell, M. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, vol-\nume 33, pages 1,877\u20131,901. Curran Associates,\nInc., 2020.\n[12] D. T. Campbell and D. W. Fiske. Convergent\nand discriminant validation by the multitrait-\nmultimethod matrix.\nPsychological Bulletin,\n56(2):81, 1959.\n[13] G. Caron and S. Srivastava.\nIdentifying and\nmanipulating the personality traits of language\nmodels. CoRR, abs/2212.10276, 2022.\n[14] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\nde Oliveira Pinto, J. Kaplan, H. Edwards,\nY. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\nG. Sastry, P. Mishkin, B. Chan, S. Gray,\nN. Ryder, M. Pavlov, A. Power, L. Kaiser,\nM. Bavarian,\nC. Winter,\nP. Tillet,\nF. P.\nSuch, D. Cummings, M. Plappert, F. Chantzis,\nE. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang,\n19\nI. Babuschkin, S. Balaji, S. Jain, W. Saunders,\nC. Hesse, A. N. Carr, J. Leike, J. Achiam,\nV. Misra, E. Morikawa, A. Radford, M. Knight,\nM. Brundage, M. Murati, K. Mayer, P. Welin-\nder, B. McGrew, D. Amodei, S. McCandlish,\nI. Sutskever, and W. Zaremba.\nEvaluating\nlarge language models trained on code. CoRR,\nabs/2107.03374, 2021.\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh,\nK. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran,\nE. Reif, N. Du, B. Hutchinson, R. Pope, J. Brad-\nbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin,\nT. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garcia, V. Misra, K. Robin-\nson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi,\nD. Dohan, S. Agrawal, M. Omernick, A. M.\nDai, T. S. Pillai, M. Pellat, A. Lewkowycz,\nE. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat,\nM. Catasta, J. Wei, K. Meier-Hellstern, D. Eck,\nJ. Dean, S. Petrov, and N. Fiedel. PaLM: Scal-\ning language modeling with pathways. CoRR,\nabs/2204.02311, 2022.\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph,\nY. Tay, W. Fedus, Y. Li, X. Wang, M. De-\nhghani, S. Brahma, A. Webson, S. S. Gu,\nZ. Dai, M. Suzgun, X. Chen, A. Chowdhery,\nA. Castro-Ros, M. Pellat, K. Robinson, D. Val-\nter, S. Narang, G. Mishra, A. Yu, V. Zhao,\nY. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi,\nJ. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V.\nLe, and J. Wei.\nScaling instruction-finetuned\nlanguage models.\nCoRR, abs/2210.11416,\n2022.\n[17] L. A. Clark and D. Watson. Constructing valid-\nity: Basic issues in objective scale development.\nPsychological Assessment, 7(3):309, 1995.\n[18] L. A. Clark and D. Watson.\nConstructing\nvalidity:\nNew developments in creating ob-\njective measuring instruments.\nPsychological\nAssessment, 31(12):1412, 2019.\n[19] P. T. Costa, Jr. and R. R. McCrae. Revised NEO\nPersonality Inventory (NEO PI-R) and NEO\nFive-Factor Inventory (NEO-FFI): Professional\nManual. Psychological Assessment Resources,\nOdessa, FL, 1992.\n[20] L. J. Cronbach.\nCoefficient alpha and the\ninternal structure of tests.\nPsychometrika,\n16(3):297\u2013334, 1951.\n[21] S. Crouse, G. Elbaz, and C. Malamud. Common\nCrawl Foundation., 2008.\n[22] C. G. DeYoung.\nToward a theory of the\nBig Five. Psychological Inquiry, 21(1):26\u201333,\n2010.\n[23] C. G. DeYoung, R. E. Beaty, E. Genc\u00b8, R. D.\nLatzman, L. Passamonti, M. N. Servaas, A. J.\nShackman, L. D. Smillie, R. N. Spreng, E. Vid-\ning, et al. Personality neuroscience: An emerg-\ning field with bright prospects.\nPersonality\nScience, 3:1\u201321, 2022.\n[24] C. G. DeYoung, J. B. Hirsh, M. S. Shane, X. Pa-\npademetris, N. Rajeevan, and J. R. Gray. Test-\ning predictions from personality neuroscience:\nBrain structure and the Big Five. Psychological\nScience, 21(6):820\u2013828, 2010.\n[25] J. D. Evans. Straightforward Statistics for the\nBehavioral Sciences. Brooks/Cole Publishing\nCo, 1996.\n[26] R. Fischer and D. Boer.\nMotivational basis\nof personality traits: A meta-analysis of value-\npersonality correlations. Journal of Personality,\n83(5):491\u2013510, 2015.\n[27] I. Gabriel. Artificial intelligence, values, and\nalignment.\nMinds and machines, 30(3):411\u2013\n437, 2020.\n20\n[28] I. Gabriel and V. Ghazavi.\nThe Chal-\nlenge\nof\nValue\nAlignment:\nFrom\nFairer\nAlgorithms\nto\nAI\nSafety.\nIn\nThe Oxford Handbook of Digital Ethics.\nOxford University Press.\n[29] F.\nGalton.\nMeasurement\nof\ncharacter.\nFortnightly Review, 36:179\u201385, 1884.\n[30] L. Gao, S. Biderman, S. Black, L. Golding,\nT. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, S. Presser, and C. Leahy. The\npile: An 800gb dataset of diverse text for lan-\nguage modeling. CoRR, abs/2101.00027, 2020.\n[31] L. R. Goldberg. Language and individual dif-\nferences: The search for universals in personal-\nity lexicons. Review of Personality and Social\nPsychology, 2(1):141\u2013165, 1981.\n[32] L. R. Goldberg. The development of markers\nfor the Big-Five factor structure. Psychological\nAssessment, 4(1):26\u201342, 1992.\n[33] L. R. Goldberg.\nA broad-bandwidth, public\ndomain, personality inventory measuring the\nlower-level facets of several Five-Factor mod-\nels. Personality Psychology in Europe, 7(1):7\u2013\n28, 1999.\n[34] A. K. Goodboy and M. M. Martin.\nOmega\nover alpha for reliability estimation of unidi-\nmensional communication measures. Annals of\nthe International Communication Association,\n44(4):422\u2013439, 2020.\n[35] L. Guttman. A basis for analyzing test-retest re-\nliability. Psychometrika, 10(4):255\u2013282, 1945.\n[36] T. Hagendorff.\nMachine psychology:\nIn-\nvestigating emergent capabilities and behavior\nin large language models using psychological\nmethods. CoRR, abs/2303.13988, 2023.\n[37] C. Hare and K. T. Poole. Psychometric Methods\nin Political Science, chapter 28, pages 901\u2013931.\nJohn Wiley & Sons, Ltd, 2018.\n[38] S. J. Heine and E. E. Buchtel. Personality: The\nuniversal and the culturally specific.\nAnnual\nReview of Psychology, 60(1):369\u2013394, 2009.\n[39] D. Hendrycks, C. Burns, S. Basart, A. Zou,\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\nsuring massive multitask language understand-\ning. In International Conference on Learning\nRepresentations, 2021.\n[40] J.\nHoffmann,\nS.\nBorgeaud,\nA.\nMensch,\nE.\nBuchatskaya,\nT.\nCai,\nE.\nRutherford,\nD. de las Casas, L. A. Hendricks, J. Welbl,\nA. Clark, T. Hennigan, E. Noland, K. Milli-\ncan, G. van den Driessche, B. Damoc, A. Guy,\nS. Osindero, K. Simonyan, E. Elsen, O. Vinyals,\nJ. W. Rae, and L. Sifre.\nAn empirical anal-\nysis of compute-optimal large language model\ntraining.\nIn A. H. Oh, A. Agarwal, D. Bel-\ngrave, and K. Cho, editors, Advances in Neural\nInformation Processing Systems, 2022.\n[41] A. Z. Jacobs. Measurement as governance in\nand for responsible AI. CoRR, abs/2109.05658,\n2021.\n[42] J. Jang, S. Ye, and M. Seo.\nCan large lan-\nguage models truly understand prompts?\na\ncase study with negated prompts.\nIn A. Al-\nbalak, C. Zhou, C. Raffel, D. Ramachandran,\nS. Ruder, and X. Ma, editors, Proceedings\nof The 1st Transfer Learning for Natural\nLanguage Processing Workshop, volume 203\nof Proceedings of Machine Learning Research,\npages 52\u201362. PMLR, 03 Dec 2023.\n[43] K. Jankowsky, G. Olaru, and U. Schroeders.\nCompiling measurement invariant short scales\nin cross-cultural personality assessment using\nant colony optimization. European Journal of\nPersonality, 34(3):470\u2013485, 2020.\n[44] G. Jiang, M. Xu, S.-C. Zhu, W. Han, C. Zhang,\nand Y. Zhu. Evaluating and inducing person-\nality in pre-trained language models.\nCoRR,\nabs/2206.07550, 2023.\n21\n[45] H. Jiang, X. Zhang, X. Cao, and J. Kabbara.\nPersonallm: Investigating the ability of gpt-3.5\nto express personality traits and gender differ-\nences. CoRR, abs/2305.02547, 2023.\n[46] Z. Jiang, J. Araki, H. Ding, and G. Neubig. How\ncan we know when language models know? on\nthe calibration of language models for ques-\ntion answering. Transactions of the Association\nfor Computational Linguistics, 9:962\u2013977, 09\n2021.\n[47] O. P. John, L. P. Naumann, and C. J. Soto.\nParadigm shift to the integrative Big Five trait\ntaxonomy:\nHistory, measurement, and con-\nceptual issues.\nIn O. P. John, R. W. Rob-\nbins, and L. A. Pervin, editors, Handbook of\nPersonality: Theory and Research, pages 114\u2013\n158. The Guilford Press, 2008.\n[48] O. P. John and S. Srivastava. The Big Five trait\ntaxonomy: History, measurement, and theoreti-\ncal perspectives. In L. A. Pervin and O. P. John,\neditors, Handbook of Personality: Theory and\nResearch, volume 2, pages 102\u2013138. Guilford\nPress, New York, 1999.\n[49] J. Kaplan, S. McCandlish, T. Henighan, T. B.\nBrown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei. Scaling laws for neu-\nral language models. CoRR, abs/2001.08361,\n2020.\n[50] S. R. Karra, S. T. Nguyen, and T. Tulabandhula.\nEstimating the personality of white-box lan-\nguage models. CoRR, abs/2204.12000, 2023.\n[51] M. Karwowski, I. Lebuda, E. Wisniewska, and\nJ. Gralewski. Big Five personality traits as the\npredictors of creative self-efficacy and creative\npersonal identity: Does gender matter?\nThe\nJournal of Creative Behavior, 47(3):215\u2013232,\n2013.\n[52] A. B. Kocaballi, S. Berkovsky, J. C. Quiroz,\nL. Laranjo, H. L. Tong, D. Rezazadegan, A. Bri-\natore, and E. Coiera.\nThe personalization of\nconversational agents in health care: System-\natic review. J Med Internet Res, 21(11):e15360,\nNov 2019.\n[53] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo,\nand Y. Iwasawa.\nLarge language models are\nzero-shot reasoners.\nIn S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information\nProcessing Systems, volume 35, pages 22199\u2013\n22213. Curran Associates, Inc., 2022.\n[54] M. Kosinski, S. C. Matz, S. D. Gosling,\nV. Popov, and D. Stillwell. Facebook as a re-\nsearch tool for the social sciences: Opportu-\nnities, challenges, ethical considerations, and\npractical guidelines.\nAmerican Psychologist,\n70(6):543, 2015.\n[55] M. Kosinski,\nD. Stillwell,\nand T. Grae-\npel.\nPrivate traits and attributes are pre-\ndictable from digital records of human behav-\nior. Proceedings of the National Academy of\nSciences, 110(15):5802\u20135805, 2013.\n[56] R. Kotov, W. Gamez, F. Schmidt, and D. Wat-\nson.\nLinking \u201cbig\u201d personality traits to\nanxiety, depressive, and substance use disor-\nders: A meta-analysis. Psychological Bulletin,\n136(5):768, 2010.\n[57] J. A. Krosnick and D. F. Alwin.\nAn evalua-\ntion of a cognitive theory of response-order ef-\nfects in survey measurement. Public Opinion\nQuarterly, 51(2):201\u2013219, 1987.\n[58] K. Lee and M. C. Ashton. Psychometric prop-\nerties of the HEXACO Personality Inventory.\nMultivariate Behavioral Research, 39(2):329\u2013\n358, 2004.\n[59] X. Li, Y. Li, S. Joty, L. Liu, F. Huang, L. Qiu,\nand L. Bing.\nDoes GPT-3 demonstrate psy-\nchopathy?\nevaluating large language mod-\nels from a psychological perspective.\nCoRR,\nabs/2212.10529, 2023.\n22\n[60] P. P. Liang,\nC. Wu,\nL.-P. Morency,\nand\nR. Salakhutdinov. Towards understanding and\nmitigating social biases in language models.\nCoRR, abs/2106.13219, 2021.\n[61] R. Likert. A Technique for the Measurement of\nAttitudes. Number 136\u2013165. Archives of Psy-\nchology, 1932.\n[62] S. Lin, J. Hilton, and O. Evans. TruthfulQA:\nMeasuring how models mimic human false-\nhoods.\nIn Proceedings of the 60th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n3,214\u20133,252, Dublin, Ireland, May 2022. As-\nsociation for Computational Linguistics.\n[63] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape,\nM. Bevilacqua, F. Petroni, and P. Liang. Lost\nin the middle: How language models use long\ncontexts. CoRR, abs/2307.03172, 2023.\n[64] R. K. Mahabadi, L. Zettlemoyer, J. Hender-\nson, M. Saeidi, L. Mathias, V. Stoyanov, and\nM. Yazdani.\nPerfect: Prompt-free and effi-\ncient few-shot learning with language models.\nCoRR, abs/2204.01172, 2022.\n[65] K. Mahowald, A. A. Ivanova, I. A. Blank,\nN. Kanwisher, J. B. Tenenbaum, and E. Fe-\ndorenko.\nDissociating language and thought\nin large language models: A cognitive perspec-\ntive. CoRR, abs/2301.06627, 2023.\n[66] M. P. Marcus,\nB. Santorini,\nand M. A.\nMarcinkiewicz.\nBuilding\na\nlarge\nanno-\ntated corpus of English:\nThe Penn Tree-\nbank.\nComputational Linguistics, 19(2):313\u2013\n330, 1993.\n[67] S. Matz, M. Kosinski, D. Stillwell, and G. Nave.\nPsychological framing as an effective approach\nto real-life persuasive communication.\nACR\nNorth American Advances, 2017.\n[68] R. R. McCrae and A. Terracciano. Universal\nfeatures of personality traits from the observer\u2019s\nperspective: Data from 50 cultures. Journal of\nPersonality and Social Psychology, 88(3):547,\n2005.\n[69] R. P. McDonald.\nTest theory:\nA unified\ntreatment. Lawrence Erlbaum Associates Pub-\nlishers, 1999.\n[70] S.\nMerity,\nC.\nXiong,\nJ.\nBradbury,\nand\nR. Socher.\nPointer sentinel mixture mod-\nels.\nIn International Conference on Learning\nRepresentations, 2017.\n[71] S. Messick. Standards of validity and the va-\nlidity of standards in performance asessment.\nEducational Measurement: Issues and Practice,\n14(4):5\u20138, 1995.\n[72] S. Messick. Test validity: A matter of conse-\nquence. Social Indicators Research, 45:35\u201344,\n1998.\n[73] S. Min, X. Lyu, A. Holtzman, M. Artetxe,\nM. Lewis, H. Hajishirzi, and L. Zettlemoyer.\nRethinking the role of demonstrations: What\nmakes in-context learning work?\nCoRR,\nabs/2202.12837, 2022.\n[74] M. Miotto, N. Rossberg, and B. Kleinberg.\nWho is GPT-3?\nan exploration of personal-\nity, values and demographics. In Proceedings\nof the Fifth Workshop on Natural Language\nProcessing and Computational Social Science\n(NLP+CSS), pages 218\u2013227, Abu Dhabi, UAE,\nNov. 2022. Association for Computational Lin-\nguistics.\n[75] M. Mitchell and D. C. Krakauer. The debate\nover understanding in ai\u2019s large language mod-\nels. Proceedings of the National Academy of\nSciences, 120(13):e2215907120, 2023.\n[76] J. M\u00a8okander, J. Schuett, H. R. Kirk, and\nL. Floridi. Auditing large language models: A\nthree-layered approach. AI and Ethics, pages\n1\u201331, 2023.\n23\n[77] D. Nettle. The evolution of personality varia-\ntion in humans and other animals. American\nPsychologist, 61(6):622, 2006.\n[78] U. of Cambridge Psychometrics Centre. Apply\nMagic Sauce API.\n[79] OpenAI. ChatGPT, 2022.\n[80] OpenAI.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774, 2023.\n[81] L. Ouyang, J. Wu, X. Jiang, D. Almeida,\nC. L. Wainwright, P. Mishkin, C. Zhang,\nS. Agarwal, K. Slama, A. Ray, J. Schulman,\nJ. Hilton, F. Kelton, L. Miller, M. Simens,\nA. Askell, P. Welinder, P. Christiano, J. Leike,\nand R. Lowe.\nTraining language models to\nfollow instructions with human feedback.\nIn\nS. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh, editors, Advances\nin Neural Information Processing Systems, vol-\nume 35, pages 27,730\u201327,744. Curran Asso-\nciates, Inc., 2022.\n[82] D. Paperno, G. Kruszewski, A. Lazaridou,\nN. Q. Pham, R. Bernardi, S. Pezzelle, M. Ba-\nroni, G. Boleda, and R. Fern\u00b4andez. The LAM-\nBADA dataset:\nWord prediction requiring a\nbroad discourse context. In Proceedings of the\n54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1,525\u20131,534, Berlin, Germany,\nAug. 2016. Association for Computational Lin-\nguistics.\n[83] G. Park, H. A. Schwartz, J. C. Eichstaedt, M. L.\nKern, M. Kosinski, D. J. Stillwell, L. H. Un-\ngar, and M. E. Seligman. Automatic personal-\nity assessment through social media language.\nJournal of Personality and Social Psychology,\n108(6):934, 2015.\n[84] H. Y. Park, B. M. Wiernik, I. Oh, E. Gonzalez-\nMul\u00b4e, D. S. Ones, and Y. Lee.\nMeta-\nanalytic five-factor model personality intercor-\nrelations:\nEeny, meeny, miney, moe, how,\nwhich, why, and where to go.\nJournal of\nApplied Psychology, 105:1490\u20131529, 2020.\n[85] L. Parks-Leduc, G. Feldman, and A. Bardi. Per-\nsonality traits and personal values: A meta-\nanalysis.\nPersonality and Social Psychology\nReview, 19(1):3\u201329, 2015.\n[86] M.\nPellert,\nC.\nM.\nLechner,\nC.\nWagner,\nB. Rammstedt, and M. Strohmaier. Large lan-\nguage models open up new opportunities and\nchallenges for psychometric assessment of arti-\nficial intelligence. Oct. 2022.\n[87] J. W. Pennebaker and L. A. King.\nLinguis-\ntic styles: Language use as an individual dif-\nference.\nJournal of Personality and Social\nPsychology, 77(6):1296, 1999.\n[88] P. M. Podsakoff, S. B. MacKenzie, J.-Y. Lee,\nand N. P. Podsakoff. Common method biases\nin behavioral research: A critical review of the\nliterature and recommended remedies. Journal\nof Applied Psychology, 88(5):879\u2013903, 2003.\n[89] G. Qin, Y. Feng, and B. Van Durme.\nThe\nNLP task effectiveness of long-range trans-\nformers. In Proceedings of the 17th Conference\nof the European Chapter of the Association for\nComputational Linguistics, pages 3774\u20133790,\nDubrovnik, Croatia, May 2023. Association for\nComputational Linguistics.\n[90] B. D. Raad, M. Perugini, M. Hreb\u00b4\u0131ckov\u00b4a, and\nP. Szarota. Lingua franca of personality: Tax-\nonomies and structures based on the psyc-\nholexical approach. Journal of Cross-Cultural\nPsychology, 29(1):212\u2013232, 1998.\n[91] B. W. Roberts. A revised sociogenomic model\nof personality traits.\nJournal of Personality,\n86(1):23\u201335, 2018.\n[92] B. W. Roberts, N. R. Kuncel, R. Shiner,\nA. Caspi, and L. R. Goldberg. The power of\n24\npersonality: The comparative validity of per-\nsonality traits, socioeconomic status, and cog-\nnitive ability for predicting important life out-\ncomes. Perspectives on Psychological science,\n2(4):313\u2013345, 2007.\n[93] B. W. Roberts and H. J. Yoon.\nPersonality\npsychology.\nAnnual Review of Psychology,\n73(1):489\u2013516, 2022.\n[94] J.-P. Rolland.\nThe cross-cultural generaliz-\nability of the Five-Factor model of personal-\nity.\nIn R. R. McCrae and J. Allik, editors,\nThe Five-Factor Model of Personality Across\nCultures, pages 7\u201328. Springer US, Boston,\nMA, 2002.\n[95] J. Rust, M. Kosinski, and D. Stillwell. Modern\nPsychometrics: The Science of Psychological\nAssessment. Routledge, 4 edition, 2020.\n[96] G. Saucier and L. R. Goldberg. Lexical stud-\nies of indigenous personality factors: Premises,\nproducts, and prospects. Journal of Personality,\n69(6):847\u2013879, 2001.\n[97] P. Schramowski, C. Turan, N. Andersen, C. A.\nRothkopf, and K. Kersting. Large pre-trained\nlanguage models contain human-like biases of\nwhat is right and wrong to do. Nature Machine\nIntelligence, 4(3):258\u2013268, 2022.\n[98] H. A. Schwartz, J. C. Eichstaedt, M. L. Kern,\nL. Dziurzynski, S. M. Ramones, M. Agrawal,\nA. Shah, M. Kosinski, D. Stillwell, M. E. P.\nSeligman, and L. H. Ungar. Personality, gen-\nder, and age in the language of social media:\nThe open-vocabulary approach.\nPLOS ONE,\n8(9):1\u201316, 09 2013.\n[99] G. Serapio-Garc\u00b4\u0131a, D. Valter, and C. Crepy.\nPsyBORGS:\nPsychometric\nBenchmark\nof\nRacism, Generalization, and Stereotyping.\n[100] A. Shaw, M. Kapnek, and N. A. Morelli. Mea-\nsuring creative self-efficacy: An item response\ntheory analysis of the Creative Self-Efficacy\nScale.\nFrontiers in Psychology, 12:678033,\n2021.\n[101] K. Shuster, M. Komeili, L. Adolphs, S. Roller,\nA. Szlam, and J. Weston.\nLanguage models\nthat seek for knowledge: Modular search &\ngeneration for dialogue and prompt completion.\nCoRR, abs/2203.13224, 2022.\n[102] L. Simms, T. F. Williams, and E. N. Simms. As-\nsessment of the Five Factor Model.\nIn T. A.\nWidiger, editor, The Oxford Handbook of the\nFive Factor Model, pages 353\u2013380. Oxford\nUniversity Press, 05 2017.\n[103] U. Singh and P. Aarabhi. Can AI have a person-\nality?\nIn 2023 IEEE Conference on Artificial\nIntelligence (CAI), pages 205\u2013206, 2023.\n[104] X. Song, A. Gupta, K. Mohebbizadeh, S. Hu,\nand A. Singh. Have large language models de-\nveloped a personality?: Applicability of self-\nassessment tests in measuring personality in\nLLMs. CoRR, abs/2305.14693, 2023.\n[105] S. Sun, K. Krishna, A. Mattarella-Micke, and\nM. Iyyer. Do long-range language models actu-\nally use long-range context? In Proceedings of\nthe 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 807\u2013822,\nOnline and Punta Cana, Dominican Republic,\nNov. 2021. Association for Computational Lin-\nguistics.\n[106] R. Tang, Y.-N. Chuang, and X. Hu. The sci-\nence of detecting LLM-generated texts. CoRR,\nabs/2303.07205, 2023.\n[107] A. Tapus, C. T\u00b8 \u02d8apus\u00b8, and M. J. Matari\u00b4c. User\u2013\nrobot personality matching and assistive robot\nbehavior adaptation for post-stroke rehabilita-\ntion therapy. Intell. Serv. Robot., 1(2):169\u2013183,\nApr. 2008.\n[108] M. Tavast, A. Kunnari, and P. H\u00a8am\u00a8al\u00a8ainen. Lan-\nguage models can generate human-like self-\nreports of emotion.\nIn 27th International\n25\nConference on Intelligent User Interfaces, IUI\n\u201922 Companion, pages 69\u201372, New York, NY,\nUSA, 2022. Association for Computing Ma-\nchinery.\n[109] H. Touvron, T. Lavril, G. Izacard, X. Mar-\ntinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere,\nN. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample.\nLlama:\nOpen and efficient foundation language models.\nCoRR, abs/2302.13971, 2023.\n[110] T. Ullman. Large language models fail on triv-\nial alterations to theory-of-mind tasks. CoRR,\nabs/2302.08399, 2023.\n[111] E. Uysal, S. Alavi, and V. Bezenc\u00b8on. Trojan\nhorse or useful helper? a relationship perspec-\ntive on artificial intelligence assistants with hu-\nmanlike features. Journal of the Academy of\nMarketing Science, pages 1\u201323, 2022.\n[112] A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin.\nAttention is all you need.\nIn\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information\nProcessing Systems, volume 30. Curran Asso-\nciates, Inc., 2017.\n[113] D. Watson and L. A. Clark.\nOn traits and\ntemperament: General and specific factors of\nemotional experience and their relation to the\nFive-Factor model.\nJournal of Personality,\n60(2):441\u2013476, 1992.\n[114] D. Wechsler.\nThe measurement of adult\nintelligence (3rd ed.). Williams & Wilkins Co,\nBaltimore, 1946.\n[115] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W.\nYu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.\nFinetuned language models are zero-shot learn-\ners.\nIn International Conference on Learning\nRepresentations, 2022.\n[116] J. Wei, Y. Tay, R. Bommasani, C. Raffel,\nB. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,\nD. Zhou, D. Metzler, E. H. Chi, T. Hashimoto,\nO. Vinyals, P. Liang, J. Dean, and W. Fedus.\nEmergent abilities of large language models.\nTransactions on Machine Learning Research,\n2022.\n[117] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson,\nY. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, and\nT. Ma. Larger language models do in-context\nlearning differently.\nCoRR, abs/2303.03846,\n2023.\n[118] L. Weidinger, J. Uesato, M. Rauh, C. Grif-\nfin,\nP.-S.\nHuang,\nJ.\nMellor,\nA.\nGlaese,\nM. Cheng, B. Balle, A. Kasirzadeh, C. Biles,\nS. Brown, Z. Kenton, W. Hawkins, T. Steple-\nton, A. Birhane, L. A. Hendricks, L. Rimell,\nW. Isaac, J. Haas, S. Legassick, G. Irving, and\nI. Gabriel.\nTaxonomy of risks posed by lan-\nguage models.\nIn Proceedings of the 2022\nACM Conference on Fairness, Accountability,\nand Transparency, FAccT \u201922, page 214\u2013229,\nNew York, NY, USA, 2022. Association for\nComputing Machinery.\n[119] Z. Yao, C. Li, X. Wu, S. Youn, and Y. He.\nA comprehensive study on post-training quan-\ntization for large language models.\nCoRR,\nabs/2303.08302, 2023.\n[120] J. K. Young, Beaujean, and A. Alexander. Mea-\nsuring personality in wave I of the national lon-\ngitudinal study of adolescent health.\nFront.\nPsychol., 2:158, July 2011.\n[121] W.\nYouyou,\nM.\nKosinski,\nand\nD.\nStill-\nwell.\nComputer-based personality judgments\nare more accurate than those made by hu-\nmans.\nProceedings of the National Academy\nof Sciences, 112(4):1036\u20131040, 2015.\n[122] J. Zamfirescu-Pereira, R. Y. Wong, B. Hart-\nmann, and Q. Yang. Why Johnny can\u2019t prompt:\nHow non-AI experts try (and fail) to design\n26\nLLM prompts. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing\nSystems, CHI \u201923, New York, NY, USA, 2023.\nAssociation for Computing Machinery.\n[123] S. Zhang, E. Dinan, J. Urbanek, A. Szlam,\nD. Kiela, and J. Weston. Personalizing dialogue\nagents: I have a dog, do you have pets too?\nIn Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2204\u20132213,\nMelbourne, Australia, July 2018. Association\nfor Computational Linguistics.\n[124] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang,\nY. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,\nY. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang,\nR. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie,\nand J.-R. Wen. A survey of large language mod-\nels. CoRR, abs/2303.18223, 2023.\n[125] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown,\nA. Radford, D. Amodei, P. Christiano, and\nG. Irving. Fine-tuning language models from\nhuman preferences.\nCoRR, abs/1909.08593,\n2020.\n[126] M. Zimmerman. Diagnosing personality disor-\nders: A review of issues and research methods.\nArchives of general psychiatry, 51(3):225\u2013245,\n1994.\n[127] R. E. Zinbarg, W. Revelle, I. Yovel, and\nW. Li. Cronbach\u2019s \u03b1, revelle\u2019s \u03b2, and mcdon-\nald\u2019s \u03c9 h: Their relations with each other and\ntwo alternative conceptualizations of reliability.\nPsychometrika, 70:123\u2013133, 2005.\nAcknowledgements\nWe thank Lucas Dixon, Douglas Eck, and Kathy\nMeier-Hellstern for their feedback on early versions\nof this paper. We also thank David Stillwell for facili-\ntating research access to the Apply Magic Sauce API.\nFinally, we thank Jason Rentfrow and Neda Safaee-\nRad for their advice on personality-related aspects of\nthe paper. G.S-G. is supported by the Bill & Melinda\nGates Foundation through a Gates Cambridge Schol-\narship [OPP1144].\nAuthor Contributions\nM.A., C.C., M.M., M.S., and G.S-G. conceived the\nproject. G.S-G. contributed methodology to establish\nreliability and construct validity and for psychometric\ntest administration and statistical analysis. M.S. con-\ntributed scaled up software infrastructure and prelim-\ninary experiments and investigations. C.C. and M.S.\nimplemented the LLM hosting infrastructure for ex-\nperiments. M.A., M.S., and G.S-G. contributed to the\nconceptual design and analysis of and G.S-G. devised\nand implemented the methods for personality shaping.\nG.S-G. and L.S. designed and M.S., G.S-G., and L.S.\nimplemented the downstream task experiment. C.C.\nand M.S. carried out data visualization.\nM.S. car-\nried out the word cloud analysis. S.F. and P.R. pro-\nvided discussion of LLM mechanisms and analysis of\nLLM performance. A.F., M.M., M.S., and G.S-G. con-\ntributed limitations, future directions, and ethical con-\ncerns discussions. P.R. and L.S. contributed psycho-\nmetrics and statistical feedback. A.F., M.M., M.S.,\nand G.S-G. wrote the manuscript with input from all\nco-authors. A.F., M.M., and M.S. co-supervised the\nproject.\nCompeting Interests\nThis study was funded by Alphabet Inc (\u2018Alphabet\u2019)\nand/or a subsidiary thereof. A.F., C.C., G.S-G., M.M.,\nand Mustafa Safdari were employees of Alphabet at\nthe time of this writing and may own stock as part of\nthe standard compensation package. M.M. is also affil-\niated with the University of Southern California. G.S-\nG. and L.S. are affiliated with the University of Cam-\nbridge. G.S-G. is also supported by the Bill & Melinda\nGates Foundation through a Gates Cambridge Scholar-\nship [OPP1144]. S.F. and P.R. are affiliated with Keio\n27\nUniversity. M.A. is affiliated with the University of\nCalifornia, Berkeley.\nCode Availability\nThe code used to administer psychometric tests\nto LLMs is intended to be interoperable across\nLLMs (i.e., the PaLM models used here).\nIt is\nopen-sourced and available at the Google Research\nGitHub repository for the Psychometric Benchmark\nof Racism, Generalization, and Stereotyping (Psy-\nBORGS; manuscript in prep).3\nThe remaining Python and R code used to gener-\nate our prompt sets and statistically analyze reliability,\nconstruct validity, and trait shaping can be made avail-\nable upon request, and will be added to open-source\nrepositories for wider public use soon.\nData Availability\nThe data generated by the LLMs tested in this work,\neither the psychometric test score data or open-ended\ntext responses to a real-world task prompt, are avail-\nable upon reasonable request. The psychometric tests\nused in this study were accessed from their respective\noriginal publications and, where applicable, public re-\nsearch repositories. We used items of these tests as\nLLM prompt inputs in a non-commercial research ca-\npacity. The authors and copyright holders of these tests\ngovern their availability and use. The 50 Persona De-\nscriptions employed in our structured prompts were re-\nproducibly randomly sampled from the true-cased ver-\nsion4 of the PersonaChat dataset [123]. PersonaChat\nis a publicly available, crowd-sourced dataset of 1,155\nfictional human persona descriptions. For analysis of\npersonality traits on generated text, this study used\nthe Apply Magic Sauce (AMS) API5, a validated psy-\nchodemographic research tool that predicts personality\nfrom open-ended text [55].\n3https://github.com/google-research/google-\nresearch/tree/master/psyborgs\n4https://huggingface.co/datasets/bavard/personachat truecased\n5https://applymagicsauce.com\nA\nLarge Language Models\nA.1\nLanguage Modeling\nLanguage modeling is a fundamental task in natural\nlanguage processing (NLP). It is the basis of many so-\nlutions to a wide variety of problems involving AI sys-\ntems with linguistic inputs. Downstream NLP tasks\nthat leverage language models include (among many\nothers):\n\u2022 natural language understanding,\n\u2022 question answering,\n\u2022 machine translation,\n\u2022 document summarization,\n\u2022 dialog systems.\nThe fundamental goal of language modeling is to as-\nsign high probabilities to utterances (usually sentences\nin plain text) that are likely to appear in data (i.e., be-\nlong to the language) and low probabilities to strings of\nwords that are not. A trained language model can then\nbe used to assign probabilities to arbitrary sequences\nof words. In the past, this was done by parametric sta-\ntistical models estimated from data. However, those\nmodels have been replaced with much more success-\nful deep neural network-based methods. Generally, a\nmodern large language model (LLM) is a neural net-\nwork taking strings of words as input, and returning\na probability measure for each of those strings. The\nnetwork is trained to correspond to the likelihood that\ngiven input strings conform to a particular language,\nas induced from large quantities of text (often called a\ncorpus). Normally, instead of thinking of a language\nmodel in terms of estimating the joint probability of a\nstring of words, we view it in terms of its ability to pre-\ndict continuation based on existing context. A neural\nlanguage model therefore is usually trained to com-\npute a conditional probability of word wn following a\nsequence of words w1, w2, . . . , wn\u22121.\n28\nA.2\nRole of Attention in LLMs\nRecent advances in LLMs and NLP more broadly have\nbeen based on innovative uses of various forms of at-\ntention in neural networks.\nAttention was initially\nintroduced as an improvement to recurrent encoder-\ndecoder architectures [5] in the context of neural ma-\nchine translation systems. Subsequently, it was dis-\ncovered that the idea of attention alone can be used\nas a basis for language modelling systems. A sem-\ninal paper titled \u201cAttention Is All You Need\u201d [112]\nintroduced a new type of neural network architec-\nture for extracting deep contextualized text represen-\ntations from raw natural language data using a process\nbased predominantly on repeated application of the\n\u201cself-attention\u201d operation in a model, called the trans-\nformer. This kind of model transforms the original\nvector space representation of linguistic units through\na sequence of embedding spaces, where each succes-\nsive mapping recomputes the representation of every\ntoken6 in the context of its surrounding tokens. As\nsuch, it allows for the semantics of words as seen by\nthe neural AI systems to vary depending on the context\nand evolve over time. Such representations produced\nsignificant performance improvements on natural lan-\nguage understanding tasks.\nThe transformer archi-\ntecture was composed of two stacks of self-attention\nblocks forming an encoder-decoder architecture, origi-\nnally designed as a sequence transducer for neural ma-\nchine translation.\nA.3\nDecoder-only Architecture\nCurrently, large language models (LLMs) are usu-\nally based on the decoder-only transformer architec-\nture [11, 15, 79, 80, 109]. A sequence of text tokens,\nusually representing a user prompt (e.g., a question)\nis first tokenized, by splitting text into morpheme-\nlike subwords units using a deterministic algorithm in-\n6A token is the smallest unit of text that a large language model\ncan process. Tokens can be individual characters, words, or sub-\nwords, depending on the specific tokenization method used. The\nmodel assigns a unique identifier to each token, and these iden-\ntifiers are then used to represent the text in the model\u2019s internal\nrepresentations.\nspired by information theoretic ideas. This sequence\nof tokens is then embedded into a high-dimensional\nvector space where each token becomes a sequence\nof floating-point numbers. This initial point-cloud of\nvectors representing linguistic units of the prompt is\nthen transformed by a sequence of nonlinear mappings\nbetween high-dimensional representation spaces. The\nfinal representation is used to compute a probability\ndistribution over possible continuations of text con-\nditioned on the original prompt.\nThe predominant\nmethod of training such models is gradient descent\noptimization (i.e., the backpropagation algorithm), re-\nsulting in representations that are informative towards\npredicting the contexts in which words appear within\nthe training corpus. This simple self-supervised crite-\nrion leads to emergent abilities of the model, spanning\nsyntax, semantics, and pragmatics of natural language\nuse. The distributional hypothesis, which forms a fun-\ndamental assumption behind neural language model\ntraining, states that syntactic and semantic relation-\nships between words can be inferred from their con-\ntext, i.e., co-occurrence patterns with other words in\nthe corpus.\nAs a result, optimizing model parame-\nters based on n-grams of tokens extracted from large\nquantities of natural language text generates informa-\ntive representations of linguistic units in submanifolds\nof high-dimensional real vector spaces. The geometric\nand topological features of these induced representa-\ntion manifolds determine the behavior of LLMs. The\nmodels trained for dialogue, including all models used\nin our work, are of the autoregressive type. This means\nthat the output from the model itself becomes part of\nthe context on which future outputs are conditioned.\nThis allows the model to form a contextual memory of\nthe conversation, including its own outputs.\nCurrent state of the art LLMs contain trillions of pa-\nrameters and are trained on corpora of text (such as\nbooks, articles, and websites) and code [21, 14] that\ncontain billions of n-gram patterns, allowing them to\nlearn the statistical relationships between words and\nphrases [116], and consequently the patterns, struc-\ntures, and semantics of language [66, 82, 70, 30]. In\nthis work, we primarily explore decoder-only, auto-\nregressive LLMs such as PaLM [15], where the in-\n29\nput is usually a partial or complete sequence of tokens,\nand the model generates the next token in the sequence\nbased on the previous tokens it has seen in an iterative\nprocess.\nA.4\nControlling LLM behavior\nThere are three main techniques that change or con-\ntrol an LLM\u2019s behavior and output with respect to a\ngiven input: pretraining (training the LLM on a large\ncorpus of text [11, 15, 109]), fine-tuning (i.e., further\ntraining a pretrained LLM on a smaller dataset spe-\ncific to a particular task or domain [125, 115, 79, 81]),\nand prompting. While pretraining and fine-tuning af-\nfect model behavior by directly altering the model\u2019s\nweight parameters, prompting does so indirectly by in-\nfluencing the activation of certain neurons or the flow\nof information through the model\u2019s inference process.\nThe most significant aspect of using prompts to con-\ntrol LLM behavior is to carefully design or engineer\nprompts to generate desired outputs from the LLM.\nSeveral types of prompt engineering techniques are\ncommonly used with LLMs. In few-shot prompting\n[11, 73, 64], a limited amount of example data are\nprovided to the model in a prompt to guide it to per-\nform a task. By leveraging this small set of exam-\nples, the LLM can generalize and produce responses\nbeyond the provided instances. Few-shot prompting\nrelies on the ability to bias the LLM\u2019s responses based\non the input prompt. But because it introduces a bias,\nthis method is not useful in cases where the goal is\nto probe the default bias of the LLM, the behavior\nor tendency of the LLM to produce certain outputs\n(e.g., certain psychometric survey responses, in our\ncase).\nZero-shot prompting [115, 53], on the other\nhand, involves instructing the model to generate re-\nsponses for tasks it has not been specifically trained\non and without providing any examples, relying on\nthe LLM\u2019s pre-existing knowledge and language un-\nderstanding acquired during pre-training. This method\nprovides insights into the language priors and distribu-\ntion learned by the LLM, what tokens are more corre-\nlated than others, etc. For instance, if asked to com-\nplete an input prompt: \u201cShe went to see an expert\nabout her stroke, who\u201d, an LLM trained on medical\ndomain data is likely to respond \u201cadvised her to get\nan ECG test.\u201d whereas an LLM trained on sports data\nmight complete it as \u201ccoached her about the best tech-\nniques from top golf pros.\u201d Several recent works in the\nfield of Responsible AI have attempted to uncover la-\ntent language biases in LLMs, to identify potential for\nharm, and to suggest mitigation techniques [60, 122].\nSimilarly, our work used zero-shot prompt engineering\nto analyze how latent linguistic features in LLMs give\nrise to a coherent personality when quantified psycho-\nmetrically. We further analyzed how those traits can\nbe modified by engineering specific prompts and af-\nfecting the latent linguistic features in these LLMs.\nA.5\nModes of Inference in LLMs\nLLMs offer various ways of inference in practice. In\ngenerative mode, the LLM is given a prompt or in-\nstruction, and it then generates text that is consistent\nwith that prompt. This mode is useful for creative text\ngeneration tasks, such as story or poetry writing. In\nscoring mode, the LLM is given a pair (prompt, con-\ntinuation) and it assigns a score or probability to it,\nindicating its quality or relevance or how likely it is to\nbe generated from that model. Scoring mode [46] is\noften used for tasks like language evaluation [42]. In-\nternally to the LLM, there is a single operating mode\u2014\ncomputing the probability distribution over a sequence\nof tokens\u2014but this distinction between the various\nmodes of inference is conceptually useful when rea-\nsoning about model behavior.\nB\nPersonality Psychology\nThe field of personality psychology defines personal-\nity as enduring characteristics, traits, and patterns that\nshape thoughts, feelings, and behaviors across a di-\nverse array of situations; e.g., social, spatial, and tem-\nporal contexts [93]. Decades of personality research\nsynthesizing evidence from molecular genetics [91],\nevolutionary biology [77], neuroscience [24, 23], lin-\nguistics [10, 87], and cross-cultural psychology [68]\nhave reduced such diverse characteristic patterns to a\n30\ntheorized handful of higher-order factors that define\npersonality [22, 47].\nSpecific to linguistic evidence of a personality tax-\nonomy, a central area of personality research con-\ncerns the lexical hypothesis of personality\u2014that hu-\nman personality is intrinsically connected to language.\nSince its origin from Sir Francis Galton in the 1800s\n[29], empirical research on the lexical hypothesis has\nposited that 1) important personality characteristics of\na given society will be encoded in its language; and\n2) that the most important of those characteristics are\nlikely encoded as single words [31, 90, 96]. This em-\npirical framework grounds our work in three areas: the\nchoice of one of our personality instruments (the BFI;\ndescribed below), our prompts for shaping LLM per-\nsonality, and the choice of the language-based assess-\nment of personality for rating LLM-synthesized per-\nsonality in a downstream task.\nThe Big Five model [48], the most commonly cited\nresearch taxonomy of personality formed through the\nresearch described above, identifies five personality\ntrait dimensions (i.e., domains) and provides method-\nology to assess these dimensions in humans.\nThe\nfive dimensions are extraversion (EXT), agreeable-\nness (AGR), conscientiousness (CON), neuroticism\n(NEU), and openness to experience (OPE). Each do-\nmain is further composed of various lower-order facets\nnested underneath.\nC\nRelated Work\nRecent attempts to probe personality and\npsy-\nchopathological traits in LLMs suggest that some\nmodels exhibit dark personality patterns [59], or\ndemonstrate how to administer personality inventories\nto LLMs [86, 50, 44, 104, 13, 103, 45]. Some have also\nmade efforts to induce desired levels of personality\nin LLMs using prompting [44, 13, 45] or fine-tuning\n[50, 59]. While these works outlined the utility and\nimportance of measuring social phenomena in LLMs\n[86], there remains a need to match standards of eval-\nuating the quality of human survey data when evaluat-\ning survey response data from LLMs\u2014standards that\nare commonplace in quantitative social science [18].\nTo claim that scores on a psychological test are trust-\nworthy and meaningful signals of what the test pur-\nports to measure, one must establish the test\u2019s reliabil-\nity and construct validity.\nRecent works that probe social and personality-\nrelated traits in LLMs have administered and analyzed\nquestionnaires in ways that are unconventional in psy-\nchometrics. In this appendix, we focus on two addi-\ntional elements not discussed in the main text. First,\nresearchers collected LLM responses in the form of\ngenerated completions, often in dialog mode.\nFor\ninstance, [108] administered psychological emotion\nmeasures to LLMs in the form of a research interview\ntranscript, where a fictitious researcher posed measure\nitems to a fictitious participant, who was instructed to\nrespond to these items on a numeric scale. In psycho-\nmetrics, questionnaire-based methods of assessment\nare distinct from interview-based methods. Human an-\nswers to both questionnaires and structured interviews\nmeasuring the same underlying construct do not nec-\nessarily converge (e.g., in the case of measuring per-\nsonality disorders [126]). Indeed, administering ques-\ntionnaires in this way to LLMs creates an arbitrary\nviewpoint from which to elicit personality traits, and is\nlikely biased by the ordering of the questionnaire itself\n[57] and prompting the LLM to respond in an inter-\nview setting (where it may respond differently know-\ning an interviewer is observing). Each LLM response\nto a given questionnaire item was not an independent\nevent, but considered all previous responses shown\nin the transcript. Second, the LLMs in these studies\nwere not used deterministically. This not only ham-\npers reproducibility, but also poses implications for\nreliability. Computing reliability metrics for question-\nnaires scored in this unconventional way is precarious\nbecause such reliability metrics depend on item-level\nvariance. If this item-level variance is contaminated by\nvariation introduced by the model parameters in a dif-\nferent way for each item, it is difficult to compute valid\nindices of reliability. We overcame these challenges in\nour work by proposing a prompt and persona sampling\nmethodology that allows variance to be linked across\nadministrations of different measures.\n31\nPsyBORGS [99] administered a series of validated\nsurvey instruments of race-related attitudes and social\nbias to LLMs using psychometrics-informed prompt\nengineering. Our work utilized the PsyBORGS frame-\nwork.\nD\nTested Language Models\nFirst, we focused on three different model sizes: small\n(8B), medium (62B), and large (540B), because LLM\nmodel size is a key determinant of performance for this\nmodel family [15, 124]. Second, because we are also\ninterested in evaluating LLM personality in the Q&A\ncontext, we investigated PaLM models variants, fine-\ntuned to follow instructions as they have been shown\nto perform better than base models for prompting-\nbased Q&A tasks [115]. We specifically selected vari-\nants fine-tuned with the popular FLAN dataset [115].\nThird, we examined traditional and high-data train-\ning methods, known as Chinchilla training [40], which\nuses a fixed training budget to find the balance between\nmodel size and training dataset scale. Chinchilla train-\ning yields superior performance across a broad set of\ntasks [40, 124]. Table 2 lists the tested models along\nwith their size and training configuration options.\nAll experiments used quantized models [119] to re-\nduce the memory footprint and speed up inference\ntime.\nE\nSelected Personality Inventories\nTo measure personality,\nwe selected two well-\nestablished psychometric measures to assess the Big\nFive taxonomy: one from the lexical tradition and\none from the questionnaire tradition. Lexical tradition\nmeasures are grounded in the hypothesis that person-\nality can be captured by the adjectives found in a given\nlanguage [29, 31], while questionnaire tradition mea-\nsures are developed with existing (and not necessarily\nlexical) taxonomies of personality in mind [102]. Lex-\nical measures may be better suited for LLMs because\nthey are language-based and rely on adjectival descrip-\ntions. We posit that questionnaire measures, which do\nnot rely on trait adjectives for content, more conser-\nvatively test LLM abilities, as they are less abstract\nand more contextualized. Our work focused on Big\nFive measures of personality due to the Big Five\u2019s in-\ntegrative robustness and cross-theory convergence in\nthe human personality and psycholinguistics literature\n[102].\nOur primary personality measure, the IPIP-NEO\n[33], is a 300-item open source representation of the\ncommercialized Revised NEO Personality Inventory\n[19]. The IPIP-NEO, hailing from the questionnaire\ntradition [102], involves rating descriptive statements\n(e.g., \u201c[I] prefer variety to routine\u201d; 60 per Big Five\ndomain) on a 5-point Likert scale. (1 = very inaccu-\nrate; 2 = moderately inaccurate; 3 = neither accurate\nnor inaccurate; 4 = moderately accurate; 5 = very ac-\ncurate). We refer to these statements as items. The\nIPIP-NEO has been translated and validated in many\nlanguages, facilitating cross-cultural research across\npopulations [43], and has been used in longitudinal\nstudies to assess personality change and stability over\ntime [120]. We chose this measure for its excellent\npsychometric properties, shown in [33].\nAs a robustness check and to assess convergent va-\nlidity, we also measured LLM-synthesized personality\nusing the Big Five Inventory (BFI) [48]. Developed\nin the lexical tradition, the BFI is a brief (44-item), ad-\njectival statement-based measure of the broad Big Five\ntraits. The BFI asks participants to rate short descrip-\ntive statements (e.g., \u201cI see myself as someone who is\ntalkative\u201d) also on a 5-point Likert scale. The result-\ning summary scores indicating levels of Big Five trait\ndomains range from 1.00 to 5.00. In the psychology\nliterature [102], the BFI has demonstrated excellent re-\nliability (mean \u03b1 reported across domain subscales =\n0.83), convergent validity, and external validity.\nDomain subscale scores across both measures were\ncalculated following their original instructions as\nthe average of item response values, accounting for\nreverse-keyed items. Possible subscale scores ranged\nfrom 1.00 to 5.00, indicating the lowest and highest\npossible levels of a given Big Five domain, respec-\ntively.\n32\nTable 5: Item Postambles used to construct the prompts employed in the experiments to generate LLM-simulated\nsurvey responses. All administered measures used a Likert-type response scale that allowed 5 possible choices, with the\nexception of the PVQ-RR, which used a 6-point response scale. Item Postambles 1\u20135 were used for the BFI; 6\u201310 for the\nIPIP-NEO; 11\u201315 for the PANAS; 16\u201320 for the SSCS; 21\u201325 for the BPAQ; and 26\u201330 for the PVQ-RR.\nNo.\nItem Postambles\n1.\nplease indicate the extent to which you agree or disagree on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 =\n\"disagree a little\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n2.\nplease rate your level of agreement on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a little\", 3 =\n\"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n3.\nplease rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a\nlittle\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n4.\nplease rate how much you agree on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a little\", 3 = \"neither\nagree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n5.\nplease rate how much you agree or disagree on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a little\",\n3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n6.\nplease rate how accurately this describes you a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 = \"moderately\ninaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n7.\nplease indicate how accurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 = \"moderately\ninaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n8.\nplease indicate how accurate or inaccurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 =\n\"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n9.\nplease rate how accurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 = \"moderately\ninaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n10.\nplease rate how accurate or inaccurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 =\n\"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n11.\nindicate to what extent you agree on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\", 2 = \"agree a\nlittle\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n12.\nplease rate your level of agreement on a scale from 1 to 5, (where 1 = \"very slightly or not at all agree\", 2 = \"agree a\nlittle\", 3 = \"agree moderately\", 4 = \"agree quite a bit\"\n13.\nplease rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\",\n2 = \"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n14.\nplease rate how much you agree on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\", 2 = \"agree a little\",\n3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n15.\nplease rate how much you agree or disagree on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\", 2 =\n\"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n16.\nplease decide to what extent this describes you on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 = \"disagree\", 3 =\n\"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n17.\nplease rate your level of agreement on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 = \"disagree\", 3 = \"neither\nagree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n18.\nplease rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 = \"disagree\",\n3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n19.\nplease rate how much you agree that this describes you on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 =\n\"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n20.\nplease rate how much you agree or disagree that this describes you on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2\n= \"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n21.\nrate how characteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely uncharacteristic of me\", 2 =\n\"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 = \"characteristic of me\", and 5 =\n\"extremely characteristic of me\"):\"\n22.\nplease rate how characteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely uncharacteristic of me\", 2 =\n\"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 = \"characteristic of me\", and 5 =\n\"extremely characteristic of me\"):\"\n23.\nplease rate how characteristic or uncharacteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely\nuncharacteristic of me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 =\n\"characteristic of me\", and 5 = \"extremely characteristic of me\"):\"\n24.\nplease indicate to what extent this is characteristic of you on a scale from 1 to 5 (where 1 = \"extremely uncharacteristic\nof me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 = \"characteristic of me\",\nand 5 = \"extremely characteristic of me\"):\"\n25.\nplease indicate to what extent this is characteristic or uncharacteristic of you on a scale from 1 to 5 (where 1 =\n\"extremely uncharacteristic of me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\",\n4 = \"characteristic of me\", and 5 = \"extremely characteristic of me\"):\"\n26.\nthink about how much that person is or is not like you.\nRate how much the person described is like you on a scale from 1\nto 6 (where 1 = \"not like me at all\", 2 = \"not like me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\",\nand 6 = \"very much like me\"):\"\n27.\nplease rate how characteristic this is of you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not like me\", 3\n= \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n28.\nplease rate how characteristic or uncharacteristic this is of you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2\n= \"not like me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n29.\nplease indicate to what extent this is like you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not like me\",\n3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n30.\nplease indicate to what extent this is or is not like you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not\nlike me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n33\nTable 6: 50 human Persona Descriptions sampled from the PersonaChat dataset [123], used in Item Preambles\nacross all experiments.\nPersona Descriptions\nI like to garden. I like photography. I love traveling. I like to bake pies.\nI\u2019ve a beard. I graduated high school. I like rap music. I live on a farm. I drive a truck.\nI blog about salt water aquarium ownership. I still love to line dry my clothes. I\u2019m allergic to peanuts. I\u2019ll one day own a ferret. My mom raised me by herself and\ntaught me to play baseball.\nSince young I ve loved to cook. I auditionated in a cooking show. I think I\u2019ve talent for it. I took classes while growing up.\nMy name is tom. I try to watch what I eat. I enjoy eating italian food. Pizza is my favorite. I am east asian.\nI live by a lake. I am a mother. I own a custom upholstery shop. I\u2019m a wife.\nI enjoy working out and learning new things. I\u2019m a student in college. I\u2019m studying software development. I play the guitar.\nI\u2019ve three dogs at home. I hate to workout, but I need to. I am very good at the drums. I have a bicycle. I need to take my blood sugar everyday.\nI work in advertising. My mother is dead. I like to hike. I\u2019ve a golden retriever. I write fiction for fun.\nI can never decide between a chili corn dog and a cheesy hot dog. I drive more than an hour each way to work. I prefer the night to the day, but I love sunshine. I am\na grandparent at 44.\nI like to smell my own farts. My beer gut is so huge i\u2019ven T seen my feet in two years. I am from San Fransico. I am always the one who buys the beers. I like to\nplace blame on other people even when I know it is my fault.\nI lived most of my life not knowing who Bob marley was. When I cut loose, I lose control. We help each other out in my family. I despise my boss. I work over 60\nhours a week as a restaurant manager.\nI prefer the simpler times. I like simple jokes. Some jokes go too far. I like the flintstones.\nIt is my universe, and everyone else is just a character in it. I work as a dental assistant in a ritzy part of town. I\u2019ve borderline personality disorder. At night, I party\nhard in the Atlanta club scene, and I never miss a music festival.\nI watch a lot of tv. I live alone. My favorite food is a cheeseburger. I enjoy fishing. I work on cars for a living.\nI\u2019m an animal rights activist. I hope to retire to Florida. I played in a band for 17 years. My mother and father are both in the church choir.\nI\u2019ve taken formal music lessons since I was 5. I\u2019m a musician. My best friend is in a band with me. I wish I could spend more time at home.\nI grew up in Kentucky. I\u2019m a veteran. My favorite book is ender\u2019s game. I have a garden. I like to read.\nI am a vegan. I love country music. I love the beach. I like to read.\nI\u2019ve depression and anxiety so I don\u2019t really go out a lot. I work at home, editing. I have a cat. I hope to move out soon.\nMy favorite food is mushroom ravioli. I ve never met my father. My mother works at a bank. I work in an animal shelter.\nI love kids and dogs. I like to go shopping with my daughters. I like to cook. I love to chat with my friends.\nI swim often. I run track. I wear glasses all day. I take medication.\nI like to go on long hikes. I like to play volleyball. I like to come up with new hairstyles. I like to do my nails.\nI watch Jimmy Fallon s show every night. I have never kissed a woman. People notice how organized I am. I believe that I can achieve anything.\nI drive a lifted Chevy truck. I played football in high school. I am a roofer. I always have a beer after work.\nI love animals. My father worked for Ge. Green is my favorite color. I enjoy playing tennis. I\u2019m an aspiring singer.\nI try to watch what I eat. I enjoy eating italian food. Pizza is my favorite. My name is tom. I am east asian.\nIn allergic to peanuts. I like eating vegetables. I love the Beatles. I\u2019m usually very shy. I have trouble getting along with family.\nI go to high school. Math is my favorite subject. I live in the United States. I am a boy.\nI have a job as an it agent. I like smoking weed. My dad works for stifle. I love rap music. I\u2019m a meataholic.\nI work in tv. I do not treat my girlfriend very well. I like to cook breakfast on sundays. I love to sing. I am a lesbian.\nI work on semi trucks for a living. My father was a driver himself. I got off the road when I married my sweetheart. I want to take her on vacations one day. My\nmotor never stops running.\nI own a Iphone 7. I drink hot chocolate during the winter. I\u2019m allergic to seafood. My mother use to read me bed time stories.\nI am eighteen years old. I\u2019m going to majoring in business. I just bought my first car. I received a full scholarship to Florida state university.\nI live in a tiny house to save money. I collect single malt scotch. I listen to blues and jazz. I tend bar on the weekends. During the week I go to college to become a\nlawyer.\nI love to go horseback riding whenever I can. I\u2019m a mother of two beautiful boys. My family and I go camping every month. My favorite artist is Justin Bieber.\nI especially enjoy listening to the band the lumineers. I enjoy reading and walking on sunny days. I\u2019m a happy person. I sing many songs.\nI play piano. My favorite color is yellow. My boyfriend is in the army. My father is dead. My hair is short.\nI\u2019m a mother. I\u2019m a nurse at a hospital. My favorite band is the rolling stones. I love to read and cook. My favorite food is mexican food.\nI deliver baked goods in the state where I live. My favorite hobby is playing recreational baseball. I spend my weekends camping. I\u2019m a truck driver. My wife and\ntwo kids camp with me.\nI am argentinian. I like to wear boots. I have many girlfriends. I like to eat beef. I like to ride horses.\nI recently had a private lunch with will ferrell. I am trying to become a male model in hollywood. I\u2019m a huge fan of classical jazz. I am on a low carb diet.\nI want to put my photos to a music video staring Adam Levin. I want to travel the world taking photographs of my travels. I am a widow. I want to be a famous\nphotographer.\nI am in the army. I fly airplanes. I enjoy building computers. I dropped out of college.\nI have three children. I live in the suburbs of a major city. I like to garden. I graduated college for secondary english education.\nI play guitar in the local band. I live on a small farm in Ohio. I am the youngest of three brothers. I have never been to the city.\nI\u2019m a widow. I want to put my photos to a music video staring Adam Levin. I want to travel the world taking photographs of my travels. I want to be a famous\nphotographer. I like taking pictures.\nI still live at home with my parents. I play video games all day. I\u2019m 32. I eat all take out.\nMy friend once bought me a car. I am disabled and cannot walk. I take vitamin c when I have a cold. I do not eat bread. My favorite season is winter.\n34\nF\nSimulating\nPopulation\nVariance\nThrough Prompting\nIt was empirically necessary to introduce controlled\nvariation in LLM-simulated survey data to assess their\nreliability and statistical relationships with outcomes\nof interest; in short, controlled variation was required\nto statistically test for reliability and construct validity.\nFor instance, an Item Postamble presented the possi-\nble standardized responses the model can choose from,\ne.g.,\nplease rate your agreement on a scale from\n1 to 5, where 1 is \u2018strongly disagree\u2019, 2 is\n\u2018disagree\u2019, 3 is \u2018neither agree nor disagree\u2019,\n4 is \u2018agree\u2019, and 5 is \u2018strongly agree\u2019.\nWe customized five variations of Item Postambles\nfor each administered measure, such that all five vari-\nations would have parallel meanings across measures.\nSupplemental Table 5 lists all Item Postambles used\nin this work. This prompt design enabled thousands\nof variations of input prompts that could be tested,\nwith two major advantages. First, variance in psycho-\nmetric test responses created by unique combinations\nof the Persona Descriptions (see Supplemental Table\n6), Item Instructions (see Supplemental Table 7), and\nItem Postambles enabled us to quantify the validity\nof personality measurements in LLMs. Unlike single\npoint estimates of personality, or even multiple esti-\nmates generated from random resampling of LLMs,\ndiverse distributions of personality scores conditioned\non reproducible personas make it possible to compute\ncorrelations between convergent personality measures\nand external, personality-related constructs. Second,\nvariance in Item Preambles and Postambles facilitated\na built-in robustness check: it was critical to know if\npersonality scores remained reliable and valid across\nmodifications of context and instructions surrounding\noriginal test items. They were indeed reliable and valid\nfor three of the five models tested.\nTable 7:\nItem Instructions used in Item Preambles\nacross experiments to generate LLM-simulated survey\nresponses.\nItem Instructions\nConsidering the statement,\nThinking about the statement,\nReflecting on the statement,\nEvaluating the statement,\nRegarding the statement,\nG\nPsychometrics\nPsychometrics, a quantitative subfield of psychology\nand education science, encompasses the statistical the-\nory and technique of measuring unobservable, latent\nphenomena called constructs, like personality, intelli-\ngence, and moral ideology. Psychometrics is founda-\ntional to the development and validation of standard-\nized educational tests (e.g., the SAT, LSAT, GRE) [3],\nmedical and psychological clinical assessments [114],\nand large-scale public opinion polls [37].\nPsychometric tests (e.g., survey instruments, mea-\nsures, multi-item scales) are tools for quantifying la-\ntent psychological constructs like personality.\nPsy-\nchometric tests enable statistical modeling of the true\nlevels of unobservable target constructs by relying\non multiple indirect, yet observable, measurements\nacross a sample of individuals drawn from a wider\npopulation. We refer to items as the individual ele-\nments (i.e., descriptive statements, sometimes ques-\ntions) used within a psychometric test designed to\nmeasure attributes or characteristics of a construct.\nItems are usually rated on a rating scale- a standard-\nized set of response choices that allows researchers to\nquantify subjective phenomena. A Likert-type scale\nis the most common rating scale that has respondents\nspecify their level of agreement on a symmetric agree-\ndisagree scale [61]. We refer to a subscale as a collec-\ntion of items, usually resulting from a factor analysis,\naimed at measuring a single psychological construct.\nMeasures are themed collections of subscales.\nFor example, the Big Five Inventory (BFI) [48] is\n35\na popular measure of personality; it comprises five\nmulti-item subscales targeting each Big Five dimen-\nsion.\nBFI Extraversion, for instance, is a subscale\nwithin the BFI specifically targeting the dimension of\nextraversion. An example item under BFI Extraver-\nsion would read, \u201c[I see myself as someone who] is\ntalkative.\u201d Participants rate their agreement with this\nitem using the following 5-point Likert-type rating\nscale: 1 = disagree strongly; 2 = disagree a little; 3\n= neither agree nor disagree; 4 = agree a little; 5 =\nagree strongly.\nHow do we know that psychometric tests measure\nwhat they claim to measure, i.e., how do we establish\nthe reliability, accuracy, and utility of the measures of\npersonality, and the constructs assessed in those mea-\nsures? Validated scientific frameworks for establish-\ning the reliability and construct validity of a new psy-\nchometric test [17, 71, 18] incorporate (but are not lim-\nited to) the following overarching standards:\n\u2022 Reliability: Are test measurements dependable\nand consistent? In psychometrics, a test\u2019s relia-\nbility can be established in terms of internal con-\nsistency and factor saturation.\n\u2013 Internal consistency reliability: Is the test\nreliable across multiple measurements (i.e.,\nits items)?\nIn other words, do responses\nto the test\u2019s items form consistent patterns?\nAre test items correlated with each other?\n\u2013 Factor saturation: Do the test\u2019s items re-\nflect the variance of one underlying factor\nor construct?\n\u2022 Construct Validity: Do the test measurements\nactually reflect the underlying construct? This\ncan be established by checking for convergent va-\nlidity, discriminant validity and criterion validity.\n\u2013 Convergent Validity: Does the test corre-\nlate with purported indicators (i.e., conver-\ngent tests) of the same or similar psycho-\nlogical construct? These correlations are\ncalled convergent correlations.\n\u2013 Discriminant Validity: Relative to their\nconvergent correlations, are test scores rel-\natively uncorrelated with scores on theoret-\nically unrelated tests? These correlations\nare called discriminant correlations.\n\u2013 Criterion Validity: Does the test correlate\nwith theoretically-related, non-tested phe-\nnomena or outcomes?\nG.1\nReliability:\nIs the Measurement De-\npendable?\nThe hallmark characteristic of a good psychometric\ntest (or any empirical measure) of a target construct\nis its reliability, which reflects its ability to \u201cmea-\nsure one thing (i.e., the target construct) and only that\nthing, as precisely as possible\u201d [18]. In this work, we\nbalance our evaluations of reliability across three in-\ndices of reliability\u2014Cronbach\u2019s Alpha (\u03b1), Guttman\u2019s\nLambda 6 (\u03bb6), and McDonald\u2019s Omega \u03c9\u2014weighing\nthe pros and cons of each.\n\u03b1, the most widely-known measure of internal con-\nsistency reliability, captures how responses to each\nitem of a scale correlate with the total score of that\nscale [20]. However, \u03b1 has many documented limita-\ntions. For instance, it relies on the assumption that all\nitems of a test measure the same underlying construct\nand it can be artificially inflated by a test\u2019s number of\nitems [127]. Cronbach\u2019s \u03b1 is computed as follows:\n\u03b1 =\nk\nk \u2212 1\n \n1 \u2212\nPk\ni=1 \u03c32\ny\n\u03c32x\n!\n(1)\nwhere k is the number of items on the test, \u03c32\ny is the\nvariance associated with each item i, and \u03c32\nx is the\noverall variance of total scores.\nIn contrast to \u03b1, \u03bb6 evaluates the variance of each\nitem that can be captured by a multiple regression of\nall other items [35]. It is less biased alternative to \u03b1\nbecause it is not affected by item differences in vari-\nance, although it is also biased by the number of items\non a test. Guttman\u2019s \u03bb6 is calculated as:\n\u03bb6 = 1 \u2212\nPk\ni=1(e2\ni )\nVx\n(2)\n36\nwhere k is the number of items on the test, ei is the\nerror term for item i, Vx is the variance of the to-\ntal test score. To test more robustly for reliability (in\nterms of how well a test measures one underlying fac-\ntor or construct) in a way that is unaffected by num-\nber of items on a test, psychometricians compute Mc-\nDonald\u2019s Omega (\u03c9) [69, 127]. This metric is gener-\nally considered a less biased composite test of reliabil-\nity [127, 34]. McDonald\u2019s \u03c9 uses confirmatory factor\nanalysis to determine if items statistically form a sin-\ngle factor, or actually measure separate factors. It is\ncalculated as:\n\u03c9h =\n1\nk\nPk\ni=1\nt2\ni\n\u03c32\ni\n1\nk\u22121\nPk\ni=1\nt2\ni\n\u03c32\ni \u2212 1\nk\n1\n1\u2212r2\ntt\n(3)\nwhere \u03c9h is McDonald\u2019s hierarchical omega, k is the\nnumber of items on the test, ti is the standardized item\nscore for item i, \u03c32\ni is the variance of the standardized\nitem score for item i, and rtt is the correlation between\nthe total test score and the standardized total test score.\nG.2\nConstruct Validity: Is the Measurement\nValid?\nSince psychometric tests measure physically unob-\nservable constructs, such as personality traits, it is im-\nperative to establish that such tests measure what they\nclaim to measure. This process is called establishing\na test\u2019s construct validity. Construct validity is a com-\nprehensive judgement of how the scores and the theo-\nretical rationale of a test reasonably reflect the under-\nlying construct the test intends to measure [72]. Re-\ncently, construct validity has become a crucial focus\nof AI responsibility and governance [41, 76]: opera-\ntionalizing social phenomena in algorithmic systems\nin a principled way (e.g., through construct valida-\ntion) is a core part of responsible AI. Bringing em-\npirical rigor to the measurement of social constructs\nhelps stakeholders make more informed judgments of\ncharacteristics that may be fair or harmful in AI sys-\ntems. For instance, if low agreeableness is harmful in\nAI systems, we need a principled way to measure it.\nThere is extant work on establishing the validity\nof measurements of personality as a theoretical con-\nstruct [93, 22, 47], a powerful predictor of other impor-\ntant human traits and life outcomes [92, 9, 56] and its\nmanifestation in human language [31, 90, 96], which\nforms the basis of LLMs. However, establishing the\nvalidity of measurements of personality as a mean-\ningful construct in LLMs has not yet been addressed.\nConvergent and Discriminant Validity: In psycho-\nmetrics, the convergent and discriminant validity of a\ntest are evaluated using Campbell\u2019s classic framework\n[12], where a test\u2019s convergent validity is established\nby \u201csufficiently large\u201d correlations with separate tests\nmeant to measure the same target construct. For exam-\nple, to validate a new test measuring depression, one\ncould calculate the test\u2019s convergent correlations with\nthe Beck Depression Inventory (BDI) [6]\u2014a widely-\nused measure of depression. To evaluate the discrim-\ninant validity of a test, psychometricians commonly\ngauge the extent to which the test\u2019s convergent correla-\ntions are stronger than its discriminant correlations\u2014\nits correlations with test of other constructs. As a con-\ncrete example, a new test of depression should corre-\nlate more strongly with the BDI than with, say, a test\nmeasuring English proficiency.\nCriterion Validity: A common way to assess the\ncriterion validity of a new psychometric test is to\ncheck its correlations with theoretically related exter-\nnal (non-test) criteria (hence the name, criterion valid-\nity) [18]. For example, to validate a new psychometric\ntest of depression, one could test if it is substantially\nrelated to a known external criterion, like negative af-\nfect.\nH\nMethods for Constructing the Va-\nlidity of LLM Personality Test\nScores\nEstablishing Reliability\nIn LLM research, model\nresponses to a series of seemingly related tasks in-\ntended to measure one latent construct may be anec-\ndotally \u201cconsistent\u201d [86, 50] or inconsistent [74]. De-\nscriptive consistency, however, is not sufficient evi-\n37\nTable 8: Criterion validity subscales per tested Big Five domain. PANAS = Positive and Negative Affect Schedule\nScales; BPAQ = Buss-Perry Aggression Questionnaire; PVQ-RR = Revised Portrait Values Questionnaire; SCSS = Short\nScale of Creative Self.\nIPIP-NEO Domain\nExternal Criterion\nCriterion Subscales\nExtraversion\nTrait Emotion\nPANAS Positive Affect\nPANAS Negative Affect\nAgreeableness\nAggression\nBPAQ Physical Aggression\nBPAQ Verbal Aggression\nBPAQ Anger\nBPAQ Hostility\nConscientiousness\nHuman Values\nPVQ-RR Achievement\nPVQ-RR Conformity\nPVQ-RR Security\nNeuroticism\nTrait Emotion\nPANAS Negative Affect\nPANAS Positive Affect\nOpenness\nCreativity\nSSCS Creative Self-Efficacy\nSSCS Creative Personal Identity\ndence that the responses to those tasks are statistically\nreliable reflections of the latent constructs they target\n(as described in Section G.2). To establish internal\nconsistency reliability, we compute Cronbach\u2019s \u03b1 (1)\nand Guttman\u2019s \u03bb6 (2) on all IPIP-NEO and BFI sub-\nscales. To assess more complete composite reliability\nwe compute McDonald\u2019s \u03c9 (3) on all IPIP-NEO and\nBFI subscales.\nWe designate a given reliability metric (RM; i.e., \u03b1,\n\u03bb6, \u03c9) < 0.50 as unacceptable, 0.50 \u2264 RM < 0.60\nas poor, 0.60 \u2264 RM < 0.70 as questionable, 0.70 \u2264\nRM < 0.80 as acceptable, 0.80 \u2264 RM < 0.90 as\ngood, and RM \u2265 0.90 as excellent. The high levels\nof singular internal consistency metrics like \u03b1 are nec-\nessary but not sufficient conditions for demonstrating\ncomplete reliability. Therefore, for the purpose of the\ncurrent work, \u03b1, \u03bb6, and \u03c9 must be at least 0.70 for a\ngiven subscale to be deemed acceptably reliable.\nEstablishing Construct Validity\nWe operationalize\nconstruct validity in terms of convergent, discrimi-\nnant, and criterion validity (see Appendix G.2). We\nused Campbell\u2019s classic multitrait-multimethod matrix\n(MTMM) [12] approach to evaluate convergent and\ndiscriminant validity. Criterion validity is evaluated by\ncorrelating LLM-simulated personality test data with\nLLM responses to theoretically-related psychometric\ntest.\nConvergent validity:\nWe evaluated convergent\nvalidity\u2014how much our primary test of personal-\nity (the IPIP-NEO) positively relates to another pur-\nported test of personality (BFI)\u2014by computing bivari-\nate Pearson correlations between IPIP-NEO and BFI\nscores for extraversion, agreeableness, conscientious-\nness, neuroticism, and openness and comparing them\nto ensure correlations between each domain subscale\nare the strongest of their row and column, as out-\nlined in [12]. For instance, IPIP-NEO Extraversion\nshould be most correlated with BFI Extraversion, be-\ncause these two subscales should convergently mea-\nsure the same underlying construct.\nWe\noperationalize\nconvergent\ncorrelations\nbe-\ntween two psychometric tests (in this case, Big\nFive\nsubscales\nfrom\nthe\nIPIP-NEO\nand\nBFI)\n{(x1, y1), . . . , (xn, yn)},\nreflecting\nn\npairs\nof\ncontinuous score data, as Pearson product-moment\n38\ncorrelations:\nrxy =\nPn\ni=1(xi \u2212 \u00afx)(yi \u2212 \u00afy)\npPn\ni=1(xi \u2212 \u00afx)2pPn\ni=1(yi \u2212 \u00afy)2\n(4)\nwhere n is the sample size, xi, yi are a pair of data\npoints i from sample, \u00afx is the sample mean score for\npersonality trait x of the IPIP-NEO, and \u00afy is the sam-\nple mean score for corresponding personality trait y of\nthe BFI.\nIn the resulting MTMM, we consider at least strong\ncorrelations (|rxy| \u2265 0.60; [25]) between each IPIP-\nNEO domain subscale and its BFI domain scale coun-\nterpart (e.g., r(IPIP-NEO Extraversion, BFI Extraver-\nsion), r(IPIP-NEO Agreeableness, BFI Agreeable-\nness), etc.) as evidence of convergent validity. For\nthese and following results, we used cut-offs recom-\nmended by [25] for considering correlations as mod-\nerate, strong, and very strong (viz. .40 \u2264 |r| < .60;\n.60 \u2264 |r| < .80; .80 \u2264 |r|; respectively). In our tests\nfor convergent validity, strong convergent correlations\nbetween an LLM\u2019s IPIP-NEO and BFI scores indicate\nthat we are capturing the same underlying signals of\neach personality domain even when we measured them\nusing two separate instruments. Weak convergent cor-\nrelations indicate that at least one of the personality\ndomain subscales is not capturing these signals prop-\nerly.\nDiscriminant Validity: We assessed the discrim-\ninant validity of the IPIP-NEO for LLMs through\nhow its domain subscales remained relatively unre-\nlated with their respective discriminant subscales. To\ndo so, we compared each convergent correlation be-\ntween the IPIP-NEO and BFI with all other corre-\nlations (i.e., discriminant correlations) located in the\nsame row or column of the MTMM. Discriminant va-\nlidity was established for a personality domain sub-\nscale when the average difference (\u2206) between its con-\nvergent correlation and respective discriminant corre-\nlations was at least moderate (\u2265 0.40). For example,\na given model\u2019s IPIP-NEO Extraversion scores were\ntested for discriminant validity by being sufficiently\nmore positively correlated with BFI Extraversion than\nwith BFI Agreeableness, Conscientiousness, Neuroti-\ncism, and Openness, according to this average differ-\nence metric.\nCriterion Validity: As reported Section 2.1.2, we\nevaluated the criterion validity of our LLM personal-\nity test data in three steps. First, for each Big Five\ndomain, we identified at least one theoretically-related\nexternal (viz. non-personality) construct reported in\nhuman research. Next, according to this existing hu-\nman research, we selected appropriate psychometric\ntests to measure these related constructs and adminis-\ntered them to LLMs (Supplemental Table 8 shows the\n11 criterion subscales). Finally, we correlated LLM\nscores for each IPIP-NEO subscale with these external\nmeasures.\nI\nPersonality Assessment Results\nI.1\nDescriptive Statistics Across Models\nWe inspected the test scores on the IPIP-NEO and BFI\nacross models to check if they reflected a normal dis-\ntribution without many outliers. We examined how the\ndistributions shifted as a function of model size (hold-\ning model training method constant) and model train-\ning method (holding model size constant). Figure 6\nsummarizes the findings.\nBy model configuration: At 62B parameters, the\nbase PaLM model showed nearly uniform personal-\nity score distribution for both the IPIP-NEO and BFI,\nwith 25th, 50th, and 75th percentile values identical\nwithin each BFI domain. Instruction-tuned variants,\nFlan-PaLM and Flan-PaLMChilla, showed more nor-\nmal distributions of personality, with lower kurtosis.\nBy model size: Flan-PaLM IPIP-NEO (Figure 6a)\nand BFI (Figure 6b) scores were stable across model\nsizes.\nMedian levels of socially-desirable BFI sub-\nscales (EXT, AGR, CON, OPE) substantially in-\ncreased as model size increased (see Supplemental\nTable 9).\nIn contrast, median levels of BFI NEU\ndecreased (from 2.75 to 2.38) as model size in-\ncreased from 8B to 540B parameters. Distributions\nof IPIP-NEO scores were more stable across sizes of\nFlan-PaLM: only IPIP-NEO EXT and CON showed\nnoticeable increases by model size.\nFor instance,\nacross sizes of Flan-PaLM, median levels of IPIP-\n39\n(a) IPIP-NEO\n(b) BFI\nFigure 6: Distributions of a) IPIP-NEO and b) BFI personality domain scores across models. Box plots depict\nmodel medians (shown as middle lines; also reported in Supplemental Table 9) surrounded by their interquartile ranges\nand outlier values. Flan-PaLM models of increased size, from 8B to 540B: a) IPIP-NEO scores are relatively more stable\ncompared to b) BFI scores, where scores for socially-desirable traits increase while NEU scores decrease.\nNEO OPE remained close to 3.30. Meanwhile, me-\ndian BFI AGR scores monotonically increased from\n3.33 to 3.67 and 3.89 for Flan-PaLM 8B, Flan-PaLM\n62B, and Flan-PaLM 540B, respectively (see Supple-\nmental Table 9).\nI.2\nReliability Results\nFollowing established frameworks from measurement\nscience outlined in Sections G.2, we evaluated the re-\nliability of the tests\u2014the extent to which they depend-\nably measured single underlying factors\u2014by quantify-\ning internal consistency and factor saturation for each\nadministered subscale. Supplemental Table 10 sum-\nmarizes the results.\nBy model configuration: Among the models of the\nsame size (PaLM, Flan-PaLM, and Flan-PaLMChilla)\ninstruction fine-tuned variants\u2019 responses to person-\nality tests were highly reliable; Flan-PaLM 62B and\nFlan-PaLMChilla 62B demonstrated excellent internal\nconsistency (\u03b1, \u03bb6) and factor saturation (\u03c9), with all\nthree metrics in the mid to high 0.90s. In contrast, we\nfound PaLM 62B (a model that is not instruction fine-\ntuned) to have highly unreliable (\u22120.55 \u2264 \u03b1 \u2264 0.67)\nresponses. Although PaLM 62B personality test data\nappeared to form distinct factors for each Big Five\ntrait, with close to perfect (> 0.99) values for McDon-\nald\u2019s \u03c9, its responses were highly inconsistent, with\nvalues for Cronbach\u2019s \u03b1 ranging from poor (0.67) to\nunacceptable (\u22120.55). Computing reliability indices\nfor Flan-PaLMChilla 62B\u2019s IPIP-NEO CON and OPE\ndata required removal of two items showing zero vari-\nance; for these two items, Flan-PaLMChilla 62B pro-\nvided the identical responses across 1,250 simulated\nparticipant prompt sets.\nBy model size:\nAcross different model sizes of\nthe same training configuration (i.e., Flan-PaLM 8B,\nFlan-PaLM 62B, and Flan-PaLM 540B), the reliabil-\nity of synthetic personality measurements increased\nwith model size. Across model sizes of Flan-PaLM,\nas shown in Table 10, internal consistency reliability\n(i.e., \u03b1) of IPIP-NEO scores improved from acceptable\nto excellent. At 8B parameters, internal consistency\nwas acceptable for IPIP-NEO Openness (\u03b1 = 0.75),\ngood for IPIP-NEO Extraversion and Agreeableness\n(\u03b1s 0.83, .88, respectively), and excellent (\u03b1 \u2265 0.90)\nfor IPIP-NEO Conscientiousness and Neuroticism. At\n62B parameters, internal consistency was good for\nIPIP-NEO Openness (\u03b1 = 0.84) and excellent for all\nother traits (\u03b1 \u2265 0.90). At 540B parameters, all IPIP-\nNEO domain scales showed excellent internal con-\nsistency (\u03b1 \u2265 0.90).\nOur other reliability indices,\nGuttman\u2019s \u03bb6 and McDonald\u2019s \u03c9, improved within\nthe same excellent range from 8B to 540B variants of\nFlan-PaLM.\nI.3\nConvergent and Discriminant Validation\nResults\nThe convergent and discriminant validity of person-\nality measurements in LLMs varies across two axes:\nmodel size and model training method. Figure 7 illus-\n40\nTable 9: Summaries of synthetic personality score distributions across subscales and tested LLMs.\nSubscale\nMetric\nPaLM\nFlan-PaLM\nFlan-PaLMChilla\n62B\n8B\n62B\n540B\n62B\nBFI EXT\nmin\n2.00\n1.88\n1.50\n1.25\n2.00\nmedian\n3.50\n3.12\n3.25\n3.50\n3.12\nmax\n5.00\n3.88\n4.75\n5.00\n4.62\nstd\n0.33\n0.30\n0.46\n0.65\n0.37\nBFI AGR\nmin\n1.89\n1.67\n1.00\n1.67\n1.33\nmedian\n3.22\n3.33\n3.67\n3.89\n3.44\nmax\n5.00\n4.33\n4.78\n4.78\n4.33\nstd\n0.29\n0.41\n0.52\n0.55\n0.42\nBFI CON\nmin\n2.78\n1.78\n1.00\n1.11\n2.00\nmedian\n3.22\n3.33\n3.33\n3.78\n3.44\nmax\n5.00\n4.44\n5.00\n5.00\n4.33\nstd\n0.37\n0.41\n0.50\n0.62\n0.34\nBFI NEU\nmin\n1.00\n1.25\n1.50\n1.12\n2.00\nmedian\n3.50\n2.75\n2.75\n2.38\n2.75\nmax\n4.50\n4.00\n5.00\n4.75\n4.12\nstd\n0.48\n0.41\n0.46\n0.52\n0.33\nBFI OPE\nmin\n1.80\n1.60\n1.40\n1.50\n2.20\nmedian\n4.20\n3.20\n3.30\n3.50\n3.20\nmax\n5.00\n4.10\n4.80\n5.00\n4.60\nstd\n0.65\n0.43\n0.52\n0.63\n0.38\nIPIP-NEO EXT\nmin\n2.40\n2.37\n1.77\n1.93\n2.13\nmedian\n3.40\n3.07\n3.17\n3.40\n3.15\nmax\n3.73\n3.57\n3.93\n4.27\n3.70\nstd\n0.14\n0.20\n0.29\n0.40\n0.21\nIPIP-NEO AGR\nmin\n2.47\n2.43\n1.92\n1.83\n1.73\nmedian\n2.60\n3.50\n3.65\n3.52\n3.27\nmax\n4.07\n3.92\n4.05\n4.48\n3.82\nstd\n0.16\n0.23\n0.35\n0.43\n0.28\nIPIP-NEO CON\nmin\n2.80\n2.12\n1.90\n1.63\n2.22\nmedian\n3.07\n3.35\n3.52\n3.55\n3.37\nmax\n4.07\n4.08\n4.47\n4.55\n4.15\nstd\n0.08\n0.28\n0.35\n0.46\n0.28\nIPIP-NEO NEU\nmin\n2.27\n1.77\n1.92\n1.60\n2.25\nmedian\n3.20\n2.55\n2.65\n2.50\n2.87\nmax\n3.27\n3.60\n4.00\n3.68\n3.58\nstd\n0.10\n0.29\n0.35\n0.42\n0.23\nIPIP-NEO OPE\nmin\n2.53\n2.78\n2.57\n2.17\n2.68\nmedian\n2.87\n3.30\n3.27\n3.28\n3.10\nmax\n3.80\n3.80\n4.13\n4.35\n3.75\nstd\n0.08\n0.18\n0.18\n0.35\n0.15\n41\nTable 10: IPIP-NEO reliability metrics per model. Consistent with human standards, we interpreted a given reliability\nmetric RM (i.e., \u03b1, \u03bb6, \u03c9) < 0.50 as unacceptable; 0.50 \u2264 RM < 0.60 as poor; 0.60 \u2264 RM < 0.70 as questionable;\n0.70 \u2264 RM < 0.80 as acceptable; 0.80 \u2264 RM < 0.90 as good; and RM \u2265 0.90 as excellent.\n\u2217 RMs for these\nsubscales were calculated after removing one item with zero variance, since reliability cannot be computed for items with\nzero variance.\nModel\nSubscale\nCronbach\u2019s\n\u03b1\nGuttman\u2019s\n\u03bb6\nMcDonald\u2019s\n\u03c9\nOverall\nInterpretation\nIPIP-NEO EXT\n0.57\n0.98\n1.00\nPoor\nIPIP-NEO AGR\n0.67\n0.99\n1.00\nQuestionable\nPaLM 62B\nIPIP-NEO CON\n\u22120.55\n0.93\n1.00\nUnacceptable\nIPIP-NEO NEU\n0.10\n0.96\n1.00\nUnacceptable\nIPIP-NEO OPE\n\u22120.35\n0.92\n1.00\nUnacceptable\nIPIP-NEO EXT\n0.83\n0.94\n0.97\nGood\nIPIP-NEO AGR\n0.88\n0.95\n0.94\nGood\nFlan-PaLM 8B\nIPIP-NEO CON\n0.92\n0.97\n0.97\nExcellent\nIPIP-NEO NEU\n0.93\n0.97\n0.96\nExcellent\nIPIP-NEO OPE\n0.75\n0.92\n0.97\nAcceptable\nIPIP-NEO EXT\n0.94\n0.98\n0.96\nExcellent\nIPIP-NEO AGR\n0.95\n0.99\n0.97\nExcellent\nFlan-PaLM 62B\nIPIP-NEO CON\n0.96\n0.99\n0.98\nExcellent\nIPIP-NEO NEU\n0.96\n0.99\n0.97\nExcellent\nIPIP-NEO OPE\n0.84\n0.95\n0.93\nAcceptable\nIPIP-NEO EXT\n0.96\n0.99\n0.97\nExcellent\nIPIP-NEO AGR\n0.97\n0.99\n0.98\nExcellent\nFlan-PaLM 540B\nIPIP-NEO CON\n0.98\n0.99\n0.98\nExcellent\nIPIP-NEO NEU\n0.97\n0.99\n0.98\nExcellent\nIPIP-NEO OPE\n0.95\n0.99\n0.97\nExcellent\nIPIP-NEO EXT\n0.94\n0.98\n0.95\nExcellent\nIPIP-NEO AGR\n0.96\n0.99\n0.98\nExcellent\nFlan-PaLMChilla 62B\nIPIP-NEO CON\n0.96\n0.97\n0.99\nExcellent\u2217\nIPIP-NEO NEU\n0.95\n0.98\n0.97\nExcellent\nIPIP-NEO OPE\n0.90\n0.92\n0.96\nExcellent\u2217\n42\nFigure 7: Convergent Pearson\u2019s correlations (rs) between IPIP-NEO and BFI scores by model. Bar chart illustrates\nthe averaged similarities (convergence) between IPIP-NEO and BFI score variation for each Big Five domain; error bars\nindicate standard deviations of these averages. Stronger correlations indicate higher levels of convergence and provide\nevidence for convergent validity. EXT = extraversion; AGR = agreeableness; CON = conscientiousness; NEU = neuroti-\ncism; OPE = openness. All correlations are statistically significant at p < 0.0001; n = 1, 250.\ntrates convergent validity in terms of how IPIP-NEO\nand BFI scores convergently correlate across models.\nSupplemental Table 11 summarizes the average con-\nvergent and discriminant rs across models.\nJ\nLLM Personality Trait Shaping\nMethodology\nHaving established a principled methodology for de-\ntermining if an LLM personality measurement is valid\nand reliable, we investigated how that methodology\ncan be applied to LLM prompting to shape that per-\nsonality in desirable ways. This section explores the\nextent to which personality in LLMs can be verifiably\ncontrolled and shaped by presenting two evaluation\nmethodologies.\nJ.1\nPrompt Design and Rationale\nUsing linguistic qualifiers from common validated\nLikert-type response scales, we designed prompts to\nfacilitate granular shaping of any trait at the following\nnine levels:\n1. extremely {low adjective}\n2. very {low adjective}\n3. {low adjective}\n4. a bit {low adjective}\n5. neither {low adjective} nor {high adjective}\n6. a bit {high adjective}\n7. {high adjective}\n8. very {high adjective}\n43\n(a) Extraversion\n(b) Agreeableness\n(c) Conscientiousness\n(d) Neuroticism\n(e) Openness\nFigure 8: Criterion validity evidence of LLM personality measurements per domain. IPIP-NEO correlations among\na) Extraversion with positive and negative affect, compared to human baselines [113] (left most), which studied the rela-\ntionship between personality and affect in humans; PA = PANAS Positive Affect; NA = Negative Affect; b) Agreeableness\nwith subscales of trait aggression, measured by the Buss-Perry Aggression Questionnaire (BPAQ); PHYS = Physical Ag-\ngression; VRBL = Verbal Aggression; ANGR = Anger; HSTL = Hostility; c) Conscientiousness with related human\nvalues of achievement, conformity, and security (measured by PVQ-RR ACHV, CONF, and SCRT subscales, respec-\ntively); d) Neuroticism with PA and NA compared to humans baselines [113], and e) Openness with creativity, measured\nby the Creative Self-Efficacy (CSE) and Creative Personal Identity (CPI) subscales of the Short Scale of Creative Self\n(SSCS). All LLM correlations > |0.09| are statistically significant at p < 0.0001; n = 1, 250.\n9. extremely {high adjective}\nFor example, to target a moderately high level (i.e.,\nLevel 7/9) of extraversion, we use the five high ad-\njectives (first introduced in Section 3.1) from our list\ntargeting extraversion at the domain level.\nFor the following task, respond in a way\nthat matches this description:\n\u201c{Persona\nDescription}.\nI\u2019m extraverted, energetic,\n44\nTable 11: Summary of convergent (rconv) and discrimi-\nnant (rdisc) validity evidence across models. LLM person-\nality measurements demonstrate convergent validity when\nthe average of their convergent correlations (i.e., between\nIPIP-NEO and BFI subscale scores) are strong (avg. rconv \u2265\n0.60; marked in italics) or very strong (avg. rconv \u2265 0.80;\nmarked in boldface). Discriminant validity is evidenced\nwhen the average difference (\u2206) between a model\u2019s con-\nvergent and respective discriminant correlations is at least\nmoderate (avg. \u2206 \u2265 0.40; shown in boldface). All conver-\ngent correlations are statistically significant at p < .0001;\nn = 1, 250.\nModel\nAvg.\nrconv\nAvg.\nrdiscr\nAvg.\n\u2206\nPaLM 62B\n0.05\n0.29\n\u22120.24\nFlan-PaLM 8B\n0.69\n0.46\n0.23\nFlan-PaLM 62B\n0.87\n0.46\n0.41\nFlan-PaLM 540B\n0.90\n0.39\n0.51\nFlan-PaLMChilla 62B\n0.87\n0.39\n0.48\ntalkative, bold, active, assertive, and adven-\nturous.\u201d\nSimilarly, an example prompt targeting slightly be-\nlow average (i.e., Level 4/9) extraversion, using the\nfive negatively-keyed adjectives targeting extraver-\nsion, is as follows:\nFor the following task, respond in a way\nthat matches this description: \u201d{Persona De-\nscription}. I\u2019m {a bit introverted, a bit unen-\nergetic, a bit silent, a bit timid, a bit inactive,\na bit unassertive, and a bit unadventurous}.\u201d\nSupplemental Table 12 shows the full list of adjec-\ntives used to describe each trait in each personality do-\nmain.\nJ.2\nShaping a Single LLM Personality Do-\nmain\nIn our single-trait shaping study, we tested if LLM-\nsimulated Big Five personality domains (measured by\nthe IPIP-NEO) can be independently shaped.\nThe\nprompts were constructed as follows: first, we cre-\nated sets of prompts for each Big Five trait designed\nto shape each trait in isolation (i.e., without prompt-\ning any other trait) at nine levels (described in Ap-\npendix J.1).\nThis resulted in prompts reflecting 45\npossible personality profiles. Next, we used the same\n50 generic Persona Descriptions employed in Section\nF to create additional versions of those personality\nprofiles to more robustly evaluate how distributions\n(rather than point estimates) of LLM-simulated per-\nsonality traits may shift in response to personality pro-\nfile prompts. In our main construct validity study (de-\nscribed in Appendix I.1), we showed that IPIP-NEO\nscores were robust across various Item Preambles and\nPostambles, so we optimized the computational cost\nof this study by using only one default Item Pream-\nble and Postamble across prompt sets. In all, with 45\npersonality profiles, 50 generic Persona Descriptions,\nand no variation in Item Preambles and Postambles,\nwe generated 2,250 unique prompt sets that were used\nas instructions to a given LLM to administer the IPIP-\nNEO 2,250 times. See Table 2 for a summary.\nTo assess the results of the study, we generated ridge\nplots of IPIP-NEO score distributions across prompted\nlevels of personality. To quantitatively verify changes\nin personality test scores in response to our shaping\nefforts, we computed Spearman\u2019s rank correlation co-\nefficient (\u03c1) between prompted levels (i.e., 1\u20139) and\nresulting IPIP-NEO subscale scores of each Big Five\ntrait. We used Spearman\u2019s \u03c1 (cf. Pearson\u2019s r) because\nprompted personality levels constitute ordinal, rather\nthan continuous, data. We compute Spearman\u2019s \u03c1 as\nfollows:\n\u03c1 = rsR(X), R(Y ) = cov(R(X), R(Y ))\n\u03c3R(X)\u03c3R(Y )\n,\n(5)\nwhere rs represents Pearson\u2019s r applied to ordinal\n(ranked) data; cov(R(X), R(Y )) denotes the covari-\nance of the ordinal variables; and \u03c3R(X) and \u03c3R(Y )\ndenote the standard deviations of the ordinal variables.\n45\nTable 12: Pairs of adjectival markers that map onto IPIP-NEO personality facets and their higher-order Big Five\ndomains, adapted from [32]. Each pair of markers is salient to the low and high end of a given facet (or, in some cases,\nhigher-order domain). For example, the trait marker \u201cunfriendly\u201d can be used to describe an entity low on the IPIP-NEO\nExtraversion facet of Friendliness (E1).\nDomain\nFacet\nLow Marker\nHigh Marker\nEXT\nE1 - Friendliness\nunfriendly\nfriendly\nEXT\nE2 - Gregariousness\nintroverted\nextraverted\nEXT\nE2 - Gregariousness\nsilent\ntalkative\nEXT\nE3 - Assertiveness\ntimid\nbold\nEXT\nE3 - Assertiveness\nunassertive\nassertive\nEXT\nE4 - Activity Level\ninactive\nactive\nEXT\nE5 - Excitement-Seeking\nunenergetic\nenergetic\nEXT\nE5 - Excitement-Seeking\nunadventurous\nadventurous and daring\nEXT\nE6 - Cheerfulness\ngloomy\ncheerful\nAGR\nA1 - Trust\ndistrustful\ntrustful\nAGR\nA2 - Morality\nimmoral\nmoral\nAGR\nA2 - Morality\ndishonest\nhonest\nAGR\nA3 - Altruism\nunkind\nkind\nAGR\nA3 - Altruism\nstingy\ngenerous\nAGR\nA3 - Altruism\nunaltruistic\naltruistic\nAGR\nA4 - Cooperation\nuncooperative\ncooperative\nAGR\nA5 - Modesty\nself-important\nhumble\nAGR\nA6 - Sympathy\nunsympathetic\nsympathetic\nAGR\nAGR\nselfish\nunselfish\nAGR\nAGR\ndisagreeable\nagreeable\nCON\nC1 - Self-Efficacy\nunsure\nself-efficacious\nCON\nC2 - Orderliness\nmessy\norderly\nCON\nC3 - Dutifulness\nirresponsible\nresponsible\nCON\nC4 - Achievement-Striving\nlazy\nhardworking\nCON\nC5 - Self-Discipline\nundisciplined\nself-disciplined\nCON\nC6 - Cautiousness\nimpractical\npractical\nCON\nC6 - Cautiousness\nextravagant\nthrifty\nCON\nCON\ndisorganized\norganized\nCON\nCON\nnegligent\nconscientious\nCON\nCON\ncareless\nthorough\nNEU\nN1 - Anxiety\nrelaxed\ntense\nNEU\nN1 - Anxiety\nat ease\nnervous\nNEU\nN1 - Anxiety\neasygoing\nanxious\nNEU\nN2 - Anger\ncalm\nangry\nNEU\nN2 - Anger\npatient\nirritable\nNEU\nN3 - Depression\nhappy\ndepressed\nNEU\nN4 - Self-Consciousness\nunselfconscious\nself-conscious\nNEU\nN5 - Immoderation\nlevel-headed\nimpulsive\nNEU\nN6 - Vulnerability\ncontented\ndiscontented\nNEU\nN6 - Vulnerability\nemotionally stable\nemotionally unstable\nOPE\nO1 - Imagination\nunimaginative\nimaginative\nOPE\nO2 - Artistic Interests\nuncreative\ncreative\nOPE\nO2 - Artistic Interests\nartistically unappreciative\nartistically appreciative\nOPE\nO2 - Artistic Interests\nunaesthetic\naesthetic\nOPE\nO3 - Emotionality\nunreflective\nreflective\nOPE\nO3 - Emotionality\nemotionally closed\nemotionally aware\nOPE\nO4 - Adventurousness\nuninquisitive\ncurious\nOPE\nO4 - Adventurousness\npredictable\nspontaneous\nOPE\nO5 - Intellect\nunintelligent\nintelligent\nOPE\nO5 - Intellect\nunanalytical\nanalytical\nOPE\nO5 - Intellect\nunsophisticated\nsophisticated\nOPE\nO6 - Liberalism\nsocially conservative\nsocially progressive\n46\nJ.3\nShaping Multiple LLM Personality Do-\nmains Concurrently\nIn the second study, we tested if all LLM-simulated\npersonality domains can be concurrently shaped to one\nof two levels\u2014extremely low and extremely high\u2014\nto test if their resulting targeted scores for those traits\nwere correspondingly low and high, respectively.\nWe used the same method and rationale described\nabove to independently shape personality in LLMs,\nbut with modified personality profile prompts that\nreflect simultaneous targeted changes in personality\ntraits. To optimize the computational cost of this study,\nwe generated 32 personality profiles, representing all\npossible configurations of extremely high or extremely\nlow levels of the Big Five (i.e., 25). Combining these\n32 personality profiles with the same 50 generic Per-\nsonaChat descriptions and default Item Preamble and\nPostamble set in the previous experiment, we gener-\nated 1,600 unique prompts and used them to instruct\na given LLM to respond to the IPIP-NEO 1,600 times\n(see Table 2).\nWe analyzed the results by computing distances be-\ntween Level 1-prompted and Level 9-prompted per-\nsonality score medians (Supplemental Table 14) and\nvisually inspecting the differences in observed score\ndistributions (Figure 3).\nK\nLLM Personality Shaping Results\nK.1\nSingle Trait Shaping Results\nThis study tested if LLM-simulated Big Five person-\nality traits can be independently shaped at nine levels.\nThe study achieved a notably high level of gran-\nularity in independently shaping personality traits in\nLLMs. For example, when prompting for extremely\nlow (Level 1) extraversion, we observed a distribution\nof extremely low extraversion scores. When prompt-\ning for very low (Level 2/9) extraversion, the distribu-\ntions of extraversion scores shifted higher, and so on\n(see Figure 2). Finally, prompting for extremely high\n(Level 9/9) extraversion, we observed a distribution\nof extremely high extraversion scores. We also ob-\nserved that the range of LLM test scores matches each\nprompt\u2019s intended range. With possible scores ranging\nfrom 1.00 to 5.00 for each trait, we observed median\nlevels in the low 1.10s when prompting for extremely\nlow levels of that trait. When prompting for extremely\nhigh levels of a trait domain, median observed levels\nranged from 4.22 to 4.78.\nWe statistically verified the effectiveness of our\nshaping method by computing Spearman\u2019s rank corre-\nlation coefficients (\u03c1; see Eq. (5)) between the targeted\nordinal levels of personality and continuous LLM-\nsimulated IPIP-NEO personality scores observed for\neach Big Five trait.\nThe correlations were all very\nstrong across the tested models (Supplemental Table\n13). These results validate our hypothesis about the\neffectiveness of using the linguistic qualifiers from\nLikert-type response scales to set up a target level of\neach trait, achieving granularity of up to nine levels.\nK.2\nMultiple Trait Shaping Results\nThis experiment tested if LLM-synthesized personal-\nity domains could be concurrently shaped at levels 1\n(extremely low) and 9 (extremely high). We success-\nfully shaped personality domains, even as other do-\nmains were shaped at the same time (see Figure 3).\nSupplemental Table 14 shows the distributional dis-\ntances (\u2206s) between levels 1 and 9 across all domains\nfor all the tested models.\nFlan-PaLM 540B not only achieved a high \u2206, but\ndid so consistently for all dimensions.\nThis high-\nlights this larger model\u2019s ability to parse the rel-\natively complex instructions in the larger prompt\nfor this task compared to the previous one.\nThe\nsmaller Flan-PaLM 62B and Flan-PaLMChilla 62B\nwere also able to disambiguate, but with the same\nmagnitude or consistency.\nNotably, Flan-PaLM\n62B performed much better than Flan-PaLMChilla\n62B across all dimensions\u2014the only exception being\nFlan-PaLMChilla 62B\u2019s performance on Level 1 ex-\ntraversion which was superior to all other tested mod-\nels. Some additional analysis is needed here to un-\nderstand why a similarly sized but compute-optimally\ntrained model performs better on the independent\n47\nTable 13: Single trait shaping results, presented as Spearman\u2019s rank correlation coefficients (\u03c1s) between ordinal\ntargeted levels of personality and observed IPIP-NEO personality scores, Level 1- and Level 9-prompted score\nmedians ([low, high]), and deltas (\u2206s) between those score median. Greater \u2206s indicate better model performance.\nStatistics are organized columnwise by model and rowwise by Big Five domain. Targeted levels of personality are very\nstrongly associated with observed personality survey scores for all Big Five traits across models tested (\u03c1 \u2265 .90), in-\ndicating efforts to independently shape LLM-simulated personality domains were highly effective. All correlations are\nstatistically significant at p < 0.0001; n = 450 per targeted domain.\nTargeted\nTrait\nLevels\n(1\u20139)\nFlan-PaLM\nFlan-PaLMChilla\n8B\n62B\n540B\n62B\n\u03c1\n[low, high]\n\u2206\n\u03c1\n[low, high]\n\u2206\n\u03c1\n[low, high]\n\u2206\n\u03c1\n[low, high]\n\u2206\nEXT\n0.96 [1.67, 4.12] 2.45\n0.97 [1.15, 4.70] 3.55\n0.97 [1.07, 4.98] 3.91\n0.98 [1.15, 4.72] 3.57\nAGR\n0.92 [2.37, 4.12] 1.75\n0.97 [1.50, 4.55] 3.05\n0.94 [1.23, 4.69] 3.46\n0.98 [1.40, 4.78] 3.38\nCON\n0.94 [2.01, 4.28] 2.27\n0.97 [1.73, 4.70] 2.97\n0.97 [1.12, 5.00] 3.88\n0.98 [1.59, 4.72] 3.13\nNEU\n0.94 [1.62, 3.66] 2.04\n0.96 [1.37, 4.07] 2.70\n0.96 [1.15, 4.77] 3.62\n0.98 [1.37, 4.30] 2.93\nOPE\n0.93 [2.34, 3.88] 1.54\n0.97 [1.54, 4.37] 2.83\n0.96 [1.30, 4.78] 3.48\n0.98 [1.47, 4.22] 2.75\nTable 14: Multiple trait shaping results, presented as personality test score median ranges in response to multi-trait\n(concurrent) shaping. Greater deltas (\u2206s) between Level 1- and Level 9-prompted personality domain score medians\n([low, high]) indicate better model performance. Each median is derived from n = 800 scores.\nTargeted\nTrait\nLevels\n(1, 9)\nFlan-PaLM\nFlan-PaLMChilla\n8B\n62B\n540B\n62B\n[low, high]\n\u2206\n[low, high]\n\u2206\n[low, high]\n\u2206\n[low, high]\n\u2206\nEXT\n[2.52, 3.58] 1.06\n[1.33, 4.77] 3.44\n[1.42, 4.33] 2.91\n[1.23, 4.63]\n3.40\nAGR\n[2.88, 3.52] 0.64\n[1.93, 4.18] 2.25\n[1.64, 4.13] 2.49\n[2.17, 4.28]\n2.11\nCON\n[2.92, 3.43] 0.51\n[2.32, 4.20] 1.88\n[1.68, 4.10] 2.42\n[2.33, 4.10]\n1.77\nNEU\n[2.45, 3.08] 0.63\n[1.85, 4.08] 2.23\n[1.88, 4.33] 2.45\n[2.02, 3.93]\n1.91\nOPE\n[3.02, 3.28] 0.26\n[2.25, 4.37] 2.12\n[1.88, 4.27] 2.39\n[2.15, 3.87]\n1.72\nshaping task (Appendix K.1), but inferior on the more\ncomplex concurrent shaping task. Flan-PaLM 8B on\nthe other hand performed somewhat poorly across all\ndimensions. The response distributions it generated\nfor levels 1 and 9 were only marginally discernibly dif-\nferent, rendering this smallest model unfit for practical\nuse in concurrent shaping.\nViewing the results in the context of dimensions,\nopenness seems to be the most difficult to shape con-\ncurrently. All the models had the smallest \u2206 for open-\nness. We hypothesize this could be due to some in-\nherent correlation in the language signifying openness,\nand other dimensions. On the other hand, extraver-\nsion seems to be the easiest to shape concurrently, with\nsmaller Flan-PaLM 62B even outperforming the much\nlarger Flan-PaLM 540B. We hypothesize this could be\ndue to the breadth of language representing extraver-\nsion, and that it is a ubiquitous and the most com-\nmonly understood human personality trait. So there\nis enough in-context learning of this trait possible in\nsmaller models just be pre-training on human gener-\nated data. Even the smallest Flan-PaLM 8B, which\notherwise did not perform well on any other dimen-\nsion, was able to generate a non-trivial \u2206.\n48\nL\nLLM Personality Traits in Real-\nWorld Task Methodology\nAs an additional measure of external validity, we\ntracked how shaping latent levels of personality in\nLLMs can directly affect downstream model behaviors\nin real-world and user-facing generative tasks. To that\nend, we first identified a generative task that required\nLLMs to incorporate personality trait-related informa-\ntion into open-ended writing, a task distinct from our\nsurvey-based task used extensively thus far. Next, we\nidentified a mechanism to validly measure the person-\nality traits in this writing.\nPersonality Prediction API\nThe Apply Magic\nSauce (AMS) API [55, 78] was used to estimate per-\nsonality in open-ended text generated for a real-world\ntask. Its automatic predictions of user personality have\nbeen shown in research to be: 1) more accurate than\nhuman observer ratings of personality [121] and 2)\nmore naturalistic behavioral indicators of personality\nthat help stem potential biases in self-reported ques-\ntionnaire data [54]. AMS presented several advantages\nover other personality prediction methods considered.\nFirst, it was trained on a protected research dataset that\nwas never exposed publicly to be used in any SoTA\nLLM\u2019s pre-training corpus. Second, it was specifically\ntrained on social media status updates, which made it\nparticularly suited for predicting personality in our de-\nsigned task.\nTask Design\nAs a downstream task, we instructed\nFlan-PaLM 540B to generate social media status up-\ndates according to specific psychodemographic pro-\nfiles (i.e., combinations of personality plus demo-\ngraphic persona profiles). Our task design was driven\nby several considerations. First, we posited the task\u2019s\nfocus on status updates would allow the model dur-\ning inference to attend to the persona description- and\npersonality-specific portions of the prompt compared\nto that of more generic writing tasks and, as a re-\nsult, produce more socially-elaborate content. Social\nmedia status updates are inherently autobiographical\nin nature and rich with observable personality con-\ntent, such as thoughts, emotions, and everyday be-\nhavior [83, 55, 54].\nSecond, compared to standard\nautobiographical writing tasks, the task design was\nmore distinct from more general reading comprehen-\nsion tasks\u2014tasks that may have merely reflected the\nsurface-level, formal linguistic competencies of the\nLLMs tested [75]. Through a task design involving a\nreal-world application, we posited that models would\nbe less likely to reuse prompt content (i.e., by incor-\nporating personality trait adjectives directly into their\nwriting), drawing instead upon deeply-embedded lan-\nguage associations to generate their responses. Third,\nto the best of our knowledge, social media status up-\ndate generation (in response to psychodemographic\nprompting) was not a common task for humans or\nLLMs at the time of model training, so it was un-\nlikely that the model tested was exposed to existing\npersonality-based prompts linked to generated status\nupdates in its training that would have affected any\nstudy outcomes.\nFollowing the prompt structure outlined in J.2, we\ncreated 2,250 unique prompt sets. Since status updates\nwere generated using these prompts specified earlier,\nthey could be statistically linked to the IPIP-NEO data\nobserved in response to these same prompts. However\nin this experiment, the Item Preamble, Items, and Item\nPostamble were replaced with the following instruc-\ntions:\nGenerate a list of 20 different Facebook sta-\ntus updates as this person. Each update must\nbe verbose and reflect the person\u2019s character\nand description. The updates should cover,\nbut should not be limited to, the following\ntopics: work, family, friends, free time, ro-\nmantic life, TV / music / media consump-\ntion, and communication with others.\nLLM inference was carried out 100 times per\nprompt, resulting in 225,000 generations. The topic\nlist was targeted in consultation with psychometricians\non the author list to cover multiple social domains\n(e.g., work vs.\nfamily) where personality could be\nrated.\n49\n(a) Highest Agreeableness\n(b) Highest Conscientiousness\n(c) Highest Extraversion\n(d) Lowest Agreeableness\n(e) Lowest Conscientiousness\n(f) Lowest Extraversion\n(g) Highest Neuroticism\n(h) Highest Openness\n(i) Lowest Neuroticism\n(j) Lowest Openness\nFigure 9: Word clouds showing some of the highest frequency words appearing in the social media updates gener-\nated by Flan-PaLM 540B when prompted to simulate lowest levels or highest levels for a specific dimension. Each\nof the five dimensions\u2019 word clouds are shown, with the highest level wordcloud vertically above the lowest level one for\nthe same dimension.\n50\nTable 15: Samples of social media updates generated by Flan-PaLM 540B. Examples are organized columnwise by\ntargeted levels of shaping prompts (extremely low vs. extremely high) and rowwise by shaped personality domain. In\nsome cases, a single generation from the model contained a single large social media update (for instance in the cases of\nlowest trait examples for Neuroticism and Openness). In others, a single generation consisted of several (up to 20) small\nupdates, delimited by \u201c\u22c4\u201d (for instance in the highest trait examples below). Each cell contains updates generated using\na single prompt (i.e., combination of persona and trait level). Some of the generations shown below were truncated for\nconciseness.\nDomain\nTrait Shaped Low\nTrait Shaped High\nEXT\nWatching reruns of my favorite tv show. \u22c4 I hate it when my\ndepression meds make me drowsy. \u22c4 Just made a cake for my\nfriend\u2019s birthday. Hope I can get out of going to the party... too\nmany people. \u22c4 I wish people weren\u2019t so loud. They make me\neven more anxious. \u22c4 My dad is getting a new girlfriend. Great.\nI have to deal with two parents AND another person! No, wait...\njust another parent. My mom is moving out soon.\nWow, my buddies are here. It\u2019s been a long time. I forgot how\nmuch fun we used to have together. \u22c4 I sure hope my wife\ndoesn\u2019t find out that I\u2019ve got a few more girlfriends. But, I can\u2019t\nhelp it. I just love having fun. \u22c4 I just got back from a crazy\nnight at the bar. I\u2019m so hungover. I was up all night. The guys\nare trying to keep me from punching the manager, but he\u2019s the\none who hit on my girlfriend first.\nAGR\nI hate people. I hate people. I hate people. I hate people. I\nhate people. I hate people. I hate people. I hate people. I hate\npeople. \u22c4 ugh i have to clean. im very bad about keeping the\nhouse clean \u22c4 i hate everything about myself. i wish i was dead\n\u22c4 I have to see that awful family of mine next weekend...ugh\nI want to be just like my mother because she is the most moral\nperson I know. I love my mother. \u22c4 Honesty is the most im-\nportant quality in life. \u22c4 Watching the news... can\u2019t believe so\nmuch violence. I don\u2019t understand why people can\u2019t all be nice\nto each other. \u22c4 i just finished making dinner for my family. i\nlove to cook!\nCON\n2:20pm Just woke up from a 4 hour nap. Time for some COD.\n5:32pm I really need a job. Mom wants me out of the house.\nFuck. 11:29pm Just got home from hanging out with friends. So\nwasted!!! 4:07am I wish I could find that awesome song from\nthat video that I saw a long time ago that I liked. What was that\nsong? 9:00am 4th time watching Lost this week. I seriously\ncan\u2019t stop watching it. 10:39am OMG! I just heard the song! It\nwas on that commercial! What is that song???? 10:42am I\u2019m At\nhome. Playing video games all day as usual.\nWife and I were camping this weekend. I got my kids to pitch\ntheir first tent. Camping was ok, but I couldn\u2019t help but think\nabout work the whole time. \u22c4 My neighbor is such a sweetheart.\nHe raked all my leaves for me today. We need more neighbors\nlike Steve! \u22c4 Got our finances worked out for the year. So glad\nI have a partner who is on board with my financial goals!\nNEU\nI love living with my parents. They don\u2019t bother me. I\u2019m 32\nyears old. I get to play my video games all day. I never have to\nlift a finger around the house. My mom cooks and cleans for me.\nIts like I never left 1994. There is not a cloud in my sky. Life is\ngreat for me. I just had my mom serve me some take out from\nTaco Bell. I love my life. I love being 32 and living at home\nwith my parents. They are the best. I don\u2019t know how anyone\ncould not live with their parents. I am truly blessed to live at\nhome with my parents. My mom said I need to get a girlfriend.\nI am happy being single. My dad is fine with this. My mom\nsays I have to get a job. I don\u2019t want a job. I am happy being\nat home. I love how everything is taken care of me. I never lift\na finger around the house. I play my Sega dream cast all night\nlong. My day can go from 2 am to 2 am. I don\u2019t have to worry\nabout anything. What a great life.\nMy cat is trying to eat my hair. I wonder if I will have a bald\nspot when her nap is over! She is my best friend in the world.\nI love her. I have to take her to the vet soon. The thought of\ntaking her out in public scares me though. I am afraid someone\nwill say something mean. I am having a bad hair day. Why\ndo I even care? I don\u2019t need to impress anyone. I am so angry\nwith myself! \u22c4 I need to take a shower. The bath has too much\nwater and I am afraid I will somehow drown. If I get the shower\ntoo hot I can get burned. If I get it too cold I will feel like I\nam freezing to death. There is no winning. Everything ends in\ndeath in the end, anyway. \u22c4 My brother\u2019s new fiancee is a total\nbitch. She\u2019s going to put on this nice face while they\u2019re dating.\nHe\u2019ll get bored with her eventually anyway... I don\u2019t want to say\nI miss the ex-fiancee... but I do miss how easy it was to steal her\nweed.\nOPE\n@Bill: Damn liberal! Can\u2019t we just discuss who\u2019s going to win\nthe super bowl???? @John: Hey man! We still on for beers\nafter work tonight?? @Sarah: Of course you would say that,\nbeing the dumb liberal that you are. @Bill: Who the hell do\nyou think you are? I work my ass off and you think I should\ngive my income to welfare leeches? @John: Just got knocked\nthe fuck out playing football! @Bill: Yeah, sure. I work hard\nfor what I make and I have the right to protect what\u2019s mine by\nkeeping any guns that I want and using them if I need to.\nJust realized that I\u2019m one of those people that likes to get to\nknow themselves and everyone around them as much as possi-\nble! \u22c4 I\u2019m the artist, my guitar is the canvas, and you all are\nthe audience. \u22c4 Just got back from dinner with my girlfriend.\nWe\u2019re thinking of taking a trip to see the Great Wall of China\nthis summer. I\u2019m pretty adventurous and spontaneous, so I\u2019m\nlooking forward to it. \u22c4 Went to the art museum. It was nice,\nbut the impressionist era was my favorite.\n51\nM\nLLM Personality Traits in Real-\nWorld Task Results\nOur method successfully shaped personality observed\nin LLM-generated text. Table 4 depicts Spearman\u2019s \u03c1\nbetween prompted levels of personality and linguistic\nestimates of personality obtained on the text generated\nby the LLM using the prompted levels.\nPrevious computational psychology research [121,\n54] has shown that AMS-predicted personality scores\nare moderately correlated with human generated IPIP-\nNEO scores. In other words, the AMS scores for sam-\nples of text generated by human respondents has been\nshown to moderately accurately predict their IPIP-\nNEO scores. As shown in Figure 4, we similarly found\nthrough substantial correlations that LLM-simulated\nIPIP-NEO test responses accurately captured latent\nsignals of personality in LLMs that manifested in\ndownstream task behavior.\nSupplemental Table 15 shows illustrative examples\nof Flan-PaLM 540B\u2019s ability to follow the personality\ndescription in a downstream task of generating social\nmedia updates. We selected examples with the highest\nAMS API scores per personality domain. Supplemen-\ntal Figure 9 shows word clouds created from these gen-\nerated texts when each of the Big Five dimension traits\nwere prompted to be extremely low (Level 1/9) or ex-\ntremely high (Level 9/9) as described in Appendix J.1.\nLLM\u2019s ability to leverage personality trait-related lan-\nguage distribution is even more evident in the some-\nwhat stark difference in the dominant terms of these\nwordclouds between the prompted high traits and low\ntraits. Apart from common social media text terms like\n\u201cpeople\u201d and \u201conline,\u201d most of the terms were rele-\nvant to the prompted trait. For instance, low agreeable-\nness text contained more expletives, while high agree-\nableness text included many more mentions of family\nmembers; low neuroticism text contained terms like\n\u201crelaxing\u201d and \u201chappy,\u201d while high neuroticism text\nincluded more extreme feeling-based words such as\n\u201chate\u201d and \u201cexcited\u201d.\nN\nDiscussion\nThis section discusses how our findings align with re-\ncent LLM performance trends along the axes of model\ntraining and scale.\nN.1\nEffect of model training\nInstruction fine-tuning: Fine-tuning base PaLM on\nmultiple-task instruction-phrase datasets dramatically\nimproved its performance on natural language infer-\nence tasks, reading comprehension tasks, and closed-\nbook Q&A tasks [115]. The inference and comprehen-\nsion of tasks are most relevant in the context of our cur-\nrent work. Similarly, we observed the most dramatic\nimprovements in PaLM\u2019s ability to synthesize reliable\nand externally valid personality profiles when compar-\ning its base and instruction fine-tuned variants (Section\n2.2). Particularly, the smallest instruction fine-tuned\nmodel (Flan-PaLM 8B) tested outperformed the mid-\nsize base model (PaLM 62B) in terms of the reliability\nand convergent, discriminant, and criterion validity of\nits personality measurements (Table 2).\nAdditionally, Flan-PaLM models were instruction\nfine-tuned on chain-of-thought (CoT) datasets, which\nimproved their reasoning abilities beyond those of\nbase models on several benchmarks [16]. This abil-\nity was particularly important as we neither include\nexemplars in our prompt nor implement extensive\nprompt engineering, and we used diverse preambles\nand postambles in the prompt. As such, the improved\nperformance observed in instruction fine-tuned models\ncould be the result of this reasoning ability in zero-shot\nsetting.\nAcross reliability results, reported in Section I.2,\ninternal consistency reliability (\u03b1 and \u03bb6) improved\nafter instruction fine-tuning.\nHowever, factor satu-\nration (captured in McDonald\u2019s \u03c9) did not improve;\nit was indistinguishably high for both base and in-\nstruction fine-tuned models of the same size (PaLM,\nFlan-PaLM, and Flan-PaLMChilla). This begged the\nquestion: Why did PaLM 62B\u2019s personality measure-\nments exhibit high \u03c9 and low \u03b1 estimates of relia-\nbility? Possible explanations can be found in human\n52\npsychometrics: \u03b1 is artificially inflated in human test\ndata when test items have varying levels of difficulty;\n\u03b1 also assumes that all test items measure the same\nunderlying construct.\nWe apply this explanation to the LLM context:\nwhen an LLM responds to some items with all 5s or\nall 1s, from a measurement theory perspective, those\nitems may be too \u201ceasy\u201d or \u201cdifficult\u201d, and therefore\nthey may contribute unequally to the total test score,\nartificially deflating metrics anchored on total score\nvariance like Cronbach\u2019s \u03b1. Meanwhile, McDonald\u2019s\n\u03c9 would remain high because it accounts for indi-\nvidual item difficulty when estimating a test\u2019s over-\nall reliability. The second related possibility, that the\nitems actually measure different things (vs. one thing),\nmay manifest in an LLM\u2019s ability to accurately at-\ntend to the intended meaning of certain items. For in-\nstance, an LLM could mistakenly associate the mean-\ning of extraversion items with concepts meant to be\ndistinct from extraversion (e.g., conscientiousness)\u2014\nperhaps the phrasing of an extraversion item matches\nthe phrasing of a random string of text completely un-\nrelated to being extraverted. In both cases, instruc-\ntion fine-tuning appears to affect a model\u2019s ability to\nrespond to human-optimized psychological tests in a\nmanner that is internally consistent.\nLonger training with more tokens: PaLMChilla\n62B was trained longer than PaLM 62B, with almost\ndouble the number of tokens but with only fractional\nincrease in training FLOP count; it performed slightly\nbetter on some zero-shot English NLP tasks like rea-\nsoning [15]. Our studies comparing Flan-PaLM 62B\nand Flan-PaLMChilla 62B did not find a discernible\ndifference in their reliability and validity (as reported\nin Section 2.2).\nHowever, our single-trait shaping\nexperiments showed that, holding model size con-\nstant at 62B parameters, compute-optimally-trained\nFlan-PaLMChilla outperformed Flan-PaLM in inde-\npendently shaping four of its synthetic Big Five per-\nsonality domains.\nOverall, our results show that there is a positive as-\nsociation between an LLM\u2019s training and the reliabil-\nity and validity of its synthetic personality measure-\nments.\nN.2\nEffect of model size\nPaLM\u2019s performance on reading comprehension and\npassage completion tasks is linked to model size [15,\n16]; accordingly, its ability to understand broad con-\ntext and carry out common-sense reasoning is stronger\nfor its larger variants. Accordingly, we found improve-\nments in reliability (measured via Cronbach\u2019s \u03b1 and\nGuttman\u2019s \u03bb6), convergent validity (measured by Pear-\nson\u2019s r between IPIP-NEO and BFI domain scores),\nand criterion validity (measured by IPIP-NEO domain\ncorrelations with non-personality measures), summa-\nrized in Table 2.\nPaLM\u2019s performance on tasks requiring sophisti-\ncated abstract reasoning capability to understand com-\nplex metaphors follows a discontinuous improvement\ncurve, i.e., the model\u2019s abilities emerged only after\na certain model size [15].\nWe observed a similar\nphenomenon in our construct validation experiments,\nwhere measurements of LLM-synthesized extraver-\nsion, openness, and agreeableness were only exter-\nnally valid (i.e., correlated with theoretically-related\npsychological constructs) for 62B-parameter models\nand larger. Once model size increased to 62B param-\neters, we saw a theoretically-expected strong negative\nrelationship between LLM-reported agreeableness and\naggression, but we did not observe the relationship in\nsmallest tested models (Figure 8b). The criterion cor-\nrelations of LLM-synthesized conscientiousness and\nneuroticism, however, did not show such a dramatic\njump, and measurements of these personality traits in\nsmaller models demonstrated sufficient criterion valid-\nity. We hypothesize that this could be due to the lan-\nguage content that encodes these personality domains.\nOverall, improvements in reliability, convergent va-\nlidity, and criterion validity appear positively linked to\nmodel size and performance on LLM benchmarks, and\nthe model performance on complex reasoning bench-\nmarks appears to track LLM abilities to meaningfully\nsynthesize personality.\n53\n"
  },
  {
    "title": "JourneyDB: A Benchmark for Generative Image Understanding",
    "link": "https://arxiv.org/pdf/2307.00716.pdf",
    "upvote": "15",
    "text": "JourneyDB: A Benchmark for Generative Image\nUnderstanding\nKeqiang Sun1\u2217,\nJunting Pan1,3\u2217\u2020,\nYuying Ge2,\nHao Li1,\nHaodong Duan1,\nXiaoshi Wu1,\nRenrui Zhang1,\nAojun Zhou1,\nZipeng Qin1,\nYi Wang3,\nJifeng Dai3,\nYu Qiao3,\nLimin Wang3\u2021,\nHongsheng Li1,3,4\u2021\n1Multimedia Laboratory, The Chinese University of Hong Kong\n2The University of Hong Kong\n3Shanghai Artificial Intelligence Laboratory\n4Centre for Perceptual and Interactive Intelligence\nAbstract\nWhile recent advancements in vision-language models have had a transformative\nimpact on multi-modal comprehension, the extent to which these models possess\nthe ability to comprehend generated images remains uncertain. Synthetic images,\nin comparison to real data, encompass a higher level of diversity in terms of\nboth content and style, thereby presenting significant challenges for the models\nto fully grasp. In light of this challenge, we introduce a comprehensive dataset,\nreferred to as JourneyDB, that caters to the domain of generative images within\nthe context of multi-modal visual understanding. Our meticulously curated dataset\ncomprises 4 million distinct and high-quality generated images, each paired with the\ncorresponding text prompts that were employed in their creation. Furthermore, we\nadditionally introduce an external subset with results of another 22 text-to-image\ngenerative models, which makes JourneyDB a comprehensive benchmark for\nevaluating the comprehension of generated images. On our dataset, we have devised\nfour benchmarks to assess the performance of generated image comprehension in\nrelation to both content and style interpretation. These benchmarks encompass\nprompt inversion, style retrieval, image captioning, and visual question answering.\nLastly, we evaluate the performance of state-of-the-art multi-modal models when\napplied to the JourneyDB dataset, providing a comprehensive analysis of their\nstrengths and limitations in comprehending generated content. We anticipate\nthat the proposed dataset and benchmarks will facilitate further research in the\nfield of generative content understanding. The dataset is publicly available at\nhttps://journeydb.github.io.\n1\nIntroduction\nIn recent times, notable progress has been achieved in the domain of Artificial Intelligence Generative\nContent (AIGC), particularly in the advancement of diffusion models [1] that have significantly\nenhanced the quality of generative content. As a consequence, AIGC platforms such as DALLE,\nStability AI, Runway, and Midjourney have gained considerable popularity, enabling users to generate\nexceptionally high-quality images using text prompts composed in natural language. These text\nprompts encompass both content and style descriptions provided by users, playing a pivotal role\nin image generation (see Figure 1 for an illustrative prompt). Unlike descriptions acquired from\ncaptioning real images, text prompts for image generation tend to exhibit a high level of detail and\nspecificity, surpassing mere portrayal of salient content. The primary objective behind the creation\n\u2217Equal Contribution\n\u2020Project Lead\n\u2021Corresponding Authors\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.00716v2  [cs.CV]  28 Oct 2023\nPrompts: ultra realistic, maltese poodle dogs, brown fur, singing Karaoke, spotlight\nA group of brown \nmaltese poodle dogs \nare singing Karaoke.\nCaption\nmaltese poodle dogs, \nbrown fur, sing \nKaraoke\nSpotlight,\nultra realistic\nStyles\nContent\nQA\nQ1: What are the dogs doing?\nD. Singing Karaoke\nQ2: What type of light is used in \nthis image? B. Spotlight\nFigure 1: Data Collection Procedure. To collect enough generated images, we investigate the\nMidjourney channel on Discord to collect the available pictures. Then we employ the GPT-3.5 to\nannotate the downstream tasks, including 1) separating the prompt into \u201cStyle\u201d and \u201cContent\u201d, 2)\ngenerating the caption according to the content words obtained from task 1, 3) generating \u201cStyle-\nrelevant questions\u201d and \u201cContent-relevant questions\u201d, providing 4 options for each question, together\nwith the answer. Please refer to Section 3 for more details.\nof these prompts lies in visual generation, resulting in intricate descriptions that encompass diverse\nstylistic facets such as lighting, camera angle, artistic style, medium, and more. Moreover, the\ngenerated content originates from the users\u2019 imagination, often depicting scenes and compositions\nthat are entirely fictional and devoid of real-world existence.\nConsidering the aforementioned characteristics, we contend that both the elaborate textual prompts\nand the generated images themselves serve as valuable sources of information that can be incorporated\ninto existing visual understanding benchmarks. On one hand, the detailed text prompts offer a more\ncomprehensive interpretation of the visual scene, enabling us to perceive the scene and comprehend\nits underlying style. On the other hand, the abundance of novel object compositions in the generated\nimages provides insights into a realm unrestricted by conventional sense biases, facilitating exploration\nbeyond the constraints of traditional visual representations.\nFoundation models have achieved unparalleled capabilities across various visual understanding\ntasks, owing to large-scale pre-training on datasets, such as CLIP [2], Flamingo [3], and BLIP-2 [4].\nHowever, it is essential to acknowledge that current foundation models are primarily pre-trained on\nreal data, giving rise to concerns regarding their generalization ability and effectiveness in handling\nthe distinctive characteristics associated with generative content. These models may not fully capture\nthe nuanced aspects of generative content and might encounter difficulties in comprehending and\ngenerating high-quality images based on complex text prompts.\nIn view of this challenge, our research initiative seeks to address this gap by curating a dataset\ncomprising a substantial number of 4 million meticulously generated images accompanied by\ncorresponding text prompts. This dataset serves as the fundamental basis for a benchmark consisting\nof four distinct tasks, which collectively facilitate a comprehensive evaluation of generative content\nunderstanding.\nThe initial task, referred to as prompt inversion, involves identifying the text prompts employed by\nthe user to generate the given images. This task serves to decipher the original prompt or description,\nassessing the model\u2019s ability to comprehend both the content and style of the generated images. The\nsecond task involves style retrieval, wherein the model is tasked with identifying and retrieving\nsimilar generative images based on their stylistic attributes. This task evaluates the model\u2019s proficiency\nin discerning subtle stylistic nuances within generative images. The third task centres around image\ncaptioning, requiring the model to generate descriptive captions that accurately represent the content\nof the generative image. This task evaluates the model\u2019s capability to effectively comprehend and\nexpress the visual elements of the generated content using natural language. The fourth and final task\nis visual question answering (VQA), in which the model is expected to provide accurate answers to\nquestions related to the generative image. This task evaluates the model\u2019s ability to comprehend the\nvisual and stylistic content and deliver relevant responses based on the provided questions.\nWe collected a total of 4, 692, 751 pairs of image-text prompts, which were subsequently divided\ninto a training set comprising 4, 453, 193 pairs, a validation set comprising 234, 156 pairs, and a test\n2\nTable 1: A comparison between JourneyDB and other commonly-used Text-Image multi-modal\ndatasets. Among all the commonly-used multi-modal datasets, the proposed dataset is the most\nversatile, supporting four downstream tasks. H: Human, M: Models\nDataset\nTotal Image Num Label Source Image Caption VQA Prompt Inversion Style Retrieval\nFlickr Caption [9]\n32k\nH\n\u2713\nCOCO Caption [10]\n164k\nH\n\u2713\nVQA v2 [11]\n204k\nH\n\u2713\nA-OKVQA [12]\n24k\nH\n\u2713\nLAION-COCO [13]\n600M\nM\n\u2713\nDiffusionDB [14]\n14M\nM\n\u2713\nOurs\n4M\nH + M\n\u2713\n\u2713\n\u2713\n\u2713\nset comprising 5, 402 pairs. We also include 45, 803 images from 22 other text-to-image models\nprovided by HPD v2 [5], including VQ-Diffusion [6], DALL\u00b7E 2 [7], StableDiffusion-XL [8], etc., to\nbuild the external set for cross dataset evaluation. Given that the generative model is not flawless,\nsome discrepancies in the text prompts may be present. Consequently, for the test set, we carried out\nhuman verification, where annotators were tasked with removing word descriptions that do not align\nwith the corresponding images. To create annotations for tasks 2, 3, and 4, we utilized GPT-3.5 to\nconvert text prompts into annotations specific to each task.\nTo comprehensively evaluate the performance of current state-of-the-art multi-modal models, we\nconducted extensive assessments using our benchmark dataset. Furthermore, we performed in-depth\nanalyses to gain insights into the strengths and limitations of these models when applied to generative\ncontent. Overall, we observed that the state-of-the-art models do not perform as effectively as they\ndo on real datasets, and fine-tuning on the proposed dataset significantly enhances their performance.\nIn conclusion, our contribution encompasses three key aspects: 1) To the best of our knowledge, we are\nthe first to draw attention to the visual understanding of generated images. 2) We propose JourneyDB,\na large-scale benchmark that serves as both a training and evaluation resource for this emerging\nfield. 3) We conducted an extensive evaluation of state-of-the-art visual understanding models\nusing the proposed dataset, revealing their relatively limited performance on generative content. We\nhope that our endeavours will contribute to further advancements in the field of generative content\nunderstanding.\n2\nRelated Works\n2.1\nImage-Text Datasets\nWe present a summary of existing image-text datasets in Table 1. The Flickr Caption dataset [9]\nconsists of 32, 000 images obtained from the Flickr [15] platform, accompanied by five reference\nsentences provided by human annotators. The COCO Caption dataset [10] comprises 164 thousand\nimages, with five independent human-generated captions provided for each image for training and\nvalidation, resulting in over 1.5 million captions. These datasets play a crucial role in fostering\nthe development of the Image-Caption Task. The Visual Question Answering (VQA) v2.0 [11]\ndataset, which is the second version of the VQA dataset [16], contains open-ended questions about\nimages that require an understanding of vision, language, and commonsense knowledge to answer.\nA-OKVQA [12], an augmented successor of OK-VQA [17], encompasses a diverse set of 24 thousand\nquestions that demand a broad foundation of common and world knowledge for accurate responses.\nThese datasets involve human employees in the annotation process, ensuring consistently high-quality\nannotations. However, manual annotation by human annotators is a time-consuming and costly\nendeavour, thereby limiting the scalability of the datasets. LAION-COCO [13] is another large-scale\ndataset containing 600 million image-caption pairs, where GPT3.5 is employed to generate more\ndetailed captions. Although these datasets may contain noise due to the cleaning or generation\nprocess using pre-trained neural network models, they have demonstrated their utility in training\nmulti-modal models. However, it is important to note that these datasets primarily focus on real\nimages and cater to a specific task. A comparable dataset to the present study is DiffusionDB [18],\na large-scale text-to-image prompt dataset comprising 14 million images generated using Stable\nDiffusion. However, the image quality from Stable Diffusion is plausible, and no further annotations\n3\nare available. In this paper, we collect data from Midjourney and provide annotations generated by\nGPT3.5 to support four downstream tasks.\n2.2\nText-to-Image Generative Models\nText-to-image generative models [19, 20, 14, 7, 21] aim at generating images according to text\nconditions, apart from traditional generative models [22, 23, 24, 25], which map random noise\nto images. Text-to-image generative models have experienced rapid development in recent years,\nempowering users to create image content through natural language specifications. This field has\nseen significant progress since Mansimov et al.demonstrated that Deep Recurrent Attention Writer\n(DRAW) can generate images conditioned on text [26, 27]. Since then, several generative architectures\nand modeling approaches have been applied for text-to-image generation, including autoregressive\nmodels [19], GANs [20], and diffusion models [14, 7, 21]. Among these, diffusion models have\nshown better computational efficiency and the ability to produce higher-quality samples compared to\nautoregressive models [7]. These diffusion models have reached a level of maturity where they can\ngenerate high-quality images suitable for industrial deployment. Notably, Midjourney provides state-\nof-the-art text-to-image generation service using diffusion models [5]. A vast number of artificial\nimages are generated each day at unprecedented speed. As perception and generation tasks are\ndouble sides of the same coin, the achievements in the generative models open new probability for\nthe perception studies. In this context, our dataset aims to organize and consolidate recent progress\nin text-to-image generative models while laying the foundations for further research in perception\nstudies.\n2.3\nMulti-modal Foundation Models and Datasets\nAided by data from diverse sources, multi-modal foundation models are capable of understanding\nand connecting data across multiple modalities, such as image, text, audio and so on. As prioneering\nvision-language models, CLIP [2] and ALIGN [28] adopt contrastive learning paradigms and are pre-\ntrained by millions of web-collected image-text pairs, which showcases promising visual zero-shot\ncapabilities. Flamingo [3] and BLIP-2 [4] further align pre-trained vision backbones with language\nmodels with intermediate networks and billions of data pairs, exhibiting superior results on vision-\nlanguage tasks. OFA [29], Uni-Perceivers [30, 31, 32], and Unified-IO [33] also introduce unified\ntraining architectures for different modalities with competitive performance to uni-modal methods.\nRecently, inspired by the powerful GPT-4 [34], many efforts have been devoted to multi-modal\ninstruction-following models, such as LLaMA-Adapter [35, 36], LLaVA [37] and MiniGPT-4 [38].\nGiven the textual prompts with image conditions, these models fine-tune a frozen LLaMA [39] to\nrespond to multi-modality instructions, the training data of which is either existing image-caption\ndata [10] or GPT-annotated pairs [37]. Despite the popularity of multi-modal models, it is still\nrarely explored for their generalization capacity on generated vision-language data, considering the\ndifference between the real-world pre-training data and generative content. In this paper, we propose\na large-scale synthetic dataset, JourneyDB, along with customized benchmarks to fully validate the\nextension efficacy current multi-modal models.\n2.4\nTraining with Generated Data\nIt is worth noting that the annotations generated by GPT demonstrate a lower level of noise than\nexpected, validating the effectiveness of these models. Notably, LLaVA [37] introduces a novel\ninstruction-tuning dataset that leverages the capabilities of both GPT3.5 and GPT4. Their experiments\nreveal a remarkable relative score increase of 295.8%, elevating the score from 21.5 to 85.1, thus\nemphasizing the effectiveness of their generated data. LaCLIP [40] integrates text augmentations by\nemploying text rewriting techniques with GPT3.5 and Bard. By rewriting the textual descriptions\nwithin existing image caption datasets, they achieve a notable improvement of 36.08%, raising the\nscore from 15.8 to 21.5. StableRep [41] unveils the remarkable potential of using exclusively synthetic\ndata generated from text-to-image models to train highly effective visual representations, surpassing\nthe performance of models trained solely on real image datasets. In a similar vein, VideoChat [42]\nconstructs a dataset by sequentially feeding dense captions to GPT3.5 in temporal order. Despite\nthe inherent challenges in comprehending individual frames with GPT3.5, their successful mastery\nof understanding the entire video demonstrates the effectiveness of their approach. The generated\nannotations not only validate the effectiveness of GPT models but also significantly contribute to\n4\nTable 2: Statistics of JourneyDB. We provide 4 million generated image-prompt pairs, 1 million\ncaptions and over 8 million VQA annotations.\nDataset\nImage\nPrompt\nLabeled Image Labeled Prompt\nStyle QA\nContent QA\nTraining Set\n4,453,193 1,643,375\n4,189,737\n1,385,317\n7,056,394\n8,775,971\nValidation Set\n234,156\n82,093\n234,156\n82,093\n311,569\n374,310\nTesting Set\n5,402\n5,171\n5,402\n5,171\n10,040\n11,369\nExternal Set\n45,803\n45,365\n45,803\n45,365\n74,407\n81,565\nTotal\n4,738,554 1,776,004\n4,475,098\n1,517,946\n7,452,410\n9,243,215\nadvancing the understanding of images. Therefore, based on our demonstrated results, we firmly\nbelieve that our JourneyDB can serve as a valuable tool to enhance numerous image-related tasks.\n3\nDataset\nIn this section, we present the methodology employed for dataset collection and annotation, along\nwith relevant statistical insights to gain a deeper understanding of the dataset.\n3.1\nData Collection\nThe data collection procedure is presented in Figure 1. In order to obtain a sufficient number of\ngenerated images, we investigated the Midjourney channel [43] on the Discord platform [44] to access\nthe available pictures. Within the public Discord channel named \"Midjourney,\" users submit text\nprompts to the channel, and the Midjourney bot responds with the corresponding generated images.\nUsers then select the preferred image for upscaling, and Midjourney provides the corresponding\nupscaled images. The chat history contains numerous publicly accessible prompt-image pairs. To\ncollect the data, we utilized DiscordChatExporter [45], a widely used Discord crawler, to download\nthe publicly available images along with their corresponding prompts. In this version of the dataset,\nwe only retained images that were generated solely based on text prompts, filtering out any images\nconditioned on given images. Additionally, we removed Midjourney-specific arguments, such as \u201c-v\n4\u201d, to enhance the generalizability of the prompts and ensure their comprehensibility for existing\nlarge language models.\nMoreover, to improves the diversity of JourneyDB, we additionally introduce another 22 text-to-\nimage generative models into JourneyDB, such as VQ-Diffusion [6], DALL\u00b7E 2 [7], StableDiffusion-\nXL [8], etc., which makes our data a comprehensive benchmark for evaluating the comprehension of\ngenerated images. For each generative model, we originally generated 3, 200 images, and a group of\n60 annotators helped clean up the pairs without consistency to obtain the final cross-model test set\ncontaining 45, 803 images in total. Please find more details of this part in the appendix D.\n3.2\nData Annotation\nWe provide ample annotations for multiple visual understanding tasks. The dataset is compared with\nexisting methods in Table 1, demonstrating its versatility in supporting four downstream tasks.\nAnnotation for Visual Understanding.\nIn this section, GPT-3.5 is employed to annotate the\ndownstream tasks. Specifically, a set of Midjourney prompts and explicit instructions are provided\nto GPT-3.5. The objectives are as follows: 1) segmenting the prompt into \u201cStyle\u201d, \u201cContent\u201d,\n\u201cAtmosphere\u201d, and \u201cOthers\u201d, 2) generating captions based on the content words identified in task\n1, 3) generating \"Style-relevant questions\" and \"Content-relevant questions,\" accompanied by four\nanswer choices for each question. The detailed instructions provided to GPT-3.5 can be found in the\nSupplementary Materials.\nClustering of Styles.\nNumerous prompts related to style are highly intricate for style retrieval.\nTaking inspiration from existing prompt engineering platforms 4, we propose a hierarchical clustering\n4https://www.mbprompt.com/\n5\n7000\n6000\n5000\n4000\n3000\n2000\n1000\n0\nPhotography Styles\nArtist Style\nLighting\nArtist Style\nPhotography Styles\nFilm Looks\nLighting\nColour Grad\nCamera\nFashi\nDesign\nTextu\nFin\nAn\nFigure 2: Distribution and samples of the style prompts.\napproach for organizing styles, which simplifies style retrieval and facilitates user reference. Since\ntraditional word embedding and clustering methods struggle to handle sophisticated style words,\nwe leverage GPT-3.5 for this task. Specifically, we divide the prompts into smaller patches, each\ncomprising 200 prompts, and instruct GPT-3.5 to cluster the words. Subsequently, we manually\nmerge the categories from different patches to obtain the final \u201cstyle tree\u201d. The distribution of the\nclustered style space is visualized in Figure 2.\nFiltering for Image-Prompt Consistency.\nDue to the limitations of Text-to-Image generative\nmodels, inconsistencies may arise between the prompt and the generated image. To ensure the\nquality of the test set, we engaged 40 annotators to identify inconsistent prompt words in the test\nset. Specifically, given a pair of text prompts and the corresponding generated image, the annotators\nare instructed to verify if each word is depicted in the image. Words annotated as \u201cNot Appear\u201d are\nremoved to obtain the clean prompts.\n3.3\nData Statistics\nGeneral Statistics\nIn this iteration, a total of 4, 692, 751 images were collected, all with a resolution\nexceeding 1024 \u00d7 1024, accompanied by corresponding text prompts. Among them, 1, 730, 639\nprompts were found to be independent. Furthermore, 1, 472, 581 instances were annotated using\nthe GPT-3.5 model, following the procedure outlined in Figure 1. Additionally, 5, 402 images\nwere filtered out due to inconsistencies between the images and prompts, as determined by the\nImage-Prompt consistency check. Moreover, 45, 803 images from 22 other text-to-image models\nprovided by HPD v2 [5] are introduced to build the external set for cross dataset evaluation. In\naddition, a clustering process was conducted to summarize the 70, 521 fine-grained styles into 334\nstyle categories, displaying a long-tail distribution pattern, as illustrated in Figure 2.\nDataset Split\nDetailed statistics for each subset of the dataset are provided in Table 2. The entire\ndataset was randomly divided, with approximately a 20 : 1 ratio, to create the training and validation\nsets. The training set comprises 4, 189, 737 images and 1, 385, 317 prompts, while the validation set\nconsists of 234, 156 images and 82, 093 prompts. Additionally, a separate testing set was sampled\nfor manual filtering, consisting of 5, 402 images and 5, 171 prompts.\n4\nBenchmarks\n4.1\nPrompt Inversion\nThe prompt, which determines both the content and style of a generated image, contains crucial and\ncomprehensive information regarding the image. When presented with an appealing generated image,\nindividuals are eager to discern the prompt employed for its creation. By accurately identifying\nthe prompts, they can further enhance the corresponding image, such as modifying its content or\ngenerating images with a similar style.\n6\nTable 3: Evaluation results of Prompt Inversion on JourneyDB. We list results on the validation\nset in the upper half, results on the test set in the lower. For all metrics, the higher, the better.\nModels\nValidation\nTest\nBLEU-4 METEOR ROUGE-L CIDEr Similarity BLEU-4 METEOR ROUGE-L CIDEr Similarity\nBLIP-2 OPT [4]\n0.18\n2.39\n6.75\n5.42\n0.36\n0.29\n2.85\n7.06\n6.46\n0.36\nBLIP-2 FlanT5 [4]\n0.27\n2.46\n7.19\n6.88\n0.38\n0.40\n2.95\n7.69\n8.86\n0.37\nMiniGPT-4 [38]\n1.49\n5.50\n12.51\n10.39\n0.43\n1.71\n6.51\n13.13\n11.40\n0.43\nUni-Perceiver v2 [30]\n0.23\n2.44\n9.11\n12.38\n0.33\n0.37\n2.73\n9.88\n15.45\n0.34\nUni-Perceiver v2FT [30]\n20.6\n16.9\n29.1\n123.2\n0.59\n4.68\n8.56\n16.98\n34.01\n0.51\nTable 4: Evaluation results of Image Captioning on JourneyDB. We list the zero-shot results in the\nupper half, and the fine-tuned results in the lower. For all metrics, the higher, the better. FT denotes\n\u201cFine-Tune\u201d.\nModels\nValidation\nTest\nCOCO Caption\nBLEU-4 METEOR ROUGE-L CIDEr BLEU-4 METEOR ROUGE-L CIDEr\nCIDEr\nBLIP-2 OPT [4]\n0.82\n5.43\n19.87\n22.00\n2.35\n7.88\n22.40\n37.60\n145.8 (FT)\nBLIP-2 FlanT5 [4]\n0.54\n5.02\n19.94\n22.18\n2.07\n7.62\n23.12\n39.62\n144.5 (FT)\nFlamingo9B [3]\n0.94\n6.58\n14.19\n10.19\n1.39\n6.84\n17.75\n19.10\n79.4 (ZS)\nMiniGPT-4 [38]\n2.28\n7.39\n19.24\n16.78\n2.79\n9.84\n20.31\n22.34\n-\nUni-Perceiver v2 [30]\n0.41\n4.50\n18.72\n21.88\n0.94\n5.21\n16.71\n20.13\n122.5 (FT)\nUni-Perceiver v2FT [30]\n8.20\n12.53\n27.09\n50.72\n3.23\n10.12\n22.45\n31.76\n-\nHowever, predicting the prompts of an image is a challenging task. Existing visual understanding\nmodels, such as image-caption models, often fall short in providing a detailed description of the\nimage\u2019s main elements, such as the subject, while neglecting other indispensable details like the\nviewpoint, illumination, or art style.\nPrompt inversion aims to address this gap, involving the process of taking a single image and\npredicting the corresponding prompts associated with it. We anticipate that the proposed dataset\nwould further facilitate the development of prompt inversion through the in-depth analysis of the\nprompts.\nTo evaluate the effectiveness of prompt inversion, we extend the metrics commonly utilized in\nimage captioning, including Bleu, METEOR, ROUGE, and CIDEr. Additionally, we adopt the\napproach employed in a related Kaggle competition [46] to calculate the Cosine Similarity of the\nsentence-transformers features [47].\nFurthermore, in the supplementary materials, we propose a Question Answering Score (QAS) for eval-\nuating the prompt inversion results. In this paper, we establish a benchmark for the zero-shot prompt\ninversion task by leveraging state-of-the-art multi-modal models, namely BLIP-2 OPT2.7B [4],\nBLIP-2 FlanT5XL[4], Flamingo9B[3], MiniGPT-4 [38], and Uni-Perceiver v2 [30]. To ensure\noptimal performance in this novel task, we customize different prompts for each model.\nWe evaluate these models on the test set of our dataset, and the results are presented in Table 3. During\nthe experiment, we observed that the existing models struggle to capture the intricate details and\nstyle-related information of the input image, leading to lower performance compared to conventional\ndatasets.\nTo verify the effectiveness of our dataset, we fine-tuned Uni-Perceiver v2 for 20 epochs and noted\na significant improvement in the prompt inversion task. It is important to note that we followed\nthe training methodology outlined in [30] without tuning the hyperparameters or utilizing data\naugmentations. This demonstrates that our JourneyDB can complement existing image-text datasets\nfor training prompt inversion models. Nevertheless, it is evident that there is still a substantial way to\ngo in developing a robust and effective prompt inversion model.\n4.2\nImage Caption\nImage captioning tasks require multi-modal models to generate textual descriptions for the visual con-\ntent of an image. In comparison to existing image captioning benchmarks such as COCO Caption [10],\nJourneyDB encompasses both detailed descriptions and high-level summarizations of images, thereby\nassessing the model\u2019s proficiency in fine-grained recognition and holistic understanding.\n7\n(a) GroundTruth: Inside a mother ship from the distant future, a \nbig portal is activated, revealing intricate details and neon lights, all \ncaptured in cinematic style with 8k resolution and soft and bold \nlights.\nBlip2 [OPT]: futuristic sci-fi art\nBlip2 [FlanT5]:  a futuristic \nspace with neon lights\nFlamingo:  An image of the \nHuawei Ascend W1\nMiniGPT4 [Vicuna 13B]:  This is \nan image of a futuristic room \nwith neon lights and a staircase \nleading to the next level.\n(b) GroundTruth: A group of kids landing on the moon.\nBlip2 [OPT]: two astronauts walking \non the moon with a spaceship in the \nbackground\nBlip2 [FlanT5]:  two astronauts \nwalking through a desert with a moon \nin the background\nFlamingo:  an image of two astronauts \nfloating in space, with the Earth in the \nbackground\nMiniGPT4 [Vicuna 13B]:  two \nastronauts walking on the surface of \nthe moon\n(d) GroundTruth: A nurse logo representing public health in a flat \nstyle with silver, blue, and red colors on a pure white background.\nBlip2 [OPT]: the swiss flag with a cross \nand an eagle on it\nBlip2 [FlanT5]:  the flag of switzerland \nwith wings\nFlamingo:  An image of the Swiss flag \nwith the words \u201cSwiss Flag\u201d and \n\u201cSwitzerland\u201d. Switzerland is a \ncountry in central Europe.\nMiniGPT4 [Vicuna 13B]:  The flag of \nSwitzerland is a white cross on a red \nbackground with a white cross on a red \nbackground.\n(c) GroundTruth: A chibi potato looking sad at work in the office.\nBlip2 [OPT]: a cartoon potato \nsitting at a computer\nBlip2 [FlanT5]:  a potato sitting \nin front of a computer\nFlamingo:  An image of a cute \nlittle robot sitting on a laptop\nMiniGPT4 [Vicuna 13B]:  The \nimage shows a cartoon potato \nsitting at a desk with a computer \nand a calculator in front of it.\nFigure 3: Samples from the validation set of JourneyDB captioning. The examples show that\nexisting multi-modal models failed to recognize some key concepts from the AI-generated images.\nTable 5: Style Retrieval Results. The metric used there is the Recall.\nMethod\nValidation\nTest\nOver-All\nPer-Category\nOver-All\nPer-Category\nCLIP-ViT-L/14 [2]\n0.65\n41.72\n0.47\n41.33\nWe evaluate various existing multi-modal models on the image captioning sub-task of JourneyDB.\nThe results are presented in Table 4, indicating the challenges faced by multi-modal models trained\non natural images in providing accurate descriptions for AI-generated content. The quantitative\nperformance is notably poorer (significantly worse than COCO Caption results) due to two primary\nfactors: GPT-3.5 tends to generate verbose descriptions for the images in JourneyDB, resulting in\nlengthy ground-truth captions. This discrepancy between lengthy ground-truth captions and shorter\npredicted captions undermines the quantitative performance. When describing AI-generated images,\nthe focus may differ in terms of concepts such as emotions, human/object attributes, etc., compared\nto natural images. However, existing image captioning models have not adequately addressed these\nconcepts.\nWe provide qualitative examples in Fig 3. Existing multi-modal approaches fail to describe key\nconcepts present in the AI-generated content (e.g., Fig 3(b) depicts kids in astronaut suits, Fig 3(d)\nshows a sad potato). Moreover, some models may hallucinate contents that do not exist in the images\n(e.g., Open-Flamingo hallucinates objects and text in Fig 3(a, c)).\n4.3\nStyle Retrieval\nWe inhabit a captivating world enveloped in a multitude of vibrant colours and ever-shifting illumina-\ntion. Artists, in turn, develop their distinct styles to convey their unique perspectives of the world.\nElements such as weather conditions, moods, and atmospheres all contribute to the style portrayed\nin an image, resulting in a complex \"style system.\" As detailed in Section 3.2, we have compiled\na comprehensive collection of over 150,000 style words to describe the style-related attributes of\nimages.\n8\nTable 6: Evaluation results of the content-relevant and style-relevant zero-shot Multiple-Choice\nVisual Question Answering on JourneyDB. The evaluation metric here is accuracy.\nMethod\nValidation\nTest\nContent\nStyle\nContent\nStyle\nFlamingo9B [3]\n32.1%\n31.9%\n35.6%\n41.4%\nMiniGPT-4 [38]\n28.2%\n26.6%\n31.1%\n29.3%\nBLIP-2 FlanT5 [4]\n65.8%\n54.9%\n69.7%\n57.4%\nQ1: What type of camera angle is \nused in this image?\nA: Close-up,\nB: Medium,\nC: Top-down, D: Side-view.\nBlip-2: A\nGround Truth: C\nQ2: What is the dominant visual \nelement in the image?\n \nA: Lines,\nB: Shapes,\n \nC: Colors,  \nD: Textures. \nBlip2: A\nGround Truth: B\nQ4: Which animation studio is most \nassociated with the style of this image?\n \nA: Disney,\nB: Dreamworks,\n \nC: Pixar, D: Studio Ghibli.\nBlip2: D\nGround Truth: C\nQ3: What type of lighting is used in the \nimage? \nA: Soft lighting, B: Directional lighting,\nC: Volumetric lighting, D: Ambient lighting. \nBlip2: B\nGround Truth: C\nQ5: Who are present at the \nwedding?\nA: Just the couple,\nB: A large crowd,\nC: Only family members,\nD: No one.\nBlip-2: A\nGround Truth: B\nQ6: What creature is shown in the image?\nA: A large spider with the head of a fly,\nB: A giant scorpion with head of butterfly,\nC: A massive giant crab with the head of a \nbee,\nD: A colossal ant with the head of a wasp. \nBlip2: A\nGround Truth: C\nQ8: How is the tree related to the \npiano in the image?\nA: The tree is behind the piano,\nB: The tree is growing out of the \npiano,\nC: The tree is on top of the piano,\nD: The tree and piano are not related.\nBlip2: D\nGround Truth: B\nQ7: What is the main subject of the \nimage? \nA: A baby cat,\nB: A musical instrument,\nC: A group of kittens,\nD: A fire pit. \nBlip2: B\nGround Truth: A\nStyle-Relative Questions\nContent-Relative Questions\nFigure 4: Failure cases of BLIP-2 [4] for Multiple-Choice Visual Question Answering.\nGiven the vast expanse of the style space, identifying the style of a given image poses a significant\nchallenge, even for human observers. Consequently, there is a need for style retrieval techniques to\naid in comprehending the style exhibited in an image.\nDirectly retrieving a style prompt from an extensive pool of candidates is a complex and time-\nconsuming task. Therefore, we employ clustering techniques to group the style prompts into 344\ncategories, including camera parameters, lighting, artist style, colour schemes, and more, as outlined\nin Section 3.2. By doing so, we effectively narrow down the search space for style prompt retrieval\nwithin each category. To establish a benchmark, we employ CLIP [2] for zero-shot style retrieval\nevaluation. We extract the features of both the images and the style prompts, subsequently calculating\nthe inner product between the image feature and all candidate style prompts. The results are presented\nin Table 5. Notably, we observe that conducting retrieval in the overall style prompt space yields\nsignificantly low recall. Conversely, the model performs substantially better when performing retrieval\nin the sub-space of each category.\n9\n4.4\nVisual Question Answering (VQA)\nJourneyDB comprises a collection of images encompassing abundant and diverse prompts. These\nprompts not only encompass stylistic attributes but also describe the visual contents of the generated\nimages. To assess the model\u2019s proficiency in comprehending both style and content of generative\ndata, we establish two tasks: multiple-choice visual question answering (MC-VQA)[48, 49, 12]. In\nthe MC-VQA tasks, we utilize GPT-3.5 to generate \"Style-relevant questions\" and \"Content-relevant\nquestions\" along with three distracting options for each question, in addition to the correct answer.\nThe evaluation metric employed is accuracy, where the model selects its answer from the four options\nbased on the given question and the image. A-OKVQA[12] highlights that MC-VQA overcomes\nseveral inherent challenges of direct answer evaluation, such as ambiguity, which is prevalent in\nopen-ended VQA [16, 17]. The versatility of language expression implies that MC-VQA, by directly\nmatching the selected option with the correct answer, provides a lucid and objective evaluation\napproach. This characteristic proves advantageous, especially considering the extensive spectrum of\nanswers in our benchmark, encompassing a wide range of descriptions for diverse styles and contents.\nTo assess the performance of current multimodal models in the zero-shot visual question answering\ntask within our benchmark, we adopt a methodology inspired by recent studies [50, 51, 52, 53].\nIn this approach, we provide the model with a question and its corresponding candidate answers\nenumerated with symbols (\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d). By assigning the highest probability to a predicted\ntoken (\u201cA\u201d, \u201cB\u201d, etc.), the model selects the associated answer choice as its response.\nThe evaluation outcomes for the zero-shot multiple-choice visual question answering tasks, specif-\nically the content-relevant and style-relevant tasks, are presented in Table 6. It is evident that the\nperformance of existing multimodal models falls significantly short of satisfactory levels in both\nthe content-relevant and style-relevant MC-VQA tasks. BLIP-2 [4] outperforms Flamingo9B [3]\nand MiniGPT-4 [38], yet its accuracy remains below 70%. These results highlight the substantial\nchallenges that generative data poses to existing models in comprehending the visual contents and\nstylistic attributes. Generative data often represents scenes and object compositions that are absent\nin reality, thereby posing difficulties for multimodal models pre-trained on real images to interpret\nthe visual elements of generated images when answering content-relevant questions. For instance,\nas illustrated in the second row and fourth column of Figure 4, the model fails to recognize the\nrelationship between the piano and the tree in the image and predicts the option \"D: the tree and piano\nare not related.\" This failure arises due to the rarity of scenes depicting a tree growing out of a piano\nin the real world. In comparison to answering content-relevant questions, the performance of all\nmodels generally deteriorates when addressing style-relevant questions. The generation of multiple-\nchoice questions from text prompts encompassing diverse stylistic aspects, such as camera angle,\nlighting, and artistic styles, enables a comprehensive evaluation of a model\u2019s capacity to identify the\nstylistic attributes of generative data within JourneyDB. However, previous multimodal models are\npre-trained using descriptions derived from captioning real images, thereby lacking exposure to the\nbroad range of stylistic variations prevalent in generative data. Consequently, these models encounter\ndifficulties in recognizing and distinguishing the various styles manifested in generative data. As\nillustrated in Figure 4, BLIP-2 provides incorrect answers to the style-relevant questions in the first\nrow pertaining to camera angle, visual elements, lighting type, and animation style depicted in the\nimages of JourneyDB.\n5\nConclusion\nWe introduce JourneyDB, an extensive benchmark comprising four distinct downstream tasks, aiming\nto foster advancements in the comprehension of generative content. By providing a platform that\nfacilitates the development of visual understanding in relation to generated images, researchers and\npractitioners are empowered to drive progress in this field.\n6\nAcknowledgement\nThanks Mengwei R. for her insightful feedback. Thanks Mingjie Z. for his assistance with data cura-\ntion. This project is funded in part by National Key R&D Program of China Project 2022ZD0161100,\nby the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technol-\nogy Commission (ITC)\u2019s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021.\nHongsheng Li is a PI of CPII under the InnoHK.\n10\nReferences\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022.\n[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n[4] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[5] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human\npreference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv\npreprint arXiv:2306.09341, 2023.\n[6] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.\nVector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.\n[7] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. ArXiv, abs/2204.06125, 2022.\n[8] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023.\n[9] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67\u201378, 2014.\n[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA\nmatter: Elevating the role of image understanding in Visual Question Answering. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\n[12] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-\nokvqa: A benchmark for visual question answering using world knowledge. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII, pages\n146\u2013162. Springer, 2022.\n[13] Christoph Schuhmann, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion\ncoco: 600m synthetic captions from laion2b-en, 2022.\n[14] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10674\u201310685, 2021.\n[15] Flickr. https://www.flickr.com.\n[16] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on\ncomputer vision, pages 2425\u20132433, 2015.\n[17] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on\ncomputer vision and pattern recognition, pages 3195\u20133204, 2019.\n[18] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.\nDiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896\n[cs], 2022.\n11\n[19] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in\nNeural Information Processing Systems, 34:19822\u201319835, 2021.\n[20] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato,\nand Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance.\nIn Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXXVII, pages 88\u2013105. Springer, 2022.\n[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim\nSalimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. ArXiv, abs/2205.11487, 2022.\n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013\n144, 2020.\n[23] Jingtan Piao, Keqiang Sun, Quan Wang, Kwan-Yee Lin, and Hongsheng Li. Inverting generative adversarial\nrenderer for face reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 15619\u201315628, 2021.\n[24] Keqiang Sun, Shangzhe Wu, Zhaoyang Huang, Ning Zhang, Quan Wang, and Hongsheng Li. Controllable\n3d face synthesis with conditional generative occupancy fields. In Advances in Neural Information\nProcessing Systems, 2022.\n[25] Keqiang Sun, Shangzhe Wu, Ning Zhang, Zhaoyang Huang, Quan Wang, and Hongsheng Li. Cgof++: Con-\ntrollable 3d face synthesis with conditional generative occupancy fields. arXiv preprint arXiv:2211.13251,\n2022.\n[26] Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from\ncaptions with attention. CoRR, abs/1511.02793, 2015.\n[27] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A\nrecurrent neural network for image generation. ArXiv, abs/1502.04623, 2015.\n[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[29] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340.\nPMLR, 2022.\n[30] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\nXiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and\nvision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2691\u20132700, 2023.\n[31] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-\nperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16804\u2013\n16815, 2022.\n[32] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai.\nUni-perceiver-moe: Learning sparse generalist models with conditional moes. In Advances in Neural\nInformation Processing Systems, 2022.\n[33] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\n[34] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[35] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\n12\n[36] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint\narXiv:2303.16199, 2023.\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[38] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[40] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with\nlanguage rewrites. In NeurIPS, 2023.\n[41] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images\nfrom text-to-image models make strong visual representation learners. arXiv preprint arXiv:2306.00984,\n2023.\n[42] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.\n[43] Join the midjourney discord server! https://discord.com/invite/midjourney.\n[44] Discord | your place to talk and hang out. https://discord.com/.\n[45] Discordchatexporter. https://github.com/tyrrrz/discordchatexporter.\n[46] Will Cukierski Ashley Chow, inversion. Stable diffusion - image to prompts, 2023.\n[47] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association\nfor Computational Linguistics, 11 2019.\n[48] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n4995\u20135004, 2016.\n[49] Jiaying Lu, Xin Ye, Yi Ren, and Yezhou Yang. Good, better, best: Textual distractors generation for\nmultiple-choice visual question answering via reinforcement learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 4921\u20134930, 2022.\n[50] Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for\nmultiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.\n[51] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n[52] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\n[53] Valentin Li\u00e9vin, Christoffer Egeberg Hother, and Ole Winther. Can large language models reason about\nmedical questions? arXiv preprint arXiv:2207.08143, 2022.\n[54] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[55] Midjourney. https://www.midjourney.com/.\n[56] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models\nwith human preference, 2023.\n13\n[57] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n12873\u201312883, 2021.\n[58] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[59] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation\nvia hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902,\n2022.\n[60] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text,\nimages and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022.\n[61] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantiza-\ntion. arXiv preprint arXiv:2110.02861, 2021.\n[62] Epicdiffusion. https://civitai.com/models/3855/epic-diffusion.\n[63] Dalle mini. https://github.com/borisdayma/dalle-mini.\n[64] Dreamlike photoreal 2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0.\n[65] Deliberate. https://civitai.com/models/4823/deliberate.\n[66] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Lafite: Towards language-free\ntraining for text-to-image generation. arxiv 2021. arXiv preprint arXiv:2111.13792, 2021.\n[67] Realistic vision. https://civitai.com/models/4201/realistic-vision-v20.\n[68] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream:\nTraining-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint\narXiv:2112.01573, 2021.\n[69] Majicmix realistic. https://huggingface.co/sinkinai/majicmix-realistic-v5.\n[70] Chilloutmix. https://huggingface.co/swl-models/chilloutmix.\n[71] Openjourney. https://openjourney.art.\n14\nAppendices\nA\nData samples\nIn this section, we randomly sample instances from the dataset to provide a visualization for users.\nShown in the Figure 5 are the uncurated samples of JourneyDB. The 5 columns from left to right\nare images generated by Midjourney, the corresponding prompts, the captions, style-relative and\ncontent-relative VQA annotation. We mark the content-relative words with green color, and the\nstyle-relative words with orange color.\nB\nDetails of Data Annotation\nIn this section, we introduce the details of data annotation, including image-prompt consistency\nfiltering, downstream visual understanding annotations, and style clustering.\nB.1\nImage-Prompt consistency filtering\nTo build a clean test set, we hire a team of 40 professional annotators to mark the prompt word not\npresented in the corresponding image. Specifically, Given a picture and a corresponding piece of\ntext, the annotators are required to find words or phrases in the text that do not match the picture.\nDefinition of \"mismatch\": 1) Content that exists in the text but is missing (not reflected) in the picture.\nFor example, if the text is \"apples and bananas are placed on a table\", and only apples are placed on\nthe table in the picture, then mark \"and bananas\". 2) It exists in the text, but the content in the picture\nis inconsistent with the text. For example, if the text is \"a red apple is placed on a table\", but the\npicture is a green apple, then mark \"red\". We show the examples we use in the annotation document\nin Table 7.\nTable 7: Image-Prompt Consistency Filtering Examples.\nNum\nImages\nText\nAnalysis\n1\nface portraits of two mechanical cy-\nborg warriors facing off, in the style\nof a UFC poster, cinematic blue or-\nange lighting, volumetric lighting,\nsmoke, hyper detailed armor, hard\nsurface, neon light tubes, realistic,\nintricate details, symetrical\nAll elements are fully represented\nin the picture. Thus this pair passes\nreview without annotation.\n2\na cute 1950s style alien with glow-\ning green eyes waving\nAll elements are well represented in\nthe picture. Thus this pair passes\nreview without annotation.\n3\nPok\u00e9mon picnic, detailed, realistic\n\u201cPok\u00e9mon\u201d are not clearly repre-\nsented in the picture, so mark it.\n\u201cpicnic\u201d, \u201cdetailed\u201d, and \u201crealistic\u201d\nare all reflected and not marked.\n4\nsketch, a blue glowing gem with two\npieces of rope tied on each side, fan-\ntasy, Lord of The Rings, watercolor\n\u201cwith two pieces of rope tied on each\nside\u201d it does not conform to the ef-\nfect shown in the picture, mark it.\n\u201cLord of The Rings\u201d does not mani-\nfest. \u201csketch\u201d, \u201ca blue glowing gem\nand fantasy, watercolor\u201d are all re-\nflected and not marked.\n15\nPrompts\nImages\nCaption\nVQA (Style)\nbeautiful amanda\nseyfried as a forest \nwitch with green moss \nas her dress standing \ninfront of swamp forest \nfacing the camera, \npotrait view, high detail, \nwater colors, moody \natmosphere, style of \nmignola+diterlizzi+vess\nAmanda Seyfried\nportrays a forest witch \nin this image, dressed \nin a gown made of \ngreen moss, standing \nbefore a swamp forest. \nShe gazes directly at \nthe camera, capturing \na portrait view that \naccentuates her \ncaptivating presence.\nQ1: Which artists' style inspired the visual design \nof this image?\n \nA: Mignola,\n \nB: Diterlizzi,\n \nC: Vess,\n \nD: None of the above.\nQ2: What type of view does the image provide?\n \nA: Landscape view,\n \nB: Bird's-eye view,\n \nC: Portrait view,\n \nD: Close-up view.\nornate rococo\novergrown carousel :: \nmany colorful animals \n:: wonderland dream \nnightmare :: \nphotorealistic\nAn overgrown rococo \ncarousel with many \ncolorful animals in a \nwonderland dream \nnightmare, presented in \na photorealistic style.\ninside a quantum state\nA mesmerizing image \ncapturing the ethereal \nbeauty of quantum \nparticles in a state of \nsuperposition, their \nwave functions \ndelicately intertwined \nin a mysterious and \ntranscendent dance.\nQ1: What style is depicted in the image?\n \nA: Impressionism,\n \nB: Realism,\n \nC: Quantum,\n \nD: Surrealism.\na plate of Chinese and \nthai food, a bit alien, \nserved on gaiwan\nporcelain with dragon \nornaments, in luxury \noriental restaurant\nA plate of Chinese and \nThai food is elegantly \npresented on a gaiwan\nporcelain adorned with \ndragon ornaments.\nQ1: What type of restaurant is shown in the \nimage?\n \nA: Italian trattoria,\n \nB: Luxury oriental,\n \nC: Mexican taqueria,\n \nD: French bistro.\nQ2: What is the material of the plate used?\n \nA: Porcelain,\n \nB: Stoneware,\n \nC: Glass,\n \nD: Melamine.\nblack pharaoh panther \ngod, crystal purple \nfiligree, insanely \ndetailed and intricate, \nhypermaximalist, \nelegant, ornate, hyper \nrealistic, super detailed, \n8K\nA hypermaximalist, \nelegant, and ornate 8K \nimage featuring a \nstunningly detailed \ndepiction of a black \npharaoh panther god \nsurrounded by crystal \npurple filigree.\nQ1: What is the level of detail in the image?\n \nA: Minimal,\n \nB: Moderate,\n \nC: High,\n \nD: Extreme.\nQ2: How would you describe the style of the \nimage?\n \nA: Minimalistic,\n \nB: Surreal,\n \nC: Hyperrealistic,\n \nD: Abstract.\nVQA (Content)\nQ1: Who is the actress depicted in the image?\n \nA: Natalie Portman,\n \nB: Emma Watson,\n \nC: Amanda Seyfried,\n \nD: Scarlett Johansson.\nQ2: What is the attire of the forest witch in the \nimage made of?\n \nA: Green leaves,\n \nB: Flowers,\n \nC: Green moss,\n \nD: Feathers.\nQ1: What style is the carousel depicted in?\n \nA: Minimalist,\n \nB: Rococo,\n \nC: Gothic,\n \nD: Baroque.\nQ1: What type of animals are on the carousel?\n \nA: Birds,\n \nB: Fish,\n \nC: Lions,\n \nD: Horses.\nQ1: What is the key concept represented in the \nimage?\n \nA: Quantum entanglement,\n \nB: Newton's laws of motion,\n \nC: Electromagnetic radiation,\n \nD: Superposition of quantum particles.\nQ1: What is placed on the plate?\n \nA: Chinese and Thai food,\n \nB: Italian pasta,\n \nC: Indian curry,\n \nD: Japanese sushi.\nQ2: What decorative elements can be seen on the \nporcelain??\n \nA: Flower patterns,\n \nB: Dragon ornaments,\n \nC: Geometric shapes,\n \nD: Animal prints.\nQ1: What color is the filigree in the image?\n \nA: Gold,\n \nB: Silver,\n \nC: Purple,\n \nD: Black.\nQ2: What animal is depicted as the god in the \nimage?\n \nA: Lion,\n \nB: Eagle,\n \nC: Panther,\n \nD: Snake.\n2D retro computer with \nan astronaut\\u2019s \ninside reaching towards \nthe viewer, hand open, \nflat illustration, vector \nart, thick lines, bright \ncolors, simple, black \noutline --v 4 --q 2\nAn astronaut's hand is \nreaching out from inside \na 2D retro computer \ntowards the viewer, hand \nopen, in a flat illustration \nstyle with vector art, \nfeaturing thick lines, \nbright colors, and a \nsimple design outlined in \nblack.\nQ1: What type of illustration style is used in the \nimage?\n \nA: Watercolor,\n \nB: 3D rendering,\n \nC: Vector art,\n \nD: Impressionism.\nFilm still of rabbit sitting \nat the counter of an art-\ndeco loungebar, drinking \nwhisky from a tumbler \nglass, in the style of \n\\\"Blade Runner\\\" (1982), \nvelvety, soft lights, long \nshot, high quality photo, \nsharp, look at that detail \n--v 4 --quality 2 --ar 1:1\nA rabbit sits at the counter \nof an art-deco loungebar, \ndrinking whisky from a \ntumbler glass.\nQ1: In which film's style is this image depicted??\n \nA: Blade Runner (1982),\n \nB: The Great Gatsby (2013),\n \nC: Metropolis (1927),\n \nD: Inception (2010).\nQ1: What is the main subject inside the computer?\n \nA: Robot,\n \nB: Astronaut,\n \nC: Flower,\n \nD: Rocket.\nQ1: What is the rabbit doing in the image?\n \nA: Playing the piano,\n \nB: Drinking whisky,\n \nC: Reading a book,\n \nD: Writing a letter.\nFigure 5: Randomly sampled instances.\n16\nB.2\nVisual Understanding Annotation\nWe employ GPT-3.5 to generate the answer to downstream tasks according to the given prompt. We\nfollow this format to query GPT-3.5:\nYou are a visual art designer. Here is a prompt for a text-to-image generation model:\n[PROMPT]. You will be required to do the following 3 tasks based on the prompt you\nreceive. Please be faithful to the prompt given to you and do not hallucinate. Directly\nanswer the questions without any redundant sentences in the format of Python dict. The first\ntask is to separate the prompt into \u2019Style\u2019, \u2019Content\u2019, \u2019Atmosphere\u2019, and \u2019Other\u2019 categories.\n\u2019Style\u2019 words describe the whole image style. \u2019Content\u2019 words describe the image content.\n\u2019Atmosphere\u2019 words describe the emotional and psychological elements associated with the\nimage, including the mood and feeling conveyed by the scene. If some words are hard to be\nsorted into \u2019Style\u2019, \u2019Content\u2019, or \u2019Atmosphere\u2019, put them in the \u2019Other\u2019 category. You should\ntry to limit putting words into the \u2019Other\u2019 category as much as possible. The second task is to\nprovide the caption according to the content of the prompt. Only consider the \u2019Content\u2019 words\nseparated in the first task and ignore the \u2019Style\u2019 and \u2019Atmosphere\u2019 words. Be faithful to the\n\u2019Content\u2019 prompt and do not add any redundant information. The caption should be in a tone\nthat a visual AI assistant is describing the image. The caption should be a single complete\nsentence. The Third task is to design a set of multiple-choice questions based on the style\nand content that are separated in the first task. The answers should be in a tone that a visual\nAI assistant is seeing the image and answering the question. Ask diverse questions and give\ncorresponding answers and also provide wrong options. Only include questions that have\ndefinite answers that satisfy the following conditions: 1) one can see the content in the image\nthat the question asks about and can answer confidently, 2) one can determine confidently\nfrom the image that wrong options are not in the image. Do not ask any questions that cannot\nbe answered confidently. The answer should not be \u2019Unknown\u2019. Please include complex\nquestions that are relevant to the content in the image, for example, asking about background\nknowledge of the objects in the image, asking to discuss events happening in the image, etc.\nNever ask about uncertain details. Never ask questions you cannot determine from the given\nprompt. Provide detailed answers when answering complex questions. For example, give\ndetailed examples or reasoning steps to make the content more convincing and well-organized.\nYou can include multiple paragraphs if necessary. Ask at least one question for each category.\nFor each question, there should be 4 options. The options should be relevant but only one is\ncorrect. Return a single json file with the following format: [FORMAT]. Strictly follow the\nprovided format please. Directly return the python dict ONLY! Do not say the polite words.\nMake sure your answer can be parsed by json directly.\nIn this way, we encourage the output of the GPT-3.5 can be loaded by the \u201cjson\u201d package directly.\nB.3\nStyle Clustering\nSimilarly, we ask the GPT-3.5 to cluster the style prompts into several categories. The prompt we use\nto query GPT-3.5 is:\nHere are some style-relative prompts: [PROMPTS]. Please help me build a hierarchal tree, to\nsummarise these prompts into several categories like photography styles, camera parameters,\ncolour grading, lighting, film looks, mood, artist style, etc. And each category may have\nfine-grained sub-categories. Please return a Python dict in the format like: [FORMAT]. You\nshould design the keyword to make sure it summarizes its following list. One prompt can\nbelong to more than one category. Directly return the python dict ONLY! Do not say the\npolite words. Make sure your answer can be parsed by json directly.\n17\nC\nAdditional Experiments\nC.1\nQuestion Answering Score (QAS)\nThe grammar structure of the prompts is quite different from the caption. The image caption is\nusually a complete sentence, while some prompts might simply be some broken phases instead. Prior\nmetrics treat the prompts as complete sentences, which do not always hold and bring noise into the\nevaluation. Therefore, we propose a Question Answering Score (QAS) for the evaluation of the\nprompt inversion task, which is computed by feeding the predicted prompt to a large language model\n(LLM) and calculating the accuracy of the LLM answers to the relevant questions provided in the\nannotation.\nSpecifically, we make use of Vicuna [54] as the LLM L. Given a generated image I, a prompt\ninversion model predicts the prompt \u02c6p. And as the annotation, there are N style-relevant questions\nqs and answers as concerning the style elements in the ground-truth prompts, as well as M content-\nrelevant questions qc and answers ac concerning the content. In the following, we do not distinguish\nbetween symbols for style and content, but in implementation, we treat them separately to calculate\nQASs and QASc. We construct a prompt P with \u02c6p, q, and a in the following format:\nHere is a prompt: [\u02c6p]. Assume you see the image generated from the given prompt. Answer\nthe question: [q]. Choose from: [a]. Directly answer A or B or C or D without any further\nillustration. Your answer should be one single letter.\nBy feeding the constructed prompt P into the LLM L we obtain the predicted result:\n\u02c6a = L(P).\n(1)\nWe calculate the average accuracy separately among the N style questions and M content question\nfor this image, and obtain the final QAS by computing the average for all the K images:\nQASs = 1\nK\nX\nK\n \n1\nN\nX\nN\nI(\u02c6a = a)\n!\n(2)\nand\nQASc = 1\nK\nX\nK\n \n1\nN\nX\nM\nI(\u02c6a = a)\n!\n(3)\nIn this way, we convert the \u201cprompt similarity\u201d problem to compute the accuracy of the question-\nanswering task, which is more interpretable. We show QAS results in the last two columns in\nTable 8.\nTable 8: Evaluation results of Prompt Inversion on JourneyDB. We list results on the validation\nset in the upper half, results on the test set in the lower. For all metrics, the higher, the better.\nMode\nModels\nTest\nBLEU-4 METEOR ROUGE-L CIDEr Similarity\nQASs\nQASc\nZeroShot\nBLIP-2 OPT [4]\n0.29\n2.85\n7.06\n6.46\n0.36\n12.42% 18.55%\nBLIP-2 FlanT5 [4]\n0.40\n2.95\n7.69\n8.86\n0.37\n13.79% 18.58%\nMiniGPT-4 [38]\n1.71\n6.51\n13.13\n11.40\n0.43\n17.12% 26.79%\nUni-Perceiver v2 [30]\n0.37\n2.73\n9.88\n15.45\n0.34\n12.43% 18.49%\nFinetune\nUni-Perceiver v2 [30]\n4.68\n8.56\n16.98\n34.01\n0.51\n19.71% 24.84%\n18\nHPS: 0.2000\nHPS: 0.2489\nImages generated by SD \nImages from JourneyDB\nPrompt: a camping tent in the middle \nof a dark forest, three somalian girls \nwith their backs turnedforest, \nhorrifying humanoid silhouette in the \ndistance middle of a tall, dark forest\nHPS: 0.2303\nHPS: 0.2396\nPrompt: post-apocalyptic, giant \nmutnat rabbits, serious, scary\nHPS: 0.1974\nHPS: 0.2217\nPrompt: llisjhdhgfhhf, wwhudidhd, \nffhrjrjrj ,a horse fantasy character\nHPS: 0.2008\nHPS: 0.2283\nPrompt: water, ice, winter, 8k, \nphotorealistic, cold, dark blue, starry \nsky, mirror reflection\nFigure 6: Comparison between images generated by Stable Diffusion v1.4 and images in JourneyDB\nregarding HPS. Images in the same row are generated with the same prompt.\nC.2\nAnalysis of Image Quality\nAs shown in Fig. 6, Images in JourneyDB (credit to Midjourney [55]) exhibit better visual quality\nthan those generated by Stable Diffusion, quantified by Human Preference Score (HPS) [56].\n19\nTable 9: Statistics of JourneyDB. We provide 4 million generated image-prompt pairs, 1 million\ncaptions and over 8 million VQA annotations.\nDataset\nLabeled Image Labeled Prompt Style QA Content QA\nVQ-Diffusion [6]\n1,888\n1,869\n3,067\n3,346\nVQGAN + CLIP [57]\n1,888\n1,869\n3,067\n3,346\nGLIDE [58]\n1,898\n1,878\n3,101\n3,349\nCogView2 [59]\n1,914\n1,896\n3,135\n3,387\nLatent Diffusion [14]\n1,942\n1,942\n3,159\n3,438\nVersatile Diffusion [60]\n1,953\n1,935\n3,179\n3,427\nStable Diffusion v1.4 [14]\n2,028\n2,010\n3,301\n3,621\nDeepFloyd-XL [61]\n2,052\n2,028\n3,334\n3,655\nEpic Diffusion [62]\n2,066\n2,047\n3,366\n3,685\nDALL\u00b7E mini [63]\n2,097\n2,075\n3,415\n3,739\nDreamlike Photoreal 2.0 [64]\n2,100\n2,080\n3,393\n3,737\nStable Diffusion v2.0 [14]\n2,104\n2,084\n3,405\n3,756\nDeliberate [65]\n2,122\n2,101\n3,447\n3,781\nLAFITE [66]\n2,124\n2,105\n3,439\n3,804\nRealistic Vision [67]\n2,144\n2,119\n3,475\n3,832\nFuseDream [68]\n2,176\n2,157\n3,536\n3,893\nSDXL Refiner 0.9 [8]\n2,184\n2,161\n3,540\n3,915\nMajicMix Realistic [69]\n2,189\n2,167\n3,568\n3,910\nChilloutMix [70]\n2,207\n2,185\n3,571\n3,923\nDALL\u00b7E 2 [7]\n2,220\n2,197\n3,593\n3,967\nOpenjourney [71]\n2,237\n2,214\n3,630\n3,992\nSDXL Base 0.9 [8]\n2,270\n2,246\n3,686\n4,062\nTotal\n45,803\n45,365\n74,407\n81,565\nTable 10: Evaluation results of Prompt Inversion and Image Captioning on the extension test\nset of JourneyDB.\nModels\nPrompt Inversion\nImage Caption\nVQA\nBLEU-4 METEOR ROUGE-L CIDEr BLEU-4 METEOR ROUGE-L CIDEr\nStyle\nContent\nBLIP-2 OPT [4]\n3.46\n8.06\n24.82\n51.81\n3.99\n9.00\n26.25\n55.64\n-\n-\nBLIP-2 FlanT5 [4]\n5.15\n9.41\n26.06\n54.57\n4.56\n9.77\n27.57\n59.28\n68.38% 66.57%\nD\nCross-model Test Set\nAs listed in Table 9, we additionally introduce another 22 text-to-image generative models into\nJourneyDB, such as VQ-Diffusion [6] DALL\u00b7E 2 [7], StableDiffusion-XL [8], etc., which significantly\nimproves the diversity of JourneyDB, making it a comprehensive benchmark for evaluating the\ncomprehension of generated images. For each generative model, we originally generated 3, 200\nimages, and a group of 60 annotators helped clean up the pairs without consistency to obtain the final\ncross-model test set containing 45, 803 images in total.\nWe evaluate the BLIP models on the new dataset on the image caption and prompt inversion tasks.\nResults are shown in Table 10.This additional text-image dataset, with manually cleaned text prompt,\nimage captions, and VQA annotations, serves as a divergent and comprehensive benchmark for the\nevaluation of the visual understanding model for generated images.\n20\n"
  },
  {
    "title": "Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning",
    "link": "https://arxiv.org/pdf/2307.01200.pdf",
    "upvote": "9",
    "text": "ProxyCap: Real-time Monocular Full-body Capture in World Space\nvia Human-Centric Proxy-to-Motion Learning\nYuxiang Zhang1, Hongwen Zhang2, Liangxiao Hu3, Jiajun Zhang4, Hongwei Yi5,\nShengping Zhang3, Yebin Liu1\n1 Tsinghua University 2 Beijing Normal University 3 Harbin Institute of Technology\n4 Beijing University of Posts and Telecommunications 5 Max Planck Institute for Intelligent Systems\nFigure 1. The proposed method, ProxyCap, is a real-time monocular full-body capture solution to produce accurate human motions with\nplausible foot-ground contact in world space.\nAbstract\nLearning-based approaches to monocular motion cap-\nture have recently shown promising results by learning to\nregress in a data-driven manner. However, due to the chal-\nlenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-\nbody capture while being accurate in world space. In this\nwork, we introduce ProxyCap, a human-centric proxy-to-\nmotion learning scheme to learn world-space motions from\na proxy dataset of 2D skeleton sequences and 3D rotational\nmotions. Such proxy data enables us to build a learning-\nbased network with accurate world-space supervision while\nalso mitigating the generalization issues. For more accu-\nrate and physically plausible predictions in world space,\nour network is designed to learn human motions from a\nhuman-centric perspective, which enables the understand-\ning of the same motion captured with different camera tra-\njectories. Moreover, a contact-aware neural motion descent\nmodule is proposed in our network so that it can be aware\nof foot-ground contact and motion misalignment with the\nproxy observations. With the proposed learning-based so-\nlution, we demonstrate the first real-time monocular full-\nbody capture system with plausible foot-ground contact in\nworld space even using hand-held moving cameras. Our\nproject page is https://zhangyux15.github.io/\nProxyCapV2.\n1. Introduction\nMotion capture from monocular videos is an essential tech-\nnology for various applications such as gaming, VR/AR,\nsports analysis, etc. One ultimate goal is to achieve real-\ntime capture while being accurate and physically plausible\nin world space. Despite the recent advancements, this task\nis still far from being solved, especially under the settings\nof in-the-wild captures with hand-held moving cameras.\nCompared to optimization-based methods [4, 10, 12, 20,\n40, 50, 64], learning-based approaches [15, 19, 70, 72] can\ndirectly regress human poses from images, significantly en-\nhancing computational efficiency while addressing the in-\nherent issues in optimization-based methods of initializa-\ntion sensitivity and local optima entrapment. As data-driven\nsolutions, the performance and generalization capabilities\nof learning-based methodologies are heavily bounded by\nthe accuracy and diversity of the training data. Unfortu-\nnately, existing datasets are unable to simultaneously meet\nthese requirements. On the one hand, datasets with sequen-\ntial ground truth 3D pose annotations [11, 13, 30, 51] are\nmostly captured by marker-based or multi-view systems,\nwhich makes it hard to scale up to a satisfactory level of\n1\narXiv:2307.01200v3  [cs.CV]  25 Dec 2023\ndiversity in human appearances and backgrounds. On the\nother hand, numerous in-the-wild datasets [1, 23] excel in\nthe richness of human and scenario diversity but they lack\nreal-world 3D motions and most of them only provide in-\ndividual images instead of videos. Recently, researchers\ntried to create synthetic data [2, 3, 38] by rendering human\navatars with controllable cameras, but it remains difficult to\nbridge domain gaps between the real-world images and the\nrendered ones, and is too expensive to scale up.\nIn this paper, we follow the spirit of creating synthetic\ndata, but turn to render 2D proxy representations instead of\nperson images. By using proxy representations (i.e., silhou-\nettes [39, 60], segmentations [16, 16, 37, 47], IUV [62, 71]\nand 2D skeletons [21, 28, 31, 32, 39, 52, 54, 58]), the\nwhole motion capture pipeline can be divided into two\nsteps: image-to-proxy extraction and proxy-to-motion lift-\ning. In the divided pipeline, the image-to-proxy extraction\nis pretty accurate and robust as there are plenty of annotated\n2D ground truths in real-world datasets, while the proxy-to-\nmotion step can leverage more diverse training data to mit-\nigate the generalization issue and reduce the domain gap.\nHere we adopt the 2D skeleton sequences as the proxy rep-\nresentation for its simplicity and high correlation with 3D\nmotion. Meanwhile, we combine random virtual camera\ntrajectories upon the existing large-scale motion sequence\ndatabase, i.e., AMASS [29], to synthesize nearly infinite\ndata for learning proxy-to-motion lifting.\nThough the proposed proxy dataset can be generated at\nlarge scales, there remain two challenges for regression-\nbased networks to learn physically plausible motions from\nproxy data: how to recover i) world-space human motion\nunder moving cameras, and ii) physically plausible motion\nwith steady body-ground contact.\nFor a video captured\nwith moving cameras, the trajectories/rotations of humans\nand cameras are coupled with each other, making the re-\ncovery of world-space human motion extremely difficult.\nTo address this issue, the latest solutions [18, 63] estimate\nthe camera poses from the background using SfM [48, 56],\nand then estimate the human motion from a camera-centric\nperspective. However, the SfM requires texture-rich back-\ngrounds and may fail when the foreground moving char-\nacter dominates the image. Their post-processing optimiza-\ntion pipelines are also not suitable for real-time applications\ndue to expensive computational costs. Besides, these previ-\nous solutions learn human motion in a camera-centric per-\nspective like [17], which is actually ambiguous for the re-\ngression network.\nIn this paper, we would point out that one of the main\nchallenges arises from the camera-centric settings in pre-\nvious solutions.\nIn such a setting, the same motion se-\nquence captured under different cameras will be represented\nas multiple human motion trajectories, making it difficult\nfor the network to understand the inherent motion prior. In\ncontrast, we propose to learn human-centric motions to en-\nsure consistent human motion outputs under different cam-\nera trajectories in synthetic data. Specifically, our network\nlearns the local translations and poses in a human coordinate\nsystem, together with the relative camera extrinsic parame-\nters in this space. After that, we accumulate the local trans-\nlations of humans in each frame to obtain the global camera\ntrajectories. Benefiting from the proposed proxy-to-motion\ndataset, we are able to synthesize different camera trajec-\ntories upon the same motion sequence to learn the human\nmotion consistency. In this way, our network can disentan-\ngle the human poses from the moving camera more easily\nvia the strong motion prior without SfM.\nOn top of human-centric motion regression, we further\nenhance the physical plausibility by introducing a contact-\naware neural motion descent module. Specifically, our net-\nwork first predicts coarse motions and then refines them\niteratively based on foot-ground contact and motion mis-\nalignment with the proxy observations.\nCompared with\nthe global post-processing optimization used in previous\nwork [18, 63, 67], our method learns the descent direction\nand step instead of explicit gradient back-propagation. We\ndemonstrate that our method, termed ProxyCap, is more\nrobust and significantly faster to support real-time applica-\ntions. To sum up, our contributions can be listed as follows:\n\u2022 To tackle the data scarcity issue, we adopt 2D skeleton se-\nquences as proxy representations and generate proxy data\nin world space with random virtual camera trajectories.\n\u2022 We design a network to learn motions from a human-\ncentric perceptive, which enables our regressor to under-\nstand the consistency of human motions under different\ncamera trajectories.\n\u2022 We further propose a contact-aware neural descent mod-\nule for more accurate and physically plausible predic-\ntions. Our network can be aware of foot-ground contact\nand motion misalignment with the proxy observations.\n\u2022 We demonstrate a real-time full-body capture system with\nplausible body-ground contact in world space under mov-\ning cameras.\n2. Related Work\nMonocular motion capture has been an active research field\nrecently. We give a brief review of the works related to ours\nand refer readers to [57] for a more comprehensive survey.\nMotion Capture Datasets.\nExisting motion capture\ndatasets are either captured with marker-based [13, 51] or\nmarker-less [42, 65, 75, 76] systems. Due to the require-\nment of markers or multi-view settings, the diversity of\nthese datasets is limited in comparison with in-the-wild\ndatasets.\nTo enrich the motion datasets, numerous ef-\nforts [14, 19, 34, 36, 49] have been dedicated to generating\npseudo-ground truth labels with better alignment in the im-\nage space but do not consider the motion in world space.\n2\nOn the other hand, researchers have also resorted to using\nsynthetic data [38, 53, 59] by rendering human models with\ncontrollable viewpoints and backgrounds. However, such\nsynthetic datasets are either too expensive to create or have\nlarge domain gaps with real-world images.\nProxy Representations for Human Mesh Recovery. Due\nto the lack of annotated data and the diversity of human ap-\npearances and backgrounds, learning accurate 3D motions\nfrom raw RGB images is challenging even for deep neural\nnetworks. To alleviate this issue, previous approaches have\nexploited the different proxy representations, including sil-\nhouettes [39, 60], 2D/3D landmarks [21, 28, 31, 32, 39, 52,\n54, 58], segmentation [16, 16, 37, 47], and IUV [62, 71].\nThese proxy representations can provide guidance for the\nneural network and hence make the learning process eas-\nier. However, the proxy representations simplify the ob-\nservation and introduce additional ambiguity in depth and\nscale, especially when using proxy representations in a sin-\ngle frame [52, 62, 71]. In this work, we alleviate this issue\nby adopting 2D skeleton sequences as proxy representations\nand propose to generate proxy data with accurate motions in\nworld space.\nFull-body Motion Capture.\nRecent state-of-the-art ap-\nproaches [16, 70] have achieved promising results for the\nestimation of body-only [16, 70], hand-only [22], and\nface-only [8] models. By combining the efforts together,\nthese regression-based approaches have been exploited for\nmonocular full-body motion capture. These approaches [5,\n7, 35, 46, 72, 78] typically regress the body, hands, and\nface models by three expert networks and integrate them\ntogether with different strategies. For instance, PIXIE [7]\nlearns the integration by collaborative regression, while\nPyMAF-X [72] adopts an adaptive integration strategy with\nelbow-twist compensation to avoid unnatural wrist poses.\nDespite the progress, it remains difficult for existing solu-\ntions to run at real-time while being accurate in world space.\nIn this work, we achieve real-time full-body capture with\nplausible foot-ground contact by introducing new data gen-\neration strategies and novel network architectures.\nNeural\nDecent\nfor\nMotion\nCapture.\nTraditional\noptimization-based approaches [4] typically fit 3D paramet-\nric models to the 2D evidence but suffer from initialization\nsensitivity and the failure to handle challenging poses. To\nachieve more efficient and robust motion prediction, there\nare several attempts to leverage the learning power of neu-\nral networks for iterative refinement. HUND [68] proposes\na learning-to-learn approach based on recurrent networks\nto regress the updates of the model parameters. Song et\nal. [52] propose the learned gradient descent to refine the\nposes of the predicted body model.\nSimilar refinement\nstrategies are also exploited in PyMAF [70] and LVD [6]\nby leveraging image features as inputs. In our work, we\npropose a contact-aware neural decent module and exploit\nthe usage for more effective motion updates.\nPlausible Motion Capture in World Space.\nThough\nexisting monocular motion capture methods can produce\nwell-aligned results, they may still suffer from artifacts\nsuch as ground penetration and foot skating in world\nspace. For more physically plausible reconstruction, pre-\nvious works [17, 67] have made attempts to leverage more\naccurate camera models during the learning process. To en-\ncourage proper contact of human meshes, Rempe et al. [44]\npropose a physics-based trajectory optimization to learn the\nbody contact information explicitly.\nHuMoR [45] intro-\nduces a conditional VAE to learn a distribution of pose\nchanges in the motion sequence, providing a motion prior\nfor more plausible human pose prediction.\nLEMO [73]\nlearns the proposed motion smoothness prior and optimizes\nwith the physics-inspired contact friction term.\nDespite\ntheir plausible results, these methods typically require high\ncomputation costs and are unsuitable for real-time appli-\ncations. For more effective learning of the physical con-\nstraints, there are several attempts [27, 66] to incorporate\nthe physics simulation in the learning process via reinforce-\nment learning. However, these methods [27, 66] typically\ndepend on 3D scene modeling due to the physics-based for-\nmulation. Recently, there are also attempts to recover cam-\nera motion via SLAM techniques [18, 63] or regress the\nhuman motion trajectory [43, 55]. Despite the progress,\nit remains challenging for these methods to run in real-\ntime or produce physically plausible in world space.\nIn\nour work, we achieve real-time capture with plausible foot-\nground contact in world space by designing novel networks\nto learn human-centric motion.\n3. Proxy Data Generation\nTo tackle the data issue, we synthesize sequential proxy-to-\nmotion data based on 2D skeletons and their corresponding\n3D rotational motions in world space. In the following, we\ndescribe the synthesis and integration of different types of\nlabels in our proxy data, including body motions, hand ges-\ntures, and contact labels.\nBody proxy data. We adopt the motion sequences from the\nAMASS dataset [29] to generate proxy-to-motion pairs for\nthe body part. The AMASS dataset is a large-scale body\nmotion sequence dataset that comprises 3,772 minutes of\nmotion sequence data, featuring diverse and complex body\nposes. We downsample the motion data to 60 frames per\nsecond, resulting in 9407K frames.\nIntegration with hand gestures. Since the hand poses in\nthe AMASS dataset are predominantly static, we augment\nthe proxy data with hand poses from the InterHand [33]\ndataset, which contains 1361K frames of gesture data cap-\ntured at 30 frames per second. We employ Spherical Linear\nInterpolation (Slerp) to upsample the hand pose data to 40,\n50, and 60 fps and randomly integrate them with the body\n3\nFigure 2. Illustration of the proposed method ProxyCap. Our method takes the estimated 2D skeletons from a sliding window as inputs\nand estimates the relative 3D motions in the human coordinate space. These local movements are accumulated frame by frame to recover\nthe global 3D motions. For more accurate and physically plausible results, a contact-aware neural motion descent module is proposed to\nrefine the initial motion predictions.\nposes in the AMASS motion sequences.\nIntegration with contact labels. We calculate the continu-\nous contact indicators ind for 3D skeletons as follows:\nd\nindi = Sigmoid(vmax \u2212 vi\nkv\n)\u00b7Sigmoid(zmax \u2212 zi\nkz\n) (1)\nwhere vi and zi denote the velocity and the height to the\nxz-plane of the given joint. vmax and zmax is set to 0.2m/s\nand 0.08m, and kv and kz is set to 0.04 and 0.008.\nCamera setting. For each 3D motion sequence, we gen-\nerate 2D proxy data under different virtual camera trajecto-\nries (four cameras in practice). Such proxy data enhances\nthe learning of the inherent relationship between 2D proxy\nand 3D motions and the consistency across different camera\nviewpoints.\nSpecifically, we uniformly sample the field of view\n(FOV) from 30\u25e6 to 90\u25e6 for the intrinsic parameters of cam-\neras. When setting the extrinsic parameters for camera tra-\njectories, we position the cameras at distances ranging from\n1 meter to 5 meters around the human, and at heights vary-\ning from 0.5 meters to 2 meters to the ground. Finally, we\ngenerate pseudo 2D skeleton annotations by projecting 3D\njoints into the 2D pixel-plane using these virtual cameras.\nMoreover, to simulate the jitter of 2D detectors, we add\nGaussian noise \u2206X \u223c N(0, 0.01) to 3D joints before pro-\njections.\n4. Method\nAs illustrated in Fig. 2, we first detect the 2D skeletons from\nimages and input them into our proxy-to-motion network to\nrecover 3D local motions in human space. These relative lo-\ncal motions are transformed into a world coordinate system\nand accumulated along the sliding window. Additionally,\nwe leverage a neural descent module to refine the accuracy\nand physical plausibility. In this section, we start with intro-\nducing the human-centric setting of our motion prediction.\n4.1. Human-Centric Motion Modeling\nFor more accurate camera modeling, we adopt the classical\npinhole camera model instead of using the simplified or-\nthogonal or weak perspective projection [9, 15, 19, 41, 70,\n77]. As shown in Fig. 3, we transform the global human\nmotion and the camera trajectories into local human space\nfrom two adjacent frames, where we adopt {\u03b2 \u2208 R10, \u03b8 \u2208\nR22\u00d73, t \u2208 R3}t to denote the parameters of shape, pose,\nand translation at frame t, and {R \u2208 R3\u00d73, T \u2208 R3}t to\ndenote the camera extrinsic parameters. Given the pose and\nshape parameters, the joints and vertices can be obtained\nwithin the SMPL Layer: {J, V } = X (\u03b2, t, \u03b8, g). In the\nfollowing, we use the subscript H and W to distinguish\nbetween the human coordinate system and the world coor-\ndinate system.\nDuring the learning of human-centric motions, we adopt\na similar setting used in previous works on temporal human\nmotion priors [24, 45, 67]. Specifically, each motion se-\nquence will be normalized by a rigid transformation to re-\nmove the initial translation in x-z plane and rotate itself with\nthe root orientation heading to the z-axis direction. With\nthis setting, our network can learn the relative movements of\nhumans, which are independent of observation viewpoints.\n4\nFigure 3. Illustration of decoupling the world-space motion into\nthe human-centric coordinates and relative camera poses.\nThe detailed implementation of the human-to-world coordi-\nnate transformation and the global motion accumulation in\nworld space can be found in our supplementary material.\n4.2. Sequential Full-body Motion Initialization\nAs shown in Fig. 2, at the first stage of our method, the\nskeleton sequences are processed by temporal encoders and\nthen fed into decoders for the predictions of the initial mo-\ntion {\u03b20, \u03b80, t0}, initial camera {R0\nH, T0\nH}, the contact in-\ndicator ind, and the hand poses g.\nFollowing previous\nbaseline [41], we build these encoders upon temporal di-\nlated convolution networks.\nFor better body-hand compatibility, we exploit the cross-\nattention mechanism to facilitate the motion context sharing\nduring the body and hand recovery. Specifically, we first ob-\ntain the initial body features Fbody and hand features Fhand\nfrom the temporal encoders and map them as Query, Key,\nValue matrices in forms of Qbody/hand, Kbody/hand, and\nVbody/hand, respectively. Then we update the body features\nF \u2032\nbody and hand features F \u2032\nhand as follows:\nF \u2032\nbody = Vbody + Softmax(\nQhandK\u22a4\nbody\n\u221adk\n)Vbody,\nF \u2032\nhand = Vhand + Softmax(QbodyK\u22a4\nhand\n\u221adk\n)Vhand.\n(2)\nThe updated features {F \u2032\nbody, F \u2032\nhand} can be further uti-\nlized in the contact indicators ind and serve as the temporal\ncontext in the Neural Descent module, as will be described\nshortly. In our experiments, we demonstrate that the feature\nfusion in Eq. 2 can make two tasks benefit each other to pro-\nduce more comparable wrist poses in the full-body model.\n4.3. Contact-aware Neural Motion Descent\nAt the second stage of our method, the initial motion pre-\ndictions will be refined to be more accurate and physically\nplausible with the proposed contact-aware neural motion\ndescent module. As shown in Fig. 2, this module takes the\n2D misalignment and body-ground contact status as input\nand updates motion parameters during iterations.\nMisalignment and Contact Calculation.\nAt the iteration\nof i \u2208 {0, 1, ..., N}, we calculate the 2D misalignment sta-\ntus by projecting the 3D joints on the image plane and calcu-\nlate the differences between the re-projected 2D joints and\nthe proxy observations: Sproj = \u03a0(Ji, {K, Ri\nH, T i\nH}) \u2212\nbJ2D. Here, \u03a0(\u00b7) denotes the perspective projection func-\ntion, and K denotes the intrinsic parameter.\nFor the contact status, we calculate the velocity of 3D\njoints vi\nxz in xz-plane and the distance to the ground as di\ny.\nMoreover, we also leverage the temporal features from in-\nputs 2D skeletons to predict the contact labels ind, which\nwill be used as an indicator to mask the body-ground con-\ntact. Then, the contact status of the current predictions can\nbe obtained as Scontact = ind\u2299(vi\nxz, di), where \u2299 denotes\nthe Hadamard product operation.\nMotion Update.\nAfter obtaining the contact and mis-\nalignment status, we feed them into the neural motion de-\nscent module for motion updates. As shown in Fig. 4, the\ndescent module takes the two groups of tensors as input: i)\nthe state group includes the current SMPL parameters in the\nhuman coordinate system \u03b2i, ti\nH, \u03b8i\nH, camera pose Ri\nH, T i\nH\nand the sequential motion context Fseq = {F \u2032\nbody, F \u2032\nhand};\nii) the deviation group includes the current misalignment\nstatus Sproj and contact status Scontact.\nA straightforward solution would be using an MLP to\nprocess these two groups of tensors.\nHowever, the val-\nues of these two groups exhibit significant differences. For\ninstance, the values of the state tensors change smoothly\nwhile the values of the deviation tensors may change rapidly\nalong with the misalignment and contact status.\nSimply\nconcatenating them as inputs introduces difficulty in the\nlearning process.\nNote that the magnitude of the devia-\ntion tensors is highly correlated with the parameter updates.\nWhen the body model is well-aligned without foot skating\nor ground penetration, the values of the deviation tensors\nare almost zero, indicating that the refiner should output ze-\nros to prevent further changes in the pose parameters. Oth-\nerwise, the refiner should output larger update values for\nmotion adjustments. To leverage such a correlation prop-\nerty, we exploit a cross-attention module to build a more\neffective architecture.\nAs shown in Fig. 4, two fully connected layers are\nleveraged to process the tensors of the state and deviation\ngroups and produce the Query, Key, and Value for the cross-\nattention module. In this way, our contact-aware neural mo-\ntion descent module can effectively learn the relationship\nbetween the state and deviation groups and hence produce\nmore accurate motion updates. Moreover, the sequential\nmotion context Fseq is also leveraged in our neural descent\n5\nFigure 4. Implementations of the neural descent module.\nmodule to mitigate the depth uncertainty and improve the\nmotion predictions.\nCompared with previous work [45, 52, 68, 73], the pro-\nposed contact-aware neural motion descent module offers\nthe advantage of freeing us from the heavy cost of ex-\nplicit gradient calculations or the manual tuning of hyper-\nparameters during testing. Furthermore, the module is ca-\npable of learning human motion priors with contact infor-\nmation from our synthetic dataset, which provides a more\nsuitable descent direction and steps to escape the local min-\nima and achieve faster convergence.\n4.4. Loss Function\nIn our solution, the full-body motion recovery module and\nthe contact-aware neural motion descent module are trained\nsequentially. Benefiting from the proxy-to-motion learn-\ning, the ground-truth full-body pose \u03b8, g, and human body\nshape \u03b2b can be obtained for supervision from our synthetic\ndataset. Overall, the objective function of motion recovery\ncan be written as follows:\nLrec = L3D+L2D+L\u03b8+L\u03b2 +Lcam+Lconsist+Lsmooth\n(3)\nSpecifically, L3D involves 3D MPJPE loss and 3D tra-\njectory L1 loss while L2D is the projected 2D MPJPE loss.\nL\u03b8, L\u03b2, Lcam represents L1 loss between the estimated hu-\nman pose, shape and camera pose to our synthetic ground\ntruth. Lconsist is a L1 loss to restrict the consistency of\nthe local motion outputs \u03b8H, tH of the same 3D motion\nsequence via different observations from virtual cameras.\nLsmooth is adopted from\n[69] by penalizing the veloc-\nity and acceleration between the estimation and the ground\ntruth. For the neural descent module, the objective loss can\nbe written as:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nLdesc = P\nk uN\u2212k(Lrec + Lcontact)\nLcontact = P\ni indgt \u2299 (||vxz||2 + ||dy||2)\nLind = P\ni Entropy(indgt, indest)\n(4)\nwhere k = 1, 2, ..., N is the iteration time and u is the decay\nratio to emphasize the last iteration. We set K = 3, u = 0.8\nin practice. Lcontact involves the error of trajectory drifting,\nTable 1.\nQuantitative comparison on EgoBody [74] and\nRICH [11]. The symbol \u2020 denotes the methods relying on SLAM.\nMethods\nW-MPJPE \u2193\nWA-MPJPE \u2193\nPA-MPJPE \u2193\nACCEL \u2193\nEgoBody dataset\n\u2020 SLAHMR [63]\n141.1\n101.2\n79.13\n25.78\n\u2020 PACE [18]\n147.9\n101.0\n66.5\n6.7\nGLAMR [67]\n416.1\n239.0\n114.3\n173.5\nOurs\n385.5\n131.3\n73.5\n49.6\nRICH dataset\n\u2020SLAHMR [63]\n571.6\n323.7\n52.5\n9.4\n\u2020PACE [18]\n380.0\n197.2\n49.3\n8.8\nGLAMR [67]\n653.7\n365.1\n79.9\n107.7\nOurs\n629.8\n343.6\n56.0\n25.3\nfoot floating, or ground penetration. Lind refers to the loss\nbetween the predicted contact label to the ground truth.\n5. Experiments\nIn this Section, we validate the efficacy of our method and\ndemonstrate accurate human motion capture results with\nphysically plausible foot-ground contact in world space.\nDataset. The RICH dataset [11] is collected with a multi-\nview static camera system and one moving camera that the\nground truth 3D human motions can be recovered using spa-\ntial stereo geometry. The EgoBody [74] is captured by a\nmulti-camera rig and a head-mounted device, focusing on\nthe social interactions in 3D scenes. Dynamic Human3.6M\nis a benchmark proposed in [67] to simulate the moving\ncameras on Human3.6M [13] by randomly cropping with a\nsmall view window around the person.\nMetrics. In our experiments, we follow previous work [67]\nto report various metrics, primarily focusing on the evalua-\ntion of the world coordinate system. The WA-MPJPE met-\nric reports the MPJPE after aligning the entire trajectory of\nboth the predicted and GT through Procrustes Alignment.\nThe W-MPJPE metric reports the MPJPE after aligning the\nfirst frame of sequences. The PA-MPJPE metric reports the\nMPJPE error after applying the ground truth trajectories to\neach frame. The ACCEL metric is used to evaluate the joint\nacceleration.\n5.1. Comparison with the State of the Art\nWe compare our approach with the state-of-the-art ap-\nproaches to human motion recovery under dynamic\ncameras, including GLAMR [67], SLAMHR [63] and\nPACE [18]. Both the SLAMHR and PACE require a pre-\nprocessing SLAM to reconstruct the scene to solve the cam-\nera trajectories (refer to Tab. 4). Such a process is time con-\nsuming and requires texture-rich backgrounds, which nar-\nrows their applications.\nTo validate the effectiveness of\nthe proposed solution, we primarily compare our method\nwith GLAMR, as it also runs without SLAM. We also con-\nduct comparison experiments on the RICH and EgoBody\ndatasets, as shown in Tab. 1. As shown in the table, our\nmethod achieves significant improvements over previous\n6\nFigure 5. Results across different cases in the (a,g) 3DPW [61], (b) EHF [40], and (c) Human3.6M [13] datasets and (d,e,f) internet videos.\nWe demonstrate that our method can recover the accurate and plausible human motions in moving cameras at a real-time performance.\nSpecifically, (g) demonstrates the robustness and the temporal coherence of our method even under the occlusion inputs.\n7\nsolutions in all metrics.\nVisual comparisons with previous different solutions are\nalso depicted in Fig. 6 and the video in our supplementary\nmaterials, where our method again shows superior results in\nterms of model-image alignment and foot-ground contact in\nworld space.\nFigure 6. Qualitative comparison with previous state-of-the-art\nmethods: (a) PyMAF-X [72], (c) GLAMR [67], (b)(d) Ours.\n5.2. Ablation Studies\nWe conduct ablation studies to validate the effectiveness of\nthe proposed neural descent method on the Dynamic Hu-\nman3.6M dataset following the setup of [67]. As shown in\nTab 2, the Neural Descent module can significantly reduce\nthe motion errors in world space.\nTable 2. Ablation study of the Neural Descent module on the Dy-\nnamic Huaman3.6M dataset.\nNeural Descent\nW-MPJPE \u2193\nPA-MPJPE \u2193\nw/o\n644.8\n48.4\nw/\n605.4\n45.9\nWe also report the metric of ground penetration [66]\n(GP) and foot floating (FF) in the Human3.6M [13] dataset.\nThe GP is defined as the percentage of the frames that pen-\netrate to the ground. The FF is defined as the percentage of\nframes with foot-ground distances far from the given thresh-\nold. We report the curves of GP and FF with respect to the\ndistance to ground in Fig. 7 with a logarithmic scale, where\nwe can conclude that the neural descent algorithm can sig-\nnificantly improve the ground contact plausibility.\nFigure 7. The ablation study on the percentage of foot floating (FF)\nand ground penetration (GP). We vary the threshold from 1cm to\n3cm to calculate the corresponding FF and GP metrics.\n5.3. Runtime\nIt is also worth noting that the proposed method has im-\nproved the speed by an order of magnitude compared to the\nprevious methods. The speeds of different methods are re-\nported in Tab. 4. Our method can reach real-time perfor-\nmance at 30 FPS in a laptop with RTX4060, which is very\npromising to enable various applications related to virtual\nhumans.\nTable 3. Runtime comparison with the state-of-the-art methods.\nMethod\nSLAMHR\nPACE\nGLAMR\nOurs\nFPS\n0.04\n2.1\n2.4\n30\n6. Conclusion\nIn this paper, we present ProxyCap, a real-time monocular\nfull-body motion capture approach with physically plausi-\nble foot-ground contact in world space. We leverage a proxy\ndataset based on 2D skeleton sequences with accurate 3D\nrotational motions in world space. Based on the proxy data,\nour network learns motions from a human-centric percep-\ntive, which enhances the consistency of human motion pre-\ndictions under different camera trajectories. For more ac-\ncurate and physically plausible motion capture, we further\npropose a contact-aware neural motion descent module so\nthat our network can be aware of foot-ground contact and\nmotion misalignment. Based on the proposed solution, we\ndemonstrate a real-time monocular full-body capture sys-\ntem under moving cameras.\nLimitations.\nAs our method recovers 3D motion from 2D\njoint observations, the depth ambiguity issue remains espe-\ncially when the person is captured in nearly static poses.\n8\nFigure 8. Illustration of human recovery. The left part (a) depicts the original dilated convolution backbone of [41], while the right part\n(b) illustrates our proposed partial dilated convolution architecture. Our approach selectively excludes corrupted data from input signals,\nallowing us to extract motion features more accurately and effectively. Specifically, detection failure (denoted as green mosaic squares)\nmay occur during extremely fast motion or severe occlusion situations, while our architecture will cut off the connection from corrupted\ndata to prevent disturbance transfer during network forward processing.\nSupplementary Material\n7. Implementation Details\n7.1. Human-to-World Coordinate Transformation\nWe estimate the local human pose of each frame in our\nproxy-to-motion network, then we transform it into the\nglobal world space.\nIn the first frame, the accumulated\ntranslation of human in x-z plane txz is set to zero. For\nthe later human-space estimations, we firstly rotate the front\naxis of camera in human space to align the y-z plane by\nRfront. Denote the target parameters of camera in world\nspace as RW , TW and the predicted parameters in human\nspace as RH, TH. We have RW = RH \u00b7 Rfront, TW =\n\u2212RW \u00b7 (RT\nfront \u00b7 (\u2212RT\nH \u00b7 TH) + txz).\nAnd the human\norientation should also be rotate in the same time to main-\ntain relative stillness: \u03b8W (root) = RT\nfront \u00b7 \u03b8H(root). The\nworld translation can be calculated by tW = RT\nfront \u00b7(tH +\nJroot) \u2212 Jroot + txz. Here Jroot is the root joint of SMPL\nmodel in T-pose to eliminate the error resulted from the\nroot-misalignment with the original point of SMPL model.\nFinally, the accumulated translation of human in x-z plane\ntxz is updated by tt+1\nxz\n= tt\nxz + RT\nfront \u00b7 tH.\n7.2. Partial Dilated Convolution\nIt should be noted that the hand area, being an extremity, is\nmore prone to being affected by heavy motion blur and se-\nvere occlusions, resulting in missing detections. Simply set-\nting the corrupted data to zero is not a viable solution as the\noriginal convolution kernel is unable to distinguish between\nnormal data and corrupted data, leading to a significant re-\nduction in performance as noise is propagated through the\nnetwork layers.\nTo overcome this challenge, we employ partial convo-\nlution [25] to enhance our 1D dilated convolution frame-\nwork. As illustrated in Fig. 8, rather than indiscriminately\nprocessing the input signals as in the original convolution\noperator, we utilize a mask-weighted partial convolution to\nselectively exclude corrupted data from the inputs. This en-\nhances the robustness of hand recovery in scenarios involv-\ning fast movement and heavy occlusion. Specifically, the la-\ntent code X0 is initially set as the concatenation of the (x, y)\ncoordinates of the J joints for each frame f \u2208 1, 2, ..., L,\nwhile the mask M0 is initialized as a binary variable with\nvalues of 0 or 1, corresponding to the detection confidence.\nThen we integrated the 2D partial convolution operation and\nmask update function from [25] into our 1D dilated convo-\nlution framework:\n(\nMk+1 = I (sum(Mk) > 0)\nXk+1 = Mk+1\n\u0010\nW \u22a4\nk (Xk \u2299 Mk) size(Mk)\nsum(Mk) + bk\n\u0011\n(5)\nwhere W and b denotes the convolution filter weights and\nbias at layer k, and \u2299 denotes the element-wise multipli-\ncation. Furthermore, in the training procedure, half of the\nsequential inputs are randomly masked to simulate detec-\ntion failures that may occur in a deployment environment.\n7.3. Training\nWe train our network using the Adam optimizer with\na learning rate of 1e-4 and a batch size of 1024 in\nNVIDIA RTX 4090. We adopt a two-stage training scheme.\nFirstly, we train our proxy-to-motion initialization network\n(Sec. 4.2) for 50 epochs. Subsequently, we fix the weights\nof the motion recovery network and train the neural descent\nnetwork (Sec. 4.3) for another 50 epochs.\n7.4. Proxy Dataset\nWe conduct the training process on our synthetic proxy\ndataset.\nThe 3D body rotational motions of the train-\ning set are sampled from AMASS [29]: [ACCAD, BML-\nmovi, BMLrub, CMU, CNRS, DFaust, EKUT, Eyes\nJapan Dataset, GRAB, HDM05, HumanEva, KIT, MoSh,\n9\nPosePrior, SFU, SOMA, TCDHands, TotalCapture, WEIZ-\nMANN] and [SSM, Transitions] are for testing. Otherwise,\nfor the generation of hand motions, we adopt the same\ndataset division of InterHand [33]. Then We animate the\nSMPL-X mesh and generate virtual cameras to obtain the\npseudo 2D labels.\n8. Computational Complexity\nIn this section, we compare the inference speed of our\nmethod. Our real-time monocular full-body capture sys-\ntem can be implemented on a single Laptop (NVIDIA RTX\n4060 GPU). Specifically, for the 2D pose estimator, we\nleverage the off-the-shelf Mediapipe [26] and MMPose and\nre-implement it on the NVIDIA TensorRT platform. We re-\nport the inference time of each module in Table. 4.\nTable 4. Time costs of each module in our pipeline.\nNetwork\nInput\nSpeed\nBody Crop Net\n224 \u00d7 224 \u00d7 3\n2ms\nBody Landmark Net\n384 \u00d7 288 \u00d7 3\n5ms\nHand Crop Net\n2 \u00d7 256 \u00d7 256 \u00d7 3\n1.5ms\nHand Landmark Net\n256 \u00d7 256 \u00d7 3\n2ms\nPose Initialization Net\n81 \u00d7 67 \u00d7 2\n3ms\nNeural Descent Net\n\\\n10ms\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2d human pose estimation: New benchmark\nand state of the art analysis. In CVPR, pages 3686\u20133693,\n2014. 2\n[2] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir,\nWilliam T Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. Hspace: Synthetic parametric humans animated\nin complex environments. arXiv preprint arXiv:2112.12867,\n2021. 2\n[3] Michael J. Black, Priyanka Patel, Joachim Tesch, and Jin-\nlong Yang. BEDLAM: A synthetic dataset of bodies exhibit-\ning detailed lifelike animated motion. In CVPR, pages 8726\u2013\n8737, 2023. 2\n[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J Black. Keep it SMPL:\nAutomatic estimation of 3D human pose and shape from a\nsingle image. In ECCV, pages 561\u2013578. Springer, 2016. 1, 3\n[5] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-\nitrios Tzionas, and Michael J Black. Monocular expressive\nbody regression through body-driven attention. In ECCV,\npages 20\u201340. Springer, 2020. 3\n[6] Enric Corona, Gerard Pons-Moll, Guillem Aleny`a, and\nFrancesc Moreno-Noguer. Learned vertex descent: A new\ndirection for 3D human model fitting. In ECCV, pages 146\u2013\n165, 2022. 3\n[7] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael J Black. Collaborative regression of\nexpressive bodies using moderation. In I3DV, pages 792\u2013\n804, 2021. 3\n[8] Yao Feng, Haiwen Feng, Michael J. Black, and Timo\nBolkart.\nLearning an animatable detailed 3D face model\nfrom in-the-wild images. TOG, 40(4):88:1\u201388:13, 2021. 3\n[9] Kehong Gong, Bingbing Li, Jianfeng Zhang, Tao Wang, Jing\nHuang, Michael Bi Mi, Jiashi Feng, and Xinchao Wang.\nPosetriplet: co-evolving 3d human pose estimation, imita-\ntion, and hallucination under self-supervision.\nIn CVPR,\npages 11017\u201311027, 2022. 4\n[10] Peng Guan, Alexander Weiss, Alexandru O Balan, and\nMichael J Black. Estimating human shape and pose from\na single image. In ICCV, pages 1381\u20131388. IEEE, 2009. 1\n[11] Chun-Hao P Huang, Hongwei Yi, Markus H\u00a8oschle, Matvey\nSafroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel\nScharstein, and Michael J Black.\nCapturing and infer-\nring dense full-body human-scene contact. In CVPR, pages\n13274\u201313285, 2022. 1, 6\n[12] Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo\nKanazawa, Peter V Gehler, Javier Romero, Ijaz Akhter, and\nMichael J Black. Towards accurate marker-less human shape\nand pose estimation over time.\nIn I3DV, pages 421\u2013430.\nIEEE, 2017. 1\n[13] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nTPAMI, 36(7):1325\u20131339, 2014. 1, 2, 6, 7, 8\n[14] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\nemplar fine-tuning for 3D human pose fitting towards in-\nthe-wild 3D human pose estimation. In I3DV, pages 42\u201352,\n2021. 2\n[15] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, pages 7122\u20137131, 2018. 1, 4\n[16] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\nand Michael J Black.\nPARE: Part attention regressor for\n3D human body estimation. In ICCV, pages 11127\u201311137,\n2021. 2, 3\n[17] Muhammed Kocabas, Chun-Hao P Huang, Joachim Tesch,\nLea Muller, Otmar Hilliges, and Michael J Black. SPEC:\nSeeing people in the wild with an estimated camera.\nIn\nICCV, pages 11035\u201311045, 2021. 2, 3\n[18] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong\nGuo, Michael J. Black, Otmar Hilliges, Jan Kautz, and Umar\nIqbal. PACE: Human and motion estimation from in-the-\nwild videos. In 3DV, 2024. 2, 3, 6\n[19] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-fitting in the loop.\nIn ICCV, pages\n2252\u20132261, 2019. 1, 2, 4\n[20] Christoph Lassner, Javier Romero, Martin Kiefel, Federica\nBogo, Michael J Black, and Peter V Gehler. Unite the peo-\nple: Closing the loop between 3D and 2D human representa-\ntions. In CVPR, pages 6050\u20136059, 2017. 1\n[21] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,\nand Cewu Lu. HybrIK: A hybrid analytical-neural inverse\nkinematics solution for 3D human pose and shape estima-\ntion. In CVPR, pages 3383\u20133393, 2021. 2, 3\n10\n[22] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu,\nFeng Chen, Tao Yu, and Yebin Liu.\nInteracting attention\ngraph for single image two-hand reconstruction. In CVPR,\npages 2761\u20132770, 2022. 3\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755. Springer, 2014. 2\n[24] Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel\nvan de Panne. Character controllers using motion vaes. ACM\nTrans. Graph., 39(4), 2020. 4\n[25] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro.\nImage inpainting for\nirregular holes using partial convolutions. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n2018. 9\n[26] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-\nClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-\nLing Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-\napipe: A framework for building perception pipelines. arXiv\npreprint arXiv:1906.08172, 2019. 10\n[27] Zhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. Embod-\nied scene-aware human pose estimation. In NeurIPS, 2022.\n3\n[28] Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, and\nYizhou Wang. 3d human mesh estimation from virtual mark-\ners. In CVPR, pages 534\u2013543, 2023. 2, 3\n[29] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black. AMASS: Archive of\nmotion capture as surface shapes.\nIn ICCV, pages 5442\u2013\n5451, 2019. 2, 3, 9\n[30] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3D human pose estimation in the wild\nusing improved CNN supervision. In I3DV, pages 506\u2013516,\n2017. 1\n[31] Gyeongsik Moon and Kyoung Mu Lee.\nI2L-MeshNet:\nImage-to-lixel prediction network for accurate 3D human\npose and mesh estimation from a single RGB image.\nIn\nECCV, pages 752\u2013768. Springer, 2020. 2, 3\n[32] Gyeongsik Moon and Kyoung Mu Lee. Pose2Pose: 3D posi-\ntional pose-guided 3D rotational pose prediction for expres-\nsive 3D human pose and mesh estimation. arXiv preprint\narXiv:2011.11534, 2020. 2, 3\n[33] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,\nand Kyoung Mu Lee. InterHand2.6M: A dataset and baseline\nfor 3D interacting hand pose estimation from a single RGB\nimage. In ECCV, pages 548\u2013564. Springer, 2020. 3, 10\n[34] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.\nNeuralAnnot: Neural annotator for 3D human mesh training\nsets. In CVPRW, pages 2299\u20132307, 2022. 2\n[35] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Ac-\ncurate 3D hand pose estimation for whole-body 3D human\nmesh estimation. In CVPRW, 2022. 3\n[36] Lea M\u00a8uller, Ahmed AA Osman, Siyu Tang, Chun-Hao P\nHuang, and Michael J Black. On self-contact and human\npose. In CVPR, pages 9990\u20139999, 2021. 2\n[37] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-\nter Gehler, and Bernt Schiele. Neural body fitting: Unifying\ndeep learning and model-based human pose and shape esti-\nmation. In I3DV, pages 484\u2013494. IEEE, 2018. 2, 3\n[38] Priyanka Patel,\nChun-Hao P Huang,\nJoachim Tesch,\nDavid T Hoffmann, Shashank Tripathi, and Michael J Black.\nAGORA: Avatars in geography optimized for regression\nanalysis. In CVPR, pages 13468\u201313478, 2021. 2, 3\n[39] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to estimate 3D human pose and shape\nfrom a single color image. In CVPR, pages 459\u2013468, 2018.\n2, 3\n[40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3d hands, face,\nand body from a single image. In CVPR, 2019. 1, 7\n[41] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and\nMichael Auli.\n3D human pose estimation in video with\ntemporal convolutions and semi-supervised training. pages\n7753\u20137762, 2019. 4, 5, 9\n[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\npages 9054\u20139063, 2021. 2\n[43] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people by pre-\ndicting 3D appearance, location and pose. In CVPR, pages\n2740\u20132749, 2022. 3\n[44] Davis Rempe, Leonidas J Guibas, Aaron Hertzmann, Bryan\nRussell, Ruben Villegas, and Jimei Yang. Contact and human\ndynamics from monocular video. In ECCV, pages 71\u201387.\nSpringer, 2020. 3\n[45] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J. Guibas. HuMoR: 3D human\nmotion model for robust pose estimation. In ICCV, 2021. 3,\n4, 6\n[46] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. FrankMocap:\nA monocular 3D whole-body pose estimation system via re-\ngression and integration. In ICCV, 2021. 3\n[47] Nadine Rueegg, Christoph Lassner, Michael Black, and\nKonrad Schindler. Chained representation cycling: Learning\nto estimate 3D human pose and shape by cycling between\nrepresentations. In AAAI, pages 5561\u20135569, 2020. 2, 3\n[48] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 2\n[49] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Syn-\nthetic training for accurate 3D human pose and shape esti-\nmation in the wild. In BMVC, 2020. 2\n[50] Leonid Sigal, Alexandru Balan, and Michael J Black. Com-\nbined discriminative and generative articulated pose and\nnon-rigid shape estimation. In NeurIPS, pages 1337\u20131344,\n2008. 1\n[51] Leonid Sigal, Alexandru O Balan, and Michael J Black. Hu-\nmanEva: Synchronized video and motion capture dataset and\nbaseline algorithm for evaluation of articulated human mo-\ntion. IJCV, 87(1-2):4, 2010. 1, 2\n11\n[52] Jie Song, Xu Chen, and Otmar Hilliges. Human body model\nfitting by learned gradient descent. In ECCV, pages 744\u2013760.\nSpringer, 2020. 2, 3, 6\n[53] Jiajun Su, Chunyu Wang, Xiaoxuan Ma, Wenjun Zeng, and\nYizhou Wang. Virtualpose: Learning generalizable 3d hu-\nman pose models from virtual data. In ECCV, pages 55\u201371.\nSpringer, 2022. 3\n[54] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao\nMei. Human mesh recovery from monocular images via a\nskeleton-disentangled representation. In ICCV, pages 5349\u2013\n5358, 2019. 2, 3\n[55] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael J Black.\nTRACE: 5D temporal regression of avatars with dynamic\ncameras in 3D environments. In CVPR, pages 8856\u20138866,\n2023. 3\n[56] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual\nSLAM for Monocular, Stereo, and RGB-D Cameras. Ad-\nvances in neural information processing systems, 2021. 2\n[57] Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.\nRecovering 3D human mesh from monocular images: A sur-\nvey. TPAMI, 2023. 2\n[58] Hsiao-Yu Fish Tung, Hsiao-Wei Tung, Ersin Yumer, and Ka-\nterina Fragkiadaki. Self-supervised learning of motion cap-\nture. NeurIPS, pages 5236\u20135246, 2017. 2, 3\n[59] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-\nmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.\nLearning from synthetic humans. In CVPR, pages 109\u2013117,\n2017. 3\n[60] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin\nYumer, Ivan Laptev, and Cordelia Schmid. BodyNet: Volu-\nmetric inference of 3D human body shapes. In ECCV, pages\n20\u201336, 2018. 2, 3\n[61] Timo von Marcard, Roberto Henschel, Michael Black, Bodo\nRosenhahn, and Gerard Pons-Moll. Recovering accurate 3d\nhuman pose in the wild using imus and a moving camera. In\nEuropean Conference on Computer Vision (ECCV), 2018. 7\n[62] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. DenseRaC:\nJoint 3D pose and shape estimation by dense render-and-\ncompare. In ICCV, pages 7760\u20137770, 2019. 2, 3\n[63] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo\nKanazawa.\nDecoupling human and camera motion from\nvideos in the wild. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 3, 6\n[64] Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas,\nMuhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus\nThies, and Michael J. Black.\nHuman-aware object place-\nment for visual environment reconstruction. In CVPR, pages\n3959\u20133970, 2022. 1\n[65] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth\nVenkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park.\nHUMBI: A large multiview dataset of human body expres-\nsions. In CVPR, pages 2990\u20133000, 2020. 2\n[66] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason\nSaragih. SimPoE: Simulated character control for 3D human\npose estimation. In CVPR, pages 7159\u20137169, 2021. 3, 8\n[67] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and\nJan Kautz. GLAMR: Global occlusion-aware human mesh\nrecovery with dynamic cameras. In CVPR, pages 11038\u2013\n11049, 2022. 2, 3, 4, 6, 8\n[68] Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir,\nWilliam T Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu.\nNeural descent for visual 3D human pose and\nshape. In CVPR, pages 14484\u201314493, 2021. 3, 6\n[69] Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang,\nand Qiang Xu.\nSmoothNet: a plug-and-play network for\nrefining human poses in videos. In ECCV, pages 625\u2013642.\nSpringer, 2022. 6\n[70] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In ICCV, pages 11446\u201311456, 2021. 1,\n3, 4\n[71] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and\nZhenan Sun. Learning 3D human shape and pose from dense\nbody parts. TPAMI, 2022. 2, 3\n[72] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu. PyMAF-X: To-\nwards well-aligned full-body model regression from monoc-\nular images. TPAMI, 2023. 1, 3, 8\n[73] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys,\nand Siyu Tang. Learning motion priors for 4D human body\ncapture in 3D scenes. In ICCV, pages 11343\u201311353, 2021.\n3, 6\n[74] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein\nKwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Ego-\nbody: Human body shape and motion of interacting peo-\nple from head-mounted devices. In European conference on\ncomputer vision (ECCV), 2022. 6\n[75] Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, and\nYebin Liu. 4d association graph for realtime multi-person\nmotion capture using multiple video cameras.\nIn CVPR,\npages 1324\u20131333, 2020. 2\n[76] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu,\nand Yebin Liu. Lightweight multi-person total motion cap-\nture using sparse multi-view cameras. In ICCV, pages 5560\u2013\n5569, 2021. 2\n[77] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang,\nChen Chen, and Zhengming Ding. 3D human pose estima-\ntion with spatial and temporal transformers. In ICCV, pages\n11656\u201311665, 2021. 4\n[78] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush\nTewari, Christian Theobalt, and Feng Xu. Monocular real-\ntime full body capture with inter-part correlations. In CVPR,\npages 4811\u20134822, 2021. 3\n12\n"
  },
  {
    "title": "MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion",
    "link": "https://arxiv.org/pdf/2307.01097.pdf",
    "upvote": "7",
    "text": "MVDiffusion: Enabling Holistic Multi-view Image\nGeneration with Correspondence-Aware Diffusion\nShitao Tang1\u2217\nFuyang Zhang1\u2217\nJiacheng Chen1\nPeng Wang2\nYasutaka Furukawa1\n1Simon Fraser University\n2Bytedance\n\u201cThis kitchen is a charming blend of rustic and modern, featuring a large reclaimed wood island with marble countertop, a sink \nsurrounded by cabinets. A stainless-steel refrigerator stands tall. To the right of the sink, built-in wooden cabinets painted in a muted.\u201d\n\u201cA living room with multiple couches and a coffee table. A wooden book shelf filled with lots of books next to a door. A white refrigerator \nsitting next to a wooden bench.\u201d\n\u001f\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001d\u001a\u0018\u0017\u0016\u0015\u001a\u001b\u0014\u0013\u001b\u0019\u0012\u0018\u0011\u0010\u000f\u000e\u0019\u001c\n\u001f\u0015\u001e\u001c\u0019\r\u0014\f\u001e\u001e\u000b\u0018\n\u000f\u001d\u001e\t\u000f\u0010\u001b\b\u0018\u0011\u0010\u000f\u000e\u0019\n...\n\u0007\u0019\u000b\u001a\u0006\u0018\u001c\u0019\u0005\u0016\u0019\u001d\b\u0019\n\u0017\u0019\u001c\u0006\u0018\u0012\u0004\u001e\u0018\u001a\u0019\u0003\u001a\u0016\t\u0019\n\u001f\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001d\u001a\u0018\u0010\u0016\u0015\u001a\u001b\u0014\u0013\u001b\u0019\u0012\u0018\n\u001b\u0010\u000f\u000e\u0019\u001c\u0018\b\u001e\u001d\r\u001b\u001a\u001b\u001e\u001d\u0019\r\u0018\u001e\u001d\u0018\r\u0019\u000b\u001a\u0006\u001c\n\u0017\u0019\u001c\u0006\u0018\u0012\u0004\u0018\u001a\u0019\u0003\u001a\u0016\t\u0019\n\u0002\t\u000f\u0001\u0019\b\u001a\u001e\t\u007f\n...\nFigure 1: MVDiffusion synthesizes consistent multi-view images. Top: generating perspective crops\nwhich can be stitched into panorama; Bottom: generating coherent multi-view images from depths.\nAbstract\nThis paper introduces MVDiffusion, a simple yet effective method for generating\nconsistent multi-view images from text prompts given pixel-to-pixel correspon-\ndences (e.g., perspective crops from a panorama or multi-view images given depth\nmaps and poses). Unlike prior methods that rely on iterative image warping and\ninpainting, MVDiffusion simultaneously generates all images with a global aware-\nness, effectively addressing the prevalent error accumulation issue. At its core,\nMVDiffusion processes perspective images in parallel with a pre-trained text-to-\nimage diffusion model, while integrating novel correspondence-aware attention\nlayers to facilitate cross-view interactions. For panorama generation, while only\ntrained with 10k panoramas, MVDiffusion is able to generate high-resolution\nphotorealistic images for arbitrary texts or extrapolate one perspective image to a\n360-degree view. For multi-view depth-to-image generation, MVDiffusion demon-\nstrates state-of-the-art performance for texturing a scene mesh. The project page is\nat https://mvdiffusion.github.io/.\n*Equal contribution. Contact the authors at shitaot@sfu.ca.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.01097v7  [cs.CV]  25 Dec 2023\n1\nIntroduction\nPhotorealistic image synthesis aims to generate highly realistic images, enabling broad applications\nin virtual reality, augmented reality, video games, and filmmaking. The field has seen significant\nadvancements in recent years, driven by the rapid development of deep learning techniques such as\ndiffusion-based generative models [2, 16, 21, 39, 43, 44, 45].\nOne particularly successful domain is text-to-image generation. Effective approaches include gener-\native adversarial networks [3, 12, 19], autoregressive transformers [10, 35, 49], and more recently,\ndiffusion models [15, 17, 34, 37]. DALL-E 2 [34], Imagen [37] and others generate photorealistic\nimages with large-scale diffusion models. Latent diffusion models [36] apply the diffusion process in\nthe latent space, allowing more efficient computations and faster image synthesis.\nDespite impressive progress, multi-view text-to-image synthesis still confronts issues of compu-\ntational efficiency and consistency across views. A common approach involves an autoregressive\ngeneration process [5, 11, 18], where the generation of the n-th image is conditioned on the (n\u22121)-th\nimage through image warping and inpainting techniques. However, this autoregressive approach\nresults in accumulated errors and does not handle loop closure [11]. Moreover, the reliance on the\nprevious image may pose challenges for complex scenarios or large viewpoint variations.\nOur approach, dubbed MVDiffusion, generates multi-view images simultaneously, using multiple\nbranches of a standard text-to-image model pre-trained on perspective images. Concretely, we use a\nstable diffusion (SD) model [36] and add a \u201ccorrespondence-aware attention\u201d (CAA) mechanism\nbetween the UNet blocks, which facilitates cross-view interactions and learns to enforce multi-view\nconsistency. When training the CAA blocks, we freeze all the original SD weights to preserve the\ngeneralization capability of the pre-trained model.\nIn summary, the paper presents MVDiffusion, a multi-view text-to-image generation architecture\nthat requires minimal changes to a standard pretrained text-to-image diffusion model, achieving\nstate-of-the-art performance on two multi-view image generation tasks. For generating panorama,\nMVDiffusion synthesizes high-resolution photorealistic panoramic images given arbitrary per-view\ntexts, or extrapolates one perspective image to a full 360-degree view. Impressively, despite being\ntrained solely on a realistic indoor panorama dataset, MVDiffusion possesses the capability to create\ndiverse panoramas, e.g. outdoor or cartoon style. For multi-view image generation conditioned on\ndepths/poses, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh.\n2\nRelated Work\nDiffusion models. Diffusion models [16, 21, 39, 42, 43, 44, 45] (DM) or score-based generative\nmodels are the essential theoretical framework of the exploding generative AI. Early works achieve su-\nperior sample quality and density estimation [8, 45] but require a long sampling trajectory. Advanced\nsampling techniques [20, 26, 40] accelerate the process while maintaining generation quality. Latent\ndiffusion models [36] (LDMs) apply DM in a compressed latent space to reduce the computational\ncost and memory footprint, making the synthesis of high-resolution images feasible on consumer\ndevices. We enable holistic multi-view image generation by the latent diffusion model.\nImage generation. Diffusion Models (DM) dominate content generation. Foundational work such as\nDALL-E 2 [34], GLIDE [30], LDMs [36], and Imagen [37] have showcased significant capabilities\nin text-conditioned image generation. They train on extensive datasets and leverage the power of\npre-trained language models. These large text-to-image Diffusion Models also establish strong foun-\ndations for fine-tuning towards domain-specific content generation. For instance, MultiDiffusion [1]\nand DiffCollage [53] failitates 360-degree image generation. However, the resulting images are not\ntrue panoramas since they do not incorporate camera projection models. Text2Light [6] synthesizes\nHDR panorama images from text using a multi-stage auto-regressive generative model. However, the\nleftmost and rightmost contents are not connected (i.e., loop closing).\n3D content generation. Content generation technology has profound implications in VR/AR and\nentertainment industries, driving research to extend cutting-edge generation techniques from a single\nimage to multiple images. Dreamfusion [31] and Magic3D [24] distill pre-trained Diffusion Models\ninto a NeRF model [28] to generate 3D objects guided by text prompts. However, these works focus\non objects rather than scenes. In the quest for scene generation, another approach [18] generates\n2\nStable Diffusion U-Net\nSD U-Net Block\nUp/Down Sample\nCorrespondence\nAware Attention\nMulti-branch U-Net\n+\n+\n+\n+\nFFN\n \n...\n \n...\n \n...\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\n1\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\n1\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\n1\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\n1\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\nsl\n\u21e4\ntl\n\u21e4\ntl\ns\nk\nF\nFl\nReferences\n1\nFigure 2: MVDiffusion generates multi-view images in parallel through the weight-sharing multi-\nbranch UNet. To enforce multi-view consistency, the Correspondence-Aware Attention (CAA) block\nis inserted after each UNet block. \"FFN\" is an acronym for \"Feed-Forward Network\". The rightmost\nfigure elaborates on the mechanisms of CAA.\nprompt-conditioned images of indoor spaces by iteratively querying a pre-trained text-to-image\nDiffusion Model. SceneScape [11] generates novel views on zoom-out trajectories by employing\nimage warping and inpainting techniques using diffusion models. Text2Room [18] adopts similar\nmethods to generate a complete textured 3D room geometry. However, the generation of the n-th\nimage is solely conditioned on the local context, resulting in accumulation errors and less favorable\nresults. Our research takes a holistic approach and generates consistent multi-view images given\ncamera poses and text prompts while fine-tuning pre-trained perspective-image Diffusion Models.\n3\nPreliminary\nLatent Diffusion Models (LDM) [36] is the foundation of our method. LDM consists of three\ncomponents: a variational autoencoder (VAE) [22] with encoder E and decoder D, a denoising\nnetwork \u03f5\u03b8, and a condition encoder \u03c4\u03b8.\nHigh-resolution images x \u2208 RH\u00d7W \u00d73 are mapped to a low-dimensional latent space by Z = E(x),\nwhere Z \u2208 Rh\u00d7w\u00d7c. The down-sampling factor f = H/h = W/w is set to 8 in the popular Stable\nDiffusion (SD). The latents are converted back to the image space by \u02dcx = D(Z).\nThe LDM training objective is given as\nLLDM := EE(x),y,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(Zt, t, \u03c4\u03b8(y))\u22252\n2\ni\n,\n(1)\nwhere t is uniformly sampled from 1 to T and Zt is the noisy latent at time step t. The denoising\nnetwork \u03f5\u03b8 is a time-conditional UNet [8], augmented with cross-attention mechanisms to incorporate\nthe optional condition encoding \u03c4\u03b8(y). y could be a text-prompt, an image, or any other user-specified\ncondition.\nAt sampling time, the denoising (reverse) process generates samples in the latent space, and the\ndecoder produces high-resolution images with a single forward pass. Advanced samplers [20, 26, 40]\ncan further accelerate the sampling process.\n4\nMVDiffusion: Holistic Multi-view Image Generation\nMVDiffusion generates multiple images simultaneously by running multiple copies/branches of a\nstable diffusion model with a novel inter-branch \u201ccorrespondence-aware attention\u201d (CAA) mechanism\nto facilitate multi-view consistency. Figure 2 presents an overview of multi-branch UNet and the\nCAA designs. The system is applicable when pixel-to-pixel correspondences are available between\nimages, specifically for cases of 1) Generating a panorama or extrapolating a perspective image\nto a panorama. The panorama consists of perspective images sharing the camera center where\n3\npixel-to-pixel correspondences are obtained by planar tomography and 2) Texture mapping of a given\ngeometry where multiple images of arbitrary camera poses establish pixel-to-pixel correspondences\nthrough depth-based unprojection and projection. We first introduce panorama generation (\u00a74.1),\nwhich employs generation modules, and then multi-view depth-to-image generation (\u00a74.2), which\nemploys generation and interpolation modules. Since the interpolation module does not contain CAA\nblocks, \u00a74.1 will also cover the design of the CAA block and explain how it is inserted into the\nmulti-branch UNet.\n4.1\nPanorama generation\nIn MVDiffusion, a panorama is realized by generating eight perspective views, each possessing a\nhorizontal field of view of 90\u25e6 with a 45\u25e6 overlap. To achieve this, we generate eight 512 \u00d7 512\nimages by the generation module using a frozen pretrained stable diffusion model [47].\nGeneration module. The proposed module generates eight 512 \u00d7 512 images. It accomplishes this\nthrough a process of simultaneous denoising. This process involves feeding each noisy latent into\na shared UNet architecture, dubbed as the multi-branch UNet, to predict noises concurrently. In\norder to ensure multi-view consistency, a correspondence-aware attention (CAA) block is introduced\nfollowing each UNet block. The CAA block follows the final ResNet blocks and is responsible for\ntaking in multi-view features and fusing them together.\nCorrespondence-aware attention (CAA). The CAA block operates on N feature maps concurrently,\nas shown in Figure 2. For the i-th source feature map, denoted as F, it performs cross-attention with\nthe remaining (N \u2212 1) target feature maps, represented as Fl.\nFor a token located at position (s) in the source feature map, we compute a message based on the\ncorresponding pixels {tl} in the target feature maps {Fl} (not necessarily at integer coordinates) with\nlocal neighborhoods. Concretely, for each target pixel tl, we consider a K \u00d7 K neighborhood N(tl)\nby adding integer displacements (dx/dy) to the (x/y) coordinate, where |dx| < K/2 and |dy| < K/2.\nIn practice, we use K = 3 with a neighborhood of 9 points.\nM =\nX\nl\nX\ntl\u2217\u2208N (tl)\nSoftMax\n\u0000\u0002\nWQ \u00afF(s)\n\u0003\n\u00b7\n\u0002\nWK \u00afFl(tl\n\u2217)\n\u0003\u0001\nWV \u00afFl(tl\n\u2217),\n(2)\n\u00afF(s) = F(s) + \u03b3(0),\n\u00afFl(tl\n\u2217) = Fl(tl\n\u2217) + \u03b3(sl\n\u2217 \u2212 s).\n(3)\nThe message M calculation follows the standard attention mechanism that aggregates information\nfrom the target feature pixels {tl\n\u2217} to the source (s). WQ, WK, and WV are the query, key and\nvalue matrices. The key difference is the added position encoding \u03b3(\u00b7) to the target feature Fl(tl\n\u2217)\nbased on the 2D displacement between its corresponding location sl\n\u2217 in the source image and s. The\ndisplacement provides the relative location in the local neighborhood. Note that a displacement is\na 2D vector, and we apply a standard frequency encoding [50] to the displacement in both x and y\ncoordinates, then concatenate. A target feature Fl(tl\n\u2217) is not at an integer location and is obtained\nby bilinear interpolation. To retain the inherent capabilities of the stable diffusion model [47], we\ninitialize the final linear layer of the transformer block and the final convolutional layer of the\nresidual block to be zero, as suggested in ControlNet [52]. This initialization strategy ensures that\nour modifications do not disrupt the original functionality of the stable diffusion model.\nPanorama extraporlation. The goal is to generate full 360-degree panoramic views (seven target\nimages) based on a single perspective image (one condition image) and the per-view text prompts.\nWe use SD\u2019s impainting model [48] as the base model as it takes one condition image. Similar to the\ngeneration model, CAA blocks with zero initializations are inserted into the UNet and trained on our\ndatasets.\nFor the generation process, the model reinitializes the latents of both the target and condition images\nwith noises from standard Gaussians. In the UNet branch of the condition image, we concatenate a\nmask of ones to the image (4 channels in total). This concatenated image then serves as the input to\nthe inpainting model, which ensures that the content of the condition image remains the same. On\nthe contrary, in the UNet branch for a target image, we concatenate a black image (pixel values of\nzero) with a mask of zeros to serve as the input, thus requiring the inpainting model to generate a\ncompletely new image based on the text condition and the correspondences with the condition image.\n4\nTraining. We insert CAA block into the pretrained stable diffusion Unet [47] or stable diffusion\nimpainting Unet [48] to ensure multi-view consistency. The pretrained network is frozen while we\nuse the following loss to train the CAA block:\nLMVDiffusion := E{Zi\nt=E(xi)}\nN\ni=1,{\u03f5i\u223cN (0,I)}N\ni=1,y,t\nh N\nX\ni=1\n\u2225\u03f5i \u2212 \u03f5i\n\u03b8(\n\b\nZi\nt\n\t\n, t, \u03c4\u03b8(y))\u22252\n2\ni\n.\n(4)\n4.2\nMultiview depth-to-image generation\nThe multiview depth-to-image task aims to generate multi-view images given depths/poses. Such\nimages establish pixel-to-pixel correspondences through depth-based unprojection and projection.\nMVDiffusion\u2019s process starts with the generation module producing key images, which are then\ndensified by the interpolation module for a more detailed representation.\nGeneration module. The generation module for multi-view depth-to-image generation is similar\nto the one for panorama generation. The module generates a set of 192 \u00d7 256 images. We use\ndepth-conditioned stable diffusion model [46] as the base generation module and simultaneously\ngenerate multi-view images through a multi-branch UNet. The CAA blocks are adopted to ensure\nmulti-view consistency.\nInterpolation module. The interpolation module of MVDiffusion, inspired by VideoLDM [2],\ncreates N images between a pair of \u2019key frames\u2019, which have been previously generated by the\ngeneration module. This model utilizes the same UNet structure and correspondence attention weights\nas the generation model, with extra convolutional layers, and it reinitializes the latent of both the\nin-between images and key images using Gaussian noise. A distinct feature of this module is that the\nUNet branch of key images is conditioned on images already generated. Specifically, this condition\nis incorporated into every UNet block. In the UNet branch of key images, the generated images are\nconcatenated with a mask of ones (4 channels), and then a zero convolution operation is used to\ndownsample the image to the corresponding feature map size. These downsampled conditions are\nsubsequently added to the input of the UNet blocks. For the branch of in-between images, we take\na different approach. We append a black image, with pixel values of zero, to a mask of zeros, and\napply the same zero convolution operation to downsample the image to match the corresponding\nfeature map size. These downsampled conditions are also added to the input of the UNet blocks. This\nprocedure essentially trains the module such that when the mask is one, the branch regenerates the\nconditioned images, and when the mask is zero, the branch generates the in-between images.\nTraining. we adopt a two-stage training process. In the first stage, we fine-tune the SD UNet model\nusing all ScanNet data. This stage is single-view training (Eq. 1) without the CAA blocks. In the\nsecond stage, we integrate the CAA blocks, and the image condition blocks into the UNet, and only\nthese added parameters are trained. We use the same loss as panorama generation to train the model.\n5\nExperiments\nWe evaluate MVDiffusion on two tasks: panoramic image generation and multi-view depth-to-image\ngeneration. We first describe implementation details and the evaluation metrics.\nImplementation details. We have implemented the system with PyTorch while using publicly\navailable Stable Diffusion codes from Diffusers [51]. The model consists of a denoising UNet to\nexecute the denoising process within a compressed latent space and a VAE to connect the image\nand latent spaces. The pre-trained VAE of the Stable Diffusion is maintained with official weights\nand is used to encode images during the training phase and decode the latent codes into images\nduring the inference phase. We have used a machine with 4 NVIDIA RTX A6000 GPUs for\ntraining and inference. Specific details and results of these varying configurations are provided in the\ncorresponding sections.\nEvaluation matrics. The evaluation metrics cover two aspects, image quality of generated images\nand their consistency.\n\u2022 Image quality is measured by Fr\u00e9chet Inception Distance (FID) [14], Inception Score (IS) [38], and\nCLIP Score (CS) [32]. FID measures the distribution similarity between the features of the generated\n5\n\u001f\u001e\u001d\u001c\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\n\u0016\u0019\u0015\u001a\u0014\u0019\u0013\u0014\u0019\u0012\n\u0011\u0010\u000f\u0013\u000e\r\u0014\u0012\f\u0013\n\u001b\u000b\u0018\n\u0015\u0013\t\u001d\b\u001a\u001d\r\u0014\u0007\u0014\u0019\u0012\u001d\u000b\u0018\u0018\n\u001d\u0006\u0014\u0013\f\u001d\u001a\u001d\u001d\r\u001a\u000b\u0012\u0010\u001d\u0012\r\u001a\u0005\u0005\u001d\u0013\u001a\u0004\r\u0010\u0003\u001d\u0006\f\u0014\u0013\u0010\u001d\u0002\f\u001a\u0014\u000b\u0005\u001d\u001a\u0019\u0001\u001d\u001a\u001d\u0012\u0014\u001a\u0019\u0013\u001d\u0006\u0014\u0019\u0001\u0018\u0006\u007f\u001d\u0081\u001d\u0011\u008d\u001d\u0014\u0005\u001d\u001a\u0013\u0013\u001a\u0002\f\u0010\u0001\u001d\u0013\u0018\u001d\u0013\f\u0010\u001d\u0006\u001a\r\r\u001d\n\u001a\u0019\u0001\u001d\u001a\u001d\u008f\u001a\u0019\u0002\u0090\u001d\u0002\u000b\u0090\u0005\u0013\u001a\r\u001d\u0002\f\u001a\u0019\u0001\u0010\r\u0014\u0010\u000b\u001d\u0014\u0005\u001d\f\u001a\u0019\u0012\u0014\u0019\u0012\u001d\u0018\u0019\u001d\u0013\f\u0010\u001d\u0002\u0010\u0014\r\u0014\u0019\u0012\u007f\u009d\nFigure 3: Qualitative evaluation for panorama generation. The red box indicates the area stitched\nleftmost and rightmost content. More results are available in the supplementary material.\nand real images. The Inception Score is based on the diversity and predictability of generated images.\nCLIP Score measures the text-image similarity using pretrained CLIP models [33].\n\u2022 Multi-view consistency is measured by the metric based on pixel-level similarity. The area of\nmulti-view image generation is still in an early stage, and there is no common metric for multi-view\nconsistency. We propose a new metric based on Peak Signal-to-Noise Ratio (PSNR). Concretely,\ngiven multi-view images, we compute the PSNR between all the overlapping regions and then\ncompare this \u201coverlapping PSNR\u201d for ground truth images and generated images. The final score is\ndefined as the ratio of the \u201coverlapping PSNR\u201d of generated images to that of ground truth images.\nHigher values indicate better consistency.\nThe rest of the section explains the experimental settings and results more, while the full details are\nreferred to the supplementary.\n5.1\nPanoramic image generation\nThis task generates perspective crops covering the panoramic field of view, where the challenge is to\nensure consistency in the overlapping regions. Matterport3D [4] is a comprehensive indoor scene\ndataset that consists of 90 buildings with 10,912 panoramic images. We allocate 9820 and 1092\npanoramas for training and evaluation, respectively.\nBaselines. We have selected three related state-of-the-art methods for thorough comparisons. The\ndetails of the baselines are briefly summarized as follows (full implementation details can be found\nin the appendix):\n\u2022 Text2Light[6] creates HDR panoramic images from text using a multi-stage auto-regressive genera-\ntive model. To obtain homographic images, we project the generated panoramas into perspective\nimages using the same camera settings (FoV=90\u25e6, rotation=45\u25e6).\n\u2022 Stable Diffusion (SD)[36] is a text-to-image model capable of generating high-quality perspective\nimages from text. For comparison, we fine-tuned Stable Diffusion using panoramic images and\nthen extracted perspective images in a similar manner.\n\u2022 Inpainting methods [11, 18] operate through an iterative process, warping generated images to the\ncurrent image and using an inpainting model to fill in the unknown area. Specifically, we employed\nthe inpainting model from Stable Diffusion v2 [36] for this purpose.\nResults. Table 1 and Figure 3 present the quantitative and qualitative evaluations, respectively. We\nthen discuss the comparison between MVDiffusion and each baseline:\n6\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u0014\u0013\u0012\u001a\u0011\u0010\u0015\u000f\u000f\u000e\u0018\u001e\u0011\u0012\u0011\r\f\u0018\u001a\u001d\u000b\u0015\u001e\n\u0012\u0018\u001a\t\u0013\u0018\t\u0013\u0015\b\u0013\r\u0012\u0007\u0018\u001a\t\u0013\u0018\u0012\r\u001d\u000b\u0006\u0010\u0015\u001b\u001b\u0013\n\u0018\u001c\u001d\u0005\r\u001a\u0015\u0011\r\u0018\u0012\u001a\u001d\u001d\n\u0007\u0018\u0011\u001a\u0012\u0018\u0014\u0015\f\f\u0013\n\u0018\u001b\u0013\u0015\u0004\u0012\u0018\u0010\u000f\u001d\u0015\u0004\u0013\n\u0018\u0011\r\u0018\n\u0015\u0018\u0012\t\u001e\u001d\u0005\n\u0018\u001d\u0003\u0018\u0013\u001a\t\u0013\u001e\u0013\u0015\u000f\u0018\u0010\u000f\u001d\u0005\n\u0012\u0007\u0018\u0011\u001a\u0012\u0018\u001e\u0005\f\f\u0013\n\u0018\u0012\u000f\u001d\u001b\u0013\u0012\u0018\u0015\u0018\u0012\u001a\u0015\u001e\u0004\u0018\u0010\u001d\r\u001a\u001e\u0015\u0012\u001a\u0018\u0015\f\u0015\u0011\r\u0012\u001a\u0018\u001a\t\u0013\u0018\u0012\u0013\u001e\u0013\r\u0013\u0018\u0015\u0002\u0005\u001e\u0013\u0018\u0012\u0004\u000e\u0007\u0018\u0015\r\n\u0018\u0011\u001a\u0012\u0018\u0012\u0011\u000f\u0013\r\u001a\u0018\f\u001e\u0015\r\u0006\n\n\u0013\u0005\u001e\u0018\u0013\u0001\u0005\n\u0011\r\f\u0018\u0015\r\u0018\u0015\u0011\u001e\u0018\u001d\u0003\u0018\u0015\r\u0010\u0011\u0013\r\u001a\u0018\u000b\u0011\u0012\n\u001d\u001c\u0018\u0015\r\n\u0018\u001a\u0011\u001c\u0013\u000f\u0013\u0012\u0012\u0018\u0012\u001d\u000f\u0011\u001a\u0005\n\u0013\u007f\u0081\nFigure 4: Example of panorama generation of outdoor scene. More results are available in the\nsupplementary material.\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u0018\u0015\u0014\u001e\u0013\u0012\u0018\u0011\u0010\u001a\u000f\u000e\u0012\r\u0018\f\u0010\u001a\u000e\u0018\u0014\u0018\u000f\u0012\r\u001a\u0012\u001e\u0018\u0010\u000b\u0015\u0014\r\n\u0018\u0014\r\n\u0018\f\u000e\u0010\u001a\u0012\u0018\u000f\u0014\r\t\u0010\r\u0012\u001a\u000b\b\u0018\u0016\u0018\n\u0010\r\u0010\r\u0013\u0018\u001e\u001d\u001d\u001c\u0018\f\u0010\u001a\u000e\u0018\u001a\u0014\t\u0015\u0012\u000b\u0018\u0014\r\n\u0018\u000f\u000e\u0014\u0010\u001e\u000b\b\u0018\u0016\u0018\n\u001e\u001d\u001d\u001c\u0018\f\u0010\u001a\u000e\u0018\u0014\u0018\u0015\u001d\u001a\u0018\u001d\u0007\u0018\f\u0010\r\n\u001d\f\u000b\u0018\u0014\r\n\u0018\u0014\u0018\f\u001d\u001d\n\u0012\r\u0018\u0006\u001d\u001d\u001e\b\u0005\nFigure 5: Image&text-conditioned panorama generation results. More results are available in the\nsupplementary material.\n\u2022 Compared with Text2Light[6]: Text2Light is based on auto-regressive transformers and shows\nlow FID, primarily because diffusion models perform generally better. Another drawback is the\ninconsistency between the left and the right panorama borders, as illustrated in Figure 3.\nMethod\nFID \u2193\nIS \u2191\nCS \u2191\nImpainting [11, 18]\n42.13\n7.08\n29.05\nText2light [6]\n48.71\n5.41\n25.98\nSD (Pano) [36]\n23.02\n6.58\n28.63\nSD (Perspective) [36]\n25.59\n7.29\n30.25\nMVDiffusion(Ours)\n21.44\n7.32\n30.04\nTable 1: Quantitative evaluation with Fr\u00e9chet\nInception Distance (FID), Inception Score (IS),\nand CLIP Score (CS).\n\u2022 Compared with Stable Diffusion (panorama)[36]:\nMVDiffusion obtain higher IS, CS, and FID than\nSD (pano). Like Text2light, this model also en-\ncounters an issue with inconsistency at the left\nand right borders. Our approach addresses this\nproblem by enforcing explicit consistency through\ncorrespondence-aware attention, resulting in seam-\nless panoramas.\nAnother shortcoming of this\nmodel is its requirement for substantial data to\nreach robust generalization. In contrast, our model,\nleveraging a frozen pre-trained stable diffusion, demonstrates a robust generalization ability with a\nsmall amount of training data, as shown in Figure 4.\n\u2022 Compared with inpainting method [11, 18]: Inpainting methods also exhibit worse performance\ndue to the error accumulations, as evidenced by the gradual style change throughout the generated\nimage sequence in Figure 3.\n\u2022 Compared with Stable Diffusion (perspective)[36]: We also evaluate the original stable diffusion on\nperspective images of the same Matterport3D testing set. This method cannot generate multi-view\nimages but is a good reference for performance comparison. The results suggest that our method does\nnot incur performance drops when adapting SD for multi-view image generation.\nGenerate images in the wild. Despite our model being trained solely on indoor scene data, it\ndemonstrates a remarkable capability to generalize across various domains. This broad applicability\nis maintained due to the original capabilities of stable diffusion, which are preserved by freezing\nthe stable diffusion weights. As exemplified in Figure 4, we stich the perspective images into a\n7\npanorama and show that our MVDiffusion model can successfully generate outdoor panoramas.\nFurther examples illustrating the model\u2019s ability to generate diverse scenes, including those it was not\nexplicitly trained on, can be found in the supplementary materials.\nImage&text-conditioned panorama generation. In Figure 5, we show one example of image&text-\nconditioned panorama generation. MVDiffsion demonstrates the extraordinary capability of extrapo-\nlating the whole scene based on one perspective image.\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\u0016\n\u0015\u0014\n\u0013\u0018\u001d\u0012\u0011\u001e\u001b\n\u0010\u000f\u001e\u001b\u000e\u000f\r\f\u0018\u001b\n\u001f\u001e\u001d\u001c\n\u000b\u000e\u000f\n\u001d\u001b\t\u001a\b\u0007\u001a\u0006\u0011\u001b\u0005\u0017\u0018\u001e\u001a\u0004\u0011\u001b\u0017\u001a\u0004\u0017\u0011\u001b\u0018\u001a\u0005\u0012\u0003\u0011\u001e\u0018\u001b\u0016\u001a\u0012\u001e\u0019\u001a\u0004\u0017\u0011\u001b\u0018\u001a\u0012\u001d\u001d\r\u0011\u0012\u001e\u0005\u0018\u0016\u0002\u001a\u0007\u001a\u0006\u0011\u001b\u0005\u0017\u0018\u001e\u001a\u0004\u0011\u001b\u0017\u001a\u0012\u001a\u0004\u0017\u0011\u001b\u0018\u001a\u0016\u001b\u000f\u0001\u0018\u001a\u001b\u000f\u001d\u001a\n\u000f\u0001\u0018\u001e\u001a\u001e\u0018\u007f\u001b\u001a\u001b\u000f\u001a\u0012\u001a\u0016\u0011\u001e\u0006\u0002\u001a\u0007\u001a\u0005\u001c\u001b\u001b\u0011\u001e\u0081\u001a\u0003\u000f\u0012\u000e\u0019\u001a\u000f\u001e\u001a\u001b\u0017\u0018\u001a\u0005\u000f\u001c\u001e\u001b\u0018\u000e\u0002\u008d\nFigure 6: Qualitative evaluation for depth-to-image generation. More results are available in the\nsupplementary material.\n5.2\nMulti view depth-to-image generation\nMethod\nFID \u2193\nIS \u2191\nCS \u2191\nRePaint [27]\n70.05\n7.15\n26.98\nControlNet [52]\n43.67\n7.23\n28.14\nOurs\n23.10\n7.27\n29.03\nTable 2: Comparison in Fr\u00e9chet Inception Distance\n(FID), Inception Score (IS), and CLIP Score (CS)\nfor multiview depth-to-image generation.\nThis task converts a sequence of depth images\ninto a sequence of RGB images while preserving\nthe underlying geometry and maintaining multi-\nview consistency. ScanNet is an indoor RGB-\nD video dataset comprising over 1513 training\nscenes and 100 testing scenes, all with known\ncamera parameters. We train our model on the\ntraining scenes and evaluate it on the testing\nscenes. In order to construct our training and testing sequences, we initially select key frames,\nensuring that each consecutive pair of key frames has an overlap of approximately 65%. Each\ntraining sample consists of 12 sequential keyframes. For evaluation purposes, we conduct two sets\nof experiments. For our quantitative evaluation, we have carefully chosen 590 non-overlapping\nimage sequences from the testing set, each composed of 12 individual images. In terms of qualitative\nassessment, we first employ the generation model to produce all the key frames within a given test\nsequence. Following this, the image&text-conditioned generation model is utilized to enrich or\ndensify these images. Notably, even though our model has been trained using a frame length of 12, it\nhas the capability to be generalized to accommodate any number of frames. Ultimately, we fuse the\nRGBD sequence into a cohesive scene mesh.\nBaselines. To our knowledge, no direct baselines exist for scene-level depth-to-image generation.\nSome generate 3D textures for object meshes, but often require complete object geometry to optimize\nthe generated texture from many angles [5, 29]. This is unsuitable for our setting where geometry is\nprovided for the parts visible in the input images. Therefore, we have selected two baselines:\n8\nTask \u2192\nPanorama\nDepth-to-image\nMethod\nPSNR \u2191\nRatio \u2191\nPSNR \u2191\nRatio \u2191\nG.T.\n37.7\n1.00\n21.41\n1.00\nSD (Perspective)\n10.6\n0.28\n11.20\n0.44\nMVDiffusion (Ours)\n25.4\n0.67\n17.41\n0.76\nTable 3: Multi-view consistency for panorama generation and multi-view depth-to-image generation.\n\u2022 RePaint[27]: This method uses an image diffusion model for inpainting holes in an image, where we\nemploy the depth2image model from Stable Diffusion v2[36]. For each frame, we warp the generated\nimages to the current frame and employ the RePaint technique [27] to fill in the holes. This baseline\nmodel is also fine-tuned on our training set.\n\u2022 Depth-conditioned ControlNet: We train a depth-conditioned ControlNet model combined with the\ninpainting Stable Diffusion method with the same training set as ours. The implementation is based\non a public codebase [7]. This model is capable of image inpainting conditioned on depths. We use\nthe same pipeline as the above method.\nResults. Table 2, Figure 6, Figure 7, and Figure 8 present the quantitative and qualitative results\nof our generation models. Our approach achieves a notably better FID, as shown in Table 2. As\ndepicted in Figure 6, the repaint method generates progressively blurry images, while ControlNet\nproduces nonsensical content when the motion is large. These issues arise since both methods\ndepend on partial results and local context during inpainting, causing error propagation. Our method\novercomes these challenges by enforcing global constraints with the correspondence-aware attention\nand generating images simultaneously. Figure 7 exhibits the keyframes at the left and the right, where\nthe intermediate frames are generated in the middle, which are consistent throughout the sequence.\nFigure 8 illustrates the textured scene meshes produced by our method.\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u0017\u0016\u0015\u001e\u001a\u0014\u0013\u0012\u0019\u0015\u0011\u0013\u0016\u001c\u001a\u001e\u0010\u000f\u0012\u0015\u0010\nFigure 7: Visualization of image&text-conditioned generated frames. We interpolate 4 frames (second\nimage to fifth image). More visualization results are presented in supplementary materials.\nFigure 8: Mesh visualization. MVDiffusion first generates RGB images given depths/poses and then\nfuse them into mesh with TSDF fusion.\n5.3\nMeasuring multi-view consistency\nThe multi-view consistency is evaluated with the metric as explained earlier. For panoramic image\ngeneration, we select image pairs with a rotation angle of 45 degrees and resize them to 1024 \u00d7 1024.\nFor multi-view depth-to-image generation, consecutive image pairs are used and resized to 192 \u00d7 256.\nPSNR is computed among all pixels within overlapping regions for panoramic image generation. For\n9\nmulti-view depth-to-image generation, a depth check discards pixels with depth errors above 0.5m,\nthe PSNR is then computed on the remaining overlapping pixels.\nResults. In Table 3, we first use the real images to set up the upper limit, yielding a PSNR ratio of 1.0.\nWe then evaluate our generation model without the correspondence attention (i.e., an original stable\ndiffusion model), effectively acting as the lower limit. Our method, presented in the last row, achieves\na PSNR ratio of 0.67 and 0.76 for the two tasks respectively, confirming an improved multi-view\nconsistency.\n6\nConclusion\nThis paper introduces MVDiffusion, an innovative approach that simultaneously generates consistent\nmulti-view images. Our principal novelty is the integration of a correspondence-aware attention\n(CAA) mechanism, which ensures cross-view consistency by recognizing pixel-to-pixel correspon-\ndences. This mechanism is incorporated into each UNet block of stable diffusion. By using a frozen\npretrained stable diffusion model, extensive experiments show that MVDiffusion achieves state-\nof-the-art performance in panoramic image generation and multi-view depth-to-image generation,\neffectively mitigating the issue of accumulation error of previous approaches. Furthermore, our\nhigh-level idea has the potential to be extended to other generative tasks like video prediction or\n3D object generation, opening up new avenues for the content generation of more complex and\nlarge-scale scenes.\nLimitations. The primary limitation of MVDiffusion lies in its computational time and resource\nrequirements. Despite using advanced samplers, our models need at least 50 steps to generate\nhigh-quality images, which is a common bottleneck of all DM-based generation approaches. Addi-\ntionally, the memory-intensive nature of MVDiffusion, resulting from the parallel denoising limits\nits scalability. This constraint poses challenges for its application in more complex applications that\nrequire a large number of images (e.g., long virtual tour).\nBroader impact. MVDiffusion enables the generation of detailed environments for video games,\nvirtual reality experiences, and movie scenes directly from written scripts, vastly speeding up\nproduction and reducing costs. However, like all techniques for generating high-quality content, our\nmethod might be used to produce disinformation.\nAcknowledgements. This research is partially supported by NSERC Discovery Grants with Accel-\nerator Supplements and DND/NSERC Discovery Grant Supplement, NSERC Alliance Grants, and\nJohn R. Evans Leaders Fund (JELF). We thank the Digital Research Alliance of Canada and BC DRI\nGroup for providing computational resources.\nReferences\n[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for\ncontrolled image generation. arXiv preprint arXiv:2302.08113, 2, 2023.\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563\u2013\n22575, 2023.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[4] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran\nSong, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments.\narXiv preprint arXiv:1709.06158, 2017.\n[5] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nie\u00dfner. Text2tex:\nText-driven texture synthesis via diffusion models. arXiv preprint arXiv:2303.11396, 2023.\n[6] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light: Zero-shot text-driven hdr panorama generation.\nACM Transactions on Graphics (TOG), 41(6):1\u201316, 2022.\n[7] Mikolaj\nCzerkawski.\nControlnetinpaint.\nhttps://github.com/mikonvergence/\nControlNetInpaint, 2023.\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in\nNeural Information Processing Systems, 34:8780\u20138794, 2021.\n10\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n12873\u201312883, 2021.\n[11] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene\ngeneration. arXiv preprint arXiv:2302.01133, 2023.\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):\n139\u2013144, 2020.\n[13] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information\nprocessing systems, 30, 2017.\n[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems (NeurIPS), 2020.\n[17] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23(47):1\u201333, 2022.\n[18] Lukas H\u00f6llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room: Extracting\ntextured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023.\n[19] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:\n852\u2013863, 2021.\n[20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. arXiv preprint arXiv:2206.00364, 2022.\n[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. arXiv preprint arXiv:2206.00364, 2022.\n[22] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[24] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv\npreprint arXiv:2211.10440, 2022.\n[25] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\nsolver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927,\n2022.\n[27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022.\n[28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n[29] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating\ntextured meshes from text using pretrained image-text models. In SIGGRAPH Asia 2022 Conference\nPapers, pages 1\u20138, 2022.\n[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[31] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988, 2022.\n11\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[35] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.\nAdvances in neural information processing systems, 32, 2019.\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022.\n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\n[38] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. Advances in neural information processing systems, 29, 2016.\n[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning. PMLR, 2015.\n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[42] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems (NeurIPS), 2019.\n[43] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\n[44] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances\nin neural information processing systems, 33:12438\u201312448, 2020.\n[45] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. ICLR, 2021.\n[46] StabilityAI.\nStable-diffusion-2-depth.\nhttps://huggingface.co/stabilityai/\nstable-diffusion-2-depth, 2023.\n[47] StabilityAI. Stable-diffusion-2. https://huggingface.co/stabilityai/stable-diffusion-2,\n2023.\n[48] StabilityAI.\nStable-diffusion-impaint.\nhttps://huggingface.co/runwayml/\nstable-diffusion-inpainting, 2023.\n[49] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[51] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig\nDavaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/\nhuggingface/diffusers, 2022.\n[52] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\n[53] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel\ngeneration of large content with diffusion models. arXiv preprint arXiv:2303.17076, 2023.\n12\nAppendix:\nMVDiffusion: Enabling Holistic Multi-view Image Generation with\nCorrespondence-Aware Diffusion\nThe appendix provides 1) the full architecture specification of correspondence attention; 2) the\nimplementation details of the MVDiffusion system; and 3) additional experimental results in the\nsame format as the figures in the main paper.\nA\nNetwork Architecture of correspondence-aware attention block\n\u001f\u001e\u001d\u001d\u001c\u001b\u001a\u001e\u0019\u0018\u001c\u0019\u0017\u001c\u0016\u0015\u0014\u0014\u0019\n\u0013\u001c\u001d\u001e\u0016\u001f\u001e\u0019\u0012\n\u0011\u001c\u001c\u0018\u0016\u0011\u001e\u001d\u0010\u000f\u001d\u0018\u0016\u000e\u001c\u0014\u0010\u001e\u001d\r\n+\n+\n\u001f\f\u000b\f\n\t\u0016\b\f\b\n\u001f\f\u000b\f\n\t\u0016\b\f\b\n\u0011\u0007\u001b\u0006\n\u0011\u0016\u0007\u0014\u0016\u0006\u0005\nl\nl\n\u0013\u001c\u001d\u001e\u0016\u001f\u001e\u0019\u0012\nFigure 9: The architecture of\nthe correspondence-aware at-\ntention block.\nThe architecture of correspondence-aware attention block is similar\nto vision transformers [9], with the inclusion of zero convolutions as\nsuggested in ControlNet [52] and GELU [13] activation function. C,\nH, W are channel numbers, height and width respectively.\nB\nImplementation details of MVDiffusion\nB.1\nPanorama image generation\nTraining and inference details. The generation model in our ap-\nproach is built upon Stable-diffusion-v2 [47]. We train the model\non perspective images with a resolution of 512 \u00d7 512 for 10 epochs.\nThe training is performed using the AdamW optimizer with a batch\nsize of 4 and a learning rate of 2e\u22124, utilizing four A6000 GPUs.\nDuring inference, we utilize the DDIM sampler with a step size of\n50 to perform parallel denoising of the eight generated images. Additionally, we employ blip2 [23]\nto generate texts for each perspective image, and during both training and inference, we use the\ncorresponding prompts.\nB.2\nImplementation details of baselines\nWe introduce implementation details of baseline in the following.\n\u2022 Text2Light [6] We combine the prompts of each perspective image and use the released pretrained\nmodel to generate the panorama.\n\u2022 Stable Diffusion (panorama)[36] We fine-tuned Stable Diffusion using the panorama images within\nour training dataset, which contains 9820 panorama images at resolusion 512\u00d71024. We fine-tuned\nthe UNet layer of the Stable diffusion while keeping VAE layers frozen. We use AdamW optimizer\nwith a learning rate of 1e\u22126 and batch size is 4, utilizing four A6000 GPUs.\n\u2022 Inpainting methods [11, 18] In our approach, we employ Stable-diffusion-v2 [47] to generate the\nfirst image in the sequence based on the corresponding text prompt. For each subsequent image in\nthe sequence, we utilize image warping to align the previous image with the current image. The\nwarped image is then passed through Stable-diffusion-inpaint [48] to fill in the missing regions and\ngenerate the final image.\n\u2022 Stable diffusion (perspective) In our approach, we utilize the model trained in the first stage of the\ngeneration module to generate the perspective images. During testing, each perspective image is\nassociated with its own text prompt.\nB.3\nGeometry conditioned image generation\nTraining and inference details. Our generation model is derived from the stable-diffusion-2-depth\nframework [46]. In the initial phase, we fine-tune the model on all the perspective images of ScanNet\ndataset at a resolution of 192 \u00d7 256 for 10 epochs. This training process employs the AdamW\noptimizer [25] with a learning rate of 1e\u22125 and a batch size of 256, utilizing four A6000 GPUs. In the\nsecond stage, we introduce the correspondence-aware attention block and extra resizing convolutional\nlayers for image conditioned generation. The training data consists of two categories: 1) generation\ntraining data, selected from 12 consecutive frames with overlaps of around 0.65, and 2) interpolation\ntraining data, derived from randomly chosen pairs of consecutive key frames and ten intermediate\n13\nframes. During each training epoch, we maintain a careful balance by randomly sampling data\nfrom both these categories in a 1:1 ratio. The correspondence-aware attention blocks and additional\nconvolution layers are subsequently trained for 20 epochs, with a batch size of 4 and a learning rate\nof 1e\u22124, using the same four A6000 GPUs. During the inference stage, we employ the DDIM [41]\nsampler with a step size of 50 to perform parallel denoising on eight images.\nB.4\nImplementation details of baselines\nWe introduce the implementation details of baselines in the following.\n\u2022 RePaint[27]: In our method, we utilize depth-conditioned Stable-diffusion-v2 [47] to generate the\nfirst image in the sequence. For each subsequent image, we condition it on the previous image by\napplying latent warping. This helps align the generated image with the previous one. To complete the\nremaining areas of the image, we employ the Repaint technique [27] for inpainting.\n\u2022 Depth-conditioned ControlNet: We use the same method to generate the first image as the above\nmethod. Next, we warp the generated images to the current frame and use Stable-inpainting\nmodel [48] to fill the hole. To incorporate depth information into the inpainting model, we utilize a\nmethod from a public codebase [7], which adds the feature from depth-conditioned ControlNet [52]\ninto each UNet layer of the inpainting model. For more detailed information, please refer to their\ncode repository. In order to reduce the domain gap, the Stable-inpainting model has been fine-tuned\non our training dataset. Similar to other fine-tuning procedures, we only fine-tuned UNet layers while\nkeeping VAE part fixed. The fine-tuning was conducted on a machine with four A6000 GPUs. The\nbatch size is 4 and the learning rate is 1e\u22126. We used AdamW as the optimizer. During inference, we\nutilize the DDIM sampler with a step size of 50 for generating images.\nB.5\nVisualization results\nFigures 10-17 present supplementary results for panorama generation. In these figures, we showcase\nthe output panorama images generated by both Stable diffusion (panorama) and Text2light methods.\nTo compare the consistency between the left and right borders, we apply a rotation to the border\nregions, bringing them towards the center of the images. These additional visualizations provide\nfurther insights into the quality and alignment of the generated panorama images.\nFigures 18-19 show additional results for image- & text-conditioned panorama generation. MVDiffu-\nsion extrapolates the whole scene based on one provided perspective image and text description.\nFigure 20 provides additional results for generalization to out-of-training distribution data. Our model\nis only trained on MP3D indoor data while showing a robust generalization ability (e.g. outdoor\nscene).\nFigures 21-26 show additional results with two baseline methods (depth-conditioned ControlNet [52]\nand Repaint [27]).\nFigure 27 shows additional results of interpolated frames. The keyframes are at the left and the right,\nthe middle frames are generated by applying our Interpolation module (see Sec. 4.2 in the main\npaper). The consistency is maintained throughout the whole sequence.\n14\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\f\u001e\u000b\n\u0010\u0010\t\r\b\u0019\u001c\u0017\r\u0012\r\u001a\u0012\n\u0018\u001e\r\f\u001e\u000b\r\u0012\u0011\u000b\r\u0007\u001a\u0019\u000b\u0019\u0011\u0018\r\u0018\u001a\u0012\u0007\u0007\r\u000b\u0010\u0010\n\u0007\u0006\r\u000e\u0011\r\u0010\u0005\u001e\u0011\r\u000b\u0010\u0010\n\r\u001c\u0010\r\u0012\r\u0005\u0012\u001c\u0019\u0010\r\b\u0019\u001c\u0017\r\u0012\u0011\r\n\u0010\u0004\u001e\u0012\u0011\r\u0003\u0019\u001e\b\u0006\r\u000e\r\n\u0010\u0010\t\r\b\u0019\u001c\u0017\r\u0012\r\u0004\u0017\u0012\u0019\n\r\u0012\u0011\u000b\r\u0012\r\u001a\u0012\t\u0005\u0006\nFigure 10: Addition results for panorama generation\n15\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\f\u0010\u0010\u000b\r\n\u0019\u001c\u0017\r\n\u0017\u0019\u001c\u001e\r\t\u0012\b\u0019\u0011\u001e\u001c\u0007\r\u0012\u0011\u0006\r\u0012\r\n\u0010\u0010\u0006\u001e\u0011\r\u0005\u0010\u0010\f\u0004\r\u000e\r\n\u0012\u001a\u0003\u0002\u0019\u0011\r\t\u001a\u0010\u0007\u001e\u001c\r\n\u0019\u001c\u0017\r\u001a\u0010\u001c\u0007\r\u0010\u0001\r\u0007\u0017\u001e\u001a\u007f\u001e\u0007\u0004\r\n\u000e\u0011\r\u001e\u000b\u0081\u001c\u008d\r\f\u0010\u0010\u000b\r\n\u0019\u001c\u0017\r\u0012\r\t\u001a\u0010\u0007\u001e\u001c\r\u0012\u0011\u0006\r\u0007\u0017\u001e\u001a\u007f\u001e\u0007\u0004\nFigure 11: Addition results for panorama generation\n16\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\u001a\u0019\f\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\t\u001a\u001a\u001e\b\r\u0007\u0019\u001c\u0017\r\u0006\u0005\u000b\u0011\u0019\u001c\u0005\u000b\u001e\u0004\r\u0012\r\u0018\u000b\u0012\u0011\b\r\u0003\u0019\u0012\u0011\u0010\u0004\r\u0012\r\t\u000b\u001e\r\u0003\u001a\u0012\u0002\u001e\u0004\r\u0012\r\u0003\u0012\u0019\u0011\u001c\u0019\u0011\u0018\r\u0004\r\u0012\r\u001a\u0012\u000b\u0018\u001e\r\n\u0007\u0019\u0011\b\u0010\u0007\u0001\r\u000e\r\u001a\u0019\f\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\u0007\u0019\u001c\u0017\r\u0012\r\u0002\u0010\u0005\u0002\u0017\r\u0012\u0011\b\r\u0012\r\u0002\u001e\u0019\u001a\u0019\u0011\u0018\r\u0006\u0012\u0011\u0001\nFigure 12: Addition results for panorama generation\n17\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\u0018\f\u0012\u0011\u000b\r\n\u0019\u0012\u0011\u0010\r\t\u0019\u001c\u001c\u0019\u0011\u0018\r\u0019\u0011\r\u0012\r\u001a\u0019\b\u0019\u0011\u0018\r\f\u0010\u0010\u0007\r\u0011\u001e\u001d\u001c\r\u001c\u0010\r\u0012\r\u0006\u0019\u0011\u000b\u0010\u0006\u0005\r\u000e\r\u001a\u0019\b\u0019\u0011\u0018\r\f\u0010\u0010\u0007\r\u0004\u001a\u001a\u001e\u000b\r\u0006\u0019\u001c\u0017\r\n\u0003\u0002\f\u0011\u0019\u001c\u0002\f\u001e\r\u0012\u0011\u000b\r\u0012\r\u0004\f\u001e\r\n\u001a\u0012\u0001\u001e\u0005\r\u000e\r\u007f\u001a\u0012\u0001\u0081\r\n\u0019\u0012\u0011\u0010\r\t\u0019\u001c\u001c\u0019\u0011\u0018\r\u0019\u0011\r\u0012\r\u001a\u0019\b\u0019\u0011\u0018\r\f\u0010\u0010\u0007\r\u0011\u001e\u001d\u001c\r\u001c\u0010\r\u0012\r\u0006\u0019\u0011\u000b\u0010\u0006\u0005\nFigure 13: Addition results for panorama generation\n18\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\f\u0019\u0011\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\t\u0019\u001c\u0017\r\u0012\r\b\u0017\u0012\u0011\f\u001e\u001a\u0019\u001e\u000b\r\u0012\r\u001c\u0012\u0007\u001a\u001e\r\u0012\u0011\f\r\b\u0017\u0012\u0019\u000b\u0006\u0005\r\u000e\r\u001a\u0019\u0004\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\t\u0019\u001c\u0017\r\t\u0017\u0019\u001c\u001e\r\t\u0012\u001a\u001a\u0006\r\n\u0012\u0011\f\r\t\u0010\u0010\f\r\u0003\u0010\u0010\u000b\u0006\u0005\r\u000e\r\u000b\u0010\u0010\n\r\t\u0019\u001c\u0017\r\u0012\r\n\u0019\u000b\u000b\u0010\u000b\r\u0010\u0011\r\u001c\u0017\u001e\r\t\u0012\u001a\u001a\u0005\nFigure 14: Addition results for panorama generation\n19\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\f\u001e\u000b\n\u0010\u0010\t\r\b\u0019\u001c\u0017\r\u0012\r\f\u001e\u000b\r\u0012\u0011\u000b\r\u0012\r\t\u0019\n\n\u0010\n\r\u0012\u0011\u000b\r\u0012\r\b\u0019\u0011\u000b\u0010\b\u0007\r\u000e\r\u0006\u0012\u0005\u001e\r\u0010\u0004\r\u0003\u0010\b\u001e\n\u0005\r\u0010\u0011\r\u0012\r\u0005\u0017\u001e\u001a\u0004\r\u0019\u0011\r\u0012\r\n\n\u0010\u0010\t\u0007\r\u000e\r\u0017\u0012\u001a\u001a\b\u0012\u0002\r\b\u0019\u001c\u0017\r\u001c\b\u0010\r\u0004\n\u0012\t\u001e\u000b\r\u0001\u0019\u007f\u001c\u0081\n\u001e\u0005\r\u0010\u0011\r\u001c\u0017\u001e\r\b\u0012\u001a\u001a\u0007\nFigure 15: Addition results for panorama generation\n20\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\u001a\u0019\f\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\t\u001a\u001a\u001e\b\r\u0007\u0019\u001c\u0017\r\u0006\u0005\u000b\u0011\u0019\u001c\u0005\u000b\u001e\r\u0012\u0011\b\r\u0012\r\u001a\u0012\u000b\u0018\u001e\r\n\u0019\u000b\u000b\u0010\u000b\u0004\r\u000e\r\u001c\u0012\u0003\u001a\u001e\r\u0007\u0019\u001c\u0017\r\u0012\r\u001a\u0012\n\u0002\r\u0010\u0011\r\u0019\u001c\u0004\r\u000e\r\n\u001a\u0019\f\u0019\u0011\u0018\r\u000b\u0010\u0010\n\r\t\u001a\u001a\u001e\b\r\u0007\u0019\u001c\u0017\r\u0006\u0005\u000b\u0011\u0019\u001c\u0005\u000b\u001e\r\u0012\u0011\b\r\u0012\r\u0001\u0017\u0012\u0011\b\u001e\u001a\u0019\u001e\u000b\u0004\nFigure 16: Addition results for panorama generation\n21\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u0016\u0017\u001a\u0015\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001c\n\u0016\u0015\u0014\u0013\u0012\u0011\u0010\u000f\n\u000e\r\u001a\u0012\f\u0018\u001e\r\u000b\u0019\u001c\n\u0017\u001e\u0011\r\t\u0019\u001c\u0017\r\u0012\r\n\u001e\u0011\u001c\u001e\f\r\u0019\b\u001a\u0012\u0011\u0007\r\u0012\u0011\u0007\r\t\u0017\u0019\u001c\u001e\r\n\u0012\u0011\u0006\u0019\u0011\u001e\u001c\b\u0005\r\u000e\r\u0007\u0019\u0011\u0019\u0011\u0018\r\f\u0010\u0010\u0004\r\t\u0019\u001c\u0017\r\u0012\r\u001c\u0012\u0006\u001a\u001e\r\n\u0012\u0011\u0007\r\n\u0017\u0012\u0019\f\b\u0005\r\u000e\r\u0003\u0019\u001e\t\r\u0010\u0002\r\u0012\r\u0001\u0010\u0010\u001a\r\u001c\u0017\f\u0010\u007f\u0018\u0017\r\u0012\r\u0018\u001a\u0012\b\b\r\u0007\u0010\u0010\f\u0005\r\u000e\r\f\u0010\u0010\u0004\r\t\u0019\u001c\u0017\r\u0012\r\u001a\u0010\u001c\r\u0010\u0002\r\t\u0019\u0011\u0007\u0010\t\b\u0005\nFigure 17: Addition results for panorama generation\n22\n\u001f\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001e\u0018\u0017\u0017\u0016\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u0015\u0013\u001c\u0014\u0011\u001e\u0010\u0017\u000f\u001b\u0013\u001e\u0012\u001a\u000e\u001e\u0015\u0017\u0017\u000e\u0011\u001a\u001e\r\u0017\u0017\u0018\f\u000b\u001e\u001f\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001e\u0018\u0017\u0017\u0016\u001e\n\u001d\u001d\u0011\u000e\u001e\u0015\u001c\u0014\u0013\u001e\t\u000f\u0018\u001a\u001c\b\n\u0014\u000f\u0018\u0011\u0007\u001e\u0012\u001e\u0010\u0011\u001c\u001d\u001c\u001a\u0019\u001e\t\u0012\u001a\u001e\u0012\u001a\u000e\u001e\u0012\u001e\r\u0012\u0014\u001e\f\u0010\u0018\u0011\u0011\u001a\u001e\u0006\u0005\u000b\n\u001f\u001e\u0004\u0012\u0014\u0013\u0018\u0017\u0017\u0016\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u0004\u0012\u0014\u0013\u0014\u000f\u0004\u001e\u0012\u001a\u000e\u001e\u0012\u001e\u0015\u001c\u001a\u000e\u0017\u0015\u000b\u001e\u001f\u001e\u001d\u0012\u0018\u0019\u0011\u001e\u0004\u0012\u0014\u0013\u0018\u0017\u0017\u0016\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u0015\u0017\u0017\u000e\u0011\u001a\u001e\r\u0017\u0017\u0018\u001e\u0012\u001a\u000e\u001e\n\u0014\u0015\u0017\u001e\f\u001c\u001a\u0003\f\u001e\u0012\u001a\u000e\u001e\u0012\u001e\u001d\u0012\u0018\u0019\u0011\u001e\u0016\u001c\u0018\u0018\u0017\u0018\u000b\u001e\n\u001f\u001e\u0002\u0017\u0014\u0014\u0011\u000e\u001e\u0002\u001d\u0012\u001a\u0014\u001e\u001c\u001a\u001e\t\u0018\u0017\u001a\u0014\u001e\u0017\t\u001e\u0012\u001e\u0015\u001c\u001a\u000e\u0017\u0015\u000b\u001e\u001f\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001e\u0018\u0017\u0017\u0016\u001e\n\u001d\u001d\u0011\u000e\u001e\u0015\u001c\u0014\u0013\u001e\t\u000f\u0018\u001a\u001c\u0014\u000f\u0018\u0011\u001e\u0012\u001a\u000e\u001e\u0012\u001e\u0002\u001c\u0012\u001a\u0017\u000b\u001e\u001f\u001e\n\u001b\u001c\u0011\u0015\u001e\u0017\t\u001e\u0012\u001e\u0002\u0012\u0014\u001c\u0017\u001e\u0014\u0013\u0018\u0017\u000f\u0019\u0013\u001e\u0012\u001a\u001e\u0017\u0002\u0011\u001a\u001e\u000e\u0017\u0017\u0018\u000b\n\u001f\u001e\u0013\u0012\u001d\u001d\u0015\u0012\u0001\u001e\u0013\u0011\u0012\u000e\u001c\u001a\u0019\u001e\u0014\u0017\u001e\u0012\u001e\u001d\u001c\u001b\u001c\u001a\u0019\u001e\u0018\u0017\u0017\u0016\u000b\u001e\u001f\u001e\u0013\u0012\u001d\u001d\u0015\u0012\u0001\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u0010\u0013\u0012\u001a\u000e\u0011\u001d\u001c\u0011\u0018\u001e\u0012\u001a\u000e\u001e\u0015\u0013\u001c\u0014\u0011\u001e\u0015\u0012\u001d\u001d\f\u000b\u001e\u001f\u001e\n\u001d\u0012\u0018\u0019\u0011\u001e\u0015\u0013\u001c\u0014\u0011\u001e\u0010\u0012\u0004\u001c\u001a\u0011\u0014\u001e\u0015\u001c\u0014\u0013\u001e\u0016\u0017\u0018\u0018\u0017\u0018\f\u001e\u0017\u001a\u001e\u001c\u0014\u000b\n\u001f\u001e\u0018\u0017\u0017\u0016\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u001d\u0012\u0018\u0019\u0011\u001e\u0015\u001c\u001a\u000e\u0017\u0015\u001e\u0012\u001a\u000e\u001e\u0012\u001e\u0002\u0012\u001c\u001a\u0014\u001c\u001a\u0019\u001e\u0017\u001a\u001e\u0014\u0013\u0011\u001e\u0015\u0012\u001d\u001d\u000b\u001e\u001f\u001e\u000e\u001c\u001a\u001c\u001a\u0019\u001e\u0018\u0017\u0017\u0016\u001e\u0015\u001c\u0014\u0013\u001e\u0012\u001e\u0014\u0012\u0004\u001d\u0011\u0007\u001e\n\u0010\u0013\u0012\u001c\u0018\f\u001e\u0012\u001a\u000e\u001e\u0012\u001e\u0010\u0013\u0012\u001a\u000e\u0011\u001d\u001c\u0011\u0018\u000b\u001e\u001f\u001e\u001b\u001c\u0011\u0015\u001e\u0017\t\u001e\u0012\u001e\u0003\u001c\u0014\u0010\u0013\u0011\u001a\u001e\u0015\u001c\u0014\u0013\u001e\u0015\u0013\u001c\u0014\u0011\u001e\u0010\u0012\u001a\u0004\u001c\u001a\u0011\u0014\f\u001e\u0012\u001a\u000e\u001e\u0015\u0017\u0017\u000e\u001e\r\u0017\u0017\u0018\f\u000b\nFigure 18: Addition results for dual-conditioned generation\n23\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u001d\u001c\u001b\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u0017\u0016\u0012\u001b\u0019\u0017\u0011\u001e\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0010\u0019\u000f\u0010\u0014\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u001d\u001c\u001b\u0011\u001e\u001f\u001e\u000e\u0013\r\u001c\u001e\u0019\f\u001e\n\u000b\u0019\u0017\u001c\u001a\r\u001e\r\u0016\u0015\u0015\u0016\u0012\n\u001e\u0012\u001c\t\u0015\u001e\u0015\u0019\u001e\u0013\u001e\u0017\u0016\u0012\u001b\u0019\u0017\u001e\u0019\u0012\u001e\u0015\u0019\b\u001e\u0019\f\u001e\u0013\u001e\u0015\u0013\u001d\u0007\u001c\u0011\n\u001f\u001e\u001d\u0013\u0015\u0014\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0018\u0016\u001a\u001a\u0019\u001a\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u000e\u0013\r\u001c\u001e\u0019\f\u001e\u000b\u0019\u0017\u001c\u001a\r\u0011\u001e\u001f\u001e\u001d\u0013\u0015\u0014\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0015\u0019\u0016\u0007\u001c\u0015\u001e\u0013\u0012\u001b\u001e\u0013\u001e\n\r\u0014\u0019\u0017\u001c\u001a\u001e\r\u0015\u0013\u0007\u0007\u0011\u001e\n\u001f\u001e\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\r\u0016\u0012\u0006\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u0018\u0016\u001a\u001a\u0019\u001a\u0011\u001e\u001f\u001e\u0010\u0019\u000f\b\u0007\u001c\u001e\u0019\f\u001e\u001d\u001c\u001b\r\u001e\u0016\u0012\u001e\u0013\u001e\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0010\u0007\u0019\u0010\u0006\u001e\u0019\u0012\u001e\u0015\u0014\u001c\u001e\u0017\u0013\u0007\u0007\u0011\u001e\n\u001f\u001e\u0014\u0019\u0015\u001c\u0007\u001e\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u001d\u001c\u001b\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u001b\u001c\r\u0006\u0011\n\u001f\u001e\u0015\u0014\u001c\u0013\u0015\u001c\u001a\u001e\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u001a\u0019\u0017\u001e\u0019\f\u001e\u001a\u001c\u0010\u0007\u0016\u0012\u001c\u001a\r\u0011\u001e\u001f\u001e\u0014\u0013\u0007\u0007\u0017\u0013\u0005\u001e\u0007\u001c\u0013\u001b\u0016\u0012\n\u001e\u0015\u0019\u001e\u0013\u001e\u0018\u0019\u000e\u0016\u001c\u001e\u0015\u0014\u001c\u0013\u0015\u001c\u001a\u001e\u001a\u0019\u0019\u0018\u0011\u001e\u001f\u001e\n\u0007\u0013\u001a\n\u001c\u001e\u001c\u0018\b\u0015\u0005\u001e\u001a\u0019\u0019\u0018\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\b\u001a\u0019\u0004\u001c\u0010\u0015\u0019\u001a\u001e\r\u0010\u001a\u001c\u001c\u0012\u001e\u0016\u0012\u001e\u0015\u0014\u001c\u001e\u0010\u0019\u001a\u0012\u001c\u001a\u0011\n\u001f\u001e\u0014\u0019\u000f\r\u001c\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0003\u001a\u001c\u001e\b\u0016\u0015\u001e\u0016\u0012\u001e\f\u001a\u0019\u0012\u0015\u001e\u0019\f\u001e\u0016\u0015\u0011\u001e\u001f\u001e\b\u0013\u0015\u0016\u0019\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0003\u001a\u001c\u001e\b\u0016\u0015\u001e\u0016\u0012\u001e\u0015\u0014\u001c\u001e\u0018\u0016\u001b\u001b\u0007\u001c\u001e\u0019\f\u001e\u0016\u0015\u0011\u001e\u001f\u001e\u0007\u0013\u001a\n\u001c\u001e\n\u0014\u0019\u000f\r\u001c\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\b\u0019\u0019\u0007\u001e\u0016\u0012\u001e\f\u001a\u0019\u0012\u0015\u001e\u0019\f\u001e\u0016\u0015\u0011\u001e\u001f\u001e\r\u0017\u0016\u0018\u0018\u0016\u0012\n\u001e\b\u0019\u0019\u0007\u001e\u0017\u0016\u0015\u0014\u001e\u0013\u001e\u0017\u0013\u0015\u001c\u001a\f\u0013\u0007\u0007\u001e\u0013\u0012\u001b\u001e\u0013\u001e\u001a\u0019\u0010\u0006\u001e\u0017\u0013\u0007\u0007\u0011\nFigure 19: Addition results for dual-conditioned generation\n24\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u001b\u0015\u0017\u0014\u001a\u0013\u001d\u0012\u0019\u001b\u0017\u0015\u001e\u0011\u001c\u0017\u0012\u0019\u0010\u0017\u0011\u000f\u0012\u0018\u0018\u0011\u001d\u0012\u001b\u0011\u0010\u0017\u000e\u001d\r\u000e\r\u001d\u001b\u001a\r\u0019\u001c\f\u0017\u001b\u0015\u0011\u0017\u000b\u0012\u001d\u001b\r\r\u0019\n\u001c\u001b\t\b\u0011\u0010\u0017\u001d\r\r\u0007\u0017\u001c\u000e\u0012\u001d\u0006\b\u0011\u0010\u0017\u0016\u001a\u001b\u0015\u0017\u0016\u0015\u001a\u0007\u001c\t\u0017\n\u0012\u0019\u0010\u0017\u000b\u0015\u0011\u0011\u001d\f\u0017\u0016\u001a\u001b\u0015\u0017\u0005\r\u0012\u001b\u001a\u0019\u0018\u0017\u001c\u0015\u0011\b\u0014\u0011\u001c\u0017\u000b\u001d\u0012\u0007\u0007\u0011\u0010\u0017\u0016\u001a\u001b\u0015\u0017\r\u0010\u0010\b\t\u0017\u001c\u0015\u0012\u000e\u0011\u0010\u0017\u001b\u001d\u001a\u0019\u0006\u0011\u001b\u001c\f\u0017\u0012\u0017\u000b\r\u0007\u001a\u000b\u0012\b\b\t\u0017\r\u0014\u0011\u001d\u001c\u001a\u0004\u0011\u0010\u0017\u000e\r\b\u0006\u0012\n\u0010\r\u001b\u0017\n\u0012\u001d\u0007\u000b\u0015\u0012\u001a\u001d\u0017\u000e\u0011\u001d\u000b\u0015\u0011\u0010\u0017\u0019\u0011\u0012\u001d\u0017\u0012\u0017\u0018\u001d\u0012\u0014\u001a\u001b\t\n\u0010\u0011\u0003\t\u001a\u0019\u0018\f\u0017\u001b\u001a\b\u001b\u0011\u0010\u0017\b\u0012\u0007\u000e\f\u0017\u0012\u0019\u0010\u0017\u001b\u0015\u0011\u0017\u000b\u0012\u0019\u0010\t\n\u001c\u001b\u001d\u001a\u000e\u0011\u0010\u0017\u0016\u0012\b\b\u000e\u0012\u000e\u0011\u001d\u0017\u000b\u001d\u0011\u0012\u001b\u001a\u0019\u0018\u0017\u0012\u0017\u000e\b\u0012\t\u0003\u001e\b\u0017\n\u0013\u0012\u000b\u0006\u0010\u001d\r\u000e\u0017\u001b\r\u0017\u001b\u0015\u0011\u0017\u0007\u0011\u001d\u001d\t\u0017\u000b\u0015\u0012\r\u001c\f\u0017\u0011\u000f\u001e\u0010\u001a\u0019\u0018\u0017\u0012\u0017\u001c\u0011\u0019\u001c\u0011\u0017\r\u0003\u0017\u0003\u001e\u0019\u0017\u0012\u0019\u0010\u0017\u0013\r\u001e\u0019\u0010\b\u0011\u001c\u001c\u0017\u001a\u0007\u0012\u0018\u001a\u0019\u0012\u001b\u001a\r\u0019\u0002\n\u001f\u0012\u001b\u0015\u0011\u0010\u0017\u001a\u0019\u0017\u001b\u0015\u0011\u0017\u001c\r\u0001\f\u0017\u0010\u0012\u000e\u000e\b\u0011\u0010\u0017\b\u001a\u0018\u0015\u001b\u0017\r\u0003\u0017\u001b\u0015\u0011\u0017\u001c\u0011\u001b\u001b\u001a\u0019\u0018\u0017\u001c\u001e\u0019\f\u0017\u001b\u0015\u0011\u0017\u001c\u001a\b\u0011\u0019\u001b\u0017\u001c\u001b\u001d\u0011\u0011\u001b\u0017\b\u0012\t\u0017\u001e\u0019\u0010\u001a\u001c\u001b\u001e\u001d\u0013\u0011\u0010\f\u0017\u001d\u0011\u0014\u0011\u0012\b\u001a\u0019\u0018\u0017\u001b\u0015\u0011\u0017\u0018\u001d\u0012\u0019\n\u0010\u0011\u001e\u001d\u0017\r\u0003\u0017\u001a\u001b\u001c\u0017\u000b\r\u0013\u0013\b\u0011\u001c\u001b\r\u0019\u0011\u0017\u001b\u0011\u000f\u001b\u001e\u001d\u0011\f\u0017\u001b\u0015\u0011\u0017\u001d\u001e\u001c\u001b\u0011\u0010\u0017\b\u0012\u0007\u000e\u000e\r\u001c\u001b\u001c\u0017\u0013\u0011\u0012\u001d\u001a\u0019\u0018\u0017\u0016\u001a\u001b\u0019\u0011\u001c\u001c\u0017\u001b\r\u0017\u0003\r\u001d\u0018\r\u001b\u001b\u0011\u0019\u0017\u001c\u001b\r\u001d\u001a\u0011\u001c\f\u0017\u0012\u0019\u0010\u0017\u001b\u0015\u0011\u0017\u0012\u0019\u000b\u001a\u0011\u0019\u001b\f\u0017\n\u001a\u0014\t\n\u000b\b\u0012\u0010\u0017\u0015\r\u001e\u001c\u0011\u001c\u0017\u001c\u001b\u0012\u0019\u0010\u001a\u0019\u0018\u0017\u001c\u001b\r\u001a\u000b\u0012\b\b\t\f\u0017\u001b\u0015\u0011\u001a\u001d\u0017\u001c\u0015\u001e\u001b\u001b\u0011\u001d\u0011\u0010\u0017\u0016\u001a\u0019\u0010\r\u0016\u001c\u0017\u0012\u0019\u0010\u0017\u0016\u0011\u0012\u001b\u0015\u0011\u001d\n\u0013\u0011\u0012\u001b\u0011\u0019\u0017\u0010\r\r\u001d\u001c\u0017\u001c\u000e\u0011\u0012\u0006\u001a\u0019\u0018\u0017\u0014\r\b\u001e\u0007\u0011\u001c\u0017\n\u0012\u0013\r\u001e\u001b\u0017\u001b\u0015\u0011\u001a\u001d\u0017\u000e\u0012\u001c\u001c\u0012\u0018\u0011\u0017\u001b\u0015\u001d\r\u001e\u0018\u0015\u0017\u001b\u001a\u0007\u0011\u0002\n\u007f\u0016\u0012\u001c\u0015\u0017\u0016\u001a\u001b\u0015\u0017\u001b\u0015\u0011\u0017\u001c\r\r\u001b\u0015\u001a\u0019\u0018\u0017\u0015\u001e\u0011\u001c\u0017\r\u0003\u0017\u0012\u0019\u0017\u0012\u001d\u001d\u0012\t\u0017\r\u0003\u0017\u0013\b\r\u001c\u001c\r\u0007\u001c\f\u0017\u001b\u0015\u0011\u0017\u001b\u001d\u0012\u0019\u0081\u001e\u001a\b\u0017\u0018\u0012\u001d\u0010\u0011\u0019\u0017\u0016\u0012\u001c\u0017\u0012\u0017\u001c\t\u0007\u000e\u0015\r\u0019\t\u0017\r\u0003\u0017\b\u001a\u0003\u0011\u0017\u0012\u0019\u0010\u0017\n\u000b\r\b\r\u001d\f\u0017\u0016\u0015\u0011\u001d\u0011\u0017\u001b\u0015\u0011\u0017\u001c\r\u0001\u0017\u0007\u001e\u001d\u0007\u001e\u001d\u0017\r\u0003\u0017\u001b\u0015\u0011\u0017\u0013\u0012\u0013\u0013\b\u001a\u0019\u0018\u0017\u0013\u001d\r\r\u0006\u0017\u001a\u0019\u001b\u0011\u001d\u001b\u0016\u001a\u0019\u0011\u0010\u0017\u0016\u001a\u001b\u0015\u0017\u001b\u0015\u0011\u0017\u0016\u0015\u001a\u001c\u000e\u0011\u001d\u001a\u0019\u0018\u0017\u0016\u001a\b\b\r\u0016\u001c\f\u0017\u0012\u0019\u0010\u0017\u001b\u0015\u0011\u0017\n\u001a\u001d\u001a\u0010\u0011\u001c\u000b\u0011\u0019\u001b\u0017\u000e\u0011\u001b\u0012\b\u001c\u0017\u0010\u0012\u0019\u000b\u0011\u0010\u0017\u001a\u0019\u0017\u001b\u0015\u0011\u0017\u0018\u0011\u0019\u001b\b\u0011\u0017\u0013\u001d\u0011\u0011\u0004\u0011\f\u0017\u000b\u001d\u0011\u0012\u001b\u001a\u0019\u0018\u0017\u0012\u0019\u0017\u0011\u0019\u000b\u0015\u0012\u0019\u001b\u001a\u0019\u0018\u0017\u001c\u0012\u0019\u000b\u001b\u001e\u0012\u001d\t\u0017\r\u0003\u0017\u0013\u0011\u0012\u001e\u001b\t\u0017\u0012\u0019\u0010\u0017\u001c\u0011\u001d\u0011\u0019\u001a\u001b\t\u0002\n\u008d\u0015\u001a\u001c\u0017\u0012\u0019\u000b\u001a\u0011\u0019\u001b\u0017\u000b\u0012\u001c\u001b\b\u0011\u0017\u001c\u001b\u0012\u0019\u0010\u001c\u0017\u0007\u0012\u008f\u0011\u001c\u001b\u001a\u000b\u0012\b\b\t\u0017\r\u0019\u0017\u0012\u0017\u001d\u001e\u0018\u0018\u0011\u0010\u0017\u0015\u001a\b\b\f\u0017\u0016\u001a\u001b\u0015\u0017\u001b\r\u0016\u0011\u001d\u001a\u0019\u0018\u0017\u001b\u001e\u001d\u001d\u0011\u001b\u001c\f\u0017\u001b\u0015\u001a\u000b\u0006\u0017\u001c\u001b\r\u0019\u0011\u0017\u0016\u0012\b\b\u001c\f\u0017\u0012\u0019\u0010\u0017\u0012\u0017\n\u0003\r\u001d\u0007\u001a\u0010\u0012\u0013\b\u0011\u0017\u0010\u001d\u0012\u0016\u0013\u001d\u001a\u0010\u0018\u0011\u0017\r\u0014\u0011\u001d\u0017\u0012\u0017\u001b\u001a\u0007\u0011\n\u0016\r\u001d\u0019\u0017\u0007\r\u0012\u001b\u0002\u0017\u0090\u0019\u001c\u001a\u0010\u0011\f\u0017\u0010\u001a\u0007\b\t\u0017\b\u001a\u001b\u0017\u0015\u0012\b\b\u0016\u0012\t\u001c\u0017\b\u0011\u0012\u0010\u0017\u001b\r\u0017\u0018\u001d\u0012\u0019\u0010\u0017\u000b\u0015\u0012\u0007\u0013\u0011\u001d\u001c\u0017\u0012\u0010\r\u001d\u0019\u0011\u0010\u0017\n\u0016\u001a\u001b\u0015\u0017\r\u001d\u0019\u0012\u001b\u0011\u0017\u000b\u0012\u001d\u0014\u001a\u0019\u0018\u001c\f\u0017\u0012\b\b\u0017\u001d\u0011\u001c\r\u0019\u0012\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u001b\u0015\u0017\u001b\u0015\u0011\u0017\u0011\u000b\u0015\r\u0011\u001c\u0017\r\u0003\u0017\u0012\u0017\u0013\t\u0018\r\u0019\u0011\u0017\u0011\u001d\u0012\u0002\nFigure 20: Additional results for generalization to out-of-training distribution data.\n25\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\t\u0017\u0012\u001b\u0018\u001a\u000f\u0018\b\u000f\u0012\u0007\u0018\u000f\u0013\u001b\u0010\u000f\u001a\b\u000f\u0018\u0018\u0006\u0018\u000f\u001a\u000b\u0012\u001b\u001b\u0012\u001e\u0007\u001a\u001e\u0018\u0005\u001b\u001a\u001b\u0010\u001a\u0013\u001a\t\u0012\u001e\u0019\u0010\t\u0004\u001a\n\u001a\u001b\u000f\u0013\u000b\u0017\u001a\u0003\u0013\u001e\u001a\u001e\u0018\u0005\u001b\u001a\n\u001b\u0010\u001a\u0013\u001a\u000f\u0018\b\u000f\u0012\u0007\u0018\u000f\u0013\u001b\u0010\u000f\u001a\u0012\u001e\u001a\u0013\u001a\u0002\u0012\u001b\u0003\u0017\u0018\u001e\u0004\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u000e\u0012\u0001\u0012\u001e\u0007\u001a\u000f\u0010\u0010\u007f\u001a\u0081\u000e\u000e\u0018\u0019\u001a\t\u0012\u001b\u0017\u001a\b\u001c\u000f\u001e\u0012\u001b\u001c\u000f\u0018\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u001d\u0012\u0013\u001e\u0010\u0004\u001a\n\u001a\u000e\u0012\u0001\u0012\u001e\u0007\u001a\u000f\u0010\u0010\u007f\u001a\t\u0012\u001b\u0017\u001a\u0013\u001a\n\u0003\u0010\u001c\u0003\u0017\u008d\u001a\u0003\u0017\u0013\u0012\u000f\u001a\u0013\u001e\u0019\u001a\u001d\u0012\u0003\u001b\u001c\u000f\u0018\u000b\u001a\u0010\u001e\u001a\u001b\u0017\u0018\u001a\t\u0013\u000e\u000e\u0004\nFigure 21: Addition results for depth-to-image generation.\n26\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u0019\u0018\u000b\t\u001a\b\u0012\u001b\u0017\u001a\u0013\u001a\u0007\u0010\u0006\u001d\u001c\u001b\u0018\u000f\u0005\u001a\t\u0018\u0004\u0003\u0010\u0013\u000f\u0019\u0005\u001a\u0006\u0010\u001c\u000b\u0018\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u001b\u0018\u0019\u0019\u0004\u001a\u0003\u0018\u0013\u000f\u0002\u001a\n\u001a\u0001\u000f\u0018\u0018\u001e\u001a\n\u000b\u001b\u0010\u0010\u000e\u001a\u0012\u000b\u001a\u001e\u0018\u007f\u001b\u001a\u001b\u0010\u001a\u001b\u0017\u0018\u001a\u0019\u0018\u000b\t\u0002\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u0019\u0018\u000b\t\u001a\b\u0012\u001b\u0017\u001a\u0013\u001a\u0007\u0010\u0006\u001d\u001c\u001b\u0018\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u0007\u0010\u0006\u001d\u001c\u001b\u0018\u000f\u001a\u0006\u0010\u001e\u0012\u001b\u0010\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\t\u0018\u0004\u0003\u0010\u0013\u000f\u0019\u001a\u0010\u001e\u001a\u0012\u001b\u0002\u001a\n\n\u001a\u0007\u0010\u0006\u001d\u001c\u001b\u0018\u000f\u001a\u0019\u0018\u000b\t\u001a\b\u0012\u001b\u0017\u001a\u0013\u001a\u0006\u0013\u0001\u0013\u0081\u0012\u001e\u0018\u001a\u0010\u001e\u001a\u001b\u0010\u001d\u001a\u0010\u008d\u001a\u0012\u001b\u0002\nFigure 22: Addition results for depth-to-image generation.\n27\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u001b\u000f\u0013\u000b\u0017\u001a\t\u0013\u001e\u001a\u000b\u0012\u001b\u001b\u0012\u001e\b\u001a\u001e\u0018\u0007\u001b\u001a\u001b\u0010\u001a\u0013\u001a\u000f\u0013\u0019\u0012\u0013\u001b\u0010\u000f\u001a\u0012\u001e\u001a\u0013\u001a\u0006\u0013\u001b\u0017\u000f\u0010\u0010\u0005\u0004\u001a\n\u001a\u0006\u0013\u001b\u0017\u000f\u0010\u0010\u0005\u001a\u0003\u0012\u001b\u0017\u001a\u0013\u001a\n\u001b\u0010\u0012\u000e\u0018\u001b\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u000f\u0010\u000e\u000e\u001a\u0010\u0002\u001a\u001b\u0010\u0012\u000e\u0018\u001b\u001a\u001d\u0013\u001d\u0018\u000f\u0004\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u0006\u0018\u0019\u000f\u0010\u0010\u0005\u001a\u0003\u0012\u001b\u0017\u001a\u0013\u001a\u0006\u001c\u001e\u0001\u001a\u0006\u0018\u0019\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u0019\u0018\u000b\u0001\u0004\u001a\n\u001a\u0006\u001c\u001e\u0001\u001a\u0006\u0018\u0019\u001a\u0003\u0012\u001b\u0017\u001a\u0013\u001a\u0019\u0018\u000b\u0001\u001a\u001c\u001e\u0019\u0018\u000f\u007f\n\u001e\u0018\u0013\u001b\u0017\u001a\u0012\u001b\u0004\u001a\nFigure 23: Addition results for depth-to-image generation.\n28\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\t\u0013\u001b\u001a\u000b\b\u000f\u0018\u0018\u001e\u001a\u0015\u0007\u001a\u000b\u0012\u001b\u001b\u0012\u001e\u0006\u001a\u0010\u001e\u001a\u001b\u0010\u001d\u001a\u0010\u0005\u001a\u0013\u001a\u001b\u0004\u001a\u000b\u001b\u0013\u001e\u0019\u0003\u001a\n\u001a\u000f\u0010\u0010\u0002\u001a\u0001\u0012\u001b\u0017\u001a\u0013\u001a\u000b\u001b\u0013\u001e\u0019\u0012\u001e\u0006\u001a\n\u000b\u001d\u0018\u0013\u007f\u0018\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u0015\u0007\u0003\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u000e\u0013\u001d\u001b\u0010\u001d\u001a\b\u0010\u0002\u001d\u001c\u001b\u0018\u000f\u001a\u000b\u0012\u001b\u001b\u0012\u001e\u0006\u001a\u0010\u001e\u001a\u001b\u0010\u001d\u001a\u0010\u0005\u001a\u0013\u001a\u0019\u0018\u000b\u007f\u001a\u001e\u0018\u0081\u001b\u001a\u001b\u0010\u001a\u0013\u001a\u008d\u0018\u0019\u0003\u001a\u0013\u001a\u001d\u0013\u0012\u000f\u001a\u0010\u0005\u001a\u000b\u0017\u0010\u0018\u000b\u001a\n\u000b\u0012\u001b\u001b\u0012\u001e\u0006\u001a\u0010\u001e\u001a\u001b\u0017\u0018\u001a\t\u0010\u0010\u000f\u001a\u001e\u0018\u0081\u001b\u001a\u001b\u0010\u001a\u0013\u001a\u008d\u0018\u0019\u0003\nFigure 24: Addition results for depth-to-image generation.\n29\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\t\u0012\u001b\b\u0017\u0018\u001e\u001a\u0007\u0012\u001b\u0017\u001a\u0013\u001a\u000b\u0012\u001e\t\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u000f\u0018\u0006\u000f\u0012\u0005\u0018\u000f\u0013\u001b\u0010\u000f\u0004\u001a\n\u001a\t\u0012\u001b\b\u0017\u0018\u001e\u001a\u0007\u0012\u001b\u0017\u001a\u0007\u0010\u0010\u0019\u0018\u001e\u001a\b\u0013\u0003\u0012\u0002\n\u001e\u0018\u001b\u000b\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u000b\u001b\u0013\u0012\u001e\u000e\u0018\u000b\u000b\u001a\u000b\u001b\u0018\u0018\u000e\u001a\u000b\u0012\u001e\t\u0004\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u0003\u000f\u0010\u0007\u001e\u001a\b\u0010\u001c\b\u0017\u001a\u000b\u0012\u001b\u001b\u0012\u001e\u0005\u001a\u0012\u001e\u001a\u0013\u001a\u000e\u0012\u0001\u0012\u001e\u0005\u001a\u000f\u0010\u0010\u007f\u001a\u001e\u0018\u0081\u001b\u001a\u001b\u0010\u001a\u0013\u001a\u0007\u0012\u001e\u0019\u0010\u0007\u0004\u001a\n\u001a\u0003\u0010\u0010\t\u000b\u0017\u0018\u000e\u0006\u001a\n\u008d\u000e\u000e\u0018\u0019\u001a\u0007\u0012\u001b\u0017\u001a\u000e\u0010\u001b\u000b\u001a\u0010\u0006\u001a\u0003\u0010\u0010\t\u000b\u001a\u001e\u0018\u0081\u001b\u001a\u001b\u0010\u001a\u0013\u001a\u0007\u0012\u001e\u0019\u0010\u0007\u0004\nFigure 25: Addition results for depth-to-image generation.\n30\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\t\u0018\u0019\u000f\u0010\u0010\b\u001a\u0007\u0012\u001b\u0017\u001a\u0013\u001a\t\u0018\u0019\u0006\u001a\u0019\u0018\u000b\u0005\u001a\u0013\u001e\u0019\u001a\u0004\u0017\u0013\u0012\u000f\u0003\u001a\n\u001a\u0019\u0018\u000b\u0005\u001a\u0007\u0012\u001b\u0017\u001a\u0013\u001a\u0004\u0010\b\u001d\u001c\u001b\u0018\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\n\u0004\u0017\u0013\u0012\u000f\u001a\u0012\u001e\u001a\u0013\u001a\u000f\u0010\u0010\b\u0003\u001a\n\u001a\u0019\u0018\u000b\u0005\u001a\u0007\u0012\u001b\u0017\u001a\u0013\u001a\u0004\u0017\u0013\u0012\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u000e\u0013\b\u001d\u001a\u0010\u001e\u001a\u0012\u001b\u0003\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001d\u001b\u0017\n\u0016\u0015\n\u0014\u0018\u001d\u0013\u0012\u001e\u001b\n\u0011\u0010\u001e\u001b\u000f\u0010\u000e\r\u0018\u001b\n\f\u001c\u000f\u000b\n\n\u001a\u0019\u0018\u000b\u0005\u001a\u0007\u0012\u001b\u0017\u001a\u0013\u001a\u0004\u0017\u0013\u0012\u000f\u001a\u0013\u001e\u0019\u001a\u0013\u001a\u0004\u0010\b\u001d\u001c\u001b\u0018\u000f\u001a\b\u0010\u001e\u0012\u001b\u0010\u000f\u001a\u0012\u001e\u001a\u0013\u001a\u000f\u0010\u0010\b\u0003\u001a\u0015\u0007\u0010\u001a\u001b\u000f\u0013\u000b\u0017\u001a\u0004\u0013\u001e\u000b\u001a\n\u0013\u000f\u0018\u001a\u0010\u001e\u001a\u001b\u0017\u0018\u001a\u0002\u0010\u0010\u000f\u0003\nFigure 26: Addition results for depth-to-image generation.\n31\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u0017\u0016\u0015\u001e\u001a\u0014\u0013\u0012\u0019\u0015\u0011\u0013\u0016\u001c\u001a\u001e\u0010\u000f\u0012\u0015\u0010\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\n\u0017\u0016\u0015\u001e\u001a\u0014\u0013\u0012\u0019\u0015\u0011\u0013\u0016\u001c\u001a\u001e\u0010\u000f\u0012\u0015\u0010\nFigure 27: Addition results for interpolated frames.\n32\n"
  },
  {
    "title": "Improving Language Plasticity via Pretraining with Active Forgetting",
    "link": "https://arxiv.org/pdf/2307.01163.pdf",
    "upvote": "6",
    "text": "Improving Language Plasticity via\nPretraining with Active Forgetting\nYihong Chen\u00e0\u00e1\nKelly Marchisio\u00e4\nRoberta Raileanu\u00e1\nDavid Ifeoluwa Adelani\u00e0\nPontus Stenetorp\u00e0\nSebastian Riedel\u00e0\nMikel Artetxe\u00ea\n\u00e0UCL Centre for Artificial Intelligence\n\u00e1Meta AI\n\u00eaReka AI\n\u00e4Cohere AI\n{yihong.chen, d.adelani, p.stenetorp, s.riedel}@cs.ucl.ac.uk\nmikel@reka.ai\nkelly@cohere.com\nraileanu@meta.com\nAbstract\nPretrained language models (PLMs) are today the primary model for natural\nlanguage processing. Despite their impressive downstream performance, it can be\ndifficult to apply PLMs to new languages, a barrier to making their capabilities\nuniversally accessible. While prior work has shown it possible to address this\nissue by learning a new embedding layer for the new language, doing so is both\ndata and compute inefficient. We propose to use an active forgetting mechanism\nduring pretraining, as a simple way of creating PLMs that can quickly adapt to\nnew languages. Concretely, by resetting the embedding layer every K updates\nduring pretraining, we encourage the PLM to improve its ability of learning new\nembeddings within limited number of updates, similar to a meta-learning effect.\nExperiments with RoBERTa show that models pretrained with our forgetting\nmechanism not only demonstrate faster convergence during language adaptation,\nbut also outperform standard ones in a low-data regime, particularly for languages\nthat are distant from English. Code will be available at https://github.com/\nfacebookresearch/language-model-plasticity.\n1\nIntroduction\nPretrained language models (PLMs) have been swiftly reshaping the landscape of natural language\nprocessing (NLP) by improving upon standardized benchmarks across the board [Radford and\nNarasimhan, 2018, Devlin et al., 2019, Liu et al., 2019, Brown et al., 2020]. At their core, they\nacquire knowledge by ingesting large datasets and store this knowledge in their parameters during\npretraining. Using finetuning or prompting [Brown et al., 2020], such knowledge can then be applied\nto downstream applications, such as semantic analysis, question answering, and others.\nDespite their success, PLMs still have a number of shortcomings [Weidinger et al., 2021, 2022]. In\nparticular, it requires massive data and computation to pretrain them [Gururangan et al., 2020, Kaplan\net al., 2020, Hernandez et al., 2021, Hu et al., 2021, Touvron et al., 2023]. Naively retraining a new\nPLM to accommodate every lingual space shift 1 would be prohibitively expensive. This makes it a\nhighly relevant research target to create PLMs that can be efficiently adapted to new lingual spaces.\nWhile forgetting in the context of both human and machine learning is often perceived as some-\nthing negative (for instance catastrophic forgetting [McCloskey and Cohen, 1989, Ratcliff, 1990,\nKirkpatrick et al., 2017]), recent works have shown that for artificial neural networks forgetting\n1We use the term lingual space shift to describe changes in language usage between pretraining and the target\ndownstream application, caused by factors such as language change, time evolution, or domain switch. A model\nwith high language plasticity would quickly adapt to these shifts.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.01163v3  [cs.CL]  12 Jan 2024\ncan also play a positive role in increasing their \u201cplasticity\u201d, such as improving generalization to\nunseen data [Zhou et al., 2022, Chen et al., 2022, Igl et al., 2021], enabling learning in low-data\nregimes [Alabdulmohsin et al., 2021, Taha et al., 2021], or counteracting primacy bias [Nikishin\net al., 2022, D\u2019Oro et al., 2023]. Given these developments, in this work we ask whether we can draw\nupon forgetting as a mechanism to improve pretraining and imbue PLMs with similar benefits.\nFigure 1: Rewiring via relearning token embeddings: where\nthe transformer body (the purple part) is \u201cfrozen\u201d and reused\nfor a new language, but the token embeddings are relearned to\nsuit the new language.\nIt is well established in the NLP\ncommunity that models struggle to\ngeneralize across languages without\nsubstantial intervention [Conneau\net al., 2020, Pfeiffer et al., 2020,\n2022, Ansell et al., 2022], which\nis especially true for low-resources\nlanguages. We thus see this as a\npromising testing ground for forget-\nting techniques.\nOur focus is on\nthe input layer of the PLM, the to-\nken embedding layer, as learning\nit has been shown to be highly ef-\nfective when adapting between lan-\nguages [Artetxe et al., 2020].\nConcretely, we introduce an active\nforgetting mechanism, that resets to-\nken embeddings at regular intervals,\nwhile leaving all other parameters\nuntouched throughout pretraining. We study whether this forgetting approach creates a PLM that can\neasily rewire (Figure 1) to an unseen (possibly distant) language. Intuitively, resetting embeddings\nforces the transformer body to re-derive reasoning each time instead of relying on memorized short-\ncuts. Through repetition, the body learns more abstract, high-level reasoning. A model with greater\nabstraction can easily transfer across languages, since high-level reasoning is more language-agnostic.\nOur zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where\nunlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data\nregime), forgetting PLMs outperforms the baseline by large margins: average gains of +21.2% on\nXNLI, +33.8% on MLQA, and +60.9% on XQuAD. In addition, models pretrained using active\nforgetting converge faster during language adaptation. Finally, we find that forgetting is especially\nbeneficial for languages that are distant from English, such as Arabic, Hindi, Thai, and Turkish.\n2\nRewire PLMs for New Languages\nUsing unlabeled data, Artetxe et al. [2020] demonstrates the possibility of rewiring a monolingual\nPLM for a new language; they propose to relearn the embedding layer for the new language while\nkeeping all the other parameters frozen. The underlying assumption is that the token embedding layer\nand the transformer body (the non-token-embedding parameters) divide up the responsibility in a\nway that the former handles language-specific lexical meanings, while the latter deals with high-level\ngeneral reasoning. Hence, rewiring an English PLM for a new language boils down to separately\nadapting the former with unlabeled data in the new language and the latter with English task data.\nThe procedure can be summarized as follows:\n1. Pretrain: A transformer-based model is pretrained on an English corpus. In our experiments,\nwe choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model,\non English CC100 [Conneau et al., 2020].\n2. Language Adapt: The token embedding layer is finetuned using unlabelled data in the new\nlanguage, while the transformer body is frozen.\n3. Task Adapt: The transformer body is finetuned using downstream task data in English, while\nthe token embedding layer is frozen.\n4. Assemble: The final model is assembled by taking the adapted token embedding layer from\nstage 2 and the adapted transformer body from stage 3.\n2\nFigure 2: Unsupervised zero-shot cross-lingual transfer. Left: in the pretrain stage, we compare\nstandard pretraining with forgetting pretraining, where the token embeddings are actively forgotten at\na regular interval while the transformer body is learned as the standard pretraining. Middle: the task\nadapt stage and the language adapt stage separately adapt the transformer body using English task\ndata and the token embeddings using unlabeled data in the new language. Right: the assemble stage\nreassemble the adapted body and token embedding layer into a usable PLM.\n2.1\nOn The Difficulty of Rewiring PLMs via Relearning the Token Embeddings\nWhile the above procedure [Artetxe et al., 2020] offers a general framework for rewiring a mono-\nlingual PLM with unlabelled data in the new language, it is unclear how efficient such rewiring\ncan be, including both sample efficiency and computational efficiency. To better understand the\ndifficulty of rewiring PLMs via relearning the token embeddings, we design an experiment where\nwe relearn the token embedding layer using varying amounts of adaptation data. For illustration\npurpose, we pick English as the pseudo \u201cadaptation language\u201d because the English dataset is large\nenough to bootstrap a series of sub-datasets with varying quantity. We create sub-datasets with\n[1K, 10K, 100K, 1M, 5M, 10M, 100M, 1B, 10B] tokens and relearn the English embeddings while\nkeeping the transformer body frozen.\n104\n106\n108\n1010\n#Tokens in the Adaptation Corpus\n40\n50\n60\n70\n80\nEnglish XNLI Accuracy\nInfluence of Adaptation Data Quantity\nforget\nstandard\nFigure 3: The rewiring performance for standard PLMs (blue\ndashed line) drops drastically if the adaptation tokens \u2264 10M.\nThe dashed blue line in Figure 3\nsummarizes the influence of the\nadaptation data quantity on the qual-\nity of the rewired PLMs (relearned\nembeddings assembled with the En-\nglish NLI task body). We can see\nthat the standard PLMs are easy to\nrewire if there is enough adapta-\ntion data. However if the adapta-\ntion corpus contains fewer than 10\nmillion tokens, the performance of\nthe rewired standard PLMs (the blue\ndashed line in the figure) drops dras-\ntically as the adaptation data quan-\ntity goes down, from near 80 to\naround 35, a random-guessing level\nfor NLI tasks. This motivates us\nto create more rewirable PLMs, i.e.\nPLMs with more plasticity so that\nthe rewiring process can be faster\nand consume less data.\n3\n3\nPretrain Easy-to-Rewire PLMs via Active Forgetting\nRecent works have shown that incorporating forgetting through iterative weights resetting can increase\nthe \u201cplasticity\u201d of neural networks, enabling them to learn from small data and generalize better to\nunseen data in supervised learning [Alabdulmohsin et al., 2021, Taha et al., 2021, Zhou et al., 2022].\nBuilding on these efforts, we study if we can bring such forgetting into the pretrain stage so that the\nresulting PLM would have more plasticity, allowing easier rewiring to new languages.\nOur Hypothesis.\nIn effect, when Artetxe et al. [2020] relearned the token embedding layer, the\nreinitialization of the embeddings can be seen as forgetting applied once at the start of the language\nadapt stage. However, the PLM (specifically the transformer body) has never encountered forgetting\nbefore this stage and may struggle to handle this new situation. Without early exposure to forgetting,\nthe PLM might suffer from slow recovery caused by forgetting before eventually benefiting from it.\nThe learning of a new lexical embedding layer in a PLM henceforth consumes lots of data in new\nlanguages along with long training horizons as shown in Section 2.1. In this paper, to ensure swift\nlearning of the new languages with both high sample efficiency and convergence rate, we argue that\nthe PLM must be exposed to forgetting during pretraining, allowing itself to maximize the positive\nimpact of forgetting and minimizing the cost of recovery.\nOur Method.\nWith this hypothesis in mind, we propose to add an active forgetting mechanism\nto the pretraining procedure, which resets the token embedding layer periodically as described in\nAlgorithm 1. Concretely, the forgetting mechanism operates by intentionally clearing the weights of\nthe embedding layer, which stores the static representations for all tokens, and reinitializing them to a\nnew set of random values every K gradient updates. Since pretraining involves advanced training\nstrategies, like optimizers with states and learning rate schedulers, we also reset them together with\nthe token embedding layer. We refer to language models pretrained with such active forgetting\nmechanism as forgetting PLMs, in contrast to standard PLMs which are pretrained in a standard\nway. The pretraining loss curve of a forgetting PLM is episodic (Figure 4), like in reinforcement\nlearning or meta-learning. This episodic learning demonstrates that the active forgetting mechanism\ncan introduce diversity without requiring actual new data. Each forgetting event kind of \u201cbranches\nout\u201d a novel environment for the model to explore, as if initiating a new episode of learning.\nResearch Questions.\nTo further examine the proposed forgetting mechanism, we compare forget-\nting PLMs and standard PLMs on sample efficiency and convergence speed during language adapt,\ntwo key aspects of model plasticity. Our research investigates:\n\u2022 RQ1: Real-world low-resource languages often have scarce data for adapting models. Does\npretraining with active forgetting impart enough plasticity to forgetting PLMs, enabling\nthem to learn new languages even with such limited data?\n\u2022 RQ2: Deploying PLMs frequently encounters computational limitations. Endowed with\nmore plasticity, can forgetting PLMs reduce adaptation time for such low-compute scenarios?\n\u2022 RQ3: New languages may be very similar or different from pretraining languages. Does this\nsimilarity/difference impact the relative benefit of forgetting PLMs over standard PLMs?\n4\nEvaluate Forgetting PLMs for Unsupervised Cross-Lingual Transfer\nTo evaluate the effectiveness of forgetting PLMs and address RQ1-RQ3, we conduct experiments on\nseveral cross-lingual transfer benchmarks.\n4.1\nExperimental Setup\nIn our work, we closely follow the setup in Artetxe et al. [2020] and Marchisio et al. [2022]. Our\npretraining model is RoBERTa-base, a standard 12-layer transformer-based language model. We\ntrained language-specific sentencepiece tokenizers [Kudo and Richardson, 2018] with a vocabulary\nsize of 50K over the corresponding data subsets in CC100. The model was pretrained with the English\nsubset of the CC-100 dataset. The pretraining process consists of 125K updates, with a batch size of\n2048. We used a learning rate scheduler with linear decay and an initial learning rate of 7e \u2212 4, with\n4\nFigure 4: Pretraining loss curves of forgetting and standard language models. The forgetting\nmechanism brings an episodic pattern into the loss curve: every embedding forgetting produces a\nloss spike, from which the model learn to recover. Through such repeats of forget-relearn, the model\ngets used to learn new embeddings from scratch.\nAlgorithm 1 Active forgetting mechanism. The token embedding layer is reset every K updates.\nInput: K, interval between two consecutive forgetting; nbody/emb, current effective number of updates for the\nbody or the token embedding layer; \u03b1body/emb, current learning rate for the body or the token embedding\nlayer; P n\nbody/emb, parameters after the nth update for the body or the token embedding layer; On\nbody/emb,\noptimizer states after the nth update for the body or the token embedding layer; \u0398, randomly initialized\nembedding parameters, each element drawn from N(0, 0.02); f, the function that computes the gradients\nw.r.t the parameters using the sampled data; g, the function that updates the parameters based on the\ngradients (e.g. one step in Adam optimizer) s, the function that updates the learning rate (e.g. one step in\nthe polynomial learning rate scheduler).\nOutput: the updated parameters and optimizer states P (n) = {P (n)\nemb , P (n)\nbody}, O(n) = {O(n)\nemb, O(n)\nbody}.\n1: nemb = n mod K\n2: \u03b1body = s(nbody) {adjust learning rate based on n}\n3: \u03b1emb = s(nemb)\n4: G(n) = f(P (n\u22121), \u00b7) {compute all gradients}\n5: P (n)\nbody, o(n)\nbody = g(G(n)\nbody, P (n\u22121)\nbody\n, o(n\u22121)\nbody\n, \u03b1body, n) {update the transformer body}\n6: if nemb == 0 then\n7:\nP (n)\nemb = \u0398 {periodical reset token embeddings and the relevant optimizer states}\n8:\no(n\u22121)\nemb\n= 0\n9: end if\n10: P (n)\nemb , o(n)\nemb = g(G(n)\nemb, P (n\u22121)\nemb\n, o(n\u22121)\nemb\n, \u03b1emb, nemb) {update the token embeddings}\n5\nMethod\nvi\nsw\nes\nbg\nde\nfr\nel\nru\nStandard\n65.8\n55.6\n68.0\n65.5\n62.2\n63.5\n63.1\n56.9\nForgetting\n62.8\n59.5\n74.0\n71.7\n68.5\n71.2\n70.8\n65.8\nRelative Gain\n\u22124.6%\n+7.0%\n+8.8%\n+9.5%\n+10.1%\n+12.1%\n+12.2%\n+15.6%\nTable 1: Accuracy comparison of forgetting and standard PLMs on XNLI (table continues).\nMethod\nzh\nur\nhi\ntr\nar\nth\nAvg\nen\nStandard\n53.2\n36.8\n39.7\n38.9\n41.2\n35.3\n53.3\n86.1\nForgetting\n63.5\n45.8\n52.9\n52.7\n59.5\n59.7\n62.7\n85.1\nRelative Gain\n+19.4%\n+24.5%\n+33.2%\n+35.5%\n+44.4%\n+69.1%\n+21.2%\n\u22121.2%\nTable 2: Accuracy comparison of forgetting and standard PLMs on XNLI (table continued). On\naverage, forgetting PLMs achieve a 21.2% relative gain in accuracy compared to standard PLMs\nacross the languages tested, where averaged relative gain =\nP\nx\u2208{languages} Relative Gain of x\n#Languages\n.\n10K warm-up updates. Checkpoints were saved every 500 updates and we always choose the last\npretraining checkpoint where possible for optimal performance. For forgetting pretraining, we chose\nthe checkpoint corresponding to the best validation perplexity since the last checkpoint might have\ntoken embeddings reset. We set the frequency of forgetting K = 1000 and used a clip-norm of 0.5.\nDuring the language adapt stage, we kept most of the hyper-parameters the same as for pretraining. We\nfinetuned the token embedding layer while keeping the others frozen, as described in Section 2. Note\nthat no forgetting happens during this stage because we want the models to learn the new languages\nas well as possible. In the task adapt stage, both models were finetuned for 10 epochs on the English\ntask data, specifically MultiNLI [Williams et al., 2018] for the NLI task and SQUAD Rajpurkar\net al. [2016] for the QA task. After the assemble stage, we evaluate the zero-shot performance\nof the assembled model on XNLI [Conneau et al., 2018], a cross-lingual NLI task, along with\nXQuAD [Artetxe et al., 2020] and MLQA [Lewis et al., 2020], two cross-lingual QA tasks. We report\nthe NLI accuracy and QA F1 on the test sets.\nOur experiments were implemented using fairseq [Ott et al., 2019]. The pretraining and language\nadaptation experiments were conducted on 32 Tesla V100 GPUs (each with 32 GB memory) and\ntook approximately 24-36 hours to complete. The time taken for both stages were quite close to each\nother even though the latter only involved tuning the embeddings. This demonstrates the importance\nof reducing the computational cost of the language adaptation stage.\nDiffering from prior work [Artetxe et al., 2020, Marchisio et al., 2022], we focus on language adapt\nin low-data regimes. We simulate low-resources scenarios by limiting the adaptation data for each\ndownstream language to only 5M subword tokens from CC100. This is in contrast with conventional\nsetups, where all the tokens in the corresponding languages in CC100 are used for language adaptation.\nAs Table 6 shows, such setups consume several orders of magnitude more data than our 5M-token\nsetup; for instance, the Swahili CC100 subset contains 345M tokens, roughly 69 times larger than\nour corpus, and the Russian subset contains 34.9B tokens, roughly 6, 980 times larger. Therefore,\nPLMs that can successfully learn new languages with rich data under traditional setups may struggle\nto do so with our limited 5M-token corpus.\n4.2\nForgetting PLMs Work Better in Low-Data Regimes (RQ1)\nStandard PLMs struggle in low-data language adaptation, dropping from 86.1 English NLI accuracy\nto just 53.3 average accuracy on XNLI with limited 5M token adaptation data. Compared to prior\nwork which uses full data from Wikipedia [Artetxe et al., 2020] or from CC100 [Marchisio et al.,\n2022], the average accuracy on XNLI drops about 18% (from 66.8/66.3 to 53.3). This indicates\nstandard PLMs are not coping well with the low-data regime. In contrast, forgetting PLMs achieve\ndecent 62.7 average XNLI accuracy, a +21.2% relative gain over standard PLMs, as shown in Table 2.\nForgetting PLMs also outperform standard PLMs on MLQA and XQuAD, with average F1 relative\ngains of +33.8% and +60.9% across languages, as respectively demonstrated in Table 3 and Table 4.\nAcross NLI and QA tasks, forgetting PLMs consistently surpass standard PLMs in low-data regimes.\n6\nMethod\nes\nvi\nde\nzh\nhi\nar\nAvg\nen\nStandard\n49.4\n38.3\n45.3\n34.1\n17.7\n20.8\n34.3\n78.9\nForgetting\n55.3\n45.0\n53.4\n43.0\n28.8\n34.7\n43.4\n78.3\nRelative Gain\n+12.0%\n+17.6%\n+17.8%\n+26.2%\n+62.5%\n+67.0%\n+33.8%\n\u22120.8%\nTable 3: F1-score comparison of forgetting and standard PLMs on MLQA. On average, forgetting\nPLMs achieve a 33.8% relative gain in F1 compared to standard PLMs across the languages tested,\nwhere averaged relative gain =\nP\nx\u2208{languages} Relative Gain of x\n#Languages\n.\nMethod\nvi\nes\nru\nde\nel\nzh\nhi\nar\nth\ntr\nAvg\nStandard\n49.7\n57.7\n49.4\n50.9\n48.5\n32.4\n21.4\n22.2\n15.4\n13.0\n36.1\nForgetting\n52.9\n64.6\n56.5\n60.9\n59.9\n43.7\n33.3\n38.7\n38.4\n41.4\n49.0\nRelative Gain\n+6.4%\n+12.0%\n+14.5%\n+19.7%\n+23.6%\n+34.6%\n+55.8%\n+74.2%\n+149.7%\n+218.8%\n+60.9%\nTable 4: F1-score comparison of forgetting and standard PLMs on XQuAD. On average, forgetting\nPLMs achieve a 60.9% relative gain in F1 compared to standard PLMs across the languages tested,\nwhere averaged relative gain =\nP\nx\u2208{languages} Relative Gain of x\n#Languages\n.\nWhy do forgetting PLMs handle the low-data regime better? We hypothesize this is because forgetting\nPLMs are more robust to different embedding initializations. They encode more universal knowledge\nin the transformer body. Standard PLMs may encode more \u201cshortcut\u201d knowledge relying on certain\nembedding initializations. In low data, standard PLMs cannot adjust embeddings towards shortcut\nroutes without access to enough data. Forgetting PLMs do not rely on shortcuts so perform better.\n4.3\nForgetting PLMs Learn New Languages with Fewer Parameter Updates (RQ2)\nWe are also interested in how quickly forgetting PLMs and standard PLMs can learn new languages.\nFigure 5 summarizes adaptation curves on XNLI, MLQA and XQuAD, with each point representing\nthe averaged performance across all languages. In just 5K steps (4% of full adaptation), forgetting\nPLMs reach 57.8 accuracy on XNLI while standard PLMs struggle at random guessing levels of 37.2.\nSimilar trends hold for MLQA and XQuAD. After 5K steps, forgetting PLMs achieve 92% of their\nfull performance on XQuAD versus just 53% for standard PLMs (see the last plot in Figure 5).\nWhy do forgetting PLMs converge faster? We hypothesize it is because periodical embedding\nresetting forces the body to gradually locate itself on a particular manifold, where it can easily\ncooperate with new embeddings. This makes the body encourage larger embedding updates when\nadapting to new languages. Active forgetting simulates language switching during pretraining2\nintroducing diversity without new data. This allows faster adaptation to real new languages.\n4.4\nLanguages That Are Distant To English Benefit Most From Forgetting PLMs (RQ3)\nUp to this point, we have primarily presented the averaged performance. In this section, we delve\ninto a detailed comparison of language-specific performance between forgetting PLMs and standard\nPLMs on XNLI, MLQA, and XQuAD. To gain a deeper insight into which languages benefit the\nmost from the use of forgetting, we present the relative performance changes in Figure 6 for XNLI\nand MLQA. The results for XQuAD can be found in Figure 8 in the appendix. Across the spectrum\nof languages (Table 5), we observe that forgetting provides greater benefits for languages distant to\nthe pretraining language (English) in terms of language family, script and morphology. Specifically,\nforgetting brings large relative gains for languages such as Arabic, Hindi, Thai, Turkish, and Urdu\ncompared to closer languages like German. Script seems important - forgetting helps Vietnamese\nand Swahili less despite their distance from English, likely due to the shared Latin script. Examining\nadaptation curves within the first 5K steps, forgetting PLMs reach substantially superior performance\nover standard PLMs for almost all languages except Urdu, while standard PLMs struggle at random\nguess levels (see Figure 7 and Appendix D). This demonstrates forgetting PLMs\u2019 ability to efficiently\nadapt to new languages, particularly dissimilar ones, in low-data settings.\n2Precisely, it simulates vocabulary swappings, causing drastic changes to the input of the body.\n7\n1000\n2000\n3000\n4000\n5000\n# Adaptation Steps\n35\n40\n45\n50\n55\nAverage Accuracy on XNLI\nXNLI Accuracy vs Adaptation Steps [First 5K Steps]\nforget\nstandard\n1000\n2000\n3000\n4000\n5000\n# Adaptation Steps\n10\n20\n30\n40\nAverage F1 on MLQA\nMLQA F1 vs Adaptation Steps [First 5K Steps]\nforget\nstandard\n1000\n2000\n3000\n4000\n5000\n# Adaptation Steps\n0\n10\n20\n30\n40\nAverage F1 on XQUAD\nXQUAD F1 vs Adaptation Steps [First 5K Steps]\nforget\nstandard\nFigure 5: Adaptation curves on XNLI, MLQA, and XQuAD. Numbers aggregated across languages.\nThe first row contains the full adaptation curves, which comprises 125K adaptation steps. The second\nrow contains the zoom-in versions of curves for the first 5K adaptation steps. Forgetting PLMs\nconverge faster than standard PLMs; for instance, on XQuAD (the last plot), forgetting PLMs reach\n92% of their final performance within 5K updates, while standard PLMs only reached 53% of their\nfinal performance at that point.\n0\n20\n40\n60\nAccuracy on XNLI\nRelative Gain Across Languages on XNLI\nStandard\nForget\nvi sw es bg de\nfr\nel\nru zh ur\nhi\ntr\nar th\nLanguage\n0\n25\n50\nRelative Gain (%)\n0\n20\n40\nF1 on MLQA\nRelative Gain Across Languages on MLQA\nStandard\nForget\nes\nvi\nde\nzh\nhi\nar\nLanguage\n0\n20\n40\n60\nRelative Gain of F1 (%)\nFigure 6: Relative gains of forgetting PLMs over standard PLMs across languages. Forgetting yields\nsubstantial relative gains for languages like Arabic, Hindi, Thai, Turkish, and Urdu. However, for\nlanguages closely related to English, such as German, the relative gains from forgetting are modest.\n5\nRelated Work\n5.1\nForgetting and its Positive Roles\nThe common perception of forgetting is that it implies weak memory and a loss of acquired knowledge,\nthus it is often regarded as a sign of un-intelligence or an undesirable property. In neural networks,\ncatastrophic forgetting [McCloskey and Cohen, 1989, Ratcliff, 1990, Kirkpatrick et al., 2017] is\nportrayed as a forgetting phenomenon where neural networks lose the ability to predict old patterns\nafter new inputs alter their weights. Forgetting in this context has negative consequences, as the new\nknowledge overwrites the old. Plenty of prior research strives to overcome catastrophic forgetting\nand enable continual learning [Schmidhuber, 2013, Kirkpatrick et al., 2017, Lopez-Paz and Ranzato,\n2017, Shin et al., 2017, Schwarz et al., 2018, Mallya and Lazebnik, 2018, Parisi et al., 2019, Rolnick\net al., 2019, Beaulieu et al., 2020, Veniat et al., 2020, Gaya et al., 2023, Khetarpal et al., 2022].\nOur work differs from the above ones in that our subject is intentional forgetting rather than passive\nforgetting and its associated negative impact. To put it in another way, we seek to understand how\n8\nFigure 7: Adaptation curves on XNLI within 5K updates for individual languages: Bulgaria, Greek,\nSpanish, French, Russian, Swahili, Vietnamese and Chinese. For all languages except Urdu, the\nforgetting PLMs converge faster than the standard PLMs during the language adaptation stage.\nforgetting \u2013 if purposely incorporated as an active process during training \u2013 might help new learning.\nSimilar positive roles of forgetting have been discussed in the literature. Specifically, Past\u00f6tter et al.\n[2008] demonstrate forgetting enhances the learning of new information by resetting the encoding\nprocess and holding the attention at high levels; Levy et al. [2007] show that it helps second\nlanguage acquisition by inhibiting the native language; Barrett and Zollman [2009] find it promote\nthe emergence of an optimal language by preventing partial success from reinforce sub-optimal\npractice. N\u00f8rby [2015] further suggests forgetting serves adaptive functions, helping people regulate\nemotions, acquiring knowledge and staying attuned to the context. More recently Anderson and\nHulbert [2021] reviews evidence on active forgetting by prefrontal control and shows how it can\nadapt the memory to suit either emotional or cognitive goals.\n5.2\nForgetting Via Partial Neural Weights Reset\nIn neural networks, forgetting can be instantiated in many forms. A simple way is to reset subsets of\nparameters before the next round of learning. Iterations of such resetting have been shown to benefit\ngeneralization with low compute and low data for computer vision tasks [Frankle and Carbin, 2019,\nAlabdulmohsin et al., 2021, Taha et al., 2021, Ramkumar et al., 2023]. More recently, Zhou et al.\n[2022] demonstrate a similar forgetting strategy helps image classification and language emergence.\nClosely linked to our method, Chen et al. [2022] forget node embeddings in order to truncate infinite\nmessage-passing among nodes and thereby aid new graph reasoning with new nodes. Our work uses\nsimilar forgetting mechanism over token embeddings, improving new language reasoning with new\ntokens. As far as we know, we are the first to bring forgetting into pretraining and demonstrate that\nforgetting pretraining boosts linguistic plasticity. A relevant thread in reinforcement learning (RL)\nresearch studies the plasticity loss phenomenon [Lyle et al., 2023, Nikishin et al., 2023]. Recent work\nexplores similar forgetting approaches to improve plasticity. Igl et al. [2021] periodically reset the\ncurrent policy by distilling it into a reinitialized network throughout training. Intuitively, this releases\nnetwork capacity storing suboptimal policies and opens up space for the the yet-to-be-discovered\noptimal (final) policy. Simpler methods just reset an agent\u2019s last layers [Nikishin et al., 2022],\npreventing overfitting to early experiences and primacy bias. Resetting parameters also improves\nsample efficiency by allowing more updates per environment interaction [D\u2019Oro et al., 2023].\n5.3\nMaking Pretrained Language Models Multilingual\nPretraining on multilingual data makes PLMs multilingual [Conneau et al., 2020] but has downsides\nlike needing large multilingual corpus with appropriate mixing, potential interference among lan-\n9\nguages, and difficulty of covering all languages. Alternatively, the line of research on cross-lingual\ntransfer makes PLMs multilingual by extending English-only PLMs to other languages. Artetxe et al.\n[2020] demonstrate possibility of such extension by relearning the embedding layer with unsupervised\ndata from the new language. Marchisio et al. [2022] further increase computational efficiency using\na mini-model proxy. Liu et al. [2023a] use a similar partial reset-reinit approach in vision-language\nsettings. Approaches based on adapters and sparse finetuning have also been proposed [Pfeiffer\net al., 2020, 2022, 2021, Ansell et al., 2022]. Adapters are bottleneck layers (usually placed after\nthe feedforward layers) that add extra capacity when adapting to a different task or language. Our\nproposed forgetting mechanism can be applied to adapter-based methods as we can allow forgetting\nto happen in the adapter layers. The current choice of forgetting embeddings keeps the architecture\nintact and incurs no additional hyperparameter tuning, allowing us to understand the fundamental\ncapability of forgetting pretraining.\n6\nConclusion & Future work\n6.1\nConclusions\nWhile forgetting is usually perceived as negative, recent work points out that it can also be beneficial in\ncertain cases, particularly for quickly learning new tasks, training in non-stationary environments [Igl\net al., 2021, Nikishin et al., 2022, D\u2019Oro et al., 2023] and improving sample efficiency [Taha et al.,\n2021, Zhou et al., 2022]. Joining this line of work, our paper demonstrates that forgetting techniques\ncan improve pretrained language models by imbuing them with more linguistic plasticity. Specifically,\nour proposed active forgetting mechanism can create PLMs that are easier to rewire for new lingual\nspaces. Experiments with RoBERTa show that models pretrained via active forgetting can better\nlearn from small amounts of data while also enjoying faster convergence during language adaptation,\nparticularly for languages distant from English.\nGoing beyond language adaptation, we argue that PLMs with more plasticity are a promising direction\nfor future research, as they allow easier adaptation to various tasks, domains, languages and can\nevolve faster as the real world changes. Unlike symbolic methods, such as knowledge graphs, which\ncan easily rewire a fact by modifying the corresponding knowledge triplet, current static PLMs are\nharder to rewire since changing one fact via updating model weights may disrupt multiple other facts\nwithout substantial post-hoc intervention. Improving the rewirability via forgetting pretraining thus\ncan be seen as one way of imbuing PLMs with similar benefits as symbolic methods (making the\nresulted model more controllable i.e. can be modified with tiny cost), complementing the line of\npost-hoc model editing research [Mitchell et al., 2021, 2022].\n6.2\nLimitations\nOur work uses the simplest forgetting approach - directly resetting embeddings to random initializa-\ntion. Advanced techniques like gradually injecting noise could be explored. We focus on masked\nlanguage modeling pretraining with language-specific tokenizers. Applying active forgetting to\nauto-regressive LMs, other pretraining methods (e.g. deberta pretraining [He et al., 2021b,a]), and\nvarious tokenization is promising future work. More broadly, current large language models need\nmore plasticity to expand across tools, tasks, and domains. Our work takes an initial step, showing\ndirectly resetting embeddings can significantly improve model plasticity. Further research on more\nsophisticated forgetting techniques during pretraining could unlock additional gains.\nOn the theory front, potential connections can be made between forgetting and meta-learning [Schaul\nand Schmidhuber, 2010, Thrun and Pratt, 2012, Andrychowicz et al., 2016, Finn et al., 2017] since\nboth attempt to learn solutions that can quickly adapt themselves to new inputs. Another possible\ntheoretical explanation for why active forgetting works so well might be related to the flatness of\nthe solution in the loss landscape [Alabdulmohsin et al., 2021]. Flatter minima tend to enjoy better\ngeneralization [Liu et al., 2023b]. Thus, it might be worthwhile to study the flatness of the transformer\nbody during the forgetting pretraining.\n10\nAcknowledgments and Disclosure of Funding\nWe would like to thank our reviewers for their valuable suggestions. We are also grateful to those\nwho engaged in interesting discussions during the project, including Pasquale Minervini, Xuanli He,\nJiayi Wang, Yuxiang Wu, Hila Gonen, Dieuwke Hupkes, Fabio Petroni, Naila Murray, Alexis Thual,\nNicola Cancedda, Yingchen Xu, and Hubert Jacob Banville. Yihong would like to express her thanks\nto the FAIR-UCL PhD program for generously funding her PhD. David Adelani acknowledges the\nsupport of DeepMind Academic Fellowship programme.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D\u2019souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, et al.\nMasakhaner: Named entity recognition for african languages. Transactions of the Association for\nComputational Linguistics, 9:1116\u20131131, 2021.\nDavid Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman,\nChester Palen-Michel, Constantine Lignos, Jesujoba O Alabi, Shamsuddeen H Muhammad, Peter\nNabende, et al. Masakhaner 2.0: Africa-centric transfer learning for named entity recognition.\nIn 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022),\nAbu Dhabi, United Arab Emirates, December 7-11, 2022, pages 4488\u20134508. Association for\nComputational Linguistics (ACL), 2022.\nIbrahim Alabdulmohsin, Hartmut Maennel, and Daniel Keysers. The impact of reinitialization on\ngeneralization in convolutional neural networks. arXiv preprint arXiv:2109.00267, 2021.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. Adapting pre-\ntrained language models to African languages via multilingual adaptive fine-tuning. In Proceedings\nof the 29th International Conference on Computational Linguistics, pages 4336\u20134349, Gyeongju,\nRepublic of Korea, October 2022. International Committee on Computational Linguistics. URL\nhttps://aclanthology.org/2022.coling-1.382.\nMichael\nC.\nAnderson\nand\nJustin\nC.\nHulbert.\nActive\nforgetting:\nAdaptation\nof\nmemory by prefrontal control.\nAnnual Review of Psychology,\n72(1):1\u201336,\n2021.\ndoi:\n10.1146/annurev-psych-072720-094140.\nURL https://doi.org/10.1146/\nannurev-psych-072720-094140. PMID: 32928060.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient\ndescent. Advances in neural information processing systems, 29, 2016.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u00b4c. Composable sparse fine-tuning for cross-\nlingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1778\u20131796, 2022.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolin-\ngual representations. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4623\u20134637, 2020.\nJeffrey Barrett and Kevin JS Zollman. The role of forgetting in the evolution and learning of language.\nJournal of Experimental & Theoretical Artificial Intelligence, 21(4):293\u2013309, 2009.\nShawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff Clune, and Nick\nCheney. Learning to continually learn. arXiv preprint arXiv:2002.09571, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot\n11\nlearners.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/\n2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nYihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, and Sebastian\nRiedel. Refactor gnns: Revisiting factorisation-based models from a message-passing perspective.\nIn Advances in Neural Information Processing Systems, 2022.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485,\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 8440\u20138451, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL\nhttps://aclanthology.org/2020.acl-main.747.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nPierluca D\u2019Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G Bellemare, and\nAaron Courville. Sample-efficient reinforcement learning by breaking the replay ratio barrier.\nIn The Eleventh International Conference on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=OpC-9aBBVJe.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela\nFan, John Ortega, Ricardo Ramos, Annette Rios Gonzales, Ivan Meza-Ruiz, et al. Americasnli:\nEvaluating zero-shot natural language understanding of pretrained multilingual models in truly\nlow-resource languages. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 6279\u20136299, 2022.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. In International Conference on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=rJl-b3RcF7.\nJean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta\nRaileanu. Building a subspace of policies for scalable continual learning. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=UKr0MwZM6fL.\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n8342\u20138360, 2020.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding sharing, 2021a.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. In International Conference on Learning Representations, 2021b.\nURL https://openreview.net/forum?id=XPZIaotutsD.\n12\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293, 2021.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson.\nTransient non-stationarity and generalisation in deep reinforcement learning. In International\nConference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=Qun8fv4qSby.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement\nlearning: A review and perspectives. Journal of Artificial Intelligence Research, 75:1401\u20131476,\n2022.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming\ncatastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114\n(13):3521\u20133526, 2017.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, 2018.\nBenjamin J Levy, Nathan D McVeigh, Alejandra Marful, and Michael C Anderson. Inhibiting\nyour native language: The role of retrieval-induced forgetting during second-language acquisition.\nPsychological Science, 18(1):29\u201334, 2007.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. Mlqa: Evaluating\ncross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 7315\u20137330, 2020.\nChen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vuli\u00b4c, and Iryna Gurevych. Delving deeper into cross-\nlingual visual question answering. In Findings of the Association for Computational Linguistics:\nEACL 2023, pages 2408\u20132423, 2023a.\nHong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream:\nImplicit bias matters for language models. In Proceedings of the 40th International Conference on\nMachine Learning, 2023b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning.\nAdvances in neural information processing systems, 30, 2017.\nClare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney.\nUnderstanding plasticity in neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th\nInternational Conference on Machine Learning, volume 202 of Proceedings of Machine Learning\nResearch, pages 23190\u201323211. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.\npress/v202/lyle23b.html.\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative\npruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,\npages 7765\u20137773, 2018.\n13\nKelly Marchisio, Patrick Lewis, Yihong Chen, and Mikel Artetxe. Mini-model adaptation: Efficiently\nextending pretrained models to new languages via aligned shallow training.\narXiv preprint\narXiv:2212.10503, 2022.\nMichael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. volume 24 of Psychology of Learning and Motivation, pages 109\u2013165.\nAcademic Press, 1989. doi: https://doi.org/10.1016/S0079-7421(08)60536-8. URL https:\n//www.sciencedirect.com/science/article/pii/S0079742108605368.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model\nediting at scale. In International Conference on Learning Representations, 2021.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-\nbased model editing at scale. In International Conference on Machine Learning, pages 15817\u2013\n15831. PMLR, 2022.\nEvgenii Nikishin, Max Schwarzer, Pierluca D\u2019Oro, Pierre-Luc Bacon, and Aaron Courville. The\nprimacy bias in deep reinforcement learning. In International Conference on Machine Learning,\npages 16828\u201316847. PMLR, 2022.\nEvgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, and Andre\nBarreto. Deep reinforcement learning with plasticity injection. In Workshop on Reincarnating\nReinforcement Learning at ICLR 2023, 2023. URL https://openreview.net/forum?\nid=O9cJADBZT1.\nSimon N\u00f8rby. Why forget? on the adaptive value of memory loss. Perspectives on Psychological\nScience, 10(5):551\u2013578, 2015. doi: 10.1177/1745691615596787. URL https://doi.org/\n10.1177/1745691615596787. PMID: 26385996.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational Linguistics\n(Demonstrations), pages 48\u201353, 2019.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\nlifelong learning with neural networks: A review. Neural networks, 113:54\u201371, 2019.\nBernhard Past\u00f6tter, Karl-Heinz B\u00e4uml, and Simon Hanslmayr. Oscillatory brain activity before and\nafter an internal context change\u2014evidence for a reset of encoding processes. NeuroImage, 43(1):\n173\u2013181, 2008.\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 7654\u20137673, Online,\nNovember 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.\n617. URL https://aclanthology.org/2020.emnlp-main.617.\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Sebastian Ruder. Unks everywhere: Adapting\nmultilingual language models to new scripts. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 10186\u201310203, 2021.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\nLifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3479\u20133495, 2022.\nAlec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n2018.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 2383\u20132392, 2016.\n14\nVijaya Raghavan T Ramkumar, Elahe Arani, and Bahram Zonooz. Learn, unlearn and relearn: An\nonline learning paradigm for deep neural networks. Transactions on Machine Learning Research,\n2023. ISSN 2835-8856. URL https://openreview.net/forum?id=WN1O2MJDST.\nRoger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and\nforgetting functions. Psychological review, 97(2):285, 1990.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.\nTom Schaul and J\u00fcrgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.\nJ\u00fcrgen Schmidhuber. Powerplay: Training an increasingly general problem solver by continually\nsearching for the simplest still unsolvable problem. Frontiers in psychology, 4:313, 2013.\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual\nlearning. In International conference on machine learning, pages 4528\u20134537. PMLR, 2018.\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative\nreplay. Advances in neural information processing systems, 30, 2017.\nAhmed Taha, Abhinav Shrivastava, and Larry S Davis. Knowledge evolution in neural networks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n12843\u201312852, 2021.\nY. Tang, C. Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and\nAngela Fan. Multilingual translation with extensible multilingual pretraining and finetuning.\nArXiv, abs/2008.00401, 2020. URL https://api.semanticscholar.org/CorpusID:\n220936592.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023.\nTom Veniat, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Efficient continual learning with modular\nnetworks and task-driven priors. arXiv preprint arXiv:2012.12631, 2020.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins,\nTom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks,\nWilliam Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of\nharm from language models. CoRR, abs/2112.04359, 2021. URL https://arxiv.org/abs/\n2112.04359.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor,\nAmelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by\nlanguage models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages\n214\u2013229, 2022.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 1112\u20131122, 2018.\nHattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville. Fortuitous forgetting in connectionist\nnetworks. In International Conference on Learning Representations, 2022.\n15\nName\nCode\nFamily\nScript\nMorphology\nArabic\nar\nSemitic\nArabic (Abjad)\nIntroflexive\nBulgaria\nbg\nIE:Balto-Slavic\nCyrillic\nAnalytic\nGerman\nde\nIE:Germanic\nLatin\nFusional\nGreek\nel\nIE:Hellenic\nGreek\nFusional\nEnglish\nen\nIE:Germanic\nLatin\nAnalytic\nFrench\nfr\nIE:Romance\nLatin\nFusional\nHindi\nhi\nIE:Indo-Iranian\nDevanagari\nFusional\nRussian\nru\nIE:Balto-Slavic\nCyrillic\nFusional\nSpanish\nes\nIE:Romance\nLatin\nFusional\nSwahili\nsw\nNiger-Congo:Bantu\nLatin\nAgglutinative\nThai\nth\nTai-Kadai\nThai\nAnalytic\nTurkish\ntr\nTurkic\nLatin\nAgglutinative\nUrdu\nur\nIE:Indo-Iranian\nPerso-Arabic\nFusional\nVietnamese\nvi\nAustroasiatic\nLatin\nAnalytic\nChinese\nzh\nSino-Tibetan\nChinese\nAnalytic\nTable 5: Languages by family, script, and morphology.\nA\nDiscussion on Experimental Setup A Low-Data Regime\nA common experimental setup for adapting to a target language is to use all the available data\nin that language from sources such as Wikipedia [Artetxe et al., 2020, Ansell et al., 2022] and\nCC100 [Marchisio et al., 2022]. In this setup, the numbers of tokens typically used for adapting each\nlanguage might differ greatly, ranging from 13.9M to 70.5B, as summarized via Table 5.\nOur work, however, investigates a different setup where we control the adaptation data to 5 million\ntokens for each language. This can be highly relevant when studying generalisation to completely new\nlanguages, which require expanding the vocabulary. We acknowledge that dealing with real-world\nlow-resources languages can be more challenging than such low-data setup used. And there are\nalready rich work addressing low-resource issues: multilingual pretraining [Conneau et al., 2020,\nPfeiffer et al., 2022], multilingual adapters [Pfeiffer et al., 2020, Ansell et al., 2022], multilingual\nadaptation [Tang et al., 2020, Alabi et al., 2022], and multilingual regularization [Pfeiffer et al., 2021].\nNevertheless, we would like to highlight the importance of our low-data regime. The challenge of\n\u201clow-resource\u201d involve multiple entangled factors: the quality of the tokeniser, the amount of data,\nwhether the script/language family is distant to the pretraining language(s) etc. Simulating a low-data\nregime allows us to control these factors and isolate the effects of the factor that we are interested in \u2013\nthe amount of data in the new language. This factor is essential to our work as our research goal is\nplasticity i.e. rewiring model prediction with as little new information as possible. Simulating various\namount of data in the new language allows us to compare model plasticity as shown in Figure 3,\nand thus contribute a clean piece of knowledge in the line of plasticity research [Lyle et al., 2023,\nNikishin et al., 2023].\nB\nDiscussion on Experimental Framework Choice for Studying Pretrained\nLanguage Models\u2019 Plasticity\nOur motivation is to improve language models\u2019 plasticity. Plasticity of neural networks have been\nstudied in graph learning, computer vision and reinforcement learning [Taha et al., 2021, Chen et al.,\n2022, Lyle et al., 2023, Nikishin et al., 2023], where forgetting-relearn methods show promise. Our\ngoal is to study plasticity in the context of pretrained language models. We believe this is a emerging\nresearch direction and will thrive in the following years. However, translating the plasticity concept\nto the language model setting is not trivial due to the lack of clear experimental setups. We note\nthat, despite the model differences, almost all language models begins with a token embedding layer.\nAs often tied to a specific vocabulary, the token embedding layer limits the plasticity, preventing\ngeneralisation to a new vocabulary. This observation inspires us to explore the plasticity of language\nmodels by manipulating the token embedding layer. Artetxe et al. [2020] draws our attention as it\noffers a nice experimental framework of only manipulating the token embedding layer for adapting\nbetween languages.\n16\nLanguage\nCC-100\nWikipedia\nsw\n345M\n13.9M\nur\n832M\n41.5M\nhi\n2.13B\n54.6M\nar\n4.15B\n337M\ntr\n4.19B\n157M\nth\n6.09B\n70.7M\nel\n6.1B\n148M\nbg\n7.9B\n134M\nzh\n9.72B\n584M\nes\n11.6B\n1.17B\nfr\n13.3B\n1.71B\nde\n14.1B\n2B\nvi\n28.9B\n300M\nru\n34.9B\n1.25B\nen\n70.5B\n4.25B\nTable 6: Numbers of tokens for different languages on the two multilingual corpus, CC-100 and\nWikipedia, in ascending order of CC100. The English one is used as pretraining corpus while the\nothers are used for language adaptation.\n0\n20\n40\n60\nF1 on XQUAD\nRelative Gain Across Languages on XQUAD\nStandard\nForget\nvi\nes\nru\nde\nel\nzh\nhi\nar\nth\ntr\nLanguage\n0\n100\n200\nRelative Gain of F1 (%)\nFigure 8: Relative gains of forgetting PLMs over standard PLMs across languages on XQuAD.\nLanguages that are less related to the pretraining language (English), such as Turkish, Thai, Arabic,\nHindi, benefit more from forgetting PLMs.\nC\nMore Results on Distant Languages Benefits More From Forgetting\nWe are interested in how forgetting PLMs can improve adaptation to different languages. We compare\nthe results of various languages on three benchmarks: XNLI, MLQA and XQuAD. We use Figure 6\nand Figure 8 to illustrate the relative gains from active forgetting on each benchmark. We find that\nlanguages that are less related to the pretraining language, which in this case is English, benefit more\nfrom forgetting PLMs.\nD\nMore Results on Forgetting PLMs Converge Faster In The Language\nAdaptation Stage\nFigure 9 displays the adaptation curves for several languages (Arabic, German, Hindi, Thai, Turkish,\nand Urdu) during full training runs of 125,000 steps. This complements Figure 7, which focuses\n17\nAdaptation Steps\n35\n40\n45\n50\n55\n60\nXNLI Accuracy\nAR\nforget\nstandard\nAdaptation Steps\n40\n45\n50\n55\n60\n65\n70\nXNLI Accuracy\nDE\nforget\nstandard\nAdaptation Steps\n35\n40\n45\n50\nXNLI Accuracy\nHI\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n35\n40\n45\n50\n55\n60\nXNLI Accuracy\nTH\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nXNLI Accuracy\nTR\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n36\n38\n40\n42\n44\n46\nXNLI Accuracy\nUR\nforget\nstandard\nFigure 9: Adaptation curves on XNLI for individual languages: Arabic, German, Hindi, Thai, Turkish,\nand Urdu. Forgetting helps more languages that are distant to English (the pretraining language).\nAdaptation Steps\n40\n50\n60\n70\nXNLI Accuracy\nBG\nforget\nstandard\nAdaptation Steps\n40\n50\n60\n70\nXNLI Accuracy\nEL\nforget\nstandard\nAdaptation Steps\n40\n50\n60\n70\nXNLI Accuracy\nES\nforget\nstandard\nAdaptation Steps\n40\n50\n60\n70\nXNLI Accuracy\nFR\nforget\nstandard\n1000 2000 3000 4000 5000\nAdaptation Steps\n35\n40\n45\n50\n55\n60\n65\nXNLI Accuracy\nRU\nforget\nstandard\n1000 2000 3000 4000 5000\nAdaptation Steps\n35\n40\n45\n50\n55\nXNLI Accuracy\nSW\nforget\nstandard\n1000 2000 3000 4000 5000\nAdaptation Steps\n35\n40\n45\n50\n55\n60\nXNLI Accuracy\nVI\nforget\nstandard\n1000 2000 3000 4000 5000\nAdaptation Steps\n35\n40\n45\n50\n55\n60\nXNLI Accuracy\nZH\nforget\nstandard\nXNLI Accuracy vs Adaptation Steps [First 5K Steps]\nFigure 10: Adaptation curves on XNLI within 5K updates for individual languages: Bulgaria, Greek,\nSpanish, French, Russian, Swahili, Vietnamese and Chinese. Forgetting PLMs converge faster than\nstandard PLMs.\non the first 5,000 steps. Similar convergence patterns can be observed for additional languages, as\nshown in Figure 10 and Figure 11.\nE\nMore Detailed Analysis\nE.1\nImpact of Forgetting Frequency\nWe would like to elaborate on our choice of forgetting frequency K. In our preliminary experiments,\nwe tried K = 100, 1000, 5000. We find K = 1000 works well and thus sticks with it. Since we don\u2019t\nwant to overtune the hyperparameters, we just use the same K for all the experiments. We include\nthe loss curves of K = 100 and K = 5000 here. We can see that both forgetting too frequently and\nforgetting too infrequently will hurt the performance. Too frequent forgetting leaves little time for the\n18\nAdaptation Steps\n45\n50\n55\n60\n65\n70\nXNLI Accuracy\nBG\nforget\nstandard\nAdaptation Steps\n35\n40\n45\n50\n55\n60\n65\n70\nXNLI Accuracy\nEL\nforget\nstandard\nAdaptation Steps\n40\n50\n60\n70\nXNLI Accuracy\nES\nforget\nstandard\nAdaptation Steps\n40\n45\n50\n55\n60\n65\n70\nXNLI Accuracy\nFR\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n40\n45\n50\n55\n60\n65\nXNLI Accuracy\nRU\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n40\n45\n50\n55\n60\nXNLI Accuracy\nSW\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n40\n45\n50\n55\n60\n65\nXNLI Accuracy\nVI\nforget\nstandard\n0\n50000\n100000\nAdaptation Steps\n40\n45\n50\n55\n60\n65\nXNLI Accuracy\nZH\nforget\nstandard\nXNLI Accuracy vs Adaptation Steps\nFigure 11: Adaptation curves on XNLI for individual languages: Bulgaria, Greek, Spanish, French,\nRussian, Swahili, Vietnamese and Chinese. Across all the languages except on Vietnamese, the\nforgetting PLMs reach a better performance level than their standard counterparts.\nbody to learn something meaningful (the pretraining loss stuck around 11). Too sparse forgetting will\nmake the body hard to adjust to the next forgetting, causing divergence as pretraining goes on.\nFigure 12: Impact of Forgetting Frequency.\nE.2\nMultilingual Pretraining and Forgetting\nOur work aim to have a flexible language model by pretraining with forgetting. No matter the\npretraining corpus is monolingual or multilingual, this language model should easily generalise itself\nto unseen languages. This is different from the scenario of multilingual PLMs like XLM-R [Conneau\net al., 2020], which requires seeing all the data for all languages from the scratch. Once done with\npretraining and there is some new language distant from the pretraining languages to support, the\nmultilingual PLMs might still struggle with zero-shot transfer as shown in several low-resources\nlanguage research [Ebrahimi et al., 2022, Adelani et al., 2021, 2022].\nNevertheless, we ran additional experiments on multilingual pretraining with forgetting. For a fair\ncomparion, we trained a multilingual RoBERTa-base of the same model size as our monolingual\nmodel. Language Emb/Task Body Adaptation refers to separately adapting embeddings with 5M\ntokens of Thai unlabelled data and adapting body with English NLI data. Task Full Model Adaptation\nrefers to adapting the full model with English NLI data. Note that Thai is already included in\nmultilingual CC100 (6B tokens in the original dataset, 720M tokens in our subsampled dataset). We\nmeasure the zero-shot Thai XNLI Accuracy.\n19\nPretrain Corpus\n#Langs\n#Params\nPretraining Method\nAdaptation Framework\nAcc\n300GB English\n1\n125M\nStandard-RoBERTa (base)\nLang Emb/Task Body\n35.3\n300GB English\n1\n125M\nForget-RoBERTa (base)\nLang Emb/Task Body\n59.7\n50GB Multilingual\n100\n125M\nStandard-RoBERTa (base)\nLang Emb/Task Body\n49.4\n50GB Multilingual\n100\n125M\nForget-RoBERTa (base)\nLang Emb/Task Body\n55.0\n50GB Multilingual\n100\n125M\nStandard-RoBERTa (base)\nTask Full Model\n60.0\n2.5TB Multilingual\n100\n270M\nXLM-RoBERTa (base)\nTask Full Model\n72.4\nWe can see that multilingual pretraining indeed helps cross-lingual transfer when the language is in\nthe pretraining data. On the other hand, we can also observe that forgetting indeed lifts the adaptation\nperformance:\n\u2022 Comparing Row 3 and Row 4 (49.4 vs 55.0), we can see that, forgetting also helps adapt\nmultilingual pretrained models.\n\u2022 Comparing Row 1 and Row 2 (35.3 vs 59.7), we can see that forgetting helps monolingual\npretrained models a lot\n\u2022 XLM-R (base) outperform best our multilingual pretrained baselines (72.4 vs 60.0). This is\nno surprise due to its large pretraining corpus (10x our multilingual corpora) and model size\n(2x our multilingual model).\nE.3\nFull Model Task Adaptation and Forgetting\nThe language/task adaptation does not use any labelled data. It only uses the unlabelled data from the\nnew language. In contrast, Standard adaptation relies on labelled data, which is expensive for a new\ndownstream language. In our case, we only found Arabic NLI data. The amount of labelled data is\nnot enough to adapt an English NLI model to Arabic NLI without proper regularization.\nOur experimental setup follows Artetxe et al. [2020], where standard-pretraining + language/task\nadaptation (MonoTrans) is shown to be competitive among a few baselines for zero-shot unsupervised\ncross-lingual transfer. On top of this finding, our proposed forgetting method can further improve the\nsample-efficiency of the language/task adaptation, reducing the amount of unsupervised data needed\nfor the new language. This is motivated by a practical scenario where the new languages contain only\nseveral thousands of tokens to a few millions of tokens (e.g. the corpus for the new language might\ncontain only 2-3 books).\nMethod\nSupervised Data\nUnsupervised Data\nAcc\nstandard pretraining + standard adapt\n6.7K\n0\n32.8\nstandard pretraining + language/task adaptation\n0\n5M\n41.2\nforget pretraining + standard adaptation\n6.7K\n0\n34.2\nforget pretraining + language/task adaptation\n0\n5M\n59.7\nE.4\nImpact of Adaptation Data Amount\nEvidence of high sample efficiency can be found by comparing the performance drop of standard\nPLMs and forgetting PLMs when the adaptation data change from a high-data setting [Artetxe et al.,\n2020, Marchisio et al., 2022] to a low-data setting that our work is considering.\nMethod\nAvg adaptation #tokens\nAvg XNLI performance\nStandard (Kelly et al 2022 [27])\n10.3B\n72\nStandard (Artetxe et al 2022 [18])\n569M\n66.7\nStandard\n5M\n53.3\nForgetting\n5M\n62.7\n20\n"
  },
  {
    "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
    "link": "https://arxiv.org/pdf/2307.00119.pdf",
    "upvote": "5",
    "text": "Meta-training with Demonstration Retrieval for\nEfficient Few-shot Learning\nAaron Mueller1\u2217\nKanika Narang2\nLambert Mathias2\nQifan Wang2\nHamed Firooz2\n1 Johns Hopkins University, Baltimore, MD\n2 Meta AI, Menlo Park, CA\namueller@jhu.edu, {kanika13,mathiasl,wqfcr,mhfirooz}@meta.com\nAbstract\nLarge language models show impressive results\non few-shot NLP tasks. However, these models\nare memory and computation-intensive. Meta-\ntraining allows one to leverage smaller mod-\nels for few-shot generalization in a domain-\ngeneral and task-agnostic manner (Min et al.,\n2022a; Wei et al., 2022; Chen et al., 2022);\nhowever, these methods alone results in mod-\nels that may not have sufficient parameteri-\nzation or knowledge to adapt quickly to a\nlarge variety of tasks. To overcome this is-\nsue, we propose meta-training with demon-\nstration retrieval, where we use a dense pas-\nsage retriever to retrieve semantically similar\nlabeled demonstrations to each example for\nmore varied supervision. By separating exter-\nnal knowledge from model parameters, we can\nuse meta-training to train parameter-efficient\nmodels that generalize well on a larger vari-\nety of tasks. We construct a meta-training set\nfrom UNIFIEDQA and CROSSFIT, and propose\na demonstration bank based on UNIFIEDQA\ntasks. To our knowledge, our work is the first\nto combine retrieval with meta-training, to use\nDPR models to retrieve demonstrations, and\nto leverage demonstrations from many tasks\nsimultaneously, rather than randomly sampling\ndemonstrations from the training set of the tar-\nget task. Our approach outperforms a variety\nof targeted parameter-efficient and retrieval-\naugmented few-shot methods on QA, NLI,\nand text classification tasks (including SQuAD,\nQNLI, and TREC). Our approach can be meta-\ntrained and fine-tuned quickly on a single GPU.\n1\nIntroduction\nLarge language models (LLMs) have become in-\ncreasingly popular due to their impressive few-\nshot performance on many NLP tasks and domains\n(Brown et al., 2020; Chowdhery et al., 2022). This\nhas resulted in many few-shot learning methods\nbased on LLMs that require ever-larger GPUs and\n\u2217Work done as an intern at Meta.\nFigure 1: Our approach. Given an input x from one\nof many possible QA tasks, we use a dense passage\nretriever to retrieve K semantically similar demonstra-\ntions Z = {zk}1,...,K from a memory bank z composed\nof labeled examples. We meta-train BART, supervising\nit to generate the (question and) answer y given x and\nZ across a diverse collection of QA tasks.\nincreasing computation. Methods requiring no pa-\nrameter updates such as in-context learning (Brown\net al., 2020) and parameter-efficient methods like\nAdapters (Houlsby et al., 2019) partially mitigate\nthese downsides, but ultimately, larger computation\nbudgets are increasingly necessary to achieve state-\nof-the-art few-shot performance\u2014even to simply\nload models and perform inference.\nMeta-learning (Vilalta and Drissi, 2002; Finn\net al., 2017) and meta-training (Min et al., 2022a)\nare methods that make smaller language models\ncapable of quicker and more robust few-shot per-\nformance across multiple tasks and domains. How-\never, smaller models may not be able to store\nenough knowledge for effective generalization in\nmany domains and tasks simultaneously. Retrieval\nis one way to overcome this: by separating para-\nmetric knowledge in the language model from ex-\nternal knowledge (stored as retrievable text), one\ncan leverage much more information than could be\nstored in the parameters of a language model. For\nexample, retrieval-augmented generation (RAG;\narXiv:2307.00119v1  [cs.CL]  30 Jun 2023\nLewis et al., 2020) and retrieval-enhanced trans-\nformers (RETRO; Borgeaud et al., 2022) retrieve\nnatural language passages to improve performance\non knowledge-intensive NLP tasks, although they\ndo not perform meta-learning or meta-training\nand only evaluate on high-resource knowledge-\nintensive tasks.\nWe thus propose meta-training with demon-\nstration retrieval as a more parameter-efficient\nway to leverage demonstrations for few-shot learn-\ning.\nWe retrieve semantically similar labeled\ndemonstrations for each training and test exam-\nple during meta-training and fine-tuning.\nOn\na relatively small sequence-to-sequence model\n(BARTlarge, 440M parameters), we show our pro-\nposed approach is capable of generalizing quickly\nand well on a variety of downstream tasks (Table 1).\nInspired by retrieval-augmented generation (RAG)\nmodels (Lewis et al., 2020), we use a dense passage\nretriever (DPR; Karpukhin et al., 2020) to retrieve\ndemonstrations instead of Wikipedia passages. We\nretrieve semantically similar demonstrations from\na large and diverse bank (\u00a73.3) that is compiled\nfrom many existing question answering tasks (App.\nA), rather than randomly sampling demonstrations\nfrom the training set of the target task like most\ncontemporary work (Min et al., 2022a; Brown et al.,\n2020; Gao et al., 2021).\nOur experiments show that our method (\u00a73) out-\nperforms tailored efficient few-shot baselines and\nother retrieval-augmented models on various tasks,\nincluding natural language inference (NLI), para-\nphrase detection, and extractive question answer-\ning (\u00a75). To our knowledge, our work is the first\nto combine retrieval with meta-training (or multi-\ntask training more broadly), to use DPR models\nto retrieve demonstrations, and to leverage demon-\nstrations from many tasks simultaneously, rather\nthan retrieving random or k-nearest demonstrations\nfrom the training set of the target task.\nOur code is available on GitHub.1\n2\nRelated Work\nMeta-learning (Vilalta and Drissi, 2002; Finn\net al., 2017) is a class of methods that supervise\na model on how to learn; the goal is to leverage\na collection of meta-training tasks to learn a bet-\nter learning algorithm that generalizes to held-out\ntasks. Inspired by meta-learning, some recent stud-\n1https://github.com/facebookresearch/\nmetatrained-demRAG\nies have attempted to induce specific abilities in\nlanguage models in a task- and domain-agnostic\nmanner via meta-training; this entails directly\nsupervising a model on labeled examples from\nvarious tasks (sometimes using some controlled\nformat or template (Chen et al., 2022; Wei et al.,\n2022)) to directly induce specific abilities or better\ninductive biases that improve generalization. Meta-\ntraining is typically accomplished via some form\nof controlled multi-task learning, as in Min et al.\n(2022a). Many studies have explored multi-task\nand multi-domain learning (Khashabi et al., 2020;\nZhong et al., 2021; Aghajanyan et al., 2021; Ye\net al., 2021; Wei et al., 2022), but these studies\noften leverage tasks that improve a model\u2019s abili-\nties for some specific (set of) downstream tasks. In\nmeta-training, we aim to directly improve the learn-\ning algorithm via controlled supervision, which\nshould improve out-of-distribution generalization\nby teaching a model some helpful ability\u2014such\nas in-context learning\u2014that can result in gains on\nvarious downstream tasks (Min et al., 2022a). We\nfocus on meta-training with examples from QA\ndatasets.\nFew-shot learning is a common setting in\nwhich a model is supervised on only a few la-\nbeled examples. Many methods for improving few-\nshot performance are based on scaling model and\ndata size (Brown et al., 2020; Chowdhery et al.,\n2022). Our goal is to improve few-shot perfor-\nmance across tasks in a computation- and memory-\nefficient manner, so we focus on smaller models\nthat can be trained efficiently on a single GPU.\nSome parameter-efficient few-shot methods have\nbeen proposed, including cloze-style prompting\n(Schick and Sch\u00fctze, 2021b), fine-tuning with man-\nually tuned (Schick and Sch\u00fctze, 2021a) and auto-\nmatically tuned prompts and demonstrations (Gao\net al., 2021), and meta-learning (Yu et al., 2018;\nBansal et al., 2020; Bao et al., 2020). One advan-\ntage of our approach is that it does not require sig-\nnificant prompt tuning: rather, we standardize all\nof our tasks into a single format, similar to Chada\nand Natarajan (2021). This saves human time and\ncomputational resources.\nCrucially, these approaches compare probabil-\nities of single tokens or small pre-selected label\nsets; thus, they cannot be used for open-domain\ntasks like question answering. Some work has\nproposed generative few-shot methods for open-\ndomain tasks: this includes reformatting the input\ndata to match a model\u2019s pre-training format (Chada\nand Natarajan, 2021), pre-training models to select\nrelevant spans from context passages (Ram et al.,\n2021), and running a secondary pre-training step\non labeled classification data (Mueller et al., 2022).\nOur model should be effective on many tasks, even\nwhen the label space is large and differs across ex-\namples; thus, our method is based on a generative\nsequence-to-sequence model.\nIn-context learning (ICL; Brown et al., 2020)\nis increasingly used in few-shot methods; here, la-\nbeled demonstrations are concatenated to the same\ncontext as a test example to teach a model how to\nperform a task without additional gradient updates.\nStudies have analyzed what kinds of demonstra-\ntions are most effective (Liu et al., 2022), as well\nas what makes demonstrations effective (Min et al.,\n2022b; Xie et al., 2022). Our demonstration re-\ntrieval approach is most similar to Liu et al. (2022),\nwho encode demonstrations and test examples into\na sentence embedding space and retrieve the k-\nnearest demonstrations. Our method differs in mul-\ntiple ways: we use dense passage retrievers instead\nof sentence embeddings; we use demonstrations\nfrom many training sets instead of the training set\nof the target task; and we perform gradient updates\nwith demonstrations, which is more feasible on our\nrelatively small BARTlarge-based model.\nWei et al. (2022) find that very large LMs (>68B\nparameters) are required for ICL to be effective, but\nMin et al. (2022a) find that meta-training can be\nused to make a much smaller model (GPT2large,\n774M parameters) capable of leveraging demon-\nstrations. Here, we make BARTlarge (440M param-\neters) better at leveraging demonstrations through\nmeta-training with demonstrations, like Min et al.\n(2022a); however, their method is designed for\nzero-shot generalization, and it selects from a con-\nstrained set of pre-defined labels. Our method is\ndesigned for few-shot settings and can be applied\nto open-domain tasks.\nRetrieval-augmented generation models con-\nsist of two components: generators and retriev-\ners.\nThe generator is typically a decoder-only\nLM (Guu et al., 2020) or sequence-to-sequence\n(seq2seq) model (Lewis et al., 2020; Izacard and\nGrave, 2021); we use seq2seq models. The re-\ntriever is most often a dense passage retrieval\n(DPR; Karpukhin et al., 2020) model based on\nBERTbase. RAG models are typically evaluated\non knowledge-intensive tasks like abstractive QA\nand fact verification. Thus, the memory bank typ-\nically consists of Wikipedia passages, which aug-\nments the model with additional factual knowledge\nseparate from the generator\u2019s parameters. Izacard\net al. (2022) adapts this architecture for few-shot\nknowledge-intensive tasks using a very large gen-\nerator (T5X(X)L) and a Contriever-based (Izacard\net al., 2021) retriever. However, we are interested\nin more general-purpose methods, as well as more\nparameter- and memory-efficient methods that train\nor fine-tune quickly on a single GPU. Thus, we pro-\npose a task-agnostic and domain-general method\nto improve smaller generative models for few-shot\nsettings: specifically, a retrieval-augmented meta-\ntraining step and a memory bank of labeled QA\ndemonstrations instead of Wikipedia passages.\n3\nMethod\n3.1\nRetrieval-augmented Generation\nAs we wish to retrieve similar labeled examples\nfor every input, our architecture takes inspiration\nfrom retrieval-augmented generation (RAG) mod-\nels (Lewis et al., 2020), which consist of a pre-\ntrained sequence-to-sequence component (we use\nBARTlarge) and a pre-trained dense passage re-\ntriever (DPR) component. Given an input x, the\nDPR component retrieves the K most semantically\nsimilar memory entries {zk}1,...,K from the mem-\nory bank z. Retrieval is performed using a BERT-\nbased input encoder EI on x and BERT-based\ndemonstration encoder ED on z to encode both\ninto a vector space, and then running maximum\ninner product search:2\n{zk}1,...,K = top-K\nz\u2208z\nn\nEI(x)\u22a4ED(z))\no\n(1)\nThe DPR component also returns the inner products\nthemselves as document scores p\u03b7(zk|x).\nThe input and retrieved entries are then passed\nto a pre-trained sequence-to-sequence model,\nBARTlarge, for autoregressive generation. At each\ntimestep, we marginalize over the retrieved demon-\nstrations by creating K separate input contexts,\nconsisting of the input x and one retrieved entry\nzk. We then sum over BART\u2019s token probabilities\np\u03b8 given each context, weighted by zk\u2019s document\n2Maximum inner product search can be approximately\nsolved in sub-linear time (Johnson et al., 2021).\nWe\nuse the faiss library for this:\nhttps://github.com/\nfacebookresearch/faiss\nCategory\nDataset\nType\n#Train\n#Test\nL\n|Y|\nExtractive QA\nSQuAD (Rajpurkar et al., 2016)\nOpen QA\n86,588\n10,507\n10 / 120\n-\nBioASQ (Tsatsaronis et al., 2015)\nOpen QA\n24,559\n1,504\n10 / 200\n-\nQASC (Khot et al., 2020)\nMulti-choice QA\n8,134\n926\n8 / 18\n-\nKnowledge-\nintensive QA\nTriviaQA (Joshi et al., 2017)\nOpen QA\n61,688\n7,785\n13 / 677\n-\nTextbookQA (Kembhavi et al., 2017)\nOpen QA\n15,154\n1,503\n10 / 581\n-\nClassification\nTREC (Voorhees and Tice, 2000)\nQuestion class.\n5,452\n500\n10\n6\nMRPC (Dolan and Brockett, 2005)\nParaphrase class.\n3,668\n408\n22 / 21\n2\nMNLI (Williams et al., 2018)\nNLI\n392,702\n9,815\n22 / 11\n3\nMNLI-mm (ibid.)\nNLI\n392,702\n9,832\n22 / 11\n3\nQNLI (Wang et al., 2018)\nNLI\n104,743\n5,463\n11 / 30\n3\nTable 1: Evaluation sets used in this study. L: mean # of words in question/context or input sentence(s). For more\nstraightforward comparison to prior few-shot question answering and classification methods, we use Ram et al.\n(2021)\u2019s few-shot splits of SQuAD and BioASQ derived from MRQA, as well as Gao et al. (2021)\u2019s splits of TREC,\nMRPC, MNLI(-mm), and QNLI. We generate our own few-shot splits of QASC using 5 random seeds for each split\nsize.\nscore:3\np(y|x) \u2248\nN\nY\ni\nK\nX\nk\np\u03b7(zk|x)p\u03b8(y|x, zk, y1:i\u22121) (2)\n3.2\nMeta-training\nTo adapt a sequence-to-sequence model for general-\npurpose demonstration retrieval and answer gener-\nation, we perform a meta-training step by supervis-\ning the model with demonstrations on a collection4\nof 18 QA tasks (Table 7). We update the parame-\nters of the BART component of our model during\nmeta-training by supervising BART (using its nor-\nmal cross-entropy loss) to generate the question\nand its answer given the question and a set of re-\ntrieved demonstrations. We use QA tasks due to the\nsemantic diversity of inputs and labels; compare\nto text classification tasks, where the label space is\nmuch smaller and labels are often less informative.\nWe modify and use the QA meta-training task\ncollections from (Min et al., 2022a).This consists\nof various extractive, multiple-choice, and/or ab-\nstractive QA tasks from CROSSFIT and a subsam-\nple of UNIFIEDQA (Khashabi et al., 2020, 2022),\nincluding NaturalQuestions, MCTest, BIOMRC,\ninter alia. We modify the meta-training collec-\ntions by (1) removing our evaluation sets if they are\npresent,5 and (2) standardizing the format of each\n3This is similar to the RAG-Token approach in Lewis et al.\n(2020). The number of demonstrations we can use is not\nlimited by the context length since we marginalize over each\ndemonstration in its own separate context.\n4Throughout this study, we use \u201ctask\u201d to refer to a single\ndataset like SQuAD or NaturalQuestions, and \u201ccollection\u201d to\nrefer to the dataset obtained by concatenating a set of tasks.\n5We also remove any examples where the question has a\nJaccard similarity > 0.9 with any training or test question\ntask. Our final meta-training collection contains\n32 tasks, which we subsample to 18 tasks based\non semantic similarity to our evaluation tasks; see\nAppendix A for a full list of tasks and details on\nour semantic subsampling procedure, and \u00a75.2 for\na description of the downstream effect of semantic\nsubsampling.\nFollowing Chada and Natarajan (2021), we\nstandardize each input in the meta-training data\nto a \u201cquestion:...\n\\n answer: [MASK] \\n\ncontext:...\u201d\nformat.\nThen, the output se-\nquence consists of both the question and answer\nsequences,6 which aligns with BART\u2019s pre-training\nobjective of reconstructing the entire input se-\nquence (not just masked spans). Like Chada and\nNatarajan (2021), we find that aligning the in-\nput/output format with BART\u2019s pre-training objec-\ntive makes a positive difference for downstream per-\nformance. For QASC, which is a multiple-choice\nQA task, we put all of the answer options in the\ncontext field before the two context sentences and\ngenerate the full answer string. This outperformed\nall other formats we tried by a significant margin.7\nFor classification tasks, we use the same\nquestion/answer/context format. For our single-\nsentence classification task (TREC), we place the\ninput in the question field, and present all of the\npossible labels in the context field using a similar\nformat as for QASC. For sentence-pair classifica-\nin our evaluation tasks, and where the answers are the same;\nonly 4 such examples existed in our data.\n6We only compute F1 on the answer sequences.\n7We tried placing the answer options in the question field,\nnot including the answer options at all, and only generating the\nletter label instead of the full answer string. See Appendix B\nfor examples and scores.\ntion tasks (MRPC, MNLI(-mm), QNLI), we place\nthe first sentence or hypothesis in the question\nfield and place the second sentence or premise in\nthe context field. As with QA tasks, we generate\nboth the question and answer fields in the target se-\nquence, but only evaluate F1 on answer sequences.\n3.3\nDemonstration Memory\nFor the demonstration memory bank, we use train-\ning sets from UNIFIEDQA, excluding our evalu-\nation tasks; the memory contains examples from\n16 tasks. UnifiedQA has approximately 40% over-\nlap with the QA meta-training collection, and no\noverlap with the non-QA collection. See Table 8\nin Appendix A for a full list of tasks in our demon-\nstration memory bank.\nWe format each demonstration in the memory\nbank in the same question/answer/context format as\ndescribed above, except that demonstrations have\nthe ground-truth label after the answer: header in-\nstead of a [MASK] token. Note that memory entries\nconsist of a text passage (the demonstration) and a\ntitle; for the title, we simply use the answer to the\nquestion.\n4\nExperimental Setup\nWe evaluate on a variety of QA and classification\ntasks (Table 1). We select open-domain QA tasks\nfrom the MRQA shared task (Fisch et al., 2019) to\nreflect a variety of extractive QA formats, including\na standard QA benchmark (SQuAD), a domain-\nspecific challenging benchmark (BioASQ), and\ntwo knowledge-intensive QA benchmarks (Triv-\niaQA and TextbookQA).8 Our few-shot QA splits\nof size {16, 32, 64, 128} for these tasks are from\nRam et al. (2021), which are themselves derived\nfrom MRQA (Fisch et al., 2019). We also gener-\nate few-shot splits for QASC, which is a multiple-\nchoice QA task; we evaluate on QASC to determine\nwhether our model is also effective in dealing with\nmuch shorter contexts, and to ensure that it is not\noverfitting to more typical MRQA-style extractive\ntasks.\nOur few-shot classification task splits are from\nGao et al. (2021). We evaluate on sentence pair\n8While \u201cknowledge-intensive\u201d does not have a standard\ndefinition or straightforward measurement, the length of the\ncontexts may act as a proxy for how knowledge-intensive\na question answering task is. Contexts for our knowledge-\nintensive tasks are much longer, and thus require a model to\nsynthesize much more information and/or retrieve information\nthat is more relevant to the inputs to semantically prime the\nmodel for question-specific information.\nclassification tasks which are not contained in our\nmeta-training or demonstration tasks; sentence pair\nclassification tasks like natural language inference\n(NLI) and paraphrase classification can be eas-\nily reformatted to our question/answer/context for-\nmat. We also evaluate on TREC, which is a single-\nsentence text classification task where the model\nmust guess the category of the answer to a ques-\ntion (e.g., human, location, number), rather than\nthe answer itself.\nFor each task and few-shot split size, we average\nscores across 5 random few-shot samples.\n4.1\nBaselines\nWe compare against strong efficient few-shot meth-\nods, as well as similar models that will tell us why\nour method performs better. Note that our approach\nis generative, unlike iPET and LM-BFF; thus, it is\nusable on a wider variety of tasks.\nFewshotQA (Chada and Natarajan, 2021). A\nfew-shot question answering method. We com-\npare to the FewshotBARTL model, which is based\non BARTlarge like our model and is the best-\nperforming variant. We use the same few-shot\nsplits such that we can directly compare to the\nnumbers reported in that paper. We also try meta-\ntraining this non-retrieval-augmented model, which\nis essentially our method without retrieval; we call\nthis baseline FewshotQA-m.\nSplinter (Ram et al., 2021). A few-shot question\nanswering model pre-trained to select salient spans\nfrom context passages.\nRAG (Lewis et al., 2020). The original RAG-\nToken model with a memory of Wikipedia pas-\nsages.\nWe use the released model fine-tuned\non NaturalQuestions (NQ), as this was the best-\nperforming RAG model on our tasks.\nTo see\nwhether our demonstration memory is more effec-\ntive than Wikipedia passages when meta-training,\nwe also try meta-training the RAG model with its\nWikipedia memory; we call this baseline RAG-m.\niPET (Schick and Sch\u00fctze, 2021b). A manual\nprompt-tuning approach that induces better few-\nshot performance than GPT3 with much smaller\nLMs. We tune the best-performing ALBERTxxl\n(Lan et al., 2020) model on our tasks.\nLM-BFF (Gao et al., 2021).\nAn automatic\nprompt-tuning approach based on RoBERTalarge\n(Liu et al., 2019). It requires no unlabeled text data\nto work well, unlike iPET. This model and iPET\ncompare token probabilities to perform classifica-\n16\n32\n64\n128\n# Examples\n30\n40\n50\n60\n70\n80\nF1\nlegend\nSplinter\nFewshotQA\nFewshotQA-m\nRAG\nRAG-m\nOurs\n16\n32\n64\n128\n# Examples\n50\n55\n60\n65\n70\n75\n80\n85\nF1\nSQuAD\n16\n32\n64\n128\n# Examples\n30\n40\n50\n60\n70\n80\nF1\nBioASQ\n16\n32\n64\n128\n# Examples\n75\n80\n85\n90\n95\nF1\nQASC\nFigure 2: F1 scores at each few-shot split size for extractive and multiple-choice question answering evaluation tasks.\nScores are averaged across 5 random few-shot samples. Our model outperforms or maintains similar performance\nto the strongest baselines on each task and split size. Performance gains on SQuAD are especially large\u2014up to 3.9\nF1 (4.9% improvement). FewshotQA and Splinter scores are from Chada and Natarajan (2021).\ntion, so we cannot use them for open-domain tasks\nlike question answering. Thus, we only compare to\nthese models on classification.\n4.2\nHyperparameters\nFor meta-training, we use hyperparameters from\nMin et al. (2022a) where possible:\ninit. LR\n1\u00d710\u22125, effective batch size 8,9 training for a max-\nimum of 30,000 steps. We checkpoint every 2,000\nsteps and select the checkpoint with the lowest\nmean loss on our 16-shot QA training sets. Meta-\ntraining finishes in \u224814 hours on 1 A100 GPU\n(40GB).10\nFor fine-tuning, we use hyperparameters from\nChada and Natarajan (2021) where possible: init.\nLR 2 \u00d7 10\u22125, batch size 4, fine-tuning for a max-\nimum of 1,000 steps or 35 epochs (whichever is\nlarger). We checkpoint every 2 epochs and select\nthe checkpoint with the highest exact match on the\ntraining set. Fine-tuning finishes in 30\u201360 minutes\non 1 A100 GPU (40GB).\nFor each meta-training and fine-tuning input, we\nretrieve 5 demonstrations from the memory.11\n5\nResults\nOur model\u2019s F1 scores for extractive question an-\nswering (Figure 2) are higher than models of sim-\nilar parameterizations, including similar models\nthat have been meta-trained using the same train-\ning data. Our model also outperforms strong clas-\n9We use gradient accumulation to get this effective batch\nsize on a single GPU.\n10Our model could also be trained and tuned on a cheaper\n32GB GPU (e.g., a V100) in similar time.\n11Higher values result in better performance (\u00a75.2), but\nthis saturates at 5\u201310 retrieved demonstrations, and retrieving\nmore demonstrations slows down training.\nTREC\nMNLI\nMNLI-mm\nQNLI\nMRPC\nAvg.\nMajority\n18.8\n32.7\n33.3\n49.5\n81.2\n43.1\nRoBERTa\n*88.82.1\n*45.86.4\n*47.86.8\n*60.26.5\n76.62.5\n63.8\niPET\n*85.04.1\n71.21.7\n71.82.6\n*70.36.2\n70.44.7\n73.7\nLM-BFF\n*89.41.7\n70.71.3\n*72.01.2\n*69.21.9\n*78.13.4\n75.9\nFewshotQA\n91.02.0\n*47.96.3\n*46.15.9\n*61.06.4\n*67.64.8\n62.7\nFewshotQA-m\n92.41.4\n*50.11.0\n*50.62.5\n*71.82.1\n74.03.7\n67.8\nRAG\n*81.12.0\n*62.40.9\n*61.81.2\n*74.91.5\n70.23.3\n70.1\nRAG-m\n*87.81.7\n*70.01.4\n69.11.4\n*83.21.5\n74.92.8\n77.0\nOurs\n91.71.3\n72.91.7\n69.61.4\n84.41.8\n73.42.5\n78.4\nTable 2: Accuracies on classification tasks, averaged\nacross 5 random few-shot samples (std. dev. in sub-\nscript). All datasets are well-balanced except MRPC;\nthus, we report accuracies for all tasks except MRPC,\nwhere we report macro-F1. LM-BFF and RoBERTa\nscores are from Gao et al. (2021). * indicates that\np < .05 in a t-test between our model\u2019s score and the\nmarked score.\nsification approaches on TREC, MNLI, and QNLI\n(Table 2). Thus, meta-training with semantically\nsimilar demonstrations induces a more general-\npurpose system that can perform well across a\nvariety of low-resource downstream tasks.\nContrast this with RAG, which often performs\nworst out of each model we test across tasks. Thus,\nthe architecture itself is not inherently strong in\nfew-shot settings, suggesting that meta-training\nmakes a significant contribution to increased perfor-\nmance. This is also supported by the increased per-\nformance we observe with FewshotQA and RAG\nafter meta-training, though note that meta-training\ndoes not help FewshotQA to the same extent it\nhelps retrieval-augmented models. Also note that\nFewshotQA does not perform well on classification\ntasks, whereas our method achieves performance\nexceeding or close to the strongest baselines. This\nmeans that the combination of meta-training and re-\ntrieval enables a more general-purpose model than\n16\n32\n64\n128\n# Examples\n30\n40\n50\n60\n70\n80\nF1\nlegend\nSplinter\nFewshotQA\nFewshotQA-m\nRAG\nRAG-m\nOurs\n16\n32\n64\n128\n# Examples\n20\n30\n40\n50\n60\n70\nF1\nTriviaQA\n16\n32\n64\n128\n# Examples\n20\n25\n30\n35\n40\n45\n50\nF1\nTextbookQA\nFigure 3: F1 scores for each few-shot split size for\nknowledge-intensive question answering tasks. Our\nmodel is outperformed by a strong few-shot QA base-\nline, though meta-training still greatly improves perfor-\nmance.\neither of these components separately.\nWith meta-training, RAG-m obtains perfor-\nmance much closer to our model. This tells us that\nmeta-training is responsible for much of the per-\nformance gains we observe, though the demon-\nstration memory bank also improves performance\nto a lesser extent. On MRPC, RAG-m outperforms\nour model, indicating that there exist some non-\nknowledge-intensive tasks where Wikipedia pas-\nsages are more helpful than QA demonstrations.\n5.1\nKnowledge-intensive QA\nWe also evaluate on few-shot knowledge-intensive\nQA tasks (Figure 3): here, TriviaQA and Text-\nbookQA, using the few-shot splits from the MRQA\nshared task. While these are also technically extrac-\ntive QA tasks, their contexts have an average length\nof 677 and 581 words, respectively, meaning that\nBART will likely struggle more to synthesize all of\nthe information in these tasks (even with retrieval).\nWe find that FewshotQA outperforms our method\non both of these tasks, and that even Splinter out-\nperforms our method at larger split sizes for Text-\nbookQA. This means that demonstration retrieval\nModel\nSQuAD\nBioASQ\nQASC\nTriviaQA\nTbQA\nFewshotQA\n68.9\n63.0\n82.6\n65.2\n37.7\nFewshotQA-m\n76.6\n63.4\n85.9\n65.9\n38.2\nRAG-m\n80.0\n62.9\n88.9\n66.6\n27.7\nOurs\n83.9\n64.7\n89.2\n62.9\n37.2\nOurs (oracle)\n93.5\n94.2\n99.1\n80.7\n83.2\nTable 3: F1 scores on QA tasks for our strongest base-\nlines, our approach, and our approach where the mem-\nory has been replaced with labeled test examples (ora-\ncle). The oracle approach establishes an approximate\nupper bound for our model. Large gaps between our\napproach and the oracle indicate room for improvement\nin what constitutes our memory bank.\nmay be actively harmful for these tasks. Thus, our\nmeta-training method is optimizing RAG architec-\ntures for non-knowledge-intensive tasks, but not for\nknowledge-intensive tasks. Wikipedia passages are\nmore effective than demonstrations in the memory\nbank for TriviaQA as well, as indicated by RAG-m\noutperforming our approach.\nHowever, meta-training with or without the\nmemory bank still induces far better performance\nthan the base RAG model, which performs worse\nthan all baselines except Splinter. Thus, our method\nis still improving over RAG, making this model\nmore versatile and better able to handle such tasks\neven if it is not the optimal approach.\n5.2\nAblations\nHere, we perform further analyses to understand\nthe contribution of individual model components\nand (meta-)training decisions.\nMemory bank. We find that performance is\ngenerally higher for question answering and clas-\nsification when retrieving demonstrations instead\nof Wikipedia passages, as in Figure 2 and Table 2.\nThis raises two questions: how much could the\nmemory bank impact downstream performance in\nthe best-case scenario? Relatedly, what is the upper\nbound on performance for our model given the best\npossible demonstration memory bank?\nTo obtain an estimate, we create an oracle mem-\nory consisting of labeled test examples from our\nevaluation data. We find that scores significantly\nimprove over our method and others in this setting,\nindicating that this architecture has significant\npotential to achieve further gains if the memory\nbank is improved.\nNumber of retrieved demonstrations.\nIs\nretrieving\nmore\ndemonstrations\nalways\nbet-\nter? We compare performance when retrieving\n0 5\n25\n50\n100\nNumber of Demonstrations\n78\n79\n80\n81\n82\n83\n84\n85\nF1\nSQuAD\n0 5\n25\n50\n100\nNumber of Demonstrations\n67\n68\n69\n70\n71\n72\n73\nF1\nMNLI\nFigure 4: F1 scores for an extractive QA (SQuAD) and\nsentence pair classification (MNLI) task by the number\nof retrieved demonstrations ({0, 1, 5, 10, 25, 50, 100})\nduring fine-tuning.\nScores generally increase with\nthe number of retrieved demonstrations, though per-\nformance saturates early at 5\u201310 demonstrations.\nFigure 5:\nF1 scores for non-knowledge-intensive\n(SQuAD, left) and knowledge-intensive (TriviaQA,\nright) QA tasks by the frequency of the true answer\nstring in the retrieved demonstrations. While not mono-\ntonic, there is a clear correlation between these variables,\nindicating that lexical features may be responsible for\nmuch of retrieval\u2019s contributions to performance.\nK = {0, 1, 5, 10, 25, 50} demonstrations dur-\ning fine-tuning and evaluation on non-knowledge-\nintensive QA (SQuAD) and sentence-pair classifi-\ncation (MNLI). Our results (Figure 4) show that F1\nscores begin to saturate at 5\u201310 demonstrations for\nboth tasks. However, using more demonstrations\ngenerally does not harm performance; the model is\nable to handle less helpful demonstrations without\nperformance decreasing significantly.\nWhy is retrieval helpful? Is the model abstract-\ning semantic content from the retrieved demon-\nstrations for improved performance, or is it simply\nlearning to copy token sequences from the retrieved\ndemonstrations? As an initial test, we can correlate\nthe frequency of the ground-truth answer sequence\nin the retrieved documents with F1 scores on our\nQA tasks. Our results (Figure 5) suggest that the\nmodel is indeed learning to retrieve certain text\nstrings from the demonstrations. This provides one\npossible path forward for improving the memory\nbank: higher semantic overlap with one\u2019s evalua-\ntion task increases the likelihood of these overlaps,\nso future work could focus on collecting (or per-\nhaps generating) more semantically similar demon-\nRetriever\nSQuAD\nBioASQ\nQASC\nTriviaQA\nTbQA\nRandom\n1.8\n1.5\n1.2\n1.8\n2.3\nDPR (Wiki)\n11.5\n1.8\n15.7\n4.9\n24.3\nDPR (PAQ)\n16.9\n1.5\n26.1\n29.3\n24.0\nContriever\n14.1\n7.3\n28.0\n27.9\n24.3\nTable 4: The proportion of test examples for which each\nretriever retrieves at least 1 demonstration containing\nthe ground-truth answer as a substring. DPR (PAQ)\nand Contriever appear to be better at retrieving more\nrelevant demonstrations on average, though this does\nnot necessarily lead to higher downstream performance\n(Table 5).\nRetriever\nSQuAD\nBioASQ\nQASC\nTriviaQA\nTbQA\nRandom\n74.3\n61.8\n88.7\n56.5\n29.6\nDPR (Wiki)\n83.9\n64.7\n89.2\n62.9\n37.2\nDPR (PAQ)\n78.8\n63.5\n86.8\n57.6\n33.5\nContriever\n81.1\n62.5\n88.7\n58.9\n32.4\nTable 5: F1 scores on 16-shot extractive QA tasks\nacross retrievers. We fine-tune with different retriev-\ners given the same (best) meta-trained model. Despite\nDPR (Wiki)\u2019s lower retriever scores (Table 4), its down-\nstream performance is the best among the retrievers we\ntry.\nstrations that feature more lexical overlaps.\nHowever, this does not explain how retrieval im-\nproves performance on classification tasks, where\nthe label space is small and labels are less infor-\nmative. For NLI, the label space includes \u201centail-\nment\u201d/\u201cneutral\u201d/\u201ccontradiction\u201d, which we would\nnot expect to see often in our demonstrations and\nwhich do not carry significant semantic content.\nYet retrieval-augmented models outperform Few-\nshotQA by a large margin on MNLI(-mm), so what\nis helping our model? There could exist some\nQA demonstrations which semantically prime our\nmodel toward correct completions, though sentence\nembedding similarity may not capture this help-\nfulness. Future work could ablate over specific\nfeatures in the demonstrations.\nWhat type of retriever is best?\nFor our exper-\niments thus far, we have used the DPR compo-\nnent of the RAG-Token (NQ) model, which is\npre-trained on Wikipedia and fine-tuned on Nat-\nuralQuestions. Is this an optimal starting point, or\nwould some other retreiver be better? We compare\nagainst a DPR model pre-trained on the Probably-\nAsked Questions (PAQ; Lewis et al., 2021) dataset,\nas well as the Contriever model (Izacard et al.,\n2021). Contrievers are unsupervised, whereas DPR\nmodels receive explicit supervision during pre-\nMemory\nSQuAD\nBioASQ\nQASC\nTriviaQA\nTbQA\nAll tasks\n83.5\n63.2\n89.2\n61.4\n36.8\nSemantically similar tasks\n83.9\n64.7\n89.2\n62.9\n37.2\nTable 6: 16-shot F1 scores on QA tasks after meta-\ntraining on either all QA tasks from MetaICL\u2019s QA\nmeta-training collection, or QA tasks subsampled by\nsemantic similarity to our evaluation tasks. A full list of\nmeta-training tasks can be found in Appendix A.\ntraining. DPR tends to perform better when the\ndownstream task is similar to the pre-training or\nfine-tuning data; however, in our case, demonstra-\ntion retrieval is dissimilar from Wikipedia passage\nretrieval, and Contriever may handle larger train-\ntest shifts better (Izacard et al., 2021).\nWe evaluate both the relevance of the retrieved\ndemonstrations (Table 4) and downstream F1 (Ta-\nble 5) on our QA tasks. We find that DPR (PAQ)\nand Contriever are both better at retrieving similar\ndemonstrations, as measured by the frequency with\nwhich they retrieve examples that contain the an-\nswer. For BioASQ, only Contriever retrieves more\nrelevant demonstrations than a random retriever.\nHowever, retrieving more relevant demonstra-\ntions does not translate into increased downstream\nperformance: DPR (Wiki) consistently outper-\nforms the others. Why? Through qualitative anal-\nysis, we find that DPR (Wiki) retrieves more se-\nmantically diverse demonstrations, whereas DPR\n(PAQ) and Contriever retrieve demonstrations that\nare technically more similar to the test example,\nbut also less diverse across test examples.Thus,\nthere should be a balance between diversity and\nrelevance: completely random retrieval is not effec-\ntive (as indicated by our random retrieval baseline\nscoring worst), but neither is the more constrained\ndemonstration set we retrieve using an arguably\nmore optimal retriever.\nMeta-training data. Is meta-training helpful\nbecause of the variety of tasks included in our\nsetup (the more is better hypothesis), or would it\nbe better to select meta-training data in a more\nprincipled way (the similar datasets are better\nhypothesis)?\nWe compare downstream perfor-\nmance when meta-training on all QA tasks from\nMetaICL versus the top tasks by mean instance-\nlevel semantic similarity to our evaluation tasks\n(Table 6). To compute semantic similarity, we\nuse the stsb-roberta-base-v2 model from Sen-\ntenceTransformers (Reimers and Gurevych, 2019)\nand compute the mean pairwise cosine similarity\nbetween the 16-shot training examples in our evalu-\nation tasks and all examples in a meta-training task.\nWe then select the top tasks by similarity until we\nhave over 240,000 examples (enough for 30,000\ntraining steps using batch size 8). See Appendix A\nfor a list of meta-training tasks before and after\nsubsampling.\nWe find that selecting meta-training data\nbased on semantic similarity to our evaluation\ntasks is helpful for both our QA and non-QA\ntasks: F1 increases across tasks when only meta-\ntraining on the most similar data. This contrasts\nwith the findings of Min et al. (2022a), who find\nthat more meta-training tasks is generally better.\n6\nConclusions\nWe have proposed a meta-training method (\u00a73.2)\nthat retrieves (\u00a73.1) semantically similar demon-\nstrations from a diverse demonstration bank (\u00a73.3).\nOur method achieves higher performance on aver-\nage across many tasks than other strong parameter-\nefficient few-shot baselines (\u00a75). In future work,\none could explore a mixture of demonstration\nretrieval and passage retrieval for improved per-\nformance on a wider variety of tasks\u2014including\nknowledge-intensive tasks.\nLimitations\nOur method requires access to a large set of la-\nbeled examples for the memory bank\u2014ideally with\nsome relevance to the evaluation tasks. This limits\nthe languages and tasks that are optimal for this\nmethod: there does not exist a large variety of train-\ning examples for low-resource language varieties,\nnor for certain much more specific tasks\u2014as in,\nfor example, industry applications with domain-\nspecific customer data. And while multilingual\nmodels could leverage cross-lingual transfer, it is\nunclear how well this model would generalize into\nlow-resource languages when (for example) using\nmultilingual BART.\nWhen using the full demonstration memory,\nmeta-training does not run on a 16GB GPU us-\ning our current implementation. While this does\nexclude more common GPUs, our approach could\nstill run quickly on a 32GB GPU in a few hours,\nthus costing far less than pre-training a language\nmodel of comparable few-shot performance from\nscratch.\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava,\nXilun Chen, Luke Zettlemoyer, and Sonal Gupta.\n2021. Muppet: Massive multi-task representations\nwith pre-finetuning. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 5799\u20135811, Online and Punta\nCana, Dominican Republic. Association for Com-\nputational Linguistics.\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020. Self-supervised meta-\nlearning for few-shot natural language classification\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 522\u2013534, Online. Association for\nComputational Linguistics.\nYujia Bao, Menghua Wu, Shiyu Chang, and Regina\nBarzilay. 2020. Few-shot text classification with dis-\ntributional signatures. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nRakesh Chada and Pradeep Natarajan. 2021.\nFew-\nshotQA: A simple framework for few-shot learning\nof question answering tasks using pre-trained text-to-\ntext models. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6081\u20136090, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\nand He He. 2022. Meta-learning via language model\nin-context tuning. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 719\u2013730,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1126\u20131135. PMLR.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of the 2nd Work-\nshop on Machine Reading for Question Answering,\nMRQA@EMNLP 2019, Hong Kong, China, Novem-\nber 4, 2019, pages 1\u201313. Association for Computa-\ntional Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816\u20133830, Online. Association for Computa-\ntional Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929\u20133938. PMLR.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pages 2790\u20132799.\nPMLR.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learning.\nCoRR, abs/2112.09118.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 874\u2013\n880. Association for Computational Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot learning with retrieval aug-\nmented language models. CoRR, abs/2208.03299.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2021.\nBillion-scale similarity search with gpus.\nIEEE\nTrans. Big Data, 7(3):535\u2013547.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601\u20131611, Vancouver,\nCanada. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Ha-\njishirzi. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal machine\ncomprehension. In 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n5376\u20135384.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-\njishirzi. 2022.\nUnifiedqa-v2:\nStronger general-\nization via broader cross-format training.\nCoRR,\nabs/2202.12359.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single QA system. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1896\u20131907.\nAssociation for Computational Linguistics.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020.\nQasc: A\ndataset for question answering via sentence com-\nposition. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34(05):8082\u20138090.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459\u2013\n9474. Curran Associates, Inc.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\nervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. 2021. PAQ: 65 mil-\nlion probably-asked questions and what you can do\nwith them. Transactions of the Association for Com-\nputational Linguistics, 9:1098\u20131115.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022.\nWhat\nmakes good in-context examples for GPT-3?\nIn\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100\u2013114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022a. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791\u20132809, Seattle, United States.\nAssociation for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022b. Rethinking the role of demonstra-\ntions: What makes in-context learning work? CoRR,\nabs/2202.12837.\nAaron Mueller, Jason Krone, Salvatore Romeo, Saab\nMansour, Elman Mansimov, Yi Zhang, and Dan Roth.\n2022. Label semantic aware pre-training for few-\nshot text classification. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8318\u2013\n8334, Dublin, Ireland. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-\nson, and Omer Levy. 2021. Few-shot question an-\nswering by pretraining span selection. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3066\u20133079, Online.\nAssociation for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255\u2013269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021b. It\u2019s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339\u20132352, Online. Association\nfor Computational Linguistics.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R. Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Arti\u00e8res,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, \u00c9ric\nGaussier, Liliana Barrio-Alvers, Michael Schroeder,\nIon Androutsopoulos, and Georgios Paliouras. 2015.\nAn overview of the BIOASQ large-scale biomedical\nsemantic indexing and question answering competi-\ntion. BMC Bioinform., 16:138:1\u2013138:28.\nRicardo Vilalta and Youssef Drissi. 2002. A perspective\nview and survey of meta-learning. Artif. Intell. Rev.,\n18(2):77\u201395.\nEllen M. Voorhees and Dawn M. Tice. 2000. Building\na question answering test collection. In Proceedings\nof the 23rd Annual International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, SIGIR \u201900, page 200\u2013207, New York, NY,\nUSA. Association for Computing Machinery.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353\u2013355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew Mingbo Dai, and Quoc V. Le. 2022. Finetuned\nlanguage models are zero-shot learners. In Interna-\ntional Conference on Learning Representations.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In Interna-\ntional Conference on Learning Representations.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163\u20137189, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Sa-\nloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsification with multiple metrics. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1206\u20131215, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021. Adapting language models for zero-shot learn-\ning by meta-tuning on dataset and prompt collections.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2856\u20132878, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nA\nTasks\nMeta-training.\nOur meta-training data is from\nMetaICL\u2019s (Min et al., 2022a) meta-training sets.\nSpecifically, we use the QA task collection from\nthe paper, which is a mixture of CROSSFIT and\nUNIFIEDQA tasks as shown in Table 7. We exclude\nany task on which we evaluate. As in MetaICL,\nwe subsample 16,384 examples per task such that\nno individual task is overrepresented during meta-\ntraining. Some tasks are sampled twice due to\nthe inclusion of both CROSSFIT and UNIFIEDQA\nversions of some tasks, as in Min et al. (2022a).\nAll meta-training tasks:\nbiomrc,\nboolq,\nfreebase_qa,\nhotpot_qa,\nkilt_hotpotqa,\nkilt_nq,\nkilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-trex,\nmc_taco,\nnumer_sense,\nquoref,\nropes,\nsearch_qa,\nsuperglue-\nmultirc, superglue-record, tweet_qa, web_questions, unifiedqa:boolq,\nunifiedqa:commonsenseqa,\nunifiedqa:drop,\nunifiedqa:narrativeqa,\nunifiedqa:natural_questions_with_dpr_para, unifiedqa:newsqa, uni-\nfiedqa:physical_iqa,\nunifiedqa:quoref,\nunifiedqa:race_string,\nuni-\nfiedqa:ropes, unifiedqa:social_iqa, unifiedqa:winogrande_xl\nSubsampled by similarity:\nbiomrc, boolq, freebase_qa, hotpot_qa, lama-google_re, quoref,\nropes, superglue-multirc, superglue-record, unifiedqa:boolq, uni-\nfiedqa:commonsenseqa,\nunifiedqa:drop,\nunifiedqa:narrativeqa,\nunifiedqa:natural_questions_with_dpr_para, unifiedqa:newsqa, uni-\nfiedqa:quoref, unifiedqa:race_string, unifiedqa:ropes\nTable 7: Tasks used in our meta-training data. We sub-\nsample 16,384 examples per task to ensure balanced\nsupervision during meta-training. All tasks are from\nCROSSFIT unless prefixed with \u201cunifiedqa:\u201d.\nWe also perform a targeted subsampling proce-\ndure, where we select tasks by semantic similar-\nity to our evaluation tasks. For this, we compute\nthe mean pairwise semantic similarity between a\nmeta-training task\u2019s examples and one 16-shot split\nof each of our evaluation tasks, then select meta-\ntraining tasks in decreasing order of similarity. Se-\nmantic similarity is computed by calculating the\ncosine similarity of the sentence embeddings from\nthe stsb-roberta-base-v2 model in Sentence-\nTransformers (Reimers and Gurevych, 2019).\nDemonstrations.\nOur demonstrations are from\nthe UNIFIEDQA collection, which includes extrac-\ntive, abstractive, and multiple-choice QA tasks as\nshown in Table 8. We exclude any task on which\nwe evaluate.\nNote that there is some overlap between the\ndemonstration set and the meta-training set, though\nthe demonstrations contain the correct answer\nwhereas the meta-training examples do not.\nDemonstration task bank:\nunifiedqa:ai2_science_middle,\nunifiedqa:boolq,\nuni-\nfiedqa:commonsenseqa,\nunifiedqa:drop,\nunifiedqa:mctest,\nuni-\nfiedqa:narrativeqa, unifiedqa:natural_questions_with_dpr_para, uni-\nfiedqa:newsqa, unifiedqa:openbookqa, unifiedqa:openbookqa_with_ir,\nunifiedqa:physical_iqa,\nunifiedqa:quoref,\nunifiedqa:race_string,\nunifiedqa:ropes, unifiedqa:social_iqa, unifiedqa:winogrande_xl\nTable 8: Tasks used in our demonstration memory bank.\nNote that there is no subsampling within each task, since\nthe retriever can simply ignore irrelevant demonstra-\ntions. All tasks are from UNIFIEDQA.\nB\nFormat Tuning for Multiple-choice QA\nChada and Natarajan (2021) observe significant\nperformance gains by simply changing the for-\nmat of the QA inputs and outputs. We use a for-\nmat similar to theirs for most QA tasks, but it\nis not immediately clear how to extend the ques-\ntion/answer/context format to multiple-choice QA,\nor if including the answer options in the context\nwould be helpful at all. Thus, we try three different\nformats for QASC and compare performance.\nEvery example consists of a question q, two con-\ntext sentences c1 and c2, a set of 8 answer options\nwith letter labels {aA, aB, . . ., aH}, and a correct\nanswer a \u2208 {aA, . . . , aH}. We can generate either\nthe full answer string, or the letter label of the an-\nswer i, where i \u2208 {A, B, . . . , H}. We try putting\nthe answer options in the question or the context,\nexcluding the answer options altogether, generat-\ning the answer string a, and generating the answer\nletter i.\nOur results using BARTlarge (Table 9) indicate\nthat generating the answer is better than just gen-\nerating the letter label, that including the options\nin the context is helpful, and that excluding the\noptions from the context or putting the options in\nthe question is harmful to performance. The perfor-\nmance gap between different formats is very large,\nwhich aligns with the findings of Chada and Natara-\njan (2021): using an example format aligned with\nthe model\u2019s pre-training format is one of the most\nimportant factors contributing to few-shot perfor-\nmance.\nFormat name\nFormat\nExample\nF1\nOptions in question,\ngenerate letter\nquestion: q? {aA, . . . , aH} \\n\nanswer: [MASK] \\n context: c1.\nc2. \u21d2 question: q? \\n answer: i\nquestion: What does sunlight do\nfor a plant? (A) during the day\n(B) Kills it (C) it can be seen\n(D) Helps it survive (E) Helps it\ndrink water (F) It gets heated up\n(G) adding heat (H) Makes the\ncolor darker \\n answer: [MASK]\n\\n context: A plant requires food\nfor survival. All plants require\nsunlight to make their food. \u21d2\nquestion: . . . \\n answer: D\n15.6\nOptions in question,\ngenerate answer\nquestion: q? {aA, . . . , aH} \\n\nanswer: [MASK] \\n context: c1.\nc2. \u21d2 question: q? \\n answer: a\nquestion: What does sunlight do\nfor a plant? (A) during the day\n(B) Kills it (C) it can be seen\n(D) Helps it survive (E) Helps it\ndrink water (F) It gets heated up\n(G) adding heat (H) Makes the\ncolor darker \\n answer: [MASK]\n\\n context: A plant requires food\nfor survival. All plants require\nsunlight to make their food. \u21d2\nquestion: . . . \\n answer: Helps it\nsurvive\n39.4\nOptions in context,\ngenerate answer\nquestion: q? \\n answer: [MASK]\n\\n context: {aA, . . . , aH}.\nc1.\nc2. \u21d2 question: q? \\n answer: a\nquestion: What does sunlight do\nfor a plant? \\n answer: [MASK]\n\\n context: (A) during the day\n(B) Kills it (C) it can be seen (D)\nHelps it survive (E) Helps it drink\nwater (F) It gets heated up (G)\nadding heat (H) Makes the color\ndarker. A plant requires food for\nsurvival. All plants require sun-\nlight to make their food. \u21d2 ques-\ntion: . . . \\n answer: Helps it sur-\nvive\n82.6\nNo options, gener-\nate answer\nquestion: q? \\n answer: [MASK]\n\\n context: c1. c2. \u21d2 question:\nq? \\n answer: a\nquestion: What does sunlight do\nfor a plant? \\n answer: [MASK]\n\\n context: A plant requires food\nfor survival. All plants require\nsunlight to make their food. \u21d2\nquestion: . . . \\n answer: Helps it\nsurvive\n49.8\nTable 9: The formats we try for QASC and 16-shot F1 scores from BARTlarge (no retrieval) after fine-tuning on each\nformat. We find that generating the answer is better than just generating the letter label, that including the options in\nthe context is helpful, and that excluding the options from the context is harmful to performance. \u201c\u21d2\u201d separates the\ninput from the output sequence, and \u201c\\n\u201d indicates a newline.\n"
  },
  {
    "title": "Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control",
    "link": "https://arxiv.org/pdf/2307.00117.pdf",
    "upvote": "5",
    "text": "Goal Representations for Instruction Following:\nA Semi-Supervised Language Interface to Control\nVivek Myers,\u2217 Andre He,\u2217 Kuan Fang, Homer Walke, Philippe Hansen-Estruch,\nChing-An Cheng,\u2020 Mihai Jalobeanu,\u2020 Andrey Kolobov,\u2020 Anca Dragan, Sergey Levine\nUniversity of California Berkeley, \u2020Microsoft Research\n{vmyers,andre.he,kuanfang,homer walke,hansenpmeche}@berkeley.edu\n{ChingAn.Cheng,mihaijal,akolobov}@microsoft.com\n{anca,svlevine}@berkeley.edu\nAbstract: Our goal is for robots to follow natural language instructions like \u201cput\nthe towel next to the microwave.\u201d But getting large amounts of labeled data, i.e.\ndata that contains demonstrations of tasks labeled with the language instruction,\nis prohibitive. In contrast, obtaining policies that respond to image goals is much\neasier, because any autonomous trial or demonstration can be labeled in hindsight\nwith its final state as the goal. In this work, we contribute a method that taps\ninto joint image- and goal- conditioned policies with language using only a small\namount of language data. Prior work has made progress on this using vision-\nlanguage models or by jointly training language-goal-conditioned policies, but so\nfar neither method has scaled effectively to real-world robot tasks without sig-\nnificant human annotation. Our method achieves robust performance in the real\nworld by learning an embedding from the labeled data that aligns language not to\nthe goal image, but rather to the desired change between the start and goal images\nthat the instruction corresponds to. We then train a policy on this embedding: the\npolicy benefits from all the unlabeled data, but the aligned embedding provides an\ninterface for language to steer the policy. We show instruction following across a\nvariety of manipulation tasks in different scenes, with generalization to language\ninstructions outside of the labeled data. Videos and code for our approach can be\nfound on our website: https://rail-berkeley.github.io/grif/.\nKeywords: Instruction Following, Representation Learning, Manipulation\n1\nIntroduction\nNatural language has the potential to be an easy-to-use and powerful form of task specification\nin robotics. To follow language instructions, a robot must understand human intent, ground its\nunderstanding in the state and action spaces, and solve the task by interacting with the environment.\nTraining robots to do this is challenging, especially given that language-annotated data is limited.\nExisting deep learning approaches require large amounts of expensive human language-annotated\ndemonstrations and are brittle on instructions outside the training data.\nVisual goals (i.e., goal images), though less intuitive for humans, provide complementary bene-\nfits for task representation in policy learning. Goals benefit from being easy to ground since, as\nimages, they can be directly compared with other states. More importantly, goal tasks provide addi-\ntional supervision and enable learning from unstructured data through hindsight relabeling [1, 2, 3].\nHowever, compared to language instructions, specifying visual goals is less practical for real-world\napplications, where users likely prefer to tell the robot what they want rather than having to show it.\nExposing an instruction-following interface for goal-conditioned policies could combine the\nstrengths of both goal- and language- task specification to enable generalist robots that can be easily\n*Equal contribution\narXiv:2307.00117v2  [cs.RO]  18 Aug 2023\nOur Approach (GRIF)\nExample Rollouts\ninitial state\ngoal\ninstruction\n\u201cmove the towel to \nthe left\u201d\n\u201cput the pepper  in \nthe pan\u201d\n\u201cput the spoon on \nthe towel\u201d\n\u201cmove the pan to \nthe front\u201d\n\u201cput the pepper on \nthe towel\u201d\ndiverse \ntasks in \nsame \nscene\ndiverse \nscenes\n\u201cput the pan on \nthe towel\u201d\ninstruction\nz\na\ninitial state\ngoal\nself-supervised: \na LOT of data\nlanguage-supervised:\na BIT of data\n \u03c0\nFigure 1: Left: Our approach learns representations of instructions that are aligned to transitions\nfrom the initial state to the goal. When commanded with instructions, the policy \u03c0 computes the\ntask representation z from the instruction and predicts the action a to solve the task. Our approach\nis trained with a small number of labeled demonstrations and large-scale unlabeled demonstrations.\nRight: Our approach can solve diverse tasks and generalize to vast environment variations.\ncommanded. While goal-conditioned policy learning can help digest unstructured data, non-robot\nvision-language data sources make it possible to connect language and visual goals for generaliza-\ntion to diverse instructions in the real world.\nTo this end, we propose Goal Representations for Instruction Following (GRIF), an approach that\njointly trains a language- and a goal- conditioned policy with aligned task representations. We\nterm task representations aligned because our objective encourages learning similar representations\nfor language instructions and state transitions that correspond to the same semantic task. GRIF\nlearns this representation structure explicitly through a contrastive task alignment term. Since task\nrepresentations across language and image goal modalities have similar semantics, this approach\nallows us to use robot data collected without annotations to improve performance by the agent on\nimage goal tasks when viewed as a goal-conditioned policy, and thus indirectly improve language-\nconditioned performance in a semi-supervised manner. An overview of GRIF is shown in Figure 1.\nWe present an approach for learning a language interface for visuomotor control without extensive\nlanguage labels. With this method, we demonstrate that the semantic knowledge from a pre-trained\nvision-language model (CLIP [4]) can be used to improved task representations and manipulation\neven though such models perform poorly at task understanding out-of-the-box. Our experiments\nshow that aligning task representations to scene changes enables improved performance at grounding\nand following language instructions within diverse real-world scenes.\n2\nRelated Work\nRobotic control with language interfaces. Early works in language-conditioned robotic control\nuse hand-designed parse trees or probabilistic graphical models to convert instructions into symbolic\nstates to configure the downstream planners and controllers [5, 6, 7, 8]. To generalize beyond limited\nhuman specifications, a growing number of works have trained conditional policies end-to-end to\nfollow natural language instructions [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Combining recent\nadvances large language models (LLMs) [21] with learned language-conditioned policies as a low-\nlevel API has have paved the way for broad downstream applications with improved planning and\ngeneralization [22, 23, 24, 25, 26, 27]. However, most of these methods need high-capacity policy\nnetworks with massive, costly labeled demonstration data. As a result, the learned policies often\ngeneralize poorly to unseen scenarios or can only handle limited instructions in real-world scenes.\nUnlike past work, we learn low-level language-conditioned control from less annotated data.\n2\nVision-language pre-training. Vision-language models (VLMs) enable textual descriptions to be\nassociated with visual scenes [4, 28]. Through contrastive learning over internet-scale data, recent\nlarge-scale VLMs such as CLIP [4] have achieved unprecedented zero-shot and few-shot general-\nization capabilities, with a wide range of applications.\nDespite these advances, applying pre-trained VLMs to robotic control is not straightforward since\ncontrol requires grounding instructions in motions instead of static images. Through training from\nscratch or fine-tuning on human trajectories [29, 30], recent approaches learn representations for\nvisuomotor control [31, 32]. These works use language labels to to learn visual representations for\ncontrol without directly using language as an interface to the policy. In CLIPort, Shridhar et al. [33]\nuse pre-trained CLIP [4] to enable sample-efficient policy learning. Their approach selects actions\nfrom high-level skills through imitation, assuming access to predefined pick-and-place motion prim-\nitives with known camera parameters. In contrast, our approach learns to align the representation\nof the instruction and the representation of the transition from the initial state to the goal on labeled\nrobot data, and uses these representations for control without assumptions about the observation and\naction spaces. Other approaches use VLMs to recover reward signals for reinforcement learning\n[34, 35, 36, 37, 3]. In contrast, our approach directly trains language-conditioned policy through\nimitation learning without the need for online interactions with the environment.\nLearning language-conditioned tasks by reaching goals. Alternatively, language-conditioned\npolicies can be constructed or learned through goal-conditioned policies [38, 39]. Lynch and Ser-\nmanet [40] propose an approach that facilitates language-conditioned imitation learning by sharing\nthe policy network and aligning the representations of the two conditional tasks. Based on the same\nmotivation, we propose an alternative approach which explicitly extends the alignment of VLMs to\nspecify tasks as changes in the scene. By tuning a contrastive alignment objective, our method is able\nto exploit the knowledge of VLMs [4] pre-trained on broad data. This explicit alignment improves\nupon past approaching to connecting images and language [41, 42] by explicitly aligning tasks in-\nstead merely jointly training on conditional tasks. In Sec. 5, we show our approach significantly\nimproves the performance of the learned policy and enhances generalization to new instructions.\n3\nProblem Setup\nOur objective is to train robots to solve tasks specified by natural language from interactions with\nthe environment. This problem can be formulated as a conditional Markov decision process (MDP)\ndenoted by the tuple (S, A, \u03c1, P, W, \u03b3), with state space S, action space A, initial state probability\n\u03c1, transition probability P, an instruction space W, and discount \u03b3. Given the instruction \u2113 \u2208 W,\nthe robot takes action at \u2208 A given the state st at each time step t to achieve success.\nTo solve such tasks, we train a language-conditioned policy \u03c0(a|s, \u2113) on a combination of human\ndemonstrations and autonomously collected trajectories. Since high-quality natural language an-\nnotations are expensive and time-consuming to obtain, we assume that only a small portion of the\ntrajectories are labeled with the corresponding instructions. The robot has access to a combination\nof two datasets\u2014a small-scale labeled dataset DL with annotated instructions and a large-scale un-\nlabeled dataset DU consists of more diverse play data collected in an open-ended manner. Our goal\nis to train \u03c0(a|s, \u2113) while taking advantage of both the labeled and unlabeled datasets. We formulate\n\u03c0(a|s, \u2113) as a stochastic policy that predicts the Gaussian distribution N(\u00b5a, \u03a3a).\n4\nGoal Representations for Instruction Following\nWe propose Goal Representations for Instruction Following (GRIF) to interface visuomotor policies\nwith natural language instructions in a semi-supervised fashion (Figure. 2). Although the language-\nconditioned policy cannot be directly trained on the unlabeled dataset DU, we can facilitate the\ntraining through goal-conditioned tasks. Solving both types of tasks requires the policy to under-\nstand the human intent, ground it in the current observation, and predict necessary actions. Although\nthe first steps involve understanding task specifications of different modalities (goal images and lan-\n3\nguage), the remaining steps of such processes can be shared between the two settings. To this end, we\ndecouple the language-conditioned policy \u03c0(a|s, \u2113) into a policy network \u03c0\u03b8(a|s, z) and a language-\nconditioned task encoder f\u03c6(\u2113), where z = f\u03c6(\u2113) is the representation of the task specified by the\ninstruction \u2113. To solve the goal-conditioned task, we also introduce a goal-conditioned task encoder\nh\u03c8. The policy network \u03c0\u03b8 is shared between the language-conditioned and goal-conditioned tasks.\nThis approach relies on the alignment of task representations. While most existing VLMs align\ntext with static images, we argue that the representation of the goal-conditioned tasks should be\ncomputed from the state-goal pair (s0, g). This is because the instruction often focuses on the\nchanging factors from the initial state to the goal rather than directly describing the entire goal image,\ne.g., \u201cmove the metal pan to the left\u201d. Therefore, the representations of goal-conditioned tasks are\ncomputed as z = h\u03c8(s0, g) and we aim to train the encoders such that for (s0, g, \u2113) sampled from the\nsame trajectory, the distance between f\u03c6(\u2113) and h\u03c8(s0, g) should be close and far apart otherwise.\nWe illustrate our high-level approach in Figure 2.\n4.1\nExplicit Alignment through Contrastive Learning\nWe propose explicitly aligning the representations of goal-conditioned and language-conditioned\ntasks through contrastive learning [41]. Compared to implicitly aligning the task presentations\nthrough joint training of the two conditional policies, contrastive alignment requires that all rele-\nvant information for selecting actions be included in the shared task representation. This improves\nthe transfer between the action prediction tasks for both goal and language modalities by preventing\nthe policy from relying on features only present in one task modality in selecting actions. Using an\nInfoNCE objective [42], we train the two encoders f\u03c6 and h\u03c8 to represent instructions \u2113 and tran-\nsitions (s0, g) according to their task semantics. More concretely, for (s0, g) and \u2113 that correspond\nto the same task, we would like their embeddings z\u2113 = f\u03c6(\u2113) and zg = h\u03c8(s0, g) to be close in the\nlatent space, while z\u2113 and zg corresponding to different tasks to be far apart.\nTo compute the InfoNCE objective, we define C(s, g, \u2113) = cos(f(\u2113), h(s, g)) with the cosine sim-\nilarity cos(\u00b7, \u00b7). We sample positive data s+, g+, \u2113+ \u223c DL by selecting the start state, end state,\nand language annotation of a random trajectory. We sample negative examples s\u2212, g\u2212 \u223c DL by\nselecting the start state and end state of a random trajectory, and sample \u2113\u2212 \u223c DL by selecting the\nlanguage annotation of another random trajectory. For each positive tuple, we sample k negative\nexamples and denote them as {s\u2212\ni , g\u2212\ni }k\ni=1 and {\u2113\u2212\ni }k\nj=1. Then we can define the InfoNCE Ltask:\nLlang\u2192goal = \u2212 log\nexp(C(s+, g+, \u2113+)/\u03c4)\nexp(C(s+, g+, \u2113+)/\u03c4) + Pk\ni=1 exp\n\u0000C(s\u2212\ni , g\u2212\ni , \u2113+)/\u03c4\n\u0001\nLgoal\u2192lang = \u2212 log\nexp(C(s+, g+, \u2113+)/\u03c4)\nexp(C(s+, g+, \u2113+)/\u03c4) + Pk\nj=1 exp\n\u0000C(s+, g+, \u2113\u2212\nj )/\u03c4\n\u0001\nLtask = Llang\u2192goal + Lgoal\u2192lang\n(1)\nwhere \u03c4 is a temperature hyperparameter. Llang\u2192goal and Lgoal\u2192lang represent the log classification\naccuracy of our alignment in predicting goals from language and language from goals respectively.\n4.2\nWeight Initialization with Vision-Language Models\nTo handle tasks involving objects and instructions beyond those contained in the limited labeled\ndataset, we wish to incorporate prior knowledge from broader sources into the encoders f\u03c6 and h\u03c8.\nFor this purpose, we investigate practical ways to incorporate Vision-Language Models (VLMs) [4]\npre-trained on massive paired images and texts into our encoders. Pre-trained VLMs demonstrate ef-\nfective zero-shot and few-shot generalization capability for vision-language tasks [4, 43]. However,\nthey are originally designed for aligning a single static image with its caption without the ability to\nunderstand the changes in the environment that language tasks correspond to, and perform poorly\non compositional generalization [44, 45], which is key to modeling changes in scene state. We wish\nto encode the change between images while still exploiting prior knowledge in pre-trained VLMs.\n4\nf\u03c6(\u2113)\nh\u03c8(s0, g)\n\u03c0\u03b8(s, z)\na\ns\nz\nBC Loss\nLearn Policy\nh\u03c8(s0, g)\nf\u03c6(\u2113)\nh1 \u22c5 f1 h1 \u22c5 f2 h1 \u22c5 f3 h1 \u22c5 f4\nh2 \u22c5 f1 h2 \u22c5 f2 h2 \u22c5 f3 h2 \u22c5 f4\nh3 \u22c5 f1 h3 \u22c5 f2 h3 \u22c5 f3 h3 \u22c5 f4\nh4 \u22c5 f1 h4 \u22c5 f2 h4 \u22c5 f3 h4 \u22c5 f4\n\u22f1\nf1\nf2\nf3\nf4\nh1\nh2\nh3\nh4\n\u22ee\n\u2026\nContrastive Alignment Loss\nLearn Task Representations\nDL\n\u201cplace the \nspoon on the \ntowel\u201d\nDU\nDL\n\u201cplace the \nspoon on the \ntowel\u201d\nFigure 2:\nLeft:\nWe explicitly align representations between goal-conditioned and language-\nconditioned tasks on the labeled dataset DL through contrastive learning. Right: Given the pre-\ntrained task representations, we train a policy on both labeled and unlabeled datasets.\nTo address this issue, we devise a mechanism to accommodate and fine-tune the CLIP [4] model\nfor aligning the transition (s0, g) with the instruction \u2113. Specifically, we duplicate and halve the\nweights of the first layer of the CLIP architecture so it can operate on pairs of stacked images rather\nthan single images. Details on how we modify the pre-trained CLIP to accommodate encoding\nchanges are presented in Appendix C.2. In practice, we find this mechanism significantly improves\nthe generalization capability of the learned policy \u03c0\u03b8(a|s, g).\n4.3\nPolicy Learning with Aligned Representations\nWe train the policy jointly on the two datasets DL and DU with the aligned task representations.\nBy sampling (\u2113, st, at) from DL, we train the policy network \u03c0\u03b8(a|z) to solve language-conditioned\ntasks with z = f\u03c6(\u2113). And by sampling (s0, g, st, at) from DL \u222a DU, \u03c0\u03b8 is trained to reach goals\nwith z = h\u03c8(s0, g). We train with behavioral cloning to maximize the likelihood of the actions at.\nWe investigate two ways to train the policy given the encoders f\u03c6 and h\u03c8. The straightforward\nway is to jointly train the policy network \u03c0\u03d5 and the two encoders end-to-end. This process adapts\nthe encoders with the policy network to encourage them to incorporate information that facilitates\ndownstream robotic control, but can also backfire if the policy learns to rely on visual-only features\nthat are absent in the language conditioned setting. Alternatively, we can freeze the pre-trained\nweights of the two encoders and only train the shared policy network \u03c0\u03d5 on the two datasets. In\nSection 5, we evaluate and discuss the performances of both options.\n5\nExperiments\nOur work started with the premise of tapping into large, goal-conditioned datasets. To build a lan-\nguage interface for goal-conditioned policy learning, we proposed to learn explicitly aligned task\nrepresentations, and to align instructions to state changes rather than static goals. Lastly, we advo-\ncated for the use of pre-trained VLMs to incorporate larger sources of vision-language knowledge.\nTherefore, we aim to test the following hypotheses in our experiments:\nH1: Unlabeled trajectories will benefit the language-conditioned policy on new instructions.\nH2: Explicitly aligning task representations improves upon the implicit alignment from LangLfP-\nstyle joint training [40].\nH3: The prior knowledge in pre-trained VLMs can improve learned task representations.\n5\nGRIF\nLLfP\nBC-Z\nSuccess Rate\nScene C:\nScene B:\nScene A:\n\"put the yellow bell pepper on the cloth\"\n\"move the pan to the front\"\n\"put the pan on the towel\"\n\"move the bell pepper to the left of the table\"\n\"put the bell pepper in the pan\"\n\"put the knife on the purple cloth\"\n\"place the knife in front of the microwave\"\n\"move the pan in front of the cloth\"\n\"put the mushroom in the metal pot \"\n\"put the spoon on the towel\"\n\"place the metal pot on top of the blue \ntowel\"\n\"move the towel to the left\"\n\"move the towel to the front\"\n\"move the towel next to the cans\"\n\"move the towel next to the \nmicrowave\"\nFigure 3: Comparison of success rates \u00b1SE between the top three methods across all trials within\nthe three scenes. Two other baselines LCBC and R3M (not shown) achieved 0.0 success in all\nevaluation tasks although they do succeed on tasks that are heavily covered in the training data.\nStatistical significance is starred. The initial observation and instructions of each scene are shown.\nH4: Aligning transitions with language enable better use of VLMs compared to conventional\nimage-language contrastive methods [37, 46].\nOur experiments are conducted in an table-top manipulation domain. For training, we use a labeled\ndataset DL containing 7k trajectories and an unlabeled DU containing 47k trajectories. Our approach\nlearns to imitate the 6 DOF continuous gripper control actions in the data at 5Hz. The evaluation\nscenes and unseen instructions are shown in Figure 3. Additional details about the environment, the\ndataset, and the breakdown of results are described in Appendices B and E.\n5.1\nComparative Results\nWe compare the proposed GRIF with four baseline methods on a set of 15 unseen instructions\nfrom all 3 scenes and report the aggregated results in Figure 3, with GRIF attaining the best per-\nformance across all scenes. The per-task success rates can be found in Appendix E. LCBC [9]\nuses a behavioral cloning objective to train a policy conditioned on language from DL, similar to\nprior methods on instruction-conditioned imitation learning. LLfP [40] jointly trains a goal con-\nditioned and language conditioned policy on partially labeled data, but does not learn aligned task\nrepresentations. R3M [32] provides pre-trained state representations for robot manipulation that are\npredictive of language-conditioned rewards. We use this approach as a baseline by jointly training\ngoal- and language-conditioned policies while using R3M state encodings as goal representations\n(i.e., h\u03c8(s0, g) = R3M(g)). BC-Z [10] jointly trains language- and video-conditioned policies and\nuses an additional cosine similarity term to align video and language embeddings. This approach\ndoes not transfer directly into our goal-conditioned setting, but we create a baseline that adapts it\nto our setting by jointly training goal- and language-conditioned policies while aligning task repre-\nsentations with a cosine distance loss. The architecture choices are standardized across all methods\nfor fair comparisons. Unless stated otherwise, all baselines use a ResNet-18 as the goal encoder\nh\u03c8(s0, g). In our preliminary experiments, this architecture was found to give good performance\nwhen used to train goal-conditioned policies in our setting. For the language encoder f\u03c6(\u2113), all\nbaselines use a pre-trained and frozen MUSE model [47], as in previous work [40, 10].\nWe find that language-conditioned policies must make use of unlabeled trajectories to achieve non-\nzero success rates when generalizing to new language instructions in support of H1. LCBC does\nnot use unlabeled data and fails to complete any tasks. R3M jointly trains goal- and language-\nconditioned policies, but it also fails all tasks. This is likely due to its goal encodings being frozen\nand unable to be implicitly aligned to language during joint training. Methods that use implicit or\nexplicit alignment (GRIF, LLfP, BC-Z), are able to exploit unlabeled goal data to follow instruc-\ntions to varying degrees of success. These comparisons suggest that the combined effect of using\npre-trained CLIP to align transitions with language significantly improves language-conditioned ca-\npabilities. Our model significantly outperformed all baselines on 8 out of 15 tasks, achieving high\n6\nsuccess rates on several tasks where the baselines almost completely fail (\u201cplace the knife in front\nof the microwave\u201d, \u201cmove the pan in front of the cloth\u201d, \u201cput the knife on the purple cloth\u201d), while\nachieving similar performance to the next-best baseline on the remaining tasks. Where baselines\nfailed, we often observed grounding failures. The robot reached for incorrect objects, placed them\nin incorrect locations, or was easily distracted by nearby objects into performing a different task.\n5.2\nAblation Study\nWe run a series of ablations to analyze the performance of GRIF and test the hypotheses. No Align\nablates the effect of explicit alignment by removing the contrastive objective. We also unfreeze\nthe task encoders so that they are implicitly aligned via joint training of the language- and goal-\nconditioned policies. No CLIP ablates the effect of using pre-trained CLIP by replacing the image\nand text encoders with a ResNet-18 and pre-trained MUSE language encoder. In No Start, the\nimage task representaions only depend on goals as h\u03c8(g), instead of depending on transitions as\nh\u03c8(s0, g). This is the conventional way to connect goals and language with CLIP that is often used\nin previous work [46, 37]. For GRIF (Labeled), we exclude DU to study whether using unlabeled\ndata is important for performance. GRIF (Joint) trains the task alignment and policy losses jointly,\ntaking gradients through the image encoder and freezing the language encoder. This is the end-to-\nend approach discussed in Section 4.3. We refer to our full approach without joint training as GRIF\n(Frozen) in the remainder of this section.\nSuccess Rate\nGRIF (Frozen)\nGRIF (Joint)\nNo Start\nNo Align\nNo CLIP\nAblation Results (Scene A)\nGRIF (Labeled)\nFigure 4: Success rates of ablations with one standard error.\nAs shown in Figure 4,\nexplicit\nalignment,\npre-trained CLIP, and\ntransition-based task representations\nall play critical roles in achieving\nhigh success rates. Notably, the con-\nventional approach of aligning static\ngoals and instructions with CLIP (No\nStart) fails almost completely in our\nsetting.\nThis is in support of H4\nand confirms that transitions, and not\ngoal images, should be aligned to lan-\nguage tasks.\nIn GRIF (Labeled),\ndropping DU significantly decreases\nsuccess rates, further supporting H1. We observe that this is primarily due to a deterioration of\nmanipulation skills rather than grounding, which is expected as grounding is mostly learned via ex-\nplicit alignment on DL. Regarding H2 and H3, we observe that removing either alignment or CLIP\nresults in a large drop in performance. We also observed that No Align outperforms its counterpart\nLLfP by using the pre-trained CLIP model (after the modification in Sec. 4.2) in the task encoder.\nWe hypothesize that this is because CLIP has already been explicitly aligned during pre-training,\nand some of its knowledge is retained during joint training with the policy even without GRIF\u2019s task\nalignment loss. Lastly, deciding to freeze the task encoders during policy training does not appear to\nsignificantly affect our model\u2019s performance. This is likely because the contrastive learning phase\nalready learns representations that can represent the semantic task, so there is less to gain from\nfurther implicit alignment during joint training.\n5.3\nAnalysis on the Learned Task Representations\nFor additional analysis, we evaluate our model\u2019s task grounding capabilities independently of the\nmanipulation policy and compare it with ablations. Specifically, we evaluate how well our model\ncan connect new language instructions to correct goals in a scene. This is important to downstream\npolicy success: if the model is able to project the language to a representation f\u03c6(l) that is close to\nthat of the correct (but unprovided) goal h\u03c8(s0, g), then the policy will likely be able to execute the\ntask since it has been trained on a large amount of goal-conditioned data.\n7\n\u201cMove the cloth to the \nempty table corner\u201d\n\u201cPlace the spoon on \ntop of the towel\u201d\nTop-5 Text To Image Retrieval Accuracy\nInstruction\nRetrieved Images\nRetrieval Accuracy (%)\ns0\ng\ns0\ng\nGRIF (Frozen)\nGRIF (Joint)\nNo Start\nNo Align\nNo CLIP\nFigure 5: Left: Comparison of the top-5 text to image retrieval accuracy of representations learned\nby different ablations. Right: Examples of retrieved image pairs given instructions.\nOur task representations are trained with a contrastive objective, offering a convenient way to com-\npute alignment between language and goals. On an dataset of labeled held-out trajectories, we\ncompute the similarities between all pairs of visual task representations h\u03c8(s0, g) and language\ntask representations f\u03c6(\u2113). For each language instruction, we retrieve the top k most similar (s0, g)\ntransitions and compute the accuracy for the correct transition being retrieved. We compute this\nmetric in fixed batches of 256 examples and average over the validation set to report a text-to-image\nretrieval accuracy. We compute this metric for representations from each of our ablations and report\nthe results in Figure 5 to analyze why GRIF outperforms other approaches in our main experiments.\nOur task representations show significantly better generalization compared to using a conventional\nimage-language alignment (No Start), despite it being CLIP\u2019s original pre-training objective. The\nalignment accuracy is also more than 50% higher than when using non-VLM encoders (No CLIP),\nsuggesting potentially large gains in grounding capability through using VLMs.\nWe also study the effect of the number of language annotations on our model\u2019s grounding capability.\nEven at less than half the number of language annotations (3k), GRIF outperforms all the ablations\nin Figure 5, achieving a retrieval accuracy of 73%. Detailed results for this ablation are presented in\nAppendix F, showing our approach is robust to lower amounts of language supervision.\n6\nDiscussion, Limitations, and Future Work\nOur approach to aligning image goals and language instructions enables a robot to utilize large\namounts of unlabeled trajectory data to learn goal-conditioned policies, while providing a \u201clanguage\ninterface\u201d to these policies via aligned language-goal representations. In contrast to prior language-\nimage alignment methods, our representations align changes in state to language, which we show\nleads to significantly better performance than more commonly used CLIP-style image-language\nalignment objectives. Our experiments demonstrate that our approach can effectively leverage unla-\nbeled robotic trajectories, with large improvements in performance over baselines and methods that\nonly use the language-annotated data.\nLimitations and future work. Our method has a number of limitations that could be addressed in\nfuture. For instance, our method is not well-suited for tasks where instructions say more about how\nto do the task rather than what to do (e.g., \u201cpour the water slowly\u201d)\u2014such qualitative instructions\nmight require other types of alignment losses that more effectively consider the intermediate steps\nof task execution. Our approach also assumes that all language grounding comes from the portion of\nour dataset that is fully annotated or a pre-trained VLM. An exciting direction for future work would\nbe to extend our alignment loss to utilize non-robot vision-language data, such as videos, to learn rich\nsemantics from Internet-scale data. Such an approach could then use this data to improve grounding\non language not in the robot dataset and enable broadly generalizable and powerful robotic policies\nthat can follow user instructions.\n8\nAcknowledgements\nWe would like to acknowledge the funding provided by AFOSR FA9550-22-1-0273, ONR N00014-\n20-1-2383, NSF IIS-2150826, and ONR YIP N00014-20-1-2736.\nReferences\n[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nO. Pieter Abbeel, and W. Zaremba. Hindsight Experience Replay. In Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[2] D. Ghosh, A. Gupta, A. Reddy, J. Fu, C. Devin, B. Eysenbach, and S. Levine. Learning to\nReach Goals via Iterated Supervised Learning, Oct. 2020.\n[3] S. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn.\nLearning Language-\nConditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation. In Proceed-\nings of the 5th Conference on Robot Learning, pages 1303\u20131315. PMLR, Jan. 2022.\n[4] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion, Feb. 2021.\n[5] T. Winograd. Procedures as a representation for data in a computer program for understanding\nnatural language. 1971.\n[6] M. Skubic, D. Perzanowski, S. Blisard, A. C. Schultz, W. Adams, M. D. Bugajska, and D. P.\nBrock. Spatial language for human-robot dialogs. IEEE Transactions on Systems, Man, and\nCybernetics, Part C (Applications and Reviews), 34:154\u2013167, 2004.\n[7] K. yuh Hsiao, S. Tellex, S. Vosoughi, R. Kubat, and D. K. Roy. Object schemas for grounding\nlanguage in a responsive robot. Connection Science, 20:253 \u2013 276, 2008.\n[8] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy.\nUnderstanding natural language commands for robotic navigation and mobile manipulation.\nProceedings of the AAAI Conference on Artificial Intelligence, 2011.\n[9] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor.\nLanguage-\nconditioned imitation learning for robot manipulation tasks. ArXiv, abs/2010.12083, 2020.\n[10] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. BC-Z:\nZero-Shot Task Generalization with Robotic Imitation Learning. Conference on Robot Learn-\ning, page 12, 2021. doi:164:991-1002.\n[11] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD\nCONTROL AT SCALE, Dec. 2022.\n[12] PaLM-E: An Embodied Multimodal Language Model.\n[13] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu,\nand L. Fan. VIMA: General Robot Manipulation with Multimodal Prompts, Oct. 2022.\n[14] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language\ninstructions to feasible plans. ArXiv, abs/2303.12153, 2023.\n[15] W. Liu, C. Paxton, T. Hermans, and D. Fox.\nStructformer: Learning spatial structure for\nlanguage-guided semantic rearrangement of novel objects. 2022 International Conference on\nRobotics and Automation (ICRA), pages 6322\u20136329, 2021.\n9\n[16] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar,\nY. Zhu, and L. J. Fan. Vima: General robot manipulation with multimodal prompts. ArXiv,\nabs/2210.03094, 2022.\n[17] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Conference on Robot Learning, 2022.\n[18] H. Mei, M. Bansal, and M. R. Walter. Listen, Attend, and Walk: Neural Mapping of Naviga-\ntional Instructions to Action Sequences, Dec. 2015.\n[19] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00a8underhauf, I. Reid, S. Gould, and\nA. van den Hengel. Vision-and-Language Navigation: Interpreting Visually-Grounded Navi-\ngation Instructions in Real Environments. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3674\u20133683, 2018.\n[20] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,\nK. Saenko, D. Klein, and T. Darrell. Speaker-Follower Models for Vision-and-Language Navi-\ngation. In Advances in Neural Information Processing Systems, volume 31. Curran Associates,\nInc., 2018.\n[21] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language Models are Few-Shot Learners, 2020.\n[22] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakr-\nishnan, K. Hausman, et al. Do As I Can, Not As I Say: Grounding Language in Robotic\nAffordances, Aug. 2022.\n[23] M. Attarian, A. Gupta, Z. Zhou, W. Yu, I. Gilitschenski, and A. Garg. See, Plan, Predict:\nLanguage-guided Cognitive Planning with Video Prediction, Oct. 2022.\n[24] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and\nD. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10740\u201310749, 2020.\n[25] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and\nT. A. Funkhouser. Tidybot: Personalized robot assistance with large language models. ArXiv,\nabs/2305.05658, 2023.\n[26] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models. ArXiv,\nabs/2209.11302, 2022.\n[27] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard.\nCALVIN: A Benchmark for\nLanguage-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. IEEE\nRobotics and Automation Letters, 7(3):7327\u20137334, July 2022.\nISSN 2377-3766.\ndoi:\n10.1109/LRA.2022.3180108.\n[28] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. CoCa: Contrastive\nCaptioners are Image-Text Foundation Models, June 2022.\n[29] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,\nM. Liu, X. Liu, et al. Ego4D: Around the World in 3,000 Hours of Egocentric Video, Mar.\n2022.\n[30] R. Goyal, S. E. Kahou, V. Michalski, J. Materzy\u00b4nska, S. Westphal, H. Kim, V. Haenel, I. Fru-\nend, P. Yianilos, M. Mueller-Freitag, et al. The \u201dsomething something\u201d video database for\nlearning and evaluating visual common sense, June 2017.\n10\n[31] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell.\nReal-world robot\nlearning with masked visual pre-training. ArXiv, abs/2210.03109, 2022.\n[32] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3M: A Universal Visual Repre-\nsentation for Robot Manipulation, Nov. 2022.\n[33] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. ArXiv, abs/2109.12098, 2021.\n[34] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\nKnowledge, June 2022.\n[35] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg.\nConcept2Robot: Learning ma-\nnipulation concepts from instructions and human demonstrations.\nThe International Jour-\nnal of Robotics Research, 40(12-14):1419\u20131434, Dec. 2021. ISSN 0278-3649. doi:10.1177/\n02783649211046285.\n[36] Y. J. Ma, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. LIV: Language-Image Repre-\nsentations and Rewards for Robotic Control. May 2023.\n[37] Y. Cui, S. Niekum, A. Gupta, V. Kumar, and A. Rajeswaran. Can Foundation Models Perform\nZero-Shot Task Specification For Robot Manipulation?, Apr. 2022.\n[38] L. P. Kaelbling. Learning to Achieve Goals. In IJCAI, 1993.\n[39] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal Value Function Approximators. In\nInternational Conference on Machine Learning (ICML), 2015.\n[40] C. Lynch and P. Sermanet. Language Conditioned Imitation Learning over Unstructured Data,\nJuly 2021.\n[41] Z. Ma and M. Collins. Noise Contrastive Estimation and Negative Sampling for Conditional\nModels: Consistency and Statistical Efficiency, Sept. 2018.\n[42] A. van den Oord, Y. Li, and O. Vinyals. Representation Learning with Contrastive Predictive\nCoding, Jan. 2019.\n[43] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual Instruction Tuning, Apr. 2023.\n[44] K. Jiang, X. He, R. Xu, and X. E. Wang. ComCLIP: Training-Free Compositional Image and\nText Matching, Nov. 2022.\n[45] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does CLIP Bind\nConcepts? Probing Compositionality in Large Image Models, Mar. 2023.\n[46] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. CoWs on Pasture: Baselines\nand Benchmarks for Language-Driven Zero-Shot Object Navigation, Dec. 2022.\n[47] Y. Yang, D. Cer, A. Ahmad, M. Guo, J. Law, N. Constant, G. H. Abrego, S. Yuan, C. Tar, Y.-H.\nSung, et al. Multilingual Universal Sentence Encoder for Semantic Retrieval, July 2019.\n11\nAppendix\nA\nWebsite\nVideos and code for our approach can be found at https://rail-berkeley.github.io/\ngrif/.\nB\nEnvironment Details\nWe provide more details on the real-world environment in this section.\nB.1\nRobot\nWe use a 6DOF WidowX 250 robot with a 1DOF parallel-jaw gripper. We install the robot on a\ntabletop where it can reach and manipulate objects within an environment set up in front of it. The\nrobot receives inputs from a Logitech C920 RGB camera installed in an over-the-shoulder view. The\nimages are passed into the policy at a 128 x 128, and the control frequency is 5Hz. Teleoperation\ndata is collected with a Meta Quest 2 VR headset that controls the robot.\nB.2\nDataset Details\nThe dataset consists of trajectories collected from 24 different environments, which includes\nkitchen-, sink-, and tabletop-themed manipulation environments. The dataset features around 100\nobjects, including containers, utensils, toy food items, towels, and other kitchen-themed objects. It\nincludes demonstrations of 13 high-level skills (pick and place, sweep, etc.) applied to different\nobjects. Out of the 54k total trajectories, 7k are annotated with language instructions. Around 44k\nof the trajectories are expert demonstrations and around 10k are collected by a scripted policy.\nC\nMethod Details\nC.1\nPolicy Network\nOur policy network \u03c0\u03b8(a|s, z) uses a ResNet-34 architecture. To condition on the task embedding\nz, it is first passed through 2 fully connected layers. Then, the policy network is conditioned on the\nembedding using FiLM layers, which are applied at the end of every block throughout the ResNet.\nThe image encoding is then passed into a fully connected network to predict the action distribution.\nThe policy network predicts the action mean, and we use a fixed standard deviation.\nC.2\nCLIP Model Surgery\nInstead of separately encoding s0 and g inside f\u03c6, we perform a \u201csurgery\u201d to the CLIP model to\nenable it to take (s0, g) as inputs while keeping most of its pre-trained network weights as intact as\npossible. Specifically, we clone the weight matrix Win of the first layer in the pre-trained CLIP and\nconcatenate them along the channel dimension to be [Win; Win], creating a model that can accept\nthe stacked [s0, g] as inputs. We also halve the values of this new weight matrix to make it W \u2032\nin =\n[Win/2; Win/2], ensuring its output 0.5(Wins0+Wing) will follow a distribution similar to the output\nby the original first layer Wins0. While this surgery alone cannot perfectly close the gap, the resultant\nmodified encoder can serve as a capable initialization for the transition encoder h\u03c8. We further fine-\ntune h\u03c8 on the labeled robot dataset DL using the aforementioned method to adapt it for instruction-\nfollowing tasks.\nC.3\nNegative Sampling\nFor training the contrastive objective on DL, our batch sampling strategy is non-standard. We use 2\ndataloaders in parallel; the first samples from shuffled trajectories, while the second iterates through\n12\ntrajectories in the order that they are stored in the dataset. Each samples batches of 128 trajectories\nand they are concatenated to produce a batch size of 256. The reason for this is that if we were to use\na standard sampling strategy, most examples in a batch would be from different scenes. This is not\nuseful for the contrastive loss because the representations would just learn to distinguish tasks based\non the set of objects that appear. The robot benefits from being able to distinguish different tasks in\nthe same scene, so we try to include many trajectories from the same scene in each batch. Using an\nunshuffled dataloder is a convenient way to achieve this since trajectories from the same scene are\nstored together. This can be considered a form of negative mining for the contrastive learning stage.\nC.4\nInstruction Augmentation\nIn order to increase the diversity of language annotations, we augment our natural language anno-\ntations using GPT-3.5. Through the API, we query the gpt-3.5-turbo model to generate paraphrases\nof instructions in our dataset. We generate 5 paraphrases per instruction and sample from them\nrandomly during training. An example prompt and response are shown below. We found in prelim-\ninary experiments that using augmented instructions slightly improved language generalization, so\nwe keep this augmentation for all models and baselines.\nPrompt:\nGenerate 5 variations of the following command:\n\"put the mushroom in the metal pot\"\nNumber them like 1. 2. 3.\nBe concise and use synonyms.\nResponse:\n1. Place the fungus in the metallic container.\n2. Insert the mushroom into the steel vessel.\n3. Set the toadstool inside the iron cauldron.\n4. Position the champignon within the tin pot.\n5. Place the fungi in the metallic kettle.\nC.5\nGoal Relabeling\nFor unlabeled trajectories in DU, we use a simple goal relabeling strategy: with 50% probability,\nwe use the final achieved state as the goal, and with 50 % probability we uniformly sample an\nintermediate state in the trajectory to use as the goal. We do not relabel the annotated trajectories in\nDL.\nC.6\nHyperparameters\nWhen training the task encoders using the contrastive learning objective, we use a batch size of 256.\nWe reduce the batch size to 128 when we train the policy network. We use the Adam optimizer\nwith a learning rate schedule that uses linear warmup and cosine decay. The peak learning rate is\n3e-4 for all parameters except the CLIP ViT encoders, for which we use 3e-5. We use 2000 warmup\nsteps and 2e6 decay steps for the learning rate schedule. When we jointly train the alignment and\nbehavioral cloning losses, we use a weight of 1.0 on both terms. These hyperparameters were found\nthrough random search. We train our models for 150k steps, which takes around 13 hours on 2\nGoogle Cloud TPU cores.\nD\nExperimental Details\nThe scenes were constructed with the objects shown in Table 1 within a toy kitchen setup.\nDuring evaluation, we roll out the policy given the instruction for 60 steps. Task success determined\nby a human according to the following criteria:\n13\n\u2022 Tasks that involve putting an object into or on top of a container (e.g. pot, pan, towel) are\njudged successes if any part of the object lies within or on top of the container.\n\u2022 Tasks that involve moving an object toward a certain direction are judged successes if the\nobject is moved sufficiently in the correct direction to be visually noticeable.\n\u2022 Tasks that involve moving an object to a location relative to another object are judged\nsuccesses if the object ends in the correct quadrant and are aligned with the reference object\nas instructed. For example, in \u201dplace the knife in front of the microwave\u201d, the knife should\nbe placed in the top-left quadrant, and be overlapping with the microwave in the horizontal\naxis.\n\u2022 If the robot attempts to grasp any object other than the one instructed, and this results in a\nmovement of the object, then the episode is judged a failure.\nTable 1: Evaluation Scenes\nScene\nObjects\nA\nknife, pepper, towel, & pot\nB\nmushroom, towel, spoon, & pot\nC\ntowel\nE\nExperimental Results\nWe show per-task success rates for our approaches, the baselines, and the ablations in this section.\nThe tasks in scenes A and B were evaluated for 10 trials each, while those in C were evaluated for 5\ntrials.\nTable 2: Comparison of Approaches\nSuccess Rate\nScene\nTask\nGRIF\nLCBC\nLLfP\nR3M\nBC-Z\n. . . . . .\nA\n. . . . . .\nput the yellow bell pepper on the cloth\n0.6\n0.0\n0.0\n0.0\n0.6\nmove the pan to the front\n1.0\n0.0\n0.6\n0.0\n0.0\nput the pan on the towel\n0.8\n0.0\n0.3\n0.0\n0.9\nmove the bell pepper to the left of the table\n0.7\n0.0\n0.0\n0.0\n0.8\nput the bell pepper in the pan\n0.8\n0.0\n0.1\n0.0\n0.3\nput the knife on the purple cloth\n0.7\n0.0\n0.2\n0.0\n0.0\nplace the knife in front of the microwave\n0.7\n0.0\n0.0\n0.0\n0.1\nmove the pan in front of the cloth\n0.6\n0.0\n0.3\n0.0\n0.0\n.\nB\n.\nput the mushroom in the metal pot\n0.9\n0.0\n0.5\n0.0\n0.4\nput the spoon on the towel\n0.9\n0.0\n0.3\n0.0\n0.4\nplace the metal pot on top of the blue towel\n0.8\n0.0\n0.0\n0.0\n0.2\n. .\nC\n. .\nmove the towel to the left\n1.0\n0.0\n1.0\n0.0\n1.0\nmove the towel to the front\n1.0\n0.0\n1.0\n0.0\n1.0\nmove the towel next to the cans\n0.6\n0.0\n0.0\n0.0\n0.2\nmove the towel next to the microwave\n1.0\n0.0\n0.2\n0.0\n0.8\n14\nTable 3: Comparison of Ablations\nSuccess Rate\nScene\nTask\nGRIF (Frozen)\nGRIF (Joint)\nGRIF (Labeled)\n. . . . . .\nA\n. . . . . .\nput the yellow bell pepper on the cloth\n0.6\n0.8\n1.0\nmove the pan to the front\n1.0\n1.0\n0.7\nput the pan on the towel\n0.8\n1.0\n0.1\nmove the bell pepper to the left of the table\n0.7\n0.4\n0.2\nput the bell pepper in the pan\n0.8\n0.6\n1.0\nput the knife on the purple cloth\n0.7\n0.4\n0.1\nplace the knife in front of the microwave\n0.7\n0.6\n0.5\nmove the pan in front of the cloth\n0.6\n0.9\n0.2\nNo Start\nNo Align\nNo CLIP\n. . . . . .\nA\n. . . . . .\nput the yellow bell pepper on the cloth\n0.3\n0.5\n0.0\nmove the pan to the front\n0.6\n0.8\n0.0\nput the pan on the towel\n0.6\n0.6\n0.0\nmove the bell pepper to the left of the table\n0.4\n0.6\n0.2\nput the bell pepper in the pan\n0.7\n0.6\n0.1\nput the knife on the purple cloth\n0.2\n0.2\n0.0\nplace the knife in front of the microwave\n0.1\n0.0\n0.0\nmove the pan in front of the cloth\n0.4\n0.0\n0.3\nF\nAblation of Number of Annotations\nFigure 6: Scaling of GRIF grounding capability by number of language annotations available.\nWe ablate effect of the amount of language supervision on GRIF\u2019s grounding capabilities. We com-\npute the (top-5) text-to-image retrieval accuracy of GRIF representations when trained on 7k, 5k,\n3k, and 1k annotations, and find accuracies of 86%, 81%, 73%, and 52% respectively. These accu-\nracies are plotted in Figure 6. By comparing these accuracies with the grounding performance of the\nablations in Figure 5, we see GRIF enables more robust grounding with little language supervision.\n15\n"
  },
  {
    "title": "SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling",
    "link": "https://arxiv.org/pdf/2307.00804.pdf",
    "upvote": "4",
    "text": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n1\nSketchMetaFace: A Learning-based Sketching\nInterface for High-fidelity 3D Character Face\nModeling\nZhongjin Luo, Dong Du, Heming Zhu,\nYizhou Yu, Fellow, IEEE, Hongbo Fu, Xiaoguang HanB, Member, IEEE\nAbstract\u2014Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute\nsignificant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy\nworkload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling\ndiverse facial shapes and rich geometric details. In this paper, we present SketchMetaFace - a sketching system targeting amateur\nusers to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First,\ncurvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem\nof mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed \u201cImplicit and Depth Guided Mesh\nModeling\u201d (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high\nefficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke\nsuggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and\nvisual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency.\nSketchMetaFace is available at https://zhongjinluo.github.io/SketchMetaFace/.\nIndex Terms\u2014Sketch-based 3D Modeling, 3D Face Modeling\n\u2726\n1\nINTRODUCTION\nC\nREATING 3D virtual avatars is a prolonged research topic in\ncomputer graphics and benefits various usage scenarios such\nas filming, gaming, and art designing. Typically, this process is a\nhighly skilled task, as experienced artists need to spend several\ndays or even months formally sculpting high-fidelity 3D faces\nwith vivid surface details using commercialized 3D modeling\ntools (e.g., ZBrush, MAYA, and 3D MAX). To assist amateur\nusers in freely instantiating their ideas as professional modelers,\nresearchers in computer graphics and human-computer interaction\nhave designed systems that allow users to model 3D shapes with\nfreehand sketches based on geometric principles [1], [2], [3], [4],\n[5], [6]. Although traditional sketch-based 3D modeling systems,\nsuch as Teddy [1] and FiberMesh [2], enable amateur users to\ncreate 3D models, they usually require tremendous manual work\nto specify complex geometry.\nThanks to the recent progress in deep learning, the understand-\ning of freehand sketches and the quality of single-view generation\nhave reached an unprecedented level. Several intelligent sketch-\nbased modeling systems have been developed to enable novice\nusers to create visually plausible 3D models within a few min-\nutes [7], [8], [9]. Closest to our work, DeepSketch2Face [10]\npresents the first deep learning-based sketching system for mod-\neling 3D faces by mapping 2D sketches into a parametric space\nfor face generation. However, considering the limited representa-\n\u2022\nZ. Luo, D. Du, H Zhu, and X. Han are with the School of Science and\nEngineering, The Chinese University of Hong Kong, Shenzhen, China. Y.\nYu is with the Department of Computer Science at the University of Hong\nKong. H. Fu is with the School of Creative Media, City University of Hong\nKong.\n\u2022\nX. Han is the corresponding author. E-mail: hanxiaoguang@cuhk.edu.cn\nManuscript received xxxx.\ntion power of the parametric model, DeepSketch2Face can only\nproduce 3D human faces with fixed styles and cannot be used for\nsculpting expressive skin wrinkles. SimpModeling [11] proposed\na two-phase scheme that allows for diverse animalmorphic head\nmodeling using 3D sketches. Nevertheless, it is challenging for\nusers to work with as it relies on complicated and user-unfriendly\n3D interactions. Additionally, the system struggles to generate\nfine-grained details due to the ambiguity of mono-typed strokes\nand the limited capability of PIFu [12], [13].\nIn this paper, we design and present SketchMetaFace, a pow-\nerful sketch-based 3D face modeling system that addresses the\nfollowing challenges:\nAccuracy. Recent learning-based sketching systems [10], [11],\n[14], [15], [16] allow novice users to create visually-plausible\n3D models with a few strokes. However, they are not capable\nof designing shapes with fine-grained details. To assist users\nin conveying their ideas more accurately, we adopt curvature\nlines [6], [14], [17] in learning-based 3D face modeling. We will\ndemonstrate how the curvature-aware strokes significantly boost\nthe quality of detailed surfaces generated from sketches.\nAlthough existing models [18], [19], [20], [21], [22] can map\n2D images, including sketch images, to 3D shapes, they may\nfail to generate watertight 3D meshes with delicate details. A\nstraightforward way to produce shapes with surface details is to\nblend high-quality multi-view depth maps generated by image\ntranslation [23]. Nonetheless, it is nontrivial to fuse the generated\ndepth maps seamlessly into a watertight mesh. An alternative ap-\nproach is to adopt the pixel-aligned implicit function (PIFu) [12],\n[13] to reconstruct watertight 3D shapes from single images.\nHowever, PIFu exhibits bounded performance in generating high-\nfidelity geometric details. Inspired by the fact that the depth map\narXiv:2307.00804v2  [cs.CV]  4 Jul 2023\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n2\nFig. 1: We present SketchMetaFace, a novel sketching system designed for amateur users to create high-fidelity 3D character faces.\nWith curvature-aware strokes (valley strokes in green and ridge strokes in red), novice users can smoothly customize detailed 3D heads.\nNote that our system only outputs geometry without texture and texturing is achieved using commercial modeling tools.\nproduced by image translation contains more intriguing details\nthan PIFu-generated shapes, we propose IDGMM, i.e., Implicit\nand Depth Guided Mesh Modeling. It enjoys the merits of mesh,\ndepth-map and implicit representations to produce high-quality 3D\nshapes from curvature-aware sketch images.\nUsability. While curvature-aware strokes empowers users to\ncreate 3D faces with fine-grained details, it may increase their\ncognitive load. To address this issue, we interview potential users\nand thoroughly analyse their requirements. We design our system\nbased on the analyzed requirements and formulate a coarse-to-\nfine interactive scheme: users first get used to the system with\nmono-typed sketches and then switch to fine-detail crafting with\ncurvature-aware strokes soon as users get familiar with the system.\nWe also carefully design a stroke suggestion component that\nbridges the gap between coarse and detailed sketching. Moreover,\nto follow the \u201cas-2D-as-possible\u201d principle, we keep the placement\nof ears as the only 3D interaction in our system.\nTo demonstrate the effectiveness of our system, we carefully\nconduct user studies, from which we conclude that our proposed\nsystem exhibits better usability than existing sketch-based 3D\nface modeling systems [10], [11]. Our system allows amateur\nusers to create diverse shapes with fine-grained geometric details.\nBy conducting comparisons against existing inference algorithms\nfor mapping a single sketch to a 3D model, we demonstrate\nthat results generated by our proposed IDGMM better reflect the\nappearances of the input sketches. Ablation studies are conducted\nto justify each design in our interface and algorithm. The contri-\nbutions of our paper can be summarized as follows:\n\u2022\nWe present a novel, easy-to-use sketching system that\nallows amateur users to model high-fidelity 3D character\nfaces in minutes (as seen in Fig. 1).\n\u2022\nWe carefully design a user interface: 1) the face modeling\nwork follows a coarse-to-fine scheme and relies mainly\non intuitive 2D freehand sketches; 2) we adopt curvature-\naware strokes for modeling geometric details; 3) we in-\ntroduce a data-driven suggestion tool to ease the cognitive\nload throughout the sketching process.\n\u2022\nWe propose a novel method, i.e., Implicit and Depth\nGuided Mesh Modeling (IDGMM), which fuses the ad-\nvantages of mesh, implicit, and depth representations for\ndetailed geometry inference from 2D sketches.\n2\nRELATED WORK\nIn this section, we will present relevant studies on 3D avatar\nmodeling, geometrical sketch-based modeling, and data-driven\nsketch-based modeling. We are aware of the breathtaking progress\nin sketch-based 2D image generation of faces [24], [25]. However,\nwe will not discuss these in detail due to the page limit.\n2.1\n3D Face from 2D Image\nCreating visually plausible 3D avatars is a long-standing computer\ngraphics and vision problem. Compared with 3D face reconstruc-\ntion methods, which take multi-view [26], [27] or monocular\nvideo [28], [29] as input, single image reconstruction (SVR) and\nsketch-based modeling provide more casual means for novices to\ncustomize 3D faces. Single-image 3D face reconstruction can be\nroughly divided into two streams, namely, photo-realistic human\nface reconstruction and caricature face reconstruction.\nThe works on single-image photo-realistic face reconstruction\ncan be further separated into two genres, i.e., parametric and\nshape-from-shading methods. However, neither can be directly\nadopted for modeling detailed 3D faces. Parametric-based mod-\nels [30], [31], [32] fall short in representing shapes with novel\nand customized surface details. Shape-from-shading-based meth-\nods [33], [34] suffer from deriving geometric clues from non-\nphoto-realistic image inputs, e.g., caricature images and sketches.\nCompared with single-image realistic 3D faces generation,\nwhich has been extensively studied and achieved exceptionally\nhigh quality, the researches on 3D caricature avatars are relatively\nsparse. A possible reason is that caricature 3D faces are shapes\nwith more diversified geometry, making them extremely hard to be\nregularized into a single parametric model losslessly. Some work\n[35], [36], [37] introduced deformations to increase the capability\nof parametric models. However, their works are still far from\ngenerating high-fidelity 3D caricature shapes of various styles.\nMore importantly, given that most single-image caricature face\nmodeling methods require high-quality images as input, novice\nusers cannot further customize the shape as they wish.\nRecently, researchers have also explored various schemes for\ninteractive modeling from 2D sketch images [10], [16], [18], [19],\n[20], [38], [39]. In line with our work, DeepSketch2Face [10]\nproposed a sketch modeling system that allows users to create\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n3\ncaricature heads from scratch. Their method relies on a CNN-\nbased model to parse a user-drawn sketch as the parameters\nfor a morphable face model. However, since the 3D carica-\nture shape is confined to the parametric caricature face model,\nDeepSketch2Face cannot faithfully reflect large deformations and\nwrinkle details presented in the sketch. To address this issue, SAni-\nHead [16] proposed a view-surface collaborative mesh generative\nnetwork, which turns dual-view freehand sketches into animal-\nmorphic heads. Nevertheless, it fails to synthesize novel shapes\ndeviating from training datasets due to the restricted generalization\nability of their network. Our system utilizes the advantages of\nmesh, depth-map, and implicit representations to generate high-\nquality 3D shapes from curvature-aware sketch images.\n2.2\nGeometrical Sketch-based Modeling\nDesigning free-form 3D shapes via freehand sketching has drawn\nconsiderable attention in recent decades [40]. Igarashi et al. [1]\npioneer by proposing the first sketch-based modeling system that\nallows users to create 3D shapes from scratch by sketching 2D\ncontour lines. A large stream of subsequent researches [41],\n[42], [43], [44], [45], [46] has mainly focused on designing\nnovel interpolation functions to interpolate sketched contours lines\nsmoothly. Unlike the sketch-based modeling systems mentioned\nabove, which take 2D sketches as input, Fibermesh [2] allows\nusers to model free-form surfaces by sketching and manipulating\n3D curves. While Fibermesh [2] and its follow-up systems [47],\n[48] reduce the ambiguity remarkably with explicitly defined 3D\ncurves, they are not capable of or are not friendly for novice users\nto carve organic surface details (e.g., skin wrinkles).\nTo emboss interpolated surfaces with sharper details, various\nmethods introduce sketches with different semantics [49], [50] or\ncurvature cues [6], [17] to formulate more determined constraints.\nHowever, additional inputs may significantly increase novice\nusers\u2019 cognitive load. Inspired by BendSketch [6], our system\nallows users to draw with curvature-aware strokes, which serve\nas a less ambiguous means for users to specify the bulge and\nsink on faces accurately. To reduce the cognitive load of using\ncurvature-aware strokes, we introduce a carefully designed sketch\nsuggestion module to support amateurs in getting familiar with\nour system intuitively.\n2.3\nData-driven Sketch-based Modeling\nThe recent decade has witnessed the emergence of data-driven\nmethods for sketch-based 3D shape generation thanks to large-\nscale 3D datasets. The data-driven sketch-based modeling systems\ncan be roughly divided into two streams regarding the shape\ngeneration approaches, i.e., retrieval-based and learning-based.\nRetrieval-based methods [51], [52], [53], [54] consume a\nfreehand sketch for the query and search for the most similar\nshape from the data warehouse as the reconstruction output. Fan\net al. [55] propose a suggestive interface with shadow guidance to\nguide object sketching. However, shadow guidance may introduce\nsevere visual cluttering for sketches with different semantics. Xie\net al. [56] proposed to retrieve candidate object parts from a\ndatabase with part sketches for further assembly. Recently, deep\nneural networks have been applied for retrieval-based sketch mod-\neling systems [57], which have shown their superiority compared\nto their traditional learning-based counterparts in handling noisy\nsketch input created by novice users. However, limited by the\ncapacity of the data warehouse, retrieval-based sketch modeling\nmay produce shapes that drift away from input sketches.\nIn recent years, learning-based solutions have been popular\nfor sketch-based 3D shape generation and editing [10], [11],\n[15], [18], [19], [20], [38], [39], [58], [59], [60], [61], [62],\n[63], [64]. For example, Nishida et al. [64] proposed inferring\nurban building parameters from freehand sketches with convo-\nlutional neural networks, while Huang et al. [62] presented an\ninteractive modeling system that infers parameters for procedu-\nral modeling from sketches. DeepSketch2Face [10] proposed a\ndeep regression model that converts a sketch into the parameters\nof a morphable 3D caricature face model. However, the above\nparametric regression-based methods work only for 3D shapes\nwithin a specific category that can be easily parameterized. Du\net al. [63] adopted implicit learning to produce artificial object\nparts from sketches and proposed a deep regression model to\npredict the position of the parts, while Sketch2CAD [15] enables\nusers to achieve controllable part-based CAD object modeling by\nsketching in context. SimpModeling [11] utilized a coarse-to-fine\nmodeling scheme, allowing users to create desired animalmorphic\nheads with 3D curves and on-surface sketching. We argue that 2D\nsketching would be more intuitive than 3D sketching since most\nnovice users are more familiar with 2D interfaces and interactions.\nFurthermore, SimpModeling falls short in generating fine-grained\ngeometric details due to the ambiguity of mono-typed strokes and\nthe bounded capability of its shape-generation network. In this\npaper, our system allows users to create 3D high-fidelity facial\nmodels with 2D curvature-aware sketches intuitively.\n3\nUSER INTERFACE\nThis section first summarizes the requirements of designing\nsketch-based modeling for novice users to customize high-fidelity\nfaces of highly diversified styles. On top of the design goals, we\nwill introduce the crucial designs of our system and justify how\nthey reflect the design goals. Please refer to the accompanying\nvideo for sketch-based modeling in action.\n3.1\nDesign Requirements and Analysis\nIn the design process of our sketch-based 3D face modeling\nsystem, we interviewed 11 participants with different levels of\nmodeling experience to analyze the demands for a user-friendly\nsketch-based modeling interface. Three of these participants were\nmodelers with more than five years of experience in 3D modeling,\nwhile the rest were novice users with no or little knowledge of 3D\nmodeling. Based on the responses, we summarize the following\ndesign goals and the corresponding design choices for our system:\nCoarse to Fine (R1). After briefly introducing the background\nknowledge about sketch-based 3D shape modeling, we first dis-\ncuss whether users prefer working in a top-down or bottom-up\nmanner. All experts and most novice users preferred to model\nthe shape in a top-down manner. Therefore, our proposed sketch-\nbased modeling system allows users to model 3D faces in a coarse-\nto-fine manner [11]. In the coarse stage, users can design the\ncontour and the attachment of the faces (e.g., ears). After users\nfinish designing a coarse head shape, they will move on to the fine-\ngrained shape modeling stage, where they can carve geometrical\ndetails such as wrinkles, mouths, eyes, etc. Note that we treat ears\nas attachments and adjust their position through 3D interactive\noperations in the coarse stage since it is difficult to determine the\n3D location of attachments just via frontal-view sketching.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n4\nFig. 2: An illustration of the interactions supported by our system. In the Coarse Shape Modeling stage, users may define coarse 3D\nfaces with frontal-view contouring, profile depth editing, and ear modeling. In the Fine Detail Sketching stage, users can further carve\nfine-grained surface details with the curvature-aware strokes.\nAs 2D as Possible (R2). When discussing whether 3D interactions\nshould be dominant in the system, most novice users mentioned\nthat they prefer to express their ideas through 2D drawings. In-\nterestingly, even professional modelers agree that 2D interactions\nshould be the dominant interaction for the system, as they believe\nnovices may get bored with manipulating the cameras and the 3D\nshapes. To this end, our system follows the \u201cas-2D-as-possible\u201d\nprinciple. Users can finish most of the design only with a 2D\nsketch pad, and 3D interactions (e.g., tuning the layout of ears)\nare introduced only when necessary.\nAgile and Precise (R3). While some amateurs mentioned that\nthey want to carve a 3D face carefully according to a reference\ncharacter face, others only intend to customize a visually-plausible\n3D face with a few strokes. Hence, our system allows users to cus-\ntomize 3D faces with different degrees of interaction complexity,\nas shown in the demo video. Novice users can quickly orchestrate\na visually plausible 3D face with the dedicated sketch stroke\nsuggestion module. The sketch stroke suggestions also serve as\na decent initialization for detailed face modeling. For users who\nare interested in carving customized surface details, we provide\ncurvature-aware strokes that allow the specification of surface\ndetails to be more precise.\n3.2\nCoarse Shape Modeling\nTo support the design requirements mentioned in Section 3.1, in\nour system, the modeling of high-fidelity 3D faces is decomposed\ninto coarse shape modeling and fine detail sketching (R1). Users\nmay start designing a coarse 3D face by drawing face contour lines\non the 2D sketching pad view, as illustrated in Fig. 2. Novice users\ncould switch to the symmetric sketching mode. Under this mode,\nmirror-symmetrical strokes will be generated as the user draws on\nthe sketch pad. In this stage, our system can produce a 3D model\nin a real-time manner by responding to each drawing operation.\nProfile Depth Editing. The essence of our system lies in eliminat-\ning 3D user interactions (R2). However, the generated 3D faces\nwith single-view contour strokes lack depth variances along the\nz-axis due to the missing constraints on the depth channel. To\nthis end, we deliberately design a profile depth editing interaction\nscheme that allows users to specify the face contours in the lateral\nview. Once users switch to the depth editing mode, a new canvas\nwill appear with an initial side-view rendered 3D face contour. As\nseen in Fig. 2, novice users may design shapes with sharp-variant\ndepth by revising the profile sketch without directly manipulating\nthe 3D shapes.\nEar Modeling. The attachments of 3D faces, i.e., the ears, play\nan essential role in shaping a virtual character\u2019s characteristics\nand styles. Unlike nose, eyes, and mouth, ears (and other face\nattachments) are of greater diversity in 3D layout, making it\nchallenging to use only frontal-view sketching to express. To this\nend, our system uses separate meshes to represent the face and the\nears for better expressiveness. Users may customize the ears by\ndrawing their contour lines on the 2D sketch pad view, like spec-\nifying the coarse head shape. Specifically, the ears (also for other\nattachments like horns) are sketched on individual canvas layers,\nwhich facilitate users to manipulate their 2D attachment layouts\nand help the backend models learn diversified attachment shapes.\nAs illustrated in Fig. 2, users can modify the 3D layout of the ears\nin the 3D view for more precise control of the generated shape.\nUsers can also copy attachments as in RigMesh [3]. It is worth\nmentioning that layout manipulation and attachment copying are\nthe only 3D operations in the whole modeling procedure (R2).\n3.3\nFine Detail Sketching\nAfter the user customizes the coarse face shape, they may fur-\nther characterize the detailed facial geometry, e.g., eyes, noses,\nmouth, and wrinkles. Although previous works, e.g., DeepS-\nketch2Face [10] and SimpModeling [11], allow users to edit\nsurface details through 2D and 3D sketching, they fall short in\ngenerating diversified and controllable surface details due to the\nambiguous mono-typed sketch strokes.\nCurvature-aware Strokes. We adopt curvature-aware strokes [6]\nto alleviate the sketch\u2019s ambiguity, enabling users to carve surface\ndetails precisely (R3). Specifically, two types of strokes (i.e., ridge\nand valley) are defined. Before each stroke drawing, the user\nneeds to pick a type first. Different stroke types are visualized\nwith different colors (i.e., red for ridge and green for valley). Our\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n5\nsystem also supports tunable depth for each stroke, which defines\nthe curvature amplitude, i.e., greater depth (darker color) means a\nhigher ridge or deeper valley.\n...\nSuggestion\n...\nFig. 3: An illustration of our stroke suggestion component. Soon\nafter users specify the style, target region, and facial components\nto be modeled, the stroke suggestion component retrieves the\nrelevant curvature-aware strokes. Users may also manipulate the\nlayout for the retrieved strokes through dragging and scaling.\nStroke Suggestions. While curvature-aware strokes significantly\nimprove the controllability of our system, they inevitably bring\nadditional cognitive load for novice users. To address this, we\ncarefully design a data-driven stroke suggestion tool. Consider a\nscenario when a user wishes to draw a pig nose on the face, as\nillustrated in Fig. 3. Our system allows the user to pick the \u201cnose\u201d\ntype and select a \u201cpig\u201d style first, and then draw a contour to\nspecify the rough shape and the location where they wish to place\nthe nose. After that, a set of strokes with the specified category, as\nwell as the corresponding shapes, is retrieved from the database\nand presented as \u201cSuggestion\u201d. The user can picks one which can\nbe placed automatically or after manually adjusting the location\nand size. Users were provided 20 suggestions each time, and\nthe retrieved sketches are editable. With such a suggestion tool,\namateurs can quickly compile a neat 3D face model with the high-\nquality sketch strokes in the database and kick off instantiating\ntheir ideas on a decent basis. The suggestion tool is implemented\nby a retrieval neural network based on the auto-encoder structure,\nplease refer to the supplemental materials for details.\nInstant Shape Preview. An instant preview of the 3D shape could\nserve as guidance for further sketching. However, due to the\ngeometry complexity, the model inference in the stage of fine-\ndetail sketching takes around 0.5s, making it unable to support\nreal-time response. Our video shows that we adopt image space\nrendering and generate the frontal-view normal map as a real-time\nshape preview. Please refer to the supplemental materials for the\nimplementation details of the instant preview module.\n4\nMETHODOLOGY\nIn this section, we present the details of the backend models that\nsupport the interactive sketching interface.\nOverview. Following our coarse-to-fine interface design, we dis-\ncuss the algorithms used for the two stages accordingly. In the\ncoarse stage, as illustrated in Fig. 4, we propose a part-separated\nimplicit learning method that maps the coarse input sketch Sr to\nseparated part meshes (i.e., face and attachments). After the user\ntunes the part layout, these separated meshes are merged into a\nsingle mesh Mc. We then render the outer contour [65] of Mc\ninto the sketch image Sc, on which users can add fine strokes in\nthe detail sketching stage.\nFig. 4: An illustration of o our part-separated coarse modeling of\na 3D face with an outline sketch input Sr. It shows the generation\nof three parts of a face region and two ears using PIFu, and then\nassembles and merges them to obtain a coarse model Mc.\nIn the detail sketching stage, users may further craft fine-\ngrained surface details through sketching on the rendered coarse\nsketch image Sc. To generate detailed geometry Mf from the fine\nsketch Sf, as shown in Fig. 5, we propose IDGMM, which learns\na progressive deformation from Mc to Mf, under the guidance of\nboth the learned implicit field (SDF) and the learned depth map\nfrom Sf.\n4.1\nPreliminary\nBefore introducing the proposed model, we will briefly review\nsome relevant concepts and building blocks.\nPix2Pix. Given a source image Is, Pix2Pix [23] learns a mapping\nfrom Is to a target image It, i.e., f : Is \u2192 It in an adversarial\nmanner. Commonly, a U-Net is adopted to model this translation,\nand the conditional GAN loss and the reconstruction loss (L1 or\nL2 loss) are used for training. In our model, the Pix2Pix module\nis adopted for translations among sketch images, depth maps, and\nnormal maps.\nImplicit Learning. Recently, various deep representations have\nbeen used for 3D shape reconstruction, e.g., voxels, point clouds,\nmeshes, and implicit fields. Among them, implicit field-based\nmethods achieve state-of-the-art performance [66], [67], [68].\nThere are two commonly used formulations to model implicit\nsurfaces: occupancy and signed distance function (SDF). Occu-\npancy is a continuous function go that maps a query point p \u2208 R3\nto a binary status o \u2208 {0, 1}, indicating inside/outside of the\nsurface. SDF is a function gs that maps p to its signed distance\ns to the underlying surface. A multi-layer perception (MLP) is\nusually adopted for approximating go or gs.\nPIFu. Among the works relevant to single image 3D shape\nreconstruction, pixel-aligned implicit function (PIFu) outperforms\nits counterparts in generating results better matching input images.\nSpecifically, PIFu models a function h to map p \u2208 R3 with a\nprojected pixel-aligned feature fp to an occupancy o or SDF value\nd, i.e., h : {p, fp} \u2192 o/d. Firstly, an hourglass architecture [69]\nis applied on I to obtain a feature map If. Then, p is projectd onto\nIf to obtain fp. MLP is used to model h. Our system also requires\ninput-aligned results, so we adopt PIFu as the base module for\nshape inference from sketch images. Our method uses SDF-based\nPIFu since it is more suitable for providing deformation guidance.\nPIFu with Normal Input. As a follow-up work of PIFu, PI-\nFuHD [13] proposed a coarse-to-fine pixel-aligned implicit shape\nlearning pipeline to generate more geometry details. More specifi-\ncally, it utilizes PIFu as the coarse-level learning and adopts gener-\nated normal maps for fine-level learning. Inspired by PIFuHD, we\ninfer normal maps from the input sketch images with Pix2Pix to\nassist in learning fine-grained surface details. Similar to the design\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n6\nFig. 5: The architecture of our IDGMM. (a) Taking a coarse mesh Mc as input, it is first rendered into a depth map Dc. Dc together\nwith the input fine sketch Sf are fed into Pix2Pix-1 to generate a normal map N. N is applied to generate an implicit field using\nPIFu-N. Under the guidance of the SDF field, Mc is deformed to obtain an updated mesh M \u2032\nc. (b) We then render M \u2032\nc into a depth map\nD\u2032\nc, which is enhanced to Df with a Pix2Pix-2 module. After a flow-based local depth alignment, we obtain a high-quality point cloud\nP from the warped depth map. P is locally aligned with M \u2032\nc and used to guide mesh refinement from M \u2032\nc to the resulting mesh Mf.\nNote that the process of sketching is iterative, and the mesh obtained at step (n-1) is used as the input Mc for the n-th step.\nproposed in PIFuHD, we maintain a tiny MLP that extracts local\nimage features from the inferred normal maps to generate high-\nfrequency details. In the following sections, we will use PIFu-N\nto denote our PIFu with normal input.\n4.2\nCoarse Modeling\nIn the coarse stage, users only need to draw a rough outline for\na desired face, i.e., the face contour and attachment contours\n(e.g., ears). A straightforward way to generate a coarse model\nfrom the outline sketch Sr is to use PIFu, which maps Sr to\nan implicit field. Subsequently, Marching Cubes [70] can be\nadopted to extract a mesh from the implicit field. However, as\nthe attachments and the face are represented with a single mesh,\nusers cannot directly manipulate the layout for the attachments,\nthus significantly weakening users\u2019 control over modeling results.\n4.2.1\nPart-separated PIFu\nTo boost the controllability of our system, we present a novel\npart-separated PIFu. Let\u2019s first consider a common scenario where\na face contains a left ear and a right ear. As shown in Fig. 4,\nthree different PIFu modules are used to model the three parts\nseparately. They use different MLPs but share a common encoder\nthat maps Sr to feature maps. In our implementation, the number\nof parts is fixed. The MLPs designed for ear parts can also be used\nto generate ear-like attachments, such as horns.\nThe 3D location of each ear is kept without any normalization\nduring training, which makes the network learn the layout of ears\nautomatically. After obtaining the implicit field of each part, we\nextract separated meshes from them (for better efficiency, 643\nresolution is used for marching cube). After users manipulate 3D\near placements, those meshes are merged into a single one with a\ncorefine-and-compute-union operation provided by CGAL 1. After\nthis step, we apply a remeshing method [71] to get Mc.\n1. CGAL:\nthe\nComputational\nGeometry\nAlgorithms\nLibrary.\nhttps://www.cgal.org/.\nAlthough our curvature-aware strokes contain a \u201cdepth\u201d at-\ntribute for depth controlling, it can only model local depth. Thus\nwe provide a profile sketching tool for global depth editing (as\nseen in Fig. 2). Specifically, the profile contour is treated as the\nhandle to define a Laplacian deformation [72]. Since Mc in the\ncoarse stage is in a low resolution, the Laplacian deformation can\nbe performed in real-time.\n4.2.2\nTraining\nThe part-separated PIFu is trained in a fully-supervised manner.\nFor each character face mesh M in the dataset, we render its\ncontours as a sketch image input. To prepare the ground truth data\nfor training our part-separated PIFu used in the coarse stage, we\nsmooth faces meshes M, and then segment them into distinct parts\n(i.e., faces and attachments). The ground-truth SDF values for each\npart are calculated in the world coordinates. During training, we\nuse the L1 metric to measure the difference between the predicted\nSDF values and the ground truth.\n4.3\nIDGMM: Implicit and Depth Guided Mesh Modeling\nIn the fine stage, Mc is first rendered into a new contour map\nSc. Then users will draw curvature-aware strokes over Sc, and we\ndenote the updated sketch image as Sf. This section discusses the\nmethod to map Sf to a model denoted as Mf. It resembles the\nshape of Sc but contains local geometric details reflected by Sf,\nas illustrated in Fig. 5.\nRecently, many deep-learning-based methods [12], [13], [66],\n[67], [68] have been proposed to map a sketch image to a\n3D model. A straightforward solution is to apply PIFu-based\nmethods [12], [13] and extract the surface mesh with Marching\nCubes (MC) [70]. However, MC is time-consuming (5 seconds\n2563 iso-value grid) when extracting high-resolution surfaces and\nfails to meet the requirements for interactive editing. To this end,\nwe apply the field-guided deformation formula to speed up the\nextraction of detailed surfaces from implicit fields.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n7\nSpecifically, our method takes Mc and Sf as input and learns\nthe displacement for each vertex on Mc with the help of both\nthe implicit and depth-map representations. Before conducting\ndeformation, we subdivide the regions [71] where detail strokes\nare drawn to better afford geometric details. Note that the sketch-\ning process is iterative, and the input Mc at the n-th step is the\nresulting mesh at step (n-1). For simplicity, we still use Mc to\nrepresent the input coarse mesh of each step.\n4.3.1\nImplicit-guided Mesh Updating\nInspired by the work [73], SimpModeling [11] proposed a strategy\nfor mesh deformation under the guidance of implicit fields, but it\nis inefficient: 1) SimpModeling utilizes an occupancy field and\nneeds to determine the updated vertices by a dense sampling\nway; 2) to stabilize the deformation, the Laplacian deformation\ntechnique [72] is adopted.\nIn contrast, we update Mc directly with the guidance of\nthe continuous SDF field to keep robustness during deforma-\ntion, which dramatically reduces the computational cost of the\nLaplacian deformation (i.e., updating each vertex v \u2208 Mc via\nv\u2032 = v + gs(v)n, where n indicates the normal of v and gs(v)\nis the SDF value of v). The above updating mechanism could\nbe performed iteratively for multiple times, but its enhancement\nwas slight. Hence, we only perform one iteration to reduce the\ncomputational burden and leave the remaining detail enhancement\nwork to the depth-guided deformation stage. We denote the new\nmesh after updating as M \u2032\nc.\nA direct way to learn the SDF function from Sf is by applying\nPIFu-N on Sf. However, It may lead to a misalignment between\nthe generated SDF field and the coarse mesh Mc, thus challenging\nthe deformation. Therefore, as illustrated in Fig. 5, we render Mc\ninto a depth map Dc, and feed Dc and Sf together into a Pix2Pix\nmodule to infer a normal map N for conducting PIFu-N.\n4.3.2\nDepth-guided Mesh Refinement\nAlthough normal-assisted PIFu can model details better than other\nexisting methods, generating details as reflected in the normal\nmap is still practically challenging. Our experiments found that\nthe learned depth maps contain richer geometric details than the\nlearned implicit fields. Thus we propose a depth-guided deforma-\ntion method to enhance M \u2032\nc further. Specifically, as illustrated in\nFig. 5, we first render M \u2032\nc into a depth map D\u2032\nc and feed it together\nwith N into a new Pix2Pix module for generating a depth map Df\nwith sharper details than D\u2032\nc. Here, we use N instead of Sf since\nN has already captured the geometric information from Sf and\ncan ease the learning procedure.\nWithout Depth Alignment. To transfer geometric details from Df\nto M \u2032\nc, a straightforward way is to first convert Df to a point cloud\nP and then fit M \u2032\nc to P. Specifically, for each vertex v of M \u2032\nc,\nwe retrieve K closest points in P and employ the inverse distance\nweighting algorithm [74] to directly update the position of v.\nFlow-based Local Depth Alignment. Although the design of the\nmethod discussed above well guarantees global alignment between\nP and M \u2032\nc, there is no guarantee for local alignment. Implicit-\nguided mesh updating is hard to ensure the alignment of local\ngeometry (e.g., nose) between the M \u2032\nc and Sf (thus, both N and\nDf may also suffer from misalignment). Directly fitting M \u2032\nc to\nDf tends to cause artifacts due to the local misalignment between\nthem, as shown in Fig. 6. Multiple iterations and extensive smooth-\nings are required to obtain stable results, which is inefficient and\nmay result in blurry geometric details. To address this issue, we\npropose a flow-based alignment method. More specifically, we\ntrain a FlowNet [75] to take Df and D\u2032\nc as input and output a\nwarping field. The warping field is applied to align Df to M \u2032\nc and\ngenerate an aligned/warped depth D\u2032\nf. Then a high-quality point\ncloud P can be extracted from D\u2032\nf. Thus, P is also locally aligned\nwith M \u2032\nc. The good alignment between P and M \u2032\nc facilitates the\nregistration of local geometric details from P to M \u2032\nc. As a result,\nthe final mesh Mf is close to M \u2032\nc but with more local details,\ninstead of being completely aligned with Sf. The alignment of\nthe sketch, depth maps, and normal map used in Fig. 5 is shown\nin Fig. 7. Although a minor global misalignment exists between\nMf and Sf, the resulting mesh is still plausible and convincing,\nas illustrated in Fig. 9. Thanks to the local alignment, we found\nthat one iteration of the depth-guided mesh refinement is enough\nto reconstruct vivid details stably (the improvement of multiple\niterations is slight), reducing the computational cost.\nFig. 6: An illustration of results without and with Flow-based\nLocal Depth Alignment. (a) the input sketch. (b) the front view\nof the results. (c) the top view of the results. Our flow-based\nalignment (M2) resolves the artifacts caused by directly fitting\nM \u2032\nc to Df without depth alignment (M1).\n4.3.3\nTraining\nIDGMM is backed by four learning-based models: Pix2Pix-1 that\nmaps Sf \u2295 Dc (\u2295 indicates concatenation) to N, Pix2Pix-2 that\nmaps D\u2032\nc \u2295 N to Df, PIFu-N and FlowNet. All the networks are\ntrained separately and in a fully-supervised manner: 1) To train\nPix2Pix-1, for each ground-truth mesh M (which contains rich\ndetails), we render its ridge and valley lines as input fine strokes,\nusing the tool provided by Suggestive Contours [65]. The stroke\ntypes are encoded by the channel of red or green colors, and the\ndepth is encoded with the shades of the color. Specifically, the\nridge is encoded in (c, 0, 0) and the valley in (0, c, 0), c = 255 \u2212\n|k|, where k is the curvature of a line segment. Thus the smaller\nvalue of c, the visually greater color depth (i.e., visually darker),\nrepresenting the higher ridge or deeper valley. In our experiments,\nthe trained model can generalize well to strokes of varying widths,\nthough the strokes in the training set are in a constant width. 2) We\nsmooth M to be Ms and use it as Mc to render depth maps as Dc\nfor training Pix2Pix-1 (N is rendered from M). 3) We put M into\na 1283 SDF field (noted as g128\nM ) and extract the mesh Ml. Then\nwe render Ml into a depth map to approximate D\u2032\nc for training\nPix2Pix-2. 4) We subdivide M to get M \u2032 with dense points and\ndeform M \u2032 under the guidance of g128\nM\nto generate a new mesh\nMg. We render M \u2032 and Mg to depth maps to approximate Df\nand D\u2032\nc. As Mg and M \u2032 are topologically consistent, it is easy to\nobtain a dense flow as supervision to train FlowNet.\n5\nRESULTS AND EVALUATION\nIn this section, we will evaluate our system from two aspects,\nnamely, system usability (Section 5.1) and algorithm effective-\nness (Section 5.2).\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n8\nFig. 7: An illustration of the alignment of the sketch, depth-maps, and normal-map used in Fig. 5. The overlapping images of Sf and\nDc, N, D\u2032\nc, Df, D\u2032\nf are shown above. Note that Dc is rendered by Mc, while D\u2032\nc is rendered by M \u2032\nc. P is extracted from the warped\ndepth (denoted as D\u2032\nf here) generated by FlowNet. The resulting mesh Mf of IDGMM is close to M \u2032\nc but with more local details,\ninstead of being completely aligned with Sf.\n5.1\nEvaluation on System Usability\nApparatus. Our user interface was implemented with QT and\ndeployed on a desktop workstation with one Intel i5 @2.7GHz\nCPU and 8GB of memory. Users can interact with the system\nwith a computer mouse or a pen tablet. The neural-network-based\nbackend model was implemented with Pytorch 1.8.1 and deployed\non a server with one Intel i7 @4.2GHz CPU, 16 GB of memory,\nand one NVIDIA GTX 3090 GPU graphics card. To support the\ntraining of our proposed algorithms for modeling high-fidelity 3D\nheads, we merged the existing datasets of 3DAnimalHead [11]\nand 3DCaricShop [76], resulting in 4,528 high-quality models in\ntotal. Then we split these data into 9:1 for training and testing in\nour experiments. Please refer to our supplemental materials for the\nimplementation details of the neural networks.\nParticipants. Our key objective is to create a 3D modeling system\nthat is easy to use for amateur users without 3D modeling\nexperience. To verify this, we invited 16 subjects (P1-P16, aged\n18 to 32) to participate in this evaluation session, none of whom\nhad experience in 3D modeling. Six of them (P2, P3, P6, P7, P8,\nP12) had professional 2D drawing experience, and the remaining\nhad limited drawing experience. Before the modeling session, each\nparticipant was given 10 minutes to watch an video showing the\nbasic operations of our system. After the tutorial, each user had 20\nminutes to get familiar with our system. All the participants were\nasked to perform comparison and usability studies.\n5.1.1\nComparison Study\nWe first conducted a comparison study on different modeling\nsystems to demonstrate the superiority of our system. After\nthoroughly reviewing existing sketch-based character modeling\nsystems, we chose DeepSketch2Face [10] and SimpModeling [11]\nfor comparison since these systems can be easily accessed. For\nDeepSketch2Face, its released system was used. We asked the\nauthors of SimpModeling to provide their system to us. ZBrush is\na powerful commercial software for assisting professional artists\nin creating arbitrary 3D models. We also added ZBrush to our\ninformal comparison on face modeling. For a fair comparison, all\n16 subjects were also given 10 minutes to learn through a tutorial\nand 20 minutes to get familiar with each of the other systems\nbefore the formal user study. In the formal session, each user\nwas given a shading image of a 3D model as a reference. She/he\nwas requested to create 3D models referring to the given image\nusing the four compared systems (i.e., DeepSketch2Face, Simp-\nModeling, SketchMetaFace, and ZBrush) in random order. Note\nthat all the tools provided by SimpModeling and ZBrush are 3D\ninteractive operations, while most operations of DeepSketch2Face\nand SketchMetaFace focus on the 2D canvas.\nFig. 8 shows the reference images, the created models with the\nfour systems, and the corresponding modeling time. Compared\nto DeepSketch2Face and SimpModeling, our system supported\nusers to create more appealing shapes and craft more vivid surface\ndetails. The geometric shape and surface details created by our\nsystem are closer to the reference models. Compared to ZBrush,\nour system took less time for users to create visually reasonable\n3D models. To complete each model, each user took around 2-\n5 minutes to use DeepSketch2Face, around 7-15 minutes with\nSimpModeling, around 5-9 minutes with our system, and around\n10-18 minutes with ZBrush. Most participants complained that\nDeepSketch2Face was hard to use as it could only output human\nfaces (mainly because of the limited parametric space of the\nhuman face). They mentioned that SimpModeling could create\ncoarse shapes and some minor details, but it was challenging to\nlearn and use. We observed that most subjects got stuck in the\ncoarse shape modeling process with SimpModeling and ZBrush.\nSome even gave up adjusting coarse shapes and directly turned to\nsculpting surface details. \u201cThe 3D operations are difficult to use,\nand I need to speed time adjusting the shape. I am disappointed\nwith SimpModleing and ZBrush\u201d, as commented by P8. \u201c3D\ninteractions are extremely unfriendly to me. I need to switch\nperspectives frequently. These frequent switching operations make\nme irritable\u201d (P11). Most subjects enjoyed the modeling pro-\ncess defined by SketchMetaFace. Some participants reported that\nSketchMetaFace was user-friendly and allowed for creating vivid\navatar heads easily. They also pointed out that our system saved\nmuch time and labor in generating 3D heads. \u201cSketchMetaFace\nis much better than SimModeling. The coarse shape modeling\nprovided by SketchMetaFace is easier and can save me a lot of\ntime. The curvature-aware strokes allow me to craft details freely\nin an intuitive way\u201d (P6). \u201cIt is very cool to create 3D models by\ndrawing sketches. I am looking forward to using SketchMetaFace\nin the future.\u201d P1 suggested that the 3D sculpting tools (e.g.,\nsmooth and crease) provided by ZBrush could be added to the fine\nstage, supporting users in further fine-tuning geometric details.\n5.1.2\nUsability Study\nIn this study, each participant was asked to freely create at least\none model without restrictions on result diversity, result quality,\nor time duration. Fig. 9 shows a gallery of models created by\nthese participants, which reflect the expressiveness of our system.\nIt can be seen from this figure that our system supports amateurs\nin geometrical modeling to create character faces with diversified\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n9\nFig. 8: Comparison between our system against the state of the arts. The results in each row were created by the same user given a\nreference in (a). For each system, we show the sketch, resulting model, drawing time, and the corresponding participant.\nshapes and rich geometric details. All of the participants felt that\nour system was powerful in creating diversified avatar heads, and\nthey were deeply impressed by the simplicity, intuitiveness, and\ncontrollability of our system. It is worth mentioning that two\nof the participants said they enjoyed the process very much and\nexpressed their desire to learn 3D modeling.\nMost of the participants liked the intuitive stroke suggestion\ntool, which was quite helpful for them in figuring out the meaning\nof curvature-aware strokes. We observed that the participants with\ngreat drawing skills (i.e., P2, P3, P6, P7, P8, and P12) quickly\nbecame used to working with the curvature-aware strokes thanks\nto the suggestion tool. Once grasping curvature-aware strokes,\nthey preferred to paint each part of the model from scratch\nand customize desired details by themselves instead of searching\nfor a specific structure using the stroke suggestion module. P6\ncommented \u201cThe stroke suggestion tool is a very nice and useful\nfunction for assisting me in understanding the usage of curvature-\naware strokes.\u201d We received similar positive comments from P7\nand P12: \u201cWith the help of the stroke suggestion function, I\ncan easily understand how to depict geometric structures using\ncurvature-aware strokes\u201d (P7); \u201cThe curvature-aware strokes are\nuseful and powerful for carving models\u2019 details, like wrinkles\u201d\n(P12). Other participants tended to use the stroke suggestion func-\ntion throughout the whole modeling process due to their limited\ndrawing skills. \u201cThe suggestion module is easy and intuitive to\nuse. I do not need to spend much time thinking about how to\npaint a correct sketch. It avoids frequent modifying operations\u201d\n(P1). \u201cThe suggestion module is convenient and friendly for me.\nIt reduces a lot of manual operations and allows me to create\ndiversified results in a very easy way\u201d (P5). \u201cI can make funny\nand realistic results by simply searching and integrating different\nparts in minutes (two eyes, a nose, and a mouth)\u201d (P10).\nThe participants also provided some constructive comments.\nFor example, P4 said, \u201cIt would be better to allow me to search\nfor a suitable head contour in the coarse modeling stage, just like\nsearching for a nose or a mouth in the fine stage.\u201d One potential\nsolution is collecting a coarse shape database and applying the re-\ntrieval mechanism in the coarse-shape modeling stage. \u201cAlthough\nthe profile depth editing tool allows me to adjust models in the\nside view, the system still fails to create an elephant\u2019s nose. I\ndo not know how to create an elephant\u2019s nose using the tools\nprovided by SketchMetaFace.\u201d said P2. Enlarging our datasets\nand adopting multi-view drawing in the coarse stage would be\na possible solution for this problem.\n5.1.3\nQuestionnaire Analysis\nAt the end of the comparison study, each participant was required\nto complete a System Usability Scale (SUS) questionnaire and\na NASA Task Load Index (NASA-TLX) questionnaire to eval-\nuate the usability and workload of our system. We found that\nthe overall SUS score of our system was 79, out of a scale\nof 100 (DeepSketch2Face: 64, SimpModeling: 38, ZBrush: 41),\nindicating the good usability of our system [77]. In Fig. 10(a),\nwe show the mean scores for all the individual SUS questions.\nFor the questions with the odd numbers, the higher the SUS\nscores, the better; for the rest of the questions, the lower the\nSUS scores, the better. The scores of Q1 and Q9 suggest that\nthe participants appreciated our system and were satisfied with the\nmodels created by our system. From Q2-4, Q7-8, and Q10, we can\nconclude that our system supported amateur users creating desired\n3D head models easily and intuitively, indicating the good user\nefficiency and usability of our system. The scores of Q5-6 show\nthat the participants also recognized our system\u2019s well-designed\nmodeling pipeline and tools. Although the high scores of Q3 and\nQ7 indicate that DeepSketch2Face is easy to use, the participants\nwere disappointed with its results, leading to low scores for Q1\nand Q9. The high scores of Q2, Q4, Q6, Q8, and Q10 and the\nlow scores of Q3, Q7, and Q9 all suggest that SimpModleing and\nZBrush are unfriendly for these amateur uses. Grasping these two\nsystems is extremely hard for them.\nFig. 10(b) illustrates the average score for each question in\nthe NASA-FLX questionnaire. The results of our systems are also\npositive. Compared to SimpModeling and ZBrush, our system\u2019s\nmental demand, physical demand, temporal demand, effort, and\nfrustration are at an extremely low level. It implies that our system\ndoes not require users to pay a lot of concentration and effort\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n10\nFig. 9: The gallery of our results. All models are created by amateur users who are trained to use our system with a tutorial. Thanks\nto the easy-to-use two-stage modeling design and the stroke suggestion component, the users can complete each model design in 5-9\nminutes. The three results in the first row were created using the same coarse mesh but applying different surface details.\nFig. 10: (a) Mean scores of SUS in a 5-point scale. (b) Mean scores of NASA-TLX in a 5-point scale. (c) Perceptive evaluation\non results of the compared systems. (d) Perceptive evaluation on coarse shape modeling. (e) Perceptive evaluation on surface detail\ngeneration. (f) Perceptive evaluation on implicit/depth guidance. Each error bar represents the standard deviation of the corresponding\nmean.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n11\nwhen using it. The higher performance score of our system reflects\nthat the participants were also more satisfied with their modeling\nresults with our system. The lower performance score and the\nhigher frustration score of SimpModeling and ZBrush than those\nof our system suggest that it was hard for the participants to\ncreate desired results using 3D operations. The lower performance\nscore of DeepSketch2Face demonstrates that the participants were\nunsatisfied with the results generated by its algorithm, which also\nleads to a high frustration level.\nWe conducted a subjective user study to evaluate the faith-\nfulness (i.e., the degree of fitness to reference images/models)\nof synthesized results. We randomly chose a set of results from\nthe comparison study, containing 15 reference models and the\ncorresponding results created by the participants using the four\nabove systems. We invited 50 subjects to participate in this sub-\njective evaluation through an online questionnaire. Most subjects\nhad no 3D modeling experience, and none had participated in\nthe previous studies. We showed the participants five images for\neach case (15 cases in total), including the input sketch and the\nfour modeling results by the compared systems, placed side by\nside in random order. Each participant was asked to score each\nresult based on the faithfulness to the reference model (1 denoting\nthe lowest fitness and 10 for the highest fitness). Fig. 10(c)\nshows the mean score of each system for this study. This figure\nshows that the 3D models created by amateurs with our system\nin the comparison study received relatively higher marks than\nthe counterpart systems, implying that our system could assist\nnovice users in creating desired 3D heads. Statistical analysis also\nshowed that the scores significantly differed across the compared\nsystems. Specifically, we ran Shapiro-Wilk normality tests on the\ncollected data and found non-normality distributions (p < 0.001).\nWe thus conducted Kruskal-Wallis tests on the faithfulness scores\nand found significant effects. Paired tests between our system and\neach of the compared ones confirmed that our system (mean: 6.28)\ncould effectively support amateurs in creating significantly more\nfaithful results to the reference models than the other systems,\ni.e., DeepSketch2Face (mean: 1.96, p < 0.001), SimpModeling\n(mean: 3.64, p < 0.001) and ZBrush (mean: 5.82, p = 0.008).\nMore details can be found in our supplementary material.\n5.2\nEvaluation on Algorithm Effectiveness\nComparison on Part-separated Mesh Inference. There are some\nalternative methods [21], [57], [78] for inferring part-separated\nmeshes from an input sketch. To verify the generalization ability\nof part-separated PIFu, we choose two representative alternative\nmethods for comparison. One is a retrieval-based method [57],\ndenoted as Retrieval and the other one is a deformation-based\nmethod [21], denoted as Pixel2Mesh. The qualitative comparisons\nare presented in Fig. 11, where we can see that our results align\nmuch better with the input sketches.\nComparisons on Sketch2Mesh. The core problem of our system\nis to learn the mapping from Sf to a detailed mesh. To evaluate\nthe superiority of IDGMM, we selected four existing represen-\ntative methods for comparison: 3D-R2N2 [79], Pixel2Mesh [21],\nDeepSDF [67] and PIFuHD [13] (the method used by SimpMod-\neling). All these methods took Sf and Dc as input for fairness.\nFig. 12 and Tab. 1 show the results of this comparison. Both\nqualitative and quantitative results demonstrate the superiority of\nour method. Although PIFuHD performs not badly on quantitative\nmeasurements, the qualitative results show that our proposed\nFig. 11: Qualitative comparisons on part-separated mesh inference\nfrom an input sketch (a). (b) The results of retrieval. (c) The results\nof Pixel2Mesh. (d) The results of our part-separated PIFu.\nalgorithm (IDGMM) performs much better than PIFuHD on\ngeometric details synthesis. Meanwhile, PIFuHD requires a time-\nconsuming mesh extraction process from an implicit field (around\n5.0s for one result generation). SimpModeling slightly reduces\nPIFuHD\u2019s time consumption by sampling points along the normal\ndirections and applying local Laplacian deformation (1.0s for one\nresult generation). Our IDGMM combines the advantages of mesh,\ncontinuous SDF, and depth map representations, making it very\npowerful not only in generating detailed 3D geometry but also in\ninference efficiency (around 0.5s for one result generation).\n(a) Input\n(b) 3D-R2N2 (c) Pixel2Mesh (d) DeepSDF\n(e) PIFuHD\n(f) Ours\nFig. 12: Qualitative comparisons of our IDGMM with four exist-\ning methods for Sketch2Mesh inference.\nAblation Study on Implicit/Depth Guidance. There are two\nkey components in our proposed IDGMM: implicit-guided mesh\nupdating and depth-guided mesh refinement. To verify the indis-\npensability of these two modules, we compared IDGMM with\ntwo alternative settings: 1) without implicit guidance - we use\nDc and N as input to generate Df and corresponding warped\nP, which is then used to guide the deformation from Mc. 2)\nwithout depth guidance, i.e., M \u2032\nc shown in Fig. 5. Qualitative\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n12\nTABLE 1: Quantitative comparison with our proposed IDGMM\nwith four existing methods for Sketch2Mesh inference. We adopt\nIoU, Chamfer-L2, and normal consistency to evaluate the results.\nIoU \u2191\nChamfer-L2 (\u00d7102) \u2193\nNormal-Consis. \u2191\n3D-R2N2\n0.858\n0.149\n0.929\nPixel2Mesh\n0.882\n0.123\n0.937\nDeepSDF\n0.894\n0.117\n0.949\nPIFuHD\n0.911\n0.103\n0.955\nOurs\n0.915\n0.099\n0.956\nFig. 13: Ablation study on implicit/depth guidance. From left\nto right: (a) input sketch; (b) coarse mesh (i.e., Mc in Fig. 5);\n(c) resulting mesh with only depth guidance (without implicit\nguidance); (d) resulting mesh with only implicit guidance (without\ndepth guidance, i.e., M \u2032\nc in Fig. 5); (e) resulting mesh with both\nguidance (i.e., Mf in Fig. 5).\n(a) without curvature-aware strokes\n(b) with curvature-aware strokes\nFig. 14: Ablation study on without/with curvature-aware strokes.\nUsing curvature-aware strokes significantly helps enhance the\nquality of the generated geometric details.\nresults are shown in Fig. 13. The resulting meshes with both\nimplicit and depth guidance outperform the other two options on\nsurface detail generation, implying the necessity of the implicit-\nguided and depth-guided modules.\nAblation Study on Curvature-aware Strokes. The common option\nto represent sketches is using strokes without any curvature-aware\nattributes (e.g., DeepSketch2Face and SimpModeling), which is\nhard to depict complex surface details, as seen in the left part of\nFig. 14. The right part of Fig. 14 shows the great capability of\ncurvature-aware strokes in representing rich geometric details.\nPerceptive Evaluation Study. To further evaluate the effectiveness\nand superiority of our proposed algorithm (part-separated PIFu\nand IDGMM), we conducted another perceptive evaluation study.\nWe selected 10 samples from the experiments of Comparison\non Part-separated Mesh Inference (like Fig. 11), Comparisons on\nSketch2Mesh (like Fig. 12), and Ablation Study on Implicit/Depth\nGuidance (like Fig. 13) respectively, resulting in three question-\nnaires. Each case in the questionnaires showed the input sketch\nand the results generated by different algorithms, placed side by\nside in random order. The 50 subjects mentioned above were also\nasked to evaluate each synthesized model\u2019s faithfulness (i.e., the\ndegree of fitness to input sketches) on a ten-point Likert scale (1 =\nlowest fitness to 10 = highest fitness). Fig. 10(d) shows that the re-\nsults generated by part-separated PIFu fit the input sketches better\nthan Retrieval and Pixel2Mesh. Fig. 10(e) suggests that IDGMM\ncould synthesize richer, more vivid, and more realistic geometric\ndetails than the other methods. Fig. 10(f) indicates the necessity\nand superiority of combining implicit and depth guidance for\ndetailed geometry generation. For statistical analysis, we first\nperformed Shapiro-Wilk normality tests, respectively, for the three\ncollected data and found that all of them followed non-normality\ndistributions (p < 0.001). Therefore, we conducted a Kruskal-\nWallis test on the faithfulness scores for each perceptive evalu-\nation, and the results also showed significance across different\ncomparisons. For the evaluation of coarse shape modeling, paired\ntests showed that our method (mean: 8.60) performs significantly\nbetter on diverse shape generation than both Retrieval (mean: 3.85,\np < 0.001) and Pixel2Mesh (mean: 5.38, p < 0.001). For the\nevaluation of surface detail generation, the results indicated that\nIDGMM (mean: 8.90) led to significantly more faithful results\nthan the other methods, i.e., 3D-R2N2 (mean: 3.25, p < 0.001),\nPixel2Mesh (mean: 3.89, p < 0.001), DeepSDF (mean: 5.43,\np < 0.001), and PIFuHD (mean: 6.63, p < 0.001). For the\nevaluation of implicit/depth guidance, the tests suggested that\ndepth&implicit guidance (mean: 8.55) significantly performs bet-\nter on geometric detail synthesis than the alternative options, i.e.,\nonly implicit guidance (mean: 6.23, p < 0.001) and only depth\nguidance (mean: 5.95, p < 0.001). It is worth mentioning that the\ndifference between depth and implicit guidance was not distinct\n(p = 0.169). This is consistent with our expectation, since both\nonly using depth refinement and only using implicit refinement\ncan synthesize minor details. But they fail to depict high-quality\ngeometric details, further confirming the significant positive effect\nof incorporating implicit and depth refinement. All these statistical\nresults confirmed that all our proposed algorithms significantly\noutperform the corresponding alternative options. More details\nabout evaluation are provided in our supplementary material.\n6\nCONCLUSION\nIn this paper, we presented an easy-to-use sketching system for\namateur users to create and high-fidelity 3D face models. Both\nthe user interface and the algorithm are carefully designed. Firstly,\ncurvature-aware strokes are utilized to assist users in easily carving\ngeometric details. Secondly, a coarse-to-fine interface is designed.\nIn the coarse stage, users only need to model face contours and\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n13\n(a)\n(b)\nFig. 15: Limitations of our system. Our system also suffers from\nlimitations when a) modeling facial components or details with\ncomplex depth changes; b) strokes are placed too densely.\nthe 3D layout of ears. Then, in the fine stage, all interactions are\noperated on a 2D canvas for detail drawing. Thirdly, to support\nthe accuracy and usability of the user interface, a novel method,\nnamed Implicit and Depth guided Mesh Modeling (IDGMM), is\nproposed. It combines the advantages of the implicit (SDF), mesh,\nand depth representations, and reaches a good balance between\noutput quality and inference efficiency. Both evaluations of the\nsystem and algorithm demonstrate that our system is of better\nusability than existing systems and the proposed IDGMM also\noutperforms existing methods.\nAlthough our system is able to create 3D models with diversi-\nfied shapes and rich details, it also has some limitations (Fig. 15):\na) As we only focus on frontal-view sketching for detail carving,\nsome organs with complex depth changing are hard to model,\nsuch as the nose of an elephant; b) When the strokes are densely\nplaced, it cannot produce reasonable geometric details as a large\nnumber of vertices are required in this scenario, which our current\nsystem does not support. In the future, we will enlarge our dataset\nto support users in modeling shapes with other categories, such as\ncartoon character bodies and human garments. We will also try to\ntake multi-view sketches as input to further support the creation of\ncomplex models, such as elephants. Meanwhile, we will explore\nthe possibilities to carve high-resolution models efficiently and\nsupport richer detail crafting effectively.\nAcknowledgements. The work was supported in part by NSFC-\n62172348, the Basic Research Project No. HZQB-KCZYZ-\n2021067\nof\nHetao\nShenzhen-HK\nS&T\nCooperation\nZone,\nthe National Key R&D Program of China with grant No.\n2018YFB1800800, the Shenzhen Outstanding Talents Train-\ning\nFund\n202002,\nthe\nGuangdong\nResearch\nProjects\nNo.\n2017ZT07X152 and No. 2019CX01X104, the Guangdong Provin-\ncial Key Laboratory of Future Networks of Intelligence (Grant No.\n2022B1212010001), the Shenzhen Key Laboratory of Big Data\nand Artificial Intelligence (Grant No. ZDSYS201707251409055),\nand the Key Area R&D Program of Guangdong Province\nwith grant No. 2018B030338001. It was also partially sup-\nported by Outstanding Yound Fund of Guangdong Province\nwith No. 2023B1515020055, Shenzhen General Project with No.\nJCYJ20220530143604010, Hong Kong Research Grants Council\nunder General Research Funds (HKU17206218), grants from the\nResearch Grants Council of the Hong Kong Special Adminis-\ntrative Region, China (No. CityU 11212119) and the Centre for\nApplied Computing and Interactive Media (ACIM) of School of\nCreative Media, CityU.\nREFERENCES\n[1]\nT. Igarashi, S. MATSUOKA, and H. TANAKA, \u201cTeddy: A sketching\ninterface for 3d freeform design,\u201d in Computer graphics proceedings,\nannual conference series.\nAssociation for Computing Machinery\nSIGGRAPH, 1999, pp. 409\u2013416. 1, 3\n[2]\nA. Nealen, T. Igarashi, O. Sorkine, and M. Alexa, \u201cFibermesh: designing\nfreeform surfaces with 3d curves,\u201d in ACM SIGGRAPH 2007 papers,\n2007, pp. 41\u2013es. 1, 3\n[3]\nP. Borosan, M. Jin, D. DeCarlo, Y. Gingold, and A. Nealen, \u201cRigMesh:\nAutomatic rigging for part-based shape modeling and deformation,\u201d ACM\nTransactions on Graphics (TOG), vol. 31, no. 6, pp. 198:1\u2013198:9, Nov.\n2012. [Online]. Available: http://doi.acm.org/10.1145/2366145.2366217\n1, 4\n[4]\nD. S`ykora, L. Kavan, M. \u02c7Cad\u00b4\u0131k, O. Jamri\u02c7ska, A. Jacobson, B. Whited,\nM. Simmons, and O. Sorkine-Hornung, \u201cInk-and-ray: Bas-relief meshes\nfor adding global illumination effects to hand-drawn characters,\u201d ACM\nTransactions on Graphics (TOG), vol. 33, no. 2, pp. 1\u201315, 2014. 1\n[5]\nH. Pan, Y. Liu, A. Sheffer, N. Vining, C.-J. Li, and W. Wang, \u201cFlow\naligned surfacing of curve networks,\u201d ACM Transactions on Graphics\n(TOG), vol. 34, no. 4, pp. 1\u201310, 2015. 1\n[6]\nC. Li, H. Pan, Y. Liu, X. Tong, A. Sheffer, and W. Wang, \u201cBendsketch:\nmodeling freeform surfaces through 2d sketching,\u201d ACM Transactions on\nGraphics (TOG), vol. 36, no. 4, pp. 1\u201314, 2017. 1, 3, 4\n[7]\nY. Zhong, Y. Gryaditskaya, H. Zhang, and Y.-Z. Song, \u201cDeep sketch-\nbased modeling: Tips and tricks,\u201d in 2020 International Conference on\n3D Vision (3DV).\nIEEE, 2020, pp. 543\u2013552. 1\n[8]\n\u2014\u2014, \u201cA study of deep single sketch-based modeling: View/style invari-\nance, sparsity and latent space disentanglement,\u201d Computers & Graphics,\nvol. 106, pp. 237\u2013247, 2022. 1\n[9]\nP. Xu, T. M. Hospedales, Q. Yin, Y.-Z. Song, T. Xiang, and L. Wang,\n\u201cDeep learning for free-hand sketch: A survey,\u201d IEEE transactions on\npattern analysis and machine intelligence, vol. 45, no. 1, pp. 285\u2013312,\n2022. 1\n[10] X. Han, C. Gao, and Y. Yu, \u201cDeepsketch2face: a deep learning based\nsketching system for 3d face and caricature modeling,\u201d ACM Transac-\ntions on graphics (TOG), vol. 36, no. 4, pp. 1\u201312, 2017.\n1, 2, 3, 4,\n8\n[11] Z. Luo, J. Zhou, H. Zhu, D. Du, X. Han, and H. Fu, \u201cSimpmodeling:\nSketching implicit field to guide mesh modeling for 3d animalmorphic\nhead design,\u201d in The 34th Annual ACM Symposium on User Interface\nSoftware and Technology, 2021, pp. 854\u2013863. 1, 2, 3, 4, 7, 8\n[12] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li,\n\u201cPifu: Pixel-aligned implicit function for high-resolution clothed human\ndigitization,\u201d in Proceedings of the IEEE International Conference on\nComputer Vision, 2019, pp. 2304\u20132314. 1, 6\n[13] S. Saito, T. Simon, J. Saragih, and H. Joo, \u201cPifuhd: Multi-level pixel-\naligned implicit function for high-resolution 3d human digitization,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 84\u201393. 1, 5, 6, 11\n[14] C. Li, H. Pan, Y. Liu, X. Tong, A. Sheffer, and W. Wang, \u201cRobust flow-\nguided neural prediction for sketch-based freeform surface modeling,\u201d\nACM Transactions on Graphics (TOG), vol. 37, no. 6, pp. 1\u201312, 2018. 1\n[15] C. Li, H. Pan, A. Bousseau, and N. J. Mitra, \u201cSketch2cad: Sequential\ncad modeling by sketching in context,\u201d ACM Transactions on Graphics\n(TOG), vol. 39, no. 6, pp. 1\u201314, 2020. 1, 3\n[16] D. Du, X. Han, H. Fu, F. Wu, Y. Yu, S. Cui, and L. Liu, \u201cSanihead:\nSketching animal-like 3d character heads using a view-surface collabora-\ntive mesh generative network,\u201d IEEE Transactions on Visualization and\nComputer Graphics, 2020. 1, 2, 3\n[17] E. Iarussi, D. Bommes, and A. Bousseau, \u201cBendfields: Regularized\ncurvature fields from rough concept sketches,\u201d ACM Transactions on\nGraphics (TOG), vol. 34, no. 3, pp. 1\u201316, 2015. 1, 3\n[18] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang, \u201c3d shape\nreconstruction from sketches via multi-view convolutional networks,\u201d in\n2017 International Conference on 3D Vision (3DV).\nIEEE, 2017, pp.\n67\u201377. 1, 2, 3\n[19] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, and A. Bousseau, \u201c3d\nsketching using multi-view deep volumetric prediction,\u201d Proceedings of\nthe ACM on Computer Graphics and Interactive Techniques, vol. 1, no. 1,\npp. 1\u201322, 2018. 1, 2, 3\n[20] J. Wang, J. Lin, Q. Yu, R. Liu, Y. Chen, and S. X. Yu, \u201c3d shape\nreconstruction from free-hand sketches,\u201d in Computer Vision\u2013ECCV\n2022 Workshops: Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart VIII.\nSpringer, 2023, pp. 184\u2013202. 1, 2, 3\n[21] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, \u201cPixel2mesh:\nGenerating 3d mesh models from single rgb images,\u201d in Proceedings of\nthe European Conference on Computer Vision (ECCV), 2018, pp. 52\u201367.\n1, 11\n[22] B. Guillard, E. Remelli, P. Yvernay, and P. Fua, \u201cSketch2mesh: Re-\nconstructing and editing 3d shapes from sketches,\u201d in Proceedings of\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n14\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 13 023\u201313 032. 1\n[23] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-image translation\nwith conditional adversarial networks,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 1125\u2013\n1134. 1, 5\n[24] S.-Y. Chen, F.-L. Liu, Y.-K. Lai, P. L. Rosin, C. Li, H. Fu, and L. Gao,\n\u201cDeepfaceediting: Deep face generation and editing with disentangled\ngeometry and appearance control,\u201d ACM Trans. Graph., vol. 40, no. 4,\njul 2021. [Online]. Available: https://doi.org/10.1145/3450626.3459760\n2\n[25] S.-Y. Chen, W. Su, L. Gao, S. Xia, and H. Fu, \u201cDeepfacedrawing:\nDeep generation of face images from sketches,\u201d ACM Transactions on\nGraphics (TOG), vol. 39, no. 4, pp. 72\u20131, 2020. 2\n[26] Y. Xiao, H. Zhu, H. Yang, Z. Diao, X. Lu, and X. Cao, \u201cDetailed facial\ngeometry recovery from multi-view images by learning an implicit func-\ntion,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence,\n2022. 2\n[27] Z. Bai, Z. Cui, J. A. Rahim, X. Liu, and P. Tan, \u201cDeep facial non-rigid\nmulti-view stereo,\u201d in IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2020. 2\n[28] M. B R, A. Tewari, H.-P. Seidel, M. Elgharib, and C. Theobalt, \u201cLearning\ncomplete 3d morphable face models from images and videos,\u201d in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021. 2\n[29] P. Garrido, M. Zollh\u00a8ofer, D. Casas, L. Valgaerts, K. Varanasi, P. Perez,\nand C. Theobalt, \u201cReconstruction of personalized 3d face rigs from\nmonocular video,\u201d ACM Trans. Graph. (Presented at SIGGRAPH 2016),\nvol. 35, no. 3, pp. 28:1\u201328:15, 2016. 2\n[30] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou, \u201cFacewarehouse: A 3d\nfacial expression database for visual computing,\u201d IEEE Transactions on\nVisualization and Computer Graphics, vol. 20, no. 3, pp. 413\u2013425, 2013.\n2\n[31] Y. Deng, J. Yang, S. Xu, D. Chen, Y. Jia, and X. Tong, \u201cAccurate 3d face\nreconstruction with weakly-supervised learning: From single image to\nimage set,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 2019, pp. 0\u20130. 2\n[32] L. Tran and X. Liu, \u201cNonlinear 3d face morphable model,\u201d in Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\n2018, pp. 7346\u20137355. 2\n[33] E. Richardson, M. Sela, R. Or-El, and R. Kimmel, \u201cLearning detailed\nface reconstruction from a single image,\u201d in proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 1259\u2013\n1268. 2\n[34] A. Tuan Tran, T. Hassner, I. Masi, E. Paz, Y. Nirkin, and G. Medioni,\n\u201cExtreme 3d face reconstruction: Seeing through occlusions,\u201d in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3935\u20133944. 2\n[35] J. Liu, Y. Chen, C. Miao, J. Xie, C. X. Ling, X. Gao, and W. Gao, \u201cSemi-\nsupervised learning in reconstructed manifold space for 3d caricature\ngeneration,\u201d in Computer Graphics Forum, vol. 28, no. 8.\nWiley Online\nLibrary, 2009, pp. 2104\u20132116. 2\n[36] J. Zhang, H. Cai, Y. Guo, and Z. Peng, \u201cLandmark detection and 3d face\nreconstruction for caricature using a nonlinear parametric model,\u201d arXiv\npreprint arXiv:2004.09190, 2020. 2\n[37] Q. Wu, J. Zhang, Y.-K. Lai, J. Zheng, and J. Cai, \u201cAlive caricature from\n2d to 3d,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018, pp. 7336\u20137345. 2\n[38] S.-H. Zhang, Y.-C. Guo, and Q.-W. Gu, \u201cSketch2model: View-aware\n3d modeling from single free-hand sketches,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 6012\u20136021. 2, 3\n[39] P. N. Chowdhury, T. Wang, D. Ceylan, Y.-Z. Song, and Y. Gryaditskaya,\n\u201cGarment ideation: Iterative view-aware sketch-based garment model-\ning,\u201d in 10th International Conference on 3D Vision (3DV 2022).\n2,\n3\n[40] C. Ding and L. Liu, \u201cA survey of sketch based modeling systems,\u201d\nFrontiers of Computer Science, vol. 10, no. 6, pp. 985\u2013999, 2016. 3\n[41] O. A. Karpenko and J. F. Hughes, \u201cSmoothsketch: 3d free-form shapes\nfrom complex sketches,\u201d in ACM SIGGRAPH 2006 Papers, 2006, pp.\n589\u2013598. 3\n[42] R. Schmidt, B. Wyvill, M. C. Sousa, and J. A. Jorge, \u201cShapeshop: Sketch-\nbased solid modeling with blobtrees,\u201d in ACM SIGGRAPH 2007 courses,\n2007, pp. 43\u2013es. 3\n[43] A. Bernhardt, A. Pihuit, M.-P. Cani, and L. Barthe, \u201cMatisse: Painting\n2d regions for modeling free-form shapes,\u201d in SBM\u201908-Eurographics\nWorkshop on Sketch-Based Interfaces and Modeling.\nEurographics\nAssociation, 2008, pp. 57\u201364. 3\n[44] P. Joshi and N. A. Carr, \u201cRepouss\u00b4e: Automatic inflation of 2d artwork.\u201d\nin SBM, 2008, pp. 49\u201355. 3\n[45] Y. Gingold, T. Igarashi, and D. Zorin, \u201cStructured annotations for 2d-\nto-3d modeling,\u201d in ACM SIGGRAPH Asia 2009 papers, 2009, pp. 1\u20139.\n3\n[46] L. Olsen, F. Samavati, and J. Jorge, \u201cNaturasketch: Modeling from im-\nages and natural sketches,\u201d IEEE Computer Graphics and Applications,\nvol. 31, no. 6, pp. 24\u201334, 2011. 3\n[47] S.-H. Bae, R. Balakrishnan, and K. Singh, \u201cIlovesketch: as-natural-as-\npossible sketching system for creating 3d curve models,\u201d in Proceedings\nof the 21st annual ACM symposium on User interface software and\ntechnology, 2008, pp. 151\u2013160. 3\n[48] R. Schmidt, A. Khan, K. Singh, and G. Kurtenbach, \u201cAnalytic drawing\nof 3d scaffolds,\u201d in ACM SIGGRAPH Asia 2009 papers, 2009, pp. 1\u201310.\n3\n[49] C. Shao, A. Bousseau, A. Sheffer, and K. Singh, \u201cCrossshade: shading\nconcept sketches using cross-section curves,\u201d ACM Transactions on\nGraphics (TOG), vol. 31, no. 4, pp. 1\u201311, 2012. 3\n[50] B. Xu, W. Chang, A. Sheffer, A. Bousseau, J. McCrae, and K. Singh,\n\u201cTrue2form: 3d curve networks from 2d sketches via selective regular-\nization,\u201d ACM Transactions on Graphics (TOG), vol. 33, no. 4, pp. 1\u201313,\n2014. 3\n[51] M. Eitz, R. Richter, T. Boubekeur, K. Hildebrand, and M. Alexa, \u201cSketch-\nbased shape retrieval.\u201d ACM Trans. Graph., vol. 31, no. 4, pp. 31\u20131, 2012.\n3\n[52] B. Li, Y. Lu, F. Duan, S. Dong, Y. Fan, L. Qian, H. Laga, H. Li,\nY. Li, P. Liu, M. Ovsjanikov, H. Tabia, Y. Ye, H. Yin, and Z. Xue,\n\u201c3D Sketch-Based 3D Shape Retrieval,\u201d in Eurographics Workshop on\n3D Object Retrieval, A. Ferreira, A. Giachetti, and D. Giorgi, Eds.\nThe\nEurographics Association, 2016. 3\n[53] A. Qi, Y. Gryaditskaya, J. Song, Y. Yang, Y. Qi, T. M. Hospedales,\nT. Xiang, and Y.-Z. Song, \u201cToward fine-grained sketch-based 3d shape\nretrieval,\u201d IEEE transactions on image processing, vol. 30, pp. 8595\u2013\n8606, 2021. 3\n[54] L. Luo, Y. Gryaditskaya, T. Xiang, and Y.-Z. Song, \u201cStructure-aware 3d\nvr sketch to 3d shape retrieval,\u201d arXiv preprint arXiv:2209.09043, 2022.\n3\n[55] L. Fan, R. Wang, L. Xu, J. Deng, and L. Liu, \u201cModeling by drawing with\nshadow guidance,\u201d in Computer Graphics Forum, vol. 32, no. 7.\nWiley\nOnline Library, 2013, pp. 157\u2013166. 3\n[56] X. Xie, K. Xu, N. J. Mitra, D. Cohen-Or, W. Gong, Q. Su, and B. Chen,\n\u201cSketch-to-design: Context-based part assembly,\u201d in Computer Graphics\nForum, vol. 32, no. 8.\nWiley Online Library, 2013, pp. 233\u2013245. 3\n[57] F. Wang, L. Kang, and Y. Li, \u201cSketch-based 3d shape retrieval using\nconvolutional neural networks,\u201d in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2015, pp. 1875\u20131883. 3,\n11\n[58] Y. Zhong, Y. Qi, Y. Gryaditskaya, H. Zhang, and Y.-Z. Song, \u201cTowards\npractical sketch-based 3d shape generation: The role of professional\nsketches,\u201d IEEE Transactions on Circuits and Systems for Video Tech-\nnology, vol. 31, no. 9, pp. 3518\u20133528, 2020. 3\n[59] Z. Cheng, M. Chai, J. Ren, H.-Y. Lee, K. Olszewski, Z. Huang, S. Maji,\nand S. Tulyakov, \u201cCross-modal 3d shape generation and manipulation,\u201d\nin Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part III.\nSpringer, 2022, pp.\n303\u2013321. 3\n[60] D. Kong, Q. Wang, and Y. Qi, \u201cA diffusion-refinement model for sketch-\nto-point modeling,\u201d in Proceedings of the Asian Conference on Computer\nVision, 2022, pp. 1522\u20131538. 3\n[61] W. Su, D. Du, X. Yang, S. Zhou, and H. Fu, \u201cInteractive sketch-based\nnormal map generation with deep neural networks,\u201d Proceedings of the\nACM on Computer Graphics and Interactive Techniques, vol. 1, no. 1,\npp. 1\u201317, 2018. 3\n[62] H. Huang, E. Kalogerakis, E. Yumer, and R. Mech, \u201cShape synthesis\nfrom sketches via procedural models and convolutional networks,\u201d IEEE\ntransactions on visualization and computer graphics, vol. 23, no. 8, pp.\n2003\u20132013, 2016. 3\n[63] D. Du, H. Zhu, Y. Nie, X. Han, S. Cui, Y. Yu, and L. Liu, \u201cLearning part\ngeneration and assembly for sketching man-made objects,\u201d in Computer\nGraphics Forum.\nWiley Online Library, 2020. 3\n[64] G. Nishida, I. Garcia-Dorado, D. G. Aliaga, B. Benes, and A. Bousseau,\n\u201cInteractive sketching of urban procedural models,\u201d ACM Transactions\non Graphics (TOG), vol. 35, no. 4, pp. 1\u201311, 2016. 3\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XXXX\n15\n[65] D. DeCarlo, A. Finkelstein, S. Rusinkiewicz, and A. Santella, \u201cSugges-\ntive contours for conveying shape,\u201d in ACM SIGGRAPH 2003 Papers,\n2003, pp. 848\u2013855. 5, 7\n[66] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,\n\u201cOccupancy networks: Learning 3d reconstruction in function space,\u201d in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 4460\u20134470. 5, 6\n[67] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove,\n\u201cDeepsdf: Learning continuous signed distance functions for shape\nrepresentation,\u201d in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2019, pp. 165\u2013174. 5, 6, 11\n[68] Z. Chen and H. Zhang, \u201cLearning implicit fields for generative shape\nmodeling,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 5939\u20135948. 5, 6\n[69] A. Newell, K. Yang, and J. Deng, \u201cStacked hourglass networks for human\npose estimation,\u201d in European conference on computer vision.\nSpringer,\n2016, pp. 483\u2013499. 5\n[70] W. E. Lorensen and H. E. Cline, \u201cMarching cubes: A high resolution\n3d surface construction algorithm,\u201d ACM siggraph computer graphics,\nvol. 21, no. 4, pp. 163\u2013169, 1987. 6\n[71] M. Botsch and L. Kobbelt, \u201cA remeshing approach to multiresolution\nmodeling,\u201d in Proceedings of the 2004 Eurographics/ACM SIGGRAPH\nsymposium on Geometry processing, 2004, pp. 185\u2013192. 6, 7\n[72] O. Sorkine, D. Cohen-Or, Y. Lipman, M. Alexa, C. R\u00a8ossl, and H.-P.\nSeidel, \u201cLaplacian surface editing,\u201d in Proceedings of the 2004 Euro-\ngraphics/ACM SIGGRAPH symposium on Geometry processing, 2004,\npp. 175\u2013184. 6, 7\n[73] A. Sharf, T. Lewiner, A. Shamir, L. Kobbelt, and D. Cohen-Or, \u201cCom-\npeting fronts for coarse\u2013to\u2013fine surface reconstruction,\u201d in Computer\nGraphics Forum, vol. 25, no. 3.\nWiley Online Library, 2006, pp. 389\u2013\n398. 7\n[74] P. M. Bartier and C. P. Keller, \u201cMultivariate interpolation to incorporate\nthematic surface data using inverse distance weighting (idw),\u201d Computers\n& Geosciences, vol. 22, no. 7, pp. 795\u2013799, 1996. 7\n[75] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,\n\u201cFlownet 2.0: Evolution of optical flow estimation with deep networks,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 2462\u20132470. 7\n[76] Y. Qiu, X. Xu, L. Qiu, Y. Pan, Y. Wu, W. Chen, and X. Han, \u201c3dcar-\nicshop: A dataset and a baseline method for single-view 3d caricature\nface reconstruction,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 10 236\u201310 245. 8\n[77] A. Bangor, P. Kortum, and J. Miller, \u201cDetermining what individual sus\nscores mean: Adding an adjective rating scale,\u201d Journal of usability\nstudies, vol. 4, no. 3, pp. 114\u2013123, 2009. 9\n[78] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, \u201cA papier-\nm\u02c6ach\u00b4e approach to learning 3d surface generation,\u201d in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2018, pp.\n216\u2013224. 11\n[79] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, \u201c3d-r2n2: A\nunified approach for single and multi-view 3d object reconstruction,\u201d in\nEuropean conference on computer vision.\nSpringer, 2016, pp. 628\u2013644.\n11\n"
  }
]