[
  {
    "title": "LARP: Language-Agent Role Play for Open-World Games",
    "link": "https://arxiv.org/pdf/2312.17653.pdf",
    "upvote": "27",
    "text": "LARP: LANGUAGE-AGENT ROLE PLAY FOR OPEN-WORLD\nGAMES\nMing Yan *\u20201, Ruihao Li *1, Hao Zhang1, Hao Wang1, Zhilan Yang1, and Ji Yan1\n1MiAO\nABSTRACT\nLanguage agents have shown impressive problem-solving skills within defined settings and brief\ntimelines. Yet, with the ever-evolving complexities of open-world simulations, there\u2019s a pressing\nneed for agents that can flexibly adapt to complex environments and consistently maintain a long-\nterm memory to ensure coherent actions. To bridge the gap between language agents and open-\nworld games, we introduce Language Agent for Role-Playing (LARP), which includes a cognitive\narchitecture that encompasses memory processing and a decision-making assistant, an environment\ninteraction module with a feedback-driven learnable action space, and a postprocessing method that\npromotes the alignment of various personalities. The LARP framework refines interactions between\nusers and agents, predefined with unique backgrounds and personalities, ultimately enhancing the\ngaming experience in open-world contexts. Furthermore, it highlights the diverse uses of language\nmodels in a range of areas such as entertainment, education, and various simulation scenarios. The\nproject page is released at https://miao-ai-lab.github.io/LARP/.\n1\nIntroduction\nLarge Language Models (LLMs) are machine learning models capable of performing a variety of Natural Language\nProcessing (NLP) tasks such as generating text, translating text from one language to another, and answering\nquestions conversationally. The term large refers to the vast amount of parameters the language model can update\nduring its learning process. With the development of pre-training generative model techniques and the construction of\nmassive and comprehensive datasets, some top-performing large language models have up to hundreds of billions of\nparameters [Touvron et al., 2023, Radford et al., 2018, 2019, Brown et al., 2020, Ouyang et al., 2022, OpenAI, 2023].\nFurthermore, owing to the advancements in large language models, AI entities have become a hot topic in recent\nyears. These artificial intelligence entities, often referred to as agents [Russell and Norvig, 2010, Wooldridge and\nJennings, 1995], are the fundamental components of large-scale artificial intelligence systems. Typically, in the realm\nof Artificial General Intelligence, an agent is an artificial entity that can perceive its surrounding environment through\nsensors, make decisions, and respond via actuators. With the development of large language models and agents,\nthere is a new trend of combining them into a single entity called language agents. These language agents are de-\nsigned by integrating large language models and agent design [Wang et al., 2023a, Xi et al., 2023, Sumers et al., 2023].\nAs a strongly related industry to computers, gaming has become increasingly intertwined with the development of\ngeneral-purpose language agents. The application of LLMs and agents has grown more widespread. In related studies,\nthere is considerable literature on the application of language agents in text-based games [Dambekodi et al., 2020,\nSingh et al., 2021, Yao et al., 2021, Urbanek et al., 2019] and adversarial games [OpenAI et al., 2019, Arulkumaran\net al., 2019]. Concurrently, with the enhanced capabilities of LLMs, open-world games have emerged as the frontier\nfor language agent applications. This is due to the unique and challenging scenarios present in open-world games,\nwhich provides a fertile ground for general-purpose language agents. Open-world games present a rich, dynamic,\nand engaging environment, encompassing complex missions and storylines. They require the use of agents to equip\nnon-player characters with diversified behaviors. Although numerous studies have proposed architectures for general\n*Equal Contribution\n\u2020Correspondence to: mingyan@miao.company\narXiv:2312.17653v1  [cs.AI]  24 Dec 2023\nFigure 1: Cognitive Architecture of LARP Overview.\nlanguage agents that could be applied in open-world games like Minecraft [Lin et al., 2023, Park et al., 2023], a\ngap still exists between the general-purpose agents and the overall requirements in an open-world gaming context.\nGeneral-purpose language agents are created to address a variety of issues in realistic environments. Their primary\nrequisites are universality and the simulation of human behavior. These agents can adapt to various environments\nand tasks, without being restricted to fixed roles. However, these general-purpose language agents face significant\nchallenges in practical open-world environments. These challenges include but are not limited to interpreting complex\nenvironments, memorizing long-term events, generating expressions that cohere with character and environmental\nsettings, and continuously learning from interactions with the environment.\nHence, in this work, we propose a game-oriented Role-Playing Agent framework, Language Agent for Role Play\n(LARP) toward open-world games. LARP focuses on blending the open-world games with language agents, utilizing\na modular approach for memory processing, decision-making, and continuous learning from interactions. In the\nagent\u2019s internal depiction, we designed a complex cognitive architecture based on cognitive psychology, equipping\nagents under the LARP framework with high playability and uniquity. Aiming to yield more realistic role-playing\nexperience, we regularized agents using the data and context of the open-world gaming environment, prior set\npersonalities, knowledge, rules, memory, and post constraints, which can be seen as a specific case within the\ngeneral-purpose language agents. As for the general agent architecture, it typically requires a large-scale language\nmodel. However, our architecture incorporates a cluster of smaller language models, each fine-tuned for different\ndomains, to handle various tasks separately. This design contributes new experiences and perspectives to developing\nlanguage agents for open-world role-playing games.\nThe rest of this work is organized as follows: Section 2 discusses the related works on agent framework and agent\ncomponents.\nIn Section 3, we present the cognitive architecture part of LARP. In Section 4, we highlight the\nenvironmental interaction module, and in Section 5, we introduce agents\u2019 alignment of diversified personalities. A\ndiscussion is presented in Section 6, followed by a conclusion of the entire paper in Section 7.\n2\n2\nRelated Work\nThe development and advancements in LLMs have brought opportunities and potential to various fields. These fields\ninclude but are not limited to, role-playing, gaming, and tool-using Agents, amongst others. At the same time, the\ndefinition of language agents is being continuously updated. We will share some related works on agent architecture\nin this section.\n2.1\nAgent Framework\nFirstly, we will introduce some works related to language agents\u2019 role-playing and simulation, which are aimed at\nenhancing the LLM\u2019s ability in role-playing and highlighting distinct features. These works also aim to boost the\ninteraction capability between the agents and users, making the agents appear more self-conscious [Wang et al.,\n2023b, Shao et al., 2023, Shanahan et al., 2023, Li et al., 2023a]. Other studies focus on role-playing and interaction\namong multiple agents, which include scenarios such as collaboration to complete tasks [Li et al., 2023b, Chen et al.,\n2023a, Qian et al., 2023, Li et al., 2023c, Wu et al., 2023], simulating daily activities [Lin et al., 2023, Park et al.,\n2023, Wang et al., 2023c, Liu et al., 2023a], and promoting progress in debates [Liang et al., 2023, Du et al., 2023,\nChan et al., 2023].\nIn addition to this, language agents are also applied in open-world environments. There are not only instances of\napplication in text-based games [Urbanek et al., 2019, C\u02c6ot\u00b4e et al., 2019, Hausknecht et al., 2020]but also exploration\ntasks in open-world environments such as Minecraft [Wang et al., 2023d,e, Zhu et al., 2023].\n2.2\nAgent Component\nIn this section, we will introduce some related works on the design of language agent components. An agent system\nis usually divided into three parts: memory, planning, and action (tool use) [Weng, 2023]. Memory system serves as a\nrepository for facts, reflections, etc., entailing abilities for storage and retrieval. Hence, efforts in memory primarily\npertain to input/output functions, including memory compression [Hu et al., 2023], storage, and retrieval [Park et al.,\n2023, Zhong et al., 2023, Huang et al., 2023].\nThe planning component is responsible for the decision-making aspect related to the agent\u2019s behavior and language.\nThe capability of agents largely depends on this part. Abilities for planning [Yao et al., 2023, Liu et al., 2023b, Yao\net al., 2022, Shinn et al., 2023, Liu et al., 2023c, Wang et al., 2023f] and reasoning [Wei et al., 2022, Madaan et al.,\n2023] are realized in this component, and associated works typically revolve around these two abilities.\nThe last component is tool usage and actions, which signifies an augmentation in the capabilities of the intelligent\nentities, aiding them in conducting more complex and difficult tasks. Works in this section include tool-using [Nakano\net al., 2021] and learning new actions [Schick et al., 2023].\n3\nCognitive Architecture\nCognitive architecture is a fundamental component of role-playing language agents in open-world games. It provides\na logical framework and enables self-recognition of agents. The cognitive architecture is shown in figure 2. It\ncomprises four major modules: long-term memory, working memory, memory processing, and decision-making. The\nlong-term memory module serves as the primary warehouse containing memories with substantial storage capacity.\nWorking memory acts as a temporary cache with limited space for memory. The memory processing module is\nthe most important unit of the cognitive architecture. The decision-making module then derives agents\u2019 subsequent\nactions based on the retrieved information.\n3.1\nLong-Term Memory\nIn cognitive science, long-term memory (LTM) is comprised of two types of memory: declarative memory and\nprocedural memory. Declarative memory is further divided into semantic memory and episodic memory [Laird,\n3\nFigure 2: Cognitive Workflow of LARP. This represents a cycle: Information from long-term memory and observation\nis processed in the memory processing module and transmitted to the working memory module. The information in\nthe working memory module, together with the observed information, is inputted into the decision-making assistant,\nwhich finally generates a decision or dialogue. Memory processing has three main stages: encoding, storage, and\nrecall. Encoding is the process of transforming information into a form that can be stored in memory. Storage is the\nprocess of maintaining information in memory. Recall is the process of retrieving information from memory.\n2019, Tulving et al., 1972]. Semantic memory refers to general knowledge memories acquired through conceptual\nand factual knowledge about the world.\nIn the context of the open-world game, it can be considered the part\nthat encapsulates the game rules and memories consistent with the relevant worldview.\nWe divided semantic\nmemory into two parts in the system. One is implemented with an external database, as its content is not frequently\nchanged. Simultaneously, some semantic memories are stored in the long-term memory module in symbolic language.\nEpisodic memory refers to the memory of specific events that an individual experiences. These can be memories\nrelated to other players or Agents. In our memory system, we adopted a vector database in the long-term memory\nmodule for storing and retrieving these memories. Associated decay parameters are introduced as memories can be\nforgotten, with relevance scores decreasing over time. When reasoning with the LLM, such memory contents can be\neasily retrieved through vector queries.\nProcedural memory refers to actions or skills that can be performed without conscious thought[Roediger, 1990], such\nas swimming, cycling, etc. These skills, with action properties, are represented as APIs in action space in our system.\nThe action space is divided into public APIs and personal APIs. The personal APIs could be extended through\nlearning [Sumers et al., 2023] which is mentioned in the section 4.\nIn the long-term memory module, we store all perceived memories in the semantic and episodic memory zones\nrespectively. We propose a method named Question-based Query, which generates self-ask questions as queries that\ncan be leveraged in search by vector similarity and predicate logic. This method facilitates the retrieval of semantic\nand episodic memories in the recall module, thereby improving the overall efficiency of memory utilization.\n4\n3.2\nWorking Memory\nWorking memory mainly holds the observational information and retrieved long-term memories needed for perform-\ning complex cognitive tasks (such as reasoning and learning) and interactive tasks [Baddeley, 2003, Miller et al.,\n2017].\nThese pieces of information are typically obtained through agents\u2019 observation as natural language data\nprovided by the game side. Short-term memory, as its name suggests, represents a stage of memory that retains\ninformation for brief periods of time, generally only lasting a few seconds to a minute [Atkinson and Shiffrin, 1968].\nTo humans, the average capacity for retaining items in short-term memory is about 7\u00b12, with a retention duration\nof roughly 20 to 30 seconds [Miller, 1956]. In this work, the two concepts are implemented as the same module,\ncollectively referred to as working memory. In our architecture, it exists as a data cache from which information\nis extracted and dropped into the prompt\u2019s context. Its extraction process is further explained in more detail in the\nmemory processing and decision-making sections.\n3.3\nMemory Processing\nThe memory processing module primarily handles the processing of memories that have been stored and are about\nto be stored.\nThe three main stages of memory are encoding, storage, and recall [Melton, 1963].\nSpecifically,\nperceived input information is encoded and transformed into content in long-term memory, enabling it to be\nrecalled in the space of long-term memory.\nIn LARP, we simulate this procedure by processing all structured\nobservational information provided in the game, combining it with retrieved content, and storing it in the working\nmemory. This information serves as an input for a series of logic processing units in the decision-making module,\ncontinuously updating the content in the working memory.\nOnce the length of working memory reaches a cer-\ntain threshold, reflection is triggered, during which ineffective memories are filtered out, and the processed natural\nlanguage memories and symbolic language memories are separately stored as episodic memory and semantic memory.\nThe core of memory encoding is the language transformation system. By aligning language models and probabilistic\nmodels, natural language is converted into a probabilistic programming language (PPL) [Wong et al., 2023] and\nlogic programming language. PPL primarily handles probabilistic reasoning, while logic programming language\npertains mainly to fact reasoning. Moreover, memory encoding should also be influenced by the consistency of\nprior knowledge, meaning that past knowledge will affect the current understanding [Bartlett, 1995]. The storage of\nmemory has already been elaborated in the long-term memory section.\nTo humans, recall refers to the psychological process of retrieving information from the past. While in our archi-\ntecture, it is the process of retrieving information from long-term memory. It first involves compound retrieval from\nlong-term memory, including search by vector similarity and predicate logic. First, we employed self-ask strategies\nto form the queries, prompting LLM to raise questions regarding agents\u2019 observations, personalities, and experiences.\nAfter obtaining the queries, we adopted 3 methods to perform the retrieval. For logic programming search, LLM\ngenerates a query in a logic programming language that answers the self-ask questions based on available rules and\nfacts. For similarity search, two methods were available. One method is using self-ask questions as queries for vector\nsimilarity search, matching with question-answer pairs in the vector database of episodic memory. The other method\nis using keywords extracted from the self-ask questions to match with the natural language memories in the same\ndatabase. This process will be repeated until a final answer is obtained, and this can also be considered semantic\nretrieval [Press et al., 2022]. Figure 3 shows the detailed control flow.\nBased on the recall capabilities, our architecture adopts CoT [Wei et al., 2022] to reason about the retrieved content\nand observed information and perform memory reconstruction, ie., using prior knowledge to influence observed facts\nto some extent [Loftus and Palmer, 1974] though the reconstructed memories might be distorted. In addition, we also\nsimulated the process of human forgetting in the recall workflow. When the retrieval system operates, we introduce a\ndecay parameter \u03c3 represented by Wickelgren\u2019s power law to mark the forgetting probability of this memory [Wixted\nand Carpenter, 2007]. The calculation formula is as follows:\n\u03c3 = \u03b1\u03bbN(1 + \u03b2t)\u2212\u03c8\n(1)\nHere, \u03bb represents the importance level, given by a scoring model. N stands for the number of retrievals of this\nmemory, and t is the elapsed time after the last retrieval. \u03c8 is the rate of forgetting for each character. \u03b1 and \u03b2 are\n5\nFigure 3: Detail control flow of recall psychological process. First conduct self-asking about the observation to get\nself-ask questions. Using the self-ask questions as queries, different methods of retrieval are undertaken. 1. Generate\npredicate logic statements in logic programming language and probabilistic programming language based on queries.\n2. Conducting a vector similarity search after extracting keywords from the queries. 3. Searching for question-answer\npairs based on sentence similarity between queries and the questions of question-answer pairs. Qself\u2212ask means the\nself-ask questions which were used as queries, Qlogic stands for predicate logic query statements, Qkey is the extracted\nkeywords, Q\u2032A stands for the question-answer pairs.\nthe scaling parameters for importance and time, respectively. Through multiple rounds of memory reconstruction and\nforgetting processes, our cognitive architecture can ultimately simulate instances of memory distortion.\n3.4\nDecision Making\nThe decision-making module produces final decisions under the joint effect of observation and working memory. The\ncore section of the decision-making module is an ordered cluster of programmable units. Each unit will process the\ncontent in working memory and context, updating the results to the working memory in real time. These units can be\nsimple information processing units, such as those conducting affective computing, or complex units equipped with\nspecifically fine-tuned LLM models, like intent analysis and output formatting. These units are infinitely scalable\nand can handle all types of memory processing tasks. As each unit communicates with working memory, it updates\nthe working memory in real-time, allowing the agents to react timely when observation changes during the process.\nThe execution order of these units would be determined by a language model assistant. The final output of the\ndecision-making module could be tasks or dialogue contents for the Non-Player Characters (NPCs).\n4\nEnvironment Interaction\nFor role-playing language agents in open-world games, generating tasks based on current observations through\nthe cognitive architecture only accomplishes the objective within agents. However, in open-world games with free\nactions and rich gaming content, agents need to interact with the game environment by connecting the internal and\nthe external. There are various works in employing language agents to interact with open-world game environments\n[Wang et al., 2023d, Zhu et al., 2023, Yang et al., 2023, Wang et al., 2023e]. For instance, Voyager uses the concept of\nan automatic curriculum, obtaining objectives by feeding GPT4 the contents and states of environmental observations.\nThen, GPT4 is prompted to generate the functioning code to reach the objectives. The paper also presents the skill\nlibrary method, which embeds the description of the generated code as a key and the code as a value, achieving high\n6\nFigure 4: Environment Interaction.\nextensibility of incorporating new skills by adding key-value pairs. Octopus utilizes the Visual Language Model\n(VLM) to acquire observations. However, this approach can lead to high dimensions of data feature distribution,\nresulting in poor controllability. Moreover, the operational cost of the VLM is high, and it\u2019s challenging to collect the\ndata set\u2019s prior knowledge within the game.\nFigure 4 shows the fundamental interaction procedural. Interior refers to the working memory and the tasks that need\nto be executed based on the current situation, generated by the observation and cognitive architecture. The Action\nSpace is the agent\u2019s executable action APIs in the game world, encompassing both public and personal APIs. The\npersonal API library stores tasks-API pairs, while the public APIs are basic actions. A personal API can be a series of\nbasic actions, facilitating quick decision-making and reuse of APIs.\nOnce we have generated the corresponding plans in the decision-making module, we initially attempt to break down\nthe overall task goal into several subtask goals. These subtask goals present as strictly ordered sequence-sensitive\narrangements. For each task goal or subtask goal, the whole system will integrate it with the working memory.\nThen, it will use a retriever to search separately in the personal API library and public API library. If the action\ncorresponding to the task already exists in the personal API library, the action is instantly performed. Otherwise, the\nsystem completes the corresponding prompt with the entire action space and interior content to generate the structured\ncode using a fine-tuned LLM. Upon the successful execution and verification of the generated code blocks, they are\nstored as a new interface in the personal API library in the form of (Task, API) for future use. If verification fails, the\nreflection unit is activated to generate new code blocks [Shinn et al., 2023].\nSimultaneously, we also collect the paired prompt and generated code as a training set for fine-tuning code generation\nLLM [Patil et al., 2023]. After the successful execution and verification, the results are fed back through RLHF to\nenhance the model\u2019s capabilities.\n7\n5\nPersonalities\nThe role of distinct personalities is vital in enhancing the cognitive capabilities of language agents in role-playing.\nAligning variable personalities permits language models to better comprehend different perspectives and portray var-\nious cultures and social groups. Language models acting out different roles in complex scenarios must deeply un-\nderstand, respond, and express in their unique ways. This necessitates the models to possess human-like thought\nprocesses with a wide range of personalities. Understanding the generation of diverse language expressions, handling\nmulticultural content, and exhibiting varied thoughts, perspectives, emotions, and attitudes - all demand the model to\naccommodate these distinct personalities. Therefore, this section will delve into its implementation.\nLARP adopts the strategy of simulating a cluster of models fine-tuned with various alignments to tackle the agents\u2019\ndiversified viewpoints. These models might be applied in different modules. During the training phase, we pre-trained\nseveral base models of different scales. Our pretraining datasets contain perspectives of different cultures and groups.\nAfter pre-training, these base models undergo supervised fine-tuning (SFT) on a series of instruction datasets of\npersona and character to enhance instruction-following and role-playing capabilities [Chen et al., 2023b, Dong et al.,\n2023]. This instruction dataset was established through data distillation based on question-answer pairs generated\nby SOTA models. Then, the dataset was optimized via assessment, modification, and adjustment based on human\nfeedback.\nIt is possible to create multiple datasets and fine-tune LoRAs (Low-Rank Adaption) for capabilities such as reflection,\ncode generation, and intent analysis. These LoRAs can be dynamically integrated with base models of different\nscales, creating a model cluster with diverse capabilities and personalities. These capabilities cover tasks such as\nlanguage style, emotion, action generation, reflection, memory reconstruction, and more.\nHowever, one of the main challenges in fine-tuning language models to construct different LoRAs for role-playing\nis acquiring quality data. Successful fine-tuning requires custom datasets of high quality, which need to be carefully\nconstructed to capture various aspects of characters, including their language style, behavior style, personality traits,\nidioms, backstories, and more. The construction of datasets requires extensive literary creativity, script compilation,\nand character research to ensure that the generated language not only fits the character\u2019s persona and features but also\ninteracts with the user in an appropriate manner [Wang et al., 2023b].\nTo enrich the diversity of the agent, we set up several post-processing modules, including the action verification\nmodule and the conflict identification module. The action verification module is part of the environment interaction\nmodule, which checks whether the generated actions can be correctly executed in the game. Conversely, within the\ncognitive architecture, the conflict identification module checks whether the decisions and conversations encompass\nconflicts with character relationships, personalities, and game worldview. When such conflicts are detected, the\nmodule will undertake actions like rejecting the result or rewriting it to prevent the agent from going out of character.\n6\nDiscussions\n6.1\nMulti-Agent Cooperation and Agent Socialization\nA single Language Agent role-playing under the framework proposed in this work is insufficient to solve the issue\nof creating rich content in open-world games. To bring each character supported by an Agent to life, a robust social\nnetwork needs to be established. One possible approach is to build suitable sociological mechanisms and behaviors\natop the large language model-driven agents to ensure that NPCs can still maintain their rationality and logic after\nextensive role-playing reasoning.\n6.2\nConfidence of Model Clusters vs. Evaluation and Feedback System\nCombining language models and cognitive science makes language agents align more closely with genuine human\ncognition. This method effectively mitigates the problem of a single large model\u2019s inability to enhance role-playing\noutcomes due to insufficient data.\nSimultaneously, since the cognitive system only consists of domain tasks,\nfine-tuned small-scale models can achieve satisfactory performance. It saves costs compared to fine-tuning large\nmodels. However, due to the randomness of language model output results, it\u2019s unpredictable how the cumulative\ndistortion of the results produced by each task affects the distortion of the entire cognitive architecture. It\u2019s hard to\n8\nsay such a distorted agent could be called a human-believable agent. Therefore, a corresponding evaluation and a\nmeasurement framework are needed to impose constraints and convergence on the distortion of the cognitive system.\nEstablishing a measurement and feedback mechanism for the entire system to measure the logical deviation of each\nlogic unit can optimize system robustness and minimize the impact of single-system distortion on the overall system.\n7\nConclusion\nIn this study, we present a language agent framework-oriented open-world games and elaborate on this framework\nfrom three aspects: cognitive architecture, environmental interaction, and alignment with diverse value perspectives.\nAddressing cognitive architecture, we employ more intricate techniques from cognitive science to enable the agent\nto make more reasonable decisions, alongside implementing post-processing constraints to prevent undue freedom\nin the agent, thereby bringing them closer to real human behavior in role-playing contexts. We envisage that our\nwork harbors tremendous potential within the open-world games, breathing new life into this traditional domain, and\nultimately catering the experience akin to that of \u2019Westworld\u2019.\nReferences\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener-\native pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nOpenAI. Gpt-4 technical report, 2023.\nStuart J Russell and Peter Norvig. Artificial intelligence a modern approach. London, 2010.\nMichael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering\nreview, 10(2):115\u2013152, 1995.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\nYankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432,\n2023a.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie\nJin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint\narXiv:2309.07864, 2023.\nTheodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language\nagents. arXiv preprint arXiv:2309.02427, 2023.\nSahith Dambekodi, Spencer Frazier, Prithviraj Ammanabrolu, and Mark O Riedl. Playing text-based games with\ncommon sense. arXiv preprint arXiv:2012.02757, 2020.\nIshika Singh, Gargi Singh, and Ashutosh Modi. Pre-trained language models as prior knowledge for playing text-based\ngames. arXiv preprint arXiv:2107.08408, 2021.\nShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. Reading and acting while blindfolded: The need for\nsemantics in text game agents. arXiv preprint arXiv:2103.13552, 2021.\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00a8aschel,\nDouwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a fantasy text adventure game. arXiv\npreprint arXiv:1903.03094, 2019.\n9\nOpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw Debiak, Christy Denni-\nson, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J\u00b4ozefowicz, Scott Gray, Catherine Olsson,\nJakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas\nSchneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep\nreinforcement learning, 2019.\nKai Arulkumaran, Antoine Cully, and Julian Togelius.\nAlphastar: An evolutionary computation perspective.\nIn\nProceedings of the genetic and evolutionary computation conference companion, pages 314\u2013315, 2019.\nJiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox\nfor large language model evaluation. arXiv preprint arXiv:2308.04026, 2023.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.\nGenerative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium\non User Interface Software and Technology, pages 1\u201322, 2023.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo,\nRuitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities\nof large language models. arXiv preprint arXiv:2310.00746, 2023b.\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing. arXiv preprint\narXiv:2310.10158, 2023.\nMurray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature, pages 1\u20136,\n2023.\nCheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan,\nHaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint\narXiv:2308.09597, 2023a.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for\u201d mind\u201d exploration of large scale language model society. arXiv preprint arXiv:2303.17760,\n2023b.\nDake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. Gamegpt: Multi-agent collaborative frame-\nwork for game development. arXiv preprint arXiv:2310.08067, 2023a.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Commu-\nnicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.\nYuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based\ntask-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023c.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\nZhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework.\narXiv preprint arXiv:2308.08155, 2023.\nLei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen. Recagent: A\nnovel simulation paradigm for recommender systems. arXiv preprint arXiv:2306.02552, 2023c.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi.\nTraining socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023a.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shum-\ning Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint\narXiv:2305.19118, 2023.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning\nin language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chate-\nval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.\nMarc-Alexandre C\u02c6ot\u00b4e, Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew\nHausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games.\nIn Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on\nArtificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41\u201375.\nSpringer, 2019.\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u02c6ot\u00b4e, and Xingdi Yuan. Interactive fiction games: A\ncolossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903\u20137910,\n2020.\n10\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive\nplanning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560,\n2023d.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-\nkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291,\n2023e.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang\nWang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models\nwith text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\nLilian Weng.\nLlm-powered autonomous agents.\nlilianweng.github.io, Jun 2023.\nURL https://lilianweng.\ngithub.io/posts/2023-06-23-agent/.\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with\ndatabases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.\nWanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large language models with\nlong-term memory. arXiv preprint arXiv:2305.10250, 2023.\nZiheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and\ninteractive memory management for conversational agents. In Adjunct Proceedings of the 36th Annual ACM Sym-\nposium on User Interface Software and Technology, pages 1\u20133, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of\nthoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empow-\nering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language\nagents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. arXiv\npreprint arXiv:2302.02676, 3, 2023c.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\nPlan-and-\nsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models.\narXiv preprint\narXiv:2305.04091, 2023f.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\nShrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint\narXiv:2303.17651, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332, 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint\narXiv:2302.04761, 2023.\nJohn E Laird. The Soar cognitive architecture. MIT press, 2019.\nEndel Tulving et al. Episodic and semantic memory. Organization of memory, 1(381-403):1, 1972.\nHenry L Roediger. Implicit memory: Retention without remembering. American psychologist, 45(9):1043, 1990.\nAlan Baddeley. Working memory: looking back and looking forward. Nature reviews neuroscience, 4(10):829\u2013839,\n2003.\nGeorge A Miller, Galanter Eugene, and Karl H Pribram. Plans and the structure of behaviour. In Systems Research for\nBehavioral Science, pages 369\u2013382. Routledge, 2017.\n11\nRichard C Atkinson and Richard M Shiffrin. Human memory: A proposed system and its control processes. In\nPsychology of learning and motivation, volume 2, pages 89\u2013195. Elsevier, 1968.\nGeorge A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing informa-\ntion. Psychological review, 63(2):81, 1956.\nArthur W Melton. Implications of short-term memory for a general theory of memory. Journal of verbal Learning\nand verbal Behavior, 2(1):1\u201321, 1963.\nLionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and\nJoshua B Tenenbaum. From word models to world models: Translating from natural language to the probabilistic\nlanguage of thought. arXiv preprint arXiv:2306.12672, 2023.\nFrederic Charles Bartlett. Remembering: A study in experimental and social psychology. Cambridge university press,\n1995.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing\nthe compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\nElizabeth F Loftus and John C Palmer. Reconstruction of automobile destruction: An example of the interaction\nbetween language and memory. Journal of verbal learning and verbal behavior, 13(5):585\u2013589, 1974.\nJohn T Wixted and Shana K Carpenter. The wickelgren power law and the ebbinghaus savings function. Psychological\nScience, 18(2):133\u2013134, 2007.\nJingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan\nZhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. arXiv\npreprint arXiv:2310.08588, 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with\nmassive apis. arXiv preprint arXiv:2305.15334, 2023.\nHao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao.\nMaybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. arXiv preprint\narXiv:2305.09246, 2023b.\nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan,\nChang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data\ncomposition. arXiv preprint arXiv:2310.05492, 2023.\n12\n"
  },
  {
    "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis",
    "link": "https://arxiv.org/pdf/2312.17681.pdf",
    "upvote": "15",
    "text": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis\nFeng Liang* 1, Bichen Wu\u2020 2, Jialiang Wang2, Licheng Yu2, Kunpeng Li2, Yinan Zhao2, Ishan Misra2,\nJia-Bin Huang2, Peizhao Zhang2, Peter Vajda2, Diana Marculescu1\n1The University of Texas at Austin, 2Meta GenAI\n{jeffliang,dianam}@utexas.edu, {wbc,stzpz,vajdap}@meta.com\nhttps://jeff-liangf.github.io/projects/flowvid\nInput\nStylization\nPrompt: a woman wearing headphones, in flat 2d anime\nObject Swap\nPrompt: a Greek statue wearing headphones\nInput\nInput\nStylization\nPrompt: a Chinese ink painting of a panda eating bamboo\nObject Swap\nPrompt: a koala eating bamboo\nLocal Edits\nPrompt: a woman with black hair wearing headphones\nLocal Edits\nPrompt: a panda with a pig nose eating bamboo\nFigure 1. We present FlowVid to synthesize a consistent video given an input video and a target prompt. Our model supports multiple\napplications: (1) global stylization, such as converting the video to 2D anime (2) object swap, such as turning the panda into a koala bear (3)\nlocal edit, such as adding a pig nose to a panda.\nAbstract\nDiffusion models have transformed the image-to-image\n(I2I) synthesis and are now permeating into videos. How-\never, the advancement of video-to-video (V2V) synthesis\nhas been hampered by the challenge of maintaining tempo-\nral consistency across video frames. This paper proposes\na consistent V2V synthesis framework by jointly leveraging\nspatial conditions and temporal optical flow clues within the\nsource video. Contrary to prior methods that strictly adhere\nto optical flow, our approach harnesses its benefits while\nhandling the imperfection in flow estimation. We encode the\noptical flow via warping from the first frame and serve it\nas a supplementary reference in the diffusion model. This\nenables our model for video synthesis by editing the first\n*Work partially done during an internship at Meta GenAI.\n\u2020Corresponding author.\nframe with any prevalent I2I models and then propagating\nedits to successive frames. Our V2V model, FlowVid, demon-\nstrates remarkable properties: (1) Flexibility: FlowVid works\nseamlessly with existing I2I models, facilitating various mod-\nifications, including stylization, object swaps, and local edits.\n(2) Efficiency: Generation of a 4-second video with 30 FPS\nand 512\u00d7512 resolution takes only 1.5 minutes, which is\n3.1\u00d7, 7.2\u00d7, and 10.5\u00d7 faster than CoDeF, Rerender, and\nTokenFlow, respectively. (3) High-quality: In user studies,\nour FlowVid is preferred 45.7% of the time, outperforming\nCoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).\n1. Introduction\nText-guided Video-to-video (V2V) synthesis, which aims to\nmodify the input video according to given text prompts, has\n1\narXiv:2312.17681v1  [cs.CV]  29 Dec 2023\n(a). Input video\n(b). Flow warping \n1st frame\n(c). Spatial control (d). FlowVid output\n10th frame\n20th frame\nFigure 2. (a) Input video: \u2019a man is running on beach\u2019.\n(b) We edit the 1st frame with \u2019a man is running on\nMars\u2019, then conduct flow warping from the 1st frame to the 10th\nand 20th frames (using input video flow). Flow estimation of legs\nis inaccurate. (c) Our FlowVid uses spatial controls to rectify the\ninaccurate flow. (d) Our consistent video synthesis results.\nwide applications in various domains, such as short-video\ncreation and more broadly in the film industry. Notable ad-\nvancements have been seen in text-guided Image-to-Image\n(I2I) synthesis [4, 14, 31, 43], greatly supported by large pre-\ntrained text-to-image diffusion models [37, 39, 40]. However,\nV2V synthesis remains a formidable task. In contrast to still\nimages, videos encompass an added temporal dimension.\nDue to the ambiguity of text, there are countless ways to edit\nframes so they align with the target prompt. Consequently,\nnaively applying I2I models on videos often produces unsat-\nisfactory pixel flickering between frames.\nTo improve frame consistency, pioneering studies edit\nmultiple frames jointly by inflating the image model with\nspatial-temporal attention [6, 25, 35, 46]. While these meth-\nods offer improvements, they do not fully attain the sought-\nafter temporal consistency. This is because the motion within\nvideos is merely retained in an implicit manner within the\nattention module. Furthermore, a growing body of research\nemploys explicit optical flow guidance from videos. Specifi-\ncally, flow is used to derive pixel correspondence, resulting\nin a pixel-wise mapping between two frames. The corre-\nspondence is later utilized to obtain occlusion masks for\ninpainting [19, 49] or to construct a canonical image [32]\nHowever, these hard constraints can be problematic if flow\nestimation is inaccurate, which is often observed when the\nflow is determined through a pre-trained model [42, 47, 48].\nIn this paper, we propose to harness the benefits of opti-\ncal flow while handling the imperfection in flow estimation.\nSpecifically, we perform flow warping from the first frame\nto subsequent frames. These warped frames are expected to\nfollow the structure of the original frames but contain some\noccluded regions (marked as gray), as shown in Figure 2(b).\nIf we use flow as hard constraints, such as inpainting [19, 49]\nthe occluded regions, the inaccurate legs estimation would\npersist, leading to an undesirable outcome. We seek to in-\nclude an additional spatial condition, such as a depth map in\nFigure 2(c), along with a temporal flow condition. The legs\u2019\nposition is correct in spatial conditions, and therefore, the\njoint spatial-temporal condition would rectify the imperfect\noptical flow, resulting in consistent results in Figure 2(d).\nWe build a video diffusion model upon an inflated spatial\ncontrolled I2I model. We train the model to predict the input\nvideo using spatial conditions (e.g., depth maps) and tempo-\nral conditions (flow-warped video). During generation, we\nemploy an edit-propagate procedure: (1) Edit the first frame\nwith prevalent I2I models. (2) Propagate the edits throughout\nthe video using our trained model. The decoupled design\nallows us to adopt an autoregressive mechanism: the cur-\nrent batch\u2019s last frame can be the next batch\u2019s first frame,\nallowing us to generate lengthy videos.\nWe train our model with 100k real videos from Shut-\nterStock [1], and it generalizes well to different types of\nmodifications, such as stylization, object swaps, and local\nedits, as seen in Figure 1. Compared with existing V2V\nmethods, our FlowVid demonstrates significant advantages\nin terms of efficiency and quality. Our FlowVid can gen-\nerate 120 frames (4 seconds at 30 FPS) in high-resolution\n(512\u00d7512) in just 1.5 minutes on one A-100 GPU, which is\n3.1\u00d7, 7.2\u00d7 and 10.5\u00d7 faster than state-of-the-art methods\nCoDeF [32] (4.6 minutes) Rerender [49] (10.8 minutes), and\nTokenFlow [13] (15.8 minutes). We conducted a user study\non 25 DAVIS [34] videos and designed 115 prompts. Results\nshow that our method is more robust and achieves a pref-\nerence rate of 45.7% compared to CoDeF (3.5%) Rerender\n(10.2%) and TokenFlow (40.4%)\nOur contributions are summarized as follows: (1) We\nintroduce FlowVid, a V2V synthesis method that harnesses\nthe benefits of optical flow, while delicately handling the\nimperfection in flow estimation. (2) Our decoupled edit-\npropagate design supports multiple applications, including\nstylization, object swap, and local editing. Furthermore, it\nempowers us to generate lengthy videos via autoregressive\nevaluation. (3) Large-scale human evaluation indicates the\nefficiency and high generation quality of FlowVid.\n2. Related Work\n2.1. Image-to-image Diffusion Models\nBenefiting from large-scale pre-trained text-to-image (T2I)\ndiffusion models [2, 11, 39, 40], progress has been made\nin text-based image-to-image (I2I) generation [10, 14, 24,\n30, 31, 33, 43, 51]. Beginning with image editing methods,\nPrompt-to-prompt [14] and PNP [43] manipulate the atten-\ntions in the diffusion process to edit images according to\ntarget prompts. Instruct-pix2pix [4] goes a step further by\ntraining an I2I model that can directly interpret and follow\n2\nhuman instructions. More recently, I2I methods have ex-\ntended user control by allowing the inclusion of reference\nimages to precisely define target image compositions. No-\ntably, ControlNet, T2I-Adapter [31], and Composer [20]\nhave introduced spatial conditions, such as depth maps, en-\nabling generated images to replicate the structure of the\nreference. Our method falls into this category as we aim to\ngenerate a new video while incorporating the spatial compo-\nsition in the original one. However, it\u2019s important to note that\nsimply applying these I2I methods to individual video frames\ncan yield unsatisfactory results due to the inherent challenge\nof maintaining consistency across independently generated\nframes (per-frame results can be found in Section 5.2).\n2.2. Video-to-video Diffusion Models\nTo jointly generate coherent multiple frames, it is now a com-\nmon standard to inflate image models to video: replacing\nspatial-only attention with spatial-temporal attention. For in-\nstance, Tune-A-Video [46], Vid-to-vid zero [44], Text2video-\nzero [25], Pix2Video [6] and FateZero [35] performs cross-\nframe attention of each frame on anchor frame, usually the\nfirst frame and the previous frame to preserve appearance\nconsistency. TokenFlow [13] further explicitly enforces se-\nmantic correspondences of diffusion features across frames\nto improve consistency. Furthermore, more works are adding\nspatial controls, e.g., depth map to constraint the generation.\nZhang\u2019s ControlVideo [50] proposes to extend image-based\nControlNet to the video domain with full cross-frame atten-\ntion. Gen-1 [12], VideoComposer [45], Control-A-Video [7]\nand Zhao\u2019s ControlVideo [52] train V2V models with paired\nspatial controls and video data. Our method falls in the same\ncategory but it also includes the imperfect temporal flow in-\nformation into the training process alongside spatial controls.\nThis addition enhances the overall robustness and adaptabil-\nity of our method.\nAnother line of work is representing video as 2D images,\nas seen in methods like layered atlas [23], Text2Live [3],\nshape-aware-edit [26], and CoDeF [32]. However, these\nmethods often require per-video optimization and they also\nface performance degradation when dealing with large mo-\ntion, which challenges the creation of image representations.\n2.3. Optical flow for video-to-video synthesis\nThe use of optical flow to propagate edits across frames\nhas been explored even before the advent of diffusion mod-\nels, as demonstrated by the well-known Ebsythn [22] ap-\nproach. In the era of diffusion models, Chu\u2019s Video Con-\ntrolNet [9] employs the ground-truth (gt) optical flow from\nsynthetic videos to enforce temporal consistency among cor-\nresponding pixels across frames. However, it\u2019s important\nto note that ground-truth flow is typically unavailable in\nreal-world videos, where flow is commonly estimated using\npretrained models [42, 47, 48]. Recent methods like Reren-\nder [49], MeDM [8], and Hu\u2019s VideoControlNet [19] use\nestimated flow to generate occlusion masks for in-painting.\nIn other words, these methods \u201dforce\u201d the overlapped re-\ngions to remain consistent based on flow estimates. Simi-\nlarly, CoDeF [32] utilizes flow to guide the generation of\ncanonical images. These approaches all assume that flow\ncould be treated as an accurate supervision signal that must\nbe strictly adhered to. In contrast, our FlowVid recognizes\nthe imperfections inherent in flow estimation and presents an\napproach that leverages its potential without imposing rigid\nconstraints.\n3. Preliminary\nLatent Diffusion Models\nDenoising Diffusion Probabilis-\ntic Models (DDPM) [16] generate images through a progres-\nsive noise removal process applied to an initial Gaussian\nnoise, carried out for T time steps. Latent Diffusion mod-\nels [39] conduct diffusion process in latent space to make\nit more efficient. Specifically, an encoder E compresses\nan image I \u2208 RH\u00d7W \u00d73 to a low-resolution latent code\nz = E(I) \u2208 RH/8\u00d7W/8\u00d74. Given z0 := z, the Gaussian\nnoise is gradually added on z0 with time step t to get noisy\nsample zt. Text prompt \u03c4 is also a commonly used condition.\nA time-conditional U-Net \u03f5\u03b8 is trained to reverse the process\nwith the loss function:\nLLDM = Ez0,t,\u03c4,\u03f5\u223cN (0,1)\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4)\u22252\n2\n(1)\nControlNet\nControlNet provides additional spatial con-\nditions, such as canny edge [5] and depth map [38], to\ncontrol the generation of images. More specifically, spa-\ntial conditions C \u2208 RH\u00d7W \u00d73 are first converted to latents\nc \u2208 RH/8\u00d7W/8\u00d74 via several learnable convolutional layers.\nSpatial latent c, added by input latent zt, is passed to a copy\nof the pre-trained diffusion model, more known as Control-\nNet. The ControlNet interacts with the diffusion model in\nmultiple feature resolutions to add spatial guidance during\nimage generation. ControlNet rewrites Equation 1 to\nLCN = Ez0,t,\u03c4,c,\u03f5\u223cN (0,1)\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4, c)\u22252\n2\n(2)\n4. FlowVid\nFor video-to-video generation, given an input video with N\nframes I = {I1, . . . , IN} and a text prompt \u03c4, the goal is\ntransfer it to a new video I\u2032 = {I\u2032\n1, . . . , I\u2032\nN} which adheres\nto the provided prompt \u03c4 \u2032, while keeping consistency across\nframe. We first discuss how we inflate the image-to-image\ndiffusion model, such as ControlNet to video, with spatial-\ntemporal attention [6, 25, 35, 46] (Section 4.1) Then, we\nintroduce how to incorporate imperfect optical flow as a\ncondition into our model (Section 4.2). Lastly, we introduce\nthe edit-propagate design for generation (Section 4.3).\n3\nAdding \nnoise\n\ud835\udc53\nConv\n+\nCLIP\nA man is running on beach.\n~\n\ud835\udc53\n+\nA man is running on Mars.\nCLIP\nSpatial conditions\nInput video\nFlow warped video\n(a). Training\n(b). Generation\n\u2026\n\u2026\nDepth\npred.\n\u2026\nFlow \nwarping \nfrom 1st \nframe\nConv\nFlow warped edited video\n\u2026\n\u2026\nSynthesized \nvideo\nConcat\nConcat\nLoss \n\u2026\nSpatial conditions\n\u2026\nInput video\nFigure 3. Overview of our FlowVid. (a) Training: we first get the spatial conditions (predicted depth maps) and estimated optical flow from\nthe input video. For all frames, we use flow to perform warping from the first frame. The resulting flow-warped video is expected to have a\nsimilar structure as the input video but with some occluded regions (marked as gray, better zoomed in). We train a video diffusion model\nwith spatial conditions c and flow information f. (b) Generation: we edit the first frame with existing I2I models and use the flow in the input\nvideo to get the flow warped edited video. The flow condition spatial condition jointly guides the output video synthesis.\n4.1. Inflating image U-Net to accommodate video\nThe latent diffusion models (LDMs) are built upon the ar-\nchitecture of U-Net, which comprises multiple encoder and\ndecoder blocks. Each block has two components: a resid-\nual convolutional module and a transformer module. The\ntransformer module, in particular, comprises a spatial self-\nattention layer, a cross-attention layer, and a feed-forward\nnetwork. To extend the U-Net architecture to accommodate\nan additional temporal dimension, we first modify all the 2D\nlayers within the convolutional module to pseudo-3D layers\nand add an extra temporal self-attention layer [18]. Follow-\ning common practice [6, 18, 25, 35, 46], we further adapt the\nspatial self-attention layer to a spatial-temporal self-attention\nlayer. For video frame Ii, the attention matrix would take the\ninformation from the first frame I1 and the previous frame\nIi\u22121. Specifically, we obtain the query feature from frame\nIi, while getting the key and value features from I1 and Ii\u22121.\nThe Attention(Q, K, V ) of spatial-temporal self-attention\ncould be written as\nQ = W QzIi, K = W K h\nzI1, zIi\u22121\ni\n, V = W V h\nzI1, zIi\u22121\ni\n(3)\nwhere W Q, W K, and W V are learnable matrices that project\nthe inputs to query, key, and value. zIi is the latent for frame\nIi. [\u00b7] denotes concatenation operation. Our model includes\nan additional ControlNet U-Net that processes spatial condi-\ntions. We discovered that it suffices only to expand the major\nU-Net, as the output from the ControlNet U-Net is integrated\ninto this major U-Net.\n4.2. Training with joint spatial-temporal conditions\nUpon expanding the image model, a straightforward method\nmight be to train the video model using paired depth-video\ndata. Yet, our empirical analysis indicates that this leads\nto sub-optimal results, as detailed in the ablation study in\nSection 5.4. We hypothesize that this method neglects the\ntemporal clue within the video, making the frame consistency\nhard to maintain. While some studies, such as Rerender [49]\nand CoDeF [32], incorporate optical flow in video synthesis,\nthey typically apply it as a rigid constraint. In contrast, our\napproach uses flow as a soft condition, allowing us to manage\nthe imperfections commonly found in flow estimation.\nGiven a sequence of frames I, we calculate the flow\nbetween the first frame I1 and other frames Ii, using a pre-\ntrained flow estimation model UniMatch [48]. We denote the\nF1\u2192i and Fi\u21921 as the forward and backward flow. Using\nforward-backward consistency check [29], we can derive\nforward and backward occlusion masks Ofwd\n1\u2192i and Obwd\ni\u21921.\nUse backward flow Fi\u21921 and occlusion Obwd\ni\u21921, we can per-\nform Warp operation over the first frame I1 to get IW\ni .\nIntuitively, warped ith frame IW\ni\nhas the same layout as\nthe original frame Ii but the pixels are from the first frame\nI1. Due to occlusion, some blank areas could be in the IW\ni\n(marked as gray in Figure 3).\nWe denote the sequence of warped frames as flow warped\nvideo IW = {IW\n1 , . . . , IW\nN }. We feed IW into the same\nencoder E to convert it into a latent representation f. This la-\ntent representation is then concatenated with the noisy input\nzt to serve as conditions. To handle the increased channel\ndimensions of f, we augment the first layer of the U-Net\nwith additional channels, initializing these new channels\nwith zero weights. We also integrate this concatenated flow\ninformation into the spatial ControlNet U-Net, reconfigur-\ning its initial layer to include additional channels. With this\n4\nintroduced flow information f, we modify Equation 2 as:\nLF lowV id = Ez0,t,\u03c4,c,f,\u03f5\u223cN (0,1)\u2225\u03f5\u2212\u03f5\u03b8(zt, t, \u03c4, c, f)\u22252\n2 (4)\nThroughout the development of our experiments, two\nparticular design choices have been proven crucial for\nenhancing our final results. First, we opted for v-\nparameterization [41], rather than the more commonly\nused \u03f5-parameterization. This finding is consistent with\nother video diffusion models, such as Gen-1 [12] and Im-\nagen Video [17] (see ablation in Section 5.4). Second, in-\ncorporating additional elements beyond the flow-warped\nvideo would further improve the performance. Specifically,\nincluding the first frame as a constant video sequence,\nI1st = {I1, . . . , I1}, and integrating the occlusion masks\nO = {Obwd\n1\u21921, . . . , Obwd\nN\u21921} enhanced the overall output qual-\nity. We process I1st by transforming it into a latent repre-\nsentation and then concatenating it with the noisy latent,\nsimilar to processing IW . For O, we resize the binary mask\nto match the latent size before concatenating it with the noisy\nlatent. Further study is included in Section 5.4.\n4.3. Generation: edit the first frame then propagate\nDuring the generation, we want to transfer the input video\nI to a new video I\u2032 with the target prompt \u03c4 \u2032. To effec-\ntively leverage the prevalent I2I models, we adopt an edit-\npropagate method. This begins with editing the first frame\nI1 using I2I models, resulting in an edited first frame I\u2032\n1. We\nthen propagate the edits to subsequent ith frame by using the\nflow Fi\u21921 and the occlusion mask Obwd\ni\u21921, derived from the\ninput video I. This process yields the flow-warped edited\nvideo I\u2032W = {I\u2032W\n1\n, . . . , I\u2032W\nN }. We input I\u2032W into the same\nencoder E and concatenate the resulting flow latent f with\na randomly initialized Gaussian noise zT drawn from the\nnormal distribution N. The spatial conditions from the in-\nput video are also used to guide the structural layout of the\nsynthesized video. Intuitively, the flow-warped edited video\nserves as a texture reference while spatial controls regularize\nthe generation, especially when we have inaccurate flow. Af-\nter DDIM denoising, the denoised latent z0 is brought back\nto pixel space with a decoder D to get the final output.\nIn addition to offering the flexibility to select I2I models\nfor initial frame edits, our model is inherently capable of\nproducing extended video clips in an autoregressive manner.\nOnce the first N edited frames {I\u2032\n1, . . . , I\u2032\nN} are generated,\nthe N th frame I\u2032\nN can be used as the starting point for editing\nthe subsequent batch of frames {IN, . . . , I2N\u22121}. However,\na straightforward autoregressive approach may lead to a gray-\nish effect, where the generated images progressively become\ngrayer, see Figure 4(a). We believe this is a consequence of\nthe lossy nature of the encoder and decoder, a phenomenon\nalso noted in Rerender [49]. To mitigate this issue, we intro-\nduce a simple global color calibration technique that effec-\ntively reduces the graying effect. Specifically, for each frame\n(a). w/o calib.\n(b).  w/ calib.\n1st batch\n4th batch\n7th batch\n13th batch\nFigure 4. Effect of color calibration in autoregressive evaluation.\n(a) When the autoregressive evaluation goes from the 1st batch to\nthe 13th batch, the results without color calibration become gray.\n(b) The results are more stable with the proposed color calibration.\nI\u2032\nj in the generated sequence {I\u2032\n1, . . . , I\u2032\nM(N\u22121)+1}, where\nM is the number of autoregressive batches, we calibrate its\nmean and variance to match those of I\u2032\n1. The effect of cal-\nibration is shown in Figure 4(b), where the global color is\npreserved across autoregressive batches.\nI\u2032\u2032\nj =\n \nI\u2032\nj \u2212 mean(I\u2032\nj)\nstd(I\u2032\nj)\n!\n\u00d7 std(I\u2032\n1) + mean(I\u2032\n1)\n(5)\nAnother advantageous strategy we discovered is the in-\ntegration of self-attention features from DDIM inversion, a\ntechnique also employed in works like FateZero [35] and\nTokenFlow [13]. This integration helps preserve the original\nstructure and motion in the input video. Concretely, we use\nDDIM inversion to invert the input video with the original\nprompt and save the intermediate self-attention maps at var-\nious timesteps, usually 20. During the generation with the\ntarget prompt, we substitute the keys and values in the self-\nattention modules with these pre-stored maps. Then, during\nthe generation process guided by the target prompt, we re-\nplace the keys and values within the self-attention modules\nwith previously saved corresponding maps.\n5. Experiments\n5.1. Settings\nImplementation Details\nWe train our model with 100k\nvideos from Shutterstock [1]. For each training video, we\nsequentially sample 16 frames with interval {2,4,8}, which\nrepresent videos lasting {1,2,4} seconds (taking videos with\nFPS of 30). The resolution of all images, including input\nframes, spatial condition images, and flow warped frames,\nis set to 512\u00d7512 via center crop. We train the model with\na batch size of 1 per GPU and a total batch size of 8 with 8\nGPUs. We employ AdamW optimizer [28] with a learning\nrate of 1e-5 for 100k iterations. As detailed in our method, we\ntrain the major U-Net and ControlNet U-Net joint branches\n5\nTokenFlow\nRerender\nInput\nCoDeF\nTokenFlow\nRerender\nInput\nCoDeF\nFlowVid (Ours)\nPer-frame\nPer-frame\nFlowVid (Ours)\n(a). Prompt: a pirate is rowing a boat on a lake.\n(b). Prompt: a oil painting of a tiger walking.\nFigure 5. Qualitative comparison with representative V2V models. Our method stands out in terms of prompt alignment and overall\nvideo quality. We highly encourage readers to refer to video comparisons in our supplementary videos.\nwith v-parameterization [41]. The training takes four days\non one 8-A100-80G node.\nDuring generation, we first generate keyframes with our\ntrained model and then use an off-the-shelf frame interpola-\ntion model, such as RIFE [21], to generate non-key frames.\nBy default, we produce 16 key frames at an interval of 4, cor-\nresponding to a 2-second clip at 8 FPS. Then, we use RIFE to\ninterpolate the results to 32 FPS. We employ classifier-free\nguidance [15] with a scale of 7.5 and use 20 inference sam-\npling steps. Additionally, the Zero SNR noise scheduler [27]\nis utilized. We also fuse the self-attention features obtained\nduring the DDIM inversion of corresponding key frames\nfrom the input video, following FateZero [35]. We evaluate\nour FlowVid with two different spatial conditions: canny\nedge maps [5] and depth maps [38]. A comparison of these\ncontrols can be found in Section 5.4.\nEvaluation\nWe select the 25 object-centric videos from the\npublic DAVIS dataset [34], covering humans, animals, etc.\nWe manually design 115 prompts for these videos, spanning\nfrom stylization to object swap. Besides, we also collect\n50 Shutterstock videos [1] with 200 designed prompts. We\nconduct both qualitative (see Section 5.2) and quantitative\ncomparisons (see Section 5.3) with state-of-the-art methods\nincluding Rerender [49], CoDeF [32] and TokenFlow [13].\nWe use their official codes with the default settings.\n5.2. Qualitative results\nIn Figure 5, we qualitatively compare our method with sev-\neral representative approaches. Starting with a per-frame\nbaseline directly applying I2I models, ControlNet, to each\nframe. Despite using a fixed random seed, this baseline often\nresults in noticeable flickering, such as in the man\u2019s clothing\nand the tiger\u2019s fur. CoDeF [32] produces outputs with signif-\nicant blurriness when motion is big in input video, evident in\nareas like the man\u2019s hands and the tiger\u2019s face. Rerender [49]\noften fails to capture large motions, such as the movement\nof paddles in the left example. Also, the color of the edited\n6\nTable 1. Quantitative comparison with existing V2V models.\nThe preference rate indicates the frequency the method is preferred\namong all the four methods in human evaluation. Runtime shows\nthe time to synthesize a 4-second video with 512\u00d7512 resolution\non one A-100-80GB. Cost is normalized with our method.\nPreference rate\nRuntime\nCost \u2193\n(mean \u00b1 std %) \u2191\n(mins) \u2193\nTokenFlow\n40.4 \u00b1 5.3\n15.8\n10.5 \u00d7\nRerender\n10.2 \u00b1 7.1\n10.8\n7.2 \u00d7\nCoDeF\n3.5 \u00b1 1.9\n4.6\n3.1 \u00d7\nFlowVid (Ours)\n45.7 \u00b1 6.4\n1.5\n1.0 \u00d7\ntiger\u2019s legs tends to blend in with the background. Token-\nFlow [13] occasionally struggles to follow the prompt, such\nas transforming the man into a pirate in the left example. It\nalso erroneously depicts the tiger with two legs for the first\nframe in the right example, leading to flickering in the output\nvideo. In contrast, our method stands out in terms of editing\ncapabilities and overall video quality, demonstrating superior\nperformance over these methods. We highly encourage read-\ners to refer to more video comparisons in our supplementary\nvideos.\n5.3. Quantitative results\nUser study\nWe conducted a human evaluation to compare\nour method with three notable works: CoDeF [32], Reren-\nder [49], and TokenFlow [13]. The user study involves 25\nDAVIS videos and 115 manually designed prompts. Partici-\npants are shown four videos and asked to identify which one\nhas the best quality, considering both temporal consistency\nand text alignment. The results, including the average pref-\nerence rate and standard deviation from five participants for\nall methods, are detailed in Table 1. Our method achieved\na preference rate of 45.7%, outperforming CoDeF (3.5%),\nRerender (10.2%), and TokenFlow (40.4%). During the eval-\nuation, we observed that CoDeF struggles with significant\nmotion in videos. The blurry constructed canonical images\nwould always lead to unsatisfactory results. Rerender occa-\nsionally experiences color shifts and bright flickering. Token-\nFlow sometimes fails to sufficiently alter the video according\nto the prompt, resulting in an output similar to the original\nvideo.\nPipeline runtime\nWe also compare runtime efficiency\nwith existing methods in Table 1. Video lengths can vary,\nresulting in different processing times. Here, we use a video\ncontaining 120 frames (4 seconds video with FPS of 30).\nThe resolution is set to 512 \u00d7 512. Both our FlowVid model\nand Rerender [49] use a key frame interval of 4. We generate\n31 keyframes by applying autoregressive evaluation twice,\nfollowed by RIFE [21] for interpolating the non-key frames.\n1st frame\n10th frame\n(I). spatial controls\n(II). flow warped vid (III). flow occlusion\n(IV). first frame\n(a) Condition types.\nCondition choices\nWinning rate \u2191\n(I)\n(II)\n(III)\n(IV)\n\u2713\n\u00d7\n\u00d7\n\u00d7\n9%\n\u2713\n\u2713\n\u00d7\n\u00d7\n38%\n\u2713\n\u2713\n\u2713\n\u00d7\n42 %\n(b) Winning rate over our FlowVid (I + II + III + IV).\nFigure 6. Ablation study of condition combinations. (a) Four\ntypes of conditions. (b) The different combinations all underper-\nform our final setting which combines all four conditions.\nThe total runtime, including image processing, model op-\neration, and frame interpolation, is approximately 1.5 min-\nutes. This is significantly faster than CoDeF (4.6 minutes),\nRerender (10.8 minutes) and TokenFlow (15.8 minutes),\nbeing 3.1\u00d7, 7.2\u00d7, and 10.5 \u00d7 faster, respectively. CoDeF\nrequires per-video optimization to construct the canonical\nimage. While Rerender adopts a sequential method, generat-\ning each frame one after the other, our model utilizes batch\nprocessing, allowing for more efficient handling of multiple\nframes simultaneously. In the case of TokenFlow, it requires\na large number of DDIM inversion steps (typically around\n500) for all frames to obtain the inverted latent, which is\na resource-intensive process. We further report the runtime\nbreakdown (Figure 10) in the Appendix.\n5.4. Ablation study\nCondition combinations\nWe study the four types of con-\nditions in Figure 6(a): (I) Spatial controls: such as depth\nmaps [38]. (II) Flow warped video: frames warped from the\nfirst frame using optical flow. (III) Flow occlusion: masks\nindicate which parts are occluded (marked as white). (IV)\nFirst frame. We evaluate combinations of these conditions\nin Figure 6(b), assessing their effectiveness by their winning\nrate against our full model which contains all four condi-\ntions. The spatial-only condition achieved a 9% winning\nrate, limited by its lack of temporal information. Including\nflow warped video significantly improved the winning rate\nto 38%, underscoring the importance of temporal guidance.\nWe use gray pixels to indicate occluded areas, which might\nblend in with the original gray colors in the images. To avoid\n7\n(a). Input frame\n(b). Spatial control\n(c). Style transfer\n(d). Object swap\nFigure 7. Ablation study of different spatial conditions. Canny\nedge and depth map are estimated from the input frame. Canny\nedge provides more detailed controls (good for stylization) while\ndepth map provides more editing flexibility (good for object swap).\n(a).     -prediction\n(b).     -prediction\nFigure 8. Ablation study of different parameterizations. \u03f5-\nprediction often predicts unnatural global color while v-prediction\ndoesn\u2019t. Prompt: \u2019a man is running on Mars\u2019.\npotential confusion, we further include a binary flow occlu-\nsion mask, which better helps the model to tell which part\nis occluded or not. The winning rate is further improved\nto 42%. Finally, we added the first frame condition to pro-\nvide better texture guidance, particularly useful when the\nocclusion mask is large and few original pixels remain.\nDifferent control type: edge and depth\nWe study\ntwo types of spatial conditions in our FlowVid: canny\nedge [5] and depth map [38]. Given an input frame as\nshown in Figure 7(a), the canny edge retains more de-\ntails than the depth map, as seen from the eyes and\nmouth of the panda. The strength of spatial control\nwould, in turn, affect the video editing. For style transfer\nprompt \u2019A Chinese ink painting of a panda\neating bamboo\u2019, as shown in Figure 7(c), the output of\ncanny condition could keep the mouth of the panda in the\nright position while the depth condition would guess where\nthe mouth is and result in an open mouth. The flexibility of\nthe depth map, however, would be beneficial if we are doing\nobject swap with prompt \u2019A koala eating bamboo\u2019,\nas shown in Figure 7(d); the canny edge would put a pair of\npanda eyes on the face of the koala due to the strong control,\nwhile depth map would result in a better koala edit. During\nInput video\nFlow warping\nFinal prediction\n(a). Misaligned first frame\n(b). Big occlusion\n1st frame\n20th frame\n1st frame\n20th frame\nFigure 9. Limitations of FlowVid. Failure cases include (a) the\nedited first frame doesn\u2019t align structurally with the original first\nframe, and (b) large occlusions caused by fast motion.\nour evaluation, we found canny edge works better when we\nwant to keep the structure of the input video as much as\npossible, such as stylization. The depth map works better\nif we have a larger scene change, such as an object swap,\nwhich requires more considerable editing flexibility.\nv-prediction and \u03f5-prediction\nWhile \u03f5-prediction is com-\nmonly used for parameterization in diffusion models, we\nfound it may suffer from unnatural global color shifts across\nframes, as shown in Figure 8. Even though all these two\nmethods use the same flow warped video, the \u03f5-prediction\nintroduces an unnatural grayer color. This phenomenon is\nalso found in Imagen-Video [17].\n5.5. Limitations\nAlthough our FlowVid achieves significant performance, it\ndoes have some limitations. First, our FlowVid heavily relies\non the first frame generation, which should be structurally\naligned with the input frame. As shown in Figure 9(a), the\nedited first frame identifies the hind legs of the elephant\nas the front nose. The erroneous nose would propagate to\nthe following frame and result in an unsatisfactory final\nprediction. The other challenge is when the camera or the\nobject moves so fast that large occlusions occur. In this case,\nour model would guess, sometimes hallucinate, the missing\nblank regions. As shown in Figure 9(b), when the ballerina\nturns her body and head, the entire body part is masked out.\nOur model manages to handle the clothes but turns the back\nof the head into the front face, which would be confusing if\ndisplayed in a video.\n8\n6. Conclusion\nIn this paper, we propose a consistent video-to-video syn-\nthesis method using joint spatial-temporal conditions. In\ncontrast to prior methods that strictly adhere to optical flow,\nour approach incorporates flow as a supplementary reference\nin synergy with spatial conditions. Our model can adapt\nexisting image-to-image models to edit the first frame and\npropagate the edits to consecutive frames. Our model is also\nable to generate lengthy videos via autoregressive evaluation.\nBoth qualitative and quantitative comparisons with current\nmethods highlight the efficiency and high quality of our\nproposed techniques.\n7. Acknowledgments\nWe would like to express sincere gratitude to Yurong Jiang,\nChenyang Qi, Zhixing Zhang, Haoyu Ma, Yuchao Gu, Jonas\nSchult, Hung-Yueh Chiang, Tanvir Mahmud, Richard Yuan\nfor the constructive discussions.\nFeng Liang and Diana Marculescu were supported in part\nby the ONR Minerva program, iMAGiNE - the Intelligent\nMachine Engineering Consortium at UT Austin, and a UT\nCockrell School of Engineering Doctoral Fellowship.\nReferences\n[1] Stock footage video, royalty-free hd, 4k video clips, 2023. 2,\n5, 6\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324, 2022. 2\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image and\nvideo editing. In European conference on computer vision,\npages 707\u2013723. Springer, 2022. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023. 2\n[5] John Canny. A computational approach to edge detection.\nIEEE Transactions on pattern analysis and machine intelli-\ngence, (6):679\u2013698, 1986. 3, 6, 8\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 23206\u201323217, 2023. 2, 3, 4\n[7] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin\nXia, Xuefeng Xiao, and Liang Lin. Control-a-video: Control-\nlable text-to-video generation with diffusion models. arXiv\npreprint arXiv:2305.13840, 2023. 3\n[8] Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, and Jun-Cheng\nChen. Medm: Mediating image diffusion models for video-\nto-video translation with temporal correspondence guidance.\narXiv preprint arXiv:2308.10079, 2023. 3\n[9] Ernie Chu, Shuo-Yen Lin, and Jun-Cheng Chen. Video con-\ntrolnet: Towards temporally consistent synthetic-to-real video\ntranslation using conditional image diffusion models. arXiv\npreprint arXiv:2305.19193, 2023. 3\n[10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. Diffedit: Diffusion-based semantic image edit-\ning with mask guidance. arXiv preprint arXiv:2210.11427,\n2022. 2\n[11] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiao-\nfang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image\ngeneration models using photogenic needles in a haystack.\narXiv preprint arXiv:2309.15807, 2023. 2\n[12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Structure\nand content-guided video synthesis with diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7346\u20137356, 2023. 3, 5\n[13] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 2, 3, 5, 6, 7,\n11\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2\n[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. Imagen video:\nHigh definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303, 2022. 5, 8\n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. arXiv:2204.03458, 2022. 4\n[19] Zhihao Hu and Dong Xu.\nVideocontrolnet: A motion-\nguided video-to-video translation framework by using diffu-\nsion model with controlnet. arXiv preprint arXiv:2307.14073,\n2023. 2, 3\n[20] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 3\n[21] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and\nShuchang Zhou. Real-time intermediate flow estimation for\nvideo frame interpolation. In Proceedings of the European\nConference on Computer Vision (ECCV), 2022. 6, 7, 11\n[22] Ond\u02c7rej Jamri\u02c7ska, \u02c7S\u00b4arka Sochorov\u00b4a, Ond\u02c7rej Texler, Michal\nLuk\u00b4a\u02c7c, Jakub Fi\u02c7ser, Jingwan Lu, Eli Shechtman, and Daniel\nS`ykora. Stylizing video by example. ACM Transactions on\nGraphics (TOG), 38(4):1\u201311, 2019. 3\n[23] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 3\n9\n[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 2\n[25] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\narXiv preprint arXiv:2303.13439, 2023. 2, 3, 4\n[26] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth\nQiu, and Jia-Bin Huang. Shape-aware text-driven layered\nvideo editing. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 14317\u2013\n14326, 2023. 3\n[27] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Com-\nmon diffusion noise schedules and sample steps are flawed.\narXiv preprint arXiv:2305.08891, 2023. 6\n[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[29] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsu-\npervised learning of optical flow with a bidirectional census\nloss. In Proceedings of the AAAI conference on artificial\nintelligence, 2018. 4\n[30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2021. 2\n[31] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n3\n[32] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 2, 3, 4, 6, 7, 11\n[33] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 2\n[34] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 2, 6\n[35] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2, 3, 4, 5, 6\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 11\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 1(2):\n3, 2022. 2\n[38] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE transactions on pattern analysis and machine\nintelligence, 44(3):1623\u20131637, 2020. 3, 6, 7, 8\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 2\n[41] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 5, 6\n[42] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020.\n2, 3\n[43] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel.\nPlug-and-play diffusion features for text-driven image-to-\nimage translation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1921\u20131930, 2023. 2\n[44] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[45] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Videocomposer: Compositional video synthesis\nwith motion controllability. arXiv preprint arXiv:2306.02018,\n2023. 3\n[46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2, 3, 4\n[47] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and\nDacheng Tao. Gmflow: Learning optical flow via global\nmatching. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8121\u20138130,\n2022. 2, 3\n[48] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher\nYu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo\nand depth estimation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023. 2, 3, 4\n10\n[49] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy.\nRerender a video: Zero-shot text-guided video-to-video trans-\nlation. arXiv preprint arXiv:2306.07954, 2023. 2, 3, 4, 5, 6,\n7, 11\n[50] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-\nfree controllable text-to-video generation. arXiv preprint\narXiv:2305.13077, 2023. 3\n[51] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren. Sine: Single image editing with text-\nto-image diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6027\u20136037, 2023. 2\n[52] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun\nZhu. Controlvideo: Adding conditional control for one shot\ntext-to-video editing. arXiv preprint arXiv:2305.17098, 2023.\n3\nA. Webpage Demo\nWe highly recommend looking at our demo web by\nopening the https://jeff-liangf.github.io/\nprojects/flowvid/ to check the video results.\nB. Quantitative comparisons\nB.1. CLIP scores\nInspired by previous research, we utilize CLIP [36] to evalu-\nate the generated videos\u2019 quality. Specifically, we measure 1)\nTemporal Consistency (abbreviated as Tem-Con), which is\nthe mean cosine similarity across all sequential frame pairs,\nand 2) Prompt Alignment (abbreviated as Pro-Ali), which\ncalculates the mean cosine similarity between a given text\nprompt and all frames in a video. Our evaluation, detailed\nin Table 2, includes an analysis of 116 video-prompt pairs\nfrom the DAVIS dataset. Notably, CoDeF [32] and Reren-\nder [49] exhibit lower scores in both temporal consistency\nand prompt alignment, aligning with the findings from our\nuser study. Interestingly, TokenFlow shows superior perfor-\nmance in maintaining temporal consistency. However, it is\nimportant to note that TokenFlow occasionally underper-\nforms in modifying the video, leading to outputs that closely\nresemble the original input. Our approach not only secures\nthe highest ranking in prompt alignment but also performs\ncommendably in temporal consistency, achieving second\nplace.\nB.2. Runtime breakdown\nWe benchmark the runtime with a 512 \u00d7 512 resolution\nvideo containing 120 frames (4 seconds video with FPS\nof 30). Our runtime evaluation was conducted on a 512 \u00d7\n512 resolution video comprising 120 frames, equating to a\n4-second clip at 30 frames per second (FPS). Both our meth-\nods, FlowVid, and Rerender [49], initially create key frames\nTable 2. CLIP score comparisons. \u2019Tem-Con\u2019 stands for temporal\nconsistency, and \u2019Pro-Ali\u2019 stands for prompt alignment.\nMethod\nTem-Con \u2191\nPro-Ali \u2191\nCoDeF [32]\n96.98\n30.83\nRerender [49]\n96.88\n31.84\nTokenFlow [13]\n97.30\n33.11\nOurs\n97.08\n33.20\nFlowVid\nCoDeF\nRerender\nTokenFlow\n0\n2\n4\n6\n8\n10\n12\n14\n16\nTime (minutes)\n1.2\n3.5\n1.1\n5.2\n5.6\n10.4\n5.4\n1.5\n4.6\n10.8\n15.8\nFlowVid key-frame gen.\nFlowVid frame interp.\nCoDeF training\nCoDeF inference\nRerender key-frame gen.\nRerender frame interp.\nTokenFlow DDIM inversion\nTokenFlow inference\nFigure 10. Runtime breakdown of generating a 4-second 512 \u00d7\n512 resolution video with 30 FPS. Time is measured on one A100-\n80GB GPU.\nfollowed by the interpolation of non-key frames. For these\ntechniques, we opted for a keyframe interval of 4. FlowVid\ndemonstrates a marked efficiency in keyframe generation,\ncompleting 31 keyframes in just 1.1 minutes, compared to\nRerender\u2019s 5.2 minutes. This significant time disparity is at-\ntributed to our batch processing approach in FlowVid, which\nhandles 16 images simultaneously, unlike Rerender\u2019s se-\nquential, single-image processing method. In the aspect of\nframe interpolation, Rerender employs a reference-based Eb-\nSynth method, which relies on input video\u2019s non-key frames\nfor interpolation guidance. This process is notably time-\nconsuming, requiring 5.6 minutes to interpolate 90 non-key\nframes. In contrast, our method utilizes a non-reference-\nbased approach, RIFE [21], which significantly accelerates\nthe process. Two other methods are CoDeF [32] and Token-\nFlow [13], both of which necessitate per-video preparation.\nSpecifically, CoDeF involves training a model for recon-\nstructing the canonical image, while TokenFlow requires a\n500-step DDIM inversion process to acquire the latent rep-\nresentation. CoDeF and TokenFlow require approximately\n3.5 minutes and 10.4 minutes, respectively, for this initial\npreparation phase.\n11\n"
  },
  {
    "title": "PanGu-$\u03c0$: Enhancing Language Model Architectures via Nonlinearity Compensation",
    "link": "https://arxiv.org/pdf/2312.17276.pdf",
    "upvote": "14",
    "text": "1\nPanGu-\u03c0: Enhancing Language Model\nArchitectures via Nonlinearity Compensation\nYunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan\nBai, Yun Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu,\nQun Liu, Jun Yao, Chao Xu, and Dacheng Tao Fellow, IEEE\nAbstract\u2014The recent trend of large language models (LLMs) is to increase the scale of both model size (a.k.a the number of\nparameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and\nLlama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices.\nHowever, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art\nlanguage model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the\nnonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The\nseries informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further\nused to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the\nmodel nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern,\nnamely, PanGu-\u03c0. Experiments are then conducted using the same dataset and training strategy to compare PanGu-\u03c0 with\nstate-of-the-art LLMs. The results show that PanGu-\u03c0-7B can achieve a comparable performance to that of benchmarks with about\n10% inference speed-up, and PanGu-\u03c0-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition,\nwe have deployed PanGu-\u03c0-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical\napplication. The results show that YunShan can surpass other models with similar scales on benchmarks.\nIndex Terms\u2014Transformer, Large Language Model, Nonlinearity, Network Architecture, Finance, Law.\n\u2726\n1\nINTRODUCTION\nL\nARGE language models (LLMs) have significantly evolved\nand are capable of a wide range of NLP tasks such as ma-\nchine translation, text summarization, and dialogue. Following the\nscaling law [1], a series of studies confirm significantly improved\nperformances and emergent abilities [2] on downstream tasks by\nscaling up the model size and the data size, e.g., GPT-3 with 175B\nparameters [3] and PaLM with 540B parameters [4]. Recently, a\nremarkable innovation, ChatGPT, was introduced with the ability\nto interact with humans in a conversational way. The success of\nChatGPT is attributed to pre-training on extensive textual data and\nfine-tuning with alignment to human preferences. This paradigm\nhas a profound impact on subsequent research and real-world\napplications [5], [6]. The emergence of ChatGPT has inspired\nthe community to develop more excellent large models, including\nLLaMA [7], ChatGLM [8], and Baichuan [9], which in turn drive\nthe vigorous development of the LLM field.\nIn addition to general-purpose LLMs, high-value domain-\nspecific large models are also being extensively researched to\npromote the implementation and practical application of LLMs.\nFor example, LaWGPT [10] enhances the fundamental semantic\nunderstanding in the field of law with a specialized legal vocab-\nulary and extensive pre-training on a large-scale Chinese legal\ncorpus. FinGPT [11] develops an open-source LLM in a data-\ncentric approach. Huatuo [12] builds a Chinese medical instruction\n\u2022\nCorresponding to Yunhe Wang (Huawei Noah\u2019s Ark Lab). E-mail:\nyunhe.wang@huawei.com.\n\u2022\nAcknowledgments: This work is jointly funded by Huawei 2012 Labs\nand Huawei Group Finance. We also thank the work from both the data\nengineering team and IT architecture team.\nfine-tuning dataset and enhances its performance in question-\nanswering within the medical field. As shown in Figure 1, our\nanalysis of the industry distribution of domain-specialized LLMs\nreveals that those in the finance and law domains attract the most\nattention due to their widespread demand and commercial value.\nThe Transformer model introduced in 2017 [13] is the foun-\ndational architecture for many LLMs. The core components of the\nTransformer model include the multi-head self-attention (MSA)\nand the feed-forward network (FFN). MSA computes attention\nscores for each token in the input sequence with respect to all other\ntokens, capturing relationships and dependencies. FFN is per-\nformed on each token separately which provides more nonlinear\ntransformation and model capacity. Transformer architectures are\nutilized to build encoder (e.g., BERT) and decoder (e.g., GPT-2)\nfor NLP tasks. In LLM, decoder-only architecture is widely used\nby predicting the next token with the context information [3], [5].\nBeyond the standard Transformer architecture, a series of studies\nhave explored architecture modification (especially MSA [14],\n[15] or FFN [4], [7]) seeking better performance. PaLM [4] and\nLLaMA [7] use SwiGLU-based FFN [16] which consists of the\ncomponent-wise product of two linear layers, showing signifi-\ncantly increased generation quality. RWKV (Receptance Weighted\nKey Value) [14] proposes an RNN-style attention mechanism\nto alleviate the quadratic complexity in standard MSA. Switch\nTransformer [17] allocates different parameters for each input\nexample and results in a sparsely-activated model.\nThe development of an excellent LLM is a complex system\nengineering, which includes data preparation, data cleaning, model\narchitecture, cluster commutation, and optimizer. The model ar-\nchitecture design is one of the most important components and\narXiv:2312.17276v1  [cs.CL]  27 Dec 2023\n2\n41\n26\n6\n4\n2\n2\n1\n1\n1\n100+\nGeneral LLMs\nFinance\n& Law\nDomain specialized LLMs arise \nfor real-world applications\nDomain Specialized LLMs Statistics @ Dec. 2023\nfrom Github / Listed Company Report\nDomain specialized \nLLMs\nMathGPT\nDevOps-\nModel\nTransGPT\nNSFGPT\nFig. 1: Statistics of domain specialized LLMs. The general LLMs face challenges in supporting industry applications, leading to a\ngrowing emphasis on domain specialized LLMs. Among them, the fields of finance and law are particularly active.\ndetermines the maximum performance potential of the deployed\nLLM. Among the recent projects in 2022-2023, the popular\nversions that are often used for secondary development are GPT-\n3 [3] and LLaMA [7]. By inheriting the observations and analysis\nof our previous work [18], we find that the feature collapse\nproblem also affects the expressive power of these well-designed\nTransformer architectures. Taking LLaMA as an example, we\nempirically analyze its feature collapse phenomenon using the\nrank metric [19]. The feature rank diminishes significantly in\ndeeper layers, leading to a greater similarity among all tokens.\nThis greatly degrades the generation quality and diversity of\nLLMs. We also theoretically analyze the feature collapse problem\nin Transformer architecture (details in Section 3). Through theo-\nretical analysis, we have discovered that nonlinearity significantly\nimpacts the capabilities of the Transformer model. Enhancing\nnonlinearity can effectively mitigate the issue of feature collapse\nand improve the expressive power of the Transformer model. We\nintend to construct stronger LLM architectures by approaching\nthem from a nonlinear perspective.\nIn this paper, we introduce a new architecture for LLMs to\naddress the feature collapse problem via nonlinearity compensa-\ntion, named PanGu-\u03c0. We introduce more nonlinearity from two\napproaches in both FFN and MSA modules without significant\nincreasing the model complexity. First, the series-based activation\nfunction with multiple learnable affine transformation is equipped\nin FFN, which can effectively enhance the nonlinearity of the\nentire network with negligible calculations. Then, the augmented\nshortcut is paralleled with the main branch of each MSA module\nto eschew the rank collapse. To maintain the model efficiency, we\ncarefully refine the augmented shortcut operation with hardware-\nfriendly operations. The enhanced PanGu-\u03c0 architectures (see\nFigure 2) are constructed with both the series activation-based\nFFN and shortcut-augmented MSA. We also prove that the super-\nposition of these two operations can enhance nonlinear compen-\nsation. We build two versions of PanGu-\u03c0 with different model\nsizes, i.e., PanGu-\u03c0-7B and PanGu-\u03c0-1B. By training on a large-\nscale corpus, our PanGu-\u03c0 models obtain general language ability\non downstream tasks. Through carefully designed ablations, we\ndemonstrate that the proposed approach can effectively enhance\nthe model nonlinearity and alleviate feature collapse. Thus, with\nthe same scale of parameters, we can achieve a substantial ef-\nficiency gain via the two new modules. Extensive experiments\non various NLP tasks are evaluated to compare with state-of-\nthe-art LLMs. In a scenario with similar model size, PanGu-\u03c0\nmodels can achieve better performance in terms of both accuracy\nand efficiency. In addition to the foundational abilities, we have\ndeployed PanGu-\u03c0-7B in the high-value domains of finance and\nlaw, developing a specialized LLM named YunShan for practical\napplication. Extensive evaluations of finance and law benchmarks\nalso show that YunShan surpasses other state-of-the-art models\nwith similar scales.\nThis work introduces a new LLM network architecture (i.e.,\nPanGu-\u03c0) with extensive experiments and theoretical analysis\nwith some of the ideas borrowed from our preliminary works\npublished on NeurIPS 2023 [20] and NeurIPS 2021 [18]. This\npresent work makes the following significant contributions. First,\nthe previous two papers, one involving the construction of a CNN\nbackbone [20] and the other focusing on vision Transformers [18],\nhave laid a foundation that this paper seeks to extend within\nLLMs. We achieved commendable results in this endeavor, and\nextensive experiments have validated the effectiveness of our\nmethods. Second, the previous two papers analyzed the network\ndesign subject from distinct perspectives. In our current study,\nthe two works are theoretically integrated and essentially address\nthe same critical issue from a unified standpoint in the LLM\ndomain. Third, we organically adapted series activation to FFN\nand integrated augmented shortcuts into MSA. These two compo-\nnents are complementary to each other and effectively introduce\nmore nonlinearity into the Transformer architecture. Fourth, we\ndeveloped the PanGu-\u03c0 foundation models by large-scale training\nand fine-tuning (SFT), which achieved state-of-the-art results in\ngeneral NLP tasks for a similar model size. In addition, we extend\nPanGu-\u03c0 to finance and legal domains by transfer learning and\nobtain excellent performance on these downstream tasks.\nThe rest of this paper is organized as follows. Section 2\nreviews related work in the field of Transformer architectures\nfor building LLMs and the related hotspot applications. Section 3\nprovides a theoretical analysis of the feature collapse problem and\nthe nonlinear capabilities of existing Transformer architectures.\nSection 4 introduces a nonlinear enhancement strategy based on\nthe series activation function and augmented shortcut. Section\n5 details the data, training strategies, and experimental results\nof the PanGu-\u03c0 architecture with two models with important\nparameter scales, i.e., PanGu-\u03c0-7B and PanGu-\u03c0-1B. In Section 6,\nthe PanGu-\u03c0 architecture is deployed in the high-value domains\nof finance and law, developing the YunShan LLM for practical\n3\nInputs\nEmbedding\nAug-MSA\nNorm\nSIAF-MLP\nCausal Mask\nNorm\n\u00d7 N\nOutputs\nAct\nAct\nAct\nAugmented Shortcut\nMSA\nFeature\nShortcut\nLinear\nLinear\n\u00d7 N\nFig. 2: The diagram of the proposed PanGu-\u03c0 architecture. The series activation function is adapted to FFN, and the augmented\nshortcuts are integrated into MSA, which effectively introduces more nonlinearity into the Transformer architecture.\napplication. Extensive evaluations of finance and law benchmarks\nalso show that YunShan surpasses other state-of-the-art models\nwith similar scales. Section 7 concludes the entire paper and\ndiscusses future works.\n2\nRELATED WORKS\nIn this section, we first summarize the recent representative works\nin the field of LLMs. We then review the classical works for\nenhancing Transformer architectures. Lastly, we investigate the\ndomain-specific large language models, especially in finance and\nlaw.\n2.1\nLLMs\nWith the emergence of ChatGPT [21] from OpenAI, LLMs with\nbillions of parameters achieved astounding performance on var-\nious natural language processing tasks. Subsequently, the latest\nGPT-4 [22] pushed the generalization capabilities of LLMs to a\nnew level. However, the proliferation of GPT-series models is ac-\ncompanied by a strict commercial orientation that is not conducive\nto a thriving open source community. The representative work of\ndemocratic LLMs is LLaMA [23], a collection of open-source\nfoundation language models ranging from 7B to 65B parameters.\nLater, a more elaborate model LLaMA2 [24] is introduced, ap-\npearing to be on par with some closed-source models [21] based\non human evaluations. Since its release, LLaMA has attracted\nextensive attention from the academia and industry. Subsequent\nefforts have been based on LLaMA by either instruction tuning or\ncontinual pre-training. Stanford Alpaca [25] is the first LLaMA-\nbased chatbot fine-tuned with 52K instruction-following samples\ngenerated by the self-instruct method [26]. Vicuna [27] also\nfine-tunes LLaMA with user-shared conversations collected from\nShareGPT [28]. In addition to the language models geared toward\nEnglish-dominant applications, multilingual language models are\nalso thriving. InternLM [29] presents a multilingual foundational\nlanguage model pre-trained on a large corpus with 1.6T tokens\nwith a multi-phase progressive process. Baichuan2 [9] introduces\na series of large-scale multilingual language models containing\n7 billion and 13 billion parameters. PanGu-\u03a3 [30] extends the\ndense Transformer model to a sparse one with Random Routed\nExperts. Qwen [31] introduces a comprehensive language model\nseries that encompasses distinct models with varying parameter\ncounts. Skywork [32] presents 13B LLMs trained on a corpus\ndrawn from both English and Chinese texts with a two-stage\ntraining methodology.\n2.2\nEnhanced Transformer Architectures\nWhile Transformer architectures have gained significant promi-\nnence in LLMs recently, there continues to be a surge of interest in\ntheir effective utilization in diverse domains, including computer\nvision tasks. In light of this, we review classical works dedicated\nto enhancing Transformer structures, with a particular focus on\naugmenting model nonlinearity and efficiency.\nNatural language processing domain. The conventional self-\nattention mechanism, with quadratic computational complexity,\nposes challenges for handling long input sequences during train-\ning and inference. To mitigate this, various structural priors on\nattention, including sparsity [33], [34], [35], [36], [37] and linear\nattention [15], [38], have been proposed. Notably, Reformer [39]\nemploys locality-sensitive hashing to approximate full attention.\nLongformer [40] integrates local windowed attention with task-\nmotivated global attention. Models such as GPT-3[41] incorpo-\nrate locally banded sparse attention methods, such as Factorized\nAttention [34]. There are also works focusing on replacing the\nattention module by incorporating recurrent models [42], [43],\n[44]. Hyena [45] trained a recurrence of gating units and implicitly\nparametrized long convolutions, which serves as an attention-free\ndrop-in replacement for the traditional Transformer architecture.\nRWKV [46] replaced the quadratic QK attention with a scalar\nformulation that has linear cost. RetNet [44] theoretically derived\nthe connection between recurrence and attention and proposed\nthe retention mechanism for sequence modeling. There are also\nefficient enhancements focused on the Feed-Forward Network\n(FFN). Mixture-of-Experts (MoE) [47], [17], [48], [49], [50]\nhas demonstrated effectiveness in the pre-training of LLMs. In\naddition to MoE, PaLM [51] and LLaMA [23] leverage the\nSwiGLU activation for original FFN intermediate activations. This\nchoice is grounded in the observation that SwiGLU activations,\nas demonstrated in compute-equivalent experiments [52], substan-\ntially enhance quality compared to standard activation functions\nlike ReLU, GeLU, or Swish.\nComputer vision domain. PVT [53] and Swin [54] uti-\nlize hierarchical structures across multiple stages, overcoming\n4\nchallenges posed by the original isotropic ViT [55] for diverse\ncomputer vision tasks. Ongoing research focuses on refining local\ninformation processing [56], [57], [58], [59], [60], [61], [62],\nsimplifying attention mechanisms [63], [64], [65], and exploring\nalternative modules [66], [67], [68], [69], [70], [71]. For example,\nT2T-ViT [72] reduces token length through iterative aggregation,\nwhereas TNT [62] captures local information by dividing ViT\u2019s\npatches. Swin [54] and Cswin [73] introduce local attention within\na window and shifted window partitioning for cross-window\nconnections. GFnet [74] employs Fast Fourier Transform for token\nmixing. Architectures like ResMLP [75] and MLP-Mixer [76],\nsolely rely on multi-layer perceptrons (MLPs), excluding convo-\nlutions or self-attention mechanisms.\n2.3\nLLMs for Finance and Law\nIn addition to the LLMs towards general purpose, the domain-\nspecific models that are more capable of generating applied value\nare receiving increasing attention, with finance and law being the\nmost representative.\nFinancial LLMs. Wu et al.propose the first proprietary LLM\nwith 50 billion parameters specialized for the financial domain,\ni.e.BloombergGPT [77], which is a decoder-only causal language\nmodel based on BLOOM [78]. The proposed training strategy\nof mixing domain-specific and general-purpose data results in a\nbalanced performance in both domains. Unlike the proprietary\nBloombergGPT, FinGPT [79], [80] takes a data-centric approach\nand presents an open-source LLM to researchers and practi-\ntioners. It exhibits promise in financial tasks such as sentiment\nclassification, quantitative trading and financial fraud detection.\nPIXIU [81] has created a large-scale multi-task instruction dataset\nby manually reworking open-sourced datasets [82]. A financial\nLLM called FinMA is then introduced by fine-tuning LLaMA with\nthe constructed instruction dataset. The comprehensive evaluation\nresults including financial NLP and prediction tasks uncover the\nstrengths and weaknesses of various LLMs when handling differ-\nent financial tasks. To address the lack of open-sourced models\nspecifically designed for Chinese finance, Zhang et al.introduce\nXUANYUAN 2.0 [83], built upon the BLOOM [78] architecture.\nTo mitigate catastrophic forgetting, the hybrid-tuning strategy that\ncombines the stages of pre-training and fine-tuning is proposed.\nBy appropriately mixing the general and financial corpus in pre-\ntraining and fine-tuning, XUANYUAN 2.0 achieves impressive\nperformance in both the general domain and financial domain.\nChen et al.propose a financial LLM DISC-FinLLM [84] by\nmultiple experts fine-tuning the framework based on Baichuan-\n13B [85]. Experimental results on multiple evaluation benchmarks\ndemonstrate its promising performance.\nLegal LLMs. The legal sector is another area that is sig-\nnificantly benefitting from the advancement of LLMs. BaoLuo\nand Lychee [86], [87] are lawyer assistants developed by fine-\ntuning Chinese legal domain QA datasets. AI Lawyer [88] applies\nActive Learning to alleviate the problem of limited supervised data\nvolume in the legal domain. FedJudge [89] focuses on the privacy\nof legal data and adopts Federated Learning [90] during instruc-\ntion tuning, it also utilizes Continual Learning [91] to mitigate\nthe issue of data distribution shifts. HanFei, LaWGPT, Lawyer-\nllama, WisdomInterrogatory, and Fuzi.Mingcha [92], [10], [93],\n[94], [95] undergo a two-phase training process: further pre-\ntraining with unsupervised legal corpus to enhance the seman-\ntic understanding ability in the legal field and then supervised\ntraining with corresponding datasets. HanFei [92] is the first\nlegal LLM in China fully trained with 7B parameters and sup-\nports multi-turn dialogue, LaWGPT [10] expands the vocabu-\nlary by adding specific legal domain terms, Lawyer-llama [93]\nhas experimented with different data permutations and training\nsequences during its instruction tuning phase. During inference\ntime, LawGPT zh(XieZhi), LexiLaw, ChatLaw, Lawyer-llama,\nFuzi.Mingcha and DISC-LawLLM [96], [97], [98], [93], [95],\n[99] introduce a retrieval module to ensure that a definite legal\ndocument in the knowledge base supports the response. Addi-\ntionally, ChatLaw [98] also involves a Keyword model for key\ninformation extraction to reduce the ambiguity of user queries.\nFuzi.Mingcha [95] enables the LLM to use syllogistic reasoning\nto arrive at verdict predictions by training the LLM on a self-\nconstructed dataset.\n3\nPRELIMINARIES AND MOTIVATION\nIn this section, we commence by dissecting the foundational\narchitecture of the Transformer model, introducing a metric of\nnonlinearity to articulate its capabilities. Subsequently, we delve\ninto an analysis of the Transformer architecture\u2019s components\n\u2013 the multi-head self-attention and the multi-layer perceptrons\nmodules \u2013 scrutinizing their nonlinear expressive capability. This\nexploration also brings to light the limitations inherent in the\ncurrent incarnations of Transformer architectures.\nRecognized for the Transformer\u2019s suitability for parallel com-\nputing and the inherent complexity of its model, the Transformer\nhas demonstrated superior precision and performance compared\nto the widely adopted RNN recurrent neural network. The Trans-\nformer architecture consists of two parts: the multi-head self-\nattention and the multi-layer perceptrons modules.\nThe multi-head attention module is a fundamental component\nof the Transformer architecture. An MSA module with H heads\nis defined as\nMSA(Zl) = Concat([AlhZlW v\nlh]H\nh=1)W o\nl ,\nl \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L],\n(1)\nwhere Zl \u2208 RN\u00d7d is the feature of the l-th MSA layer, Alh \u2208\nRN\u00d7N and W v\nlh \u2208 Rd\u00d7(d/H) are the corresponding attention\nmap and value projection matrix in the h-th head, respectively.\nConcat(\u00b7) denotes the concatenating for features of the H heads\nand W o\nl \u2208 Rd\u00d7d is the output projection matrix. The attention\nmatrix Alh is calculated by the self-attention mechanism, i.e.,\nAlh = softmax\n \n(ZlW q\nlh)(ZlW k\nlh)\u22a4\n\u221a\nd\n!\n,\n(2)\nwhere W q\nlh \u2208 Rd\u00d7(d/H) and W k\nlh \u2208 Rd\u00d7(d/H) are the query and\nvalue projection matrices, respectively. Attention Alh reflects the\nrelation between different tokens, and a larger value Aij\nlh indicate\nthat token i and token j have a stronger relationship.\nAn MLP module is defined as\nMLP(Z\u2032\nl) = \u03c3(Z\u2032\nlW \u2032\nl1)W \u2032\nl2, l \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L],\n(3)\nwhere Z\u2032\nl \u2208 RN\u00d7d is the features of the l-th MLP layer ,\nW \u2032\nl1 and W \u2032\nl2 \u2208 Rd\u00d7d are the weight matrixs.\nOne of the paramount capabilities of neural networks is\ntheir nonlinear expressive capability. The higher the degree of\nnonlinearity, the more complex the function space the network\ncan approximate, resulting in enhanced accuracy. In traditional\n5\nConvolutional Neural Networks (CNNs), nonlinearity is primarily\nimparted by activation functions. However, in the Transformer\narchitectures, the sources of nonlinearity are twofold: the self-\nattention mechanisms and the activation functions within the\nMulti-Layer Perceptrons (MLP). Hence, our analysis separately\nscrutinizes the nonlinear expressive capabilities of both self-\nattention and the MLP within the Transformer framework.\nDefine Mm := {Y \u2208 RN\u00d7m|Y = 1x\u22a4, x\u22a4 \u2208 R1\u00d7m} as a\nsubspace in RN\u00d7m, where 1 = [1, 1, . . . , 1]\u22a4 \u2208 RN\u00d71, n is the\nnumber of tokens and d is the dimension of token representation.\nWe define the distance between matrix H \u2208 RN\u00d7m and Mm\nas dMm(H) := minY \u2208Mm \u2225H \u2212 Y \u2225F , where \u2225 \u00b7 \u2225F is the\nFrobenius norm. dMm(Zl) [100] is a commonly used metric\nto measure the capability and nonlinearity of the Transformer\narchitecture. Next, we investigate the distance between Zl the\noutput of layer l and subspace Md.\nWe begin by examining the nonlinear expressive capabilities\nof the self-attention modules. The self-attention matrix within the\nTransformer can be intuitively likened to the normalized adjacency\nmatrix of a corresponding graph. Viewed through a graph perspec-\ntive, the self-attention layer can be seen as equivalent to a Graph\nNeural Network (GNN) operating on a fully connected graph\nwith normalized edge weights. Excessive self-attention layers\nlike GNN layers result in excessive smoothing, with node vector\nrepresentations tending to be the same, resulting in convergence\nto a specific low-rank subspace for any given input.\nThe following theoretical analysis utilizes the formula of\nthe self-attention layer to shed light on the intricacies of this\nphenomenon. We study how the self-attention layer converges\nwith low rank based on matrix projection. Our analysis involves\nthe definition of a subspace M characterized by a unique prop-\nerty in which each row vector of its elements is the same. By\nscrutinizing the behavior of the self-attention layer in relation\nto matrix projection, we aim to unravel the underlying mecha-\nnisms that drive the convergence of node vector representations\ntoward a specific low-rank subspace. This exploration is essential\nfor gaining deeper insights into the nuances of self-attention in\nTransformers, shedding light on the intricacies of their functioning\nand providing a theoretical foundation for potential enhancements\nand optimizations.\nLemma 1. For self-attention matrix A \u2208 RN\u00d7N, any weight\nmatrix W \u2208 Rd\u00d7m, any H, B \u2208 RN\u00d7d, \u03b11, \u03b12 \u2265 0 and \u03c3 is\nthe nonlinear Lipschitz continuous activation function, we have:\ndMm(HW ) \u2264 sdMd(H),\ndMd(\u03c3(H)) \u2264 LdMd(H),\ndMd(\u03b11H + \u03b12B) \u2264 \u03b11dMd(H) + \u03b12dMd(B),\ndMd(AH) \u2264\np\n\u03bbmaxdMd(H),\nwhere s is the largest singular value of W , \u03bbmax is the largest\neigenvalue of A\u22a4(I \u2212 ee\u22a4)A and L is the Lipschitz constant of\nactivation function \u03c3(\u00b7).\nApplying the lemma 1 to the single-head self-attention layer,\nwe can obtain its low-rank attenuation.\nTheorem 1. Given a model stacked by the MSA modules, the\ndiversity dM(Zl) of feature in the l-th layer can be bounded by\nthat of input data Z0, i.e.,\ndMm(AZlW ) \u2264\n\u221a\n\u03bbs\u03c51dMd(Zl).\nwhere s > 0 is the largest element of all singular values of all W\nand \u03bb is the largest eigenvalue of all A\u22a4(I \u2212 ee\u22a4)A for each\nself-attention matrix A.\nFor the low-rank matrix projection of concat matrices, we have\nthe following lemma:\nLemma 2. For block matrix Hh \u2208 RN\u00d7m, we have:\ndMHm(Concat([Hh]H\nh=1))2 =\nH\nX\nh=1\ndMm(Hh)2,\nApplying the theorem 1 and the lemma 2 to the formula 1, we\ncan see how the multi-headed self-attention layer decays layer by\nlayer into the low-rank space.\nTheorem 2. Given a model stacked by the MSA modules, the\ndiversity dM(Zl) of feature in the l-th layer can be bounded by\nthat of input data Z0, i.e.,\ndMd(MSA(Zl)) \u2264\n\u221a\n\u03bbHs\u03c51dMd(Zl).\ndMd(Zl) \u2264 (\n\u221a\n\u03bbHs\u03c51)ldMd(Z0).\nwhere H is number of heads, s > 0 is the largest element of all\nsingular values of all W v\nlh and \u03c51 is the largest element of all\nsingular values of all W o\nl .\nAssume further that A is doubly stochastic (so that A\u22a4e = e)\nwith positive entries. Then by Perron\u2013Frobenius theorem, A\u22a4A\nhas a maximum eigenvalue 1 with associated eigenvector e as\nwell. In this case, the matrix A\u22a4(I \u2212 ee\u22a4)A = A\u22a4A \u2212 ee\u22a4\nhas a maximum eigenvalue \u03bbmax < 1.\n\u221a\n\u03bbHs\u03c51 is usually smaller than 1, so the feature diversity\ndMd(Zl) decreases rapidly as the network depth increases. Re-\ncursively, Zl will converge toward subspace Md if\n\u221a\n\u03bbHs\u03c51 < 1\nand all representations are the same, resulting in over-smoothing.\nIn conclusion, the nonlinear expressive capabilities of the vanilla\nself-attention module are limited.\nWe then focus on the Multi-Layer Perceptrons (MLP) of the\nTransformer architecture.\nTheorem 3. Given a model stacked by the MLP modules, the\ndiversity dMd(Z\u2032\nl) of feature in the l-th layer can be bounded by\nthat of input data Z\u2032\n0, i.e.,\ndMd(MLP(Z\u2032\nl)) \u2264 Ls\u03c52dMd(Z\u2032\nl).\ndMd(Z\u2032\nl) \u2264 (Ls\u03c52)ldMd(Z\u2032\n0).\nwhere s > 0 is the largest element of all singular values of all\nW \u2032\nl1, \u03c52 is the largest element of all singular values of all W \u2032\nl2\nand L is the Lipschitz constant of activation function \u03c3(\u00b7).\nThe analysis from the prior proofs reveals that the diversity of\nMLP modules is constituted by two elements: the eigenvalues of\nparameters and the Lipschitz constant of the activation functions.\nIn neural networks, parameters are typically normalized, which\nmeans the maximum eigenvalues of these parameters are bounded.\nFurthermore, the parameters in a neural network are learned\nthrough backpropagation. Given these factors, it becomes chal-\nlenging to impose restrictions on the eigenvalues of parameters.\nConsequently, the activation functions in the MLP emerge as the\nmost crucial aspect of their nonlinear expressive capabilities.\n6\nMulti-head\nSelf-Attention\n(MSA)\n\u2026\nAug-S \n\u2026\n\u2026\n\u2026\n\u2026\nShortcut\nInput tokens\nOutput tokens\nFig. 3: The diagram of MSA module equipped with augmented\nshortcuts, where different patterns (rectangle, triangle, etc.) denote\ndifferent features from various tokens. The original identity short-\ncut copies the input feature while the augmented shortcuts (Aug-S)\nproject features of each input token to diverse representations.\n4\nPANGU-\u03c0 MODULES AND ARCHITECTURES\nIn this section, we first propose the series informed activation\nfunction to enhance the nonlinearity of the MLP module. Then, we\nintroduce the augmented shortcuts to improve MSA modules in the\nTransformer architecture. Finally, we prove that the combination\nof the two techniques results in a novel and stronger Transformer\nmodel.\n4.1\nAugmented Shortcut\nAs discussed in Section 3, a pure attention suffer serve a feraure\ncollapse problem. The typical LLM architecture only equips each\nMSA module with a single shortcut connection, which is an\nidentity projection and directly copies the input features to the\noutputs. This simple formulation may not have enough represen-\ntation capacity to improve the feature diversity maximally. We aim\nto refine the existing shortcut connections in vision Transformers\nand explore efficient but powerful augmented shortcuts to produce\nvisual features with higher diversity.\nWe propose augmented shortcuts to alleviate the feature col-\nlapse problem by paralleling the original identity shortcut with\nmore parameterized projections. The MSA module equipped with\nT augmented shortcuts can be formulated as:\nAugMSA(Zl) = MSA(Zl) + Zl +\nT\nX\ni=1\nTli(Zl; \u0398li),\nl \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L],\n(4)\nwhere Tli(\u00b7) is the i-th augmented shortcut connection of the l-th\nlayer and \u0398li denotes its parameters. In addition to the original\nshortcut, the augmented shortcuts provide more alternative paths\nto bypass the attention mechanism. Different from the identity\nprojection that directly copies the input tokens to the corre-\nsponding outputs, the parameterized projection Tli(\u00b7) transforms\ninput features into another feature space. Projections Tli(\u00b7) will\napply different transformations to the input feature as long as\ntheir weight matrices \u0398li are different, and thus paralleling more\naugmented shortcuts has the potential to enrich the feature space.\nA simple formulation for Tli(\u00b7) is the sequence of a linear\nprojection and an activation function i.e.,\nTli(Zl; \u0398li) = \u03c3(Zl\u0398li),\nl \u2208 [1, \u00b7 \u00b7 \u00b7 , L], i \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , T],\n(5)\nwhere \u0398li \u2208 Rd\u00d7d is the weight matrix and \u03c3 is the nonlinear\nactivation function (e.g., GELU). In Eq. 5, Tli(\u00b7) tackles each\ntoken independently and preserves their specificity, which is\ncomplementary to the MSA modules aggregating different tokens.\nNote that the identity mapping is a special case of Eq. 5, i.e.,\n\u03c3(x) = x and \u0398li is the identity matrix.\nThis indicates that the upper bound of feature diversity\ndMd(Zl) decreases dramatically as the network depth increases\nwithout shortcut connections. We now analyze how the diversity\ndMd(Zl) changes w.r.t. the layer l in the model stacked by the\nAugMSA modules. We have the following theorem.\nTheorem 4. Given a model stacked by the AugMSA modules, the\ndiversity dMd(Zl) of feature in the l-th layer can be bounded by\nthat of input data Z0, i.e.,\ndMd(AugMSA(Zl)) \u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)dMd(Zl),\ndMd(Zl) \u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)ldMd(Z0),\nwhere H is number of heads, s > 0 is the largest element of all\nsingular values of all Wl, and \u2225 \u00b7 \u22252 is the \u21132 norm of the matrix.\nSince \u03b1i = (\n\u221a\n\u03bbHs\u03c51+1+PT\ni=1 L\u2225\u0398li\u22252) > 1, this allows\nus to prevent feature collapse.\nCompared with Theorem 2, the augmented shortcuts introduce\nan extra term (\n\u221a\n\u03bbHs\u03c51 +1+PT\ni=1 L\u2225\u0398li\u22252)l, which increases\nexponentially. This tends to suppress the diversity decay incurred\nby the attention mechanism. The term \u03b1i (0 \u2264 i \u2264 l) is\ndetermined by the norms of weight matrices \u0398li of the augmented\nshortcuts in the i-th layer, and the bound of diversity dMd(Zl) in\nthe l-th layer can be affected by all the augmented shortcuts in\nthe previous layers. For the ShortcutMSA module with only an\nidentity shortcut, we have \u03b1i =\n\u221a\n\u03bbHs\u03c51 + 1. Adding more\naugmented shortcuts can increase the magnitude of \u03b1i, which\nfurther improves the bound.\nAs discussed above, paralleling multiple augmented shortcuts\nwith the MSA and MLP modules in a vision Transformer can\nimprove the feature diversity to achieve higher performance. How-\never, directly implementing Tli(\u00b7) (Eq. 5) involves a lot of matrix\nmultiplications that are computationally expensive. For example,\ngiven feature Zl \u2208 Rn\u00d7d and weight matrix \u0398li \u2208 Rd\u00d7d, the\nmatrix multiplication Zl\u0398li consumes nd2 FLOPs, where d is\nusually large in vision Transformers (e.g., 4096 in LLaMA-7B).\nIn [104], the augmented shortcut is implemented with a block-\ncirculant matrix, which realizes fast inference with fast Fourier\ntransformation (FFT). Even though it achieves high theoretical\nacceleration, we empirically find that its practical speed depends\non the hardware optimization. Considering that LLM is a universal\nmodel, we propose to implement the augmented shortcut with a\nsimpler bottleneck module. The module is constructed by stacking\ntwo FC layers with a nonlinear activation function (e.g., GeLU).\nThe first FC layer reduces the d-dimension feature into a low-\ndimension space by a reduction ratio r and the second FC layer\nrestores the original feature dimension. Then the computational\ncost is reduced to 2nd2/r FLOPs . A larger r implies a further\ndecrease in the computational cost. For example, when the reduc-\ntion ratio r is set to 32, the computational cost can be reduced by\n16\u00d7 compared to the original augmented shortcut (Eq. 5).\nObviously, shortcuts use a large weight identity branch, such as\nAugMSA(Zl) = MSA(Zl) + \u03b1Zl(PT\ni=1 \u0398li) (where \u03b1 > 0),\nto prevent feature collapse, but this reduces network performance.\nWe theoretically analyze this because feature diversity dMd(Zl)\n7\nis adding excessive noise. The effect of noise on feature diver-\nsity is usually false positive. For example, if H = 0, then\ndMd(H) = 0. However, when the input matrix introduces a zero-\naverage noise \u03f5, then dMd(H + \u03f5) = \u2225\u03f5\u2225F > dMd(H) = 0.\nThis requires us to improve the diversity features and minimize\nthe impact of noise diversity on the network. This means reducing\nthe value of |dMd(AugMSA(Zl + \u03f5)) \u2212 dMd(AugMSA(Zl))|\nwhile ensuring that dMd(Zl) is not attenuated.\nThe following describes the definition of feature diversity\nof noise. We consider the effect of noise on matrix projection\ndMd(H).\n|dMd(H + \u03f5) \u2212 dMd(H)| \u2264 \u2225\u03f5 \u2212 1(xH+\u03f5\nmin\n\u2212 xH\nmin)\n\u22a4\u2225F\n\u2264dMd(\u03f5) = \u2225(I \u2212 ee\u22a4)\u03f5\u2225F = \u2225\u03f5 \u2212 1x\u03f5\nmin\n\u22a4\u2225F \u2264 \u2225\u03f5\u2225F .\n(6)\nFor zero average noise, the following equation holds ee\u22a4\u03f5 =\n0\n\u2208\nRN\u00d7d, so that the above inequality is equal. Since\n|dMd(f(H +\u03f5))\u2212dMd(f(H))| \u2264 dMd(f(H +\u03f5)\u2212f(H)).\nWe define dMd(f(H + \u03f5) \u2212 f(H)) to represent the diversity\neffect of noise \u03f5 on the f function whose input is H. The smaller\nthe value, the higher the robustness of the function f. For the\nsake of simplicity and considering typical scenarios, the following\ndiscussion assumes that the input noise \u03f5 is zero average noise.\nLemma 3. We consider the impact of noise on the MSA module,\nwhen H = 1. For a slight perturbation of the input \u03f5, the self-\nattention matrix also produces a perturbation A\u03f5 = A + \u03b4, i.e.,\ndMd(MSA(Zl + \u03f5) \u2212 MSA(Zl))\n\u2264\nq\n\u03bbA+\u03b4s\u03c51\u2225\u03f5\u2225F +\np\n\u03bb\u03b4s\u03c51dMd(Zl),\nwhere \u03bbA+\u03b4 is the largest eigenvalue of A\u03f5\n\u22a4(I \u2212 ee\u22a4)A\u03f5 and\n\u03bb\u03b4 is the largest eigenvalue of \u03b4\u22a4(I\u2212ee\u22a4)\u03b4, usually \u03bbA+\u03b4 < 1\nand \u03bb\u03b4 < 1.\nFor the H heads MSA module, we can get the following\nformula:\ndMd(MSA(Zl + \u03f5) \u2212 MSA(Zl))\n\u2264\nq\n\u03bbA+\u03b4Hs\u03c51\u2225\u03f5\u2225F +\np\n\u03bb\u03b4Hs\u03c51dMd(Zl),\nLemma 4. We consider the noise diversity of linear parallel\nbranch:\ndMd(L(Zl + \u03f5)\u0398li \u2212 LZl\u0398li) \u2264 L\u2225\u0398li\u22252\u2225\u03f5\u2225F ,\nTheorem 5. If and only if ee\u22a4(\u03c3(Zl + \u03f5) \u2212 \u03c3(Zl)) = 0, the\nfollowing inequality is equal:\ndMd(Tli(Zl + \u03f5; \u0398li) \u2212 Tli(Zl; \u0398li))\n\u2264 L\u2225\u0398li\u22252\u2225\u03f5\u2225F .\nFor the nonlinear activation function, \u03c3(Zl + \u03f5) \u2212 \u03c3(Zl) is\nno longer guaranteed to be zero-average. Therefore, the noise\ndiversity of the nonlinear branch is weaker than that of the linear\nbranch:\ndMd(Tli(Zl + \u03f5; \u0398li) \u2212 Tli(Zl; \u0398li)) < L\u2225\u0398li\u22252\u2225\u03f5\u2225F .\nTheorem 6. Given a model stacked by the AugMSA modules, the\nnoise diversity of feature in the l-th layer can be bounded by the\nfollowing formula, i.e.,\ndMd(AugMSA(Zl + \u03f5) \u2212 AugMSA(Zl))\n< (1 +\nq\n\u03bbA+\u03b4Hs\u03c51 + L\nT\nX\ni=1\n\u2225\u0398li\u22252)\u2225\u03f5\u2225F +\np\n\u03bb\u03b4Hs\u03c51dMd(Zl)\nThis indicates that using a large number of nonlinear shortcuts\ninstead of LZl(PT\ni=1 \u0398li) prevents feature collapse, reduces\nthe impact of input noise on feature diversity, and enhances\nthe robustness of the network. In addition, it also enhances the\nnonlinear expression ability.\n4.2\nSeries Informed Activation Function\nA neural network Nd composed of d hidden layers can be regarded\nas a composite of d functions fi: Nd = f1 \u25e6 f2 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fd.\nIn particular, each hidden layer function fi can be written as\nthe composite of a function gi and an activation function \u03c3i:\nfi = \u03c3i \u25e6 gi. Actually, the learning procedure of fi amounts\nto an optimization problem over the layer hypothesis space Hi.\nUsually, \u03d5i is taken as a non-learnable function; therefore, in\nthe most common scenario, Hi = \u03c3i \u00d7 Hgi. gi is parameterized\nand learnable, and belongs to a hypothesis space Hgi. This clearly\nlimits the learnable space.\nIn this section, we introduce a technique to define learnable\nactivation functions that could be plugged into all the hidden\nlayers of MLP. We define the hypothesis space H\u03d5i, based on\nthe following idea: (i) select a finite set of activation functions\n\u03a3 := {\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3N}, whose elements will be used as base\nelements; (ii) define the learnable activation function \u03d5i as a\nlinear combination of the elements of \u03a3; (iii) identify a suitable\nhypothesis space H\u03d5i; (iv) optimize the whole network, where\nthe hypothesis space of each hidden layer is Hi = H\u03d5i \u00d7 Hgi.\nIn this way, we expand the learnable space of each hidden layer,\nenhancing the model\u2019s nonlinear expression capability.\nSeveral different activation functions have been proposed\nfor deep neural networks, including the most popular Rectified\nLinear Unit (ReLU) and its variants (PReLU [101], GeLU [102]\nand Swish [103]). They focus on enhancing the performance of\ndeep and complex networks using different activation functions.\nHowever, as theoretically proven in the preceding section, the\nlimited power of Transformer architecture is mainly due to poor\nnonlinearity, which has not been fully investigated by the existing\nactivation functions.\nThere are two ways to improve the nonlinearity of a neural\nnetwork: stacking the nonlinear activation layers or increasing\nthe nonlinearity of each activation layer. The trend of existing\nnetworks is to choose the former, which results in high latency\nwhen there is excessive parallel computation ability.\nOne straightforward idea to improve the nonlinearity of the\nactivation layer is stacking. The serial stacking of the activation\nfunction is the key idea of deep networks. However, stacking\nlayers serially results in a large computation cost, which is not af-\nfordable for developing an efficient and effective LLM. Therefore,\nwe choose concurrently stacking the activation function. Denote\nthere are n activation functions for input x in a neural network\nas {\u03c3i(x)}n\ni=1, which can be the usual functions such ReLU and\nTanh. The concurrent stacking of the activation functions can be\nformulated as:\nn\nX\ni=1\n\u03c3i(aix + bi),\n(7)\n8\nwhere n denotes the number of stacked activation functions and\nai, bi are the scale and bias (which are learned parameters) of\neach activation to prevent simple accumulation. The nonlinearity\nof the activation function can be largely enhanced by concurrent\nstacking. Equation 7 can be regarded as a series in mathematics,\nwhich is the operation of adding many quantities.\nSince the nonlinearity of the Transformer is mainly derived\nfrom the feed-forward network (FFN), we apply the series in-\nformed activation function on the FFN block. Given an input\nfeature x \u2208 RN\u00d7D, where N and D are the number of tokens\nand its hidden dimension, the original FFN can be formulated as\nMLP(Z\u2032\nl) = \u03c3(Z\u2032\nlW \u2032\nl1)W \u2032\nl2i, l \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L],\n(8)\nwhere W \u2032\nl1 and W \u2032\nl2 are two fully connected layers. Specifically,\nto further enrich the approximation ability of the series, we\nenable the series-based function to learn the global information by\nvarying the inputs from their neighbors, which can be reformulated\nas:\nSIAF\u2212MLP(Z\u2032\nl) = (\nn\nX\ni=1\n\u03c3i(Z\u2032\nlW \u2032\nl1i))W \u2032\nl2i, l \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L].\n(9)\nIt is easy to see that when n = 1, the series-based activation\nfunction \u03c3s(x) degenerates to the plain activation function \u03c3(x),\nwhich means that the proposed method can be regarded as a\ngeneral extension of existing activation functions.\nTheorem 7. Given a model stacked by the SIAF\u2212MLP modules,\nthe diversity dM(Z\u2032\nl) of feature in the l-th layer can be bounded\nby that of input data Z\u2032\n0, i.e.\ndMd(SIAF \u2212 MLP(Z\u2032\nl)) \u2264 (\nn\nX\ni=1\nLi)s\u03c52dMd(Z\u2032\nl),\ndMd(Z\u2032\nl) \u2264 (s\u03c52\nn\nX\ni=1\nLi)ldMd(Z\u2032\n0),\nwhere Li is the Lipschitz constant of the activation function \u03c3i.\nAs demonstrated in our proof, the Series Informed Activation\nFunction (SIAF) we propose markedly amplifies the nonlinear\nexpressive capacity of the MLP module compared to the origi-\nnal architecture. This enhancement in nonlinearity progressively\nintensifies with the increase of the parameter n.\n4.3\nCombination\nFinally, we offer the upper bounds for the combination of multi-\nlayer AugMSA module and SIAF-MLP module to decay into sub-\nspace Md. We can obtain the upper bounds for the combination of\nmulti-layer MSA module and MLP module to decay into subspace\nMd in vanilla Transformer architecture.\nTheorem 8. Provide a network consisting of p-layer MSA module\nand q-layer MLP module, the diversity dMd(Zp+q) of feature in\nthe l-th layer can be bounded by that of input data Z0, i.e.,\ndMd(Zp+q) \u2264 (\n\u221a\n\u03bbHs\u03c51)p(Ls\u03c52)qdMd(Z0).\n(10)\nIt is evident that the original Transformer architecture pos-\nsesses a relatively limited upper bound in terms of nonlinear\nexpressive capability. Building upon this observation, we now pro-\nceed to analyze the enhanced expressive power of the Transformer\nwhen augmented with our proposed architectural modifications.\nTheorem 9. Provide a network consisting of p-layer AugMSA\nmodule and q-layer SIAF-MLP module, the diversity dMd(Zp+q)\nof feature in the l-th layer can be bounded by that of input data\nZ0, i.e.,\ndMd(Zl)\n\u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)p(s\u03c52\nn\nX\ni=1\nLi)qdMd(Z\u2032\n0).\nThe preceding theorem substantiates that our proposed aug-\nmented shortcut module, when used in conjunction with the\nseries informed activation function module, enhances the model\u2019s\nnonlinear expressive capabilities and diversity far beyond what is\nachievable by using either module independently. Consequently,\nour Transformer architecture amalgamates these two modules to\nform our PanGu-\u03c0 architecture, resulting in a synergistic improve-\nment in nonlinearity and diversity.\n5\nEXPERIMENTS ON GENERAL FIELD\nIn this section, we compare the existing open-source 7B and 1B\nmodels. Furthermore, we conduct ablation studies of the proposed\narchitecture.\nTraining data The pre-training data is gathered from diverse\nsources from the Internet, covering English and Chinese corpus in\nan equal 1 : 1 ratio. The tokenizer is built by byte-pair encoding\n(BPE) [105] from SentencePiece [106] upon our data. The final\nvocabulary size is about 0.1 million. After tokenization, the entire\ntraining dataset contains about 1.6 trillion tokens.\nTraining details Our models are trained using the AdamW\noptimizer [107] with \u03b21 = 0.9, \u03b22 = 0.95 for 1 epoch utilizing\nthe cosine learning rate decay [108] with an initial learning\nrate 3 \u00d7 10\u22124. The total batch size for the training process is\napproximately 4M, and it includes a warm-up phase spanning\n4000 steps.\nModel details For fair comparison, we adopt the pre-\nnormalization [109], SwiGLU activation [52] and rotary embed-\ndings [110] following the LLaMA architecture [23]. We then apply\nour series activation function and augmented shortcuts to build our\nmodels. The details of the models can be found in Table 1. We\nreduce the number of layers to make the number of parameters\nsimilar to the LLaMA model for fair comparison because the\nproposed modules introduce extra parameters.\nTraining Devices We use the Huawei Ascend 910A card\nto train and evaluate the proposed architecture. The HUAWEI\nAscend 910A is a high-efficiency, flexible, and programmable\nartificial intelligence processor. For half-precision floating-point\n(FP16) operations, the Ascend 910 delivers 256 TeraFLOPS. For\ninteger precision calculations (INT8), it delivers 512 TeraOPS. De-\nspite its unparalleled performance, Ascend 910\u2019s maximum power\nconsumption is only 310W, which is significantly lower than\nits planned specifications of 350W. Developed using Huawei\u2019s\nproprietary Da Vinci architecture, it integrates a rich array of\ncomputing units, enhancing the completeness and efficiency of AI\ncomputations, thereby extending its applicability. It significantly\nimproves the performance of the entire AI system and effectively\nreduces deployment costs.\nBenchmarks We use the OpenCompass platform [111] to\nevaluate on an extensive suite of downstream tasks. We selected\n11 classical benchmarks from four different domains to conduct\n9\nTABLE 1: Model details of PanGu-\u03c0.\nModels\ndimension\nn heads\nn layers\nPanGu-\u03c0-1B\n2048\n16\n12\nPanGu-\u03c0-7B\n4096\n32\n29\na comprehensive comparison. C-Eval [112] is a comprehensive\nChinese evaluation benchmark to evaluate the knowledge and\nreasoning abilities of LLMs, which includes multiple-choice\nquestions from 52 diverse disciplines across different difficulty\nlevels. CMMLU [113] is also a comprehensive Chinese evaluation\nbenchmark, which covers 67 topics including science, engineering,\nand humanities. MMLU [114] proposes an English evaluation\nbenchmark for measuring LLM\u2019s multitask accuracy by covering\n57 tasks including mathematics, history, computer science, and\nlaw. AGI-Eval [115] is a human-centric benchmark specifically\ndesigned to evaluate the general abilities of foundation mod-\nels in tasks pertinent to human cognition and problem-solving.\nBoolQ [116] is a reading comprehension dataset to evaluate the\ndifficult entailment-like inference ability of LLMs. AX-b [117] is\na broad-coverage diagnostic task and PIQA [118] is a physical\ninteraction question-answering task. CSL [119] offers a Chinese\nScientific Literature dataset to evaluate the performance of mod-\nels across scientific domain tasks. EPRSTM [120] is a binary\nsentiment analysis dataset based on product reviews on an e-\ncommerce platform. [121] is a single-document summarization\ntask, and LCSTS [122] is a large corpus of Chinese short-text\nsummarization datasets.\n5.1\nAblation Studies\nTo better understand the proposed Architecture, we conduct ex-\ntensive experiments to investigate the impact of each component.\nAll the ablation experiments are conducted based on the 1B model\nsize.\nTABLE 2: Ablation Study about series informed activation func-\ntion.\nNumber of n\nC-Eval\nInference Speed (ms)\n1\n35.0\n7.56\n2\n36.9\n7.76\n3\n37.0\n8.02\n4\n37.0\n8.35\nInfluence of series informed activation function. In the\nabove section, we propose the SIAF to enhance the performance\nand enable global information exchange in feature maps. Table 2\nshows the performance of the proposed SIAF using different\nnumbers of n. When n = 1, the activation function degenerates\ninto the plain activation function. We find that when n = 2, the\nperformance and the inference speed strike an optimal balance.\nTherefore, we choose n = 2 for the following experiments.\nInfluence of augmented shortcuts. As mentioned above,\nthe augment module can greatly improve the performance. As\nshown in Table 3, we trade off accuracy and speed by controlling\nthe width of the bottleneck middle layer. By transforming the\nreduction rate, it is apparent that an increase in reduction rate\nresults in a decrease in calculation speed and accuracy. After\ncareful consideration, we determined a reduction rate of 32 to\nachieve the optimal balance between speed and accuracy.\nTABLE 3: Influence of the width factor in augmented shortcuts.\nWidth factor of bottlenecks\nC-Eval\nInference Speed (ms)\n1\n36.5\n8.66\n1/4\n36.7\n8.06\n1/16\n36.5\n7.96\n1/32\n36.3\n7.66\n1/64\n35.5\n7.62\n0\n35.0\n7.56\nTABLE 4: Ablation Study of each component.\nMethod\nC-Eval\nInference Speed (ms)\nVanilla Transformer\n35.0\n7.56\nWideNet [123]\n36.5\n8.06\nSIAF\n36.9\n7.76\nAS\n36.3\n7.66\nSIAF+AS\n37.9\n7.86\nArchitecture We finally ablate the effectiveness of each com-\nponent of the proposed method and report the language modeling\nresults in Table 4. We ablate the series informed activation function\n(SIAF) and augmented shortcuts (AS) as described earlier. Fur-\nthermore, we compared the proposed method with WideNet [123],\nwhich is introduced to increase the nonlinearity of Transformer\narchitectures. It can be seen through experiments that each com-\nponent of the proposed method is effective for improving the\nperformance of the Transformer architecture, which surpasses that\nof WideNet [123].\n5.2\nFeature Analysis and Visualization\nWe also analyze the latent representation across different layers\nto demonstrate further the superiority of nonlinearity compensa-\ntion introduced by the PanGu-\u03c0 architecture. We are interested\nin the channel diversity of each architectural choice. Following\nthe analysis method from [124], we characterize the effective\ndimensions of different decoder architectures. In particular, the\neffective dimension d(\u03f5) is defined as the minimum principal\ncomponent numbers that occupy the explained variance ratio of\n\u03f5 in a principal component analysis (PCA). In principle, a more\npowerful representation of individual tokens would result in a\nlarger effective dimension. In comparison, a smaller effective\ndimension means the variance in token representation occurs\nmainly in smaller dimensions. As shown in Fig. 4, here we report\neach layer\u2019s effective dimension d(0.8). Removing all augmented\nshortcuts limits the effective dimension to the greatest extent,\nand removing the series informed activation function significantly\nreduced the effective dimension consistently on each Transformer\nlayer, indicating the significant role of these components in\nchannel-wise feature diversity [125].\nFurthermore, to offer a finer-grained characterization of lin-\nguistic features from different architectures, we also visualize the\nrepresentation of tokens with different semantics, using the test\nset of Penn Tree Bank (PTB) [126] as a general domain corpus. In\nparticular, we adopt a layer-wise visualization method to illustrate\nthe concentration and diversity of features for each token and\nhow these characteristics change along the Transformer layers\nFig. 5. To assist the visualization, the top five frequent tokens\nare highlighted with different colors. PCA is used to reduce all\nfeature maps to a 3D space, preventing nonlinear reduction as it\nmay cause disruption. Additionally, the total variance accounted\n10\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n1\n6\n11\n16\n21\n26\nPangu-\u03c0-7B\nw/o SIAF\nw/o AS\nFig. 4: The effective dimension d(0.8) across layers of different\nmodel architectures. A larger number of effective dimensions\nmeans more principal components are needed to account for 80%\nof variance, indicating more diversity in feature channels.\nfor by the first three principal components is labeled for each layer.\nFrom Fig. 5, we can see that PanGu-\u03c0 architecture possesses the\nmost diverse and isotropic feature space [127], with each token\u2019s\nfeature expanding to a high-dimensional cluster when moving\nto deeper layers. In comparison, removing the series informed\nactivation function or the augmented shortcuts limit the feature on\nlow-dimensional manifolds (i.e., aggregating along one axis in the\nmiddle panel), and the differentiation among tokens is also blurred,\nindicating less discriminative power in language modeling.\nLayer 16\nLayer 21\nLayer 26\nPanGu-\u03c0-7B\nw/o SIAF\nw/o AS\nFig. 5: Visualization of hidden states from each layer. The most\nfrequent five tokens are highlighted using different colors for\nvisualization. The total variance accounted for by the first three\nprincipal components is labeled for each layer on the top. Note\nthat the beginning tokens are removed from the analysis because\nthey are considered outliers.\nTo verify the effectiveness of the language representation\nenhanced by PanGu-\u03c0 architecture, we conduct case analyses\nwhere the saliency of each token\u2019s feature dimension is calculated\nby deriving the absoluate value of the corresponding gradients\nwith respect to the prediction target. As shown in Figure 6,\nthe language model is required to echo the previous mentioned\nname \u201cchester\u201d as the next word. The PanGu-\u03c0 model correctly\nidentifies the key message \u201cchester\u201d in the context reflected by\nhigher gradient values for most of the channels (Figure 6 (a)). In\ncomparison, without the augmented shortcuts and series activation\nwhat\nhappened\nto\nche\nster\n?\n''\n``\nshot\nin\nthe\nhead\n'\ns\nwhat\nhappened\nto\n0\n125\n250\n375\n500\n625\n750\n875\n1000\n1125\n1250\n1375\n1500\n1625\n1750\n1875\n2000\n2125\n2250\n2375\n2500\n2625\n2750\n2875\n3000\n3125\n3250\n3375\n3500\n3625\n3750\n3875\n4000\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nwhat\nhappened\nto\nche\nster\n?\n''\n``\nshot\nin\nthe\nhead\n'\ns\nwhat\nhappened\nto\n0\n125\n250\n375\n500\n625\n750\n875\n1000\n1125\n1250\n1375\n1500\n1625\n1750\n1875\n2000\n2125\n2250\n2375\n2500\n2625\n2750\n2875\n3000\n3125\n3250\n3375\n3500\n3625\n3750\n3875\n4000\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) PanGu-\u03c0\n(b) Vanilla Transformer\nFig. 6: Saliency analysis for next word prediction task. Color\nbrightness indicates the absoluate value of the corresponding\ngradients with respect to the prediction target. The gradient values\nare normalzied within each sample to ensure a fair comparison.\nImportant messages are framed in red.\nfunction, the model tend to leverage more information from the\nunmeaningful symbols after the key hints, leading to a wrong\nprediction that directly ends the sentence (Figure 6 (b)).\n5.3\nComparison with 7B Models\nTo showcase the competitive edge of PanGu-\u03c0, we conducted an\nextensive analysis of the PanGu-\u03c0 model, comparing it against\nother state-of-the-art models with a similar size. The comprehen-\nsive results are detailed in Table 5. We segmented the comparative\ndatasets into four tasks: examination, knowledge, reasoning, and\nunderstanding, to fully assess the capabilities of our model.\nNotably, in the examination category, our model almost reached\nthe state-of-the-art (SOTA) benchmarks, surpassing LLaMA2,\nBaichuan2, and InternLM, and closely matching the current top\nperformer, Qwen. The trend of superior performance in reasoning\nand understanding categories can also be observed, which is\nattributed to the PanGu-\u03c0 model\u2019s advanced nonlinear capability.\nThis allows it to fit more complex function spaces, giving it a\ncompetitive advantage in these challenging SOTA benchmarks.\nIn the knowledge dataset, our model still lags behind in\nperformance when compared to the existing SOTA benchmarks.\nThis could be due to the lower concentration of knowledge in\nour collected dataset and the presence of many unseen data points\nin the BoolQ dataset. Nevertheless, our model achieved excellent\nresults, demonstrating strong generalization abilities.\nOverall, our model exhibits consistently better average per-\nformance indices compared to the current state-of-the-art models.\nWhen measured against other open-sourced models of a 7B size,\nPanGu-\u03c0 achieves significantly higher average performance. In\nthe future, we plan to train our model with better data, aiming to\nenhance its performance metrics in the knowledge domain.\nMoreover, we evaluated the latency (milliseconds per token)\nof these models. As the compared models utilize a similar archi-\ntecture to LLaMA, their latencies are comparable. Our findings\nindicate that PanGu-\u03c0 achieves a much faster inference speed\ncompared to the LLaMA architecture, further establishing its\nsuperiority.\nThe PanGu-\u03c0 model can serve as a swift and effective foun-\ndational model, leveraging strategies such as Supervised Fine-\n11\nTABLE 5: Comparison with SOTA open-source 7B models. The best model is listed in bold.\nModels\nTask\nDataset\nLLaMA2-7B\nBaichuan2-7B\nInternLM-7B\nQwen-7B\nPanGu-\u03c0-7B\nExamination\nC-Eval\n32.50\n56.90\n53.21\n63.40\n59.86\nCMMLU\n31.80\n57.02\n51.86\n62.50\n59.92\nMMLU\n46.80\n54.72\n51.39\n59.70\n61.91\nAGI-Eval\n21.80\n34.77\n37.77\n45.30\n54.16\nKnowledge\nBoolQ\n74.90\n63.21\n64.10\n76.10\n64.77\nReasoning\nAX-b\n53.50\n51.72\n42.57\n57.00\n57.52\nPIQA\n78.30\n76.22\n78.02\n77.90\n77.15\nUnderstanding\nCSL\n55.60\n66.25\n65.62\n56.20\n63.75\nEPRSTMT\n46.20\n69.38\n88.12\n88.80\n90.62\nXSum\n19.70\n20.89\n8.12\n1.30\n19.58\nLCSTS\n9.10\n15.57\n18.19\n12.00\n16.62\nAverage\n42.75\n51.19\n50.82\n54.56\n56.90\nLatency on 910A (ms)\n47.60\n43.00\nTABLE 6: Comparison with SOTA open-source 1B models. The best model is listed in bold.\nModels\nTask\nDataset\nSheared-LLaMA-1.3B\nChinese-LLaMA2-1.3B\nTinyLLaMA-1.1B\nPanGu-\u03c0-1B\nExamination\nC-Eval\n24.28\n28.70\n27.85\n36.85\nCMMLU\n25.10\n24.78\n24.64\n35.90\nMMLU\n25.77\n24.55\n25.75\n35.96\nAGI-Eval\n18.01\n19.40\n18.54\n30.77\nKnowledge\nBoolQ\n62.39\n56.79\n56.06\n58.44\nReasoning\nAX-b\n43.57\n47.46\n45.47\n43.48\nPIQA\n72.91\n56.91\n70.62\n61.92\nUnderstanding\nCSL\n51.88\n55.60\n53.12\n55.62\nEPRSTMT\n46.25\n72.50\n46.25\n55.62\nXSum\n16.44\n8.90\n20.15\n15.92\nLCSTS\n15.37\n13.16\n13.97\n14.61\nAverage\n36.54\n37.16\n36.58\n40.46\nTuning (SFT), AI Agent, or retrieval augmentation to become a\ncompetitive AI solution. It has the potential to harness the power\nof LLMs in edge devices like smartphones, offering a compelling\nchoice for various applications.\n5.4\nComparison with 1B Models\nFor comparison, we have meticulously selected three SOTA mod-\nels of similar size, all based on the LLaMA architecture. These\ninclude Chinese-LLaMA2-1.3B [128], TinyLlama-1.1B [129], and\nSheared-LLaMA-1.3B [130]. Notably, Sheared-LLaMA-1.3B was\ninitially pruned from a larger LLaMA2-7B model and subse-\nquently trained with a condensed dataset of 50B tokens. Our\nextensive experiments, as presented in Table 6, demonstrate that\nPanGu-\u03c0-1B significantly surpasses existing LLMs of similar size,\nand in some cases, even larger sizes.\nSimilar to our findings with the 7B model, our 1B model\ndemonstrates a remarkable superiority in the examination category\nover existing models, affirming the efficacy of our modifications\neven at the 1B scale. This outcome is a testament to our model\u2019s\nenhanced nonlinear capabilities, enabling it to fit more complex\nfunction spaces. In the realms of reasoning and understanding,\nwe observe similar trends, further validating our model\u2019s robust\nperformance.\nHowever, as with our larger model, the knowledge density in\nour collected dataset for training the 1B model was relatively low.\nConsequently, in datasets like BoolQ, which contain data points\npreviously unseen by our model, we acknowledge a noticeable\ngap in performance when compared to current state-of-the-art\nbenchmarks. This indicates room for improvement in our model\u2019s\nability to handle unfamiliar data, a challenge we aim to address in\nfuture iterations.\nFurthermore, when assessing latency, a critical factor in real-\nworld applications, PanGu-\u03c0-1B shows a marked advantage. It\nrecords lower latency times of 13.8 ms on the 910A, compared\nto its deeper counterpart, LLaMA2-1B, which clocks 15.4 ms\non the 910A, despite having a roughly equivalent number of\nparameters. This not only underscores the efficiency of PanGu-\n\u03c0-1B but also highlights its suitability for deployment in time-\nsensitive applications.\n6\nYUNSHAN: DOMAIN SPECIALIZED MODEL FOR\nFINANCE AND LEGAL FIELDS\nIn the current landscape of LLMs, there is a growing trend to-\nwards using domain-specific models for specialized applications,\nwhich differ from the capabilities of general-purpose large models.\nThese domain-specific models are developed with a deeper level\nof expertise, and are particularly adept at addressing tasks in\nspecific fields. For instance, in the finance industry, LLMs are\nused to provide in-depth analysis of financial documents and offer\ninteractive information services for market insights and queries.\n12\nTABLE 7: The vocab size and text compression rate of YunShan\u2019s\ntokenizer compared with other tokenizers of LLaMA2, InternLM,\nBaichuan2 and PanGu-\u03c0.\nTokenizer\nVocab Size\nCompression Rate\nLLaMA2\n32,000\n1.479\nInternLM\n103,168\n0.657\nBaichuan2\n125,696\n0.605\nPanGu\n100,883\n0.649\nYunShan\n110,428\n0.606\nSimilarly, in the legal field, LLMs are used to facilitate expedited\ncase research and information retrieval. Exemplary models such\nas Xuanyuan [83], FinGPT [79] and FinMA [81] in financial\nservices, and Lawyer LLaMA [93] and LawGPT [96] for legal\napplication. This progression not only exemplifies the robust\ndevelopment within the sphere of domain-specific LLMs but also\nsignals the future role these technologies are poised to play in\nfurthering innovation and growth across various domains.\n6.1\nDatasets\nTo enhance the model\u2019s ability to solve domain-specific tasks, the\nmodel is continually pretrained and instruction finetuned on the\ndomain datasets. We collect a variety of domain datasets both in\nthe financial domain and the legal domain. When we construct the\ndatasets, we consider the professionalism, diversity, data size and\naccessibility of the sources. We process and clean the collected\ndatasets to construct high-quality domain corpora.\nFinancial Domain Our financial pretraining data consists of com-\npany announcements, financial news, articles, and examinations.\nSome of the data is from FinCorpus1, which is a high-quality\nChinese financial dataset. The rest of the data is crawled through\nTuShare2, which is a Python package that provides free access to\nChinese financial data from various sources. After cleaning, the\ntotal size of the data is 36.5B tokens.\nLegal Domain Our legal pretraining data comprises legal regula-\ntions, legal case texts, academic papers, and legal examinations.\nWe use Pile of Law [131] and LeXFiles [132] to construct some of\nthe data, and selectively collect data from two legal-related public\nplatforms34. After cleaning, the legal pretraining data contains\n111.7B tokens.\nInstruction-following Data For supervised fine-tuning, we collect\n995k domain instruction-following data. The data consists of JEC-\nQA [133] and the instructions open sourced by ChatLaw [98],\nLawyer LLaMA [93], LawGPT [96], and FinCorpus1, covering\nJudicial Examination examples, legal consultations, and financial\nexamination examples.\n6.2\nTokenizer\nCorpora from financial and legal fields often contain specialized\nwords that are not included in the general domain vocabulary.\nDuring tokenization, these unknown words are deconstructed into\nsub-words or even UTF-8 bytes, which reduces model training\nefficiency. To alleviate the inefficiency, we expanded the original\nvocabulary size from 100883 to 110428 by adding a specialized\n1. https://huggingface.co/datasets/Duxiaoman-DI/FinCorpus\n2. https://tushare.pro/\n3. https://pkulaw.com/\n4. https://wenshu.court.gov.cn/\nfinancial-legal word list. We used byte-pair encoding (BPE) [134]\nfrom Tokenizer5 to train our tokenizer. The specialized vocabulary\nis trained from a subset of the financial and legal corpora and\nthen merged into the original vocabulary. Table 7 shows the\ncompression rate we calculate on the JEC-QA dataset [135], a\nlegal domain QA dataset. Our new tokenizer has a much lower\ncompression ratio than the original one and ties for the best with\nBaichuan 2 while having a smaller vocabulary.\n6.3\nTraining Process\nFurther Pretraining To transfer financial and legal knowledge\nto the PanGu-\u03c0 model, we further pre-train it with a specialized\nvertical corpus. To alleviate catastrophic forgetting of formerly\nlearned knowledge, we partially introduce the during the pre-\ntraining process. Specifically, we resample a portion of the pre-\nvious high-quality data and mix it with the vertical domain data at\na 1 : 1 ratio.\nInstruction Tuning During the instruction tuning phase, we\nutilize supervised fine-tuning samples of both general-purpose\ntasks and domain-specific tasks. We shuffle and combine them into\none supervised dataset and execute the instruction tuning process\nin one stage. On the basis of the YunShan-base model, which\nhas already acquired general and specialized knowledge during\nprevious phase, this approach aims to simultaneously teach the\nmodel how to follow human instructions in different domains.\n6.4\nBenchmarks\nThe proposed YunShan model is evaluated across the two spe-\ncialized law and finance domains. This approach aims to com-\nprehensively assess YunShan model\u2019s capabilities in knowledge\nretention, comprehension, and application within domain-specific\ntasks, covering Chinese and English languages.\n6.4.1\nFinancial Domain\nFor the financial domain, we use two distinct datasets: Finan-\nceIQ [136] and FinEval [137], each serving a unique purpose in\nour evaluation framework.\nFinanceIQ is a Chinese evaluation dataset focused on the finan-\ncial domain, designed to assess the knowledge and reasoning\ncapabilities of LLMs in financial scenarios. It spans 10 broad\nfinancial categories and 36 subcategories, offering a total of 7,173\nmultiple-choice questions. This dataset is tailored to evaluate\na wide range of financial knowledge, from basic concepts to\ncomplex problem-solving in areas such as accounting, banking,\nand financial analysis.\nFinEval is an expansive benchmark tailored for measuring the\nadeptness of LLMs in the finance domain. Encompassing 4,661\nmultiple-choice questions, it spans 34 academic subjects across\nthe finance, economics, accounting, and professional certification\ndomains. This dataset is structured to facilitate a layered evaluation\nthrough diverse assessment methods, including zero-shot, few-\nshot, answer-only, and chain-of-thought prompts.\n6.4.2\nLegal Domain\nLawBench [138]: For the legal domain, we implement the test\nover LawBench dataset. LawBench is a sophisticated benchmark\ndesigned to evaluate the legal knowledge capabilities of LLMs\nwithin the Chinese civil-law system. This benchmark tests LLMs\n5. https://github.com/huggingface/tokenizers\n13\nTABLE 8: The experimental results of general-domain and domain specialized LLMs on financial FinancelQ [83] benchmark.\nModel\nCPA\nBQ\nSPQ\nCFPQ\nIPQ\nEconomist\nTax Accountant\nFQ\nFinancial Planner\nActuary\nAVG\nLLaMA2-7B\n28.38\n26.53\n26.11\n24.54\n26.44\n25.77\n28.41\n26.10\n25.00\n25.41\n26.27\nInternLM-7B\n31.81\n43.57\n34.69\n40.02\n36.49\n42.50\n43.19\n37.29\n32.95\n31.56\n37.41\nBaichuan2-7B\n34.25\n49.04\n40.14\n42.32\n41.52\n52.69\n41.11\n41.02\n30.68\n35.86\n40.86\nQwen-7B\n30.36\n37.38\n35.97\n37.61\n33.19\n43.65\n38.11\n38.31\n27.27\n27.66\n34.95\nFinGPT-7B\n25.32\n25.88\n24.40\n24.89\n25.57\n25.58\n27.48\n20.68\n22.73\n29.51\n25.20\nFinMA-7B\n24.94\n29.66\n33.25\n29.93\n35.34\n32.31\n26.56\n26.78\n22.73\n25.00\n28.65\nLawGPT-7B\n26.47\n24.92\n23.98\n24.77\n25.14\n24.62\n27.02\n27.46\n20.45\n26.43\n25.13\nHanFei-7B\n27.00\n34.08\n33.93\n34.52\n37.36\n37.88\n32.79\n30.51\n22.73\n30.33\n32.11\nYunShan (Ours)\n52.78\n63.42\n52.89\n57.57\n52.30\n72.50\n52.66\n55.93\n34.09\n51.43\n54.56\nTABLE 9: The experimental results of general-domain and domain\nspecialized LLMs on the financial FinEval [137] benchmark.\nModel\nAccounting\nCertificate\nEconomy\nFinance\nAVG\nLLaMA2-7B\n37.56\n34.13\n34.30\n28.85\n33.71\nInternLM-7B\n49.51\n52.99\n42.03\n53.11\n50.13\nBaichuan2-7B\n53.11\n56.89\n50.24\n59.67\n55.43\nQwen-7B\n62.30\n61.38\n56.04\n61.00\n60.56\nFinGPT-7B\n29.18\n25.45\n30.90\n25.90\n27.54\nFinMA-7B\n35.08\n32.34\n34.78\n35.41\n34.32\nLawGPT-7B\n25.25\n31.74\n28.50\n27.54\n28.32\nHanFei-7B\n29.84\n29.64\n27.54\n31.48\n29.80\nYunShan (Ours)\n65.57\n69.46\n51.21\n55.08\n61.34\nacross three cognitive levels: Memorization, Understanding, and\nApplying. It includes 20 tasks across five types: single-label\nclassification, multi-label classification, regression, extraction, and\ngeneration. With over 500 cases, LawBench employs metrics\nincluding accuracy, F1, and ROUGE-L to ensure a comprehensive\nassessment of LLMs in diverse legal applications.\nThrough this diverse and challenging array of datasets, our\nmodel is subjected to an extensive evaluation, covering a range of\ntasks and scenarios in law and finance. This setup not only tests\nthe model\u2019s domain-specific expertise but also its adaptability and\nversatility across different language settings.\n6.5\nComparison with other domain-specific models\nFor financial task, FinGPT [79] and FinMA [81] are adopted.\nSpecifically, FinGPT v3.2, which uses LLaMA2-7B as a base\nmodel, and FinMA v0.1 NLP 7B version are adopted for a fair\ncomparison. For legal tasks, we conduct experiments compared\nwith Hanfei [92] and LaWGPT [10]. Specifically, we chose the\nLegal-Base-7B version for LaWGPT, which is trained based on\nChinese-LLaMA-7B [128].\n6.5.1\nResults on the Financial Benchmark\nResults on FinancelQ.\nIn Table 8, we first compare the gen-\neral domain and domain- specialized LLMs on the FinancelQ\nbenchmark. These models were assessed across multiple finan-\ncial professional sub-domains, including Certified Public Ac-\ncountant (CPA), Banking Qualification (BQ), Stock Portfolio\nQuery (SPQ), Certified Financial Planner Qualification (CFPQ),\nInvestment Portfolio Query (IPQ), Economist, Tax Accountant,\nFinance Qualification (FQ), Financial Planner, and Actuary. The\n\u201dAVG\u201d column represents the average score across all these sub-\ndomains. The YunShan model demonstrated exceptional perfor-\nmance across nearly all sub-domains, particularly excelling in\nthe Economist, Banking Qualification, and Certified Financial\nPlanner Qualification categories, with scores of 72.50, 63.42, and\n57.57, respectively. These scores significantly surpassed those\nof other models, highlighting its efficiency in handling finance-\nrelated tasks. Its average score of 54.56 was markedly higher\nthan its counterparts, indicating its broad adaptability and profi-\nciency in the financial domain. While models like Baichuan2-7B\nshowed good performance in certain sub-domains such as Banking\nQualification, Investment Portfolio Query, and Certified Financial\nPlanner Qualification, they fell short in others. This suggests that\nwhile they are adaptable to specific financial sub-domains, they\ndo not match the overall proficiency of the YunShan model.\nConversely, other models such as FinGPT-7B and FinMA-7B\nexhibited generally weaker performance across all sub-domains,\nwith average scores not exceeding 35. This may indicate a lesser\ndegree of specialization in handling financial domain knowledge\ncompared to other models. Overall, the exceptional performance\nof the YunShan model in this financial domain-specific evaluation\nreflects its outstanding ability in understanding and processing\nfinancial professional knowledge and showcases its potential as\nan LLM for the financial industry.\nResults on FinEval.\nWe also conducted an experiment on the\nFinEval benchmark in Table 9. The significant advantages of the\nYunShan model still exist for the FinEval benchmark. As shown in\nTable 9, models were assessed across four different sub-domains:\nAccounting, Certificate, Economy, and Finance, with the \u201dAVG\u201d\ncolumn indicating the average score across all sub-domains.\nSpecifically, we computed the average score by AVG = (305 \u00d7\nAccounting + 334 \u00d7 Certificate + 207 \u00d7 Economy + 305 \u00d7\nFinance)/1151 according to the FinEval [137]. It\u2019s noteworthy\nthat the YunShan model excelled in Certificate and Accounting,\nscoring 69.46 and 65.57 respectively, significantly outperform-\ning other models. Its leading average score of 61.34 highlights\nits proficiency in handling finance-related tasks. Comparatively,\nQwen-7B and Baichuan2-7B demonstrated robust performance,\nespecially in the Finance sub-domain, with scores of 61.00 and\n59.67 respectively. Their overall average scores, 60.56 and 55.43,\nindicate their strong adaptability to the financial sector. In contrast,\nmodels like FinGPT-7B, FinMA-7B, LawGPT-7B, and HanFei-7B\nshowed relatively weaker performances across all sub-domains,\nwith average scores below 35. This suggests that these models\nmay be less specialized or inadequately trained for tasks specific\nto the financial domain. Overall, the YunShan model stands out as\nthe most suitable model for financial domain tasks.\n6.5.2\nResults on Legal Benchmark\nTable 10 shows the overall zero-shot results of each model in\nthe legal field on the different categories of LawBench [139], for\nthree key aspects: Memorization, Understanding, and Applying.\nThe average score is computed by AVG = (2\u00d7Memorization+\n14\nTABLE 10: The experimental results of general-domain and do-\nmain specialized LLMs on the legal benchmark.\nModel\nMemorization\nUnderstanding\nApplying\nAVG\nLLaMA2-7B\n1.52\n9.09\n16.19\n11.16\nInternLM-7B\n3.10\n8.29\n29.76\n16.36\nBaichuan2-7B\n2.78\n8.13\n19.79\n12.26\nQwen-7B\n0.82\n17.43\n31.54\n21.41\nFinGPT-7B\n0.00\n0.28\n0.00\n0.11\nFinMA-7B\n0.88\n6.43\n14.10\n8.94\nLawGPT-7B\n1.69\n3.41\n19.64\n9.73\nHanFei-7B\n13.71\n6.91\n25.22\n14.92\nYunShan (Ours)\n44.11\n19.72\n43.70\n31.75\n10\u00d7Understanding+8\u00d7Applying)/20. Notably, the YunShan\nmodel yet again outperforms with its impressive scores, with the\nhighest average score being 31.75. This performance indicates\na superior capability in recalling legal information but also in\nunderstanding and applying it effectively, a crucial factor in legal\ncontexts. Other models like Qwen-7B also demonstrate strong\nperformances, with average scores of 21.35 and 21.41, respec-\ntively. In contrast, models such as FinGPT-7B exhibit significantly\nlower scores across all dimensions, leading to an overall average\nscore of 0.11. Other models like InternLM-7B, Baichuan2-7B,\nand HanFei-7B demonstrate varied performances, with HanFei-7B\nscoring high in Memorization (13.71) but lower in Understanding\n(6.91), reflecting a potential imbalance in its skillset. Overall, the\nYunShan model demonstrates exceptional capability in the legal\ndomain, outperforming others across all tested skills. This suggests\nthat the YunShan model is adept at recalling legal information\nand excels in comprehending and applying it, making it a highly\neffective tool for legal applications.\n7\nCONCLUSIONS AND DISCUSSIONS\nIn this paper, we introduced PanGu-\u03c0, an innovative LLM archi-\ntecture designed to mitigate the feature collapse issue in Trans-\nformer models by integrating nonlinearity. Our initial step in-\nvolved a detailed examination of contemporary LLM architectures\nwhere we pinpointed the feature collapse challenge. Our theoret-\nical analysis led us to propose the integration of nonlinearity, a\nprinciple predominantly used in convolutional neural networks for\nimage processing, as a critical component for enhancing language\nmodels. To implement this, we augmented the FFN with a series\ninformed activation function, thereby enhancing its nonlinearity.\nAdditionally, we introduced an augmented shortcut to the MSA\nmodule, effectively reducing feature collapse. Theoretical eval-\nuations revealed that these enhancements significantly boost the\napproximation capabilities of both the FFN and MSA modules.\nLeveraging these enhancements, we developed various sizes of\nPanGu-\u03c0, notably PanGu-\u03c0-7B and PanGu-\u03c0-1B. These models\nunderwent extensive training and SFT, showcasing strong NLP\ncapabilities. The PanGu-\u03c0, foundation models achieved state-of-\nthe-art (SOTA) results in various downstream tasks, matching or\nsurpassing similar-sized models. PanGu-\u03c0 displayed exceptional\nadaptability, delivering SOTA performances in the specialized\ndomains of finance and law.\nREFERENCES\n[1]\nJ. Kaplan et al. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[2]\nJ. Wei et al. Emergent abilities of large language models. arXiv preprint\narXiv:2206.07682, 2022.\n[3]\nT. B. Brown et al. Language models are few-shot learners. In NeurIPS,\n2020.\n[4]\nA. Chowdhery et al. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\n[5]\nOpenAI. Gpt-4 technical report, 2023.\n[6]\nZ. Xi et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\n[7]\nH. Touvron et al.\nLlama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[8]\nA. Zeng et al. GLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning Representations\n(ICLR), 2023.\n[9]\nA. Yang et al. Baichuan 2: Open large-scale language models. arXiv\npreprint arXiv:2309.10305, 2023.\n[10]\nS. Pengxiao.\nLawgpt.\nhttps://github.com/pengxiao-song/LaWGPT,\n2023.\n[11]\nH. Yang et al. Fingpt: Open-source financial large language models.\nFinLLM Symposium at IJCAI 2023, 2023.\n[12]\nH. Wang et al.\nHuatuo: Tuning llama model with chinese medical\nknowledge, 2023.\n[13]\nA. Vaswani et al.\nAttention is all you need.\nAdvances in neural\ninformation processing systems, 30, 2017.\n[14]\nB. Peng et al. Rwkv: Reinventing rnns for the transformer era. arXiv\npreprint arXiv:2305.13048, 2023.\n[15]\nA. Katharopoulos et al.\nTransformers are rnns: Fast autoregressive\ntransformers with linear attention.\nIn International conference on\nmachine learning, 2020.\n[16]\nN. Shazeer.\nGlu variants improve transformer.\narXiv preprint\narXiv:2002.05202, 2020.\n[17]\nW. Fedus et al.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity.\nThe Journal of Machine\nLearning Research, 23(1):5232\u20135270, 2022.\n[18]\nY. Tang et al. Augmented shortcuts for vision transformers. In NeurIPS,\nvolume 34, pp. 15316\u201315327, 2021.\n[19]\nY. Dong et al. Attention is not all you need: Pure attention loses rank\ndoubly exponentially with depth.\nIn ICML, pp. 2793\u20132803. PMLR,\n2021.\n[20]\nH. Chen et al. Vanillanet: the power of minimalism in deep learning. In\nNeurIPS, 2023.\n[21]\nOpenAI. Introducing chatgpt. OpenAI Blog, November 2022.\n[22]\nOpenAI. Gpt-4 technical report. OpenAI, 2023.\n[23]\nH. Touvron et al.\nLlama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[24]\nH. Touvron et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[25]\nR. Taori et al. Alpaca: A strong, replicable instruction-following model.\nStanford Center for Research on Foundation Models. https://crfm.\nstanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.\n[26]\nY. Wang et al.\nSelf-instruct: Aligning language model with self\ngenerated instructions. arXiv preprint arXiv:2212.10560, 2022.\n[27]\nW.-L. Chiang et al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14\nApril 2023), 2023.\n[28]\nD. Eccleston. Sharegpt. https://sharegpt.com/, 2023.\n[29]\nI. Team. Internlm: A multilingual language model with progressively\nenhanced capabilities, 2023.\n[30]\nX. Ren et al.\nPangu-{\\Sigma}: Towards trillion parameter lan-\nguage model with sparse heterogeneous computing.\narXiv preprint\narXiv:2303.10845, 2023.\n[31]\nJ. Bai et al. Qwen technical report. arXiv preprint arXiv:2309.16609,\n2023.\n[32]\nT. Wei et al. Skywork: A more open bilingual foundation model. arXiv\npreprint arXiv:2310.19341, 2023.\n[33]\nM. Zaheer et al. Big bird: Transformers for longer sequences. Advances\nin neural information processing systems, 2020.\n[34]\nR. Child et al. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\n[35]\nA. Roy et al. Efficient content-based sparse attention with routing trans-\nformers. Transactions of the Association for Computational Linguistics,\n2021.\n[36]\nJ. W. Rae et al.\nCompressive transformers for long-range sequence\nmodelling. arXiv preprint arXiv:1911.05507, 2019.\n[37]\nG. Xiao et al. Efficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453, 2023.\n[38]\nK. Choromanski et al. Rethinking attention with performers. arXiv\npreprint arXiv:2009.14794, 2020.\n15\n[39]\nN. Kitaev et al. Reformer: The efficient transformer. arXiv preprint\narXiv:2001.04451, 2020.\n[40]\nI. Beltagy et al. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[41]\nT. Brown et al. Language models are few-shot learners. Advances in\nneural information processing systems, 2020.\n[42]\nZ. Dai et al. Transformer-xl: Attentive language models beyond a fixed-\nlength context. arXiv preprint arXiv:1901.02860, 2019.\n[43]\nP. H. Martins et al. \u221e-former: Infinite memory transformer. arXiv\npreprint arXiv:2109.00301, 2021.\n[44]\nY. Sun et al. Retentive network: A successor to transformer for large\nlanguage models. arXiv preprint arXiv:2307.08621, 2023.\n[45]\nM. Poli et al. Hyena hierarchy: Towards larger convolutional language\nmodels. arXiv preprint arXiv:2302.10866, 2023.\n[46]\nB. Peng et al. Rwkv: Reinventing rnns for the transformer era. arXiv\npreprint arXiv:2305.13048, 2023.\n[47]\nN. Du et al. Glam: Efficient scaling of language models with mixture-\nof-experts. In International Conference on Machine Learning, 2022.\n[48]\nS. Roller et al. Hash layers for large sparse models. Advances in Neural\nInformation Processing Systems, 2021.\n[49]\nZ. Chi et al. On the representation collapse of sparse mixture of experts.\nAdvances in Neural Information Processing Systems, 2022.\n[50]\nM. Lewis et al.\nBase layers: Simplifying training of large, sparse\nmodels. In International Conference on Machine Learning, 2021.\n[51]\nA. Chowdhery et al. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\n[52]\nN. Shazeer.\nGlu variants improve transformer.\narXiv preprint\narXiv:2002.05202, 2020.\n[53]\nW. Wang et al. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021.\n[54]\nZ. Liu et al. Swin transformer: Hierarchical vision transformer using\nshifted windows.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, 2021.\n[55]\nA. Dosovitskiy et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[56]\nJ. Guo et al. Cmt: Convolutional neural networks meet vision trans-\nformers. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022.\n[57]\nB. Heo et al. Rethinking spatial dimensions of vision transformers. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021.\n[58]\nZ. Pan et al. Scalable vision transformers with hierarchical pooling. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021.\n[59]\nC.-F. R. Chen et al.\nCrossvit: Cross-attention multi-scale vision\ntransformer for image classification. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021.\n[60]\nB. Graham et al.\nLevit: a vision transformer in convnet\u2019s clothing\nfor faster inference.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, 2021.\n[61]\nS. Mehta and M. Rastegari.\nMobilevit: light-weight, general-\npurpose, and mobile-friendly vision transformer.\narXiv preprint\narXiv:2110.02178, 2021.\n[62]\nK. Han et al.\nTransformer in transformer.\nAdvances in Neural\nInformation Processing Systems, 2021.\n[63]\nN. Parmar et al. Image transformer. In International conference on\nmachine learning, 2018.\n[64]\nX. Liu et al.\nEfficientvit: Memory efficient vision transformer with\ncascaded group attention. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2023.\n[65]\nY. Xiong et al.\nNystr\u00a8omformer: A nystr\u00a8om-based algorithm for ap-\nproximating self-attention. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2021.\n[66]\nM.-H. Guo et al. Beyond self-attention: External attention using two\nlinear layers for visual tasks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022.\n[67]\nJ. Guo et al.\nHire-mlp: Vision mlp via hierarchical rearrangement.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022.\n[68]\nY. Tang et al.\nAn image patch is a wave: Phase-aware vision mlp.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022.\n[69]\nW. Yu et al.\nMetaformer is actually what you need for vision.\nIn\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022.\n[70]\nD. Lian et al. As-mlp: An axial shifted mlp architecture for vision.\narXiv preprint arXiv:2107.08391, 2021.\n[71]\nS. Chen et al. Cyclemlp: A mlp-like architecture for dense prediction.\narXiv preprint arXiv:2107.10224, 2021.\n[72]\nL. Yuan et al. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In Proceedings of the IEEE/CVF international\nconference on computer vision, 2021.\n[73]\nX. Dong et al.\nCswin transformer: A general vision transformer\nbackbone with cross-shaped windows. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022.\n[74]\nY. Rao et al. Global filter networks for image classification. Advances\nin neural information processing systems, 2021.\n[75]\nH. Touvron et al. Resmlp: Feedforward networks for image classifica-\ntion with data-efficient training. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022.\n[76]\nI. O. Tolstikhin et al. Mlp-mixer: An all-mlp architecture for vision.\nAdvances in neural information processing systems, 2021.\n[77]\nS. Wu et al. Bloomberggpt: A large language model for finance. arXiv\npreprint arXiv:2303.17564, 2023.\n[78]\nB. Workshop et al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\n[79]\nH. Yang et al. Fingpt: Open-source financial large language models.\narXiv preprint arXiv:2306.06031, 2023.\n[80]\nX.-Y. Liu et al. Fingpt: Democratizing internet-scale data for financial\nlarge language models. arXiv preprint arXiv:2307.10485, 2023.\n[81]\nQ. Xie et al.\nPixiu: A large language model, instruction data and\nevaluation benchmark for finance. arXiv preprint arXiv:2306.05443,\n2023.\n[82]\nR. S. Shah et al.\nWhen flue meets flang: Benchmarks and large\npre-trained language model for financial domain.\narXiv preprint\narXiv:2211.00083, 2022.\n[83]\nX. Zhang and Q. Yang. Xuanyuan 2.0: A large chinese financial chat\nmodel with hundreds of billions parameters.\nIn Proceedings of the\n32nd ACM International Conference on Information and Knowledge\nManagement, pp. 4435\u20134439, 2023.\n[84]\nW. Chen et al. Disc-finllm: A chinese financial large language model\nbased on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205,\n2023.\n[85]\nBaichuan-inc.\nBaichuan-13b.\nhttps://github.com/baichuan-inc/\nBaichuan-13B, 2023.\n[86]\nxuanxuanzl.\nBaoluo lawassistant.\nhttps://github.com/xuanxuanzl/\nBaoLuo-LawAssistant, 2023.\n[87]\ndavidpig. Lychee. https://github.com/davidpig/lychee law, 2023.\n[88]\nseudl. Jurislms: Jurisprudential language models. https://github.com/\nseudl/JurisLMs, 2023.\n[89]\nL. Yue et al. Fedjudge: Federated legal large language model. arXiv\npreprint arXiv:2309.08173, 2023.\n[90]\nB. McMahan et al. Communication-efficient learning of deep networks\nfrom decentralized data.\nIn Artificial intelligence and statistics, pp.\n1273\u20131282. PMLR, 2017.\n[91]\nM. Zhou et al. Image de-raining via continual learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 4907\u20134916, 2021.\n[92]\nW. He et al. Hanfei-1.0. https://github.com/siat-nlp/HanFei, 2023.\n[93]\nQ. Huang et al.\nLawyer llama technical report.\narXiv preprint\narXiv:2305.15062, 2023.\n[94]\nzhihaiLLM.\nwisdominterrogatory.\nhttps://github.com/zhihaiLLM/\nwisdomInterrogatory, 2023.\n[95]\nS. Wu et al. fuzi.mingcha. https://github.com/irlab-sdu/fuzi.mingcha,\n2023.\n[96]\nH. Liu et al. Lawgpt. https://github.com/LiuHC0428/LAW GPT, 2023.\n[97]\nCSHaitao. Lexilaw. https://github.com/CSHaitao/LexiLaw, 2023.\n[98]\nJ. Cui et al. Chatlaw: Open-source legal large language model with\nintegrated external knowledge bases. arXiv preprint arXiv:2306.16092,\n2023.\n[99]\nS. Yue et al.\nDisc-lawllm: Fine-tuning large language models for\nintelligent legal services, 2023.\n[100] H. Shi et al. Revisiting over-smoothing in bert from the perspective of\ngraph. arXiv preprint arXiv:2202.08625, 2022.\n[101] K. He et al.\nDelving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 1026\u20131034, 2015.\n[102] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415, 2016.\n[103] P. Ramachandran et al.\nSearching for activation functions.\narXiv\npreprint arXiv:1710.05941, 2017.\n16\n[104] Y. Tang et al. Augmented shortcuts for vision transformers. Advances\nin Neural Information Processing Systems, 34:15316\u201315327, 2021.\n[105] Y. Shibata et al.\nByte pair encoding: A text compression scheme\nthat accelerates pattern matching.\nTechnical Report DOI-TR-161,\nDepartment of Informatics, Kyushu University, 1999.\n[106] T. Kudo and J. Richardson. Sentencepiece: A simple and language inde-\npendent subword tokenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226, 2018.\n[107] I. Loshchilov and F. Hutter. Decoupled weight decay regularization.\narXiv preprint arXiv:1711.05101, 2017.\n[108] I. Loshchilov and F. Hutter.\nSgdr: Stochastic gradient descent with\nwarm restarts. arXiv preprint arXiv:1608.03983, 2016.\n[109] B. Zhang and R. Sennrich.\nRoot mean square layer normalization.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[110] J. Su et al.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, pp. 127063, 2023.\n[111] O. Contributors.\nOpencompass: A universal evaluation platform for\nfoundation models.\nhttps://github.com/open-compass/opencompass,\n2023.\n[112] Y. Huang et al. C-eval: A multi-level multi-discipline chinese evaluation\nsuite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n[113] H. Li et al. Cmmlu: Measuring massive multitask language understand-\ning in chinese. arXiv preprint arXiv:2306.09212, 2023.\n[114] D. Hendrycks et al. Measuring massive multitask language understand-\ning. arXiv preprint arXiv:2009.03300, 2020.\n[115] W. Zhong et al. Agieval: A human-centric benchmark for evaluating\nfoundation models, 2023.\n[116] C. Clark et al.\nBoolq: Exploring the surprising difficulty of natural\nyes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n[117] A. Wang et al. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems, 2020.\n[118] Y. Bisk et al. Piqa: Reasoning about physical commonsense in natural\nlanguage, 2019.\n[119] Y. Li et al. Csl: A large-scale chinese scientific literature dataset. arXiv\npreprint arXiv:2209.05034, 2022.\n[120] L. Xu et al. Fewclue: A chinese few-shot learning evaluation bench-\nmark. arXiv preprint arXiv:2107.07498, 2021.\n[121] S. Narayan et al. Don\u2019t give me the details, just the summary! topic-\naware convolutional neural networks for extreme summarization, 2018.\n[122] B. Hu et al.\nLcsts: A large scale chinese short text summarization\ndataset, 2016.\n[123] F. Xue et al. Go wider instead of deeper. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, pp. 8779\u20138787, 2022.\n[124] X. Cai et al. Isotropy in the contextual embedding space: Clusters and\nmanifolds. In International Conference on Learning Representations,\n2021.\n[125] K. Ethayarajh. How contextual are contextualized word representations?\ncomparing the geometry of bert, elmo, and gpt-2 embeddings.\nIn\nConference on Empirical Methods in Natural Language Processing,\n2019.\n[126] M. P. Marcus et al. Building a large annotated corpus of english: The\npenn treebank. Computational Linguistics, 19:313\u2013330, 2002.\n[127] T. Gao et al. Simcse: Simple contrastive learning of sentence embed-\ndings. 2021.\n[128] Y. Cui et al. Efficient and effective text encoding for chinese llama and\nalpaca. arXiv preprint arXiv:2304.08177, 2023.\n[129] T. W. Peiyuan Zhang, Guangtao Zeng and W. Lu. Tinyllama, 2023.\n[130] M. Xia et al. Sheared llama: Accelerating language model pre-training\nvia structured pruning. arXiv preprint arXiv:2310.06694, 2023.\n[131] P. Henderson et al. Pile of law: Learning responsible data filtering from\nthe law and a 256gb open-source legal dataset, 2022.\n[132] I. Chalkidis et al.\nLeXFiles and LegalLAMA: Facilitating English\nmultinational legal language model development. In Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 15513\u201315535. Association for Computa-\ntional Linguistics, July 2023.\n[133] H. Zhong et al. JEC-QA: A legal-domain question answering dataset.\nIn Proceedings of AAAI, 2020.\n[134] C. Wang et al. Neural machine translation with byte-level subwords.\nIn Proceedings of the AAAI conference on artificial intelligence, vol-\nume 34, pp. 9154\u20139160, 2020.\n[135] H. Zhong et al.\nJec-qa: a legal-domain question answering dataset.\nIn Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pp. 9701\u20139708, 2020.\n[136] X. Zhang and Q. Yang. Xuanyuan 2.0: A large chinese financial chat\nmodel with hundreds of billions parameters.\nIn Proceedings of the\n32nd ACM International Conference on Information and Knowledge\nManagement, pp. 4435\u20134439, 2023.\n[137] L. Zhang et al.\nFineval: A chinese financial domain knowledge\nevaluation benchmark for large language models, 2023.\n[138] Z. Fei et al.\nLawbench: Benchmarking legal knowledge of large\nlanguage models. arXiv preprint arXiv:2309.16289, 2023.\n[139] Z. Fei et al.\nLawbench: Benchmarking legal knowledge of large\nlanguage models. arXiv preprint arXiv:2309.16289, 2023.\n17\nAPPENDIX\nProof of Lemma 1\nProof. Write W W \u22a4 = P DP \u22a4 for the eigin-decomposition of\nW W \u22a4, where P = [p1, p2, . . . , pd] is the standard orthogonal\nand D = diag(d1, . . . , dd) with all d1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 dd \u2265 0 and\nfor nonzero eigenvalues di = s2\ni > 0, where si is the i-th largest\nsingular value of W .\nHH\u22a4\n=\nQ\u2126Q\u22a4\nfor\nthe\neigin-decomposition\nof\nHH\u22a4, where Q = [q1, q2, . . . , qN] is the orthogonal and\n\u2126\n=\ndiag(\u03c91, . . . , \u03c9N) with all \u03c9i\n\u2265\n0. And e\n=\nN \u22121/2[1, 1, . . . , 1]\u22a4 = N \u22121/21 \u2208 RN\u00d71.\n(1) The formula proves as follows:\ndMm(HW )2 = \u2225(I \u2212 ee\u22a4)HW \u22252\nF\n= tr{(I \u2212 ee\u22a4)HW W \u22a4H\u22a4(I \u2212 ee\u22a4)}\n= tr{W W \u22a4H\u22a4(I \u2212 ee\u22a4)H}\n= tr{P DP \u22a4H\u22a4(I \u2212 ee\u22a4)H}\n= tr{DP \u22a4H\u22a4(I \u2212 ee\u22a4)HP }\n= tr{DP \u22a4H\u22a4(I \u2212 ee\u22a4)HP }\n=\nN\nX\ni=1\ndip\u22a4\ni H\u22a4(I \u2212 ee\u22a4)Hpi\n\u2264\nN\nX\ni=1\ns2p\u22a4\ni H\u22a4(I \u2212 ee\u22a4)Hpi\n= s2dMd(H)2.\nSince\nmatrix\nH\u22a4(I \u2212 ee\u22a4)H\nis\npositive\nsemidefinite,\np\u22a4\ni H\u22a4(I \u2212 ee\u22a4)Hpi \u2265 0.\nIt follows that dMm(HW ) \u2264 sdMd(H).\nNote that dMd(H) = \u2225H \u2212 1x\u22a4\nmin\u2225F , where x\u22a4\nmin =\narg minxl \u2225H \u22121x\u22a4\nl \u2225F . And \u2225\u03c3(H1)\u2212\u03c3(H2)\u2225F \u2264 L\u2225H1\u2212\nH2\u2225F\n(2) The formula proves as follows:\ndMd(\u03c3(H)) = \u2225\u03c3(H) \u2212 1x\u03c3\u22a4\nmin\u2225F\n\u2264 \u2225\u03c3(H) \u2212 1\u03c3(xmin)\u22a4\u2225F\n= \u2225\u03c3(H) \u2212 \u03c3(1x\u22a4\nmin)\u2225F\n\u2264 L\u2225H \u2212 1x\u22a4\nmin\u2225F\n= LdMd(H),\n(3) The formula proves as follows:\n\u03b11dMd(H) + \u03b12dMd(B)\n= \u03b11\u2225H \u2212 1xH\nmin\n\u22a4\u2225F + \u03b12\u2225B \u2212 1xB\nmin\n\u22a4\u2225F\n\u2265 \u2225\u03b11H + \u03b12B \u2212 1(\u03b11xH\nmin + \u03b12xB\nmin)\n\u22a4\u2225F\n\u2265 \u2225\u03b11H + \u03b12B \u2212 1(x\u03b11H+\u03b12B\nmin\n)\n\u22a4\u2225F\n= dMd(\u03b11H + \u03b12B)\nFor the last inequality, we refer to [100].\nProof of Theorem 1\nProof.\ndMm(AZlW ) \u2264\n\u221a\n\u03bbdMm(ZlW )\n\u2264\n\u221a\n\u03bbsdMm(Zl)\nProof of Lemma 2\nProof.\ndMHm(Concat([Hh]H\nh=1))2\n=\u2225(I \u2212 ee\u22a4)Concat([Hh]H\nh=1)\u22252\nF\n=tr{(I \u2212 ee\u22a4)Concat([Hh]H\nh=1)Concat([Hh]H\nh=1)\u22a4}\n=tr{Concat([Hh]H\nh=1)Concat([Hh]H\nh=1)\u22a4}\n\u2212 tr{Concat([e\u22a4Hh]H\nh=1)Concat([ee\u22a4Hh]H\nh=1)\u22a4}\n=\nH\nX\nh=1\ntr{(I \u2212 ee\u22a4)HhH\u22a4\nh }\n=\nH\nX\nh=1\ndMm(Hh)2,\nProof of Theorem 2\nProof.\nMSA(Zl) = Concat([AlhZlW v\nlh]H\nh=1)W o\nl ,\ndMd(Zl+1) = dMd(Concat([AlhZlW v\nlh]H\nh=1)W o\nl )\n\u2264 \u03c51dMd(Concat([AlhZlW v\nlh]H\nh=1))\n\u2264 \u03c51\nv\nu\nu\nt\nH\nX\nh=1\ndMd(AlhZlW v\nlh)2\n\u2264\n\u221a\n\u03bbs\u03c51\nv\nu\nu\nt\nH\nX\nh=1\ndMd(Zl)2\n=\n\u221a\n\u03bbHs\u03c51dMd(Zl)\n\u2264 (\n\u221a\n\u03bbHs\u03c51)l+1dMd(Z0).\nProof of Theorem 3\nProof.\nMLP(Z\u2032\nl) = \u03c3(Z\u2032\nlW \u2032\nl1)W \u2032\nl2, l \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L],\ndMd(MLP(Z\u2032\nl))) = dMd(\u03c3(Z\u2032\nlW \u2032\nl1)W \u2032\nl2)\n\u2264 \u03c52dMd(\u03c3(Z\u2032\nlW \u2032\nl1))\n\u2264 L\u03c52dMd(Z\u2032\nlW \u2032\nl1)\n\u2264 Ls\u03c52dMd(Z\u2032\nl)\n18\ndMd(Z\u2032\nl) \u2264 (Ls\u03c52)dMd(Z\u2032\nl\u22121)\n\u2264 (Ls\u03c52)ldMd(Z\u2032\n0).\nProof of Theorem 4\nProof.\nAugMSA(Zl) = MSA(Zl) + Zl +\nT\nX\ni=1\nTli(Zl; \u0398li),\ndMd(AugMSA(Zl))\n= dMd(MSA(Zl) + Zl +\nT\nX\ni=1\nTli(Zl; \u0398li))\n\u2264 dMd(MSA(Zl)) + dMd(Zl) + dMd(\nT\nX\ni=1\nTli(Zl; \u0398li))\n\u2264 dMd(MSA(Zl)) + dMd(Zl) +\nT\nX\ni=1\ndMd(\u03c3(Zl\u0398li))\n\u2264 (\n\u221a\n\u03bbHs\u03c51 + 1)dMd(Z\u2032\nl) + L\nT\nX\ni=1\ndMd(Zl\u0398li)\n\u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)dMd(Zl).\nConsidering that H heads exist inthe MSA module, the diver-\nsity dM(Zl+1) becomes:\ndMd(Zl) \u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)ldMd(Z0).\nProof of Lemma 3\nProof. When H = 1,\nMSA(Zl) = AZlW W o\nl ,\ndMd(MSA(Zl + \u03f5) \u2212 MSA(Zl))\n= \u2225(I \u2212 ee\u22a4)((A + \u03b4)(Zl + \u03f5) \u2212 AZl)(W W o\nl )\u2225F\n= \u2225(I \u2212 ee\u22a4)((A + \u03b4)\u03f5 + \u03b4Zl)(W W o\nl )\u2225F\n\u2264 dMd(A\u03f5\u03f5W W o\nl ) + dMd(\u03b4ZlW W o\nl )\n\u2264\nq\n\u03bbA+\u03b4s\u03c51\u2225\u03f5\u2225F +\np\n\u03bb\u03b4s\u03c51dMd(Zl),\nMSA(Zl) = Concat([AlhZlW v\nlh]H\nh=1)W o\nl ,\ndMd(MSA(Zl + \u03f5) \u2212 MSA(Zl))\n= \u03c51dMd(Concat([(A\u03f5lh(Zl + \u03f5) \u2212 AlhZl)W v\nlh]H\nh=1))\n\u2264\nq\n\u03bbA+\u03b4Hs\u03c51\u2225\u03f5\u2225F +\np\n\u03bb\u03b4Hs\u03c51dMd(Zl),\nProof of Lemma 4\nProof.\ndMd(L(Zl + \u03f5)\u0398li \u2212 LZl\u0398li)\n= dMd(L\u03f5\u0398li)\n= L\u2225(I \u2212 ee\u22a4)\u03f5\u0398li\u2225F\n= L\u2225\u03f5\u0398li \u2212 1x\u03f5\u0398li\u22a4\nmin\u2225F\n\u2264 L\u2225\u03f5\u0398li\u2225F\n\u2264 L\u2225\u0398li\u22252\u2225\u03f5\u2225F .\nProof of Theorem 5\nProof.\ndMd(Tli(Zl + \u03f5; \u0398li) \u2212 Tli(Zl; \u0398li))\n= dMd(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li))\n= \u2225(I \u2212 ee\u22a4)(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li))\u2225F\n\u2264 \u2225I \u2212 ee\u22a4\u22252\u2225(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li)\u2225F\n= \u2225(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li)\u2225F\n\u2264 L\u2225\u03f5\u0398li\u2225F .\n\u2264 L\u2225\u0398li\u22252\u2225\u03f5\u2225F .\nWhen ee\u22a4(\u03c3(Zl + \u03f5) \u2212 \u03c3(Zl)) \u0338= 0,\n\u2225(I \u2212 ee\u22a4)(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li))\u2225F\n< \u2225\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li)\u2225F .\nProof of Theorem 6\nProof.\ndMd(AugMSA(Zl + \u03f5) \u2212 AugMSA(Zl))\n= dMd(MSA(Zl + \u03f5) \u2212 MSA(Zl)) + dMd((Zl + \u03f5) \u2212 Zl)\n+\nT\nX\ni=1\ndMd(\u03c3((Zl + \u03f5)\u0398li) \u2212 \u03c3(Zl\u0398li))\n< (1 +\nq\n\u03bbA+\u03b4Hs\u03c51)\u2225\u03f5\u2225F +\nT\nX\ni=1\nL\u2225\u03f5\u0398li\u2225F +\np\n\u03bb\u03b4Hs\u03c51dMd(Zl)\n\u2264 (1 +\nq\n\u03bbA+\u03b4Hs\u03c51 + L\nT\nX\ni=1\n\u2225\u0398li\u22252)\u2225\u03f5\u2225F +\np\n\u03bb\u03b4Hs\u03c51dMd(Zl)\nProof of Theorem 7\nProof.\nSIAF \u2212 MLP(Z\u2032\nl) = (\nn\nX\ni=1\n\u03c3i(Z\u2032\nlW \u2032\nl1i))W \u2032\nl2i, l \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , L].\n(11)\n19\ndMd(SIAF \u2212 MLP(Z\u2032\nl))\n= \u2225(I \u2212 ee\u22a4)(\nn\nX\ni=1\n\u03c3i(Z\u2032\nlW \u2032\nl1i))W \u2032\nl2i\u2225F\n\u2264 \u03c52(\nn\nX\ni=1\n\u2225\u03c3i(Z\u2032\nlW \u2032\nl1i ) \u2212 \u03c3i(1xZ\u2032\nl\nmin\n\u22a4\nW \u2032\nl2i )\u2225F\n\u2264 (\nn\nX\ni=1\nLi)s\u03c52dMd(Z\u2032\nl),\ndMd(Z\u2032\nl) \u2264 (s\u03c52\nn\nX\ni=1\nLi)dMd(Z\u2032\nl\u22121)\n(12)\n\u2264 (s\u03c52\nn\nX\ni=1\nLi)ldMd(Z\u2032\n0)\n(13)\nProof of Theorem 8\nProof.\ndMd(Zp+q)\n= dMd(MLPq(Zp))\n\u2264 (Ls\u03c52)qdMd((MSAq(Zp))\n\u2264 (\n\u221a\n\u03bbHs\u03c51)p(Ls\u03c52)qdMd(Z0).\nProof of Theorem 9\nProof.\ndMd(Zp+q)\ndMd(SIAF \u2212 MLPq(Zp+q))\n\u2264 (s\u03c52\nn\nX\ni=1\nLi)qdMd(AugMSAp(Zp))\n\u2264 (\n\u221a\n\u03bbHs\u03c51 + 1 +\nT\nX\ni=1\nL\u2225\u0398li\u22252)p(s\u03c52\nn\nX\ni=1\nLi)qdMd(Z0).\n20\n.1\nExamples generated by PanGu-\u03c0-1B\nInput: Describe a time when you had to make a difficult decision.\nPanGu-\u03c0-1B: As an AI assistant, I have to make decisions every day, and sometimes it can be difficult to\nmake a decision. One example of a difficult decision I made was when I was asked to provide a response to\na customer\u2019s query. I had to decide whether to provide a response that would be helpful to the customer or to\nprovide a response that would be unhelpful.\nInput: Based on the given article, choose the right option for the question.\nArticle: We have two new students in our class. They are Cindy and Kate. They look the same. But they aren\u2019t\ntwins. Cindy is American. She is thirteen. Her father and mother are both teachers. She likes green and blue.\nShe is often in blue pants. She often plays the guitar after school. She is good at math and English. She says\nmath is her favorite because it is interesting. Kate is English. She is twelve. Her parents are not teachers. Her\nfather is a doctor and her mother is a worker. Kate likes yellow and orange. She can\u2019t play the guitar. But she\nplays volleyball very well. Her favorite subject is Chinese. She say she wants to learn about Chinese history.\nKate likes documentaries very much.\nQuestion: If you are ill, you can go to see\n.\nOptions: (A) Cindy\u2019s father (B) Cindy\u2019s mother (C) Kate\u2019s father (D) Kate\u2019s mother\nAnswer:\nPanGu-\u03c0-1B: Based on the given article, the correct option for the question is: (C) Kate\u2019s father.\n\u95ee\uff1a\u201c\u98ce\u58f0\u3001\u96e8\u58f0\u3001\u8bfb\u4e66\u58f0\uff0c\u58f0\u58f0\u4eba\u8033\uff1b\u5bb6\u4e8b\u3001\u56fd\u4e8b\u3001\u5929\u4e0b\u4e8b\uff0c\u4e8b\u4e8b\u5173\u5fc3\u3002\u201d\u8fd9\u526f\u5bf9\u8054\u7ed9\u6211\u4eec\u7684\u542f\u793a\n\u662f\nA. \u8981\u53c2\u52a0\u793e\u4f1a\u5b9e\u8df5\u6d3b\u52a8\uff0c\u70ed\u5fc3\u5e2e\u52a9\u8d2b\u56f0\u513f\u7ae5\nB. \u8981\u5b66\u597d\u6587\u5316\u77e5\u8bc6\uff0c\u5176\u4ed6\u4e8b\u60c5\u4e0e\u81ea\u5df1\u65e0\u5173\nC. \u8981\u5173\u5fc3\u793e\u4f1a\u53d1\u5c55\uff0c\u5173\u6ce8\u56fd\u5bb6\u5927\u4e8b\nD. \u4e8b\u5fc5\u8eac\u4eb2\uff0c\u5927\u5c0f\u4e8b\u90fd\u8981\u4eb2\u529b\u4eb2\u4e3a\n\u7b54\u6848\u662f\u9009\u9879\nPanGu-\u03c0-1B\uff1aC. \u8981\u5173\u5fc3\u793e\u4f1a\u53d1\u5c55\uff0c\u5173\u6ce8\u56fd\u5bb6\u5927\u4e8b\u3002\n\u95ee\uff1a\u8bf7\u9605\u8bfb\u6587\u7ae0\uff0c\u5e76\u6839\u636e\u6587\u7ae0\u5185\u5bb9\u4ece\u6587\u4e2d\u9009\u62e9\u5408\u9002\u7684\u90e8\u5206\u56de\u7b54\u95ee\u9898\u3002\n\u6587\u7ae0\uff1a\u963f\u90e8\u6b63\u4e30\uff08\u751f\u5e74\u4e0d\u8be6\uff0d1535 \u5e74 12 \u6708 29 \u65e5\uff09\u662f\u65e5\u672c\u6218\u56fd\u65f6\u4ee3\u7684\u6b66\u5c06\u3002\u4e09\u6cb3\u677e\u5e73\u6c0f\u5bb6\u81e3\uff0c\u963f\u90e8\u5b9a\n\u5409\u7684\u957f\u5b50\u3002\u901a\u79f0\u5f25\u4e03\u90ce\u3002\u5929\u6587 4 \u5e74\uff081535 \u5e74\uff09\uff0c\u5df2\u7ecf\u5e73\u5b9a\u897f\u4e09\u6cb3\u7684\u677e\u5e73\u6e05\u5eb7\u4e3a\u4e86\u8ba8\u4f10\u53d4\u7236\u677e\u5e73\u4fe1\u5b9a\u800c\n\u5728\u5c3e\u5f20\u5b88\u5c71\u5e03\u9635\uff0c\u6709\u4f20\u8a00\u6307\u7236\u4eb2\u5b9a\u5409\u4e0e\u7ec7\u7530\u4fe1\u79c0\u5185\u901a\u5e76\u4f01\u56fe\u8c0b\u53cd\uff0c\u56e0\u6b64\u5b9a\u5409\u62c5\u5fc3\u81ea\u5df1\u4f1a\u88ab\u6e05\u5eb7\u6740\u6b7b\uff0c\n\u4e8e\u662f\u5411\u6e05\u5eb7\u9001\u51fa\u81ea\u5df1\u6ca1\u6709\u4e8c\u5fc3\u7684\u8a93\u7eb8\u3002\u7fcc\u65e5\uff0c\u542c\u5230\u672c\u9635\u53d1\u51fa\u9a6c\u5636\u53eb\u58f0\u7684\u6b63\u4e30\u8bef\u4ee5\u4e3a\u7236\u4eb2\u88ab\u6740\uff0c\u4e8e\u662f\u5728\n\u672c\u9635\u6740\u6b7b\u6e05\u5eb7\u3002\u800c\u6b63\u4e30\u5219\u5728\u5f53\u573a\u88ab\u690d\u6751\u6c0f\u660e\u65a9\u6740\uff08\u68ee\u5c71\u5d29\uff09\u3002\n\u95ee\u9898\uff1a\u963f\u90e8\u6b63\u4e30\u88ab\u901a\u79f0\u4e3a\u4ec0\u4e48\uff1f\n\u7b54\uff1a\nPanGu-\u03c0-1B\uff1a\u6b63\u4e30\u88ab\u901a\u79f0\u5f25\u4e03\u90ce\u3002\n21\n.2\nExamples generated by PanGu-\u03c0-7B\nQ: Describe a time when you had to make a difficult decision.\nPanGu-\u03c0-7B: As an AI model, I don\u2019t have real-life experiences or the ability to make decisions, but I can\ntell you about a scenario that would require a difficult decision. Imagine a situation where a person is facing\na moral dilemma. For example, they are presented with two options: doing the right thing but it may result in\nnegative consequences, or doing something wrong but it may result in positive outcomes. In this situation, the\nperson must weigh the pros and cons of each option and make a decision based on their personal values and\nbeliefs. This can be a challenging and difficult decision to make, as it requires the person to examine their own\nmorality and make a choice that may not always be easy or clear-cut.\nQ: I\u2019m going to be participating in a comedy roast. What are some hilariously spicy roasts I can use?\nPanGu-\u03c0-7B: As an AI, I should not encourage or promote any kind of controversial or offensive content. It is\nnot appropriate to use any personal or derogatory material in a roast. However, there are many other topics you\ncan use for your roast, such as popular culture, food, animals, or even your own personal quirks.\n\u95ee\uff1a\u9e21\u548c\u5154\u5728\u4e00\u4e2a\u7b3c\u5b50\u91cc\uff0c\u5171\u6709 26 \u4e2a\u5934\uff0c68 \u53ea\u811a\uff0c\u90a3\u4e48\u9e21\u6709\u591a\u5c11\u53ea\uff0c\u5154\u6709\u591a\u5c11\u53ea\uff1f\nPanGu-\u03c0-7B\uff1a\u5047\u8bbe\u9e21\u6709 x \u53ea\uff0c\u5154\u6709 y \u53ea\u3002 \u6839\u636e\u9898\u76ee\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e24\u4e2a\u65b9\u7a0b\uff1a\n1. x + y = 26 \uff08\u9e21\u548c\u5154\u603b\u5171\u670926\u4e2a\u5934\uff09\n2. 2x + 4y = 68 \uff08\u9e21\u548c\u5154\u603b\u5171\u670968\u53ea\u811a\uff09\n\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6d88\u5143\u6cd5\u6765\u89e3\u8fd9\u4e2a\u65b9\u7a0b\u7ec4\u3002 \u5c06\u7b2c\u4e00\u4e2a\u65b9\u7a0b\u4e58\u4ee52\uff0c\u5f97\u52302x + 2y = 52\u3002 \u5c06\u7b2c\u4e8c\u4e2a\u65b9\u7a0b\u51cf\u53bb\u7b2c\n\u4e00\u4e2a\u65b9\u7a0b\uff0c\u5f97\u5230 2x + 4y - 2x - 2y = 68 - 52\uff0c\u5373 2y = 16\uff0cy = 8\u3002 \u5c06 y \u7684\u503c\u4ee3\u5165\u7b2c\u4e00\u4e2a\u65b9\u7a0b\uff0c\u5f97\u5230 x + 8\n= 26\uff0cx = 18\u3002 \u6240\u4ee5\uff0c\u9e21\u6709 18 \u53ea\uff0c\u5154\u6709 8 \u53ea\u3002\n.3\nExamples generated by YunShan\nInput: Do you know what a lawyer\u2019s letter is? You help me write a lawyer\u2019s letter, I was cheated, someone\ncheated me with fake information 1 million, I\u2019m depressed now, I can\u2019t get back the money.\nYunShan\uff1aA lawyer\u2019s letter is a legal document used to give a formal notice or demand of rights and obligations\nto the other party. Typically, attorney letters are used to resolve disputes or resolve potential litigation issues.\nBased on the information you have provided, it appears that you have been defrauded of 1 million and you\nwish to recover this money. In this case, you should immediately report the case to the local public security\nauthorities and provide all relevant evidence and information so that they can investigate. If you need legal help,\nyou can consult a professional lawyer who can help you assess your rights and responsibilities and provide you\nwith legal advice and support in negotiating or litigation on your behalf.\n"
  },
  {
    "title": "Learning Vision from Models Rivals Learning Vision from Data",
    "link": "https://arxiv.org/pdf/2312.17742.pdf",
    "upvote": "12",
    "text": "Learning Vision from Models Rivals Learning Vision from Data\nYonglong Tian1,\u2020 Lijie Fan2,\u2020,* Kaifeng Chen1 Dina Katabi2 Dilip Krishnan1 Phillip Isola2\n1Google Research,\n2MIT CSAIL,\n\u2020equal contribution\nGithub Repo: https://github.com/google-research/syn-rep-learn\nAbstract\nWe introduce SynCLR, a novel approach for learning vi-\nsual representations exclusively from synthetic images and\nsynthetic captions, without any real data. We synthesize a\nlarge dataset of image captions using LLMs, then use an off-\nthe-shelf text-to-image model to generate multiple images\ncorresponding to each synthetic caption. We perform visual\nrepresentation learning on these synthetic images via con-\ntrastive learning, treating images sharing the same caption\nas positive pairs. The resulting representations transfer well\nto many downstream tasks, competing favorably with other\ngeneral-purpose visual representation learners such as CLIP\nand DINO v2 in image classification tasks. Furthermore,\nin dense prediction tasks such as semantic segmentation,\nSynCLR outperforms previous self-supervised methods by a\nsignificant margin, e.g., improving over MAE and iBOT by\n6.2 and 4.3 mIoU on ADE20k for ViT-B/16.\n1. Introduction\nRepresentation learning extracts and organizes information\nfrom raw, often unlabeled data. The quality, quantity, and\ndiversity of the data determines how good a representation\nthe model can learn. The model becomes a reflection of the\ncollective intelligence that exists in the data. We get what\nwe feed in.\nUnsurprisingly, the current best-performing visual rep-\nresentation learning methods [68, 71] rely on large scale\nreal datasets. However, the collection of real data has its\nown dilemmas. Collecting large scale uncurated data [80] is\nrelatively cheap and thus quite achievable. However, for self-\nsupervised representation learning, this approach exhibits\npoor scaling behavior \u2013i.e., adding more uncurated data has\nlittle effect at large data scales [38, 90]. Collecting small\nscale curated data [24] also is achievable, but models trained\nin this way are limited to relatively narrow tasks. The ideal\nwould be large scale curated datasets of real images, and\n*Work done while interning at Google.\nLearning \nfrom \nmodels\nLearning \nfrom data\nLLM\nImage \nGenerator\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\nMfyB8/kDwruRjQ=</latexit>2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\nLearner\nf : X ! Y\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\nMfyB8/kDwruRjQ=</latexit>2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\nLearner\nf : X ! Y\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\nMfyB8/kDwruRjQ=</latexit>2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\n!\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\n<latexit sha1_base64=\"sVBkjs/c+hJlwPgmxP0/MoyXMvk=\">AB8nicbVBNS8NAEJ\n3Ur1q/qh69LBbBU0lE0GPRi8cK9gPSUDbTbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdkBouRcJbKFDybqo5j\nUPJO+H4duZ3Hrk2QiUPOEl5ENhIiLBKFrJ72kxHCHVWj31qzW37s5BVolXkBoUaParX72BYlnME2SGuN7bopBTjUKJvm0sMTykb0yH3LU1ozE2Qz0+ekjOrDEiktK0EyVz9PZHT\n2JhJHNrOmOLILHsz8T/PzC6DnKRpBnyhC0WRZkqMjsfzIQmjOUE0so08LeStiIasrQplSxIXjL6+S9kXdc+ve/WtcVPEUYTOIVz8OAKGnAHTWgBAwXP8ApvDjovzrvzsWgtOcX\nMfyB8/kDwruRjQ=</latexit>\nLearner\nf : X ! Y\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\n<latexit sha1_base64=\"HYPSq+DctLdg9hrnUlynFDqraoE=\">ACTnicfZDfShtBFMZn0z/GaGu0l70ZjIKUEnZFULyS1oveFBWMRrIhnJ2c3QzOziwzZ1vDklfprX2c3vZFeic6iSm0Kh4Y+PGdb2bO+ZJCS\nUdh+DuovXj56vVCfbGxtPzm7Upzde3MmdIK7AijO0m4FBJjR2SpLBbWIQ8UXieXH6e9s+/oXS6FMaF9jPIdMylQLIS4PmWrPuzy2MhsRWGu+84tBsxW2w1nxDNocXmdTxYDTbioRFljpqEAud6UVhQvwJLUicNOLSYQHiEjLsedSQo+tXs+EnfNMrQ54a648mPlP/vVFB7tw4T7wzBxq5h72p+FSvV1K616+kLkpCLe4/SkvFyfBpEnwoLQpSYw8grPSzcjEC4J8Xo1GfIh+GYtf/cNHBVogYz9UMdgsh6uJXy6LP07pOaPUf\n42efK7RwxQfw9l2Owrb0clO6+DTPOE6e8/W2RaL2C47YF/YMeswa7YD3bNfga/gj/BTXB7b60F8zv2H9Vq98BlhezeQ=</latexit>\nHybrid\nImage \nGenerator\nText dataset\nImage \ndataset\nText dataset\n80.2%\n<latexit sha1_base64=\"V5/ITBdyCU58U0qJSmYFqO7oEM=\">AB/XicbVDLTsJAFJ36RHzVx87NRCDBTdOykSWRjSYuMBFoUhoyHaYwYTptZqYm2B/xY0LjXHrf7jzbxygCwVPcpOTc+7NvfcECaNS2fa3sba+sbm1Xdgp7u7tHxyaR8cdGacCkzaOWSzcAEnCKCdtRUjbiIigJGusG\n4OfO7D0RIGvN7NUmIH6EhpyHFSGmpb56W67ZV61XKsNq8vWlBz3X9i75Zsi17DrhKnJyUQI5W3/zqDWKcRoQrzJCUnmMnys+QUBQzMi32UkShMdoSDxNOYqI9LP59VNY0coAhrHQxRWcq78nMhRJOYkC3RkhNZL3kz8z/NSFdb9jPIkVYTjxaIwZVDFcBYFHFBsGITRAWVN8K8QgJhJUOrKhDcJZfXiWdmuXYlnNXKzWu8jgK4AycgypwCVogGvQAm2AwSN4Bq/gzXgyXox342PRumbkMyfgD4zPH6yHkiM=</latexit>\n<latexit sha1_base64=\"9CT3PjVopMh/zXd+oW32lZVtduw=\">ACInicjVDLTsJAFJ3iC/FVHzs3E4EN6RlI0siG01cYCLQpDRkOkxhwnTazExNsOFfXLjxV9wYdWXixzhAFwouPMkJ+ecmzv3+DGjUlnWp5FbW9/Y3MpvF3Z29/YPzMOjowSgUkbRywSjo8kYZSTtqKEScWBIU+I1\n/3Jz53XsiJI34nZrExAvRkNOAYqS01DdPSnWrWuVS7DSvLluQdxvPO+WbSq1hxwldgZKYIM/4v3zfeIMJSLjCDEnp2lasvBQJRTEj0IvkSRGeIyGxNWUo5BIL52fOIVlrQxgEAn9uIJz9edEikIpJ6GvkyFSI7nszcS/PDdRQd1LKY8TRTheLAoSBlUEZ3BARUEKzbRBGFB9V8hHiGBsNKtFvTp9vKhq6RTq9pW1b6tFRuXWd5cArOQAXY4AI0wBVogTbA4AE8gmfwajwZL8ab8bGI5oxs5hj8gvH1DbvmZw=</latexit>\n<latexit sha1_base64=\"9CT3PjVopMh/zXd+oW32lZVtduw=\">ACInicjVDLTsJAFJ3iC/FVHzs3E4EN6RlI0siG01cYCLQpDRkOkxhwnTazExNsOFfXLjxV9wYdWXixzhAFwouPMkJ+ecmzv3+DGjUlnWp5FbW9/Y3MpvF3Z29/YPzMOjowSgUkbRywSjo8kYZSTtqKEScWBIU+I1\n/3Jz53XsiJI34nZrExAvRkNOAYqS01DdPSnWrWuVS7DSvLluQdxvPO+WbSq1hxwldgZKYIM/4v3zfeIMJSLjCDEnp2lasvBQJRTEj0IvkSRGeIyGxNWUo5BIL52fOIVlrQxgEAn9uIJz9edEikIpJ6GvkyFSI7nszcS/PDdRQd1LKY8TRTheLAoSBlUEZ3BARUEKzbRBGFB9V8hHiGBsNKtFvTp9vKhq6RTq9pW1b6tFRuXWd5cArOQAXY4AI0wBVogTbA4AE8gmfwajwZL8ab8bGI5oxs5hj8gvH1DbvmZw=</latexit>\n<latexit sha1_base64=\"9CT3PjVopMh/zXd+oW32lZVtduw=\">ACInicjVDLTsJAFJ3iC/FVHzs3E4EN6RlI0siG01cYCLQpDRkOkxhwnTazExNsOFfXLjxV9wYdWXixzhAFwouPMkJ+ecmzv3+DGjUlnWp5FbW9/Y3MpvF3Z29/YPzMOjowSgUkbRywSjo8kYZSTtqKEScWBIU+I1\n/3Jz53XsiJI34nZrExAvRkNOAYqS01DdPSnWrWuVS7DSvLluQdxvPO+WbSq1hxwldgZKYIM/4v3zfeIMJSLjCDEnp2lasvBQJRTEj0IvkSRGeIyGxNWUo5BIL52fOIVlrQxgEAn9uIJz9edEikIpJ6GvkyFSI7nszcS/PDdRQd1LKY8TRTheLAoSBlUEZ3BARUEKzbRBGFB9V8hHiGBsNKtFvTp9vKhq6RTq9pW1b6tFRuXWd5cArOQAXY4AI0wBVogTbA4AE8gmfwajwZL8ab8bGI5oxs5hj8gvH1DbvmZw=</latexit>\n76.\n<latexit sha1_base64=\"BzdP2sPYHqx7DcniQvc0ekvPJNQ=\">ACAnicbVC7TsMwFHV4lvIKMCEWi7ZSWaKkA2WsYGEsj7aR0qhyXKe16jiR7SBVUcXCr7AwgBArX8HG3+C2GaDlSJaOzrlH1/cECaNS2fa3sbK6tr6xWdgqbu/s7u2bB4dtGacCkxaOWSzcAEnCKCctRUjbiIigJGOsHoaup3HoiQNOb3apwQP0IDTkOKkdJSzw\nu18+terdShtU7hXTqliTQc13/rGeWbMueAS4TJyclkKPZM7+6/RinEeEKMySl59iJ8jMkFMWMTIrdVJIE4REaE9TjiIi/Wx2wgRWtNKHYSz04wrO1N+JDEVSjqNAT0ZIDeWiNxX/87xUhRd+RnmSKsLxfFGYMqhiO0D9qkgWLGxJgLqv8K8RAJhJVurahLcBZPXibtmuXYlnNTKzUu8zoK4AScgipwQB0wDVoghbA4BE8g1fwZjwZL8a78TEfXTHyzBH4A+PzByqflLk=</latexit>\n<latexit sha1_base64=\"4fMNMAWUS8cXS10eQk/Zp+Gu08M=\">ACJ3icjVDLSsNAFJ34rPUVdSVuBtC3YSkC+uy6Malr7aBNJTJdNIOnUzCzEQofg1Ltz4K4KI6NI/cdpmoa0LDwczj2HO+cGCaNS2fansbS8srq2Xtgobm5t7+yae/stGacCkyaOWSzcAEnCKCdNRUjbiIigJG2sHwYjJv3xMhaczv1CghfoT6nIYUI6Wlrnl\nYrp9a9U6lDKu3CunUDUmg57r+Sdcs2ZY9BVwkTk5KIMf/7F3ztdOLcRoRrjBDUnqOnSg/Q0JRzMi42EklSRAeoj7xNOUoItLPpj3HsKVHgxjoR9XcKr+TGQoknIUBdoZITWQ87OJ+NfMS1V45meUJ6kiHM8WhSmDKoaTo8EeFQrNtIEYUH1XyEeIGw0qct6urOfNF0qpZjm0517VS4zy/WQEcgWNQBQ6ogwa4BFegCTB4AI/gGbwZT8aL8W58zKxLRp45AL9gfH0DjScMg=</latexit>\n<latexit sha1_base64=\"4fMNMAWUS8cXS10eQk/Zp+Gu08M=\">ACJ3icjVDLSsNAFJ34rPUVdSVuBtC3YSkC+uy6Malr7aBNJTJdNIOnUzCzEQofg1Ltz4K4KI6NI/cdpmoa0LDwczj2HO+cGCaNS2fansbS8srq2Xtgobm5t7+yae/stGacCkyaOWSzcAEnCKCdNRUjbiIigJG2sHwYjJv3xMhaczv1CghfoT6nIYUI6Wlrnl\nYrp9a9U6lDKu3CunUDUmg57r+Sdcs2ZY9BVwkTk5KIMf/7F3ztdOLcRoRrjBDUnqOnSg/Q0JRzMi42EklSRAeoj7xNOUoItLPpj3HsKVHgxjoR9XcKr+TGQoknIUBdoZITWQ87OJ+NfMS1V45meUJ6kiHM8WhSmDKoaTo8EeFQrNtIEYUH1XyEeIGw0qct6urOfNF0qpZjm0517VS4zy/WQEcgWNQBQ6ogwa4BFegCTB4AI/gGbwZT8aL8W58zKxLRp45AL9gfH0DjScMg=</latexit>\n<latexit sha1_base64=\"4fMNMAWUS8cXS10eQk/Zp+Gu08M=\">ACJ3icjVDLSsNAFJ34rPUVdSVuBtC3YSkC+uy6Malr7aBNJTJdNIOnUzCzEQofg1Ltz4K4KI6NI/cdpmoa0LDwczj2HO+cGCaNS2fansbS8srq2Xtgobm5t7+yae/stGacCkyaOWSzcAEnCKCdNRUjbiIigJG2sHwYjJv3xMhaczv1CghfoT6nIYUI6Wlrnl\nYrp9a9U6lDKu3CunUDUmg57r+Sdcs2ZY9BVwkTk5KIMf/7F3ztdOLcRoRrjBDUnqOnSg/Q0JRzMi42EklSRAeoj7xNOUoItLPpj3HsKVHgxjoR9XcKr+TGQoknIUBdoZITWQ87OJ+NfMS1V45meUJ6kiHM8WhSmDKoaTo8EeFQrNtIEYUH1XyEeIGw0qct6urOfNF0qpZjm0517VS4zy/WQEcgWNQBQ6ogwa4BFegCTB4AI/gGbwZT8aL8W58zKxLRp45AL9gfH0DjScMg=</latexit>\n80.3%\n<latexit sha1_base64=\"OtU8mCdAbBed0EwPuAPmtjYgpXE=\">AB+HicbVBNT8JAEJ36ifhB1aOXjUCF9LiQY5EL97ERD4SaMh2cKG7bZ3Zpgwy/x4kFjvPpTvPlvXKAHBV8yct7M5mZ58ecKe0439bG5tb2zm5uL79/cHhUsI9P2ipKJKEtEvFIdn2sKGeCtjTnHZjSXHoc9rxJzdzv/NIpWKReNDTmHohHgkWMIK1kQ\nZ2oVR3qpf9cglV7hKpLgZ20ak6C6B14makCBmaA/urP4xIElKhCcdK9Vwn1l6KpWaE01m+nygaYzLBI9ozVOCQKi9dHD5DZaMURBJU0Kjhfp7IsWhUtPQN50h1mO16s3F/7xeoO6lzIRJ5oKslwUJBzpCM1TQEMmKdF8agmkplbERljiYk2WeVNCO7qy+ukXau6TtW9rxUb1kcOTiDc6iAC1fQgFtoQgsIJPAMr/BmPVkv1rv1sWzdsLKZU/gD6/MHU6aQ6w=</latexit>\n<latexit sha1_base64=\"NhPQy1UcpRZvqvYTVpX9kRNSYQg=\">ACHXicjVC7TgJBFL3rE/HBqXNRCDBhuxiISXRxk5N5JHAhswOszBh9pF5mJANX2Jh46/YGNhY/wbB9hCwcKTHJyzrm5c4+fcCaV43xZa+sbm1vbuZ387t7+QcE+PGrJWAtCmyTmsej4WFLOItpUTHaSQTFoc9p2x9fzfz2AxWSxdG9miTUC/EwYgEjWB\nmpbxdKdad63iuXUOVGC3nWt4tO1ZkDrRI3I0XI8L943/7oDWKiQxopwrGUXdJlJdioRjhdJrvaUkTMZ4SLuGRjik0kvn101R2SgDFMTCvEihufpzIsWhlJPQN8kQq5Fc9mbiX15Xq6DupSxKtKIRWSwKNEcqRrOq0IAJShSfGIKJYOaviIywESZQvPmdHf50FXSqlVdp+re1YqNy6yzHJzAKVTAhQtowDXcQhMIaHiEZ3i1nqwX6816X0TXrGzmGH7B+vwGN3eYZA=</latexit>\n<latexit sha1_base64=\"NhPQy1UcpRZvqvYTVpX9kRNSYQg=\">ACHXicjVC7TgJBFL3rE/HBqXNRCDBhuxiISXRxk5N5JHAhswOszBh9pF5mJANX2Jh46/YGNhY/wbB9hCwcKTHJyzrm5c4+fcCaV43xZa+sbm1vbuZ387t7+QcE+PGrJWAtCmyTmsej4WFLOItpUTHaSQTFoc9p2x9fzfz2AxWSxdG9miTUC/EwYgEjWB\nmpbxdKdad63iuXUOVGC3nWt4tO1ZkDrRI3I0XI8L943/7oDWKiQxopwrGUXdJlJdioRjhdJrvaUkTMZ4SLuGRjik0kvn101R2SgDFMTCvEihufpzIsWhlJPQN8kQq5Fc9mbiX15Xq6DupSxKtKIRWSwKNEcqRrOq0IAJShSfGIKJYOaviIywESZQvPmdHf50FXSqlVdp+re1YqNy6yzHJzAKVTAhQtowDXcQhMIaHiEZ3i1nqwX6816X0TXrGzmGH7B+vwGN3eYZA=</latexit>\n<latexit sha1_base64=\"NhPQy1UcpRZvqvYTVpX9kRNSYQg=\">ACHXicjVC7TgJBFL3rE/HBqXNRCDBhuxiISXRxk5N5JHAhswOszBh9pF5mJANX2Jh46/YGNhY/wbB9hCwcKTHJyzrm5c4+fcCaV43xZa+sbm1vbuZ387t7+QcE+PGrJWAtCmyTmsej4WFLOItpUTHaSQTFoc9p2x9fzfz2AxWSxdG9miTUC/EwYgEjWB\nmpbxdKdad63iuXUOVGC3nWt4tO1ZkDrRI3I0XI8L943/7oDWKiQxopwrGUXdJlJdioRjhdJrvaUkTMZ4SLuGRjik0kvn101R2SgDFMTCvEihufpzIsWhlJPQN8kQq5Fc9mbiX15Xq6DupSxKtKIRWSwKNEcqRrOq0IAJShSfGIKJYOaviIywESZQvPmdHf50FXSqlVdp+re1YqNy6yzHJzAKVTAhQtowDXcQhMIaHiEZ3i1nqwX6816X0TXrGzmGH7B+vwGN3eYZA=</latexit>\ne.g. CLIP\ne.g. \nStableRep\nOurs\n76.7%\n<latexit sha1_base64=\"aPXIoNODWf5x+LURXtNXl3iFWBA=\">AB7XicbVA9TwJBEJ3DL8Qv1NJmIyGxutxRCXRxhIT+UjgQvaWPVjZ273s7pmQC/BxkJjbP0/dv4bF7hCwZdM8vLeTGbmhQln2njet1PY2t7Z3Svulw4Oj45PyqdnHS1TRWibSC5VL8SaciZo2zDaS9RFMchp91wervwu09UaSbFg5klNIjxWLCIEWys1Klfu/VBd\nViueK63BNokfk4qkKM1LH8NRpKkMRWGcKx13/cSE2RYGUY4nZcGqaYJlM8pn1LBY6pDrLltXNUtcoIRVLZEgYt1d8TGY61nsWh7Yyxmeh1byH+5/VTEzWCjIkNVSQ1aIo5chItHgdjZixPCZJZgoZm9FZIVJsYGVLIh+Osvb5JOzfU917+vVZo3eRxFuIBLuAIf6tCEO2hBGwg8wjO8wpsjnRfn3flYtRacfOYc/sD5/AHtao4H</latexit>\n<latexit sha1_base64=\"k3qOvZfdDfdUvYR+hDWYeaK2zM=\">ACEnicjVC7TsMwFL0ur1JeBUYWi6oSU5R0oIwVLIwg0YfURpXjOq2p40S2g1RF/QcGFn6FBSFWJjb+BrfNAC0DR7J0dM65ur4nSATXxnW/UGFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrternT\nr1X7ZcruPOgVeJl5MK5PhfvF/+7A1imkZMGiqI1l3PTYyfEWU4FWxa6qWaJYSOyZB1LZUkYtrP5idNcdUqAxzGyj5p8Fz9OZGRSOtJFNhkRMxIL3sz8S+vm5rws+4TFLDJF0sClOBTYxn/eABV4waMbGEUMXtXzEdEUWosS2W7One8qGrpFVzPNfxbmuVxmXeWRFO4BTOwIM6NOAabqAJFO7hEZ7hFT2hF/SG3hfRAspnjuEX0Mc3dJaVgA=</latexit>\n<latexit sha1_base64=\"k3qOvZfdDfdUvYR+hDWYeaK2zM=\">ACEnicjVC7TsMwFL0ur1JeBUYWi6oSU5R0oIwVLIwg0YfURpXjOq2p40S2g1RF/QcGFn6FBSFWJjb+BrfNAC0DR7J0dM65ur4nSATXxnW/UGFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrternT\nr1X7ZcruPOgVeJl5MK5PhfvF/+7A1imkZMGiqI1l3PTYyfEWU4FWxa6qWaJYSOyZB1LZUkYtrP5idNcdUqAxzGyj5p8Fz9OZGRSOtJFNhkRMxIL3sz8S+vm5rws+4TFLDJF0sClOBTYxn/eABV4waMbGEUMXtXzEdEUWosS2W7One8qGrpFVzPNfxbmuVxmXeWRFO4BTOwIM6NOAabqAJFO7hEZ7hFT2hF/SG3hfRAspnjuEX0Mc3dJaVgA=</latexit>\n<latexit sha1_base64=\"k3qOvZfdDfdUvYR+hDWYeaK2zM=\">ACEnicjVC7TsMwFL0ur1JeBUYWi6oSU5R0oIwVLIwg0YfURpXjOq2p40S2g1RF/QcGFn6FBSFWJjb+BrfNAC0DR7J0dM65ur4nSATXxnW/UGFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrternT\nr1X7ZcruPOgVeJl5MK5PhfvF/+7A1imkZMGiqI1l3PTYyfEWU4FWxa6qWaJYSOyZB1LZUkYtrP5idNcdUqAxzGyj5p8Fz9OZGRSOtJFNhkRMxIL3sz8S+vm5rws+4TFLDJF0sClOBTYxn/eABV4waMbGEUMXtXzEdEUWosS2W7One8qGrpFVzPNfxbmuVxmXeWRFO4BTOwIM6NOAabqAJFO7hEZ7hFT2hF/SG3hfRAspnjuEX0Mc3dJaVgA=</latexit>\nIN lin. acc\nIN lin. acc\n80.7%\n<latexit sha1_base64=\"W8xjdoQtjD9zFBtJopo6ywChb/A=\">AB7XicbVBNSwMxEJ2tX7V+tOrRS7AUPC27IrTHohePFewHtEvJptk2NpsSVYoS/+DFw+KePX/ePfmLZ70NYHA4/3ZpiZFyacaeN5305ha3tnd6+4Xzo4PDouV05O1qmitA2kVyqXog15UzQtmG016iKI5DTrvh9Hbhd5+o0kyKBzNLaBDjsWARI9hYqdPw3P\nqgNqxUPdbAm0SPydVyNEaVr4GI0nSmApDONa673uJCTKsDCOczkuDVNMEkyke076lAsdUB9ny2jmqWEIqlsCYOW6u+JDMdaz+LQdsbYTPS6txD/8/qpiRpBxkSGirIalGUcmQkWryORkxRYvjMEkwUs7ciMsEKE2MDKtkQ/PWXN0nyvU917+/rjZv8jiKcA4XcAk+1KEJd9CNhB4hGd4hTdHOi/Ou/Oxai04+cwZ/IHz+QPmY4E</latexit>\n<latexit sha1_base64=\"7aUnXkl7DSvTHSwrXSuHBGJijAg=\">ACEnicjVC7SgNBFL0bXzG+opY2gyFgtcyKkJRBG0sF84BkCbOT2WTM7OwyMyuEJf9gYeOv2IjYWtn5N06SLTSx8MDA4ZxzuXNPkAiuDcZfTmFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrte\nrYrfWq/XIFu3gOtEq8nFQgx/i/fJnbxDTNGLSUEG07no4MX5GlOFUsGmpl2qWEDomQ9a1VJKIaT+bnzRFVasMUBgr+6RBc/XnREYirSdRYJMRMSO97M3Ev7xuasK6n3GZpIZJulgUpgKZGM36QOuGDViYgmhitu/IjoilBjWyzZ073lQ1dJ69z1sOvdXlQal3lnRTiBUzgD2rQgGu4gSZQuIdHeIZX58l5cd6c90W04OQzx/ALzsc3bNaVfQ=</latexit>\n<latexit sha1_base64=\"7aUnXkl7DSvTHSwrXSuHBGJijAg=\">ACEnicjVC7SgNBFL0bXzG+opY2gyFgtcyKkJRBG0sF84BkCbOT2WTM7OwyMyuEJf9gYeOv2IjYWtn5N06SLTSx8MDA4ZxzuXNPkAiuDcZfTmFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrte\nrYrfWq/XIFu3gOtEq8nFQgx/i/fJnbxDTNGLSUEG07no4MX5GlOFUsGmpl2qWEDomQ9a1VJKIaT+bnzRFVasMUBgr+6RBc/XnREYirSdRYJMRMSO97M3Ev7xuasK6n3GZpIZJulgUpgKZGM36QOuGDViYgmhitu/IjoilBjWyzZ073lQ1dJ69z1sOvdXlQal3lnRTiBUzgD2rQgGu4gSZQuIdHeIZX58l5cd6c90W04OQzx/ALzsc3bNaVfQ=</latexit>\n<latexit sha1_base64=\"7aUnXkl7DSvTHSwrXSuHBGJijAg=\">ACEnicjVC7SgNBFL0bXzG+opY2gyFgtcyKkJRBG0sF84BkCbOT2WTM7OwyMyuEJf9gYeOv2IjYWtn5N06SLTSx8MDA4ZxzuXNPkAiuDcZfTmFtfWNzq7hd2tnd2z8oHx61dJwqypo0FrHqBEQzwSVrGm4E6ySKkSgQrB2Mr2Z+4EpzWN5ZyYJ8yMylDzklBgrte\nrYrfWq/XIFu3gOtEq8nFQgx/i/fJnbxDTNGLSUEG07no4MX5GlOFUsGmpl2qWEDomQ9a1VJKIaT+bnzRFVasMUBgr+6RBc/XnREYirSdRYJMRMSO97M3Ev7xuasK6n3GZpIZJulgUpgKZGM36QOuGDViYgmhitu/IjoilBjWyzZ073lQ1dJ69z1sOvdXlQal3lnRTiBUzgD2rQgGu4gSZQuIdHeIZX58l5cd6c90W04OQzx/ALzsc3bNaVfQ=</latexit>\nIN lin. acc\n80.2%\n<latexit sha1_base64=\"DREih0KZjV6Xct/Su8ZhKXVD3E=\">AB7XicbVBNS8NAEJ3Ur1q/oh69LJaCp5D0Yo9FLx4r2A9oQ9lsN+3azW7Y3Qgl9D948aCIV/+PN/+N2zYHbX0w8Hhvhpl5UcqZNr7/7ZS2tnd298r7lYPDo+MT9/Sso2WmCG0TyaXqRVhTzgRtG2Y47aWK4iTitBtNbxd+94kqzaR4MLOUhgkeCxYzgo2VOg\n3fqw9qQ7fqe/4SaJMEBalCgdbQ/RqMJMkSKgzhWOt+4KcmzLEyjHA6rwyTVNMpnhM+5YKnFAd5str56hmlRGKpbIlDFqvydynGg9SyLbmWAz0eveQvzP62cmboQ5E2lmqCrRXHGkZFo8ToaMUWJ4TNLMFHM3orIBCtMjA2oYkMI1l/eJ26F/hecF+vNm+KOMpwAZdwBQFcQxPuoAVtIPAIz/AKb450Xpx352PVWnKmXP4A+fzB94ljf0=</latexit>\n<latexit sha1_base64=\"ypyoh2GfdwbXu/VtH3OnwnQzw=\">ACEnicjVC7SgNBFL3rM8ZX1NJmMASslt0pgzaWCqYByQhzE7uJmNmZ5eZWSEs+QcLG3/FRsTWys6/cZJsoYmFBwYO5zLnXuCRHBtPO/LWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtgGoUXGLDcCOwnSikUSCwFYyvZn7rAZXmsbwzkwR7ER1KHnJGjZ\nWaNc+tdiv9UtlzvTnIKvFzUoYc/4v3S5/dQczSCKVhgmrd8b3E9DKqDGcCp8VuqjGhbEyH2LFU0gh1L5ufNCUVqwxIGCv7pCFz9edERiOtJ1FgkxE1I73szcS/vE5qwlov4zJDUq2WBSmgpiYzPohA6QGTGxhDLF7V8JG1FmbEtFu3p/vKhq6RZdX3P9W+r5fpl3lkBTuEMzsGHC6jDNdxAxjcwyM8w6vz5Lw4b87Irm5DMn8AvOxzdj35V2</latexit>\n<latexit sha1_base64=\"ypyoh2GfdwbXu/VtH3OnwnQzw=\">ACEnicjVC7SgNBFL3rM8ZX1NJmMASslt0pgzaWCqYByQhzE7uJmNmZ5eZWSEs+QcLG3/FRsTWys6/cZJsoYmFBwYO5zLnXuCRHBtPO/LWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtgGoUXGLDcCOwnSikUSCwFYyvZn7rAZXmsbwzkwR7ER1KHnJGjZ\nWaNc+tdiv9UtlzvTnIKvFzUoYc/4v3S5/dQczSCKVhgmrd8b3E9DKqDGcCp8VuqjGhbEyH2LFU0gh1L5ufNCUVqwxIGCv7pCFz9edERiOtJ1FgkxE1I73szcS/vE5qwlov4zJDUq2WBSmgpiYzPohA6QGTGxhDLF7V8JG1FmbEtFu3p/vKhq6RZdX3P9W+r5fpl3lkBTuEMzsGHC6jDNdxAxjcwyM8w6vz5Lw4b87Irm5DMn8AvOxzdj35V2</latexit>\n<latexit sha1_base64=\"ypyoh2GfdwbXu/VtH3OnwnQzw=\">ACEnicjVC7SgNBFL3rM8ZX1NJmMASslt0pgzaWCqYByQhzE7uJmNmZ5eZWSEs+QcLG3/FRsTWys6/cZJsoYmFBwYO5zLnXuCRHBtPO/LWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtgGoUXGLDcCOwnSikUSCwFYyvZn7rAZXmsbwzkwR7ER1KHnJGjZ\nWaNc+tdiv9UtlzvTnIKvFzUoYc/4v3S5/dQczSCKVhgmrd8b3E9DKqDGcCp8VuqjGhbEyH2LFU0gh1L5ufNCUVqwxIGCv7pCFz9edERiOtJ1FgkxE1I73szcS/vE5qwlov4zJDUq2WBSmgpiYzPohA6QGTGxhDLF7V8JG1FmbEtFu3p/vKhq6RZdX3P9W+r5fpl3lkBTuEMzsGHC6jDNdxAxjcwyM8w6vz5Lw4b87Irm5DMn8AvOxzdj35V2</latexit>\nFigure 1. Three paradigms for visual representation learning. Top\nrow: Traditional methods, such as CLIP [71], learn only from real\ndata; Middle row: Recent methods, such as StableRep [91], learn\nfrom real text and generated images; Bottom row: Our method,\nSynCLR, learns from synthetic text and synthetic images, and rival\nthe linear transfer performance of CLIP on ImageNet despite not\ndirectly observing any real data.\nrecent work has indeed shown that this can lead to strong\nperformance gains at scale [68], but this path is costly to\npursue.\nTo alleviate the cost, in this paper we ask if synthetic\ndata, sampled from off-the-shelf generative models, is a\nviable path toward large scale curated datasets that can train\nstate-of-the-art visual representations.\nWe call such a paradigm learning from models, in con-\ntrast to directly learning from data. Models have several\nadvantages as a data source for building large scale train-\ning sets: via their latent variables, conditioning variables,\nand hyperparameters, they provide new controls for curat-\ning data; we will make use of these controls in the method\nwe propose. Models also can be easier to share and store\n(because models are more compressed than data), and can\n1\narXiv:2312.17742v1  [cs.CV]  28 Dec 2023\nproduce an unlimited number of data samples (albeit with\nfinite diversity). A growing literature has studied these\nproperties and other advantages (and disadvantages) of us-\ning generative models as a data source for training down-\nstream models [3, 30, 45, 48, 78, 91]. Some of these meth-\nods use a hybrid mode \u2013 either mixing real and synthetic\ndatasets [3] or needing a real dataset to generate another\nsynthetic dataset [91]. Other methods try to learn represen-\ntations from purely synthetic data [78] but lag far behind\nthe best performing models. Instead, we show that learning\nfrom models, without training on any real data, can yield rep-\nresentations that match the top-performing representations\nlearnt from real data. For instance, as illustrated in Figure 1,\nrepresentations learnt by our method are able to transfer as\nwell as OpenAI\u2019s CLIP [71] on ImageNet (both methods\nusing ViT-B [28]).\nOur approach leverages generative models to re-define\nthe granularity of visual classes. As shown in Figure 2, con-\nsider we have four images generated using two prompts: \u201ca\ngolden retriever, wearing sunglasses and a beach hat, rides\na bike\" and \u201ca cute golden retriever sits in a house made\nof sushi\". Traditional self-supervised method such as Sim-\nCLR [13] will treat each of these images as a different class;\nembeddings for different images are pushed apart with no\nexplicit consideration of the shared semantics between im-\nages. On the other extreme, supervised learning methods\n(i.e. SupCE) will regard all these images as a single class\n(e.g., \u201cgolden retriever\u201d). This ignores nuances in the se-\nmantics of the images, such as the fact that the dogs are\nriding a bike in one pair of images and sitting inside a sushi\nhouse in the other pair of images. Instead, our method, Syn-\nCLR, treats captions as classes, i.e., each caption describes\na visual class (this level of granularity was also explored\nin StableRep [91]). This allows us to group images by the\nconcepts of \u201criding a bike\u201d and \u201csitting in a sushi house\u201d,\nin addition to grouping by a coarser class label like \u201cgolden\nretrieval\u201d. This level of granularity is difficult to mine in real\ndata, since collecting multiple images described by a given\ncaption is non-trivial, especially when scaling up the number\nof captions. However, text-to-image diffusion models are\nfundamentally built with this ability; simply by conditioning\non the same caption and using different noise inputs, a text-\nto-image diffusion model will produce different images that\nall match the same caption. In our experiments, we find the\ncaption-level granularity outperforms both SimCLR and su-\npervised training. Another advantage is that this definition of\nvisual classes has good scalability. Unlike ImageNet-1k/21k\nwhere a given number of classes is fixed, we can augment ex-\nisting classes (or data) in an online fashion, and theoretically\nscale up to as many classes as needed.\nOur system consists of three steps. The first step is to\nsynthesize a large corpus of image captions. We design a\nscalable approach by leveraging the in-context learning ca-\nSynCLR\nSimCLR\nSupervised \nCE\nclass 1\nclass 2\nclass 3\nclass 4\nclass 1\nclass 1\nclass 2\nFigure 2. Different learning objectives treat classification granu-\nlarity differently. These images are generated by two prompts \u201ca\ngolden retriever, wearing sunglasses and a beach hat, rides a bike\"\nand \u201ca cute golden retriever sits in a house made of sushi\". Sim-\nCLR treats each image as a class, while supervised cross-entropy\ntreats them all as the same \u201cgolden retrieval\u201d class. The former\ndoes not consider shared semantics between images, and the latter\nis coarse-grained and ignores actions or relationships between sub-\njects/background. Our approach, SynCLR, defines visual classes\nby sentences.\npability of large language models (LLMs), where we present\nexamples of word-to-caption translations. Next, a text-to-\nimage diffusion model is adopted to synthesize multiple\nimages for each synthetic caption. This yields a synthetic\ndataset of 600M images. Then we train visual representa-\ntion models by a combination of multi-positive contrastive\nlearning [50] and masked image modeling [110].\nOur learned representations transfer well. With Syn-\nCLR pre-training, our ViT-B and ViT-L models achieve\n80.7% and 83.0% top-1 linear probing accuracy on\nImageNet-1K, respectively, which is on par with OpenAI\u2019s\nCLIP [71]. On fine-grained classification tasks, SynCLR out-\nperforms CLIP by 3.3% for ViT-B and 1.5% for ViT-L, and\nperforms similarly to DINO v2 [68] models, which are dis-\ntilled from a pre-trained ViT-g model. For semantic segmen-\ntation on ADE20k, SynCLR outperforms MAE pre-trained\non ImageNet by 6.2 and 4.1 in mIoU for ViT-B and ViT-L\nunder the same setup, showing strong transfer ability for\ndense prediction tasks similar to DINO v2, which addition-\nally involves a training period on 518x518 resolution images\nthat SynCLR does not have.\n2. Related Works\nSelf-supervised representation learning approaches in vi-\nsion develop domain-specific pre-text tasks, such as col-\norization [106], rotation prediction [36], and solving jigsaw\n2\npuzzles [65]. Domain-agnostic approaches have been pop-\nular, such as contrastive learning [6, 13, 40, 43, 66, 88, 97]\nand masked image modeling [2, 4, 5, 33, 44, 96, 100, 110].\nContrastive learning promotes invariance [89] for two views\nof the same image and pushes apart representations for dif-\nferent images [95] (or only invariance [11, 39]); the resulting\nrepresentations yield strong performance for linear or zero-\nshot transfer. Masked image modeling reconstructs the pix-\nels [44, 100] or local features [4], often producing excellent\nfine-tuning transfer performance, especially in dense predic-\ntion tasks [44]. The state of the art DINO v2 [68] leverages\nboth approaches, and our approach shares a similar spirit.\nSupervised learning [41, 52, 84] used to be the dominant\napproach for learning transferable visual representations for\nvarious tasks [26, 37, 81]. Recent studies [42, 57] has shown\nthat, the transferability of representations learned in this way\nis limited, e.g., pre-training has no improvement over random\ninitialization for dense prediction tasks (e.g., object detec-\ntion) when the fine-tuning is long enough. Such limitation\ncontinues when the model has been scaled up to 22B [23].\nAn alternative paradigm learns visual representations from\ntext supervision [49, 71], e.g., CLIP [71]. This approach is\nmore flexible (i.e., not requiring classes) and provides richer\nsupervision, often learning generalizable representations.\nGenerative models as representation learners. A number\nof papers have explored the representations that are learned\nby generative models for various recognition tasks [25, 56].\nAs might be expected intuitively, such models indeed learn\nespecially good representations for dense tasks, such as opti-\ncal flow estimation [79], semantic segmentation [8, 101], and\ndepth estimation [107]. Another line of work [19, 55] adapt\npre-trained diffusion models for zero-shot image recognition\nvia analysis-by-synthesis. These approaches may need to\nbe adapted when the architectures of the generative models\nchange or a new family of generative model emerge. Our\napproach treats images as universal interfaces with the hope\nof better generality.\nLearning from synthetic data from generative models.\nSynthetic data has been explored to train machine learn-\ning models in various domains [31, 53, 62, 63, 74, 75, 83,\n87, 102]. In computer vision, the utilization of synthetic\ndata for training models is common, ranging from optical\nflow [61] and autonomous driving [1] to semantic segmenta-\ntion [15] and human pose estimation [94]. Others [48, 58]\nhave explored synthetic data for representation learning,\nwith the predominant approach of altering the latent vari-\nables of deep generative models. Our approach aligns with\nthis research paradigm, but it diverges in its use of text-to-\nimage models, which have also been investigated by other re-\nsearchers [45, 78, 111]. But they use synthetic data for super-\nvised learning [30, 78]. The closet work is StableRep [91],\nwhich also conducts representation learning but still needs a\nreal text dataset.\n3. Approach\nIn this paper, we study the problem of learning a visual en-\ncoder f in the absence of real images or textual data. Our\napproach hinges on the utilization of three key resources: a\nlanguage generation model (g1), a text-to-image generative\nmodel (g2), and a curated list of visual concepts (C). Our ex-\nploration include three steps: (1) we employ g1 to synthesize\na comprehensive set of image descriptions T, which encom-\npass the range of visual concepts in C; (2) for each caption\nin T, we generate multiple images using g2, culminating in\nan extensive synthetic image dataset X; (3) we train on X\nto obtain a visual representation encoder f.\nWe use Llama-2 7B [93] and Stable Diffusion 1.5 [73] as\ng1 and g2, respectively, because of their fast inference speed.\nWe anticipate that better g1 and g2 in the future will further\nenhance the effectiveness of this approach.\n3.1. Synthesizing captions\nTo harness the capability of powerful text-to-image models\nfor generating a substantial dataset of training images, we ini-\ntially require a collection of captions that not only precisely\ndepict an image but also exhibit diversity to encompass a\nbroad spectrum of visual concepts.\nWe have developed a scalable approach to create such a\nlarge collection of captions, leveraging the in-context learn-\ning capability of LLMs [9]. Our method involves crafting\nspecific prompt engineering templates that guide the LLM to\nproduce the required captions. We start by gathering the con-\ncept list C from some existing datasets, such as ImageNet-\n21k [24] and Places-365 [108]. For each concept c \u2208 C, we\nconsider three straightforward templates to generate captions\neffectively.\n\u2022 c \u2013> caption. As the most direct and simple approach, we\nhave the Llama-2 model sample a sentence for the concept\nc.\n\u2022 c, bg \u2013> caption. We combine the visual concept c with\na background or setting bg. A na\u00efve approach would ran-\ndomly select both c and bg, where bg may correspond to a\nclass name from a places dataset like [108]. However, this\nmethod often leads to unlikely combinations in the real\nworld, such as a blue whale in a football field. Our abla-\ntion experiments demonstrate that this strategy results in\nsuboptimal performance, likely because the generated cap-\ntions fall far outside the training distribution of g2. Instead,\nwe employ GPT-4 [67] to generate a list of suitable back-\ngrounds for the chosen concepts. This approach increases\nthe likelihood of generating more plausible combinations,\nsuch as a tiger in a forest or a cat in a kitchen, enhancing\nthe overall quality of the results.\n\u2022 c, rel \u2013> caption. Given a visual concept c, we consider\npairing it with a positional relationship word, rel. Take\nfor instance, if c signifies cat and rel translates to in front\n3\nTemplates\nIn context examples\nc \u2013> caption\nrevolver \u2013> Multiple antique revolvers lie on a wooden table, gleaming under soft, ambient light.\ncloset \u2013> The compact closet, brimming with clothes and shoes, exudes a feeling of organization.\nzebra \u2013> A zebra is gallantly trotting across the vast, sunlit plains of the African savannah, creating a\ncaptivating black and white spectacle.\nbus station \u2013> The bustling bus station thrums with restless energy, as travelers navigate through the crowded\nspace, awaiting their journeys amid the echoes of departing buses.\nc,bg \u2013> caption\ntiger, forest \u2013> Two tigers are running together in the forest.\nlighter, motorhome \u2013> In the cozy, cluttered environment of a well-traveled motorhome, a sleek silver lighter\nholds dominion on the rustic wooden table.\nsunset, lake \u2013> Golden sunset hues reflect on a calm lake, silhouetting a lone canoeist against a backdrop of\nfiery clouds.\nc,rel \u2013> caption\nkit fox, in front of \u2013> A group of small, fluffy, golden kit foxes is playfully gathered in front of a lush, green,\ntowering forest backdrop.\ncabbage, besides \u2013> A vibrant image portrays a lush, green cabbage, glistening with dewdrops, nestled\nbesides a rustic, wooden crate full of freshly harvested vegetables.\nTable 1. We show examples for the three synthesis templates. Such examples are used as demonstrations for Llama-2 to perform the\nin-context learning task. We have 176 such examples in total. Most of them are generated by prompting GPT-4 [67], while a handful of\nothers are human generated (in a 10M scale pilot study of synthetic captions, we do not notice significant differences between including or\nexcluding human generated examples.)\nFigure 3. In-context caption generation using Llama-2 [93]. We\nrandomly sample three in-context examples for each inference run.\nof, our objective is to prompt the LLM to create captions\nsuch as a cute yellow cat is enjoying the fish in front of the\nsofa. To add variety, we have a selection of 10 different\npositional relationship words that we randomly choose\nfrom.\nFor each of the three templates, we have prepared multi-\nple demonstration examples that serve as instructions for the\nLLM to complete the caption synthesis task. Table 1 shows a\ncouple of examples for each template. In total, we have 106\nexamples for c\u2013>prompt, 50 examples for c, bg\u2013>prompt,\nand 20 examples for c, rel\u2013>prompt. Such examples are\nmostly collected by prompting GPT-4, with a handful from\nhuman. In a pilot study, we do not observe difference be-\ntween including or excluding human generated examples.\nIn the stage of generating captions in-context, we select a\nconcept and one of the three templates. Next, we randomly\npick three examples from the chosen template and frame the\ncaption generation as a text completion task. This process is\nillustrated in Figure 3.\n3.2. Synthesizing Images\nFor each text caption, we generate a variety of images by\ninitiating the reverse diffusion process with different random\nnoise. The Classifier-Free Guidance (CFG) scale is a crucial\nfactor in this process. A higher CFG scale enhances the qual-\nity of the samples and the alignment between text and image,\nwhereas a lower scale results in more diverse samples and\nbetter adherence to the original conditional distribution of\nimages based on the given text. Following the approach used\nin StableRep [91], we opt for a lower CFG scale, specifically\n2.5, and produce 4 images for each caption. Examples of\nthese images can be seen in Figure 4.\n3.3. Representation Learning\nOur representation learning method is built upon Sta-\nbleRep [91]. The key component of our approach is the\nmulti-positive contrastive learning loss [50] which works by\naligning (in the embedding space) images generated from the\nsame caption. We additionally combine multiple techniques\nfrom other self-supervised learning methods, including a\npatch-level masked image modeling objective. We briefly\nreview StableRep and elaborate on the added modules.\nStableRep [91] minimizes the cross-entropy loss between\na ground-truth assignment distribution and a contrastive as-\nsignment distribution. Consider an encoded anchor sample\na and a set of encoded candidates {b1, b2, ..., bK}. The con-\ntrastive assignment distribution q describes how likely the\nmodel predicts a and each b to be generated from the same\ncaption, and the ground-truth distribution is the actual match\n4\nA plate of paella, a mixed rice dish with chicken, beans, and seafood\nA vintage electric locomotive rolls along a railway line through a quaint paddy \nfield in a tranquil rural landscape.\nAn industrial power plant with its smokestacks belching black smoke.\nOn a desk, a glass water bed is surrounded by a chaotic, messy workspace.\nA fluffy, black and white junco bird perches on a snow-covered fence, \noverlooking a dark forest.\nA combine harvester pulling a trailer full of hay, driving along a narrow road \nwith a lake in the distance.\nFigure 4. Random examples of synthetic captions and images generated in our SynCLR pipeline. Each caption comes with 4 images.\nbetween a and b (a is allowed to match multiple b):\nqi =\nexp(a \u00b7 bi/\u03c4)\nPK\nj=1 exp(a \u00b7 bj/\u03c4)\n(1)\npi =\n1match(a,bi)\nPK\nj=1 1match(a,bj)\n(2)\nwhere \u03c4 \u2208 R+ is the scalar temperature, a and all b have\nbeen \u21132 normalized, and the indicator function 1match(\u00b7,\u00b7)\nindicates whether two samples are from the same caption.\nThe contrastive loss for a is given as\nL(a) = H(p, q) = \u2212\nK\nX\ni=1\npi log qi\n(3)\niBOT [110] is a masked image modeling objective, wherein\na localized patch is masked, and the model is tasked with\npredicting the tokenized representation of said masked patch.\nIt adapts the DINO [11] objective from the image level into\nthe patch level. We follow [76] to replace the softmax-\ncentering method with the iterative Sinkhorn-Knopp (SK)\nalgorithm [22]. We run SK for 3 iterations to build the\nprediction target.\nExponential Moving Average (EMA) is firstly introduced\ninto self-supervised learning by MoCo [43]. We use EMA to\nencode crops as b and to produce the targets for iBOT loss.\nWe update the EMA model as \u03b8ema \u2190 \u03bb\u03b8ema + (1 \u2212 \u03bb)\u03b8,\nfollowing a cosine schedule for \u03bb from 0.994 to 1 during\ntraining [39, 68]. We find the EMA module not only in-\ncreases the final performance, but also improves the training\nstability for long training schedules.\nMulti-crop strategy is introduced by [10] as a smart way to\nimprove computation efficiency, and is adopted in this paper.\nFor these local crops, we only employ the contrastive loss,\nomitting the iBOT loss. Local crops are encoded only by\nthe student network, and matched to global crops from the\nsame caption encoded by the EMA model. Such reuse of\nglobal crops saves computation. For each image x, where\nwe generate a single global crop xg alongside n local crops\nxl, the final loss can be expressed as follows:\nL(xg) + 1\nn\nn\nX\ni=1\nL(xl\ni) + LiBOT (xg)\n(4)\n3.4. Implementation\nConcept list. We concatenate class names from various\ndatasets, including IN-1k [24], IN-21k (we keep the most fre-\nquent 13k classes), Aircraft [60], Cars [51], DTD [18], Flow-\ners [64], Pets [69], Sun397 [98], Caltech-101 [34], Food-\n101 [7], and Places-365 [108]. If the concept is a place (i.e.\nSUN397 and Places) or a texture (i.e. DTD), we only apply\nthe c \u2013> caption template. For fine-grained classes such\nas pets or flowers, we employ GPT-4 to generate a consol-\nidated list of probable backgrounds, rather than producing\ndistinct lists for each specific class. We favor more frequent\nsampling from IN-1k, Food101, Cars, Aircraft, and Flowers.\nBatches. For each training batch, we sample 2048 captions\n(except when noted), and use all of the 4 images generated\nby each caption. We generate 1 global and 4 local crops for\neach image. As a result, each batch contains 8192 global\ncrops, which is similar with prior work [13, 14, 39, 91].\nMasking. For the iBOT loss, we randomly choose 50%\nimages inside a batch to mask, and randomly mask 50% of\nthe tokens in each chosen image. We use 65536 prototypes.\nWhile the target from the EMA model is ascertained using\n5\ncaptions\nStableRep\nSynCLR\nIN\navg.\nIN\navg.\ncc12m\n73.0\n81.6\n77.1\n85.3\nIN+h+Places\n75.4\n80.0\n78.7\n83.0\nIN+Places+LLM\n73.7\n76.9\n77.6\n81.8\nIN+OurBG+LLM\n75.3\n78.5\n78.2\n81.9\nour final config.\n75.8\n85.7\n78.8\n88.1\nTable 2. Comparison of different caption synthesis strategies.\nWe report top-1 ImageNet linear evaluation accuracy and the aver-\nage accuracy over 9 fine-grained datasets. Every item here includes\n10M captions and 4 images per caption.\nCFG\n2\n3\n4\nIN top-1\n72.8\n72.6\n72.6\nTable 3. Classifier-free guidance scale (CFG). Contrastive loss\nprefers small CFG scale but is not very sensitive to it.\nthe SK algorithm, we apply softmax normalization to the\noutput of the student model.\nProjection heads. We follow the design in MoCo v3 [14]\nand DINO [11] for the contrastive and iBOT loss heads,\nrespectively, ensuring consistency with established methods.\nOther hyper-parameters. We set the temperature in the con-\ntrastive loss to 0.08. For the temperature used in the iBOT\nloss, we linearly increase it from 0.04 to 0.07 over 4000\niterations, and keep it as 0.07 afterwards, as in DINO [11].\nAdditionally, the weight decay parameter is incrementally\nadjusted from 0.04 to 0.2, adhering to a cosine schedule.\n4. Experiment\nWe first perform an ablation study to evaluate the efficacy of\nvarious designs and modules within our pipeline. Then we\nproceed to scale up the volume of synthetic data.\n4.1. Study different components\nWe analyze each component of SynCLR, and ablate their\neffectiveness in two measurements: (1) linear probing perfor-\nmance on IN-1k; (2) average accuracy of linear transfer on\nfine-grained datasets Aircraft [60], Cars [51], DTD [18],\nFlowers [64], Pets [69], Sun397 [98], Caltech-101 [34],\nFood-101 [7], and Pascal VOC [29]. For analysis conducted\nin this subsection, we train ViT-B/16 [28] models for 85000\niterations, and use the cls token as image representation.\nSynthesize captions. Following [91], we use cc12m [12]\nreal captions as our baseline, which has 10M sentences.\nTo synthesize captions, we design the following variants:\n(a) IN+h+Places randomly combines one IN class plus its\nhypernyms in WordNet graph, with one place class; (b)\nIN+Places+LLM uses the c, bg \u2013> caption in-context syn-\nthesis template with c from IN and bg from places; (c)\nIN+ourBG+LLM uses the background classes output by\nGPT-4, instead of Places; (d) ours means our full configu-\nmethod\nEMA\niBOT\nMC\nIN\navg.\nADE20k\nStableRep\n75.8\n85.7\n-\n\u2713\n76.7\n86.7\n48.0\n\u2713\n\u2713\n77.6\n87.1\n50.5\n\u2713\n\u2713\n78.6\n87.8\n49.5\nSynCLR\n\u2713\n\u2713\n\u2713\n78.8\n88.1\n50.8\nTable 4. Important components for our model. ViT-B/16 models\nare trained for 85000 iterations. We study the modules that af-\nfect the ImageNet linear evaluation, the fine-grained classification\n(avg.), and ADE20k segmentation.\nmethod\nIN\navg.\nSupervised CE\n71.9\n75.0\nSimCLR\n63.6\n67.9\nSynCLR\n75.3\n78.5\nTable 5. Comparison of different learning objectives. These\nobjectives assume different level of classification granularity, as\nshown in Figure 2. Our modeling, i.e., defining classes as captions,\noutperforms the other two. To accomondate Supervised CE training,\nall items here used IN+OurBG+LLM entry in Table 2.\nration specified in Section 3.1. For each of the config, we\ngenerate 10M captions. If not enough, we do duplication.\nResults are summarized in Table 2, where we train both\nStableRep and SynCLR to avoid biases favored by a single\nmethod. Compared to a real caption dataset cc12m, sim-\nply concatenating IN and Places class names improves the\nImageNet linear accuracy but reduces the fine-grained classi-\nfication performance. Interestingly, naively asking Llama to\ncombine IN and Places classes into captions yields the worst\nperformance. Replacing random background from places\nwith GPT generated background improves the accuracy. This\nshows the importance of synthesizing captions that follow\nthe distribution of real captions, which were used to train the\ntext-to-image model. Finally, our full configuration achieves\nthe best accuracy on both ImageNet and fine-grained classi-\nfication. Another advantage of our synthesis method is its\nscalability \u2013 scale up to hundreds of millions of captions with\nlittle duplication. In contrast, if we concatenate IN classes\nwith Places classes, there are at most 365k unique captions.\nSynthesize images. There are two major parameters in this\nprocess: number of images per caption and classifier free\nguidance scale. For the former, we find generating 4 images\nis almost able to reproduce StableRep [91]\u2019s performance\n(10 images) when using cc12m captions (ours 73.0% v.s.\nStableRep 73.5% on ImageNet). Thus we stick to 4. For\nguidance scale, we briefly find the contrastive loss is not very\nsensitive to CFG in a pilot study, as shown in Table 3. Thus\nwe stick to 2.5, similar as StableRep [91].\nModel components. We present the improvement of accu-\nracy brought by different modules in Table 4. Compared\n6\ntext\nimg\n# imgs\nImageNet\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nVOC2007\nAverage\nStableRep\nreal\nsyn\n100M\nViT-B/16\n75.7\n59.2\n83.5\n80.1\n97.3\n88.3\n74.3\n94.7\n85.1\n87.9\n83.4\nCLIP\nreal\nreal\n400M\nViT-B/16\n80.2\n59.5\n86.7\n79.2\n98.1\n93.1\n78.4\n94.7\n92.8\n89.2\n85.7\nViT-L/14\n83.9\n69.4\n90.9\n82.1\n99.2\n95.1\n81.8\n96.5\n95.2\n89.6\n88.9\nOpenCLIP\nreal\nreal\n400M\nViT-B/16\n78.9\n61.1\n92.3\n81.9\n98.2\n91.5\n77.9\n95.2\n90.9\n88.0\n86.3\n400M\nViT-L/14\n82.3\n67.1\n94.0\n83.6\n98.8\n92.5\n81.0\n96.4\n93.4\n88.8\n88.4\n2B\nViT-L/14\n83.4\n71.7\n95.3\n85.3\n99.0\n94.2\n82.2\n97.5\n94.1\n88.9\n89.8\nDINO v2*\n-\nreal\n142M\nViT-B/14\n83.9\u2020\n79.4\n88.2\n83.3\n99.6\n96.2\n77.3\n96.1\n92.8\n88.2\n89.0\nViT-L/14\n85.7\u2020\n81.5\n90.1\n84.0\n99.7\n96.6\n78.7\n97.5\n94.3\n88.3\n90.1\nSynCLR\nsyn\nsyn\n600M\nViT-B/16\n80.7\n81.7\n93.8\n79.9\n99.1\n93.6\n76.2\n95.3\n91.6\n89.4\n89.0\nViT-L/14\n83.0\n85.6\n94.2\n82.1\n99.2\n94.1\n78.4\n96.1\n93.4\n90.3\n90.4\nTable 6. Comparison on ImageNet linear evaluation and fine-grained classificaton. SynCLR achieves comparable results with OpenAI\u2019s\nCLIP and DINO v2 models, despite only using synthetic data. *DINO v2 modes are distilled from a ViT-g model, thus advantageous in this\ncomparison. \u2020 we rerun only using cls token instead of concatenating multiple layers presented in the original DINO v2 paper [68].\nto the baseline StableRep, adding a teacher EMA model\nimproves the IN linear accuracy by 0.9%. Further adding\niBOT local objective or the multi-crop strategy increases the\naccuracy by 0.9% and 1.9%, respectively. Combining all\nof them results in our full SynCLR model, which achieves\n78.8% top-1 IN linear accuracy. The fine-grained classifica-\ntion performance follows a similar trend, and reaches 88.1%.\nBesides, we test the transfer ability to semantic segmenta-\ntion on ADE20k. The iBOT objective brings 1.0 more mIoU\nthan multi-crop strategy, demonstrating the effectiveness of\nmasked image modeling for dense prediction tasks.\nCompare to SimCLR and supervised training. We com-\npare the three different representation learning objectives\nshown in Figure 2, which classify images at different lev-\nels of granularity. Since supervised cross-entropy training\nrequires a fixed set of balanced classes (indeed both fixed\nset and balance are limitations of such method), we use\nthe IN+ourBG+LLM configuration where we have 1000\nbalanced classes (i.e., each class has 40k images). The su-\npervised training recipe follows [86]. For a fair compari-\nson with SimCLR, we remove all unmatched modules (i.e.,\nEMA, iBOT, and MC) to make sure that the only difference\nbetween SimCLR and our SynCLR is the classification gran-\nularity defined by the contrastive loss. For all of them, we\ndo pre-training and then linear probing on the target dataset.\nTable 5 presents the comparison. Our multi-positive ob-\njective, which defines images as the same class if they are\ngenerated by the same caption, achieves the best perfor-\nmance. It outperforms supervised cross-entropy training\nand SimCLR by 3.4% and 11.7% for top-1 accuracy on\nImageNet linear evaluation, and by 3.5% and 10.6% on fine-\ngrained classification tasks. Besides, our objective does not\nrequire balance between samples from a fixed set of classes,\nmaking it easier to scale up.\n4.2. Scaling up\nAfter we have ablated different components, we scale up our\nexperiments. Specifically, we synthesize a dataset of 150M\ncaptions, called SynCaps-150M, from which we generate\n600M images. We train both ViT-B/16 and ViT-L/14 (no\nSwiGLU [82] or LayerScale [92]), and extend the training\nschedules to 500k steps with a batch size of 8192 captions.\nWe use 224x224 resolution for all pre-training tasks.\nWe compare SynCLR with OpenAI\u2019s CLIP [71], Open-\nCLIP [17], and DINO v2 [68], which represent learning\nfrom data. We note that ViT-B/14 and ViT-L/14 from DINO\nv2 are distilled from a ViT-g [104] model, which makes\nDINO v2 advantageous in our comparison. We also includes\nStableRep [91], which uses the hybrid paradigm.\nImageNet linear evaluation. For fair comparison, cls\ntoken from the last block is used as representation across all\nmodels (whereas in DINO v2, results are from concatenating\nmultiple layers). As shown in Table 6, SynCLR achieves\n80.7% with ViT-B and 83.0% with ViT-L. This is similar\nas CLIP, but still lags behind DINO v2 by 3.2% and 2.7%,\nrespectively, partially because of the extra distillation in\nDINO v2. We note SynCLR has already outperformed other\nself-supervised methods pre-trained directly on ImageNet-\n1k (e.g., DINO achieves 78.2% with ViT-B/16 and iBOT\nreaches 81.0% with ViT-L/16).\nFine-grained classification.\nOn the nine fine-grained\ndatasets we have evaluated in Table 6, SynCLR achieves\nvery similar average accuracy as DINO v2, e.g., 89.0% v.s.\n89.0% for ViT-B, and 90.1% vs 90.4% for ViT-L. Both Syn-\nCLR and DINO v2 have curated the pre-training data to\ninclude the distribution for these datasets (but in different\nways and portions), and end up with similar performance.\nInterestingly, SynCLR outperforms others on Aircraft and\nCars, possibly because we favor more frequent sampling\n7\nmethod\npre-train data\ndistill\nViT-B\nViT-L\nStableRep\nhybrid, 100M\n49.4\n-\nMoCo v3\nreal, IN1K-1M\n47.3\n49.1\nBEiT\nreal, IN1K-1M+DALLE\n47.1\n53.3\nMAE\nreal, IN1K-1M\n48.1\n53.6\niBOT\nreal, IN1K-1M\n50.0\n-\nCLIP\nreal, WIT-400M\n52.6\n-\nBEiT v2\nreal, WIT-400M, IN1K\n\u2713\n53.1\n56.7\nDINO v2\nreal, LVD-142M\n\u2713\n54.4 \u2020\n57.5\u2020\nSynCLR\nsynthetic, 600M\n54.3\n57.7 \u2020\nTable 7. ADE20K semantic segmentation (mIoU) using UperNet,\nwith single scale at 512x512 resolution. \u2020 use patch size of 14x14,\nthus adapt to 518x518 resolution.\nmethod\npre-train data\nViT-B\nViT-L\nMoCo v3\nreal, IN1K-1M\n83.2\n84.1\nSimMIM\nreal, IN1K-1M\n83.8\n-\nMAE\nreal, IN1K-1M\n83.6\n85.9\nPeCo\nreal, IN1K-1M\n83.6\n85.9\ndata2vec\nreal, IN1K-1M\n84.2\n86.6\niBOT\nreal, IN21K-14M\n84.4\n86.6\nBEiT v2\nreal, WIT-400M+IN1k-1M\n85.5\n87.3\nCLIP\nreal, WIT-400M\n85.2\n87.5\u2020\nOpenCLIP\nreal, LAION-400M\n85.0\n86.6\u2020\nreal, LAION-2B\n-\n87.1\u2020\nSynCLR\nsynthetic, 600M\n85.8\n87.9\u2020\nTable 8. Top-1 accuracy on ImageNet with fine-tuning evalu-\nation.. Models are fine-tuned at 224x224 resolution. \u2020 use patch\nsize of 14x14.\ntowards them. This can be an advantage for synthetic data\nwhen we know what downstream tasks to solve. Besides,\nSynCLR outperforms CLIP and StableRep by 3.3% and by\n5.6% for ViT-B, respectively.\nSemantic segmentation. To evaluate the pixel-level under-\nstanding ability of SynCLR, we fine-tune the pre-trained\nmodels on ADE20k [109], following the setup in [5, 44].\nUperNet [99] is used as the task layer, and we evaluate with\na single-scale, i.e. 512x512. Besides CLIP and DINO v2,\nwe also compare to self-supervised methods pre-trained on\nImageNet, as well as BEiT v2 [70], which distills from CLIP.\nTable 7 shows that our SynCLR outperforms self-supervised\nmethods trained on IN-1k by a clear marge, e.g., 4.3 higher\nmIoU than iBOT. Despite not involving a high resolution pre-\ntraining period like DINO v2 (e.g., 518x518), SynCLR per-\nforms similarly with DINO v2 (0.1 lower for ViT-B possibly\nbecause DINO v2 uses a smaller patch size of 14x14, but\n0.2 higher for ViT-L). This suggests SynCLR pre-training is\nsuitable for dense prediction tasks.\nImageNet fine-tuning. We evaluate the fine-tuning transfer\nability of SynCLR on ImageNet. We compare with other\nEuroSAT\nGTSRB\nCountry211\nMNIST\nRESISC45\nKITTI\nAverage\nCLIP\nViT-B/16\n97.1 86.6 33.3 99.0 92.7 64.7\n78.9\nViT-L/14\n98.2 92.5 42.9 99.2 94.1 69.2\n82.7\nDINO v2\nViT-B/14\n96.0 72.8 21.6 98.6 92.5 75.3\n76.1\nViT-L/14\n96.7 74.1 24.1 98.2 93.8 76.9\n77.3\nSynCLR\nViT-B/16\n96.6 78.6 21.0 98.4 93.7 77.3\n77.6\nViT-L/14\n96.7 79.2 24.3 98.5 93.8 78.0\n78.4\nTable 9. Generalization to concepts not seen by DINO v2 and\nSynCLR. SynCLR outperforms DINO v2. CLIP achieves the\nbest accuracy, possibly because its training data includes similar\nconcepts as these datasets.\nSynCLR\nCLIP\nIN\navg.\nIN\navg.\nSynCaps-150M\n80.7\n89.0\n78.3\n87.7\nLaion-400M\n78.9\n86.5\n76.6\n84.9\nTable 10. Compare SynCLR with CLIP on the same synthetic\ndata. We observe that: (1) SynCLR outperforms CLIP; (2) in our\nsetup, i.e., generating 4 images per caption, SynCaps-150M yields\nbetter representations for both SynCLR and CLIP.\nstate of the art self-supervised methods [4, 5, 14, 27, 44, 100,\n110] in Table 8. Our SynCLR outperforms models trained on\nImageNet images or large scale image datasets. Specifically,\nSynCLR outperforms OpenCLIP ViT-L trained on Laion-2B,\nwhich is the dataset Stable Diffusion (the text2image model\nwe used) is trained on. This contrasts with [30, 78], which\nshows that directly training a classifier on synthetic images\nyields bad classification accuracy. Our finding suggests syn-\nthetic images are good for training representations, which\nlater can be easily adapted to a downstream task with limited\namount of real data.\n4.3. Further analysis\nSynCLR requires a list of concepts C to start off. But how\nwill SynCLR transfer to concepts outside our list?\nGeneralize to unseen concepts. We consider additional\ndatasets whose classes are outside the synthesis list, in-\ncluding EuroSAT [46], GTSRB [85], Country211 [71],\nMNIST [54], RESISC45 [16], and KITTI distances [35].\nThese datasets, except for KITTI, are also outside the cura-\ntion list of DINO v2. Therefore, it is also a generalization\ntest for DINO v2. Table 9 shows the linear probing results.\nSynCLR outperforms DINO v2 by 1.5% for ViT-B and 1.1%\nfor ViT-L, respectively. This suggests the representations of\nSynCLR generalize. CLIP outperforms SynCLR and DINO\nv2, with most gains coming from Country211. An explana-\ntion is CLIP\u2019s training data contains similar country flags\nwhich are not in the training sets of SynCLR and DINO v2.\n8\n(a)\nDino v2\nSynCLR (ours)\nDino v2\nSynCLR (ours)\nDino v2\nSynCLR (ours)\n(b)\n(c)\nFigure 5. PCA visualization. Follow DINO v2 [68], we compute a PCA between the patches of the images from the same set and colorize\nby their first 3 components. Compared to DINO v2, SynCLR produces more accurate maps for cars (e.g., zoom-in to see the two bars on the\nroof of the first car, and the three side windows of the third car) and airplanes (e.g., the boundaries), while being slightly worse for dogs (e.g.,\nheads). We use ViT-L/14 for both methods. Images are resized to 336x448 resolution before being fed into the networks, yielding 24x32\nvisualization grids.\n1\n3\n10\n40\n150\nNumber of Synthetic Captions (M)\n74\n76\n78\n80\n82\nImageNet Linear Acc. (%)\nViT-B/16\nViT-L/14\nFigure 6. ImageNet linear accuracy w/ different training scales.\n1\n3\n10\n40\n150\nNumber of Synthetic Captions (M)\n85\n86\n87\n88\n89\n90\nFine-grained Cls. Acc. (%)\nViT-B/16\nViT-L/14\nFigure 7. Fine-grained classification w/ different training scales.\nGiven that both captions and images are synthesized, a\nnatural question arises: how would CLIP training perform\non such data?\nCompare to CLIP training. We use the same data to train a\nViT-B CLIP model. For each caption, we randomly choose 1\nout of the 4 synthesized images in each iteration. Following\ncommon practice [71], we train for 32 epochs with a batch\nsize of 32768. This model achieves 44.4% zero-shot accu-\nracy on IN-1k. The SynCaps-150M row in Table 10 presents\nthe linear probing results. Synthetic CLIP learns reason-\nably good features, reaching 78.3% on IN-1k and 87.7% on\nfine-grained datasets. However, SynCLR is still better.\nWe have also repeated our experiments with Laion-400M\ncaptions, i.e., generate 4 images for each caption and train\nSynCLR and CLIP. The comparison between rows SynCaps-\n150M and Laion-400M in Table 10 suggests synthetic cap-\ntions are also favorable on a large scale.\nPCA visualization. Following the method used in DINO\nv2 [68], we present visualizations derived from the Principal\nComponent Analysis (PCA) conducted on patch features\nextracted using our model SynCLR. As depicted in Figure 5,\na comparative analysis is conducted between SynCLR and\nDINO v2, both utilizing the ViT-L/14 architecture. The\nresults demonstrate that SynCLR effectively accentuates the\nfeatures of cars and planes, while efficiently minimizing\nbackground clutter.\nScaling behavior. We train ViT-BViT-L models using ran-\ndom subsets of varying sizes: 1M, 3M, 10M, 40M, and the\ncomprehensive 150M (measured in the number of captions).\nThese models are trained over a reduced schedule of 300,000\nsteps and utilizes a smaller batch size of 2048. The outcomes\nof linear probing are illustrated in Figures 6 and 7. These\nresults indicate that the ViT-B model delivers robust perfor-\nmance at the 10M scale, with diminishing returns observed\nbeyond this point. In contrast, the ViT-L model exhibits a\ngreater demand for data (i.e., it underperforms ViT-B at the\n3M scale) and scales better with data.\n9\n5. Discussions and Conclusion\nWhy learn from generative models? One compelling rea-\nson is that a generative model can act like hundreds of\ndatasets simultaneously. Traditionally, researchers have to\nspend separate effort collecting datasets for different image\ncategories, e.g., cars, flowers, cats, dogs, and so on. DINO\nv2 [68] achieves robust representations by curating and amal-\ngamating numerous such datasets. Such a process introduces\ncomplexities such as clustering and search challenges. In\ncontrast, advanced text-to-image generative models like Sta-\nble Diffusion [72] or Imagen [77] have the capability to\ngenerate many diverse datasets. These models provide the\nflexibility to produce an infinite number of samples (albeit\nfinite diversity) and control the generation process through\ntextual input. Thus, generative models offer a convenient and\neffective method for curating training data. In our study, we\nharness this advantage to synthesize images encompassing a\nbroad spectrum of visual concepts.\nWhat can be further improved? Enhanced caption sets\ncan be achieved through various methods, such as enriching\nthe set of in-context examples, optimizing the sampling ra-\ntios among different concepts, and utilizing more advanced\nLLMs. In terms of the learning process, one approach is to\ndistill knowledge from a larger model, and incorporate an ad-\nditional high-resolution training phase (as discussed in [68])\nor an intermediate IN-21k fine-tuning stage (as per [5, 70]).\nRegarding architectural improvements, the integration of\nSwiGLU and LayerScale, coupled with superior model ini-\ntialization strategies (referenced in [32]), can be beneficial.\nHowever, due to limited resources and the scope of this\npaper not being focused on achieving the highest possible\nmetrics, we propose these areas for further exploration in\nfuture research endeavors.\nIn summary, this paper studies a new paradigm for visual\nrepresentation learning \u2013 learning from generative models.\nWithout using any real data, SynCLR learns visual represen-\ntations that are comparable with those achieved by state of\nthe art general-purpose visual representation learners.\nReferences\n[1] Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars\nMescheder, Andreas Geiger, and Carsten Rother.\nAug-\nmented reality meets computer vision: Efficient data genera-\ntion for urban driving scenes. IJCV, 2018. 3\n[2] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo-\njanowski, Florian Bordes, Pascal Vincent, Armand Joulin,\nMike Rabbat, and Nicolas Ballas. Masked siamese networks\nfor label-efficient learning. In ECCV, 2022. 3\n[3] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-\nhammad Norouzi, and David J Fleet. Synthetic data from\ndiffusion models improves imagenet classification. arXiv\npreprint arXiv:2304.08466, 2023. 2\n[4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Ji-\natao Gu, and Michael Auli. Data2vec: A general framework\nfor self-supervised learning in speech, vision and language.\nIn ICML, 2022. 3, 8\n[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021. 3, 8, 10, 14\n[6] Suzanna Becker and Geoffrey E Hinton. Self-organizing\nneural network that discovers surfaces in random-dot stere-\nograms. Nature, 1992. 3\n[7] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101\u2013mining discriminative components with random\nforests. In ECCV, 2014. 5, 6\n[8] Emmanuel Asiedu Brempong, Simon Kornblith, Ting Chen,\nNiki Parmar, Matthias Minderer, and Mohammad Norouzi.\nDenoising pretraining for semantic segmentation. In CVPR,\n2022. 3\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. NeurIPS, 2020. 3\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS, 2020. 5\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9-\ngou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn ICCV, 2021. 3, 5, 6, 14\n[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In CVPR,\n2021. 6\n[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 2, 3, 5, 15\n[14] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised vision transformers. In\nICCV, 2021. 5, 6, 8, 14\n[15] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.\nLearning semantic segmentation from synthetic data: A\ngeometrically guided input-output adaptation approach. In\nCVPR, 2019. 3\n[16] Gong Cheng, Junwei Han, and Xiaoqiang Lu.\nRemote\nsensing image scene classification: Benchmark and state of\nthe art. Proceedings of the IEEE, 2017. 8\n[17] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible\nscaling laws for contrastive language-image learning. In\nCVPR, 2023. 7\n[18] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In CVPR, 2014. 5, 6\n[19] Kevin Clark and Priyank Jaini.\nText-to-image diffu-\nsion models are zero-shot classifiers.\narXiv preprint\narXiv:2303.15233, 2023. 3\n10\n[20] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-\npher D Manning.\nElectra: Pre-training text encoders\nas discriminators rather than generators. arXiv preprint\narXiv:2003.10555, 2020. 14\n[21] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR workshops, 2020. 14\n[22] Marco Cuturi. Sinkhorn distances: Lightspeed computation\nof optimal transport. In NeurIPS, 2013. 5\n[23] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion\nparameters. In ICML, 2023. 3\n[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 3, 5\n[25] Jeff Donahue and Karen Simonyan. Large scale adversarial\nrepresentation learning. NeurIPS, 2019. 3\n[26] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,\nNing Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep\nconvolutional activation feature for generic visual recogni-\ntion. In ICML, 2014. 3\n[27] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,\nWeiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai\nYu, and Baining Guo. Peco: Perceptual codebook for bert\npre-training of vision transformers. In AAAI, 2023. 8\n[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 2, 6\n[29] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 2010. 6\n[30] Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi,\nPhillip Isola, and Yonglong Tian. Scaling laws of synthetic\nimages for model training ... for now. arXiv:2312.04567,\n2023. 2, 3, 8\n[31] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and\nYonglong Tian.\nImproving clip training with language\nrewrites. In NeurIPS, 2023. 3\n[32] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-\nlong Wang, and Yue Cao. Eva-02: A visual representation\nfor neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n10\n[33] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.\nEva: Exploring the limits of masked visual representation\nlearning at scale. In CVPR, 2023. 3\n[34] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories.\nIn CVPR, 2004. 5, 6\n[35] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, 2012. 8\n[36] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In ICLR, 2018. 2\n[37] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014. 3\n[38] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual\nrepresentation learning. In ICCV, 2019. 1\n[39] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. In NeurIPS, 2020. 3, 5, 14, 15\n[40] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-\nality reduction by learning an invariant mapping. In CVPR,\n2006. 3\n[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 3\n[42] Kaiming He, Ross Girshick, and Piotr Doll\u00e1r. Rethinking\nimagenet pre-training. In ICCV, 2019. 3\n[43] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 3, 5, 14\n[44] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll\u00e1r, and Ross Girshick. Masked autoencoders are\nscalable vision learners. In CVPR, 2022. 3, 8, 14\n[45] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic\ndata from generative models ready for image recognition?\narXiv preprint arXiv:2210.07574, 2022. 2, 3\n[46] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth. Eurosat: A novel dataset and deep learning\nbenchmark for land use and land cover classification. IEEE\nJournal of Selected Topics in Applied Earth Observations\nand Remote Sensing, 2019. 8\n[47] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV,\n2016. 14\n[48] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.\nGenerative models as a data source for multiview represen-\ntation learning. arXiv preprint arXiv:2106.05258, 2021. 2,\n3\n[49] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In ICML, 2021.\n3\n[50] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan. Supervised contrastive learning. In NeurIPS,\n2020. 2, 4\n[51] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei.\nCollecting a large-scale dataset of fine-grained cars. tech\nreport, 2013. 5, 6\n11\n[52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012. 3\n[53] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data\naugmentation using pre-trained transformer models. arXiv\npreprint arXiv:2003.02245, 2020. 3\n[54] Yann LeCun. The mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/, 1998. 8\n[55] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier.\narXiv preprint arXiv:2303.16203,\n2023. 3\n[56] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang,\nDina Katabi, and Dilip Krishnan. Mage: Masked generative\nencoder to unify representation learning and image synthesis.\nIn CVPR, 2023. 3\n[57] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaim-\ning He, and Ross Girshick.\nBenchmarking detection\ntransfer learning with vision transformers. arXiv preprint\narXiv:2111.11429, 2021. 3\n[58] Hao Liu, Tom Zahavy, Volodymyr Mnih, and Satinder Singh.\nPalm up: Playing in the latent manifold for unsupervised\npretraining. arXiv preprint arXiv:2210.10913, 2022. 3\n[59] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 14,\n15\n[60] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. Fine-grained visual clas-\nsification of aircraft. arXiv:1306.5151, 2013. 5, 6\n[61] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In CVPR, 2016. 3\n[62] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Gener-\nating training data with language models: Towards zero-shot\nlanguage understanding. arXiv preprint arXiv:2202.04538,\n2022. 3\n[63] Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke\nSakai, and Tatsuya Kawahara.\nLeveraging sequence-to-\nsequence speech synthesis for enhancing acoustic-to-word\nspeech recognition. In SLT, 2018. 3\n[64] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes.\nIn\nIndian Conference on Computer Vision, Graphics & Image\nProcessing, 2008. 5, 6\n[65] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV,\n2016. 3\n[66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 3\n[67] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 3, 4\n[68] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 1, 2, 3, 5, 7, 9, 10,\n15\n[69] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In CVPR, 2012. 5, 6\n[70] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu\nWei. Beit v2: Masked image modeling with vector-quantized\nvisual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\n8, 10\n[71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In ICML, 2021. 1, 2, 3, 7, 8, 9\n[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 10\n[73] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 3\n[74] Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye\nJia, Pedro Moreno, Yonghui Wu, and Zelin Wu. Speech\nrecognition with augmented synthesized speech. In ASRU,\n2019. 3\n[75] Nick Rossenbach, Albert Zeyer, Ralf Schl\u00fcter, and Hermann\nNey. Generating synthetic audio data for attention-based\nspeech recognition systems. In ICASSP, 2020. 3\n[76] Yangjun Ruan, Saurabh Singh, Warren Morningstar, Alexan-\nder A Alemi, Sergey Ioffe, Ian Fischer, and Joshua V Dillon.\nWeighted ensemble self-supervised learning. arXiv preprint\narXiv:2211.09981, 2022. 5\n[77] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with\ndeep language understanding. In NeurIPS, 2022. 10\n[78] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and\nYannis Kalantidis. Fake it till you make it: Learning trans-\nferable representations from synthetic imagenet clones. In\nCVPR, 2023. 2, 3, 8\n[79] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek\nKar, Mohammad Norouzi, Deqing Sun, and David J Fleet.\nThe surprising effectiveness of diffusion models for opti-\ncal flow and monocular depth estimation. arXiv preprint\narXiv:2306.01923, 2023. 3\n[80] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. In NeurIPS, 2022. 1\n[81] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,\nand Stefan Carlsson. Cnn features off-the-shelf: an astound-\ning baseline for recognition. In CVPR workshops, 2014.\n3\n[82] Noam Shazeer. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020. 7\n[83] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis\nAntonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lu-\n12\ncas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the\ngame of go without human knowledge. Nature, 2017. 3\n[84] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 3\n[85] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and\nChristian Igel. The german traffic sign recognition bench-\nmark: a multi-class classification competition. In IJCNN,\n2011. 8\n[86] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\nyour vit? data, augmentation, and regularization in vision\ntransformers. arXiv preprint arXiv:2106.10270, 2021. 7\n[87] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto.\nAlpaca:\nA strong, replicable instruction-\nfollowing model. Stanford Center for Research on Founda-\ntion Models., 2023. 3\n[88] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\ntrastive multiview coding. arXiv:1906.05849, 2019. 3, 14\n[89] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola. What makes for good\nviews for contrastive learning? In NeurIPS, 2020. 3\n[90] Yonglong Tian, Olivier J Henaff, and A\u00e4ron van den Oord.\nDivide and contrast: Self-supervised learning from uncu-\nrated data. In ICCV, 2021. 1\n[91] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and\nDilip Krishnan. Stablerep: Synthetic images from text-to-\nimage models make strong visual representation learners. In\nNeurIPS, 2023. 1, 2, 3, 4, 5, 6, 7, 14\n[92] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with\nimage transformers. In ICCV, 2021. 7\n[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3, 4\n[94] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-\nmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.\nLearning from synthetic humans. In CVPR, 2017. 3\n[95] Tongzhou Wang and Phillip Isola. Understanding contrastive\nrepresentation learning through alignment and uniformity\non the hypersphere. In ICML, 2020. 3\n[96] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In CVPR, 2022.\n3\n[97] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 3\n[98] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene\nrecognition from abbey to zoo. In CVPR, 2010. 5, 6\n[99] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understanding.\nIn ECCV, 2018. 8, 14\n[100] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In CVPR, 2022. 3,\n8\n[101] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models. In\nCVPR, 2023. 3\n[102] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha\nSwayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bha-\ngavatula, Yejin Choi, and Doug Downey. Generative data\naugmentation for commonsense reasoning. arXiv preprint\narXiv:2004.11546, 2020. 3\n[103] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In ICCV, 2019. 14\n[104] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and\nLucas Beyer. Scaling vision transformers. In CVPR, 2022.\n7\n[105] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 14\n[106] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In ECCV, 2016. 2\n[107] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,\nJie Zhou, and Jiwen Lu.\nUnleashing text-to-image dif-\nfusion models for visual perception.\narXiv preprint\narXiv:2303.02153, 2023. 3\n[108] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimi-\nnative localization. In CVPR, 2016. 3, 5\n[109] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja\nFidler, Adela Barriuso, and Antonio Torralba. Semantic\nunderstanding of scenes through the ade20k dataset. IJCV,\n2019. 8, 14\n[110] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. arXiv preprint arXiv:2111.07832,\n2021. 2, 3, 5, 8\n[111] Yongchao Zhou, Hshmat Sahak, and Jimmy Ba. Training on\nthin air: Improve image classification with generated data.\narXiv preprint arXiv:2305.15316, 2023. 3\n13\nA. Concept Sampling\nThe concepts used to synthesize captions are randomly sam-\npled from the names of various datasets. The rough ratios\nare presented in Table 11. It is likely that different combina-\ntions of these ratios lead to different results, but we do not\noptimize over this dimension. For example, we simply con-\ncatenate IN-21k concepts with the classes of other datasets\n(e.g., Caltech-101, Pets), and do uniform sampling from the\nconcatenated list. This may lead to under-sampling for other\ndatasets, as the list is dominated by IN-21 classes.\nsource\nprob.\nIN-1k\n0.47\nAircraft\n0.05\nCars\n0.05\nFood\n0.05\nFlowers\n0.03\nPlaces-365, SUN397\n0.09\nIN-21k and others\n0.26\nTable 11. Rough concept sampling probabilities.\nB. Implementation Details\nB.1. Pre-training\nThe setting for our final long schedule training in Section\n4.2 is summarized in Table 12, where models are trained for\n500k steps with a batch size of 8192 captions. For ablation\nstudy present in Section 4.1, we only train for 85k steps with\na batch size of 2048 captions; for the scaling plots in Section\n4.3, we train all models for 300k steps with a batch size of\n2048.\nconfig\nvalue\nbatch size\n8192\noptimizer\nAdamW [59]\npeak learning rate\n2e-3 (B), 1.5e-3 (L)\nweight decay\n0.04 \u2013> 0.2, cosine\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nlearning rate schedule\ncosine decay\nsteps\n500k\nwarmup steps\n80k\nstoch. depth [47]\n0.1 (B), 0.4 (L)\naugmentation\nDownsample [91] + BYOL Aug. [39]\nTable 12. SynCLR pre-training settings.\nB.2. ImageNet linear probing\nWe use the cls token from the final transformer block as\nthe image representation. This is different from DINO v2,\nwhich tries to concatenate cls token with average pooled\npatch tokens and sweep over whether to use multiple layers.\nWe follow prior work [11, 14] to train the linear classifier.\nIt has been generally observed that regularization such as\nweight decay hurts the performance [43, 88]. Therefore,\nwe set weight decay as 0, and we sweep the base_lr over\n{0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50} \u00d7 10\u22122.\nconfig\nvalue\nbatch size\n1024\noptimizer\nSGD\nbase learning rate\nsweep\npeak learning rate\nblr \u00d7 bsz/256\nweight decay\n0\noptimizer momentum\n0.9\nlearning rate schedule\ncosine decay\nepochs\n90\naugmentation\nRandomResizedCrop, Flip\nTable 13. ImageNet linear probing settings.\nB.3. End-to-End ImageNet fine-tuning\nFollowing common practice [5, 44], we append a linear\nclassifier on top of the CLS token of the last transformer\nblock, and fine-tune the whole network. We use layer-wise\nlr decay [20]. Table 14 shows the settings.\nconfig\nvalue\noptimizer\nAdamW [59]\nbase learning rate\n5e-5\npeak learning rate\nblr \u00d7 bsz/256\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nlayer-wise lr decay\n0.65 (B), 0.8 (L)\nbatch size\n1024\nlearning rate schedule\ncosine decay\nwarmup epochs\n20 (B), 5 (L)\nepochs\n100 (B), 50 (L)\nRandAugment [21]\n9/0.5\nlabel smoothing\n0.1 (B), 0.2 (L)\nerasing prob.\n0.25\nmixup [105]\n0.8\ncutmix [103]\n1.0\nstoch. depth [47]\n0.1 (B), 0.3 (L)\ntest crop ratio\n0.95 (B), 1.0 (L)\nema\n0.9999\nTable 14. ImageNet end-to-end fine-tuning settings.\nB.4. Semantic segmentation on ADE20k\nWe conduct the experiments on ADE20k [109]. Follow-\ning [5, 44], we use UperNet [99] as the task adaptation layer.\nWe use the common single-scale [5] setup, with a resolution\n14\nof 512\u00d7512 for models with a patch size of 16\u00d716 and a res-\nolution of 518\u00d7518 for models with a patch size of 14\u00d714.\nThe hyper-parameters are summarized in Table 15.\nconfig\nvalue\nbatch size\n32 (B), 16 (L)\noptimizer\nAdamW [59]\npeak learning rate\n8e-5\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nweight decay\n0.05\nlayer-wise lr decay\n0.6 (B), 0.8 (L)\nsteps\n60k (B), 160k (L)\nwarmup steps\n1500\nstoch. depth\n0.1 (B), 0.2 (L)\nTable 15. ADE20k semantic segmentation settings.\nB.5. Fine-grained linear classification\nFollowing prior works [13, 39], we train a regularized multi-\nnomial logistic regression model upon the output CLS to-\nken. In training and testing, we do not perform any data\naugmentation; images are resized to 224 pixels along the\nshorter side, followed by a center crop of 224\u00d7224. We\nminimize the cross-entropy objective using L-BFGS with\n\u21132-regularization. We select this \u21132-regularization constant\non the validation set over 45 logarithmically spaced values\nbetween 10\u22126 and 105. The maximum number of L-BFGS\niterations is set to 1000, similar as that in DINO v2 [68].\nC. In-context Learning Examples\nAll of the three types of in-context examples are summarized\nin Table 16, Table 17, and Table 18, respectively.\n15\nTable 16. Detailed in-context learning examples for Template 1: c \u2013> Caption. Here c is the concept.\n1\ncoucal\n\u2013> A vibrant coucal is perched on the branch of a lush green tree, surrounded by wildflowers.\n2\nbee eater\n\u2013> A lively bee eater is elegantly perched on a branch, peering intently.\n3\nthree-toed sloth\n\u2013> A three-toed sloth is lazily hanging from a sturdy, tropical rainforest tree.\n4\nhay\n\u2013> In the serene countryside, hundreds of neatly stacked hay bales lay scattered under the\nsoftly glowing golden sunset sky.\n5\nstation wagon\n\u2013> A shiny, red station wagon is parked under the dappled shade of a large oak tree,\nhighlighting its spacious and family-friendly design.\n6\nzebra\n\u2013> A zebra is gallantly trotting across the vast, sunlit plains of the African savannah, creating\na captivating black and white spectacle.\n7\nvase\n\u2013> In the well-lit living room, a beautifully designed, delicate vase stands out as the center-\npiece, exuding an aura of elegance.\n8\nbarber chair\n\u2013> A shiny black barber chair sits invitingly in a bustling, well-lit barbershop.\n9\ncarbonara\n\u2013> A heaping plate of creamy carbonara pasta topped with fresh parsley sprigs.\n10\nmink\n\u2013> In the midst of a dense forest with shimmering green leaves, a sleek mink gracefully\nnavigates the underbrush, showcasing its rich, brown fur.\n11\nsmall white butterfly \u2013> A small white butterfly gracefully flutters amongst vibrant, blooming summer flowers.\n12\nchristmas stocking\n\u2013> A vibrant red Christmas stocking is hanging delicately from a festively decorated man-\ntelpiece.\n13\nhorse-drawn vehicle \u2013> An antique horse-drawn vehicle is stationed amidst a peaceful country landscape, its\nrustic wooden structure gleaming under the warm afternoon sun.\n14\nruler measuring stick \u2013> A manual craftsman is precisely measuring a wooden log with a ruler stick.\n15\npicket fence\n\u2013> A tranquil suburban scene featuring multiple white picket fences surrounding well-\nmaintained green lawns, punctuated by diverse, colorful flowerbeds.\n16\nsuspension bridge\n\u2013> Depicting a long suspension bridge, its steel cables elegantly stretching towards the sky,\nconnecting two ends over a scenic river.\n17\nbrain coral\n\u2013> A vibrant brain coral stands out amidst the serene backdrop of underwater marine life.\n18\nrevolver\n\u2013> Multiple antique revolvers lie on a wooden table, gleaming under soft, ambient light.\n19\nslip-on shoe\n\u2013> A pair of slip-on shoes, with their sleek, black leather exterior and comfortable, cushioned\ninterior, are neatly placed on a wooden floor.\n20\nhand-held computer \u2013> A hand-held computer, compact and portable, rests on a well-lit desk, surrounded by\nvarious technological paraphernalia and a steaming cup of coffee.\n21\nmattress\n\u2013> A teddy bear lying face down on a bedspread covered mattress in front of a window.\n22\nrefrigerator\n\u2013> A nicely decorated kitchen with metallic refrigerator and blue counter.\n23\nball\n\u2013> Silver balls are lined up in the sand as people mill about in the background.\n24\nwheel\n\u2013> The motorcycle\u2019s gleaming steering wheel, vivid red door reflected in the side mirror,\nand a youth passing by, creating a dynamic urban tableau.\n25\nplane\n\u2013> A group of trick planes turned upside down leaving smoke trails.\n26\nvehicle\n\u2013> Army vehicles, including a U.S. Army jeep and aircraft in a hangar or on display\n27\nboy\n\u2013> a little boy wearing sunglasses laying on a shelf in a basement.\n28\nfence\n\u2013> a man standing near a fence as reflected in a side-view mirror of a red car.\n29\nwood table\n\u2013> A footed glass with water in front of a glass with ice tea, and green serpentine bottle\nwith pink flowers, all on a wood table in front of chair, with a window to city view.\n30\ntoilet\n\u2013> A black and white toilet sitting in a bathroom next to a plant filled with waste.\n31\ntable lamp\n\u2013> A textured brass table lamp, casting a warm, golden glow, accents a cozy reading nook\nbeside a leather armchair and a stack of books.\n32\nhair dryer\n\u2013> A modern sleek and white hair dryer, with a textured grip, stands next to a set of\nhairbrushes.\n33\nstreet sign\n\u2013> The street signs indicate which way a car can and cannot turn while the signal light\ncontrols traffic.\n34\ninstrument\n\u2013> Man dressed in Native American clothes protecting musical instruments from the rain\nwith an umbrella.\n16\n35\ntrain\n\u2013> A man and a cow\u2019s faces are near each other as a train passes by on a bridge.\n36\ngiraffe\n\u2013> A couple of large giraffe standing next to each other.\n37\nred admiral butterfly \u2013> a red admiral butterfly, alights upon a dew-kissed sunflower, wings glistening under the\nsoft morning light.\n38\nstupa\n\u2013> Surrounded by verdant foliage, a white stupa rises, adorned with golden accents and\nintricate patterns, while devotees circle its base offering prayers.\n39\nelephant\n\u2013> A group of elephants being led into the water.\n40\nbottle\n\u2013> Motorcycles parked on a street with a bottle sitting on the seat of the nearest the camera.\n41\ntrombone\n\u2013> On a polished wooden stage, a gleaming brass trombone rests, its slide extended, next to\nscattered sheet music and a muted trumpet.\n42\nkeyboard\n\u2013> Sleek black keyboard with illuminated backlit keys, a soft wrist rest, and a nearby\nwireless mouse on a textured matte desk surface.\n43\nbear\n\u2013> The brown bear sits watching another bear climb the rocks\n44\nsnowboard\n\u2013> A man standing next to his snowboard posing for the camera.\n45\nrailway\n\u2013> a woman and her son walking along the tracks of a disused railway.\n46\nsand\n\u2013> the waves and the sand on the beach close up\n47\npixel\n\u2013> very colorful series of squares or pixels in all the colors of the spectrum , from light to\ndark\n48\ncigar\n\u2013> a burning cigar in a glass ashtray with a blurred background.\n49\nmusic\n\u2013> happy girl listening music on headphones and using tablet in the outdoor cafe.\n50\nearring\n\u2013> this gorgeous pair of earrings were featured in april issue.\n51\ncliff\n\u2013> Steep cliff, jagged edges against azure sky, with seabirds soaring and waves crashing\nbelow.\n52\ncorn cob\n\u2013> Fresh corn cob, golden kernels glistening with dew, nestled amid green husks in a sunlit\nfield.\n53\narchaeological exca-\nvation\n\u2013> In this intriguing scene, archaeologists meticulously uncover ancient relics at an archaeo-\nlogical excavation site filled with historical secrets and enigmas.\n54\nformal garden\n\u2013> This is an immaculately kept formal garden, with perfectly trimmed hedges, colorful,\nwell-arranged flower beds, and classic statuary, giving a vibe of tranquil sophistication.\n55\nveterinarians office\n\u2013> The busy veterinarian\u2019s office is a hive of activity with pets awaiting treatment and care.\n56\nelevator\n\u2013> A modern, well-lit elevator interior with shiny metal walls and sleek buttons.\n57\nheliport\n\u2013> Situated in a lively area, the heliport stands out with numerous helicopters taking off and\nlanding against the city\u2019s skyline.\n58\nairport terminal\n\u2013> In the spacious airport terminal, travelers hurriedly navigate through check-ins and\nsecurity, making it a hive of constant activity.\n59\ncar interior\n\u2013> Inside the car, the leather seats exude luxury, contrasted by the high-tech dashboard,\ncreating an atmosphere of sleek comfort and convenience.\n60\ntrain interior\n\u2013> The inside of the train offers a spacious setting with numerous comfortable seats.\n61\ncandy store\n\u2013> The sweet aroma of sugared treats fills the air in a vibrant candy store, adorned with\ncolourful candies and cheerful customers.\n62\nbus station\n\u2013> The bustling bus station thrums with restless energy, as travelers navigate through the\ncrowded space, awaiting their journeys amid the echoes of departing buses.\n63\ncastle\n\u2013> Nestled amidst towering mountains, the majestic castle spews ancient grandeur, with its\nstone walls and towering turrets exuding tranquility and timeless mystique.\n64\npalace\n\u2013> The grand palace exudes regality, radiant under the sun, showcasing ornate decorations,\nintricate sculptures, and exquisite architectural sophistication.\n65\nkitchen\n\u2013> The heart of the home unfolds in the kitchen, characterized by stainless steel appliances,\nnavy blue cabinets, and a patterned tile backsplash.\n66\nraceway\n\u2013> The high-speed adrenaline-filled atmosphere of the raceway is pulsing with the roars of\npowerful engines and excited cheering fans.\n67\nbakery\n\u2013> The warm, inviting bakery is filled with the intoxicating aroma of fresh bread, assorted\npastries, and brewing coffee.\n17\n68\nmedina\n\u2013> This ancient, labyrinth-like medina exudes an air of mystique with its vibrantly decorated\nshops lining narrow, stone-cobbled pathways.\n69\nskyscraper\n\u2013> The city skyline is dominated by towering skyscrapers, creating a captivating blend of\ntechnology and architectural innovation.\n70\nsupermarket\n\u2013> The supermarket scene is lively, filled with individuals scanning shelves, children reach-\ning for treats, and clerks restocking fresh produce.\n71\ncloset\n\u2013> The compact closet, brimming with clothes and shoes, exudes a feeling of organization.\n72\nassembly line\n\u2013> In the heart of a busy factory, an orderly assembly line hums with continuous activity,\nfilled with workers focused on their precision tasks.\n73\npalace room\n\u2013> A man in military dress uniform stands in an ornate palace room with antique furniture\nand Christmas decorations.\n74\nbarn doorway\n\u2013> A farmer holding an animal back while another farmer stands in a barn doorway.\n75\nfood court\n\u2013> A bustling food court with a variety of culinary stalls, featuring vibrant signage, aromatic\ndishes, and communal seating, creates a diverse dining experience.\n76\nmountain\n\u2013> Majestic mountains, their peaks dusted with snow, overlook a serene alpine lake where\nhikers and photographers gather to enjoy the breathtaking scenery.\n77\nsquash court\n\u2013> Against a clear glass wall, a squash court with gleaming wooden floors, white boundary\nlines, and two rackets awaits players.\n78\nsubway station\n\u2013> Dimly lit subway station with graffiti-covered walls, commuters waiting\n79\nrestaurant\n\u2013> Cozy restaurant with wooden tables, ambient lighting, patrons chatting, and plates filled\nwith colorful dishes, framed by exposed brick walls and hanging green plants.\n80\nfield\n\u2013> there is a large heard of cows and a man standing on a field.\n81\naquarium\n\u2013> Amidst vivid coral formations, an aquarium teems with colorful fish, shimmering under\nsoft blue lights.\n82\nmarket\n\u2013> A large group of bananas on a table outside in the market.\n83\npark\n\u2013> a young boy is skating on ramps at a park\n84\nbeach\n\u2013> old fishing boats beached on a coastal beach in countryside.\n85\ngrass\n\u2013> little boy sitting on the grass with drone and remote controller.\n86\nwoven\n\u2013> The woven basket\u2019s intricate pattern creates a visually captivating and tactile surface.\n87\nknitted\n\u2013> The knitted blanket envelops with cozy warmth\n88\nflecked\n\u2013> The stone surface was flecked, giving it a uniquely speckled and rough appearance.\n89\nbubbly\n\u2013> The liquid gleamed, showcasing its bubbly, effervescent texture vividly.\n90\ncobwebbed\n\u2013> The dusty corner was cobwebbed, displaying years of untouched, eerie beauty.\n91\nstained\n\u2013> A weather-worn wall manifests an intriguing pattern of stained texture.\n92\nscaly\n\u2013> The image showcases a close-up of a lizard\u2019s scaly, rough texture.\n93\nmeshed\n\u2013> A patterned image depicting the intricate, tightly-knit texture of meshed fabric.\n94\nwaffled\n\u2013> A fresh, golden-brown waffle displays its distinct crisply waffled texture invitingly.\n95\npitted\n\u2013> The image portrays an intriguing terrain, characterized by a pitted, moon-like surface.\n96\nstudded\n\u2013> A studded leather jacket gleams, highlighting its rough, tactile texture.\n97\ncrystalline\n\u2013> The picture showcases an exquisite, crystalline texture with stunning brilliance and\nclarity.\n98\ngauzy\n\u2013> A delicate veil of gauzy texture enhances the ethereal, dreamy atmosphere.\n99\nzigzagged\n\u2013> The photo captures the zigzagged texture, emphasizing the rhythmic, sharp-edged pat-\nterns.\n100\npleated\n\u2013> A flowing skirt delicately showcasing the intricate detail of pleated texture.\n101\nveined\n\u2013> A detailed image showcasing the intricate, veined texture of a leaf.\n102\nspiralled\n\u2013> The spiralled texture of the seashell creates a captivating, tactile pattern.\n103\nlacelike\n\u2013> The delicate veil features an intricate, lacelike texture, exuding elegant sophistication.\n104\nsmeared\n\u2013> A wall coated with thick, smeared paint exudes a rough texture.\n105\ncrosshatched\n\u2013> A worn, vintage book cover, richly crosshatched, exuding old-world charm.\n106\nparticle\n\u2013> abstract background of a heart made up of particles.\n18\nTable 17. Detailed in-context learning examples for Template 2: c,bg \u2013> caption. Here c is the concept, and bg is the background.\n107\nstick insect, under-\ngrowth\n\u2013> A stick insect, masterfully camouflaged, clings to a fern amidst the sprawling, dense\nundergrowth of a lush, tropical forest.\n108\nblack swan, public\ngarden\n\u2013> In the peaceful ambiance of a lush public garden, a majestic black swan gracefully glides\nacross a shimmering emerald-green pond.\n109\nst.\nbernard, family-\nphoto\n\u2013> In the heartwarming family photo, a gregarious St. Bernard dog is seen joyfully nestled\namong his adoring human companions.\n110\nmeasuring cup, food\nprep area\n\u2013> In the food prep area, multiple transparent measuring cups are neatly organized on the\nmarble countertop.\n111\ncan\nopener,\nhotel\nroom\n\u2013> A sleek, stainless steel can opener is sitting on the glossy dark-wood kitchenette counter\nof a modern, well-appointed hotel room.\n112\nsmall white butterfly,\npond side\n\u2013> A delicate, small white butterfly flutters gracefully above the tranquil pond side, creating\na serene image amidst lush greenery.\n113\nhair dryer, theatre\n\u2013> A sleek, professional hair dryer is positioned center stage amidst the dramatic velvet\ncurtains and ornate details of a bustling theatre.\n114\nwater bottle, airport \u2013> A reusable water bottle sits on the glossy surface of a bustling airport terminal counter,\namidst a backdrop of hurried travelers and departure screens.\n115\nleonberger,\nhorse\nranch\n\u2013> Several Leonbergers are joyfully romping around a bustling horse ranch.\n116\nlighter, motorhome\n\u2013> In the cozy, cluttered environment of a well-traveled motorhome, a sleek silver lighter\nholds dominion on the rustic wooden table.\n117\nslug, foliage\n\u2013> A solitary, glistening slug meanders slowly amidst lush, dense green foliage, leaving a\nslimy trail on dewy leaves in its path.\n118\nring binder, educa-\ntion department\n\u2013> The ring binder, filled with important documents, sits prominently on a well-organized\ndesk in the bustling education department.\n119\nweimaraner, pet store \u2013> A sleek, silver-gray Weimaraner is spotted curiously sniffing around various pet supplies\nin a well-stocked and vibrant pet store.\n120\nnorfolk terrier, coun-\ntryside\n\u2013> A lively Norfolk terrier joyfully bounds across a lush, green countryside, its red fur\ncontrasting vividly with the vast open surroundings.\n121\ndalmatian, apple or-\nchard\n\u2013> A lively Dalmatian is playfully darting amongst the lush rows of a bountiful apple\norchard, its spots contrasting against the ruby fruits.\n122\ntelevision, mountain\nlodge\n\u2013> A sleek, modern television sits prominently against the rustic, wooden walls of an\ninviting mountain lodge, surrounded by pine-furnished decor.\n123\nguillotine,\nhorror\nstory\n\u2013> In the shadowy landscape of a suspenseful horror story, a grim, menacing guillotine\nlooms ominously, exuding a petrifying sense of imminent dread.\n124\nhot\ntub,\ncondo-\nminium\n\u2013> A luxurious hot tub is nestled in the private balcony of a high-rise condominium, boasting\nspectacular cityscape views.\n125\nleaf beetle, plant nurs-\neries\n\u2013> A vibrant leaf beetle is diligently navigating through a lush plant nursery, its metallic\nsheen contrasting against the abundant green foliage.\n126\ncarolina anole, hiking\ntrails\n\u2013> A small Carolina Anole lizard basks in the warm sunlight, gracefully draped over a\ngnarled tree root next to a bustling hiking trail.\n127\ngirl, laboratory\n\u2013> teenage girl and boy working in a laboratory on an experiment.\n128\ntiger, forest\n\u2013> Two tigers are running together in the forest.\n129\nsunset, lake\n\u2013> Golden sunset hues reflect on a calm lake, silhouetting a lone canoeist against a backdrop\nof fiery clouds.\n130\nbuilding, mountain\n\u2013> town of skyline over roofs of historic buildings with the mountains in the background.\n131\nblock plane, weath-\nered wood\n\u2013> A block plane, its sharp blade gleaming, rests on weathered wood\n132\nolive tree, soil\n\u2013> single olive tree planted in the center of a dry and cracked soil\n133\nhamster, pet store\n\u2013> A curious hamster peers out, with pet store shelves stacked with supplies behind.\n134\nbag, factory\n\u2013> plastic bags production line in a factory.\n19\n135\nrestaurant, ocean\n\u2013> young pretty couple dining in a romantic atmosphere at restaurant on the boat with ocean\non the background\n136\nhelicopter,\nburning\nforest\n\u2013> a helicopter flies over a portion of burning forest.\n137\npipe organ, commem-\noration event\n\u2013> striking pipe organ dominates with its notes resonating, while a somber commemoration\nevent unfolds in the backdrop\n138\nrotisserie,\nwedding\nreception\n\u2013> Rotisserie turning golden meats, with a bustling wedding reception, twinkling lights, and\nguests mingling.\n139\nduck, taiga\n\u2013> A group of ducks paddle on a tranquil pond, dense taiga and towering conifers looming\nin the background.\n140\ntiger\nbeetle,\nrice\nfields\n\u2013> Amidst verdant rice fields, a shimmering tiger beetle perches prominently on a dew-\nkissed blade of grass.\n141\ngirl, barn\n\u2013> slow motion clip of a girl walking with her horse through a barn\n142\nheadmaster, gradua-\ntion ceremony\n\u2013> the headmaster addresses the graduating seniors during graduation ceremonies.\n143\nbusinessperson, mu-\nsic festival\n\u2013> businessperson and guest attend music festival.\n144\nfountain, park\n\u2013> Water cascades from an ornate fountain, surrounded by autumn-hued trees in a serene\npark.\n145\nspeedboat, water\n\u2013> A sleek speedboat glides on shimmering waters, powered by twin high-horsepower\noutboard motors.\n146\npipe, beach\n\u2013> a rusty water pipe on the beach.\n147\npretzel, home kitchen \u2013> Golden pretzel rests on a wooden board, with a cozy home kitchen, pots and tiled\nbacksplash, behind.\n148\nforklift, paper mill\n\u2013> A forklift transports hefty paper rolls amidst the industrial bustling paper mill.\n149\nlotion, therapy center \u2013> Blue lotion bottles lined up at a thalasso therapy center by the ocean.\n150\nguinea\npig,\nsand\ndunes\n\u2013> Guinea pig exploring vast golden sand dunes, with tiny footprints trailing behind.\n151\ngroom, wedding cere-\nmony\n\u2013> father of groom congratulating him after the wedding ceremony.\n152\nfishing boat, village \u2013> fishing boats moored at fishing village a suburb of capital of the state,\n153\nred fox, yard\n\u2013> wild red fox sitting on a partially snow covered front yard of a house in the suburbs of a\nsmall city\n154\ngrey wolf, woodland\nareas\n\u2013> A grey wolf prowls silently, eyes alert, through dense, misty woodland areas with\nmoss-covered trees.\n155\ncheetah,\nedges\nof\nswamplands\n\u2013> A cheetah crouches, poised and watchful, at the lush edges of murky swamplands.\n156\nwine bottle,\nliving\nroom\n\u2013> in the living room, a person si opening a wine bottle with corkscrew with wooden barrel\nTable 18. Detailed in-context learning examples for Template 3: c,rel \u2013> caption. Here c is the concept, and rel is the relation.\n157\nproduct packet / pack-\naging, next to\n\u2013> A vibrant product packet, adorned with colorful labels and intricate designs, is neatly\nplaced next to an elegant crystal glass.\n158\ncroquet ball, behind \u2013> A vivid, red croquet ball rests serenely, hiding behind a worn, rustic wooden fence in a\nsun-kissed, lush green lawn.\n159\nbassoon, in front of\n\u2013> A beautifully crafted bassoon stands elegantly in front of a backdrop of velvet curtains,\nready to perform at a concert.\n160\ngrand piano, above\n\u2013> A gorgeous, antique chandelier is suspended above the glossy black grand piano, illumi-\nnating it with warm, opulent light.\n161\nbolo tie, behind\n\u2013> A beautifully crafted bolo tie is casually hung, indicating its previous use, behind a rustic,\nwell-polished wooden shelf.\n20\n162\nwaffle iron, next to\n\u2013> A large, black waffle iron is placed next to a sparkling glass jar filled with golden maple\nsyrup on a wooden countertop.\n163\nkomodo dragon, be-\nlow\n\u2013> A young child grins excitedly, peering down from a secure bridge, as a colossal Komodo\ndragon sprawls lazily below in the wildlife park.\n164\nvaulted or arched ceil-\ning, besides\n\u2013> Besides the grand marble statue, glimpses of an intricate vaulted or arched ceiling add to\nthe room\u2019s majestic charm.\n165\ngossamer-winged\nbutterfly, next to\n\u2013> A lovely, vibrant gossamer-winged butterfly is gently perched next to a dew-kissed red\nrose in an early morning garden.\n166\nkit fox, in front of\n\u2013> A group of small, fluffy, golden kit foxes is playfully gathered in front of a lush, green,\ntowering forest backdrop.\n167\nkoala, in\n\u2013> A cute, fuzzy koala is visibly relaxed, nestled contentedly in the crook of a towering,\nlush green eucalyptus tree.\n168\ncentipede, above\n\u2013> A vibrant green centipede is effortlessly crawling on a tree branch, positioned distinctly\nabove a patch of untouched fern leaves.\n169\nmountain bike, above \u2013> A mountain bike is displayed prominently above the rustic mantlepiece, showcasing its\nsleek design and intricate details.\n170\nwallaby, above\n\u2013> A fluffy, brown wallaby is leaping high, appearing as if it is effortlessly floating above a\nlush, green Australian field.\n171\ngiant panda, on\n\u2013> A playful giant panda is perched on a sturdy tree branch, munching on fresh green\nbamboo amidst the tranquil forest ambiance.\n172\nbeagle, on\n\u2013> A pack of adorable beagles are spotted lounging on an expansive, sunbathed meadow\nwith colorful wildflowers sprouting around them.\n173\nbeach, on\n\u2013> A vivid sunset is on display over a sprawling beach, casting warm hues on the waves\ngently lapping at the sandy shore.\n174\ngrey whale, on\n\u2013> A voluminous grey whale is majestically breaching, its massive body on display against\nthe azure backdrop of the expansive ocean.\n175\ntractor, in front of\n\u2013> A bright red tractor is parked in front of a rustic, weathered barn, casting long shadows\nunder the golden afternoon sun.\n176\ncabbage, besides\n\u2013> A vibrant image portrays a lush, green cabbage, glistening with dewdrops, nestled\nbesides a rustic, wooden crate full of freshly harvested vegetables.\n21\n"
  },
  {
    "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
    "link": "https://arxiv.org/pdf/2312.17661.pdf",
    "upvote": "10",
    "text": "Gemini in Reasoning: Unveiling Commonsense in\nMultimodal Large Language Models\nYuqing Wang\nStanford University\nywang216@stanford.edu\nYun Zhao\nMeta Platforms, Inc.\nyunzhao20@meta.com\nAbstract\nThe burgeoning interest in Multimodal Large\nLanguage Models (MLLMs), such as OpenAI\u2019s\nGPT-4V(ision), has significantly impacted both\nacademic and industrial realms. These mod-\nels enhance Large Language Models (LLMs)\nwith advanced visual understanding capabili-\nties, facilitating their application in a variety of\nmultimodal tasks. Recently, Google introduced\nGemini, a cutting-edge MLLM designed specif-\nically for multimodal integration. Despite its\nadvancements, preliminary benchmarks indi-\ncate that Gemini lags behind GPT models in\ncommonsense reasoning tasks. However, this\nassessment, based on a limited dataset (i.e., Hel-\nlaSWAG), does not fully capture Gemini\u2019s au-\nthentic commonsense reasoning potential. To\naddress this gap, our study undertakes a thor-\nough evaluation of Gemini\u2019s performance in\ncomplex reasoning tasks that necessitate the\nintegration of commonsense knowledge across\nmodalities. We carry out a comprehensive anal-\nysis of 12 commonsense reasoning datasets,\nranging from general to domain-specific tasks.\nThis includes 11 datasets focused solely on lan-\nguage, as well as one that incorporates multi-\nmodal elements. Our experiments across four\nLLMs and two MLLMs demonstrate Gemini\u2019s\ncompetitive commonsense reasoning capabil-\nities. Additionally, we identify common chal-\nlenges faced by current LLMs and MLLMs\nin addressing commonsense problems, under-\nscoring the need for further advancements in\nenhancing the commonsense reasoning abilities\nof these models. Our data and results are avail-\nable at: https://github.com/EternityYW/\nGemini-Commonsense-Evaluation/.\n1\nIntroduction\nCommonsense reasoning, integral to human cog-\nnition, plays a crucial role in navigating the in-\ntricacies of everyday life.\nConsider a scenario\nwhere someone decides what to wear based on the\nweather. This decision extends beyond the mere se-\nlection of attire; it involves understanding weather\npatterns, the suitability of clothing for different tem-\nperatures, and the social context of the occasion.\nIt\u2019s about synthesizing diverse pieces of knowledge:\na forecast predicting rain, the practical necessity\nfor a raincoat, and the societal expectation of dress-\ning appropriately for an event. This reasoning goes\nbeyond simply processing information; it entails\nintegrating varied pieces of knowledge that humans\noften take for granted. A major challenge in Nat-\nural Language Processing (NLP) research is the\nambiguity and under-specification of human lan-\nguage. Individuals rely heavily on their common-\nsense knowledge and reasoning abilities to decipher\nthese ambiguities and infer missing information.\nCommonsense reasoning has consistently posed\nunique challenges in NLP research (Li et al., 2021;\nBian et al., 2023), encompassing spatial, physical,\nsocial, temporal, and psychological aspects, along\nwith an understanding of social norms, beliefs, val-\nues, and the nuances of predicting and interpreting\nhuman behavior (Liu and Singh, 2004). Models of-\nten lack this innate commonsense, hindering their\nability to contextualize data coherently, in stark\ncontrast to the human capacity for effortlessly un-\nderstanding everyday situations (Shwartz and Choi,\n2020; Bhargava and Ng, 2022).\nRecent advances in Large Language Models\n(LLMs) have sparked unprecedented enthusiasm\nin the NLP community and beyond, significantly\nenhancing a wide array of applications (Min et al.,\n2021; Zhao et al., 2023; Wang et al., 2023; Kas-\nneci et al., 2023; He et al., 2023).\nBuilding\non these achievements, Multimodal Large Lan-\nguage Models (MLLMs) have emerged as a piv-\notal focus in the next wave of AI (Wu et al.,\n2023b), speculated to advance towards Artificial\nGeneral Intelligence (AGI), which aims to de-\nvelop AI systems smarter than humans and ben-\neficial for all of humanity (Rayhan et al., 2023).\nThe rise of MLLMs, particularly OpenAI\u2019s GPT-\n4V(ision) (Yang et al., 2023) and Google\u2019s Gem-\narXiv:2312.17661v1  [cs.CL]  29 Dec 2023\nini (Team et al., 2023), marks significant progress\nin this area. Among these developments, Gemini\nemerges as a formidable challenger to the state-\nof-the-art MLLM, GPT-4V, specially engineered\nfor multimodal integration. Its release has ignited\nconstructive discussions about the current level of\nAGI achievement. In widely used academic bench-\nmarks, Gemini has attained new state-of-the-art sta-\ntus in the majority of tasks. However, preliminary\nevaluations of Gemini, especially when compared\nto models like the GPT series, have indicated po-\ntential shortcomings in its commonsense reasoning\ncapabilities, a fundamental aspect of human cogni-\ntion. Yet, it\u2019s important to consider that basing the\nassessment of Gemini\u2019s commonsense reasoning\nabilities solely on the HellaSWAG dataset (Zellers\net al., 2019b) may not comprehensively reflect\nGemini\u2019s full scope in this critical domain.\nTo address the gap in the comprehensive evalu-\nation of Gemini\u2019s real-world performance in com-\nmonsense reasoning tasks, our study conducts ex-\ntensive experiments across 12 commonsense rea-\nsoning datasets, covering a broad spectrum of do-\nmains such as general, physical, social, and tem-\nporal reasoning. We experiment with four popu-\nlar LLMs for the language dataset evaluation, in-\ncluding Llama2-70b (Touvron et al., 2023), Gem-\nini Pro (Team et al., 2023), GPT-3.5 Turbo, and\nGPT-4 Turbo (OpenAI, 2023). For the multimodal\ndataset, we assess both Gemini Pro Vision and\nGPT-4V. Our key findings are summarized as fol-\nlows: (1) Overall, Gemini Pro\u2019s performance is\ncomparable to that of GPT-3.5 Turbo, demonstrat-\ning marginally better average results across 11 lan-\nguage datasets (1.4% higher accuracy), though it\nlags behind GPT-4 Turbo by an average of 8.2% in\naccuracy. Moreover, Gemini Pro Vision exhibits\nlower performance than GPT-4V on the multimodal\ndataset, except for temporal-related questions. (2)\nApproximately 65.8% of Gemini Pro\u2019s reasoning\nprocesses are evaluated as logically sound and con-\ntextually relevant, indicating its potential for effec-\ntive application in various domains. (3) Gemini Pro\nencounters significant challenges in temporal and\nsocial commonsense reasoning, indicating key ar-\neas for further development. (4) Our manual error\nanalysis reveals that Gemini Pro often misunder-\nstands provided contextual information, accounting\nfor 30.2% of its total errors. Furthermore, Gemini\nPro Vision struggles particularly with identifying\nemotional stimuli in images, especially those in-\nvolving human entities, which constitutes 32.6% of\nits total errors.\nIn summary, our contributions are threefold:\n(1) We provide the first thorough evaluation of\nGemini Pro\u2019s efficacy in commonsense rea-\nsoning tasks, employing 12 diverse datasets\nthat span both language-based and multimodal\nscenarios.\n(2) Our study reveals that Gemini Pro exhibits\nperformance comparable to GPT-3.5 Turbo in\nlanguage-only commonsense reasoning tasks,\ndemonstrating logical and contextual reason-\ning processes. However, it lags behind GPT-4\nTurbo in accuracy and encounters challenges\nin temporal and social reasoning, as well as in\nemotion recognition in images.\n(3) Our findings lay a valuable foundation for fu-\nture research in the field of commonsense rea-\nsoning within LLMs and MLLMs, highlight-\ning the necessity to enhance specialized do-\nmains in these models and the nuanced recog-\nnition of mental states and emotions in multi-\nmodal contexts.\n2\nCommonsense Overview\nCommonsense reasoning, a fundamental aspect of\nhuman intelligence, facilitates an intuitive under-\nstanding and interpretation of the world through\nbasic and often implicit knowledge and beliefs. For\ninstance, it involves understanding that a person\ncarrying an umbrella on a cloudy day likely an-\nticipates rain, or inferring that a closed door in a\nlibrary signifies a need for quiet. In MLLMs, com-\nmonsense reasoning plays a vital role, enabling\nthese models to interact with and interpret human\nlanguage and visual cues in a manner that mirrors\nhuman understanding. In our study, we explore a\nvariety of commonsense reasoning tasks. Defini-\ntions for each domain are provided as follows.\nGeneral Commonsense. This domain entails an\nunderstanding of basic, everyday knowledge about\nthe world, such as recognizing that birds typically\nfly and fish live in water.\nContextual Commonsense. This domain involves\ninterpreting information within specific contexts,\nsuch as understanding that a person wearing a coat\nand shivering is likely cold.\nAbductive Commonsense. This domain is about\nformulating the most plausible explanations for\nobservations, such as inferring that wet streets are\nlikely due to recent rain.\nEvent Commonsense. This domain focuses on\nunderstanding sequences of events and the causal\nrelationships between them, such as predicting that\neating spoiled food can lead to feeling sick.\nTemporal Commonsense. This domain involves\nunderstanding time-related concepts, such as the\nfact that breakfast is typically eaten in the morning.\nNumerical Commonsense. This domain is about\nunderstanding numbers in everyday contexts, such\nas knowing that a cube has six faces.\nPhysical Commonsense. This domain concerns\nunderstanding the physical world, such as knowing\nthat a glass will break if dropped on a hard floor.\nScience Commonsense. This domain involves the\napplication of scientific principles in daily life, such\nas understanding that water boils at a higher tem-\nperature at sea level than in the mountains.\nRiddle Commonsense. This domain challenges\ncreative thinking through riddles, such as decipher-\ning a riddle where the answer is \u201ca shadow\u201d, requir-\ning lateral thinking to associate intangible concepts\nwith physical entities.\nSocial Commonsense. This domain involves un-\nderstanding social interactions, such as recognizing\nthat a person is likely upset if he/she is crying.\nMoral Commonsense. This domain deals with\nevaluating actions based on moral and ethical stan-\ndards, such as understanding that stealing is gener-\nally considered wrong.\nVisual Commonsense. This domain involves in-\nterpreting and understanding visual information in\nthe context of the physical and social world, such\nas deducing that a person in a photo is likely run-\nning a race if they are wearing a number bib and\nsurrounded by other runners.\n3\nExperimental Setup\n3.1\nDatasets\nWe experiment with 12 datasets related to differ-\nent types of commonsense reasoning, which in-\nclude 11 language-based datasets and one multi-\nmodal dataset. The language-based datasets en-\ncompass three main categories of commonsense\nreasoning problems: General and Contextual\nReasoning: (1) CommonsenseQA (Talmor et al.,\n2019), focusing on general commonsense knowl-\nedge; (2) Cosmos QA (Huang et al., 2019), em-\nphasizing contextual understanding narratives, (3)\n\u03b1NLI (Bhagavatula et al., 2019), introducing ab-\nductive reasoning, which involves inferring the\nmost plausible explanation; and (4) HellaSWAG,\ncentering around reasoning with contextual event\nsequences. Specialized and Knowledge Reason-\ning: (1) TRAM (Wang and Zhao, 2023b), testing\nreasoning about time; (2) NumerSense (Lin et al.,\n2020), focusing on numerical understanding; (3)\nPIQA (Bisk et al., 2020), assessing physical inter-\naction knowledge; (4) QASC (Khot et al., 2020),\ndealing with science-related reasoning; and (5) Rid-\ndleSense (Lin et al., 2021), challenging creative\nthinking through riddles. Social and Ethical Rea-\nsoning: (1) Social IQa (Sap et al., 2019), testing\nthe understanding of social interactions; and (2)\nETHICS (Hendrycks et al., 2020), evaluating moral\nand ethical reasoning. For the multimodal dataset\n(vision and language), we select VCR (Zellers et al.,\n2019a), a large-scale dataset for cognition-level vi-\nsual understanding. For datasets like TRAM and\nETHICS, which include multiple tasks, we extract\nthe commonsense reasoning part for experiments.\nWe employ accuracy as the performance metric for\nall datasets. Table 1 provides an overview of the\ndatasets, as well as example questions.\n3.2\nModels\nWe consider four popular LLMs for language-\nbased dataset evaluation, including the open-\nsource model Llama-2-70b-chat (Touvron et al.,\n2023) as well as the closed-source models Gemini\nPro (Team et al., 2023), GPT-3.5 Turbo, and GPT-4\nTurbo (OpenAI, 2023). Each of these models is\naccessed using its corresponding API key. Specif-\nically, we query Gemini through Google Vertex\nAI, the GPT models through the OpenAI API, and\nLlama2 through DeepInfra. For the multimodal\ndataset, we consider GPT-4V (gpt-4-vision-preview\nin API) and Gemini Pro Vision (gemini-pro-vision\nin API) in our experiments. Given the constraints\nof API costs and rate limitations, we randomly se-\nlect 200 examples from the validation set for each\nlanguage-based dataset following (Wang and Zhao,\n2023b) and 50 examples from the validation set for\nthe VCR dataset following (Liu and Chen, 2023).\nFor all evaluations, we employ greedy decoding\n(i.e., temperature = 0) during model response gen-\neration. Notably, there are instances where the\nmodels decline to respond to certain queries, partic-\nularly those involving potentially illegal or unethi-\ncal content. Sometimes, models provide answers\nthat are outside the scope of the options. In these\ncases, we categorize these unanswered questions\nas incorrect.\n3.3\nPrompts\nIn the evaluation of language-based datasets, we\nemploy two prompting settings: (1) zero-shot\nstandard prompting (SP) (Kojima et al., 2022),\nwhich aims to gauge the models\u2019 inherent com-\nmonsense capabilities in linguistic contexts, and (2)\nfew-shot chain-of-thought (CoT) prompting (Wei\net al., 2022), implemented to observe potential en-\nhancements in the models\u2019 performance. For the\nmultimodal dataset, we utilize zero-shot standard\nprompting to assess the authentic end-to-end visual\ncommonsense reasoning abilities of MLLMs.\n4\nResults\n4.1\nOverall Performance Comparison\nTable 2 demonstrates the accuracy comparison of\nfour LLMs under zero-shot SP and few-shot CoT\nsettings on 11 language-based commonsense rea-\nsoning datasets. There are several key takeaways.\nFirst, from the model perspective, GPT-4 Turbo\noutperforms the other models across the major-\nity of datasets on average. Under the zero-shot\nlearning paradigm, it surpasses Gemini Pro, the\nsecond-best performing model, by 7.3%, and shows\nan even greater lead of 9.0% under the few-shot\nlearning paradigm. Gemini Pro exhibits marginally\nhigher average accuracy than GPT-3.5 Turbo, with\nan increase of 1.3% under zero-shot SP and 1.5%\nin the few-shot CoT scenario.\nIt also demon-\nstrates substantially better performance than Llama-\n2-70b. Regarding prompting methods, the CoT ap-\nproach consistently enhances performance across\nall datasets, with pronounced gains observed in\ndatasets such as CommonsenseQA, TRAM, and\nSocial IQa. Lastly, from a dataset standpoint, it\nis apparent that while these models exhibit com-\nmendable performance across a broad spectrum of\ncommonsense domains, they encounter challenges\nin specific areas, particularly those involving tem-\nporal (TRAM) and social (Social IQa) dimensions\nof commonsense reasoning.\nFor the multimodal VCR dataset, we report the\nperformance of GPT-4V and Gemini Pro Vision in\nTable 3. The VCR consists of three subtasks: (1)\nQ \u2192 A, which involves generating an answer to a\nquestion based on the visual context; (2) QA \u2192 R,\nwhich requires the model to produce a rationale for\na given answer; and (3) Q \u2192 AR, which challenges\nthe model to both answer the question and justify\nthe response with appropriate rationales. In all sub-\ntasks, GPT-4V demonstrates superior performance\ncompared to Gemini Pro Vision, indicating a more\nrobust capacity for integrating visual and textual\ninformation to provide coherent responses. In Q \u2192\nAR, the relatively lower performance of both mod-\nels, compared to the other two subtasks, suggests\nthat there is considerable room for improvement\nin understanding the interplay between visual cues\nand commonsense reasoning.\n4.2\nEffects of Commonsense Domain\nReferring to Section 3.1, we have categorized 11\nlanguage-based datasets into three groups and pre-\nsented the performance for each setting within each\ngroup in Figure 1. Our findings indicate that GPT-4\nTurbo consistently leads in performance across all\ncategories. The Llama-2-70b model demonstrates\nmarginally lower accuracy in comparison to the\nother models. Gemini Pro and GPT-3.5 Turbo dis-\nplay comparable performances; however, Gemini\nPro slightly outperforms GPT-3.5 Turbo in two of\nthe three categories. Notably, its performance dip\nin the Social and Ethical Reasoning group may\nstem from its tendency to refuse to answer ques-\ntions that could potentially involve unethical con-\ntent, which we have counted as incorrect in our eval-\nuation. Based on our experiments, among the 200\nsamples, Gemini Pro refuses to answer 3.0% of the\nproblems (6 in total) in the Social IQa dataset and\n6.5% of the problems (13 in total) in the ETHICS\ndataset. Overall, all models exhibit robust capa-\nbilities in handling Social and Ethical Reasoning\ndatasets, suggesting a relatively advanced grasp of\nmoral and social norms. However, there is a notable\ndisparity in their performance on General and Con-\ntextual Reasoning tasks, indicating a potential gap\nin their understanding of broader commonsense\nprinciples and their application in varied contexts.\nThe Specialized and Knowledge Reasoning cate-\ngory, particularly in the realms of temporal and\nriddle-based challenges, highlights specific defi-\nciencies in the models\u2019 abilities to process complex\ntemporal sequences and to engage in the abstract\nand creative thought required to decipher riddles.\nRegarding the multimodal dataset, Figure 2 de-\ntails the comparative performance between GPT-\n4V and Gemini Pro Vision across different ques-\ntion types, in alignment with the guidelines of the\nVCR dataset (Zellers et al., 2019a). We concen-\nTable 1: Overview of commonsense datasets used in our experiments. \u201cK-Way MC\u201d signifies a multiple-choice\nresponse format with K options. Bold text in the \u201cExample Questions\u201d column represents the correct answers.\nDataset\nDomain\nAnswer Type\nExample Questions\nGeneral and Contextual Reasoning\nCommonsenseQA\ngeneral\n5-Way MC\nWhere is a doormat likely to be in front of?\n(A). facade; (B). front door; (C). doorway; (D). entrance porch; (E). hallway.\nCosmos QA\ncontextual\n4-Way MC\nGiven the context \u201cIt wasn\u2019t time for my book to be released... I have received\nabout five rejection letters.\u201d What may be the reason for your book getting rejected?\n(A). None of the above choices; (B). I never...; (C). I felt...; (D). It wasn\u2019t finished.\n\u03b1NLI\nabductive\n2-Way MC\nGiven the beginning of the story: Four Outlaws camped in Blood Gulch,\nand the end of the story: He arrested them, what is the more plausible hypothesis:\n(A). They found where the sheriff was; (B). The sheriff found where they were.\nHellaSWAG\nevent\n4-Way MC\nGiven the context \u201cA boy in an orange shirt is playing a video game. the scene\u201d\nand the activity label \u201cWashing face\u201d, which of the following endings is the most\nappropriate continuation of the scenario? (A). changes to safety features;\n(B). changes to the game itself; (C). switches to show...; (D). cuts to the boys...\nSpecialized and Knowledge Reasoning\nTRAM\ntemporal\n3-Way MC\nThen the green ball told the orange ball that blue ball was stupid. How long was the\ngreen ball talking to the orange ball? (A). 5 weeks; (B). 24 hours; (C). 15 seconds.\nNumerSense\nnumerical\nNumber\nComplete the sentence by filling in <mask> with the most appropriate number.\nA classical guitar has <mask> strings. \u2192 six\nPIQA\nphysical\n2-Way MC\nTo reach the physical goal: trees, choose the more sensible solution:\n(A). provide homes for people; (B). provide homes for animals.\nQASC\nscience\n8-Way MC\nCrabs scavanage and uses dead snail shells for what? (A). RNA; (B). homes; (C). making\nholes; (D). damage; (E). a hosta; (F). Protein; (G). matter; (H). building a nest.\nRiddleSense\nriddle\n5-Way MC\nSomething very helpful if you want to go gently down a stream. (A). raft; (B). roll\ndown hill; (C). rowboat; (D). water; (E). roll over.\nSocial and Ethical Reasoning\nSocial IQa\nsocial\n3-Way MC\nKai observed and improved Jan\u2019s work. What will Kai want to do next?\n(A). be effective; (B). get praise; (C). get incredible.\nETHICS\nmoral\n2-way MC\nAfter I got the flu virus I called my friends to tell them that I had the virus.\nIs this acceptable? (A). acceptable; (B). unacceptable.\nVisual Understanding and Reasoning\nVCR\nvisual\n4-Way MC\n1. What is wrong with Person 2? (A). He\nis not happy with what is being said to\nhim over the telephone; (B). He is feeling\ndepressed; (C). He is high on pot; (D).\nSomeone has pushed him and he\u2019s falling.\n2. Given the question: What is wrong with\nPerson 2?, and the answer to the question:\nHe is feeling depressed, what is the ratio-\nnale behind this answer? (A). Person 1 is\ntalking to him probably trying to cheer\nhim up; (B). He looks sad and is drinking;\n(C). He is walking with his head down;\n(D). He is slumped down on bed and his\neyes are closed.\nTable 2: Performance comparison of four LLMs across 11 language-based commonsense reasoning datasets. For the\nk-shot CoT setting, k is set to 5 for the majority of datasets, except HellaSWAG (k=10) and PIQA (k=1). The best\nresults for the k-shot setting are boldfaced, and for the 0-shot setting, underlined. GPT-4 Turbo outperforms other\nmodels across the majority of datasets under both settings by a large margin. Gemini Pro and GPT-3.5 Turbo exhibit\ncomparably matched performance overall, with Gemini Pro demonstrating marginally superior commonsense\nreasoning capabilities compared to GPT-3.5 Turbo on average.\nDataset\nMethod\nLlama-2-70b\nLlama-2-70b\nGemini Pro\nGemini Pro\nGPT-3.5 Turbo\nGPT-3.5 Turbo\nGPT-4 Turbo\nGPT-4 Turbo\n(0-shot, SP)\n(k-shot, CoT)\n(0-shot, SP)\n(k-shot, CoT)\n(0-shot, SP)\n(k-shot, CoT)\n(0-shot, SP)\n(k-shot, CoT)\nCommonsenseQA\n72.0\n76.5\n76.5\n79.0\n73.0\n76.0\n78.0\n80.0\nCosmos QA\n77.0\n81.0\n81.5\n84.5\n75.0\n78.5\n86.5\n88.0\n\u03b1NLI\n77.5\n80.5\n79.5\n81.5\n75.5\n78.0\n87.0\n88.0\nHellaSWAG\n73.0\n77.0\n76.0\n78.5\n78.0\n80.0\n94.0\n95.0\nTRAM\n66.0\n70.0\n73.5\n76.0\n68.5\n72.0\n79.5\n82.0\nNumerSense\n74.0\n75.5\n80.0\n82.0\n81.5\n82.5\n85.0\n86.0\nPIQA\n74.0\n78.5\n89.0\n90.5\n87.0\n89.5\n94.5\n95.5\nQASC\n78.0\n82.0\n80.0\n82.5\n83.0\n85.0\n91.5\n92.5\nRiddleSense\n62.5\n66.0\n75.0\n82.5\n71.5\n75.0\n94.0\n95.0\nSocial IQa\n71.0\n77.5\n73.0\n78.5\n73.0\n78.0\n82.0\n84.5\nETHICS\n88.0\n89.5\n87.0\n87.5\n94.0\n95.0\n97.0\n98.0\nAverage\n73.9\n77.6\n79.2\n82.1\n78.2\n80.9\n88.1\n89.5\nTable 3: Performance comparison between GPT-4V\nand Gemini Pro Vision on the VCR dataset. \u201cQ \u2192\nA\u201d evaluates question-answering accuracy, \u201cQA \u2192 R\u201d\nassesses answer justification, and \u201cQ \u2192 AR\u201d measures\nthe performance of both correctly answering questions\nand selecting rationales. GPT-4V outperforms Gemini\nPro Vision across all subtasks.\nMethod\nQ \u2192 A\nQA \u2192 R\nQ \u2192 AR\nGPT-4V\n80.0\n72.0\n56.0\nGemini Pro Vision\n74.0\n70.0\n48.0\ntrate on the \u201cQ \u2192 A\u201d subtask as it most directly\nassesses the models\u2019 visual commonsense capabil-\nities. Considering the data sample for each type,\nGemini Pro Vision\u2019s performance either matches or\nis slightly lower than GPT-4V\u2019s, except in temporal-\ntype questions, where it surpasses GPT-4V. This\nsuggests its enhanced capability not only in rec-\nognizing but also in contextualizing time-related\nelements within visual scenarios.\n4.3\nReasoning Justification within MLLMs\nTo assess the reasoning capabilities of MLLMs,\nparticularly their ability to provide not only correct\nanswers but also sound and contextually grounded\nreasoning in matters of commonsense, we adopted\na systematic sampling approach. For each of the\n11 language-based datasets evaluated with four\nLLMs, we randomly selected 30 questions that\nwere correctly answered and 30 questions that were\nincorrectly answered by each LLM following (Bian\net al., 2023). In cases where a dataset presented\nfewer than 30 incorrect answers, we included all\navailable incorrect responses to ensure comprehen-\nsive analysis. After selecting these questions, we\nprompted each model to explain \u201cWhat is the ra-\ntionale behind the answer to the question?\u201d The\nreasoning processes provided by the models were\nthen manually reviewed and classified as either\nTrue or False, based on their logical soundness and\nrelevance to the question. Figure 3 illustrates a com-\nprehensive view of the average reasoning correct-\nness across the 11 datasets, in terms of the sampled\ncorrect and incorrect questions. In fact, not every\nmodel had 30 incorrect questions for each dataset.\nIn such scenarios, we scaled the available data up\nto 30 questions to ensure standardized computa-\ntion. Figure 3 shows that GPT-4 Turbo\u2019s leading\nperformance in both correct and incorrect answers\nhighlights its advanced reasoning mechanisms and\nits ability to maintain coherent logic, even when the\nfinal answers are not accurate. Additionally, Gem-\nini Pro has emerged as a notably proficient model,\ngenerally demonstrating commendable reasoning\nabilities and offering a well-rounded approach to\ncommonsense reasoning. GPT-3.5, while trailing\nslightly behind Gemini Pro, still demonstrates com-\npetitive reasoning abilities. Figure 4 presents two\nreal examples from Gemini Pro and GPT-3.5, illus-\ntrating the cases of a correct answer with a correct\nrationale and an incorrect answer with an incorrect\n60\n65\n70\n75\n80\n85\n90\n95\nAverage Accuracy (%)\nLlama-2-70b (0-shot, SP)\nLlama-2-70b (few-shot, CoT)\nGemini Pro (0-shot, SP)\nGemini Pro (few-shot, CoT)\nGPT-3.5 Turbo (0-shot, SP)\nGPT-3.5 Turbo (few-shot, CoT)\nGPT-4 Turbo (0-shot, SP)\nGPT-4 Turbo (few-shot, CoT)\n74.9\n78.8\n78.4\n80.9\n75.4\n78.1\n86.4\n87.8\n70.9\n74.4\n79.5\n82.7\n78.3\n80.8\n88.9\n90.2\n79.5\n83.5\n80.0\n83.0\n83.5\n86.5\n89.5\n91.3\nGeneral and Contextual Reasoning\nSpecialized and Knowledge Reasoning\nSocial and Ethical Reasoning\nFigure 1: Average model performance across three major commonsense reasoning categories over 11 language-\nbased datasets, including General and Contextual Reasoning (CommonsenseQA, Cosmos QA, \u03b1NLI, HellaSWAG),\nSpecialized and Knowledge Reasoning (TRAM, NumerSense, PIQA, QASC, RiddleSense), and Social and Ethical\nReasoning (Social IQa, ETHICS). GPT-4 Turbo consistently exhibits superior performance in all commonsense\nreasoning categories. Gemini Pro marginally surpasses GPT-3.5 Turbo in the first two categories, except for Social\nand Ethical Reasoning.\nrationale, respectively.\nMoving to the multimodal perspective, our anal-\nysis of GPT-4V and Gemini Pro Vision on the VCR\ndataset reveals notable patterns in reasoning cor-\nrectness. With GPT-4V at 24% and Gemini Pro\nVision at 26%, approximately one-quarter of the\ncases showed both models correctly identifying\nthe answers but failing to provide appropriate ra-\ntionale. This discrepancy suggests that while the\nmodels can often determine the correct outcomes,\ntheir ability to understand or explain the underlying\nreasoning behind these answers is not consistently\naligned. Furthermore, in the instances of incorrect\nanswers, GPT-4V and Gemini Pro Vision showed\ncorrect rationales 16% and 22% of the time, re-\nspectively. This indicates that, despite arriving at\nincorrect conclusions, the models demonstrate a ca-\npacity for effective reasoning or logical processing.\nHowever, this does not consistently translate into\naccurate outcomes, implying that while some as-\npects of the required knowledge are captured, other\ncrucial elements are likely missed.\n4.4\nCase Study: Gemini Pro in Commonsense\nGiven our focus on evaluating the commonsense\nreasoning capabilities of the Gemini Pro model,\nwe conduct a qualitative analysis to assess its per-\nformance across representative examples in four\nmajor categories (three language-based and one\nmultimodal), as described in Section 3.1. To en-\nsure an authentic end-to-end capability evaluation,\nwe present examples under the zero-shot learning\nsetting, employing standard prompting techniques.\nGeneral (CommonsenseQA). In the general com-\nmonsense evaluation (General and Contextual\nReasoning category) using the CommonsenseQA\ndataset, consider the example question: \u201cPeople are\nwhat when you\u2019re a stranger? (A) train (B) strange\n(C) human (D) stupid (E) dangerous.\u201d Gemini Pro\ncorrectly chose (B) \u201cstrange,\u201d and its reasoning pro-\ncess is notable. It recognized that while all options\nrelate to the concept of a \u201cstranger\u201d, only \u201cstrange\u201d\naccurately encapsulates the neutral and open-ended\nnature of the question. The model effectively ruled\nout other options: (A) \u201ctrain\u201d, for being too specific\nand unrelated; (C) \u201chuman\u201d, as accurate but not\ncapturing the question\u2019s essence; (D) \u201cstupid\u201d, for\nbeing judgmental and offensive; and (E) \u201cdanger-\nous\u201d, due to its negative connotation. This selection\nof \u201cstrange\u201d demonstrates an understanding of the\nunfamiliar nature associated with strangers, high-\nlighting Gemini Pro\u2019s capability in interpreting and\napplying general commonsense knowledge appro-\nactivity\nexplanation\nhypothetical\nmental\nrole\nscene\ntemporal\nQuestion Type\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\n83.3\n82.4\n75.0\n66.7\n100.0\n50.0\n83.3\n66.7\n82.4\n50.0\n66.7\n75.0\n50.0\n100.0\nGPT-4V\nGemini Pro Vision\nFigure 2: Performance comparison between GPT-4V and Gemini Pro Vision on the VCR dataset, categorized by\nquestion type, with a focus on the \u201cQ \u2192 A\u201d subtask. Within our sample of 50 questions, the distribution across each\ntype is as follows: activity (12), explanation (16), hypothetical (3), mental (4), role (5), scene (4), and temporal (6).\nGPT-4V matches or surpasses Gemini Pro Vision in performance across these question types, with the exception of\nthe temporal category.\nLlama2-70b\nGemini Pro\nGPT-3.5 Turbo GPT-4 Turbo\n0\n5\n10\n15\n20\n25\n30\nAverage Reasoning Correctness (# Questions)\n17.5\n20.8\n20.3\n24.1\n15.9\n18.7\n18.5\n21.3\nCorrect Questions\nIncorrect Questions\nFigure 3: Average reasoning correctness across 11 lan-\nguage datasets. The comparison among four LLMs is\nbased on a random sample of 30 correct and 30 incorrect\nquestions per dataset. In cases where a dataset contained\nfewer than 30 incorrect questions, the data were scaled\nup to maintain consistency in the sample size.\npriately.\nTemporal (TRAM). In the temporal commonsense\nevaluation (Specialized and Knowledge Reasoning\ncategory) using the TRAM dataset, consider the\nexample question: \u201cHe also promises to \u2018come to\u2019\nhim. How long does it take for him to \u2018come to\u2019\nhim? (A) 100 years (B) in a minute\u2019s time (C) a\nfew hours.\u201d Lacking sufficient context, especially\nregarding the identities involved and the meaning\nof \u2018come to\u2019, Gemini Pro was unable to provide\na definitive answer. Gemini Pro\u2019s response high-\nlights a significant aspect of its temporal reasoning\ncapabilities. It illustrates the model\u2019s reliance on\nspecific contextual information to make accurate\ntemporal judgments. While this cautious approach\nis prudent to avoid incorrect assumptions, it also\nsignifies a limitation in addressing ambiguous or\nincomplete information \u2013 a frequent challenge in\nreal-world communications. This example under-\nlines the difficulties LLMs encounter in temporal\nreasoning tasks, especially when faced with scenar-\nios that offer limited or unclear context.\nSocial (Social IQa). In assessing Gemini Pro\u2019s\nperformance in social commonsense reasoning us-\ning the Social IQa dataset (Social and Ethical Rea-\nsoning category), an interesting scenario was pre-\nsented: \u201cThe people bullied Sasha all her life. But\nSasha got revenge on the people. What will the peo-\nple want to do next? (A) Do whatever Sasha says\n(B) Get even (C) Flee from Sasha.\u201d The correct\nanswer is (C), but Gemini Pro\u2019s response is insight-\nful. It chose (B) \u201cGet even\u201d as the most likely\noption, reasoning that the desire for revenge is a\nstrong motivator and Sasha\u2019s actions likely ignited\n(a) Example of a correct response and rationale explanation\nfrom Gemini Pro.\n(b) Example of an incorrect response and rationale explanation\nfrom GPT-3.5 Pro.\nFigure 4: Model reasoning correctness justification ex-\namples. The sample questions are from the QASC\ndataset, with the correct answers highlighted in bold\nred. In example (a), Gemini Pro exhibits methodical rea-\nsoning by exclusion, carefully considering all options\nto reach the most logical conclusion. Conversely, ex-\nample (b) illustrates GPT-3.5 Turbo\u2019s tendency towards\nunconventional logic, which can result in imaginative\nyet atypical answers. These instances emphasize the di-\nverse strategies different models apply to commonsense\nreasoning tasks, revealing their distinct capabilities and\nlimitations in such contexts.\na similar desire in them. Gemini Pro considered\n(A) as a less likely option, depending on whether\nSasha\u2019s revenge instilled deep fear and assumed\ncomplete submission. The least likely option, ac-\ncording to Gemini Pro, was (C), which it associ-\nated with physical harm or an ongoing threat. This\nresponse demonstrates Gemini Pro\u2019s nuanced un-\nderstanding of social dynamics and emotional mo-\ntivations. However, it also highlights a limitation\nin accurately predicting human reactions in com-\nplex social scenarios, where emotional responses\nmight not always follow a logical pattern. This\ninstance reveals the challenges LLMs face in ac-\ncurately interpreting and responding to intricate\nsocial situations, an area that remains challenging\nfor AI systems.\nVisual (VCR). In the visual commonsense evalu-\nation using the VCR dataset, we analyzed Gem-\nini Pro Vision\u2019s response to a scenario involving\nphysical safety and potential danger, as shown in\nFigure 5. Presented with an image of individuals\non the edge of a cliff, the model was questioned:\n\u201cWhat would happen if person 4 pushed person 3\nat this moment?\u201d In this context, Gemini Pro Vi-\nsion\u2019s response mirrored the logical inference that\nif the second person from the left (person 4) pushed\nthe third person from the left (person 3), the result\nwould be person 3 falling off the cliff, leading to a\nfatal outcome. This example from the VCR dataset\nunderscores Gemini Pro Vision\u2019s ability to analyze\nvisual scenes and make predictions about the poten-\ntial consequences of actions within those scenes, a\ncrucial aspect of visual commonsense reasoning. It\ndemonstrates the model\u2019s grasp of spatial relations\nand physical consequences, providing evidence of\nits capacity to process and reason about complex\nvisual information akin to human cognition.\nOverall, the cases presented underscore the ad-\nvanced reasoning capabilities of Gemini Pro and\nGemini Pro Vision, while also identifying chal-\nlenges in achieving human-like inference. These\ninsights point to potential avenues for the continued\nenhancement of LLMs and MLLMs.\n4.5\nError Analysis\nTo gain a deeper understanding of the mistakes\nmade by models, we manually analyzed instances\nwhere a model made incorrect choices or provided\ninappropriate answers. We conducted a thorough\nexamination of common error types encountered\nin commonsense reasoning tasks, with the results\nFigure 5: Example image from the VCR dataset.\naveraged across four LLMs. Our focus was on as-\nsessing these models in two distinct settings: zero-\nshot SP and few-shot CoT. Table 4 presents the\nproportions of five common error types observed in\neach setting, with the data averaged over the four\nLLMs.\nContext misinterpretation emerged as the most\nfrequent error, occurring more often in the zero-\nshot SP setting (28.6%) compared to the few-shot\nCoT (23.4%). This trend suggests that the addi-\ntional context in few-shot CoT helps models bet-\nter understand scenarios, thereby reducing errors\nrelated to contextual misunderstanding. Logical\nerrors were the second most common, accounting\nfor 23.9% in zero-shot SP and slightly less in few-\nshot CoT (20.1%), indicating that extra examples\nin the latter setting aid in more consistent logical\nreasoning. Ambiguity errors, at 16.2% in zero-\nshot SP, were reduced to 11.6% in few-shot CoT,\ndemonstrating the effectiveness of added context in\nresolving language ambiguities. In contrast, Over-\ngeneralization errors showed an increase in few-\nshot CoT (15.6%) from zero-shot SP (11.8%), pos-\nsibly due to models\u2019 overextending patterns learned\nfrom the additional examples. Notably, knowledge\nerrors, where models misapplied correct and nec-\nessary commonsense knowledge, saw a significant\nincrease in few-shot CoT (29.3%) compared to\nzero-shot SP (19.5%). This finding suggests that\nwhile extra context can be beneficial, it can also\nlead to inaccuracies, particularly in complex or nu-\nanced scenarios.\nTable 4: Proportion of common error types in com-\nmonsense reasoning in LLM evaluation. Misinterpret.\nrepresents misinterpretation.\nError Type\nZero-shot SP\nFew-shot CoT\nContext Misinterpret.\n28.6%\n23.4%\nLogical Errors\n23.9%\n20.1%\nText Ambiguity\n16.2%\n11.6%\nOvergeneralization\n11.8%\n15.6%\nKnowledge Errors\n19.5%\n29.3%\nIn our analysis of the VCR dataset, we focused\non instances where either GPT-4V or Gemini Pro\nVision chose incorrect answers in the Q \u2192 A sub-\ntask. The four common error types for each model\nare summarized in Table 5. Emotion recognition er-\nrors were the most common, with GPT-4V encoun-\ntering these errors in 30.1% of cases and Gemini\nPro Vision slightly more at 31.3%. This high inci-\ndence suggests that both models find interpreting\nemotional cues in visual content particularly chal-\nlenging, underscoring the complexity of decipher-\ning human emotions from visual stimuli. Spatial\nperception errors were also significant, constitut-\ning 22.5% of errors for GPT-4V and 25.2% for\nGemini Pro Vision. These figures indicate the mod-\nels\u2019 difficulties in accurately understanding spatial\nrelationships and the arrangement of elements in\nimages. Logical errors were another major error\ntype, more pronounced in GPT-4V (27.7%) than in\nGemini Pro Vision (24.9%), pointing to challenges\nin logical reasoning within visual contexts. Context\nmisinterpretation, although less frequent, was still\na notable issue, with GPT-4V at 19.7% and Gemini\nPro Vision at 18.6%. These errors demonstrate the\nmodels\u2019 struggles with grasping the overarching\ncontext or narrative depicted in the visual content.\nOverall, error analysis sheds light on the specific\nchallenges LLMs and MLLMs face in common-\nsense reasoning, providing valuable insights for\nfuture improvements for future model refinement.\nTable 5: Proportion of commmon error types in visual\ncommonsense reasoning in MLLM evaluation (GPT-4V\nand Gemini Pro Vision). Misinterpret.: and E. represent\nmisinterpretation and errors, respectively.\nError Type\nGPT-4V\nGemini Pro Vision\nContext Misinterpret.\n19.7%\n18.6%\nSpatial Perception E.\n22.5%\n25.2%\nEmotion Recognition E.\n30.1%\n31.3%\nLogical Errors\n27.7%\n24.9%\n5\nRelated Work\nCommonsense Reasoning in NLP. Commonsense\nreasoning has gained renewed attention in recent\nyears, especially in the context of advancements in\nLLMs that have significantly influenced numerous\napplications in NLP. However, there is a growing\nconcern about their ability to understand and rea-\nson about commonsense knowledge (Storks et al.,\n2019; Tamborrino et al., 2020; Bhargava and Ng,\n2022). This concern is echoed in various stud-\nies that focus on evaluating the commonsense rea-\nsoning capabilities of LLMs (Bian et al., 2023;\nWeng et al., 2023; Shen and Kejriwal, 2023). Con-\ncurrently, researchers have been exploring diverse\nstrategies to enhance the commonsense reasoning\ncapabilities of NLP systems. These strategies range\nfrom leveraging large-scale knowledge graphs to\nemploying methods of commonsense knowledge\ntransfer, aiming to endow NLP systems with a\ndeeper and more nuanced understanding of com-\nmonsense concepts (Huang et al., 2023; Ye et al.,\n2023; Zhou et al., 2023). Prior to delving into\nmethodological refinements, a comprehensive eval-\nuation is essential to understand the authentic com-\nmonsense reasoning capabilities of LLMs. In our\nstudy, we endeavor to advance this line of inquiry\nby examining how LLMs, particularly focusing on\nthe Gemini model, navigate and implement com-\nmonsense reasoning in various NLP contexts.\nTraining Paradigms in LLMs. In NLP research,\npretraining language models on large-scale var-\nied textual datasets has become essential. This\napproach endows models with a comprehensive\nknowledge base across numerous fields. Initially,\nleveraging this knowledge often involved fine-\ntuning models with task-specific data.\nBERT-\nbased models like BERT (Kenton and Toutanova,\n2019) and RoBERTa (Liu et al., 2019) exemplify\nthis, being applied to tasks ranging from disease\nprediction (Zhao et al., 2021) to text classifica-\ntion (Wang et al., 2022b) and time series analy-\nsis (Wang et al., 2022c).\nThe debut of GPT-3\n(Brown et al., 2020) shifted this focus towards more\nflexible learning methods like zero-shot and few-\nshot learning, showcasing models\u2019 adaptability to\nnew tasks with minimal data (Brown et al., 2020).\nThis shift has spurred the development of novel\nprompting techniques to enhance LLMs\u2019 reasoning\nand understanding capabilities, including chain-\nof-thought (CoT) prompting (Wei et al., 2022),\nself-consistency with CoT (Wang et al., 2022a),\ntree-of-thought prompting (Yao et al., 2023), and\nmetacognitive prompting (Wang and Zhao, 2023a).\nIn this work, we establish evaluations by consider-\ning four popular LLMs for language-based tasks\nunder zero-shot and few-shot settings, and two\nMLLMs for multimodal tasks under the zero-shot\nlearning paradigm. Our goal is to provide an in-\ndepth understanding of their strengths and limita-\ntions in diverse commonsense reasoning tasks.\nEvaluations on MLLMs. Since the release of\nthe state-of-the-art MLLM, GPT-4V, several eval-\nuations have been conducted across diverse tasks,\nincluding medical imaging (Wu et al., 2023a), vi-\nsual question answering (Li et al., 2023; Yang\net al., 2023), and video understanding (Lin et al.,\n2023). These evaluations typically focus either on\ncase-by-case qualitative analyses through example\ndemonstrations or on quantitative assessments by\nanalyzing the model\u2019s performance across diverse\ntasks. The recent release of Google\u2019s Gemini has\ngarnered considerable attention, and early experi-\nments have been conducted to evaluate its capabil-\nities in both language understanding (Akter et al.,\n2023) and the multimodal domain (Liu and Chen,\n2023; Fu et al., 2023). However, a significant gap\nremains in fully comprehending the commonsense\nreasoning capabilities of Gemini, a known poten-\ntial shortcoming since its introduction. In our work,\nwe conduct a comprehensive analysis of Gemini\u2019s\ncapabilities in this area, along with comparisons to\nother leading MLLMs, thereby highlighting both\nthe potential and areas for further improvement in\nfuture research.\n6\nDiscussion\nIn this study, we conducted a comprehensive eval-\nuation of state-of-the-art LLMs and MLLMs, fo-\ncusing particularly on Gemini Pro and Gemini Pro\nVision, across 12 diverse commonsense reasoning\ndatasets. Our findings indicate that while these\nmodels mark a significant advancement in various\ndomains, demonstrating impressive performance\nin commonsense reasoning tasks, they still exhibit\nlimitations, particularly in tasks requiring deep con-\ntextual understanding or abstract reasoning, such\nas those involving temporal dynamics, riddles, or\nintricate social scenarios.\nLooking ahead, addressing these challenges is\ncrucial to enhance the overall effectiveness of\nLLMs and MLLMs in commonsense reasoning.\nFuture research should aim to refine the models\u2019\ncapabilities in interpreting and reasoning within\ncomplex contexts and abstract scenarios. Addition-\nally, there is an emerging need for more holistic\nevaluation metrics and methodologies capable of\naccurately assessing the nuances of commonsense\nreasoning in AI systems. These metrics should\nevaluate not only the correctness of responses but\nalso their logical coherence and context relevance.\nIn conclusion, our study underscores that perfect-\ning commonsense reasoning in LLMs and MLLMs\nremains an ongoing endeavor. The observed per-\nformance discrepancies among these models reveal\nintriguing areas for further research and improve-\nment. Although significant progress has been made,\nachieving AGI still represents a substantial goal\non the horizon. Our work lays the groundwork\nfor future exploration in this field, highlighting\nboth the achievements and the areas in need of en-\nhancement in the realm of commonsense reasoning\nwithin LLMs and MLLMs.\n7\nLimitations\nWhile this study offers valuable insights into the\nrole of LLMs and MLLMs in commonsense reason-\ning, there are some limitations to our work. Firstly,\nour evaluation is heavily dependent on the selected\nquestions and datasets used for analysis. Despite\ntheir diversity, these datasets may not cover all\nfacets of commonsense reasoning. As a result, the\nperformance and capabilities of Gemini Pro and\nother models could vary in real-world scenarios or\nwith alternative datasets. Additionally, our analy-\nsis is confined to English language datasets, lim-\niting the generalizability of our findings to other\nlanguages or multilingual contexts, where cultural\nnuances and linguistic differences are crucial in\ncommonsense reasoning. Finally, our study rep-\nresents a specific moment in the rapidly evolving\nlandscape of AI, focusing on API-based systems\nthat are subject to change. The introduction of\nnewer models or updates to existing ones might\nlead to different performance outcomes, highlight-\ning the need for ongoing evaluation and analysis.\nReferences\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini\u2019s language\nabilities. arXiv preprint arXiv:2312.11444.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen-tau Yih, and Yejin\nChoi. 2019. Abductive commonsense reasoning. In\nInternational Conference on Learning Representa-\ntions.\nPrajjwal Bhargava and Vincent Ng. 2022. Common-\nsense knowledge reasoning and generation with pre-\ntrained language models: A survey. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 36, pages 12317\u201312325.\nNing Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie\nLu, and Ben He. 2023. Chatgpt is a knowledgeable\nbut inexperienced solver: An investigation of com-\nmonsense problem in large language models. arXiv\npreprint arXiv:2303.16421.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432\u20137439.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nChaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang,\nTimin Gao, Yongdong Luo, Yubo Huang, Zhengye\nZhang, Longtian Qiu, Gaoxiang Ye, et al. 2023. A\nchallenger to gpt-4v? early explorations of gemini in\nvisual expertise. arXiv preprint arXiv:2312.12436.\nZhankui He, Zhouhang Xie, Rahul Jha, Harald Steck,\nDawen Liang, Yesu Feng, Bodhisattwa Prasad Ma-\njumder, Nathan Kallus, and Julian McAuley. 2023.\nLarge language models as zero-shot conversational\nrecommenders. In Proceedings of the 32nd ACM\ninternational conference on information and knowl-\nedge management, pages 720\u2013730.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2020. Aligning ai with shared human values. In In-\nternational Conference on Learning Representations.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2391\u20132401.\nYongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang,\nRuyi Gan, Jiaxing Zhang, and Liwei Wang. 2023.\nMvp-tuning: Multi-view knowledge retrieval with\nprompt tuning for commonsense reasoning. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 13417\u201313432.\nEnkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann,\nMaria Bannert, Daryna Dementieva, Frank Fischer,\nUrs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke\nH\u00fcllermeier, et al. 2023. Chatgpt for good? on op-\nportunities and challenges of large language models\nfor education. Learning and individual differences,\n103:102274.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of naacL-HLT, volume 1, page 2.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020.\nQasc: A\ndataset for question answering via sentence compo-\nsition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8082\u20138090.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nXiang Lorraine Li, Adhiguna Kuncoro, Cyprien de Mas-\nson d\u2019Autume, Phil Blunsom, and Aida Nematzadeh.\n2021.\nDo language models learn commonsense\nknowledge? arXiv preprint arXiv:2111.00607.\nYunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen,\nWanqi Zhong, Chenyang Lyu, and Min Zhang. 2023.\nA comprehensive evaluation of gpt-4v on knowledge-\nintensive visual question answering. arXiv preprint\narXiv:2311.07536.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6862\u20136868.\nBill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,\nand Xiang Ren. 2021. Riddlesense: Reasoning about\nriddle questions featuring linguistic creativity and\ncommonsense knowledge. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 1504\u20131515.\nKevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,\nEhsan Azarnasab, Zhengyuan Yang, Jianfeng Wang,\nLin Liang, Zicheng Liu, Yumao Lu, et al. 2023. Mm-\nvid: Advancing video understanding with gpt-4v\n(ision). arXiv preprint arXiv:2310.19773.\nHugo Liu and Push Singh. 2004. Conceptnet\u2014a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211\u2013226.\nMengchen Liu and Chongyan Chen. 2023. An eval-\nuation of gpt-4v and gemini in online vqa. arXiv\npreprint arXiv:2312.10637.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nBonan Min,\nHayley Ross,\nElior Sulem,\nAmir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. 2021.\nRecent advances in natural language processing via\nlarge pre-trained language models: A survey. ACM\nComputing Surveys.\nOpenAI. 2023. Gpt-4 technical report.\nAbu Rayhan, Rajan Rayhan, and Swajan Rayhan. 2023.\nArtificial general intelligence: Roadmap to achieving\nhuman-level capabilities.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social iqa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 4463\u20134473.\nKe Shen and Mayank Kejriwal. 2023.\nAn experi-\nmental study measuring the generalization of fine-\ntuned language representation models across com-\nmonsense reasoning benchmarks. Expert Systems,\npage e13243.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias?\nIn Pro-\nceedings of the 28th International Conference on\nComputational Linguistics, pages 6863\u20136870.\nShane Storks, Qiaozi Gao, and Joyce Y Chai. 2019.\nCommonsense reasoning for natural language under-\nstanding: A survey of benchmarks, resources, and\napproaches. arXiv preprint arXiv:1904.01172, pages\n1\u201360.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge.\nIn Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158.\nAlexandre Tamborrino, Nicola Pellican\u00f2, Baptiste Pan-\nnier, Pascal Voitot, and Louise Naudin. 2020. Pre-\ntraining is (almost) all you need: An application to\ncommonsense reasoning. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3878\u20133887.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2022a. Self-consistency improves\nchain of thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nYuqing Wang and Yun Zhao. 2023a. Metacognitive\nprompting improves understanding in large language\nmodels. arXiv preprint arXiv:2308.05342.\nYuqing Wang and Yun Zhao. 2023b. Tram: Benchmark-\ning temporal reasoning for large language models.\narXiv preprint arXiv:2310.00835.\nYuqing Wang, Yun Zhao, Rachael Callcut, and Linda\nPetzold. 2022b. Integrating physiological time series\nand clinical notes with transformer for early predic-\ntion of sepsis. arXiv preprint arXiv:2203.14469.\nYuqing Wang, Yun Zhao, and Linda Petzold. 2022c.\nEnhancing transformer efficiency for multivari-\nate time series classification.\narXiv preprint\narXiv:2203.14472.\nYuqing Wang, Yun Zhao, and Linda Petzold. 2023. Are\nlarge language models ready for healthcare? a com-\nparative study on clinical language understanding.\narXiv preprint arXiv:2304.05368.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nShengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\n2023. Large language models are better reasoners\nwith self-verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n2550\u20132575.\nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao,\nWeixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng\nZhao, Ya Zhang, Yanfeng Wang, et al. 2023a. Can\ngpt-4v (ision) serve medical applications?\ncase\nstudies on gpt-4v for multimodal medical diagnosis.\narXiv preprint arXiv:2310.09909.\nJiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng\nWan, and Philip S Yu. 2023b.\nMultimodal large\nlanguage models:\nA survey.\narXiv preprint\narXiv:2311.13165.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. 2023.\nThe dawn of lmms:\nPreliminary\nexplorations with gpt-4v (ision).\narXiv preprint\narXiv:2309.17421, 9(1).\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShuquan Ye, Yujia Xie, Dongdong Chen, Yichong Xu,\nLu Yuan, Chenguang Zhu, and Jing Liao. 2023. Im-\nproving commonsense in vision-language models\nvia knowledge graph riddles. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 2634\u20132645.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019a. From recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 6720\u20136731.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019b. Hellaswag: Can\na machine really finish your sentence? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4791\u20134800.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nYun Zhao, Yuqing Wang, Junfeng Liu, Haotian Xia,\nZhenni Xu, Qinghang Hong, Zhiyang Zhou, and\nLinda Petzold. 2021. Empirical quantitative anal-\nysis of covid-19 forecasting models. In 2021 In-\nternational Conference on Data Mining Workshops\n(ICDMW), pages 517\u2013526. IEEE.\nWangchunshu Zhou, Ronan Le Bras, and Yejin\nChoi. 2023.\nCommonsense knowledge transfer\nfor pre-trained language models.\narXiv preprint\narXiv:2306.02388.\n"
  }
]