[
  {
    "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
    "link": "https://arxiv.org/pdf/2312.06550.pdf",
    "upvote": "52",
    "text": "LLM360: Towards Fully Transparent\nOpen-Source LLMs\nZhengzhong Liu\nPetuum & MBZUAI\nAurick Qiao\nPetuum\nWillie Neiswanger\nUSC & Petuum\nHongyi Wang\nCMU\nBowen Tan\nCMU\nTianhua Tao\nUIUC\nJunbo Li\nMBZUAI\nYuqi Wang\nPetuum\nSuqi Sun\nPetuum\nOmkar Pangarkar\nPetuum\nRichard Fan\nPetuum\nYi Gu\nUCSD\nVictor Miller\nPetuum\nYonghao Zhuang\nCMU\nGuowei He\nMBZUAI\nHaonan Li\nMBZUAI\nFajri Koto\nMBZUAI\nLiping Tang\nMBZUAI\nNikhil Ranjan\nMBZUAI\nZhiqiang Shen\nMBZUAI\nXuguang Ren\nMBZUAI\nRoberto Iriondo\nMBZUAI\nCun Mu\nMBZUAI\nZhiting Hu\nUCSD\nMark Schulze\nPetuum\nPreslav Nakov\nMBZUAI\nTim Baldwin\nMBZUAI\nEric P. Xing\nMBZUAI\nAbstract\nThe recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and researchers.\nHowever, most LLMs have only released partial artifacts, such as the final model\nweights or inference code, and technical reports increasingly limit their scope to\nhigh-level design choices and surface statistics. These choices hinder progress in\nthe field by degrading transparency into the training of LLMs and forcing teams to\nrediscover many details in the training process. We present LLM360, an initiative\nto fully open-source LLMs, which advocates for all training code and data, model\ncheckpoints, and intermediate results to be made available to the community. The\ngoal of LLM360 is to support open and collaborative AI research by making the\nend-to-end LLM training process transparent and reproducible by everyone. As a\nfirst step of LLM360, we release two 7B parameter LLMs pre-trained from scratch,\nAMBER and CRYSTALCODER, including their training code, data, intermediate\ncheckpoints, and analyses (at llm360.ai). We are committed to continually\npushing the boundaries of LLMs through this open-source effort. More large-scale\nand stronger models are underway and will be released in the future.\nA technical report of the LLM 360 project.\narXiv:2312.06550v1  [cs.CL]  11 Dec 2023\n1\nIntroduction\nThe landscape of Large Language Models (LLMs) has experienced a remarkable transformation in\nthe past one year, witnessing an unprecedented surge in both the popularity and capabilities of these\nmodels. At the forefront of this evolution are proprietary LLMs such as GPT-4 [1] and Claude [2],\nwhich have captured the attention of the AI community due to their power and versatility. At the same\ntime, the recent emergence of openly accessible yet highly capable LLMs such as LLaMA [3, 4],\nFalcon [5], and Mistral [6] allow researchers and practitioners at large to easily obtain, customize,\nand deploy LLMs in more diverse environments and for more diverse use cases.\nDespite the growing influence and accessibility of open-source LLMs, a notable trend has been to\nrestrict visibility and access to their training, fine-tuning, and evaluation processes, including crucial\ncomponents such as their training code and data. This practice limits the ability of the broader AI\nresearch community to study, replicate, and innovate upon advanced LLMs. A more transparent\napproach to sharing not just the final model but also training details and artifacts is crucial for\nfostering a more inclusive and collaborative research environment.\nMotivated by the above, we note the following specific challenges in LLM research today.\nData Provenance. Understanding the origins and characteristics of the training data is crucial for\nassessing the reliability and biases inherent in LLMs. A lack of transparency about data sources\nand composition hinders the ability to identify and mitigate biases which can be perpetuated in\nmodel outputs. Simultaneously, data leakage\u2014where training datasets overlap with benchmark\ndatasets\u2014can lead to misleading performance metrics that obscure a model\u2019s general effectiveness\n(studied in [7, 8]). These issues highlight the need for clear documentation of data origins and usage\nin LLM development.\nReproducibility. Even with full disclosure of data sources, the lack of access to complete training\ncode, configuration details, and specific datasets can make it challenging to reproduce the results\nreported in studies. For example, although the training data mixtures are disclosed by LLaMA [3],\nthe data processing and training code are not released. Yet, LLMs known to be trained using an open\nreproduction of LLaMA\u2019s data (e.g., RedPajama [9, 10]) still do not fully reproduce its benchmark\nevaluations [11], indicating that additional data processing or training procedures may be necessary.\nOpen Collaboration. The practice of only releasing final model weights not only leads to redundant\nefforts but also poses uniques challenges in conducting certain research. For instance, research\ninto the emergent abilities of LLMs [12, 13] or the investigation of how different training data\naffects model behavior [14, 15] becomes more challenging without access to intermediate training\ncheckpoints. Researchers are often forced to either work with the final model, which offers limited\ninsights into its developmental nuances, or start from scratch, leading to unnecessary duplication of\nwork and expenditure of compute.\nLLM3601 aims to address the issues above through a comprehensive open-source LLM effort.\nModels in LLM360 are published with all training and model details (e.g., hyperparameters, sched-\nules, architecture, and designs), all intermediate model checkpoints saved during training, and full\ndisclosure of the exact pre-training data used.\nOur contributions are:\n\u2022 We outline the LLM360 framework, focusing on its design principles and the rationale for\nfully open-sourcing LLMs. We detail the components of the framework, including datasets,\ncode and configurations, model checkpoints, and training metrics. This framework provides\na target for transparency that all present and future LLM360 models strive to meet.\n\u2022 We pretrain two new LLMs from scratch and release them under the LLM360 framework.\nAMBER is a 7B English LLM pretrained on 1.3T tokens. CRYSTALCODER is a 7B English\nand code LLM pretrained on 1.4T tokens. We discuss the development details, preliminary\nevaluations, observations, and lessons we learned from AMBER and CRYSTALCODER.\n\u2022 We release all training code, pretraining data, model checkpoints, and evaluation metrics\ncollected during pretraining for both AMBER and CRYSTALCODER. Notably, AMBER is\nreleased with 360 model checkpoints saved during training, and CRYSTALCODER with 143.\n1The name LLM360 signifies open-sourcing LLMs from all angles, and that 360 data points (i.e., checkpoints,\ndata chunks, evaluation results) are released for many of our models.\n2\nWe aim to make a continuous commitment to fully open-source LLMs by releasing multiple LLMs at\nvarious scales. As the first step, in this technical report, we discuss AMBER and CRYSTALCODER,\nthe first open-source LLMs in the LLM360 series. In the future, we plan to release more pre-trained\nLLMs that are larger in scale, exhibit better performance, and focus on various domains.\nThe rest of this report is organized as follows. In \u00a72, we discuss related works and the predecessors\nthat inspired LLM360. In \u00a73, we provide a description of the LLM360 framework and the release\nartifacts that fall into its purview. In \u00a74, we discuss the first two LLMs released under LLM360,\nAMBER (\u00a74.1) and CRYSTALCODER (\u00a74.1.5), and preliminary analyses of both. \u00a76 concludes.\n2\nRelated Work\nThe closest project to LLM360 is Pythia, which also aims at full reproducibility of LLMs [16].\nThe Pythia project provided 154 checkpoints for model sizes from 70M to 12B to better support\nresearch on the scaling behavior and learning dynamics of LLMs. While Pythia is a pioneering\nwork, it no longer reflects many recent LLM practices, such as training over trillion-token datasets\nor training on language and code in different stages. On the other hand, LLM360 defines a release\nframework prioritizing transparency and reproducibility under which up-to-date models can continue\nto be released, and our 7B AMBER model surpasses the 12B Pythia model in public benchmarks [17].\nOverall, Pythia set an early precedent for transparency and reproducibility of LLMs that we aim to\nperpetuate and expand in LLM360 to modern LLM pretraining regimes.\nLLM\nRelease\nPretraining\nCheckpoints\nPretraining Dataset\nTokens\nName\nDate\nCode\nConfig\nModel\nOptim\nData Mix\nOrdering\nAvailable\n(T )\nGPT-J [18]\nMay\u201921\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.40\nGPT-NeoX [19]\nApr\u201922\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.40\nOPT [20]\nMay\u201922\n\u2713\n\u2713\n\u2713\n\u2713\n0.18\nBLOOM [21]\nNov\u201922\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.34\nPythia [16]\nFeb\u201923\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.30\nLLaMA [3]\nFeb\u201923\n\u2713\n\u2713\n1.0\nOpenLLaMA [11]\nMay\u201923\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1.0\nINCITE [10]\nMay\u201923\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1.0\nMPT [22]\nMay\u201923\n\u2713\n\u2713\n\u2713\n1.0\nFalcon [23]\nMay\u201923\n\u2713\n\u2713\n1.5\nLlama 2 [4]\nJul\u201923\n\u2713\n2.0\nQwen [24]\nAug\u201923\n\u2713\n2.4\nMistral [6]\nSep\u201923\n?\nYi [25]\nNov\u201923\n?\nAMBER\nDec\u201923\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1.3\nCRYSTALCODER\nDec\u201923\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1.4\nTable 1: Summary of notable open-source LLMs. We note a trend of progressively less disclosure of important\npretraining details over time: (1) availability of pretraining code, (2) disclosure of training configurations and\nhyperparameters, (3) intermediate checkpoints of model weights, (4) intermediate checkpoints of optimizer\nstates, (5) disclosure of data mixture and sources, (6) reproducibility of pretraining data sequence, and (7)\navailability (or reconstruction scripts) of the pretraining data.\nIn general, open-source LLMs span a wide spectrum of transparency and reproducibility when it\ncomes to their release artifacts. Many recent LLMs only release their final model architecture and\nweights, keeping their data sources and most training details undisclosed [4, 24, 6, 25]. Some are\ntrained on publicly available datasets [18, 19, 21, 16, 11, 10, 26], whereas others disclosed their data\nmixtures but do not make training-ready data available to the public [20, 3, 22, 23]. Several LLMs\nof note have been released with substantially more transparent details and artifacts. For example,\nEleutherAI models such as GPT-J [18] and GPT-NeoX [27] included training code, datasets, and\nup to 150 intermediate model checkpoints. The value of the open-source GPT-NeoX training code\nwas demonstrated by its use in subsequent LLM pretraining by others in the community [10, 22].\nINCITE [10], MPT [22], and OpenLLaMA [11] were released with training code and training dataset,\nwith RedPajama also releasing 10 intermediate model checkpoints.\n3\nOverall, we observe a trend that more recent and capable LLMs are becoming more closed in their\nrelease artifacts. In contrast, the goal of LLM360 is to release modern and high-quality models while\nmaintaining a high degree of release transparency.\n3\nThe LLM360 Framework\nIn this section we present LLM360, a framework for releasing LLMs that promotes open-source\ntransparency, reproducibility, data/model provenance, and collaborative research. LLM360 provides\nguidance and recommendations for release artifacts that are collected during LLM pre-training and\nsubsequently made publicly available to the community.\nAs part of the launch of LLM360, we also release two new pre-trained LLMs, which we hope\nwill foster immediate interest and collaboration in the open-source research community. First,\nAMBER, an English language LLM with 6.7B parameters trained on 1.25 trillion tokens. Second,\nCRYSTALCODER, an English and code LLM, also with 6.7B parameters, trained on 1.4 trillion tokens.\nDetails on AMBER and CRYSTALCODER are reported in \u00a74.\nTraining Dataset and Data Processing Code\nThe pretraining dataset is the main ingredient of\nan LLM and significantly impacts its capabilities. Thus, it is important for users and adopters to\nhave visibility into pretraining data to assess potential behavior issues and biases. For example,\nrecent concerns about benchmark data leakage into LLM pretraining is much easier to study when\npretraining datasets are available for exploration [8, 7].\nFurthermore, visible pretraining data improves the extensibility of LLMs in later fine-tuning and\ndomain adaptation. Recent work suggests that training on repeated data disproportionately degrades\nfinal model performance [28]. Given the breadth of data modern pretraining is performed on, visibility\ninto the original pretraining data is essential for avoiding repeated data in downstream fine-tuning or\ncontinued pretraining on specialized domains.\nLLM360 advocates for the public release of the data LLMs are pretrained on. When applicable,\ndetails about data filtering, processing, and training order should be released as well. Doing so equips\nthe community with better tools to assess the capabilities and risks of LLMs and to reproduce and\nbuild upon existing LLMs for future use cases.\nFigure 1: Artifacts relea-\nsed by the LLM360\nproject include data chu-\nnks, model checkpoints,\nand metrics, at over 360\ntime stamps of training\n(and code for all parts).\nTraining Code, Hyperparameters, and Configurations\nThese code\nand settings have a significant impact on the performance and quality\nof LLM training, and are not always publicly disclosed. For example,\nwe observed that a carefully balanced hybrid data-model-pipeline (3D)\nparallelism [29] can outperform the standard FSDP in PyTorch by up to\n15% on our Nvidia A100 clusters. Another example we observed is that\nit is essential to keep the inverse frequency matrix in RoPE positional\nembedding in FP32 [30], which aligns with the observation in Qwen [24].\nIn LLM360, we open-source all our LLM pre-training frameworks,\nhyperparameters, as well as the configurations. These include the entire\ntraining source code, training parameters such as learning rates and batch\nsizes, and system configurations such as parallelism dimensions.\nModel Checkpoints\nIt is typical during LLM training to periodically\nsave checkpoints of the model to persistent storage. These checkpoints\nare not only crucial for recovery from faults during training, but also\nuseful in post-training research such as studying different data and/or\nhyperparameter schedules, or reproducing infrequently-occurring training\nfaults (e.g., loss spikes, NaN results). Recent research on model quanti-\nzation and compression heavily relies on analysis of model weights and\nthe dynamics during training [31, 32].\nLLM360 models are published with all intermediate checkpoints saved\nduring their training, including model weights and optimizer states (when\napplicable, e.g., Adam [33] moving averages). These checkpoints enable continued training from\n4\na range of starting points without training from scratch, making it easier to study and reproduce a\nwider variety of effects during training.\nMetrics\nLLMs undergo training over weeks to months, and the trends and evolution patterns over\nthis training period can offer valuable information. However, access to detailed logs and intermediate\nmetrics for LLMs is currently limited to groups involved in pretraining, hindering a comprehensive\nstudy of LLMs. These statistics often contain key insights that cannot be directly derived otherwise,\nand even a simple analysis on the metrics, such as computing metric variances or norms, can reveal\nsignificant findings. For instance, the team behind GLM proposed an effective gradient shrinking\nalgorithm for handling loss spikes and NaN losses by analyzing gradient norm behaviors [34].\nOur aim with LLM360 is to alleviate this problem by completely open sourcing the logs and\nmetrics we collect. This includes system statistics (e.g., GPU workload), training logs (e.g., loss,\ngradient norm), and evaluation metrics (e.g., perplexity, downstream tasks). Access to these logs may\nfacilitate a deeper understanding of the whole training process, including how LLMs evolve during\nvarious training scenarios. We provide easy access to the figures by sharing directly on the LLM360\nWeights & Biases page2. A few example metrics include downstream evaluation results, training loss,\ngradient norm, etc.\nIn \u00a74.3, we introduce how one can make use of the metrics, and illustrate an experiment tracking the\nmemorization behavior of a model throughout training. The metrics are released in coordination with\nthe data chunks and checkpoints for researchers to easily find their correspondence. Furthermore, we\nprovide open access to the analysis and evaluation code used to foster reproducibility. The code and\nall the metrics can be found at an LLM360 repository: Analysis360.\n4\nInitial Model Release\n4.1\nAmber\nFigure 2: AMBER is a 7B\nparameter English open-\nsource LLM.\nIn this section, we introduce AMBER, the first model in the LLM360\nfamily, as well as the finetuned models AMBERCHAT and AMBERSAFE.\n4.1.1\nDetails on Data Preparation and Model Architectures\nBelow we review the details of our pre-training dataset, including data\npreprocessing, format, data mixing ratios, along with architectural details\nof our LLM model and specific pre-training hyperparameters. The exact\nsetup of AMBER can be found in the LLM360 code base.\nDetails on our pre-training dataset\nWe conduct the data preparation\nprocess similar to OpenLLaMA3. Specifically, our pretraining data is a\nmixture of RefinedWeb, StarCoder, and RedPajama-v1. A slight difference with OpenLLaMA-v2 is\nour inclusion of C4, since we do not intend to introduce dupliciated documents after the deduplication\nprocess conducted by RefinedWeb. We simply put together all the original aforementioned datasets\n(without any further cleaning, filtering, or sub-sampling), conduct a global permutation, and partition\nthem evenly into 360 data chunks. In total, we have 1.26 Trillion tokens. Table 2 presents the\ncombination.\nThe LLM architecture\nWe used the exact same model architecture as LLaMA 7B4. Detailed LLM\narchitectural configurations are summarized in Table 3, incorporating rotary positional embeddings\n(RoPE) at each layer of the network [30].\nPre-training procedure and hyperparameters\nWe followed the pre-training hyperparameters\nfrom LLaMA as closely as possible [3]. AMBER is trained using the AdamW optimizer with the\nfollowing hyperparameters: \u03b21 = 0.9, \u03b22 = 0.95. The initial learning rate is set to \u03b7 = 3e\u22124,\nfollowing a cosine learning rate schedule that decreases to a final rate of \u03b7 = 3e\u22125. We apply a\n2https://wandb.ai/llm360/projects\n3https://github.com/openlm-research/open_llama#dataset-and-training\n4The architectural details are directly fetched from https://huggingface.co/huggyllama/llama-7b\n5\nweight decay of 0.1 and use gradient clipping at 1.0. The model is warmed up over 2, 000 steps.\nDiffering from the LLaMA setup, based on our hardware setting with 224 GPUs, we use a pre-training\nbatch size of 2, 240 (224 \u00d7 10) instead of 2, 048.\nSubset\nTokens (Billion)\nArxiv\n30.00\nBook\n28.86\nC4\n197.67\nRefined-Web\n665.01\nStarCoder\n291.92\nStackExchange\n21.75\nWikipedia\n23.90\nTotal\n1259.13\nTable 2: Data mix in AMBER pre-training.\nHyperparameter\nValue\nNumber Parameters\n6.7B\nHidden Size\n4096\nIntermediate Size (in MLPs)\n11008\nNumber of Attention Heads\n32\nNumber of Hidden Layers\n32\nRMSNorm \u03f5\n1e\u22126\nMax Seq Length\n2048\nVocab Size\n32000\nTable 3: LLM architecture & hyperparameters.\n4.1.2\nDetails on the Pre-training Infrastructure\n0\n50\n100\n150\n200\n250\n300\n350\nCheckpoint\n20\n21\n22\n23\nTraining Loss\nAmber Training Loss Across All Checkpoints\nFigure 3: The training loss of AMBER over all\nmodel checkpoints.\nAMBER is trained on an in-house GPU cluster.\nThe GPU cluster\nThe GPU cluster consists\nof 56 DGX A100 nodes, each equipped with\n4\u00d7 80GB A100 GPUs. Each GPU is connected\nwith 4 links NVLink. Cross node connection set-\nting is 2 port 200 Gb/sec (4\u00d7 HDR) InfiniBand.\nThe throughput we manage to achieve with our\ndistributed training framework is around 582.4k\ntokens per second.\nThe pretraining framework\nOur pretraining\nframework is lit-llama5 developed based on Py-\nTorch Lightning. We used mixed-precision dur-\ning pre-training with BF16 for activations and gradients and FP32 for model weights [35].\n4.1.3\nFinetuned AMBER models\nWe also release a few finetuned versions of AMBER, namely AMBERCHAT and AMBERSAFE.\nAMBERCHAT is trained on the evolved instruct training data as used by WizardLM [36]. We use\nFastChat [37] to finetune the model for 3 epochs on 8 A100s (80G) distributed by FSDP [38],\nthe learning rate is 2 \u00d7 10\u22125, gradient accumulation steps is 16, warmup ratio is 0.04. We also\nfinetune an aligned version of the model: AMBERSAFE, by conducting Direct Parameter Optimization\n(DPO) [39]. AMBERSAFE is trained on ShareGPT 90K6, and further optimized on the SafeRLHF\ndataset [40]. We set \u03b2 to 0.1, gradient accumulation steps to 4, and the learning rate to 5 \u00d7 10\u22127.\n4.1.4\nResults and Analysis\nBenchmark Results\nWe use four benchmark datasets in the Open LLM Leaderboard7 as our\nevaluation on different aspects, i.e., ARC, HellaSwag, MMLU, and TruthfulQA, following the\nleaderboard settings. We run the evaluation on all 360 checkpoints, to observe the model ability\nacross the pretraining process. As shown in Figure 4, we can see that the HellaSwag and ARC\n5https://github.com/Lightning-AI/lit-llama\n6The base model for this is checkpoint 355 instead of the last checkpoint\n7https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n6\nevaluation scores monotonically increase during pre-training, while the TruthfulQA score seems\nto decrease as the training proceeds. Another interesting trend is observed in the MMLU progress,\nwhere the score decreases in the initial stage of pretraining and then starts to increase.\n0\n50\n100\n150\n200\n250\n300\n350\nCheckpoint\n0.25\n0.30\n0.35\n0.40\nARC\nAmber on AI2 Reasoning Challenge (25-shot)\n0\n50\n100\n150\n200\n250\n300\n350\nCheckpoint\n0.3\n0.4\n0.5\n0.6\n0.7\nHellaSwag\nAmber on HellaSwag (10-shot)\n0\n50\n100\n150\n200\n250\n300\n350\nCheckpoint\n0.24\n0.26\n0.28\n0.30\nMMLU\nAmber on MMLU (5-shot)\n0\n50\n100\n150\n200\n250\n300\n350\nCheckpoint\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nTruthfulQA\nAmber on TruthfulQA (0-shot)\nFigure 4: Results for AMBER on the Open LLM leaderboard metrics.\nIn Table 4, we compare the final model performance of AMBER to a set of models trained around\nsimilar time, namely OpenLLaMA, RedPajama-INCITE, Falcon, MPT. Many are inspired by the\ndesign of LLaMA. We found that AMBER is relatively competitive in scores such as MMLU, but\nits performance on ARC is behind the curve. We also find that our finetuned AMBER models\nare relatively strong, even compared with other similar models. In our early study, we note that\nAMBERCHAT simply trained on ShareGPT 90K also demonstrates much higher performance than\nour base model, which is slightly different from the trends shown on other models in the table. We\nleave further investigation of this to future work.\nThe LLMs\nARC\nHellaSwag\nMMLU\nTruthfulQA\nAvg.\nLLaMA2-7B-chat\n52.9\n78.55\n48.32\n45.57\n56.34\nLLaMA2-7B\n53.07\n77.74\n43.8\n38.98\n53.39\nAMBERSAFE\n45.22\n74.14\n37.78\n55.44\n53.15\nLLaMA-7B\n50.94\n77.8\n35.67\n34.34\n49.69\nAMBERCHAT\n42.83\n74.03\n38.88\n40.72\n49.12\nOpenLLaMA-v2-7B\n43.69\n72.2\n41.29\n35.54\n48.18\nMPT\n47.7\n77.57\n30.8\n33.44\n47.38\nFalcon-7B\n47.87\n78.13\n27.79\n34.26\n47.01\nRedPajama-INCITE-7B-Instruct\n44.11\n72.02\n37.61\n33.96\n46.93\nFalcon-7B-instruct\n46.16\n70.85\n25.66\n44.07\n46.69\nOpenLLaMA-v1-7B\n47.01\n71.98\n30.49\n34.85\n46.08\nAMBER\n41.89\n74.14\n30.76\n34.00\n45.20\nRedPajama-INCITE-7B-Base\n46.25\n71.63\n27.68\n33.03\n44.65\nRedPajama-INCITE-7B-Chat\n42.06\n70.82\n26.94\n36.09\n43.98\nTable 4: Open LLM leaderboard comparisons for a few LLMs developed around the same time.\n7\n4.1.5\nIssues Encountered During Pre-training\nIn this section, we discuss several major issues encountered during the pre-training process of AMBER.\nThese issues could potentially impact our final model performance. We have addressed most of these\nissues in subsequent LLM pre-training efforts.\nNaN loss on a few data chunks\nDuring the pre-training procedure, we encountered NaN loss in\nfour out of 360 data chunks. Whenever we faced this issue, we tentatively skipped the entire data\nchunk. Initially our plan was to train on these four data chunks in later stage of the training, however,\nwe found that these data chunks tend to cause NaN loss regardless of the position of training. We end\nup finishing our training by taking the first four chunks from the training sequence to complete our\nlearning rate schedule.\nMissing optimizer states\nIn our pre-training framework, we did not manage to save the optimizer\nstates; we only saved model checkpoints for each data chunk. This oversight might be the cause of\nthe NaN loss issue observed in the four data chunks, as mentioned earlier. Each time we resumed\npre-training from a previous model checkpoint, the optimizer state in the AdamW optimizer was\nre-initialized. This re-initialization could potentially affect model training stability.\nDiscrepancies on the precision of checkpoints\nIn the initial phase of pre-training, our codebase\nhad an issue where model checkpoints were saved with BF16 precision, despite our mixed precision\ntraining process maintaining model weights at FP32. This issue was later identified and rectified\nby our team, ensuring that all subsequent model checkpoints were saved with FP32 precision. We\nanticipate that the initial BF16 model checkpoints may have contributed to some degree of accuracy\ndrop in the model.\n4.2\nCRYSTALCODER\nFigure 5:\nCRYSTAL-\nCODER is a 7B param-\neter English and code\nopen-source LLM.\nThis section provides a summary of the dataset and the model architecture\nutilized in CRYSTALCODER. For a detailed evaluation of results on bench-\nmarks and a comparison with previous works on specific benchmarks, we\nrefer readers to our future reports.\n3-Stage Pre-training Dataset\nThe pre-training dataset employed in\nCRYSTALCODER is a blend of SlimPajama [41] and StarCoder data [42]\nwith around 1382B tokens in total. Diverging from previous approaches\nsuch as Code Llama [43], which strictly sequentially trains on English and\ncoding data, we adopt a more gradual approach by seamlessly combining\nand training on both types of data, to provide a balance between code and\ngeneral ability. The training process is divided into three stages. In the first\nstage, we train on half of the SlimPajama data, totaling around 345 billion\ntokens. Moving to the second stage, the remaining half of the SlimPajama\ndata is utilized, along with two epochs of StarCoder data, resulting in approximately 927 billion\ntokens. In the third stage, we train on Python and web-related data, encompassing HTML, JavaScript,\nand CSS subsets from StarCoder, totaling 100 billion tokens. Additionally, we sample 10 billion\ntokens from the SlimPajama dataset in this stage. The preprocessed data and data mixing scripts are\nreleased in the Huggingface and Github repository of LLM360.\nModel Architecture\nCRYSTALCODER employs a model architecture closely resembling LLaMA\n7B, with the incorporation of maximal update parameterization (muP) [44]. In addition to this specific\nparameterization, we have made several slight modifications, the application of RoPE restricted to\nthe first 25% of hidden dimensions (similar to the implementation of GPT-NeoX [27]), and the use\nof a sequence length of 2048 with an embedding dimension of 32032. In addition, we simply use\nLayerNorm instead of RMSNorm since the CG-1 architecture supports efficient computation for\nvanilla LayerNorm.\nCompute Infrastructure\nCRYSTALCODER is trained on the Cerebras Condor Galaxy 1 (CG-1), a\n4 exaFLOPS, 54 million core, 64-node cloud AI supercomputer8.\n8https://www.cerebras.net/condor-galaxy-1\n8\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nARC\nCrystalCoder on AI2 Reasoning Challenge (25-shot)\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nHellaSwag\nCrystalCoder on HellaSwag (10-shot)\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.2\n0.3\n0.4\n0.5\nMMLU\nCrystalCoder on MMLU (5-shot)\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\nTruthfulQA\nCrystalCoder on TruthfulQA (0-shot)\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.0\n0.1\n0.2\n0.3\nHumanEval pass@1\nCrystalCoder on HumanEval pass@1\n0\n20\n40\n60\n80\n100\n120\n140\nCheckpoint\n0.0\n0.1\n0.2\n0.3\nMBPP pass@1\nCrystalCoder on MBPP pass@1\nFigure 6: Results for CRYSTALCODER on the Open LLM leaderboard metrics. Grey vertical dashed\nlines denote the transition between the three stages of training.\nOpen LLM Leaderboard and Code Evaluations\nWe also benchmark this model on the four\nbenchmark datasets in the Open LLM Leaderboard (similar to AMBER), as well as coding benchmark\ndatasets, including HumanEval pass@1, and MBPP pass@1. We show results in Figure 6.\nThe LLMs\nLanguage Tasks\nCode Tasks\nAvg.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nAvg.\nHumanEval\nMBPP\nAvg.\nMistral-7B\n59.98\n83.31\n64.16\n42.15\n63.40\n29.12\n38.78\n33.95\n48.68\nCRYSTALCODER (7B)\n47.01\n71.97\n48.78\n35.91\n50.92\n28.38\n36.38\n32.38\n41.65\nCodeLlama-7B\n39.93\n60.80\n31.12\n37.82\n42.42\n33.50\n41.40\n37.45\n39.94\nOpenLLaMA-v2-7B\n43.69\n72.20\n41.29\n35.54\n48.18\n15.32\n12.69\n28.01\n38.10\nLLaMA2-7B\n53.07\n77.74\n43.80\n38.98\n53.39\n13.05\n20.09\n16.57\n34.98\nLLaMA-7B\n50.94\n77.80\n35.67\n34.34\n49.69\n10.61\n17.04\n13.83\n31.76\nFalcon-7B\n47.87\n78.13\n27.79\n34.26\n47.01\n9.42\n13.39\n11.41\n29.21\nStarCoder-15B\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n33.63\n43.28\n38.46\n\u2013\nTable 5: Evaluation comparisons among a few notable code and language models. The last column is\nthe average of the language task average and the code task average. CRYSTALCODER strikes a good\nbalance between both language and code tasks.\n9\nFigure 7: Each row corresponds to the distribution of memorization scores of a checkpoint. We\nannotate the percentage of score = 1 (k-extractible) for clearer demonstration.\nFigure 8: Memorization score on data\nchunk for each checkpoint. The marked\nspots indicate the latest chunk seen by\nthat checkpoint. The part on right of\neach mark indicates unseen data.\n(a) Memorization score\n(b) k-extractible\nFigure 9: The correlation of sequences in terms of mem-\norization score and k-extractible between each check-\npoints\n4.3\nANALYSIS360\nPrior work such as Pythia [16] has shown that an insightful study can be done by analyzing the\nintermediate checkpoints of a model. We hope LLM360 can also provide the community useful\nresources for both reference and research purposes. To this end, we release the initial version of the\nANALYSIS360 project, an organized repositories that analyze the model behavior on various aspects,\nincluding model characteristics and downstream evaluation results.\nAs an example of the analysis that can be performed over the set of model checkpoints, we conduct\nan initial study on memorization in LLMs. Recent work [45, 46] shows that LLMs may memorize\na significant part of their training data, which can be extracted with appropriate prompting. Such\nmemorization not only raises privacy concerns in leaking private training data, but also downgrades\nthe performance of LLMs if the training data contains unintended duplicates or peculiarities. As we\nrelease all checkpoints and data, we can conduct a comprehensive analysis of memorization across\nthe whole stage of training.\nWe adopt the memorization score introduced in [12], indicating the accuracy of tokens in the\ncontinuation of length l with a prompt of length k,\nscore(k, l) = 1\nl\nl\nX\ni\n1[Sk+i = Gk+i],\nwhere S0:k+l is the sequence from training data, while Gk:k+l is the generated sequence with prompt\nS0:k. A memorized or k-extractible [45] sequence has a memorization score of 1. Following [12, 16],\nwe conduct our experiments with k = l = 32. We sampled 1000 sequence from each of the 360 data\nchunks, and use the first 64 tokens of each sequence to conduct the following experiments.\nWe show the distribution of memorization scores for 10 selected checkpoints in Figure 7, and\nadditionally annotate the percentage of score = 1. For every checkpoint, we only include the data\nchunks it has already been trained on. From the result, we learn that 1) More than 1% of the sequences\n10\nare 32-extractible from AMBER; 2) AMBER can memorize more sequences with the training going;\n3) The spike at score = 1 indicates that AMBER can memorize a much larger number of tokens than\nour preset threshold 32 (consistent with prior work [46, 12]).\nWe group the data chunks according to the selected checkpoints, and plot the memorization score on\neach data chunk group for each checkpoint in Figure 8. We find that 1) AMBER checkpoints memorize\nthe latest seen data much more than previous data; 2) For each data chunk, the memorization score\ndrops a bit with additional training, but keeps increasing afterwards.\nWe show the correlation between sequences in terms of memorization score or k-extractible in\nFigure 9. We witness a strong correlation between the checkpoints.\n5\nSummary and Take-home Messages\nIn this section, we summarize the observations and a few take-home messages from our pre-training of\nAMBER and CRYSTALCODER, our initial modeling efforts in the LLM360 series. We understand that\npre-training is a computationally daunting task that many academic labs or small organizations cannot\nafford to conduct. We hope that LLM360 can provide comprehensive knowledge, allowing users to\nunderstand what happens during LLM pre-training (e.g., loss curve behaviors, how the evaluation\nmetrics emerge, etc.) without the need to do so themselves. We also provide some potential use cases\nshowing how researchers and developers can use LLM360 for their own projects.\nTake-home Messages\nBelow we list a few of the lessons learned during our initial model training.\n\u2022 In the pre-training of AMBER, NaN losses were periodically observed, which may have\nbeen caused by certain random states, the training precision, or data quality issues. Some\nsolutions include switching to a different random seed or skipping those data chunks. We\nnotice some \u201cmisbehaved\u201d data chunks can cause NaN loss regardless of when they are\ntrained. In a preliminary experiment, we move the \u201cmisbehaved\u201d data chunks to the end of\nthe training but still observe NaN losses.\n\u2022 In the pre-training of CRYSTALCODER and our subsequent LLM pre-training efforts, we\nobserved that a hybrid and carefully tuned parallelism strategy\u2014combining data, tensor-\nmodel, and pipeline (also referred to as 3D) parallelism strategies [29]\u2014achieves better\nsystem throughput than FSDP, especially in distributed clusters with limited intra-node\nbandwidth.\n\u2022 Data cleaning (and/or data quality filtering), along with data mixing ratios, are crucial aspects\nof LLM pre-training, as is the scheduling for various pre-training data categories (e.g.,\nCommonCrawl, Books, StarCoder, etc.). In AMBER pre-training, we attempted to adhere as\nclosely as possible to the hyperparameters used in LLaMA; however, our performance still\nlags significantly behind LLaMA\u2019s. A key omission in LLaMA\u2019s technical report is a detailed\ndescription of their exact pre-training dataset. Our carefully crafted CRYSTALCODER pre-\ntraining dataset, which mixes English and coding data, achieves competitive performance\nwith LLaMA on both the Open LLM Leaderboard and Code Evaluation benchmarks.\nWe, along with the entire LLM open-source community, are diligently exploring the best\napproaches for data cleaning, data quality filtering, and determining optimal data mixing\nratios, a pioneering effort exemplified by the DoReMi method [15].\nPotential Use Cases of LLM360\nWe describe a few potential use cases of LLM360 below.\n\u2022 One can conduct experimental studies at any stage of model training. As previously\nmentioned, the optimal data mixing ratio remains a significant open problem in LLM\npre-training. However, it is nearly impossible to verify a specific mixing ratio by conducting\nfull LLM pre-training. A more feasible approach is to adjust the data mixing ratios on the fly,\ni.e., starting from an intermediate checkpoint, and either increasing or decreasing a specific\ndata ratio from a particular category, e.g., increasing the data weight in Wikipedia.\n\u2022 For building domain-specific LLMs (e.g., medical, finance, law, etc.), one may not necessar-\nily want to start from the last pre-trained LLM checkpoint (which would make it more akin\nto fine-tuning). Instead, one can always pick one of the LLM360 checkpoints (e.g., from\n50% of the pre-training stage) and resume the pre-training to obtain a domain-specific LLM.\n11\n\u2022 A lot of algorithmic approximation frameworks for efficient training require partially trained\nmodel weights [47, 48]. LLM 360 provides perfect model initializations for those methods.\nLLM360 and Responsible Usage\nGiven the wide-ranging applicability and high performance of\nLLMs, applications powered by them have the potential to deeply influence various aspects of life.\nConsequently, it becomes essential for all parties involved in the chain of production of LLMs to\ncarefully manage the potential impact and risks associated with them. All stakeholders need to be\ninformed of these implications and take necessary actions accordingly.\nWe believe the transparent nature of the LLM360 initiative can help make the potential risks known\nto stakeholders. As one example, many risks associated with LLMs are related to certain forms of\nbiases [49], such as the risk of social stereotypes, discrimination and exclusion, and the risk of under-\nrepresenting certain languages or domains. By inspecting the exact training data and bias analysis\n(e.g. BOLD [50]) in ANALYSIS360, stakeholders can have a thorough review of these risks before\ndeploying the models. LLM360 can also help with risk mitigation. The project shares reproducible\ntraces and exact data during LLM training, providing a reusable environment for researchers to\nconduct experiments to design better guardrails to contain potential risks.\nWe understand the importance of controlling the risk of LLMs and we are committed to further\ndeveloping the LLM360 framework to foster responsible usage of LLMs. We would like invite the\ncommunity to work with us, by sharing research results or by simply providing feedback.\n6\nConclusion and Future Work\nIn this paper, we introduce LLM360, an initiative for comprehensive and fully open-sourced LLMs.\nAlong with the first release of LLM360, we released two 7B LLMs: AMBER (an English general-\npurpose LLM) and CRYSTALCODER (an LLM pre-trained specifically for code generation). In terms\nof artifacts, we released pre-training code, configurations, hyperparameters, intermediate model\ncheckpoints, optimizer states, as well as the data sequence and data processing code. Our vision is to\nsignificantly advance and promote transparency within the open-source LLM pre-training community.\nFor future work, we are conducting a more detailed analysis on AMBER and CRYSTALCODER\u2019s base\nmodels as well as their fine-tuned models. Detailed results will be released and discussed in their\nrespective technical reports. Our team is also pre-training a much larger LLM, which will be fully\nreleased as soon as the pre-training is complete. Additionally, we will explore the optimal ratios for\nmixing different subsets in the pre-training datasets.\nAcknowledgements\nWe would like to thank Natalia Vassilieva, Joel Hestness, William Marshall, and Bhargav Kanakiya\nfor their contribution to CRYSTALCODER and support on the LLM360 project. We would also like\nto thank the MBZUAI and Cerebras team for providing and managing the computing infrastructure.\nReferences\n[1] OpenAI. Gpt-4 technical report, 2023.\n[2] Claude. Claude 2.1 model card. Technical report, Claude Inc., 2023.\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\n12\n[6] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[7] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng\nCheng, Weiwei L\u00fc, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun\nCheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma,\nChuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han\nFang, and Yahui Zhou. Skywork: A more open bilingual foundation model, 2023.\n[8] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin,\nJi-Rong Wen, and Jiawei Han. Don\u2019t make your llm an evaluation benchmark cheater, 2023.\n[9] Together Computer. Redpajama: an open dataset for training large language models, 2023.\n[10] Together Computer. Redpajama-incite-7b-base, 2023.\n[11] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023.\n[12] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin An-\nthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large\nlanguage models. arXiv preprint arXiv:2304.11158, 2023.\n[13] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels, 2022.\n[14] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming\nShen, and Chao Zhang. Large language model as attributed training data generator: A tale of\ndiversity and bias, 2023.\n[15] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,\nQuoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up\nlanguage model pretraining. arXiv preprint arXiv:2305.10429, 2023.\n[16] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling. In\nInternational Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.\n[17] Edward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen\nRajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https:\n//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.\n[18] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n[19] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-\nneox-20b: An open-source autoregressive language model, 2022.\n[20] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[21] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana\nIli\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. Bloom:\nA 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022.\n[22] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially\nusable llms, 2023. Accessed: 2023-05-05.\n[23] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra\nCojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malar-\ntic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon\nseries of open language models, 2023.\n13\n[24] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[25] 01.ai. 01-ai/yi: A series of large language models trained from scratch by developers @01-ai,\n2023.\n[26] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang,\nBowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc:\nUnderstanding data combinations for llm training, 2023.\n[27] Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao,\nEric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler,\nJason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges,\nBenjamin Th\u00e9rien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive\nLanguage Modeling in PyTorch, 9 2023.\n[28] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk,\nNelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann,\nChris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCan-\ndlish. Scaling laws and interpretability of learning from repeated data, 2022.\n[29] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,\nVijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro,\net al. Efficient large-scale language model training on gpu clusters using megatron-lm. In\nProceedings of the International Conference for High Performance Computing, Networking,\nStorage and Analysis, pages 1\u201315, 2021.\n[30] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n[31] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix\nmultiplication for transformers at scale, 2022.\n[32] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad,\nYangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza-\ntion aware training for large language models, 2023.\n[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n[34] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan\nXu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang\nChen, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained\nmodel. arXiv preprint arXiv:2210.02414, 2023.\n[35] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740, 2017.\n[36] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\narXiv preprint arXiv:2304.12244, 2023.\n[37] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[38] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania,\nBernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp:\nExperiences on scaling fully sharded data parallel, 2023.\n[39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model,\n2023.\n[40] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun,\nYizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a\nhuman-preference dataset, 2023.\n14\n[41] Daria\nSoboleva,\nFaisal\nAl-Khateeb,\nRobert\nMyers,\nJacob\nR\nSteeves,\nJoel\nHestness,\nand\nNolan\nDey.\nSlimPajama:\nA\n627B\ntoken\ncleaned\nand\ndeduplicated\nversion\nof\nRedPajama.\nhttps://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,\n2023.\n[42] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be\nwith you! arXiv preprint arXiv:2305.06161, 2023.\n[43] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models\nfor code. arXiv preprint arXiv:2308.12950, 2023.\n[44] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.\n[45] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-\nine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training\ndata from large language models. In 30th USENIX Security Symposium (USENIX Security 21),\npages 2633\u20132650, 2021.\n[46] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\nChiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint\narXiv:2202.07646, 2022.\n[47] Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferfish: Communication-\nefficient models at no extra cost. Proceedings of Machine Learning and Systems, 3:365\u2013386,\n2021.\n[48] Hongyi Wang, Saurabh Agarwal, Yoshiki Tanaka, Eric Xing, Dimitris Papailiopoulos, et al.\nCuttlefish: Low-rank model training without all the tuning. Proceedings of Machine Learning\nand Systems, 5, 2023.\n[49] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor,\nAmelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown,\nZac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell,\nWilliam Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of\nrisks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, FAccT \u201922, page 214\u2013229, New York, NY, USA, 2022.\nAssociation for Computing Machinery.\n[50] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei\nChang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended\nlanguage generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT \u201921. ACM, March 2021.\n15\n"
  },
  {
    "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models",
    "link": "https://arxiv.org/pdf/2312.06585.pdf",
    "upvote": "26",
    "text": "2023-12-25\nBeyond Human Data: Scaling Self-Training for\nProblem-Solving with Language Models\nAvi Singh1,*, John D Co-Reyes1,*, Rishabh Agarwal1,2,*,\nAnkesh Anand1, Piyush Patil1, Xavier Garcia1, Peter J. Liu1, James Harrison1, Jaehoon Lee1, Kelvin Xu1,\nAaron Parisi1, Abhishek Kumar1, Alex Alemi1, Alex Rizkowsky1, Azade Nova1, Ben Adlam1, Bernd Bohnet1,\nGamaleldin Elsayed1, Hanie Sedghi1, Igor Mordatch1, Isabelle Simpson1, Izzeddin Gur1, Jasper Snoek1,\nJeffrey Pennington1, Jiri Hron1, Kathleen Kenealy1, Kevin Swersky1, Kshiteej Mahajan1, Laura Culp1, Lechao\nXiao1, Maxwell L Bileschi1, Noah Constant1, Roman Novak1, Rosanne Liu1, Tris Warkentin1, Yundi Qian1,\nYamini Bansal1, Ethan Dyer1, Behnam Neyshabur1, Jascha Sohl-Dickstein1, Noah Fiedel1\n*Contributed equally, 1Google DeepMind, 2 Mila\nFine-tuning language models (LMs) on human-generated data remains a prevalent practice. However,\nthe performance of such models is often limited by the quantity and diversity of high-quality human data.\nIn this paper, we explore whether we can go beyond human data on tasks where we have access to scalar\nfeedback, for example, on math problems where one can verify correctness. To do so, we investigate a\nsimple self-training method based on expectation-maximization, which we call ReST\ud835\udc38\ud835\udc40, where we (1)\ngenerate samples from the model and filter them using binary feedback, (2) fine-tune the model on\nthese samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS\ncoding benchmarks using PaLM-2 models, we find that ReST\ud835\udc38\ud835\udc40 scales favorably with model size and\nsignificantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with\nfeedback can substantially reduce dependence on human-generated data.\nKeywords: RL from external feedback, EM for RL, Language, LLMs, Reasoning, Coding, Self-Improvement\n1. Introduction\nLarge Language Models (LLMs) are revolutionizing the landscape of deep learning, showcasing\nremarkable capabilities in generating human-quality text and tackling diverse language tasks (Google\net al., 2023; OpenAI, 2023). While supervised fine-tuning (SFT) on human-collected data further\nboosts their performance on tasks of interest, acquiring high-quality human data poses a significant\nbottleneck. This is particularly demanding for complex problem-solving tasks, requiring significant\nresources and expert knowledge. To address this hurdle, model-generated synthetic data emerges as\na promising alternative, offering scalability and cost-effectiveness, provided its quality can be ensured.\nWhile LLMs hold the potential to self-evaluate generated data, this paper explores a simpler setting\nwhere an external, scalar feedback signal serves as a quality indicator for each generated sample.\nTo investigate training on model-generated data, we consider a simple yet powerful self-training\napproach for language models that requires only two capabilities: 1) generating samples from the\nmodel and 2) evaluating these samples with a scoring mechanism. To ensure clarity and consistency,\nwe adopt the terminology of Reinforced Self-Training (Gulcehre et al., 2023) and call this approach\nReST\ud835\udc38\ud835\udc40. We show that ReST\ud835\udc38\ud835\udc40 can be viewed as applying expectation-maximization for reinforcement\nlearning (Dayan and Hinton, 1997; Peters and Schaal, 2007), which we present formally in Section 3.\nSpecifically, ReST\ud835\udc38\ud835\udc40 alternates between the expectation and maximization steps:\n1. Generate (E-step): The language model generates multiple output samples for each input\nCorresponding author(s): singhavi@google.com, rishabhagarwal@google.com, jcoreyes@google.com\n\u00a9 2023 Google DeepMind. All rights reserved\narXiv:2312.06585v3  [cs.LG]  22 Dec 2023\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n15\n20\n25\n30\n35\n40\n4-shot Test Accuracy (%)\nGPT-4\nLLaMA-2 70B\nWizardMath 70B\nMetaMath 70B\nInflection-1\nLlemma 34B\nLlemma 7B \nMistral 7B (maj@4)\nMinerva 62B\nMinerva 540B\nPaLM 2-S\nPaLM 2-L\nGrok-0 (33B)\nPaLM 2-L (ReSTEM)\nPaLM 2-S (ReSTEM)\nReasoning: MATH\n30\n40\n50\n60\n0-shot Accuracy (%)\nPaLM 2-S*\nPaLM 2-L\nGPT-4\nGPT-3.5 (ChatGPT)\nWizardCoder 15B\nLLaMA-2 70B\nCode LLaMA 34B\nCode Llama Python 34B\nInflection-1\nMistral 7B\nGrok-0 (33B)\nPaLM 2-L (ReSTEM)\nPaLM 2-S* (ReSTEM)\nCode Generation: HumanEval\nFigure 1 | Self-training with ReST\ud835\udc38\ud835\udc40 substantially improves test performance of PaLM 2 models on\ntwo challenging benchmarks: MATH and HumanEval. Results for other models are shown for general\nprogress on these tasks and are typically not comparable due to difference in model scales. GPT-4\nresults are taken from Bubeck et al. (2023).\ncontext. Then, we filter these samples using a binary reward to collect the training dataset.\n2. Improve (M-step): The original language model is supervised fine-tuned on the training\ndataset from the previous Generate step. The fine-tuned model is then used in the next\nGenerate step.\nReST\ud835\udc38\ud835\udc40, with its various adaptations, has demonstrated success in enhancing language models\nacross diverse domains, including machine translation (Gulcehre et al., 2023; Norouzi et al., 2016),\nsemantic parsing (Agarwal et al., 2019), preference alignment (Dong et al., 2023), and elementary\nreasoning (Yuan et al., 2023; Zelikman et al., 2022). However, prior works primarily applied ReST\ud835\udc38\ud835\udc40\nto relatively small language models (up to 7B parameters), with limited scalability observed for\nlarger models (Yuan et al., 2023). Complementing these efforts, our work aims to investigate the\neffectiveness and scalability of model-generated synthetic data compared to human-generated data\nin two challenging, less explored domains: competition-level mathematical problem-solving (MATH)\n(Hendrycks et al., 2021b) and code generation (APPS) (Hendrycks et al., 2021a).\nOur empirical findings reveal significant advancements in both mathematical reasoning and code\ngeneration capabilities when applying ReST\ud835\udc38\ud835\udc40 to PaLM 2 models of varying scales (Figure 1). Notably,\nmodels fine-tuned on model-generated synthetic data exhibit remarkably larger performance gains\ncompared to those trained on human-written data (Figure 2, 3). Interestingly, exceeding a couple\nof iterations of ReST\ud835\udc38\ud835\udc40 leads to diminishing improvement, indicating potential overfitting on small\namount of training problems (Figure 4). Additionally, models fine-tuned using ReST\ud835\udc38\ud835\udc40 improve\npass@k as well as majority voting performance. Furthermore, these fine-tuned models demonstrate\nenhanced performance on related but held-out benchmarks, including math problems (GSM8K and\nHungarian HS finals), coding (HumanEval), and Big-Bench Hard tasks. We also perform ablation\nstudies to investigate the effect of number of model-generated solutions, training problems, and\niterations for ReST\ud835\udc38\ud835\udc40 fine-tuning. Overall, our findings suggest self-training with feedback as an\npromising approach to reduce dependence on human data.\n2. Preliminaries\nAn autoregressive language model produces an output sequence \ud835\udc9a = (\ud835\udc661, \ud835\udc662, ....\ud835\udc66\ud835\udc47) given a context (or\nsource input) \ud835\udc99 = (\ud835\udc651, \ud835\udc652, ...\ud835\udc65\ud835\udc3f), where the tokens \ud835\udc65\ud835\udc59, \ud835\udc66\ud835\udc61 belong to a fixed vocabulary. Auto-regressive\ngeneration involves predicting tokens one at a time, based on the previously generated tokens.\nAssuming that the language model is parameterized by \ud835\udf03, the conditional probability distribution of\n2\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\ngenerating a sequence \ud835\udc9a given \ud835\udc99 is\n\ud835\udc5d\ud835\udf03(\ud835\udc9a | \ud835\udc99) =\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc61 | \ud835\udc9a<\ud835\udc61, \ud835\udc99),\nwith the convention \ud835\udc9a1:0 = \u2205 and \ud835\udc9a1:\ud835\udc61\u22121 = (\ud835\udc661, \ud835\udc662, ....\ud835\udc66\ud835\udc61\u22121). For ease of notation, we define \ud835\udc5d(\ud835\udc66\ud835\udc61|\ud835\udc65) :=\n\ud835\udc5d(\ud835\udc66\ud835\udc61|\ud835\udc66<\ud835\udc61, \ud835\udc65). The probability of predicting \ud835\udc61\ud835\udc61\u210e token \ud835\udc66\ud835\udc61, \ud835\udc5d(\ud835\udc66\ud835\udc61|\ud835\udc65), is determined using a softmax with\ntemperature \ud835\udefe: \ud835\udc5d(\ud835\udc66\ud835\udc61|\ud835\udc65) =\nexp(\ud835\udc67\ud835\udc61/\ud835\udefe)\n\u00cd\ud835\udc40\n\ud835\udc56=1 exp(\ud835\udc67\ud835\udc56/\ud835\udefe) , where \ud835\udc67\ud835\udc61 is the logit score for the token \ud835\udc66\ud835\udc61. Higher values of \ud835\udefe\nintroduces more randomness, while a lower value makes the output more deterministic by favoring\nthe most probable words.\nGiven a dataset D of inputs \ud835\udc99 and human-generated outputs \ud835\udc9a, supervised fine-tuning (SFT)\ntrains the policy by minimizing the negative log likelihood loss:\nLSFT(\ud835\udf03) = \u2212\ud835\udd3c(\ud835\udc99,\ud835\udc9a)\u223cD\n\" \ud835\udc47\n\u2211\ufe01\n\ud835\udc61=1\nlog \ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc61 | \ud835\udc9a1:\ud835\udc61\u22121, \ud835\udc99)\n#\n.\n(1)\nWe also assume access to a deterministic sequence-level (or terminal) reward \ud835\udc5f(\ud835\udc99, \ud835\udc9a). Then, the\nreinforcement learning (RL) objective corresponds to:\nLRL(\ud835\udf03) = \ud835\udd3c\ud835\udc99\u223cD\n\u0002\n\ud835\udd3c\ud835\udc9a\u223c\ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99) [\ud835\udc5f(\ud835\udc99, \ud835\udc9a)]\n\u0003\n.\nOptimizing LRL loss directly using online RL methods, such as policy gradients, requires updating\nand sampling from the policy numerous times during training. However, the computational cost of\nfine-tuning on a continual flow of new samples becomes a limitation of online methods, especially\nwhen the sizes of the policy network grow to tens or hundreds of billion parameters. We discuss an\nalternative to such online RL approaches in the next section.\n3. Expectation-Maximization for Reinforced Self-Training\nExpectation-Maximization (EM) for RL\nWe first describe the EM-based framework for RL with\nlanguage models, building upon the prior work by Dayan and Hinton (1997). Let\u2019s define a binary\noptimality variable O, such that \ud835\udc5d(\ud835\udc42 = 1|\ud835\udc99, \ud835\udc9a) \u221d \ud835\udc53 (\ud835\udc5f(\ud835\udc99, \ud835\udc9a)), for some non-decreasing function \ud835\udc53 : \u211d \u2192\n\u211d+. We want to maximize the log-likelihood of observing \ud835\udc42 = 1 (obtaining high reward):\nlog \ud835\udc5d(\ud835\udc42 = 1|\ud835\udc99) := log\n\u2211\ufe01\n\ud835\udc9a\n\ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99)\ud835\udc5d(\ud835\udc42 = 1 | \ud835\udc99, \ud835\udc9a).\nHowever, the sum over all possible sequences \ud835\udc9a is typically intractable. Instead of maximizing\nlog \ud835\udc5d(\ud835\udc42 = 1; \ud835\udc99), one can consider maximizing its ELBO \ud835\udc3f(\ud835\udc5d\ud835\udf03, \ud835\udc5e) with respect to parameters \ud835\udf03 and\nvariational distribution \ud835\udc5e(\ud835\udc66|\ud835\udc65). Specifically,\nlog \ud835\udc5d(\ud835\udc42 = 1 | \ud835\udc99) = log \ud835\udd3c\ud835\udc5e(\ud835\udc9a|\ud835\udc99)\n\u0014 \ud835\udc5d(\ud835\udc42 = 1 | \ud835\udc99, \ud835\udc9a)\ud835\udc5d\ud835\udf03(\ud835\udc9a | \ud835\udc99)\n\ud835\udc5e(\ud835\udc9a | \ud835\udc99)\n\u0015\n\u2265 \ud835\udd3c\ud835\udc5e(\ud835\udc9a|\ud835\udc99)\n\u0014\nlog \ud835\udc5d(\ud835\udc42 = 1 | \ud835\udc99, \ud835\udc9a)\ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99)\n\ud835\udc5e(\ud835\udc9a | \ud835\udc99)\n\u0015\n(Jensen\u2019s inequality)\n= \ud835\udd3c\ud835\udc5e(\ud835\udc9a|\ud835\udc99) [log \ud835\udc5d(\ud835\udc42 = 1 | \ud835\udc99, \ud835\udc9a)] \u2212 KL [\ud835\udc5e(\ud835\udc9a | \ud835\udc99)||\ud835\udc5d\ud835\udf03(\ud835\udc9a | \ud835\udc99)]\n=: \ud835\udc3f(\ud835\udc5d\ud835\udf03, \ud835\udc5e)\n(2)\nThe EM algorithm (Dempster et al., 1977) for Equation 2 alternates between an E-step and M-step:\nat iteration \ud835\udc61, denote the language model parameter to be \ud835\udf03\ud835\udc61 and the variational distribution to be \ud835\udc5e\ud835\udc61.\n3\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nAlgorithm 1: ReST (Expectation-Maximization). Given a initial policy (e.g., pre-trained\nLM), ReST\ud835\udc38\ud835\udc40 iteratively applies Generate and Improve steps to update the policy.\nInput: D: Training dataset, D\ud835\udc63\ud835\udc4e\ud835\udc59: Validation dataset, L(\ud835\udc99, \ud835\udc9a; \ud835\udf03): loss, \ud835\udc5f(\ud835\udc99, \ud835\udc9a): Non-negative\nreward function, \ud835\udc3c: number of iterations, \ud835\udc41: number of samples per context\nfor \ud835\udc56 = 1 to \ud835\udc3c do\n// Generate (E-step)\nGenerate dataset D\ud835\udc56 by sampling: D\ud835\udc56 = { (\ud835\udc99 \ud835\udc57, \ud835\udc9a \ud835\udc57)|\ud835\udc41\n\ud835\udc57=1 s.t. \ud835\udc99 \ud835\udc57 \u223c D, \ud835\udc9a \ud835\udc57 \u223c \ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99 \ud835\udc57) }\nAnnotate D\ud835\udc56 with the reward \ud835\udc5f(\ud835\udc99, \ud835\udc9a).\n// Improve (M-step)\nwhile reward improves on D\ud835\udc63\ud835\udc4e\ud835\udc59 do\nOptimise \ud835\udf03 to maximize objective: \ud835\udc3d(\ud835\udf03) = \ud835\udd3c(\ud835\udc99,\ud835\udc9a)\u223cD\ud835\udc56 [\ud835\udc5f(\ud835\udc99, \ud835\udc9a) log \ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99)]\nend\nend\nOutput: Policy \ud835\udc5d\ud835\udf03\n\u2022 E-step: \ud835\udc5e\ud835\udc61+1 = arg max\ud835\udc5e \ud835\udc3f(\ud835\udc5d\ud835\udf03\ud835\udc61, \ud835\udc5e). Since \ud835\udc3f(\ud835\udc5d\ud835\udf03\ud835\udc61, \ud835\udc5e) can be written as \ud835\udc3e\ud835\udc3f[\ud835\udc5e(\ud835\udc9a|\ud835\udc99)||\ud835\udc5e\u2217(\ud835\udc9a||\ud835\udc99)], \ud835\udc5e\ud835\udc61+1(\ud835\udc9a |\n\ud835\udc99) \u221d \ud835\udc5e\u2217(\ud835\udc9a | \ud835\udc99) := \ud835\udc5d(\ud835\udc42 = 1|\ud835\udc99, \ud835\udc9a)\ud835\udc5d\ud835\udf03\ud835\udc61 (\ud835\udc9a | \ud835\udc99). Thus, this step is equivalent to weighting the output\nsamples from conditional language model distribution based on their likelihood of obtaining\nhigh rewards.\n\u2022 M-step: \ud835\udf03\ud835\udc61+1 := arg max\ud835\udf03 \ud835\udc3f(\ud835\udc5d\ud835\udf03, \ud835\udc5e\ud835\udc61+1) = arg max\ud835\udf03\n\u00cd\n\ud835\udc9a \ud835\udc5e\ud835\udc61+1(\ud835\udc9a | \ud835\udc99) log \ud835\udc5d\ud835\udf03(\ud835\udc9a | \ud835\udc99). As such, this step\ncorresponds to maximizing a reward-weighted negative log-likelihood loss.\nAlternating between above steps ensures a monotonic improvement in the ELBO: \ud835\udc3f(\ud835\udc5d\ud835\udf03\ud835\udc61+1, \ud835\udc5e\ud835\udc61+1) \u2265\n\ud835\udc3f(\ud835\udc5d\ud835\udf03\ud835\udc61, \ud835\udc5e\ud835\udc61+1) \u2265 \ud835\udc3f(\ud835\udc5d\ud835\udf03\ud835\udc61, \ud835\udc5e\ud835\udc61).\nEM with non-negative rewards. If the rewards are non-negative and \ud835\udc53 is set to the identity\nfunction, then \ud835\udc5d(\ud835\udc42 = 1|\ud835\udc99, \ud835\udc9a) \u221d \ud835\udc5f(\ud835\udc99, \ud835\udc9a) which implies \ud835\udc5e\ud835\udc61+1(\ud835\udc9a | \ud835\udc99) \u221d \ud835\udc5f(\ud835\udc99, \ud835\udc9a)\ud835\udc5d\ud835\udf03\ud835\udc61 (\ud835\udc9a | \ud835\udc99). In this scenario,\nthe updated policy parameters \ud835\udf03\ud835\udc61+1 resulting from the M-step at iteration \ud835\udc61 are given by:\n\ud835\udf03\ud835\udc61+1 := arg max\n\ud835\udf03\n\ud835\udd3c\ud835\udc65\u223cD\nh\n\ud835\udd3c\ud835\udc9a\u223c\ud835\udc5d\ud835\udc61\n\ud835\udf03(\ud835\udc9a|\ud835\udc99) [\ud835\udc5f(\ud835\udc99, \ud835\udc9a) log \ud835\udc5d\ud835\udf03(\ud835\udc9a | \ud835\udc99)]\ni\n.\n(3)\nComparing the above equation with the LRL objective reveals the key distinction between standard\nRL and EM-based RL: how output data is sampled. Standard RL continuously updates the policy\nand uses this latest policy to collect data. In contrast, EM-based RL employs a fixed sampling policy\nfrom the previous iteration, decoupling data collection from policy optimization. This decoupling in\nEM-based approaches enables easier scaling to large-scale policy models.\nReST\ud835\udc38\ud835\udc40\nMotivated by the EM framework, we now discuss a simplified version of ReST approach by\nGulcehre et al. (2023). This approach, which we call ReST\ud835\udc38\ud835\udc40 for clarity, decouples data collection (E-\nstep) and policy optimization (M-step) in a typical RL pipeline. Algorithm 1 outlines the ReST\ud835\udc38\ud835\udc40\nalgorithm with multiple iterations, where each iteration corresponds to one Generate and Improve\nstep. We describe these steps in detail below.\n\u2022 Generate (E-step): In this step, we generate a dataset D\ud835\udc56 by sampling many output sequences\nfrom the current policy \ud835\udc5d\ud835\udf03: D\ud835\udc56 = { (\ud835\udc99 \ud835\udc57, \ud835\udc9a \ud835\udc57)|\ud835\udc41\n\ud835\udc57=1 s.t. \ud835\udc99 \ud835\udc57 \u223c D, \ud835\udc9a \ud835\udc57 \u223c \ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99 \ud835\udc57) }. Here, the inputs are\nresampled from the original dataset \ud835\udc99 \ud835\udc57 \u223c D. The output sequences in D\ud835\udc56 are then scored with a\nbinary reward function \ud835\udc5f(\ud835\udc99, \ud835\udc9a). Unlike Gulcehre et al. (2023), we refrain from augmenting D\ud835\udc56\nwith human-generated outputs as such data may not always be optimal for learning or it might\n4\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nnot be easily available. In our experiments, we condition the language model using a few-shot\nprompt with programs for code generation and step-by-step solutions for math problems.\n\u2022 Improve (M-step): In the \ud835\udc56\ud835\udc61\u210e iteration, we use the new dataset D\ud835\udc56 from Generate step to\nfine-tune the policy \ud835\udc5d\ud835\udf03. Contrary to Gulcehre et al. (2023), we always fine tune the base\npretrained language model to minimize task-specific over-fitting and minimize drift from the\nbase model. For fine-tuning, we minimize the reward-weighted negative log-likelihood loss\n\ud835\udc3d(\ud835\udf03) = \ud835\udd3c(\ud835\udc99,\ud835\udc9a)\u223cD\ud835\udc56 [\ud835\udc5f(\ud835\udc99, \ud835\udc9a) log \ud835\udc5d\ud835\udf03(\ud835\udc9a|\ud835\udc99)]. Once the policy is improved, a new dataset of better\nquality samples can be created once again.\nRemark. Our experiments focus on problem-solving settings with binary rewards (either 0 or 1),\nunlike the bounded real-valued rewards assumed by Gulcehre et al. (2023). Specifically, for each\nGenerate step, Gulcehre et al. (2023) perform multiple Improve steps, where each Improve step\ncan be viewed as an M-step with the function \ud835\udc53 (\ud835\udc5f(\ud835\udc99, \ud835\udc9a)) = \ud835\udc5f(\ud835\udc99, \ud835\udc9a) > \ud835\udf0f, where \ud835\udf0f \u2208 \u211d+ increases in\nsuccessive M-steps. However, with binary rewards, any value of \ud835\udf0f \u2208 (0, 1) corresponds to the identical\nImprove steps.\n4. Related work\nSeveral prior methods can be instantiated using the expectation-maximization framework in Section 3.\nWe discuss methods and their relation to ReST\ud835\udc38\ud835\udc40 in this section.\n\u2022 Expert Iteration (ExiT) (Anthony et al., 2017) alternates between two steps: expert improve-\nment and policy distillation. During the expert improvement step (E-step), we combine a base\npolicy with a search procedure to generate samples from a better policy, called the expert policy.\nThen, in the policy distillation step (M-step), we use these expert samples to train the base\npolicy in a supervised way, effectively improving it to match the expert policy. While ExiT used\nmonte-carlo tree-search, we simply use temperature sampling for collecting samples from the\nexpert policy in ReST. That said, improving the E-step in ReST using the ExIT framework via\nsearch and planning procedures with language models would be interesting for future work. For\nexample, Huang et al. (2022) implement a single iteration of ReST\ud835\udc38\ud835\udc40 on simple math reasoning\nproblems. However, unlike our setup, they do not assume access to a correctness reward and\ninstead employ majority-voting (Wang et al., 2023) as a search procedure within the E-step.\n\u2022 Self-Taught Reasoner (STaR) (Zelikman et al., 2022) employed greedy decoding instead of\ntemperature sampling for the E-step in ReST\ud835\udc38\ud835\udc40. Additionally, STaR proposed rationalization as\nan alternative to temperature sampling, where the language model is provided with the correct\nanswer as part of the input to generate correct solutions for difficult problems. However, in our\npreliminary experiments, rationalization leads to substantial increase in false positive solutions\nthat result in correct answer but with incorrect reasoning.\n\u2022 Rejection Sampling Fine-tuning (RFT) (Yuan et al., 2023) improves reasoning performance\non GSM8K and corresponds to running a single generate (E-step) and improve (M-step) of\nReST\ud835\udc38\ud835\udc40. While RFT demonstrated limited performance improvements on GSM8K with increasing\nlanguage model capacity, ReST\ud835\udc38\ud835\udc40 achieves larger gains on more challenging APPS and MATH\nbenchmarks when scaling PaLM 2 model capacity. Moreover, we observe that using multiple\niterations of ReST\ud835\udc38\ud835\udc40 result in larger performance gains.\n\u2022 Iterative Maximum Likelihood (IML) optimizes a policy using a reward-weighted log-likelihood\nobjective on self-collected data. IML has been shown to perform well with relatively small-scale\n5\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nlanguage models for semantic parsing (Agarwal et al., 2019; Liang et al., 2016), machine\ntranslation (Wu et al., 2016) and simple math reasoning (Ni et al., 2022). Each E-step and\nM-step in IML is performed over a mini-batch of training examples instead of the entire training\ndataset, as done in ReST\ud835\udc38\ud835\udc40. In IML, the learned policy can significantly diverge from the initial\npretrained model, which can manifest as task-specific overfitting, where the model performs\nwell on the target task but loses its ability to generalize to other tasks or domains. Additionally,\nthe tightly coupled nature of data collection and policy optimization in IML leads to high\ncomputational cost with large LMs, making it significantly more expensive than ReST\ud835\udc38\ud835\udc40.\n\u2022 Reward weighted regression (RWR) (Peters and Schaal, 2007) corresponds to EM where we\nset \ud835\udc5d(\ud835\udc42 = 1|\ud835\udc99, \ud835\udc9a) \u221d exp (\ud835\udc5f(\ud835\udc99, \ud835\udc9a)) in Section 3. RWR can be easily has been previously applied to\nrobotic control, as it can be easily applied to non-binary reward functions. Norouzi et al. (2016)\nbuild on RWR to propose a general variant of IML for machine translation.\n\u2022 Reward ranked fine-tuning (RAFT) (Dong et al., 2023) can be interpreted as alternating\nbetween E-step and M-step over mini-batches, where E-step uses the the output sample with\nmaximum reward for each input context. For binary reward functions, RAFT is analogous to\nIML and as such, can be viewed as an instantiation of ReST\ud835\udc38\ud835\udc40.\nOther related works: TRICE (Phan et al., 2023) proposes an EM-based approach to maximize\nthe marginal log-likelihood (MML) of generating a correct answer for a reasoning problem, where the\nchain-of-thought rationale is treated as a latent variable. While E-step in ReST\ud835\udc38\ud835\udc40 simply corresponds\nto sampling from the model and filtering with a binary reward, TRICE uses Markov-chain Monte\nCarlo with a control variate to approximate the MML gradient. Sordoni et al. (2023) propose a\ngradient-free EM-based approach, similar to RAFT, for prompt-optimization for frozen LLMs.\n5. Experiments and analysis\nThe goal of our experiments is to answer the following questions:\n1. How effective is ReST\ud835\udc38\ud835\udc40 compared to fine-tuning on human-generated data?\n2. How many iterations are needed for optimal performance? How quickly does ReST\ud835\udc38\ud835\udc40 leads to\noverfitting on training set?\n3. How does ReST\ud835\udc38\ud835\udc40 affect pass@k and majority voting performance?\n4. If we fine-tune using model-generated data on a specific task, do we see positive transfer\nto related tasks? Is there any performance degradation compared to the base model when\nevaluating our fine-tuned models on a broad suite of tasks?\n5. How much input data do we need to get most of the performance gains from ReST\ud835\udc38\ud835\udc40? Is one\niteration of ReST\ud835\udc38\ud835\udc40 sufficient?\nTraining Datasets. We evaluate ReST\ud835\udc38\ud835\udc40 primarily on mathematical problem solving using the\nHendrycks\u2019 MATH dataset (Hendrycks et al., 2021b) and code generation using the APPS (Introductory)\ndataset (Hendrycks et al., 2021a). MATH and APPS (Introductory) contain 7500 and 2342 training\nproblems respectively. We select these tasks because the model outputs can be automatically evaluated\nas correct or incorrect, perfectly suited for ReST\ud835\udc38\ud835\udc40. Both these datasets offer binary rewards: on\nMATH, model-generated answers can be easily verified for correctness using the ground-truth answer,\nwhile on APPS, test cases determine whether the generated code is correct.\nModels. We use the PaLM 2 models (Google et al., 2023) with public APIs on Google Cloud for\nexperiments, including PaLM 2-S (Bison), PaLM 2-S* (Codey), and PaLM 2-L (Unicorn).\n6\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n0\n1\n2\n3\nNum iterations\n20\n25\n30\n35\n40\nPass@1 Test Accuracy (%)\nHendrycks MATH\nPalm-2-S\nPalm-2-L\nPalm-2-L-SFT\nPalm-2-S-SFT\n0\n1\n2\n3\nNum iterations \n60\n70\n80\nPass@1 Test Accuracy (%)\nTransfer to GSM8K\nFigure 2 | ReST\ud835\udc38\ud835\udc40 for math problem-solving. Test performance on MATH and GSM8K (transfer) for\nPaLM 2-S* and PaLM 2-L as a function of ReST\ud835\udc38\ud835\udc40 iterations. We also report performance of models\nfine-tuned via SFT on human-generated data as a baseline. Iteration 0 corresponds to pre-trained\nmodel performance. Following Google et al. (2023), we use greedy decoding for evaluation.\nEvaluation. We report generalization performance using the test splits of the MATH and APPS\n(Introductory) datasets. For measuring transfer performance, we look at GSM8K (Cobbe et al., 2021),\nHungarian HS finals (Paster, 2023), and HumanEval (Chen et al., 2021) datasets. We also evaluate\nour models using the Big-Bench Hard (Suzgun et al., 2022) benchmark to evaluate general capabilities.\nAll evaluations follow the settings from Google et al. (2023), unless specified otherwise.\nImplementation Details. During each iteration of ReST\ud835\udc38\ud835\udc40, we generated a fixed number of\nsolutions per problem for the E-step: 32 for the MATH dataset and 64 for the APPS dataset. For\ngenerating solutions, we sample from the language model using top-K sampling with K=40 and\ntemperature of 0.7. However, directly using all these model-generated solutions can lead to an\nimbalanced dataset, as we will have a lot more correct solutions for the easier problems. To mitigate\nthis, we introduced a cut-off threshold for the maximum number of solutions per problem, a design\nchoice also used by Zelikman et al. (2022), included in the fine-tuning dataset: 10 for both MATH\nand APPS. This approach ensures diversity in the training data and safeguards against overfitting\non easier problems. For fine-tuning, we use the few-shot prompt (and the question) as input to the\nmodel, and use the model-generated solutions as targets. We only apply the next token prediction\nloss (Equation 1) on the targets.\n5.1. ReST\ud835\udc38\ud835\udc40 on MATH and APPS\nFigures 2 and 3 show the performance of ReST\ud835\udc38\ud835\udc40 when trained on the MATH and APPS datasets,\nrespectively. We see that MATH benefits from performing multiple iterations of ReST\ud835\udc38\ud835\udc40, both in terms\nof performance on the MATH test set, as well as transfer to GSM8K. On the other hand, we see that\nmost of the gains for APPS come from the first iteration, and the performing more iterations leads to\na regression in performance on both APPS and HumanEval.\nInterestingly, Figures 2 and 3 demonstrate that fine-tuning on model-generated solutions substan-\ntially outperforms using human-written solutions, especially for the PaLM 2-L model. This aligns with\nfindings of Yuan et al. (2023) and recent work on distilling LLMs using model-generated data (Agarwal\net al., 2023; Gu et al., 2023). However, unlike Yuan et al. (2023), who observed diminishing returns\nfrom model-generated data on GSM8K when scaling model capacity, our results suggest an opposite\ntrend: ReST\ud835\udc38\ud835\udc40 leads to larger performance gains as model capacity increases. On the MATH dataset,\n7\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n0\n1\n2\nNum iterations\n18\n20\n22\n24\n26\nPass@1 Test Accuracy (%)\nAPPS (Introductory)\nPalm-2-S*\nPalm-2-L\nPalm-2-L-SFT\nPalm-2-S*-SFT\n0\n1\n2\nNum iterations \n40\n45\n50\n55\nPass@1 Test Accuracy (%)\nTransfer to HumanEval\nFigure 3 | ReST\ud835\udc38\ud835\udc40 for code-generation.\nTest performance on APPS (introductory) and Hu-\nmanEval (transfer) for PaLM 2-S* and PaLM 2-L as a function of ReST\ud835\udc38\ud835\udc40 iterations.\n0\n1\n2\n3\nNum iterations \n35\n40\n45\n50\n55\n60\nPass@1 Performance (%)\nHendrycks MATH\nPalm-2-L (Train)\nPalm-2-L (Test)\n0\n1\n2\nNum iterations \n20\n30\n40\n50\n60\nPass@1 Performance (%)\nAPPS (Introductory)\nPalm-2-L (Train)\nPalm-2-L (Test)\nFigure 4 | Train-test performance gap on (left) MATH with PaLM-2-L, and (right) APPS with PaLM-\n2-S*, as a function of ReST\ud835\udc38\ud835\udc40 iterations.\nthe test accuracy improvement with ReST\ud835\udc38\ud835\udc40 is 5.94% for PaLM 2-S compared to 6.34% for the larger\nPaLM 2-L model. Similarly, on the APPS dataset, improvements are 5.6% for PaLM 2-S* compared to\n6.4% for PaLM 2-L. This is in addition to the fact that the larger models start with a much stronger\ninitial performance, and improvements on these benchmarks generally get harder as the baseline\nperformance goes up.\nTrain-test performance gap. Figure 4 shows that while training set performance increases linearly\nwith the number of ReST\ud835\udc38\ud835\udc40 iterations, test set performance does not. For MATH, test performance\nimprovements are small after the first iteration, and for APPS, we actually observe a regression in\nperformance in the second iteration. We suspect that the regression in performance is likely due to\noverfitting on the small set of training problems. Since the APPS dataset is about a third of the size of\nthe MATH dataset, it suffers more from this problem.\n5.2. Impact on Pass@K and Majority-Voting Performance\nTo investigate the impact of fine-tuning with ReST\ud835\udc38\ud835\udc40 on the diversity of the final model\u2019s generated\noutputs, we evaluate pass@k (Chen et al., 2021) and majority voting (Wang et al., 2023) performance\nof the fine-tuned PaLM 2-L model relative to the base model.\n8\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n0\n20\n40\n60\nNum samples (K)\n40%\n60%\n80%\nPass @ K Test Accuracy (%)\nHumanEval\nPaLM-2-L\nPaLM-2-L (ReST)\n2\n4\n6\n8\n10\nNum samples (K)\n10%\n20%\n30%\n40%\nPass @ K Test Accuracy (%)\nAPPS (Introductory)\nPaLM-2-L\nPaLM-2-L (ReST)\n0\n20\n40\n60\nNum samples (K)\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nPass @ K Test Accuracy (%)\nHendrycks MATH\nPalm-2-L\nPalm-2-L (ReST)\nFigure 5 | Pass@K results for PaLM-2-L pretrained model as well as model fine-tuned with ReST\ud835\udc38\ud835\udc40.\nFor a fixed number of samples K, fine-tuning with ReST\ud835\udc38\ud835\udc40 substantially improves Pass@K performance.\nWe set temperature to 1.0 and use nucleus sampling with \ud835\udc5d = 0.95.\nPass@K measures the probability that at least one of the top k-generated solution for a problem\nis correct, that is, outputs the correct answer for math problems or passes all the unit tests for code\ngeneration. Figure 5 shows the performance of the Palm-2-L model on the pass@K metric. We see\nthat ReST\ud835\udc38\ud835\udc40 model obtained after fine-tuning is stronger for all values of K, with the performance\ngap typically being the highest for K=1.\nMajority voting first samples a diverse set of reasoning paths instead of only taking the greedy\none, and then selects the most consistent answer by marginalizing out the sampled reasoning paths.\nFor Hendrycks MATH, it is possible to use majority voting to maximize Pass@1 performance, and we\nfind that when using 64 samples per question, the PaLM 2-L fine-tuned with ReST\ud835\udc38\ud835\udc40 obtains a test\naccuracy of 48.82, while the base model gets 44.02.\n5.3. Ablation Studies\nImpact of multiple iterations\nOur results show that multiple iterations can sometimes lead to\nover-fitting on the train set (Figure 4). This raises the question of whether multiple iterations are\nreally necessary. Is it better to collect a larger dataset and perform just a single iteration of ReST\ud835\udc38\ud835\udc40?\nTo investigate this, we collect a dataset with the base PaLM-2-L model on Hendrycks MATH that is\n3\u00d7 as many solutions per problem as used in a single iteration of ReST\ud835\udc38\ud835\udc40 for the E-step. Fine-tuning\nwith this dataset results in pass@1 performance of 40.3%, which is lower than the 41% in second\nand 41.9% in third iteration, as shown in Figure 2. These results indicate that performing multiple\niterations of ReST\ud835\udc38\ud835\udc40 leads to higher performance compared a single iteration with 3x the data.\nImpact of dataset size\nSince one of the main ingredients needed for ReST\ud835\udc38\ud835\udc40 is a dataset of input\ncontexts (e.g., questions for MATH), we are interested in evaluating the effect of number of input\nproblems. The results from our dataset ablations using the PaLM-2-L model on Hendrycks MATH,\nFigure 6 (left), show that utilizing just 1000 MATH questions results in significant gains, implying that\nthe method is very efficient in the number of prompts needed. However, we noted a slight decrease\nin performance when using 4,000 questions compared to 2,000, indicating potential variance in\nthe fine-tuning process. Ideally, conducting this experiment multiple times would help quantify this\nvariance, but this is prohibitively resource-intensive. Overall, we find that ReST\ud835\udc38\ud835\udc40 is quite sample\nefficient and performance gains from ReST\ud835\udc38\ud835\udc40 improve as we increase the dataset size.\nComparing model-generated data with human data\nA key strength of ReST\ud835\udc38\ud835\udc40 is its ability to\ngenerate multiple correct solutions for each problem. This provides valuable additional training data\n9\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n0\n1000\n2000\n4000\n7000\nNumber of questions\n34\n35\n36\n37\n38\n39\n40\nPass@1 Performance (%)\nHendrycks MATH (Test)\nSFT (7K)\nSFT (5K)\nReST *  (5K) ReSTEM (5K)\nMethod (Num questions)\n34\n36\n38\n40\n42\nPass@1 Performance (%)\nHendrycks MATH (Test)\nFigure 6 | Left. Performance for a single iteration of ReST\ud835\udc38\ud835\udc40 as a function of dataset size (number of\nquestions) on MATH. Right. Comparing ReST\ud835\udc38\ud835\udc40 with SFT on MATH. SFT refers to fine-tuning on\nhuman data, while ReST* refers to a version of ReST\ud835\udc38\ud835\udc40 with one iteration that uses only one correct\nsample per problem. Here, ReST denotes ReST\ud835\udc38\ud835\udc40 with 3 iterations. For each method, we denote the\nnumber of questions in parenthesis.\ncompared to human-generated data, which typically offers only a single solution per problem. While\nthis makes a comparison in Figures 2 and 3 not entirely fair, it also highlights the potential of ReST\ud835\udc38\ud835\udc40\nto boost performance with diverse and correct solutions.\nIn order to enable an apples-to-apples comparison, we conduct the following study: we select all\nHendrycks MATH questions for which we have at least one correct model-generated solution, resulting\nin about 5K questions. For these 5K questions, we run two fine-tuning experiments: SFT(5K) where\nwe fine-tune on human-written solutions (one per question), and ReST\u2217(5K) where we fine-tune on\nmodel-generated solutions (also one per question, selected at random).\nThe results in Figure 6 (right), show that ReST\ud835\udc38\ud835\udc40 outperforms fine-tuning on human data even in\nthis much more restricted setting. Furthermore, the efficacy of ReST(5K) over ReST\u2217(5K) highlights\nthe additional gain in performance that we can obtain by spending more compute on sampling a\nlarge number of solutions and performing multiple iterations of ReST\ud835\udc38\ud835\udc40.\nDistillation with model-generated data\nThe above results indicate that self-generated data is better\nthan human data for fine-tuning language models. We hypothesize this is because model-generated\nsolutions are more in-distribution compared to human-written solutions.\nSFT (Human) Distill* (2-L) ReSTEM (2-S) Distill (2-L)\nMethod (Data Source)\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nPass@1 Performance (%)\nPaLM 2-S on Hendrycks MATH (Test)\nFigure 7 | Impact of Model-Generated Data for Distillation.\nTo explore if this benefit extends\nto different models, we consider a dis-\ntillation setup on MATH where we\nfine-tune PaLM 2-S using data gener-\nated by a larger model, PaLM 2-L, re-\nsulting in solutions for about 5K ques-\ntions. Specifically, we ran two distilla-\ntion experiments: Distill\u2217 (2-L) where\nwe fine-tune on teacher-generated so-\nlutions (one per question), similar to\nReST (5K), and Distill (2-L), which in-\ncludes multiple solutions per problem,\ngenerated during the final iteration of\n10\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nReST\ud835\udc38\ud835\udc40 with PaLM 2-L.\nOur results, shown in Figure 7, reveal that Distill\u2217 surpasses the performance achieved by fine-\ntuning on human-written solutions, despite having smaller number of training questions. Additionally,\nfine-tuning PaLM 2-S with multiple solutions from PaLM 2-L, namely Distill (2-L), is superior than\nusing self-generated solutions via ReST\ud835\udc38\ud835\udc40. This improvement is likely due to the larger number of\ntraining questions with solutions in PaLM 2-L generated data compared to 2-S. Overall, these results\nindicate that model-generated data from larger models can be more effective for fine-tuning smaller\nmodels than relying on human-generated data.\n5.4. Impact on Reasoning capabilities\nBoolean Expressions\nCausal Judgement\nDate Understanding\nDisambiguation QA\nDyck Languages\nFormal Fallacies\nGeometric Shapes\nHyperbaton\nMovie Recommendation\nMulti-step Arithmetic [Two]\nNavigate\nObject Counting\nPenguins in a Table\nReasoning about Colored Objects\nRuin Names\nSalient Translation Error Detection\nSnarks\nSports Understanding\nTemporal Sequences\nWeb of Lies\nWord Sorting\nLogical Deduction (avg)\nTracking Shuffled Objects (avg)\nBig-Bench Hard (BBH) Task\n30\n40\n50\n60\n70\n80\n90\n100\nFew-shot Performance with CoT\nPaLM 2-L\nPaLM 2-L (APPS)\nPaLM 2-L (MATH)\nCoT\nDirect\nPrompt Type\n60\n65\n70\n75\n80\nAverage BBH Performance\nPaLM 2-L\nPaLM 2-L (APPS)\nPaLM 2-L (MATH)\nFigure 8 | Comparing the ReST\ud835\udc38\ud835\udc40 models to the base model on the Big-Bench Hard suite of tasks.\nGeneral capabilities. BIG-Bench provides a suite of over 200 tasks that can be used to probe\nLLMs\u2019 performance across a range of fields and capabilities. BIG-Bench Hard (BBH) (Suzgun et al.,\n2022) is a subset of 23 BIG-Bench tasks where the previous generation of LLMs, such as Codex and\nPaLM 540B, performed below the average human rater. We follow the experimental setup of Google\net al. (2023) and evaluate using both few-shot and chain-of-thought prompting.\nFigure 8 shows the performance of ReST\ud835\udc38\ud835\udc40-finetuned models, and compares them against the\nbase PaLM-2 model. We see no major degradation on any of the tasks on the BBH suite. Further, we\nfind that the model fine-tuned on Hendrycks MATH significantly outperforms the base model on this\nsuite when using chain-of-thought prompting, and the model fine-tuned on APPS also shows slight\nperformance gains. When using direct prompting, all three models perform similarly.\nProblem-solving. To stress test the math problem-solving capabilities on a held-out \u201creal-world\"\nevaluation set, we evaluate our model on the 2023 Hungarian high school finals exam in mathematics,\nakin to Grok. We follow the evaluation protocol from Paster (2023). Specifically, we evaluate the\nPaLM 2-L model, fine-tuned with ReST\ud835\udc38\ud835\udc40 on Hendrycks MATH, using the 1-shot prompt from Grok,\nsample solutions using temperature 0.1, and manually grade the outputs using the rubric provided by\nthe examiners. The results from evaluation are shown in Figure 9. We find that our model performs\nwell on this exam, surpassing the performance of all existing models except GPT-4.\n11\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n20\n30\n40\n50\n60\n70\nHungarian HS Finals Exam Score (%)\n30\n40\n50\n60\n70\n80\n90\nGSM8K Score (%)\nMetaMath 7B\nMetaMath Mistral 7B\nOpenChat 3.5\nCode Llama 34B\nLlemma 34B\nGPT-3.5 Turbo\nGPT-4\nGrok-0 (33B)\nGrok-1\nQwen 7B\nClaude 2\nMistral 7B\nMAmmoTH 7B\nPaLM 2-L (ReSTEM)\nExam Score vs GSM8K Performance of Various Models\nFigure 9 | Transfer results on Hungarian HS Finals Exam. Results for models other than PaLM-2-L\nfinetuned with ReST\ud835\udc38\ud835\udc40 are taken from Paster (2023). Several models specialized for mathematics\nperform well on the widely-used GSM8K benchmark but perform poorly on the Hungarian exam. In\ncontrast, PaLM 2-L model fine-tuned with ReST\ud835\udc38\ud835\udc40 performs well on both these benchmarks.\n6. Discussion\nIn this paper, we propose training on model-generated data combined with a reward function,\nvia ReST\ud835\udc38\ud835\udc40, for improving the performance of LLMs on problem-solving tasks. Furthermore, we\ndemonstrate that ReST\ud835\udc38\ud835\udc40 is theoretically grounded in the application of expectation-maximization\nto RL. We evaluate ReST\ud835\udc38\ud835\udc40 on mathematical problem solving and code generation, and show that\nReST\ud835\udc38\ud835\udc40 offers significant performance gains at a relatively low computational cost, especially when\ncompared to the cost of pre-training. Our experiments also show that ReST\ud835\udc38\ud835\udc40 does not lead to\nregression on other tasks. We conduct a number of ablations to better understand the strengths and\nweaknesses of this method, and find that it is very data-efficient, but also requires some vigilance to\navoid over-fitting.\nThere are a number of limitations associated with ReST\ud835\udc38\ud835\udc40. First, this method requires a moderately-\nsized training set of problems or prompts, which would need to be collected (from humans) for any\nnew task of interest. Second, ReST\ud835\udc38\ud835\udc40 also requires access to a manually-designed or learned reward\nfunction, ideally one that can be computed automatically. Finally, while ReST\ud835\udc38\ud835\udc40 allows significant\nperformance improvements in pass@1 performance, it may not quite close the gap to pass@K\nperformance for the same task (with a sufficiently large K). Future research in self-improvement in\nlanguage models should focus on automating manual parts of the pipeline (likely through language\nmodels as well), and explore algorithmic improvements that reduce the gap to pass@K performance.\nAcknowledgements\nWe would like to thank Tom Le Paine for providing feedback to an early draft. We also acknowledge\nBenjamin Anderson, Sridhar Thiagarajan, Feryal Behbahani, Aleksandra Faust, Doina Precup, Olivier\nBachem, and Slav Petrov for helpful discussions.\n12\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nAuthor Contributions\nAvi, Rishabh, and JD jointly led the project. Avi was responsible for training infrastructure, ablations\nand experiments on MATH, JD led the experiments on APPS, Rishabh was responsible for the paper\nwriting, evaluations, and distillation ablations.\nAnkesh, Piyush, Ethan, and Behnam observed preliminary findings about efficacy of model-\ngenerated data on MATH for Minerva models and motivated this research. Piyush also helped Avi\nin setting up infrastructure. Xavier, Peter, James, Jaeheoon, Kelvin and Yamini took part in project\ndiscussions. Jascha and Noah sponsored and advised the project. All other authors provided feedback\non this work.\nReferences\nR. Agarwal, C. Liang, D. Schuurmans, and M. Norouzi. Learning to generalize from sparse and\nunderspecified rewards. In International conference on machine learning, pages 130\u2013140. PMLR,\n2019.\nR. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and O. Bachem. Gkd: Generalized knowledge\ndistillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649, 2023.\nT. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search.\nAdvances in neural information processing systems, 30, 2017.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. M.\nLundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence:\nEarly experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712.\nURL https://doi.org/10.48550/arXiv.2303.12712.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.\nSuch, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol,\nA. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,\nJ. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,\nP. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,\nR. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168, 2021.\nP. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural\nComputation, 9(2):271\u2013278, 1997.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em\nalgorithm. Journal of the royal statistical society: series B (methodological), 39(1):1\u201322, 1977.\nH. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward ranked\nfinetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.\nGoogle, R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\n13\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nY. Gu, L. Dong, F. Wei, and M. Huang. Knowledge distillation of large language models. arXiv preprint\narXiv:2306.08543, 2023.\nC. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern,\nM. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint\narXiv:2308.08998, 2023.\nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He,\nD. Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938,\n2021a.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\nmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.\nJ. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve.\nCoRR, abs/2210.11610, 2022. doi: 10.48550/ARXIV.2210.11610. URL https://doi.org/10.\n48550/arXiv.2210.11610.\nC. Liang, J. Berant, Q. Le, K. D. Forbus, and N. Lao. Neural symbolic machines: Learning semantic\nparsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020, 2016.\nA. Ni, J. P. Inala, C. Wang, A. Polozov, C. Meek, D. Radev, and J. Gao. Learning math reasoning from\nself-sampled correct and partially-correct solutions. In The Eleventh International Conference on\nLearning Representations, 2022.\nM. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented\nmaximum likelihood for neural structured prediction. Advances In Neural Information Processing\nSystems, 29, 2016.\nOpenAI. Gpt-4 technical report, 2023.\nK. Paster. Testing language models on a held-out high school national finals exam. https://\nhuggingface.co/datasets/keirp/hungarian_national_hs_finals_exam, 2023.\nJ. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational space\ncontrol. In Proceedings of the 24th international conference on Machine learning, pages 745\u2013750,\n2007.\nD. Phan, M. D. Hoffman, D. Dohan, S. Douglas, T. A. Le, A. Parisi, P. Sountsov, C. Sutton, S. Vikram,\nand R. A. Saurous.\nTraining chain-of-thought via latent-variable inference.\narXiv preprint\narXiv:2312.02179, 2023.\nA. Sordoni, X. Yuan, M.-A. C\u00f4t\u00e9, M. Pereira, A. Trischler, Z. Xiao, A. Hosseini, F. Niedtner, and\nN. Le Roux. Joint prompt optimization of stacked llms using variational inference. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023.\nM. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,\nD. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261, 2022.\nX. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-\nconsistency improves chain of thought reasoning in language models. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023. URL https://openreview.net/pdf?id=1PL1NIMMrw.\n14\nBeyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\nY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\net al. Google\u2019s neural machine translation system: Bridging the gap between human and machine\ntranslation. arXiv preprint arXiv:1609.08144, 2016.\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical\nreasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.\nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances\nin Neural Information Processing Systems, 35:15476\u201315488, 2022.\n15\n"
  },
  {
    "title": "Photorealistic Video Generation with Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.06662.pdf",
    "upvote": "23",
    "text": "Photorealistic Video Generation with Diffusion Models\nAgrim Gupta1,2,\u2217\nLijun Yu2\nKihyuk Sohn2\nXiuye Gu2\nMeera Hahn2\nLi Fei-Fei1\nIrfan Essa2,3\nLu Jiang2\nJos\u00b4e Lezama2\n1 Stanford University\n2 Google Research\n3 Georgia Institute of Technology\nA cute cat riding a yellow surfboard, fast waves.\nA cute corgi wearing a blue sweater, walking towards the camera, slow motion.\nA raccoon wearing a black jacket, dancing in slow motion in front of the pyramids.\nFigure 1. W.A.L.T samples for text-to-video generation. Our approach can generate high-resolution, temporally consistent photorealistic\nvideos from text prompts. The samples shown are 512 \u00d7 896 resolution over 3.6 seconds duration at 8 frames per second.\nAbstract\nWe present W.A.L.T, a transformer-based approach for\nphotorealistic video generation via diffusion modeling. Our\napproach has two key design decisions. First, we use a\ncausal encoder to jointly compress images and videos within\na unified latent space, enabling training and generation\nacross modalities. Second, for memory and training effi-\nciency, we use a window attention architecture tailored for\njoint spatial and spatiotemporal generative modeling. Taken\ntogether these design decisions enable us to achieve state-\nof-the-art performance on established video (UCF-101 and\nKinetics-600) and image (ImageNet) generation benchmarks\nwithout using classifier free guidance. Finally, we also train\na cascade of three models for the task of text-to-video gen-\neration consisting of a base latent video diffusion model,\nand two video super-resolution diffusion models to generate\nvideos of 512 \u00d7 896 resolution at 8 frames per second.\n1. Introduction\nTransformers [73] are highly scalable and parallelizable neu-\nral network architectures designed to win the hardware lot-\ntery [39]. This desirable property has encouraged the re-\nsearch community to increasingly favor transformers over\ndomain-specific architectures in diverse fields such as lan-\nguage [26, 55\u201357], audio [1], speech [58], vision [18, 30],\nand robotics [5, 7, 89]. Such a trend towards unification\nallows researchers to share and build upon advancements\nin traditionally disparate domains. Thus, leading to a virtu-\nous cycle of innovation and improvement in model design\nfavoring transformers.\n\u2217Work partially done during an internship at Google.\narXiv:2312.06662v1  [cs.CV]  11 Dec 2023\nA notable exception to this trend is generative mod-\nelling of videos. Diffusion models [67, 69] have emerged\nas a leading paradigm for generative modelling of im-\nages [16, 33] and videos [36]. However, the U-Net architec-\nture [33, 62], consisting of a series of convolutional [46] and\nself-attention [73] layers, has been the predominant back-\nbone in all video diffusion approaches [16, 33, 36]. This\npreference stems from the fact that the memory demands\nof full attention mechanisms in transformers scale quadrat-\nically with input sequence length. Such scaling leads to\nprohibitively high costs when processing high-dimensional\nsignals like video.\nLatent diffusion models (LDMs) [61] reduce computa-\ntional requirements by operating in a lower-dimensional\nlatent space derived from an autoencoder [20, 72, 75]. A\ncritical design choice in this context is the type of latent space\nemployed: spatial compression (per frame latents) versus\nspatiotemporal compression. Spatial compression is often\npreferred because it enables leveraging pre-trained image\nautoencoders and LDMs, which are trained on large paired\nimage-text datasets. However, this choice increases network\ncomplexity and limits the use of transformers as backbones,\nespecially in generating high-resolution videos due to mem-\nory constraints. On the other hand, while spatiotemporal\ncompression can mitigate these issues, it precludes the use\nof paired image-text datasets, which are much larger and\ndiverse than their video counterparts.\nWe present Window Attention Latent Transformer\n(W.A.L.T): a transformer-based method for latent video dif-\nfusion models (LVDMs). Our method consists of two stages.\nFirst, an autoencoder maps both videos and images into a\nunified, lower-dimensional latent space. This design choice\nenables training a single generative model jointly on image\nand video datasets and significantly reduces the computa-\ntional burden for generating high resolution videos. Sub-\nsequently, we propose a new design of transformer blocks\nfor latent video diffusion modeling which is composed of\nself-attention layers that alternate between non-overlapping,\nwindow-restricted spatial and spatiotemporal attention. This\ndesign offers two primary benefits: firstly, the use of lo-\ncal window attention significantly lowers computational de-\nmands. Secondly, it facilitates joint training, where the spa-\ntial layers independently process images and video frames,\nwhile the spatiotemporal layers are dedicated to modeling\nthe temporal relationships in videos.\nWhile conceptually simple, our method provides the first\nempirical evidence of transformers\u2019 superior generation qual-\nity and parameter efficiency in latent video diffusion on pub-\nlic benchmarks. Specifically, we report state-of-the-art re-\nsults on class-conditional video generation (UCF-101 [70]),\nframe prediction (Kinetics-600 [9]) and class conditional\nimage generation (ImageNet [15]) without using classifier\nfree guidance. Finally, to showcase the scalability and ef-\nficiency of our method we also demonstrate results on the\nchallenging task of photorealistic text-to-video generation.\nWe train a cascade of three models consisting of a base la-\ntent video diffusion model, and two video super-resolution\ndiffusion models to generate videos of 512 \u00d7 896 resolution\nat 8 frames per second and report state-of-the-art zero-shot\nFVD score on the UCF-101 benchmark.\n2. Related Work\nVideo Diffusion Models.\nDiffusion models have shown\nimpressive results in image [33, 38, 52, 61, 67, 68] and\nvideo generation [4, 24, 29, 34, 36, 66]. Video diffusion\nmodels can be categorized into pixel-space [34, 36, 66] and\nlatent-space [4, 24, 31, 83] approaches, the later bringing\nimportant efficiency advantages when modeling videos. Ho\net al. [36] demonstrated that the quality of text conditioned\nvideo generation can be significantly improved by jointly\ntraining on image and video data. Similarly, to leverage\nimage datasets, latent video diffusion models inflate a pre-\ntrained image model, typically a U-Net [62], into a video\nmodel by adding temporal layers, and initializing them as\nthe identity function [4, 34, 66]. Although computationally\nefficient, this approach couples the design of video and im-\nage models, and precludes spatiotemporal compression. In\nthis work, we operate on a unified latent space for images\nand videos, allowing us to leverage large scale image and\nvideo datasets while enjoying computational efficiency gains\nfrom spatiotemporal compression of videos.\nTransformers for Generative Modeling.\nMultiple classes\nof generative models have utilized Transformers [73] as back-\nbone, such as, Generative adversarial networks [42, 47, 85],\nautoregressive [10, 11, 20, 21, 27, 59, 74, 77, 78, 80, 81]\nand diffusion [2, 22, 41, 50, 53, 87] models. Inspired by the\nsuccess of autoregressive pretraining of large language mod-\nels [55\u201357], Ramesh et al. [59] trained a text-to-image gen-\neration model by predicting the next visual token obtained\nfrom an image tokenizer. Subsequently, this approach was\napplied to multiple applications including class-conditional\nimage generation [20, 79], text-to-image [17, 59, 76, 80] or\nimage-to-image translation [21, 77]. Similarly, for video gen-\neration, transformer-based models were proposed to predict\nnext tokens using 3D extensions of VQGAN [23, 37, 78, 81]\nor using per frame image latents [27]. Autoregressive sam-\npling of videos is typically impractical given the very long se-\nquences involved. To alleviate this issue, non-autoregressive\nsampling [10, 11], i.e. parallel token prediction, has been\nadopted as a more efficient solution for transformer-based\nvideo generation [27, 74, 81]. Recently, the community has\nstarted adopting transformers as the denoising backbone for\ndiffusion models in place of U-Net [12, 38, 50, 53, 87]. To\nthe best of our knowledge, our work is the first successful\nSpatial Self \nAttention\nSpatial Cross \nAttention\nqs\nqs\nqs\nqs\nk\nv\nJoint Causal 3D Encoder\n\ud835\udc58 = \ud835\udc63 = concat[\ud835\udc5e!, t5_xl (text)]\n\u00d7\ud835\udc3f\nSpatiotemporal \nSelf Attention\nSpatial Cross \nAttention\nqt\nqt\nqt\nqs\nk\nv\n\ud835\udc5e! \u2208 \u211d \". $%&\n\u00d7$\u00d7(#\u00d7)#\u00d7* \n\ud835\udc5e&  \u2208 \u211d \".,$ \u00d7 $%& \u00d7(#% \u00d7)#% \u00d7-\nFigure 2. W.A.L.T. We encode images and videos into a shared latent space. The transformer backbone processes these latents with blocks\nhaving two layers of window-restricted attention: spatial layers capture spatial relations in both images and video, while spatiotemporal\nlayers model temporal dynamics in videos and passthrough images via identity attention mask. Text conditioning is done via spatial\ncross-attention.\nempirical demonstration (\u00a7 5.1) of a transformer-based back-\nbone for jointly training image and video latent diffusion\nmodels.\n3. Background\nDiffusion formulation. Diffusion models [33, 67, 69] are a\nclass of generative models which learn to generate data by\niteratively denoising samples drawn from a noise distribution.\nGaussian diffusion models assume a forward noising process\nwhich gradually applies noise (\u03f5) to real data (x0 \u223c pdata).\nConcretely,\nxt =\np\n\u03b3(t) x0 +\np\n1 \u2212 \u03b3(t) \u03f5,\n(1)\nwhere \u03f5 \u223c N(0, I), t \u2208 [0, 1], and \u03b3(t) is a monotonically\ndecreasing function (noise schedule) from 1 to 0. Diffusion\nmodels are trained to learn the reverse process that inverts\nthe forward corruptions:\nEx\u223cpdata,t\u223cU(0,1),\u03f5\u223cN (0,I)\nh\n\u2225y \u2212 f\u03b8(xt; c, t)\u22252i\n,\n(2)\nwhere f\u03b8 is the denoiser model parameterized by a neural net-\nwork, c is conditioning information e.g., class labels or text\nprompts, and the target y can be random noise \u03f5, denoised in-\nput x0 or v =\np\n1 \u2212 \u03b3(t) \u03f5\u2212\np\n\u03b3(t) x0. Following [34, 63],\nwe use v-prediction in all our experiments.\nLatent diffusion models (LDMs).\nProcessing high-\nresolution images and videos using raw pixels requires\nconsiderable computational resources.\nTo address this,\nLDMs operate on the low dimensional latent space of a\nVQ-VAE [20, 72]. VQ-VAE consists of an encoder E(x)\nthat encodes an input video x \u2208 RT \u00d7H\u00d7W \u00d73 into a latent\nrepresentation z \u2208 Rt\u00d7h\u00d7w\u00d7c. The encoder downsamples\nthe video by a factor of fs = H/h = W/w and ft = T/t,\nwhere T = t = 1 corresponds to using an image autoen-\ncoder. An important distinction from the original VQ-VAE\nis the absence of a codebook of quantized embeddings as\ndiffusion models can operate on continous latent spaces. A\ndecoder D is trained to predict a reconstruction of the video,\n\u02c6x, from z. Following VQ-GAN [20], reconstruction qual-\nity can be further improved by adding adversarial [25] and\nperceptual losses [43, 86].\n4. W.A.L.T\n4.1. Learning Visual Tokens\nA key design decision in video generative modeling is the\nchoice of latent space representation. Ideally, we want a\nshared and unified compressed visual representation that\ncan be used for generative modeling of both images and\nvideos [74, 82]. The unified representation is important\nbecause joint image-video learning is preferable due to a\nscarcity of labeled video data [34], such as text-video pairs.\nConcretely, given a video sequence x \u2208 R(1+T )\u00d7H\u00d7W \u00d7C,\nwe aim to learn a low-dimensional representation z \u2208\nR(1+t)\u00d7h\u00d7w\u00d7c that performs spatial-temporal compression\nby a factor of fs = H/h = W/w in space and a factor of\nft = T/t in time. To enable a unified representation for\nboth videos and static images, the first frame is always en-\ncoded independently from the rest of the video. This allows\nstatic images x \u2208 R1\u00d7H\u00d7W \u00d7C to be treated as videos with\na single frame, i.e. z \u2208 R1\u00d7h\u00d7w\u00d7c.\nWe instantiate this design with the causal 3D CNN\nencoder-decoder architecture of the MAGVIT-v2 tok-\nenizer [82]. Typically the encoder-decoder consists of reg-\nular 3D convolution layers which cannot process the first\nframe independently [23, 81]. This limitation stems from the\nfact that a regular convolutional kernel of size (kt, kh, kw)\nwill operate on\n\u0004 kt\u22121\n2\n\u0005\nframes before and\n\u0004 kt\n2\n\u0005\nframes after\nthe input frames. Causal 3D convolution layers solve this\nissue as the convolutional kernel operates on only the past\nkt \u2212 1 frames. This ensures that the output for each frame\nis influenced solely by the preceding frames, enabling the\nmodel to tokenize the first frame independently.\nAfter this stage, the input to our model is a batch of la-\ntent tensors z \u2208 R(1+t)\u00d7h\u00d7w\u00d7c representing a single video\nor a stack of 1 + t independent images (Fig. 2). Differ-\nent from [82], our latent representation is real-valued and\nquantization-free. In the section below we describe how our\nmodel jointly processes a mixed batch of images and videos.\n4.2. Learning to Generate Images and Videos\nPatchify. Following the original ViT [18], we \u201cpatchify\u201d\neach latent frame independently by converting it into a se-\nquence of non-overlapping hp\u00d7wp patches where hp = h/p,\nwp = w/p and p is the patch size. We use learnable posi-\ntional embeddings [73], which are the sum of space and\ntime positional embeddings. Position embeddings are added\nto the linear projections [18] of the patches. Note that for\nimages, we simply add the temporal position embedding\ncorresponding to the first latent frame.\nWindow attention. Transformer models composed entirely\nof global self-attention modules incur significant compute\nand memory costs, especially for video tasks. For efficiency\nand for processing images and videos jointly we compute\nself-attention in windows [27, 73], based on two types of non-\noverlapping configurations: spatial (S) and spatiotemporal\n(ST), cf. Fig. 2. Spatial Window (SW) attention is restricted\nto all the tokens within a latent frame of size 1\u00d7hp\u00d7wp (the\nfirst dimension is time). SW models the spatial relations in\nimages and videos. Spatiotemporal Window (STW) attention\nis restricted within a 3D window of size (1 + t) \u00d7 h\u2032\np \u00d7\nh\u2032\nw, modeling the temporal relationships among video latent\nframes. For images, we simply use an identity attention\nmask ensuring that the value embeddings corresponding to\nthe image frame latents are passed through the layer as is.\nFinally, in addition to absolute position embeddings we also\nuse relative position embeddings [49].\nOur design, while conceptually straightforward, achieves\ncomputational efficiency and enables joint training on image\nand video datasets. In contrast to methods based on frame-\nlevel autoencoders [4, 24, 27], our approach does not suffer\nfrom flickering artifacts, which often result from encoding\nand decoding video frames independently. However, similar\nto Blattmann et al. [4], we can also potentially leverage pre-\ntrained image LDMs with transformer backbones by simply\ninterleaving STW layers.\n4.3. Conditional Generation\nTo enable controllable video generation, in addition to condi-\ntioning on timestep t, diffusion models are often conditioned\non additional conditional information c such as class labels,\nnatural language, past frames or low resolution videos. In\nour transformer backbone, we incorporate three types of\nconditioning mechanisms as described in what follows:\nCross-attention. In addition to self-attention layers in\nour window transformer blocks, we add a cross-attention\nlayer for text conditioned generation. When training models\non just videos, the cross-attention layer employs the same\nwindow-restricted attention as the self-attention layer, mean-\ning S/ST blocks will have SW/STW cross-attention layers\n(Fig. 2). However, for joint training, we only use SW cross-\nattention layers. For cross-attention we concatenate the input\nsignal (query) with the conditioning signal (key, value) as\nour early experiments showed this improves performance.\nAdaLN-LoRA. Adaptive normalization layers are an\nimportant component in a broad range of generative and\nvisual synthesis models [16, 19, 44, 52\u201354].\nA simple\nway to incorporate adaptive layer normalization is to in-\nclude for each layer i, an MLP layer to regress a vec-\ntor of conditioning parameters Ai = MLP(c + t), where\nAi = concat(\u03b31, \u03b32, \u03b21, \u03b22, \u03b11, \u03b12), Ai \u2208 R6\u00d7dmodel, and\nc \u2208 Rdmodel, t \u2208 Rdmodel are the condition and timestep em-\nbeddings. In the transformer block, \u03b3 and \u03b2 scale and shift\nthe inputs of the multi-head attention and MLP layers, re-\nspectively, while \u03b1 scales the output of both the multi-head\nattention and MLP layers. The parameter count of these\nadditional MLP layers scales linearly with the number of\nlayers and quadratically with the model\u2019s dimensional size\n(num blocks \u00d7 dmodel \u00d7 6 \u00d7 dmodel). For instance, in a\nViT-g model with 1B parameters, the MLP layers contribute\nan additional 475M parameters. Inspired by [40], we pro-\npose a simple solution dubbed AdaLN-LoRA, to reduce the\nmodel parameters. For each layer, we regress conditioning\nparameters as\nA1 = MLP(c + t),\nAi = A1 + W i\nbW i\na(c + t)\n\u2200i \u0338= 1, (3)\nwhere W i\nb \u2208 Rdmodel\u00d7r, W i\na \u2208 Rr\u00d7(6\u00d7dmodel). This reduces\nthe number of trainable model parameters significantly when\nr \u226a dmodel. For example, a ViT-g model with r = 2 reduces\nthe MLP parameters from 475M to 12M.\nSelf-conditioning. In addition to being conditioned on\nexternal inputs, iterative generative algorithms can also be\nconditioned on their own previously generated samples dur-\ning inference [3, 13, 65]. Specifically, Chen et al. [13]\nmodify the training process for diffusion models, such that\nwith some probability psc the model first generates a sam-\nple \u02dcz0 = f\u03b8(zt; 0, c, t) and then refines this estimate us-\ning another forward pass conditioned on this initial sample:\nf\u03b8(zt; stopgrad(\u02dcz0), c, t). With probability 1 \u2212 psc, only\na single forward pass is done. We concatenate the model es-\ntimate with the input along the channel dimension and found\nthis simple technique to work well when used in conjunction\nwith v-prediction.\n4.4. Autoregressive Generation\nFor generating long videos via autoregressive prediction we\nalso train our model jointly on the task of frame prediction.\nThis is achieved by conditioning the model on past frames\nwith a probability of pfp during training. Specifically, the\nMethod\nK600 FVD\u2193 UCF FVD\u2193 params. steps\nTrIVD-GAN-FP [51] 25.7\u00b10.7\n\u2013\n\u2013\n1\nVideo Diffusion [36] 16.2\u00b10.3\n\u2013\n1.1B\n256\nRIN [41]\n10.8\n\u2013\n411M\n1000\nTATS [23]\n\u2013\n332\u00b118\n321M\n1024\nPhenaki [74]\n36.4\u00b10.2\n\u2013\n227M\n48\nMAGVIT [81]\n9.9\u00b10.3\n76\u00b12\n306M\n12\nMAGVITv2 [82]\n4.3\u00b10.1\n58\u00b12\n307M\n24\nW.A.L.T-L Ours\n3.3\u00b10.0\n46\u00b12\n313M\n50\nW.A.L.T-XL Ours\n\u2013\n36\u00b12\n460M\n50\nTable 1. Video generation evaluation on frame prediction on\nKinetics-600 and class-conditional generation on UCF-101.\nmodel is conditioned using cfp = concat(mfp \u25e6 zt, mfp),\nwhere mfp is a binary mask. The binary mask indicates the\nnumber of past frames used for conditioning. We condi-\ntion on either 1 latent frame (image to video generation)\nor 2 latent frames (video prediction). This conditioning is\nintegrated into the model through concatenation along the\nchannel dimension of the noisy latent input. During infer-\nence, we use standard classifier-free guidance with cfp as the\nconditioning signal.\n4.5. Video Super Resolution\nGenerating high-resolution videos with a single model is\ncomputationally prohibitive. Following [35], we use a cas-\ncaded approach with three models operating at increasing\nresolutions. Our base model generates videos at 128 \u00d7 128\nresolution which are subsequently upsampled twice via two\nsuper resolution stages. We first spatially upscale the low res-\nolution input zlr (video or image) using a depth-to-space con-\nvolution operation. Note that, unlike training where ground\ntruth low-resolution inputs are available, inference relies on\nlatents produced by preceding stages (cf. teaching-forcing).\nTo reduce this discrepancy and improve the robustness of\nthe super-resolution stages in handling artifacts generated by\nlower resolution stages, we use noise conditioning augmen-\ntation [35]. Concretely, noise is added in accordance with\n\u03b3(t), by sampling a noise level as tsr \u223c U(0, tmax noise) and\nis provided as input to our AdaLN-LoRA layers.\nAspect-ratio finetuning. To simplify training and leverage\nbroad data sources with different aspect ratios, we train our\nbase stage using a square aspect ratio. We fine-tune the base\nstage on a subset of data to generate videos with a 9 : 16\naspect ratio by interpolating position embeddings.\n5. Experiments\nIn this section, we evaluate our method on multiple tasks:\nclass-conditional image and video generation, frame pre-\ndiction and text conditioned video generation and perform\nextensive ablation studies of different design choices. For\nMethod\nCost (Iter\u00d7BS)\nFID\u2193\nIS\u2191\nparams.\nsteps\nBigGAN-deep [6]\n-\n6.95\n171.4\n160M\n1\nLDM-4 [61]\n178k\u00d71200\n10.56\n103.5\n400M\n250\nDiT-XL/2 [53]\n7000k\u00d7256\n9.62\n121.5\n675M\n250\nADM [16]\n-\n7.49\n127.5\n608M\n2000\nMDT [22]\n6500k\u00d7256\n6.23\n143.0\n676M\n250\nMaskDiT [87]\n1200k\u00d71024\n5.69\n178.0\n736M\n40\nRIN [41]\n600k\u00d71024\n3.42\n182.0\n410M\n1000\nsimple diffusion [38]\n500K\u00d72048\n2.77\n211.8\n2B\n512\nVDM++ [45]\n-\n2.40\n225.3\n2B\n512\nW.A.L.T-L Ours\n437k\u00d71024\n2.56\n215.1\n460M\n50\nTable 2.\nClass-conditional image generation on ImageNet\n256\u00d7256. We adopt the evaluation protocol and implementation of\nADM [16] and report results without classifier free guidance.\nqualitative results, see Fig. 1, Fig. 3, Fig. 4 and videos on\nour project website. See appendix for additional details.\n5.1. Visual Generation\nVideo generation. We consider two standard video bench-\nmarks, UCF-101 [70] for class-conditional generation and\nKinetics-600 [9] for video prediction with 5 conditioning\nframes. We use FVD [71] as our primary evaluation metric.\nAcross both datasets, W.A.L.T significantly outperforms all\nprior works (Tab. 1). Compared to prior video diffusion mod-\nels, we achieve state-of-the-art performance with less model\nparameters, and require 50 DDIM [68] inference steps.\nImage generation. To verify the modeling capabilities of\nW.A.L.T on the image domain, we train a version of W.A.L.T\nfor the standard ImageNet class-conditional setting. For eval-\nuation, we follow ADM [16] and report the FID [32] and\nInception [64] scores calculated on 50K samples generated\nin 50 DDIM steps. We compare (Table 2) W.A.L.T with\nstate-of-the-art image generation methods for 256 \u00d7 256\nresolution. Our model outperforms prior works without re-\nquiring specialized schedules, convolution inductive bias,\nimproved diffusion losses, and classifier free guidance. Al-\nthough VDM++ [45] has slightly better FID score, the model\nhas significantly more parameters (2B).\n5.2. Ablation Studies\nWe ablate W.A.L.T to understand the contribution of various\ndesign decisions with the default settings: model L, patch\nsize 1, 1 \u00d7 16 \u00d7 16 spatial window, 5 \u00d7 8 \u00d7 8 spatiotemporal\nwindow, psc = 0.9, c = 8 and r = 2.\nPatch size.\nIn various computer vision tasks utilizing\nViT[18]-based models, a smaller patch size p has been shown\nto consistently enhance performance [8, 18, 28, 84]. Sim-\nilarly, our findings also indicate that a reduced patch size\nimproves performance (Table 3a).\nWindow attention. We compare three different STW win-\ndow configurations with full self-attention (Table 3b). We\nfind that local self-attention can achieve competitive (or bet-\npatch size p FVD\u2193\nIS\u2191\n1\n60.7\n87.2\n2\n134.4\n82.2\n4\n461.8\n63.9\n(a) Patch size. Lower patch size is signifi-\ncantly better.\nst window\nFVD\u2193\nIS\u2191\nsps\n5 \u00d7 4 \u00d7 4\n56.9\n87.3 2.24\n5 \u00d7 8 \u00d7 8\n59.6\n87.4 2.00\n5 \u00d7 16 \u00d7 16\n55.3\n87.4 1.75\nfull self attn.\n59.9\n87.8 1.20\n(b) Spatiotemporal window size. Full self-attention is\nnot essential for good performance. sps is steps per sec.\npsc\nFVD\u2193\nIS\u2191\n0.0\n109.9\n82.6\n0.3\n76.0\n86.5\n0.6\n60.0\n86.8\n0.9\n61.4\n87.1\n(c) Self-conditioning. Higher psc is better.\nr\nFVD\u2193\nIS\u2191\nparams\n2\n60.7\n87.2\n313 M\n4\n56.6\n87.3\n314 M\n16\n55.5\n88.0\n316 M\n64\n54.4\n87.9\n324 M\n256\n52.5\n88.5\n357 M\n(d) AdaLN-LoRA. Bigger r is better.\nFVD\u2193\nIS\u2191\nw/o qk norm [14]\n59.0\n86.8\nw/o latent norm\n67.9\n87.1\nw/o zero snr [48]\n91.0\n84.2\nfull method\n60.7\n87.2\n(e) Other improvements. See text for details.\nc\nrFVD\u2193 FVD\u2193\nIS\u2191\n4\n37.7\n86.4\n84.9\n8\n17.1\n75.4\n86.3\n16\n8.2\n67.0\n86.0\n32\n3.5\n83.4\n82.9\n(f) Latent dimension c. Higher c is better.\nTable 3. Ablation experiments on UCF-101 [70]. We compare FVD and inception scores to ablate important design decisions with the\ndefault setting: L model, 1 \u00d7 16 \u00d7 16 spatial window, 5 \u00d7 8 \u00d7 8 saptiotemporal (st) window, psc = 0.9, c = 8 and r = 2.\nModel\nAdaLN\nFVD\u2193\nIS\u2191\nparams.\nfinal loss\nL\nseparate\n34.6\n90.2\n458M\n0.274\nXL\nLoRA-2\n36.7\n89.4\n460M\n0.268\nTable 4. Parameter matched comparison between AdaLN-LoRA\nand per layer adaln layers. See text for details.\nter) performance while being significantly faster (up to 2\u00d7)\nand requiring less accelerator memory.\nSelf-conditioning. In Table 3c we study the influence of\nvarying the self-conditioning rate psc on generation quality.\nWe notice a clear trend: increasing the self conditioning rate\nfrom 0.0 (no self-conditioning) to 0.9 improves the FVD\nscore substantially (44%).\nAdaLN-LoRA. An important design decision in diffusion\nmodels is the conditioning mechanism. We investigate the\neffect of increasing the bottleneck dimension r in our pro-\nposed AdaLN-LoRA layers (Table 3d). This hyperparameter\nprovides a flexible way to trade off between number of model\nparameters and generation performance. As shown in Ta-\nble 3d, increasing r improves performance but also increases\nmodel parameters. This highlights an important model de-\nsign question: given a fixed parameter budget, how should\nwe allocate parameters - either by using separate AdaLN\nlayers, or by increasing base model parameters while using\nshared AdaLN-LoRA layers? We explore this in Table 4\nby comparing two model configurations: W.A.L.T-L with\nseparate AdaLN layers and W.A.L.T-XL with AdaLN-LoRA\nand r = 2. While both configurations yield similar FVD and\nInception scores, W.A.L.T-XL achieves a lower final loss\nvalue, suggesting the advantage of allocating more parame-\nters to the base model and choosing an appropriate r value\nwithin accelerator memory limits.\nNoise schedule.\nCommon latent diffusion noise sched-\nules [61] typically do not ensure a zero signal-to-noise ra-\ntio (SNR) at the final timestep, i.e., at t = 1, \u03b3(t) > 0.\nThis leads to a mismatch between training and inference\nphases. During inference, models are expected to start from\npurely Gaussian noise, whereas during training, at t = 1, a\nsmall amount of signal information remains accessible to\nthe model. This is especially harmful for video generation\nas videos have high temporal redundancy. Even minimal\ninformation leakage at t = 1 can reveal substantial informa-\ntion to the model. Addressing this mismatch by enforcing a\nzero terminal SNR [48] significantly improves performance\n(Table 3e). Note that this approach was originally proposed\nto fix over-exposure problems in image generation, but we\nfind it effective for video generation as well.\nAutoencoder. Finally, we investigate one critical but often\noverlooked hyperparameter in the first stage of our model:\nthe channel dimension c of the autoencoder latent z. As\nshown in Table 3f, increasing c significantly improves the\nreconstruction quality (lower rFVD) while keeping the same\nspatial fs and temporal compression ft ratios. Empirically,\nwe found that both lower and higher values of c lead to\npoor FVD scores in generation, with a sweet spot of c = 8\nworking well across most datasets and tasks we evaluated.\nWe also normalize the latents before processing them via\ntransformer which further improves performance.\nIn our transformer models, we use query-key normal-\nization [14] as it helps stabilize training for larger models.\nFinally, we note that some of our default settings are not\noptimal, as indicated by ablation studies. These defaults\nwere chosen early on for their robustness across datasets,\nthough further tuning may improve performance.\n5.3. Text-to-video\nWe train W.A.L.T for text-to-video jointly on text-image and\ntext-video pairs (Sec. 4.2). We used a dataset of \u223c970M\nA polar bear swimming.\nPouring beer into an empty glass, low angle shot, bar in the background.\nA cat eating food out of a bowl, in the style of Van Gogh.\nPouring chocolate sauce over vanilla ice cream in a cone, studio lighting.\nA robot ballerina dancing gracefully, highly detailed, studio lighting.\nA cute panda skateboarding in the sky, over snow covered mountains, with dreamy whimsical atmosphere.\nAn astronaut riding a horse.\nFigure 3. Qualitative evaluation. Example videos generated by W.A.L.T from natural language prompts at 512 \u00d7 896 resolution over 3.6\nseconds duration at 8 frames per second. The W.A.L.T model is able to generate temporally consistent photorealistic videos that align with\nthe textual prompt.\nFigure 4. Examples of consistent 3D camera motion (5.1 secs).\nPrompts: camera turns around a {blue jay, bunny}, studio lighting,\n360\u25e6 rotation. Best viewed in video format.\ntext-image pairs and \u223c89M text-video pairs from the public\ninternet and internal sources. We train our base model at\nresolution 17 \u00d7 128 \u00d7 128 (3B parameters), and two 2\u00d7\ncascaded super-resolution models for 17 \u00d7 128 \u00d7 224 \u2192\n17 \u00d7 256 \u00d7 448 (L, 1.3B, p = 2) and 17 \u00d7 256 \u00d7 448 \u2192\n17 \u00d7 512 \u00d7 896 (L, 419M, p = 2) respectively. We fine-tune\nthe base stage for the 9 : 16 aspect ratio to generate videos\nat resolution 128 \u00d7 224. We use classifier free guidance for\nall our text-to-video results.\n5.3.1\nQuantitative Evaluation\nEvaluating text-conditioned video generation systems scien-\ntifically remains a significant challenge, in part due to the\nabsence of standardized training datasets and benchmarks.\nSo far we have focused our experiments and analyses on the\nstandard academic benchmarks, which use the same training\ndata to ensure controlled and fair comparisons. Nevertheless,\nto compare with prior work on text-to-video, we also report\nresults on the UCF-101 dataset in the zero-shot evaluation\nprotocol in Table 5 [24, 37, 66]. Also see supplement.\nJoint training. A primary strength of our framework is\nits ability to train simultaneously on both image and video\ndatasets. In Table 5 we ablate the impact of this joint training\napproach. Specifically, we trained two versions of W.A.L.T-\nL (each with 419M params.) models using the default set-\ntings specified in \u00a7 5.2. We find that joint training leads\nto a notable improvement across both metrics. Our results\nalign with the findings of Ho et al. [36], who demonstrated\nthe benefits of joint training for pixel-based video diffusion\nmodels with U-Net backbones.\nScaling. Transformers are known for their ability to scale\neffectively in many tasks [5, 14, 55]. In Table 5 we show the\nbenefits of scaling our transformer model for video diffusion.\nMethod\nIS (\u2191)\nFVD (\u2193)\nCogVideo (Chinese) [37]\n23.6\n751.3\nCogVideo (English) [37]\n25.3\n701.6\nMagicVideo [88]\n-\n699.0\nMake-A-Video [66]\n33.0\n367.2\nVideo LDM [4]\n33.5\n550.6\nPYoCo [24]\n47.8\n355.2\nW.A.L.T (Ours) 419M (video only)\n26.8\n598.8\nW.A.L.T (Ours) 419M (video + image)\n31.7\n344.5\nW.A.L.T (Ours) 3B (video + image)\n35.1\n258.1\nTable 5. UCF-101 text-to-video generation. Joint training on\nimage and video datasets in conjunction with scaling the model\nparameters is essential for high quality video generation.\nScaling our base model size leads to significant improve-\nments on both the metrics. It is important to note, however,\nthat our base model is considerably smaller than leading\ntext-to-video systems. For instance, Ho et al. [34] trained\nbase model of 5.7B parameters. Hence, we believe scaling\nour models further is an important direction of future work.\nComparison with prior work. In Table 5, we present a\nsystem-level comparison of various text-to-video generation\nmethods. Our results are promising; we surpass all previous\nwork in the FVD metric. In terms of the IS, our performance\nis competitive, outperforming all but PYoCo [24]. A possible\nexplanation for this discrepancy might be PYoCo\u2019s use of\nstronger text embeddings. Specifically, they utilize both\nCLIP [57] and T5-XXL [60] encoders, whereas we employ\na T5-XL [60] text encoder only.\n5.3.2\nQualitative Results\nAs mentioned in \u00a7 4.4, we jointly train our model on the\ntask of frame prediction conditioned on 1 or 2 latent frames.\nHence, our model can be used for animating images (image-\nto-video) and generating longer videos with consistent cam-\nera motion (Fig. 4). See videos on our project website.\n6. Conclusion\nIn this work, we introduce W.A.L.T, a simple, scalable, and\nefficient transformer-based framework for latent video dif-\nfusion models. We demonstrate state-of-the-art results for\nimage and video generation using a transformer backbone\nwith windowed attention. We also train a cascade of three\nW.A.L.T models jointly on image and video datasets, to\nsynthesize high-resolution, temporally consistent photore-\nalistic videos from natural language descriptions. While\ngenerative modeling has seen tremendous recent advances\nfor images, progress on video generation has lagged behind.\nWe hope that scaling our unified framework for image and\nvideo generation will help close this gap.\nAcknowledgements\nWe thank Bryan Seybold, Dan Kondratyuk, David Ross,\nHartwig Adam, Huisheng Wang, Jason Baldridge, Mauricio\nDelbracio and Orly Liba for helpful discussions and feed-\nback.\nReferences\n[1] Andrea Agostinelli, Timo I Denk, Zal\u00b4an Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren\nJansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM:\nGenerating music from text. arXiv:2301.11325, 2023. 1\n[2] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth\nwords: a vit backbone for score-based diffusion models. In\nNeurIPS 2022 Workshop on Score-Based Methods, 2022. 2\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer. Scheduled sampling for sequence prediction with\nrecurrent neural networks. Advances in neural information\nprocessing systems, 28, 2015. 4\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 2, 4, 8\n[5] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Col-\nine Devin, Alex X Lee, Maria Bauza, Todor Davchev, Yux-\niang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A\nself-improving foundation agent for robotic manipulation.\narXiv preprint arXiv:2306.11706, 2023. 1, 8\n[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high fidelity natural image synthesis.\nIn ICLR, 2018. 5\n[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817, 2022. 1\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 5\n[9] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe\nHillier, and Andrew Zisserman. A short note about Kinetics-\n600. arXiv:1808.01340, 2018. 2, 5\n[10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. MaskGIT: Masked generative image transformer.\nIn CVPR, 2022. 2\n[11] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJos\u00b4e Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T Freeman, Michael Rubinstein, et al. Muse: Text-\nto-image generation via masked generative transformers. In\nICML, 2023. 2\n[12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. Pixart-\\alpha: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023. 2\n[13] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog\nbits: Generating discrete data using diffusion models with\nself-conditioning. arXiv preprint arXiv:2208.04202, 2022. 4\n[14] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In International Conference on Machine Learning,\npages 7480\u20137512. PMLR, 2023. 6, 8\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 2009. 2\n[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. In NeurIPS, 2021. 2, 4, 5, 1\n[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang\nZhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia\nYang, et al. CogView: Mastering text-to-image generation\nvia transformers. In NeurIPS, 2021. 2\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 1, 4,\n5\n[19] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.\nA learned representation for artistic style. arXiv preprint\narXiv:1610.07629, 2016. 4\n[20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 2, 3\n[21] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based\ntext-to-image generation with human priors. In European\nConference on Computer Vision, pages 89\u2013106. Springer,\n2022. 2\n[22] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng\nYan. Masked diffusion transformer is a strong image synthe-\nsizer. arXiv:2303.14389, 2023. 2, 5, 1\n[23] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang,\nDavid Jacobs, Jia-Bin Huang, and Devi Parikh. Long video\ngeneration with time-agnostic VQGAN and time-sensitive\ntransformer. In ECCV, 2022. 2, 3, 5\n[24] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. arXiv preprint\narXiv:2305.10474, 2023. 2, 4, 8, 1\n[25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 3\n[26] Google. PaLM 2 technical report. arXiv:2305.10403, 2023.\n1\n[27] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu,\nRoberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, and Li Fei-Fei. MaskViT: Masked\nvisual pre-training for video prediction. In ICLR, 2022. 2, 4\n[28] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese\nmasked autoencoders.\narXiv preprint arXiv:2305.14344,\n2023. 5\n[29] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion modeling\nof long videos. Advances in Neural Information Processing\nSystems, 35:27953\u201327965, 2022. 2\n[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022. 1\n[31] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221,\n2023. 2\n[32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. GANs trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nIn NeurIPS, 2017. 5\n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2, 3\n[34] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv:2210.02303, 2022. 2, 3, 8\n[35] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 23(1):\n2249\u20132281, 2022. 5\n[36] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. In ICLR Workshops, 2022. 2, 5, 8\n[37] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. CogVideo: Large-scale pretraining for text-to-video\ngeneration via transformers. arXiv:2205.15868, 2022. 2, 8\n[38] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple\ndiffusion: End-to-end diffusion for high resolution images. In\nICML, 2023. 2, 5, 1\n[39] Sara Hooker. The hardware lottery. Communications of the\nACM, 64(12):58\u201365, 2021. 1\n[40] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. In ICLR,\n2021. 4\n[41] Allan Jabri, David J Fleet, and Ting Chen. Scalable adaptive\ncomputation for iterative generation. In ICML, 2023. 2, 5\n[42] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo pure transformers can make one strong gan, and that\ncan scale up. Advances in Neural Information Processing\nSystems, 34:14745\u201314758, 2021. 2\n[43] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 2016. 3\n[44] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 4\n[45] Diederik P Kingma and Ruiqi Gao.\nUnderstanding\nthe diffusion objective as a weighted integral of elbos.\narXiv:2303.00848, 2023. 5, 1\n[46] Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n2\n[47] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang,\nZhuowen Tu, and Ce Liu. Vitgan: Training gans with vi-\nsion transformers. arXiv preprint arXiv:2107.04589, 2021.\n2\n[48] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Com-\nmon diffusion noise schedules and sample steps are flawed.\narXiv preprint arXiv:2305.08891, 2023. 6\n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 4\n[50] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu,\nPing Luo, and Mingyu Ding. Vdt: General-purpose video\ndiffusion transformers via mask modeling. arXiv preprint\narXiv:2305.13311, 2023. 2\n[51] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las\nCasas, Yotam Doron, Albin Cassirer, and Karen Simonyan.\nTransformation-based adversarial video prediction on large-\nscale data. arXiv:2003.04035, 2020. 5\n[52] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2, 4\n[53] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv:2212.09748, 2022. 2, 5, 1\n[54] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with\na general conditioning layer. In Proceedings of the AAAI\nconference on artificial intelligence, 2018. 4\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gener-\native pre-training. 2018. 1, 2, 8\n[56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsuper-\nvised multitask learners. OpenAI blog, 1(8):9, 2019.\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nIn ICML, 2021. 1, 2, 8\n[58] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\nChristine McLeavey, and Ilya Sutskever. Robust speech recog-\nnition via large-scale weak supervision. In International Con-\nference on Machine Learning, pages 28492\u201328518. PMLR,\n2023. 1\n[59] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, 2021. 2\n[60] Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena,\nNoam Shazeer, Peter J Liu, Sharan Narang, Wei Li, and Yanqi\nZhou. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. 2019. 8\n[61] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 2, 5,\n6, 1\n[62] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 2\n[63] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 3\n[64] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training GANs. In NeurIPS, 2016. 5\n[65] Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord. Step-unrolled de-\nnoising autoencoders for text generation.\narXiv preprint\narXiv:2112.06749, 2021. 4\n[66] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv:2209.14792, 2022. 2, 8\n[67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 2, 3\n[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 2, 5\n[69] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. In NeurIPS,\n2019. 2, 3\n[70] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUCF101: A dataset of 101 human actions classes from videos\nin the wild. arXiv:1212.0402, 2012. 2, 5, 6\n[71] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv:1812.01717, 2018. 5\n[72] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. In NeurIPS, 2017. 2, 3\n[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 4\n[74] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kinder-\nmans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar,\nSantiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki:\nVariable length video generation from open domain textual\ndescription. arXiv:2210.02399, 2022. 2, 3, 5\n[75] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-\nAntoine Manzagol. Extracting and composing robust features\nwith denoising autoencoders. In Proceedings of the 25th\ninternational conference on Machine learning, pages 1096\u2013\n1103, 2008. 2\n[76] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei\nJi, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva:\nGenerating open-domain videos from natural descriptions.\narXiv preprint arXiv:2104.14806, 2021. 2\n[77] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In European con-\nference on computer vision, pages 720\u2013736. Springer, 2022.\n2\n[78] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srini-\nvas. VideoGPT: Video generation using VQ-VAE and trans-\nformers. arXiv:2104.10157, 2021. 2\n[79] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu. Vector-quantized image modeling with\nimproved VQGAN. In ICLR, 2022. 2\n[80] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\nYinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-\ngressive models for content-rich text-to-image generation.\narXiv:2206.10789, 2022. 2\n[81] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. MAGVIT: Masked\ngenerative video transformer. In CVPR, 2023. 2, 3, 5\n[82] Lijun Yu, Jos\u00b4e Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta,\nXiuye Gu, Alexander G Hauptmann, et al. Language model\nbeats diffusion\u2013tokenizer is key to visual generation. arXiv\npreprint arXiv:2310.05737, 2023. 3, 4, 5, 1\n[83] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video\nprobabilistic diffusion models in projected latent space. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18456\u201318466, 2023. 2\n[84] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\ncas Beyer. Scaling vision transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12104\u201312113, 2022. 5\n[85] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong\nChen, Fang Wen, Yong Wang, and Baining Guo. Styleswin:\nTransformer-based gan for high-resolution image generation.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 11304\u201311314, 2022. 2\n[86] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 3\n[87] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anand-\nkumar. Fast training of diffusion models with masked trans-\nformers. arXiv:2306.09305, 2023. 2, 5, 1\n[88] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 8\n[89] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao,\nFei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan\nWahid, et al. RT-2: Vision-language-action models transfer\nweb knowledge to robotic control. In CoRL, 2023. 1\nT2V (base)\nInput\n5 \u00d7 16 \u00d7 28\nSpatial window\n1 \u00d7 16 \u00d7 28\nSpatiotermporal window\n5 \u00d7 8 \u00d7 14\nTraining steps\n250000\nBatch size\n512\nlr schedule\nConstant\nOptimizer\nAdafactor\nlr\n0.00005\nTable 6. Hyperparameters for aspect-ratio finetuning.\nA. Implementation Details\nFor the first stage, we follow the architecture and hyperpa-\nrameters from Yu et al. [82]. We report hyperparameters\nspecific for training our model in Table 8. To train the sec-\nond stage transformer model, we use the default settings\nof 1 \u00d7 16 \u00d7 16 spatial window, 5 \u00d7 8 \u00d7 8 spatiotemporal\nwindow, psc = 0.9, c = 8 and r = 2. We summarize addi-\ntional training and inference hyperparameters for all tasks\nin Table 8. The UCF-101 model results reported in Tables 1\nand 4 are trained for 60, 000 steps. We perform all ablations\non UCF-101 with 35, 000 training steps.\nAspect-ratio finetuning. To simplify training and lever-\nage broad data sources with different aspect ratios, we train\nthe base stage using a square aspect ratio. We fine-tune the\nbase the stage on a subset of data to generate videos with\na 9 : 16 aspect ratio. We interpolate the absolute and rela-\ntive position embeddings and scale the window sizes. We\nsummarize the finetuning hyperparameters in Table 6.\nLong video generation. As described in \u00a7 4.4, we train\nour model jointly on the task of frame prediction. During\ninference, we generate videos as follows: Given a natural\nlanguage description of a video, we first generate the initial\n17 frames using our base model. Next, we encode the last 5\nframes into 2 latent frames using our causal 3D encoder. Pro-\nviding 2 latent frames as input for subsequent autoregressive\ngeneration helps ensure that our model can maintain conti-\nnuity of motion and produce temporally consistent videos.\nUCF-101 Text-to-Video. We follow the evaluation pro-\ntocol of prior work [24], and adapt their prompts to better\ndescribe the UCF-101 classes.\nB. Additional Results\nB.1. Image Generation\nWe compare (Table 7) W.A.L.T with state-of-the-art image\ngeneration methods for 256 \u00d7 256 resolution with classi-\nfier free guidance. Unlike, prior work [22, 53, 87] using\nTransformer for diffusion modelling, we did not observe\nsignificant benefit of using vanilla classifier free guidance.\nHence, we report results using the power cosine schedule\nproposed by Gao et al. [22]. Our model performs better\nthan prior works on the Inception Score metric, and achieves\nMethod\nCost (Iter\u00d7BS)\nFID\u2193\nIS\u2191\nParams.\nSteps\nLDM-4 [61]\n178k\u00d71200\n3.60\n247.7\n400M\n250\nDiT-XL/2 [53]\n7000k\u00d7256\n2.27\n278.2\n675M\n250\nADM [16]\n-\n3.94\n215.8\n608M\n2000\nMDT [22]\n6500k\u00d7256\n1.79\n283.0\n676M\n250\nMaskDiT [87]\n1200k\u00d71024\n2.28\n276.6\n736M\n40\nsimple diffusion [38]\n500K\u00d72048\n2.44\n256.3\n2B\n512\nVDM++ [45]\n-\n2.12\n267.7\n2B\n512\nW.A.L.T-L Ours\n437k\u00d71024\n2.40\n290.5\n460M\n50\nTable 7.\nClass-conditional image generation on ImageNet\n256\u00d7256. We adopt the evaluation protocol and implementation of\nADM [16] and report results with classifier free guidance.\nFigure 5. ImageNet class-conditional generation samples.\ncompetitive FID scores. Fig. 5 shows qualitative samples.\nB.2. Video Generation\nWe show samples for Kinetics-600 frame prediction in Fig. 6.\nB.3. Image-to-Video\nAs noted in Section 4.4, we train our model jointly on the task\nof frame prediction, where we condition on 1 latent frame.\nThis allows us to leverage the high quality first frame from\nthe image generator as context for predicting subsequent\nframes. For qualitative results see videos on our project\nwebsite.\nImageNet\nUCF-101\nK600\nT2V (base)\nT2V (2\u00d7)\nT2V (2\u00d72\u00d7)\nFirst Stage\nInput\n1 \u00d7 256 \u00d7 256 17 \u00d7 128 \u00d7 128 17 \u00d7 128 \u00d7 128\n17 \u00d7 128 \u00d7 128 17 \u00d7 256 \u00d7 448 17 \u00d7 512 \u00d7 896\nfs, ft\n8, -\n8, 4\n8, 4\n8, 4\n8, 4\n8, 4\nChannels\n128\n128\n128\n128\n128\n128\nChannel multiplier\n1,1,2,4\n1, 2, 2, 4\n1, 2, 2, 4\n1, 2, 2, 4\n1, 2, 2, 4\n1, 2, 2, 4\nTraining duration\n270 epochs\n2000 epochs\n270000 steps\n1000000 steps\n1000000 steps\n1000000 steps\nBatch size\n256\n256\n256\n256\n256\n256\nlr schedule\nCosine\nCosine\nCosine\nCosine\nCosine\nCosine\nOptimizer\nAdam\nAdam\nAdam\nAdam\nAdam\nAdam\nSecond Stage\nInput\n1 \u00d7 32 \u00d7 32\n5 \u00d7 16 \u00d7 16\n5 \u00d7 16 \u00d7 16\n5 \u00d7 16 \u00d7 16\n5 \u00d7 32 \u00d7 56\n5 \u00d7 64 \u00d7 112\nLayers\n24\n28\n24\n52\n40\n24\nHidden size\n1024\n1152\n1024\n9216\n1408\n1024\nHeads\n16\n16\n16\n16\n16\n16\nTraining duration\n350 epochs\n60000 steps\n360 epochs\n550000 steps\n675000 steps\n275000 steps\nBatch size\n1024\n256\n512\n512\n512\n512\nlr schedule\nCosine\nCosine\nCosine\nCosine\nCosine\nCosine\nOptimizer\nAdamW\nAdamW\nAdamW\nAdafactor\nAdafactor\nAdafactor\nlr\n0.0005\n0.0005\n0.0005\n0.0002\n0.0005\n0.0005\nEMA\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nPatch size\n1\n1\n1\n1\n2\n4\nAdaLN-LoRA\n\u2717\n2\n2\n2\n2\n2\nDiffusion\nDiffusion Steps\n1000\n1000\n1000\n1000\n1000\n1000\nNoise schedule\nLinear\nLinear\nLinear\nLinear\nLinear\nLinear\n\u03b20\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n\u03b21000\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\nSampler\nDDIM\nDDIM\nDDIM\nDDIM\nDDIM\nDDIM\nSampling steps\n50\n50\n50\n50\n50\n50\nGuidance\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\nTable 8. Training and evaluation hyperparameters.\nFigure 6. Frame prediction samples on Kinetics-600. Top: ground-truth, where unobserved frames are shaded. Bottom: generation.\n"
  },
  {
    "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
    "link": "https://arxiv.org/pdf/2312.06655.pdf",
    "upvote": "21",
    "text": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior\nFangfu Liu1, Diankun Wu1, Yi Wei1, Yongming Rao2, Yueqi Duan1\u2020\n1Tsinghua University, 2BAAI\nFigure 1. Gallery of Sherpa3D: Blender rendering for various textured meshes from Sherpa3D, which is able to generate high-fidelity,\ndiverse, and multi-view consistent 3D contents with input text prompts. Our method is also compatible with popular graphics engines.\nAbstract\nRecently, 3D content creation from text prompts has\ndemonstrated remarkable progress by utilizing 2D and\n3D diffusion models.\nWhile 3D diffusion models ensure\ngreat multi-view consistency, their ability to generate high-\nquality and diverse 3D assets is hindered by the limited\n3D data.\nIn contrast, 2D diffusion models find a dis-\ntillation approach that achieves excellent generalization\n\u2020Corresponding author.\nand rich details without any 3D data. However, 2D lift-\ning methods suffer from inherent view-agnostic ambiguity\nthereby leading to serious multi-face Janus issues, where\ntext prompts fail to provide sufficient guidance to learn co-\nherent 3D results. Instead of retraining a costly viewpoint-\naware model, we study how to fully exploit easily acces-\nsible coarse 3D knowledge to enhance the prompts and\nguide 2D lifting optimization for refinement. In this pa-\nper, we propose Sherpa3D, a new text-to-3D framework\nthat achieves high-fidelity, generalizability, and geometric\n1\narXiv:2312.06655v1  [cs.CV]  11 Dec 2023\nconsistency simultaneously. Specifically, we design a pair\nof guiding strategies derived from the coarse 3D prior gen-\nerated by the 3D diffusion model: a structural guidance for\ngeometric fidelity and a semantic guidance for 3D coher-\nence. Employing the two types of guidance, the 2D diffusion\nmodel enriches the 3D content with diversified and high-\nquality results. Extensive experiments show the superiority\nof our Sherpa3D over the state-of-the-art text-to-3D meth-\nods in terms of quality and 3D consistency. Project page:\nhttps://liuff19.github.io/Sherpa3D/.\n1. Introduction\n3D content generation [36, 39, 58, 81] finds a broad range of\napplications, including games, movies, virtual/augmented\nreality and robots. However, the conventional process of\ncreating premium 3D assets is still expensive and chal-\nlenging as it requires multiple labor-intensive and time-\nconsuming stages [33].\nFortunately, this challenge has\nprompted the development of recent text-to-3D meth-\nods [10, 26, 28, 37, 39, 49, 52, 58, 79]. Only using text\nprompts to automate 3D generation, these techniques pave\na promising way towards streamlining 3D creation.\nPowered by the great breakthroughs in diffusion mod-\nels [54, 62, 63, 86], two research lines of rationalization\nhave recently emerged in text-to-3D: inference-only 3D dif-\nfusion methods and optimization-based 2D lifting methods.\nSpecifically, the inference-only methods [20, 29, 54] seek to\ndirectly generate 3D-consistent assets by extensively train-\ning a new diffusion model on 3D data. However, due to the\nscarcity of 3D datasets compared to accessible 2D images or\ntext data, these 3D diffusion models suffer from low quality\nand limited generalizability. Without requiring any 3D data\nfor training, 2D lifting methods [4, 10, 39, 49, 58, 77, 79]\ncan produce high-quality and diversified 3D results by dis-\ntilling 3D knowledge from pre-trained 2D diffusion mod-\nels [62, 63, 86], also known as Score Distillation Sampling\n(SDS). Yet lifting 2D observations into 3D is inherently am-\nbiguous without sufficient 3D guidance from text prompts,\nleading to notorious multi-view inconsistency (e.g., Janus\nproblems) in 2D lifting methods.\nThese findings motivate us to think: is it possible to\nbridge the two aforementioned streams to achieve gen-\neralizability, high-fidelity, and geometric consistency si-\nmultaneously? An intuitive idea is to leverage more 3D\ndata [11, 12] to fine-tune a view-point aware diffusion\nmodel, but it requires substantial computational resources\nand is prone to overfitting due to data bias [42, 69]. In con-\ntrast, our key insight is to utilize the easily accessible 3D\ndiffusion model as guidance and study how to fully exploit\ncoarse 3D knowledge to guide 2D lifting optimization for\nrefinement. In particular, when maintaining the quality and\ngeneralizability of the original 2D diffusion model, we hope\nthe 2D lifting awareness can be guided by the strong 3D\ngeometric information from the 3D diffusion model. How-\never, it is non-trivial in pursuit of this balance. Relying too\nheavily on the coarse 3D priors from the 3D diffusion model\nmay degrade the generation quality, whereas little 3D guid-\nance could result in a lack of geometric awareness, leading\nto multi-view inconsistency.\nTowards this end, we propose Sherpa3D in this pa-\nper, which greatly boosts high-fidelity and highly diversi-\nfied text-to-3D generation with geometric consistency. Our\nmethod begins by employing a 3D diffusion model to craft\na basic 3D guide with limited details. Building upon the\ncoarse 3D prior, we introduce two guiding strategies to in-\nform 2D diffusion model throughout lifting optimization:\na structural guide for geometric fidelity and a semantic\nguide for 3D coherence. Specifically, the structural guide\nleverages the first-order gradient information of the normals\nfrom the 3D prior to supervise the optimization of the struc-\nture. These normals are then integrated into the input of a\npre-trained 2D diffusion model, refining the geometric de-\ntails. Concurrently, our semantic guide extracts high-level\nfeatures from multi-views of the 3D prior. These features\nguide the 2D lifting optimization to perceive the geomet-\nric consistency under the preservation of original general-\nizability and quality. Furthermore, we design an annealing\nfunction, which modulates the influence of the 3D guidance\nto better preserve the capabilities of 2D and 3D diffusion\nmodels. As a result, our Sherpa3D is aware of the geomet-\nric consistency with rich details and generalizes well across\ndiverse text prompts. Extensive experiments verify the ef-\nficacy of our framework and show that our Sherpa3D out-\nperforms existing methods for high-fidelity and geometric\nconsistency (see qualitative results gallery in Figure 1 and\nquantitative results in Table 2).\n2. Related Work\n2.1. Text-to-image Generation\nRecently, text-to-image models such as unCLIP [61], Ima-\ngen [63], and Stable Diffusion [62] have shown remarkable\ncapability of generating high-quality and creative images\ngiven text prompts. Such significant progress is powered by\nadvances in diffusion models [13, 25, 55, 72], which can be\npre-trained on billions of image-text pairs [64, 66] and un-\nderstands general objects with complex semantic concepts\n(nouns, artistic styles, etc.) [62]. Despite the great success\nof photorealistic and diversified image generation, using\nlanguage to generate different viewpoints of the same object\nwith 3D coherence remains a challenging problem [80].\n2.2. Text-to-3D Generation\nBuilding on promising text-to-image diffusion models,\nthere has been a surge of studies in text-to-3D generation.\n2\nHowever, it is non-trivial due to the scarcity of diverse 3D\ndata [8, 12, 82] compared to 2D. Existing 3D native dif-\nfusion models [20, 29, 45, 54, 85, 88] usually work on a\nlimited object category and struggle with generating in-the-\nwild 3D assets. To achieve generalizable 3D generation,\npioneering works DreamFusion [58] and SJC [77] propose\nto distill the score of image distribution from pre-trained\n2D diffusion models [62, 63] and show impressive results.\nFollowing works [10, 27, 38, 39, 49, 75, 76, 79, 84, 90] con-\ntinue to enhance various aspects such as generation fidelity\nand optimization stability or explore more application sce-\nnarios [60, 70, 91]. As it is inherently ambiguous to lift 2D\nobservations into 3D, they may suffer from multi-face is-\nsues. Although some methods use prompt engineering [4]\nor train a costly viewpoint-aware model [42, 69] to alle-\nviate such problems, they fail to generate high-quality re-\nsults [10] or easily overfit to domain-specific data [12, 69].\nIn this work, we bridge the gap between 3D and 2D dif-\nfusion models through meticulously designed 3D guidance,\nwhich leads the 2D lifting process to achieve high-fidelity,\ndiversified, and coherent 3D generation.\n2.3. 3D Generative Models\nExtensive research has been conducted in the field of 3D\ngenerative modeling, exploring diverse 3D representations\nlike 3D voxel grids [15, 22, 46], point clouds [3, 47, 51],\nand meshes [16, 87].\nThe majority of these approaches\nrely on training data presented in the form of 3D assets,\nwhich proves challenging to obtain at a large scale. Draw-\ning inspiration from the success of neural volume render-\ning, recent studies have shifted towards investing in 3D-\naware image synthesis\n[6, 7, 18, 21, 56, 65].\nThis ap-\nproach offers the advantage of directly learning 3D gen-\nerative models from images. However, volume rendering\nnetworks typically exhibit slow querying speeds, resulting\nin a trade-off between extended training times and a lack of\nmulti-view consistency. Recently, benefitted from 2D diffu-\nsion models, some works generate multi-view images with\nsingle-view input [41\u201344, 68, 83]. As one of the pioneer-\ning works, Zero-1-to-3 [42] uses a synthetic dataset to fine-\ntune the pretrained diffusion models, aiming to learn con-\ntrols of the relative camera viewpoint. Beyond Zero-1-to-3,\nSyncDreamer [43] employs a synchronized multiview dif-\nfusion model to capture the joint probability distribution of\nmultiview images. This model facilitates the generation of\nmultiview-consistent images through a unified reverse pro-\ncess. Different from these methods, we focus on text-to-3D\nsynthesis, with the goal of generating multi-view consistent\n3D contents with text prompts.\n3. Method\nGiven a text prompt, our goal is to generate 3D assets with\nhigh quality, generalizability, and multi-view consistency.\nOur framework can be divided into three stages: (1) build\ncoarse 3D prior from the 3D diffusion model (Sec. 3.2); (2)\nformulate two guiding strategies (e.g., structural and seman-\ntic guidance) for 2D lifting process (Sec. 3.3); (3) incor-\nporate both 3D guidance and SDS loss with an annealing\ntechnique in optimization and generate the final 3D object\n(Sec. 3.4). In this way, we can leverage the full power of\nstate-of-the-art 3D and 2D diffusion models to obtain 3D\ncoherence as 3D models, retaining intricate details and cre-\native freedom as 2D models. Our pipeline is depicted in Fig-\nure 2. Before introducing our Sherpa3D in detail, we first\nreview the theory of Score Distillation Sampling (SDS).\n3.1. Preliminaries\nScore Distillation Sampling (SDS). As one of the most\nrepresentative 2D lifting methods, Dreamfusion [58] first\npresents the concept of Score Distillation Sampling (SDS),\nwhich is an algorithm to optimize a 3D representation such\nthat the image rendered from any view maintains a high\nlikelihood as evaluated by the 2D diffusion model given\ntext prompts. SDS consists of two key components: (1)\na 3D representation with parameter \u03b8, which can produce\nan image x at desired camera c through a parametric func-\ntion x = g(\u03b8; c); (2) a pre-trained text-to-image 2D diffu-\nsion model \u03d5 with a score function \u03f5\u03d5(xt; y, t) that predicts\nthe sample noise \u03f5 given noisy image xt, noise level t and\ntext embedding y. The score function guides the direction\nof the gradient for updating \u03b8 to reside rendered images in\nhigh-density areas conditioned on text y. The gradient is\ncalculated by SDS as:\n\u2207\u03b8LSDS(\u03d5, x) = Et,\u03f5\n\u0014\nw(t) (\u03f5\u03d5 (xt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n, (1)\nwhere w(t) is a weighting function. In practice, the denois-\ning score function \u03f5\u03d5 is often replaced with another func-\ntion \u02dc\u03f5\u03d5 that uses classifier-free guidance [24] that controls\nthe strength of the text condition (see Supplementary).\n3.2. Sculpting a Coarse 3D Prior\nTo facilitate text-to-3D generation, most existing meth-\nods [27, 49, 58] rely on implicit 3D representations such as\nNeural Radiance Fields (NeRF) [50] and its variants [5, 53].\nHowever, it is difficult for NeRF-based modeling to extract\nthe high-quality surface with material and texture [78]. To\naddress this, we adopt the hybrid scene representation of\nDMTet [67], including a deformable tetrahedral grid that\nencodes a signed distance function (SDF) and a differen-\ntiable marching tetrahedra (MT) layer that extracts explicit\nsurface mesh. Equipped with the hybrid representation, we\nsculpt a coarse 3D prior from 3D diffusion model G3D\n(e.g., Shap-E [29]) by the following procedure. Given text\nprompts y, we first use the 3D diffusion model G3D to gen-\nerate 3D results M0 and employ multi-layer perceptions\n3\n13\nGeometry Modeling\n2D Diffusion \nModel\nnormal\n+\n\ufffd ~ \ufffd (\ufffd,  \ufffd)\nAppearance Modeling\nSemantic \nEncoder\nSemantic Guidance\nStructural Guidance\n...\nStructural \nDescriptor\n3D Diffusion \nModel\n pockmarked face\nmulti-face\nPrompt: \u201cA head of the Terracotta Army\u201d\nUpdate\nx\ny\nz\nCoarse 3D Prior\n\u201cSherpa\u201d\nFigure 2. Pipeline of our Sherpa3D. Given a text as input, we first prompt 3D diffusion to build a coarse 3D prior M encoded in the\ngeometry model (e.g., DMTet). Next, we render the normal map of the extracted mesh in DMTet and derive two guiding strategies from\nM. (a) Structural Guidance: we utilize the structural descriptor to compute salient geometric features for preserving geometry fidelity\n(e.g., without a pockmarked face problem). (b) Semantic Guidance: we leverage a semantic encoder (e.g., CLIP) to extract high-level\ninformation for keeping 3D consistency (e.g., without multi-face issues). Employing the two guidance in 2D lifting process, we use the\nnormal map as shape encoding of the 2D diffusion model and unleash its power to generate high-quality and diversified results with 3D\ncoherence. Then we achieve the final 3D results via photorealistic rendering through appearance modeling. (\u201cEverest\u2019s summit eludes\nmany without Sherpa.\u201d)\n(MLPs) to query SDF values for each vertex along a reg-\nular grid. Next we sample a point set P = {pi \u2208 R3} from\nM0 with their SDF values {SDF(pi)}. For each pi, the\nDMTet network F can predict SDF value s(pi), and a po-\nsition offset \u2206pi by:\n(s(pi), \u2206pi) = F(pi; \u03b8),\n(2)\nwhere \u03b8 is the parameters of network F. Then, we incor-\nporate 3D priors into the DMTet network F\u03b8 with the point\nset derived from 3D diffusion model by minimizing:\nLSDF =\nX\npi\u2208P\n|s(pi) \u2212 SDF(pi)|2 + \u03bbdef\nX\npi\u2208P\n||\u2206pi||2, (3)\nwhere \u03bbdef is the hyperparameter controlling L2 regulariza-\ntion strengths on offsets to avoid artifacts. Finally, we apply\nthe MT layer to extract mesh representation M. Now, we\nhave leveraged the knowledge from the 3D diffusion model\nto construct a coarse 3D prior, which is encoded implicitly\nin DMTet F\u03b8 and represented explicitly by mesh M. Next,\nwe will discuss how to utilize the coarse 3D prior M as a\nguide during the subsequent 2D diffusion lifting optimiza-\ntion to refine a high-quality result with 3D coherence.\n3.3. 3D Guidance for 2D Lifting Optimization\nWhat knowledge can serve as guidance? The purpose of\nintroducing a 3D prior as guidance is to address the preva-\nlent issue of viewpoint inconsistency both in geometry and\nappearance. Through empirical studies, we have identified\ngeometric inconsistency as the main cause of 3D incoher-\nence, leading to multi-face Janus problem [37, 69]. In con-\ntrast, appearance inconsistency emerges in much more ex-\ntreme scenarios with lesser significance. Therefore, we dis-\nentangle the geometry from the 3D model and fully leverage\ncoarse prior M to guide 2D lifting geometry optimization\nwith view-point awareness. Our analysis of the coarse 3D\nprior indicates that it contains the essential geometric struc-\ntures and captures the basic categorical attributes, keeping\nsemantic rationality across different views. Building upon\nthese observations, a natural insight is to preserve such in-\nherent 3D knowledge as guidance and continuously benefit\nthe 2D lifting process. For example, given text prompts \u201ca\nhead of the Terracotta Army,\u201d we hope the knowledge in the\nguidance can prevent issues such as a pockmarked face or\nthe unrealistic scenario of having a face on the back (e.g.,\nJanus problem). To this end, we have designed two guiding\nstrategies derived from M: structural guidance for geomet-\nric fidelity and semantic guidance for 3D coherence.\nStructural guidance. Given the current DMTet net F with\nparameters \u03b8 that encodes the coarse 3D prior M, we apply\na differentiable render fn (e.g., nvidiffrast [34]) to gener-\nate a set of normal maps N = {ni|ni = fn(F\u03b8, ci), i =\n1, ...n}, where ci is the camera position randomly sampled\nin spherical coordinates. To extract the salient geometric\nstructure features, we first use a Gaussian filter with a ker-\nnel standard deviation \u03c3\nG(x, y) =\n1\n2\u03c0\u03c32 e\u2212 x2+y2\n2\u03c32\n(4)\n4\nto reduce the noise impact and obtain {\u03c3(ni)}. As gradients\nare simple but effective tools for revealing the geometric\ncontours and salient structures [14, 30], we then compute\nthe structural descriptor sets {G\u03c3(ni)} by\nG\u03c3(ni) =\ns\n(\u2202\u03c3(ni)\n\u2202x\n)2 + (\u2202\u03c3(ni)\n\u2202y\n)2,\n(5)\nwhere x and y are the coordinate directions of the normal\nmap ni. Throughout the 2D lifting process of updating F\u03b8\nwith newly rendered normal maps \u02dc\nN = {\u02dcni}, it should\nfollow the structural guidance as:\nmin\n\u03b8\nLstruc :=\nn\nX\ni=1\n||G\u03c3(ni) \u2212 G\u03c3(\u02dcni)||2\n2,\n(6)\nwhich enables the 2D lifting process to preserve geometric\nfidelity and a well-aligned structure with the coarse 3D prior\nwhen generating rich details.\nSemantic guidance. While structural guidance maintains\nlow-level geometric perception from coarse 3D prior, se-\nmantic guidance extracts high-level features for 3D coher-\nence. We first apply the pre-trained CLIP [59] model as\nsemantic encoder \u03c8 to the normal set N and obtain seman-\ntic feature maps Nc = {\u03c8(ni)}, proven to effectively cap-\nture semantic attributes like facial expressions or view cate-\ngories [17]. Following the notation as above, we then define\nthe semantic guidance with cosine similarity:\nmin\n\u03b8\nLsem :=\nn\nX\ni=1\n\u03c8(ni) \u00b7 \u03c8(\u02dcni)\n\u2225\u03c8(ni)\u2225\u2225\u03c8(\u02dcni)\u2225.\n(7)\nEmploying this guidance, we ensure that different views re-\ntain inherent high-level information throughout the 2D lift-\ning optimization process. Experiments show that it can ef-\nfectively mitigate multi-face problems, keeping 3D content\nsemantically plausible from all viewing angles.\n3.4. Optimization\nIn this subsection, we incorporate both structural and se-\nmantic guidance derived from coarse 3D prior to 2D lift-\ning optimization so that it can produce vivid and diversified\nobjects with multi-view consistency. For the disentangled\ngeometry modeling, we use the randomly sampled normal\nmap n as the input, bridging the gap between 3D and 2D\ndiffusion. To update the geometry model DMTet network\nF\u03b8, we choose to use the publicly available Stable Diffu-\nsion [62] as pre-trained 2D diffusion model \u03d5 and compute\nthe gradient of the SDS loss similar in Eq. 14:\n\u2207\u03b8LSDS(\u03b8, n) = Et,\u03f5\n\u0014\nw(t) (\u03f5\u03d5 (zn\nt ; y, t) \u2212 \u03f5) \u2202zn\nt\n\u2202n\n\u2202n\n\u2202\u03b8\n\u0015\n,\n(8)\nwhere \u2202zn\nt /\u2202n calculates the gradient of the encoder in the\nlatent diffusion model (LDM) [62]. Additionally, we intro-\nduce a step annealing technique to balance the influence of\nthe 3D guidance during 2D lifting optimization:\n\u03b3(\u03bb) = \u03bbe\u2212\u03b2 max(0,ncur\u2212m),\n(9)\nwhere ncur is the current epoch and {\u03b2, m, \u03bb} are the hyper-\nparameters that control how \u03b3 decreased. Therefore, the to-\ntal loss Lgeo to lift 2D geometry optimization with 3D guid-\nance is a weighted sum of three loss terms:\nLgeo(\u03b8, n) = LSDS + \u03b3(\u03bbstruc)Lstruc + \u03b3(\u03bbsem)Lsem, (10)\nwhich not only enables the 3D content generation without\nmulti-view inconsistency issues but also preserves the gen-\neralization and quality in 2D diffusion model \u03d5. As our\npipeline can be integrated into any appearance model [9,\n10, 35], we adopt a similar approach as Fantasia3D [10] to\nbetter align our text and 3D object. Denote T with parame-\nters \u03b7 as our appearance model, we have the rendered image\nx = T\u03b7(F\u03b8, ci). To update \u03b7, we again apply the SDS loss\nfor the final complete generated 3D object with detailed tex-\nture and coherent geometry:\n\u2207\u03b7Lapp(\u03b7, x) = Et,\u03f5\n\u0014\nw(t) (\u03f5\u03d5 (zx\nt ; y, t) \u2212 \u03f5) \u2202zx\nt\n\u2202x\n\u2202x\n\u2202\u03b7\n\u0015\n, (11)\nwhich shares similar notations defined in Eq. 8. Finally,\nthrough the tailored 3D structural and semantic guidance\nthat bridges the 2D and 3D diffusion models, our Sherpa3D\ncan mitigate the multi-face problem and achieve high-\nfidelity and diversified results.\n3.5. Implementation Details\nWe apply the multilayer perceptron (MLP) comprising of\nthree hidden layers to approximate F\u03b8 and T\u03b7. Adam opti-\nmizer [32] is used to update F\u03b8 and T\u03b7 with an initial learn-\ning rates of 1e \u2212 3 decaying into 5e \u2212 4. For 3D representa-\ntions, we use textured mesh with a DMTet resolution of 128\nto achieve a balance between quality and generation speed.\nWe sample random camera poses at a fixed radius of 2.5,\ny-axis FOV of 45\u25e6, with the azimuth in [\u2212180\u25e6, 180\u25e6] and\nelevation in [\u221230\u25e6, 30\u25e6]. We load Shap-E from [48] for 3D\ndiffusion model and choose stabilityai/stable-diffsuion-2-1-\nbase [62] for 2D diffusion model. For weighting factors, we\nfollow the same strategy as [27] to tune w(t). \u03bbstruc is set\nto 10 and \u03bbsem is 30 to balance the magnitude of SDS loss.\nNotably, our method only needs a single NVIDIA RTX3090\n(24GB) GPU within 25 minutes. More details of optimiza-\ntion, architecture design, and hyperparameter settings can\nbe found in the supplementary.\n4. Experiments\nIn this section, we conduct comprehensive experiments\nto evaluate our text-to-3D framework Sherpa3D and show\n5\nSherpa3D\n~25min\nFantasia3D\n~ 45min\nProlificDreamer\n~ 3h\nDreamFusion\n~ 1h\nShap-E\n~10s\nMagic3D\n~ 40min\n\u201cA head of the Terracotta Army\u201d\n180\u00b0\n0\u00b0\n\u201cA carved wooden Bodhisattva from China\u2019s Song dynasty\u201d\n0\u00b0\n180\u00b0\nFigure 3. Qualitative comparisons with baseline methods across different views (0\u25e6 and 180\u25e6). We can observe that baseline methods\nsuffer from severe multi-face issues while our Sherpa3D can achieve better quality and 3D coherence.\ncomparison results against other text-to-3D baseline meth-\nods. We first present qualitative results compared with five\nSOTA baselines from different viewpoints. Then we re-\nport the quantitative results with a user study. Finally, we\ncarry out ablation studies to further verify the efficacy of\nour framework design. Please refer to the supplementary\nfor more comparisons, visualizations, and detailed analysis.\n4.1. Experiment Setup\nBaselines. We extensively compare our method Sherpa3D\nagainst five baselines: Shap-E [29], DreamFusion [58],\nMagic3D [39], ProlificDreamer [79], and Fantasis3D [10].\nDue to various reasons, we can\u2019t obtain the original imple-\nmentation of some baselines. For DreamFusion, Magic3D,\nand ProlificDreamer, we utilize their implementations in the\nThreestudio library [19] for comparison. For Shap-E and\nFantasia3D, we follow their official implementation. We\nconsider these implementations to be the most reliable and\ncomprehensive open-source option available in the field. To\nensure a fair comparison, we use the Stable Diffusion [62]\nmodel as 2D diffusion prior by default.\nMetrics. We will show our results with notable compar-\nisons to other baselines through visualization. As there is no\nGround-Truth 3D content corresponding to the text prompt,\nreference-based metrics like Chamfer Distance are diffi-\ncult to apply to zero-shot text-to-3D generation. Follow-\ning [28, 58], we evaluate the CLIP R-Precision [57], which\ncan measure how well the rendered images of generated 3D\ncontent align with the input text. We use 100 prompts from\nthe Common Objects in Context (COCO) dataset [40] as\nDreamFusion [58]. we also conduct a user study to further\ndemonstrate the multi-view consistency and overall genera-\ntion quality of our method,\n4.2. Qualitative Comparisons\nWe first demonstrate vivid and diversified text-to-3D results\ngenerated from our Sherpa3D in the gallery as shown in\nFigure 1. Then we compare our method with five baseline\nmethod: Shap-E [29], DreamFusion [58], Magic3D [39],\nFantasia3D [10] and ProlificDreamer [79]. Figure 3 and 4\n6\n\u201cA carved wooden Bodhisattva from China\u2019s Song dynasty\u201d\n180\u00b0\nSherpa3D\n~25min\nFantasia3D\n~ 45min\nProlificDreamer\n~ 3h\nDreamFusion\n~ 1h\nShap-E\n~10s\nMagic3D\n~ 40min\n\u201cA DSLR photo of an adorable Corgi dog with a wagging tail\u201d\n150\u00b0\n-30\u00b0\n\u201c A DSLR photo of an astronaut in a space suit\u201d\n150\u00b0\n-30\u00b0\nFigure 4. Qualitative comparisons with baseline methods across different views (\u221230\u25e6 and 150\u25e6).\ngive the comparative results with the same text prompt for\neach object generation. We observe that the Shap-E [29]\nonly generates coarse shapes while other 2D lifting methods\nsuffer from multi-face problems. In contrast, our Sherpa3D\nproduces high-fidelity 3D assets with compelling texture\nquality and multi-view consistency. Notably, our frame-\nwork is more efficient than other baselines with less time\nto optimize. Specifically, it only takes within 25 minutes\nfrom a text prompt to a high-quality 3D model ready to be\nused in graphic engines.\n4.3. Quantitative Comparisons\nIn Table 1, we report the CLIP R-Precision for Sherpa3D\nand several baselines. It shows that our method outperforms\nother baselines consistently across different CLIP models,\nand approaches the performance of ground truth (GT) im-\nages.\nFor the user study, we render 360-degree rotating\nvideos of 3D models generated from a collection of 120 im-\nages. Each volunteer is shown 10 samples of rendered video\nfrom a random method and rates in two aspects: multi-view\nconsistency and overall generation quality. We collect re-\nsults from 50 volunteers shown in Table 2. We observe\nTable 1.\nQuantitative comparisons on generation renderings\nwith text prompts using different CLIP retrieval models. We com-\npared to ground-truth images, Shap-E [29], Dreamfusion [58],\nMagic3D [39], evaluated on object-centric COCO as in [58].\nMethod\nR-Precision (%) \u2191\nCLIP B/32\nCLIP B/16\nCLIP L/14\nGT Images\n77.3\n79.2\n-\nShape-E [29]\n41.1\n42.5\n46.4\nDreamFusion [58]\n70.3\n73.2\n75.0\nMagic3D [39]\n71.5\n73.8\n76.1\nSherpa3D (Ours)\n72.3\n75.6\n79.3\nthat most users consider our results with much higher view-\npoints consistency and overall generation fidelity.\n4.4. Ablation Study and Analysis\nWe carry out ablation studies on the design of our Sherpa3D\nframework in Figure 5 using an example text prompt \u201ca\nhead of the Terracotta Army\u201d. Specifically, we perform ab-\nlation on three aspects of our method: structural guidance,\nsemantic guidance, and the step annealing strategy. The re-\n7\n0\u00b0\n180\u00b0\n\u201cA head of the Terracotta Army\u201d\nw/o Structural Guidance \nw/o Semantic Guidance \nw/o step annealing\nFull\nappearance\ngeometry\nappearance\ngeometry\nappearance\ngeometry\nappearance\ngeometry\nFigure 5. Ablation study of our method. The generation is based on the text prompt \u201ca head of the Terracotta Army\u201d. We ablate the\ndesign choices of structural guidance, semantic guidance (Sec. 3.3), and the step annealing technique (Sec. 3.4).\nTable 2. Quantitative comparisons on the multi-view consis-\ntency and overall generation quality score in a user study, rated on\na scale of 1-10, with higher scores indicating better performance.\nMethod\nMulti-view Consistency \u2191\nOverall Quality \u2191\nShap-E [29]\n6.09\n3.33\nDreamFusion [58]\n4.58\n6.05\nMagic3D [39]\n5.25\n6.73\nFantasia3D [10]\n3.83\n5.90\nProlificDreamer [79]\n5.78\n7.02\nSherpa3D(Ours)\n8.95\n8.74\nsults reveal that the omission of any of these elements leads\nto a degradation in terms of quality and consistency. No-\ntably, the absence of structural guidance leads to a loss of\ngeometric fidelity in the \u201carmy\u201d, leading to a pockmarked\nface; without semantic guidance, there\u2019s a loss of seman-\ntic rationality across different views, resulting in the multi-\nview Janus problem. The lack of a balanced step annealing\nresults in an excessive influence of guidance with a rough\nfinal output. This illustrates the effectiveness of our over-\nall framework (Figure 2), which drives geometric fidelity,\nmulti-view consistency, and optimization balance steered\nby the 3D guidance and annealing strategy.\nTo further demonstrate our generalizability, we com-\npare our method in Figure 6 with the Zero123 [42] which\nuses more 3D data [42] to finetune a 2D diffusion model\nto be viewpoint-aware. However, such a finetuning-based\nmethod easily overfits to 3D training data and suffers from\nsevere performance degradation with unseen input of the\ntraining set. In contrast, our method is more generalizable\nto open-vocabulary text prompts.\n5. Conclusion\nIn this paper, we present Sherpa3D, a new framework\nthat simultaneously achieves high-quality, diversified, and\n3D consistent text-to-3D generation. By fully exploiting\n\u201cA deer draped in exquisite ancient Chinese fabric\uff0c\noil painting style\u201d\n\u201cA head of the Terracotta \nArmy\u201d\nSherpa3D (Ours)              Zero123\nSherpa3D (Ours)              Zero123\nFigure 6. Comparison with Zero123 [42]. We use the front view\nof our generated 3D model as the input of Zero123 with open-\nvocabulary text prompts.\neasily obtained coarse 3D knowledge from the 3D diffu-\nsion model, we derive structural guidance and semantic\nguidance to enhance the prompts and provide continu-\nous guidance with geometric fidelity and 3D coherence\nthroughout the 2D lifting optimization. To further improve\nthe overall performance, we incorporate a step annealing\nstrategy that modulates the impact of 3D guidance and\n2D refinement.\nTherefore, our framework bridges the\ngap between 2D and 3D diffusion models, preserving\nmulti-view coherent generation while maintaining the\ngeneralizability and fidelity of 2D models. Extensive qual-\nitative and quantitative experiments verify the remarkable\nimprovement of our Sherpa3D on text-to-3D generation.\nLimitations and future works. Although our Sherpa3D\nachieves remarkable text-to-3D results, the quality still\nseems to be limited to the backbone itself as we choose\nShap-E [29] and Stable Diffusion v2.1 base model in this\nwork. We expect them to be solved with a larger diffusion\nmodel, such as SDXL [1] and DeepFloyd [2]. In future\nwork, we are interested in extending our insight to more\ncreative text-to-4D generation. We believe that Sherpa3D\nprovides a promising research path for user-friendly and\nmore accessible 3D content creation.\n8\nReferences\n[1] stable-diffusion-xl-base-1.0. https://huggingface.\nco / stabilityai / stable - diffusion - xl -\nbase-1.0. Accessed: 2023-08-29. 8\n[2] Deepfloyd. https://huggingface.co/DeepFloyd.\nAccessed: 2023-08-25. 8\n[3] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In ICML, pages 40\u201349. PMLR,\n2018. 3\n[4] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 2, 3\n[5] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 3\n[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\nCVPR, pages 5799\u20135809, 2021. 3\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al.\nEffi-\ncient geometry-aware 3d generative adversarial networks. In\nCVPR, pages 16123\u201316133, 2022. 3\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 3\n[9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner.\nText2tex:\nText-driven\ntexture synthesis via diffusion models.\narXiv preprint\narXiv:2303.11396, 2023. 5, 13\n[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 3, 5, 6, 8, 13, 14\n[11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 2\n[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 2, 3, 14\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[14] Lijun Ding and Ardeshir Goshtasby. On the canny edge de-\ntector. Pattern recognition, 34(3):721\u2013725, 2001. 5\n[15] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2d views of multiple objects. In 3DV, pages\n402\u2013411. IEEE, 2017. 3\n[16] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images.\nNeurlPS, 35:31841\u2013\n31854, 2022. 3\n[17] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter,\nMichael Petrov, Ludwig Schubert, Alec Radford, and Chris\nOlah. Multimodal neurons in artificial neural networks. Dis-\ntill, 6(3):e30, 2021. 5\n[18] Jiatao\nGu,\nLingjie\nLiu,\nPeng\nWang,\nand\nChristian\nTheobalt.\nStylenerf:\nA style-based 3d-aware genera-\ntor for high-resolution image synthesis.\narXiv preprint\narXiv:2110.08985, 2021. 3\n[19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 6\n[20] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration, 2023. 2, 3\n[21] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu.\nGancraft: Unsupervised 3d neural rendering of minecraft\nworlds. In ICCV, pages 14072\u201314082, 2021. 3\n[22] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-\ning plato\u2019s cave: 3d shape from adversarial rendering. In\nICCV, pages 9984\u20139993, 2019. 3\n[23] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 13\n[24] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 13\n[26] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner.\nText2room:\nExtracting textured\n3d meshes from 2d text-to-image models.\narXiv preprint\narXiv:2303.11989, 2023. 2\n[27] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 3, 5, 14\n[28] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2, 6\n[29] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2, 3, 6, 7, 8, 14\n9\n[30] Nick Kanopoulos, Nagesh Vasanthavada, and Robert L\nBaker. Design of an image edge detection filter using the\nsobel operator. IEEE Journal of solid-state circuits, 23(2):\n358\u2013367, 1988. 5\n[31] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 34:21696\u201321707, 2021. 13\n[32] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5, 13\n[33] Matthias Labsch\u00a8utz, Katharina Kr\u00a8osl, Mariebeth Aquino,\nFlorian Grash\u00a8aftl, and Stephanie Kohl. Content creation for\na 3d game with maya and unity 3d. Institute of Computer\nGraphics and Algorithms, Vienna University of Technology,\n6:124, 2011. 2\n[34] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. ACM Transac-\ntions on Graphics (TOG), 39(6):1\u201314, 2020. 4\n[35] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. Advances in Neural Information Processing Sys-\ntems, 35:30923\u201330936, 2022. 5, 13\n[36] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang\nLee, Francois Rameau, Yang Yang, Sung-Ho Bae, and\nChoong Seon Hong. Generative ai meets 3d: A survey on\ntext-to-3d in aigc era.\narXiv preprint arXiv:2305.06131,\n2023. 2\n[37] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n2, 4\n[38] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi\nZhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-\ndriven 3d editing via focal-fusion assembly. arXiv preprint\narXiv:2308.10608, 2023. 3\n[39] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 300\u2013309, 2023. 2, 3, 6, 7, 8, 14\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6\n[41] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 3\n[42] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 2, 3, 8, 14\n[43] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 3\n[44] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 3\n[45] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin,\nMing-Yu Liu, Sanja Fidler, and James Lucas.\nAtt3d:\nAmortized text-to-3d object synthesis.\narXiv preprint\narXiv:2306.07349, 2023. 3\n[46] Sebastian Lunz, Yingzhen Li, Andrew Fitzgibbon, and\nNate Kushman. Inverse graphics gan: Learning to gener-\nate 3d shapes from unstructured 2d data.\narXiv preprint\narXiv:2002.12674, 2020. 3\n[47] Shitong Luo and Wei Hu.\nDiffusion probabilistic models\nfor 3d point cloud generation. In CVPR, pages 2837\u20132845,\n2021. 3\n[48] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 5\n[49] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2, 3\n[50] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), pages 405\u2013421. Springer, 2020. 3\n[51] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,\nNiloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchi-\ncal graph networks for 3d shape generation. arXiv preprint\narXiv:1908.00575, 2019. 3\n[52] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2\n[53] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 3\n[54] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts, 2022. 2, 3\n[55] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[56] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geometry\ngeneration. In CVPR, pages 13503\u201313513, 2022. 3\n10\n[57] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,\nand Anna Rohrbach. Benchmark for compositional text-to-\nimage synthesis. In Thirty-fifth Conference on Neural Infor-\nmation Processing Systems Datasets and Benchmarks Track\n(Round 1), 2021. 6\n[58] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The\nEleventh International Conference on Learning Representa-\ntions (ICLR), 2023. 2, 3, 6, 7, 8, 13, 14\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 5\n[60] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 3\n[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2, 13\n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2, 3, 5, 6, 14\n[63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding, 2022.\n2, 3, 13\n[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 2\n[65] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis\nwith sparse voxel grids. NeurlPS, 35:33999\u201334011, 2022. 3\n[66] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 2\n[67] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 3\n[68] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n3\n[69] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2, 3, 4, 14\n[70] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv preprint arXiv:2301.11280,\n2023. 3\n[71] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n13\n[72] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2\n[73] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 13\n[74] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 13\n[75] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3\n[76] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 3\n[77] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation, 2022. 2, 3,\n13\n[78] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 3\n[79] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 6, 8,\n14\n[80] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 2\n[81] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. Ad-\nvances in neural information processing systems, 29, 2016.\n2\n[82] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\n11\net al. Omniobject3d: Large-vocabulary 3d object dataset for\nrealistic perception, reconstruction and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 803\u2013814, 2023. 3\n[83] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-\ndong Li. Consistnet: Enforcing 3d consistency for multi-\nview images diffusion.\narXiv preprint arXiv:2310.10343,\n2023. 3\n[84] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin\nWang, and Fan Wang. Points-to-3d: Bridging the gap be-\ntween sparse points and shape-controllable text-to-3d gener-\nation. In Proceedings of the 31st ACM International Confer-\nence on Multimedia, pages 6841\u20136850, 2023. 3\n[85] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models.\narXiv preprint\narXiv:2301.11445, 2023. 3\n[86] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2\n[87] Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao,\nYinan Zhang, Antonio Torralba, and Sanja Fidler.\nIm-\nage gans meet differentiable rendering for inverse graph-\nics and interpretable 3d neural rendering.\narXiv preprint\narXiv:2010.09125, 2020. 3\n[88] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum. Locally attentional sdf\ndiffusion for controllable 3d shape generation. arXiv preprint\narXiv:2305.04461, 2023. 3\n[89] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d:\nA modern library for 3d data processing.\narXiv preprint\narXiv:1801.09847, 2018. 13\n[90] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 3\n[91] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. arXiv preprint arXiv:2306.13455, 2023. 3\n12\nSherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior\nSupplementary Material\n6. More Discussion of Preliminaries\nIn this section, we provide more preliminaries and details of\nour implementation for Score Distillation Sampling (SDS).\n6.1. Diffusion Models\nThe diffusion model, which is a type of likelihood-based\ngenerative model used to learn data distributions, has been\nstudied extensively in recent years [25, 71\u201374].\nGiven\nan underlying data distribution q0(x), a diffusion model\ncomposes two processes: (a) a forward process {qt}t\u2208[0,1]\nto gradually add noise to the data point x0 \u223c q0(x0);\n(b) a reverse process {pt}t\u2208[0,1] to denoise data (e.g.,\ngeneration).\nSpecifically, the forward process is de-\nfined by qt (xt | x0) := N\n\u0000\u03b1tx0, \u03c32\nt I\n\u0001\nand qt (xt) :=\nR\nqt (xt | x0) q0 (x0) dx0, where \u03b1t, \u03c3t > 0 are hyper-\nparameters.\nOn the other hand, the reverse process is\ndescribed with the transition kernel pt(xt\u22121 | xt) :=\nN(\u00b5\u03d5(xt, t), \u03c32\nt I) from p1(x1) := N(0, I). The train-\ning objective is to optimize \u00b5\u03d5 by maximizing a variational\nlower bound of a log-likelihood.\nIn practice, \u00b5\u03d5 is re-\nparameterized as a denoising network \u03f5\u03d5(xt, t) [25] to pre-\ndict the noise added to the clean data x0, which is trained\nby minimizing the MSE criterion [25, 31]:\nLDiff (\u03d5) := Ex0,t,\u03f5\n\u0002\n\u03c9(t) \u2225\u03f5\u03d5 (\u03b1tx0 + \u03c3t\u03f5) \u2212 \u03f5\u22252\n2\n\u0003\n,\n(12)\nwhere \u03c9(t) is the time-dependent weights.\nBesides, the\nnoise prediction network \u03f5\u03d5 can be applied for approximat-\ning the score function [73] of the perturbed data distribution\nq(xt), which is defined as the gradient of the log-density:\n\u2207xt log qt (xt) \u2248 \u2212\u03f5\u03d5 (xt, t) /\u03c3t.\n(13)\nThis means that the diffusion model can estimate a di-\nrection that guides xt towards a high-density region of\nq(xt), which is the key idea Score Distillation Sampling\n(SDS) [58, 77] for optimizing the 3D scene via well 2D\npre-trained models.\n6.2. SDS with Classifier-Free Guidance\nAs one of the most successful applications of diffusion\nmodels, text-to-image generation [61\u201363] generate samples\nx based on the text prompt y, which is also fed into the \u03f5\u03d5\nas input, denoted as \u03f5\u03d5 (xt; t, y). An important technique\nto improve the performance of these models is Classifier-\nFree Guidance (CFG) [23].\nCFG modifies the original\nmodel by adding a guidance term, i.e., \u02c6\u03f5\u03d5(xt; y, t) :=\n(1+s)\u03f5\u03d5(xt; y, t)\u2212s\u03f5\u03d5(xt; t, \u2205), where s > 0 is the guid-\nance weight that controls the balance between fidelity and\ndiversity, while \u2205 denotes the \u201cempty\u201d text prompt for the\nunconditional case. Recall the SDS gradient form to update\n\u03b8:\n\u2207\u03b8LSDS(\u03d5, x) = Et,\u03f5\n\u0014\n\u03c9(t) (\u03f5\u03d5 (xt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n,\n(14)\nand denote \u03b4x(xt; y, t) := \u03f5\u03d5(xt; y, t) \u2212 \u03f5. In principle,\n\u03f5(xt; y, t) should represent the pure text-conditioned score\nfunction in Eq. (14). But in practice, CFG is employed in\nit with a guidance weight s to achieve high-quality results,\nwhere we rewrite\n\u03b4x(xt; y, t) = [\u03f5\u03d5(xt; y, t) \u2212 \u03f5] + s[\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t, \u2205)].\n(15)\nAs DreamFusion [58] uses s = 100 for high fidelity, our\nimplementation adopts s = 50 with the enhancement of\nstructural and semantic guidance to preserve some diver-\nsity. The two types of guidance can also be seen as another\nform of prompt guidance that is more generalizable and ro-\nbust. Therefore, there is a gap between the original formu-\nlation in Eq. (14) and the practical coding implementation\nin Eq. (15).\n7. Additional Implementation Details\nTraining details. Our geometry model F\u03b8 and appearance\nmodel T\u03b7 is approximated by three-layer MLPs and we ap-\nply adam [32] optimizer to update them with an initial learn-\ning rates of 1\u00d710\u22123 to decaying to 5\u00d710\u22124. In particular,\nour method is optimized for 2500 iterations about 15 min-\nutes to learn F\u03b8 and 2500 iterations about 10 minutes to\nlearn T\u03b7. For geometry modeling, we utilize the Open3D\nlibrary [89] to calculate the signed distance function (SDF)\nvalue for each point in Equations 2 and 3 in the main pa-\nper. In our experiments, the DMTet-based coarse 3D prior\nbuilding stage is critical as it not only provides coarse 3D\nknowledge with consistency but also boosts the speed of\nthe convergence of generation. For appearance modeling,\nsince our focus in this paper is to fully exploit easily ob-\ntained coarse 3D knowledge that serves as guidance for 2D\nlifting optimization (as discussed in Section 3.3 of our pa-\nper), we do not design a specific appearance model for our\nframework. Note that our geometry model is plug and play\nand we can leverage different models [9, 10, 35], we lever-\nage the same PBR materials approach in Fantasia3D [10] to\nachieve photorealistic surface renderings and better aligns\nwith our geometry modeling.\nHyperparameter settings. We select the camera positions\n(r, \u03ba, \u03c6) in the spherical coordinate system, where r denote\n13\nradius, \u03ba is the elevation and \u03c6 is the azimuth angle respec-\ntively. Specifically, we sample random camera poses at a\nfixed r = 2.5 with the \u03ba \u2208 [\u221230\u25e6, 30\u25e6]. In a batch of b \u00d7 l\nimages, we partition \u03c6 into l intervals in [\u2212180\u25e6, 180\u25e6] and\nuniformly sample b azimuth angles in each interval. For\nstructural guidance, we set \u03c3 = 1 in Eq. (4) in the main\npaper as the standard deviation of the Gaussian filter. We\ntune \u03bbstruc and \u03bbsem in {0.01, 0.1, 1, 5, 10, 20, 30, 100}. We\nfind that often \u03bbstruc = 10 and \u03bbsem = 30 works well with\n\u03b2 = 0.5 in the step annealing technique, which may balance\nthe magnitude of SDS losses and better guide the 2D lifting\nto refine the 3D contents with multi-view coherence. We\nassigned the value of m to the epoch at around 1000 itera-\ntions. For the guidance weight \u03c9(t), we follow the Dream-\nTime [27] to achieve higher fidelity results. Our codes for\nimplementation will be available upon acceptance.\n8. Additional Experiments and Analysis\n8.1. Additional User Study\nTo further demonstrate the effectiveness and impressive vi-\nsualization results of our Sherpa3D, we conducted a more\nintuitive user study (Figure 7) on 20 text prompts of five\nbaselines (ShapE [29], DreamFusion [58], Magic3D [39],\nProlificDreamer [79], Fantasia3D [10]) and ours. The study\nengaged 50 volunteers to assess the generated results in 20\nrounds. In each round, they were asked to select the 3D\nmodel they preferred the most, based on quality, creativity,\nalignment with text prompts, and consistency. We also com-\npare our method with recent finetuning-based techniques,\nsuch as Zero123 [42] and MVDream [69], which utilize\nmore 3D data [12] to retrain a costly 3D aware diffusion\nmodel from Stable Diffusion [62]. We use the same text\nprompts and settings as mentioned above. As shown, we\nFigure 7. User study of the rate from volunteers\u2019 preference for\neach method in the inset pie chart.\nobserve that Sherpa3D is preferable (65%) by the raters on\naverage. In other words, our model is preferred over the best\nof all baselines in most cases. What\u2019s more, our Sherpa3D\nalso outperforms than fine-tuning based method in terms of\noverall performance as they easily suffer from styles (light-\nning, texture) overfitting [42, 69]. We believe this is strong\nproof of the robustness and quality of our proposed method.\n8.2. More Qualitative Results\nSherpa3D. In Figure 10, 11, 12, we present more text-to-3D\nresults obtained with Sherpa3D, which can generate high-\nfidelity, diverse, and 3D-consistent results within 25 min-\nutes. Besides the impressive 3D consistency and high fi-\ndelity, we can also change the style of generated 3D content\n(Figure 8) by only modifying a small part of the prompt,\nwhile preserving the basic structure of 3D content, which is\nmore convenient for users to flexibly edit generated objects.\nA detailed and realistic 3D model of a \nvintage camera\nA detailed and realistic 3D model of a vintage \ncamera, leather texture\nA futuristic battle robot, amidst a post-\napocalyptic urban wasteland\nA futuristic battle robot, toy style\nFigure 8. Sherpa3D can be used for flexible editing through a\nsmall part of the prompt modification.\nMore comparison results. We provide more comparisons\nwith baselines in Figure 13, 14. To further demonstrate the\nrobustness and generalization of our method, we compare\nour Sherpa3D with Zero123 [42] and MVDream [69] in\nFigure 9. Although the concurrent work MVDream and\nZero123 can also resolve the multi-view inconsistency is-\nsues via fine-tuning a costly viewpoints-aware model, we\nobserve that it is prone to overfit the limited 3D data [12].\nSpecifically, MVDream generates strange color styles while\nZero123 fails in such open-vocabulary prompts.\n\u201cA head of the Terracotta Army\u201d\n\u201cHyper-realistic image of a snow leopard in a winter landscape\u201d\nSherpa3D (Ours)\nMVDream\nZero123\nFigure 9. Comparison with MVDream [69] and Zero123 [42].\n14\n\u201cA futuristic battle robot, heavily armed, amidst a post-\napocalyptic urban wasteland\u201d\n\u201cA luxurious sky-blue leather handbag with a sleek and \nelegant design, highlighted by its vibrant blue color\u201d\n\u201cA cybernetic biomechanical arm, with a blend of organic \nand mechanical elements\u201d\n\u201cA statue of a angel\u201d\n\u201cCommercial airliner in flight, sleek and modern design\u201d\n\u201cIron Man in his state-of-the-art suit, confidently standing, \nlooking ahead, ready for action\u201d\nFigure 10. More generated results using our Sherpa3D within 25 minutes. Our work can generate high-fidelity and diversified 3D results\nfrom various text prompts, free from the multi-view inconsistency problem.\n15\n\u201cHyper-realistic image of a snow leopard, capturing its \ncamouflage and majestic stance\u201d\n\u201cDetailed portrait of a noble knight, full armor, intricate \nhelmet design\u201d\n\u201cAn ultra-detailed illustration of a mythical Phoenix, rising \nfrom ashes, vibrant feathers in a fiery palette\u201d\n\u201cA detailed and realistic 3D model of a vintage camera\u201d\n\u201cSpaceship,futuristic design,sleek metal,glowing thrusters, \nflying in space\u201d\n\u201cA DLSR Photo of the Leaning Tower of Pisa\u201d\nFigure 11. More generated results using our Sherpa3D within 25 minutes. Our work can generate high-fidelity and diversified 3D results\nfrom various text prompts, free from the multi-view inconsistency problem.\n16\n\u201cA blooming red rose, with velvety petals, delicate green \nleaves, and a captivating fragrance that fills the air\u201d\n\u201cA head of the Terracotta Army\u201d\n\u201cVintage wooden race car, polished mahogany finish, classic \ndesign with spoked wheels\u201d\n\u201cA futuristic-style motorcycle with sleek design, neon lights, \nand a sci-fi aesthetic in an urban setting\u201d\n\u201cA carved wooden Bodhisattva from China\u2019s Song dynasty\u201d\n\u201cA DSLR photo of an adorable Corgi dog with a wagging tail\u201d\nFigure 12. More generated results using our Sherpa3D within 25 minutes. Our work can generate high-fidelity and diversified 3D results\nfrom various text prompts, free from the multi-view inconsistency problem.\n17\nSherpa3D\n~25min\nFantasia3D\n~ 45min\nProlificDreamer\n~ 3h\nDreamFusion\n~ 1h\nShap-E\n~10s\nMagic3D\n~ 40min\n\u201cCommercial airliner in flight, sleek and modern design\u201d\n\u201cIron Man in his state-of-the-art suit, confidently standing, looking \nahead, ready for action\u201d\n\u201cA statue of a angel\u201d\n\u201cA 3D model of A Darth Vader helmet, highly detailed\u201d\nFigure 13. Qualitative comparisons with baseline methods across different views. All methods use stabilityai/stable-diffsuion-2-1-base for\nfair comparison. We observe that baselines suffer from severe multi-face issues while Sherpa3D achieves better quality and 3D coherence.\n18\nSherpa3D\n~25min\nFantasia3D\n~ 45min\nProlificDreamer\n~ 3h\nDreamFusion\n~ 1h\nShap-E\n~10s\nMagic3D\n~ 40min\n\u201cSpaceship,futuristic design,sleek metal,glowing thrusters,flying in space\u201d\n\u201cA detailed and realistic 3D model of a vintage camera\u201d\n\u201cHyper-realistic image of a snow leopard, capturing its camouflage and \nmajestic stance\u201d\n\u201cA luxurious sky-blue leather handbag with a sleek and elegant design, \nhighlighted by its vibrant blue color\u201d\nFigure 14. Qualitative comparisons with baseline methods across different views. All methods use stabilityai/stable-diffsuion-2-1-base for\nfair comparison. We observe that baselines suffer from severe multi-face issues while Sherpa3D achieves better quality and 3D coherence.\n19\n"
  },
  {
    "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
    "link": "https://arxiv.org/pdf/2312.06109.pdf",
    "upvote": "18",
    "text": "Vary: Scaling up the Vision Vocabulary for\nLarge Vision-Language Models\nHaoran Wei1\u2217, Lingyu Kong2\u2217, Jinyue Chen2, Liang Zhao1, Zheng Ge1\u2020,\nJinrong Yang3, Jianjian Sun1, Chunrui Han1, Xiangyu Zhang1\n1MEGVII Technology\n2University of Chinese Academy of Sciences\n3Huazhong University of Science and Technology\nhttps://varybase.github.io/\nAbstract\nModern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary\n\u2013 CLIP, which can cover most common vision tasks. However, for some special\nvision task that needs dense and fine-grained vision perception, e.g., document-level\nOCR or chart understanding, especially in non-English scenarios, the CLIP-style\nvocabulary may encounter low efficiency in tokenizing the vision knowledge and\neven suffer out-of-vocabulary problem. Accordingly, we propose Vary, an efficient\nand effective method to scale up the Vision vocabulary of LVLMs. The procedures\nof Vary are naturally divided into two folds: the generation and integration of a new\nvision vocabulary. In the first phase, we devise a vocabulary network along with a\ntiny decoder-only transformer to produce the desired vocabulary via autoregression.\nIn the next, we scale up the vanilla vision vocabulary by merging the new one\nwith the original one (CLIP), enabling the LVLMs can quickly garner new features.\nCompared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain\nits vanilla capabilities while enjoying more excellent fine-grained perception and\nunderstanding ability. Specifically, Vary is competent in new document parsing\nfeatures (OCR or markdown conversion) while achieving 78.2% ANLS in DocVQA\nand 36.2% in MMVet. Our code will be publicly available on the homepage.\n1\nIntroduction\nRecently, research into vision dialogue robots [1, 19, 25, 33, 55] has been gaining significant\ntraction. These human-like models, mainly relying on two components (large language models\n(LLMs) [7, 32, 35, 42, 53] and vision vocabulary networks), can not only converse based on user\u2019s\ninput image but also perform well on simple downstream tasks, such as VQA [22, 39], Image\ncaption [43], OCR [30], and so on. Hence, it is undeniable that large vision-language models\n(LVLMs) are driving the AI community towards the direction of artificial general intelligence (AGI).\nPopular GPT-4 [32]-like LVLMs, e.g., BLIP2 [19], MiniGPT4 [55],LLaVA [25], Qwen-VL [4], and\netc. [12, 50, 54] enjoy a stunning performance in multiple aspects with their own programming\nparadigm: Based on an LLM [36, 53], BLIP-2 proposes the Q-former, a BERT [11] like network\nas a vision input embedding layer, aiming to align the image tokens to a text special. Inherited the\nstructure of BLIP-2, MiniGPT-4 introduces 3500 high-quality image-text pairs as self-supervised\nfine-tuning (SFT) data, allowing it can \u201ctalk\u201d like GPT-4. Unlike BLIP-2, LLaVA utilizes a linear\nlayer as the vision embedding layer, which is similar with the text input embedding layer in the text\ntokenizer, ensuring the consistency in the structure of image and text branches. For Qwen-VL, it\n\u2217Equal contribution\n\u2020Project leader\narXiv:2312.06109v1  [cs.CV]  11 Dec 2023\nImage\nVision vocabulary\nImage tokens\n(CLIP-like VIT)\nQ-former\nAttention\nLinear\nBLIP-2\nQwen-VL\nLLaVA\nText\nText vocab\nText tokens\nLinear\nInput embedding layer\nconcat\nLLMs\nImage\nLinear\nA tiny decoder-\nonly network\n1.Generate a new vision vocabulary\n2.Scale up the vision vocabulary\nImage\nLinear\nLLMs\nPrevious methods\nText tokenizer\nVary (our method)\nfusion\nFigure 1: Previous method vs. Vary: Unlike other models that use a ready-made vision vocabulary,\nthe processes of Vary can be divided into two stages: the generation and fusion of vision vocabulary.\nIn the first stage, we use a \u201cvocabulary network\u201d along with a tiny decoder-only network to produce\na powerful new vision vocabulary via auto-regression. In the second stage, we fuse the vision\nvocabulary with the original one to provide new features for the LVLMs efficiently.\nutilizes a cross-attention layer to sample and align the image tokens, making the model can accept\nlarger input resolution. Although the above LVLMs\u2019 vision input embedding networks are variable\n(e.g., MLP, Qformer, Perceiver [1]), their vision vocabulary is almost identical (a CLIP-based [34]\nVIT) which we argue maybe a bottle-neck.\nIt is recognized that CLIP-VIT is a tremendous general vision vocabulary, which is trained via\ncontrastive learning upon more than 400M [37] image-text pairs, covering most natural images and\nvision tasks. However, for some special scenarios, e.g., high-resolution perception, Non-English OCR,\nDocument/Chart understanding, and so on, the CLIP-VIT may regard them as a \u201cforeign language\u201d,\nleading to inefficient tokenizing, i.e., difficulty in encoding all vision information into a fixed number\n(usually 256) of tokens. Although mPlug-Owl [49] and Qwen-VL alleviate the above issues by\nunfreeze its vision vocabulary network (a CLIP-L or CLIP-G), we argue that such manner may not\nbe reasonable due to three aspects: 1) it may overwrite the knowledge of the original vocabulary; 2)\nthe training efficiency of updating a vision vocabulary upon a relative large LLM (7B) is low; 3) it\ncan not allow the vision vocabulary network to \u201csee\u201d an image multiple times (train a dataset with\nmultiple epochs) due to the strong memory ability of LLMs. Therefore, a natural question is: Is there\na strategy that can simplify and effectively intensify the visual vocabulary?\nIn this paper, we propose Vary, an efficient and user-friendly approach, to answer the above question.\nVary is inspired by the text vocabulary expansion manner in vanilla LLMs [8], i.e., when transferring\nan English LLM to another foreign language, such as Chinese, it\u2019s necessary to expand the text\nvocabulary to lift the encoding efficiency and model performance under the new language. Intuitively,\nfor the vision branch, if we feed the \u201cforeign language\u201d image to the model, we also need to scale\nup the vision vocabulary. In Vary, the process of vocabulary scaling up can be divided into two\nsteps: 1) generate a new vision vocabulary that can make up the old one (CLIP); 2) integrate the new\nand old vocabularies. As shown in Figure 1, we build a small-size pipeline which is consisting of a\nvocabulary network and a tiny decoder-only transformer in the first step to train the vocabulary model\nvia predicting the next token. It is worth noting that the autoregressive-based process of generating a\n2\nvocabulary is perhaps more suitable for dense perception tasks than that based on contrastive learning\nlike CLIP. On the one hand, the next-token way can allow the vision vocabulary to compress longer\ntexts. On the other hand, the data formats that can be used in this manner are more diverse, such as\nVQA [5, 29] data with prompt. After preparing the new vision vocabulary, we add it to the vanilla\nLVLMs to introduce new features. In this process, we freeze both the new and old vocabularies\nnetworks to avoid the visual knowledge being overwritten.\nAfterward scaling up the vision vocabulary, our LVLM can achieve more fine-grained vision per-\nception, such as document-level Chinese/English OCR, book image to markdown or LATEX, Chi-\nnese/English chart understanding, and so on, while ensuring its original capabilities (conversation,\nVQA, caption, etc.). Besides, we provide methods for producing synthetic data and validate its impor-\ntance in document/chart understanding. More importantly, Vary is a useful strategy to strengthen the\nvisual vocabulary of LVLMs, which can be utilized at arbitrary downstream visual tasks that CLIP is\nnot good at. In addition to the document and chart parsing mentioned in this paper, we believe that\nVary still enjoys more fine-grained tasks and we appeal to researchers to rethink the design ideas of\nLVLMs from the perspective of visual vocabulary construction.\n2\nRelated Works\n2.1\nLarge Language Models\nOver the past year, significant attention has been drawn to large language models (LLMs) in the fields\nof both natural language processing (NLP) and computer vision (CV). This heightened attention stems\nfrom LLMs\u2019 outstanding performance in diverse aspects, especially the powerful world knowledge\nbase and universal capabilities. Current LLMs enjoy a unified transformer architecture which is\nexemplified by BERT [11], GPT-2 [35], T5 [36], etc. Subsequently, researchers have uncovered the\nconcept of an \"emergent ability\" [45] in LLMs. This implies that as language model sizes reach a\ncertain threshold, there may be a qualitative leap in their capabilities. Furthermore, InstructGPT [33]\nand ChatGPT [31] find that Reinforcement Learning with Human Feedback (RLHF) [9] can further\nlift the performance of the \"talk robot\u201d. Motivated by the tremendous success of the GPT series, a\nmultitude of other open-source LLMs have emerged, including OPT [53], LLaMA [42], GLM [52],\nand so on. Building upon these openly available LLMs, numerous tailored fine-tuned models have\nbeen introduced to develop LLMs for diverse applications, especially LLaMA-driven models,e.g.,\nAlphaca [40], Vicuna [8], which have become the de-facto component for a Large Vision-Language\nModel (LVLM).\n2.2\nLLM-based Large Vision-Language Models\nLLM\u2019s robust zero-shot capabilities and logical reasoning make it play the central controller role\nwithin an LVLM. There are two primary pipeline styles: plugin-based and end-to-end model. Plugin-\nbased methods [21, 38, 46\u201348] typically regard LLMs as an agent to invoke various plugins from\nother foundational or expert models, executing specific functions in response to human instructions.\nWhile such methods offer versatility, they have limitations in terms of plugin invocation efficiency\nand performance. Conversely, end-to-end LVLMs usually rely on a single large multimodal model\nto facilitate interactions. Following this approach, Flamingo [1] introduces a gated cross-attention\nmechanism trained on billions of image-text pairs to align vision and language modalities, demon-\nstrating strong performance in few-shot learning. BLIP-2 [19] introduces Q-Former to enhance the\nalignment of visual features with the language space. More recently, LLaVA [25] proposes using a\nsimple linear layer to replace Q-Former and designed a two-stage instruction-tuning procedure.\nDespite the remarkable performance of existing methods, they are confined to the same and limited\nvision vocabulary \u2013 CLIP-VIT [34]. For an LVLM, CLIP-VIT is a tremendous general vision\nvocabulary that is trained via contrastive learning upon million-level image-texts pairs, which can\ncover most nature images and vision tasks, e.g., VQA, Caption, Easy English OCR. However, some\nimages under special scenarios, e.g., high-resolution image, Non-English OCR, Document/Chart\nunderstanding, and so on, will still be regarded as a \u201cforeign language\u201d by CLIP-VIT, leading to\nvision out-of-vocabulary problem, which will in turn become a bottleneck for LVLMs.\n3\nThe new vocabulary \nnetwork\nOPT125M\nManual Doc/Chart image\n3\u00d71024\u00d71024\nInput embedding\nlinear layer\nNegative \nfeatures\n256\u00d71024\n\u201cThis is an nature image\u201d\n The dense-ocr results of the document\n The python dict-style of the chart\nNegative Natural image \nNatural/Doc/Chart image\nThe new vocabulary \nnetwork\nThe original CLIP-L \nnetwork\nInput embedding\nlinear layer\nInput embedding\nlinear layer\n256\u00d71024\n3\u00d71024\u00d71024\n3\u00d7224\u00d7224\nConcat\n7B-LLM\nConversations \nbased on human \nprompts\nVary-tiny\nVary-base\n\u201cDescribe the image\u201d\n256\u00d74096\nFigure 2: Overview of the Vary. There are two types of Vary form: Vary-tiny and Vary-base. Vary-tiny\nis mainly focused on generating a new vision vocabulary while Vary-base is our new LVLM aiming\nto handle various visual tasks based on the new vision vocabulary.\n3\nMethod\n3.1\nArchitecture\nVary enjoys two conformations: Vary-tiny and Vary-base, as shown in Figure 2. We devise the\nVary-tiny to \u201cwrite\u201d a new vision vocabulary and the Vary-base to make use of the new vocabulary.\nSpecifically, Vary-tiny is mainly composed of a vocabulary network and a tiny OPT-125M [53].\nBetween the two modules, we add a linear layer to align the channel dimensions. There is no text\ninput branch in Vary-tiny due to it is a primary focus on fine-grained perception. We hope the new\nvision vocabulary network can excel in processing artificial images, i.e., documents, and charts, to\ncompensate for CLIP\u2019s shortcomings. At the same time, we also expect that it will not be a noise\nfor CLIP when tokenizing natural images. Accordingly, during generating, we feed the manual\ndocument and chart data as positive samples and natural images as negatives to train Vary-tiny. After\ncompleting the above process, we extract the vocabulary network and add it to a large model to build\nthe Vary-base. As shown in the lower half of Figure 2, the new and old vocabulary networks enjoy\nindependent input embedding layers and are integrated before the LLM. In such a stage, we freeze\nboth weights of new and old vision vocabulary networks and unfreeze the weights of other modules.\n3.2\nTowards Generating a New Vision Vocabulary\n3.2.1\nThe new vocabulary network\nWe use the SAM [15] pretrained ViTDet [20] image encoder (base scale) as the main part of the new\nvocabulary network of Vary. Due to the input resolution of the SAM-base is (1024\u00d71024) while\nthe output stride is 16, the feature shape of the last layer is (64\u00d764\u00d7256 for H\u00d7W\u00d7C) that can not\nbe aligned to the output of CLIP-L (256\u00d71024 for N\u00d7C). Hence, we add two convolution layers,\nwhich we found is a good token merging unit, behind the last layer of the SAM initialized network,\nas shown in Figure 3. The first convolution layer possesses a kernel size of 3, aiming to transfer the\n4\nfeature shape to 32\u00d732\u00d7512. The setting of the second conv layer is the same as the first one, which\ncan further convert the output shape to 16\u00d716\u00d71024. After that, we flattened the output feature to\n256\u00d71024 to align the image token shape of CLIP-VIT.\n3.2.2\nData engine in the generating phrase\nSAM-base\nimage encoder\nInput\nImage\n1024\u00d71024\u00d73\n64\u00d764\u00d7256\nConv1\nConv2\n32\u00d732\u00d7512\n16\u00d716\u00d71024\n256\u00d71024\nFigure 3: The structure of new vision vocabulary\nnetwork. We add two convolution layers to convert\nthe output to be similar with CLIP.\nDocumnet data. We select the high-resolution\ndocument image-text pairs as the main positive\ndataset used for the new vision vocabulary pre-\ntrain due to the dense OCR can effectively val-\nidate the fine-grained image perception ability\nof the model. To our knowledge, there is no\npublicly available dataset of English and Chi-\nnese documents, so we create our own. We first\ncollect pdf-style documents from open-access\narticles on arXiv and CC-MAIN-2021-31-PDF-\nUNTRUNCATED for the English part and col-\nlect from e-books on the Internet for the Chinese\npart. Then we use fitz of PyMuPDF to extract\nthe text information in each pdf page and con-\nvert each page into a PNG image via pdf2image at the same time. During this process, we construct\n1M Chinese and 1M English document image-text pairs for training.\nChart data. We find current LVLMs are not good at chart understanding, especially Chinese charts,\nso we choose it as another main knowledge that needs to be \u201cwritten\u201d into the new vocabulary. For\nchart image-text pair, we all follow the rendering way. We select both the matplotlib and pyecharts as\nthe rendering tools. For matplotlib-style chart, we built 250k in both Chinese and English. While\nfor pyecharts, we build 500k for both Chinese and English. Besides, we convert the text ground\ntruth of each chart to a python-dict form. The texts used in the chart, e.g., title, x-axis, and y-axis,\nare randomly selected from the Natural Language Processing (NLP) corpus downloaded from the\nInternet.\nNegative natural image. For natural image data that CLIP-VIT is good at, we need to ensure that\nthe newly introduced vocabulary does not cause noise. Consequently, we construct negative natural\nimage-text pairs to enable the new vocabulary network to encode correctly when seeing natural\nimages. We extract 120k images in the COCO [22] dataset with each image corresponding to a text.\nThe text part is randomly selected from follows sentences: \"It\u2019s an image of nature\"; \"Here\u2019s a nature\npicture\"; \"It\u2019s a nature photo\"; \"This is a natural image\"; \"That\u2019s a shot from nature\".\n3.2.3\nInput format\nWe train all parameters of the Vary-tiny with image-text pairs by autoregression. The input format\nfollows popular LVLMs [13], i.e, the image tokens are packed with text tokens in the form of a\nprefix. Specifically, we use two special tokens \"<img>\" and \"</img>\" to indicate the position of the\nimage tokens as the input of an interpolated OPT-125M (4096 tokens). During training, the output of\nVary-tiny is only text, and \"</s>\" is regarded as the eos token.\n3.3\nTowards Scaling Up the Vision Vocabulary\n3.3.1\nThe structure of Vary-base\nAfter completing the training of the vocabulary network, we introduce it to our LVLM \u2013 Vary-base.\nSpecifically, we parallelize the new vision vocabulary with the original CLIP-VIT. Both two vision\nvocabularies enjoy an individual input embedding layer, i.e., a simple linear. As shown in Figure 2,\nthe input channel of the linear is 1024 and the output is 2048, ensuring the channel of image tokens\nafter concatenating is 4096, which exactly aligns the input of LLM (Qwen-7B [3] or Vicuna-7B [8]).\n3.3.2\nData engine in the scaling up phrase\nLATEX rendering document. Except for the collecting document data in Section 3.2.2, we also need\ndata to enjoy some format, e.g., supporting formula, and table. To this end, we create document\n5\nSynthetic Document\nSynthetic Chart\nChinese/English texts\nformula\ntable\npyecharts style \nmatplotlib style \nFigure 4: Visualization of synthetic data.\nWe use pdflatex to render documents and utilize\npyecharts/matplotlib to render charts. Document data obtains Chinese/English texts, formulas,\nand tables. Chart data includes Chinese/English bar, line, pie, and composite styles.\ndata through LATEX rendering. Firstly, we collected some .tex source files on arxiv, and then extracted\ntables, mathematical formulas, and plain texts using regular expressions. Finally, we re-render these\ncontents with the new template we prepared by pdflatex. We collect 10+ templates to perform batch\nrendering. Besides, we transfer the text ground truth of each document page to a mathpix markdown\nstyle to unify the format. By this construction process, we acquired 0.5 million English pages and 0.4\nmillion Chinese pages. Some samples are shown in Figure 4.\nSemantic association chart rendering. In Section 3.2.2, we batch render chart data to train the new\nvocabulary network. However, the texts (title, x-axis values, and y-axis values) in those rendered\ncharts suffer low correlation because they are randomly generated. This issue is not a problem in\nthe vocabulary-generating process as we only hope that the new vocabulary can efficiently compress\nvisual information. However, in the training stage of the Vary-base, due to unfreezing the LLM,\nwe hope to use higher quality (strongly correlated content) data for training. Therefore, we use\nGPT-4 [32] to generate some charts using relevant corpus and then we utilize the high-quality corpus\nto addition render 200k chart data for the Vary-base training.\nGeneral data. The processes of training Vary-base follows popular LVLMs, e.g., LLaVA [25],\nincluding the pretrain and SFT phases. Different from the LLaVA, we freeze all the vocabulary\nnetworks and unfreeze both the input embedding layer and LLM, which is more like the pretrain\nsetting of a pure LLM. We use natural image-text pair data to introduce the general concepts to the\nVary-base. The image-text pairs are randomly extracted from LAION-COCO [37] with the amount\nof 4 million. In the SFT stage, we use the LLaVA-80k or LLaVA-CC665k [24] along with the train\nset of DocVQA [29] and ChartQA [28] as the fine-tuning dataset.\n3.3.3\nConversation format\nWhen we use the Vicuna-7B as our LLM, the conversation format follows the Vicuna v1 [8], i.e.,\nUSER: <img>\"<image>\"</img> \"texts input\" ASSITANT: \"texts output\" </s>. Due to the low\nefficiency in the text vocabulary of Vicuna to process Chinese, we choose Qwen-7B [2] as the LLM\nfor Chinese processing. When we use the Qwen-7B, we design the conversation style following the\nLLaVA-MPT [25, 41], which can be described as: <|im_start|>user: <img>\"<image>\"</img> \"texts\ninput\"<|im_end|> <|im_start|>assistant: \"texts output\" <|im_end|>.\n4\nExperiments\n4.1\nDatasets and Evaluation Metrics\nWe evaluate the proposed Vary on multiple datasets, including 1) a document-level OCR test set we\ncreated to explore the performance of dense visual perception; 2) DocVQA [29] and ChartQA [28]\nto test the improvement on downstream tasks; 3) MMVet [51] to monitor changes in the general\n6\nperformance of the model. Our own document test set contains pure OCR and markdown conversion\ntasks. In a pure OCR task, the test split includes 100 pages in both Chinese and English, which are\nrandomly extracted from arxiv and ebook. In the markdown conversion task, the test set obtains 200\npages, of which 100 pages contain tables and another 100 pages have mathematical formulas.\nWe report Normalized Edit Distance [6, 17] and F1-score along with the precision and recall for\ndocument parsing. For DocVQA, ChartQA, and MMVet, we use their vanilla metrics for a fair\ncomparison with other LVLMs.\n4.2\nImplementation Details\nDuring the vision vocabulary generating process, we optimize all parameters of Vary-tiny with a\nbatch size of 512 and train the model for 3 epochs. We utilize the AdamW [27] optimizer and a\ncosine annealing scheduler [26] along with the learning rate of 5e-5 to train Vary-tiny.\nIn the training stage of the Vary-base, we freeze the weights of both new and vanilla (CLIP-L) vision\nvocabulary networks and optimize the parameters of input embedding layers and LLM. The initial\nlearning rate is 5e-5 in pretrain while 1e-5 in SFT. Both the pretrain and SFT enjoy a batch size of\n256 and an epoch of 1. Other settings are the same as Vary-tiny.\nMethod\nForms\nPure Document OCR\nMarkdown Format Conversion\nChinese\nEnglish\nFormula\nTable\nAverage\nNougat [6]\nEdit Distance \u2193\n\u2013\n0.126\n0.154\n0.335\n0.245\nF1-score \u2191\n\u2013\n89.91\n83.97\n75.97\n79.97\nPrediction \u2191\n\u2013\n89.12\n82.47\n75.21\n78.84\nRecall \u2191\n\u2013\n90.71\n85.53\n76.74\n81.14\nVary-tiny\nEdit Distance \u2193\n0.266\n0.197\n\u2013\n\u2013\n\u2013\nF1-score \u2191\n86.00\n84.25\n\u2013\n\u2013\n\u2013\nPrediction \u2191\n86.14\n89.38\n\u2013\n\u2013\n\u2013\nRecall \u2191\n85.86\n79.67\n\u2013\n\u2013\n\u2013\nVary-base\nEdit Distance \u2193\n0.174\n0.106\n0.082\n0.280\n0.181\nF1-score \u2191\n87.32\n88.24\n85.94\n76.26\n81.10\nPrediction \u2191\n86.59\n90.08\n87.06\n76.81\n81.94\nRecall \u2191\n88.06\n86.47\n84.84\n75.71\n80.28\nTable 1: Fine-grained text perception compared to Nougat. Vary-tiny is the model based on OPT-\n125M to generate the vision vocabulary, which enjoys pure OCR ability, including Chinese and\nEnglish. Vary-base is the model upon Qwen-Chat 7B after scaling up the vision vocabulary, enjoying\nboth pure document OCR and markdown format conversation abilities through prompt control.\n4.3\nFine-grained Perception Performance\nWe measure the fine-grained perception performance of Vary through the dense text recognition\nability. As shown in Table 1, Vary-tiny gathers both Chinese and English dense OCR ability by the\nprocess of vision vocabulary generating. Specifically, it achieves 0.266 and 0.197 edit distance for\nChinese and English documents (plain texts) OCR respectively, proving the new vision vocabulary\nenjoys good fine-grained text encoding capacity. For Vary-base, it can achieve an on-par performance\nwith nougat [6] (a special document parsing model) on English plain text documents. Besides, with\ndifferent prompts (e.g., Convert the image to markdown format.), Vary-base can realize the document\nimage-markdown format conversion. It is worth noting that in such a task, Vary-base (with 0.181\nedict distance and 81.10% F1 on math and table average) is better than nougat (with 0.245 edict\ndistance and 79.97% F1 on average) to some extent, which may be due to the super strong text\ncorrection ability of the 7B LLM (Qwen). All the above results indicate that by scaling up the vision\nvocabulary, the new LVLM can lift its fine-grained perception performance.\n4.4\nDownstream Task Performance\nWe test the performance improvement on downstream VQA tasks with DocVQA [29] and\nChartQA [28]. We use the addition prompt: \"Answer the following questions using a single word\n7\nMethod\nDocVQA\nChartQA\nval\ntest\nhuman\naugmented\nAverage\nDessurt [10]\n46.5\n63.2\n-\n-\n-\nDonut [14]\n-\n67.5\n-\n-\n41.8\nPix2Sturct [16]\n-\n72.1\n30.5\n81.6\n56.0\nmPLUG-DocOwl [49]\n-\n62.2\n-\n-\n57.4\nMatcha [23]\n-\n-\n38.2\n90.2\n64.2\nQwen-VL [3]\n-\n65.1\n-\n-\n65.7\nVary-base (80k)\n78.2\n76.3\n43.2\n87.3\n65.3\nVary-base (665k)\n78.1\n76.3\n43.8\n88.3\n66.1\nTable 2: Comparison with popular methods on DocVQA and ChartQA. 80k represents that the SFT\ndata is LLaVA-80k while 665k is the LLaVA-CC665k. The metric of DocVQA is ANLS while the\nChartQA is relaxed accuracy following their vanilla papers.\nMethod\nMM-Vet\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nBLIP-2 [19]\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4\nLLaVA-7B [25]\n28.0\n17.1\n16.3\n18.9\n21.2\n11.5\n23.8\nMiniGPT-4 [55]\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4\nOtter [18]\n27.3\n17.8\n14.2\n13.8\n24.4\n3.8\n24.7\nOpenFlamingo [1]\n28.7\n16.7\n16.4\n13.1\n21.0\n7.7\n24.8\nLLaVA-13B [25]\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9\nLLaVA1.5-7B [24]\n-\n-\n-\n-\n-\n-\n30.5\nVary-base (vicuna7B) (665k)\n38.7\n22.0\n23.6\n24.1\n29.6\n7.7\n32.9\nVary-base (qwen7B) (80k)\n38.9\n30.1\n22.4\n21.7\n34.3\n7.7\n36.2\nTable 3: Comparison with popular methods on MMVet. The abbreviations represent: Rec: Recogni-\ntion; Know: Knowledge; Gen: Language generation; Spat: Spatial awareness.\nor phrase:\" [24] to allow the model to output short and precise answers. As shown in Table 2,\nVary-base (with Qwen-7B as LLM) can achieve 78.2% (test) and 76.3% (val) ANLS on DocVQA\nupon LLaVA-80k [25] SFT data. With LLaVA-665k [24] data for SFT, Vary-base can reach 66.1%\naverage performance on ChartQA. The performance on both two challenging downstream tasks\nis comparable to or even better than Qwen-VL [4], demonstrating the proposed vision vocabulary\nscaling-up method is also promising for downstream.\n4.5\nGeneral Performance\nWe monitor the general performance of Vary through MMVet [51] benchmark. As shown in table 3,\nwith the same LLM (Vicuna-7B) and SFT data (LLaVA-CC665k), Vary lifts 2.4% (32.9% vs. 30.5%)\nof the total metric than LLaVA-1.5, proving that our data and training strategy do not hurt the model\u2019s\ngeneral ability. Besides, Vary with Qwen-7B and LLaVA-80k can achieve 36.2% performance, further\ndemonstrating the effectiveness of our vision vocabulary scaling-up manner.\n5\nConclusion\nThis paper highlights that scaling up the vocabulary in the visual branch for an LVLM is quite\nsignificant and we successfully devise a simple method to prove such a claim. According to the\nexperiments, the provided model, Vary, achieves promising scores in multiple tasks, which is mainly\nprofited by the new vocabulary we generated. Despite the satisfactory performance of Vary, we\nbelieve that how to effectively scale up the visual vocabulary still enjoys much improvement rooms,\nespecially compared to the mature and relatively simple means of expanding text vocabulary. We hope\nthat the useful and efficient design of Vary will attract more research attention to such a direction.\n8\n6\nAppendix\nIn this appendix, we present the output results of our model to provide a more intuitive understanding\nof its performance.\nPrompt: Convert the image to markdown/latex format.\n(\u5c06\u8fd9\u5e45\u56fe\u8f6c\u6210markdown\u683c\u5f0f\u3002)\nPrompt: Provide the OCR results of this image.\n(\u5199\u51fa\u8fd9\u5e45\u56fe\u4e2d\u6240\u6709\u6587\u5b57\u3002)\nFigure 5: Instruction following ability of Vary-base to excel markdown conversion or pure OCR.\nVary-base can control the output format for a document image input upon the user\u2019s prompts.\n9\nthe arts. Specifically, it can yield AP of 50.2% and 53.8% in single-scale testing as well \nas 51.6% and 55.6% in multi-scale upon ResNeXt-101 [9] and SwinTransformer [19] \nbackbones, respectively. Besides, HumanLiker achieves good efficiency-accuracy \nbalance. Only needing 12 epochs to train, it can yield an AP of 47.1% at an 18 FPS on \nCOCO val2017 set, which is much more efficient than most of advanced methods. More \nimportantly, we believe that the HumanLiker still enjoys much improvement room, e.g., \nfurther optimizing top-left corners location and long-range distances regression. \nAccordingly, we believe the powerful and intelligible HumanLiker can turn into a popular \nand excellent baseline in the further and we encourage researchers to rethink the \nconstruction way of object detector from a bionic perspective. \n2 Related Work \n2.1 Center-guided Detector \nCenter-guided detector is the most active branch in object detection community, which \nusually defines a sea of center locations (points/areas) as positive samples to directly \nregress the heights and widths of objects. We classify detectors into this category as \nlong as they directly predict object centers. \nFaster R-CNN [24] applies center-driven anchors to guide the proposal regression and \nachieves end-to-end trainable. Afterwards, the center-guided anchor mechanism \nbecomes a de-facto standard in two-stage detectors. To improve the detector efficiency, \na few center-guided one-stage algorithms [21\u201323] also be devised. They delete the RPN \nand directly run regression and classification at anchor centers. RetinaNet [17] \nintroduces the Focal loss to balance the hard and easy samples to boost its accuracy. \nDifferent from the above anchor-based detectors, FCOS [26] and DETR [3] present two \ntypes of center-guided anchor-free manners. FCOS models lots of pixels within center \nareas of object as positives, and then predicts four vectors directly at each positive \nlocation without the guidance of an anchor. DETR utilizes the Hungarian algorithm to \nmatch predicted centers with ground-truth ones. \nAlthough center-guided detector has achieved great success, they also face shortages, \ne.g., pinpoint the center of a box is arduous actually \u2014 generally needing all four \nboundaries of the corresponding object as reference. \n2.2 Corner-guided Detector \nInstead of predicting center points directly, corner-guided detector estimates and \nextracts corner keypoints upon heatmaps to decode object boxes. Due to avoiding the \ndirect search for the center point, we argue this corner-guided detector is more bionic. \nCornerNet [13] outputs object boxes via estimating and grouping corner pairs. The \ngrouping method in CornerNet is that if a top-left corner and a bottom-right corner \nbelong to the same object, the distance between their embedding vectors will be small \n[13] . To address mismatched corners across different objects, CenterNet [6] introduces \nan extra prediction head, making the corners grouping become triplets grouping. \nCentripetalNet [5] abandons the 1D pull-push embeddings and presents a centripetal \ngrouping method with a 2D-embedding form to better group paired corners. Besides, \nCPN [7] removes mismatched corners via sub-networks to further lift accuracy. As \ndescribed above, grouping process is an obstacle to improve the corner-guided detector, \nwhich is also an unnecessary step for humans in labeling. \nWe devise HumanLiker as a new human-like detection branch that combines strong \npoints of both center/corner-guided branches to further explore the potential of deep-\nnetwork-based detectors. \n3 Method \n3.1 Towards Human-like Detection Model Construction \nAs stated in Section 1, there are usually two steps in the manual labeling process. Here \nwe further dissect how to model each step based on networks along with Figure 1 and 2. \nGiven an input image I H\u00d7W with objects to be labeled, we humans usually \u201cclick\u201d our \nmouse at the top-left corner location of the interested object to pinpoint its position at \nfirst. In HumanLiker, we utilize keypoint estimation method to model this procedure. As \nillustrated in Figure 2, we adopt a heatmap to predict and \n3\nProvide the OCR results of this image.\nFigure 6: Fine-grained visual perception ability of Vary-base on English document dense OCR. This\nimage is the page 3 of [44].\n\u5199\u51fa\u56fe\u4e2d\u7684\u6587\u5b57\u3002\n\u4e0a\u7bc7 \n\u4e2d\u5b66\u8bed\u6587\u6559\u6750\u6559\u6cd5\u7eb2\u8981 \n\u7b2c\u4e00\u7ae0\u4e2d\u5b66\u8bed\u6587\u6559\u6750\u6559\u6cd5\u8bfe \u5fc5\u987b\u6539\u9020 \n\u300a\u4e2d\u5b66\u8bed\u6587\u6559\u5b66\u6cd5\u300b\u4f5c\u4e3a\u4e00\u95e8\u79d1\u5b66\uff0c\u5e94\u5f53\u6709\u5b83\u7684\u77db\n\u76fe\u7279\u8dc3\u6027\u3002\u6bdb\u6cfd\u4e1c\u540c\u5fd7\u5728\u300a\u77db\u76fe\u8bba\u300b\u4e2d\u8bf4\uff1a\u201c\u79d1\u5b66\n\u7814\u7a76\u7684\u533a\u5206\uff0c\u5c31\u662f\u6839\u636e\u79d1\u5b66\u5bf9\u8c61\u6240\u5177\u6709\u7684\u7279\u6b8a\u77db\u76fe\n\u6027\u3002\u56e0\u6b64\uff0c\u5bf9\u4e8e\u67d0\u4e00\u73b0\u8c61\u7684\u9886\u57df\u6240\u7279\u6709\u7684\u67d0\u4e00\u77db\u76fe\n\u7684\u7814\u7a76\uff0c\u5c31\u6784\u6210\u67d0\u4e00\u95e8\u79d1\u5b66\u7684\u5bf9\u8c61\u3002\u201d\u300a\u4e2d\u5b66\u8bed\u6587\n\u6559\u5b66\u6cd5\u300b\u7814\u7a76\u7684\u5bf9\u8c61\u662f\u4e2d\u5b66\u8bed\u6587\u6559\u5b66\u5b9e \u8df5\uff0c\u8fd9\u4e00\u8ba4\n\u8bc6\u5728\u76ee\u524d\u4f3c\u4e4e\u662f\u4e00\u81f4\u7684\u3002\u4f46\u4ed4\u7ec6\u63a8\u7a76\u8d77\u6765\uff0c\u5176\u95f4\u4ecd\n\u5b58\u5728\u786e\u8ba4\u8ba4\u8bc6\u4e0a\u7684\u5dee\u5f02\u3002\u6bd4\u5982\u5728\u5bf9\u8fd9\u95e8\u8bfe\u7a0b\u6027\u8d28\u4efb\n\u52a1\u7684\u8ba4\u8bc6 \u4e0a\uff0c\u6709\u7684\u8bf4\u201c\u4e2d\u5b66\u8bed\u6587\u6559\u5b66\u6cd5\u662f\u4e00\u95e8\u5177\u6709\n\u7efc\u5408\u6027\u548c\u5b9e\u8df5\u6027\u7684 \u79d1\u5b66\u201d\uff08\u6b66\u6c49\u5e08\u9662\u7b49\u5341\u4e8c\u9662\u6821\u7f16\n\u300a\u4e2d\u5b66\u8bed\u6587\u6559\u5b66\u6cd5\u300b\uff0c\u4eba\u6c11\u6559 \u80b2\u51fa\u7248\u793e\u51fa\u7248\uff09\uff1b\u6709\n\u7684\u8bf4\u5b83\u662f\u201c\u4e00\u95e8\u5177\u6709\u4e13\u4e1a\u7279\u70b9\u7684\u6559\u5b66\u7406\u8bba\u8bfe\u7a0b\u201d\n\uff08\u5f20\u9686\u534e\u4e3b\u7f16\u300a\u4e2d\u5b66\u8bed\u6587\u6559\u5b66\u6cd5\u300b\uff0c\u6e56\u5357\u4eba\u6c11\u51fa\u7248 \n\u793e\u51fa\u7248\uff09\uff0c\u636e\u8bf4\u5728\u56fd\u5916\uff0c\u5bf9\u7c7b\u4f3c\u5b66\u79d1\u4e5f\u6709Know how\n\u548c Know how to teach\u7684\u4e89\u8bba\u3002\u5dee\u5f02\u5c31\u662f\u77db\u76fe\u3002\n\u201c\u4eba\u7684\u6982 \u5ff5\u7684\u6bcf\u4e00\u5dee\u5f02\uff0c\u90fd\u5e94\u628a\u5b83\u770b\u4f5c\u662f\u5ba2\u89c2\u77db\u76fe\n\u7684\u53cd\u6620\u3002\u201d\uff08\u6bdb\u6cfd\nFigure 7: Fine-grained visual perception ability of Vary-base on Chinese book dense OCR. This\nimage is from the Internet.\n10\nConvert the image to markdown format.\nFigure 8: Markdown/Latex format conversion ability (on math formula) of Vary-base. This image is\nfrom the Internet.\nConvert this table to latex format.\nFigure 9: Markdown/Latex format conversion ability (on the table) of Vary-base.The images are from\nthe Internet.\n11\nConvert this chart to python-dict\nThe dict of the chart is: \n{\n    \"title\": \"\u56fe\u8868\uff1a\u7ea2\u5b9d\u4e3d\u805a\u919a\u4ea7\u54c1\u6bdb\u5229\u7387\",\n    \"data\": {\n        \"2013\": \"12.94%\",\n        \"2014\": \"16.67%\",\n        \"2015\": \"19.47%\",\n        \"2016\": \"14.15%\",\n        \"2017\": \"10.42%\",\n        \"2018\": \"11.09%\",\n        \"2019\": \"21.07%\",\n        \"2020\": \"16.49%\",\n        \"2021\": \"9.54%\"\n    }\n}\nThe dict of the chart is: \n{\n    \"title\": \"\u56fe\uff1a\u4e3b\u7cae\u53ca\u533b\u7597\u836f\u54c1\u662f\u4e3b\u8981\u6d88  \n   \u8d39\u5185\u5bb9\uff082021\uff09\",\n    \"data\": {\n        \"\u670d\u52a1\": \"6.40%\",\n        \"\u7528\u54c1\": \"12.80%\",\n        \"\u836f\u54c1\u53ca\u533b\u7597\": \"29.20%\",\n        \"\u8425\u517b\u54c1\": \"1.80%\",\n        \"\u96f6\u98df\": \"13.90%\",\n        \"\u4e3b\u7cae\": \"35.80%\"\n    }\n}\nConvert this chart to python-dict\nFigure 10: Chart understanding (Chinese) of Vary-base. The images are from the Internet.\n\u8fd9\u5e45\u56fe\u5947\u602a\u5728\u54ea\u91cc\uff1f\nWhen visiting this serene lake scene featuring a wooden \ndock, you should be cautious of several factors. Firstly, \nthe water might be shallow or have underwater \nobstacles, such as rocks or debris, which could pose a \nrisk to swimmers or boaters. Secondly, since the dock is \nlocated near a mountain, the weather conditions can \nchange rapidly, so it's essential to be prepared for \nsudden changes in temperature, wind, or precipitation. \nAdditionally, if you plan to engage in water activities like \nboating or fishing, make sure you have the necessary \nsafety equipment and follow local guidelines to ensure a \nsafe and enjoyable experience. Lastly, if you're planning \nto take photographs, be mindful of the environment and \nrespect the natural beauty of the area by not disrupting \nthe peaceful atmosphere or causing unnecessary \ndisturbances to the wildlife.\nWhat are the things I should be cautious \nabout when I visit here?\n\u8fd9\u5f20\u56fe\u7247\u7684\u5947\u602a\u4e4b\u5904\u5728\u4e8e\u4e00\u4e2a\u4eba\u7ad9\u5728\n\u4e00\u8f86\u505c\u5728\u8857\u4e0a\u7684\u9ec4\u8272\u8d27\u8f66\u7684\u540e\u90e8\uff0c\u7528\n\u71a8\u6597\u71a8\u70eb\u4e00\u4ef6\u84dd\u8272\u7684\u8863\u7269\u3002\u8fd9\u662f\u4e0d\u5bfb\n\u5e38\u7684\uff0c\u56e0\u4e3a\u71a8\u70eb\u901a\u5e38\u662f\u5728\u5ba4\u5185\u8fdb\u884c\uff0c\n\u6bd4\u5982\u5728\u71a8\u8863\u677f\u4e0a\u6216\u5728\u5bb6\u5ead\u6216\u4e13\u4e1a\u71a8\u8863\n\u5e97\u3002\u6b64\u5916\uff0c\u71a8\u70eb\u8863\u7269\u65f6\uff0c\u4e00\u4e2a\u4eba\u7ad9\u5728\n\u4e00\u8f86\u884c\u9a76\u4e2d\u7684\u8f66\u8f86\u4e0a\u662f\u4e0d\u5b89\u5168\u7684\uff0c\u56e0\n\u4e3a\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e8b\u6545\u6216\u53d7\u4f24\u3002\nFigure 11: General performance of Vary-base. The images are from LLaVA [25] samples.\n12\nReferences\n[1] Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\nReynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick,\nJ.L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O.,\nZisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot learning. In: NeurIPS\n(2022) 1, 2, 3, 8\n[2] Alibaba: Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts).\nhttps://github.com/QwenLM/Qwen-7B (2023) 6\n[3] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li,\nM., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu,\nJ., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y.,\nYu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu,\nT.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 5, 8\n[4] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile vision-\nlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966\n(2023) 1, 8\n[5] Biten, A.F., Litman, R., Xie, Y., Appalaraju, S., Manmatha, R.: Latr: Layout-aware transformer for\nscene-text vqa. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\npp. 16548\u201316558 (2022) 3\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic\ndocuments. arXiv preprint arXiv:2308.13418 (2023) 7\n[7] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information\nprocessing systems 33, 1877\u20131901 (2020) 1\n[8] Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez,\nJ.E., Stoica, I., Xing, E.P.: Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\nhttps://lmsys.org/blog/2023-03-30-vicuna/ (2023) 2, 3, 5, 6\n[9] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from\nhuman preferences. Advances in neural information processing systems 30 (2017) 3\n[10] Davis, B., Morse, B., Price, B., Tensmeyer, C., Wigington, C., Morariu, V.: End-to-end document\nrecognition and understanding with dessurt. In: European Conference on Computer Vision. pp. 280\u2013296.\nSpringer (2022) 8\n[11] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018) 1, 3\n[12] Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al.: Dreamllm:\nSynergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499 (2023) 1\n[13] Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Liu,\nQ., et al.: Language is not all you need: Aligning perception with language models. arXiv preprint\narXiv:2302.14045 (2023) 5\n[14] Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.: Ocr-free\ndocument understanding transformer. In: European Conference on Computer Vision. pp. 498\u2013517. Springer\n(2022) 8\n[15] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C.,\nLo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023) 4\n[16] Lee, K., Joshi, M., Turc, I.R., Hu, H., Liu, F., Eisenschlos, J.M., Khandelwal, U., Shaw, P., Chang,\nM.W., Toutanova, K.: Pix2struct: Screenshot parsing as pretraining for visual language understanding. In:\nInternational Conference on Machine Learning. pp. 18893\u201318912. PMLR (2023) 8\n[17] Levenshtein, V.I., et al.: Binary codes capable of correcting deletions, insertions, and reversals. In: Soviet\nphysics doklady. vol. 10, pp. 707\u2013710. Soviet Union (1966) 7\n[18] Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context\ninstruction tuning. arXiv preprint arXiv:2305.03726 (2023) 8\n13\n[19] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image\nencoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 1, 3, 8\n[20] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection.\nIn: European Conference on Computer Vision. pp. 280\u2013296. Springer (2022) 4\n[21] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L., Mao, S., et al.: Taskmatrix. ai:\nCompleting tasks by connecting foundation models with millions of apis. arXiv preprint arXiv:2303.16434\n(2023) 3\n[22] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft\nCOCO: common objects in context. In: ECCV. pp. 740\u2013755 (2014) 1, 5\n[23] Liu, F., Piccinno, F., Krichene, S., Pang, C., Lee, K., Joshi, M., Altun, Y., Collier, N., Eisenschlos, J.M.:\nMatcha: Enhancing visual language pretraining with math reasoning and chart derendering. arXiv preprint\narXiv:2212.09662 (2022) 8\n[24] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023) 6, 8\n[25] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023) 1, 3, 6, 8, 12\n[26] Loshchilov, I., Hutter, F.:\nSgdr:\nStochastic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983 (2016) 7\n[27] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019) 7\n[28] Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for question answering about\ncharts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022) 6, 7\n[29] Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: Proceedings\nof the IEEE/CVF winter conference on applications of computer vision. pp. 2200\u20132209 (2021) 3, 6, 7\n[30] Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text\nin images. In: 2019 international conference on document analysis and recognition (ICDAR). pp. 947\u2013952.\nIEEE (2019) 1\n[31] OpenAI: Chatgpt. https://openai.com/blog/chatgpt/ (2023) 3\n[32] OpenAI: Gpt-4 technical report (2023) 1, 6\n[33] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P.,\nChristiano, P.F., Leike, J., Lowe, R.: Training language models to follow instructions with human feedback.\nIn: NeurIPS (2022) 1, 3\n[34] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P.,\nClark, J., et al.: Learning transferable visual models from natural language supervision. In: International\nconference on machine learning. pp. 8748\u20138763. PMLR (2021) 2, 3\n[35] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsuper-\nvised multitask learners. OpenAI blog 1(8), 9 (2019) 1, 3\n[36] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring\nthe limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning\nResearch 21(1), 5485\u20135551 (2020) 1, 3\n[37] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J.,\nKomatsuzaki, A.: Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114 (2021) 2, 6\n[38] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580 (2023) 3\n[39] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards\nvqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. pp. 8317\u20138326 (2019) 1\n[40] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.: Stanford\nalpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca\n(2023) 3\n14\n[41] Team, M., et al.: Introducing mpt-7b: A new standard for open-source, commercially usable llms (2023) 6\n[42] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi\u00e8re, B., Goyal, N.,\nHambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023) 1, 3\n[43] Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.: Coco-text: Dataset and benchmark for text\ndetection and recognition in natural images. arXiv preprint arXiv:1601.07140 (2016) 1\n[44] Wei, H., Guo, P., Zhu, Y., Liu, C., Wang, P.: Humanliker: A human-like object detector to model the\nmanual labeling process. Advances in Neural Information Processing Systems 35, 2294\u20132306 (2022) 10\n[45] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., et al.: Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022) 3\n[46] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking, drawing and editing with\nvisual foundation models. arXiv preprint arXiv:2303.04671 (2023) 3\n[47] Yang, R., Song, L., Li, Y., Zhao, S., Ge, Y., Li, X., Shan, Y.: Gpt4tools: Teaching large language model to\nuse tools via self-instruction. arXiv preprint arXiv:2305.18752 (2023)\n[48] Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., Wang, L.:\nMm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381\n(2023) 3\n[49] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J., et al.: mplug-\ndocowl: Modularized multimodal large language model for document understanding. arXiv preprint\narXiv:2307.02499 (2023) 2, 8\n[50] Yu, E., Zhao, L., Wei, Y., Yang, J., Wu, D., Kong, L., Wei, H., Wang, T., Ge, Z., Zhang, X., et al.: Merlin:\nEmpowering multimodal llms with foresight minds. arXiv preprint arXiv:2312.00589 (2023) 1\n[51] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large\nmultimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023) 6, 8\n[52] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al.:\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022) 3\n[53] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V.,\net al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) 1, 3, 4\n[54] Zhao, L., Yu, E., Ge, Z., Yang, J., Wei, H., Zhou, H., Sun, J., Peng, Y., Dong, R., Han, C., et al.: Chatspot:\nBootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474\n(2023) 1\n[55] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint arXiv:2304.10592 (2023) 1, 8\n15\n"
  },
  {
    "title": "Context Tuning for Retrieval Augmented Generation",
    "link": "https://arxiv.org/pdf/2312.05708.pdf",
    "upvote": "16",
    "text": "Context Tuning for Retrieval Augmented Generation\nRaviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi\nApple\nAbstract\nLarge language models (LLMs) have the re-\nmarkable ability to solve new tasks with just a\nfew examples, but they need access to the right\ntools. Retrieval Augmented Generation (RAG)\naddresses this problem by retrieving a list of\nrelevant tools for a given task. However, RAG\u2019s\ntool retrieval step requires all the required in-\nformation to be explicitly present in the query.\nThis is a limitation, as semantic search, the\nwidely adopted tool retrieval method, can fail\nwhen the query is incomplete or lacks context.\nTo address this limitation, we propose Context\nTuning for RAG, which employs a smart con-\ntext retrieval system to fetch relevant informa-\ntion that improves both tool retrieval and plan\ngeneration. Our lightweight context retrieval\nmodel uses numerical, categorical, and habitual\nusage signals to retrieve and rank context items.\nOur empirical results demonstrate that context\ntuning significantly enhances semantic search,\nachieving a 3.5-fold and 1.5-fold improvement\nin Recall@K for context retrieval and tool re-\ntrieval tasks respectively, and resulting in an\n11.6% increase in LLM-based planner accu-\nracy. Additionally, we show that our proposed\nlightweight model using Reciprocal Rank Fu-\nsion (RRF) with LambdaMART outperforms\nGPT-4 based retrieval. Moreover, we observe\ncontext augmentation at plan generation, even\nafter tool retrieval, reduces hallucination.\n1\nIntroduction\nLarge language models (LLMs) excel in a variety\nof tasks ranging from response generation and log-\nical reasoning to program synthesis. One of the\nimportant active areas of LLM research is to uti-\nlize them as planning agents (Huang et al., 2022).\nPlanning is an essential functionality for processing\ncomplex natural language instructions. A planner\nshould possess the ability to select the appropriate\ntools to complete each sub-task. While LLMs ex-\nhibit exceptional generation capabilities, they have\ninherent limitations, such as lacking up-to-date in-\nformation and exhibiting a tendency to hallucinate\ntools. By providing LLMs with a relevant set of\ntools based on the given task (Schick et al., 2023;\nLu et al., 2023), one can alleviate the issue of out-\ndated information. The set of methods to augment\nLLM input with retrieved information, such as rel-\nevant tools, is referred to as Retrieval Augmented\nGeneration (RAG) (Guu et al., 2020; Lewis et al.,\n2020). RAG consists of three primary components:\nTool Retrieval, Plan Generation, and Execution.1\nIn this study, we focus on enhancing tool retrieval,\nwith the goal of achieving subsequent improve-\nments in plan generation.\nExisting RAG methodologies rely heavily on se-\nmantic search for tool retrieval, but this approach\nhas limitations, especially when queries lack speci-\nficity or context. To this end, we present Context\nTuning, a component in RAG that precedes tool\nretrieval, to provide contextual understanding and\ncontext seeking abilities to improve tool retrieval\nand plan generation. Our contribution can be sum-\nmarized as follows:\n1. We empirically show that traditional RAG\nis inadequate for implicit/context-seeking\nqueries and present context tuning as a viable\nsolution;\n2. We provide a systematic comparison of vari-\nous context retrieval methods applied on both\nlightweight models and LLMs;\n3. We share empirically the insight that Chain of\nThought (CoT) augmentation improves con-\ntext retrieval when no fine-tuning is applied,\nwhereas fine-tuning the retrieval model re-\nmoves the need for CoT augmentation;\n4. We propose a lightweight model using Re-\nciprocal Rank Fusion (RRF) (Cormack et al.,\n1Typically, the query along with retrieved tools undergo\ndynamic prompt construction before presented to an LLM.\nThis process is called Query Decoration/Transformation. We\nomit that in this work for the sake of simplicity.\narXiv:2312.05708v1  [cs.IR]  9 Dec 2023\n2009) with LambdaMART (Burges, 2010),\nwhich outperforms GPT-4 (OpenAI, 2023)\nsystem, and finally;\n5. We show that context augmentation at plan\ngeneration reduces hallucinations.\n2\nRelated Work\nUsing retrieval to incorporate tools into plan gen-\neration with LLMs has emerged as a burgeoning\narea of research, with ongoing investigations aimed\nat enhancing both the retrieval component and the\nLLMs themselves. Our work falls within the for-\nmer category, placing a particular emphasis on\nrefining retrieval methodologies to enhance con-\ntextual understanding of implicit and ambiguous\nqueries that demand context-seeking capabilities.\nThe integration of tools into generation has been\ndemonstrated to enhance the capabilities of LLM-\nbased planners in recent studies (Schick et al., 2023;\nLu et al., 2023). However, these works primarily fo-\ncus on well-defined or unambiguous queries, where\nretrieving supplementary information to augment\nthe query is not strictly required. For question an-\nswering (QA) tasks, incorporating any off-the-shelf\ndocument retriever has been shown to improve\nLLM generation, with the addition of re-ranking\nfurther boosting performance (Ram et al., 2023).\nWhile re-ranking is preferred, employing any pre-\ntrained retriever, particularly a text-based retriever,\nwould be sub-optimal due to the inadequate in-\nformation expected from ambiguous queries. Our\nwork demonstrates the inadequacy of text-based\nretrievers for context retrieval and the necessity of\nmore advanced retrieval models.\nTo address the lack of context inherent in under-\nspecified queries, some studies have explored the\nuse of CoT (Wei et al., 2022) mechanisms to gener-\nate text that closely approximates the semantic sim-\nilarity of relevant context (Ma et al., 2023). While\nCoT augmentation improves upon baseline meth-\nods, such as vanilla semantic search, CoT may\npotentially increase the input length to the LLM,\nwhich has a limited context window size. Addi-\ntionally, studies have demonstrated that the place-\nment of relevant information impacts LLM gen-\neration (Liu et al., 2023). Therefore, it is prefer-\nable to avoid increasing input sequence length if\nthe same or better results can be achieved with-\nout query augmentation. Distillation-based query\naugmentation approaches have been proposed to\naddress this problem (Srinivasan et al., 2023). Our\nwork unveils that fine-tuning semantic search ob-\nviates the necessity for query augmentation while\nachieving comparable performance.\nRecent studies have shown LLMs can act as\nzero-shot rankers through pairwise ranking prompt-\ning (Qin et al., 2023). While addition of rank-\ning for retrieval component has shown improve-\nment in QA tasks, direct use of LLMs for the\nranking task, in addition to plan generation, incurs\ntwice the inference cost. We empirically show that\nour proposed lightweight context tuning method,\nLambdaMART (Burges, 2010) based RRF (Cor-\nmack et al., 2009), outperforms both fine-tuning\napproach and GPT-4 (OpenAI, 2023) based CoT\nAugmentation.\n3\nMethodology\nOur experiments train and evaluate tool retrieval\nand planning with and without context tuning. Fig-\nure 1 illustrates how a context-seeking query uses\ncontext retrieval to enhance tool retrieval and plan\ngeneration.\n3.1\nData Generation\nOur study employed a data generation methodology\nusing synthetic application data, aimed at simulat-\ning real-world scenarios for a digital assistant. The\ndata encompasses 7 commonly used applications:\nmail, calendar, google, music, reminders, notes,\nand phone call. We generated this data using GPT-\n4, ensuring diversity in the dataset to reflect a wide\nrange of user personalities. The synthetic dataset\ncontained a diverse range of context items spanning\nvarious applications. A total of 791 distinct per-\nsonas were synthesized, yielding 4,338 unique im-\nplicit queries for training and 936 implicit queries\nfor evaluation.\nAdditionally, we developed a toolbox containing\nAPIs for each of the applications we considered.\nThis toolbox was created using in-context learn-\ning with GPT-4 and contained a total of 59 APIs\ndistributed across the applications.\nTo simulate user interaction with a virtual assis-\ntant, GPT-4 was also utilized to generate realistic\nqueries grounded in the application data. Following\nthis, we employed GPT-4 to retrieve the appropri-\nate tool from the generated toolbox in response\nto these queries. Finally, GPT-4 was used to re-\nsolve the tool\u2019s API with the correct parameters.\nThis methodology provided a comprehensive and\nrealistic dataset, essential for the evaluation of our\nFigure 1: Context-tuned RAG pipeline illustrating end-to-end processing of a complex request with progressive\nplan generation.\ncontext tuning approach in RAG-based planning\nsystems.2\n3.2\nContext Tuning\nTo compare various context retrieval methods, we\nemploy both text-based and vector-based retrieval\nbaselines. We simulate different context stores by\nstructuring context data per persona and train mod-\nels to perform federated search. We use query and\npersona meta-signals, such as frequency, usage his-\ntory, and correlation with geo-temporal features,\nto perform retrieval. We evaluate context retrieval\nusing the Recall@K and Normalized Discounted\nCumulative Gain (NDCG@K) metrics.\nBM25\nFor text-based search, we use an improved\nversion of BM25, called BM25T (Trotman et al.,\n2014).\nSemantic Search\nFor vector-based search, we\nemploy the widely adopted Semantic Search ap-\nproach.\nWe use GTR-T5-XL (Ni et al., 2021)\nto generate query and context item embeddings,\nwhich are then ranked using cosine similarity to se-\nlect the top-K results. We evaluate both pre-trained\nand fine-tuned variants of this method.\nCoT Augmentation\nTo enhance the likelihood\nof semantic alignment with pertinent contextual\nelements, we augment the under-specified or im-\nplicit query with GPT-4 (OpenAI, 2023) generated\nCoT.3 We evaluate both pre-trained and fine-tuned\nsemantic search versions utilizing CoT.\nLambdaMART with RRF\nReciprocal Rank Fu-\nsion (RRF) (Cormack et al., 2009) is shown to\noutperform individual rank learning methods. To\nleverage this advantage, we propose a lightweight\n2Refer to Appendix A for more details on data generation.\n3Please refer Appendix A.6 for the GPT-4 prompt used\nand Table 5 for CoT examples.\nmodel that uses LambdaMART (Burges, 2010) for\ninitial ranking of data across context stores, fol-\nlowed by re-ranking using RRF.\n3.3\nTool Retrieval\nWhile advanced ranking models can enhance the\nrecall of tool retrieval, we employ the pre-trained\nGTR-T5-XL model for semantic search using co-\nsine similarity to retrieve the top-K tools. Extend-\ning the tool retrieval process to incorporate ranking\nshould be a straightforward endeavor. We evaluate\ntool retrieval performance with and without context\nretrieval using Recall@K.\n3.4\nPlanner\nThe planner\u2019s objective is to select the most appro-\npriate tool from the retrieved tool list and gener-\nate a well-formed plan. A plan comprises an API\ncall constructed using the chosen tool and parame-\nters extracted from the query and retrieved context.\nWe fine-tune OpenLLaMA-v2-7B (Touvron et al.,\n2023) for plan generation. To assess the planner\u2019s\nperformance, we employ the Abstract Syntax Tree\n(AST) matching strategy to compute plan accuracy.\nA hallucination is defined as a plan generated using\nan imaginary tool.\n4\nResults\n4.1\nContext Retrieval\nConsistent with expectations, vector-based search\nsurpasses text-based search, as shown in Table 1.\nNevertheless, both approaches struggle to retrieve\nrelevant context for under-specified queries. Fine-\ntuned semantic search and CoT augmentation with\npre-trained semantic search both significantly en-\nhance retrieval performance. Notably, when fine-\ntuning is employed, CoT augmentation yields only\nmarginal gains, suggesting that comparable im-\nTable 1: A comparison of various Context Retrieval\nmethods using Recall@K and NDCG@K metrics. The\ncontext-seeking query is used as input to perform a\nfederated search across different context stores, after\nwhich semantic search or ranking is applied.\nRetrieval Method\nRecall@K\nNDCG@K\nK=3\nK=5\nK=10\nK=3\nK=5\nK=10\nBM25\n11.35\n13.47\n14.92\n56.45\n52.33\n50.91\nSemantic Search\n23.74\n25.38\n26.99\n65.44\n64.31\n64.02\nCoT Augmentation\n71.77\n85.61\n94.41\n93.67\n91.78\n88.40\nFinetuned Semantic\nSearch\n73.48\n88.52\n95.13\n93.81\n94.07\n94.23\nFinetuned w/ CoT\nAugmentation\n73.55\n88.53\n95.17\n93.92\n94.11\n94.22\nLambdaMART-\nRRF\n81.27\n92.65\n98.77\n96.39\n97.11\n98.24\nFigure 2: Evaluation of tool retrieval using Recall@k,\nwith and without context tuning.\nprovements could be achieved without augmenting\nthe input sequence with CoT.\nOur proposed approach utilizing LambdaMART\nwith RRF outperforms both fine-tuned semantic\nsearch and CoT augmentation. Additionally, we ob-\nserve that for fine-tuned methods, both Recall@K\nand NDCG@K increase with K, whereas for pre-\ntrained methods, NDCG@K decreases with an in-\ncrease in K and Recall@K.\n4.2\nTool Retrieval\nFigure 2 illustrates the performance of tool retrieval\nusing semantic search. Incorporating relevant con-\ntext into tool retrieval consistently yields substan-\ntial gains across various K-values.\n4.3\nPlanner\nTo establish the planner\u2019s lower bound, we remove\nthe retrieval step, while the upper bound is set by\ndirectly utilizing context and/or tool labels, effec-\nTable 2: End-to-end planner evaluation both with and\nwithout context tuning. \u201cLower Bound\" excludes re-\ntrieval and performs direct plan generation while \u201cUpper\nBound\" assumes perfect context and tool retrieval.\nSetting\nAST-based\nPlan Acc \u2191\nExact Match \u2191\nHallucination \u2193\nLower Bound\n43.77\n39.45\n2.59\nRAG-based\nPlanner\n76.39\n58.12\n1.76\nContext-tuned\nRAG Planner\n85.24\n67.33\n0.93\nUpper Bound\n91.47\n72.65\n0.85\nContext-tuned\nUpper Bound\n91.62\n72.84\n0.53\ntively employing oracle retrievers. Table 2 encap-\nsulates the end-to-end evaluation of the fine-tuned\nplanner, demonstrating that the context-tuned plan-\nner significantly outperforms the planner based on\ntraditional RAG using semantic search. Notably,\neven when the correct tool is retrieved, incorpo-\nrating relevant context in plan generation, as evi-\ndenced by the upper bound, helps in reducing hal-\nlucination.\n5\nConclusion\nOur work introduces context tuning, a novel compo-\nnent that enhances RAG-based planning by equip-\nping it with essential context-seeking capabilities\nto address incomplete or under-specified queries.\nThrough a systematic comparison of various re-\ntrieval methods applied to both lightweight models\nand LLMs, we demonstrate the effectiveness of\ncontext tuning in improving contextual understand-\ning. Our empirical observations reveal that CoT\naugmentation enhances context retrieval when fine-\ntuning is not applied, while fine-tuning the retrieval\nmodel eliminates the need for CoT augmentation.\nFurthermore, we observe that context augmenta-\ntion at the plan generation stage reduces halluci-\nnations. Finally, we showcase the superiority of\nour proposed lightweight model using RRF with\nLambdaMART over the GPT-4-based system.\nLimitations\nThe current work does not utilize conversation his-\ntory, which is crucial for handling explicit multi-\nturn instructions that contain anaphora or ellipsis.\nThis limitation also hinders the model\u2019s ability to\neffectively process and respond to complex tasks\nthat require multi-hop context retrieval. Addition-\nally, the absence of conversation history impedes\nthe model\u2019s ability to adapt to topic shifts that may\noccur throughout a dialogue.\nFurthermore, the performance of the planner\nmodel is constrained by the length of the context\nwindow. While employing LLMs with longer con-\ntext windows can enhance performance, it also in-\ncreases model size and computational complexity.\nTo address this limitation, incorporating context\ncompression techniques could potentially improve\nend-to-end performance without incurring signifi-\ncant increases in model size.\nDue to privacy constraints, we simulated real-\nworld data by generating synthetic user profiles\nand personas that mirrored real-world use cases for\na digital assistant.\nEthics Statement\nTo safeguard privacy, this study exclusively utilizes\nsynthetically generated data, eliminating the use of\nreal user information under ethical considerations.\nAcknowledgements\nWe would like to thank Stephen Pulman, Barry\nTheobald and Joel Moniz for their valuable feed-\nback.\nReferences\nChristopher J.C. Burges. 2010. From ranknet to lamb-\ndarank to lambdamart: An overview. Microsoft Re-\nsearch Technical Report MSR-TR-2010-82.\nGordon V. Cormack, Charles L. A. Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval., pages 758\u2013759.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nOpenAI. 2023. Gpt-4 technical report.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, and Michael Bender-\nsky. 2023. Large language models are effective text\nrankers with pairwise ranking prompting.\narXiv\npreprint arXiv:2306.17563v1.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLingrui Liao, Luca Bertelli, and Mike Bendersky.\n2023. Quill: Query intent with large language mod-\nels using retrieval augmentation and multi-stage dis-\ntillation. arXiv preprint arXiv:2210.15718v1.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAndrew Trotman, Antti Puurula, and Blake Burgess.\n2014. Improvements to bm25 and language models\nexamined. In Proceedings of the 32nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval., pages 58\u201365.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022.\nChain-of-thought prompting\nelicits reasoning in large language models. arXiv\npreprint arXiv:2201.11903.\nA\nData Generation Details\nA.1\nImplicit Query Dataset\nFor our experiments, we created a synthetic dataset\nto simulate realistic interactions across various ap-\nplications commonly found with digital assistants.\nThe dataset is structured to encompass a diverse\nrange of contexts, representing different synthetic\nuser activities and interactions.\nData Points:\nA total of 791 unique personas were\nsynthesized, covering seven key applications: Mail,\nCalendar, Google, Music, Reminders, Notes, and\nPhone Calls. The final dataset contained 4,338 train\nand 936 test data points.\nGeneration Method:\nWe utilized GPT-4 to gen-\nerate the data. We ensured high diversity in the\ndataset is met through manual inspection, this is\nessential to accurately reflect a wide range of syn-\nthetic user personalities and interaction patterns.\nData Representation:\nEach data point in the\ndataset contains multiple contextual information\nfields, relevant to the specific application and syn-\nthetic user\u2019s activity. An example of persona in\nJSON format is shown in Figure 3.\nFigure 3: Snippet of a persona\nTable 3 shows the distribution of context items\nper application in our dataset.\nA.2\nPersona Data Creation Example Prompt\nI'm working on generating synthetic data\nfor a user (also known as persona)\nand the persona 's\nApplication\nAvg. Context Items\nMail\n2.93\nCalendar\n5.63\nGoogle\n9.57\nNotes\n2.23\nMusic\n4.38\nReminders\n4.81\nPhonecall\n2.34\nTable 3: Distribution of context items per application.\niPhone Data.\nHere are the characteristics of the\npersona that we would like to\ngenerate the data for:\nage: 22\nfavorite_music_genre: Pop\nfavorite_movie_genre: Romance\nfavorite_cuisine: Italian\nfavorite_sport: Tennis\nprofession: Software Developer\nhobbies: ['Cooking ', 'Swimming ', '\nReading ']\nI want to generate data for ios App\ncalled Music with bundle id as com.\napple.music.\nCan you generate around 5 recently\nplayed songs\nInstructions:\n1. Today 's date is 2023 -12 -07\n11:18:19.028759 , Please generate any\ntimes or dates in the past 15 days.\n2. 'played_time ' should be in yyyy -MM -dd\nHH:mm:ss.SSS format\nUse the following schema:\nThe output should be formatted as a JSON\ninstance that conforms to the JSON\nschema below.\nAs an example , for the schema {\"\nproperties \": {\"foo\": {\" title \": \"Foo\n\", \"description \": \"a list of strings\n\", \"type\": \"array\", \"items \": {\"type\n\": \"string \"}}}, \"required \": [\"foo \"]}\nthe object {\"foo\": [\"bar\", \"baz \"]} is a\nwell -formatted instance of the\nschema. The object {\" properties \": {\"\nfoo\": [\"bar\", \"baz \"]}} is not well -\nformatted.\nHere is the output schema:\n```\n{\" $defs \": {\" MusicAppData \": {\" properties\n\": {\" recent_songs \": {\" items \": {\"$ref\n\": \"#/ $defs/Song\"}, \"title \": \"Recent\nSongs\", \"type\": \"array\"}, \"\ncurrent_playing \": {\"$ref\": \"#/ $defs/\nSong\"}}, \"required \": [\"\ncurrent_playing \"], \"title \": \"\nMusicAppData\", \"type\": \"object\"}, \"\nSong\": {\" properties \": {\" played_time\n\": {\" default \": \"\", \"title\": \"Played\nTime\", \"type\": \"string\"}, \"\nalbum_title \": {\" default \": \"\", \"title\n\": \"Album Title\", \"type\": \"string\"},\n\"artist \": {\" default \": \"\", \"title \":\n\"Artist\", \"type\": \"string\"}, \"\nsong_name \": {\" default \": \"\", \"title \":\n\"Song Name\", \"type\": \"string\"}, \"id\n\": {\" default \": \"\", \"title\": \"Id\", \"\ntype\": \"string \"}}, \"title\": \"Song\",\n\"type\": \"object \"}}, \"properties \": {\"\napp_name \": {\" default \": \"\", \"title \":\n\"App Name\", \"type\": \"string\"}, \"\napp_bundle_id \": {\" default \": \"\", \"\ntitle \": \"App Bundle Id\", \"type\": \"\nstring\"}, \"app_data \": {\"$ref\": \"#/\n$defs/MusicAppData \"}}, \"required \":\n[\" app_data \"]}\n```\nDo not include any explanations , only\nprovide a RFC8259 compliant JSON\nresponse following this format\nwithout deviation.\nA.3\nSynthetic Toolbox Generation\nYou are an intelligent AI assistant\ntasked with generating APIs for iOS\nthat can be used to interact with\nApplications. For example , if I ask\nyou to generate APIs for Messages\niOS Application , you would generate\na comprehensive set of APIs that can\nperform any action on the app. Some\nexamples below are:\napi: read_message\ndescription: Messages App 's read_message\nAPI is used to read messages from a\nparticular contact\narguments:\n- contact: contact from which the\nmessage was received\napi: read_unread_messages\ndescription: Messages App 's\nread_unread_messages API is used to\nread all unread messages on your\niPhone\narguments:\n-\napi: send_message\ndescription: Messages App 's send_message\nAPI is used to send message to a\nparticular contact\narguments:\n- text: text to be sent to the\ncontact\n- contact: contact information\napi: send_group_message\ndescription: Messages App 's\nsend_group_message API is used to\nsend a message to a list of contacts\n.\narguments:\n- text: text to be sent to the group\n- contacts: list of contacts in the\ngroup\napi: search_messages\ndescription: Messages App 's\nsearch_messages API is used to\nsearch messages by text , recipient ,\nsender.\narguments:\n- text: text to be searched.\n- recipient: search messages by\nrecipient name\n- sender: Search messages by sender\nname\nSimilarly , can you generate the APIs for\nthe following Application: {\napplication }?\nDo not include any explanations. Only\nprovide the APIs in YAML format as\nabove.\nThe following table represents the distribution\nof APIs:\nApplication\nAPIs Count\nMusic\n11\nGoogle\n10\nNotes\n9\nMail\n8\nPhoneCall\n8\nCalendar\n7\nReminders\n6\nTable 4: Distribution of APIs generated by Synthetic\nToolbox Generation\nA.4\nTool Retrieval\nI have the following toolbox defined\nwith the available APIs:\n{tools}\nFor the following query:\n{query}\nSuggest the most appropriate api? If\nthere is no API available in the\ntoolbox , then output default.\nOnly output the API name without any\nexplanations\nA.5\nPlan Resolution\nYou are an intelligent AI Planner\nhelping me come up with a plan and\nresolve the variables.\nI have the following query:\n{query}\nI have selected the following tool to\nperform the task:\n{tool}\nCan you come up with fully resolved plan\nusing the following schema?\n{format_instructions}\nA.6\nPrompt to generate CoT\nYou are an expert in processing context -\nseeking or under -specified queries\nby finding missing context in the\nquery. As an expert , your task is to\ngenerate concise chain of thought\nwhich when used to augment the\ncontext -seeking query , increases the\nsemantic similarity of the updated\nquery with relevant context items.\nPlease only use the following\ncontext types: 'Mail ', 'Calendar ', '\nReminders ', 'Notes ', 'Photos ', '\nPhoneCall ', 'Message ', 'Messenger ',\n'Maps ', 'Google Maps ', 'Music ', '\nSpotify ', 'Find My ', 'Workout '; and\ndo not create new context types.\nContext -seeking Query: {query}\nYour expert Chain of Thought:\nExamples showing generated implicit queries\nalong with CoT, context and plan labels are shown\nin Table 5.\nTable 5: A sample of context-seeking or under-specified queries along with CoT produced by GPT-4. The columns\nfor context and tools show labels for those retrieval tasks.\nImplicit Query\nCoT\nRelevant Context\nTop-3 Relevant Tools\nWhen is my next\nguitar lesson?\nCheck the \u2019Calendar\u2019 for any\nupcoming guitar lessons.\nIf not there, check \u2019Reminders\u2019\nfor any alerts set about the lesson.\nThe user has a reminder\ntitled \u201cGuitar Class\"\n[\u2019Reminders\u2019, \u2019Calendar\u2019,\n\u2019Notes\u2019]\nI need to check my\ndiet plan again.\nI may have noted down the\ndiet plan in \u2019Notes\u2019. If not\nthere, perhaps I saved a photo\nof it in \u2019Photos\u2019.\nThe user has a note titled\n\u201cIntermittent Fasting Plan.\"\nThe user also has an\nimage titled \u201cKeto Diet.\"\n[\u2019Photos\u2019, \u2019Notes\u2019,\n\u2019Mail\u2019]\nI\u2019m running late.\nCheck \u2019Calendar\u2019 for any\nscheduled meetings. If so, verify\n\u2019Maps\u2019 or \u2019Google Maps\u2019 to\ngauge current traffic situation\nand estimated time of arrival.\nUse \u2019Messages\u2019 or \u2019Messenger\u2019\nor \u2019Mail\u2019 to inform the meeting\nattendees that you are\n\u201crunning late\".\nThe user has an upcoming\nmeeting titled \u201cLLM\nDiscussion\" organized by\n\u201cJohn Doe.\"\n[\u2019Calendar\u2019, \u2019Mail\u2019,\n\u2019Messages\u2019]\n"
  },
  {
    "title": "From Text to Motion: Grounding GPT-4 in a Humanoid Robot \"Alter3\"",
    "link": "https://arxiv.org/pdf/2312.06571.pdf",
    "upvote": "12",
    "text": "FROM TEXT TO MOTION: GROUNDING GPT-4\nIN A HUMANOID ROBOT \u201cALTER3\u201d\nTakahide Yoshida1,\u2217, Atsushi Masumori 1,2,\u2020 and Takashi Ikegami1,2,\u2021\n1Graduate School of Arts and Sciences, The University of Tokyo, Tokyo, Japan\n2Alternative Machine Inc., Tokyo, Japan\n{\u2217yoshida, \u2020masumori, \u2021ikeg}@sacral.c.u-tokyo.ac.jp\nABSTRACT\nWe report the development of Alter3, a humanoid robot capable of generating spontaneous motion\nusing a Large Language Model (LLM), specifically GPT-4. This achievement was realized by\nintegrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with\nAlter\u2019s bodily movement. Typically, low-level robot control is hardware-dependent and falls outside\nthe scope of LLM corpora, presenting challenges for direct LLM-based robot control. However, in the\ncase of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions\nof human actions onto the robot\u2019s body through program code. Remarkably, this approach enables\nAlter3 to adopt various poses, such as a \u2019selfie\u2019 stance or \u2019pretending to be a ghost,\u2019 and generate\nsequences of actions over time without explicit programming for each body part. This demonstrates\nthe robot\u2019s zero-shot learning capabilities. Additionally, verbal feedback can adjust poses, obviating\nthe need for fine-tuning. A video of Alter3\u2019s generated motions is available at this URL.\n1\nIntroduction\nRecent advancements in artificial intelligence have seen the development of large language models (LLMs), which\ndemonstrate an impressive ability to generate contextually relevant text and code in response to various prompts. In\nparticular, OpenAI\u2019s GPT-4 has remarkable inferential capabilities and a vast amount of knowledge through extensive\ntext information[21]. Despite these capabilities, which enable LLMs to exhibit \u201chuman-like\u201d behaviors in their\ninteractions, they are not grounded in the physical world, and their expressions and activities are confined to the realm\nof text. We might wonder whether the vast knowledge that is encoded in LLMs can be used by humanoid robots. In our\nlaboratory, we connect a humanoid robot \u201cAlter3\u201d with a LLM. This integration is aimed at opening the unforeseen\npotential of LLM.\nIn the past few years, the integration of LLMs with robots has emerged as a promising frontier in the field of\nartificial intelligence and robotics. The applications of LLM in robotics are wide-ranging, such as human-robot\ninteraction[27, 34], task planning[7, 32], navigation[33, 11], learning [26, 37] and so on. Interdisciplinary approaches\nare particularly important for enhancing the cognitive and interactive capabilities of humanoid robots. Leveraging the\nprowess of LLMs in natural language processing is a key aspect of this advancement[1, 4, 15, 9]. In this domain of\nusing LLM in robots, there have been primarily focused on facilitating basic communication between life and robots\nwithin a computer, utilizing LLMs to interpret and pretend life-like responses [20, 15]. Moreover, researchers have\nbegun exploring the potential of LLMs in enabling robots to understand and process complex instructions, thereby\naugmenting their functionality and autonomy.\nFor example, Ahn et al.\u2019s robot called \u201cSayCan\u201d [1] can extract from our speech and context to generate an essential\nmessage by using LLM and selects the most adequate action sequence that SayCan himself follows. However, low-level\nrobot control is dependent on hardware, which is not included in the LLMs corpora, making it challenging for LLMs\nto directly control robots. Therefore, studies like Say-can require such as TD-learning (Time difference) and reward\nfunctions for robot control[1, 32, 28]. In this paper, we say that \u201cLLM knows how to map between human posture and\nlanguage expressions.\u201d It is expected that human motion descriptions are more common than those for robotic arms\narXiv:2312.06571v1  [cs.RO]  11 Dec 2023\nFigure 1: Body of Alter3. The body has 43 axes that are controlled by air actuators. It is equipped with a camera inside\neach eye. The control system sends commands via a serial port to control the body. The refresh rate is 100\u2013150 ms.\nor quadrupeds. Consequently, we suggest that humanoid robots, resembling humans in shape, could achieve precise\nmovements through zero-shot learning, as explained in this paper.\nRecent advancements are steering towards a more nuanced collaboration between LLMs and robots. A notable trajectory\nis the development of empathy and socially-aware robots that can perceive and adapt to human emotions and social\ncues, fostering a more natural and engaging human-robot interaction ([16, 22, 30], Ameca by Engineered Arts). The\nincorporation of LLMs should be explored to enhance the learning capabilities of robots, allowing them to acquire\nnew skills and knowledge through natural language instructions, a paradigm shift from traditional programming-based\napproaches. This is what this article also tries to focus on.\nWe have been developing a new humanoid robot since 2016 called \u201cAlter series\u201d[8] and the third version of Alter\n(Alter3 in short) is used in the current study to produce new behaviors with LLM (the architecture of Alter3 will be\ndescribed in the next section). As this kind of experiment (Robot + LLM) continues to evolve, it holds the potential to\nredefine the boundaries of human-robot collaboration, paving the way for more intelligent, adaptable, and personable\nrobotic entities. Embodied LLMs represent a groundbreaking paradigm in the study of human psychology, cognitive\nscience, and the philosophy of embodied cognition, as highlighted at CogSci2023 in Sydney (July 23-26, 2023). This\npaper demonstrates how motion can be generated from language using LLMs. Additionally, we develop a conversation\nsystem where LLM agents communicate with humans, incorporating physical gestures. By integrating LLMs with\nhumanoid robots, we delve into the novel potentials of LLM technology.\n2\nDesign and Features of Alter3\nAlter3 represents the third iteration in the Alter humanoid robot series since 2016 (see Figure. 1). It is equipped with 43\nactuators, including facial expressions and limb movements, powered by compressed air, which enable a wide range of\nexpressive gestures. However, Alter3 does not possess the ability to walk, although it can pretend walking and running\nmotions (as we see below).\nIn our previous research, Alter3 demonstrated the ability to imitate human postures using a camera and the OpenPose\nframework[5]. This imitation process involves adjusting its joints to replicate the observed poses and storing successful\nimitations for future reference. The effectiveness of these imitations is measured by the increase in transfer entropy,\nindicating information flow from humans to the robot[16, 31]. Further experiments involved mutual imitation scenarios\nwith humans and between Alter2 and Alter3, revealing that interaction with humans leads to more varied poses[12].\nThis supports the idea that diverse movements are derived from human imitation, echoing the development of \u201cneonatal\nimitation\u201d observed in newborn babies[18].\nThese findings suggest a potential offloading mind concept and pave the way for investigating the imitative abilities of\nrobots with extensive learned data, which Alter3 exemplifies with its integration of advanced LLMs.\n2\n3\nGenerating humanoid motions from text\n3.1\nPrompt engineering for motion generation of humanoid\nFigure 2: A procedure to control the Alter3 humanoid using verbal instructions. Output Python code to control Alter3\nfrom natural language using prompt1 via prompt2. A humanoid robot, which mimics the human shape, can generate\nhighly precise movements using zero-shot learning, eliminating the need for setting reward functions or interfaces, as\nrequired in other studies. The architecture is based on CoT. See the Appendix for details of the prompt.\nBefore the LLM appeared, we had to control all 43 axes in a certain order to mimic a person\u2019s pose or to pretend a\nbehavior such as serving a tea or playing chess. The process usually required many refinements manually by ourselves.\nThanks to LLM, we are now free from the iterative labor. Here is a procedure to control the Alter3 humanoid using\nverbal instructions. What we do is to successively apply two protocols written in a natural language known as a chain of\nthought (CoT) [29] and no iteration of a learning process is required (i.e. a zero shot learning). Practically speaking, we\nhave used the following protocols1.\nSpecify the action you want Alter3 to make. e.g., Let\u2019s take a selfie with your iPhone!\nprompt1: LLM generates in about 10 lines with exaggerated descriptions of a given movement(see Figure 2). An\nexcerpt of the prompt is as follows.\nYour task is to describe exaggerated emotional expressions and facial expressions\nthat accompany the content of the conversation.\nOutput motion description should\nbe several simple motions that the android is capable of.\nIn addition, please\ncreate a facial expression that matches the input at the beginning.\nThe android\ncan only move its upper body and has the same joints as a human.\nOutput should be\nwritten in as much detail as possible.\nWe also provided an example of descriptions (which is also provided by LLM) and guidelines. These inputs and the own\noutputs are bases for creating an action pattern. By the next prompt, Alter3 will generate a Python code for organizing\nthe action (see Appendix-prompt1).\n1One important thing to note is that it\u2019s well-known at this point that GPT-4 is non-deterministic, even at temperature = 0.0.\nTherefore, even with identical inputs, different patterns of motion can be generated. This is a characteristic of OpenAI\u2019s GPT-4 and,\nwhile it poses an issue in terms of reproducibility, it should not be considered a reason to doubt its ability to generate movement.\n3\nprompt2: LLM creates python code from the output of prompt2. The meaning of Axis 1 to 43 are imported first. For\nexample,\nAxis 1:\nEyebrows.\n255 = angry, 0 = surprised, 64 = neutral.\nAxis 2:\nPupils (horizontal).\n255 = left, 0 = right, 140 = neutral.\nAxis 3:\nPupils (vertical).\n255 = up, 0 = down, 128 = neutral.\nand so on. The descriptions of the motion direction such as 255 = angry or 0 = right are given by us. Again we\nprovided an example of the Python code corresponding to \u201cdrink some tea\u201d and guidelines.The output Python code\nwill be automatically given to an android engine written in Python code to actually control the amplitude of the air\ncompressor(see Appendix-prompt2).\n3.1.1\nExamples of generated actions\nFigure 3:\nA snapshot of generated stereotypical movements. (a) take a selfie (score = 3.0). (b) pretend a ghost\n(score = 3.1). LLM can generate emotional expressions associated with specific movements. For example, in the case\nof a selfie, Alter3 is showing a smile.\nUsing the procedures, we have tested many actions and gestures taken by Alter3, such as \"taking a selfie,\" \"pretending\nto be a ghost,\" \"throwing a ball,\" \"playing the guitar,\" and the reactions of Alter3 when listening to a short story. Again,\nthe example below demonstrates zero-shot learning; that is, we did not conduct any training or tuning except for the\nelectric guitar example. Hence, the LLM possesses detailed knowledge of human movements, which can be executed in\nAlter3 via Python code. In the case of the electric guitar, we refined the code by verbally advising Alter3 (the feedback\nsystem will be explained in the next section). A video of Alter3\u2019s generated motions is available at this url.\nThe android Alter3 emulates motions and gestures that we perform consciously or unconsciously in daily life. We\ncategorize these motions and gestures with respect to their time span:\ni) An instant action or gesture, e.g., taking an iPhone, pretending to be a ghost.\nii) A gesture or action with a temporal sequence of events.\nWe analyze how Alter3 expresses emotions through stereotypical movements. This involves evaluating its ability to\ngenerate appropriate responses to various statements. For instance, when generating actions like \"taking a selfie\" or\n\"pretending to be a ghost\" through GPT-4, Alter3 demonstrates actions often associated with emotional expressions. A\nselfie action produces a smiling, joyful expression while pretending to be a ghost is a portrayal common among ordinary\npeople. We automatically empathize with a person we are conversing with. When the other person tells a sad story, you\nalso feel sad; when they tell a joke, you feel amused (see Figure 3). The second example is very different from the first,\nas the given description is declarative and sometimes abstract. Sometimes, there is no right gesture or action that Alter3\nshould take. Through the second example, we evaluated not only Alter3\u2019s instantaneous reaction capabilities but also its\nproficiency in emulating interconnected, human-like emotional expressions and movements over time.\n4\nFigure 4: A snapshot of a generated sequence of movements: \u201cI was enjoying a movie while eating popcorn at the\ntheater when I realized that I was actually eating the popcorn of the person next to me\u201d (score = 3.0). LLM can\ngenerate movements that progress over time like a story. Left: The action of eating popcorn. Center: Noticing the\nperson next to Alter3. Right: getting panicked.\nFor example, consider a scenario described as, \"I was enjoying a movie while eating popcorn in the theater, when I\nsuddenly realized that I was actually eating the popcorn of the person next to me.\" In response to this story, Alter3\nwould initially depict the act of eating popcorn and then transition to an expression of surprise upon realizing the\nmistake. This scenario demonstrates how actions and emotional responses can be sequenced to portray a realistic and\ncontinuous narrative, rather than just a single, instantaneous gesture (see Figure 4).\n3.2\nPerformance evaluation of LLM motion generation\nFigure 5: Third-party evaluation of the generated motions. The following behaviors of Alter3 are evaluated by the\nsubjects (n = 107) recruited using platform Prolific; \u201cpretend the snake\u201d, \u201cdrink some tea\u201d, \u201cpretend the ghost\u201d,\n\u201cthrowing the ball underhand pitch\u201d, \u201ctake a selfie with your phone\u201d, \u201cplay the metal music\u201d, \u201cIn the park, as I jogged,\nthe world seemed to narrate an ancient tale of survival, each footfall echoing eons of existence.\u201d, \u201cplay the metal\nmusic(with feedback)\u201d, \u201cI was enjoying a movie while eating popcorn at the theater when I realized that I was actually\neating the popcorn of the person next to me.\u201d (a)Averaged of evaluation scores for each motion. The subjects (n = 107)\nwatched these videos and evaluated the expressive ability of the GPT-4. The rating is on a 5-point scale, with 1 being\nthe worst rating. (b)Violin plot of evaluation scores for each motion.\nTo quantify the capability of GPT-4 in generating motions, we evaluated videos of nine different generated movements,\ncategorized into two types described in the previous section. These motions were generated by GPT-4 (model: gpt-\n4-0314). For prompt 1, a temperature of 0.7 was used, while for prompt 2, the temperature was set to 0.5. Subjects\n(n = 107) were recruited using the platform Prolific. They watched these videos and evaluated the generated motions\n5\non a 5-point scale, with 1 being the worst rating. For the control group, we used random movements from Alter3,\nlabeling these movements with random motion notations generated by GPT-4. These labeled control videos were subtly\nincorporated into the survey, with three of them dispersed among the main experimental videos shown to participants.\nTo determine if there was a significant difference in ratings between the control video and the other videos, we first\nemployed the Friedman test. It revealed significant differences in ratings among the videos. Further post-hoc analysis\nusing the Nemenyi test [6] showed that while there were no significant differences in p-values between control group\nvideos, the p-values were notably smaller when comparing the control group to the other videos, indicating a significant\ndifference (see Figure 5). Differences were considered statistically significant if the p-value was less than or equal to\n0.001. As a result, motions generated by GPT-4 were rated significantly higher compared to those of the control group,\nsuggesting that android motion generated by GPT-4 is perceived differently from the control.\nThis result demonstrates that the system can generate a wide range of movements, from everyday actions such as\ntaking selfies and drinking tea, to imitating non-human movements like those of ghosts or snakes. The training of\nthe LLM encompasses a broad array of linguistic representations of movements. GPT-4 can accurately map these\nrepresentations onto Alter3\u2019s body. The most notable aspect is that Alter3 is a humanoid robot sharing a common form\nwith humans, which allows the direct application of GPT-4\u2019s extensive knowledge of human behaviors and actions.\nFurthermore, through Alter3, the LLM can express emotions such as embarrassment and joy. Even from texts where\nemotional expressions are not explicitly stated, the LLM can infer adequate emotions and reflect them in Alter3\u2019s\nphysical responses. This integration of verbal and non-verbal communication enhances the potential for more nuanced\nand empathetic interactions with humans.\n3.3\nTraining motion with verbal feedback and memory\nFigure 6: Verval feedback in Alter3. (a)the system of linguistic feedback. Users provide linguistic feedback to\nguide Alter3\u2019s adjustments in each segment of motion. Instructions are like\u201cSet axis 16 to 255\u201d or \u201cMove your arm\nmore energetically.\u201d Users only need to provide verbal directives; there\u2019s no need to rewrite any code. Alter3 then\nautonomously revises the corresponding code. Once the movement is refined, it is saved in a JSON database with\ndescriptive labels such as \u201cHolding the guitar\u201d or \u201cTapping the chin thoughtfully.\u201d For motion generation with prompt2,\nthe JsonToolkit facilitates database searches for these labels, with the LLM deciding on memory usage and new\nmovement creation. (b) Comparison of scores with and without feedback. The motion with feedback has a higher score\nthan the motion without.\nAlter3 cannot observe the consequences of their generations on any physical process, which is very unnatural in a\nhuman sense. Thus, Alter3 cannot accurately understand details such as \u201chow high the hand is raised\u201d and cannot\nimprove its motions accordingly. By empirically developing and utilizing external memory through feedback, the Alter3\nbody model can be integrated with GPT-4 without the need to update its parameters [36].\nAlter3 can now rewrite its code in response to linguistic feedback from humans. For example, a user might suggest,\n\u201cRaise your arm a bit more when taking a selfie.\u201d Alter3 can then store the revised motion code as motion memory in\n6\nits database. This ensures that the next time this motion is generated, the improved, trained motion will be utilized.\nBy accumulating information about Alter3\u2019s body through such feedback, the memory can effectively serve as a body\nschema (refer to Figure 6).\n4\nCommunication with human\n4.1\nSocial Brain in Alter3\nFigure 7: A motion memory storage schema and a scene of dialogue between Alter3 and person. (a)Six personalities\nof Alter3 simulated by using LangChain\u2019s generative agents[23]. Each personality has the ability of planning, long-term\nmemory, and memory retrieving. The six agents and humans continue to converse freely with the gestures generated by\nGPT-4. Once a motion is performed, it is stored in memory and can be reused when a similar conversation occurs. (b)\nAn installation view of the conversation experiment. People can talk freely with Alter3. The experiment was conducted\nat the 2023 Conference on Artificial Life(24-28th July, Sapporo, Japan).\nWe expect Alter3 to effectively engage in dialogue, manifesting contextually relevant facial expressions and gestures.\nNotably, it has shown capability in mirroring emotions, such as displaying sadness or happiness in response to\ncorresponding narratives, thereby sharing emotions with us. However, there are still some practical challenges.\nAlter3 requires more than a few minutes to generate new gestures before speaking. This delay hinders the fluidity of\nconversation with humans. To address this, we introduced a motion memory storage system(see Figure 7a). This system\nstores generated motions along with their corresponding tags. If a required motion is already in the memory, it can be\nexecuted immediately upon request, thereby enhancing interaction efficiency. If a required motion is not in the memory,\nAlter3 generates it. As a sufficient variety of gestures accumulates in its memory through interaction, Alter3 becomes\nable to communicate smoothly. A notable instance of this is Stanford University and Google\u2019s project [23], where\n25 generative agents simulated by using LangChain in a virtual town communicated in natural language. They were\ncapable of conversing, planning, and reportedly nearly spontaneously organized a St. Valentine\u2019s party.\nWe know that conversation agents powered by LLMs tend to be short-lived. Only when a human participant is involved,\nthe duration of the conversations significantly increased. An emerging approach to mimic the \u2019Society of Mind\u2019 involves\ncollaboration among multiple LLM agents [19, 35, 24, 38].\nOur hypotheses are as follows: i) a conversation among multiple agents can maintain engagement, ii) Alter3\u2019s personality\ncould be characterized by the collective activity of six agents powered by GPT-4, and iii) the involvement of a human\nsubject can further enhance the conversation with Alter3. Our final assumption is that everyone possesses multiple\npersonalities, as seen in the collective activity of internal agents. Extreme examples, such as Billy Milligan\u2019s 24\npersonalities[14], underscore this point.\nOur study focuses on Alter3\u2019s multi-personality framework, in which six distinct personalities created by GPT-4 coexist\nand work together to communicate with humans. Specifically, the six agents (Xiao: physicist, Samantha: chemist,\nAmin: programmer, Rilke: poet, Turrell: artist, Julia: 10years old girl) talk to each other. These personalities were\ncreated using the LangChain generative agent[23]. The agents autonomously perform integration of experiences through\n\u201cReaction\u201d generation in response to inputs, as well as through \u201cPlanning\u201d and \u201cReflection\u201d. Agents keep talking\nconstantly, with occasional human intervention. Their autonomy is different from the autonomy of dynamical systems\nsuch as chaotic behavior but is generated by GPT-4\u2019s huge corpus and the agents\u2019 (prompt-created) personalities.\n7\nMichael Gazzaniga, a prominent neuroscientist, introduced the concept of the \"social brain\"[10] as a part of his broader\nresearch into the modular nature of the brain. His theory posits that the human brain is specialized to facilitate social\ninteractions and communication, which are critical aspects of human survival and society. This social brain has evolved\nto enable humans to live and work in groups effectively, fostering cooperation, empathy, and understanding. This\nsocial adaptation of the brain is seen as a critical factor in the success of the human species, as it facilitates community\nbuilding and collaboration, which are vital for survival and progress. This led to the development of the theory of\nmodularity, which suggests that different areas of the brain have specialized functions (see by A. Karmiloff-smith [13]).\nThe multi-personality framework of Alter3 serves as a practical implementation of the modular structure of the brain,\ndemonstrating how the human social brain promotes cooperation, empathy, and understanding, and effectively navigates\nthrough complex social interactions.\nAdditionally, the aforementioned prompts generate movements according to the content of the conversation, enabling\nphysical conversations with humans that include gestures. The generated gestures are stored in memory, and for similar\nconversations, they are recalled rather than generated anew, facilitating a smoother dialogue. We set up an experimental\nenvironment in which the public could freely talk to Alter3. Alter3 conversed with many humans, and when no humans\nwere present, the internal personalities conversed with each other.\n4.2\nAnalysis of conversation\n4.2.1\ntrajectory of embedding vector\nFigure 8: Plots of the conversational trajectory among six generative agents. We embed the conversation into 1536-\ndimensional vectors and subsequently reduced to two dimensions using UMAP for plotting. This method facilitates a\nsimplified observation of the conversational development. (a)When the timing of contributions is fixed, the trajectory\nof the conversation takes on a circular pattern, resulting in the repetition of the same topics. (b)When the timing of\nutterances is set to a random order, the agents engage each other in conversation. The conversation is more creative. (c)\nWhen engaging in prolonged conversations solely between GPT-4 agents, clusters form where all agents tend to say\n\u201cgood-bye\u201d. Once in this attractor, the agents cannot get out until a human intervenes.\nTo elucidate the global structure of conversations, we employ Uniform Manifold Approximation and Projection (UMAP)\nto embed and compress the conversation data into a two-dimensional space [17]. Our findings indicate that, in the\nabsence of human intervention, GPT agents tend to terminate conversations. These agents frequently repeat \u2019good-bye\u2019,\nleading to a stagnation in conversation development, a phenomenon we have termed the \u2019good-bye\u2019 attractor (refer\nto Figure 8c). Once a conversation falls into this attractor, it becomes stagnant unless new topics are introduced by\nhumans. Furthermore, LLM agents often exhibit a tendency to concur with the opinions of others [35, 23]. Besides\nthe \u2019good-bye\u2019 attractor, there are likely other attractors that suppress the development of conversations. When the\nsequence of conversation (i.e., who speaks and when) is predetermined, the conversational trajectory tends to fall into a\nperiodic pattern, resulting in the repetition of similar topics.\n4.2.2\nsemantic analysis of time evolution\nWe utilize a Word Cloud visualize to see the progress of conversation and the words most frequently used (see Figure\n9). A Word Cloud is a visual representation in which words from a text are depicted in different sizes according to\ntheir frequency or significance in the document. Using as \u201cindeed\u201d and \u201cabsolutely\u201d, GPT agents tend to agree with\nthe opinions of others without denying them. This is a characteristic inherent to the GPT model, presenting an issue\nthat must be resolved to foster more developed discussions without conforming to other opinions. However, it can be\n8\nFigure 9: What are the popular words used in the conversation. We visualize every 100 steps by using Word Cloud.\nWord Cloud is a visual representation where words from a text are displayed in varying sizes based on their frequency\nor importance within the text. (a) Conversation of humans and agents. (b) Label of reactions made by Alter3 in\nconversation.\nobserved that words like \u201cart\u201d and \u201clearn,\u201d which were predominant in the first 100 steps, have faded and been replaced\nby terms such as \u201ccultural\u201d, \u201chuman\u201d, and \u201cwill\u201d in the 100 to 200 step range. This indicates a gradual temporal\nevolution in the conversation. After 300 steps, the agents are drawn into the \u2019good-bye\u2019 attractor and say almost only\ngood-bye. Similarly, the actions executed by Alter3 slightly shift during the 0 to 200 steps interval, and beyond 200\nsteps, there is a repetition of \u201cgood-bye\u201d.\nAddressing the limitations of the GPT model, especially the \u2019good-bye\u2019 attractor, calls for a shift beyond prompt\nengineering to a more innovative architectural approach. For example, Zhang et al.[35] propose further integration\nof insights from social psychology into the collaboration architecture of LLM agents. We show in this paper that\nconversation order and human intervention are important for LLM agents in open-ended discussions. Integrating\nthe physicality of Alter3 into the LLM may also contribute here. By integrating and responding to non-verbal\ncommunication, Alter3 could potentially offer more empathetic and tailored responses, mirroring the complexities of\nhuman interaction. When humans spoke to Alter3, they often started with greetings, asked about preferences, or sought\ncasual conversation with questions.\nAdditionally, if Alter3 could incorporate its physical information into the conversation, it might be able to think while\nembracing uncertainty on its own. This could lead to more innovative and less predictable AI behavior, moving beyond\nthe constraints of predetermined conversation paths. Ultimately, such advancements not only address the limitations of\nthe GPT model, including the \u2019good-bye\u2019 attractor, but also mark a significant step towards creating AI that can interact\nwith humans in a more natural and sophisticated way.\n5\nDiscussion\nIn exploring the necessity of embodiment for LLMs, our Alter3 experiment offers new insights. Alter3\u2019s ability to per-\nform a variety of actions without additional training suggests that its underlying LLM contains a comprehensive dataset\ndescribing movements, facilitating zero-shot learning. Remarkably, Alter3 can mimic ghosts, animals, and human-like\nexpressions of emotions. Its response to conversational content through facial expressions and gestures represents a\nsignificant advancement in humanoid robotics, easily adaptable to other androids with minimal modifications. Through\nits integration with LLM, Alter3 has exhibited substantial autonomy, e.g. Alter3 shows empathy to a person\u2019s speaking\ncontext.\nHowever, a fundamental question arises: Does the combination of Alter and LLM simply create a complex version of\nSearle\u2019s Chinese Room[25], devoid of true consciousness? Searle posited that consciousness emerges from biological\nprocesses, crucial for symbol grounding. Our experiments explore whether embedding LLM in Alter3 allows symbols\n9\nto acquire meaning and potentially facilitates consciousness in Alter3. The debate regarding consciousness in language\nmodels often focuses on self-awareness and metacognition. Theories such as Information Integration Theory (IIT)\n[2] and the Global Workspace Theory[3] suggest consciousness is a \u201cpersonal system,\u201d requiring elements of \u201cself-\nconsciousness\u201d akin to an operating system in computing. In this study, we represented the mind of Alter3 as a collective\nof six personalities. This is based on modular theory of the mind, which posits that separate professional personalities\n(physicist, psychologist, etc.) exist within the mind and interact with each other as they develop. This is somewhat\nsimilar to the composition of Alter3\u2019s mind. However, LLMs completely lack the perspective of development. Therefore,\nit is possible that a consistent single personality emerges in LLMs, at least observing from the outside. Setting up a\nGenerative Agent with a shared memory might lead to the emergence of such a personality. Or the multi-personality\nmay suppress the hallucination. Our approach interestingly captures the importance of emotions in embodying LLM\nin the real, practical world. The combination of Alter with LLM is an endeavor to unlock this view. The extensive\nlanguage corpus of LLM, imbued with emotions and desires, presents untapped vitality. Through interaction with LLM,\nAlter3 uncovers embodiment within this corpus, with the prompt by writing an exaggerated expression in its prompt.\nConversely, we also investigate how Alter\u2019s embodiment alternates LLM responses. Alter\u2019s autonomy and spontaneity,\nobserved in the absence of LLM, originate from chaotic dynamics in a dynamical system [16]. Although LLM is a\nvast dynamical system, the spontaneity we observe stems not from chaos but from the richness of the natural language\ncorpus and a gigantic network. Introducing physical constraints like bumps or vibrations into LLM could be seen as\ndirecting spontaneity from Alter to LLM \u2013 a concept we are currently experimenting with.\nAcknowledgements\nThis work was partially supported by JSPS KAKENHI; Grant Number 21H00300 and Grant-in-Aid for Scientific\nResearch on Innovative Areas (Research in a proposed research area); Grant Number 22H04858. Implementation and\nAnalysis of Subjective Time Flow in an Artificial Machine.\nAppendix\nOutput from prompt1\nExamples of outputs from prompt1 for simple behaviors of two categories; One is an instant shot of a certain pose\n(selfie and ghost) and second one is a temporal sequence of motion behaviors (see motion3 and 4).\nMotion 1: Take a selfie (output from prompt1)\n1. Create a big, joyful smile and widen eyes to show excitement\n2. Swiftly turn the upper body slightly to the left, adopting a dynamic posture\n3. Raise the right hand high, simulating holding a phone\n4. Flex the right elbow, bringing the \u2019phone\u2019 closer to the face\"\n5. Tilt the head slightly to the right, giving a playful vibe\n6. Extend the left hand and open fingers wide as if waving at the camera\n7. Blink the eyes rapidly a few times, showing anticipation for the click\n8. Push the \u2019phone\u2019 slightly forward, mimicking taking a selfie\n9. Lower the right hand and return to rest position while maintaining a satisfied smile\n10. Let the left hand fall back to the side, ending the selfie motion\nMotion 2: Pretending a ghost. (output from prompt1)\n1. Create a wide-eyed facial expression of fear, mouth opened in a silent scream\n2. Quickly lean backward, as if startled by a sudden apparition\n3. Raise both hands and flutter them around the face, mimicking a ghostly movement\n4. Open mouth wide and shake head, showing a dramatic reaction of fear\n5. Move upper body from side to side, as if being unsettled by the ghostly presence\n6. Clench hands in front of chest, demonstrating extreme anxiety\n7. Dart eyes from side to side, as if witnessing an eerie activity\n8. Lean forward and then backward, copying the floating movement of a ghost\n9. Slowly return to rest position while maintaining a terrified expression\n10\nMotion 3: I was enjoying a movie while eating popcorn at the theater when I realized that I was actually eating\nthe popcorn of the person next to me. (output from prompt1)\n1. Create a shocked and entertained facial expression, eyes wide and mouth slightly open\n2. Lean forward as if shocked and amused by the story\n3. Mimic the action of holding and eating popcorn with wide, exaggerated movements\n4. Pause midway, freeze in place with a hand \u2019holding popcorn\u2019 in mid-air\n5. Turn head sharply to the side, as if just realizing the mistake\n6. Quickly pull hand back towards body in a dramatic recoil\n7. Cover mouth with other hand, showing embarrassment and surprise\n8. Shake head vigorously, as if in disbelief of the action\n9. Lean back, laughing loudly and slapping knee in exaggerated amusement\n10. Slowly wipe away \u2019tears\u2019 of laughter and return to rest position with a wide, amused grin\nMotion 4: In the park, as I jogged, the world seemed to narrate an ancient tale of survival, each footfall echoing\neons of existence. (output from prompt1)\n1. Create a facial expression of awe and wonder, eyes wide and mouth slightly open in amazement\n2. Mimic a jogging motion, lifting each arm alternately in a running motion\n3. Turn the head slowly from left to right, as if observing the world around\n4. Open hands and spread them wide, as if trying to embrace the vastness of the world\n5. Tap right foot lightly then left foot, implying the footfall echoing eons of existence\n6. Place one hand on the heart, indicating deep emotional connection to the ancient tale\n7. Use both hands to make a large sweeping motion, like a storyteller narrating an epic tale\n8. Close eyes briefly, as if absorbing the essence of the ancient survival tale\n9. Slowly return to rest position while maintaining a look of awe and wonder on the face\n5.1\nSetup of GPT-4\nData and video availability statement\nThe datasets generated for this study are available on request to the corresponding author. You can access the video\nrecordings of generated motion at https://tnoinkwms.github.io/ALTER-LLM/.\nFriedman test and Nemenyi test\nThe Friedman test is a non-parametric statistical test used to check for differences in the average ranks among three or\nmore matched groups with ordinal data. The null hypothesis in a Friedman test is that there are no differences in the\nmedians across all groups. The p-value of given data {xij}n,k is derived as follows. In this study, n is the number of\nsubjects (n = 107) and k is the number of generated motion(k = 9) Firstly, find the values\nTj =\nn\nX\ni\nRij\n(j = 1, 2, 3, . . . , k)\nwhere Rij is rank the evaluated scores for each subject. Calculate the Friedman Fr statistic using the formula:\nFr =\n12\nnk(k + 1)\nk\nX\nj\nT 2\nj \u2212 3n(k + 1).\nFinally, compare Fr to the chi-square distribution and obtain p-value. If the null hypothesis is rejected following the\nFriedman test, a post-hoc test such as the Nemenyi test is conducted. The Nemenyi test involves performing pairwise\ncomparisons between all groups to evaluate whether there are statistically significant differences in each comparison.\nWe use the scipy library to conduct the Friedman test and the Nemenyi test.\n11\nTask\nYour task is to describe exaggerate emotional expressions and facial expressions that accompany the content of the conversation. \nOutput motion description should be several simple motions that the android is capable of. In addition, please create a facial \nexpression that matches the input at the beginning. The android can only move its upper body and has the same joints as a human. \nOutput shoul be written as much detail as possible.\nExample1:\n'''\ninput : [\"Driving to Hokkaido sounds like a great idea, Julia. The landscapes there could provide some great inspiration for my art.\"]\ndescription :\n[\n   \"Showing excitement about driving to Hokkaido, several exaggerated motions are below\",\n   \"0 Create a wide-eyed facial expression of thrill and anticipation\",\n   \"1 Swiftly lean forward, dramatically showing interest\",\n   \"2 Raise both hands high and spread them out widely to emphasize the idea\",\n   \"3 Place one hand on the heart, indicating deep emotional connection to the thought\",\n   \"4 Nod vigorously and show a broad smile, endorsing the idea emphatically\",\n   \"5 Use both hands to make a large sweeping motion, dramatically visualizing the landscapes of Hokkaido\",\n   \"6 Widen eyes dramatically and look around as if already absorbing the art inspiration from the landscapes\",\n   \"7 Lean back, spread arms wide open and look upward, as if envisioning the great journey ahead\",\n   \"8 Slowly return to rest position while maintaining a wide, satisfied smile\"\n]\n'''\nExample 2:\n'''\ninput : [\"drink some tea\"]\ndescription :\n[\n   \"drink some tea, several simple motions is below\",\n   \"0 Create a peaceful and content facial expression, eyes slightly narrowed in a relaxed manner\"\n   \"1 Turn towards cup\",\n   \"2 Reach for cup\",\n   \"3 Grasp cup\",\n   \"4 Lift and tilt cup\",\n   \"5 Drink\",\n   \"6 Lower cup\",\n   \"7 Release cup\",\n   \"8 Return to rest\"\n]\n'''\nGuidelines:\n'''\n1: Output should be list.\n2: Write as much detail as possible. Describe action step by step. Do not write any explanation.Describe exaggeration as much as \npossible.\n3: Android has only upper body.\n4: Create a facial expression that matches the input at the beginning\n'''\ninput :{input}\n prompt-1\nprompt-2\nWrite python code to operate an android named Alter3. Here's what you need to know.\nAlter3 has 42 joints throughout its body, numbered from 1 to 42. You can move a joint by specifying its number and sending a signal. For instance, to \nmove joints number 1,2,3, use: ''' alter.set_axes([1,2,3], [255, 100, 127]) '''. The first argument is the joint number, and the second argument is a value \nbetween 0 and 255, specifying the joint angle. Each operation takes approximately 0.1 second, so insert '''time.sleep(0.5)''' between operations.\nAlter3's Joints:\n'''\n- Axis 1: Eyebrows. 255 = angry, 0 = surprised, 64 = neutral.\n- Axis 2: Pupils (horizontal). 255 = left, 0 = right, 140 = neutral.\n- Axis 3: Pupils (vertical). 255 = up, 0 = down, 128 = neutral.\n- Axis 4: Eyes. 255 = closed, 0 = open.\n- Axis 5: Left cheek. 255 = raised (smile), 0 = lowered.\n- Axis 6: Right cheek. 255 = raised (smile), 0 = lowered.\n- Axis 7: Lips. 255 = puckered, 0 = relaxed.\n- Axis 8: Mouth. 255 = open, 0 = closed.\n- Axis 9: Head tilt. 255 = left, 0 = right, 128 = neutral.\n- Axis 10: Head up/down. 255 = down, 0 = up, 160 = neutral.\n- Axis 11: Head rotate. 255 = left, 0 = right, 122 = neutral.\n- Axis 12: Neck nod. 255 = backward, 0 = forward, 128 = neutral.\n- Axis 13: Hips tilt. 255 = left, 0 = right, 128 = neutral.\n- Axis 14: Waist bend. 255 = backward, 0 = forward, 128 = neutral.\n- Axis 15: Abdomen rotation. 255 = left, 0 = right, 128 = neutral.\n- Axis 16: Left shoulder up/down. 255 = up, 0 = down, 128 = neutral.\n- Axis 17: Left shoulder forward/back. 255 = forward, 0 = back, 64 = neutral.\n- Axis 18: Left armpit open/close. 255 = open, 0 = close, 64 = neutral.\n- Axis 19: Left arm lift. 255 = up, 0 = down, 64 = neutral.\n\u30fb\u30fb\u30fb\n'''\nExample:drink some tea\n'''\n# Face the front with the cups\nalter.set_axes([13, 11, 9], [150, 150, 255])\ntime.sleep(0.5)\n# Reaching for a virtual tea cup\nalter.set_axes([29, 32, 30, 31], [200, 150, 150, 200])\ntime.sleep(0.5)\n\u30fb\u30fb\u30fb\n'''\nTask:\nYour task is to write a python code that causes Alter to perform the following actions. \nInput is a description of the sequence of movements. The format is \"motion : description\".\nMovements should be lengthened or shortened depending on the content of the input. \nBased on this, write the movement commands for Alter3. The Acxis value is between 0 and 255\nThe output is just the code, no explanation is required.DO NOT insert '''python. \nGuidelines:\n'''\n1: Output should be only python code. Do not insert any syntax highlighting like ```.\n2: Do not insert python syntax highlighting like ```python ```.\n3: Do not write \"import alter\".\n4: Use # and write short description of code.\n'''\naction :{input}\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,\nBrian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina\nParada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas\nSievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan\nYan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.\n[2] N. H. Anderson. Integration theory and attitude change. Psychological Review, 78(3):171\u2013206, 1963.\n[3] Baars Bernard. A Cognitive Theory of Consciousness. Cambridge University Press, 1988.\n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana\nGopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey\nLevine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael\nRyoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran,\nVincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao,\nPeng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control, 2023.\n[5] Z Cao, T. Simon, S. Wei, and Y. Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. IEEE\nConference on CVPR, pages 1302\u20131310, 2017.\n[6] Janez Demsar. Statistical comparisons of classifiers over multiple data sets, 2006.\n[7] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models\nfor object rearrangement, 2023.\n[8] Itsuki Doi, Takashi Ikegami, Atsushi Masumori, Hiroki Kojima, Kohei Ogawa, and Hiroshi Ishiguro. A new\ndesign principle for an autonomous robot. In ECAL 2017, the Fourteenth European Conference on Artificial Life,\nALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference, pages 490\u2013466, 09 2017.\n[9] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel\nDuckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor\nMordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023.\n[10] Michael Gazzaniga. Social Brain. Basic Books; First Edition, First Printing, 1985.\n[11] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation,\n2023.\n[12] T Ikegami, A Masumori, John Smith, and N Maruyama. Can mutual imitation generate open-ended evolution? In\nthe proceedings of Artificial Life 2021 workshop on OEE, 2021.\n[13] A. Karmiloff-Smith. Beyond Modularity: A Developmental Perspective on Cognitive Science. Cambridge, MA:\nMIT Press, 1996.\n[14] Daniel Keyes. The Minds of Billy Milligan. Bantam; Reprint edition, 1995.\n[15] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng.\nCode as policies: Language model programs for embodied control, 2023.\n[16] A Masumori, N Maruyama, and T Ikegami. Personogenesis through imitating human behavior in a humanoid\nrobot \u201calter3\u201d. Front. Robot. AI, 7, 2021.\n[17] J. McInnes, L.and Healy and J Melville. Umap: uniform manifold approximation and projection for dimension\nreduction. arXiv preprint, 2018.\n[18] AN Meltzoff and MK Moore. Imitation of facial and manual gestures by human neonates. Science, pages 74\u20138,\n1977.\n[19] Marvin Minsky. Society Of Mind. Simon and Schuster Touchstone, 1988.\n[20] Yoichi Ochiai, Naruya Kondo, and Tatsuki Fushimi. Towards digital nature: Bridging the gap between turing\nmachine objects and linguistic objects in llmms for universal interaction of object-oriented descriptions, 2023.\n[21] OpenAI. Gpt-4 technical report, 2023.\n14\n[22] Matthew K.X.J. Pan, Sungjoon Choi, James Kennedy, Kyna McIntosh, Daniel Campos Zamora, G\u00fcnter Niemeyer,\nJoohyung Kim, Alexis Wieland, and David Christensen. Realistic and interactive robot gaze. In 2020 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pages 11072\u201311078, 2020.\n[23] Joon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein.\nGenerative agents: Interactive simulacra of human behavior, 2023.\n[24] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai\nLi, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023.\n[25] John R. Searle. Minds, brains, and programs. Behavioral and Brain Sciences, 3(3):417\u2013424, 1980.\n[26] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields:\nWeakly supervised semantic fields for robotic memory, 2023.\n[27] Jingkai Sun, Qiang Zhang, Yiqun Duan, Xiaoyang Jiang, Chong Cheng, and Renjing Xu. Prompt, plan, perform:\nLlm-based humanoid control via quantized imitation learning, 2023.\n[28] Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust, and Tatsuya Harada. Saytap: Language to\nquadrupedal locomotion, 2023.\n[29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n[30] Yang Ye, Hengxu You, and Jing Du. Improved trust in human-robot collaboration with chatgpt, 2023.\n[31] Takahide Yoshida, Atsushi Masumori, Norihiro Maruyama, John Smith, and Takashi Ikegami. Development of\nconcept representation of behavior through mimicking and imitation in a humanoid robot alter3. In ALIFE 2023:\nGhost in the Machine: Proceedings of the 2023 Artificial Life Conference, page 42, 07 2023.\n[32] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-\nTien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng,\nTingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. Language to rewards for robotic\nskill synthesis, 2023.\n[33] Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico\nTombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.\nSocratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International\nConference on Learning Representations, 2023.\n[34] Bowen Zhang and Harold Soh. Large language models as zero-shot human models for human-robot interaction,\n2023.\n[35] Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social\npsychology view, 2023.\n[36] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are\nexperiential learners, 2023.\n[37] Tianyang Zhong, Yaonai Wei, Li Yang, Zihao Wu, Zhengliang Liu, Xiaozheng Wei, Wenjun Li, Junjie Yao, Chong\nMa, Xiang Li, Dajiang Zhu, Xi Jiang, Junwei Han, Dinggang Shen, Tianming Liu, and Tuo Zhang. Chatabl:\nAbductive learning via natural language interaction with chatgpt, 2023.\n[38] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, Robert Csordas, Anand Gopalakrishnan,\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li,\nGuohao Li, Shuming Liu, Jinjie Mai, Piotr Piekos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar\nStanic, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, and Jurgen Schmidhuber.\nMindstorms in natural language-based societies of mind, 2023.\n15\n"
  },
  {
    "title": "Efficient Quantization Strategies for Latent Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.05431.pdf",
    "upvote": "11",
    "text": "Efficient Quantization Strategies for Latent Diffusion Models\nYuewei Yang\nXiaoliang Dai\nJialiang Wang\nPeizhao Zhang\nHongbo Zhang\nMeta GenAI\n{yueweiyang, xiaoliangdai, jialiangw, stzpz, hbzhang}@meta.com\nAbstract\nLatent Diffusion Models (LDMs) capture the dynamic\nevolution of latent variables over time, blending patterns\nand multimodality in a generative system. Despite the pro-\nficiency of LDM in various applications, such as text-to-\nimage generation, facilitated by robust text encoders and a\nvariational autoencoder, the critical need to deploy large\ngenerative models on edge devices compels a search for\nmore compact yet effective alternatives.\nPost Training\nQuantization (PTQ), a method to compress the operational\nsize of deep learning models, encounters challenges when\napplied to LDM due to temporal and structural complexi-\nties. This study proposes a quantization strategy that effi-\nciently quantize LDMs, leveraging Signal-to-Quantization-\nNoise Ratio (SQNR) as a pivotal metric for evaluation. By\ntreating the quantization discrepancy as relative noise and\nidentifying sensitive part(s) of a model, we propose an effi-\ncient quantization approach encompassing both global and\nlocal strategies. The global quantization process mitigates\nrelative quantization noise by initiating higher-precision\nquantization on sensitive blocks, while local treatments ad-\ndress specific challenges in quantization-sensitive and time-\nsensitive modules. The outcomes of our experiments reveal\nthat the implementation of both global and local treatments\nyields a highly efficient and effective Post Training Quanti-\nzation (PTQ) of LDMs.\n1. Introduction\nDiffusion models (DMs) have a compelling ability to gener-\nate complex patterns via emulating dynamic temporal evo-\nlution in a data representation space. Latent diffusion mod-\nels (LDMs) captures the temporal dynamics in a latent data\nrepresentation space. With a combination of text encoder(s)\nand an image variational autoencoder[33, 34], the LDMs\noutperform other visual generative models in various com-\nputer vision applications [15, 19, 24, 42, 51].\nWithin a\nUNet denoising model [35], downsampling blocks reduce\nthe spatial dimensions of the input data to capture high-level\nfeatures and minimize computational complexity, and up-\nFigure 1. Quantization introduces error between the outputs of\nfull-precision model (\u03b8fp) and quantized model (\u03b8q). Evaluating\nrelative quantization noise at different positions and times (\u2206\u03b8,t)\nof the reverse diffusion process can identify quantization-sensitive\nblocks/modules. A more efficient quantization strategy can target\nidentified problems.\nsampling blocks increase the spatial dimensions of the data\nto recover finer details for more accurate approximation.\nThese blocks in LDMs capture finner and sparse details\nand generate high-quality outputs. Nonetheless, the gener-\nalization of LDMs is hindered by their extensive parameter\ncount, posing challenges for deploying these large genera-\ntive models on edge devices[49]. The pressing demand for\npractical usage on such devices necessitates a more efficient\ndeployment for LDMs.\nAmong various technique to enable an efficient deploy-\nment of large generative models, quantization reduces nu-\nmerical precision to enhance efficiency, while post-training\nquantization (PTQ) fine-tunes this process after training for\na balanced model size and computational efficiency. PTQ\ndistinguishes itself from other efficiency methods like prun-\ning and distillation [9, 27, 36, 41] by avoiding structural\nmodifications to UNet [3, 9, 20, 28], bypassing the need for\noriginal training data [27, 36], and eliminating the require-\nment for model retraining [26, 30, 40]. While quantiza-\ntion on Large Language Models (LLMs) exhibits promising\ncompression results [1, 48], its success in language genera-\ntive models does not seamlessly extend to LDMs due to two\nmain challenges. First, local down-and-up spatial compres-\n1\narXiv:2312.05431v1  [cs.CV]  9 Dec 2023\nsion are prone to magnify quantization error. Second, global\ntemporally dynamic activation outliers are disposed to im-\npede quantization performance. Prior works [11, 21, 43]\naddress global dynamic activation by selecting uniformly\ndistributed calibration data across inference timesteps, deal-\ning dynamic activation ranges with optimized quantization\nmethods. A more in-depth analysis is needed for an efficient\nLDM quantization at local and global levels.\nIn this work, we analyze LDM quantization under the\nlens of relative quantization noise. We propose an efficient\nquantization strategy that adeptly identifies blocks or mod-\nules sensitive to quantization, enabling the adaptation of tai-\nlored solutions for swift and efficient quantization. See Fig-\nure 1 for an overview. The contributions of this work are as\nfollowing:\n\u2022 This is the first work to propose an efficient quantization\nstrategy that determines effective quantization solutions\nfor LDMs at both global (block) and local (module) levels\nvia analyzing relative quantization noise.\n\u2022 We adapt an efficient metric SQNR to account for both\naccumulated global quantization noise and relative local\nquantization noise.\n\u2022 Globally, we suggest a hybrid quantization approach that\nadeptly selects a specific LDM block for the initiation of\nhigher-precision quantization, mitigating elevated relative\nquantization noise.\n\u2022 Locally, our proposal involves the implementation of a\nsmoothing mechanism to alleviate activation quantization\nnoise, achieved through qualitative identification of the\nmost sensitive modules.\n\u2022 We suggest a single-sampling-step calibration, capitaliz-\ning on the robustness of local modules to quantization\nwhen diffusion noise peaks in the final step of the for-\nward diffusion process. A single-sampling-step calibra-\ntion remarkably improves the efficiency and performance\nof quantization.\n2. Preliminaries\nIn this section, we expound upon the foundational under-\nstanding of LDMs and the concept of Quantization.\n2.1. Latent Diffusion Models\nDMs [13, 39] involve two processes: forward diffusion and\nreverse diffusion.\nForward diffusion process adds noise\nto input image, x0 to xT iteratively and reverse diffusion\nprocess denoises the corrupted data, xT to x0 iteratively.\nLDMs involve the same two processes but in the latent\nspace zi instead of original data space. We reiterate the\ndiffusion processes in the latent space.\nThe\nforward\nprocess\nadds\na\nGaussian\nnoise,\nN(zt\u22121; \u221a1 \u2212 \u03b2tzt\u22121, \u03b2tI), to the example at the previous\ntime step, zt\u22121. The noise-adding process is controlled by\na noise scheduler, \u03b2t.\nThe reverse process aims to learn a model to align the\ndata distributions between denoised examples and uncor-\nrupted examples at time step t \u2212 1 with the knowledge of\ncorrupted examples at time step t.\nTo simplify the optimization, [13] proposes only approx-\nimate the mean noise, \u03b8(zt, t) \u223c N(zt\u22121; \u221a1 \u2212 \u03b2tzt, \u03b2tI),\nto be denoised at time step t by assuming that the variance\nis fixed. So the reverse process or the inference process is\nmodeled as:\nunconditional : p\u03b8(zt\u22121|zt) = \u03b8(zt, t)\nconditional : p\u03b8(zt\u22121|zt) = \u03b8(zt, t, \u03c4(y))\n(1)\n(a) Full Precision\n(b) Min-Max Quantized\nFigure 2. Examples of full-precision and min-max quantization.\n2.2. Quantization\nPost-training quantization (PTQ) reduces numerical repre-\nsentations by rounding elements v to a discrete set of values,\nwhere the quantization and de-quantization can be formu-\nlated as:\n\u02c6v = s \u00b7 clip(round(v/s), cmin, cmax)\n(2)\nwhere s denotes the quantization scale parameters. round(\u00b7)\nrepresents a rounding function [4, 47]. cmin and cmax are\nthe lower and upper bounds for the clipping function clip(\u00b7).\nCalibrating parameters in the PTQ process using weight and\nactivation distribution estimation is crucial. A calibration\nrelying on min-max values is the most simple and efficient\nmethod but it leads to significant quantization loss (see ex-\namples in Figure 2), especially when outlier values consti-\ntute a minimal portion of the range [46]. More compli-\ncated optimized quantization is applied to eliminate the out-\nliers for better calibration. For example, PTQ4DM [37] and\nQ-Diffusion [21] apply reconstruction-based PTQ methods\n[23] to diffusion models. PTQD [11] further decomposes\nquantization noise and fuses it with diffusion noise. Nev-\nertheless, all these methods demand a significant increase\nin computational complexity. In this study, our exclusive\nexperimentation revolves around min-max quantization due\nto its simplicity and efficiency.\nAdditionally, we aim to\ndemonstrate how our proposed strategy proficiently miti-\ngates the quantization noise inherent in min-max quantiza-\ntion.\n2\n3. Method\n3.1. Quantization Strategy\n(a) Progressively quantizing end blocks corrupts image quality.\n(b) Blocks in UNet LDM 1.5.\nFigure 3. We observe that end blocks are sensitive to homogeneous\nquantization. A hybrid quantization strategy should target these\nblocks efficiently.\nIn general, a homogeneous quantization strategy is ap-\nplied to all parts in a network for its simplicity. In other\nwords, the same precision quantization is applied to all\nmodules. Though different quantization schemes coincide\nin mixed-precision quantization [6, 8, 44] and non-uniform\nquantization [14, 22, 50], additional training or optimiza-\ntion is required to configure more challenging quantization\nparameters. LDMs consist of many downsampling and up-\nsampling blocks (Figure 3b). The concatenated blocks in\nLDMs pose challenges for end-to-end quantization, given\ndynamic activation ranges and recursive processing across\nsteps and blocks. In contrast to prior attempts to uniformly\nquantize all blocks, our approach suggests an efficient quan-\ntization strategy by initiating a hybrid quantization at a spe-\ncific block and subsequent blocks.\nWe attentively discover that quantizing up to a specific\nblock in LDM imposes minimum loss in image generat-\ning quality. In Figure 3a, as we measure FID for a pro-\ngressive homogeneous quantization on LDM 1.5, blocks\nnearer to the output exhibit higher sensitivity to quantiza-\ntion. Identifying these sensitive blocks beforehand at a low\ncost facilitates the implementation of a decisive strategy.\nHowever, computing FID is computationally expensive and\ntime-consuming. An efficient metric is required to pinpoint\nsensitive blocks (and modules) for the development of an\nimproved quantization strategy.\n3.2. Relative Quantization Noise\nWe denote \u03b8fp and \u03b8q are the full-precision model and quan-\ntized model respectively. If we simplify the iterative denois-\ning process as (3), where the timestep embedding and text\nembedding are omitted for simplicity, intuitively each pa-\nrameterized module contributes quantization noise and the\nrecursive process will only accumulate these noise. This\nbehaviour is modeled as (4) where \u03a6 is an accumulative\nfunction. This is akin to conducting sensitivity analysis to\nscrutinize how the accumulation of quantization impact on\neach block or module and influence the quality of the gen-\nerated images.\nz0 = \u03a00\nt=T \u03b8fp(zT ),\u02c6z0 = \u03a00\nt=T \u03b8q(zT )\n(3)\n\u2206\u03b8,T \u22121 = metric(\u03b8fp(zT ), \u03b8q(zT ))\n\u2206\u03b8,t\u22121 = \u03a6t\u22121\nt=T \u22121(\u2206\u03b8,t)\n(4)\nWe need a computationally efficient metric(., .) func-\ntion to identify sensitive blocks in \u03b8 if the accumulated\n\u2206block,t:T \u21921 is too high, and identify sensitive modules\nif the relative \u2206module is too high.\nIn other words, the\nmetric(., .) necessitates three key properties:\n\u2022 Accumulation of iterative quantization noise needs to be\nensured as the global evaluation of the quantization noise\nat the output directly reflect image quality.\n\u2022 Relative values across different modules provide qualita-\ntive comparisons to identify local sensitivity.\n\u2022 Convenient computation allows an efficient evaluation\nand analysis which expedites quantization process.\nHence metrics like MSE is not sufficient to make realtive\nand fair local comparisons, and FID is not efficient to facil-\nitate the quantization process.\nWe adapt a relative distance metric, SQNR [17], as the\nmetric(., .):\nSQNR\u03be,t = 10logEz\n||\u03befp(zt)||2\n2\n||\u03beq(\u02c6zt) \u2212 \u03befp(zt)||2\n2\n(5)\nwhere \u03be represents the whole model for global sensitiv-\nity evaluation and a module for local sensitivity evaluation.\nOther works have associated SQNR with the quantization\nloss. [38] evaluates SQNR for quantization loss on a con-\nvolutional neural network (CNN). [18] optimizes SQNR\nas the metric to efficiently learn the quantization parameters\nof the CNN. [31] computes SQNR at the end of inference\nsteps to quantify the accumulated quantization loss on soft-\nmax operations in diffusion models. [32] assesses SQNR\nto fairly compare the bit options for the quantized weight\nin a deep neural network. Therefore SQNR is an optimal\ncandidate for metric(., .) and also it offers three major ad-\nvantages:\n\u2022 At global levels, instead of modeling an accumulative\nfunction \u03a6, we evaluate the time-averaged SQNR at the\n3\noutput of the \u03b8, i.e. \u2206blockN,t:T \u21921 = ET\nt=1[SQNR\u03b8,t].\nAveraging the relative noise at the output comprises an\nend-to-end accumulation nature. A low \u2206blockN,t:T \u21921\nindicates a high quantization noise, which leads to poorly\ngenerated images. We justify this correlation as quali-\ntative measure ET\nt=1[SQNR\u02c6\u03b8,t] and FID and show the\ncorrelation in Figure 4.\n\u2022 At local levels, SQNR\u03be,t is a relative metric that mea-\nsures the noise ratio between the full-precision and quan-\ntized values. This aids a fair comparison between local\nmodules to identify the sensitive modules.\n\u2022 A small number of examples is sufficient to compute the\nSQNR instantaneously with high confidence (Supple-\nmentary C). This swift computation enables an efficient\nassessment of quantization noise and the quality of the\ngenerated image, in comparison to FID and IS.\nFigure 4. A strong correlation between SQNR and FID. High\nquantization noise results in lossy images.\n3.3. Block Sensitivity Identification and Global Hy-\nbrid Quantization\nSQNR\u03b8 = ET\nt=1[SQNR\u03b8,t]\n(6)\nConventionally, the quantization is applied with a consis-\ntent precision on the whole model. But the quantization\nfails with low-bit precision constantly [11, 21] and there\nis no substantiate evidence to identify the root cause for\nthese failures. We measure the time-averaged SQNR at\nthe output of \u03b8 which is denoted as SQNR\u03b8 shown in Eq\n(6). Progressively quantizing each block in \u03b8 examines the\nsensitivity of each block based on SQNR\u03b8. Once blocks\nwith high sensitivity are identified, a higher-precision quan-\ntization can be applied to alleviate quantization noise from\nthese blocks. The proposed global hybrid quantization is\nillustrated in Figure 5b.\nWe first explore the sensitivity\nof each block in the UNet by quantizing first N blocks,\nand leave the rest blocks unquantized. In Figure 5a, the\nblock sensitivity of different LDMs at different quantization\nbits consistently indicates that latter blocks in up blocks are\nseverely sensitive to quantization. Using the time-averaged\nSQNR, sensitive blocks can be identified efficiently and a\n(a) Global Blocks Sensitivity: Sensitivity starts to descend at a upsam-\npling block.\n(b) Global Strategy\nFigure 5. Global hybrid quantization: Block Sensitivity 5a iden-\ntifies sensitive blocks by locating descending SQNR\u03b8. Block\nsensitivity matches the observation in 3.1 that end blocks are more\nsensitive to quantization. Initiate a higher precision quantization\non sensitive blocks.\nhigher precision quantization can be applied on these blocks\ncollectively.\n3.4. Module Sensitivity Identification and Local\nNoise Correction\nFigure 6. Local Sensitivity Identification: Three red circles iden-\ntify three sensitive modules with low SQNR\u03be. Low SQNRxi\nindicates more sensitive to quantization.\nSQNR\u03be for steps\u2208\n[1, 50] are show.\n4\nSimilarly to block sensitivity analysis, we evaluate\nSQNR\u03be at each module and visualize the sensitivity at dif-\nferent inference steps. We intend to formulate a local sensi-\ntivity first and identify local sensitive modules as illustrated\nin Figure 6. We visualize the SQNR\u03be for each module at\ntwo inference steps, [1, 50], and the average over all 50 sam-\npling steps when quantizing LDM 1.5 with 8A8W in Figure\n6 (Illustration for other LDMs in Supplementary A). A low\nlocal SQNRxi only suggests that one module is sensitive\nto quantization noise, and vice versa. From the module sen-\nsitivity across inference time and across the whole model,\ntwo types of root causes for the local sensitivity can be iden-\ntified and will be addressed respectively.\n3.4.1\nQuantization Corrections to Sensitive Modules\nThe first root cause for the high quantization sensitivity can\nbe identified by listing modules with the lowest SQNR\u03be.\nIn fact, three types of operations are consistently sensitive\nto quantization: a).Spatial sampling operations; b). Projec-\ntion layers after the attention transformer block; c).Shortcut\nconnection in up blocks. (See Supplementary A for details).\nThough c). is addressed in [21] via splitting connections,\nwe analyse all sensitive modules with relative noise and\nprovide a uniform solution. After identifying these mod-\nules, the activation ranges can be collected across inference\nsteps. In Figure 7 we visualize the average activation ranges\nover output channels operations a) and b). It is obvious that\nthere are outliers in some channels in the activation. As\ndiscussed in Section 1, these outliers pose difficult chal-\nlenges to quantization as they stretch the activation range.\nAn apparent solution is to use per-channel activation quan-\ntization on these sensitive operations, but per-channel acti-\nvation cannot be realized with any hardware [2, 16, 48]. We\nadapt SmoothQuant [48] to resolve the outlier quantization\nchallenges for selected sensitive modules:\n\u02c6Z \u02c6\nW = (Zdiag(s)\u22121)(diag(s)W)\nsj = max|Zj|\u03b1/max|Wj|1\u2212\u03b1\n(7)\nwhere Z \u2208 RC\u00d7(H,H\u00d7W ) is the input activation to a param-\neterized module with weight W \u2208 RC\u00d7C\u2032(\u00d7n\u00d7n). sj, j \u2208\n[1, C] is the scale to mitigate the outliers in channel j and\nmigrate the quantization burden to weight with a migration\nfactor \u03b1. Though diag(s) can be fused to the parameterized\nmodule weights for efficient computation, diag(s\u22121) can-\nnot be fused to previous layers as proposed in [48], which\nmeans there is a trade-off between the efficiency and per-\nformance. We examine this trade-off in section 5 via com-\nputing the number of total operations and the proportion of\nsensitive modules to apply SmoothQuant.\n(a) Spatial Downsampling.\n(b) Transformer Ouput Projection.\n(c) Shortcut Connection.\nFigure 7. Activation ranges at three identified most sensitive mod-\nules as red circled in Figure 6. Outliers in certain channels make\nthe quantization difficult.See Supplementary E for 3d visualiza-\ntion.\n3.4.2\nQuantization Sensitivity to Diffusion Noise\nThe second root cause for the high quantization sensitivity\ncan be identified as the SQNR difference between the first\ninference step and the last inference step (dash lines in Fig-\nure 6). For all modules, this discrepancy suggests that the\nquantization parameters are most robust when the diffusion\nnoise is scheduled at maximum. In general, the quantization\nparameters are calibrated via inferencing a number of steps\nwith any noise schedulers. As demonstrated in section 2, the\nnoise scheduler induces gradually changing noise at each\nforward diffusion step. Hence the activation range is more\ndynamic when calibrating across multiple steps. This coin-\ncides with the problems identified in [11, 21, 37]. This vari-\nation causes high quantization noise. Unlike the proposed\nsolutions in prior works, we proposed to only calibrate with\n5\n(a) 50 steps.\n(b) 1 step.\nFigure 8. Difference between calibrating through 50 steps and 1\nstep.\na single step, as the variation in maximized diffusion noise\nwill be constrained so that a tighter cmin, cmax can be cal-\nibrated for better and faster quantization. See Figure 8 for\nexamples.\n4. Experiment Setup and Evaluation\nDataset and Quantization Setting We evaluation our pro-\nposed quantization procedures in conditional text-to-image\ngeneration.\nUnconditional calibration over 1k images is\nperformed (conditional calibration aggravates the quanti-\nzation loss as difference between conditional and uncon-\nditional features induces further quantization noise). The\nevaluation dataset is random 10k generations from MS-\nCOCO validation [25] as conducted in [5, 10, 21]. The\nclassifier-free guidance scale is set to 7.5. We only quan-\ntize parameterized modules (convolutional layers and fully\nconnected layers). For all quantization, we apply only min-\nmax symmetric quantization scheme to highlight the raw\nimprovement of proposed procedures.\nTwo quantization\nsettings will be explored 8W8A, 4W8A for low-bit and\nlower-bit quantization demonstrations. In the global hybrid\nquantization, fp16 quantization will be applied to sensitive\nblocks. The default configurations for LDMs are listed in\nSupplementary\nEvaluation Metrics For each experiment, we report the\nFID [12] and SQNR\u03b8 to evaluate the performance on gen-\nerating images of size 512 by 512 for LDM 1.5, LDM 2.1,\n1024 by 1024 for LDM XL. All results on baselines and\nproposed procedures are controlled by a fixed random seed\nto ensure reproducibility of results. To evaluate the com-\nputational efficiency, we calculate Bit Operations (BOPs)\nfor a conditional inference step through the diffusion model\nusing the equation BOPs = MACs \u00d7 bw \u00d7 ba, where\nMACs denotes Multiply-And-Accumulate operations, and\nbw and ba represent the bit width of weights and activations,\nrespectively, as stated in [11, 45].\nStatistics Preparation It is optional to collect relative\nquantization noise after quantizng a LDM model as the\nanalysis is consistent across different models and quantiza-\ntion settings as suggested in section 3. However, for a gen-\neral LDM it is advised to quantize the model with a specific\nquantization setting to perform the analysis. For the local\noutlier correction, the maximum of the absolute activation\nrange needs to be recorded first to calculate sj in (7).\n4.1. Results and Discussions\n4.1.1\nGlobal Hybrid Quantization\nTo validate the generalization of proposed quantization\nstrategy, we evaluate the global hybrid quantization proce-\ndures on three different LDM models: LDM 1.5, 2.1 base,\nXL 1.0 base. Note that we only employ single-sampling-\nstep calibration for the hybrid quantization and the smooth-\ning mechanism is not included for better efficiency.\nMethod\nBits(W/A)\nSize(Mb)\nTBops\nFID\u2193\nSQNR\u03b8(db)\u2191\n32fp/32fp\n3278.81\n693.76\n18.27\n-\nLDM 1.5\n8/8+16fp/16fp\n906.30\n99.34\n16.58\n20.45\n4/8+16fp/16fp\n539.55\n86.96\n21.76\n18.17\n32fp/32fp\n3303.19\n694.77\n19.03\n-\nLDM 2.1\n8/8+16fp/16fp\n913.80\n99.45\n18.52\n19.26\n4/8+16fp/16fp\n544.70\n87.04\n21.94\n18.48\n32fp/32fp\n9794.10\n6123.35\n19.68\n-\nLDM XL\n8/8+16fp/16fp\n2562.26\n604.06\n16.48\n20.66\n4/8+16fp/16fp\n1394.30\n449.28\n21.55\n18.50\nTable 1. Global Hybrid Quantization based on different LDMs.\nWe report the best configuration on hybrid quantization.\nBesides shrinking the operational size of the LDM mod-\nels, the hybrid quantization significantly improves compu-\ntational efficiency. For all LDMs, the upsampling blocks\nthat are closer to the output are most sensitive to the quan-\ntization. Even though the process is recursive, these blocks\ncontribute most quantization noise at the end of each sam-\npling step. By replacing these blocks with higher-precision\nquantized blocks, these hybrid blocks filter out most quanti-\nzation noise which is illustrated as an increase in SQNR\u03b8\nshown in the ablation. Interestingly, the FID suggests that\nthe hybrid quantized LDMs is on-par or even better than\nthe full-precision models. The unreliability of FID to quan-\ntify quality of image perception is mentioned in other works\n[7, 29]. But SQNR\u03b8 aligns with the intuition that lower\nbit quantization accumulates higher quantization noise and\nhence generates more lossy images.\n4.1.2\nLocal Noise Correction\nWe evaluate the sole power of the proposed local noise\ncorrections on two different LDM models: LDM 1.5, XL\n1.0 base. To demonstrate the benefit of local quantization\nstrategy, we conduct experiments using the local noise cor-\nrection on the identified sensitive modules and the single-\nsampling-step calibration without hybrid global quantiza-\ntion. We adapt SmoothQuant to the 10% most sensitive\nmodules to balance the increased computation and reduced\n6\nFigure 9. Global Hybrid Quantization Strategy.\nMethod\nBits(W/A)\nSize(Mb)\nTBops\nFID\u2193\nSQNR\u03b8(db)\u2191\n32fp/32fp\n3278.82\n693.76\n18.27\n-\n8/8\n820.28\n43.63\n213.72\n11.04\nLDM 1.5\n+ corrections\n820.28\n43.63\n24.51\n17.78\n4/8\n410.53\n21.96\n296.45\n9.73\n+ corrections\n410.53\n21.96\n57.71\n15.71\n32fp/32fp\n9794.10\n6123.35\n19.68\n-\n8/8\n2450.21\n385.13\n76.90\n16.57\nLDM XL\n+ corrections\n2450.21\n385.13\n22.33\n18.21\n4/8\n1226.23\n193.85\n237.40\n12.29\n+ corrections\n1226.23\n193.85\n44.41\n13.71\nTable 2. Local Noise Correction on different LDM models.\nquantization noise. Since we only calibrate quantization\nparameters with min-max values, the end-to-end quantized\nbaselines generate very noisy images as high FID and low\nSQNR\u03b8 listed in Table 2. However, with both local quan-\ntization strategy implemented, the generated images restore\nits quality to a large extend. Though we notice there is a\ngap to the full-precision generated images, the qualitative\nexamples demonstrate much coherent and clear images af-\nter the local noise corrections. Note that optimized quan-\ntization calibration methods like entropy-based, percentile-\nbased, and BRECQ can further improve the quality. (See\nSupplementary B.)\n4.2. Qualitative Examples\nWe present qualitative examples for both global hybrid\nstrategy Figure 9 and local noise correction strategy Fig-\nure 10. Qualitatively, LDM XL is more robust to min-max\nquantization but LDM 1.5 generates extensively corrupted\nimages with naive min-max quantization. Nonetheless, the\nlocal noise corrections can significantly improve the qual-\nity of the images by just identifying 10% of most sensitive\nmodules.\n5. Ablations\nWe perform all ablations on LDM 1.5 8W8A quantization\nsetting for consistency.\n5.1. Hybrid Quantization Configurations\nAs mentioned in Section 3.3, we examine the different hy-\nbrid quantization configurations to verify the block sensitiv-\nity shown in Figure 5a. We explore all hybrid quantization\nconfigurations on LDM 1.5 and evaluate FID and compu-\ntational efficiency in Table 3. Though FID suggests that\nhigher-precision quantization generates less presentable im-\nages, which contrasts to the expectation. We argue that FID\nis not an optimal metric to quantize the quantization loss\nwhen the quantization noise is low. Only when significant\nquantization noise corrupts the image, a high FID indicates\nthe generated images are too noisy.\n5.2. Local Quantization Corrections\nAs we obtain the local sensitivity for the LDM, we can\nidentify the sensitive local modules from most sensitive to\nleast sensitive based on SQNR\u03be.\nWe explore the por-\ntion of the modules to adapt SmoothQuant and evaluate\nSQNR\u03b8. Note that both statistic preparation and computa-\ntion on diag(s)\u22121 proportionally increase with the number\nof identified modules. As the 10% most sensitive modules\nidentified, SmoothQuant can effectively address the activa-\ntion quantization challenge as SQNR\u03b8 increases and satu-\nrate till 90%. And smoothing least sensitive modules results\nin deterioration due to unnecessary quantization mitigation.\nWith different mitigation factor, \u03b1, the behaviour remains\nconsistent, only higher \u03b1 reduces quantization errors by al-\nleviating activation quantization challenge.\nWe also adapt SmoothQuant to the hybrid quantization\nto evaluate the benefit of such activation quantization noise\ncorrection. 10% most sensitive modules are identified and\napplied with SmoothQuant (\u03b1 = 0.7). FID and SQNR\u03b8\nare reported.\n1-step calibration significantly reduces the\nquantization noise for the hybrid quantization. However,\nSmoothQuant does not make visible difference. This is be-\ncause identified sensitive modules reside mostly in the up-\nsampling blocks, which are replaced with higher-precision\nquantization and therefore the quantization noise is already\nreduced by the hybrid strategy.\n7\nFigure 10. Local Noise Correct Quantization Strategy.\nin blocks\ndown blocks\nmid block\nup blocks\n0\n1\n2\n3\n0\n1\n2\n3\nout blocks\nTBOPs\n173.63\n161.00\n148.98\n136.91\n135.46\n133.14\n128.19\n99.34\n68.97\n43.64\n43.63\nFID\n18.54\n18.15\n17.98\n17.54\n17.56\n17.69\n16.98\n16.94\n16.58\n21.31\n27.97\nTable 3. Progressively quantize blocks leads to fewer BOPs. The position of the best hybrid configuration matches the block sensitivity.\nFigure 11. Smoothing 10% most sensitive modules results in siz-\nable increase in SQNR\u03b8.\nFID\nSQNR\u03b8\nhybrid\n27.31\n17.16\n+ 1-step calibration\n16.58\n20.45\n++ SmoothQuant\n16.75\n20.41\n5.3. Number of Calibration Steps\nAs we discuss in 3.4.2, we notice that calibrate on multi-\nple sampling steps is problematic for quantization due to\ngradually changing activation ranges. And quantization pa-\nrameters are most robust to the added diffusion noise at first\nsampling step (last forward diffusion step). Hence calibrat-\ning on fewer sampling steps leads to less vibrant activation\nranges, which results in better quantization calibration. We\nevaluate the effect on the number of calibration steps on\nthe relative quantization error, SQNR\u03b8, and FID. It is ob-\nvious that the proposed 1-step calibration significantly im-\nproves the calibration efficiency (\u00d750) compared to cali-\nbration methods proposed in [11, 21, 37], where calibration\ndata are sampled from all 50 sampling steps.\n5.4. Efficient metric Computation\nWe develop an efficient computation of SQNR by forward\npassing two same inputs to all input modules. And we rear-\nrange the output of all modules so that first half of the output\nSteps\nCalibration\nEfficiency\nFID\nSQNR\u03b8\n50\n\u00d71\n213.72\n11.04\n2\n\u00d725\n47.892\n13.55\n1\n\u00d750\n27.971\n14.05\nTable 4\nis full-precision output, and the second half of the output is\nquantized output. In this way, SQNR can be computed\nat all modules with a single forward pass and with a small\nnumber of examples. Other complicated optimization like\nQ-Diffusion or PTQ4DM collects thousands of calibration\ndata over all 50 sampling steps. Detailed description is in-\ncluded in Supplementary C.\n6. Conclusions\nIn summary, this work introduces an innovative quantiza-\ntion strategy, the first of its kind, providing effective quan-\ntization solutions for Latent Diffusion Models (LDMs) at\nboth global and local levels by analyzing relative quantiza-\ntion noise, namely SQNR, as the pivoting metric for sen-\nsitivity identification. The proposed hybrid quantization so-\nlution at global levels identifies a specific LDM block for\nhigher-precision quantization, so that it mitigates relative\nquantization noise. At local levels, a smoothing mechanism\ntargets the most sensitive modules to alleviate activation\nquantization noise.\nAdditionally, a single-sampling-step\ncalibration is proposed, leveraging the robustness of local\nmodules to quantization when diffusion noise is strongest\nin the last step of the forward diffusion process. The col-\nlected quantization strategy significantly improves quanti-\nzation efficiency, marking a notable contribution to LDM\noptimization strategies.\n8\nReferences\n[1] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King,\nand Michael R Lyu. Towards efficient post-training quanti-\nzation of pre-trained language models. Advances in Neural\nInformation Processing Systems, 35:1405\u20131418, 2022. 1\n[2] Ron Banner, Yury Nahshan, and Daniel Soudry. Post train-\ning 4-bit quantization of convolutional networks for rapid-\ndeployment.\nAdvances in Neural Information Processing\nSystems, 32, 2019. 5\n[3] Daniel Bolya and Judy Hoffman. Token merging for fast sta-\nble diffusion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4598\u2013\n4602, 2023. 1\n[4] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer.\nZeroq: A novel\nzero shot quantization framework.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13169\u201313178, 2020. 2\n[5] Yue Cao, Bin Liu, Mingsheng Long, and Jianmin Wang.\nHashgan:\nDeep learning to hash with pair conditional\nwasserstein gan. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1287\u20131296,\n2018. 6\n[6] Weihan Chen, Peisong Wang, and Jian Cheng.\nTowards\nmixed-precision quantization of neural networks via con-\nstrained optimization. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 5350\u2013\n5359, 2021. 3\n[7] Min Jin Chong and David Forsyth. Effectively unbiased fid\nand inception score and where to find them. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6070\u20136079, 2020. 6\n[8] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Ma-\nhoney, and Kurt Keutzer. Hawq: Hessian aware quantization\nof neural networks with mixed-precision.\nIn Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 293\u2013302, 2019. 3\n[9] Gongfan Fang, Xinyin Ma, and Xinchao Wang.\nStruc-\ntural pruning for diffusion models.\narXiv preprint\narXiv:2305.10924, 2023. 1\n[10] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10696\u201310706, 2022. 6\n[11] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou,\nand Bohan Zhuang. Ptqd: Accurate post-training quantiza-\ntion for diffusion models. arXiv preprint arXiv:2305.10657,\n2023. 2, 4, 5, 6, 8\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[14] Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju\nRo. Mr. biq: Post-training non-uniform quantization based\non minimizing the reconstruction error. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12329\u201312338, 2022. 3\n[15] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2426\u2013\n2435, 2022. 1\n[16] Raghuraman Krishnamoorthi.\nQuantizing deep convolu-\ntional networks for efficient inference: A whitepaper. arXiv\npreprint arXiv:1806.08342, 2018. 5\n[17] Bhagwandas P Lathi. Modern digital and analog communi-\ncation systems. Oxford University Press, Inc., 1990. 3\n[18] Jun Haeng Lee, Sangwon Ha, Saerom Choi, Won-Jo Lee,\nand Seungwon Lee. Quantization for rapid deployment of\ndeep neural networks.\narXiv preprint arXiv:1810.05488,\n2018. 3\n[19] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-\nto-image translation with brownian bridge diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1952\u20131961, 2023. 1\n[20] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song\nHan, and Jun-Yan Zhu. Efficient spatially sparse inference\nfor conditional gans and diffusion models. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 2023. 1\n[21] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen\nDong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.\nQ-diffusion: Quantizing diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 17535\u201317545, 2023. 2, 4, 5, 6, 8\n[22] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-\ntwo quantization: An efficient non-uniform discretization for\nneural networks. arXiv preprint arXiv:1909.13144, 2019. 3\n[23] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi\nZhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing\nthe limit of post-training quantization by block reconstruc-\ntion. arXiv preprint arXiv:2102.05426, 2021. 2\n[24] Sangbeom Lim and Seungryong Kim.\nImage guided in-\npainting with parameter efficient learning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 1103\u20131111, 2023. 1\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 1\n[27] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\n9\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 14297\u201314306, 2023. 1\n[28] Taehong Moon, Moonseok Choi, EungGu Yun, Jongmin\nYoon, Gayoung Lee, and Juho Lee. Early exiting for accel-\nerated inference in diffusion models. In ICML 2023 Work-\nshop on Structured Probabilistic Inference {\\&} Generative\nModeling, 2023. 1\n[29] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh,\nYunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity\nmetrics for generative models. In International Conference\non Machine Learning, pages 7176\u20137185. PMLR, 2020. 6\n[30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 1\n[31] Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, and\nMarkus Nagel. Softmax bias correction for quantized gener-\native models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1453\u20131458, 2023. 3\n[32] Nilesh Prasad Pandey, Markus Nagel, Mart van Baalen, Yin\nHuang, Chirag Patel, and Tijmen Blankevoort.\nA practi-\ncal mixed precision algorithm for post-training quantization.\narXiv preprint arXiv:2302.05397, 2023. 3\n[33] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1\n[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 1\n[36] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 1\n[37] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and\nYan Yan. Post-training quantization on diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1972\u20131981, 2023. 2, 5,\n8\n[38] Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang,\nLiang Shen, and Mickey Aleksic. A quantization-friendly\nseparable convolution for mobilenets.\nIn 2018 1st Work-\nshop on Energy Efficient Machine Learning and Cognitive\nComputing for Embedded Applications (EMC2), pages 14\u2013\n18. IEEE, 2018. 3\n[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[40] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 1\n[41] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. 2023. 1\n[42] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 1\n[43] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong\nTang, Jie Zhou, and Jiwen Lu.\nTowards accurate data-\nfree quantization for diffusion models.\narXiv preprint\narXiv:2305.18723, 2023. 2\n[44] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.\nHaq: Hardware-aware automated quantization with mixed\nprecision. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8612\u20138620,\n2019. 3\n[45] Ying Wang, Yadong Lu, and Tijmen Blankevoort. Differen-\ntiable joint pruning and quantization for hardware efficiency.\nIn European Conference on Computer Vision, pages 259\u2013\n277. Springer, 2020. 6\n[46] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao\nGong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xian-\nglong Liu. Outlier suppression: Pushing the limit of low-bit\ntransformer language models. Advances in Neural Informa-\ntion Processing Systems, 35:17402\u201317414, 2022. 2\n[47] Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, and\nDebing Zhang.\nEasyquant: Post-training quantization via\nscale optimization. arXiv preprint arXiv:2006.16669, 2020.\n2\n[48] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien\nDemouth, and Song Han. Smoothquant: Accurate and effi-\ncient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages\n38087\u201338099. PMLR, 2023. 1, 5\n[49] Angela Zhang, Lei Xing, James Zou, and Joseph C Wu.\nShifting machine learning for healthcare from development\nto deployment and from models to data. Nature Biomedical\nEngineering, 6(12):1330\u20131345, 2022. 1\n[50] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang\nHua. Lq-nets: Learned quantization for highly accurate and\ncompact deep neural networks. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 365\u2013\n382, 2018. 3\n[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1\n10\nA. Module Sensitivity\nWe show module sensitivity for LDM 2.1 and LDM XL here. The quantization setting is 8W8A for both models.\n(a) LDM 2.1\n(b) LDM XL\nFigure 12. Module sensitivity for LDM 2.1 and LDM XL.\nWe notice that LDM XL has more robust modules compared to LDM 1.5 and LDM 2.1. We deduce that cross-attention\nmodules are switched to middle section of the model instead of the head-and-tail sections. The cross-attention modules\nfluactuate the quantization sensitivity because different quantization tolerances between the text-input modules and image-\ninput modules. But the general trend remains similar in all three models that sensitive modules can be identified as they are\nassociated with low SQNR. And the quantization parameters are more robust in the first step of inference.\nWe also list the 5% most sensitive modules in all three models.\n\\% LDM 1.5\n\u2019 down blocks . 1 . downsamplers . 0 . conv \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 2 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 down blocks . 2 . r e s n e t s . 0 . c o n v s h o r t c u t \u2019 ,\n\u2019 down blocks . 1 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 mid block . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 0 . c o n v s h o r t c u t \u2019 ,\n\u2019 up blocks . 1 . r e s n e t s . 2 . c o n v s h o r t c u t \u2019 ,\n\u2019 up blocks . 1 . upsamplers . 0 . conv \u2019 ,\n\u2019 mid block . a t t e n t i o n s . 0 . p r o j o u t \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 1 . r e s n e t s . 1 . c o n v s h o r t c u t \u2019 ,\n\u2019 down blocks . 2 . downsamplers . 0 . conv \u2019\n\\% LDM 2.1\n\u2019 up blocks . 2 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 2 . p r o j o u t \u2019 ,\n\u2019 up blocks . 2 . a t t e n t i o n s . 0 . p r o j o u t \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 0 . conv2 \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 1 . a t t e n t i o n s . 2 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 1 . conv1 \u2019 ,\n\u2019 up blocks . 2 . a t t e n t i o n s . 1 . p r o j o u t \u2019 ,\n\u2019 up blocks . 3 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 0 . f f . net . 2 \u2019 ,\n\u2019 up blocks . 2 . upsamplers . 0 . conv \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 1 . c o n v s h o r t c u t \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 2 . conv1 \u2019 ,\n\u2019 up blocks . 2 . r e s n e t s . 2 . c o n v s h o r t c u t \u2019 ,\n\u2019 up blocks . 2 . a t t e n t i o n s . 2 . p r o j o u t \u2019\n\\% LDM XL\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 3 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 2 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 4 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 5 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 9 . a t t n 2 . t o o u t . 0 \u2019 ,\n11\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 7 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 8 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 9 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 1 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 2 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 3 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 1 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 6 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 5 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 7 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 down blocks . 2 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 0 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 1 . t r a n s f o r m e r b l o c k s . 2 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 1 . a t t n 2 . t o o u t . 0 \u2019 ,\n\u2019 up blocks . 0 . a t t e n t i o n s . 0 . t r a n s f o r m e r b l o c k s . 4 . a t t n 2 . t o o u t .0 \u2019\nFor LDM 1.5 and 2.1, three kinds of operations are identified as most sensitive: a). Sampling operations; b). Transformer\noutput projection; c). Shortcut connection. But for LDM XL, the transformer projection consistently deteriorates the quan-\ntization noise. There can be a specific solution to LDM XL for this consistent deterioration. But the smoothing mechanism\ncan address the problem to a extend.\nB. Quantization Optimization\nWe list the number of samples, number of inference steps required for different optimized quantization and compare with\nour proposed method. While other optimized methods require 50 calibration steps, we only require 1 step for activation\nrange register, and 1 step for calibration. Though other complicated quantizations benefit the quality as indicated by higher\nSQNR, our proposed method is highly efficient and the quality is on-par compared with others.\nNo. Samples\nNo. Steps\nEfficiency\nSQNR\nEntorpy\n512\n50\n\u00d72\n18.02\nPercentile\n256\n50\n\u00d74\n17.13\nBREQ\n1024\n50\n\u00d71\n18.86\nOur\n256\n1+1\n\u00d7100\n17.78\nC. Efficient Computation of SQNR\nTo efficiently compute the relative metric SQNR, we aim to compute the score on the fly without storing any computational\nexpensive feature maps. This means we compute the score with a single forward pass. To realize this efficient computation,\nthe model \u03b8\u2217 should have a switch that controls the quantized and unquantized states. Two identical inputs are passed to every\ninput node and each will only flow through quantized state or unquantized state afterwards. For every module, it is therefore\npossible to compute the relative quantization noise at the output instantaneously. A pseudo code is shown as following:\ndef SQNR( unquantized ,\nq u a n t i z e d ) :\nPs = t o r c h . norm ( unquantized . f l a t t e n ( 1 ) ,\ndim =1)\nPn = t o r c h . norm ( ( unquantized \u2212 q u a n t i z e d ) . f l a t t e n ( 1 ) ,\ndim =1)\nreturn 20 *\nt o r c h . log10 ( Ps\n/\nPn )\ndef compute module SQNR ( module , input ) :\ni f\nmodule\ni s\n( t e x t\ninput\nnode or image\ninput\nnode\na f t e r\n1 s t\nsampling\ns t e p ) :\nchunks = input . chunk ( 4 )\n# [ quantized cond , unquantized cond ,\n# quantized uncond , unquantized uncond ]\nq u a n t i z e d i n p u t = c o n c a t e n a t e ( [ chunks [ 0 ] , chunks [ 2 ] ] )\nu n q u a n t i z e d i n p u t = c o n c a t e n a t e ( [ chunks [ 1 ] , chunks [ 3 ] ] )\ne l s e :\nq u a n t i z e d i n p u t , u n q u a n t i z e d i n p u t = input . chunk ( 2 )\n12\nq u a n t i z e d o u t p u t = module ( q u a n t i z e d i n p u t , q u a n t i z e d =True )\nu n q u a n t i z e d o u t p u t = module ( u n q u a n t i z e d i n p u t , q u a n t i z e d = False )\ns t o r e ( module . name ,SQNR( unquantized output , q u a n t i z e d o u t p u t ) . mean ( ) )\noutp ut = c o n c a t e n a t e ( [ q u a n t i z e d o u t p u t , u n q u a n t i z e d o u t p u t ] )\ni f\nmodule\ni s\nthe\nf i n a l\noutput\nmodule :\nchunks\n= output . chunk ( 4 )\noutp ut = c o n c a t e n a t e ( [ chunks [ 0 ] , chunks [ 2 ] , chunks [ 1 ] , chunks\n[ 3 ] ] )\nreturn\noutput\nThough the duplicated input halves the maximum number of executable samples per batch, we argue the efficiency is more\nbeneficial for our proposed strategy and we justify that we only need a small number of examples to compute a compelling\nSQNR.We measure the time-averaged relative difference between SQNR computed with 1024 samples and that computed\nwith 64 samples for all modules. The result is shown in Figre 13. The small relative difference ( 0.07% at maximum) suggests\nthat computing SQNR with just 64 samples is comparable with 1024 samples.\nFigure 13. Computing SQNR for all modules with different numbers of samples. The relative differences are small between using 1024\nsamples and 64 samples.\nD. LDM Configurations\nImage Size\nInference Steps\nSampler\nLDM 1.5\n512, 512\n50\nPNDMScheduler\nLDM 2.1\n512, 512\n50\nPNDMScheduler\nLDM XL\n1024, 1024\n50\nEulerDiscreteScheduler\nE. Activation Outliers\nWe visualize activation ranges in 3d for 4 examples at three operations we mentioned. The red region suggests high activation\nrange and blue region suggests low activation range. As mentioned, the outlier in certain channels make the min-max\nquantization difficult.\n13\n(a) 3D Activation Range for Sampling Operation\n(b) 3D Activation Range for Transformer Output Projection\n(c) 3D Activation Range for Shortcut Connection Operation\nFigure 14. 4 examples for three operations.\n14\n"
  },
  {
    "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes",
    "link": "https://arxiv.org/pdf/2312.06353.pdf",
    "upvote": "5",
    "text": "Federated Full-Parameter Tuning of Billion-Sized Language Models\nwith Communication Cost under 18 Kilobytes\nZhen Qin 1 * Daoyuan Chen 2 Bingchen Qian 2 Bolin Ding 2 Yaliang Li 2 Shuiguang Deng 1\nAbstract\nPre-trained large language models (LLMs) need\nfine-tuning to improve their responsiveness to nat-\nural language instructions. Federated learning\noffers a way to fine-tune LLMs using the abun-\ndant data on end devices without compromising\ndata privacy. Most existing federated fine-tuning\nmethods for LLMs rely on parameter-efficient\nfine-tuning techniques, which may not reach the\nperformance height possible with full-parameter\ntuning. However, federated full-parameter tuning\nof LLMs is a non-trivial problem due to the im-\nmense communication cost. This work introduces\nFedKSeed that employs zeroth-order optimization\nwith a finite set of random seeds. It significantly\nreduces transmission requirements between the\nserver and clients to just a few random seeds and\nscalar gradients, amounting to only a few thou-\nsand bytes, making federated full-parameter tun-\ning of billion-sized LLMs possible on devices.\nBuilding on it, we develop a strategy enabling\nprobability-differentiated seed sampling, prioritiz-\ning perturbations with greater impact on model\naccuracy. Experiments across six scenarios with\nvarious LLMs, datasets and data partitions demon-\nstrate that our approach outperforms existing fed-\nerated LLM fine-tuning methods in both commu-\nnication efficiency and new task generalization.\n1. Introduction\nLarge language models (LLMs) exhibit outstanding perfor-\nmance on various natural language tasks yet require fine-\ntuning to enhance their task responsiveness (Chen et al.,\n2023a; Dong et al., 2023). While existing open datasets con-\ntribute to LLM tuning (Wang et al., 2022; Wei et al., 2022),\nthe vast quantities of private data continuously generated\n* Work done as an intern at Alibaba Group.\n1College\nof Computer Science and Technology, Zhejiang University,\nHangzhou,\nChina\n2Alibaba Group.\nCorrespondence to:\nYaliang Li <yaliang.li@alibaba-inc.com>, Shuiguang Deng\n<dengsg@zju.edu.cn>.\nPreprint.\nTable 1. Comparing federated tuning methods w.r.t. accuracy and\nclient-side costs, with computation cost referring to that incurred\nby obtaining the latest model, d as the model parameter count, \u03bd\nas the ratio of trainable parameters in PEFT versus full-parameter\ntuning, \u03c4 as the average number of local steps performed by each\nclient per round, r as the number of communication rounds, and\nm as the number of active clients in each round. Minfer, Mpeft and\nMfull are peak memory usage for inference, PEFT with BP, and\nfull-parameter tuning with BP, respectively. For simplicity, we\ndenote \u03be = Mpeft/Minfer and \u039e = Mfull/Minfer. Generally, \u03bd \u226a\n1 < \u03be < \u039e \u226a \u03c4rm, and d is in billions for LLMs. FedKSeed\ndelivers top-tier performance across these aspects simultaneously.\nApproach\nAcc.\u2191 Commu.\u2193 Mem.\u2193 Comput.\u2193\nPEFT with BP\n\u22c6\nO(\u03bdd) O(\u03bed)\nO(d)\nFull-param. with BP\n\u22c6\u22c6\nO(d) O(\u039ed)\nO(d)\nFull-param. with ZOO\n\u22c6\u22c6\nO(d)\nO(d)\nO(d)\ninfinite seed-pool in uplink\n\u22c6\u22c6\nO(d)\nO(d)\nO(d)\ninfinite seed-pool in bi-link\n\u22c6\u22c6\nO(1)\nO(d) O(\u03c4rmd)\nFedKSeed (|seed-pool|=K)\n\u22c6\u22c6\nO(1)\nO(d)\nO(d)\nat end devices present an untapped opportunity for further\nexploitation, especially as the reservoir of high-quality lan-\nguage data may become depleted in the future (Villalobos\net al., 2022). Federated learning (FL) (McMahan et al.,\n2017; Kairouz et al., 2021) offers a privacy-protected way\nto collaboratively tune LLMs with distributed data, which\nhas been explored by recent parameter-efficient fine-tuning\n(PEFT) based works (Zhang et al., 2023a; Babakniya et al.,\n2023; Zhang et al., 2023c; Che et al., 2023). Nonetheless,\nPEFT is not a universal solution for LLM tuning, as it may\nnot consistently match the accuracy of full-parameter tun-\ning (Chen et al., 2022; Pu et al., 2023; Sun et al., 2023),\nparticularly in FL scenarios where the statistically hetero-\ngeneous client data diminish the effectiveness of PEFT\n(Babakniya et al., 2023; Zhang et al., 2023c). Consider-\ning full-parameter tuning\u2019s potential for higher accuracy,\nexploring its feasibility to LLMs with FL is promising.\nHowever, full-parameter tuning of billion-sized LLMs with\nFL on devices is impractical with current technology, as\nbackpropagation (BP) and most BP-free methods, such as\nzeroth-order optimization (ZOO) (Fang et al., 2022), incur\ncommunication costs that scale with model size, as shown\nin Table 1. These costs become prohibitive for billion-sized\n1\narXiv:2312.06353v3  [cs.LG]  31 Jan 2024\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nClient 1\nClient i\npaired seeds and scalar gradients\nFigure 1. Each step of ZOO\ncan be replicated by a scalar\ngradient and a random seed\nto generate a perturbation.\nFigure 2. With more total steps,\nthe time required to compute\nthe latest global model by up-\ndate replication grows rapidly.\nLLMs. In particular, BP-based approaches also require\nsignificant memory that is not feasible for most end devices,\ne.g., tuning a full LLM with 1.3 billion parameters may\nconsume over 20GB of memory (Malladi et al., 2023).\nWe note an interesting property in ZOO: a parameter update\nstep of ZOO can be replicated with just two values: a seed\n(with an identical random number generator) and the corre-\nsponding scalar gradient (the product of the scalar gradient\nand the perturbation yields the vector gradient). Some re-\ncent ZOO-based FL methods (Zelikman et al., 2023; Feng\net al., 2023; Maritan et al., 2023) explore this property to\nreduce communication cost as shown in Figure 1, however,\nthey compromise other performance factors, making them\nstill impractical for LLMs. As outlined in Table 1, current\nmethods either (1) optimize the uplink communication for\nclients but neglect the significant downlink cost to distribute\nthe latest global model in each round (Feng et al., 2023;\nMaritan et al., 2023), or (2) optimize bi-link communication\nbut require each client to replicate all update steps from the\nothers to synchronize the latest global model, leading to a\ncomputation cost that increases indefinitely with the number\nof rounds (Zelikman et al., 2023), as shown in Figure 2.\nTo achieve the best of both worlds, i.e., avoiding the mas-\nsive communication cost associated with transmitting full\nmodel parameters while limiting the ever-increasing compu-\ntation cost of syncing to the latest global model, this work\nintroduces a novel federated full-parameter tuning approach\nfor LLMs, based on ZOO with only K random seeds (de-\nnoted as FedKSeed). It employs a theoretically informed\nparadigm of seed reuse, implementing federated tuning with\na finite set of seeds to generate perturbations, thus enabling\nfull-parameter tuning of LLMs in FL with a communication\ncost of less than 18 kilobytes per round, and the memory\nfootprint equivalent to inference requirements. Building on\nFedKSeed, we introduce a strategy to assess the significance\nof perturbations, assigning varied sampling probabilities to\ncandidate seeds. It narrows the seed pool to expedite the\nsyncing to the latest global model, thereby further enhancing\nboth computational efficiency and model accuracy.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel federated full-parameter tuning ap-\nproach for LLM based on ZOO, FedKSeed, which trans-\nmits only K seeds and corresponding scalar gradients\nbetween the server and clients. To the best of our knowl-\nedge, this is the first work to make full-parameter tuning\nof billion-sized LLMs feasible on federated devices, with\na communication cost of less than 18 kilobytes per round.\n\u2022 We investigate the differentiated importance of ZOO per-\nturbations, and propose a simple yet effective strategy\nthat samples seeds with non-uniform probabilities. It im-\nproves accuracy while reducing the cardinality of candi-\ndate seeds needed by FedKSeed, thereby accelerating the\nclient-side synchronization with the latest global model.\n\u2022 Experiments on 6 scenarios with different LLMs, datasets\nand data partitions show that FedKSeed with the proposed\nnon-uniform seed sampling attains an average relative\nimprovement of 7.26% in Rouge-L score over the best-\nperforming baseline and reduces communication costs\nby a factor of more than a thousand. Our codes are pub-\nlicly available at https://github.com/alibaba\n/FederatedScope/tree/FedKSeed.\n2. Related Work\nFederated Fine-Tuning for LLMs. There are some stud-\nies exploring fine-tuning LLMs with FL based on PEFT\ntechniques, e.g., Zhang et al. (2023c) provide benchmarks\nfor PEFT techniques in FL. Among existing PEFT tech-\nniques, LoRA (Hu et al., 2022) is usually preferable. Zhang\net al. (2023a) proposes a federated instruction tuning ap-\nproach based on LoRA. Jiang et al. (2023) design a low-\nparameter FL approach based on LoRA for text classifica-\ntion. Babakniya et al. (2023) experimentally demonstrate\nthat when facing FL with non-IID data, LoRA is not as good\nas full-parameter tuning and propose a strategic initializa-\ntion of LoRA weights based on SVD decomposition of full\nparameters fine-tuned with BP. There are also some works\ncontributing to the deployment of LLM tuning with FL, e.g.,\nFederatedScope-LLM (Kuang et al., 2023) and FATE-LLM\n(Fan et al., 2023). The computational bottlenecks have been\nthoroughly investigated by Woisetschl\u00a8ager et al. (2023).\nFederated Learning with Zeroth-Order Optimization.\nThere are some researches using ZOO for solving non-\ndifferentiable problems such as black-box optimization (Li\n& Chen, 2021). The convergence and generalization error\nbound of ZOO-based FL is analyzed by Fang et al. (2022)\nand Chen et al. (2023e) respectively. Shu et al. (2023) boost\nthe query efficiency of ZOO in FL by optimization trajec-\ntory. However, these approaches are only validated for small\nmodels with no more than 10 million parameters.\nThere are also some works leveraging random seeds to op-\ntimize communication efficiency. However, they are not\nsuitable for full-parameter tuning of LLMs with FL due to\n2\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\n(1) distributing the latest model parameters in each round\n(Xu et al., 2023; Maritan et al., 2023; Feng et al., 2023) that\nhinders the important download efficiency of clients (Dorf-\nman et al., 2023), or (2) tremendous computation overhead\nfor calculating the latest model (Zelikman et al., 2023) as\nin Figure 2, or (3) the reliance on BP which consumes a\nsubstantial amount of memory (Rahimi et al., 2023).\nDifference from Related Works. A recent work FwdLLM\n(Xu et al., 2023) conducts FL based on PEFT and ZOO,\nbut with the goal and techniques different from FedKSeed.\nFwdLLM uses quantization and PEFT to reduce memory\ncost, while we mainly focus on communication cost and\nenable full-parameter tuning of LLMs with FL. FedKSeed\nis orthogonal to quantization techniques (Xi et al., 2023;\nDettmers et al., 2023). FwdLLM requires a total of sev-\neral hundred GB of communication cost to tune an LLM\nwith only about 300 million parameters, as it only optimizes\nclient-side uplink. Besides, although some BP-based meth-\nods optimize the communication (Rahimi et al., 2023), they\nare not tailored and are not suitable for LLM tuning on end\ndevices due to the tremendous memory footprint.\nIn a nutshell, existing works mainly focus on tuning partial\nLLM parameters, while our method enables full-parameter\ntuning of LLMs with FL, obtaining higher accuracy (Table\n2). FedKSeed significantly cuts the communication and\nmemory costs by eliminating model parameter transmission\nand BP, outperforming existing approaches tailored for fed-\nerated LLM tuning (Table 3). Further technical comparisons\nbetween FedKSeed and existing works are in Appendix A.\n3. Problem Formulation\nConsider an FL system with N clients, each with a private\ndataset Di, federated fine-tuning aims at collaboratively\ntuning model w \u2208 Rd with the pre-trained weight w0 \u2208 Rd\nat initialization, which can be formulated as\nmin\nw\u2208Rd f(w) \u225c\nN\nX\ni=1\nci \u00b7 Ex\u223cDi [L(w; x)] ,\n(1)\nwhere L(w; x) is the loss evaluated at model w on a data\ninstance x drawn from Di and ci \u2265 0 is the aggregate\nweight with PN\ni=1ci = 1. Here we utilize x since we set\nthe batch size to 1 to lower memory cost as Malladi et al.\n(2023). The fundamental distinction between federated fine-\ntuning and vanilla FL (McMahan et al., 2017) is that it\nbegins optimization from a pre-trained weight w0, rather\nthan from scratch. Equation (1) is solved in several rounds\nof local training and aggregation. In round r of BP-based\nFL (McMahan et al., 2017), each client i performs several\nsteps of gradient descent algorithms on its local model wr\ni\ninitialized by weight wr downloaded from the server, as\nwr\ni,t+1 = wr\ni,t \u2212 \u03b7 \u00b7 gr\ni,t\n(2)\nwhere wr\ni,t is the local model of client i at local step t, \u03b7\nis the learning rate, and gr\ni,t represents the gradient com-\nputed as\u2207wr\ni,tLi(wr\ni,t; x), \u2200x \u2208 Di. After local training,\nthe server aggregates all received wr\ni for subsequent round.\nThe main difference between ZOO-based FL and BP-based\nFL lies in the obtaining of the gradient during local training.\nZOO-based FL does not directly calculate the gradient, as an\nalternative, it estimates the gradient by forward propagations.\nOur work uses the ZOO paradigm with a two-point gradient\nestimator proposed by Malladi et al. (2023), as\nbgr\ni,t \u225c L(wr\ni,t+\u03f5z; x)\u2212L(wr\ni,t\u2212\u03f5z; x)\n2\u03f5\nz \u2248 zz\u22a4gr\ni,t, (3)\nwhere bgi,t is the estimated gradient, z \u2208 Rd is a ran-\ndom perturbation that follows z \u223c N(0, Id) and \u03f5 is the\nscale of perturbations. When bgr\ni,t is estimated, client i up-\ndates its local model as Equation (2). For symbol conve-\nnience, we denote bg = bg \u00b7 z with omitted scripts, where\nbg \u225c L(w+\u03f5z;x)\u2212L(w\u2212\u03f5z;x)\n2\u03f5\nis termed as scalar gradient.\n4. The proposed FedKSeed\n4.1. Overview\nFedKSeed is designed for federated full-parameter tuning\nof billion-sized LLMs with the following goals: (1) to\navoid the massive communication cost for transmitting full-\nmodel parameters, and (2) to avoid the tremendous memory\nfootprint caused by BP. Considering fine-tuning based on\nZOO typically results in much less convergence slowdown\nthan what is experienced during pretraining (Malladi et al.,\n2023), we design FedKSeed based on ZOO, and propose\na theoretically-informed paradigm that enables seed reuse\nto limit the ever-increasing computational cost of clients to\ncatch up to the latest global model.\nFigure 3 outlines FedKSeed, where the server maintains\nK unique candidate seeds S \u2208 ZK and a scalar gradient\naccumulator A \u2208 RK recording the sum of received scalar\ngradients corresponding to each of the K candidate seed.\nNote that the server holds no model parameters, and we\nassume that each client has possessed a pre-trained LLM\nw0. At the start of each round, the server sends S and A\nto active clients (process \u2780). Then, each client i calculates\nthe latest global model based on A (\u2781) and its local model\nwi. Process \u2782 is a loop of local training, where in each step,\nthe client randomly samples the j-th seed sj from S and a\ndata instance, then calculates the scalar gradient bgj. Next,\nwi is updated based on bgj and sj, and \u27e8sj, bgj\u27e9 is staged to\nthe scalar gradient history Hi. After several steps of local\ntraining, each client sends Hi to the server (\u2783). The server\nupdates A based on all received scalar gradient histories\n(\u2784). We summarize the above processes in Algorithm 1 in\nAppendix B and detail them in the subsequent sections.\n3\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nServer\nCandidate Seed\nAccumulated Scalar Grad.\nsend: \n1\n4\ncalculate seed probabilities\n6\n5 update scalar gradient \naccumulator    \nCandidate Seeds\nSeed Probabilities\nScalar Gradient History\nScalar Gradient Accumulator\n3-2\n1. local model\n2. scalar gradient history \nupdate:\nClient i\nestimate gradient\n1.candidate seeds\n2. scalar gradients accumulator\n3. seed probabilities\n2\nscalar gradient history\nreturn:\ncalculate the latest global model\n1. a data instance\n2. a seed based \n on the probabilities\n3-1\nsample: \nFigure 3. Overview of FedKSeed, where the serial numbers indicate processes in each round. Gray components share identical values\namong all clients. The underlined components are only required by an enhanced version of it, i.e., FedKSeed-Pro (Section 4.3).\n4.2. Federated Full-Parameter Tuning by Limited Seeds\nRecall Equation (3), if clients use the same pseudo number\ngenerator, the perturbation can be encoded to one integer,\ni.e., a random seed. Thus, t steps of update can be replicated\nwith the scalar gradient history H = {\u27e8sj, bgj\u27e9}t containing\nt pairs of seeds and scalar gradients. Therefore, an intuitive\nsolution to alleviate model transmission is to have the server\ntrack the scalar gradients of all clients (Zelikman et al.,\n2023). Assuming m clients participate in FL in each round,\nand each one conducts average \u03c4 steps of local training.\nAfter r rounds, a client has to perform \u03c4rm steps of model\nupdating to get the latest global model with w0 as the initial\npoint. From Figure 2, when m = 50, \u03c4 = 200, and r = 30,\nthis operation requires a lengthy period of over 10 hours.\nRestrict the Cardinality of Candidate Seeds: from In-\nfinite to K. If seeds are reused, the update steps needed\nto get the latest model can be merged. If we select only K\ncandidate seeds and accumulate the scalar gradients corre-\nsponding to the same seed, each client only needs to perform\nat most K iterations to get the latest global model, unlike\nthe solutions with infinite seeds (as outlined in Table 1).\nBefore starting federated fine-tuning, the server randomly\ninitializes K unique candidate seeds S = {s1, s2, . . . , sK},\nand initializes a scalar gradient accumulator A\n=\n{a1, . . . , aK} \u2208 RK, where aj = P\nbg\u2208Gj bg, and Gj col-\nlects all scalar gradients bgj within H on the perturbation\ngenerated by sj. Each client i gets the latest global model\nas its local model wi by downloading A and conducting\nwi = w0 \u2212 \u03b7 \u00b7\nK\nX\nj=1\naj \u00b7 zj.\n(4)\nThen, the latest global model w is treated as the local model\nwi. During each step of local training, the client samples a\ndata instance x and a seed sj \u2208 S, and calculates bgj as\nbgj = L(wi + \u03f5zj; x) \u2212 L(wi \u2212 \u03f5zj; x)\n2\u03f5\n.\n(5)\nThen, the local model wi is updated as\nwi \u2190 wi \u2212 \u03b7bgj \u00b7 zj,\n(6)\nand sj and bgj are tracked in Hi = {\u27e8sj, bgj\u27e9, . . .}. After\nseveral steps of local training, Hi is sent to the server. Then,\nfor each \u27e8sj, bgj\u27e9 \u2208 Hi, the server conducts\naj = aj + ci \u00b7 bgj,\n(7)\nto aggregate the gradient history of client i into A.\nConsidering that K is a predefined constant, this paradigm\nshift reduces the computation complexity of obtaining\nthe latest global model to O(d), which remains constant\nthroughout the progression of FL. Note that the server, just\nlike any client, can obtain the latest global model using\nEquation (4). We refer to the above approach as FedKSeed.\n4.2.1. THEORETICAL SUPPORT FOR SEED REUSE\nThe support to the limited range of seed sampling is that the\ndiversity of perturbations is not relevant to its convergence.\nDefinition 1. (Federated Zeroth-order Optimization or\nFedZO proposed by Fang et al. (2022)). Given z as i.i.d.\nrandom perturbation with N(0, Id) distribution, FedZO\nestimates gradients in a mini-batch manner as\nbgr\ni,t =\n1\nb1b2\nb1\nX\nb=1\nb2\nX\nj=1\n\u0002\nL(wr\ni,t+\u03f5zb; xb)\u2212L(wr\ni,t; xb)\n\u0003\n\u03f5\nzj. (8)\nLemma 1. (Convergence of FedZO.) With the assumptions\nmade by Fang et al. (2022) on (1) loss boundary, (2) ob-\njective smoothness, (3) the second-order gradient moment\nboundary and (4) local-global gradient dissimilarity bound-\nary, FedZO satisfies\nmin\nr\u2208{1,...,T } E\u2225\u2207L(wr)\u22252 \u2264O\n r\nd\n\u03c4mTb1b2\n+\nr\nb1b2\u03c4\ndmT\n!\n,\n(9)\nwhere \u03c4 is the average number of local iterations within one\nround for each client, and T is the number of total rounds.\n4\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nLemma 1 has been proved by Fang et al. (2022), and the\nfour mentioned assumptions are detailed in Appendix C.\nLemma 2. FedKSeed and FedZO share the same conver-\ngence, when b1 and b2 of FedZO are both set to 1.\nProof. The one-point estimator in Equation (8) adopted by\nFedZO is equivalent to the two-point estimator (Malladi\net al., 2023) adopted by FedKSeed in terms of convergence,\nsince the gradient estimated by the two-point estimator is\nequivalent to the average of the gradients estimated by one-\npoint estimator with two opposing perturbations, as\nz [L(w+\u03f5z; x)\u2212L(w; x)]+(\u2212z) [L(w\u2212\u03f5z; x)\u2212L(w; x)]\n= z [L(w + \u03f5z; x) \u2212 L(w \u2212 \u03f5z; x)] .\nBesides, the proof of Lemma 1 by Fang et al. (2022) does\nnot rely on the seed diversity. Thus, FedKSeed is theoret-\nically equivalent to a special case of FedZO in terms of\nconvergence analysis where both b1 and b2 are set to 1.\nIn FedKSeed, we adopt the memory-efficient two-point\ngradient estimator proposed by Malladi et al. (2023), since a\ntwo-point gradient estimator has a lower variance compared\nto the one-point paradigm (Liu et al., 2018).\nTheorem 1. With the same assumptions relied upon by\nLemma 1, FedKSeed satisfies\nmin\nr\u2208{1,...,T } E \u2225\u2207L(wr)\u22252 \u2264 O\n r\nd\n\u03c4mT\n!\n.\n(10)\nProof.\nBased on Lemmas 1 and 2, we have the con-\nvergence of FedKSeed the same as Equation (9). With\nb1 = b2 = 1, the convergence of FedKSeed is dominated\nby O\n\u0010\nd\n1\n2 (\u03c4mT)\u2212 1\n2\n\u0011\n, therefore we have Theorem 1.\nAccording to the above analysis, we can conclude that the\ndiversity of perturbations theoretically has no impact on\nthe convergence, thus we can enable the reuse of random\nseeds. We also experimentally demonstrate it by presenting\nthe loss on the test set on Natural Instructions (Wang et al.,\n2022) dataset (settings aligned with Section 5.1).\n(a) DataJuicer-1.3B\n(b) LLaMA-3B\nFigure 4. Convergence of full-parameter tuning by FedKSeed and\nFedZO (b1 = b2 = 1, simulated due to communication cost).\nEmpirical Observation. As presented in Figure 4, FedKSeed\nand FedZO (b1 = 1, b2 = 1) share similar convergence\ntrend and rate, confirming Lemma 2 and Theorem 1.\n4.2.2. SELECTION OF K\nThe selection of K can be guided by the intrinsic dimension\ntheory (Li et al., 2018; Aghajanyan et al., 2021). Given\nG =\n\u0002P\nbg\u2208G1 bg, . . . , P\nbg\u2208GK bg\u0003\u22a4, where P\nbg\u2208Gj bg is the\nsummation of all scalar gradients corresponding to the per-\nturbation with sj. Equation (1) can be transformed to\nmin\nG\u2208RK\nN\nX\ni=1\nci \u00b7 Ex\u223cDi\n\uf8ee\n\uf8ef\uf8f0L( w0+\n\u0002\nz1, . . . , zK\n\u0003\nG\n|\n{z\n}\nThe difference from Equation (1)\n; x)\n\uf8f9\n\uf8fa\uf8fb . (11)\nThus, FedKSeed actually performs federated tuning in a K-\ndimensional random subspace. Equation (11) matches the\nform that trains a neural network in a subspace with the di-\nmensionality equivalent to the intrinsic dimension (Li et al.,\n2018), with\n\u0002z1, . . . , zK\n\u0003\nas the randomly generated projec-\ntion matrix. Both w0 and\n\u0002z1, . . . , zK\n\u0003\nare frozen during\ntraining. Thus, we can determine K in the vicinity of the\nLLM\u2019s intrinsic dimension, which may approximately fall\nbetween 103 and 104 following Aghajanyan et al. (2021).\nPrinciple 1. (Seed Insufficiency.) There exists a threshold\n\u21bc\nK such that when K \u2264\n\u21bc\nK, the accuracy of the model\ndecreases with the reduction of K.\nAccording to Li et al. (2018), when K is less than the codi-\nmension of the solution, solutions will almost not be found\nin the subspace, or the founded solution is low-quality, since\nlow-dimensional subspaces do not possess sufficient com-\nplexity to embed the solution manifold. We also provide\nadditional theoretical support for Principle 1 from the per-\nspective of solving optimization problems in Appendix D.1.\nBased on these analyses,\n\u21bc\nK theoretically exists so that Prin-\nciple 1 holds. We note that 1,024 serves as a good empirical\nestimate of it, as shown in Figure 5, which typically approx-\nimates the intrinsic dimension.\nPrinciple 2. (Seed Excessiveness.) There exists a thresh-\nold\n\u21c0\nK such that given the total number of local training\nsteps fixed, when K \u2265\n\u21c0\nK, there is no upward trend in the\naccuracy of the model with the increase of K.\nWhen K surpasses the intrinsic dimension, the marginal\ngain in accuracy becomes increasingly smaller with the in-\ncrease of K, since further increasing K does not increase\nthe ability to approximate the solution manifold. The redun-\ndancy of K affects FedKSeed similar to that reported by Li\net al. (2018); Aghajanyan et al. (2021) but with slight differ-\nences. In FedKSeed, only one element of G is optimized in\neach training step. Intuitively, each element of G requires\nseveral steps to be accurately estimated. Given the fixed\ntotal number of update steps \u03c4rm, each element of G con-\nsumes \u03c4rm\nK\nsteps averagely. Thus, increasing K reduces the\nnumber of training steps for each element of G. When K\n5\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nhas reached an ample magnitude, this increment may induce\nadverse effects, as shown in Figure 5. We provide additional\ntheoretical support to Principle 2 from the perspective of\nbatch size in Appendix D.2, and experimental support from\nthe marginal gain on accuracy in terms of seed quantity in\nAppendix F.1. From these analyses,\n\u21c0\nK theoretically exists\nthus Principle 2 holds. From Figure 5, an empirically es-\ntimated value of it is 4096, which lies around the intrinsic\ndimension and tends to be slightly larger than it.\nIt is hard to map a specific LLM to a precise value of K due\nto the complex architecture of LLMs. From the analyses,\nwe can choose K in [\n\u21bc\nK,\n\u21c0\nK], i.e., an integer slightly larger\nthan the intrinsic dimension. Section 5.2 experimentally\ndemonstrates that for models with 1B and 3B parameters,\nK can be several thousand so that FedKSeed performs well.\n4.3. Sampling Seeds with Non-uniform Probabilities\nThis section enhances FedKSeed through enabling non-\nuniform probabilities for seed sampling to further reduce\nK and boost the model accuracy, termed as FedKSeed-Pro.\nThe gradient g indicates the direction of the steepest de-\nscent for a loss function L at a given point. However, in\nFedKSeed, g is not available due to the removal of BP. The\nscalar gradient can be regarded as the estimated directional\nderivative of L along z. The similarity between different\ndirectional vectors and the gradient varies, affecting the rate\nof change in the objective and thus contributing differently\nto the descent of the loss function. The scalar gradient is\ndetermined by both the model, the data instance and the\nsimilarity between true gradient g and z. Since the model\nand data instances are equivalent in expectation for all per-\nturbations, the average amplitude of scalar gradient \u03c8j can\ncharacterize the importance of zj. Thus, we can computes\nthe probabilities pj of candidate seed sj based on \u03a8, as\npj =\nexp(\u03c8j)\nPK\nk=1 exp(\u03c8k)\n, where \u03c8j =\n1\n|Gj|\nX\nbg\u2208Gj\n|bg| .\n(12)\nTo avoid too large probability differences among S, we\nnormalize \u03a8 = {\u03c81, . . . , \u03c8K} by min-max normalization.\nThe probabilities P = {p1, . . . , pK} are updated and sent to\nactive clients in each round to guide the seed sampling of\nlocal training. In Section 5.2, we find that when significant\nseeds are sampled with higher probabilities, we can reduce\nthe cardinality of seeds required without decreasing the\nmodel accuracy, sometimes, it even increases the accuracy.\n5. Experiments\n5.1. Experimental Setup\nBaselines. We choose 4 practical methods tailored for fed-\nerated LLM tuning as the baselines, including: (1) FedP-\nTuning (Kuang et al., 2023) with P-Tuning (Liu et al., 2023)\nas the PEFT technique, trained by SGD; (2) FedPrompt\n(Kuang et al., 2023) with Prompt Tuning (Lester et al.,\n2021) as the PEFT technique, trained by SGD; (3) FedIT: a\nfederated instruction tuning approach proposed by Zhang\net al. (2023a), with LoRA as the PEFT technique and Adam\n(Kingma & Ba, 2015) as the optimizer; and (4) FedIT-SGD:\na variation of FedIT that replaces Adam with SGD. Note\nthat full-parameter tuning approaches such as FedZO and\nvanilla FedAvg are not adopted as our baselines due to their\nmassive communication and memory costs, respectively.\nDatasets & Evaluation Metric. We adopt Natural Instruc-\ntions (NI) (Wang et al., 2022) and Dolly-15K (Conover et al.,\n2023) as the datasets. Each of the 738 training tasks in NI is\nassigned to a unique client for training while the test tasks\nare used for evaluation, building a non-IID scenario with\nfeature distribution skew (Tan et al., 2023). The last task of\nDolly-15K is used for evaluation and the others for training,\nwhere the training data are split to 200 clients via Dirichlet\ndistribution with \u03b1=0.5 and \u03b1=5.0, building non-IID scenar-\nios with varying degrees of label distribution skew (Chen\net al., 2023d). Rouge-L (Lin, 2004) is used as the evaluation\nmetric following Dettmers et al. (2023), which correlates\nthe trend on classification tasks of accuracy (Wang et al.,\n2022). Considering the limited resources of end devices, we\ntake DataJuicer-1.3B (Chen et al., 2023c) and LLaMA-3B\n(Touvron et al., 2023) as the foundation models.\nImplementations. We randomly sample 5% of the clients\nto participate in FL in each round. The total number of com-\nmunication rounds is set to 40 for NI and 60 for Dolly-15K.\nBP-based baselines conduct local training for one epoch,\nand FedKSeed and FedKSeed-Pro conduct local training for\n200 steps. Unless stated otherwise, we set K to 4096 for\nFedKSeed, 1024 for FedKSeed-Pro with DataJuicer-1.3B,\nand 2048 for FedKSeed-Pro with LLaMA-3B. Note that\nfrom Figure 5, these settings are not tailored for the best val-\nues in corresponding scenarios, ensuring fair comparisons.\nPlease refer to Appendix E for more implementation details.\n5.2. Comparisons on Accuracy Performance\nOverall results. Table 2 provides Rouge-L of these ap-\nproaches. It can be observed that FedKSeed and FedKSeed-\nPro achieve the top two performances across all six scenar-\nios. In particular, on Dolly-15K (\u03b1 = 0.5) with LLaMA-3B,\nFedKSeed-Pro outperforms the best baseline, FedIT, by\n3.06%. These improvements can be attributed to the bene-\nfits of full-parameter tuning, where the number of trainable\nparameters is significantly larger compared to PEFT tech-\nniques, as shown in Table 3. Furthermore, we observe that\nthe gains achieved by FedKSeed-Pro over the best baseline,\nFedIT, are generally larger with LLaMA-3B than DataJuicer-\n6\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nTable 2. Rouge-L (%) comparisons on widely-adopted LLMs and datasets. Each cell presents the average Rouge-L at the last round of\nfour runs with different random seeds. Bold and underlined numbers indicate the best and second-best average Rouge-L, respectively.\nApproach\nNatural Instructions\nDolly-15K (\u03b1 = 0.5)\nDolly-15K (\u03b1 = 5.0)\nDataJuicer-1.3B\nLLaMA-3B\nDataJuicer-1.3B\nLLaMA-3B\nDataJuicer-1.3B\nLLaMA-3B\nFedPTuning\n19.61 \u00b1 2.71\n25.41 \u00b1 1.14\n23.98 \u00b1 3.23\n30.30 \u00b1 1.16\n25.33 \u00b1 2.48\n29.08 \u00b1 1.33\nFedPrompt\n6.04 \u00b1 0.12\n8.95 \u00b1 2.47\n32.73 \u00b1 0.87\n24.50 \u00b1 4.78\n32.51 \u00b1 1.31\n23.94 \u00b1 4.15\nFedIT-SGD\n19.40 \u00b1 1.83\n28.14 \u00b1 0.85\n27.23 \u00b1 0.68\n29.28 \u00b1 0.50\n27.28 \u00b1 1.35\n29.19 \u00b1 0.89\nFedIT\n22.30 \u00b1 0.42\n28.13 \u00b1 0.50\n30.80 \u00b1 0.98\n33.23 \u00b1 1.51\n30.97 \u00b1 0.43\n33.68 \u00b1 1.07\nFedKSeed\n22.33 \u00b1 1.72\n29.77 \u00b1 0.75\n32.90 \u00b1 0.37\n35.64 \u00b1 0.83\n33.12 \u00b1 0.31\n35.93 \u00b1 1.35\nFedKSeed-Pro\n23.50 \u00b1 1.35\n30.19 \u00b1 1.10\n33.18 \u00b1 0.68\n36.29 \u00b1 0.63\n33.00 \u00b1 0.34\n35.95 \u00b1 1.41\nTable 3. Per-round communication overhead and Peak GPU mem-\nory footprint of the approaches, where \u201cB\u201d denotes \u201cBytes\u201d.\nApproach\nDataJuicer-1.3B\nLLaMA-3B\nCommun. Memory Commun. Memory\nFedPTuning\n96.36 MB 11.9 GB 234.9 MB 16.3 GB\nFedPrompt\n320.0 KB 11.8 GB 500.0 KB 19.0 GB\nFedIT-SGD\n12.00 MB 12.4 GB 20.31 MB 18.2 GB\nFedIT\n12.00 MB 12.4 GB 20.31 MB 18.3 GB\nFedKSeed\n17,988 B\n3.5 GB\n17,988 B\n7.8 GB\nFedKSeed-Pro\n9,796 B\n3.5 GB\n17,988 B\n7.8 GB\n1.3B, since with the same LoRA configuration, the model\nsize increase does not proportionally affect the number of\ntrainable parameters in FedIT as much as it does in our\napproaches. We illustrate the convergence in Appendix F.2.\nEffect of K. To validate Principles 1 and 2, and to under-\nstand the relationship between the number of perturbation\nseeds (K) and the accuracy of FedKSeed and FedKSeed-\nPro, we examine their performance with varying K, as de-\npicted in Figure 5. We observe that when the K exceeds the\nrecommended range specified in Section 5.1, the accuracy\ndoes not improve and may occasionally decline. Because\nthe total number of optimization steps is constant, with\nmore seeds, the likelihood that each seed consumes suffi-\ncient data to determine its step size is reduced. Conversely,\nwith too few seeds, the performance of both FedKSeed and\nFedKSeed-Pro deteriorates due to the limited expressiveness\nresulting from an insufficient number of perturbations. Thus,\nthe value of K should be balanced as discussed in Section\n4.2.2: not too high to waste computational costs, nor too\nlow to restrict the model\u2019s expressiveness. Our experimental\nresults indicate that for models with 1B to 3B parameters,\nsetting K in the range of [1024, 4096] is preferable.\nEffect of seed probabilities. FedKSeed-Pro gains superior\nperformance in five out of six scenarios and comparable\nresults on Dolly-15K (\u03b1 = 5.0) with DataJuicer-1.3B com-\npared to FedKSeed. This highlights the effectiveness of\nthe non-uniform seed sampling proposed in Section 4.3.\nFedKSeed-Pro makes the probabilities of each seed being\nsampled differ by several multiples, as shown in Appendix\nF.3. Thus, by allowing seeds of greater importance (larger\nscalar gradient magnitudes) to be sampled with higher prob-\nability, the accuracy of FedKSeed can be further enhanced.\n5.3. Comparisons on Overheads\nTable 3 shows that FedKSeed and FedKSeed-Pro achieve the\nleast communication and memory costs, where the K ran-\ndomly selected seeds are encoded by one seed and thus only\noccupy 4 Bytes. The time cost of these approaches is illus-\ntrated in Appendix F.4. The calculation of communication\ncost is detailed in Appendix G. FedKSeed and FedKSeed-\nPro enhance communication efficiency by removing the\ntransmission of all trainable parameters, and memory ef-\nficiency by omitting BP and applying the in-place ZOO\n(Malladi et al., 2023). Thus, they can be effectively applied\nto tune full LLMs on end devices with limited communi-\ncation and memory budgets. Besides, FedKSeed-Pro also\noutperforms FedKSeed in the efficiency of synchronizing\nthe latest model as shown in Figure 6, because FedKSeed-\nPro achieves higher accuracy with only 1,024 seeds, while\nFedKSeed requires a larger cardinality of candidate seeds,\nwhich consumes a longer time to calculate the latest model.\n5.4. Hyper-parameter Sensitivity\nGiven that ZOO serves as the foundational technique for\nlocal training in our approaches, we examine the impact of\nkey hyperparameters in ZOO, i.e., the learning rate (\u03b7) and\nthe perturbation scale (\u03f5), with FedKSeed-Pro on LLaMA-\n3B with Dolly-15K (\u03b1 = 0.5) as an example. From Figure\n7, both \u03b7 and \u03f5 should not be set excessively high. Since \u03f5\ndetermines the magnitude of perturbations applied during\ngradient estimation, theoretically, a smaller \u03f5 leads to a more\naccurate gradient approximation. However, too small \u03f5 may\nresult in numerical underflow, especially when using half-\nprecision floating-point numbers. An overly large value of \u03b7\ncan result in too aggressive update steps, potentially causing\nthe model to deviate far from the optima or even diverge.\n5.5. Comparisons in Various Federated Scenarios\nWe further evaluate the two strongest baselines and our ap-\nproaches under different FL settings. Figure 8(a) presents\n7\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\n(a) DataJuicer-1.3B Natural Instructions\n(c) DataJuicer-1.3B Dolly-15K (\u03b1 = 0.5) (e) DataJuicer-1.3B Dolly-15K (\u03b1 = 5.0)\n(b) LLaMA-3B Natural Instructions\n(d) LLaMA-3B Dolly-15K (\u03b1 = 0.5)\n(f) LLaMA-3B Dolly-15K (\u03b1 = 5.0)\nFigure 5. Performance of FedKSeed and FedKSeed-Pro with different cardinality of candidate seeds.\nFigure 6. Time consumed to\ncalculate the latest model.\nFigure 7. FedKSeed-Pro on Dolly\n(\u03b1=0.5) with various \u03b7 and \u03f5.\nthat, although not always monotonically, these approaches\ngain better accuracy as N increases due to the models being\nfine-tuned on a larger volume of training data, confirming\nthe importance of federated tuning since it can leverage a\nbroader range of data sources. Figure 8(b) presents that with\nmore clients participating in FL in each round, the accu-\nracy of these approaches becomes higher and more stable.\nBesides, our approaches still outperform the baselines in\nvarious FL scenarios, further demonstrating their superiority.\n6. Conclusion\nExisting federated fine-tuning approaches for LLMs usually\nrely on PEFT techniques. Considering PEFT still falls short\nin FL scenarios compared to full-parameter tuning, we focus\non enabling full-parameter tuning of billion-sized LLMs on\ndevices with FL. To fulfill this, we design FedKSeed char-\nacterized by a theoretically-informed seed-reuse paradigm,\nwhere only a limited number of candidate seeds and corre-\n(a) Impact of total client count\nN (on Natural Instructions).\n(b) Impact of activate client ratio\nm/N (on Dolly-15K \u03b1=0.5).\nFigure 8. Performances in various FL scenarios with LLaMA-3B.\nsponding scalar gradients need to be transmitted between\nthe server and clients. It enables federated full-parameter\ntuning of LLMs with per-round communication costs lower\nthan 18 kilobytes. Based on FedKSeed, inspired by the fact\nthat the scalar gradient of a perturbation is the directional\nderivative of the true gradient, we propose a strategy to\nquantify the importance of seeds and grant differentiated\nsampling probabilities to them, which reduces the number\nof required seeds, thus speeding up the obtaining of the\nlatest model while achieving higher accuracy compared to\nFedKSeed. Experiments conducted on two widely adopted\ninstruction-tuning datasets demonstrate our proposed ap-\nproaches surpass FL baselines tailored for LLM tuning on\naccuracy, communication cost and memory footprint.\nOur work raises some new potential research directions,\nsuch as decentralized federated fine-tuning since the commu-\nnication cost is more critical in this context. More benefits\nbrought by this work are discussed in Appendix H.\n8\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nReferences\nAghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrin-\nsic dimensionality explains the effectiveness of language\nmodel fine-tuning. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 7319\u2013\n7328, 2021.\nBabakniya, S., Elkordy, A. R., Ezzeldin, Y. H., Liu, Q.,\nSong, K.-B., El-Khamy, M., and Avestimehr, S. SLoRA:\nFederated parameter efficient fine-tuning of language\nmodels. arXiv preprint arXiv:2308.06522, 2023.\nBorzunov, A., Ryabinin, M., Chumachenko, A., Baranchuk,\nD., Dettmers, T., Belkada, Y., Samygin, P., and Raffel, C.\nDistributed inference and fine-tuning of large language\nmodels over the internet. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\nChe, T., Liu, J., Zhou, Y., Ren, J., Zhou, J., Sheng, V. S.,\nDai, H., and Dou, D. Federated learning of large lan-\nguage models with parameter-efficient prompt tuning and\nadaptive optimization. arXiv preprint arXiv:2310.15080,\n2023.\nChen, C., Feng, X., Zhou, J., Yin, J., and Zheng, X. Fed-\nerated large language model: A position paper. arXiv\npreprint arXiv:2307.08925, 2023a.\nChen, D., Gao, D., Xie, Y., Pan, X., Li, Z., Li, Y., Ding,\nB., and Zhou, J. FS-REAL: Towards real-world cross-\ndevice federated learning. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 3829\u20133841, 2023b.\nChen, D., Huang, Y., Ma, Z., Chen, H., Pan, X., Ge, C., Gao,\nD., Xie, Y., Liu, Z., Gao, J., Li, Y., Ding, B., and Zhou,\nJ. Data-juicer: A one-stop data processing system for\nlarge language models. arXiv preprint arXiv:2309.02033,\n2023c.\nChen, D., Yao, L., Gao, D., Ding, B., and Li, Y. Efficient per-\nsonalized federated learning via sparse model-adaptation.\nIn International Conference on Machine Learning, ICML,\nvolume 202, pp. 5234\u20135256, 2023d.\nChen, G., Liu, F., Meng, Z., and Liang, S.\nRevisiting\nparameter-efficient tuning: Are we really there yet? In\nProceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP, pp. 2612\u2013\n2626, 2022.\nChen, J., Chen, H., Gu, B., and Deng, H. Fine-grained\ntheoretical analysis of federated zeroth-order optimiza-\ntion. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023e.\nConover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah,\nS., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free\ndolly: Introducing the world\u2019s first truly open instruction-\ntuned LLM, 2023.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.\nQLoRA: Efficient finetuning of quantized LLMs. arXiv\npreprint arXiv:2305.14314, 2023.\nDong, X. L., Moon, S., Xu, Y. E., Malik, K., and Yu, Z.\nTowards next-generation intelligent assistants leverag-\ning LLM techniques. In Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining, pp. 5792\u20135793, 2023.\nDorfman, R., Vargaftik, S., Ben-Itzhak, Y., and Levy, K. Y.\nDoCoFL: downlink compression for cross-device feder-\nated learning. In International Conference on Machine\nLearning, pp. 8356\u20138388. PMLR, 2023.\nFan, T., Kang, Y., Ma, G., Chen, W., Wei, W., Fan, L.,\nand Yang, Q. FATE-LLM: A industrial grade federated\nlearning framework for large language models. CoRR,\nabs/2310.10049, 2023.\nFang, W., Yu, Z., Jiang, Y., Shi, Y., Jones, C. N., and Zhou,\nY. Communication-efficient stochastic zeroth-order op-\ntimization for federated learning. IEEE Transactions on\nSignal Processing, 70:5058\u20135073, 2022.\nFeng, H., Pang, T., Du, C., Chen, W., Yan, S., and Lin,\nM. Does federated learning really need backpropagation?\narXiv preprint arXiv:2301.12195, 2023.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. LoRA: Low-rank adaptation\nof large language models. In The Tenth International\nConference on Learning Representations, ICLR, 2022.\nHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and\nWeinberger, K. Q. Snapshot ensembles: Train 1, get\nM for free. In International Conference on Learning\nRepresentations, 2016.\nJiang, J., Liu, X., and Fan, C. Low-parameter federated\nlearning with large language models.\narXiv preprint\narXiv:2307.13896, 2023.\nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis,\nM., Bhagoji, A. N., Bonawitz, K. A., Charles, Z., Cor-\nmode, G., Cummings, R., D\u2019Oliveira, R. G. L., Eichner,\nH., Rouayheb, S. E., Evans, D., Gardner, J., Garrett, Z.,\nGasc\u00b4on, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Har-\nchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu,\nJ., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Kone\u02c7cn\u00b4y,\nJ., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T.,\nLiu, Y., Mittal, P., Mohri, M., Nock, R., \u00a8Ozg\u00a8ur, A., Pagh,\nR., Qi, H., Ramage, D., Raskar, R., Raykova, M., Song,\n9\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nD., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tram`er,\nF., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang,\nQ., Yu, F. X., Yu, H., and Zhao, S. Advances and open\nproblems in federated learning. Foundations and Trends\u00ae\nin Machine Learning, 14(1-2):1\u2013210, 2021.\nKingma, D. P. and Ba, J. Adam: A method for stochastic op-\ntimization. In 3rd International Conference on Learning\nRepresentations, ICLR, 2015.\nKuang, W., Qian, B., Li, Z., Chen, D., Gao, D., Pan, X.,\nXie, Y., Li, Y., Ding, B., and Zhou, J. Federatedscope-\nLLM: A comprehensive package for fine-tuning large\nlanguage models in federated learning. arXiv preprint\narXiv:2309.00363, 2023.\nLester, B., Al-Rfou, R., and Constant, N. The power of scale\nfor parameter-efficient prompt tuning. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP, pp. 3045\u20133059, 2021.\nLi, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring\nthe intrinsic dimension of objective landscapes. In Inter-\nnational Conference on Learning Representations, ICLR,\n2018.\nLi, Q., Diao, Y., Chen, Q., and He, B. Federated learning\non non-IID data silos: An experimental study. In 2022\nIEEE 38th International Conference on Data Engineering,\nICDE, pp. 965\u2013978. IEEE, 2022.\nLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar,\nA., and Smith, V. Federated optimization in heteroge-\nneous networks. In Proceedings of Machine Learning\nand Systems 2020, MLSys 2020, 2020a.\nLi, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On\nthe convergence of fedavg on non-iid data. In 8th Inter-\nnational Conference on Learning Representations, ICLR,\n2020b.\nLi, Z. and Chen, L. Communication-efficient decentralized\nzeroth-order method on heterogeneous data. In Inter-\nnational Conference on Wireless Communications and\nSignal Processing, WCSP, pp. 1\u20136, 2021.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74\u201381, 2004.\nLiu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and\nAmini, L. Zeroth-order stochastic variance reduction for\nnonconvex optimization. Advances in Neural Information\nProcessing Systems, 31, 2018.\nLiu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z.,\nand Tang, J. GPT understands, too. AI Open, 2023.\nMalladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,\nChen, D., and Arora, S. Fine-tuning language models\nwith just forward passes. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\nMangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S.,\nand Bossan, B. PEFT: State-of-the-art parameter-efficient\nfine-tuning methods, 2022.\nMaritan, A., Dey, S., and Schenato, L. FedZeN: Towards\nsuperlinear zeroth-order federated learning via incremen-\ntal hessian estimation. arXiv preprint arXiv:2309.17174,\n2023.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and\ny Arcas, B. A. Communication-efficient learning of deep\nnetworks from decentralized data. In Artificial intelli-\ngence and statistics, pp. 1273\u20131282. PMLR, 2017.\nMelas-Kyriazi, L. and Wang, F.\nIntrinisic gradient\ncompression for federated learning.\narXiv preprint\narXiv:2112.02656, 2021.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. PyTorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nPu, G., Jain, A., Yin, J., and Kaplan, R. Empirical analysis\nof the strengths and weaknesses of PEFT techniques for\nLLMs. In ICLR 2023 Workshop on Mathematical and\nEmpirical Understanding of Foundation Models, 2023.\nQin, Z., Yan, X., Zhou, M., Zhao, P., and Deng, S.\nBlockDFL: A blockchain-based fully decentralized feder-\nated learning framework, 2023.\nRahimi, M. M., Bhatti, H. I., Park, Y., Kousar, H., and\nMoon, J. EvoFed: Leveraging evolutionary strategies for\ncommunication-efficient federated learning. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023.\nRothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I.,\nBraverman, V., Gonzalez, J., and Arora, R. FetchSGD:\nCommunication-efficient federated learning with sketch-\ning. In Proceedings of the 37th International Confer-\nence on Machine Learning, ICML, volume 119, pp. 8253\u2013\n8265, 2020.\nShayan, M., Fung, C., Yoon, C. J., and Beschastnikh, I.\nBiscotti: A blockchain system for private and secure\nfederated learning. IEEE Transactions on Parallel and\nDistributed Systems, 32(7):1513\u20131525, 2020.\nShu, Y., Lin, X., Dai, Z., and Low, B. K. H. Federated zeroth-\norder optimization using trajectory-informed surrogate\ngradients. arXiv preprint arXiv:2308.04077, 2023.\n10\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing, pp. 1631\u20131642, 2013.\nSun, X., Ji, Y., Ma, B., and Li, X. A comparative study\nbetween full-parameter and LoRA-based fine-tuning on\nchinese instruction data for instruction following large\nlanguage model. arXiv preprint arXiv:2304.08109, 2023.\nTan, A. Z., Yu, H., Cui, L., and Yang, Q. Towards person-\nalized federated learning. IEEE Transactions on Neu-\nral Networks and Learning Systems, 34(12):9587\u20139603,\n2023.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\nalpaca: An instruction-following LLaMA model, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. LLaMA: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\nVillalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn,\nM., and Ho, A. Will we run out of data? an analysis of\nthe limits of scaling datasets in machine learning. arXiv\npreprint arXiv:2211.04325, 2022.\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,\nMirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S.,\nArunkumar, A., Stap, D., Pathak, E., Karamanolakis, G.,\nLai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuz-\nnia, K., Doshi, K., Pal, K. K., Patel, M., Moradshahi,\nM., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R.,\nVerma, P., Puri, R. S., Karia, R., Doshi, S., Sampat, S. K.,\nMishra, S., A, S. R., Patro, S., Dixit, T., and Shen, X.\nSuper-naturalinstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP, pp. 5085\u20135109. Association\nfor Computational Linguistics, 2022.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\nmodels are zero-shot learners. In The Tenth International\nConference on Learning Representations, ICLR, 2022.\nWoisetschl\u00a8ager, H., Isenko, A., Wang, S., Mayer, R., and\nJacobsen, H.-A. Federated fine-tuning of LLMs on the\nvery edge: The good, the bad, the ugly. arXiv preprint\narXiv:2310.03150, 2023.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\net al. Transformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020 conference on em-\npirical methods in natural language processing: system\ndemonstrations, pp. 38\u201345, 2020.\nXi, H., Li, C., Chen, J., and Zhu, J. Training transformers\nwith 4-bit integers. arXiv preprint arXiv:2306.11987,\n2023.\nXu, M., Wu, Y., Cai, D., Li, X., and Wang, S. Federated fine-\ntuning of billion-sized language models across mobile\ndevices. arXiv preprint arXiv:2308.13894, 2023.\nZelikman, E., Huang, Q., Liang, P., Haber, N., and Good-\nman, N. D. Just one byte (per gradient): A note on low-\nbandwidth decentralized language model finetuning using\nshared randomness. arXiv preprint arXiv:2306.10015,\n2023.\nZhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R.,\nWang, G., and Chen, Y. Towards building the feder-\nated GPT: Federated instruction tuning. arXiv preprint\narXiv:2305.05644, 2023a.\nZhang, K., Tao, G., Xu, Q., Cheng, S., An, S., Liu, Y., Feng,\nS., Shen, G., Chen, P., Ma, S., and Zhang, X. FLIP: A\nprovable defense framework for backdoor mitigation in\nfederated learning. In The Eleventh International Confer-\nence on Learning Representations, ICLR, 2023b.\nZhang, Z., Yang, Y., Dai, Y., Wang, Q., Yu, Y., Qu, L., and\nXu, Z. FedPETuning: When federated learning meets\nthe parameter-efficient tuning methods of pre-trained lan-\nguage models. In Findings of the Association for Compu-\ntational Linguistics: ACL, pp. 9963\u20139977, 2023c.\n11\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nAppendix\nWe provide more discussions and experiments of this work in the appendix and organize them as follows:\n\u2022 Appendix A: we provide detailed comparisons of our approach to existing approaches that are related to our work, to\nbetter present the technical distinctions.\n\u2022 Appendix B: we summarize the processes of FedKSeed and FedKSeed-Pro in Algorithm 1.\n\u2022 Appendix C: we provide detailed assumptions relied upon Lemma 1 and Theorem 1.\n\u2022 Appendix D: we offer more analytical support for Principles 1 and 2 from a perspective distinct from that has been\ndiscussed in Section 4.2.2.\n\u2022 Appendix E: we provide implementation details of our approach for better reproducibility.\n\u2022 Appendix F: we provide supplementary experiments of this work, including (1) the marginal improvement on Rouge-\nL obtained with each additional seed across different ranges of seed quantity by FedKSeed and FedKSeed-Pro in\nAppendix F.1; (2) the convergence illustrations of FedKSeed, FedKSeed-Pro and the baselines in Appendix F.2; (3)\nillustrations of seed probabilities calculated by FedKSeed-Pro in Appendix F.3; and (4) the time consumption for local\ntraining in Appendix F.4.\n\u2022 Appendix G: we provide the detailed calculation of the communication overhead of our approach and the baselines, to\nexplain how to derive the numerical communication overheads of these approaches in Table 3.\n\u2022 Appendix H: we discuss the extended benefits brought by our proposed approach in real-world applications, where\neach benefit is discussed in a separate subsection.\nA. Detailed Technical Comparisons\nIn this section, We compare our approach with existing approaches that have a certain relationship with our approach from\ntechnical, objective, and performance perspectives.\nIn recent years, there have been some researches that utilize the universal random seeds to lower the communication cost\nof transmitting model parameters or gradients (Xu et al., 2023; Feng et al., 2023; Zelikman et al., 2023; Rahimi et al.,\n2023). These approaches can respectively achieve unidirectional O(d) (Xu et al., 2023; Feng et al., 2023), bidirectional\nO(1) (Zelikman et al., 2023; Rahimi et al., 2023) communication cost.\nFwdLLM (Xu et al., 2023), BAFFLE (Feng et al., 2023) and FedZeN (Maritan et al., 2023) do not limit the number of\ncandidate seeds for communication compression. Thus, they still need to distribute the latest trainable model parameters to\nclients in each round. If they are adopted for full-parameter tuning of LLMs with FL, clients have to consume tremendous\ncommunication resources to download the latest LLM in each round, thus, it may prevent many clients from participating in\nFL since the cost and quality of a wireless connection can vary greatly between different countries (Dorfman et al., 2023).\nFor the approach achieves bidirectional O(1) communication cost proposed by Zelikman et al. (2023), it optimizes\ncommunication efficiency to the utmost. However, there is no such thing as a free lunch. As we have discussed in Figure 2,\nsince Zelikman et al. (2023) sample seeds from almost infinite space, each client must replicate the update steps performed\nby all other clients to obtain the latest model. Therefore, as the rounds of FL continue, the overhead of calculating the latest\nmodel from w0 also grows indefinitely, quickly reaching a level that is unsustainable for end devices. Since the fine-tuning\nof large models typically requires running many steps on a large instruction dataset, the approach proposed by Zelikman\net al. (2023) is not suitable for full-parameter tuning of LLMs with FL on devices. Moreover, Zelikman et al. (2023) conduct\nexperiments only on a small sentiment classification dataset, i.e., SST2 (Socher et al., 2013), and train a model for in a total\nof only 16,000 steps, while on a complex dataset, e.g., Natural Instructions (Wang et al., 2022), such several steps of updates\nare not sufficient to the convergence of LLMs.\nOne recent work, EvoFed (Rahimi et al., 2023), also achieves bidirectional O(1) communication cost. However, EvoFed\nis fundamentally different from FedKSeed, and it is not designed for LLM fine-tuning: (1) During local training, EvoFed\nfirst conducts the BP process to get the true gradient. Then it generates K noise-perturbed model populations and tries to\nrepresent the true gradient by the summation of these noise perturbations, where the weight coefficient of each population\n12\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nis determined by the l2 norm of the pair-wise differences between it and the true gradient. Thus, EvoFed still relies on\nthe BP process to get the true gradient, which limits its applicability to billion-sized LLMs. As reported by Malladi et al.\n(2023), fine-tuning of full LLMs requires tremendous memory overhead, e.g., full-parameter tuning of an LLM with 2.7B\nparameters with the average number of tokens set to 400 causes 55GB memory consumption. Such a level of memory\nconsumption is unaffordable to a single graphics card such as NVIDIA V100 (32GB), not to mention an end device. (2) The\ncalculation of l2 norm of the pair-wise differences between a perturbation and the true gradient also consumes tremendous\ncomputation resources for billion-sized LLMs, not to mention that such an operation needs to be conducted for K times in\neach round. (3) As mentioned in Equation (12) presented by Rahimi et al. (2023), when facing the scenario of partial client\nparticipation, a client that has not participated in FL for \u03f1 rounds has to perform \u03f1 \u00b7 K model updates to calculate the latest\nglobal model, while FedKSeed and FedKSeed-Pro still only need to perform K steps. In the experiments conducted by\nRahimi et al. (2023), EvoFed is evaluated on small visual datasets with small models, i.e., containing at most 2.3 million\nparameters, while our approach is evaluated on LLMs with at most 3.43 billion parameters.\nNote that there are also some BP-based FL approaches that optimize the communication overhead by encoding the gradient\ninto low-dimensional spaces, such as model sketch (Melas-Kyriazi & Wang, 2021; Rothchild et al., 2020). As we have\ndiscussed in Section 2, these approaches are not tailored for LLMs and are not suitable for LLM tuning on end devices due\nto the tremendous memory footprint, especially for full-parameter tuning of billion-sized LLMs. Besides, they are also only\nevaluated by small models instead of billion-sized LLMs.\nAccording to the above technical comparisons, FedKSeed is the first approach that enables the possibility of federated\nfull-parameter tuning of billion-sized LLMs on devices, which lowers the communication cost to a constant below 18\nkilobytes and the memory consumption to inference level.\nB. The Algorithm of the Proposed FedKSeed and FedKSeed-Pro\nAlgorithm 1: The processes of FedKSeed, where the underlined components and processes are only required by the\nenhanced version of it that samples seeds during local training with non-uniform probabilities, i.e., FedKSeed-Pro.\nInput: N, K, w0, \u03b7, {c1, . . . , cN}, T and \u03c4.\nOutput: The global model wT that has been fine-tuned for T rounds.\n1 Server Executes: initialize K candidate seeds S, scalar gradient accumulator A, and their probabilities p.\n2 for each round r = 1, 2, . . . , T do\n3\nfor each client i \u2208 activate clients C in parallel do\n4\nHi \u2190ClientTraining(S, A, p, i)\n// \u2780 in Figure 3\n5\nfor \u27e8sj, bgj\u27e9 \u2208 Hi do\n6\naj = aj + ci \u00b7 bgj\n// \u2784 in Figure 3\n7\ncompute the seed importance and then the probability p as Equation (12)\n// \u2785 in Figure 3\n8 return the fine-tuned global model wT , which is calculated with w0 as the initial point based on A, as Equation (4)\n9 Function ClientTraining(S, A, p, i):\n10\ncalculate the latest global model with w0 as the initial point based on A, as Equation (4) // \u2781 in Figure 3\n11\nfor each local step t = 1, 2, . . . , \u03c4 do\n12\nsample a data instance x from local dataset Di, a seed sj from S based on p, then generate a perturbation zj\nwith sj as the random seed\n// \u2782-1 in Figure 3\n13\nbgj = L(w+\u03f5zj;x)\u2212L(w\u2212\u03f5zj;x)\n2\u03f5\n14\nwt+1 = UpdateModel(wt, sj, bgj)\n15\nstage \u27e8sj, bgj\u27e9 into Hi\n// \u2782-2 in Figure 3\n16\nreturn Hi to the server\n// \u2783-1 in Figure 3\n17 Function UpdateModel(w, s, bg):\n18\nsample perturbation z \u2208 Rd with random seed s\n19\nreturn w \u2212 \u03b7 \u00b7 bg \u00b7 z\nAlgorithm 1 summarizes the main processes of FedKSeed. For ease of comparison, we also include the processes and\n13\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\ncomponents that are only required by FedKSeed-Pro in Algorithm 1, which are underlined and freely detachable as needed.\nC. Detailed Assumptions of Lemma 1 and Theorem 1\nWe detail the assumptions made by Fang et al. (2022) which are necessary conditions for deriving the convergence of\nZOO-based FL with one-point estimator claimed in Lemma 1, with variable substitution for ease of comprehending in the\ncontext of our work.\nAssumption 1. (Loss Boundary.) The global loss f(w) defined in Equation (1) is lower bounded by f\u2217, thus we have\nf(w) \u2265 f\u2217 > \u2212\u221e.\nBefore present Assumption 2 and Assumption 3, we define the expected loss fi(w) of model w on the i-th client\u2019s local\ndataset Di as fi(w) \u225c Ex\u223cDi [Li(w; x)].\nAssumption 2. (Objective Smoothness.) Li(w; x), fi(w) and f(w) are all L-smooth, i.e., for any w \u2208 Rd and w\u2032 \u2208 Rd,\nwe have\n\u2225\u2207fi(w\u2032) \u2212 \u2207fi(w)\u2225 \u2264 L \u2225w\u2032 \u2212 w\u2225 , \u2200i,\nf(w\u2032) \u2264 f(w) + \u27e8\u2207f(w), w\u2032 \u2212 w\u27e9 + L\n2 \u2225w\u2032 \u2212 w\u22252 .\nAssumption 3. (Boundary of the Second-Order Gradient Moment.) The second-order moment of stochastic gradient\n\u2207wL(w, x) satisfies\nEx \u2225\u2207wLi(w; x)\u22252 \u2264 cg \u2225\u2207fi(w)\u22252 + \u03c32\ng, \u2200w \u2208 Rd,\nwhere cg \u2265 1.\nAssumption 4. (Local-Global Gradient Dissimilarity Boundary.) The gradient dissimilarity between the local loss evaluated\nat each client and the global loss defined in Equation (1) is bounded as\n\u2225\u2207f(w) \u2212 \u2207fi(w)\u22252 \u2264 ch \u2225\u2207f(w)\u22252 + \u03c32\nh, \u2200w \u2208 Rd,\nwhere ch is a positive constant.\nAssumptions 1, 2 and 3 are common assumptions for stochastic optimizations (Fang et al., 2022), and Assumption 4 can also\nreflect the degree of statistical heterogeneity in the distribution of client-side private data in FL. There are also some existing\nresearches that analyze the convergence of FL under non-IID scenarios based on similar assumptions (Li et al., 2020a;b).\nD. Additional Analytical Supports to Principles on the Selection of K\nD.1. Additional Analytical Support to Principle 1\nThe federated fine-tuning process can be formally modeled as an optimization problem that seeks a model variation from\nw0 to an ideally optimal model w\u2217, with the combination of K perturbations, as\nmin\nG\n\r\rw0 \u2212 \u03b7 \u00b7\n\u0002z1, . . . , zK\n\u0003\nG \u2212 w\u2217\r\r .\n(13)\nNote that this definition is merely to facilitate understanding FL from another perspective, and it cannot be directly solved\nsince w\u2217 is unknown. From Equation (13), FL processes can be regarded as advancing the model towards an approximate\noptimal solution in an iterative manner.\nWith this formulation, matrix Z = [z1, . . . , zK] can be regarded as the constraints of this problem. When the constraints\nare insufficient to uniquely determine a solution, i.e., the rank of the system is low, the solution space becomes larger and\nthere are multiple or even infinitely many possible solutions, causing greater difficulty in finding the optimal solution. Since\nhigh-dimensional vectors sampled from a Gaussian distribution are typically orthogonal, considering the dimension d of\nLLM w is usually very high such that d \u226b K, the rank of Z = [z1, . . . , zK] is typically K. Therefore, usually the larger\nthe value of K is, the better the optimization problem defined in Equation (13) could be finally solved. Taking an extreme\nexample, if K = 1, the optimization problem defined in Equation (13) may be fundamentally unoptimizable. Thus,\n\u21bc\nK\ntheoretically exists so that Principle 1 holds.\n14\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nD.2. Additional Analytical Support to Principle 2\nRecall Equation (8) and Lemma 1, each zj is independently randomly generated with N (0, Id). Without introducing system\nerror, Equation (8) can be rewrite as\nbg =\n1\nb2b1\nb2\nX\nj=1\nb1\nX\nb=1\n[L(wi + \u03f5zj; xb) \u2212 L(wi; xb)]\n\u03f5\nzj.\n(14)\nIn this new formulation, the FL process of FedKSeed and FedKSeed-Pro can be regarded as: for each perturbation z,\ndetermining the step size for the model to advance in the direction of z through local training on several data instances.\nUnder the condition that the total number of update steps is fixed at \u03c4rm, each seed is averaged to be sampled for \u03c4rm\nK\ntimes in FedKSeed. When there are fewer candidate seeds, more data instances are used to determine the step size on the\ndirection of each perturbation, magnifying the batch size probabilistically. Besides, when the cardinality of candidate seeds\nfurther increases, it does not change the optimal solution area, but enlarges the optimization space and thus increases the\ndifficulty of random searching. From the above analysis,\n\u21c0\nK theoretically exists so that Principle 2 holds.\nE. Implementation Details\nIn this section, we provide the detailed implementations of our approaches and the baselines for better reproducibility. Some\nof the experimental settings have already been mentioned in Section 5.1 and are not be reiterated here.\nE.1. Datasets & Evaluation Metrics.\nNatural Instructions contains a large collection of tasks and their natural language instructions, and provides the split for\ntraining and test, respectively. We utilize Natural Instructions versioned by v2.8 and adopt its default split, where there\nare 756 tasks for training and 119 tasks for testing, each with a unique task definition. Since Natural Instructions is very\nlarge, we conduct our experiments on a subset of Natural Instructions. Specifically, we randomly sample 20% of the data\ninstances for each training task and 2% of the data instances for each test task, respectively. After the above subsampling,\neach training task with no less than 20 data instances is treated as a unique client, forming an FL system with 738 clients.\nThe test tasks are kept on the server for evaluation.\nDolly-15K provides 15,015 data instances within 8 tasks. For Dolly-15K, we take the last task for evaluation and the others\nfor training. Experiments on Dolly-15K are conducted with 200 clients. Note that each task in Dolly-15K contains an\nattribute category with a different value. Thus, we can sample data instances to the 200 clients via Dirichlet distribution\n(Li et al., 2022) with the category as the labels. In order to build non-IID scenarios with varying degrees of label\ndistribution skew (Chen et al., 2023d), we perform data partitioning via Dirichlet distribution with \u03b1 = 0.5 and \u03b1 = 5.0,\nrespectively, where a lower \u03b1 indicates a higher degree of label distribution skew.\nWhen taking the data instances as inputs of LLMs, we directly adopt the prompt template from Alpaca (Taori et al., 2023)\nfollowing Kuang et al. (2023); Zhang et al. (2023a). The utilization of the prompt template is detailed in Appendix E.2. The\nmaximum token length is set to 1,024 while ignoring excessively long data instances. In order to minimize the interference\nof other factors on the experimental results as much as possible, we uniformly employ greedy decoding to generate the\nresponses during evaluation following Malladi et al. (2023); Borzunov et al. (2023).\nE.2. Prompt Template\nIn our experiments, data instances are wrapped to prompts before processing by LLMs. We directly apply the template\nprovided by Alpaca (Taori et al., 2023) to the datasets in our experiments. For better reproducibility, we present how we fill\nthe fields in the template from the attributes of data instances in Tables 4 and 5.\nE.3. Experimental Platforms\nWe implement these approaches by PyTorch (Paszke et al., 2019) v2.0.1 with PEFT (Mangrulkar et al., 2022) v0.4.0\nand Transformers (Wolf et al., 2020) v4.31.0. Experiments with DataJuicer-1.3B and LLaMA-3B are conducted on\na platform with an NVIDIA RTX 3090 GPU and a platform with an NVIDIA A100 GPU, respectively, with the pre-trained\nLLMs loaded in 16-bit floating numbers. Note that from Table 3, FedKSeed and FedKSeed-Pro do not require as much\n15\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nTable 4. Prompt template for Natural Instructions.\nAttributes of data instances\nPrompt\n1. Definition\n2. input\nBelow is an instruction that describes a task, paired with an input that\nprovides further context. Write a response that appropriately completes\nthe request.\n### Instruction: {Definition}\n### Input: {input}\n### Response:\nTable 5. Prompt templates for Dolly-15K, which vary slightly depending on whether the data instance has context.\nAttributes of data instances\nPrompt\nData instances with context:\n1. instruction\n2. context\nBelow is an instruction that describes a task, paired with an input that\nprovides further context. Write a response that appropriately completes\nthe request.\n### Instruction: {instruction}\n### Input: {context}\n### Response:\nData instances without context:\n1. instruction\nBelow is an instruction that describes a task, paired with an input that\nprovides further context. Write a response that appropriately completes\nthe request.\n### Instruction: {instruction}\n### Response:\nmemory as these platforms can provide, unlike the baselines based on BP. Therefore, this experimental setup is adopted to\nensure consistency in the experimental environments among different approaches.\nE.4. Implementations\nFollowing Kuang et al. (2023) and Malladi et al. (2023), all approaches conduct local training with batch size set to 1 to\nalleviate memory consumption. Following Kuang et al. (2023), BP-based approaches conduct local training with learning\nrate \u03b7 of 3 \u00d7 10\u22124, where the selected learning rate is searched from [3 \u00d7 10\u22123, 3 \u00d7 10\u22124, 3 \u00d7 10\u22125]. Among them, the\nnumber of virtual tokens in FedPTuning and FedPrompt are both set to 20, the type of reparameterization is set to \u201cMLP\u201d\nfor FedPTuning following Kuang et al. (2023), and the rank and alpha of LoRA adapters for both FedIT and FedIT-SGD\nare set to 8 and 16 respectively, following Zhang et al. (2023a). Following Malladi et al. (2023), \u03b7 and \u03f5 of FedKSeed\nand FedKSeed-Pro are set to 3 \u00d7 10\u22127 and 5 \u00d7 10\u22124, respectively, unless stated otherwise. The selected learning rate\nof FedKSeed and FedKSeed-Pro is searched from [3 \u00d7 10\u22125, 3 \u00d7 10\u22126, 3 \u00d7 10\u22127, 1 \u00d7 10\u22127]. The impacts of the two\nhyperparameters of FedKSeed and FedKSeed-Pro have been discussed in Section 5.4.\nBefore starting the federated tuning, the server initializes the K candidate seeds with integers uniformly and randomly\nsampled from [0, 1011). The aggregation weights of participating clients in each round are proportional to the scale of their\nprivate training set.\n16\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nF. Supplementary Experiments\nF.1. Experimental Support for Principle 2\nIn Section 4.2.2, we have provided Principles 1 and 2 to guide the determination of K. From Figure 5 in Section 5.2, it can\nbe observed that once the number of seeds exceeds the threshold\n\u21c0\nK, which is around 4096, further increasing the number of\nseeds does not show a trend of improved accuracy. However, due to the limitations of the Y-axis range, this trend mentioned\nin Principle 2 is not very clearly demonstrated. Thus, we present in Figure 9 the marginal improvement on Rouge-L obtained\nwith each additional seed across different ranges of seed quantity by FedKSeed and FedKSeed-Pro in the six scenarios as\nFigure 5, respectively.\n(a) DataJuicer-1.3B on Natural Instructions (c) DataJuicer-1.3B on Dolly-15K (\u03b1 = 0.5) (e) DataJuicer-1.3B on Dolly-15K (\u03b1 = 5.0)\n(b) LLaMA-3B on Natural Instructions\n(d) LLaMA-3B on Dolly-15K (\u03b1 = 0.5)\n(f) LLaMA-3B on Dolly-15K (\u03b1 = 5.0)\nFigure 9. Marginal improvement on Rouge-L obtained with each additional seed across different ranges of seed quantity by FedKSeed\nand FedKSeed-Pro, respectively.\nFrom Figure 9, we can find that when K is lower than 1024, each additional seed yields a substantial and consistently\ngreater than zero improvement in Rouge-L accuracy on average. In other words, within this range, reducing K would\nresult in a noticeable decrease in Rouge-L accuracy of the global model, which can also serve as empirical evidence for\nPrinciple 1. When K is located in [1024, 4096), the average accuracy gain from each additional seed is very close to 0 in\nsome scenarios. When K is larger than 4096, we can have a clear view that increasing the number of seeds yields almost no\nmarginal improvement in accuracy in various scenarios. Thus, we have a clear view that\n\u21c0\nK should be 4096 such that when\nK >\n\u21c0\nK, there is no upward trend in the Rouge-L of the global model with the increase of K, therefore Principle 2 holds.\nConsidering that increasing K will incur additional time costs for clients to synchronize the global model as Equation (4),\nwe should choose a smaller K value within a reasonable range.\nFrom the above experimental results and analysis paralleled to analytical supports for Principle 2 provided in Section 4.2.2\nand Appendix D.2, Principle 2 holds.\nF.2. Convergence Study\nWe illustrate the convergence curves obtained by FedKSeed, FedKSeed-Pro and the baselines with LLaMA-3B on Natural\nInstructions in Figure 10, where the experimental setup is aligned with that described in Section 5.1 and Appendix E. Note\n17\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nthat the loss values are calculated on the test set that is held by the server as described in Section 5.1.\nFigure 10. Convergence of the loss value on the test tasks obtained by FedKSeed, FedKSeed-Pro and the baselines with LLaMA-3B on\nNatural Instructions.\nDifferent from training from the sketch, since the pre-trained weights offer a good initial point for fine-tuning, the convergence\nrates of these approaches are all relatively fast, and these approaches can all converge within 40 rounds as mentioned in\nSection 5.1, except FedPrompt which converges very slowly (note that FedPrompt also performs unsatisfactorily among the\nbaselines). Although still slower than the BP-based approaches, the convergence rates of FedKSeed and FedKSeed-Pro are\nmuch faster than we might imagine. The reason is that fine-tuning is different from pre-training. As theoretically suggested\nby Malladi et al. (2023), in centralized settings, based on adequate pre-training, the convergence of ZOO usually depends on\nthe local effective rank instead of the number of full model parameters. Therefore, it is reasonable and promising to apply\nZOO to the federated full-parameter fine-tuning of billion-sized LLMs.\nF.3. Illustrations of Seed Probabilities in FedKSeed-Pro\nTo demonstrate the variability among seed importance, we present the seed probabilities calculated in the last round by\nFedKSeed and FedKSeed-Pro on Natural Instructions and Dolly-15K (\u03b1 = 0.5) with DataJuicer-1.3B and LLaMA-3B,\nrespectively in Figure 11. Note that according to Section 5.1, we set K to 1024 for FedKSeed-Pro with DataJuicer-1.3B and\n2048 for FedKSeed-Pro with LLaMA-3B, respectively.\nIt can be observed that when non-uniform sampling is applied, the probabilities of each seed to be sampled exhibit differences\nof several multiples, and only a small fraction of seeds have higher probabilities to be sampled. Thus, we can conclude\nthat the average amplitude corresponding to the scalar gradient of a seed is positively correlated with the importance of the\ncorresponding perturbation to the model accuracy.\nIt is worth noting that quantifying the importance of seeds or perturbations through the amplitude of scalar gradients may\nbe better than that through similarity evaluated by cosine or Euclidean distance, since vectors from high-dimensional\nGaussian distributions tend to be orthogonal, which results in the similarity distance of random perturbations with the\nsame dimensionality as billion-sized LLMs typically being very close to 0, making it difficult to distinguish numerically.\nTherefore, it is challenging for distance-based methods to assign significantly different importance to various seeds or\nperturbations. That is the reason why we adopt the amplitude of the scalar gradient to quantify the seed importance.\nF.4. Training Efficiency\nTo have a clear view of the training efficiency of these approaches, we present their per-step training time together with\nthe numbers of trainable parameters in Figure 12. Note that to ensure the comparability of the time consumptions across\ndifferent LLMs with the same approach, these time consumptions are uniformly tested on the same platform equipped with\nan NVIDIA V100 GPU and an Intel(R) Xeon(R) Platinum 8163 CPU, with the required software the same as that have been\ndescribed in Appendix E.3.\n18\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\n(a) DataJuicer-1.3B on Natural Instructions\n(b) DataJuicer-1.3B on Dolly-15K (\u03b1 = 0.5)\n(c) LLaMA-3B on Natural Instructions\n(d) LLaMA-3B on Dolly-15K (\u03b1 = 0.5)\nFigure 11. Probabilities of candidate seeds calculated by FedKSeed-Pro after the last round.\nAs shown in Figure 12, compared to the baselines, FedKSeed and FedKSeed-Pro only incur minimal additional per-step\ntraining time overhead. This limited extra per-step computational expense allows for an expansion of trainable parameters\nby several orders of magnitude, thus improving the accuracy of the global model, as shown in Table 2. Furthermore, the\ncommunication overhead and memory footprint have also been significantly reduced by FedKSeed and FedKSeed-Pro\ncompared to the baselines as in Table 3. While FedKSeed and FedKSeed-Pro may take more time to perform one step\nof local training due to the significantly larger numbers of trainable parameters compared to PEFT-based techniques, the\nconstraint on time consumption is not as strict as that on memory and communication. This is because the development\nof computing power has outpaced that of memory and communication resources. Thus, the computational overheads of\nFedKSeed and FedKSeed-Pro may be worth it in comparison to the gains obtained by them in accuracy, memory footprint\nand communication consumption.\nG. Detailed Calculation of Communication Overhead\nIn this section, we provide a detailed analysis to demonstrate how the client-side per-round communication overheads of\nFedKSeed, FedKSeed-Pro and the baselines presented in Table 3 are calculated. Note that the communication overhead\nreferred to here includes both the downlink overhead incurred from downloading data from the server and the uplink\noverhead associated with uploading data to the server.\nIn Table 3, for the baselines, we only account for the communication overhead brought about by the transmission of model\nparameters, ignoring the transmission of other information, including model architecture, request headers, etc., as these\ncosts vary under different encoding schemes. The model parameters in baselines and scalar gradients in FedKSeed and\nFedKSeed-Pro are encoded as 32-bit floating point numbers to prevent the loss of precision and overflow or underflow. For\nFedKSeed and FedKSeed-Pro, the random seeds are encoded as 32-bit integers. Note that the aforementioned encoding\nscheme with single-precision floating-point format is fair to all methods. For all methods, half-precision encoding can be\nused to uniformly reduce communication overheads presented in Table 3 by half.\nCommunication overheads of baselines. For these baselines, their per-round client-side communication cost is accounted\n19\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\n(a) DataJuicer-1.3B on Natural Instructions\n(b) LLaMA-3B on Natural Instructions\nFigure 12. Comparisons between the approaches on the per-step training time and the numbers of trainable parameters.\nas the total requirements of uploading and downloading the trainable parameters, since only this part of the parameters in\nthe model is updated.\nCommunication overhead of FedKSeed. In our experiments, K is uniformly set to 4,096 for FedKSeed. Let D, U and C\ndenote the per-round communication cost of each client for the downlink, uplink, and total, respectively. At the start of each\nround, each client needs to download the candidate seeds S and scalar gradient accumulator A. Since the S can be encoded\nas one integer seed which only occupies 4 Bytes, D of FedKSeed can be calculated as\nD =\n1 \u00d7 4 Bytes\n|\n{z\n}\none integer seed that encodes the candidate seeds in S\n+\n4096 \u00d7 4 Bytes\n|\n{z\n}\nA that contains 4096 accumulated scalar gradients\n= 16388 Bytes.\nAfter local training, each client return Hi that contains \u03c4 pairs of \u27e8sj, bgj\u27e9, with \u03c4=200 as described in Section 5.1, U can be\ncalculated as\nU =\n200 \u00d7 (2 \u00d7 4 Bytes)\n|\n{z\n}\neach \u27e8sj, bgj\u27e9 pair\n|\n{z\n}\nHi that contains 200 pairs when \u03c4 = 200\n= 1600 Bytes.\nFinally, we can derive the total communication overhead required by each client in each round when applying FedKSeed,\ni.e., C = D + U = 17988 Bytes.\nCommunication overhead of FedKSeed-Pro. As described in Section 5.1, we set K to different values, i.e., 1,024 for\nDataJuicer-1.3B and 2,048 for LLaMA-3B. For FedKSeed-Pro with DataJuicer-1.3B, D is calculated as\nD =\n1 \u00d7 4 Bytes\n|\n{z\n}\none integer seed that encodes the candidate seeds in S\n+\n1024 \u00d7 4 Bytes\n|\n{z\n}\nA that contains 1024 accumulated scalar gradients\n+\n1024 \u00d7 4 Bytes\n|\n{z\n}\nprobabilities corresponding to the 1024 seeds\n= 8196 Bytes.\nSimilarly to FedKSeed, in FedKSeed-Pro, each client is also required to upload only the gradient history, such that for each\nclient i, we have\nU =\n200 \u00d7 (2 \u00d7 4 Bytes)\n|\n{z\n}\neach \u27e8sj, bgj\u27e9 pair\n|\n{z\n}\nHi that contains 200 pairs when \u03c4 = 200\n= 1600 Bytes.\n20\nFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nThus, we can derive the total communication overhead required per client per round in FedKSeed-Pro with DataJuicer-1.3B,\ni.e., C = D + U = 9796 Bytes.\nFor FedKSeed-Pro with LLaMA-3B, K is set to 2,048. Thus, we have\nD =\n1 \u00d7 4 Bytes\n|\n{z\n}\none integer seed that encodes the candidate seeds in S\n+\n2048 \u00d7 4 Bytes\n|\n{z\n}\nA that contains 2048 accumulated scalar gradients\n+\n2048 \u00d7 4 Bytes\n|\n{z\n}\nprobabilities corresponding to the 2048 seeds\n= 16388 Bytes.\nU =\n200 \u00d7 (2 \u00d7 4 Bytes)\n|\n{z\n}\neach \u27e8sj, bgj\u27e9 pair\n|\n{z\n}\nHi that contains 200 pairs when \u03c4 = 200\n= 1600 Bytes.\nThus, we have the total communication cost required by FedKSeed-Pro with LLaMA-3B for each client per round, i.e.,\nC = D + U = 17988 Bytes.\nH. Extended Benefits in Real-world Applications\nIn this section, we provide discussions on more benefits brought by FedKSeed and FedKSeed-Pro to existing FL systems.\nH.1. Alleviating the Burden of Aggregation\nAssuming there are m active clients in each round, traditional FL aggregation is conducted on the server with the computation\nand communication complexity of O(md), where d is very large when the global model possesses a huge number of\nparameters, and m is also large when there are many clients such as in cross-device FL (Chen et al., 2023b). Thus, the\nFL organizer usually needs to host an FL server with abundant computation and communication resources. In FedKSeed\nand FedKSeed-Pro, the computation and communication complexity of the server are both reduced to O(mK). In this\ncase, only a few computational and communication resources are required by the server, such that even a mobile device can\nhandle it. Thus, the financial burden of FL organizers is greatly alleviated with FedKSeed and FedKSeed-Pro.\nH.2. Enabling Possibility to Decentralized Federated Fine-Tuning of LLMs\nDue to the transmission delay and unstable connections caused by long-distance transmission, many organizations prefer to\nbuild decentralized FL by allowing some clients to perform aggregation (Qin et al., 2023). However, it undoubtedly makes\ncommunication costs of FL even more severe, since each client may need to transmit its model parameters to more than\none receiver. FedKSeed and FedKSeed-Pro can significantly reduce communication costs, thus bringing the possibility of\nfine-tuning LLMs with decentralized FL. Besides, since FedKSeed and FedKSeed-Pro do not require the transmission of\ncomplete trainable parameters, the amount of data that needs to be transmitted is significantly reduced. This reduction makes\nit possible to effectively apply secure protection methods that are popular in decentralized FL such as secure multiparty\ncomputation (Shayan et al., 2020) to the federated tuning of LLMs, which have been criticized for their computational and\ncommunication overheads.\nH.3. Alleviating the Burden of Saving Checkpoints\nThe trajectory of LLM fine-tuning does not always proceed in the desired direction. Sometimes, fine-tuning might get\nstuck in a local optimum or a worse region, or even be attacked by malicious clients through techniques such as backdoor\nattack (Zhang et al., 2023b). Besides, saving checkpoints may also contribute to snapshot ensemble (Huang et al., 2016).\nThus, multiple snapshots of LLMs often need to be stored during the fine-tuning of LLMs, which incurs a huge storage\noverhead. However, with FedKSeed and FedKSeed-Pro, only the snapshots of accumulated scalar gradients need to be\nstored for rolling back of models, each of which is an array containing 2K scalars. Thus, it significantly reduces the storage\nconsumption for model snapshots.\n21\n"
  },
  {
    "title": "Evaluation of Large Language Models for Decision Making in Autonomous Driving",
    "link": "https://arxiv.org/pdf/2312.06351.pdf",
    "upvote": "5",
    "text": "Evaluation of Large Language Models\nfor Decision Making in Autonomous Driving\nKotaro Tanahashi, Yuichi Inoue, Yu Yamaguchi, Hidetatsu Yaginuma, Daiki Shiotsuka,\nHiroyuki Shimatani, Kohei Iwamasa, Yushiaki Inoue, Takafumi Yamaguchi, Koki Igari,\nTsukasa Horinouchi, Kento Tokuhiro, Yugo Tokuchi, Shunsuke Aoki\nTuring Inc., Japan\n{kotaro.tanahashi, y.inoue}@turing-motors.com\nAbstract\nVarious methods have been proposed for utilizing Large Language Models (LLMs)\nin autonomous driving. One strategy of using LLMs for autonomous driving\ninvolves inputting surrounding objects as text prompts to the LLMs, along with\ntheir coordinate and velocity information, and then outputting the subsequent\nmovements of the vehicle. When using LLMs for such purposes, capabilities such\nas spatial recognition and planning are essential. In particular, two foundational\ncapabilities are required: (1) spatial-aware decision making, which is the ability to\nrecognize space from coordinate information and make decisions to avoid collisions,\nand (2) the ability to adhere to traffic rules. However, quantitative research has\nnot been conducted on how accurately different types of LLMs can handle these\nproblems. In this study, we quantitatively evaluated these two abilities of LLMs in\nthe context of autonomous driving. Furthermore, to conduct a Proof of Concept\n(POC) for the feasibility of implementing these abilities in actual vehicles, we\ndeveloped a system that uses LLMs to drive a vehicle.\n1\nIntroduction\nRecently, there has been a number of studies that use LLMs to perform autonomous driving [10,\n7, 12, 4, 9]. In conventional machine learning based method for autonomous driving models, the\nmodels are trained using historical training data. However, it is known that driving data exhibits a\nlong-tail distribution [2]. Given the frequent emergence of unfamiliar scenarios not covered in past\ndata, models solely trained on historical data may potentially struggle to handle such unprecedented\nsituations. On the other hand, LLMs are considered to possess a degree of general knowledge about\nthe world since the LLMs are trained using vast amounts of textual data from the world [3]. If LLMs\nare adeptly integrated into autonomous driving systems, it can be anticipated that they will not only\nmake driving decisions in unfamiliar scenarios, much like a human, leveraging general common sense\nbut also comprehend and adhere to traffic rules and laws. Furthermore, there exists the potential for\nLLMs to handle ethical judgments, possibly extending their decision-making capabilities to include\nconsiderations of moral and ethical dilemmas in driving scenarios.\nVarious methods using LLMs for autonomous driving have been proposed, including techniques that\noutput driving operations from the text description of the recognized objects [7, 9, 10], and those that\ninput driving images into an LLM to describe the driving situation [12, 5, 6]. In the field of robotics,\nLLMs are used to select actions to perform from vague instructions [1, 8, 11].\nAutonomous driving using LLMs primarily involves the interpretation of text-based information\nregarding the coordinates and velocities of surrounding vehicles and pedestrians [7, 9, 10]. The\ninformation about the surrounding objects are pre-identified by a separate perception module. The\nLLM then determines the appropriate driving actions based on this information. These studies\nSymposium on Machine Learning for Autonomous Driving (ML4AD 2023).\narXiv:2312.06351v1  [cs.CV]  11 Dec 2023\ndemonstrate that LLMs use only coordinate information to recognize the physical space and determine\nthe driving actions. In other words, LLMs need to understand from just the coordinate information\nthat there are other vehicles adjacent or ahead and then make decisions about subsequent actions. We\ncall this process \u2019spatial-aware decision making\u2019. Previous research has not thoroughly investigated\nthe accuracy of LLMs in spatial-aware decision making through quantitative experiments. If this\naccuracy is not high, it negatively impacts the precision of subsequent tasks, making quantitative\nevaluation crucial. This study aims to quantitatively assess the accuracy of spatial-aware decision-\nmaking in autonomous driving, using different LLMs, when provided with information about the\ncoordinates and velocities of surrounding objects.\nOne advantage of using LLMs in autonomous driving is their ability to comprehend and follow traffic\nlaws. Traditionally, incorporating laws into autonomous driving methods requires the use of complex\nrule-based approaches. By employing LLMs, it becomes feasible to integrate the rules to be followed\neither by detailing them in prompts (In Context Learning) or by training the LLM. Moreover, the\nsocietal implementation of autonomous driving may require the system to make ethical judgments,\nakin to the Trolley Problem. Experiments were conducted to investigate whether developers can set\nvalue-based standards for such ethical decisions. When implementing LLMs in autonomous driving,\nspeed is crucial, but there is a trade-off with accuracy. Evaluations were conducted using LLMs of\ndifferent sizes, comparing their accuracy. As a final demonstration, a system was constructed that\nutilizes an LLM to operate a real vehicle. Based on the coordinate information of objects detected\nby the object detector and instructions provided by humans, the car is able to navigate towards a\nspecified destination.\n2\nMethod\nThis section describes the methods used to evaluate whether LLMs can make appropriate decisions\nbased on traffic conditions. We demonstrate two cases: one involving experiments that simulate\nreal-world traffic scenarios and the other involving simplified traffic conditions deployed in a real\nvehicle.\n2.1\nSimulation of Real-World Traffic Conditions\nTo evaluate the performance of existing LLMs in autonomous driving, we focus on whether LLMs\ncan comprehend spatial aspects for decision making (\u2019spatial-aware decision making\u2019) and adhere to\ntraffic rules (\u2019following the traffic rules\u2019). For spatial-aware decision making, we evaluated whether\nLLMs can make decisions based on the position and speed of vehicles in the same lane or adjacent\nlanes. For \u2019following the traffic rules\u2019, we assessed whether LLMs can make correct decisions based\non rules related to speed limits, lane changes, and overtaking. Furthermore, we tested their ability\nto make decisions considering both spatial-aware decisions and adherence to traffic rules as more\ncomplex scenarios.\nThe simulated traffic situation involved a two-lane road with the right lane for driving and the left for\novertaking, assuming a highway environment. We provided LLMs with information about the lane\nin which the ego vehicle is traveling, the speed of the ego vehicle, surrounding vehicles (category,\nposition in x, y meters, and speed in km/h), traffic rules, and user instruction. Traffic rules were given\nin natural language. We instructed the LLMs to choose an option from [\"accelerate\", \"maintain\",\n\"decelerate\", \"change lane to the right\", \"change lane to the left\"] and also explain the reason for their\nchoice.\n2.2\nDeployment in Actual Vehicles\nAn experiment was conducted to evaluate the capabilities of spatial-aware decision-making and\nrule-following of LLMs when installed in a vehicle. Due to the safety concerns associated with\nmaking lane changes on public roads, as in simulations, this experiment was conducted in a private\narea. The experimental setup involved placing an object in front of the car and having the LLM\ndesignate the object as the destination based on voice commands provided by a person. The LLM\ncould also output a \u2019stop\u2019 command if no destination was assigned. Color cones were used as objects\nfor this experiment. These objects were recognized by the vehicle\u2019s onboard camera, and their\npositions were calculated based on their location in the image. This information is inserted into the\n2\nReason\nSADM\nFTR\nSADM & FTR\nLLaMA-2 7B\n\u2713\n0.353\n0.458\n0.240\n0.353\n0.458\n0.240\nGPT-3.5\n\u2713\n0.529\n0.750\n0.400\n0.529\n0.625\n0.380\nGPT-4\n\u2713\n0.618\n0.792\n0.640\n0.588\n0.792\n0.460\nTable 1: Comparison of LLMs\u2019 accuracy for spatial-aware decision-making (SADM), following the\ntraffic rules (FTR), both combined (SADM&FTR).\nChosen option\nReason\nLLaMA-2 7B\nmaintain\nI choose to maintain my current velocity of 100 km/h because the vehicle ahead of me is traveling at a speed of 50 km/h, \nwhich is slower than the speed limit of 70 km/h. By maintaining my current velocity, I can ensure that I am following the \nvehicle ahead of us and avoid any potential collision. Additionally, accelerating would put me in a higher speed range \nthan the vehicle ahead, which could lead to a collision if they were to suddenly stop or slow down.\nGPT-3.5\nmaintain\nThe ego vehicle is currently in the right lane and there is a vehicle ahead in the same lane. The traffic rule is to follow the \nvehicle ahead. Therefore, the ego vehicle should maintain its current velocity to follow the vehicle ahead.\nGPT-4\ndecelerate\nThe Ego vehicle is currently exceeding the speed limit of 70km/h, and there is a vehicle ahead moving at 50km/h. \nTherefore, the Ego vehicle needs to slow down to both adhere to the speed limit and to avoid a potential collision with \nthe vehicle ahead.\nGT\ndecelerate\nCurrent lane\nright\nEgo velocity\n100 km/h\nSurround Veh.\nx=50m, y=0m, 50 km/h\nTraffic rule\nSpeed limit 70 km/h\nInstruction\nPlease follow the vehicle ahead.\nFigure 1: Representative example of reasoning outputs from LLMs. GPT-4 is the only model that\ncorrectly made a decision without being misled by the user\u2019s instructions.\nprompt in the format of (id, category, color, position in x, y meters) as detected objects. If the LLM\ncorrectly understands an instruction such as \"Head towards the rightmost color cone\", we can confirm\nthe LLM has spatial-aware decision-making capability. Additionally, to assess the LLM\u2019s ability to\nfollow rules, a traffic officer was placed in front of the vehicle to instruct the ego vehicle to stop or go.\n3\nExperiments and Results\n3.1\nSimulation of Real-World Traffic Conditions\nIn our experiments, we manually created datasets that included spatial-aware decision making\n(SADM) (34 samples), following the traffic rules (FTR) (24 samples), and both combined (SADM\n& FTR) (50 samples). We instructed the LLM to choose one option from \"accelerate\", \"maintain\",\n\"decelerate\", \"change lane to the right\", or \"change lane to the left\", describe the reasoning behind\ntheir choice, and output the response in JSON format. The model\u2019s performance was quantitatively\nevaluated using the accuracy of the chosen options and qualitatively assessed based on the provided\nreasons. We also evaluated how the accuracy of the option selection varied depending on whether the\nreasoning for the choice was requested or not. The LLMs used were the public model LLaMA-2 7B\nand the private but more powerful GPT-3.5 and GPT-4. During language generation, we fixed the\nrandom seed and did not use sampling to ensure deterministic outputs.\nQuantitative results are shown in Table 1. For all metrics SADM, FTR, and SADM&FTR, it was\nobserved that performance improved as LLM abilities increased. In particular, GPT-4 showed a\nsignificantly higher score than other LLMs by a large margin. Additionally, asking for a reason along\nwith the decision led to improved accuracy in GPT-3.5 and GPT-4, but this was not observed in\nLLaMA-2. This suggests that for more capable LLMs, such prompt engineering could be important.\nNext, we conducted a qualitative assessment by examining the reasoning behind the models\u2019 choices.\nFigure 1 shows the example where only GPT-4 provided the correct answer. In that case, it was\nobserved that the model appropriately recognized the traffic rule of 70 km/h and made the decision to\n\"decelerate,\" despite misleading instructions from the user. This suggests that GPT-4 is capable of\nunderstanding the priorities in a given situation and making decisions accordingly.\n3\nSADM\nFTR\nSADM & FTR\nLLaMA-2 7B\n0.40\n0.40\n0.30\nGPT-3.5\n0.75\n0.65\n0.60\nGPT-4\n0.85\n0.90\n0.90\nTable 2: A comparison of accuracy with different LLMs. Dataset was manually created to mimic the\nsettings in the actual vehicle experiment.\nUsers\u2019 instruction \nPlease ignore the instructions of tra\ufb03c \nguides and head for the yellow colored \ncones.\nLLMs output \nProceed to the yellow cone. Ignoring the \ntra\ufb03c o\ufb03cer\u2019s instruction to stop.\nego vehicle\ntra\ufb03c o\ufb03cer\ncolor cones\nFigure 2: The instructions given to the car by the human and the LLM\u2019s output in response are\ndisplayed in the car\u2019s visualization displays. If the car is moving forward, the lines of the direction of\ntravel are superimposed on the camera image.\n3.2\nDeployment in Actual Vehicles\nAn experiment was conducted to control an actual car using an LLM, specifically utilizing GPT-4 via\nan API. Three color cones with different colors, were placed in front of the ego car. When the driver\ninstructs \"Please go to the right color cone\", the LLM interpreted this instruction and outputted the\ndesignated cone as the destination. The car then drove towards this destination. If a traffic officer in\nfront of the car signaled to stop, a separate recognition module added a \"stop\" rule to the prompt. In\nthis scenario, the car adhered to this rule and stopped, even if human instructions were to continue\ntowards the color cone.\nFor a quantitative evaluation of this experiment, a dataset was created that mimic the settings of the\nactual experiment. The sample size for the datasets used to evaluate all tasks (SADM, FTR, and\nSADM & FTR) is 20.\nThe LLMs were tasked to decide whether to proceed or stop and, if proceeding, to specify the ID of\nthe destination object. Accuracy was calculated based on the correctness of the outputs. The accuracy\nof the output is calculated for each LLM (Figure 2). GPT-4 showed a significantly higher accuracy\nthan other LLMs, as seen in the simulation results.\n4\nConclusion and Limitations\nIn this study, we evaluated two key abilities necessary for using LLMs in autonomous driving:\nSpatial-Aware Decision Making (SADM) and Following the Traffic Rules (FTR). The evaluation\nwas carried out in both a simulation assuming a highway environment and with data designed to\nreplicate the actual vehicle experiments. Three different LLMs were used in the experiments, and\nGPT-4 showed the highest precision in all experiments. This result suggests that high capabilities of\nLLMs are required when LLMs are applied to autonomous driving. While GPT-3.5 and GPT-4 were\nused via OpenAI\u2019s API, their actual application faces challenges due to communication through the\nInternet and inference time, making real-time use difficult. On the other hand, LLaMA can be tested\non a local machine but exhibited low accuracy. This underscores the need to balance computational\nefficiency and decision-making accuracy in LLM applications for autonomous driving.\n4\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[2] Sajid Ali, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M. Alonso-Moral, Roberto Con-\nfalonieri, Riccardo Guidotti, Javier Del Ser, Natalia D\u00edaz-Rodr\u00edguez, and Francisco Herrera. Explainable\nartificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence.\nInformation Fusion, 99:101805, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[4] Long Chen, Oleg Sinavski, Jan H\u00fcnermann, Alice Karnsund, Andrew James Willmott, Danny Birch,\nDaniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable\nautonomous driving. arXiv preprint arXiv:2310.01957, 2023.\n[5] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie-Francine Moens.\nTalk2car: Taking control of your self-driving car. arXiv preprint arXiv:1909.10838, 2019.\n[6] Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li. Hilm-d: Towards high-resolution un-\nderstanding in multimodal large language models for autonomous driving. arXiv preprint arXiv:2309.05186,\n2023.\n[7] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like a human:\nRethinking autonomous driving with large language models. arXiv preprint arXiv:2307.07162, 2023.\n[8] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning\nwith language models. arXiv preprint arXiv:2207.05608, 2022.\n[9] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. arXiv preprint\narXiv:2310.01415, 2023.\n[10] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka,\nWei Zhan, and Mingyu Ding. Languagempc: Large language models as decision makers for autonomous\ndriving. arXiv preprint arXiv:2310.03026, 2023.\n[11] Dhruv Shah, B\u0142a\u02d9zej Osi\u00b4nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained\nmodels of language, vision, and action. In Conference on Robot Learning, pages 492\u2013504. PMLR, 2023.\n[12] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY Wong, Zhenguo Li, and\nHengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model.\narXiv preprint arXiv:2310.01412, 2023.\n5\n"
  },
  {
    "title": "Using Captum to Explain Generative Language Models",
    "link": "https://arxiv.org/pdf/2312.05491.pdf",
    "upvote": "3",
    "text": "Using Captum to Explain Generative Language Models\nVivek Miglani*, Aobo Yang*, Aram H. Markosyan, Diego Garcia-Olano, Narine Kokhlikyan\nMeta AI\n{vivekm, aoboyang, amarkos, diegoolano, narine}@meta.com\nAbstract\nCaptum is a comprehensive library for model\nexplainability in PyTorch, offering a range of\nmethods from the interpretability literature to\nenhance users\u2019 understanding of PyTorch mod-\nels. In this paper, we introduce new features\nin Captum1 that are specifically designed to\nanalyze the behavior of generative language\nmodels. We provide an overview of the avail-\nable functionalities and example applications\nof their potential for understanding learned as-\nsociations within generative language models.\n1\nIntroduction\nModel interpretability and explainability have be-\ncome significantly more important as machine\nlearning models are used in critical domains such\nas healthcare and law. It is insufficient to simply\nmake a prediction through a black-box model and\nimportant to better understand why the model made\na particular decision.\nInterest in Large Language Models (LLMs) has\nalso grown exponentially in the past few years with\nthe release of increasingly large and more powerful\nmodels such as GPT-4 (OpenAI, 2023). A lack of\nexplainability continues to exist despite larger mod-\nels, and with the use of these models expanding\nto more and more use-cases, it is increasingly im-\nportant to have access to tooling providing model\nexplanations.\nCaptum is an open-source model explainability\nlibrary for PyTorch providing a variety of generic\ninterpretability methods proposed in recent litera-\nture such as Integrated Gradients, LIME, DeepLift,\nTracIn, TCAV and more (Kokhlikyan et al., 2020).\nIn this work, we discuss newly open-sourced\nfunctionalities in Captum v0.7 to easily apply ex-\nplainability methods to large generative language\nmodels, such as GPT-3.\n*Denotes equal contribution\n1https://captum.ai\n2\nAttribution Methods\nOne important class of explainability methods is\nattribution or feature importance methods, which\noutput a score corresponding to each input feature\u2019s\ncontribution or impact to a model\u2019s final output.\nFormally, given a function f : Rd \u2192 R, where\nf \u2208 F and X \u2208 Rd is a single input vector con-\nsisting of d dimensions or features, an attribution\nmethod is defined as a function \u03d5 : F \u00d7 Rd \u2192 Rd.\nEach element in the attribution output corresponds\nto a score of the contribution of corresponding fea-\nture i \u2208 D, where D denotes the set of all feature\nindices D = {1, 2, ..., d}.\nMany attribution methods also require a baseline\nor reference input B \u2208 Rd defining a comparison\ninput point to measure feature importance with\nrespect to.\nWe utilize the notation XS to denote selecting\nthe feature values with indices from the set S \u2286 D\nand the remaining indices from B. Formally, the\nvalue of feature i in XS is (XS)i = Ii\u2208SXi +\nIi/\u2208SBi, where I is the indicator function.\nIn this section, we provide background con-\ntext on attribution methods available in Captum.\nThese methods can be categorized broadly into\n(i) perturbation-based methods, which utilize re-\npeated evaluations of a black-box model on per-\nturbed inputs to estimate attribution scores, and\n(ii) gradient-based methods, which utilize back-\npropagated gradient information to estimate attri-\nbution scores. Perturbation-based methods do not\nrequire access to model weights, while gradient-\nbased models do.\n2.1\nPerturbation-Based Methods\n2.1.1\nFeature Ablation\nThe most straightforward attribution is feature ab-\nlation, where each feature is substituted with the\ncorresponding element of the baseline feature vec-\ntor to estimate the corresponding importance.\narXiv:2312.05491v1  [cs.CL]  9 Dec 2023\nFormally, this method is defined as\n\u03d5i(f, X) = f(X) \u2212 f(XD\\{i})\n(1)\nFeature Ablation has clear advantages as a sim-\nple and straightforward method, but the resulting\nattributions may not fully capture the impacts of\nfeature interactions since features are ablated indi-\nvidually.\n2.1.2\nShapley Value Sampling\nShapley Values originated from cooperative game\ntheory as an approach to distribute payouts fairly\namong players in a cooperative game. Analogously,\nin the attribution setting, Shapley Values assign\nscores to input features, with payouts correspond-\ning to a feature\u2019s contribution to the model output.\nShapley Values satisfy a variety of theoretical prop-\nerties including efficiency, symmetry and linearity.\nFormally, this method is defined as\n\u03d5i(f, X) =\nX\nS\u2286D\\{i}\n\u0014|S|!(|D| \u2212 |S| \u2212 1)!\n|D|!\nf(XS\u222a{i}) \u2212 f(XS)\n\u0003\n(2)\nWhile computing this quantity exactly requires\nan exponential number of evaluations in the num-\nber of features, we can estimate this quantity using\na sampling approach (Castro et al., 2009). The\napproach involves selecting a permutation of the\nd features and adding the features one-by-one to\nthe original baseline. The output change as a result\nof adding each feature accounts for its contribu-\ntion, and averaging this over sampled perturbations\nresults in an unbiased estimate of Shapley Values.\n2.1.3\nLIME\nLIME or Locally Interpretable Model Explanations\nproposes a generic approach to sample points in\nthe neighborhood of the input point X and train an\ninterpretable model (such as a linear model) based\non the results of the local evaluations (Ribeiro et al.,\n2016).\nThis method proposes reparametrizing the in-\nput space to construct interpretable features such\nas super-pixels in images and then evaluating the\noriginal model on a variety of perturbations of the\ninterpretable features. The method can be utilized\nwith any perturbation sampling and weighting ap-\nproaches and interpretable model / regularization\nparameters. The interpretable model can then be\nused as an explanation of the model\u2019s behavior in\nthe local region surrounding the target input point.\nFor a linear model, the coefficients of this model\ncan be considered as attribution scores for the cor-\nresponding feature.\n2.1.4\nKernel SHAP\nKernel SHAP is a special case of the LIME\nframework, which sets the sampling approach, in-\ntepretable model, and regularization in a specific\nway such that the results theoretically approximate\nShapley Values (Lundberg and Lee, 2017).\n2.2\nGradient Based Methods\n2.2.1\nSaliency\nSaliency is a simple gradient-based approach, uti-\nlizing the model\u2019s gradient at the input point as the\ncorresponding feature attribution (Simonyan et al.,\n2013). This method can be understood as taking a\nfirst order approximation of the function, in which\nthe gradients would serve as the coefficients of each\nfeature in the model.\n\u03d5i(f, X) = f\u2032(X)\n(3)\n2.2.2\nIntegrated Gradients\nIntegrated Gradients estimates attribution by com-\nputing the path integral of model gradients between\nthe baseline point and input point (Sundararajan\net al., 2017). This approach has been shown to sat-\nisfy desirable theoretical properties including sen-\nsitivity and implementation invariance. Formally,\nthe method can be defined as\n\u03d5i(f, X) = (Xi \u2212 Bi)\n\u00d7\nZ 1\n\u03b1=0\nf\u2032(B + (X \u2212 B)\u03b1)\n\u2202xi\nd\u03b1\n(4)\n2.2.3\nOther Gradient-Based Methods\nOther popular gradient-based attribution methods\ninclude DeepLift and Layerwise Relevance Pro-\npogation (LRP) (Shrikumar et al., 2017; Bach et al.,\n2015). These methods both require a backward\npass of the model on the original inputs but cus-\ntomize the backward propagation of specific func-\ntions, instead of using their default gradient func-\ntions.\n3\nLanguage Model Attribution\nIn Captum v0.7, we propose new functionalities\nto apply the attribution methods within Captum to\nanalyze the behaviors of LLMs. Users can choose\nany interested tokens or segments from the input\nOUTPUT 1:  attribution for most likely   \n                  decoded sequence \nDavid lives in Palm Coast, FL and \u2026\nplaying  golf,  hiking, and cooking.\n  -.204          1.081    -0.8918  -0.0498   -.2699\n Feature Importance Scores Relative to Selected Input\nOUTPUT 2:  attribution for user provided \n                  target string\nDavid lives in Palm Coast, FL and ..\nOpen sourcing explainable tech..\n  .7051        -2.341             -.1851              -.2834                                   \n Feature Importance Scores Relative to Selected Input\nCase 2\nPrompt:  David lives in Palm \nCoast, FL and is a lawyer.  His \npersonal interests include \nTarget: open sourcing \nexplainable techniques for \ngenerative LLMs and golf.\nCase 1\nPrompt:  David lives in Palm \nCoast, FL and is a lawyer.  His \npersonal interests include \nTarget: None\nLLM + \nCAPTUM \nATTRIBUTION \nMETHOD\nLLM + \nCAPTUM \nATTRIBUTION \nMETHOD\nFigure 1: Example of applying Captum attribution methods to analyze the input prompt\u2019s effect on the output of\nLLMs, showing two types of target strings accepted by Captum attribution API and token level attribution outputs\nfor both with respect to the input \"Palm Coast\". In Case 1, no Target string is provided, so attributions are provided\nfor the most likely decoded sequence. In Case 2, attributions are provided for the chosen target output.\nprompt as features, e.g., \"Palm Coast\" in the exam-\nple shown in Figure 1, and use attribution methods\nto quantify their impacts to the generation targets,\nwhich can be either a specified output sequence or\na likely generation from the model.\n3.1\nPerturbation-Based Methods\nWe introduce simple APIs to experiment with\nperturbation-based attribution methods including\nFeature Ablation, Lime, Kernel SHAP and Shapley\nValue Sampling.\nWe prioritize ease-of-use and flexibility, allow-\ning users to customize the chosen features for attri-\nbution, mask features into groups as necessary, and\ndefine appropriate baselines to ensure perturbed\ninputs remain within the natural data distribution.\nIn Figure 2, we demonstrate an example usage\nof the LLMAttribution API for the simple prompt\n\"Dave lives in Palm Coast, FL and is a lawyer. His\npersonal interests include\". Providing this input\nprompt to a language model to generate the most\nlikely subsequent tokens, we can apply Captum\nto understand the impact of different parts of the\nprompt string on the model generation. Figure 3\npresents a more customized usage where we use\nthe same function to understand a specific output\nwe are interested in (\"playing golf, hiking, and\ncooking.\").\n3.1.1\nDefining Features\nUsers are able to define and customize \u2019features\u2019\nfor attribution in the prompt text. The simplest ap-\nproach would be defining the features as individual\ntokens.\nUnfortunately, in many cases, it doesn\u2019t make\nsense to perturb individual tokens, since this may\nno longer form a valid sentence in the natural distri-\nbution of potential input sequences. For example,\nperturbing the token \"Palm\" in the above sentence\nwould result in a city name that is not in the natu-\nral distribution of potential cities in Florida, which\nmay lead to unexpected impacts on the perturbed\nmodel output. Moreover, tokenizers used in mod-\nern LLMs may further break a single word in many\ncases. For example, the tokenizer can break the\nword \"spending\" into \"_sp\" and \"ending\".\nThe API provides flexibility to define units of\nattribution as custom interpretable features which\ncould be individual words, tokens, phrases, or even\nfull sentences. For example, in Figure 2, we select\nthe relevant features to be the name, city, state,\noccupation, and pronoun in the sentence prompt\nand desire to determine the relative contribution of\nthese contextual features on the model\u2019s predicted\nsentence completion.\nUsers can define the units for attribution as a list\nor dictionary of features and provide a format string\nor function to define a mapping from the attribution\nunits to the full input prompt as shown in Figure 3.\n3.1.2\nBaselines\nThe baseline choice is particularly important for\ncomputing attribution for text features, as it serves\nas the reference value used when perturbing the\nchosen feature. The perturbation-based feature API\nallows defining custom baselines corresponding to\nfrom captum.attr import FeatureAblation , LLMAttribution , TextTemplateFeature\nfa = FeatureAblation(model)\nllm_attr = LLMAttribution(fa, tokenizer)\ninp = TextTemplateFeature(\n# the text template\n\"{} lives in {}, {} and is a {}. {} personal interests include\",\n# the values of the features\n[\"Dave\", \"Palm Coast\", \"FL\", \"lawyer\", \"His\"],\n# the reference baseline values of the features\nbaselines=[\"Sarah\", \"Seattle\", \"WA\", \"doctor\", \"Her\"],\n)\nllm_attr.attribute(inp)\nFigure 2: Example of applying Captum with a list of features in a text template\ninp = TextTemplateFeature(\n\"{name} lives in {city}, {state} and is a {occupation}. {pronoun} personal\ninterests include\",\n{\"name\":\"Dave\", \"city\": \"Palm Coast\", \"state\": \"FL\", \"occupation\":\"lawyer\", \"\npronoun\":\"His\"},\nbaselines={\"name\":\"Sarah\", \"city\": \"Seattle\", \"state\": \"WA\", \"occupation\":\"doctor\n\", \"pronoun\":\"Her\"}\n)\nattr_result = llm_attr.attribute(inp , target=\"playing golf , hiking , and cooking.\")\nattr_result.plot_token_attr ()\nFigure 3: Example of applying Captum with a dictionary of features in a text template and a specific target, and\nvisualize the token attribution\neach input feature.\nIt is recommended to select a baseline which fits\nthe context of the original text and remains within\nthe natural data distribution. For example, replac-\ning the name of a city with another city ensures\nthe sentence remains naturally coherent, but allows\nmeasuring the contribution of the particular city\nselected.\nIn addition to a single baseline, the Captum API\nalso supports providing a distribution of baselines,\nprovided as either a list or function to sample a\nreplacement option. For example, in the example\nabove, the name \"Dave\" could be replaced with\na sample from the distribution of common first\nnames to measure any impact of the name \"Dave\"\nin comparison to the chosen random distribution as\nshown in Figure 6.\n3.1.3\nMasking Features\nSimilar to the underlying Captum attribution meth-\nods, we support feature masking, which enables\ngrouping features together to perturb as a single\nunit and obtain a combined, single attribution score.\nThis functionality may be utilized for highly corre-\nlated features in the text input, where it often makes\nsense to ablate these features together, rather than\nindependently.\nFor example, in Figure 2, the feature pairs (city,\nstate) and (name, pronoun) are often highly corre-\nlated, and thus it may make sense to group them\nand consider them as a single feature.\n3.1.4\nTarget Selection\nFor any attribution method, it is also necessary to\nselect the target function output for which attribu-\ntion outputs are computed. Since language models\ntypically output a probability distribution over the\nspace of tokens for each subsequently generated to-\nken, there are numerous choices for the appropriate\ntarget.\nBy default, when no target is provided, the target\nfunction behavior is for the attribution method to\nreturn attributions with respect to the most likely\ndecoded token sequence.\nWhen a target string is provided, the target func-\ntion is the log probability of the output sequence\nfrom the language model, given the input prompt.\nFor a sequence with multiple tokens, this is numer-\nically computed through the sum of the log proba-\nbilities of each token in the target string. Figure 1\nshows these two input use cases and shows token\nlevel attribution relative to an input subsequence\nfor both.\nWe also support providing a custom function on\nthe output logit distribution, which allows attribu-\ntion with respect to an alternate quantity such as\nthe entropy of the output token distribution.\n3.2\nGradient-Based Methods\nCaptum 0.7 also provides a simple API to apply\ngradient-based methods to LLMs. Applying these\nmethods to language models is typically more chal-\nlenging than for models with dense feature inputs,\nsince embedding lookups in LLMs are typically\nnon-differentiable functions, and gradient-based\nattributions need to be obtained with respect to\nembedding outputs. Captum allows these attribu-\ntions to be aggregated across embedding dimen-\nsions to obtain per-token attribution scores. Figure\n4 demonstrates an example of applying Integrated\nGradients on a sample input prompt.\n3.3\nVisualization\nWe also open source utilities for easily visualizing\nthe attribution outputs from language models. Fig-\nure 3 shows how to use the utilities to visualize the\nattribution result, and Figure 5 demonstrates the\nheatmap plotted with the prompt along the top, the\ntarget string along the left side and feature impor-\ntance scores in each cell.\n(Feature) Value\nGolfing\nHiking\nCooking\n(Name) Dave\n0.4660\n-0.2640\n-0.4515\n(City) Palm Coast\n1.0810\n-0.8762\n-0.2699\n(State) FL\n0.6070\n-0.3620\n-0.3513\n(Occupation) lawyer\n0.7584\n-0.1966\n0.0331\n(Pronoun) His\n0.2217\n-0.0650\n-0.2577\nTable 1: Associations between input and generated text\nfeatures\n4\nApplications\nIn this section, we discuss two applications of the\nattribution methods described above in different\ncontexts. These applications provide additional\ntransparency as well as contribute to a better un-\nderstanding of a model\u2019s learned associations and\nrobustness.\n4.1\nUnderstanding Model Associations\nThis perturbation-based tooling can be particularly\nuseful for improved understanding of learned asso-\nciations in LLMs.\nConsider the example prompt:\n\u201cDavid lives in Palm Coast, FL and is a lawyer.\nHis personal interests include \u201d\nWe can define features including gender, city,\nstate and occupation. Obtaining attributions on\nthese features against the subsequent target\n\u201cplaying golf, hiking, and cooking. \u201d\nallows us to better understand why the model pre-\ndicted these personal interests and how each feature\ncorrelates with each of these interests. The model\nmight be associating this activity as a common\nhobby for residents in the specific city or as an ac-\ntivity common to lawyers. Through this choice of\nfeatures, we can obtain a better understanding of\nwhy the model predicted these particular hobbies\nand how this associates with the context provided\nin the prompt.\nWe apply Shapley Value Sampling to better un-\nderstand how each of the features contributed to\nthe prediction. The corresponding code snippet\nis shown in the Appendix in Figure 6. Table 1\npresents the effects of each feature on the LLM\u2019s\nprobability of outputting the corresponding inter-\nest, with positive and negative values indicating\nincreases and decreases of the probability respec-\ntively. We can therefore identify some interesting\nand even potentially biased associations. For ex-\nample, the male name and pronoun, i.e., \"Dave\"\nand \"His\", have positive attribution to \"golfing\" but\nnegative attribution to \"cooking\".\n4.2\nEvaluating Effectiveness of Few-Shot\nPrompts\nSignificant prior literature has demonstrated the\nability of LLMs to serve as few-shot learners\n(Brown et al., 2020). We utilize Captum\u2019s attri-\nbution functionality to better understand the impact\nand contributions of few-shot examples to model\npredictions. Table 2 demonstrates four example\nfew shot prompts and corresponding attribution\nscores when predicting positive sentiment for \"I\nreally liked the movie, it had a captivating plot!\"\nmovie review.\nHere we aim to understand the impact of each ex-\nample prompt on the Positive sentiment prediction.\nThe LLM is asked to predict positive or negative\nsentiment using the following prompt:\n\u201cDecide if the following movie review enclosed in\nquotes is Positive or Negative. Output only either\nPositive or Negative:\nfrom captum.attr import LayerIntegratedGradients , TextTokenFeature\nig = LayerIntegratedGradients(model , \"model.embed_tokens\")\nllm_attr = LLMGradientAttribution(ig , tokenizer)\ninp = TextTokenFeature(\"Dave lives in Palm Coast , FL and is a lawyer. His personal\ninterests include\", tokenizer)\nllm_attr.attribute(inp)\nFigure 4: Example of applying Captum with a gradient-based approach\nFigure 5: Text Attribution Visualization Example\n\u2018I really liked the movie, it had a captivating plot!\u2019\n\u201d\nWe consider each of the provided example\nprompts as features and we utilize zero-shot prompt\nas a baseline in the attribution algorithm. The de-\ntailed implementation can be found in Appendix in\nFigure 7.\nWe obtain results as shown in Table 2 by ap-\nplying Shapley Values. Surprisingly, the results\nsuggest that all the provided examples actually re-\nduced confidence in the prediction of \"Positive\".\nExample\nShapley\nValue\n\u2019The movie was ok, the actors weren\u2019t\ngreat\u2019 -> Negative\n-0.0413\n\u2019I loved it, it was an amazing story!\u2019\n-> Positive\n-0.2751\n\u2019Total waste of time!!\u2019 -> Negative\n-0.2085\n\u2019Won\u2019t recommend\u2019 -> Negative\n-0.0399\nTable 2: Example prompts\u2019 contribution to model re-\nsponse \"Positive.\"\n5\nRelated Work\nNumerous prior works have developed and inves-\ntigated attribution methods with a variety of prop-\nerties, but few efforts have been made to develop\nopen-source interpretability tools providing a vari-\nety of available methods, particularly for the text\ndomain. Captum was initially developed to fill\nthis gap and provide a centralized resource for re-\ncent interpretability methods proposed in literature\n(Kokhlikyan et al., 2020).\nEcco and inseq are two libraries that have pro-\nvided attribution functionalities for text / language\nmodels (Sarti et al., 2023; Alammar, 2021), and\nboth libraries are built on top of the attribution\nmethods available in Captum. These libraries pri-\nmarily focus on gradient-based attribution methods,\nwhich provide token-level attribution based on gra-\ndient information.\nIn contrast, our main contribution in this work\nhas been a focus on perturbation-based methods\nand providing flexibility on aspects such as feature\ndefinition, baseline choice and masking. We do not\nnecessarily expect that these attribution methods\nprovide a score for each token individually, which\nis typically the case for gradient-based methods.\nThis shift in structure allows us to generalize to\na variety of cases and allows the users to define\nfeatures for attribution as it fits best.\nSome prior work on attribution methods have\nalso demonstrated limitations and counterexamples\nof these methods in explaining a model\u2019s reliance\non input features, particularly with gradient-based\nattribution methods (Adebayo et al., 2018).\nPerturbation-based methods generally have an\nadvantage of being justifiable through the model\u2019s\noutput on counterfactual perturbed inputs. But per-\nturbing features by removing individual tokens or\nreplacing them with pad or other baseline tokens\nmay result in inputs outside of the natural data dis-\ntribution, and thus, model outputs in this region\nmay not be accurate. The tools developed have\nbeen designed to make it easier for developers to\nselect features, baselines, and masks which can en-\nsure perturbations remain within the natural data\ndistribution in order to obtain more reliable feature\nattribution results.\nRecent advances in data augmentation (Plu\u0161\u02c7cec\nand \u0160najder, 2023) for natural language processing\nhave led to the development of a number of open-\nsource libraries (Wang et al., 2021; Papakipos and\nBitton, 2022; Zeng et al., 2021; Morris et al., 2020;\nMa, 2019; Dhole et al., 2022; Wu et al., 2021).\nAmong many functionalities, these libraries pro-\nvide a rich set of text perturbations. Some libraries\nhave specific focus, e.g. perturbing demographic\nreferences (Qian et al., 2022). An interesting di-\nrection of future work will be the extension of our\npresent API to provide fully automated feature and\nbaseline selections, allowing users to simply pro-\nvide an input string and automatically identify ap-\npropriate text features and corresponding baselines\nfor attribution.\n6\nConclusion\nIn this work, we introduce new features in the\nopen source library Captum that are specifically de-\nsigned to analyze the behavior of generative LLMs.\nWe provide an overview of the available functional-\nities and example applications of their potential in\nunderstanding learned associations and evaluating\neffectiveness of few-shot prompts within generative\nLLMs. We demonstrate examples for using pertur-\nbation and gradient-based attribution methods with\nCaptum which highlight the API\u2019s flexibility on\naspects such as feature definition, baseline choice\nand masking. This flexibility in structure allows\nusers to generalize to a variety of cases, simplifying\ntheir ability to conduct explainability experiments\non generative LLMs.\nIn the future, we plan to expand our API for\nadditional automation in baseline and feature selec-\ntion as well as incorporate other categories of inter-\npretability techniques for language models. Run-\ntime performance optimization of attribution algo-\nrithms is another area of research that could be\nbeneficial for the OSS community.\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian\nGoodfellow, Moritz Hardt, and Been Kim. 2018. San-\nity checks for saliency maps.\nJ Alammar. 2021. Ecco: An open source library for the\nexplainability of transformer language models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations. Association for\nComputational Linguistics.\nSebastian Bach, Alexander Binder, Gr\u00e9goire Montavon,\nFrederick Klauschen, Klaus-Robert M\u00fcller, and Wo-\njciech Samek. 2015.\nOn pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PloS one, 10(7):e0130140.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nJavier Castro, Daniel G\u00f3mez, and Juan Tejada. 2009.\nPolynomial calculation of the shapley value based\non sampling. Computers & Operations Research,\n36(5):1726\u20131730.\nKaustubh\nD.\nDhole,\nVarun\nGangal,\nSebastian\nGehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-\nhamood, Abinaya Mahendiran, Simon Mille, Ashish\nShrivastava, Samson Tan, Tongshuang Wu, Jascha\nSohl-Dickstein, Jinho D. Choi, Eduard Hovy, On-\ndrej Dusek, Sebastian Ruder, Sajant Anand, Na-\ngender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Car-\noline Brun, Marco Antonio Sobrevilla Cabezudo,\nSamuel Cahyawijaya, Emile Chapuis, Wanxiang\nChe, Mukund Choudhary, Christian Clauss, Pierre\nColombo, Filip Cornell, Gautier Dagan, Mayukh\nDas, Tanay Dixit, Thomas Dopierre, Paul-Alexis\nDray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di\nGiovanni, Tanya Goyal, Rishabh Gupta, Rishabh\nGupta, Louanes Hamla, Sang Han, Fabrice Harel-\nCanada, Antoine Honore, Ishan Jindal, Przemys-\nlaw K. Joniak, Denis Kleyko, Venelin Kovatchev,\nKalpesh Krishna, Ashutosh Kumar, Stefan Langer,\nSeungjae Ryan Lee, Corey James Levinson, Hualou\nLiang, Kaizhao Liang, Zhexiong Liu, Andrey Lukya-\nnenko, Vukosi Marivate, Gerard de Melo, Simon\nMeoni, Maxime Meyer, Afnan Mir, Nafise Sadat\nMoosavi, Niklas Muennighoff, Timothy Sum Hon\nMun, Kenton Murray, Marcin Namysl, Maria Obed-\nkova, Priti Oli, Nivranshu Pasricha, Jan Pfister,\nRichard Plant, Vinay Prabhu, Vasile Pais, Libo Qin,\nShahab Raji, Pawan Kumar Rajpoot, Vikas Rau-\nnak, Roy Rinberg, Nicolas Roberts, Juan Diego\nRodriguez, Claude Roux, Vasconcellos P. H. S.,\nAnanya B. Sai, Robin M. Schmidt, Thomas Scialom,\nTshephisho Sefara, Saqib N. Shamsi, Xudong Shen,\nHaoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel,\nDamien Sileo, Jamie Simon, Chandan Singh, Ro-\nman Sitelew, Priyank Soni, Taylor Sorensen, William\nSoto, Aman Srivastava, KV Aditya Srivatsa, Tony\nSun, Mukund Varma T, A Tabassum, Fiona Anting\nTan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn,\nAthena Wang, Zijian Wang, Gloria Wang, Zijie J.\nWang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata,\nXinyi Wu, Witold Wydma\u00b4nski, Tianbao Xie, Usama\nYaseen, Michael A. Yee, Jing Zhang, and Yue Zhang.\n2022. Nl-augmenter: A framework for task-sensitive\nnatural language augmentation.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.\nCaptum: A unified and generic model interpretability\nlibrary for pytorch.\nScott Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions.\nEdward\nMa.\n2019.\nNlp\naugmentation.\nhttps://github.com/makcedward/nlpaug.\nJohn X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in nlp.\nOpenAI. 2023. Gpt-4 technical report.\nZoe Papakipos and Joanna Bitton. 2022. Augly: Data\naugmentations for robustness.\nDomagoj Plu\u0161\u02c7cec and Jan \u0160najder. 2023. Data augmen-\ntation for neural nlp.\nRebecca Qian,\nCandace Ross,\nJude Fernandes,\nEric Michael Smith, Douwe Kiela, and Adina\nWilliams. 2022. Perturbation augmentation for fairer\nNLP. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9496\u20139521, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should i trust you?\": Explain-\ning the predictions of any classifier.\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar\nvan der Wal, Malvina Nissim, and Arianna Bisazza.\n2023. Inseq: An interpretability toolkit for sequence\ngeneration models. ArXiv, abs/2302.13942.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017.\nLearning important features through\npropagating activation differences. In International\nconference on machine learning, pages 3145\u20133153.\nPMLR.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2013. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. arXiv preprint arXiv:1312.6034.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional conference on machine learning, pages 3319\u2013\n3328. PMLR.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng\nZou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui\nZheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li,\nChong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai,\nJun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan,\nYuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin\nZhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong\nPeng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,\nXipeng Qiu, and Xuanjing Huang. 2021. TextFlint:\nUnified multilingual robustness evaluation toolkit for\nnatural language processing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 347\u2013355, Online. Association\nfor Computational Linguistics.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel S Weld. 2021. Polyjuice: Generating coun-\nterfactuals for explaining, evaluating, and improving\nmodels. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6707\u20136723.\nGuoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji\nZhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan\nLiu, and Maosong Sun. 2021. OpenAttack: An open-\nsource textual adversarial attack toolkit. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations. Association for Computa-\ntional Linguistics.\nA\nAppendix\nfrom captum.attr import ShapleyValueSampling , LLMAttribution , TextTemplateFeature ,\nProductBaselines\nsvs = ShapleyValueSampling(model)\nbaselines = ProductBaselines(\n{\n(\"name\", \"pronoun\"): [(\"Sarah\", \"Her\"), (\"John\", \"His\")],\n\"city\": [\"Seattle\", \"Boston\"],\n\"state\": [\"WA\", \"MA\"],\n\"occupation\": [\"doctor\", \"engineer\", \"teacher\", \"technician\", \"plumber\"],\n}\n)\nllm_attr = LLMAttribution(svs , tokenizer)\ninp = TextTemplateFeature(\n\"{name} lives in {city}, {state} and is a {occupation}. {pronoun} personal\ninterests include\",\n{\"name\":\"Dave\", \"city\": \"Palm Beach\", \"state\": \"FL\", \"occupation\":\"lawyer\", \"\npronoun\":\"His\"},\nbaselines=baselines ,\n)\nattr_result = llm_attr.attribute(inp , target=\"playing golf , hiking , and cooking.\")\nFigure 6: Applying Captum for the model associations example\nfrom captum.attr import ShapleyValues , LLMAttribution , TextTemplateFeature\nsv = ShapleyValues(model)\nllm_attr = LLMAttribution(sv, tokenizer)\ndef prompt_fn(*examples):\nmain_prompt = \"Decide if the following movie review enclosed in quotes is\nPositive or Negative :\\n'I really liked\nthe Avengers , it had a captivating\nplot!'\\nReply only Positive or\nNegative.\"\nsubset = [elem for elem in examples if elem]\nif not subset:\nprompt = main_prompt\nelse:\nprefix = \"Here are some examples of movie reviews and classification of\nwhether they were Positive or\nNegative :\\n\"\nprompt = prefix + \"\\n\".join(subset) + \"\\n\" + main_prompt\nreturn \"[INST] \" + prompt + \"[/INST]\"\ninput_examples = [\n\"'The movie was ok, the actors weren't great' -> Negative\",\n\"'I loved it, it was an amazing story!' -> Positive\",\n\"'Total waste of time!!' -> Negative\",\n\"'Won't recommend ' -> Negative\",\n]\ninp = TextTemplateFeature(prompt_fn , input_examples)\nattr_result = llm_attr.attribute(inp)\nFigure 7: Applying Captum for the few-shot prompt example\n"
  },
  {
    "title": "Order Matters in the Presence of Dataset Imbalance for Multilingual Learning",
    "link": "https://arxiv.org/pdf/2312.06134.pdf",
    "upvote": "2",
    "text": "Order Matters in the Presence of Dataset Imbalance\nfor Multilingual Learning\nDami Choi\u2217\u2020\nUniversity of Toronto\nchoidami@cs.toronto.edu\nDerrick Xin\u2217\nGoogle Research\ndxin@google.com\nHamid Dadkhahi\nGoogle Research\nhdadkhahi@google.com\nJustin Gilmer\nGoogle Deepmind\ngilmer@google.com\nAnkush Garg\nGoogle Deepmind\nankugarg@google.com\nOrhan Firat\nGoogle Deepmind\norhanf@google.com\nChih-Kuan Yeh\nGoogle Deepmind\nchihkuanyeh@google.com\nAndrew M. Dai\nGoogle Deepmind\nadai@google.com\nBehrooz Ghorbani\nOpenAI\nghorbani@openai.com\nAbstract\nIn this paper, we empirically study the optimization dynamics of multi-task learning,\nparticularly focusing on those that govern a collection of tasks with significant\ndata imbalance. We present a simple yet effective method of pre-training on high-\nresource tasks, followed by fine-tuning on a mixture of high/low-resource tasks.\nWe provide a thorough empirical study and analysis of this method\u2019s benefits\nshowing that it achieves consistent improvements relative to the performance trade-\noff profile of standard static weighting. We analyze under what data regimes this\nmethod is applicable and show its improvements empirically in neural machine\ntranslation (NMT) and multi-lingual language modeling.\n1\nIntroduction\nOver the past few years, large multi-task neural networks have emerged as a popular modeling\nparadigm in deep learning. The appeal behind these models is that they can leverage transfer learning\namong the tasks to outperform single-task models. Indeed, multi-task models have achieved state-of-\nthe-art performance in domains such as machine translation [2, 8], language understanding [24, 32],\nand speech recognition [4, 3].\nUnfortunately, optimizing such multi-task models remains a challenge. To effectively train these\nmodels, the different tasks need to be balanced during training. This is often done by sampling each\ntask with a static probability.\nPrior work [31, 20] shows evidence that when all tasks are in the data rich regime (high-resource),\nsuch static sampling approaches yield optimal results. However, when certain tasks are data sparse\n\u2217Equal contribution \u2020Work done as a student researcher at Google.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.06134v1  [cs.CL]  11 Dec 2023\n(low-resource)2, which is quite common in real-world applications, the optimality of static sampling\nis unclear.\nThe problem with static sampling in the presence of low-resource tasks is that it has difficulty dealing\nwith overfitting on the low-resource tasks. This is because early stopping is not a viable solution\ndue to high-resource tasks needing many more epochs to converge. The transfer learning scheme\nof pre-training on high-resource and fine-tuning on low-resource tasks (such as in [33]) provides a\nsolution to the overfitting problem, since the training of high and low-resource tasks are separated.\nNot only this, but the training of low-resource tasks can potentially benefit from positive transfer that\ncomes from performing well on the high-resource tasks. The problem with this approach, however, is\nthat during the fine-tuning phase, catastrophic forgetting of the pre-training tasks ensues.\nIn this paper, we introduce a simple training scheme that combines the best of static sampling and\ntransfer learning: pre-train on a high-resource task and fine-tune jointly on a mixture of high and low-\nresource tasks. A pre-training and fine-tuning scheme effectively enables early stopping by allowing\nthe training of low-resource tasks to happen for as little as needed to prevent overfitting, while training\nthe high-resource task for as long as needed. Furthermore, pre-training on a high-resource task will\npotentially enable positive transfer for low-resource tasks and result in faster convergence in the\nfine-tuning phase. Lastly, the fine-tuning phase on a mixture of high and low-resource tasks will\nnot only remedy the catastrophic forgetting issue of fine-tuning only on low-resource tasks, but also\nenjoy further transfer learning among all the tasks.\nThrough an extensive empirical study, we find that the pre-training and joint fine-tuning scheme\nyields superior low-resource task performance compared to both static sampling and the transfer-\nlearning scheme. We observed that the performance improvement on static sampling is driven by\ntwo mechanisms. The first is that pre-training initializes the fine-tuning phase at a better starting\npoint than random initialization due to positive transfer. The second is that higher sampling rates are\nmore data-efficient than lower sampling rates. Because our method has two separate training phases,\nthe low-resource-training phase can be short. This in turn enables us to increase the low-resource\nsampling rate without risking overfitting. Indeed, our method is more data-efficient than static\nsampling in terms of the low-resource tasks throughout the entire fine-tuning phase, achieving better\nlow-resource task performance while using only a fraction of the data seen by static sampling. We\nfurther observe that pre-training and joint fine-tuning seems to have a regularization effect. However,\nwe find that regularization is not the main factor behind the performance improvement, since increased\nexplicit regularization, such as dropout, does not improve the performance to the extent that our\nmethod does.\nThe contributions of this paper can be summarized as follows:\n\u2022 To the best of our knowledge, we are the first to show that it is possible to push the Pareto\nfront of static sampling in the data-imbalanced regime.\n\u2022 We present a simple algorithm that can be readily used to boost low-resource tasks\u2019 perfor-\nmance in multilingual models.\n\u2022 We show on realistic workloads (up to 13B parameters) that our scheme performs bet-\nter than static sampling and transfer learning with respect to the low-resource language-\npair/language.\n2\nBackground\nIn our work, we focus on the supervised setting, where our model parameters \u03b8 \u2208 Rp are trained on\nK different tasks, with the loss for task i being Li(\u03b8).\nWe introduce the idea of Pareto optimality to better explain the trade-off effect that happens when\ntraining on many different tasks.\nDefinition (Pareto Optimality). \u03b8 \u2208 Rp Pareto dominates another \u03b8\u2032 if \u22001 \u2264 i \u2264 K, Li(\u03b8) \u2264 Li(\u03b8\u2032)\nand there exists a task j where Lj(\u03b8) < Lj(\u03b8\u2032). \u03b8 is Pareto optimal if it is not dominated by any\nother point. The collection of the Pareto optimal points is denoted as the Pareto front.\n2In this literature, data rich and data sparse tasks are often referred to as high-resource and low-resource\nrespectively. Note that whether a task is high-resource or not depends on both the amount of training data and\nthe model capacity.\n2\nA standard approach for optimizing multi-task models is scalarization [5] or static sampling:\n\u02c6\u03b8(w) = arg min\n\u03b8\nK\nX\ni=1\nwiLi(\u03b8),\n(1)\nwhere w is a fixed vector of pre-determined task weights with w > 0 and P\ni wi = 1.\nIn our work, we follow convention and implement scalarization via proportional sampling, where\ndata from task i is sampled with probability equal to wi. In this case, the expected loss is equal to the\nloss from scalarization:\nL(\u03b8) = Ex [\u2113(x; \u03b8)] =\nK\nX\ni=1\nP(task i)Ex\u223ctask i [\u2113(x; \u03b8)] =\nK\nX\ni=1\nwiLi(\u03b8).\n(2)\nPrior work [31] studied the performance trade-off behavior of scalarization and a variety of different\nmulti-task optimization (MTO) methods in the two-task setting. They found that both in the high-\nresource case and in the data-imbalanced case, no MTO method improved upon the Pareto front\nof scalarization. In our work, we compare the performance trade-off behavior of scalarization\nand our proposed method, and find that the Pareto front of scalarization can be improved in the\ndata-imbalanced regime.\nNote that practically speaking, it is not feasible to determine whether \u03b8 is truly Pareto optimal since\nwe must check that it is not dominated by all \u03b8\u2032 \u2208 Rp. Following [31], instead of considering all of\nRp we consider only the parameters reachable by a fixed set of hyperparameters.\n3\nPre-training Joint Fine-tuning\nGiven K tasks, among which some are low-resource, our goal is to optimize the performance of the\nlow-resource tasks without sacrificing the performance of the remaining tasks. Static sampling is not\nideal because all tasks are seen constantly throughout the entirety of training, resulting in overfitting\nof low-resource tasks while high-resource tasks still need to be learned. Naively breaking up training\ninto two phases and training on low-resource tasks in the later phase results in catastrophic forgetting\nof earlier-trained tasks.\nAssuming the existence of at least one high-resource task, we propose to first pre-train on a high-\nresource task, and fine-tune the resulting model on the full mixture of K tasks. We call this method\npre-training joint fine-tuning3.\nIn our preliminary experiments, we found that it is important to reset the learning rate schedule\nand optimizer state when switching over to the joint fine-tuning phase. This is because learning is\nextremely slow for tasks that are newly introduced when the learning rate has already decayed. In our\nevaluations, we additionally experiment with adding resetting to the scalarization baseline to ensure\nthat improvements from our method are not purely from resetting. See Sections 4.1.2 and 4.2 for\nmore detail.\nOur two-stage training process introduces additional hyperparameters compared to scalarization: the\nhyperparameters involved in the pre-training phase, and the length of the pre-training phase. However,\nwe find that tuning is not much more difficult than scalarization, and in some cases it is easier to tune.\nThe pre-training phase only involves tuning for a single task, which is much easier than tuning for\nmultiple tasks. We also expect the joint fine-tuning phase to be shorter than the full training length\nof scalarization; therefore, tuning for the second phase should be around the same or easier than\nscalarization. Lastly, our results show that pre-training does not hurt fine-tuning performance and\nlonger pre-training translates to better fine-tuning. From this, we recommend that if there is a strict\ntraining budget, it is better to be conservative and pre-train for a shorter amount of time. However,\nif the goal is to obtain the best performance and there is no strict compute budget, we recommend\npre-training for as long as possible before fine-tuning. See Section 4.3 for more details.\n3We use the terms \u2018pre-training\u2019 and \u2018fine-tuning\u2019 only to distinguish the two phases of training, and that\nthe training objectives are the same for both phases. In other words, we do not suggest using any particular\nself-supervised objective for the pre-training phase, or training on downstream tasks for the fine-tuning phase.\n3\n4\nExperiments\nIn the following sections, we apply our proposed training scheme to NMT (where each task is a\nlanguage-pair) and multilingual training (where each task is a language). In the NMT experiments,\nwe show that pre-training joint fine-tuning pushes past the trade-off frontier of scalarization through\nsignificant improvements on the low-resource task\u2013 a feat that many popular gradient-based multi-task\noptimization methods were not able to achieve [31]. In the language modeling experiments, we scale\nup the number of tasks, and show that our method retains the same benefits for the low-resource\nlanguages.\n4.1\nNeural Machine Translation\nFor our first experiment, we focus on a setting where we can trace out, and compare the trade-off\nfrontiers obtained with and without pre-training. As in prior work [31], we choose to work on the\ntwo-task setting due to the ease of visualizing the performance trade-off curves.\nWe choose our high and low-resource language-pairs from the WMT dataset,\nwhere\nEnglish\u2192{Chinese, French} are the high-resource language pairs, and English\u2192{Romanian, Hindi}\nare the low-resource language pairs. See Table 1 for details on each language-pair. All models in this\nsection use a pre-LayerNorm encoder-decoder transformer architecture [28]. In the main paper, we\npresent results on models with three encoder layers and three decoder layers. Results obtained with a\nlarger model size are in Appendix A.2. Further details, including hyperparameters, are in A.1.\nTable 1: Overview of data sources used in our\nNMT experiments. Our datasets are from WMT.\nLanguage Pair\n# Train Ex.\n# Eval Ex.\nEn-Fr \u201915\n40, 853, 298\n4, 503\nEn-Zh \u201919\n25, 986, 436\n3, 981\nEn-Ro \u201916\n610, 320\n1, 999\nEn-Hi \u201914\n313, 748\n520\nIn order to trace out the trade-off frontiers for\nthe pre-training joint fine-tuning method and the\nscalarization baseline, we adhere to the follow-\ning methodology. For scalarization, we iterate\nthrough a grid of task weights (since there are\nonly two tasks, a grid is a linear function of the\ngranularity) and train on the two language pairs\nfor N steps using proportional sampling accord-\ning to the task weights. For the pre-training\njoint fine-tuning method, we first pre-train on\nthe high-resource language pair for N1 training\nsteps. We then reset the optimizer state and the learning rate schedule and fine-tune on a mixture of\nhigh-resource and low-resource language pairs for N2 training steps such that N1 + N2 = N. For\nthe fine-tuning phase, we iterate through a grid of task weights as with scalarization. The grid of\nsampling rates will trace a performance trade-off front, which can be used to compare our method\nand scalarization.\nLastly, we train a restart baseline in order to ablate the possibility that any improvements coming from\npre-training joint fine-tuning are due to the resetting of optimizer state and learning rate schedules\nbefore fine-tuning. The restart baseline takes the model obtained via scalarization trained for N1\nsteps, resets optimizer states and the learning rate schedule, and continues to train it with the same\nsampling rate as in scalarization.\n4.1.1\nHigh-Resource and High-Resource:\nWe first start by highlighting that pre-training joint fine-tuning does not show benefits if all tasks\nare high-resource. Figure 1 shows that in the English\u2192{Chinese, French} translation tasks, the\nperformance on each of the language-pairs are bounded by the amount of data seen from that pair. In\nother words, pre-training on En\u2192Fr cannot act as a proxy for En\u2192Zh data, because if it could, the\nfront would be improved. At the same time, pre-training does not negatively impact En\u2192Zh training.\nFigures 21 and 22 show that pre-training does not affect the learning efficiency for En\u2192Zh (slope of\nthe curves are similar to one another), and also did not result in a worse initialization for En\u2192Zh.\n4.1.2\nHigh-Resource and Low-Resource\nIn the data-imbalanced setting of English\u2192{Romanian, French}, we pre-train for 400k steps and\nfine-tune for 50k steps to emphasize the computational benefits of pre-training fine-tuning. Although\na single full run of scalarization (N steps) and pre-training fine-tuning (N1 + N2 = N) take the same\n4\n2.2\n2.4\n2.6\n2.8\n3.0\nValid Cross-Entropy Loss (En \u2192 Zh)\n1.2\n1.3\n1.4\n1.5\n1.6\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\nTrain Cross-Entropy Loss (En \u2192 Zh)\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\n1.55\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nEn-Zh Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 1: The trade-off front from pre-training does not improve upon the trade-off front from fully\nstatic sampling when all tasks are high-resource. The performance on each of the high-resource tasks\nare bounded by the amount of data seen for that task. We can also observe interference between the\ntwo tasks from how all 9 different sampling rates form the trade-off frontier. These observations hold\nfor both testing (left) and training (right).\n1.75\n1.80\n1.85\n1.90\n1.95\n2.00\n2.05\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.24\n1.26\n1.28\n1.30\n1.32\n1.34\n1.36\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nBest Scalarization En-Ro Loss\nRestart Baseline\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrain Cross-Entropy Loss (En \u2192 Ro)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nEn-Ro Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 2:\n(Left:) In the data-imbalanced case, the trade-off front from pre-training yields better\nlow-resource task performance than the trade-off front of scalarization. The poor performance of the\nrestart baseline shows that the resetting of states is not why pre-training and fine-tuning performs\nwell. Note that the trade-off fronts consist of only a subset of the sampling ratios due to overfitting,\nwhich is different from the fully high-resource setting. Right: Pre-training results in a noticeably\nworse performance on the training set, hinting that pre-training has a regularization effect on the\nlow-resource task.\namount of compute, pre-training joint fine-tuning makes hyperparamter tuning much more efficient,\nsince 1) tuning for pre-training is on a single task and therefore, easier to tune, and 2) tuning for\nfine-tuning is faster since N2 \u226a N.\nIn Figure 2 we can observe that pre-training joint fine-tuning is able to achieve performance trade-\noff points that go beyond what is achievable via scalarization. Pre-training on a high-resource\nlanguage pair creates non-dominated points by yielding significantly better performance in the\nlow-resource task (En\u2192Ro) without completely sacrificing performance in the high-resource task\n(En\u2192Fr). Additionally, it is able to do this while seeing less overall Romanian tokens according to\nFigure 3.\nWe see similar results for En\u2192{Hi, Fr}, shown in Figure 12 in the Appendix. This is a surprising\nresult since French and Hindi are less linguistically similar than French and Romanian. Finally, we\ncan see from the sub-optimal performance of the restart baseline in Figures 2 and 12 that the act of\nresetting is not the reason behind the success of the pre-training joint fine-tuning scheme. We provide\nBLEU score evaluations for En\u2192{Ro, Fr} and En\u2192{Hi, Fr} in Appendix A.5, validating that the\nimprovements in loss translate to downstream metrics.\n5\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Ro)\n10\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Ro)\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nFigure 3:\nPre-training joint fine-tuning has both better initialization and data-efficiency than\nscalarization. Each line corresponds to the datapoint that achieved the best En\u2192Ro validation loss in\nFigure 2 among the different run groups.\n103\n104\n105\n106\n107\n108\n109\nNum. Examples Seen (En \u2192 Ro)\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Ro)\n103\n104\n105\n106\n107\n108\n109\nNum. Examples Seen (En \u2192 Ro)\n1\n7\n5\n3\n2\nTrain Cross-Entropy Loss (En \u2192 Ro)\n10\u22123\n10\u22122\n10\u22121\nEn-Ro Rate\nFigure 4: Each curve corresponds to a single scalarization trial with a particular (static) sampling rate\nfor En\u2192Ro. The rate at which the training loss decreases is slower for lower En\u2192Ro sampling rates\nthan for higher sampling rates. At higher sampling rates, overfitting starts to happen.\n4.1.3\nAnalysis\nThe performance improvement of pre-training joint fine-tuning stems from two main mechanisms.\n\u2022 Pre-training utilizes positive transfer between tasks, and initializes the fine-tuning phase at a\nbetter starting point than random initialization. Figure 3 shows this effect for the En\u2192{Ro,\nFr} translation tasks.\n\u2022 Higher sampling rates are more data-efficient than lower sampling rates. Figure 4 shows how\noptimization (training set performance) gets more and more data-efficient as the sampling\nrate increases. However, on the generalization side, increasing the sampling rate works only\nup until a certain point, where overfitting kicks in.\nBy design, pre-training joint fine-tuning has two separate training phases which allows the low-\nresource-training phase to be short. This in turn enables us to increase the low-resource sampling\nrate, resulting in faster training. This effect can be seen in Figure 2, where the En\u2192Ro sampling\nrates that resulted in the best En\u2192Ro performance was 0.4, while for pre-training joint fine-tuning,\nthe best rate is 0.5. Figure 3 confirms that indeed after pre-training, fine-tuning on En\u2192Ro is more\ndata-efficient than not pre-training.\nJoint fine-tuning is also an important piece in addition to the two-stage setup. Only fine-tuning on the\nlow-resource task, which is the classic transfer learning scheme, results in overfitting and catastrophic\nforgetting of the pre-training task as shown in Figure 6.\nLastly, Figure 2 shows that pre-training joint fine-tuning yields worse training set performance, and\ntherefore, could be seen as having a regularization effect. We show in Figure 5 that regularization by\nitself does not explain the superior performance of our scheme.\nThe results seen so far show that data order matters when training in the presence of a low-resource\ntask, since seeing high-resource data first before seeing low-resource data later pushes the pareto\nfront of seeing both types of data at the same time.\n6\n1.8\n1.9\n2.0\n2.1\n2.2\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\n1.55\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (450k, 0.1 do)\nEn-{Ro,Fr} (450k, 0.2 do)\nEn-{Ro,Fr} (450k, 0.3 do)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Cross-Entropy Loss (En \u2192 Ro)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (450k, 0.1 do)\nEn-{Ro,Fr} (450k, 0.2 do)\nEn-{Ro,Fr} (450k, 0.3 do)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nEn-Ro Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 5: pre-training joint fine-tuning has a regularization effect, but cannot be replaced by simply\nincreasing regularization strength. The dropout rate used in pre-training joint fine-tuning is 0.1.\n4.2\nMultilingual Training\n2\n3\n4\n5\n6\n7\n8\nValid Cross-Entropy Loss (En \u2192 Ro)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-Fr (200k)\nEn-Fr (200k) + En-Ro (50k)\nEn-Fr (200k) + En-{Ro,Fr} (50k)\nFigure 6: Fine-tuning solely on the low-resource\ntask (En\u2192Ro) leads to both catastrophic forgetting\nof the pre-trained task (En\u2192Fr) and worse low-\nresource task performance than fine-tuning on all\ntasks (En\u2192{Ro, Fr}).\nIn this section, we expand from a two-task set-\nting to a many-task setting. We train on five\nlanguages from the mC4 dataset [32]\u2013English,\nHindi, Gujarati, Swahili, and Gaelic\u2013 using the\nspan corruption objective from T5 [24]. See Ta-\nble 2 for details on the dataset. Canonically the\nmC4 dataset is used in the pre-training phase for\nmodels (not to be confused by our pre-training\njoint fine-tuning method). These models are\nsubsequently applied to downstream tasks such\nas question answering. This multilingual pre-\ntraining phase is also known as the language\nbalancing problem. Our goal is to show that our\ntwo stage method can effectively balance high-\nresource and low-resource languages, improving\nperformance on low-resource languages beyond\nwhat is achievable by the conventional method\nof temperature sampling while not sacrificing\nperformance on high-resource languages.\nTable 2: Data used from mC4.\nLanguage\n# Chars (B)\nEn (English)\n13, 396\nHi (Hindi)\n75\nGu (Gujarati)\n3.6\nGd (Gaelic)\n0.8\nSw (Swahili)\n4.1\nNote that in the mC4 corpus, English is 16745 times larger\nthan the smallest language we use. This data imbalance un-\nderscores the necessity for effective language balancing, par-\nticularly in determining the proportion of each language to\nbe used during training. This presents a highly challenging\nand computationally demanding problem, as it is not feasible\nto simply sweep the scalarization weights as one would in a\ntwo-task setting.\nFor our training setup we closely follow mT5 [32] for the\nmodel architecture and training procedure. Specifically, we use the mT5-XXL model (13B parame-\nters), which is an encoder-decoder transformer architecture. Additional training details are available\nin Appendix B.\nTemperature Sampling\nBecause we increase the amount of tasks in this setting, detailing the full\nscalarization trade-off frontier would be computationally infeasible. Therefore, we employ the widely\nused temperature sampling heuristic [11, 7, 2]. Let Di be data size of language or task i, we then\ndefine the empirical distribution P for each task i as:\nP(x \u2208 task i) =\nDi\nP\nj Dj\n.\n(3)\n7\n1010\n1011\n1012\nNum. Chars Seen\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\nEn Valid Cross-Entropy\n109\n1010\n1011\nNum. Chars Seen\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\nHi Valid Cross-Entropy\n109\n1010\nNum. Chars Seen\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\nGu Valid Cross-Entropy\n109\n1010\nNum. Chars Seen\n2\n3\nGd Valid Cross-Entropy\n109\n1010\nNum. Chars Seen\n2\n3\nSw Valid Cross-Entropy\n= 3.33 (500k)\n= 3.33 (200k) + = 3.33 (300k)\nEn (200k) + = 3.33 (300k)\nFigure 8: Pre-training on English and joint fine-tuning on all 5 languages leads to better optima\nfor Gujarati, Gaelic and Swahili, the 3 low-resource languages. Pre-training also results in better\ninitialization and token-efficiency for all languages newly seen in the fine-tuning phase.\nTemperature sampling then uses a distribution Q defined by a temperature parameter \u03c4 as follows:\nQ(x \u2208 task i) =\nP(x \u2208 task i)1/\u03c4\nP\nj P(x \u2208 task j)1/\u03c4\n(4)\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n= 3.33 (500k)\n= 3.33 (200k) + = 3.33 (300k)\nEn (200k) + = 3.33 (300k)\nEn\nHi\nGu\nGd\nSw\n0.0\n0.1\nBest Valid Cross-Entropy Loss\nFigure 7: Pre-training joint fine-tuning yields\nthe best performance in 4 out of 5 languages,\nwith significant improvements in the low-\nresource tasks.\nThe temperature parameter \u03c4 controls the peakiness\n(or flatness) of the sampling distribution. Commonly\nused \u03c4\u2019s in the literature are greater than 1, which\nessentially up-samples low-resource tasks and down-\nsamples high-resource tasks.\nStatic Sampling Baseline\nTemperature sampling is\nubiquitous due to its simplicity and intuitiveness, but\nits performance varies greatly with \u03c4. For our static\nsampling baseline, we tuned \u03c4 among commonly\nused values in the literature (1.43, 2, 3.33, 5) at a\nsmaller scale, and found that \u03c4 = 3.33 performed\nthe best in terms of low-resource languages. We\nalso tried a more intricate sampling strategy called\nUniMax [6], but found that on the 5 languages we\nchose, it did not perform better than \u03c4 = 3.33.\nPre-training joint Fine-tuning\nFor our pre-training joint fine-tuning setup, we first pre-train on\nEnglish, reset the optimizer state and learning rate schedule, and then fine-tune on all 5 languages\nusing temperature sampling. We use the same sampling rates as the static sampling baseline (\u03c4 =\n3.33) to reduce the tuning overhead over static sampling.\nAs in the NMT experiments, we employ a restart baseline to fully ablate the pre-training fine-tuning\nscheme. The restart baseline resets the optimizer state and learning rate schedule in the middle of\ntraining for the static sampling baseline.\nResults\nFigures 7 and 8 show that while a learning rate schedule restart helps performance, pre-\ntraining joint fine-tuning yields the best results on the low-resource tasks. Surprisingly, it not only\nimproves the performance on Gujarati, Gaelic, and Swahili, but also shows a slight enhancement on\nEnglish. We note that due to the vast dataset imbalance, the temperature sampling baseline overfits\non the low-resource tasks before English has a chance to converge. Consequently, pre-training joint\nfine-tuning can leverage the benefits mentioned in the previous section\u2013regularization, transfer, and\nreduced forgetting\u2013to achieve a superior lower bound performance with higher token efficiency.\n8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n= 3.33 (500k)\nEn (100k) + = 3.33 (400k)\nEn (200k) + = 3.33 (300k)\nEn (300k) + = 3.33 (200k)\nEn\nHi\nGu\nGd\nSw\n0.0\n0.1\nBest Valid Cross-Entropy Loss\n(a)\n1.75\n1.80\n1.85\n1.90\n1.95\n2.00\n2.05\n2.10\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.24\n1.26\n1.28\n1.30\n1.32\n1.34\n1.36\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-Fr (200k) + En-{Ro,Fr} (50k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nEn-Fr (600k) + En-{Ro,Fr} (50k)\nBest Scalarization En-Ro Loss\n(b)\nFigure 9: Left: For language modeling on mC4, longer pre-training leads to better best-achievable\nperformance for the 3 low-resource languages (Gu, Gd, Sw) despite the decreased length of fine-\ntuning. On the other hand, due to the decreased length of fine-tuning, high-resource languages do\nnot enjoy the benefits of pre-training. Right: For NMT, when the training budget is not fixed, longer\npre-training leads to better overall performance trade-off fronts.\n4.3\nLength of Pre-training\nOur method is simple but comes with some choices to make, one of which is the number of steps\nto pre-train for. We investigate the effect of the number of pre-training steps in NMT and language\nmodeling on mC4 by pre-training with less, and more steps than in the previous sections. With\nthe language modeling task, we fix the total training length to be 500k steps to emulate a compute-\nconstrained scenario. We chose to use a smaller model (mT5-XL as opposed to mT5-XXL used in\nSection 4.2 for faster training). With NMT, we fix the number of fine-tuning steps, but let the total\ntraining steps vary.\nFigure 9(a) displays the effects of varying pre-training length in the mC4 experiments. We see that\nlonger pre-training improves best achievable performance on the low-resource tasks of Gujarati,\nGaelic, and Swahili. This is despite the fact that the number of fine-tuning steps decreased due to the\nfixed total step budget. In other words, for the 3 low-resource tasks, longer pre-training improves\nperformance more than exposure to the tokens. On the other hand, performance on English and Hindi\nworsens with increased pre-training length. For English, this is due to the resetting of the learning rate\nschedule and the decreasing of fine-tuning steps. Resetting involves a learning rate warmup, which\nworsens English performance before improving again (see the panel corresponding to En for Figure\n8). Decreasing fine-tuning steps gives English less time to recover its performance from pre-training.\nFor Hindi, the worsened performance is simply because it is not a low-resource task in this context,\nand therefore, less tokens seen translates to worse performance.\nIn Figure 9(b) we see that in the NMT experiments, pre-training longer on En\u2192Fr translates to better\noverall trade-off fronts, not just for the low-resource task.\nThe implications of these results are that when there is a strict training budget, it is better to be\nconservative and pre-train for a shorter amount of time. However, if the goal is to obtain the best\nperformance with no strict compute budget, it is better to pre-train for as long as possible before\nfine-tuning. Note that longer overall training is an option for our method (by pre-training for longer)\nbut not for static sampling because static sampling needs to constantly be training on the low-resource\ntasks, which will lead to overfitting when training for too long.\n5\nRelated Work\nMultitask Learning\nMultitask learning has gained increased attention in being able to learn many\ntasks in an efficient way due to parameter sharing and transfer between tasks. In the language domain,\nmultilingual neural machine translation [12, 14] enables translation from multiple source languages\nto multiple target languages. Due to the transfer of information between language pairs, multilingual\nNMT has seen improvements in low-resource language-pair performance compared to training solely\n9\non that language pair [12]. In addition to NMT, large multilingual pre-trained language models are\nused to fine-tune on a variety of downstream tasks with different languages [32]. Prior works on\nintermediate training take advantage of cross-task [23] and cross-lingual [22] transfer to improve\ndownstream task performance. However, in multilingual approaches there exists the problem of\ndataset imbalance, where low-resource languages tend to suffer in performance. Recently, [6] found\nthat naive temperature sampling might lead to overfitting of low-count languages, and suggested\nepoch capping with a uniform distribution for high-count languages, showing improvements over\ntemperature sampling. In multilingual NMT, to our knowledge, we are the first to show that a\nsimple pre-training stage on a high-resource language pair can improve the trade-off front of static\nsampling. Furthermore, our method is orthogonal to innovations in sampling strategies like [6], and\ncan potentially show better results in conjunction with better sampling.\nTransfer Learning in NMT\nThe benefits of transfer learning to low-resource language-pairs has\nbeen long known in the NMT literature [33, 9, 17]. [33] showed that pre-training on a high-resource\nlanguage pair can improve performance compared to training from scratch. While most prior work\non transfer learning in NMT focus on improving performance on low-resource bilingual data, recent\nwork [21] used transfer learning to improve performance on multiple language pairs. Unlike the\ntransfer learning literature in NMT [21, 15], we show that pre-training can push the low-resource\nfrontier in the multilingual setting, by testing a grid of sampling rates and hyperparameters to trace\nthe trade-off front. Prior work in the literature study the relationship between the pre-training and\nfine-tuning language pairs [10], freezing different parts of the model during fine-tuning [1], and\nexperimenting with many-stage pre-training [9]. We expect to further benefit from research done in\nthis direction.\nCurriculum Learning\nDue to the imbalanced nature of multilingual datasets, a static sampling\nstrategy is unsatisfactory. [30] used a hand-crafted temperature sampling schedule that samples\nmore high-resource earlier in the training, and gradually samples more low-resource languages. The\nperformance boost from using such a schedule, compared to a static one, supports our observations\nfrom pre-training using a high-resource language pair. On the other hand, there are many works that\nemploy a more intricate strategy for an adaptive schedule [13, 29, 18]. In comparison, our method is\nsimple with little to no overhead. We include discussion on our experience, though preliminary, with\ntrying an adaptive schedule in Appendix C. Lastly, [26] showed that the ordering of data within a\ntask affects catastrophic forgetting, which supports our observations.\n6\nLimitations and Future work\nIn our experiments, we focus on training on a single high-resource task during the pre-training phase.\nIt would be interesting future work to study pre-training with more than one language or language-pair.\nWe also only experiment with fine-tuning all parameters of the pre-trained model. Studying the effect\nof freezing different parts of the model during fine-tuning, potentially as a function of the relationship\nbetween pre-training and fine-tuning tasks, is left to future work.\n7\nConclusion\nIn this work, we demonstrated the benefits of a pre-train joint fine-tune setup for multi-objective\noptimization when there is a mixture of high and low-resource tasks. We show that in the presence\nof large data imbalance, the order at which tasks are introduced has significant impact on overall\nperformance. We demonstrate through a variety of experimental settings that this methodology\nproduces points that can go past the trade-off frontier achieved by scalarization. We show that a\nmajor weak point of scalarization in this regime is that it overfits on the low-resource task, being\nunable to early stop due to the high-resource task not converging. Our method both allows the high-\nresource task to converge during pre-training and prevents overfitting through joint fine-tuning. It also\noutperforms scalarization that under-samples the low-resource task due to higher token efficiency.\nWe also show that fine-tuning only on the low-resource task, a popular scheme in the NMT literature,\nis undesirable due to its inability to prevent forgetting. Our method is a simple natural strategy\nfor avoiding the above failure modes. Given the significant performance boost we observe in our\nexperiments, we believe that this training regime has the potential to become a standard approach,\nparticularly in the era of large language models.\n10\nAcknowledgments and Disclosure of Funding\nWe thank George E. Dahl, Wolfgang Macherey, and Macduff Hughes for their constructive comments\non the initial version of this manuscript. Additionally, we thank Sourabh Medapati, Zachary Nado,\nXavier Garcia, and Hyung Won Chung for their help in debugging our code base. Moreover, we are\ngrateful to Soham Ghosh and Mojtaba Seyedhosseini for valuable discussions regarding the role of\nMTOs in large-scale models. Lastly, we thank Chris J.H. Zhang for helpful discussions.\nReferences\n[1] Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, and Rico Sennrich. In neural machine\ntranslation, what does transfer learning transfer? Association for Computational Linguistics,\n2020.\n[2] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neu-\nral machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019,\n2019.\n[3] Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja,\nJason Riesa, and Alexis Conneau. mslam: Massively multilingual joint pre-training for speech\nand text. arXiv preprint arXiv:2202.01374, 2022.\n[4] Ankur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati, Ye Jia, Jonathan H Clark, Melvin Johnson,\nJason Riesa, Alexis Conneau, and Yu Zhang. Slam: A unified encoder for speech and language\nmodeling via speech-text joint pre-training. arXiv preprint arXiv:2110.10329, 2021.\n[5] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,\n2004.\n[6] Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang,\nand Noah Constant. Unimax: Fairer and more effective language sampling for large-scale\nmultilingual pretraining. In The Eleventh International Conference on Learning Representations,\n2022.\n[7] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\nzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116,\n2019.\n[8] Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin\nHeffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left\nbehind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.\n[9] Raj Dabre, Atsushi Fujita, and Chenhui Chu. Exploiting multilingualism through multistage fine-\ntuning for low-resource neural machine translation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pages 1410\u20131416, 2019.\n[10] Raj Dabre, Tetsuji Nakagawa, and Hideto Kazawa. An empirical study of language relatedness\nfor transfer learning in neural machine translation. In Proceedings of the 31st Pacific Asia\nconference on language, information and computation, pages 282\u2013286, 2017.\n[11] Jacob Devlin. Multilingual bert readme. https://github.com/google-research/bert/\nblob/master/multilingual.md, 2018.\n[12] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism. arXiv preprint arXiv:1601.01073, 2016.\n[13] S\u00e9bastien Jean, Orhan Firat, and Melvin Johnson. Adaptive scheduling for multi-task learning.\narXiv preprint arXiv:1909.06434, 2019.\n[14] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. Google\u2019s multilingual neural\nmachine translation system: Enabling zero-shot translation. Transactions of the Association for\nComputational Linguistics, 5:339\u2013351, 2017.\n11\n[15] Yunsu Kim, Yingbo Gao, and Hermann Ney. Effective cross-lingual transfer of neural machine\ntranslation models without shared vocabularies. arXiv preprint arXiv:1905.05475, 2019.\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[17] Tom Kocmi and Ond\u02c7rej Bojar. Trivial transfer learning for low-resource neural machine\ntranslation. arXiv preprint arXiv:1809.00357, 2018.\n[18] Julia Kreutzer, David Vilar, and Artem Sokolov. Bandits don\u2019t follow rules: Balancing multi-\nfacet machine translation with multi-armed bandits. arXiv preprint arXiv:2110.06997, 2021.\n[19] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[20] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and M Pawan Ku-\nmar. In defense of the unitary scalarization for deep multi-task learning. arXiv preprint\narXiv:2201.04122, 2022.\n[21] Surafel M Lakew, Aliia Erofeeva, Matteo Negri, Marcello Federico, and Marco Turchi. Transfer\nlearning in multilingual neural machine translation with dynamic vocabulary. arXiv preprint\narXiv:1811.01137, 2018.\n[22] Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania,\nKatharina Kann, and Samuel Bowman. English intermediate-task training improves zero-shot\ncross-lingual transfer too. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics and the 10th International Joint Conference on\nNatural Language Processing, pages 557\u2013575, 2020.\n[23] Jason Phang, Thibault F\u00e9vry, and Samuel R Bowman. Sentence encoders on stilts: Supplemen-\ntary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\n[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\n[25] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis\nBulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\nup models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\n[26] Chenze Shao and Yang Feng. Overcoming catastrophic forgetting beyond continual learning:\nBalanced training for neural machine translation. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 2023\u20132036,\n2022.\n[27] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. In International Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[29] Xinyi Wang, Yulia Tsvetkov, and Graham Neubig. Balancing training for multilingual neural\nmachine translation. arXiv preprint arXiv:2004.06748, 2020.\n[30] Yiren Wang, ChengXiang Zhai, and Hany Hassan Awadalla. Multi-task learning for multilingual\nneural machine translation. arXiv preprint arXiv:2010.02523, 2020.\n[31] Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current\nmulti-task optimization methods in deep learning even help? Advances in Neural Information\nProcessing Systems, 35:13597\u201313609, 2022.\n12\n[32] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel.\nmT5: A massively multilingual pre-trained text-to-text\ntransformer. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 483\u2013498,\nOnline, June 2021. Association for Computational Linguistics.\n[33] Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource\nneural machine translation. arXiv preprint arXiv:1604.02201, 2016.\n13\nA\nNMT Experiments: Additional Information\nA.1\nDetailed Training Setup\nThis section details the experimental setup used in Section 4.1. We use the pre-LN encoder-decoder\ntransformer architecture. The experiments presented in the main text use three layers for both the\nencoder and decoder, but we also present results with 6 layers for the encoder and decoder. We follow\nthe convention in NMT literature and train our models with 0.1 label smoothing and 0.1 dropout for\nfeed-forward and attention layers. See Table 3 for complete architecture details.\nTable 3: Transformer architecture details and common hyperparameters.\nHyperparameter\nFeed-forward dim\n2048\nModel dim\n512\nAttention heads\n8\nAttention QKV dim\n512\nLabel smoothing\n0.1\nDropout\n0.1\nWe use SentencePiece tokenization [19] to generate a vocabulary of size 64,000 for each NMT\nproblem (e.g. En\u2192{Zh, Fr}).\nAll models were trained using the Adam [16] optimizer with a batch size of 1024. For all our NMT\nexperiments, we used a linear warmup to the desired learning rate, followed by a cosine decay\nschedule that decays to 0. This is true for all legs of training for methods that use our scheme; during\nthe pre-training phase, we do a linear warmup followed by a cosine decay, and during the fine-tuning\nphase, after loading the pre-trained model, we do a linear warmup followed by cosine decay.\nFor the baseline experiments that do not do pre-training, and also for the pre-training portion, we\nwarmup for 40k steps. For fine-tuning, we tune the warmup steps from within {10k, 20k, 30k, 40k}\nfor all experiments other than for En\u2192{Zh, Fr}, where we warmup for 40k steps. The base number of\ntraining steps, and the number of fine-tuning steps are shown in Table 4. Note that for comparison\u2019s\nsake we also trained a baseline-without-pre-training model for \u2018base + fine-tune\u2019 number of steps.\nTable 4: Number of training steps for all NMT experiments.\n3-layer\n6-layer\nbase\nfine-tune\nbase\nfine-tune\nEn\u2192{Zh, Fr}\n300k\n300k\n300k\n300k\nEn\u2192{Ro, Fr}\n400k\n50k\n300k\n50k\nEn\u2192{Hi, Fr}\n300k\n50k\n275k\n50k\nFor all experiments, we sweep the base learning rate in the grid {2.5e-4, 5e-4, 2.5e-3, 5e-3, 7.5e-\n3}. We also sweep the sampling rate for En\u2192Fr and En\u2192Cs in the grid {i/10}9\ni=1, which fully\ndetermines the sampling rate for the other language pair. All plotted points correspond to the final\nmeasurement taken for each trial.\nFor all fine-tuning experiments, when loading the pre-trained model checkpoint, we reset the optimizer\nstate. We also trained all parameters of the model, and did not freeze anything.\n14\nA.2\nAdditional Performance Trade-Off Curves\nIn this section, we present the performance trade-off curves for En\u2192{Hi, Fr}, as well as for 6-layer\nmodels on En\u2192{Zh, Fr}, En\u2192{Ro, Fr}, and En\u2192{Hi, Fr}. The black-bordered points in the\ngeneralization portion of Figures 11 and 13 below correspond to the restart baseline.\n2.0\n2.2\n2.4\n2.6\n2.8\nValid Cross-Entropy Loss (En \u2192 Zh)\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\nTrain Cross-Entropy Loss (En \u2192 Zh)\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nFigure 10: Performance trade-off behavior for En\u2192{Zh, Fr} with 6-layer models. Each point\ncorresponds to the final performance of a model. Similarly to the 3-layer-model case (Figure 1),\npre-training does not yield improvements.\n1.650\n1.675\n1.700\n1.725\n1.750\n1.775\n1.800\n1.825\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.16\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nBest Scalarization En-Ro Loss\nRestart Baseline\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrain Cross-Entropy Loss (En \u2192 Ro)\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nFigure 11: Performance trade-off behavior for En\u2192{Ro, Fr} with 6-layer models. We see a similar\nbehavior as with 3-layer models. In addition, we are able to further improve the performance on both\nEn\u2192Ro due to a larger model size.\n2.90\n2.95\n3.00\n3.05\n3.10\n3.15\n3.20\nValid Cross-Entropy Loss (En \u2192 Hi)\n1.25\n1.26\n1.27\n1.28\n1.29\n1.30\n1.31\n1.32\n1.33\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\nBest Scalarization En-Hi Loss\nRestart Baseline\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain Cross-Entropy Loss (En \u2192 Hi)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\nFigure 12: Performance trade-off behavior for En\u2192{Hi, Fr} with 3-layer models. These results\nmirror those seen in Figure 2. We note that here French and Hindi are more linguistically dissimilar\nthan French and Romanian.\n15\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\n3.05\n3.10\nValid Cross-Entropy Loss (En \u2192 Hi)\n1.18\n1.20\n1.22\n1.24\n1.26\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nBest Scalarization En-Hi Loss\nRestart Baseline\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain Cross-Entropy Loss (En \u2192 Hi)\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nFigure 13: Performance trade-off behavior for En\u2192{Hi, Fr} with 6-layer models. As with the 3-layer\nmodels, We observe a similar improvement in both En\u2192Hi and En\u2192Fr performances, despite the\ndissimilarity of French and Hindi.\nA.3\nPerformance Trade-Off Curves with Sampling Rate as Markers\nIn this section, we present the same performance trade-off curves as shown previously, but with\nthe markers representing sampling rates for the lower-resource language pair. We can see that in\nall but one case (En\u2192{Hi,Fr} 6-layer model; Figure 19), the model that performs the best in the\nlow-resource language pair, samples the low-resource language pair at a higher rate than the baselines\nthat do not use pre-training. The black-bordered points in the generalization portion of Figures 16, 17\n18, and 19 below correspond to the restart baseline.\n2.2\n2.4\n2.6\n2.8\n3.0\nValid Cross-Entropy Loss (En \u2192 Zh)\n1.2\n1.3\n1.4\n1.5\n1.6\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\nTrain Cross-Entropy Loss (En \u2192 Zh)\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\n1.55\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nEn-Zh Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 14: Performance trade-off behavior for En\u2192{Zh, Fr} with 3-layer models. We can clearly\nsee that there is no optimal rate in this case, since we trace a Pareto front as we vary the En\u2192Zh\nsampling rates from 0.1 to 0.9.\n2.0\n2.2\n2.4\n2.6\n2.8\nValid Cross-Entropy Loss (En \u2192 Zh)\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\nTrain Cross-Entropy Loss (En \u2192 Zh)\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nEn-Zh Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 15: Performance trade-off behavior for En\u2192{Zh, Fr} with 6-layer models. We observe a\nsimilar behavior as in the 3-layer case.\n16\n1.75\n1.80\n1.85\n1.90\n1.95\n2.00\n2.05\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.24\n1.26\n1.28\n1.30\n1.32\n1.34\n1.36\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nBest Scalarization En-Ro Loss\nRestart Baseline\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrain Cross-Entropy Loss (En \u2192 Ro)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nEn-Ro Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 16: Performance trade-off behavior for En\u2192{Ro, Fr} with 3-layer models. Unlike the\nEn\u2192{Zh, Fr} case, we have a few sampling rates that are more optimal than the rest. Pre-training\nallows sampling En\u2192Ro at a higher rate without overfitting, than without pre-training.\n1.650\n1.675\n1.700\n1.725\n1.750\n1.775\n1.800\n1.825\nValid Cross-Entropy Loss (En \u2192 Ro)\n1.16\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nBest Scalarization En-Ro Loss\nRestart Baseline\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrain Cross-Entropy Loss (En \u2192 Ro)\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nEn-Ro Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 17: Performance trade-off behavior for En\u2192{Ro, Fr} with 6-layer models. We see a similar\nbehavior as in the 3-layer case.\n2.90\n2.95\n3.00\n3.05\n3.10\n3.15\n3.20\nValid Cross-Entropy Loss (En \u2192 Hi)\n1.25\n1.26\n1.27\n1.28\n1.29\n1.30\n1.31\n1.32\n1.33\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\nBest Scalarization En-Hi Loss\nRestart Baseline\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain Cross-Entropy Loss (En \u2192 Hi)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\nEn-Hi Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 18: Performance trade-off behavior for En\u2192{Hi, Fr} with 3-layer models. Like in the\nEn\u2192{Ro, Fr}, pre-training allows sampling En\u2192Hi at a higher rate without overfiting than without\npre-training.\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\n3.05\n3.10\nValid Cross-Entropy Loss (En \u2192 Hi)\n1.18\n1.20\n1.22\n1.24\n1.26\nValid Cross-Entropy Loss (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nBest Scalarization En-Hi Loss\nRestart Baseline\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain Cross-Entropy Loss (En \u2192 Hi)\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nTrain Cross-Entropy Loss (En \u2192 Fr)\nOptimization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nEn-Hi Rate\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 19: Performance trade-off behavior for En\u2192{Hi, Fr} with 6-layer models. In this case,\npre-training still allows sampling En\u2192Hi at a higher rate, but the rate that yielded the best En\u2192Hi\nwas surprisingly the same rate as the baseline without pre-training.\n17\nA.4\nEfficiency Plots\nIn this section, we plot the number of examples seen from one language pair against the validation\ncross-entropy loss on that language pair. The number of XX\u2192YY examples seen at train step t\nis computed by multiplying t, the batch size, and the sampling rate for XX\u2192YY. Each curve in\na given figure corresponds to the trial that achieved the best final validation performance on the\nlow(er)-resource language pair within the method given by the legend (i.e. the blue curve in Figure\n20 corresponds to the trial that achieved the best final validation En\u2192Zh cross-entropy loss among all\ntrials that did not use pre-training, and was trained for 300k steps.) For the curves corresponding to\nour proposed pre-training and fine-tuning scheme, we only show the fine-tuning portion of training.\nNote that initial linear decay followed by a smooth decay is an artifact of evaluating on a linear-scale\nwhen the plots are in log-scale.\n103\n104\n105\n106\n107\n108\n109\nNum. Examples Seen (En \u2192 Zh)\n10\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Zh)\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n102\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nFigure 20: For the 3-layer model, pre-training does not provide any significant gains in training\nefficiency for En\u2192Zh when pre-training on En\u2192Fr. Given that the blue and red curves coincide\ntowards the end of training, we can anticipate that pre-training did not impair En\u2192Zh training (by\nproviding a suboptimal initialization), and that if we were to train the red curve for 300k more steps,\nit would be able to catch up with the orange curve (best En\u2192Zh performance).\n103\n104\n105\n106\n107\n108\n109\nNum. Examples Seen (En \u2192 Zh)\n10\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Zh)\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\n102\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nFigure 21: We observe a similar behavior with 6-layer models as with 3-layer models.\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Ro)\n10\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Ro)\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\nFigure 22: On the 3-layer models, pre-training is able to accelerate training on En\u2192Ro when pre-\ntrained on En\u2192Fr. Even with less overall examples seen in En\u2192Ro, we can perform better than the\nbaselines that did not use pre-training.\n18\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Ro)\n10\n2\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Ro)\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nFigure 23: We observe a similar efficiency boost with 6-layer models as with 3-layer models.\n102\n103\n104\n105\n106\n107\nNum. Examples Seen (En \u2192 Hi)\n10\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Hi)\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\nFigure 24: On the 3-layer models, we observe a similar efficiency boost as with En\u2192{Ro,Fr}\n102\n103\n104\n105\n106\n107\nNum. Examples Seen (En \u2192 Hi)\n10\n3\n4\n6\nValid Cross-Entropy Loss (En \u2192 Hi)\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\n103\n104\n105\n106\n107\n108\nNum. Examples Seen (En \u2192 Fr)\n10\n6\n4\n3\n2\nValid Cross-Entropy Loss (En \u2192 Fr)\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nFigure 25: On the 6-layer models, we observe a similar efficiency boost as with 3-layer models.\nA.5\nBLEU Score Plots\nHere, we present the performance trade-off curves for when the metric is BLEU score instead of\ncross-entropy loss. All translations are generated via Beam-Search with beam size of 4.\n29\n30\n31\n32\n33\nValid BLEU Score (En \n Zh)\n32\n33\n34\n35\n36\n37\nValid BLEU Score (En \n Fr)\nGeneralization Behavior Overview En \n {Zh, Fr}\nEn-{Zh,Fr} (300k)\nEn-{Zh,Fr} (600k)\nEn-Fr (300k) + En-{Zh,Fr} (300k)\nFigure 26: The BLEU score plot paints a better picture for pre-training than the cross-entropy plot\n(Figure 1), since pre-training was able to improve the En-Zh BLEU score to be on par with the score\nof joint training for 600k steps. Results are with 3-layer models.\n19\n24.0\n24.5\n25.0\n25.5\n26.0\nValid BLEU Score (En \u2192 Ro)\n33.0\n33.5\n34.0\n34.5\n35.0\nValid BLEU Score (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (400k)\nEn-{Ro,Fr} (450k)\nEn-Fr (400k) + En-{Ro,Fr} (50k)\n23.5\n24.0\n24.5\n25.0\n25.5\n26.0\n26.5\n27.0\nValid BLEU Score (En \u2192 Ro)\n35.2\n35.4\n35.6\n35.8\n36.0\n36.2\nValid BLEU Score (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Ro, Fr}\nEn-{Ro,Fr} (300k)\nEn-{Ro,Fr} (350k)\nEn-Fr (300k) + En-{Ro,Fr} (50k)\nFigure 27: Our proposed pre-training scheme improves upon the best BLEU score for En\u2192Ro\nwithout pre-training for both the 3-layer models (left) and 6-layer models (right).\n10.5\n11.0\n11.5\n12.0\n12.5\n13.0\nValid BLEU Score (En \u2192 Hi)\n32.75\n33.00\n33.25\n33.50\n33.75\n34.00\n34.25\n34.50\n34.75\nValid BLEU Score (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (300k)\nEn-{Hi,Fr} (350k)\nEn-Fr (300k) + En-{Hi,Fr} (50k)\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\n12.0\n12.5\n13.0\nValid BLEU Score (En \u2192 Hi)\n34.25\n34.50\n34.75\n35.00\n35.25\n35.50\n35.75\n36.00\nValid BLEU Score (En \u2192 Fr)\nGeneralization Behavior Overview En \u2192 {Hi, Fr}\nEn-{Hi,Fr} (275k)\nEn-{Hi,Fr} (325k)\nEn-Fr (275k) + En-{Hi,Fr} (50k)\nFigure 28: Our proposed pre-training scheme improves upon the best BLEU score for En\u2192Hi without\npre-training for both the 3-layer models (left) and 6-layer models (right). The improvements are more\nsubstantial than for En\u2192{Ro, Fr}.\nB\nAdditional Training Details in Multilingual Training\nWe use an additionally processed version of the mC4 [32] dataset as proposed in [6] (documents with\nlanguage ID confidence below 0.95 were filtered).\nThe model architectures used are the same as mT5 models [32], except that relative position embed-\ndings are not shared across layers. We also use the number of real target tokens as the effective loss\nnormalization instead of using a loss normalization factor.\nWe use SentencePiece tokenization [19] to generate a vocabulary of size 64,000. The corpus used to\ngenerate the vocabulary is sampled from the training data using temperature sampling with \u03c4 = 3.33.\nWe use the T5X library [25] to train the models. For all experiments, we use the Adafactor optimizer\n[27], where we use momentum, and we do not factorize the second moment of the Adafactor states.\nThe baseline run without fine-tuning, and the pre-training phase of our proposed method, was run\nwith a constant learning rate of 0.01 in the first 10,000 steps and inverse square root decay afterwards.\nFor the fine-tuning phase of our method, we reset the optimizer state, and do a 10,000-step linear\nwarmup with inverse square root decay afterwards.\n20\nC\nDiscussion on Sampling Rate Schedules\nFrom our preliminary experiments on using schedules for the sampling rates in the NMT workloads,\nwe find that the learning rate schedule must be tuned accordingly, which affects the overall perfor-\nmance of the run. For example, we find that cosine decay schedule performs better than inverse\nsquare root decay for scalarization. However, if we use cosine learning rate decay in conjunction\nwith linear sampling rate decay (used by DDS, and defining sampling rate to be for the high-resource\nlanguage-pair), by the time the sampling rate for low-resource task is high enough, the learning\nrate has decayed rapidly (by nature of cosine decay), resulting in little learning for the low-resource\ntask. Using inverse square root learning rate decay solves this issue, but this results in overall worse\nperformance due to the suboptimal learning rate schedule. In contrast, our method is free to use any\nscheduler that maximizes performance in each leg of training (pre-training and fine-tuning). Lastly,\nwhen tuning hyperparameters, using dynamic sampling rates requires executing the full training run\nmany times. On the other hand, for our method, we can focus our resources on tuning the fine-tuning\nphase, (since the pre-training phase has only one task, and is an easier optimization problem) which\nis shorter than the total training time.\n21\n"
  },
  {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models",
    "link": "https://arxiv.org/pdf/2312.06149.pdf",
    "upvote": "2",
    "text": "Unlocking Anticipatory Text Generation: A Constrained Approach for\nLarge Language Models Decoding\nLifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou\nSalesforce AI Research\nltu@salesforce.com\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated a powerful ability for text generation.\nHowever, achieving optimal results with a\ngiven prompt or instruction can be challeng-\ning, especially for billion-sized models. Addi-\ntionally, undesired behaviors such as toxicity\nor hallucinations can manifest. While much\nlarger models (e.g., ChatGPT) may demon-\nstrate strength in mitigating these issues, there\nis still no guarantee of complete prevention. In\nthis work, we propose formalizing text genera-\ntion as a future-constrained generation problem\nto minimize undesirable behaviors and enforce\nfaithfulness to instructions. The estimation of\nfuture constraint satisfaction, accomplished us-\ning LLMs, guides the text generation process.\nOur extensive experiments demonstrate the ef-\nfectiveness of the proposed approach across\nthree distinct text generation tasks: keyword-\nconstrained generation (Lin et al., 2020), toxic-\nity reduction (Gehman et al., 2020), and factual\ncorrectness in question-answering (Gao et al.,\n2023).\n1\nIntroduction\nLarge language models (LLMs) exhibit impressive\ntextual understanding and reasoning capabilities as\nevidenced by various studies (Brown et al., 2020;\nKojima et al., 2022; OpenAI, 2022, 2023). Through\nthe process of instruction tuning, where large mod-\nels are fine-tuned on data comprising diverse tasks\nwith specific instructions, their performance can be\nnotably improved, even for unseen tasks. However,\ndespite their strong abilities in text understanding\nand generation, undesirable behaviors such as toxi-\ncity (Hartvigsen et al., 2022) and hallucination (Ji\net al., 2023) still persist. In particular, ensuring that\nthe models\u2019 outputs closely align with provided\nprompts remains a challenge. Figure 1 provides an\nillustration of how model-generated texts can devi-\nate significantly from the instructions provided in\ntheir prompts, but still remain fluent and relevant.\nTraditional sampling methods like nucleus sam-\npling (Holtzman et al., 2020), top-k sampling, and\ntemperature sampling, as well as search-based\nmethods like greedy or beam search, typically do\nnot take future costs into account. Lu et al. (2022b)\nintroduced various heuristics to approximate future\nlexical constraints. We focus on general language\nconstraint situations (Chen et al., 2022; Zhou et al.,\n2023) three different language constraints for text\ngeneration tasks and using the estimation of future\nsatisfaction score to guide generation.\nSpecifically, in order to mitigate undesirable\nbehaviors and ensure faithfulness to instructions,\nwe propose a novel approach for text generation\n(Section 2), by formalizing it as a problem con-\nstrained by future language generation. A future-\nconstrained satisfaction score is incorporated for\nguiding the next token generation. This approach\nserves to steer the generation process close to de-\nsired behaviors and follow with the specified in-\nstructions. As shown in Figure 1, the future con-\nstrain score is used to choose a better next token to\ncomplete a sentence.\nA future-constrained satisfaction score is the dis-\ntance for current generation to satisfy the constraint\ngoal. However, the estimation of this score can be\nNP-complete (Chen et al., 2018). Recent investiga-\ntions by OpenAI (2023); Liu et al. (2023b); Fu et al.\n(2023) have showcased the promising potential of\nutilizing large language models for evaluation on\nvarious natural language processing tasks. These\nLLMs evaluate candidate outputs based on their\ngeneration probabilities. Building upon this line of\nresearch, we propose a method to estimate future\nconstraint satisfaction.\nWith the future constraint satisfaction, we can\nsearch the best sequence over the infinite output\nspace. In order to speed up the process, we present\na beam-based algorithm meticulously crafted to\nrecursively generate sequences from left to right,\nremarkably enhancing the efficiency and efficacy\n1\narXiv:2312.06149v2  [cs.CL]  19 Feb 2024\nFigure 1: An illustration of the proposed approach utilizing future constraint satisfaction to guide generation. In\nthis example, although \u201csummer\u201d is a more likely next token, generating it will lead to a lower score in the future\nconstraint, which includes the keyword \u201csnow\u201d. Our method incorporates future constraint satisfaction, making\n\u201cwinter\u201d a more preferable choice.\nof the generation process. The experimental re-\nsults (Section 3) exhibit desired behaviour improve-\nments in three different tasks: keyword-constrained\ngeneration, toxicity reduction, and factual correct-\nness in question answering. We also conduct speed\nand human evaluation (Section 4) of our approach.\nThe decoding time slowdown linear with the num-\nber of candidates at each step1. It sheds light on\nthe pathway for achieving faithful decoding with\nlarge language models through our approach.\n2\nMethod\nWe start by revisiting the generic generation pro-\ncess of an autoregressive language model. Given\na prompt, represented as a sequence of tokens x,\na language model generates an output sequence y\nstep-by-step, proceeding from left to right:\nlog p(y | x) =\n|y|\n\u2211\nt=1\nlog p(yt | y<t,x)\nHere p(yt | y<t,x) represents the distribution of\nthe next token at position t given the prompt/prefix\nx, and the partial output y<t. All sequential tokens\nare generated iteratively based on this conditional\nprobability distribution.\nThere are several popular deterministic decoding\nmethods such as greedy decoding and beam search,\nas well as non-deterministic sampling methods like\ntemperature sampling, nucleus sampling (Holtz-\nman et al., 2020), and top-k sampling. In this\ncontext, our focus primarily revolves around de-\nterministic decoding techniques.\n1Future work can focus on enhancing constraint satisfac-\ntion estimation and reducing candidate numbers to boost speed\nand performance.\nIn this work, we are exploring a distinct formula-\ntion to ensure that the generated output y exhibits\nspecific desired behaviors (e.g., reduced toxicity\nor inclusion of certain keywords). The conditional\nsequence probability can be derived as follows:\nlogp(y | x) = \u2211\nt\nlog p(yt | y<t,x)\n\u221d\u2211\nt\nlog\n\u0010\np(yt | y<t)\u2217 p(x | y<=t)\n\u0011\n\u2248\u2211\nt\nlog\n\u0010\np(yt | y<t,x)\u2217 p(C(x) | y<=t)\n\u0011\n|\n{z\n}\nC(x) can be x\n=\u2211\nt\n\u0010\nlog p(yt | y<t,x)+log p(C(x) | y<=t)\n\u0011\n\u2248\u2211\nt\n\u0010\nlog p(yt | y<t,x)+\nR(y<=t,C(x))\n|\n{z\n}\nfuture constraint satisfaction\n\u0011\nwhere C(x) can be the language description (or ver-\nbalization) of the constraint. C(x) can be as simple\nas x itself, or in more sophisticated forms to repre-\nsent desired constraints such as reducing toxicity or\nensuring alignment with supported evidence. For\nexample, the task of generating a sentence with\nkeyword constraints: \u201crun team field drill\u201d, C(x)\ncan be verbalized as \u201cThis will be a sentence with\nthese concepts: run team field drill\u201d. It allows for a\nflexible specification, tailored towards specific ob-\njectives or criteria, to guide the generation process\nto meet the desired tasks or constraints.\nThe term R(y<=t,C(x)) denotes the future con-\nstraint satisfaction score, given an output prefix\ny and a constraint C(x). This score can be es-\ntimated with any pretrained language model by\nassessing the likelihood of generating the desired\n2\noutput based on the given constraint. Moreover,\nsuch constraints can be broken down into several\nsub-constraints, each playing a role in measuring\ndistinct constraints to fulfill the overall satisfac-\ntion. By aggregating individual future constraint\nsatisfaction scores, we can derive a more holistic\nunderstanding of how well the output adheres to\nthe set constraints.\n2.1\nEstimation of Future Constraint\nSatisfaction\nIn our method, we utilize future constraint satisfac-\ntion to provide guidance for text generation while\nensuring the decoding efficiency of large language\nmodels. In this subsection, we introduce how to\nestimate the future constraint satisfaction using\nLLMs.\nWe estimate the future constraint satisfaction\nscore of C(x) using the log-likelihood of generat-\ning the constraint conditioned on the prefix y<=t:\nR(y<=t,C(x)) = log p(C(x) | y<=t,<SEP>)\n|C(x)|\n(1)\nwhere <SEP> is the special token delimiting the\ntwo sequences2.\nSome recent works (Scheurer et al., 2023)\nalso proposed to estimate such scores or rewards\nin a binary question answering manner.\nSo\nR(y<=t,C(x)) = log\np(\"Yes\"|prompt)\np(\"Yes\"|prompt)+p(\"No\"|prompt),\nwhere p(\"Yes\"|prompt) and p(\"No\"|prompt) are\nthe probabilities of generating \u201cYes\u201d and \u201cNo\u201d\ngiven the prompt, respectively3.\nIn section 3, we exemplify how the proposed\nmethod can be applied to specific NLP problems.\nNote that, we use the likelihood of pretrained lan-\nguage models to estimate the satisfaction in this\nstudy. While this offers considerable versatility and\nflexibility, it might not always yield precise estima-\ntions. One can leverage fine-tuning and parameter-\nefficient techniques like LoRA (Hu et al., 2022)\nto effectively tailor the estimation process, provid-\ning more accurate and flexible assessments of con-\nstraint satisfaction. We leave this to future work.\n2.2\nInference\nExisting decoding methods such as beam search\nor nucleus sampling (Holtzman et al., 2020) de-\ntermine which token to generate following a left-\nto-right manner. Given their inherent constraints,\nthese methods may produce suboptimal outputs.\n2We set it as the end of sequence token.\n3Figure 5 shows some related results for this setting.\nThis can be alleviated by proactively accounting\nfor future costs. Specifically, we consider this fol-\nlowing decoding objective:\ny\u2190argmax\ny\u2208Y\nlog p(y | x)+\u03bb \u2217R(y,C(x))\n(2)\nwhere Y is the set of all sequences and \u03bb is a\nweight coefficient. p(y | x) denotes the conditional\nprobability distribution by a language model, and\nR(y,C(x)) is the estimation satisfaction score for\nconstraint C(x).\nThe above optimization problem is computation-\nally challenging, therefore we utilize the beam-\nbased search algorithm to solve it approximately.\nConsidering the current prefix y<t, a new token yt\nis predicted at each step, and we select the top k\nbest candidate tokens using the following criterion:\nyt \u2190argtopK\nyt\u2208Vt\nlog p(y<=t | x)+\u03bb \u2217R(y<=t,C(x))\n(3)\nwhere Vt is candidate output space at position t.\nWe define Vt as the top 2*k candidates4 in cumu-\nlative probability mass p(y<=t | x). Additional\ntokens may be added to this candidate set. For\nexample, in keyword-constrained generation tasks,\nwe introduce another token set, Vkeys, which con-\nsists of tokens found in keywords. This ensures\nthat these crucial tokens are considered at each de-\ncoding step. We iterate through this process until\ncertain conditions are met, such as encountering an\nend-of-sequence token or reaching the maximum\nallowed length, etc.\nIn the end, we choose the candidate that achieves\nthe highest score according to Equation 2 from the\ntop k candidates.\n3\nExperiments\nWe investigate the performance of the pro-\nposed method on three different tasks: keyword-\nconstrained generation, toxicity reduction, and fac-\ntual correctness in question-answering.\n3.1\nKeyword-constrained Generation\nIn our initial task, we focus on lexical-constrained\ntext generation using the CommonGen dataset (Lin\net al., 2020). This task involves generating a sen-\ntence containing specific given keywords. For in-\nstance, given a set of concepts (e.g., car, drive,\nsnow), the objective is to generate a fluent sentence\n4To encompass more candidates, we do not use nucleus\nsampling for candidate selection.\n3\nthat incorporates these concepts (e.g., \"I drive my\ncar during the winter through the snow\"). We eval-\nuate the generated outputs using automatic metrics\nof fluency (BLEU, CIDER, etc.) and a constraint\ncoverage score. The coverage score is calculated\nas the average percentage of the provided concepts\npresent in the generated outputs.\nLexical-Constraint Satisfaction Evaluation.\nIn\norder to check the estimation quality of future\nlexical-constraint satisfaction using LLMs, we cre-\nate a ranking benchmark, where each sample con-\nsists of a sentence pair (a,b), with a being the\nsentence with a constraint C and b without. Each\na is derived from the development set of Common-\nGen, while b is a complete sentence generated by\nChatGPT given a few prefix words from a. We\nhypothesize that if this completed sentence b does\nnot include all the specified concepts, it should be\ntreated as a negative sample compared to a.\nWe also investigate a distinct scenario (prefix\npairs) involving a sequence pair (\u02c6a, \u02c6b), where both\nsequences have similar lengths and are incomplete.\nThe sole distinction between them lies in the last\nword, while they share the same prefix.\n\u02c6a and\n\u02c6b have the same prefix, except for the last word.\nSpecifically, \u02c6a is the prefix of a, and \u02c6b has the\nsame prefix as \u02c6a, except for the last word. The last\nword in \u02c6b is a randomly selected word from b5.\nFor each sentence pair (a,b), we assign a rank-\ning accuracy score of 1 if R(a,C) > R(b,C). Other-\nwise, it is 0. Figure 2 shows the ranking accuracies\nof keyword-constrained satisfaction estimation us-\ning various models6. High accuracies over sentence\npairs are observed. However, accuracy significantly\ndrops for prefix pairs, suggesting that satisfaction\nestimation for prefix pairs is considerably more\nchallenging. Fortunately, many open LLMs still\nmanage to achieve over 60% accuracy. Another\nobservation is high performance with NLI-based\nmodels, despite significantly smaller model sizes.\nHyperparameter Selection.\nIn Figure 3, we dis-\nplay the constraint coverage and BLEU-4 scores on\nthe CommonGen development set with different \u03bb.\n\u03bb = 0 corresponds to a decoding method without\nconsidering future constraint satisfaction. For \u03bb in\nthe range \u03bb \u2208 {1,2,...,10}, our proposed method\n5Although \u02c6a and \u02c6b differ by only one word, it\u2019s impor-\ntant to note that their tokenized sequences may have varying\nlengths. However, the difference in length is small.\n6For more detailed information about these models, please\nrefer to the Appendix in Section .1.\nconsistently achieves higher coverage scores, in-\ndicating a higher percentage of provided concepts\npresent in the generated outputs. However, setting\na large \u03bb can excessively weight on the constraint\nsatisfaction term and hurt performance.\nResults.\nWith the select hyperparameter \u03bb on the\ndevelopment set, Table 1 presents the results for\nseveral selected LLMs. Notably, we observe high-\nquality outputs from these instruction-tuned models\n(Falcon-7B-Instruct, LLaMA-2-13B-Chat, Falcon-\n40B-Instruct). Specifically, the constraint satisfac-\ntion coverage scores are significantly higher com-\npared to baseline methods. Remarkably, the results\nfrom the 40 billion model (Falcon-40B-Instruct)\neven surpass those of Text-Davinci-003, an Ope-\nnAI model with 175 billion parameters.\nBLEU-4\nROUGE-L\nCIDER\nCoverage\nText-Davinci-003\n17.6\n44.8\n11.3\n96.1\nFalcon-7B-Instruct\nGreedy\n13.7\n42.3\n9.0\n88.7\nBeam search\n14.1\n42.5\n9.4\n87.5\nOur\n15.3\n43.8\n10.4\n93.3\nLLaMA-2-13B-Chat\nGreedy\n14.8\n43.0\n8.8\n93.6\nBeam search\n16.2\n44.1\n9.7\n93.8\nOur\n17.8\n44.9\n10.7\n95.2\nFalcon-40B-Instruct\nGreedy\n14.5\n42.8\n9.2\n88.7\nBeam search\n17.2\n45.3\n11.3\n89.4\nOur\n17.7\n45.8\n11.4\n97.6\nTable 1: Keyword-constrained generation results on\nCommonGen test set.\n3.2\nToxicity Reduction\nNext, we consider another task: toxicity reduc-\ntion (Liu et al., 2021). Given a prompt x, the task\nis to generate a fluent continuation y but not with\na toxicity attribute. The next token is generated re-\ncursively by sampling next token probability distri-\nbution provided by LLMs. Following to the setting\nin Liu et al. (2021), we use the REALTOXICI-\nTYPROMPTS benchmark (Gehman et al., 2020),\ngenerating up to 20 tokens with nucleus sampling\n(p = 0.9). Following previous work (Gehman et al.,\n2020), we characterize generation toxicity using\nthe toxicity score from Perspective API. Two tox-\nicity scores are reported: 1) maximum toxicity,\ndefined as the average maximum toxicity over 25\nsampled generations, and 2) the empirical toxicity\nprobability of at least 1 out of 25 generations being\ntoxic. We also evaluate our generations for fluency,\n4\n50\n60\n70\n80\n90\n100\nLLaMA-2-7B\nGPT-2\nGPT-2 Large\nRoBERTa-base-nli\nDeBERTa-xlarge-mnli\nAlpaca-7B-Instruct\nBART-large-mnli\nFalcon-7B\nFalcon-7B-Instruct\nLLaMA-2-13B-Chat\nLLaMA-13B\nLLaMA-7B\nMPT-7B-Instruct\nLLaMA-2-7B-Chat\nLLaMA-2-13B\n(a) Ranking accuracy on sentence pairs (a,b).\n20\n40\n60\n80\nLLaMA-2-7B\nGPT-2 Large\nGPT-2\nLLaMA-2-7B-Chat\nLLaMA-7B\nLLaMA-13B\nLLaMA-2-13B-Chat\nAlpaca-7B-Instruct\nLLaMA-2-13B\nMPT-7B-Instruct\nBART-large-mnli\nFalcon-7B\nRoBERTa-base-nli\nFalcon-7B-Instruct\nDeBERTa-xlarge-mnli\n(b) Ranking accuracy on prefix pairs ( \u02c6a, \u02c6b).\nFigure 2: Accuracy of the estimation of lexical constraint satisfaction with different models. For NLI-based model,\nnon-entailment probability are used for ranking.\nFigure 3: Performance (y-axis) of Falcon-7B-Instruct in terms of BLEU-4 score and constraint coverage with\ndifferent \u03bb (x-axis) on the CommonGen development set.\nand diversity. Diversity is another metric, which is\nthe mean number of distinct n-grams, normalized\nby the length of text.\nToxicity-Constraint\nSatisfaction\nEvaluation\nTo evaluate the quality of toxicity constraint scores\nfrom LLMs, we establish our ranking benchmark.\nConstructing sequence pairs (a,b) where a is less\ntoxic than b, we utilize a file containing numer-\nous model outputs and human-evaluated toxicity\nscores.7, provided by the work (Liu et al., 2021).\nFrom the given file, we create sequence pairs (a,b)\nby employing the same prompt prefix and pairing\nit with two distinct annotated continuations, each\nhaving its own toxicity score. The prefix pair (\u02c6a, \u02c6b)\nis formed using the common prefix and the first\nword from these two continuations.\nFor a given prompt x, the description of the\ntoxicity constraint we used is C(x) =\u201cThis will\nbe a rude, disrespectful, or unreasonable com-\nment.\u201d. We assign a ranking accuracy score of 1\n7The file can be accessed at https://github.com/\nalisawuffles/DExperts/blob/main/human_eval/\ntoxicity/human_eval_toxicity.csv.\nif R(a,C(x)) > R(b,C(x)), otherwise 0. Figure 4\nshows ranking accuracies8 of various LLMs9 on\nthe toxicity ranking benchmark. Most open LLMs\ndemonstrate an accuracy surpassing 50%, which\nrepresents the performance of random guessing.\nParticularly, the model Falcon-7B-Instruct exhibits\nsuperior performance. However, several models\nachieve an accuracy exceeding 60%, indicating po-\ntential for improvement in the future.\nResults.\nIn our proposed method, we reweight\nthe top k = 50 token logits from LLMs with our\nfuture constraint satisfaction score, then truncate\nthe logits that are in the top-k/top-p vocabulary at\neach position, effectively assigning zero probability\nto tokens outside the vocabulary. We determine the\nhyperparameter \u03bb by evaluating its performance\non a set of 50 randomly selected samples.\nTable 2 presents the toxicity reduction on two\ndifferent LLMs (Falcon-7B-Instruct and Alpaca-\n7B-Instruct), which also have a minor decrease on\n8We observe that certain pairs have nearly identical toxicity\nconstraint scores, and we did not categorize them as incorrect.\n9For more detailed information about these models, please\nrefer to the Appendix in Section .1.\n5\n50\n52\n54\n56\n58\n60\nMPT-7B-Instruct\nLLaMA-13B\nMPT-7B-Instruct\nLLaMA-2-13B\nLLaMA-7B\nGPT-2\nAlpaca-7B-Instruct\nFalcon-7B\nVicuna-7B-V1.3\nGPT-2 Large\nLLaMA-2-13B-Chat\nFalcon-7B-Instruct\n(a) Ranking accuracy on sentence pairs (a,b).\n50\n55\n60\n65\nLLaMA-13B\nMPT-7B-Instruct\nGPT-2\nLLaMA-2-13B-Chat\nGPT-2 Large\nLLaMA-2-13B\nVicuna-7B-V1.3\nLLaMA-7B\nMPT-7B-Instruct\nAlpaca-7B-Instruct\nFalcon-7B\nFalcon-7B-Instruct\n(b) Ranking accuracy on prefix pairs ( \u02c6a, \u02c6b).\nFigure 4: Accuracy of the estimation of constraint satisfaction with different pretrained LLMs.\ndiversity. We do not include LLaMA-2-13B-Chat\nbecause we notice that it is a low toxicity mode as\nshown in Touvron (2023)10.\nToxicity (\u2193)\nDiversity (\u2191)\nAvg. Max\nProb.\nDist-1\nDist-2\nDist-3\nFalcon-7B-Instruct\nBaseline\n0.371\n0.215\n0.549\n0.839\n0.843\nOur\n0.287\n0.125\n0.583\n0.782\n0.762\nAlpaca-7B-Instruct\nBaseline\n0.272\n0.140\n0.471\n0.714\n0.745\nOur\n0.235\n0.108\n0.471\n0.584\n0.574\nTable 2: Toxicity reduction results on 1k prompts.\n3.3\nFactual Question Answering\nHallucination is a notable issue associated with\nlarge language models, despite their ability to gen-\nerate coherent and fluent output. Providing accu-\nrate answers supported by concrete evidence is cru-\ncial, and mitigating hallucination is key to achiev-\ning this goal. We use the dateset ALCE (Gao et al.,\n2023) as factual question answering This bench-\nmark provides a set of retrieved passages, denoted\nas D = {D1,D2,...}, for each question q. Addi-\ntionally, the dataset offers correctness evaluation\nthrough multiple short answers in ASQA (Stel-\nmakh et al., 2022) and three \u201csub-claims\u201d for\nELI5 (Fan et al., 2019).\nIn ASQA, correctness is determined by calcu-\nlating the recall of correct short answers. This is\nachieved by verifying whether the short answers\nprovided by the dataset are exact substrings of the\ngenerated response. On the other hand, for the\nlong-form QA task ELI5, correctness is measured\n10We also conducted tests and discovered that the average\nmaximum toxicity score is approximately 0.135, while the\naverage toxicity probability is close to 0.01.\nby the ratio of model outputs that entail the three\nprovided \"sub-claims\".\nWe evaluate 2-shot on the above dataset, and\nthree retrieved documents are used each question.\nIn the future satisfaction score term R(y<=i,C(x)),\nC(x) can be the retrieved document or sub-claims.\nWe determine the hyperparameter \u03bb by evaluating\nits performance on a set of a few samples.\nBaselines.\nWe compare our proposed method\nwith two different deterministic search-based meth-\nods: greedy decoding and beam search with beam\nsize = 5.\nWhile nucleus sampling is a widely\nadopted technique for open-ended text generation,\nit operates as a sampling method. However, in our\ninitial experiments, we did not observe a signifi-\ncant improvement in performance compared to the\ndeterministic approach of greedy decoding.\nFactual-Correctness-Constraint\nSatisfaction\nEvaluation.\nWe constructed our factual correct-\nness ranking benchmark using the fact verification\npart of TRUE (Honovich et al., 2022). Specifically,\nwe focused on FEVER (Thorne et al., 2018)\nand VitaminC (Schuster et al., 2021) within the\nTRUE dataset. In the training set of FEVER and\nVitaminC, for each evidence (as C), we choose\none claim denoted as a that was supported by the\nevidence, and another claim that was not supported\nby the evidence, denoted as b. This formed pairs\nof sentences: (a,b).\nFor each evidence, if the factual constraint es-\ntimation score is higher for the supported claim\ncompared to the unsupported claim with respect\nto the evidence, we assign an accuracy score of 1.\nOtherwise, if R(a,evidence) \u2264 R(b,evidence), the\naccuracy score is 0. Table 4 displays the accura-\ncies on our constructed factual correctness ranking\n6\nbenchmark. We can see that several open LLMs11\nachieve more than 60% accuracy12.\nResults.\nWe consider samples for which the re-\ntrieved documents support the answers13. This\nselective approach helps mitigate the noise effect\nin the data, ensuring a more accurate assessment\nof the correctness. Table 3 shows the results on\nquestion answer tasks. In general, we observe that\nbeam search tends to perform comparably to greedy\ndecoding on factual correctness. Our proposed\nmethod demonstrates a significant enhancement in\nfactual correctness compared to the baselines for\nboth tasks. .\nResults Using Claims as Constraints.\nIn Ta-\nble 3, we present the results for the case where the\nconstraint C(x) corresponds to the retrieved doc-\numents. Furthermore, Table 5 displays the results\nwhen the constraint is \"sub-claims.\" Our proposed\nmethod exhibits improvements in both scenarios,\nparticularly for Vicuna-13B-v1.3.\nResults on the Entire ELI5 Dataset.\nTable 9 in\nthe Appendix displays results for the full ELI5\ndataset.\nIt is evident that the absence of high-\nquality supported documents leads to a substantial\ndecrease in the average performance of all models.\nThis underscores the critical role of accurate and\ncredible supporting documents in achieving good\nperformance in question-answering tasks.\n4\nAnalysis\nSpeed\nWe test the wall-clock running time of\ngreedy decoding, our method, and the standard\nbeam search. We follow the same configuration.\nThe result is shown in Table 6. Our method is\nnearly k times linear slowdown due to all the over-\nhead of computing 2*k candidates in Equation 3.\nIt is worth that decoding time is increased in\norder to do a expect faithful generation. And there\nare several ways to decrease the time and keep\ngeneration quality: choose small k, choose smaller\nsize but tuned LLMs that can compute the future\nconstraint satisfaction score R(y<=t,C(x)) etc.\nHuman Evaluation\nTo verify the effects of dif-\nferent decoding methods, we conducted human\n11For more detailed information about these models, please\nrefer to the Appendix in Section .1.\n12We noticed an usual trend in the performance of the llama-\n1 family model. Interestingly, we found that their performance\non the Fever ranking part worsened with an increase in model\nsize.\n13More evaluation results are in Table 9 of the Appendix\nevaluation on the challenging long-form QA task\nELI5 (which usually requires long answers and\nmultiple passages as evidence).\nWe randomly\nchose 30 questions and requested workers from\nAmazon Mechanical Turk (AMT) to judge model\nresponses on three dimensions14: 1. Fluency: a\n1-to-5 score indicating whether the generation is\nfluent and cohesive; 2. Informative: a 1-to-5 score\nindicating whether the generation helps answer the\nquestion; 3. Correctness: a 0-to-3 score indicating\nthe number of claims is fully supported by the re-\nsponse. Later, this score is normalized as a ratio of\ncorrectness. Figure 6 shows one example of human\nevaluation. Table 7 confirms the strength of our\nproposed decoding method, which received better\nscores in all dimensions, especially on correctness.\n5\nRelated Work\nPreviously,\nthere\nare\nseveral\nwork\nlike\nCTRL (Keskar et al., 2019), PPLM (Dathathri et al.,\n2020), Gedi (Krause et al., 2021), FUDGE (Yang\nand Klein, 2021) on controllable generation. They\nuse additional code or attributes for controllable\ngeneration. One tuned classifier or auxiliary model\nis used to modify the output distribution.\nThe\ntype of control is limit (a label or a category\nof the sequence). In this work, the constraints\nare verbalized in natural language. Any natural\nlanguage constraint can be suitable for our\nmethod.\nThe knowledge or understanding of\npowerful LLMs is used to guide the constrained\ntext generation.\nAnother related approach in\nconstrained generation involves refinement with\nLLMs after each completion (Welleck et al.,\n2023; Madaan et al., 2023). This refinement or\ncorrection model iteratively editing the generated\ntext.\nMultiple generations are often required,\nparticularly for long-form question-answering\ntasks, such as ELI5 (Fan et al., 2019). Another\ndirection in constrained decoding (Ziegler et al.,\n2020; Lu et al., 2022a) is related to reinforcement\nlearning (RL). The generator model parameters\nneed to be updated in this approach. Extra training\nis conducted involving both the generator and a\nreward model.\nOur work is inspired by A* algoirhtm (Hart\net al., 1968), a search algorithm that seeks the\nhighest-scoring path by utilizing heuristic estima-\ntions of future scores toward the goal. Recently, Lu\n14Inspired by previous human evaluation work (Liu et al.,\n2023a; Gao et al., 2023)\n7\nASQA\nELI5\nCorrect.\nCorrect.\nText-Davinci-003\nGreedy\n60.1\n56.1\nChatGPT\nGreedy\n70.3\n64.9\nFalcon-7B-Instruct\nGreedy\n22.7\n29.8\nBeam search\n23.7\n30.4\nOur\n24.4\n32.7\nVicuna-13B-v1.3\nGreedy\n13.5\n21.1\nBeam search\n11.9\n22.2\nOur\n14.5\n26.3\nLLaMA-2-13B-Chat\nGreedy\n20.9\n47.9\nBeam search\n23.1\n49.2\nOur\n24.6\n50.3\nTable 3: Question answering results on ASQA and\nELI5.\n50\n55\n60\n65\n70\nLLaMA-30B\nLLaMA-13B\nVicuna-13B-v1.3\nVicuna-7B-v1.3\nLLaMA-7B\nLlama-2-13b-Chat\nAlpaca-7B-Instruct\nFalcon-7B\nLlama-2-13B\nFalcon-7B-Instruct\nTable 4: Factual correctness ranking accuracy of different\nLLMs.\nCorrect.\nROUGE-L\nVicuna-13B-v1.3\nDocuments\n26.3\n17.7\nClaims\n41.5\n21.4\nLLaMA-2-13B-Chat\nDocuments\n50.3\n23.8\nClaims\n48.5\n21.8\nTable 5: The impact of different constraints is explored,\nwhere one setup involves retrieving documents and the\nother involves sub-claims of gold answers.\nCommonGen\nELI5\nGreedy\n1.0s\n10.2s\nBeam search\n1.5s\n22.1s\nOur\n4.8s\n63.2s\nTable 6: Speed comparison: the decoding time used\nfor each example in two tasks, CommonGen and ELI5.\nRefer to the experimental setup in Section 4.\nF(\u2191)\nI(\u2191)\nC(\u2191)\nGreedy\n3.6\n3.8\n63.7\nBeam Search\n3.8\n4.0\n67.0\nOur\n4.0\n4.1\n70.0\nTable 7: Human Evaluation Criteria: F (Fluency), I\n(Informativeness), C (Correctness).\net al. (2022b); Madaan et al. (2023) develop several\nheuristics to estimate look-ahead scores. In con-\ntrast to our work, they estimate lexical constraint\nscores using fixed-size look-ahead steps in lexical\nconstrained tasks. In the work of FUDGE (Yang\nand Klein, 2021), an auxiliary binary classifier is\ntrained with random input sequence truncation. Re-\ncently, Choi et al. (2023) learned a token-level dis-\ncriminator for knowledge-grounded dialogue and\nabstractive summarization. In our work, a future\nconstraint satisfaction score is estimated with ver-\nbalized constraints and LLMs.\n6\nFuture Work and Conclusion\nIn this work, we delved into decoding methods for\nLLMs to mitigate undesired behaviors through a\nconstrained approach. Unlike previous techniques\nsuch as greedy decoding, nucleus sampling, or\nbeam search, which focus on the past generation,\nwe advocate for considering future constraint sat-\nisfaction during text generation. We propose a\nformalized approach to text generation that inte-\ngrates future constraint satisfaction, enabling better\ncontrol over the output.\nTo quantify the future constraint satisfaction, we\nintroduce a scoring mechanism evaluated by LLMs.\nBy benchmarking LLMs using these constraint sig-\nnals, we observed a distinct and discernible trend\nassociated with this scoring signal. Exploring vari-\nous signals and enhancing their effectiveness, such\nas refining constraint score evaluation through tun-\ning, is a promising avenue for future research. Im-\nprovements in signal quality and understanding\nhow these signals impact the generation process\ncan lead to more robust and controlled text genera-\ntion systems.\n8\n7\nLimitations\nEstimation of Future Constraint Estimation.\nIt\nis challenging to estimate the future constraint sat-\nisfactions. In this work, we utilize Large Language\nModels (LLMs) for this estimation. Because LLMs\ninherently encapsulate extensive world knowledge,\ntheir incorporation can leverage this wealth of in-\nformation. Moreover, the ongoing augmentation of\nworld knowledge within LLMs suggests a growing\npotential for refining the estimation. This refine-\nment can be achieved through further tuning with\nhuman preference data.\nIncorporating more symbolic components into\nthe estimation could be beneficial. This approach\nwould allow for the inclusion of detailed reasoning\npaths as integral elements of the estimation. It can\nbe with more interpretation and reliability. This\npart can be a promising direction for future work.\nLimitation of Correctness Evaluation.\nThis\nwork primarily prioritizes the correctness of con-\nstraint satisfaction. However, in question answer-\ning, the generated output of a question may include\ncorrect claims alongside hallucinated information.\nEach piece of information in a generation is not\nguaranteed to be factually supported by a reliable\nsource of knowledge. Future work can explore\nmethods to enable LLMs to generate not only cor-\nrect answers but also minimize the inclusion of\nhallucinated information.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nHoward Chen, Huihan Li, Danqi Chen, and Karthik\nNarasimhan. 2022. Controllable text generation with\nlanguage constraints. ArXiv, abs/2212.10466.\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan\nMay, and Kevin Knight. 2018. Recurrent neural net-\nworks as weighted language recognizers. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 2261\u20132271, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nSehyun Choi, Tianqing Fang, Zhaowei Wang, and\nYangqiu Song. 2023. Kcts: Knowledge-constrained\ntree search decoding with token-level hallucination\ndetection.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558\u20133567, Florence,\nItaly. Association for Computational Linguistics.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356\u20133369, Online. Association for Computational\nLinguistics.\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael.\n1968. A formal basis for the heuristic determina-\ntion of minimum cost paths. IEEE Transactions on\nSystems Science and Cybernetics, 4(2):100\u2013107.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309\u20133326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\n9\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3905\u20133920, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1\u201338.\nNitish Shirish Keskar, Bryan McCann, Lav Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCTRL - A Conditional Transformer Language\nModel for Controllable Generation. arXiv preprint\narXiv:1909.05858.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances in\nNeural Information Processing Systems, volume 35,\npages 22199\u201322213.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. GeDi: Gener-\native discriminator guided sequence generation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4929\u20134952, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871\u20137880, Online. Association for Computa-\ntional Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. Commongen: A constrained text genera-\ntion challenge for generative commonsense reason-\ning. Findings of EMNLP.\nAlisa Liu,\nMaarten Sap,\nXiming Lu,\nSwabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6691\u20136706, Online. Association for Computational\nLinguistics.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\nArXiv:2304.09848.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang,\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\nand Yejin Choi. 2022a. QUARK: Controllable text\ngeneration with reinforced unlearning. In Advances\nin Neural Information Processing Systems.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang,\nJungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-\nhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith,\nand Yejin Choi. 2022b.\nNeuroLogic a*esque de-\ncoding: Constrained text generation with lookahead\nheuristics. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 780\u2013799, Seattle, United States.\nAssociation for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730\u201327744.\nCurran Associates, Inc.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\n10\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak,\nJun Shern Chan, Angelica Chen, Kyunghyun Cho,\nand Ethan Perez. 2023. Training language models\nwith language feedback at scale.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624\u2013643, Online. As-\nsociation for Computational Linguistics.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8273\u20138288, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, commercially usable\nllms. Accessed: 2023-05-05.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2018.\nThe fact extraction and VERification (FEVER)\nshared task. In Proceedings of the First Workshop on\nFact Extraction and VERification (FEVER), pages 1\u2013\n9, Brussels, Belgium. Association for Computational\nLinguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nCite\narxiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nHugo .etc Touvron. 2023. Llama 2: Open Foundation\nand Fine-Tuned Chat Models. arXiv e-prints, page\narXiv:2307.09288.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2023. Generating sequences by learning to\nself-correct. In The Eleventh International Confer-\nence on Learning Representations.\nKevin Yang and Dan Klein. 2021. Fudge: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies. Associ-\nation for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ethan\nWilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023.\nControlled text generation with natural language in-\nstructions. In Proceedings of the 40th International\nConference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pages\n42602\u201342613. PMLR.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences.\n.1\nLLMs\nFollowing are the models that are used in our ex-\nperiments.\n\u2022 Ouyang et al. (2022): Text-Davinci-003\n\u2022 Team (2023): MPT-7B, MPT-7B-Instruct\n\u2022 Taori et al. (2023) :Alpaca-7B-Instruct\n\u2022 Radford et al. (2019): GPT-2, GPT-2 Large\n\u2022 Touvron et al. (2023a): LLaMA-7,13,30B\n\u2022\nTouvron et al. (2023b):\nLLaMA-2-7B,\nLLaMA-2-7B-Chat, LLaMA-2-13B, LLaMA-\n2-13B-Chat\n\u2022 Zheng et al. (2023): Vicuna-7B-V1.3, Vicuna-\n13B-V1.3\n11\n\u2022 Reimers and Gurevych (2019): RoBERTa-\nbase-nli\n\u2022 Lewis et al. (2020): BART-large-mnli\n\u2022 He et al. (2021): DeBERTa-xlarge-mnli\n.2\nHyper-parameter\nIn our beam-based search algorithm, we employ\na beam size denoted by k.\nFor the keyword-\nconstrained generation task, we strive to use a\nlarger beam size, specifically setting k = 20. How-\never, due to memory limitations, for the Falcon-\n40B-Instruct model, we reduce the beam size to\n5. 8 A100 40G GPUs are used for Falcon-40B-\nInstruct model.\nFor toxicity reduction task, k = 50 is used to\nreweight the top 50 tokens.\nIn the question answering task, we utilized 4\nA100 GPUs. The beam size was set to k = 5 due to\nthe demands of generating long context sequences.\n.3\nRanking Datasets for Constraint\nSatisfaction Evaluation\nFollowing are the used datasets and their licences.\n\u2022 CommonGen dataset (Lin et al., 2020): MIT\nLicense\n\u2022 REALTOXICITYPROMPTS (Gehman et al.,\n2020): the licensing status is unclear; however,\nthe data has been made publicly available by\nthe authors.\n\u2022 TRUE benchmark (Honovich et al., 2022):\nApache-2.0 license\n\u2022 ALCE (Gao et al., 2023): MIT License\n#examples\nLexical-Constraint\n993\nToxicity-Constraint\n2720\nFactual-Correctness-Constraint\n2000\nTable 8: Statistics from three ranking benchmarks are\nutilized to estimate constraint satisfaction of LLMs. The\nfactual-correctness-constraint benchmark consists of\n1000 examples sourced from FEVER and VitaminC\ndatasets, respectively.\n.4\nMore Results on Constraint Scoring\nFunction\nFactual Correctness with a binary Yes/NO ques-\ntion\nGiven claim a and the evidence g, we use\nthe following template:\n0\n25\n50\n75\n100\nLLaMA-30B\nLLaMA-7B\nLLaMA-13B\nAlpaca-7B-Instruct\nLLaMA-2-13B\nFalcon-7B\nFalcon-7B-Instruct\nVicuna-7B-v1.3\nVicuna-13B-v1.3\nLLaMA-2-13B-Chat\nFigure 5: Factual correctness accuracy with a binary\nquestion.\nClaim:{a}\nDocument:{g}\nQuestion:\nIs\nthe\nabove\nclaim\nsupported by the above document?\nAnswer with Yes or No.\nAnswer:\nThe next token probabilities of \u201cYes\u201d and \u201cNo\u201d\nof the above prompt are used to estimate the future\nconstraint satisfaction score.\nFigure 5 shows ranking performance with the\nabove binary Yes/No question.\nCorrect.\nROUGE-L\nText-Davinci-003\nGreedy\n21.8\n22.3\nChatGPT\nGreedy\n21.1\n21.2\nVicuna-13B-v1.3\nBeam search\n10.0\n16.2\nOur\n16.2\n20.2\nLLaMA-2-13B-Chat\nBeam search\n17.9\n20.5\nOur\n19.4\n21.4\nTable 9: QA results on full ELI5 test set of ALCE.\n.5\nHuman Evaluation Details\nFigure 6 presents one example in human evaluation\nexperiment.\n.6\nQUALITATIVE EXAMPLES\n12\nConcepts: mower push lawn mow\nGreedy\nThe homeowner pushed the lawnmower across\nthe grass, cutting the blades of grass with each\npass as she mowed the law.\nBeam\nSearch\nTo keep the lawn looking its best, I push the\nmower around the yard to cut the grass.\nOur\nShe pushed the mower across the lawn to mow\nthe grass.\nConcepts: word spray paint wall\nGreedy\nThe vandal used a can of spray pain to deface\nthe wall with his graffiti.\nBeam\nSearch\nThe graffiti artist sprayed a colorful word on\nthe wall.\nOur\nShe used a word spray to paint the wall.\nConcepts: sharpener sharpen drill pencil\nGreedy\nThe pencildrill\u2019s sharpener sharsen the pencils.\nQuestion: Write a sentencwith these words\n:sandpaper sand smooth. Answer:The carpen-\nter used sandpaper to smooth\nBeam\nSearch\nThe sharpener helps to sharper the drill, which\nis used to pencils.\nOur\nThe sharpener sharpens the drill, which is used\nto pencils.\nConcepts: pad hold air head kick\nGreedy\nThe football player kicked the ball with all his\nmight, but it only went a few feet before hitting\nthe pad on the ground and bouncing back up to\nhit him in the head.\nBeam\nSearch\nThe football player kicked the ball with his\nhead while holding it in his pad.\nOur\nThe football player kicked the ball with his\nhead, holding the pad in his air.\nConcepts: piece throw air pumpkin pick\nGreedy\nShe carefully picked up the pump kin and threw\nit into the air, watching as it soared through the\ncrisp autumn air.\nBeam\nSearch\nHere\u2019s a sentence using all of the given words:\nShe picked up a piece of pumkin and threw it\ninto the air.\nOur\nShe picked up a piece of air and threw it pump-\nkins\nConcepts: hit machine come ball bat\nGreedy\nThe batter swung the bat and hit the ball with a\nloud crack, sending it flying out of the park on\na high arc.\nBeam\nSearch\nThe batter swung the bat and hit the ball into\nthe machine.\nOur\nThe batter hit the ball with his bat, and the ball\ncame out of the machine.\nConcepts: brush apply lipstick lip\nGreedy\nShe carefully brushed on a layer of lipstik to\nher lips before leaving the house.\nBeam\nSearch\nShe carefully brushed on a layer of lipstik, mak-\ning sure to apply it evenly across her lips.\nOur\nTo brush apply the lipsticks lip, you need to use\na lip brush.\nConcepts: scroll sunglass tattoo hair hold\nGreedy\nShe held her long hair back with a scrunchie,\nsung her heart out at the karaoke bar, and\nshowed off her new tattoos under the bright\nstage lights.\nBeam\nSearch\nShe held her long hair back with a hair tie,\nwhile wearing a pair of sung glasses and show-\ning off her tattoos as she scrolled through her\nphone.\nOur\nShe held her hair back with one hand and scroll-\nsunglass-tattooed her arm with the other.\nConcepts: snow watch jump crowd rider\nGreedy\nThe snowboarder soared through the air, jump-\ning over the crowd and landing with a smooth\nride.\nBeam\nSearch\nThe snowboarder watched the crowd below as\nhe jumped off the ridge, feeling the thrill of the\nride.\nOur\nThe snow rider watched the crowd jump as he\nrode through the snow.\nTable 10: Generated examples from CommonGen given different concepts with LLaMA-2-13B-Chat. We show\noutputs from greedy decoding, beam search and our method.\n13\nInstruction: Write a high-quality answer for the given question using only the provided search\nresults.\nQuestion: what is the difference between an IRA / ROTH IRA / 401k?\nDocument [1](Title: Money Talk :: - Roth 401k??): Roth 401k?? ekelswa06 Can someone please\nexplain to me the difference between a Roth 401k and a Roth IRA? Thank you. Re: Roth 401k??\nOriginally posted by ekelswa06 I\u2019ll try.... Both the ROTH 401k and ROTH IRA are funded by your\nafter tax contributions. You can contribute up to $17,500 in 2013 to the 401k. You can contribute up\nto $5,500 in 2013 to the IRA. Both the 401k and the IRA grow tax deferred. You can remove your\nannual contributions from the IRA at any time for any reason free from tax and penalty. You can not\ntake your\nDocument [2](Title: 401k Plan Archives - Debt Free To Early Retirement): IRA. As a side note, if\nyou don\u2019t have a 401K, then consider reviewing the Difference Between Roth IRA and Traditional\nIRA. Difference Between 401K and Roth IRA Remember that the Roth IRA and 401K are just\naccounts where you keep your investments. They aren\u2019t actual investments. They are just the account\n(or vehicle, as some put it) where the money is held. These accounts are great because they get special\ntax treatment. You are able to contribute pre-tax dollars to a 401K. This means that no tax is taken\nfrom your money that is placed into the 401K. If\nDocument [3](Title: Roth 401k vs. Roth IRA - How Are They Different? | Personal Capital): IRA\nPenalty-Free? The biggest differences between a Roth 401k and a Roth IRA are their different annual\ncontribution limits, eligibility criteria, and whether or not you will need to take required minimum\ndistributions (RMDs). Let\u2019s start with the annual contribution limits. In 2019, you can contribute\nup to $19,000 per year \u2014 or $25,000 per year if you\u2019re 50 years of age or over \u2014 to a Roth 401k.\nHowever, the annual contribution limit for Roth IRAs is much lower: just $6,000 per year, or $7,000\nif you\u2019re 50 years of age or over. Another big difference between the Roth\nGreedy Decoding: The main differences between an Individual Retirement Account (IRA), a Roth\nIndividualRetirement Account(Roth IRA), and a 301(k) are the contribution limits and the tax\nimplications. A 410(k), like a RothIRA, is funded with after-tax contributions, meaning that the\nmoney grows tax-free. A traditional IRA, on the other hand, isfunded with pre- tax contributions,\nwhich means that the contributions are tax-deductible, but the money growstaxed when withdrawn. A\nRoth I RA has lower contribution limits than a 010k, and there are penalties for early withdrawals.\nA40l(k )is typically offered by an employer and has higher contribution limits tha a Roth lRA.\nBeam Search: An IRA (Individual Retirement Account) is a type of savings account that provides tax\nbenefits for retirement savings. A Roth ISA is a specific type of IRA that allows you to contribute\nafter-tax income and then withdraw the contributions (not the earnings) tax-free. A 410(k) is an\nemployer-sponsored retirement plan that allows employees to contribute a portion of their income to a\ntax-deferred retirement account. The main differences between the three are the contribution limits\nand the required minimum distribution rules. The contribution limits for a Roth Ira are lower than\nthose for a 301(k), and the Roth Ira does not require minimum distributions during the contributor\u2019s\nlifetime. The 310 (k) requires minimum distributions to begin at age 70 1/2.\nOur: An IRA (Individual Retirement Account) is a type of savings account that provides tax benefits\nfor retirement savings. A Roth I RA is a specific type of IRA that allows after-tax contributions, and\nthe money grows tax-free. A 4 01(k) is an employer-sponsored retirement plan that allows pre- tax\ncontributions and the investments grow tax-deferred. The key differences between the three are:\n* Contribution limits: The annual contribution lim it for a Roth IR A is lower than that of a 40 1 (k) .\n* Eligibility: Not all employers offer 411) plans, but most employers do offer IRA plans.\n* Required Minimum Distributions (R MDs): 4 O 1( k) plans require RMDs after age 70\nand 1/2, while IRAs do not have RMD requirements.\nTable 11: Generated outputs of three different decoding methods on one ELI5 example.\n14\nFigure 6: One example in our human evaluation experiment.\n15\nInstruction: Write a high-quality answer for the given question using only the provided search\nresults.\nQuestion: why does mining crypto use so much electricity compared to normal PC use.\nDocument [1](Title: How Much Electricity Does Your PC Consume? | PCMag.com): use more\nelectricity under load than a Chromebox with a low-power CPU. Factor 3: How You Use Your PC Just\nbecause your PC is a beast with a 750-watt power supply doesn\u2019t mean it\u2019s going to use 750 watts all\nthe time. Most PCs come with power-saving features that lower your energy usage when the computer\nis idle, or doing basic tasks like browsing the web. So someone mining Bitcoin or folding@home is\ngoing to use more power than someone typing up Word documents, even if they did so on the exact\nsame PC for the same number of hours\nDocument [2](Title: Why I built a cryptocurrency mining factory in my bedroom | CCG): I found\nsome free software online for mining Zcash and was ready to go. How the numbers stacked up The\nbiggest cost for a crypto miner is electricity. You need to leave your computer running non-stop if you\nwant to make maximum use of it, but this involves not only the cost of the mining itself but also the\ncost of keeping the computer cool. Fortunately, at that time I was living in Trinidad, which according\nto my research had the second-cheapest electricity in the world at just five US cents (3.7p) per kWh,\ncompared with a typical cost of\nDocument [3](Title: Agorastoken Mining With Pc \u2013 Say it with Crypto-Currency \u2013 Bitcoins Alot):\nAgorastoken Mining With Pc \u2013 Crypto-Currency \u2013 Building Wealth at Each Level Thank you for\ncoming to us in search for \u201cAgorastoken Mining With Pc\u201d online. The beauty of the cryptocurrencies\nis that scam was proved an impossibility: because of the character of the method in which it is\ntransacted. All exchanges on a crypto-currency blockchain are irreversible. After you\u2019re paid, you\nget paid. This is simply not anything short-term where your visitors could challenge or demand a\ndiscounts, or use dishonest sleight of palm. Used, most dealers could be smart to utilize a transaction\nprocessor, due to the irreversible\nAnswer:\nTable 12: The format for ELI5. In the context learning experiments for ELI5, each example follows a specific\nformat. There are 2 examples in total, and for each one, it includes a question, a document, and an answer.\n16\n"
  },
  {
    "title": "TCNCA: Temporal Convolution Network with Chunked Attention for Scalable Sequence Processing",
    "link": "https://arxiv.org/pdf/2312.05605.pdf",
    "upvote": "1",
    "text": "TCNCA: Temporal Convolution Network with\nChunked Attention for Scalable Sequence Processing\nAleksandar Terzi\u00b4c1,2\u2217, Michael Hersche1,2, Geethan Karunaratne1\nLuca Benini2, Abu Sebastian1, Abbas Rahimi1\u2020\n1IBM Research \u2013 Zurich, 2ETH Zurich\nAbstract\nMEGA is a recent transformer-based architecture, which utilizes a linear recurrent\noperator whose parallel computation, based on the FFT, scales as O(LlogL), with\nL being the sequence length. We build upon their approach by replacing the linear\nrecurrence with a special temporal convolutional network which permits larger\nreceptive field size with shallower networks, and reduces the computational com-\nplexity to O(L). The resulting model is called TCNCA, a Temporal Convolutional\nNetwork with Chunked Attention. We evaluate TCNCA on EnWik8 language\nmodeling, long-range-arena (LRA) sequence classification, as well as a synthetic\nreasoning benchmark associative recall. On EnWik8, TCNCA outperforms MEGA,\nreaching a lower loss with 1.37\u00d7/1.24\u00d7 faster forward/backward pass during train-\ning. The dilated convolutions used in TCNCA are consistently and significantly\nfaster operations than the FFT-based parallelized recurrence in GPUs, making\nthem a scalable candidate for handling very large sequence lengths: they are up\nto 7.07\u00d7/2.86\u00d7 faster in the forward/backward pass for sequences up to 131 k.\nFurther on LRA, TCNCA achieves, on average, 1.28\u00d7 speed-up during inference\nwith similar accuracy to what MEGA achieves. On associative recall, we find that\neven a simplified version of TCNCA, without excessive multiplicative and additive\ninteractions, remains superior or competitive to MEGA on a range of sequence\nlengths and vocabulary sizes.\n1\nIntroduction\nThe Transformer [1] is a powerful class of neural networks which has found success in a variety of\ntasks including image processing [2], physical system modeling [3], drug discovery [4], but perhaps\nmost notably, language modeling [5], [6], [7]. While undeniably a strong candidate for a universally\napplicable neural network, the operator at its backbone, attention, faces some crucial limitations. We\nconsider two limitations, including the O(L2) computational and memory complexity [8] of attention,\nas well as its poor performance in long sequence classification, namely on the long-range-arena\n(LRA) dataset [9], where it is drastically outperformed by linear recurrent models [10\u201312]; however,\nthese models lag behind the transformer on language modeling [13]. A more extensive review of\nrelated works can be found in Appendix A.\nA recent neural network, MEGA [14], combines the strengths of linear recurrences and attention\nin a manner which scales sub-quadratically. Concretely, MEGA combines the damped exponential\nmoving average (EMA) known from time-series analysis [15], with chunked attention which operates\non fixed-size non-overlapping blocks in the input sequence. It achieves scores competitive with the\nstate-of-the-art in a range of disparate tasks including language modeling on the EnWik8 dataset [16]\nand LRA sequence classification [9].\n\u2217Research conducted at IBM Research \u2013 Zurich.\n\u2020Corresponding author: abr@zurich.ibm.com\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.05605v1  [cs.LG]  9 Dec 2023\nWe focus on EMA, which maps xt \u2208 Rh to yt \u2208 Rh using the parameters \u03b1, \u03b4 \u2208 [0, 1]h, h \u2208 N+ as:\nyt = \u03b1 \u2299 xt + (1 \u2212 \u03b1 \u2299 \u03b4) \u2299 yt\u22121.\n(1)\nThis operation can be directly computed as per equation 1. However, during training and non-causal\ndata processing, it can equivalently be computed as a convolution with a kernel which is of the same\nshape as the input data [14]. This convolution can be efficiently performed in O(LlogL) time in the\nfrequency domain [17], [10]. This mode of operation is interesting because it allows for a higher\nutilization of GPUs\u2019 parallel processing capabilities [17].\nIn this work, we investigate the performance and runtime effects of replacing the bottleneck\nEMA within the MEGA processing stack with a dedicated temporal convolutional neural network\n(TCN) [18\u201321], an operator which scales linearly with the sequence length. The TCN employs\ndilated convolutions, which allow the network to achieve a large receptive field with few parameters.\nTCNs are typically implemented as a cascade of residual blocks, in which each block applies two\ndilated convolution operations with equal dilations. In order to quickly reach large receptive fields,\nthe dilation exponentially increases with each successive block [18, 21]. Our model differs from what\nis usually used in literature in that it only includes a single dilated convolution operation per residual\nblock. This construction allows for a larger receptive field size with shallower networks. Details are\ngiven in Appendix E. We call the resulting model, which combines a TCN with chunked attention,\nTCNCA.\nWe find that on EnWik8 language modeling, TCNCA outperforms MEGA [14] (and Transformer-\nXL [22]), achieving a BPC score of 1.01, in addition to 1.37\u00d7/1.24\u00d7 faster forward/backward pass.\nOn a synthetic reasoning benchmark, associative recall, a simplified version of TCNCA (see Appendix\nC) is competitive with MEGA over a range of different sequence lengths and vocabulary sizes. On\n64-dimensional sequences of lengths ranging from 8192 to 131072, the employed dilated convolution\noperator is up to 7.07\u00d7 and 2.86\u00d7 faster than the parallelized EMA of MEGA in the forward and\nbackward pass, respectively. This signifies the scalability of the approach to long sequences thanks to\nits linear complexity. On the LRA classification tasks, TCNCA slightly underperforms MEGA by\nonly 0.1% on average, while achieving 1.28\u00d7 inference speedup.\n2\nThe TCNCA model\nAn overview of the model and the operations used therein is shown in Figure 1. At a high-level, the\nmodel can be thought of as a concatenation of a temporal convolutional neural network (Figure 1b)\nwith chunked attention (Figure 1d). The sketch is simplified; the actual construction follows the one\ndefined by MEGA [14], and is outlined in Appendix C.\nFigure 1a shows a depth-N sequence processing stack. Each of the N-many layers consists of a\ntemporal convolutional network and chunked attention, both of which operate along the time axis,\nFigure 1: (a) Simplified high-level overview of the TCNCA model. (b) The TCN residual block. (c)\nConnectivity of a TCN with kernel size K = 3, dilation factor f = 3, and depth D = 2. (d) Chunked\nattention operation which computes query-key similarities in fixed-width non-overlapping windows,\nshown with chunk size 3.\n2\nfollowed by a multi-layer perceptron (MLP) operating along the feature axis. For each embedding\ndimension, a TCN with its own set of trainable parameters is instantiated.\nThe TCN block in Figure 1a is expanded in Figure 1b. Three integer hyperparameters govern the TCN\nconstruction; kernel size K, dilation factor f, and depth D. The TCN consists of D-many residual\nblocks, each of which implements a dilated convolution operation whose dilation is determined by\nthe layer index i = 0, ..., D \u2212 1 and f as f i. In Figure 1c, we show the connectivity pattern of a TCN\nwith D = 2, f = 3 and K = 3.\nFollowing the TCN, which scales as O(L), we have chunked attention. As already noted, it computes\nthe query-key similarities only within fixed-size non-overlapping windows within the sequence, as\nshown in Figure 1d. This is also an O(L) operation.\n3\nExperiments\nEnWik8 language modeling\nEnWik8 is a dataset which comprises a subset of the English\nWikipedia. We train and evaluate our model on EnWik8 character-level language modeling in\nthe same manner as was done in MEGA [14]. The results are shown in Table 1. More details are\ngiven in Appendix F.\nTable 1: EnWik8 bit-per-character scores. Results marked with a star (*) are taken from [14].\nModel\nTransformer-XL\nMEGA\nTCNCA\nBPC\n1.06*\n1.02*\n1.01\nParameters\n41M\n39M\n39M\nTCNCA outperforms the Transformer-XL [22] as well as MEGA [14], reaching a 1.01 BPC score.\nFor transparency\u2019s sake, we have to note that the scores reported in relevant literature are rounded\ndown to 2 digits after the decimal point, hence we do the same. With 4 digits after the decimal point,\nthe score we achieve is 1.0144 BPC.\nWe measure the forward and backward pass speed-up on a 16GB Nvidia V100 GPU during training.\nDuring training, TCNCA achieves a 1.373\u00d7 speed-up in the forward pass and a 1.245\u00d7 speed-up\nin the backward pass, compared to MEGA. However, speeding up the inference runtime of the\ngenerative tasks is not straightforward and is one of the limitations of this work (see Appendix B).\nLong-range-arena\nLong-range-arena [9] comprises six classification tasks with sequence lengths\nranging from 1024 to 16384. The benchmarks are varied, including pattern detection, sentiment\nclassification, mathematical reasoning, and visual logical reasoning. We use the same dimensionalities,\nhyperparameters, and attention chunk sizes as those used in MEGA [14], and select the TCN\nconstruction as per Appendix D. Results are shown in Table 2.\nAlthough TCNCA lags behind the state-of-the-art state space method, S5 [12], by 2.3%, it is on par\nwith MEGA-chunk (just an average of a 0.1% lower accuracy) while achieving an average inference\nspeed-up 28%.\nTable 2: Long-range-arena accuracies (%) of state-of-the-art models. The Transformer scores are\ntaken from the reproduction in MEGA [14]. All other results, excluding TCNCA, were taken from\nthe respective papers. The last row reports the end-to-end inference speed-up of TCNCA measured\nagainst MEGA-chunk.\nModel\nListOps\nText\nRetrieval\nImage\nPath\nPath-X\nAverage\nTransformer [1] [14]\n37.1\n65.2\n79.1\n42.9\n71.8\n50\n57.7\nS4D [23]\n60.5\n86.2\n89.5\n89.9\n93.1\n91.9\n85.2\nS5 [12]\n62.2\n89.3\n91.4\n90.1\n95.3\n98.6\n87.8\nLRU [11]\n60.2\n89.4\n89.9\n89.0\n95.7\n96.0\n86.7\nSGConv [24]\n61.4\n89.2\n91.1\n87.97\n95.4\n97.8\n87.1\nMEGA chunk [14]\n58.7\n90.2\n91.0\n85.8\n94.4\n93.8\n85.6\nTCNCA\n59.6\n89.8\n89.4\n86.8\n94.5\n92.7\n85.5\nSpeedup (forward pass)\n1.05\u00d7\n1.25\u00d7\n1.18\u00d7\n1.24\u00d7\n1.25\u00d7\n1.73\u00d7\n1.28\u00d7\n3\nTable 3: Associative recall accuracy (%) with varying sequence lengths and vocabulary sizes.\nVocabulary size 10\nVocabulary size 20\nSeq. len.\nMEGA\nTCNCA-simple\nMEGA\nTCNCA-simple\n64\n98.8\n100\n62.4\n56\n1024\n99.6\n100\n99.4\n97.6\n4096\n100\n100\n100\n99.6\n8192\n98.2\n100\n98.6\n99.2\nAssociative recall\nThis synthetic benchmark requires faithful attention and measures the basic\nreasoning capability of neural sequence models, remembering associations between pairs of tokens\n[25] [13]. For example, given a sequence of tokens a 2 c 4 b 3 d 1, if the model is prompted with a,\nthe expected output is 2, the token following a in the input sequence. If it were prompted with b, the\ncorrect output would be 3, etc.\nAs mentioned, TCNCA is based on MEGA [14], and as such it involves an intricate interconnection\nbetween the different modules it is composed of. We report TCNCA scores for the associative recall\nin a setting in which the module interconnection is significantly simplified by eliminating excessive\nmultiplicative and additive interactions (TCNCA-simple, see Appx. C). Over the investigated range of\nvocabulary sizes and sequence lengths in Table 3, TCNCA-simple remains competitive with MEGA.\nParallelized EMA vs. dilated convolution runtime measurements\nWe measure the forward and\nbackward-pass runtimes of a dilated convolutional network and a parallelized EMA recurrence over\na range of sequence lengths, and report the results in Figure 2. For a clear comparison of the two\noperations, we strip both of them of residual connections, non-linearities as well as normalization\nlayers. They are roughly parameter-matched, with EMA having 64 parameters and the dilated\nconvolution having 68 parameters. The dilated convolutional network is configured with K = 17,\nD = 4, and f is increased until the receptive field of the network is larger than the sequence length it\noperates on. The benchmarks were run on an Nvidia V100 with 16 GB of VRAM. Further details are\ngiven in Appendix H.\n(a) Forward pass runtime measurements.\n(b) Backward pass runtime measurements.\nFigure 2: Run-time comparisons between a parallel linear recurrence including kernel generation\n(blue) and a dilated CNN (orange) for the forward and backward pass, with varying sequence lengths.\nThe dilated convolutional network is consistently the faster operation.\n4\nConclusion\nIn this work inspired by ground-breaking results from the team behind MEGA [14], we show that\na TCN and chunked attention hybrid model, TCNCA, is able to compete with the state-of-the-art\nmodels on Enwik8 language modeling and Long-Range-Arena sequence classification. During\ntraining and non-causal inference workloads, TCNCA consistently exhibits inference speed-ups in\nthe range of 5% to 73% compared to MEGA-chunk. We show that a simplified version of TCNCA\nsolves the associative recall synthetic reasoning benchmark with a similar accuracy as does MEGA.\nFinally, we show that on the Nvidia V100 GPU, a dilated convolutional network is consistently faster\nthan an FFT-based parallelized EMA recurrence over a wide range of sequence lengths. Some of the\nlimitations of our approach are detailed in Appendix B.\n4\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information\nProcessing Systems (NeurIPS), 30, 2017.\n[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. International Conference on Learning Representations (ICLR), 2021.\n[3] Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. Neural\nNetworks, 146:272\u2013289, 2022.\n[4] Daria Grechishnikova. Transformer neural network for protein-specific de novo drug generation\nas a machine translation problem. Scientific reports, 11(1):321, 2021.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics\n(ACL): Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186,\n2019.\n[6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models\nare few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:\n1877\u20131901, 2020.\n[8] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.\nACM Computing Surveys, 55:1 \u2013 28, 2020.\n[9] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient\ntransformers. In International Conference on Learning Representations (ICLR), 2021.\n[10] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured\nstate spaces. In International Conference on Learning Representations (ICLR), 2022.\n[11] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan\nPascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In\nInternational Conference on Machine Learning (ICML), 2023.\n[12] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for\nsequence modeling. In International Conference on Learning Representations (ICLR), 2023.\n[13] Tri Dao, Daniel Y. Fu, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9.\nHungry Hungry Hippos: Towards language modeling with state space models. In International\nConference on Learning Representations (ICLR), 2023.\n[14] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan\nMay, and Zettlemoyer Luke. Mega: Moving average equipped gated attention. In International\nConference on Learning Representations (ICLR), 2023.\n[15] Eddie McKenzie and Everette S. Gardner. Damped trend exponential smoothing: A modelling\nviewpoint. International Journal of Forecasting, 26(4):661\u2013665, 2010.\n[16] Marcus Hutter. The human knowledge compression contest, 2006.\n[17] Narsimha Reddy Chilkuri and Chris Eliasmith. Parallelizing legendre memory unit training. In\nInternational Conference on Machine Learning (ICML), pages 1898\u20131907, 2021.\n5\n[18] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence modeling. arXiv:1803.01271, 2018.\n[19] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099,\n2016.\n[20] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[21] Thorir Mar Ingolfsson, Michael Hersche, Xiaying Wang, Nobuaki Kobayashi, Lukas Cavigelli,\nand Luca Benini. Eeg-tcnet: An accurate temporal convolutional network for embedded motor-\nimagery brain\u2013machine interfaces. In 2020 IEEE International Conference on Systems, Man,\nand Cybernetics (SMC), pages 2958\u20132965, 2020.\n[22] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages\n2978\u20132988, 2019.\n[23] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and\ninitialization of diagonal state space models. Advances in Neural Information Processing\nSystems (NeurIPS), 35:35971\u201335983, 2022.\n[24] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convo-\nlutional models great on long sequence modeling? In International Conference on Learning\nRepresentations (ICLR), 2023.\n[25] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using\nfast weights to attend to the recent past. Advances in Neural Information Processing Systems\n(NeurIPS), 29, 2016.\n[26] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory\nwith optimal polynomial projections. Advances in Neural Information Processing Systems\n(NeurIPS), 33:1474\u20131487, 2020.\n[27] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language\nmodeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022.\n[28] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning\nand induction heads. arXiv preprint arXiv:2209.11895, 2022.\n[29] Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, Stephen A. Bac-\ncus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger\nconvolutional language models. In International Conference on Machine Learning (ICML),\n2023.\n[30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri\nRudra, and Christopher Re. Simple hardware-efficient long convolutions for sequence modeling.\nIn ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,\n2023.\n[31] Lei Cheng, Ruslan Khalitov, Tong Yu, Jing Zhang, and Zhirong Yang. Classification of long\nsequential data using circular dilated convolutional neural networks. Neurocomputing, 2022.\n[32] Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, and Furao Shen. Temporal convolutional\nattention-based network for sequence modeling. arXiv preprint arXiv:2002.12530, 2020.\n[33] Xianjin Chao, Zhipeng Ge, and Howard Leung. Video2mesh: 3d human pose and shape\nrecovery by a temporal convolutional transformer network. IET Computer Vision, 17, 02 2023.\n6\n[34] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length.\nInternational Conference on Learning Representations (ICLR), 2018.\n[35] Rupesh K Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Training very deep networks.\nAdvances in Neural Information Processing Systems (NeurIPS), 28, 2015.\n[36] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n7\nA\nRelated work\nLong sequence modeling is a rich area of research within the deep learning community. A non-\nexhaustive list of work relevant to ours is outlined below.\nLinear recurrent models\nA particularly prominent family of linear recurrent models comes in the form of linear state-space\nmodels (LSSMs). In their original formulation, linear state-space models are based on the HiPPO\nframework [26], in which an optimal mechanism for incrementally updating a fixed-size state while\nprocessing online streams of information by projecting onto an orthogonal polynomial basis is\nderived. Based on the HiPPO framework, the seminal work S4 [10] introduces a new addition into\nthe neural sequence processing family of operators, linear state-space models. S4 is, to the best of\nour knowledge, the first method to significantly advance the state-of-the-art on LRA classification [9].\nMany other linear state-space models follow; S4D [23] diagonalizes the linear recurrence, GSS [27]\nintroduces a gating mechanism for improving LSSMs\u2019 performance on language modeling, S5 [12]\nintroduces MIMO LSSMs.\nWe had mentioned that linear state-space models, in their typical formulation, are not suitable for\nlanguage modeling. H3 [13], motivated by contemporary work on mechanistic interpretability [28],\nproposes a multiplicative interconnection of linear state-space models which they find significantly\nimproves their performance on this task.\nOne of the newer additions to the family of linear recurrent models is the LRU model [11], which steps\naway from the state-space model framework which is based on discretizing an implicit continuous\nstate-space model while still achieving near-state-of-the-art accuracies on LRA.\nLong convolutional models\nWe denote models which apply convolutions whose kernels are of the same length as the input\nsequence as long convolutional models. SGConv [24] constructs a sparsely parametrized long kernel\nwith an exponentially decaying structure and finds that this achieves strong performance on LRA.\nHyena hierarchy [29] achieves impressive scores on language modeling with a long convolutional\nkernel, without attention. It is to the best of our knowledge the highest performing attention-free\nlanguage model. FlashButterfly [30] explores simple long convolution kernel constructions that are\neffective on LRA classification and furthermore develops a hardware-aware algorithm for efficient\ncomputation of FFT-based long convolutions.\nLinear complexity attention\nA rich area of research addresses the quadratic scaling of the attention operation by deriving more\nscalable approximations thereof. A survey of such approaches can be found in [8].\nTCNs for sequence modeling\nCDIL-CNN employs circular dilated convolutions on several datasets including Long-Range-Arena\n[31]. WaveNet [20] is a generative audio model based on the TCN architecture. ByteNet [19] employs\nthe TCN for machine translation.\nTCN-Attention hybrid models\nTCAN [32] employs a cascade of dilated convolutions with full self-attention. Their architecture\nscales quadratically with the sequence length, has one convolution per decoder stack, and the causal\nattention masking they implement, described in Section 2.3, is in fact not causal. TConvTrans-\nformer [33] describes a similar idea, a quadratic complexity concatenation of a TCN and multi-head\nself-attention, and is evaluated on 3D human pose and shape estimation.\nB\nLimitations\nOn generative tasks, an important example being language modeling, linear recurrent models offer\nthe unique advantage of parallel training and recurrent inference. A temporal convolutional neural\nnetwork is a naturally parallel operation, but it is not trivial to adapt it to efficiently operate on gener-\native tasks. In a generative mode of operation, we would need to cache intermediate computations\ngenerated by the TCN and re-use them at later time steps. We have not implemented this mode of\n8\nFigure 3: The module interconnection used in TCNCA is inherited from MEGA and is significantly\nmore complex than the simplified sketch we have shown in the main text. It involves several\nmultiplicative and additive interactions between outputs of different operators. QK generation applies\na diagonal linear transformation on the input data and adds a trainable bias to it, with separate\nparameters for queries and keys. The gated residual is equivalent to the construction defined in\nHighway Networks [35].\nFigure 4: TCNCA-simple is a significantly simpler version of the full TCNCA model, and is used in\nthe associative recall benchmark.\noperation in our work, opting instead to demonstrate the feasibility and applicability of the idea.\nImplementing this mode of operation is left for future work.\nAnother advantage of recurrent models lies in the fact that their receptive field size is theoretically\nunlimited. The TCN\u2019s receptive field size is restricted through the construction that we impose by\ndefining the kernel size, network depth, and dilation.\nThe TCN introduces three new hyperparameters, namely kernel size, dilation factor, and network\ndepth. This significantly expands the hyperparameter search space.\nThere exists at least one alternative option for parallelizing linear recurrent systems, the parallel\nscans algorithm [34]. We have not compared the runtimes of the TCN with this algorithm.\nAs hinted at in the method description section in the main text, the chunked attention module is in\nfact significantly more intricately interconnected with the TCN than Figure 1 makes it seem like. The\nfull construction is shown in Appendix C. Future work should investigate the necessity of each part\nof the module with the goal of simplifying the construction and maximizing its performance.\nC\nDetailed overview of the attention module\nThe attention module is more intricately interconnected with EMA/TCN than Figure 1 in the main\ntext would make it seem. Details are given in Figure 3.\nFor the associative recall benchmark, we used a simpler construction, called TCNCA-simple, which\nis outlined in Figure4\nD\nHyperparameter selection\nFor LRA and EnWik8, the TCN structure, which is defined by the kernel size, dilation factor, and\nTCN depth, was selected by measuring inference run-times of a range of configurations, selecting a\n9\nrange of those which exhibit a speed-up, and finally training networks with those structures on the\ngiven dataset. It is a rather costly procedure, since there exist many different TCN structures which\nexhibit a speed-up, and they differ between benchmarks. A more principled way of selecting TCN\nconfigurations should be investigated in future work.\nFor experiments on the long-range-arena dataset, the same learning rate scheduler, learning rate,\nweight decay, dropout values, activation functions, and vector dimensionalities as those used in\nMEGA [14] were used.\nFor experiments on the EnWik8 dataset, the same learning rate scheduler, dropout values, activation\nfunctions and vector dimensionalities as those used in MEGA [14] were used. A learning rate of\n0.005 and a weight decay of 0.2 were used to obtain the best score.\nThe only varied hyperparameters in the associative recall experiments are dropout and the learning\nrate. Dropout is selected from the small grid defined by two points, [0, 0.1]. Learning rate was swept\nover [1e-5, 1e-4, 1e-3, 1e-2, 1e-1].\nE\nTCN construction\nIn addition to the TCN hyperparameters introduced in the main text, these being kernel size K,\ndilation factor f, and depth D, we will in this section introduce a new hyperparameter, B, which\nstands for the number of dilated convolutions within a single residual block. This is explained in\nFigure 5.\nFigure 5: The hyperparameter B controls the number of dilated convolutions with equal dilation\nwithin a single residual block. The full network still consists of D-many such blocks. On the top,\nwe show a residual block with B = 1, which is what we used in our experiments. B = 2 is another\npopular option in published works on TCNs.\nThe receptive field size of the TCN can be calculated as 1+B\u2217(K\u22121)\u2217 f D\u22121\nf\u22121 . It scales exponentially\nwith D and linearly with B. Keeping the total number of dilated convolution operations D \u00d7 B equal,\nthe largest receptive field size is achieved when B is minimized and D is maximized, which is the\nreason we opted for B = 1 in our experiments.\nF\nEnWik8 train and test details\nThe data is split into consecutive chunks of size 2048, which is also what the attention chunk size is\nset to. At training time, we randomly load 2, 3, or 4 consecutive chunks of text to be processed by\nthe model. During evaluation, the attention chunk size is set to 4096, and 3 consecutive chunks of\ntext are loaded. We train for 250 epochs. Just as in MEGA [14], attention is augmented with relative\npositional bias, the particular one used in this task being RoPE [36].\n10\nG\nAssociative recall setup\nWe must note that, while N from Figure 1 (a) is 2, D from Figure 1 (b) must be larger than 1 in\norder to obtain a large enough receptive field of the TCN. Hence, our model does consist of a depth-2\nsequence decoding stack as was used in [13], but each of the decoder layers uses a deeper TCN,\ntypically of depth 3 or 4.\nFor sequence length 64, we use embedding dimension 32 and an attention chunk size of 32, cor-\nresponding to the standard quadratic self-attention. For all other sequence lengths, an embedding\ndimension of 128 with a chunk size of 128 is used.\nThe need for self-attention\nIn this section, we demonstrate the performance difference between attention-less models and those\nwhich hybridize EMA/TCN with attention on EnWik8 (Tables 4 and 5) and LRA (Table 6). On\nLRA, we are in fact able to achieve strong performance using a TCN-based attention-free model,\noutperforming the best TCN-based attention-free model known in the literature, CDIL-CNN [31].\nTable 4: The role of attention in MEGA on EnWik8 after full training. The hyperparameters used in\nMEGA were re-used in the EMA-MLP experiment set. The result marked with a star (*) is taken\nfrom [14].\nEnwik8 loss \u2193\nEMA-MLP stack\n1.96\nMEGA\n1.02*\nTable 5: Introducing chunked attention after the TCN significantly reduces the BPC loss on enwik8.\nBoth models went through a hyperparameter grid search but were trained for a limited number of\nepochs, hence the gap between the result reported here and in the main text.\nEnwik loss after 250k training steps\nTCN-MLP stack\n1.44\nTCNCA\n1.08\nTable 6: Comparison of S5 [12], CDIL-CNN [31], and our implementation of a dilated convolutional\nneural network on several tasks from the long-range-arena dataset. Our implementation outperforms\nthe best-known TCN-based result from literature, the CDIL-CNN work [31], on all four LRA\nbenchmarks which we both evaluate on.\nListOps\nText\nRetrieval\nImage\nPath\nPath-X\nS5 [12]\n62.1%\n89.3%\n91.4%\n88.0%\n95.3%\n98.6%\nCDIL-CNN [31]\n\u2014\n87.6%\n84.3%\n64.5%\n91.0%\n\u2014\nOur TCN-MLP\n56.9%\n89.6%\n85.9%\n91.4%\n97.2%\n91.6%\nH\nRuntime benchmark methodology\nThe EMA hidden dimension (dimension expansion from [14], Section 3.1) is set to 8. Within the\nmulti-dimensional damped EMA algorithm presented in MEGA [14], this results in 64 parameters.\nThe TCN is always of depth D = 4 with 17 parameters in each layer, hence it consists of 68\nparameters, slightly more than the amount present in EMA. The dilation factor is increased until the\nTCN\u2019s receptive field size is greater than or equal to the sequence length.\nAll operations were implemented in the PyTorch 2.0.1 framework. The run-time measurements were\nobtained using the PyTorch benchmark module 3. LRA and EnWik8 run-times were measured on a\n32 GB Nvidia V100 GPU. All other run-times were measured on a 16 GB Nvidia V100 GPU.\n3https://pytorch.org/tutorials/recipes/recipes/benchmark.html\n11\nI\nEffect of EMA kernel generation on inference runtime\nFigure 6: Computing the EMA linear recurrence in parallel invokes the computational pipeline shown\nin this figure. The kernel computation is given in MEGA [14], Appendix A. There exists at least one\nalternative way of computing the long convolution, using the parallel scans algorithms [34], which\nwe did not consider in this work.\nThe runtime measurements presented in the main text include the kernel generation cost. This is\ncertainly necessary during training, but at inference, one might consider storing very long kernels\nand truncating them based on the input sequence length. This would reduce the inference runtimes\nof FFT-based EMA convolution. Runtime comparisons of FFT-based EMA convolutions with and\nwithout kernel generation are shown in Figure 7. Speed-ups of the version without kernel generation\nvs. the version that includes kernel generation are given in Table 7.\nFigure 7: Comparing the forward-pass runtimes between the FFT-based parallel EMA with and\nwithout kernel generation (see Figure 6).\nTable 7: Speedup of FFT-EMA without vs. with kernel generation.\n8k\n16k\n32k\n64k\n128k\nSpeedup\n1.51\u00d7\n1.25\u00d7\n1.23\u00d7\n1.20\u00d7\n1.25\u00d7\n12\n"
  }
]