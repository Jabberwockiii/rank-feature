[
  {
    "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
    "link": "https://arxiv.org/pdf/2307.04725.pdf",
    "upvote": "60",
    "text": "Published as a conference paper at ICLR 2024\nANIMATEDIFF: ANIMATE YOUR PERSONALIZED\nTEXT-TO-IMAGE DIFFUSION MODELS WITHOUT\nSPECIFIC TUNING\nYuwei Guo1\nCeyuan Yang2\u2020 Anyi Rao3\nZhengyang Liang2\nYaohui Wang2\nYu Qiao2\nManeesh Agrawala3\nDahua Lin1,2\nBo Dai2\n1The Chinese University of Hong Kong\n2Shanghai Artificial Intelligence Laboratory\n3Stanford University\n(cartoon) 1boy, dark skin, play-\ning guitar, concert, ...\n(oil painting) black pearl pirate\nship, night time, sea, . . .\n(realistic)\na\nLamborghini\non\nroad, fireworks, high detail, . . .\nzoom-in\nrolling\nzoom-out + rolling\nright + up\nFigure 1: AnimateDiff directly turns existing personalized text-to-image (T2I) models to the corre-\nsponding animation generators with a pre-trained motion module. First row: results by combining\nAnimateDiff with three personalized T2Is in different domains; Second row: results of further com-\nbining AnimateDiff with MotionLoRA (s) to achieve shot type controls. Best viewed with Acrobat\nReader. Click the images to play the animation clips.\nABSTRACT\nWith the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion)\nand corresponding personalization techniques such as DreamBooth and LoRA,\neveryone can manifest their imagination into high-quality images at an afford-\nable cost. However, adding motion dynamics to existing high-quality personal-\nized T2Is and enabling them to generate animations remains an open challenge.\nIn this paper, we present AnimateDiff, a practical framework for animating per-\nsonalized T2I models without requiring model-specific tuning. At the core of\nour framework is a plug-and-play motion module that can be trained once and\nseamlessly integrated into any personalized T2Is originating from the same base\n\u2020Corresponding Author.\n1\narXiv:2307.04725v2  [cs.CV]  8 Feb 2024\nPublished as a conference paper at ICLR 2024\nT2I. Through our proposed training strategy, the motion module effectively learns\ntransferable motion priors from real-world videos. Once trained, the motion mod-\nule can be inserted into a personalized T2I model to form a personalized animation\ngenerator. We further propose MotionLoRA, a lightweight fine-tuning technique\nfor AnimateDiff that enables a pre-trained motion module to adapt to new motion\npatterns, such as different shot types, at a low training and data collection cost. We\nevaluate AnimateDiff and MotionLoRA on several public representative personal-\nized T2I models collected from the community. The results demonstrate that our\napproaches help these models generate temporally smooth animation clips while\npreserving the visual quality and motion diversity. Codes and pre-trained weights\nare available at https://github.com/guoyww/AnimateDiff.\n1\nINTRODUCTION\nText-to-image (T2I) diffusion models (Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022;\nRombach et al., 2022) have greatly empowered artists and amateurs to create visual content using\ntext prompts. To further stimulate the creativity of existing T2I models, lightweight personalization\nmethods, such as DreamBooth (Ruiz et al., 2023) and LoRA (Hu et al., 2021) have been proposed.\nThese methods enable customized fine-tuning on small datasets using consumer-grade hardware\nsuch as a laptop with an RTX3080, thereby allowing users to adapt a base T2I model to new do-\nmains and improve visual quality at a relatively low cost. Consequently, a large community of\nAI artists and amateurs has contributed numerous personalized models on model-sharing platforms\nsuch as Civitai (2022) and Hugging Face (2022). While these personalized T2I models can generate\nremarkable visual quality, their outputs are limited to static images. On the other hand, the ability\nto generate animations is more desirable in real-world production, such as in the movie and cartoon\nindustries. In this work, we aim to directly transform existing high-quality personalized T2I models\ninto animation generators without requiring model-specific fine-tuning, which is often impractical\nin terms of computation and data collection costs for amateur users.\nWe present AnimateDiff, an effective pipeline for addressing the problem of animating personalized\nT2Is while preserving their visual quality and domain knowledge. The core of AnimateDiff is an\napproach for training a plug-and-play motion module that learns reasonable motion priors from\nvideo datasets, such as WebVid-10M (Bain et al., 2021). At inference time, the trained motion\nmodule can be directly integrated into personalized T2Is and produce smooth and visually appealing\nanimations without requiring specific tuning. The training of the motion module in AnimateDiff\nconsists of three stages. Firstly, we fine-tune a domain adapter on the base T2I to align with the\nvisual distribution of the target video dataset. This preliminary step guarantees the motion module\nconcentrates on learning the motion priors rather than pixel-level details from the training videos.\nSecondly, we inflate the base T2I together with the domain adapter and introduce a newly initialized\nmotion module for motion modeling. We then optimize this module on videos while keeping the\ndomain adapter and base T2I weights fixed. By doing so, the motion module learns generalized\nmotion priors and can, via module insertion, enable other personalized T2Is to generate smooth and\nappealing animations aligned with their personalized domains. The third stage of AnimateDiff, also\ndubbed as MotionLoRA, aims to adapt the pre-trained motion module to specific motion patterns\nwith a small number of reference videos and training iterations. We achieve this by fine-tuning\nthe motion module with the aid of Low-Rank Adaptation (LoRA) (Hu et al., 2021). Remarkably,\nadapting to a new motion pattern can be achieved with as few as 50 reference videos. Moreover, a\nMotionLoRA model requires only approximately 30M of additional storage space, further enhancing\nthe efficiency of model sharing. This efficiency is particularly valuable for users who are unable to\nbear the expensive costs of pre-training but desire to fine-tune the motion module for specific effects.\nWe evaluate the performance of AnimateDiff and MotionLoRA on a diverse set of personalized T2I\nmodels collected from model-sharing platforms (Civitai, 2022; Hugging Face, 2022). These models\nencompass a wide spectrum of domains, ranging from 2D cartoons to realistic photographs, thereby\nforming a comprehensive benchmark for our evaluation. The results of our experiments demon-\nstrate promising outcomes. In practice, we also found that a Transformer (Vaswani et al., 2017)\narchitecture along the temporal axis is adequate for capturing appropriate motion priors. We also\ndemonstrate that our motion module can be seamlessly integrated with existing content-controlling\n2\nPublished as a conference paper at ICLR 2024\napproaches (Zhang et al., 2023; Mou et al., 2023) such as ControlNet without requiring additional\ntraining, enabling AnimateDiff for controllable animation generation.\nIn summary, (1) we present AnimateDiff, a practical pipeline that enables the animation generation\nability of any personalized T2Is without specific fine-tuning; (2) we verify that a Transformer archi-\ntecture is adequate for modeling motion priors, which provides valuable insights for video genera-\ntion; (3) we propose MotionLoRA, a lightweight fine-tuning technique to adapt pre-trained motion\nmodules to new motion patterns; (4) we comprehensively evaluate our approach with representative\ncommunity models and compare it with both academic baselines and commercial tools such as Gen-\n2 (2023) and Pika Labs (2023). Furthermore, we showcase its compatibility with existing works for\ncontrollable generation.\n2\nRELATED WORK\nText-to-image diffusion models. Diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021;\nSong et al., 2020) for text-to-image (T2I) generation (Gu et al., 2022; Mokady et al., 2023; Podell\net al., 2023; Ding et al., 2021; Zhou et al., 2022b; Ramesh et al., 2021; Li et al., 2022) have gained\nsignificant attention in both academic and non-academic communities recently. GLIDE (Nichol\net al., 2021) introduced text conditions and demonstrated that incorporating classifier guidance leads\nto more pleasing results. DALL-E2 (Ramesh et al., 2022) improves text-image alignment by lever-\naging the CLIP (Radford et al., 2021) joint feature space. Imagen (Saharia et al., 2022) incorporates\na large language model (Raffel et al., 2020) and a cascade architecture to achieve photorealistic re-\nsults. Latent Diffusion Model (Rombach et al., 2022), also known as Stable Diffusion, moves the\ndiffusion process to the latent space of an auto-encoder to enhance efficiency. eDiff-I (Balaji et al.,\n2022) employs an ensemble of diffusion models specialized for different generation stages.\nPersonalizing T2I models. To facilitate the creation with pre-trained T2Is, many works focus on\nefficient model personalization (Shi et al., 2023; Lu et al., 2023; Dong et al., 2022; Kumari et al.,\n2023), i.e., introducing concepts or styles to the base T2I using reference images. The most straight-\nforward approach to achieve this is complete fine-tuning of the model. Despite its potential to\nsignificantly enhance overall quality, this practice can lead to catastrophic forgetting (Kirkpatrick\net al., 2017; French, 1999) when the reference image set is small. Instead, DreamBooth (Ruiz et al.,\n2023) fine-tunes the entire network with preservation loss and uses only a few images. Textual In-\nversion (Gal et al., 2022) optimize a token embedding for each new concept. Low-Rank Adaptation\n(LoRA) (Hu et al., 2021) facilitates the above fine-tuning process by introducing additional LoRA\nlayers to the base T2I and optimizing only the weight residuals. There are also encoder-based ap-\nproaches that address the personalization problem (Gal et al., 2023; Jia et al., 2023). In our work,\nwe focus on tuning-based methods, including overall fine-tuning, DreamBooth (Ruiz et al., 2023),\nand LoRA (Hu et al., 2021), as they preserve the original feature space of the base T2I.\nAnimating personalized T2Is. There are not many existing works regarding animating personal-\nized T2Is. Text2Cinemagraph (Mahapatra et al., 2023) proposed to generate cinematography via\nflow prediction. In the field of video generation, it is common to extend a pre-trained T2I with\ntemporal structures. Existing works (Esser et al., 2023; Zhou et al., 2022a; Singer et al., 2022; Ho\net al., 2022b,a; Ruan et al., 2023; Luo et al., 2023; Yin et al., 2023b,a; Wang et al., 2023b; Hong\net al., 2022; Luo et al., 2023) mostly update all parameters and modify the feature space of the\noriginal T2I and is not compatible with personalized ones. Align-Your-Latents (Blattmann et al.,\n2023) shows that the frozen image layers in a general video generator can be personalized. Re-\ncently, some video generation approaches have shown promising results in animating a personalized\nT2I model. Tune-a-Video (Wu et al., 2023) fine-tune a small number of parameters on a single\nvideo. Text2Video-Zero (Khachatryan et al., 2023) introduces a training-free method to animate a\npre-trained T2I via latent wrapping based on a pre-defined affine matrix.\n3\nPRELIMINARY\nWe introduce the preliminary of Stable Diffusion (Rombach et al., 2022), the base T2I model used in\nour work, and Low-Rank Adaptation (LoRA) (Hu et al., 2021), which helps understand the domain\nadapter (Sec. 4.1) and MotionLoRA (Sec. 4.3) in AnimateDiff.\n3\nPublished as a conference paper at ICLR 2024\nStable Diffusion. We chose Stable Diffusion (SD) as the base T2I model in this paper since it is\nopen-sourced and has a well-developed community with many high-quality personalized T2I models\nfor evaluation. SD performs the diffusion process within the latent space of a pre-trained autoen-\ncoder E(\u00b7) and D(\u00b7). In training, an encoded image z0 = E(x0) is perturbed to zt by the forword\ndiffusion:\nzt = \u221a \u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, \u03f5 \u223c N(0, I ),\n(1)\nfor t = 1, . . . , T, where pre-defined \u00af\u03b1t determines the noise strength at step t. The denoising\nnetwork \u03f5\u03b8(\u00b7) learns to reverse this process by predicting the added noise, encouraged by an MSE\nloss:\nL = EE(x0),y,\u03f5\u223cN (0,I ),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4\u03b8(y))\u22252\n2\n\u0003\n,\n(2)\nwhere y is the text prompt corresponding to x0; \u03c4\u03b8(\u00b7) is a text encoder mapping the prompt to a\nvector sequence. In SD, \u03f5\u03b8(\u00b7) is implemented as a UNet (Ronneberger et al., 2015) consisting of\npairs of down/up sample blocks at four resolution levels, as well as a middle block. Each network\nblock consists of ResNet (He et al., 2016), spatial self-attention layers, and cross-attention layers\nthat introduce text conditions.\nLow-rank adaptation (LoRA). LoRA (Hu et al., 2021) is an approach that accelerates the fine-\ntuning of large models and is first proposed for language model adaptation. Instead of retraining\nall model parameters, LoRA adds pairs of rank-decomposition matrices and optimizes only these\nnewly introduced weights. By limiting the trainable parameters and keeping the original weights\nfrozen, LoRA is less likely to cause catastrophic forgetting (Kirkpatrick et al., 2017). Concretely,\nthe rank-decomposition matrices serve as the residual of the pre-trained model weights W \u2208 Rm\u00d7n.\nThe new model weight with LoRA is\nW\u2032 = W + \u2206W = W + ABT ,\n(3)\nwhere A \u2208 Rm\u00d7r, B \u2208 Rn\u00d7r are a pair of rank-decomposition matrices, r is a hyper-parameter,\nwhich is referred to as the rank of LoRA layers. In practice, LoRA is only applied to attention layers,\nfurther reducing the cost and storage for model fine-tuning.\n4\nANIMATEDIFF\nInference\nProj. Out \n(zero initialize)\ndule (Temporal Transformer)\nSelf-\nAttention\n\u00d7N\n+\nEnc.\nion Priors\n3. (optional) Adapt to New Patterns\n5~20 Ref.\nVideos\nPretrained Image Layers (frozen)\nDomain Eraser (trainable at stage 1)\nMotion Module (trainable at stage 2)\nMotion LoRA (trainable at stage 3)\n<prompts>\n<prompts>\nImage Layers of Personalized T2I\nIterative Denoise\nComponent Modules of \nAnimateDiff\nTraining\nPipeline\nInsertion\nProj. Out \n(zero initialize)\ne (Temporal Transformer)\nSelf-\nAttention\n\u00d7N\nnc.\nn Priors\n3. (optional) Adapt to New Patterns\n20~50 Ref.\nVideos\nPretrained Image Layers (frozen)\nDomain Reliever\nMotion Module\nMotion LoRA\n<prompts>\n<prompts>\nImage Layers of Personalized T2I\nIterative Denoise\nMotion Modules of\nAnimateDiff\nTraining\nPipeline\nModule Insert\nzt\nzt\u22121\n(trainable at stage 1)\n(trainable at stage 2)\n(trainable at stage 3)\nFigure 2: Inference pipeline.\nThe core of our method is learning transferable mo-\ntion priors from video data, which can be applied to\npersonalized T2Is without specific tuning. As shown\nin Fig. 2, at inference time, our motion module (blue)\nand the optional MotionLoRA (green) can be directly\ninserted into a personalized T2I to constitute the an-\nimation generator, which subsequently generates ani-\nmations via an iterative denoising process.\nWe achieve this by training three components of Ani-\nmateDiff, namely domain adapter, motion module, and\nMotionLoRA. The domain adapter in Sec. 4.1 is only\nused in the training to alleviate the negative effects\ncaused by the visual distribution gap between the base\nT2I pre-training data and our video training data; the\nmotion module in Sec. 4.2 is for learning the motion\npriors; and the MotionLoRA in Sec. 4.3, which is optional in the case of general animation, is for\nadapting pre-trained motion modules to new motion patterns. Sec.4.4 elaborates on the training\n(Fig. 3) and inference of AnimateDiff.\n4.1\nALLEVIATE NEGATIVE EFFECTS FROM TRAINING DATA WITH DOMAIN ADAPTER\nDue to the difficulty in collection, the visual quality of publicly available video training datasets\nis much lower than their image counterparts. For example, the contents of the video dataset We-\nbVid (Bain et al., 2021) are mostly real-world recordings, whereas the image dataset LAION-\nAesthetic (Schuhmann et al., 2022) contains higher-quality contents, including artistic paintings\nand professional photography. Moreover, when treated individually as images, each video frame\n4\nPublished as a conference paper at ICLR 2024\nProj. In\nProj. Out \n(zero initialize)\nMotion Module (Temporal Transformer)\nSelf-\nAttention\n\u00d7N\n+\nPosition Enc.\nSelf-/Cross-Attention \nQ = WQz + Adapter(z)\nz = Wproj.z + Adapter(z)\n\u2026\nResNet Block\nPretrained Image Layers (frozen)\nDomain Adapter\nMotion Module\nMotionLoRA\nImage Layers of P\nIterative De\nzt\n(trainable at stage 1)\n(trainable at stage 2)\n(trainable at stage 3)\nPretrained \nImage Layers\n1. Alleviate Negative Effects\n<prompts>\nProj. In\nProj. Out \n(zero initialize)\nMotion Module (Temporal Transformer)\nSelf-\nAttention\n\u00d7N\n+\nPosition Enc.\nSelf-/Cross-Attention \nQ = WQz + Adapter(z)\nz = Wproj.z + Adapter(z)\n\u2026\nResNet Block\nSampled\nFrames\n2. Learn Motion Priors\n3. (optional) Adapt to New Patterns\nVideo\nDataset\n20~50 Ref.\nVideos\nsample\n<prompts>\n<prompts>\nPretrained \nImage Layers\nMSE Loss\nPretrained Image Layers (frozen)\nDomain Adapter\nMotion Module\nMotionLoRA\n(trainable at stage 1)\n(trainable at stage 2)\n(trainable at stage 3)\nFigure 3: Training pipeline of AnimateDiff. AnimateDiff consists of three training stages for the\ncorresponding component modules. Firstly, a domain adapter (Sec. 4.1) is trained to alleviate the\nnegative effects caused by training videos. Secondly, a motion module (Sec. 4.2) is inserted and\ntrained on videos to learn general motion priors. Lastly, MotionLoRA (Sec. 4.3) is trained on a few\nreference videos to adapt the pre-trained motion module to new motion patterns.\ncan contain motion blur, compression artifacts, and watermarks. Therefore, there is a non-negligible\nquality domain gap between the high-quality image dataset used to train the base T2I and the target\nvideo dataset we use for learning the motion priors. We argue that such a gap can limit the quality\nof the animation generation pipeline when trained directly on the raw video data.\nTo avoid learning this quality discrepancy as part of our motion module and preserve the knowledge\nof the base T2I, we propose to fit the domain information to a separate network, dubbed as\ndomain adapter. We drop the domain adapter at inference time and show that this practice helps\nreduce the negative effects caused by the domain gap mentioned above. We implement the domain\nadapter layers with LoRA (Hu et al., 2021) and insert them into the self-/cross-attention layers in\nthe base T2I, as shown in Fig. 3. Take query (Q) projection as an example. The internal feature z\nafter projection becomes\nQ = WQz + AdapterLayer(z) = WQz + \u03b1 \u00b7 ABT z,\n(4)\nwhere \u03b1 = 1 is a scalar and can be adjusted to other values at inference time (set to 0 to remove the\neffects of domain adapter totally). We then optimize only the parameters of the domain adapter on\nstatic frames randomly sampled from video datasets with the same objective in Eq. (2).\n4.2\nLEARN MOTION PRIORS WITH MOTION MODULE\nTo model motion dynamics along the temporal dimension on top of a pre-trained T2I, we must 1)\ninflate the 2-dimensional diffusion model to deal with 3-dimensional video data and 2) design a\nsub-module to enable efficient information exchange along the temporal axis.\nNetwork Inflation. The pre-trained image layers in the base T2I model capture high-quality con-\ntent priors. To utilize the knowledge, a preferable way for network inflation is to let these image\nlayers independently deal with video frames. To achieve this, we adopt a practice similar to recent\nworks (Ho et al., 2022b; Wu et al., 2023; Blattmann et al., 2023), and modify the model so that it\ntakes 5D video tensors x \u2208 Rb\u00d7c\u00d7f\u00d7h\u00d7w as input, where b and f represent batch axis and frame-\ntime axis respectively. When the internal feature maps go through image layers, the temporal axis\nf is ignored by being reshaped into the b axis, allowing the network to process each frame inde-\npendently. We then reshape the feature map to the 5D tensor after the image layer. On the other\nhand, our newly inserted motion module ignores the spatial axis by reshaping h, w into b and then\nreshaping back after the module.\nModule Design. Recent works on video generation have explored many designs for temporal mod-\neling. In AnimateDiff, we adopt the Transformer (Vaswani et al., 2017) architecture as our motion\nmodule design, and make minor modifications to adapt it to operate along the temporal axis, which\nwe refer to as \u201ctemporal Transformer\u201d in the following sections. We experimentally found this\ndesign is adequate for modeling motion priors. As illustrated in Fig. 3, the temporal Transformer\nconsists of several self-attention blocks along the temporal axis, with sinusoidal position encoding\nto encode the location of each frame in the animation. As mentioned above, the input of the motion\nmodule is the reshaped feature map whose spatial dimensions are merged into the batch axis. When\n5\nPublished as a conference paper at ICLR 2024\nwe divide the reshaped feature map along the temporal axis, it can be regarded as vector sequences\nwith length of f, i.e., {z1, ..., zf; zi \u2208 R(b\u00d7h\u00d7w)\u00d7c}. The vectors will then be projected and go\nthrough several self-attention blocks, i.e.\nzout = Attention(Q, K, V ) = Softmax(QKT /\u221ac) \u00b7 V,\n(5)\nwhere Q = W Qz, K = W Kz, and V = W V z are three separated projections. The attention mech-\nanism enables the generation of the current frame to incorporate information from other frames.\nAs a result, instead of generating each frame individually, the T2I model inflated with our motion\nmodule learns to capture the changes of visual content over time, which constitute the motion dy-\nnamics in an animation clip. Note that sinusoidal position encoding added before the self-attention\nis essential; otherwise, the module is not aware of the frame order in the animation. To avoid any\nharmful effects that the additional module might introduce, we zero initialize (Zhang et al., 2023)\nthe output projection layers of the temporal Transformer and add a residual connection so that the\nmotion module is an identity mapping at the beginning of training.\n4.3\nADAPT TO NEW MOTION PATTERNS WITH MOTIONLORA\nWhile the pre-trained motion module captures general motion priors, a question arises when we\nneed to effectively adapt it to new motion patterns such as camera zooming, panning and rolling,\netc., with a small number of reference videos and training iterations. Such efficiency is essential for\nusers who cannot afford expensive pre-training costs but would like to fine-tune the motion module\nfor specific effects. Here comes the last stage of AnimateDiff, also dubbed as MotionLoRA (Fig. 3),\nan efficient fine-tuning approach for motion personalization. Considering the architecture of the\nmotion module and the limited number of reference videos, we add LoRA layers to the self-attention\nlayers of the motion module in the inflated model described in Sec. 4.2, then train these LoRA layers\non the reference videos of new motion patterns.\nWe experiment with several shot types and get the reference videos via rule-based data augmenta-\ntion. For instance, to get videos with zooming effects, we augment the videos by gradually reducing\n(zoom-in) or enlarging (zoom-out) the cropping area of video frames along the temporal axis. We\ndemonstrate that our MotionLoRA can achieve promising results even with as few as 20 \u223c 50 ref-\nerence videos, 2,000 training iterations (around 1 \u223c 2 hours) as well as about 30M storage space,\nenabling efficient model tuning and sharing among users. Benefited by the low-rank property, Mo-\ntionLoRA also has the composition capability. Namely, individually trained MotionLoRA models\ncan be combined to achieve composed motion effects at inference time.\n4.4\nANIMATEDIFF IN PRACTICE\nWe elaborate on the training and inference here and put the detailed configurations in supplementary\nmaterials.\nTraining. As illustrated in Fig. 3, AnimateDiff consists of three trainable component modules to\nlearn transferable motion priors. Their training objectives are slightly different. The domain adapter\nis trained with the original objective as in Eq. (2). The motion module and MotionLoRA, as part\nof an animation generator, use a similar objective with minor modifications to accommodate higher\ndimension video data. Concretely, a video data batch x1:f\n0\n\u2208 Rb\u00d7c\u00d7f\u00d7h\u00d7w is first encoded into\nthe latent codes z1:f\n0\nframe-wisely via the pre-trained auto-encoder of SD. The latent codes are then\nnoised using the defined forward diffusion schedule as in Eq. (1)\nz1:f\nt\n= \u221a \u00af\u03b1tz1:f\n0\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03f51:f.\n(6)\nThe inflated model inputs the noised latent codes and corresponding text prompts and predicts the\nadded noises. The final training objective of our motion modeling module is:\nL = EE(x1:f\n0\n),y,\u03f51:f \u223cN (0,I ),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(z1:f\nt\n, t, \u03c4\u03b8(y))\u22252\n2\ni\n.\n(7)\nIt\u2019s worth noting that when training the domain adapter, the motion module, and the MotionLoRA,\nparameters outside the trainable part remain frozen.\n6\nPublished as a conference paper at ICLR 2024\nRCNZ Cartoon 3d\nTUSUN\nepiC Realism\nToonYou\na golden Labrador, nat-\nural lighting, ...\ncute Pallas\u2019s Cat walk-\ning in the snow, . . .\nphoto of 24 y.o woman,\nnight street, . . .\ncoastline,\nlighthouse,\nwaves, sunlight, . . .\nMeinaMix\nRealistic Vision\nMoXin\nOil painting\n1girl, white hair, purple\neyes, dress, petals, ...\na cyberpunk city street,\nnight time, . . .\na bird sits on a branch,\nink painting, . . .\nsunset, orange sky, fish-\ning boats, waves, . . .\nFigure 4: Qualitative Result. Each sample corresponds to a distinct personalized T2I. Best viewed\nwith Acrobat Reader. Click the images to play the animation clips.\nInference. At inference time (Fig. 2), the personalized T2I model will first be inflated in the same\nway discussed in Section 4.2, then injected with the motion module for general animation genera-\ntion, and the optional MotionLoRA for generating animation with personalized motion. As for the\ndomain adapter, instead of simply dropping it during the inference time, in practice, we can also in-\nject it into the personalized T2I model and adjust its contribution by changing the scaler \u03b1 in Eq. (4).\nAn ablation study on the value of \u03b1 is conducted in experiments. Finally, the animation frames can\nbe obtained by performing the reverse diffusion process and decoding the latent codes.\n5\nEXPERIMENTS\nWe implement AnimateDiff upon Stable Diffusion V1.5 and train motion module using the WebVid-\n10M (Bain et al., 2021) dataset. Detailed configurations can be found in supplementary materials.\n5.1\nQUALITATIVE RESULTS\nEvaluate on community models. We evaluated the AnimateDiff with a diverse set of representative\npersonalized T2Is collected from Civitai (2022). These personalized T2Is encompass a wide range\nof domains, thus serving as a comprehensive benchmark. Since personalized domains in these T2Is\nonly respond to certain \u201ctrigger words\u201d, we abstain from using common text prompts but refer to the\nmodel homepage to construct the evaluation prompts. In Fig. 4, we show eight qualitative results of\nAnimateDiff. Each sample corresponds to a distinct personalized T2I. In the second row of Figure\n1, we present the outcomes obtained by integrating AnimateDiff with MotionLoRA to achieve shot\ntype controls. The last two samples exhibit the composition capability of MotionLoRA, achieved\nby linearly combining the individually trained weights.\nCompare with baselines. In the absence of existing methods specifically designed for animating\npersonalized T2Is, we compare our method with two recent works in video generation that can be\nadapted for this task: 1) Text2Video-Zero (Khachatryan et al., 2023) and 2) Tune-a-Video (Wu\n7\nPublished as a conference paper at ICLR 2024\nTune-A-Video\nAnimateDiff\nT2V-Zero\nAnimateDiff\na raccoon is playing guitar, soft lighting, . . .\na horse galloping on the street, best quality, . . .\nFigure 5: Qualitative Comparison. Best viewed with Acrobat Reader. Click the images to play the\nanimation clips.\nTable 1: Quantitative comparison. A higher score indicates superior performance.\nMethod\nUser Study (\u2191)\nCLIP Metric (\u2191)\nText.\nDomain.\nSmooth.\nText.\nDomain.\nSmooth.\nText2Video-Zero\n1.620\n2.620\n1.560\n32.04\n84.84\n96.57\nTune-a-Video\n2.180\n1.100\n1.615\n35.98\n80.68\n97.42\nOurs\n2.210\n2.280\n2.825\n31.39\n87.29\n98.00\net al., 2023). We also compare AnimateDiff with two commercial tools: 3) Gen-2 (2023) for text-\nto-video generation, and 4) Pika Labs (2023) for image animation. The results are shown in Fig. 5.\n5.2\nQUANTITATIVE COMPARISON\nWe conduct the quantitative comparison through user study and CLIP metrics. The comparison\nfocuses on three key aspects: text alignment, domain similarity, and motion smoothness. The results\nare shown in Table 1. Detailed implementations can be found in supplementary materials.\nUser study. In the user study, we generate animations using all three methods based on the same\npersonalized T2I models. Participants are then asked to individually rank the results based on the\nabove three aspects. We use the Average User Ranking (AUR) as a preference metric where a higher\nscore indicates superior performance. Note that the corresponding prompts and images are provided\nfor reference for text alignment and domain similarity evaluation.\nCLIP metric. We also employed the CLIP (Radford et al., 2021) metric, following the approach\ntaken by previous studies (Wu et al., 2023; Khachatryan et al., 2023). When evaluating domain\nsimilarity, it is important to note that the CLIP score was computed between the animation frames\nand the reference images generated using the personalized T2Is.\n5.3\nABLATIVE STUDY\nDomain adapter. To investigate the impact of the domain adapter in AnimateDiff, we conducted\na study by adjusting the scaler in the adapter layers during inference, ranging from 1 (full impact)\nto 0 (complete removal). As illustrated in Figure 6, as the scaler of the adapter decreases, there is\nan improvement in overall visual quality, accompanied by a reduction in the visual content distri-\nbution learned from the video dataset (the watermark in the case of WebVid (Bain et al., 2021)).\nThese results indicate the successful role of the domain adapter in enhancing the visual quality of\nAnimateDiff by alleviating the motion module from learning the visual distribution gap.\nMotion module design. We compare our motion module design of the temporal Transformer with\nits full convolution counterpart, which is motivated by the fact that both designs are widely em-\nployed in recent works on video generation. We replace the temporal attention with 1D temporal\nconvolution and ensured that the two model parameters were closely aligned. As depicted in sup-\nplementary materials, the convolutional motion module aligns all frames to be identical but does not\nincorporate any motion compared to the Transformer architecture.\n8\nPublished as a conference paper at ICLR 2024\nalpha = 1.0\nalpha = 0.0\nalpha = 1.0\nalpha = 0.0\nscaler = 1.0\nalpha = 1.0\nalpha = 0.0\nalpha = 1.0\nalpha = 0.0\nscaler = 0.0\nFigure 6: Ablation on domain adapter. We adjust the scaler of the adapter from 1 to 0 to gradually\nremove its effects. In this figure, we show the first frame of the generated animation.\nrank=2 (\u223c1M)\nrank=128 (\u223c36M)\nN=5\nN=50\nN=1000\nFigure 7: Ablation on MotionLoRA\u2019s efficiency. Two samples on the left: with different network\nrank; Three samples on the right: with different numbers of reference videos.\nBest viewed with\nAcrobat Reader. Click the images to play the animation clips.\nEfficiency of MotionLoRA. The efficiency of MotionLoRA in AnimateDiff was examined in terms\nof parameter efficiency and data efficiency. Parameter efficiency is crucial for efficient model train-\ning and sharing among users, while data efficiency is essential for real-world applications where\ncollecting an adequate number of reference videos for specific motion patterns may be challenging.\nTo investigate these aspects, we trained multiple MotionLoRA models with varying parameter scales\nand reference video quantities. In Fig. 7, the first two samples demonstrate that MotionLoRA is ca-\npable of learning new camera motions (e.g., zoom-in) with a small parameter scale while maintain-\ning comparable motion quality. Furthermore, even with a modest number of reference videos (e.g.,\nN = 50), the model successfully learns the desired motion patterns. However, when the number of\nreference videos is excessively limited (e.g., N = 5), significant degradation in quality is observed,\nsuggesting that MotionLoRA encounters difficulties in learning shared motion patterns and instead\nrelies on capturing texture information from the reference videos.\n5.4\nCONTROLLABLE GENERATION.\ncity street, neon, fog, closeup portrait photo of young\nwoman in dark clothes, . . .\nFigure 8: Controllable generation. Best viewed\nwith Acrobat Reader. Click the images to play the\nanimation clips.\nThe separated learning of visual content and\nmotion priors in AnimateDiff enables the di-\nrect application of existing content control\napproaches for controllable generation.\nTo\ndemonstrate this capability, we combined An-\nimateDiff with ControlNet (Zhang et al., 2023)\nto control the generation with extracted depth\nmap sequence. In contrast to recent video edit-\ning techniques (Ceylan et al., 2023; Wang et al.,\n2023a) that employ DDIM (Song et al., 2020)\ninversion to obtain smoothed latent sequences,\nwe generate animations from randomly sam-\npled noise. As illustrated in Figure 8, our re-\nsults exhibit meticulous motion details (such as hair and facial expressions) and high visual quality.\n6\nCONCLUSION\nIn this paper, we present AnimateDiff, a practical pipeline directly turning personalized text-to-\nimage (T2I) models for animation generation once and for all, without compromising quality or\n9\nPublished as a conference paper at ICLR 2024\nlosing pre-learned domain knowledge. To accomplish this, we design three component modules in\nAnimateDiff to learn meaningful motion priors while alleviating visual quality degradation and en-\nabling motion personalization with a lightweight fine-tuning technique named MotionLoRA. Once\ntrained, our motion module can be integrated into other personalized T2Is to generate animated\nimages with natural and coherent motions while remaining faithful to the personalized domain. Ex-\ntensive evaluation with various personalized T2I models also validates the effectiveness and gener-\nalizability of our AnimateDiff and MotionLoRA. Furthermore, we demonstrate the compatibility of\nour method with existing content-controlling approaches, enabling controllable generation without\nincurring additional training costs. Overall, AnimateDiff provides an effective baseline for person-\nalized animation and holds significant potential for a wide range of applications.\n7\nETHICS STATEMENT\nWe strongly condemn the misuse of generative AI to create content that harms individuals or spreads\nmisinformation. However, we acknowledge the potential for our method to be misused since it\nprimarily focuses on animation and can generate human-related content. It is also important to\nhighlight that our method incorporates personalized text-to-image models developed by other artists.\nThese models may contain inappropriate content and can be used with our method.\nTo address these concerns, we uphold the highest ethical standards in our research, including ad-\nhering to legal frameworks, respecting privacy rights, and encouraging the generation of positive\ncontent. Furthermore, we believe that introducing an additional content safety checker, similar to\nthat in Stable Diffusion (Rombach et al., 2022), could potentially resolve this issue.\n8\nREPRODUCIBILITY STATEMENT\nWe provide comprehensive implementation details for the training and inference of our method in\nsupplementary materials, aiming to enhance the reproducibility of our approach. We also make both\nthe code and pre-trained weights open-sourced to facilitate further investigation and exploration.\nACKNOWLEDGEMENT\nThis project is funded in part by Shanghai AI Laboratory (P23KS00020, 2022ZD0160201), CUHK\nInterdisciplinary AI Research Institute, and the Centre for Perceptual and Interactive Intelligence\n(CPIl) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK.\nREFERENCES\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 1728\u20131738, 2021.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 22563\u201322575, 2023.\nDuygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mitra. Pix2video: Video editing using image\ndiffusion. arXiv preprint arXiv:2303.12688, 2023.\nCivitai. Civitai. https://civitai.com/, 2022.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin Neural Information Processing Systems, 34:8780\u20138794, 2021.\n10\nPublished as a conference paper at ICLR 2024\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers.\nAdvances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\nZiyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image\ngeneration via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022.\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\nmanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023.\nRobert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences,\n3(4):128\u2013135, 1999.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nRinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nDesigning an encoder for fast personalization of text-to-image models.\narXiv preprint\narXiv:2302.12228, 2023.\nGen-2. Gen-2: The next step forward for generative ai. https://research.runwayml.\ncom/gen2/, 2023.\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696\u201310706, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33:6840\u20136851, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022b.\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pre-\ntraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nHugging Face. Huggingface. https://huggingface.co/, 2022.\nXuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou,\nHuisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization\nwith text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023.\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion mod-\nels are zero-shot video generators. IEEE International Conference on Computer Vision (ICCV),\n2023.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,\n114(13):3521\u20133526, 2017.\n11\nPublished as a conference paper at ICLR 2024\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 1931\u20131941, 2023.\nWei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang, Guohao Li, Zhanpeng Wang, Zhifan Feng,\nQiaoqiao She, Yajuan Lyu, et al. Upainting: Unified text-to-image diffusion generation with\ncross-modal guidance. arXiv preprint arXiv:2210.16031, 2022.\nHaoming Lu, Hazarapet Tunanyan, Kai Wang, Shant Navasardyan, Zhangyang Wang, and\nHumphrey Shi. Specialist diffusion: Plug-and-play sample-efficient fine-tuning of text-to-image\ndiffusion models to learn any unseen style.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 14267\u201314276, 2023.\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,\nJingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 10209\u201310218, 2023.\nAniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, and Jun-Yan Zhu.\nText-guided synthesis of eulerian cinemagraphs, 2023.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for\nediting real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 6038\u20136047, 2023.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nPika Labs. Pika labs. https://www.pika.art/, 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation, 2015.\n12\nPublished as a conference paper at ICLR 2024\nLudan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin\nJin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and\nvideo generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 10219\u201310228, 2023.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500\u2013\n22510, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b:\nAn open large-scale dataset for training next generation image-text models.\narXiv preprint\narXiv:2210.08402, 2022.\nJing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung.\nInstantbooth: Personalized text-to-image\ngeneration without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\ndata. arXiv preprint arXiv:2209.14792, 2022.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nWen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-\nshot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599,\n2023a.\nYaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan\nHe, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian\nChen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video\ngeneration with cascaded latent diffusion models, 2023b.\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. IEEE International Conference on Computer Vision (ICCV), 2023.\nShengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Drag-\nnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv\npreprint arXiv:2308.08089, 2023a.\nShengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan\nYang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely\nlong video generation. arXiv preprint arXiv:2303.12346, 2023b.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image\ndiffusion models. IEEE International Conference on Computer Vision (ICCV), 2023.\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo:\nEfficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022a.\nYufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu,\nJinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17907\u2013\n17917, 2022b.\n13\n"
  },
  {
    "title": "Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation",
    "link": "https://arxiv.org/pdf/2307.03869.pdf",
    "upvote": "20",
    "text": "Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation\nAditya Sanghi\nPradeep Kumar Jayaraman\u2021\nArianna Rampini\u2021\nJoseph Lambourne\nHooman Shayani\nEvan Atherton\nSaeid Asgari Taghanaki\nAutodesk Research\nFigure 1: Sketch-A-Shape is a zero-shot sketch-to-3D generative model. Here we show how our method can generalize\nacross voxel, implicit, and CAD representations and synthesize consistent 3D shapes from a variety of inputs ranging from\ncasual doodles to professional sketches with different levels of ambiguity.\nAbstract\nSignificant progress has recently been made in creative\napplications of large pre-trained models for downstream\ntasks in 3D vision, such as text-to-shape generation. This\nmotivates our investigation of how these pre-trained models\ncan be used effectively to generate 3D shapes from sketches,\nwhich has largely remained an open challenge due to the\nlimited sketch-shape paired datasets and the varying level\nof abstraction in the sketches. We discover that condition-\ning a 3D generative model on the features (obtained from a\nfrozen large pre-trained vision model) of synthetic render-\nings during training enables us to effectively generate 3D\nshapes from sketches at inference time. This suggests that\nthe large pre-trained vision model features carry semantic\nsignals that are resilient to domain shifts, i.e., allowing us\nto use only RGB renderings, but generalizing to sketches\nat inference time. We conduct a comprehensive set of ex-\nperiments investigating different design factors and demon-\nstrate the effectiveness of our straightforward approach for\ngeneration of multiple 3D shapes per each input sketch re-\ngardless of their level of abstraction without requiring any\npaired datasets during training.\n1. Introduction\nThroughout history, humans have used drawings and\nother visual representations to communicate complex ideas,\nconcepts, and information.\nAs hand-drawn sketches have a high level of abstrac-\ntion, they allow unskilled artists or even young children to\nconvey semantic information about 3D objects [21], while\nproviding trained professionals with a way to quickly ex-\npress important geometric and stylistic details of a 3D de-\nsign. The ability to create 3D models which can capture the\nessence of simple doodles while accurately reproducing 3D\nshapes described by concept design sketches, will make 3D\nmodelling more accessible to the general public, while al-\nlowing designers to rapidly explore many different design\n1\narXiv:2307.03869v1  [cs.CV]  8 Jul 2023\nideas and create virtual models that more accurately reflect\nthe shape, size, and characteristics of real-world objects and\nenvironments.\nPrevious studies have endeavored to employ deep learn-\ning techniques in generating 3D shapes from sketches\n[39, 20, 16, 35], yet there are several limitations that hin-\nder their widespread application. Firstly, there is a lack of\n(Sketch, 3D shape) paired data at a large scale which forces\nmost methods to be trained on synthetic datasets or data col-\nlected on only few categories. Even when a small number\nof categories of paired sketch-shape data has been collected\n[35], current methods fail to generalize to different levels\nof abstractions in the sketches, ranging from casual doo-\ndles to detailed professional drawings. Finally, most of the\npresent methods incorporate strong inductive biases, such\nas view information [79], differentiable rendering [20] and\ndepth estimation [39, 16], thereby constraining their gener-\nalizability across 3D representations.\nTo overcome the challenge of limited availability of\npaired data, a potential solution is to use prior knowledge\nencoded in large pre-trained image-text models. Recently,\nthese large pre-trained models have been successfully ap-\nplied to the 3D domain in creative ways, such as guid-\ning the optimization of differentiable 3D representations\n[28, 56, 38] or to generate 3D shapes from text prompts us-\ning interchangeability of text-image embeddings [61, 62],\nor using them for representation learning [78, 63].\nIn this paper, we introduce a straightforward yet effective\napproach called Sketch-A-Shape, for generating 3D shapes\nfrom sketches in a zero-shot setting using pre-trained vision\nmodels. Our method is based on the idea that 3D shape ren-\ndering features derived from large-scale pre-trained models\n(such as CLIP [57] and DINOv2 [53]) possess robust lo-\ncal semantic signals that can withstand domain shifts from\nrenderings to sketches. In Sketch-A-Shape, we first train a\nVQ-VAE to acquire shape embeddings. Following this, a\nmasked transformer is trained to model the distribution of\nshape embeddings conditioned on local semantic features\nfrom an image encoder that is pre-trained and frozen. Dur-\ning inference, the masked transformer is conditioned on lo-\ncal semantic features of the sketch instead, in order to pro-\nduce the 3D shape. Our findings suggest that with some ar-\nchitectural design choices, this straightforward method en-\nables us to generate several 3D shapes that can generalize\nacross sketches of varying complexities.\nTo sum up, we make the following contributions:\n\u2022 We propose Sketch-A-Shape, the first zero-shot ap-\nproach for sketch-to-3D generation, leveraging large-\nscale pre-trained models to outdo the need of paired\nsketch-3D dataset.\n\u2022 We experimentally show the generalization capability\nof our method among various datasets (section 4) with\ndifferent levels of sketch abstraction, going from sim-\nple doodles to professional sketches.\n\u2022 We conduct thorough experiments to examine the dif-\nferent components of Sketch-A-Shape that contribute\nto the success of zero-shot shape generation via sketch.\n2. Related Work\n3D Generative Models.\nSignificant progress has been\nmade in the field of generative models for the creation of\n3D shapes in various formats such as voxels [70, 9, 66, 30],\nCAD [71, 29, 36, 72], implicit representations [42, 7, 54],\nmeshes [48, 19], and point clouds [49, 3, 75, 37, 74, 34].\nRecent research on 3D generative models has focused pri-\nmarily on the development of generative models based on\nVQ-VAE [47, 8, 73, 77, 62, 72], GAN[4, 65, 17], or diffu-\nsion models [80, 40, 50, 26]. The present study concentrates\non connecting the sketch modality with 3D shapes across\nthree different 3D representations: voxels, CAD, and im-\nplicit representation. Although our approach is based on\nVQ-VAE, it can be easily extended to GAN or diffusion-\nbased generative models.\n3D Zero-Shot Learning. Large pre-trained language and\n2D vision models have been creatively used in several\ndownstream 3D vision tasks. Initial works focused on us-\ning vision-text models such as CLIP [57] for 3D shape gen-\neration using text [61], optimizing nerfs [28], deforming\nmeshes [44], stylizing meshes [46] and animating avatars\n[25] . More recently, text-to-image models such as Stable\nDiffusion [59] and Imagen [60], have been used for text-to-\nshape generation [56, 38], single-view reconstruction [76],\nand adding texture to 3D shapes [43]. To the best of our\nknowledge, our work is the first to explore zero-shot 3D\nshape generation from sketches by leveraging a pre-trained\nmodel.\n3D Shape Generation from Sketch. Several supervised\nlearning methods have been used to generate 3D shapes\nfrom sketches. Works such as [39] use a neural net to esti-\nmate depth and normals from a set of viewpoints for a given\nsketch, which are then integrated into a 3D point cloud.\n[10] proposes to use a CNN to predict the initial shape and\nthen refine the shape using novel viewpoints using another\nneural network. Another work [31] represent the 3D shape\nand its occluding contours in a joint VAE latent space dur-\ning training which enables them to retrieve a sketch dur-\ning inference and generate a 3D shape. Sketch2Mesh [20]\nuses an encoder-decoder architecture to represent and re-\nfine a 3D shape to match the target external contour us-\ning a differentiable render. Methods such as [69, 79] em-\nploy a domain adaption network between unpaired sketch\nand rendered image data to boost performance on abstract\nhand-drawn sketches. To address the ambiguity problem of\nsketches, [79] introduces an additional encoder-decoder to\nextract separate view and shape sketch features, while [16]\nproposes a sketch translator module to fully exploit the spa-\ntial information in a sketch and generate suitable features\nfor 3D shape prediction. Recently, [35] trains a diffusion\nmodel for generation of 3D point clouds conditioned on\nsketches using a multi-stage training, and fine-tuning tech-\nnique. However, we take the novel approach of not training\non paired shape-sketch data at all and instead rely on the ro-\nbustness of the local semantic features from a frozen large\npre-trained image encoder such as CLIP.\n3. Method\nOur approach strives to generate 3D shapes from\nsketches of different complexities, solely employing syn-\nthetic renderings, and without the requirement of a paired\ndataset of sketches and shapes. The training data consists\nof 3D shapes, each denoted by S, which can be repre-\nsented as a voxel grid, implicit (e.g. occupancy), or CAD,\nand their r multi-view renderings (I1:r). Our approach in-\nvolves two training stages. In the first stage, the shapes\nare transformed into discrete sequences of indices (shape\nembeddings), denoted by Z, using a discrete auto-encoder\nframework [52]. In the second stage, the distribution of\nthese indices is modeled using a transformer-based gener-\native model that is conditioned on features of shape ren-\nderings obtained from a frozen pre-trained model. These\nshape rendering features are a grid of local features from\nthe frozen pre-trained model which are converted into a se-\nquence of local features then conditioned to the transformer\nthrough a cross-attention mechanism. During inference, we\nuse an iterative decoding scheme [6] to generate the shape\nindices iteratively based on features of the sketch. Once the\nshape indices are generated, we can use the decoder of the\nauto-encoder to generate the 3D shape. The overall process\nis illustrated in Figure 2 .\n3.1. Stage 1: Training Discrete Auto-encoder\nIn the first stage, we use an auto-encoder framework\nto capture the shape distribution into a compressed se-\nquence of discrete indices (shape embeddings) among vari-\nous modalities. To achieve this, we opt for the Vector Quan-\ntized Variational Auto-encoder (VQ-VAE) architecture [52]\nwhich efficiently models the 3D shape in a compressed la-\ntent space, circumventing posterior collapse and enabling\nthe generation of high-quality 3D shapes. The dataset of\n3D shapes S, are transformed using an encoder, E(.), into\na sequence of discrete indices Z, pointing to a shape dictio-\nnary, which their distributions are then modeled in stage 2\nusing a transformer-based generative model. This is shown\nbelow:\nZ = V Q(E(S)),\nS\n\u2032 = D(Z)\n(1)\nThe shape S\n\u2032 is then generated from Z using a decoder,\nD(.), with a reconstruction loss Lrec(S, S\u2032). We also use\nthe commitment loss [52] to encourage encoder output E(.)\ncommits to an embedding in the shape dictionary, and expo-\nnential moving average strategy [58, 81] to encourage dic-\ntionary enteries to gradually be pulled toward the encoded\nfeatures. When dealing with voxel representation, we lever-\nage a 3D convolution based on the ResNet architecture [23]\nfor both the encoder and decoder network. Whereas with\nimplicit representation, we rely on a ResNet-based encoder\nand an up-sampling process for the decoder that generates\na higher resolution volume, which is then queried locally\nto obtain the final occupancy [42, 55]. In the case of CAD\nrepresentation, we use the SkexGen VQ-VAE architecture\n[72]. More details of the architectures are provided in the\nsupplementary material.\n3.2. Stage 2: Masked Transformer\nThe goal of stage 2 is to train a prior model which can\neffectively generate shape indices conditioned on a sketch\nat inference time. We achieve this by modelling the se-\nquence of discrete indices (shape embedding Z), produced\nfrom stage 1, using a conditional generative model. We use\na bi-directional transformer [6] based network which is con-\nditioned on the features of the synthetic 3D renderings using\na cross-attention mechanism. During training, we randomly\nmask a fraction of shape indices with a special mask token\n[6] to produce Zmsk. The training objective then becomes\nhow to fully unmask the masked indices using the help of\nthe provided conditional information. The training objec-\ntive is to minimize:\nLmask = \u2212\nE\nZ,C\u2208D[log p(Z|Zmsk, C)]\n(2)\nHere, C represents the conditional information from a\ngiven shape S which are obtained from the multi-view im-\nage renderings of the 3D shape. At each iteration, we ran-\ndomly sample a view to render an image of the 3D shape,\nwhich is then converted to local features using a locked pre-\ntrained model. The choice of pre-trained model is an impor-\ntant design criteria which we investigate thoroughly in Sec-\ntion 4.4, and find that using large models trained on diverse\ndata produces the most robust semantic local features which\nallow domain shift from synthetic renderings to sketches.\nThe local features sequence can be obtained from dif-\nferent parts of the pre-trained network, which we investi-\ngate in Section 4.5. Our findings indicate that utilizing the\nfeature grid output of the deeper layers in the pre-trained\nmodels yields better results. This is because deeper layers\ngenerate more semantic features, and the grid structure of\nthe feature preserves its local properties. We convert this\ngrid into a sequence using a mapping network compris-\ning of several MLP layers. Finally, we take features ob-\nFigure 2: An overview of the Sketch-A-Shape method. The top row shows the two training stages. The bottom row shows\nhow 3D shape is generated via sketch at inference time.\ntained and add learnable positional encoding before apply-\ning cross-attention with the shape indices\u2019 features at each\ntransformer layer. The choice of conditioning is also an im-\nportant design choice which we discuss in Section 4.6. Ad-\nditionally, we replace the local features sequence with a null\nembedding sequence 5% of the time to allow for classifier-\nfree guidance during inference.\n3.3. Inference\nDuring the generation phase, we first convert the sketch\ninto a sequence of local features using the same frozen pre-\ntrained model utilized during training.\nThese local fea-\ntures are semantically robust and serve as the conditioning\nquery for the transformer. We employ an iterative decoding\nscheme with a cosine schedule, similar to the one proposed\nin Mask-GIT [6]. The process begins with a completely\nmasked set of indices, which are gradually unmasked in\nparallel using the conditional information provided by the\nlocal features sequence from the sketch. At each time step,\nthe transformer predicts the complete unmasked shape se-\nquence, of which a specific fraction of the highest confi-\ndence masked tokens are accepted. These selected tokens\nare designated as unmasked for the remaining steps, while\nthe rest of the tokens are reset to masked, except for the al-\nready unmasked tokens from the previous steps. For each\ntime step, we also apply classifier-free guidance [24] with\na guidance scale of 3. This process continues until all the\ntokens are unmasked. Finally, the completely unmasked to-\nkens are converted into the 3D object using the shape de-\ncoder trained in stage 1. It is worth noting that we can\nrestart the same process multiple times to generate differ-\nent 3D shapes for the same sketch query.\n4. Experiments\nIn this section, we present the results of our experiments\nevaluating the accuracy and fidelity of the generated out-\nput produced by our model. We conducted each experiment\nthree times for each metric and reported the average result\nfor each. The experimental setup details are provided in the\nsupplementary material with additional results that may be\nof interest.\nTraining Dataset. Our experimentation utilizes two sub-\nsets of the ShapeNet(v2) dataset [5].\nThe first subset,\nShapeNet13, consists of 13 categories from ShapeNet,\nwhich were also employed in previous studies [9, 42].\nIn line with Sketch2Model [79], we adopt the same\ntrain/val/test partition. The second subset, ShapeNet55, in-\ncludes all 55 categories of ShapeNet and we follow the same\nsplit as [62]. We use the DeepCAD [71] dataset to train our\nCAD model.\nEvaluation Sketch Dataset. One advantage of our method\nis that it\u2019s not trained on paired (shape, sketch) datasets.\nTherefore, to comprehensively evaluate its performance, we\ntested it on various sketch datasets that range from pro-\nfessional to non-expert sketches. Specifically, we utilized\nthe ShapeNet-Sketch dataset [79], which comprises 1300\nfree-hand sketches across ShapeNet13. In addition, we em-\nployed the ImageNet-Sketch dataset [67], which contains\n50 sketch images for 1000 ImageNet classes obtained from\nGoogle, encompassing a range of professional to non-expert\nsketches.\nMoreover, we utilized the TU-Berlin Sketch\ndataset [13], which includes 20,000 non-expert sketches of\n250 object categories. Lastly, QuickDraw Dataset [1, 21]\nis a collection of 50 million drawings across 345 cate-\ngories, contributed by players of the game Quick, Draw!\n[32]. ImageNet-Sketch, TU-Berlin Sketch, and QuickDraw\ndatasets also lack ground-truth 3D models, and we only uti-\nlized the categories of ShapeNet for these datasets. To eval-\nuate our CAD model we use synthetic edge map sketches\nbut don\u2019t train the model using edge maps as augmentation.\nEvaluation Metrics. To evaluate our method on different\nsketch datasets we use two metrics: classification accuracy\nand human evaluation which are outlined below.\n1. Classifier Accuracy. As we are dealing with sketch\ndata that lacks ground-truth 3D models, we use the\nAccuracy (Acc) metric to ensure that the generated\nshape for a given sketch corresponds to its category.\nTo achieve this, we employ a pre-trained shape clas-\nsifier, as implemented in [61, 62]. We use this met-\nric for all datasets: ImageNet-Sketch [67], TU-Berlin\n[13], ShapeNet-Sketch [79], and QuickDraw [1]. We\nrefer to this metric as IS-Acc, TU-Acc, SS-Acc, and\nQD-Acc, respectively. As our method can generate\nmultiple shape per sketch query, we report the mean\nacross 5 sampled shapes for a given sketch query.\n2. Human Perceptual Evaluation. We also use Amazon\nSageMaker Ground Truth and crowd workers from the\nMechanical Turk workforce [45] to evaluate how well\nour generated 3D models preserve important geomet-\nric and stylistic details from the sketches.\n4.1. Qualitative Results\nIn Figure 3, we visualize sample generated 3D shapes in\ndifferent representations such as voxel, implicit, and CAD\nfrom sketches of different domains. As shown, our method\nperforms reasonably well on different types of sketches\n(from simple to professional drawings), particularly when\nthere is ambiguity (such as the view angle of drawings)\ngiven the nature of 2D sketches.\n4.2. Human Perceptual Evaluation\nIn addition to generating shapes in the same broad ob-\nject category as abstract hand drawn sketches, our method\nis also able to incorporate geometric and stylistic details\nfrom a sketch or concept design into the final 3D model. To\ndemonstrate this quantitatively, we run a human perceptual\nevaluation using Amazon SageMaker Ground Truth and\ncrowd workers from the Mechanical Turk workforce [45].\nWe evaluate 691 generated models, conditioned on sketches\nfrom TU-Berlin [13], ShapeNet-Sketch [79], ImageNet-\nSketch [67] and QuickDraw [21].\nThe human evaluation is posed as a two-alternative\nforced choice study [14]. The crowd workers are shown\nimages with a sketch on the left hand side and two images\nof generated 3D models on the right hand side. An exam-\nple is shown in Figure 4. One of the generated models was\nconditioned on the sketch shown, while the other was condi-\ntioned on a randomly selected sketch from the same object\ncategory. The crowd workers are asked the question \u201cWhich\nof the 3D models on the right hand side best matches the\nsketch on the left hand side?\u201d. The study is designed to\nmeasure the extent to which humans perceive our generated\n3D models as preserving the shape and stylistic details pre-\nsented in the sketch, as opposed to simply creating a model\nfrom the same object category.\nWe show each image to 7 independent crowd workers\nand count the number of images for which 4 or more of\nthem correctly identify the 3D model which was condi-\ntioned on the sketch. The results are shown in Table 1. On\naverage 71.1% of our generated 3D models are correctly\nidentified by a majority of the crowd workers. We note that\nthe sketches in TU-Berlin and ShapeNet-Sketch give rise\nto generations which were easier for the crowd workers to\nidentify, with 74.9% and 73.1% being selected correctly.\nWhile these sketches often have a high level of abstrac-\ntion, they communicate enough detail about the shape for\nour method to create distinctive 3D models which humans\ncan identify. While ImageNet-Sketch contains superior art-\nwork, often with shading, shadows and other cues to the\n3D nature of the shapes, many of the pictures contain full\nscenes with backgrounds and additional superfluous details.\nThis makes the generation of single objects more challeng-\ning, which is reflected by the fact that only 68.1% are cor-\nrectly identified by the crowd workers. We note qualita-\ntively that in cases where shaded sketches do not contain\nbackgrounds or additional clutter the generated results look\nbetter, indicating the utility of our method for quickly gen-\nerating 3D models from concept designs. The sketches in\nthe QuickDraw dataset are sourced from the from the online\ngame Quick, Draw! [32], in which contributors are asked to\ndrawn a shape in less than 20 seconds. QuickDraw is the\nmost abstract and noisy sketch dataset, with many of the\nsketches being drawn with a computer mouse. While our\nmethod typically generates 3D shapes of the correct cate-\ngory, only 67.9% of the generations are correctly identified\nby the crowd workers.\n4.3. Comparison with Supervised Models\nAs there is currently a lack of zero-shot methodologies\nfor generating shapes from sketches, we compared our re-\nsults to those of a supervised approach called Sketch2Model\n[79], which was trained on a dataset of paired sketch-3D\nshapes.\nWe evaluated both methods using our classifier\naccuracy metric, and the results are presented in Table 2.\nOur model was not exposed to any sketch-3D pairings dur-\ning training, but it displays superior generation capabili-\nties compared to Sketch2Model across different datasets.\nFigure 3: Generated 3D shapes from sketches of different domains. Rows 1\u20135, and 6 refer to voxel, and implicit representa-\ntions, respectively. The last three rows show different 3D samples generated per sketch in voxel, implicit, and CAD formats.\nFigure 4: An example of the images shown to the Mechani-\ncal Turk crowd workers. The question is \u201cWhich of the 3D\nmodels on the right hand side best matches the sketch on the\nleft hand side?\u201d and the options are \u201cTop\u201d and \u201cBottom\u201d.\nDataset\n% correctly identified\nAll\n71.1%\nTU-Berlin\n74.9%\nShapeNet-Sketch\n73.1%\nImageNet-Sketch\n68.1%\nQuickDraw\n67.9%\nTable 1: Results of the human perceptual evaluation by\ndataset. We show the percentage of generated models for\nwhich the majority of the 7 crowd workers reviewing each\nimage correctly identified the 3D model conditioned on the\nsketch.\nMethod\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\nS2M [79]\n27.4\n19.8\n26.0\n12.0\nOurs\n58.8\n81.5\n79.7\n74.2\nTable 2: Classification accuracy comparison with the super-\nvised method Sketch2Model.\nWe attribute this difference in performance to several fac-\ntors. Firstly, we believe that Sketch2Model may be more\neffective for single-category training rather than for the 13\ncategories in the ShapeNet dataset. Additionally, because\nSketch2Model is a supervised method, it was not exposed\nto out-of-distribution sketches during training, which may\nhave caused its performance to deteriorate. We provide fur-\nther details and qualitative comparison with Sketch2Model\nand other supervised methods in the supplementary mate-\nrial.\n4.4. Investigating Pre-Trained Models\nThis section involves an extensive study of several pre-\ntrained models that are open-sourced and trained on dif-\nferent datasets. The results are present in Table 3. There\nare 3 major things we investigate through this experiment.\nFirst, we investigate the importance of utilizing local grid\nfeatures of pre-trained models. Typically, pre-trained mod-\nels possess a global projection vector that is employed for\ndownstream tasks like classification. We compare the effi-\ncacy of conditioning our generative model with the global\nprojection vector (row 1) versus the local grid features (row\n2). Our findings demonstrate that leveraging local grid fea-\ntures yields better performance compared to the global pro-\njection vector for most of the datasets. Furthermore, even\nfrom a visual standpoint, we observe that local grid features\npreserve more local details. It is worth noting that these\naccuracies are further improved by utilizing classifier-free\nguidance (CFG), as illustrated in row 3.\nNext, we investigate the role of size of pre-trained mod-\nels and find that increasing the size of the same class of\npre-trained model, despite being trained on the same data,\nresults in better zero-shot performance. This phenomenon\nis evident in the case of the ViT-based [12] CLIP model,\nwhere upgrading from the B-32 model to the L-14 model\nyields a significant improvement in performance. This trend\nis also observed in the ResNet-based [23] models. Inter-\nestingly, it is worth mentioning that the ResNet-based [23]\nmodels perform worse than their corresponding ViT-based\n[12] CLIP models. This could be attributed to the ResNet\nmodels\u2019 emphasis on high-frequency, textural features [18].\nFinally, we explore how different datasets impact the\ntraining of these models.\nOur findings indicate that the\nmodel\u2019s performance remains comparable when trained\non extensive datasets such as LAION-2B [64], DINOv2\nDataset [53] or OpenAI internal dataset [57].\nHowever,\nwhen we reduce the dataset size significantly, such as in the\ncase of the masked autoencoder [22] trained on 400 times\nless data from ImageNet [11], its performance significantly\ndeclines. Despite being trained on the reconstruction objec-\ntive, we believe that the masked autoencoder\u2019s performance\ndrop is primarily due to the significantly reduced dataset\nsize, as it still performs reasonably well on this task. Addi-\ntionally, it is important to highlight that language supervi-\nsion is unnecessary to acquire resilient features from exten-\nsive pre-trained models, as demonstrated by the outcomes\nof DINOv2.\n4.5. Accuracy across Different Layers\nIn this experiment, we explore the optimal layer of the\nvision transformer (L-14 model) from which we extract the\nlocal conditioning features. Table 4 summarizes our find-\nings. We note that as we delve deeper into the vision trans-\nformer architecture, the features extracted from the deeper\nResolution\nCFG\nNetwork\nDataset\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\n1 x 512\n\u00d7\nB-32 [57]\nOpenAI [57]\n36.65\n61.14\n62.86\n55.96\n50 x 768\n\u00d7\nB-32 [57]\nOpenAI [57]\n37.85\n63.25\n63.78\n52.79\n50 x 768\n\u2713\nB-32 [57]\nOpenAI [57]\n38.86\n65.86\n67.36\n49.19\n197 x 768\n\u2713\nB-16 [57]\nOpenAI [57]\n38.47\n71.66\n70.72\n61.10\n257 x 1024\n\u2713\nL-14 [57]\nOpenAI [57]\n55.45\n77.15\n74.53\n69.06\n144 x 3072\n\u2713\nRN50x16 [57]\nOpenAI [57]\n34.61\n70.81\n58.82\n59.00\n196 x 4096\n\u2713\nRN50x64 [57]\nOpenAI [57]\n46.93\n73.79\n59.41\n64.19\n257 x 1024\n\u2713\nOpen-L-14 [27]\nLAION-2B [64]\n54.63\n77.60\n69.03\n68.35\n256 x 1024\n\u2713\nDINO-L-14 [53]\nDINOv2 [53]\n39.73\n71.12\n72.10\n55.94\n197 x 1024\n\u2713\nMAE-L [22]\nImageNet [11]\n19.31\n30.52\n38.79\n26.65\n257 x 1280\n\u2713\nMAE-H [22]\nImageNet [11]\n18.70\n31.63\n37.47\n31.42\nTable 3: Ablations on using different pre-trained models. Resolution indicates the size of the local features obtained from\npre-trained model whereas CFG stands for classifier free guidance.\nL-14 Layer\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\n1\n17.16\n22.21\n24.16\n11.13\n7\n16.93\n25.03\n33.24\n12.52\n13\n29.43\n51.50\n59.64\n40.98\n19\n54.03\n75.77\n74.45\n63.83\n24\n55.45\n77.15\n74.53\n69.06\nTable 4: Accuracy across 24 layers of L-14 architecture.\nlayers contain more significant semantic information lead-\ning to higher accuracy. Moreover, this indicates that the\nmodel maintains the positional correlation between patches\ninstead of treating them as global information repositories\nas visually we can see local semantic generation.\n4.6. Design Choices for Conditioning\nTable 5 presents our investigation into the impact of the\nmapping network\u2019s size and the attention mechanism used\nfor conditioning the image features to the transformer. Our\nresults show that incorporating a mapping layer does en-\nhance the model\u2019s performance, with the optimal number of\nMLP layers being two. Furthermore, our findings suggest\nthat cross-attention with a learnable positional embedding\nis the most effective conditioning mechanism, as evidenced\nby the deteriorated performance on removing positional em-\nbedding or using self attention as shown in the last two rows\nof the table.\n4.7. Effect of Augmentation\nIn our final investigation, we explore whether the addi-\ntion of data augmentation improves the accuracy of shape\ngeneration across datasets. The results are summarized in\nTable 6. We make two noteworthy observations. Firstly,\neven without data augmentation, our method performs rel-\natively well, indicating the robustness of pre-trained mod-\nM\nAtt. type\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\n0\ncross\n55.45\n77.15\n74.53\n69.06\n1\ncross\n54.69\n77.91\n77.19\n69.17\n2\ncross\n57.52\n79.41\n78.18\n70.53\n3\ncross\n52.10\n76.85\n76.85\n68.03\n2\nno pos\n55.81\n78.75\n78.02\n71.05\n2\nself\n49.33\n76.97\n75.42\n69.07\nTable 5: Ablation on design choices for conditioning. M is\nthe number of MLP layers in mapping network (0 meaning\njust a linear layer). Note, self stands for self attention, cross\nfor cross attention with positional embedding and no pos\nstands for cross attention with no positional embedding.\nAugmentation\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\nNo Aug\n57.52\n79.41\n78.18\n70.53\nAffine (A)\n61.52\n78.96\n77.95\n74.19\nColor (C)\n58.37\n78.03\n77.15\n71.24\nCanny (CN)\n53.15\n79.04\n80.48\n68.81\nCN + A + C\n58.96\n81.48\n79.71\n74.20\nTable 6: Effect of different augmentations on the shapes.\nels.\nSecondly, different types of augmentations have a\nmore significant impact on certain datasets than others. For\ninstance, affine transformation significantly enhances the\nperformance of QuickDraw and ImageNet Sketch, while\ncanny edge augmentation is more effective for the ShapeNet\nSketch dataset.\nConsequently, we decide to train a net-\nwork with all augmentations and find that, on balance across\ndatasets, it performs the best.\n5. Conclusion\nIn this paper, we demonstrate how a 3D generative model\nconditioned on local features from a pre-trained large-scale\nimage model such as CLIP can be used to generate 3D\nshapes from sketches. We show how this method can gen-\nerate multiple shapes for different abstraction of sketches\nand can be applied to multiple 3D representations. Future\nwork will involve training on much larger and diverse 3D\nshape datasets and consequently testing on different styles\nof sketches and levels of details.\nReferences\n[1] The quick, draw!\ndataset.\nhttps://github.com/\ngooglecreativelab/quickdraw-dataset.\n[2] Sketch2model: View-aware 3d modeling from single free-\nhand sketches.\nhttps://github.com/bennyguo/\nsketch2model.\n[3] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In International conference on\nmachine learning, pages 40\u201349. PMLR, 2018.\n[4] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022.\n[5] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\nlis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and\nFisher Yu. Shapenet: An information-rich 3d model reposi-\ntory, 2015. cite arxiv:1512.03012.\n[6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315\u201311325, 2022.\n[7] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5939\u20135948, 2019.\n[8] An-Chieh Cheng, Xueting Li, Sifei Liu, Min Sun, and Ming-\nHsuan Yang. Autoregressive 3d shape generation via canon-\nical mapping. In Computer Vision\u2013ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part III, pages 89\u2013104. Springer, 2022.\n[9] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A unified approach for\nsingle and multi-view 3d object reconstruction. In European\nconference on computer vision, pages 628\u2013644. Springer,\n2016.\n[10] Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A\nEfros, and Adrien Bousseau. 3d sketching using multi-view\ndeep volumetric prediction.\nProceedings of the ACM on\nComputer Graphics and Interactive Techniques, 1(1):1\u201322,\n2018.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[13] Mathias Eitz, James Hays, and Marc Alexa. How do humans\nsketch objects?\nACM Trans. Graph. (Proc. SIGGRAPH),\n31(4):44:1\u201344:10, 2012.\n[14] G.T. Fechner. Elemente der Psychophysik. Number pt. 1 in\nElemente der Psychophysik. Breitkopf und H\u00a8artel, 1860.\n[15] Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath\nSridhar. Shapecrafter: A recursive text-conditioned 3d shape\ngeneration model, 2022.\n[16] Chenjian Gao, Qian Yu, Lu Sheng, Yi-Zhe Song, and Dong\nXu.\nSketchsampler: Sketch-based 3d reconstruction via\nview-dependent depth sampling. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part I, pages 464\u2013479. Springer,\n2022.\n[17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. In Advances In Neural\nInformation Processing Systems, 2022.\n[18] Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich,\nManli Shu, Micah Goldblum, Andrew Gordon Wilson, and\nTom Goldstein. What do vision transformers learn? a visual\nexploration. arXiv preprint arXiv:2212.06727, 2022.\n[19] Thibault Groueix,\nMatthew Fisher,\nVladimir G Kim,\nBryan C Russell, and Mathieu Aubry. A papier-m\u02c6ach\u00b4e ap-\nproach to learning 3d surface generation. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 216\u2013224, 2018.\n[20] Benoit Guillard, Edoardo Remelli, Pierre Yvernay, and Pas-\ncal Fua. Sketch2mesh: Reconstructing and editing 3d shapes\nfrom sketches.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 13023\u201313032,\n2021.\n[21] David Ha and Douglas Eck.\nA neural representation of\nsketch drawings. In International Conference on Learning\nRepresentations, 2018.\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022.\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016.\n[24] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\n[25] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022.\n[26] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. In SIG-\nGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022.\n[27] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, July 2021. If you use this software, please cite it as\nbelow.\n[28] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022.\n[29] Pradeep Kumar Jayaraman, Joseph G Lambourne, Nishkrit\nDesai, Karl DD Willis, Aditya Sanghi, and Nigel JW Morris.\nSolidgen: An autoregressive model for direct b-rep synthe-\nsis. arXiv preprint arXiv:2203.13944, 2022.\n[30] Danilo Jimenez Rezende, SM Eslami, Shakir Mohamed, Pe-\nter Battaglia, Max Jaderberg, and Nicolas Heess. Unsuper-\nvised learning of 3d structure from images. Advances in neu-\nral information processing systems, 29, 2016.\n[31] Aobo Jin, Qiang Fu, and Zhigang Deng. Contour-based 3d\nmodeling through joint embedding of shapes and contours.\nIn Symposium on interactive 3D graphics and games, pages\n1\u201310, 2020.\n[32] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, , and N.\nFox-Gieg. The quick, draw! - a.i. experiment. https:\n//quickdraw.withgoogle.com/.\n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[34] Wei-Jan Ko, Hui Huang, Yu-Liang Kuo, Chen-Yi Chiu, Li-\nHeng Wang, and Wei-Chen Chiu.\nRpg: Learning recur-\nsive point cloud generation. 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages\n544\u2013551, 2021.\n[35] Di Kong, Qiang Wang, and Yonggang Qi.\nA diffusion-\nrefinement model for sketch-to-point modeling. In Proceed-\nings of the Asian Conference on Computer Vision, pages\n1522\u20131538, 2022.\n[36] Joseph George Lambourne, Karl Willis, Pradeep Kumar Ja-\nyaraman, Longfei Zhang, Aditya Sanghi, and Kamal Rahimi\nMalekshan.\nReconstructing editable prismatic cad from\nrounded voxel models. In SIGGRAPH Asia 2022 Confer-\nence Papers, pages 1\u20139, 2022.\n[37] Chun-Liang Li, Manzil Zaheer, Yang Zhang, Barnabas Poc-\nzos, and Ruslan Salakhutdinov.\nPoint cloud gan.\narXiv\npreprint arXiv:1810.05795, 2018.\n[38] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin.\nMagic3d: High-\nresolution text-to-3d content creation.\narXiv preprint\narXiv:2211.10440, 2022.\n[39] Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis,\nSubhransu Maji, and Rui Wang.\n3d shape reconstruction\nfrom sketches via multi-view convolutional networks.\nIn\n2017 International Conference on 3D Vision (3DV), pages\n67\u201377. IEEE, 2017.\n[40] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021.\n[41] Sarah Boyes Maxseiner.\nSketch Quality Prediction Using\nTransformers. PhD thesis, Virginia Tech, 2023.\n[42] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019.\n[43] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600,\n2022.\n[44] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492\u2013\n13502, 2022.\n[45] Abhishek Mishra.\nMachine learning in the aws cloud:\nAdd intelligence to applications with amazon sagemaker and\namazon rekognition, 2019.\n[46] Shailesh Mishra and Jonathan Granskog. Clip-based neu-\nral neighbor style transfer for 3d assets.\narXiv preprint\narXiv:2208.04370, 2022.\n[47] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and\nShubham Tulsiani.\nAutosdf: Shape priors for 3d com-\npletion, reconstruction and generation.\narXiv preprint\narXiv:2203.09516, 2022.\n[48] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter\nBattaglia. PolyGen: An autoregressive generative model of\n3D meshes. In Hal Daum\u00b4e III and Aarti Singh, editors, Pro-\nceedings of the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine Learning\nResearch, pages 7220\u20137229. PMLR, 13\u201318 Jul 2020.\n[49] Charlie Nash and Christopher KI Williams.\nThe shape\nvariational autoencoder: A deep generative model of part-\nsegmented 3d objects. In Computer Graphics Forum, vol-\nume 36, pages 1\u201312. Wiley Online Library, 2017.\n[50] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022.\n[51] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts, 2022.\n[52] Aaron\nvan\nden\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu.\nNeural discrete representation learning.\narXiv preprint arXiv:1711.00937, 2017.\n[53] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023.\n[54] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019.\n[55] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger.\nConvolutional occupancy\nnetworks. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part III 16, pages 523\u2013540. Springer, 2020.\n[56] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022.\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021.\n[58] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32, 2019.\n[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022.\n[61] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18603\u201318613,\n2022.\n[62] Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman\nShayani, Amir Hosein Khasahmadi, Srinath Sridhar, and\nDaniel Ritchie.\nTextcraft: Zero-shot generation of high-\nfidelity and diverse shapes from text.\narXiv preprint\narXiv:2211.01427, 2022.\n[63] Kristofer Schlachter, Benjamin Ahlbrand, Zhu Wang, Ken\nPerlin, and Valerio Ortenzi. Zero-shot multi-modal artist-\ncontrolled retrieval and exploration of 3d object sets. In SIG-\nGRAPH Asia 2022 Technical Communications, SA \u201922, New\nYork, NY, USA, 2022. Association for Computing Machin-\nery.\n[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[65] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 33:20154\u201320166, 2020.\n[66] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\nOctree generating networks: Efficient convolutional archi-\ntectures for high-resolution 3d outputs. In Proceedings of\nthe IEEE international conference on computer vision, pages\n2088\u20132096, 2017.\n[67] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. Learning robust global representations by penalizing\nlocal predictive power. In Advances in Neural Information\nProcessing Systems, pages 10506\u201310518, 2019.\n[68] Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen,\nand Stella X Yu.\n3d shape reconstruction from free-hand\nsketches. arXiv preprint arXiv:2006.09694, 2020.\n[69] Lingjing Wang, Cheng Qian, Jifei Wang, and Yi Fang. Un-\nsupervised learning of 3d model reconstruction from hand-\ndrawn sketches. In Proceedings of the 26th ACM interna-\ntional conference on Multimedia, pages 1820\u20131828, 2018.\n[70] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. In D.\nLee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems,\nvolume 29. Curran Associates, Inc., 2016.\n[71] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: A\ndeep generative network for computer-aided design models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6772\u20136782, 2021.\n[72] Xiang Xu, Karl DD Willis, Joseph G Lambourne, Chin-\nYi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Fu-\nrukawa.\nSkexgen: Autoregressive generation of cad con-\nstruction sequences with disentangled codebooks.\narXiv\npreprint arXiv:2207.04632, 2022.\n[73] Xingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischin-\nski, Daniel Cohen-Or, and Hui Huang.\nShapeformer:\nTransformer-based shape completion via sparse representa-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6239\u20136249,\n2022.\n[74] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 4541\u20134550, 2019.\n[75] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-\ningnet: Point cloud auto-encoder via deep grid deformation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 206\u2013215, 2018.\n[76] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta.\nShelf-\nsupervised mesh prediction in the wild. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8843\u20138852, 2021.\n[77] Biao Zhang, Matthias Nie\u00dfner, and Peter Wonka.\n3dilg:\nIrregular latent grids for 3d generative modeling.\narXiv\npreprint arXiv:2205.13914, 2022.\n[78] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng\nLi. Pointclip: Point cloud understanding by clip. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8552\u20138562, 2022.\n[79] Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu.\nSketch2model: View-aware 3d modeling from single free-\nhand sketches. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6012\u2013\n6021, 2021.\n[80] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021.\n[81] Adrian \u0141a\u00b4ncucki, Jan Chorowski, Guillaume Sanchez, Ri-\ncard Marxer, Nanxin Chen, Hans J. G. A. Dolfing, Sameer\nKhurana, Tanel Alum\u00a8ae, and Antoine Laurent. Robust train-\ning of vector quantized bottleneck models, 2020.\nSupplementary Material\nCategory\n% correctly identified\nloudspeaker\n68.0%\nairplane\n58.5%\nbench\n80.0%\ncar\n69.4%\nboat\n64.3%\nmonitor\n70.0%\nlamp\n89.1%\ntable\n80.0%\ncabinet\n83.3%\nsofa\n71.4%\ngun\n65.0%\nchair\n83.6%\nphone\n52.5%\nTable 7: Results of the human perceptual evaluation by\nclass. We show the percentage of generations the major-\nity of the 7 crowd workers reviewing each image correctly\nidentified the 3D model generated by conditioning on the\nsketch.\nA. Human Perceptual Evaluation\nIn Subsection 4.2 (main paper) we show the results of\nthe human perceptual study broken down according to the\ndataset which the target sketch came from. In addition, the\nresults can be broken down based on object category of the\ntarget sketch, as shown in Table 7. We see a wide range\nof performance across the different object categories, with\n\u201clamps\u201d being correctly identified 89.1% of the time, while\nphones are identified just 52.5% of the time, little better\nthan random. Categories which perform well, such as chair,\ntable and sofa, tend to have distinctive shapes which are\neasy to communicate with sketches. Categories like air-\nplane and gun produce good models, but these are not dis-\ntinctive enough for the human evaluators to distinguish the\ncorrect 3D model from a random model of in the same cat-\negory. Lower performance on these categories may also re-\nlate to the difficultly of drawing objects of these types. We\nbelieve having texture can further improve the human per-\nceptual results.\nAs each generated model is rated by 7 individual crowd\nworkers, we can count the number of raters who correctly\nidentified the generated model, giving us a \u201cshape recog-\nnizably score\u201d from 0 to 7. In Figure 5 we show exam-\nples from selected categories with the highest and lowest\n\u201cshape recognizably scores\u201d. For \u201cairplane\u201d category the\nleast recognizable model appears to be in the wrong cate-\ngory, due to the unusual orientation of the sketch. The most\nand least recognizable sketch in the \u201cbench\u201d category both\nMethod\nType\nIOU \u2191\nSketch2Mesh [20]\nSupervised\n0.195\nSketch2Model [79]\nSupervised\n0.205\nSketch2Point [68]\nSupervised\n0.163\nSketchSampler [16]\nSupervised\n0.244\nours\nZero-shot\n0.292\nTable 8: IOU Results Comparison. It can be seen that our\nmethod outperforms supervised methods.\ncome from the Imagenet-Sketch dataset. The sketch for the\nmost recognizable model contains a single bench while the\nsketch for the least recognizable model also contains back-\nground elements like trees and a lampost. For the \u201cgun\u201d\ncategory the most recognizable model is actually from a\nsketch which looks nothing like a gun. The least recogniz-\nable model is a generic gun which does not closely follow\nthe shape of the sketch. The Figure shows how the human\nevaluation is measure the ability of our method to generate\ndistinctive shapes reflecting the geometry of the sketches as\nwell as general generation quality.\nB. Comparison with Supervised Methods\nB.1. Quantitative comparison\nWe evaluate the quality of generated shapes on the\nShapeNet-Sketch dataset [79] using Intersection over Union\n(IOU) with 323 voxel shapes, as shown in [41]. This is\nthe only dataset among the four we assessed that includes\nground truth 3D voxels. We compare our results to those of\nother supervised methods presented in Table 8, exactly as in\n[41]. Our generative model generates 5 shapes based on a\ngiven sketch query in the ShapeNet-Sketch dataset and av-\nerages the IOU results. Although our method is not trained\non any sketch data, it outperforms the supervised baseline.\nThis indicates that the pre-trained model\u2019s learned features\nare effective in enabling our method to generate 3D shapes\nusing sketches in a zero-shot manner.\nB.2. Qualitative comparison\nWe additionally provide a qualitative comparison with\nSketch2Model [79] and SketchSampler [16].\nFor this\ncomparison, we considered diverse sketches with differ-\nent levels of abstraction in the same classes of ShapeNet\nfrom four datasets: TU-Berlin [13], ShapeNet-Sketch [79],\nImageNet-Sketch [67], and QuickDraw [1]. Implementa-\ntion details can be found in Appendix C. Results are in\nFigure 6.\nMost recognizable\nLeast recognizable\nFigure 5: Generated results which were most and least rec-\nognizable for the human evaluators.\nWe can see that Sketch2Model reconstructed meshes\noften grasp the overall sketch shape, but they appear too\nsmooth and lack geometric details. This method was orig-\ninally intended for a single category scenario, as presented\nin the paper. However, this is often unpractical.\nSimilarly, SketchSampler fails to generalize to abstract\nor out-of-distribution sketches. The resulting point clouds\npresent artifacts and outliers, especially in the direction of\nthe point of view of the sketch (shapes proportion are only\npreserved when the point clouds are seen from this point of\nview). Unlike our approach, SketchSampler is designed for\nprofessional sketches only, with reliable shapes and fine-\ngrained details. Thus, it cannot deal with sketches with sig-\nnificant deformation or only expressing conceptual ideas,\nlike the ones in QuickDraw [1].\nC. Architecture and Experiment Details\nTraining Details. We use the Adam Optimizer [33] with\na fixed learning rate of 1e-4 for training. The network is\ntrained for 300 epochs during Stage 1 and for 250 epochs\nduring Stage 2. We do not employ any learning rate sched-\nuler during training. We train the 323 voxel model solely on\nthe ShapeNet13 dataset, while the Implicit model is trained\non the ShapeNet55 subset. The CAD model is trained on\nthe DeepCAD dataset [72]. This is done to demonstrate\nthe versatility and adaptability of our method to different\ndatasets.\nStage 1 Details. For both the Implicit VQ-VAE and 323\nVQ-VAE we use a codebook size of 512, a grid size of\n83 and embedding dimensions of size 64. We employ the\nResNet architecture for the 323 VQ-VAE, for both the en-\ncoder and decoder. In the case of Implict VQ-VAE, we\nuse the ResNet architecture for the encoder whereas we\nuse a decoder that produces a higher resolution volume,\nwhich is then queried locally to obtain the final occupancy\n[42, 55, 15]. The pretrained VQ-VAE from SkexGen [72]\nis used for the CAD representation which is composed of\nthree Transformer encoders and decoders for the topology,\ngeometry and extrusions of a CAD model. The models out-\nput 4 + 2 + 4 = 10 codes, with a total codebook size of\n1000.\nStage 2 Details.\nFor Stage 2, we utilize a bidirectional\nTransformer with 8 attention blocks, 8 attention heads, and\na token size of 256. We use 24 renderings [9] for both the\nShapeNet13 and ShapeNet55 experiments. During infer-\nence, we run the Transformer for 15 steps with classifier-\nfree guidance, and the scale parameter is set to 3.\nThe\nCLIP ViT-L/14 model is employed in all experiments, ex-\ncept in Table 3 of the main paper, where we conduct an\nablation study over different pre-trained models. For all ex-\nperiments, except Table 4, we incorporate cross-attention\nwith learnable positional embedding and a mapping net-\nwork consisting of 2 layers of MLP. We do not apply any\naugmentation for the quantitative experiments, except for\nthe results presented in Table 6 and Table 2 of the main pa-\nper. For the CAD results, we used a CLIP ViT-B/32 model.\nSketch2Model.\nThe authors of Sketch2Model released\nShapeNet-Synthetic as the training dataset [79], which con-\nsists of synthetic sketches of objects from 13 categories\nfrom ShapeNet. These objects have been rendered from 20\ndifferent views. For training Sketch2Model, we used the\nofficial implementation provided in [2], along with the rec-\nommended hyperparameters. This implementation uses a\nstep-type learning rate policy, beginning from 1e \u2212 4 and\nSketch2Model [79]\nSketchSampler [16]\nOurs\nSketch2Model [79]\nSketchSampler [16]\nOurs\nFigure 6: Generated shapes with different methods. We can see that, compared to our method, Sketch2Model meshes lack\ngeometric details and appear smoothed, while SketchSampler pointclouds present artifacts and outliers.\ndecreasing by 0.3 every 800 epochs, and trains for 2000\nepochs with the Adam optimizer. We trained the model\non all 13 categories of ShapeNet-Synthetic using the same\ntraining/test split of the original paper.\nSketchSampler. This method employs as training dataset\nSynthetic-LineDrawing [16], a paired sketch-3D dataset\nbased on 3D models from ShapeNet. In our experiments,\nwe used the official implementation, cited in the original\npaper [16]. In particular, we used the pre-trained model re-\nleased by the authors, and pre-processed the input sketches\nto be in the same format of Synthetic-LineDrawing ones.\nD. Comparison with Point\u00b7E\nFurthermore, we conducted a comparison between our\nwork and Point\u00b7E [51], as illustrated in the table provided\nbelow (Row 1). The results clearly demonstrate the supe-\nFigure 7: Results of our method on natural images\nrior performance of our method, indicating the merit of our\ndesign choices.\nMethod\nQD-Acc\u2191\nTU-Acc\u2191\nSS-Acc\u2191\nIS-Acc\u2191\nPoint\u00b7E\n12.6\n40.1\n43.2\n18.9\nOurs (CLIP)\n58.8\n81.5\n79.7\n74.2\nOurs (DINOv2)\n39.7\n71.1\n72.1\n55.9\nFigure 8: Failure cases of our method\nE. Natural Images Results\nWe explored the applicability of our method to natural\nimages, as it is robust to domain shifts between renderings\nand sketches. The outcomes are depicted in Figure 7, indi-\ncating the efficacy of our method in generating natural im-\nages, including those with backgrounds. We believe that\nthis discovery would be of interest to the Single View Re-\nconstruction community.\nF. Failure Cases\nThis section demonstrates the limitations of our method,\nas illustrated in Figure 8.\nThe outcomes reveal that our\nmethod encounters difficulties in generalizing to shapes that\nare beyond those present in ShapeNet13, as depicted in the\nfirst row. Furthermore, our method also faces challenges\nwhen dealing with sketches that depict multiple shapes, as\nshown in the second row. Lastly, our method experiences\ndifficulties in accurately reproducing the local details of\nshapes, which we consider to be an intriguing direction for\nfuture work.\nG. Societal Impact\nThe societal impact of Sketch-to-3D technology can be\nsignificant in various fields such as architecture, product de-\nsign, gaming, and entertainment. With the help of Sketch-\nto-3D technology, designers and architects can create real-\nistic 3D models quickly and efficiently, reducing the overall\ntime and cost of the design process. However, it is important\nto note that the widespread adoption of Sketch-to-3D tech-\nnology could also lead to job displacement in certain indus-\ntries. As with any technological advancement, it is crucial\nto consider the potential social and economic impacts and\nwork towards ensuring a smooth transition for workers and\ncommunities affected by such changes.\nH. Future Work\nWe aim to concentrate on expanding this method to han-\ndle bigger 3D datasets for our future work. Additionally,\nwe think that enhancing the Stage 1 VQ-VAE can aid in\npreserving the local features of the 3D shape. Lastly, an in-\ntriguing avenue to explore would be to combine sketch with\ntext conditioning, resulting in a more adaptable generative\nmodel.\nI. More Qualitative Results\nAdditional results are provided in Figure 9 and Figure\n10.\nFigure 9: Additional outcomes of our 323 voxel model on ShapeNet13 and implicit model on ShapeNet55 are presented.\nThe initial row displays the outcomes of 323 voxel model from QuickDraw, the second row from TU Berlin, the third row\nfrom ShapeNet Sketch, and the fourth row from Imagenet Sketch. The fifth row displays the outcomes of implicit model\nfrom QuickDraw, the sixth row from TU Berlin and the seventh row from Imagenet Sketch. The last two rows exhibit several\nresults obtained from a single sketch.\nFigure 10: More results from our sketch-CAD model. The last four rows show multiple generation results for each sketch.\n"
  },
  {
    "title": "Semantic-SAM: Segment and Recognize Anything at Any Granularity",
    "link": "https://arxiv.org/pdf/2307.04767.pdf",
    "upvote": "19",
    "text": "Semantic-SAM: Segment and Recognize Anything at\nAny Granularity\nFeng Li\u2660\u2217, Hao Zhang\u2660\u2217, Peize Sun\u266f, Xueyan Zou\u00a7, Shilong Liu\u00b6, Chunyuan Li\u2021\nJianwei Yang\u20211, Lei Zhang\u20202, Jianfeng Gao\u20212\n\u2660 HKUST\n\u2021 Microsoft Research, Redmond\n\u2020 IDEA\n\u266f HKU\n\u00a7 UW-Madison\n\u00b6 Tsinghua\n\u2217 Equal Contribution 1. Project Lead 2. Equal Advisory Contribution\nAbstract\nIn this paper, we introduce Semantic-SAM, a universal image segmentation model\nto enable segment and recognize anything at any desired granularity. Our model\noffers two key advantages: semantic-awareness and granularity-abundance. To\nachieve semantic-awareness, we consolidate multiple datasets across granularities\nand train on decoupled objects and parts classification. This allows our model\nto facilitate knowledge transfer among rich semantic information. For the multi-\ngranularity capability, we propose a multi-choice learning scheme, enabling each\nclick point to generate masks at multiple levels that correspond to multiple ground-\ntruth masks. Notably, this work represents the first attempt to jointly train a model\non SA-1B, generic, and part segmentation datasets. Experimental results and visu-\nalizations demonstrate that our model successfully achieves semantic-awareness\nand granularity-abundance. Furthermore, combining SA-1B training with other\nsegmentation tasks, such as panoptic and part segmentation, leads to performance\nimprovements. We will provide code and a demo for further exploration and\nevaluation at https://github.com/UX-Decoder/Semantic-SAM.\n1\nIntroduction\nThe universal and interactive AI systems that follow human intents have shown their potential in\nnatural language processing [46, 47] and controllable image generation [52, 66]. However, such a\nuniversal system for pixel-level image understanding remains less explored. We argue that a universal\nsegmentation model should possess the following important properties: universal representation,\nsemantic-awareness, and granularity-abundance. Regardless of the specific image domain or prompt\ncontext, the model is capable of acquiring a versatile representation, predicting segmentation masks\nin multi-granularity, and understanding the semantic meaning behind each segmented region.\nPrevious works [31, 70, 58] attempted to investigate these properties, but only achieved part of the\ngoals. The main obstacles impeding the progress of such a universal image segmentation model can\nbe attributed to limitations in both model architecture flexibility and training data availability.\n\u2022 Model Architecture. The existing image segmentation model architectures are dominated by the\nsingle-input-single-output pipeline that discards any ambiguity. While this pipeline is prevalent\nin both anchor-based CNN architectures [24] and query-based Transformer architectures [4, 11],\nand has demonstrated remarkable performance in semantic, instance, and panoptic segmentation\ntasks [39, 68, 30], it inherently restricts the model to predict multi-granularity segmentation\nmasks in an end-to-end manner. Although clustering postprocessing techniques [13] can produce\nmultiple masks for a single object query, they are neither efficient nor effective solutions for a\ngranularity-aware segmentation model.\nPreprint. Under review.\narXiv:2307.04767v1  [cs.CV]  10 Jul 2023\nFigure 1: Our model is capable of dealing with various segmentation tasks including open-set and\ninteractive segmentation. (a) Our model can do instance, semantic, panoptic segmentation, and part\nsegmentation. (b) Our model is able to output multi-level semantics with different granularities. The\nred point on the left-most image is the click.(c) We connect our model with an inpainting model to\nperform multi-level inpainting. The prompts are \"Spider-Man\" and \"BMW car\", respectively. Note\nthat only one click is needed to produce the results in (b) and (c), respectively.\n\u2022 Training Data. Scaling up segmentation datasets that possess both semantic-awareness and\ngranularity-awareness is a costly endeavor. Existing generic object and segmentation datasets such\nas MSCOCO [39] and Objects365 [53] offer large amounts of data and rich semantic information,\nbut only at the object level. On the other hand, part segmentation datasets such as Pascal Part [9],\nPartImageNet [23], and PACO [49] provide more fine-grained semantic annotations, but their\ndata volumes are limited. Recently, SAM [31] has successfully scale up the multi-granularity\nmask data to millions of images, but it does not include semantic annotations. In order to achieve\nthe dual objectives of semantic-awareness and granularity-abundance, there is a pressing need to\nunify segmentation training on various data formats to facilitate knowledge transfer. However,\nthe inherent differences in semantics and granularity across different datasets pose a significant\nchallenge to joint training efforts.\nIn this paper, we introduce Semantic-SAM, a universal image segmentation model designed to enable\nsegmenting and recognizing objects at any desired granularity. Given one click point from a user, our\nmodel addresses the spatial ambiguity by predicting masks in multiple granularities, accompanied\nby semantic labels at both the object and part levels. As shown in Figure 1, our model generates\nmulti-level segmentation masks ranging from the person head to the whole truck.\nThe multi-granularity capability is achieved through a multi-choice learning design [37, 22] incorpo-\nrated into the decoder architecture. Each click is represented with multiple queries, each containing\na different level of embedding. These queries are trained to learn from all available ground-truth\nmasks representing different granularities. To establish a correspondence between multiple masks\nand ground-truths, we employ a many-to-many matching scheme to ensure that a single click point\ncould generate high-quality masks in multiple granularities.\nTo accomplish semantic-awareness with a generalized capability, we introduce a decoupled classifica-\ntion approach for objects and parts, leveraging a shared text encoder to encode both objects and parts\nindependently. This allows us to perform object and part segmentation separately, while adapting the\nloss function based on the data type. For instance, generic segmentation data lacks part classification\nloss, whereas SAM data does not include classification loss.\nTo enrich semantics and granularity within our model, we consolidate seven datasets on three types\nof granularities, including generic segmentation of MSCOCO [39], Objects365 [53], ADE20k [68],\npart segmentation of PASCAL Part [9], PACO [49], PartImagenet [23], and SA-1B [31]. Their\n2\ndata formats are reorganized to match our training objectives accordingly. After joint training, our\nmodel obtains a strong performance across a variety of datasets. Notably, we find that learning\nfrom interactive segmentation could improve generic and part segmentation. For example, by jointly\ntraining SA-1B promptable segmentation and COCO panoptic segmentation, we achieve a gain of 2.3\nbox AP and a gain of 1.2 mask AP. In addition, through comprehensive experiments, we demonstrate\nthat our granularity completeness is better than SAM with more than 3.4 1-IoU.\n2\nData Unification: Semantics and Granularity\nIn order for multi-level semantics, we include seven datasets that contain different granularity-\nlevel masks. The datasets are SA-1B, COCO panoptic, ADE20k panoptic, PASCAL part, PACO,\nPartImageNet, and Objects365. Within them, COCO and ADE20k panoptic datasets contain object-\nlevel masks and class labels. PASCAL part, PACO, and PartImageNet contain part-level masks and\nclass labels. SA-1B contains up to 6-level masks without labels, while Objects365 contains abundant\nclass labels for object-level instances. The details of these datasets are shown in Table 1. We further\nvisualize the data distribution of different data type in Fig 2.\nType\nData\n#Images\nSemantic Concept\nGranularity Level\nPart\nObject\nPart\nWhole\nClass-agnostic\nSA-1B\n11B\n\u2717\n\u2717\n\u2713\n\u2713\nObject-level\nObjects365\n1.7M\n\u2717\n365\n\u2717\n\u2713\nCOCO\n110K\n\u2717\n133\n\u2717\n\u2713\nADE20K\n20K\n\u2717\n150\n\u2717\n\u2713\nPart-level\nPACO-LVIS\n45K\n201\n75\n\u2713\n\u2713\nPartImageNet\n16K\n13\n11\n\u2713\n\u2713\nPascal Part\n5K\n30\n20\n\u2713\n\u2713\nTable 1: The data statistics in Semantic-SAM.\nFigure 2: Semantics-Granularity 2D chart.\n3\nSemantic-SAM\n3.1\nModel\nOur Semantic-SAM follows [33] to exploit a query-based mask decoder to produce semantic-aware\nand multi-granularity masks. In addition to the generic queries, it supports two types of prompts\nincluding point and box, similar to SAM [31]. The overall pipeline is shown in Fig. 3.\nWe represent both click and box prompts into anchor boxes as a unified format. In particular, we\nconvert user click point (x, y) into an anchor box (x, y, w, h) with small width w and height h, so\nthat the anchor box can closely approximate the point. To capture different granularities of masks,\neach click is first encoded to position prompt and combined with K different content prompts, where\neach content prompt is represented as a trainable embedding vector for a given granularity level. Here\nwe empirically choose K = 6, considering there are at most 6 levels of masks per user click for the\nmajority of images in SA-1B [31]. More specifically, a click/box b = (x, y, w, h) is encoded into K\ncontent embeddings and one position embedding, respectively. We represent its content embeddings\nas a set of query vectors Q = (q1, \u00b7 \u00b7 \u00b7 , qK). For the i-th query,\nqi = qlevel\ni\n+ qtype\ni\n,\n(1)\nwhere\n\u2022 qlevel is the embedding for granularity level i,\n\u2022 qtype distinguishes the query type, chosen from either the click or the box embeddings.\nThe position embedding of c is implemented via sine encoding. Assuming that the output image\nfeature from vision encoder is F, the mask decoder of the proposed Semantic-SAM represents the\nclick on the input image as:\nO = DeformDec(Q, b, F) with O = (o1, \u00b7 \u00b7 \u00b7 , oK),\n(2)\nwhere DeformDec(\u00b7, \u00b7, \u00b7) is a deformable decoder that takes query feature, reference box, and\nimage features as input to output queried features. oi is the model output for the ith input query qi.\n3\nVision tokens\nVision Encoder\nContent Prompts\nPoint\nSemantic SAM\nPosition Prompts\nInteractive whole-part\nBox\nDuplicate\nGeneric\nGeneric\n\u2026\n\u2026\n\u2026\n\u2026\nMultiple-choice Learning\n\u2026\nTruck\nWindow/Truck\nText Encoder\nGeneric segmentation (i.e, COCO)\nPart segmentation(i.e, Pascal Part)\nClass-agnostic multi-granularity (SAM)\nTraining GT Types:\nHungarian Matching\nreferring\nFigure 3: Semantic-SAM is a universal segmentation framework that can take multiple types of\nsegmentation data including generic, part, and class-agnostic segmentation data. The Vision Encoder\nis used to extract image features. The mask decoder can do both generic segmentation and promptable\nsegmentation with various types of prompts. For point and box, we input them via anchor boxes to\nthe mask decoder. Since there is an ambiguity of granularity for a point input, we duplicate each\npoint 6 times and give them different levels of embeddings. The output masks of point prompts match\nwith multiple GT masks of different granularities.\nEach oi = (ci, mi) consists of the predicted semantic category ci and mask mi, which are used to\nconstruct the concept recognition loss and mask prediction loss, respectively.\n3.2\nTraining\nFigure 4: Decoupled object and part classification.\nRecognize Anything.\nAs we train with var-\nious types of data with different semantic an-\nnotations, in which some contain object-level\nannotations (COCO), some contain both ob-\nject and part-level annotations (Pascal Part),\nand SA-1B has no semantic annotations but\ncontains masks of all semantic levels. Note\nthat a large number of part concepts are\nshared across different objects, for example,\nhead for all animals. We aim to transfer the part concept knowledge across objects trained with\nonly object-level annotations in our joint training. To address this discrepancy between semantic\nannotations and better transfer semantics of different granularity, we propose to decouple object and\npart recognition. As shown in Fig 4, we utilize a shared text encoder to encode objects and parts,\nwhich are used to perform object and part segmentation separately.\nImportantly, while all types of segmentation data share a unified format, the loss varies for different\ndata types. We summarize the loss items to construct the training objective in Semantic-SAM in\nTable 2. It is the part-level data that bridges the gap to recognize semantic concepts between part and\nobject levels, and it is the use of SAM data in Hungarian matching that bridges the gap to segment\nmasks at any granularity.\nData\nRecognize Anything\nSegment at Any Granularity\nPart\nObject\nBox\nMask\n#GT in Matching\nSAM data\n\u2717\n\u2717\n\u2713\n\u2713\nMany\nObject-level data\n\u2717\n\u2713\n\u2713\n\u2713\nOne\nPart-level data\n\u2713\n\u2713\n\u2713\n\u2713\nOne\nTable 2: The loss items to construct the training objective in Semantic-SAM. The four loss items\nare part classification, object classification, box loss and mask loss, respectively. The last column\nindicates the number of ground-truth mask in the matching.\nSegment at any granularity.\nTo endow the model with a multi-granularity segmentation ability,\nwe propose a many-to-many matching method during training. We found that SAM fails in providing\n4\nFigure 5: Inteactive learning strategy comparison between a) One-to-one: traditional interactive\nsegmentation models that focus on object-level, i.e, SEEM, b) Many-to-one: multi-choice learning\nfor single granularity, i.e, SAM, and c) Many-to-many: ours. We enforce the model to predict all the\npossible granularities of a single click for more controllable segmentation. d) As a result, our output\ngranularity are richer to generate diverse output masks.\ngood multi-level segmentation results with a single click because SAM uses many-to-one matching\nduring training. In other words, the three SAM-predicted masks for each click only match with one\nGT mask. This causes that points located in masks of small levels cannot predict large masks with\nhigh quality according to our observation. In contrast, to enable multi-level mask prediction with a\nsingle click, we fully leverage the structures in both data and algorithm. First, we re-organize the\ndata by clustering multiple GT masks of different levels sharing the same click. To allow multiple\npredictions of the same click to match with the GT masks, we employ the Hungarian algorithm to\nenable the many-to-many matching. The similarity matrix and scores vary based on the availability\nof different segmentation data components.\nFor box input and generic segmentation, we follow existing methods. Specifically, to generate a mask\nfrom an input box, we follow a similar idea as in denoising training (DN) [33]. We add noises to\nground-truth boxes to simulate inaccurate box inputs from users, and these noised boxes serve as\nspatial prompts for the decoder. The model is trained to reconstruct the original boxes and masks\ngiven noised boxes. For the content part of box prompts, we adopt a learnable token as a general\nprompt. Note that this is the only difference from DN, as DN uses ground-truth label embedding as\nthe content prompts. For generic segmentation, we follow the same pipeline as in Mask DINO [33].\nDiscussion.\nAs shown in Fig. 5, compared with previous interactive segmentation models, Semantic-\nSAM differs from previous segmentation models in two aspects. Firstly, we train the model to output\nall the possible segmentation masks with one click. Secondly, our output granularities are richer to\ngenerate diverse output masks.\n4\nExperiments\n4.1\nExperimental Setup\nImplementation Details. In our experiments, we jointly train on three types of data, as shown in\nTable 1. We implement our model based on Mask DINO [33] . Mask DINO is a unified detection\nand segmentation framework which simultaneously predicts box and mask. We follow [33] to use\n300 latent queries and nine decoder layers for all segmentation tasks. For the visual backbone, we\nadopt pre-trained Swin-T/L [41] by default. For the language backbone, we adopt the pre-trained\nbase model in UniCL [62]. As SA-1B [31] dominates the data, during training, we first train on only\nSA-1B data. Then, we add object and part-level data to jointly train the three types of data. During\ntraining, the image resolution is 1024 \u00d7 1024 for all data. We use AdamW [43] as the optimizer. We\nuse large-scale jittering for object and part-level data and did not use data augmentations for SA-1B\ndata, as SA-1B images are abundant. We set the learning rate to 0.0001, which is decayed at 0.9 and\n0.95 fractions of the total number of steps by 10.\nEvaluation. We mainly evaluate two datasets, including COCO Val2017 and a subset of SA-1B [31]\nwith 1000 images. For evaluation metrics, we evaluate PQ and AP for generic and part segmentation\ndatasets. For single-granularity interactive segmentation, we report Point (Max) and Point (Oracle).\nMax denotes we select the output mask with the maximum confidence score. Oracle denotes we\nselect the output mask with the max IoU by calculating the IoU between the prediction and target\nmask. For multi-granularity interactive segmentation, we report 1-IoU@All Granularity that matches\n5\nMethod\nType\nTraining Data\nPQ\nmIoU\nAP\nAPs\nAPm\nAPl\nbox\nmask\nbox\nmask\nbox\nmask\nbox\nmask\nMask2Former (T) [11]\nClose-set\nCOCO\n53.2\n63.2\n46.1\n43.3\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nX-Decoder (T) [69]\nOpen-set\nCOCO+VL\n52.6\n62.4\n43.6\n41.3\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nOpenSeed (T) [65]\nOpen-set\nCOCO+O365\n55.4\n63.8\n51.2\n47.1\n34.5\n27.4\n54.3\n50.4\n66.2\n66.8\nSemantic-SAM (T) (ours)\nOpen-set\nCOCO\n54.6\n63.2\n50.1\n46.1\n34.4\n27.1\n53.2\n49.4\n66.1\n66.1\nSemantic-SAM (T) (ours)\nOpen-set\nCOCO+SAM\n55.2\n63.4\n52.3(+2.2)\n47.4(+1.3)\n36.1(+1.7)\n28.3(+1.2)\n55.6(+2.4)\n50.7(+1.3)\n67.3\n66.2\nTable 3: Results for Semantic-SAM and other panoptic segmentation models on COCO val2017.\nOur model is jointly trained on COCO [8] and [31] (1/10 data) and directly evaluates COCO.\nMethod\nType\nTraining Data\nAP\nAPs\nAPm\nAPl\nbox\nmask\nbox\nmask\nbox\nmask\nbox\nmask\nVLPart [55]\nOpen-set\nPascal Part\n\u2212\n27.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nSemantic-SAM (ours)\nOpen-set\nPascal Part\n27.0\n30.5\n16.6\n19.1\n38.1\n41.6\n43.8\n49.1\nSemantic-SAM (ours)\nOpen-set\nPascal Part+SAM\n28.0\n31.4\n17.3\n19.9\n40.0\n42.5\n45.7\n49.7\nTable 4: Results for Semantic-SAM and other part segmentation models on Pascal Part. Our model is\njointly trained on Pascal Part [15] and SA-1B [31] (1/10 data) and directly evaluates Pascal Part.\nall the possible ground-truth masks for a single click to the multi-granularity predictions and then\ncalculate the average IoU of all granularities.\n4.2\nSemantic Segmentation of Anything\nGeneric Segmentation\nAs shown in Table 3, to validate the compatibility of multi-granularity\ninteractive segmentation and generic segmentation, we jointly train with SA-1B [31] (1/10 data) and\nCOCO panoptic segmentation. The result indicates that interactive segmentation with SAM can\nsignificantly help the instance-level detection and segmentation with a performance improvement\nof +2.2 AP on the box and +1.3 AP on the mask. Notably, OpenSeed [65] and Semantic-SAM are\nboth based on Mask DINO [33]. Our joint training with SA-1B even outperforms OpenSeed which is\ntrained with Object365 [53]. In addition, adding SA-1B mainly improves small object detection (APs\nand APm), as there are a large number of small objects in SA-1B.\nPart Segmentation\nWe also validate the compatibility of joint training SA-1B (1/10 data) and part\nsegmentation. As shown in Table 4, adding SA-1B brings a decent performance improvement on\nPascal Part [15].\nSingle-granularity Interactive Segmentation\nIn Table 5, we evaluate the 1-click mIoU (denoted\nas 1-IoU) for SAM and our model on COCO Val2017. Our model outperforms SAM under the same\nsettings.\nMulti-granularity Interactive Segmentation\nIn Table 6, we compare SAM [31] and our model on\nthe output granularities for a single click. We adopt a Hungarian Matching to match all the possible\nMethod\nCOCO\nPoint (Max)\nPoint (Oracle)\n1-IoU\n1-IoU\nSAM (B)\n52.1\n68.2\nSAM (L)\n55.7\n70.5\nSemantic-SAM (T)\n54.5\n73.8\nSemantic-SAM (L)\n57.0\n74.2\nTable 5: Comparison with previous models on point interactions. Both SAM [31] and our model are\ntrained with only SA-1B and directly evaluate on COCO Val2017 for fair comparison. Max denotes\nwe select the output with the max confidence score prediction. Oracle denotes we select the output\nwith the max IoU by calculating the IoU between the prediction and target mask.\n6\nMethod\nGranularity\n1-IoU@All Granularity\nSAM (B)\u2020\n3\n75.6\nSAM (L)\u2020\n3\n82.5\nSAM (H)\u2020\n3\n83.5\nSAM (B)\u2020\u2217\n6\n79.3\nSAM (L)\u2020\u2217\n6\n85.6\nSAM (H)\u2020\u2217\n6\n86.5\nSemantic-SAM(T)\n6\n88.1\nSemantic-SAM(L)\n6\n89.0\nTable 6: Granularity comparison between SAM and our model on a subset of SA-1B with 1000\nimages. We did not train on this subset of images but SAM did. For each click, we evaluate all the\npossible ground-truth masks to calculate the 1-IoU@All Granularity. SAM [31] and Semantic-SAM\nadopts three and six prompts for a single click of a mask, respectively. \u2020 denotes that SAM has been\ntrained on this validation subset while we did not. \u2217 denotes that we click two points for a single\nmask to produce six output masks.\nMethod\nMatch\n1-IoU@All Granularity\nSemantic-SAM(T)\nMany-to-one\n73.2\nSemantic-SAM(T)\nMany-to-many\n88.1\nTable 7: Different match strategy comparison on output granularity.\ntarget masks with the predicted masks for the click and calculate the average IoU score. As SAM has\nonly three prompts, we also sample two clicks from a single mask to produce six output masks for a\nfair comparison. Notably, SAM has been trained on this validation set while we did not.\n4.3\nAbaltions\nMatch Strategy\nAs shown in Table 7, we compare different match strategies in our model. When\nusing many-to-many matching to match all the possible ground-truth masks for each click, the\n1-IoU@All Granularity performance is significantly improved. This validates our matching strategy\nis effective to learn complete granularities.\nBox Interactive Evaluation\nWe also evaluate the 1-IoU given boxes in Table 8. We achieve\nbetter performance compared with object-level interactive segmentation model SEEM [70] and\nmulti-granularity model SAM [31].\nIncreasing SA-1B Training data\nIn Table 9, we show the performance improvement on COCO\nVal 2017 when training with more SA-1B data. The performance is saturated after using more than\n15% of the total data. It indicates that we do not need to train with the whole SA-1B data to get a\ngood zero-shot performance.\n4.4\nVisualization\nWe compare our model with SAM to show that our model can output more levels of high-quality\nmasks, as shown in Fig. 6.\nMethod\nBox 1-IoU\nSAM [31](B)\n50.7\nSEEM [70](T)\n73.7\nSemantic-SAM(T)\n76.1\nTable 8: Box 1-IoU evaluation on COCO Val2017. Both SEEM [70] and our model are trained on\nCOCO and we additionally train on SA-1B [31].\n7\nMethod\nData Portion of SA-1B\nCOCO\nPoint (Max)\nPoint (Oracle)\n1-IoU\n1-IoU\nSAM (L)\n100%\n55.7\n70.5\nSemantic-SAM (L)\n3%\n55.2\n73.5\nSemantic-SAM (L)\n15%\n56.7\n73.6\nSemantic-SAM (L)\n30%\n55.7\n73.7\nSemantic-SAM (L)\n50%\n55.3\n73.9\nSemantic-SAM (L)\n100%\n57.0\n74.2\nTable 9: Comparison of using different portions of SA-1B [31] data. Our model is only trained with\nSA-1B and directly evaluated on COCO Val2017.\nMulti-Level Masks Our model outputs more meaningful granularities of masks. SAM outputs three\nmasks at most and different levels of outputs are sometimes duplications, While, the output masks of\nour model are more diverse.\nMask Qualities It is also proved that our model output masks with higher quality. SAM sometimes\noutputs masks with artifacts such as holes or islands especially for large masks when the click is\nwithin a small-scale mask, while our model output high-quality masks for all levels.\nCompare with SA-1B Ground-truth Granularity\nWe output more meaningful granularity on\nSAM data compared with the original annotation.\nQuery semantics\nWe also find that each point content prompt embeddings learns to correspond\nto a fixed granularity. As shown in Fig. 7, when we visualize masks in a specific order of the\ncorresponding content embeddings, the masks follow the order from small to large in each row\nconsistently. This proves that each content embedding represents a semantic granularity level in our\nmodel.\n5\nRelated works\n5.1\nGeneric Segmentation\nSegmenting visual concepts is well-documented within the expansive field of computer vision [17,\n16, 71, 45]. Broad segmentation methodologies comprise several subdivisions, such as instance\nsegmentation, semantic segmentation, and panoptic segmentation [24, 6, 30], each catering to a\nunique semantic degree. For example, semantic segmentation\u2019s goal is to detect and assign a label to\neach pixel in an image according to its corresponding semantic class [7, 11, 42]. Conversely, instance\nsegmentation seeks to cluster pixels associated with the same semantic class into distinct object\ninstances [24, 3, 33]. Panoptic segmentation is the hybrid of these two tasks. Recently, Transformer-\nbased methods [56, 4] have contributed to significant progress in segmentation tasks [38, 11, 33, 26,\n64]. Generic object detection and segmentation have led to the development of abundant datasets,\nsuch as MSCOCO [39], LVIS [21], Objects365 [53], PASCAL [15],CityScapes [12],ADE20k [68],\netc.\n5.2\nPart Segmentation\nBeyond generic segmentation, part segmentation aims to more fine-grained visual understanding.\nMost early works were bottom-up methods by grouping super-pixels into parts and then objects\n[1, 20, 2]. Later, based on high-performance object detection networks [51, 24], top-down methods\nwere developed by firstly detecting an object and then parsing it to part segmentation [34, 63, 27]. To\nsegment the scene in multi-granularity, part-aware panoptic segmentation [13] is introduced. PPS [13]\nestablishes the baseline through assembling panoptic and part segmentation models. JPPF [25]\nsimplifies the model by a shared image encoder for both panoptic segmentation and part segmentation.\nBy representing thing, stuffs, and parts as object queries, Panoptic-PartFormer [35] proposes a unified\narchitecture based on Transformer. While part segmentation data is much expensive than object\ndetection and segmentation data, a number of public datasets are available. Datasets for specific\n8\n(a) Ours\n(b) SAM\n\u2248\n(a) Ours\n(b) SAM\n\u2248\n(a) Ours\n(b) SAM\n(c) GT\n(c) GT\n(c) GT\nFigure 6: (a)(b) are the output masks of our model and SAM, respectively. The red points on the\nleft-most image of each row are the use clicks. (c) shows the GT masks that contain the user clicks.\nThe outputs of our model have been processed to remove duplicates.\ndomains include cars [54], birds [57], and fashion [29]. General objects include Pascal-Part [9],\nPartImageNet [23], ADE20K [67], Cityscapes-Panoptic-Parts [44], and PACO [49]. More recently,\nSAM [31] provides a large-scale multi-granularity class-agnostic segmentation dataset. Our work is\njointly trained on these datasets and contributes to a multi-granularity segmentation model.\n5.3\nOpen-Vocabulary Segmentation\nWhile generic segmentation and part segmentation have made remarkable progress, they can only\nsegment the image in a close-set vocabulary. To expand the vocabulary size, recent works leverage\nthe visual-semantic knowledge from large-scale foundation models like CLIP [48], ALIGN [28]\nand Diffusion models [60] to various segmentation tasks. LSeg [32], OpenSeg [18], GroupViT [59]\n9\nPrompt 1\nPrompt 2\nPrompt 3\nPrompt 4\nPrompt 5\nPrompt 6\nFigure 7: We visualize the prediction of each content prompt embedding of points with a fixed order\nfor our model. We find all the output masks are from small to large. This indicates each prompt\nembedding represents a semantic level.\nachieves open-vocabulary semantic segmentation ability on ADE20K and PASCAL. DenseCLIP [50]\nand MaskCLIP [14] achieves open-vocabulary instance and panoptic segmentation on COCO dataset.\nMore recently, X-Decoder [69] proposes a unified approach to tackle various segmentation and vision-\nlanguage tasks for open-vocabulary segmentation, OpenSeeD [65] proposes to use a large amount of\ndetection data and a joint training method to improve segmentation. To segment open-vocabulary\nmasks in part-level, VLPart [55] leverages three part segmentation datasets and learns from the dense\ncorrespondence [5] between base objects and novel objects. Our work unifies these tasks into one\narchitecture and builds up open-vocabulary segmentation in multi-granularity.\n5.4\nInteractive Segmentation\nInteractive segmentation refers to the process of separating objects by actively integrating user inputs.\nThis enduring challenge has seen notable advancements [36, 19, 61, 40, 10, 31]. Previous works only\nfocus on a small set of data or semantic-agnostic instance masks. Recently, SAM [31] enlarges the\ntraining data from 0.12M COCO images to 10M SAM fine-grained images. And SEEM [70] enriches\nthe modality to language and function to both generic and grounded segmentation with an impressive\ncompositionality.\n6\nConclusion\nIn this paper, we have presented Semantic-SAM, which can segment and recognize anything at any\ndesired granularity. Apart from performing generic open-vocabulary segmentation, Semantic-SAM\ndemonstrates the advantages of semantic awareness and granularity abundance. To achieve such\nadvantages, we have proposed improvements on data, model, and training where we utilized datasets\nfrom multiple granularity and semantic levels, multi-choice learning for training, and a universal\nframework for modeling. Comprehensive experiments and visualizations have verified the semantic\nawareness and granularity abundance of our model. Further, Semantic-SAM is the first successful\nattempt to jointly train on SA-1B and other classic segmentation datasets. Experimental results also\nshow that training with SA-1B improves other tasks such as panoptic and part segmentation.\nReferences\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection\nand hierarchical image segmentation. IEEE transactions on pattern analysis and machine\nintelligence, 33(5):898\u2013916, 2010.\n[2] Pablo Arbel\u00e1ez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik.\nMultiscale combinatorial grouping. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 328\u2013335, 2014.\n[3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\nYolact: Real-time instance\nsegmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 9157\u20139166, 2019.\n10\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, pages 213\u2013229. Springer, 2020.\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE transactions on pattern analysis and machine intelligence,\n40(4):834\u2013848, 2017.\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n[8] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server.\narXiv preprint arXiv:1504.00325, 2015.\n[9] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille.\nDetect what you can: Detecting and representing objects using holistic models and body parts,\n2014.\n[10] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and Hengshuang Zhao. Fo-\ncalclick: towards practical interactive image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 1300\u20131309, 2022.\n[11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.\nMasked-attention mask transformer for universal image segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1290\u20131299, 2022.\n[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3213\u20133223, 2016.\n[13] Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaoxiao Wen, and Gijs Dubbelman. Part-\naware panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5485\u20135494, 2021.\n[14] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with\nmaskclip. arXiv preprint arXiv:2208.08984, 2022.\n[15] Mark Everingham and John Winn. The pascal visual object classes challenge 2012 (voc2012)\ndevelopment kit. Pattern Analysis, Statistical Modelling and Computational Learning, Tech.\nRep, 8(5), 2011.\n[16] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection\nwith discriminatively trained part-based models. IEEE transactions on pattern analysis and\nmachine intelligence, 32(9):1627\u20131645, 2009.\n[17] King-Sun Fu and JK Mui. A survey on image segmentation. Pattern recognition, 13(1):3\u201316,\n1981.\n[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-vocabulary image segmentation.\narXiv preprint arXiv:2112.12143, 2021.\n[19] Leo Grady. Random walks for image segmentation. IEEE transactions on pattern analysis and\nmachine intelligence, 28(11):1768\u20131783, 2006.\n[20] Matthias Grundmann, Vivek Kwatra, Mei Han, and Irfan Essa. Efficient hierarchical graph-\nbased video segmentation. In 2010 ieee computer society conference on computer vision and\npattern recognition, pages 2141\u20132148. IEEE, 2010.\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5356\u20135364, 2019.\n[22] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning\nto produce multiple structured outputs. Advances in neural information processing systems, 25,\n2012.\n[23] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xiaoding Yuan, Jie-Neng Chen, Shuai\nLiu, Cheng Yang, and Alan Yuille. Partimagenet: A large, high-quality dataset of parts. arXiv\npreprint arXiv:2112.00933, 2021.\n11\n[24] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of\nthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.\n[25] Sravan Kumar Jagadeesh, Ren\u00e9 Schuster, and Didier Stricker. Multi-task fusion for efficient\npanoptic-part segmentation. arXiv preprint arXiv:2212.07671, 2022.\n[26] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer:\nOne transformer to rule universal image segmentation. arXiv preprint arXiv:2211.06220, 2022.\n[27] Ruyi Ji, Dawei Du, Libo Zhang, Longyin Wen, Yanjun Wu, Chen Zhao, Feiyue Huang, and\nSiwei Lyu. Learning semantic neural tree for human parsing. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIII 16, pages\n205\u2013221. Springer, 2020.\n[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In ICML, 2021.\n[29] Menglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui, Claire Cardie, Bharath Hariharan,\nHartwig Adam, and Serge Belongie. Fashionpedia: Ontology, segmentation, and an attribute\nlocalization dataset. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part I 16, pages 316\u2013332. Springer, 2020.\n[30] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9404\u20139413, 2019.\n[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything, 2023.\n[32] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language-\ndriven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022.\n[33] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask\ndino: Towards a unified transformer-based framework for object detection and segmentation.\narXiv preprint arXiv:2206.02777, 2022.\n[34] Qizhu Li, Anurag Arnab, and Philip HS Torr. Holistic, instance-level human parsing. arXiv\npreprint arXiv:1709.03612, 2017.\n[35] Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, and Dacheng Tao. Panoptic-\npartformer: Learning a unified model for panoptic part segmentation. In European Conference\non Computer Vision, pages 729\u2013747. Springer, 2022.\n[36] Yin Li, Jian Sun, Chi-Keung Tang, and Heung-Yeung Shum. Lazy snapping. ACM Transactions\non Graphics (ToG), 23(3):303\u2013308, 2004.\n[37] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image segmentation with latent\ndiversity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 577\u2013585, 2018.\n[38] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo,\nand Tong Lu. Panoptic segformer: Delving deeper into panoptic segmentation with transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 1280\u20131289, 2022.\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[40] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Simpleclick: Interactive image\nsegmentation with simple vision transformers. arXiv preprint arXiv:2210.11006, 2022.\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.\n[42] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se-\nmantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3431\u20133440, 2015.\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[44] Panagiotis Meletis, Xiaoxiao Wen, Chenyang Lu, Daan de Geus, and Gijs Dubbelman.\nCityscapes-panoptic-parts and pascal-panoptic-parts datasets for scene understanding. arXiv\npreprint arXiv:2004.07944, 2020.\n12\n[45] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz, and\nDemetri Terzopoulos. Image segmentation using deep learning: A survey. IEEE transactions\non pattern analysis and machine intelligence, 2021.\n[46] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\n[47] OpenAI. Gpt-4 technical report, 2023.\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021.\n[49] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo,\nRui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song,\nAbhimanyu Dubey, and Dhruv Mahajan. PACO: Parts and attributes of common objects. In\narXiv preprint arXiv:2301.01795, 2023.\n[50] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang,\nJie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware\nprompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18082\u201318091, 2022.\n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. Advances in neural information processing\nsystems, 28:91\u201399.\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[53] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and\nJian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 8430\u20138439, 2019.\n[54] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai, Hao Su, Hongdong\nLi, and Ruigang Yang. Apollocar3d: A large 3d car instance understanding benchmark for\nautonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5452\u20135462, 2019.\n[55] Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping Luo, Saining Xie, and Zhicheng Yan.\nGoing denser with open-vocabulary part segmentation, 2023.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[57] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The\ncaltech-ucsd birds-200-2011 dataset. technical report, 2011.\n[58] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang.\nSeggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n[59] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong\nWang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18134\u201318144,\n2022.\n[60] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello.\nOpen-vocabulary panoptic segmentation with text-to-image diffusion models. arXiv preprint\narXiv:2303.04803, 2023.\n[61] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep interactive object\nselection. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 373\u2013381, 2016.\n[62] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao.\nUnified contrastive learning in image-text-label space. In CVPR, 2022.\n[63] Lu Yang, Qing Song, Zhihui Wang, and Ming Jiang. Parsing r-cnn for instance-level human anal-\nysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 364\u2013373, 2019.\n[64] Hao Zhang, Feng Li, Huaizhe Xu, Shijia Huang, Shilong Liu, Lionel M Ni, and Lei Zhang. Mp-\nformer: Mask-piloted transformer for image segmentation. arXiv preprint arXiv:2303.07336,\n2023.\n13\n[65] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang,\nand Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv\npreprint arXiv:2303.08131, 2023.\n[66] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[67] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nScene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633\u2013641, 2017.\n[68] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset, 2018.\n[69] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language.\narXiv preprint arXiv:2212.11270, 2022.\n[70] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.\nSegment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n[71] Zhengxia Zou, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A\nsurvey. arXiv preprint arXiv:1905.05055, 2019.\n14\n"
  },
  {
    "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
    "link": "https://arxiv.org/pdf/2307.04686.pdf",
    "upvote": "19",
    "text": "VAMPNET: MUSIC GENERATION VIA\nMASKED ACOUSTIC TOKEN MODELING\nHugo Flores Garc\u00eda1,2\nPrem Seetharaman1\nRithesh Kumar1\nBryan Pardo2\n1 Descript Inc.\n2 Northwestern University\nhugofg@u.northwestern.edu\nABSTRACT\nWe introduce VampNet, a masked acoustic token mod-\neling approach to music synthesis, compression, inpaint-\ning, and variation. We use a variable masking schedule\nduring training which allows us to sample coherent mu-\nsic from the model by applying a variety of masking ap-\nproaches (called prompts) during inference. VampNet is\nnon-autoregressive, leveraging a bidirectional transformer\narchitecture that attends to all tokens in a forward pass.\nWith just 36 sampling passes, VampNet can generate co-\nherent high-fidelity musical waveforms. We show that by\nprompting VampNet in various ways, we can apply it to\ntasks like music compression, inpainting, outpainting, con-\ntinuation, and looping with variation (vamping). Appropri-\nately prompted, VampNet is capable of maintaining style,\ngenre, instrumentation, and other high-level aspects of the\nmusic. This flexible prompting capability makes VampNet\na powerful music co-creation tool. Code 3 and audio sam-\nples 4 are available online.\n1. INTRODUCTION\nIn recent years, advances in discrete acoustic token mod-\neling have resulted in significant leaps in autoregressive\ngeneration of speech [1,2] and music [3]. Meanwhile, ap-\nproaches that use non-autoregressive parallel iterative de-\ncoding have been developed for efficient image synthe-\nsis [4, 5].\nParallel iterative decoding promises to allow\nfaster inference than autoregressive methods and is more\nsuited to tasks like infill, which require conditioning on\nboth past and future sequence elements.\nIn this work, we combine parallel iterative decoding\nwith acoustic token modeling, and apply them to music\naudio synthesis. To the best of our knowledge, ours is the\nfirst 1 extension of parallel iterative decoding to neural au-\ndio music generation. Our model, called VampNet, can be\n1 While our work was under peer review, Google released SoundStorm\n[6], which leverages a similar parallel iterative decoding approach to ours.\n\u00a9 H. Flores Garc\u00eda, P. Seetharaman, R. Kumar, and B.\nPardo. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution:\nH. Flores Garc\u00eda, P. Seetharaman,\nR. Kumar, and B. Pardo, \u201cVampNet: Music Generation via\nMasked\nAcoustic Token Modeling\u201d, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.\nFigure 1. VampNet overview. We first convert audio into\na sequence of discrete tokens using an audio tokenizer. To-\nkens are masked, and then passed to a masked generative\nmodel, which predicts values for masked tokens via an effi-\ncient iterative parallel decoding sampling procedure at two\nlevels. We then decode the result back to audio.\nflexibly applied to a variety of applications via token-based\nprompting. We show that we can guide VampNet\u2019s gener-\nation with selectively masked music token sequences, ask-\ning it to fill in the blanks. The outputs of this procedure can\nrange from a high-quality audio compression technique to\nvariations on the original input music that match the orig-\ninal input music in terms of style, genre, beat and instru-\nmentation, while varying specifics of timbre and rhythm.\nUnlike auto-regressive music models [2, 3], which can\nonly perform music continuations \u2013 using some prefix au-\ndio as a prompt, and having the model generate music that\ncould plausibly come after it \u2013 our approach allows the\nprompts to be placed anywhere. We explore a variety of\nprompt designs, including periodic, compression, and mu-\nsically informed ones (e.g. masking on the beat). We find\nthat our model responds well to prompts to make loops and\nvariations, thus the name VampNet 2 . We make our code\nopen source 3 and highly encourage readers to listen to our\naudio samples 4 .\n2 To vamp is to repeat a short passage of music with variation.\n3 https://github.com/hugofloresgarcia/vampnet\n4 audio samples: https://tinyurl.com/bdfj7rdx\narXiv:2307.04686v2  [cs.SD]  12 Jul 2023\n2. BACKGROUND\nTwo-stage approaches to generative modeling have gained\ntraction in image [4, 5, 7, 8] and audio [2, 3, 6, 9] synthe-\nsis, largely in part due to their computational efficiency. In\nthe first stage, a discrete vocabulary of \u201ctokens\u201d is learned\nfor the domain of interest. The input is put through an en-\ncoder to obtain these tokens, which can be converted back\ninto the input domain via a corresponding decoder. In the\nsecond stage, a model is trained to generate tokens, and is\noptionally given some conditioning (e.g. previous tokens,\na text description, a class label) to guide generation.\n2.1 Stage 1: Tokenization\nIn images, visual tokenization has been leveraged for state-\nof-the-art classification [10] and synthesis [4,7,8,11]. The\nmost popular approach is to use vector quantization on a la-\ntent space. Similar approaches have been explored for au-\ndio [12], but until recently such approaches have been re-\nstricted to low sampling rates (e.g. 16khz), or have been re-\nstricted to speech audio. The \u201csampling rate\u201d of the latent\nspace (the number of latent vectors required every second\nto represent audio) is a critical aspect of the tokenization\nscheme. The lower the sampling rate of the latent space,\nthe easier the next stage (generation) will be to accom-\nplish. Recently, methods based on residual vector quan-\ntization [13,14] have been proposed for audio tokenization\nat high compression rates with good reconstruction quality\nof high-sample-rate audio.\nThe primary work we leverage for audio tokenization is\nthe Descript Audio Codec (DAC) [15]. With DAC, audio is\nencoded into a sequence of tokens via a fully convolutional\nencoder. The output of this encoder is then quantized us-\ning a hierarchical sequence of vector-quantizers [11]. Each\nquantizer operates on the residual error of the quantizer be-\nfore it. Because of this residual vector quantization, DAC\nis able to reconstruct audio with very high quality, at a high\ncompression ratio. It, along with its predecessors [13,14],\nare instrumental in enabling audio language models like\nAudioLM [2], MusicLM [3], and VALL-E [1]. While we\nlater briefly describe our tokenizer, the key contributions\nof our work are applicable to the output of any audio tok-\nenizer and our specific audio tokenizer is not the focus of\nthis work.\n2.2 Stage 2: Generation\nGiven audio encoded as tokens, the common approach is to\nuse an autoregressive model [16] for generation. State-of-\nthe-art (SOTA) audio generation approaches like AudioLM\n[2], MusicLM [3], and JukeBox [17] use this approach,\ngenerating each acoustic token in the sequence in a step-\nby-step fashion using transformer-based [18] decoder-only\nmodels. Autoregressive sampling is slow in nature due to\nthe high number of steps required at inference time [4].\nFurther, autoregressive models inherently restrict down-\nstream applications, as each generated token is only condi-\ntioned on the previous tokens. For an autoregressive model\nto perform tasks like inpainting (\u201cfilling in the middle\u201d),\none must re-arrange the data during training [19].\nIn language, masked modeling has been used exten-\nsively as a pre-training procedure for high-quality seman-\ntic representations [20]. This procedure has also been ex-\ntended for representation learning in images [21] and au-\ndio [22].\nMasked modeling for representation learning\ngenerally has a constant mask probability. For example,\nin BERT [20], tokens are masked 15% of the time during\ntraining. It has been shown that this approach is equiva-\nlent to a single-step discrete diffusion model [23], that uses\nmasking for its noising procedure. Therefore, we can ex-\ntend masked modeling to masked generative modeling by\nvarying the probability of masking a token during training.\nThis was done for image generation in MaskGIT [4], and\nin language [23]. Similar to diffusion modeling [24, 25],\nwhich seeks to synthesize data starting from random noise\nthrough a series of denoising steps, masked generative\nmodeling seeks to synthesize data starting from completely\nmasked data through a series of \u201cunmasking\u201d steps.\nKey to the efficiency of MaskGIT and related ap-\nproaches is a parallel iterative decoding procedure. In par-\nallel iterative decoding, the model predicts every token in\nthe output sequence in a single forward pass. However,\nafter just one forward pass of the model, the output often\ndoes not have high quality. The output of the first sam-\npling step is re-masked, with a lower masking probability,\nand then put through the model again. In this way, masked\ngenerative models can efficiently refine their output, result-\ning in high quality generation.\nIn unconditional generation tasks, the model is asked\nto generate a realistic sample from the target data distribu-\ntion from scratch, without any guidance. This is a difficult\nproblem, as many target data distributions are highly multi-\nmodal. Unconditional generative models are susceptible to\nmode collapse [26], blurry samples, mode averaging, and\nother issues [27]. Therefore, some conditioning is helpful\nas it provides some signal for the model to resolve the mul-\ntimodality. Conditioning is also a commonly used method\nto guide the output of the system towards desired content.\nConditioning can take the form of a class label, a genre\ntag or lyrics [17], or an associated text description [3,8,28].\nConditioning can also be applied at every timestep, like\nthe semantic tokens of AudioLM [2], or aligned text or\nphonemes for text-to-speech generation [1].\nIn this work,we adopt a masked generative modeling\napproach with a parallel iterative decoding procedure, in-\nspired by work in vision such as MaskGIT [4] and Paella\n[5], as illustrated in Figure 1. We do not apply any con-\nditioning beyond that provided by the unmasked tokens in\nour encoded audio. As we show later, different approaches\nto masking, applied at inference time, can be used to steer\ngeneration in useful and artistic ways.\nIn training, tokens are masked randomly throughout the\nsequence. The model is then asked to predict the value of\neach of the masked tokens in a single forward pass, but it\nis conditioned on all of the unmasked tokens, both in the\nfuture as well as in the past. We vary the number of tokens\nFigure 2. Training, sampling, and prompting VampNet. Training: we train VampNet using Masked Acoustic Token\nModeling, where we randomly mask a portion of a set of input acoustic tokens and learn to predict the masked set of\ntokens, using a variable masking schedule. Coarse model training masks coarse tokens. Coarse-to-fine training only masks\nfine tokens. Sampling: we sample new sequences of acoustic tokens from VampNet using parallel iterative decoding,\nwhere we sample a subset of the most confident predicted tokens each iteration. Prompting: VampNet can be prompted in\na number of ways to generate music. For example, it can be prompted periodically, where every Pth timestep in an input\nsequence is unmasked, or in a beat-driven fashion, where the timesteps around beat markings in a song are unmasked.\nthat are masked during training, allowing us to generate\naudio at inference time through a sampling procedure. We\nnow describe our method in more detail.\n3. METHOD\nWe adapt the procedure of Masked Visual Token Modeling,\nproposed in MaskGIT [4] to audio, accounting for several\nkey differences between the vision and audio domain. We\ncall our approach Masked Acoustic Token Modeling.\n3.1 Masked Acoustic Token Modeling\nWe first train an audio tokenizer based on the techniques\ndescribed in DAC [15].\nUnlike the visual tokens of\nMaskGIT, our acoustic tokens are hierarchical in nature\ndue to residual vector quantization. As a first step, the au-\ndio signal x is encoded at each time step t as a a D di-\nmensional latent vector Z. We then quantize Z using N\nvector quantizers. Quantizer 1 produces \u02c6\nZ1, a quantized\napproximation of Z that has residual error R1 = Z \u2212 \u02c6Z1.\nThereafter, the residual from each quantizer i is passed to\nthe next quantizer i + 1, which produces a quantized ap-\nproximation of the remaining residual error: Ri \u2248\n\u02c6\nZi+1.\nVector Z is reconstructed by summing the output of the N\nquantizers: Z = PN\ni=1 \u02c6Zi.\nSince the encoded signal is represented as a quantized\nvector of N discrete tokens at each timestep, we have N\ntokens that can be masked or unmasked at each timestep.\nRather than attempt to generate all tokens at once, we in-\nstead split the N tokens into Nc \u201ccoarse\u201d tokens, and Nf\n\u201cfine\u201d tokens, as in AudioLM. We then train two generative\nmodels: one that generates the fine tokens given the coarse\ntokens as conditioning, and one that generates the coarse\ntokens given a sequence of coarse tokens. To generate a\nsample (Figure 1), we chain the two models together. First,\nwe apply the coarse model to generate a sequence of coarse\ntokens. Then, we apply the coarse-to-fine model to gener-\nate the fine tokens. We decode the tokens to a 44.1khz\nwaveform using the decoder of our audio tokenizer.\n3.2 Training procedure\nLet Y \u2208 RT \u00d7N be a matrix representing the output of the\nencoder for some audio segment. Each element yt,n in Y\nis a token from the nth level codebook at timestep t. Let\nYM be the set of all masked tokens in Y and YU be the\nset of all unmasked tokens in Y. The model generates a\nprobability distribution over the set of possible codebook\nvalues for each token y \u2208 YM, given the unmasked tokens\nand the model parameters \u03b8. The training objective is to\nmaximize the probability of the true tokens. This corre-\nsponds to minimizing the negative log likelihood.\nL = \u2212\nX\n\u2200y\u2208YM\nlog p(y|YU, \u03b8)\n(1)\nTo predict the masked tokens, we use a multi-layer bidi-\nrectional transformer, which predicts the probabilities of\neach possible token at every timestep, for every quantizer.\nIf each quantizer has a codebook size of C possible values,\nand there are N quantizers, then the last layer of the net-\nwork will be a fully connected layer of shape (E, CN),\nwhere E is the dimensionality of the output of the last\nlayer. We then reshape this output into (EN, C), and com-\npute the cross-entropy loss between the ground-truth one-\nhot token and the predicted token. Because the transformer\nis bidirectional, it can attend to all tokens in the input se-\nquence to optimize the loss for each token.\nFor the coarse-to-fine generative model, the input se-\nquence always contains Nc coarse tokens, and the masking\noperation is restricted to the Nf fine tokens. The last layer\nof this network only predicts masked fine tokens. Other-\nwise, the training procedure for both models is identical.\n3.3 Sampling\nWe follow the same iterative confidence-based sampling\napproach used in MaskGIT. More concretely, given YM as\nthe set of masked tokens and YU as the set of unmasked\ntokens, do:\n1. Estimate. For each masked token y in YM, estimate\nthe conditional probability distribution over its vo-\ncabulary of codebook values V .\n2. Sample. For each masked token, sample from the\ndistribution to generate an associated token estimate\n\u02c6y \u2208 V . We don\u2019t use any sampling tricks in this\nstep, sampling from the categorical probability dis-\ntribution for each token as-is.\n3. Rank by Confidence. Compute a confidence mea-\nsure for each of the sampled tokens by taking their\nprediction log-probabilities and adding temperature-\nannealed Gumbel noise to them:\nconfidence(\u02c6yt) = log(p(\u02c6yt)) + temp \u00b7 gt\n(2)\nwhere \u02c6yt is a token estimate at timestep t, gt is\nan i.i.d sample drawn from Gumbel(0,1) [29], and\ntemp is a hyperparameter that is linearly annealed to\n0 over the number of sampling iterations. Then, sort\nthe set of sampled token estimates by the confidence\ncomputed above. We find that high temperature val-\nues (e.g. > 6.0) result in higher quality samples.\n4. Select. Pick the number of tokens to mask at the\nnext sampling iteration, k, according to the mask-\ning schedule 5 . Take the k lowest confidence es-\ntimates and toss them out, re-masking their tokens.\nPlace the remaining high-confidence token estimates\nin YU, removing their tokens from YM.\n5. Repeat Return to step 1 until the number of itera-\ntions has been reached.\n3.4 Prompting\nInteractive music editing can be enabled by incorporating\nhuman guidance in the sampling procedure through the\nconditioning prompt of unmasked tokens. Because our ap-\nproach isn\u2019t conditioned on any signal other than the in-\nput audio itself, we find that various types of prompts are\nuseful for obtaining coherent samples, as they lower the\namount of multimodality when sampling from the model.\nLike AudioLM, we can prompt our model with prefix au-\ndio of some duration (usually between 1 and 4 seconds),\nand it will provide a continuation of that audio. Unlike Au-\ndioLM, and other auto-regressive approaches, we can also\nprompt our model with suffix audio, and it will generate\n5 k = \u03b3( t\ntT )D, where t is the current iteration, tT is the total number\nof iterations, and D the total number of tokens in the sequence. The\nscheduling function \u03b3 is a cosine schedule.\naudio that leads up into that suffix. We can provide prefix\nand suffix audio, and the model will generate the remaining\naudio, such that it is appropriate, giventhe specified prefix\nand suffix.\nWe can also apply a \u201cperiodic\u201d prompt, where all but\nevery Pth timestep are masked.The lower P is, the more\nthe generated audio will sound like the original, as the\nmodel is highly conditioned. For example if P = 2, then\nthe model is essentially behaving like a upsampler, imput-\ning the tokens for every other timestep. As P increases,\nthe model shifts from behaving in a compression mode to\na generative mode, creating variations that match the style\nof the original.\nAnother useful style of prompt are \u201ccompression\u201d\nprompts, where all codebooks other than the most coarse-\ngrained are masked. This gives the model strong condi-\ntioning on every timestep, so the model is likely to produce\naudio that closely matches the original. We can combine\nthis prompt with a periodic prompt with low P for even\nmore extreme compression ratios. Given the bitrate of the\ncodec B , which has number of codebooks N, a downsam-\npling rate P for the periodic prompt, and a number of kept\ncodebooks Nk, we can achieve a bitrate of B/P(N \u2212Nk).\nFinally, we can design music-specific prompts, which\nexploit knowledge about the structure of the music. More\nconcretely, we explore beat-driven prompting, where\ntimesteps that fall on or around the beat are left unmasked.\nThe model is left to create music between these beats,\nresulting in interesting variations on the original music.\nThese prompts can all be combined to create a very use-\nful music creation tool. In concert with a well designed\nuser interface, VampNet shows promise as the basis for a\nnext-generation music editing and creation suite.\n4. EXPERIMENTS\nOur experiments aim to evaluate VampNet\u2019s capability\nto both compress and generate music, given the various\nprompting strategies described in Section 3.4. For our ob-\njective audio quality measures, we use a multiscale mel re-\nconstruction error and the Fr\u00e9chet Audio Distance (FAD).\nMel-reconstruction error is defined as the L1 distance be-\ntween log-mel spectrograms at various time-scales,\nDF,M = || \u02c6SF,M \u2212 SF,M||1\n(3)\nwhere F is the FFT size of each spectrogram, and\nM is the number of mel-frequency bins. We use F \u2208\n[2048, 512] and M \u2208 [150, 80], with a hop size of 1\n4 the\nFFT size. Mel-reconstruction is valuable as a metric for\ncompression quality, but not for generation quality, since\nit is likely that models produce audio that does not match\none to one with the original target audio. For generation\nquality, we use FAD, which measures the overlap between\ndistributions of real and generated audio.\nUnlike mel-\nreconstruction, FAD is geared more towards evaluating if\nsample quality falls within the data distribution of the real\naudio, and can be used to evaluate generation quality.\nFigure 3. Mel reconstruction error (top) and Fr\u00e9chet Au-\ndio Distance (FAD, bottom) for VampNet samples taken\nwith varying numbers of sampling steps, taken using a pe-\nriodic prompt of P = 16. The samples were generated\nby de-compressing tokens at an extremely low bitrate (50\nbps), effectively generating variations of the input signals.\n4.1 Dataset\nSimilar to JukeBox [17], we collect a large dataset of pop-\nular music recordings. Our dataset consists of 797k tracks,\nwith a sampling rate of 32 khz. These tracks are resam-\npled to 44.1kHz to make compatible with our tokenizer.\nOur dataset contains music from thousands of artists across\ngenres described in Echo Nest\u2019s Every Noise at Once 6 .\nWe use a subset of 2k tracks for validation, and another\nsubset of 2k tracks for testing. We ensure that there is no\nartist overlap between train, validation, and test tracks. In\naddition, we collect a set of music and non-music data\n(speech, environmental sound), which we used to train\nour tokenizer, using the datasets described in DAC [15].\nAll audio is normalized to -24dbFS. We do not use any\nmetadata about these files during training, as our model is\ntrained unconditionally.\n4.2 Network Architecture and Hyperparameters\nThe audio tokenizer model we use takes as input 44.1kHz\naudio, and compresses it to a bitrate of 8kbps using 14\ncodebooks, with a downsampling rate of 768x. The latent\nspace therefore is at 57Hz, with 14 tokens to predict at ev-\nery timestep. We designate 4 of these tokens as the coarse\ntokens, and the remaining 10 as the fine tokens. Refer to\nthe Descript Audio Codec [15] for details on the tokenizer\narchitecture. We train the tokenizer for 250k steps.\nThe VampNet architecture (for both coarse and coarse-\nto-fine models) consists of a bidirectional transformer [18]\nwith relative attention [30] and an embedding dimension\nof 1280 and 20 attention heads. The coarse model has 20\nattention layers, while the coarse-to-fine model has 16. We\ntrain the coarse and coarse-to-fine model for 1M and 500k\nsteps, respectively. We train with the AdamW optimizer\n[31] with \u03b21 and \u03b22 set to 0.9 and 0.999, respectively. We\n6 https://everynoise.com/engenremap.html\nFigure 4.\nMultiscale Mel-spectrogram error (top) and\nFr\u00e9chet Audio Distance (FAD, bottom) for VampNet 10s\nsamples taken with a different types of prompts.\nuse the learning rate scheduler introduced by Vaswani et\nal [18] with a target learning rate of 0.001 and 10k warmup\nsteps. We use a dropout of 0.1, and a batch size of 25, with\na GPU memory budget of 72GB.\n4.3 Efficiency of VampNet\nWe first validate that VampNet can generate realistic music\naudio in a low number of steps. To do this, we run Vamp-\nNet using one of our prompts (the periodic prompt, with\nP = 16) on our test set, on 10-second excerpts. We vary\nthe number of sampling steps in [1, 4, 8, 12, 36, 64, 72], and\nreport metrics for each sampling step.\n4.4 Effect of prompts\nWe seek to understand how VampNet responds to different\nprompts, as discussed in Section 3.4. The prompts range\nfrom \u201ccompression\u201d prompts, which compress music to a\nlow bitrate, to more creative \u201cgenerative\u201d prompts. We ex-\namine whether compression and generative prompts exist\non a continuum, and whether decompression from low bi-\ntrates results in generative behavior.\nWe draw 2000 10-second examples from our evaluation\ndataset, encode them into token streams with our audio to-\nkenizer, and manipulate the token streams in four ways:\n1. Compression prompt:\nC codebooks are left un-\nmasked, starting from the coarsest codebook. All\nother tokens are masked. We set Nk = 1.\n2. Periodic prompt:\nevery Pth timestep is left un-\nmasked. In an unmasked timestep, tokens from ev-\nery codebook are unmasked. All other tokens (e.g.\ntokens in timesteps that do not correspond to the pe-\nriod P) are masked. We set P \u2208 [8, 16, 32].\n3. Prefix and suffix (inpaint) prompts: a segment at the\nbeginning and at the end of the sequence is left un-\nmasked. All other tokens are masked. This prompt\nis parameterized by a context length in seconds. We\nset the context to be either 1 second or 2 seconds,\nwhich corresponds to 57 or 114 timesteps.\n4. Beat-driven prompt: we first process the audio wave-\nform with a beat tracker [32]. Then, around each de-\ntected beat, we unmask timesteps to the right of the\nbeat. We examine a 75ms unmasked section around\neach beat, which is about 4 timesteps per beat.\nAfter manipulating the input token streams with our\nprompts, we generate new musical signals from these\nmasked token streams using VampNet, and compute FAD\nand mel-reconstruction error between the generated signals\nand the input signals from our music dataset. We include\na noisy token stream baseline, where a portion (as dictated\nby mask ratio r) of the tokens in the input token stream are\nreplaced with random tokens. We also include as baseline\nthe codec by itself, as well as the coarse-to-fine model.\nFinally, we examine how these prompts can be com-\nbined - specifically the compression and periodic prompts.\nBy manipulating the hyperparameters of these prompts (C\nand P), we can shift the model behavior from compression\nto generation. As more timesteps are masked, the model\nmust generate plausible musical excerpts that connect the\nunmasked timesteps, that may not match the input music.\n5. RESULTS AND DISCUSSION\nResults for our experiment varying the number of sam-\npling steps used to generate samples with VampNet are\nshown on Figure 3. We find that VampNet achieves the\nlowest FAD with 36 sampling steps, although 12 sampling\nsteps achieves comparable performance. In practice, we\nfind that samples taken with 24 steps achieve a fair trade-\noff between generation quality and compute speed, with\n10-second samples taking around 6 seconds to sample on\nan NVIDIA RTX3090. In contrast, to generate 10 seconds\nof audio with an autoregressive model would require 574\nsteps, which would take around 1 min to generate 10 sec-\nonds of audio, given an autoregressive model with the same\nnumber of parameters as ours, and the same tokenizer.\nResults for our study on the effect of each prompt are\nshown in Figure 4. First, we note that while the noisy token\nbaseline has comparable mel reconstruction to all prompts,\nit performs very poorly in terms of FAD. This indicates that\nwhile our prompting strategies may result in audio that is\nnot a perfect match to the original input audio, it still falls\ninside the distribution of plausible music.\nOf our proposed prompts, we find that beat-driven\nprompts perform best, achieving the lowest FAD of all\nprompts. A notable result here is that the periodic prompt\nwith P = 16 (35 conditioning timesteps) performs on par\nwith inpainting with 1 second of context (57 conditioning\ntimesteps). Therefore, prompt techniques that spread out\nthe conditioning tokens throughout the sequence (periodic\nprompts) are able to use fewer conditioning timesteps to\ngenerate samples of comparable quality to those generated\nby sampling techniques that place all of the conditioning\ntokens at the start and end of the sequences (inpainting).\nQualitatively, we also find that beat-driven prompts can\nkeep a steadier tempo than other prompts, though their out-\nputs tend to resemble the original music closer than peri-\nFigure 5. Mel-spectrogram error (top) and Fr\u00e9chet Audio\nDistance (FAD) (bottom) for VampNet samples at varying\nbitrates. A baseline is provided by replacing tokens in the\ninput sequence with random tokens, per noise ratio r.\nodic prompts. In practice, a mix of beat-driven, periodic,\nand inpainting prompts can be employed to steer of Vamp-\nNet in creative ways. To illustrate, we highly encourage\nthe reader to listen to the accompanying sound samples 7 .\nWe then combined periodic and compression prompting\nto show how the model\u2019s behavior shifts between recon-\nstruction and generation tasks, as more tokens are masked\naway. Results for this experiment are shown in Figure 5.\nAt higher bitrates, (600 bps and above), VampNet is able\nto accurately reconstruct the original music signal, achiev-\ning low mel-spectrogram error and FAD values with re-\nspect to the evaluation music audio. At bitrates of 200bps\nand below, VampNet has comparable reconstruction qual-\nity to the noisy token baselines, indicating that the sam-\npled VampNet signals no longer resemble the input audio\nin terms of fine-grained spectral structure. However, the\nFAD for VampNet samples at low bitrates is much lower\nthan the FAD for noisy baselines. This indicates that even\nthough VampNet isn\u2019t able to reconstruct the input music\nsignal at low bitrates, it is still able to generate coherent\naudio signals with musical structure, that are closer to the\ndistribution of \u201creal music\u201d than our noisy baseline.\n6. CONCLUSION\nWe introduced VampNet, a masked acoustic token mod-\neling approach to music generation. VampNet is bidirec-\ntional, and can be prompted a variety of ways using an\ninput audio file. Through different prompting techniques,\nVampNet can operate in a continuum between music com-\npression and generation, and is an excellent tool for gener-\nating variations on a piece of music. With VampNet, a mu-\nsician could record a short loop, feed it into VampNet, and\nhave VampNet create musical variations on the recorded\nidea every time the looped region repeats. In future work,\nwe hope to investigate the interactive music co-creation po-\ntential of VampNet and its prompting techniques, as well as\nexplore the representation learning capabilities of masked\nacoustic token modeling.\n7 audio samples: https://tinyurl.com/bdfj7rdx\n7. REFERENCES\n[1] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu,\nZ. Chen, Y. Liu, H. Wang, J. Li et al., \u201cNeural codec\nlanguage models are zero-shot text to speech synthe-\nsizers,\u201d arXiv preprint arXiv:2301.02111, 2023.\n[2] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov,\nO. Pietquin, M. Sharifi, O. Teboul, D. Grangier,\nM. Tagliasacchi, and N. Zeghidour, \u201cAudiolm: a lan-\nguage modeling approach to audio generation,\u201d arXiv\npreprint arXiv:2209.03143, 2022.\n[3] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti,\nA. Caillon,\nQ. Huang,\nA. Jansen,\nA.\nRoberts,\nM.\nTagliasacchi\net\nal.,\n\u201cMusiclm:\nGenerating\nmusic\nfrom\ntext,\u201d\narXiv\npreprint\narXiv:2301.11325, 2023.\n[4] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T.\nFreeman, \u201cMaskgit: Masked generative image trans-\nformer,\u201d in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022, pp.\n11 315\u201311 325.\n[5] D. Rampas, P. Pernias, E. Zhong, and M. Aubre-\nville, \u201cFast text-conditional discrete denoising on\nvector-quantized\nlatent\nspaces,\u201d\narXiv\npreprint\narXiv:2211.07292, 2022.\n[6] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov,\nN. Zeghidour, and M. Tagliasacchi, \u201cSoundstorm: Ef-\nficient parallel audio generation,\u201d 2023.\n[7] P. Esser, R. Rombach, and B. Ommer, \u201cTaming trans-\nformers for high-resolution image synthesis,\u201d in Pro-\nceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 2021, pp. 12 873\u201312 883.\n[8] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,\nand B. Ommer, \u201cHigh-resolution image synthesis\nwith latent diffusion models,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition., 2022.\n[9] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Syn-\nnaeve, Y. Adi, and A. D\u00e9fossez, \u201cSimple and control-\nlable music generation,\u201d 2023.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn,\nX. Zhai,\nT. Unterthiner,\nM. Dehghani,\nM. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is\nworth 16x16 words: Transformers for image recogni-\ntion at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[11] A. Van Den Oord, O. Vinyals et al., \u201cNeural discrete\nrepresentation learning,\u201d Advances in neural informa-\ntion processing systems, vol. 30, 2017.\n[12] C. G\u00e2rbacea, A. van den Oord, Y. Li, F. S. Lim,\nA. Luebs, O. Vinyals, and T. C. Walters, \u201cLow bit-rate\nspeech coding with vq-vae and a wavenet decoder,\u201d\nin ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 735\u2013739.\n[13] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and\nM. Tagliasacchi, \u201cSoundstream: An end-to-end neu-\nral audio codec,\u201d IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 30, pp. 495\u2013\n507, 2021.\n[14] A. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi, \u201cHigh\nfidelity neural audio compression,\u201d arXiv preprint\narXiv:2210.13438, 2022.\n[15] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and\nK. Kumar, \u201cHigh-fidelity audio compression with im-\nproved rvqgan,\u201d 2023.\n[16] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever\net al., \u201cImproving language understanding by genera-\ntive pre-training,\u201d 2018.\n[17] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, \u201cJukebox: A generative model for\nmusic,\u201d arXiv preprint arXiv:2005.00341, 2020.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin,\n\u201cAttention is all you need,\u201d Advances in neural infor-\nmation processing systems, vol. 30, 2017.\n[19] M. Bavarian,\nH. Jun,\nN. Tezak,\nJ. Schulman,\nC. McLeavey, J. Tworek, and M. Chen, \u201cEfficient train-\ning of language models to fill in the middle,\u201d arXiv\npreprint arXiv:2207.14255, 2022.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n\u201cBert:\nPre-training of deep bidirectional trans-\nformers for language understanding,\u201d arXiv preprint\narXiv:1810.04805, 2018.\n[21] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Gir-\nshick, \u201cMasked autoencoders are scalable vision learn-\ners,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp.\n16 000\u201316 009.\n[22] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin,\nR. Pang, and Y. Wu, \u201cW2v-bert:\nCombining con-\ntrastive learning and masked language modeling for\nself-supervised speech pre-training,\u201d in 2021 IEEE Au-\ntomatic Speech Recognition and Understanding Work-\nshop (ASRU).\nIEEE, 2021, pp. 244\u2013250.\n[23] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and\nR. van den Berg, \u201cStructured denoising diffusion mod-\nels in discrete state-spaces,\u201d Advances in Neural In-\nformation Processing Systems, vol. 34, pp. 17 981\u2013\n17 993, 2021.\n[24] Y. Song and S. Ermon, \u201cGenerative modeling by esti-\nmating gradients of the data distribution,\u201d Advances in\nneural information processing systems, vol. 32, 2019.\n[25] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion\nprobabilistic models,\u201d Advances in Neural Information\nProcessing Systems, vol. 33, pp. 6840\u20136851, 2020.\n[26] A. Srivastava, L. Valkov, C. Russell, M. U. Gutmann,\nand C. Sutton, \u201cVeegan: Reducing mode collapse in\ngans using implicit variational learning,\u201d Advances in\nneural information processing systems, vol. 30, 2017.\n[27] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung,\nA. Radford, and X. Chen, \u201cImproved techniques for\ntraining gans,\u201d Advances in neural information pro-\ncessing systems, vol. 29, 2016.\n[28] H. Chang,\nH. Zhang,\nJ. Barber,\nA. Maschinot,\nJ. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T.\nFreeman, M. Rubinstein et al., \u201cMuse: Text-to-image\ngeneration via masked generative transformers,\u201d arXiv\npreprint arXiv:2301.00704, 2023.\n[29] E. J. Gumbel, \u201cStatistical theory of extreme values\nand some practical applications; a series of lectures.\u201d\nWashington, 1954.\n[30] P. Shaw, J. Uszkoreit, and A. Vaswani, \u201cSelf-attention\nwith relative position representations,\u201d arXiv preprint\narXiv:1803.02155, 2018.\n[31] I. Loshchilov and F. Hutter, \u201cFixing weight decay\nregularization in adam,\u201d CoRR, vol. abs/1711.05101,\n2017. [Online]. Available:\nhttp://arxiv.org/abs/1711.\n05101\n[32] C. J. Steinmetz and J. D. Reiss, \u201cWaveBeat: End-to-\nend beat and downbeat tracking in the time domain,\u201d\nin 151st AES Convention, 2021.\n"
  },
  {
    "title": "Large Language Models for Supply Chain Optimization",
    "link": "https://arxiv.org/pdf/2307.03875.pdf",
    "upvote": "16",
    "text": "Large Language Models for Supply Chain Optimization\nBeibin Li1, Konstantina Mellou1, Bo Zhang2, Jeevan Pathuri2, and Ishai Menache1\n1Microsoft Research\n2Microsoft Cloud Supply Chain\nAbstract\nSupply chain operations traditionally involve a variety of complex decision making prob-\nlems. Over the last few decades, supply chains greatly benefited from advances in computa-\ntion, which allowed the transition from manual processing to automation and cost-effective\noptimization. Nonetheless, business operators still need to spend substantial efforts in ex-\nplaining and interpreting the optimization outcomes to stakeholders.\nMotivated by the\nrecent advances in Large Language Models (LLMs), we study how this disruptive technol-\nogy can help bridge the gap between supply chain automation and human comprehension\nand trust thereof. We design OptiGuide \u2013 a framework that accepts as input queries in\nplain text, and outputs insights about the underlying optimization outcomes. Our frame-\nwork does not forgo the state-of-the-art combinatorial optimization technology, but rather\nleverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we\nused supplier B instead of supplier A for a given demand?). Importantly, our design does\nnot require sending proprietary data over to LLMs, which can be a privacy concern in some\ncircumstances. We demonstrate the effectiveness of our framework on a real server place-\nment scenario within Microsoft\u2019s cloud supply chain. Along the way, we develop a general\nevaluation benchmark, which can be used to evaluate the accuracy of the LLM output in\nother scenarios.\n1\nIntroduction\nModern supply chains are complex, containing multiple tiers of suppliers, customers, and service\nproviders [1]. Optimization tools have been widely utilized for decision making in such supply\nchains. These tools not only automate some of the decision making processes, but also result\nin efficiency gains and substantial cost reductions across many industries [2]. However, some of\nthe automated processes require involving business operators, for understanding and explaining\ncertain decisions, providing what-if analysis, and even overriding some optimization outcomes.\nIn many cases, these operators are not equipped with the necessary background in optimization,\nresulting in time-consuming back-and-forth interactions with program managers, data scientists\nand engineers.\nLarge language models (LLMs) have recently emerged as a promising tool for assisting\nhumans with a wide variety of tasks, such as writing documents, presenting work, coding and\nhealth diagnosis [3, 4, 5]. Generative multimodal LLMs, such as OpenAI\u2019s GPT-4, are being\nrapidly integrated within co-pilots, for answering questions and increasing productivity through\nsimple, language based interactions with technology [6].\nIn this paper, we study how state-of-the-art LLMs can be applied for reasoning about\nsupply chain optimization. Using LLMs in our context is challenging. First, the underlying\noptimization problems are often large scale combinatorial optimization problems, and solving\n1\narXiv:2307.03875v2  [cs.AI]  13 Jul 2023\nShow me the shipping plan.\n\u00a0\nHere is the plan. You can hover your mouse on each node or\nedge to see more details.\nS1\nS2\nS3\nR1\nR2\nC1\nC2\nC3\n80\n50\n100\n20 L + 20 D\n30 L + 20 D\n10 L\n30 L + 100 D\n\u00c6\nIs it possible for Roastery 1 to be exclusively used by Cafe 2?\n\u00a0\nIf we exclusively transport beans from Roastery 1 to Cafe 2,\nthe cost would amount to $2570, representing an approxi-\nmate increase of 2570\u22122470\n2470\n\u2248 4% compared to the current\nplan, which costs $2470. Would you like to implement this\nchange for future planning purposes?\n\u00c6\nType your question here...\nFigure 1: OptiGuide interface.\nthem directly is currently out of reach for LLMs [4].\nSecond, one needs to align the large\nfoundation models to answer the domain-specific questions. Due to the large scale, fully training\nthese models is not possible, and even middle-ground solutions such as fine-tuning LLMs require\nsubstantial compute and engineering investments [7]. Last but not least, any use of LLMs in\nbusiness-critical operations, should have solutions when \u201cthings go wrong\u201d, including diagnosing\nof and recovering from mistakes and hallucinations [8].\nIn view of these challenges, we design and implement OptiGuide \u2013 a framework that employs\nLLMs to interpret supply chain optimization solutions. A key idea behind OptiGuide is not\nto replace optimization technology by LLMs, but rather use optimization solvers in tandem\nwith LLMs. In our design (see Figure 2 for system architecture), the LLM is responsible for\ntranslating the human query to \u201coptimization code\u201d, which is in turn used by an optimization\nsolver to produce the necessary output; the output then passes through the LLM for producing\nthe answer in human language (English). This architecture is used both for textual explanations\nand visualizations of the optimization solution, as well as for answering what-if queries. To\naddress what-if queries, OptiGuide uses the LLM to appropriately modify the input to the\noptimization solver, and then reruns the solver under the hood to produce an answer.\nTo enable OptiGuide, we solve multiple technical challenges. First, we circumvent all forms\nof costly training, by applying in-context learning, namely \u201cteaching\u201d the LLM about the\ndomain directly through the query\u2019s prompt (i.e., as part of the inference). This requires careful\nco-design of the optimization code and the prompt with the understanding that the prompt can\nbe space constrained. For example, we write the code in certain functional form that can be\nefficiently mapped to questions asked by humans. We also design a simple safeguard mechanism\nthat confronts output mistakes.\nTo evaluate the effectiveness of OptiGuide, we introduce an evaluation benchmark that\nincludes (i) a variety of common supply chain scenarios, and (ii) an evaluation methodology\nthat incorporates new metrics for quantifying accuracy, generalizability within a scenario, and\nextrapolation capability to unseen scenarios. We test OptiGuide on five different scenarios and\n2\n\u00a0 User\nCoder\nSafeguard\nInterpreter\nAgents\nLLM\nHelper\nSolver\nDatabase\nDocument\nApp Specific Components\n1 User question\n2 ICL\n3 Code\n4 Input\n5 Output Logs\n6 Result\n7 Answer\n8 Final answer\nFigure 2: The OptiGuide framework.\nobtain 93% accuracy on average using GPT-4. We view the benchmark and methodology as\ncontributions that stand on their own, and can be used to evaluate future approaches. We\nare in the process of open-sourcing our benchmark.\nFinally, we deploy OptiGuide for the\nserver deployment optimization used in Microsoft Azure\u2019s supply chain. We discuss some of the\nengineering challenges, and report initial promising results from our evaluation.\nWe believe that this paper sets important foundations, which can be used by other organiza-\ntions for explaining optimization outcomes through LLMs. There are several future directions\nthat emerge from our study, for example, using smaller models that can be trained with modest\nresources. As a longer-term goal, it is natural to expand the scope of LLMs beyond explainabil-\nity, to facilitate interactive optimization (e.g., \u201cplease provide a more load-balanced solution\u201d,\n\u201cplease use at most two suppliers\u201d). With the constant advances of LLM technology, it will\nbe fascinating to examine whether LLMs can be utilized not only as translators, but also for\nrefining and improving optimization outcomes.\nThe rest of the paper is organized as follows. In Section 2, we provide the necessary back-\nground on supply chain optimization and current LLM technology. In Section 3, we describe the\ndesign of OptiGuide. Section 4 describes our evaluation benchmark, and OptiGuide\u2019s evaluation\nresults. In Section 5, we outline our findings from OptiGuide\u2019s deployment in Azure\u2019s supply\nchain. We discuss future perspectives in Section 6.\n2\nBackground and Motivation\nIn this section, we provide brief background on decision making in supply chain operations, and\nelaborate on the notion of explainability. We then describe current capabilities and limitations\nof LLMs, and conclude with a simple supply chain example, which will be useful for explaining\nour solution approach.\n3\n2.1\nDecision Making in Supply Chains\nA supply chain may be defined as \u201can integrated network of facilities and transportation options\nfor the supply, manufacture, storage, and distribution of materials and products\u201d [9]. A simple\nsupply chain may consist of a company (e.g., a service provider) and the set of its suppliers and\ncustomers [1]. However, most supply chains nowadays contain multiple tiers with suppliers of\nsuppliers, customers of customers, and hierarchies of service providers [1]. This results in highly\ncomplex global networks where decisions must be optimized across multiple layers to satisfy\ncustomer demand while guaranteeing operational efficiency.\nDecision making in supply chains spans different time-scales: starting from the design of the\nsupply chain network (e.g., location of factories), planning (e.g., procurement of supply), and\nexecution (e.g., transportation of goods). This leads to many types of decisions; a few examples:\n\u2022 How many factories should we open, where, and with what manufacturing capacity?\n\u2022 What suppliers should we use?\n\u2022 How much inventory should we keep in stock and at which locations?\n\u2022 How should we transport intermediate and finished goods efficiently?\nThe complexity of the decision-making often requires the design of optimization approaches\nthat can incorporate a multitude of constraints and objectives, and still generate good quality\nsolutions in plausible running times. To this end, different aspects of the supply chain (facility\nlocation, inventory planning, routing) may be optimized separately or considered jointly (e.g.,\ninventory planning integrated with routing [10]). Common solution approaches for these op-\ntimization problems include Mixed Integer Programming based techniques and heuristics that\ncan tackle the large scale of the problem.\n2.2\nExplainability\nBusiness operators and planners involved in decision-making need to maintain a good under-\nstanding of the optimization outcomes. This allows them to not only address customer questions,\nbut also react to unexpected events, and resolve inefficiencies and bottlenecks. However, the\nunderstanding is often challenging due to the complexity of the decision process (e.g., large\nscale, solution obtained by \u201cblack-box\u201d algorithm, etc.) and lack of optimization expertise.\nFor concreteness, we provide below some examples of questions that operators may wish to\nanswer.\nQ1\nWhat is the cost breakdown for each fulfilled demand?\nQ2\nHow much excess inventory have I had per month in the past year?\nQ3\nWhat would happen if the demand at a particular location increased by 10%?\nQ4\nCan I reduce a factory\u2019s manufacturing capacity by 5% and still meet the demand?\nQ5\nWhy was a particular supplier selected for a demand?\nQ6\nHow would selecting a different transportation option affect the delivery timelines and the\noverall cost?\nThese and other questions aim at explaining the outcome of supply chain decisions.\nThey\ninclude analyzing the current solution (input and output), investigating historical trends, and\nexploring what-if scenarios.\n4\nObtaining insights on optimization decisions may require involving multiple professionals\nwith different roles. Suppose that planners may wish to understand why a demand has not\nbeen fulfilled on time. They often surface the concern to the program managers, who involve\ndomain experts, such as data scientists or the engineers that developed the optimization system.\nThe domain experts in turn may need to write additional code and often rerun the optimization\nto extract the relevant insights.\nThis overall process might be very time-consuming for all\nparties involved and can cause significant delays in the decision making process.\nIn some applications, teams maintain some custom tools that allow decision makers to reason\nabout certain decisions. For example, application dashboards can provide visualizations or even\nallow enforcing some actions (e.g., fix a specific supplier for a demand). However, given the\nengineering overhead of maintaining the tools, they are typically limited to the most common\nuse cases.\nThe notion of explainability is certainly not novel, and has drawn attention in both academia\nand industry. There have been numerous studies on explaining ML/AI [11, 12]. In the opti-\nmization context, IBM Decision Optimization [13] provides answers to a fixed set of queries\nthat the user may choose to activate. See also [14] and references therein.\n2.3\nLarge Language Models\nOverview.\nA large language model (LLM) is a foundation model [15] trained on extensive\ntext data using deep learning techniques, such as Transformer neural networks; ELMo [16],\nBERT [17], Turing NLG [18, 19], GPT-3 [20], GPT-4 [3], PaLM [21], PaLM-E [22], LLaMA\n[23], and Vicuna [24] are some examples of widely used LLMs. In the training phase, a LLM\nlearns statistical patterns, word relationships, and contextual information from diverse sources,\nsuch as books, articles, websites, and code repositories. LLMs are used for a variety of tasks in\nthe inference phase [4], including chatbots, translation, writing assistance, coding [25, 26, 27],\nplanning [28], poem and story composition.\nUsing LLMs in applications.\nMultiple strategies can be employed to adapt LLMs for a\nspecific application.\nThe most common approaches are fine-tuning and in-context learning.\nFine-tuning is a classic approach for \u201ctransfer learning\u201d aimed at transferring knowledge from\na pre-trained LLM to a model tailored for a specific application [29]. Typically, this process\ninvolves tweaking some weights of the LLM. While fine-tuning approaches can be made efficient\n[30, 31], they still necessitate model hosting in GPUs. This requirement can prove excessively\ncostly for many applications. In-context learning [32] is an alternative cheaper approach, which\ninvolves incorporating a few training examples into the prompt (or query). The idea here is to\nappend the prompt with domain-specific examples and have the LLM learn from these \u201cfew-\nshot\u201d examples. A key advantage of this approach is that it does not require model parameter\nupdates.\nPrompt engineering.\nIn a production setting, developers often send prompts (aka, queries)\nto the model, which can be appended with domain-specific examples for obtaining higher-quality\nanswers. A collection of prompt management tools, such as ChatGPT Plugin [33], GPT function\nAPI call [34], LangChain [35], AutoGPT [36], and BabyAGI [37], have been designed to help\nengineers integrate LLMs in applications and services. The prompt size is measured in the\nnumber of tokens, which is proportional to the query size. LLMs can only process a limited\nnumber of tokens because of resource limitations, which is a strict constraint that developers\nand tools need to find workarounds for.\n5\nS1\nS2\nS3\nR2\nR1\nC2\nC1\nC3\n$5\n$4\n$6\n$3\n$2\n$7\n$5\n$3\n$6\n$4\n$5\n$2\nLight: $3\nDark: $5\nLight: $5\nDark: $6\nLight: 20\nDark: 20\nDemand (units)\nLight: 30\nDark: 20\nLight: 40\nDark: 100\nShipping Cost\n(per unit)\nShipping Cost\n(per unit)\nRoast Cost\n(per unit)\n150\n50\n100\nSupply (units)\n(a) Problem setup.\nS1\nS2\nS3\nR2\nR1\nC2\nC1\nC3\n80\n50\n100\n20 Light\n20 Dark\n30 Light\n20 Dark\n10 Light\n30 Light\n100 Dark\n(b) Optimal plan (units).\nFigure 3: A simple supply chain example: coffee roasting company.\nPrivacy.\nUsing domain-specific information in the prompt may involve proprietary data,\nwhich users may prefer not to reveal to LLM hosts. Even if LLM providers offer service level\nagreements (SLAs) for privacy, passive eavesdropping attackers might still intercept the data.\nTherefore, many organizations would prefer utilizing LLMs in a privacy-preserving way, namely\nkeeping the proprietary data in-house.\nMistakes.\nNaturally, LLMs might provide sub-optimal outcomes, such as inaccuracies and\neven hallucinations [38]. There are generic tools that tackle this problem [39, 40, 41], however\none may need domain specific tools for better outcomes. One example is fixing code generated\nby LLMs [42, 43, 44, 45].\n2.4\nA Simple Example\nWe now describe a simple supply chain example that will be useful for illustrating our approach.\nThe supply chain.\nConsider a coffee roasting company that roasts two types of coffee (light\nand dark roast). The company sources coffee beans from three different suppliers, it roasts them\nin one of its two roasting facilities, and then ships them to one of its three retail locations for\nselling to customers. The goal is to fulfill the demand in each retail location, while minimizing\nthe total cost. The total cost consists of the cost of purchasing the coffee from the suppliers, the\nroasting cost in each facility, and the shipping cost of the end product to the retail locations.\nAn illustration is given in Figure 3.\nModel formulation.\nWe can model this problem as a Mixed Integer Program.\nLet xs,r\ndenote the number of units purchased from supplier s for roasting facility r, and yL\nr,\u2113 and yD\nr,\u2113\nthe amount of light and dark roast sent to retail location \u2113 from roasting facility r. Each supplier\ns has a capacity Cs, and each retail location \u2113 has demand DL\n\u2113 and DD\n\u2113 for light and dark roast\nrespectively. There is a cost cs,r for each unit purchased from supplier s for roasting facility\nr, a shipping cost of gr,\u2113 for each unit sent to retail location \u2113 from roasting facility r, and a\nroasting cost hL\nr and hD\nr per unit of light roast and dark roast respectively in facility r. The\n6\noptimization problem is the following:\nminimize\n\u0010 X\ns,r\nxs,r \u00b7 cs,r +\nX\nr,\u2113\nyL\nr,\u2113 \u00b7 hL\nr +\nX\nr,\u2113\nyD\nr,\u2113 \u00b7 hD\nr +\nX\nr,\u2113\n(yL\nr,\u2113 + yD\nr,\u2113) \u00b7 gr,\u2113\n\u0011\n(Objective)\nsubject to\nX\nr\nxs,r \u2264 Cs\n\u2200s\n(Supplier capacity constraint)\nX\ns\nxs,r =\nX\n\u2113\n(yL\nr,\u2113 + yD\nr,\u2113)\n\u2200r\n(Conservation of flow constraint)\nX\nr\nyL\nr,\u2113 \u2265 DL\n\u2113\n\u2200\u2113\n(Light coffee demand constraint)\nX\nr\nyD\nr,\u2113 \u2265 DD\n\u2113\n\u2200\u2113\n(Dark coffee demand constraint)\nxs,r, yL\nr,\u2113, yD\nr,\u2113 \u2208 Z+\n\u2200s, r, \u2113\n(Integrality constraint)\nExplainability.\nLet us now zoom into the example from Figure 3. The optimal solution is\ndepicted in Figure 3b. We see that in the optimal plan, both roasteries produce light and dark\ncoffee; the first roastery sources its beans from supplier 3, while the second from suppliers 1\nand 2. The first two retail locations then obtain all their coffee from the first roastery, while\nthe third retail location is supplied by both roasteries. A user may ask the following questions:\nQ1\nWhat would happen if the demand at retail location 1 increased by 10%?\nQ2\nWhat would happen if the demands at all retail locations doubled?\nQ3\nWhy are we using supplier 3 for roasting facility 1?\nQ4\nCan I use roasting facility 1 only for retail location 2?\nQ5\nWhat if supplier 3 can now provide only half of the quantity?\nQ6\nThe per-unit cost from supplier 3 to roasting facility 1 is now $5. How does that affect\nthe total cost?\nQ7\nWhy does Roastery 1 produce more light coffee than Roastery 2?\nQ8\nWhy does supplier 1 ship more to Roastery 2 than Roastery 1?\nQ9\nWhy not only use one supplier for Roastery 2?\n3\nThe LLM Framework\nLarge-scale supply chain management entails multiple functions, such as extensive data gather-\ning, data processing and analysis, optimization processes and communication and enforcement\nof decisions across multiple stakeholders. While LLMs and supporting tools may handle part\nof these functions, there is a need for an end-to-end framework that will address the underly-\ning challenges in a systematic way. In this section, we describe the design of our framework,\nOptiGuide.\n7\n3.1\nSystem Overview\nThe OptiGuide framework, depicted in Figure 2, consists of three sets of entities: agents, LLMs,\nand application-specific components. When a user poses a question ( 1 ), the coder takes the\nquestion and formulates it as an in-context learning (ICL) question ( 2 ) for the LLM. The\nLLM then generates code ( 3 ) to answer the question. The safeguard checks the validity of\nthe code and aborts the operation in case of a mistake; otherwise the safeguard feeds the code\nto an application specific component ( 4 ), such as a database engine or an optimization solver\n(depending on the query). The component processes the code and produces results, which are\nlogged in a file ( 5 ). We note that obtaining the final result may involve multiple iterations ( 2\nto\n5 ) where the query is automatically refined until the desired output is achieved. Finally,\nthe output logs from the component are fed back into the LLM ( 6 ). The LLM analyzes the\nlogs and generates a human-readable answer ( 7 ) that is sent back to the user ( 8 ). We now\nprovide an overview of the different entities and components. More details can be found in\nAppendix B.\n3.1.1\nAgents\nHi, GPT, read the following sample Q&As and documentation of helper code.\nSample questions and code answers below:\nquestion: Is it possible for Roastery 1 to be exclusively used by Cafe 2?\ncode: \nfor c in cafes:\n    if c != \"cafe2\":\n        m.addConstr(y_light[\"roastery1\", c] == 0, \"\")\n        m.addConstr(y_dark[\"roastery1\", c] == 0, \"\")\nquestion: What would happen if roastery 2 produced at least as much light coffee as roastery 1?\ncode:\nm.addConstr(sum(y_light['roastery1',c] for c in cafes) \\\n            <= sum(y_light['roastery2',c] for c in cafes), \"_\")\n\u2026\nSome helpful helper documentation below, which you can use:\nplot_shipping_decision() -> str:\n# Plot the shipping decisions and network to a figure\n# Outputs:\n#     filename (str): the path of the plotted visualization.\n# \u2026\nQuestion: What if we prohibit shipping from supplier 1 to roastery 2? Show me the new plan and compare with the previous \nresult\nCode: \nm.addConstr(x['supplier1', 'roastery2'] == 0, 'force not ship\u2019)\nm.optimize()\nplot_shipping_decision()\nprint(f'The gap is ${m.objVal - prev_obj}. ')\nPrompt\nOutput\nFigure 4: Coder prompt for the running example\nAgents facilitate the interaction between users, the LLM, and application-specific compo-\nnents. The coder converts raw user questions into specific ICL queries. The conversion includes\nsupplying the application context, providing ample training examples, and restructuring the\nuser\u2019s query, as exemplified in Figure 4. The safeguard operates as a quality control checkpoint.\nIt scrutinizes the code for potential discrepancies and initiates self-debugging upon encounter-\ning failures. When OptiGuide cannot successfully address a query, the safeguard would either\ninitiate a new iteration with a proposed fix, or generate an error message for the user. The\ninterpreter takes the output logs, tables, graphs, etc., and generates a human friendly response\nto the user\u2019s query.\n8\n3.1.2\nApplication Specific Components\nDifferent applications may have different types of components; we provide an overview of the\nmost common ones. OptiGuide is designed in a modular way, so that using OptiGuide for a\ndifferent application requires only switching to a new set of components.\nThe database is a systematically arranged collection of data in various formats, such as CSV,\nSQL, JSON, Parquet, which are queried to extract answers. The solver can be a commercial\ninteger programming solver, such as Gurobi. OptiGuide can query the solver output directly,\nor the output can be stored and queried from the database. If a question demands profound\ndomain knowledge or historical context, OptiGuide consults documents to enhance the depth\nand relevance of the response. The helper is an optional component. It consists of a set of\nfunctions written by application engineers, for simplifying the code produced by LLMs. For\nexample, a complex data analysis workflow can be simplified to a single helper function call.\nSomeone asked \u201cWhat if we prohibit shipping from supplier 1 to roastery 2? Show me the new plan and compare with \nthe previous result\u201d.\nHere are some results I found from coding.\n--------- Terminal Outputs are Below ---------\nFound heuristic solution: objective 3280.0000000\nPresolve removed 2 rows and 1 columns\nPresolve time: 0.00s\nPresolved: 10 rows, 17 columns, 33 nonzeros\nVariable types: 0 continuous, 17 integer (0 binary)\nFound heuristic solution: objective 3275.0000000\nRoot relaxation: objective 2.760000e+03, 10 iterations, 0.00 seconds (0.00 work units)\nExplored 1 nodes (10 simplex iterations) in 0.01 seconds (0.00 work units)\nThread count was 32 (of 32 available processors)\nSolution count 3: 2760 3275 3280 \nOptimal solution found (tolerance 1.00e-04)\nBest objective 2.760000000000e+03, best bound 2.760000000000e+03, gap 0.0000%\nOptimal cost: 2760.0\nplan plotted in \"C:/Users/abc/new_solution.png\".\nThe gap is $290.\n--------- Terminal Outputs are Above ---------\nPlease use simple English and explain the answer in HTML format.\nThe revised cost now amounts to $2760, indicating an 11.7% increase compared to the previous plan's cost of $2470.\n<br>\nSee the new shipping plan: \n<img src=\"C:/Users/abc/new_solution.png\" alt=\"New Shipping Plan\">\n<br>\nPrompt\nOutput\nFigure 5: Interpreter prompt for the running example\n3.2\nA Running Example\nWe illustrate OptiGuide\u2019s data flow via the user question, \u201cWhat if we prohibit shipping from\nsupplier 1 to roastery 2? Show me the new plan and compare with the previous result\u201d. First,\nthe coder converts this question into an in-context learning query for the LLM, see Figure 4\nfor the prompt. In addition to the question itself, the prompt contains (i) training examples,\nnamely pairs of questions and code answers, and (ii) a documentation of the helper functions.\nIntuitively, (ii) supplements (i) by providing additional context into what the code does.\nSubsequently, the LLM generates code that adds a new constraint (green region in Figure\n4). The safeguard then extracts the code from the LLM\u2019s response, and OptiGuide calls the\noptimization solver to resolve the planning problem, yielding a result depicted in the yellow\nregion in Figure 5. This result is then fed into the LLM by the interpreter, which produces a\n9\nresponse. Finally, OptiGuide presents the response to the user alongside a visualization of the\nplan (green region in Figure 5) and a comparison with the original cost. Note that OptiGuide\npreserves privacy, since the domain-specific data remains in either the solver or database, and\nis never transferred to the LLM. Additional examples are provided in Figure 6.\nIs it possible for Roastery 1 to be exclusively used by Cafe 2?\n\u00a0\nWe need to write Python code to test the what-if scneario.\nfor c in cafes:\nif c != \"cafe2\":\nm.addConstr(\ny_light[\"roastery1\", c] == 0, \"\")\nm.addConstr(\ny_dark[\"roastery1\", c] == 0, \"\")\nThe result has m.objV al = $2570, which is higher than\n$2470 from the previous solution.\nIf we exclusively transport beans from Roastery 1 to Cafe 2,\nthe cost would amount to $2570, representing an approxi-\nmate increase of 2570\u22122470\n2470\n\u2248 4% compared to the current\nplan, which costs $2470. Would you like to implement this\nchange for future planning purposes?\n\u00c6\nNo. Who is the contact person for supplier 1?\n\u00a0\nWe can use SQL command to find out the information.\nSELECT contact FROM suppliers WHERE id=\"supplier1\";\nSafeguard Alert: sensitive information. Approval required!\nTicket submitted.\nTo answer this question, we need approval from administrator\nto access privileged information. We just created a ticket and\nwill get back to you in 12 hours...\n\u00c6\nWaiting for response...\nFigure 6: An illustration of questions answered by OptiGuide. The\ngray dashed boxes\nrep-\nresent thoughts that occur in the backend. Users can configure OptiGuide to display these\nthoughts or not.\n4\nEvaluation Benchmark\nIn this section, we develop a benchmark for evaluating the performance of our framework on a\nvariety of supply chain optimization problems. The benchmark and the methodology around it\ncan guide future efforts for using LLMs in supply chain optimization.\n10\n4.1\nScenarios and Data\nTo evaluate our framework, we selected a variety of optimization problems that capture multiple\ntypes of decisions that may be relevant in different supply chain settings.\nSpecifically, our\ndataset includes a facility location scenario, a multi-commodity network flow for distribution\nof products, workforce assignment optimization, the traveling salesman problem, as well as the\ncoffee distribution scenario from Section 2.4. The code for all problems is in Python and the\nGurobi optimization solver [46] is used to obtain the optimal solution; Appendix C provides the\ncode for the coffee distribution problem as an example.\nOur next step is to generate a repository of questions and code answers for each scenario.\nSome of these question-answer pairs will be used as examples for in-context learning, while others\nfor evaluating OptiGuide\u2019s performance. To create a large set of questions, we write macros for\neach question, which results in generating question sets of closely related question-answer pairs.\nAn example of a macro for a question set is the following:\nQUESTION: What if we prohibit shipping from {{VALUE-X}} to {{VALUE-Y}}?\nVALUE-X: random.choice(suppliers)\nVALUE-Y: random.choice(roasteries)\nGROUND-TRUTH: model.addConstr(x[{{VALUE-X}}, {{VALUE-Y}}] == 0)\nIn order to increase the diversity in the question sets, we also ask GPT to rephrase the\nquestions while preserving their meaning. For instance, GPT might rephrase the generated\nquestion \u201cWhy would we ship beans from Supplier 1 to Roastery 2\u201d to \u201cWhat benefits are\nassociated with the choice of shipping beans from Supplier 1 to Roastery 2?\u201d.\nWe note that the question sets for all problems that are used in the benchmark were created\nfrom scratch and kept in house, so that the LLMs have not observed these data as part of their\ntraining.\n4.2\nEvaluation Methodology\nThe goal of our evaluation is to assess the accuracy of LLMs in answering user questions for\nsupply chain optimization problems. Unfortunately, existing metrics, such as pass@k which is\nused for analyzing coding accuracy [27, 47], are not well suited for explainability through code\n(intuitively, the metrics are \u201ctoo forgiving\u201d). We therefore propose a different methodology\nwhich is inspired by the unit-test approach used in software development.\nOur evaluation proceeds as follows. For each scenario we run R experiments. Each exper-\niment consists of T question sets. Each question set consists of Q test questions and answers.\nThe LLM is asked to write the code and answer for a test question; it is given three chances\nto produce a response in case of an evident error (runtime or syntax). We then evaluate the\ncorrectness of the final answer. Note that we do not necessarily evaluate whether the generated\ncode matches exactly with our ground-truth code, as there are different ways to obtain the\ncorrect response. The following example demonstrates a scenario where the generated code is\nquite different, but the optimization outcome would be the same.\n1. model.addConstr(x[\u2019supplier1\u2019, \u2019roastery2\u2019] == 0, \u2019force not ship\u2019)\n2. shipping_cost_from_supplier_to_roastery[(\u2019supplier1\u2019, \u2019roastery2\u2019)] = 1e10\nAccuracy.\nWe define the accuracy metric AC as the average success rate across all scenarios,\nexperiments and question sets. Formally,\nAC =\n1\nSR\nS\nX\ns=1\nR\nX\nr=1\n1\nTs\nTs\nX\nt=1\n1(qt),\n11\nQUESTION: \n    Can we use caf\u00e9 {{VALUE-X}} for roastery {{VALUE-Y}}\nVALUE-X: random.choice(cafes)\nVALUE-Y: random.choice(roasteries)\nCode: m.addConstr(x[{{VALUE_X}}, {{VALUE_Y}}] == 1)  \nGenerate 30 distinct questions from Macro\n(No repetition)\nQUESTION: Can we use caf\u00e9 S1 for roastery D1\nCode: m.addConstr(x[\u201cS1\u201d, \u201cD1\u201d]== 1)  \nQUESTION: Can we use caf\u00e9 S9 for roastery D9\nCode: m.addConstr(x[\u201cS9\u201d, \u201cD9\u201d]== 1)  \nQUESTION: Can we use caf\u00e9 S1 for roastery D2\nCode: m.addConstr(x[\u201cS1\u201d, \u201cD2\u201d]== 1)  \nRewrite the question with LLM\n(Code answer remains the same)\nShould we use S1 for D1\nWhat can happen if S1 is used for D1\nFor D1, how about using S1?\n\u2026\n\u2026\nQuestion Set \ud835\udc61 \nIs S1 usable for D2?\nIs it appropriate to use S1 for D2?\nCould S1 serve as a solution for D2?\n\u2026\nIs it feasible to employ S9 for D9?\nCan D9 be handled by using S9?\nCan D9 benefit from the usage of S9?\n\u2026\nTest Question\nExamples\n(select for the prompt)\nFigure 7: In-distribution evaluation\nwhere qt is the question set, and 1(qt) is the indicator whether it passed successfully. The LLM\npasses a question set if and only if it successfully answers all questions in the question set.\nIn-distribution and out-of-distribution evaluation.\nAs common practice, we evaluate\nour framework in both \u2018in-distribution\u2019 and \u2018out-of-distribution\u2019 [48] settings. For in-distribution\nevaluation (Figure 7), the test question and the examples used in the prompt are from the same\nquestion set. In contrast, for out-of-distribution evaluation (Figure 8), the example questions\nare extracted from different question sets.\nExample selection.\nAs the number of tokens that can be provided as input to the LLMs is\nlimited, we explore different approaches for selecting the training examples for each query. The\napproaches can be evaluated both for in-distribution and out-of-distribution evaluation. One\napproach is random selection, where a fixed number of example questions is selected uniformly\nat random. Another approach is based on nearest neighbors, where we select examples that are\nsimilar to the test question; similarity is based on the text embedding [49] of the questions as\ndetermined by the model text-embedding-ada-002 [20]. We also experiment with different sizes\nof the example set (0, 1, 3, 5, or 10 examples).\n4.3\nPerformance\nSetup.\nFor each scenario s, we run R = 10 experiments. In each experiment we evaluate\nTs \u2265 10 question sets. Each question set qt usually contains 10 \u2212 30 questions and answers.\nWe use both text-davinci-003 [20] and GPT-4 [3] for our evaluation. Performance results across\ndifferent LLMs, example selection approaches, and example set sizes are summarized in Table\n1.\nObservations.\nGPT-4 consistently outperforms text-davinci-003 in both in-distribution and\nout-of-distribution evaluation. As expected, both models show higher accuracy on in-distribution\n12\nQuestion \nSet 1\nQuestion \nSet 2\nQuestion \nSet T\n\u2026\nQuestion 1-i\nQuestion 1-ii\nQuestion 1-xxx\n\u2026\nQuestion 2-i\nQuestion 2-ii\nQuestion 2-xv\n\u2026\nQuestion T-i\nQuestion T-ii\nQuestion T-xxx\n\u2026\nGenerate and Rewrite\nTest Question\nExamples\n(select for the prompt)\nFigure 8: Out-of-distribution evaluation\nTable 1: Accuracy across different LLMs, example selection approaches, and example set sizes.\nEach experiment was run 10 times and the average accuracy is reported.\nIn-distribution\nOut-of-distribution\n# Examples\nModel\nRandom\nNearest\nRandom\nNearest\n0\ntext-davinci-003\n0.32\nGPT-4\n0.59\n1\ntext-davinci-003\n0.78\n0.78\n0.39\n0.44\nGPT-4\n0.85\n0.90\n0.66\n0.66\n3\ntext-davinci-003\n0.90\n0.92\n0.49\n0.44\nGPT-4\n0.90\n0.92\n0.74\n0.69\n5\ntext-davinci-003\n0.93\n0.93\n0.52\n0.48\nGPT-4\n0.92\n0.93\n0.78\n0.73\n10\ntext-davinci-003\n0.92\n0.93\n0.67\n0.61\nGPT-4\n0.93\n0.93\n0.84\n0.80\n13\ncompared to out-of-distribution evaluation. GPT-4 performs relatively much better in out-of-\ndistribution evaluation, demonstrating its stronger reasoning and generalization capabilities;\nanother sign for these capabilities is the 59% accuracy even without any training examples.\nIncreasing the number of examples results in improved accuracy across the board. We also note\nthat the gap between text-davinci-003 and GPT-4 decreases with the size of the example set.\nThe nearest neighbor selection approach yields slight performance improvements for in-\ndistribution evaluation. Interestingly, when the size of the example set is greater than one,\nrandom selection outperforms nearest neighbor for out-of-distribution evaluation. One explana-\ntion here is that selecting examples based on text similarity results in overfitting, and random\nselection results in more diverse training examples.\n5\nOptiGuide for Azure\u2019s Supply Chain\nIn this section, we demonstrate OptiGuide\u2019s capabilities on the server fulfillment supply chain\nof Microsoft Azure. We start with providing the necessary details for the decisions involved in\nAzure\u2019s supply chain. We then outline the steps for deploying OptiGuide in production, and\nprovide examples of user interactions and early feedback we obtained. We conclude this section\nby describing preliminary performance results.\n5.1\nThe Azure Supply Chain\nThe rapid growth of the cloud industry requires cloud providers to continuously deploy addi-\ntional capacity to keep up with the demand. This is achieved by acquiring new clusters of\nservers and deploying them in the data centers. The Microsoft Azure supply chain encompasses\na broad array of processes including demand forecasting, strategic foresight, hardware semantic\nsearch, fulfillment planning, and document management. Due to complexity and large scale,\nthe optimization of Azure\u2019s supply chain is assigned to different subsystems. We focus here on\none such subsystem called Intelligent Fulfillment System (IFS), which deals with assigning and\nshipping servers from the warehouse to the data centers.\nMain decisions.\nFor each demand for cloud capacity, the main decisions consist of (i) the\nhardware supplier that will be used to fulfill the demand, (ii) the timeline of the deployment - in\nparticular, the cluster\u2019s dock-date (which determines the date of shipping from the warehouse),\nand (iii) the cluster\u2019s deployment location in the data center (selection of a row of tiles to place\nthe cluster on). The goal is to minimize the total cost that consists of multiple components, such\nas delay/idle cost of the clusters compared to their ideal dock-date and shipping costs, while re-\nspecting a multitude of constraints. Examples of constraints include capacity constraints on the\nsuppliers and the data centers, location preferences for demands and compatibility constraints.\nThe underlying optimization problem is formulated as a Mixed Integer Program (MIP) where\nthe total input data size is around 500 MB. The optimal solution is obtained hourly using\nGurobi. More details about the optimization problem can be found in Appendix A.\nStakeholders.\nThe main consumers of IFS are planners. These are professionals that have the\nbusiness context, so when they receive the outcome of the optimization, they can confirm that it\nmeets business needs (or override decisions otherwise) and ensure the execution of the decisions\nis completed as planned. However, the increased complexity of the underlying optimization\nproblem in combination with the global scale of decision making (hundreds of data centers)\nprevents immediate clarity in the reasoning behind each decision. Consequently, planners often\nreach out to the engineers (including data scientists) that develop the optimization system\n14\nFigure 9: Screenshot of OptiGuide in Microsoft Azure production. We anonymized names and\ndata by using generic values.\n15\nfor obtaining additional insights. Oftentimes, planners and engineers have multiple rounds of\ninteraction around understanding an issue or exploring what-if scenarios.\nCommon questions.\nWe summarize below the main types of questions that are raised by\nplanners:\nQ1\n[Management] Does the system support a particular region, resource, or supplier?\nQ2\n[Availability] Is a resource available or allocated?\nQ3\n[Decisions] Why did the system make decision \u2018x\u2019 related to supplier/demand selection,\ntime, and location?\nQ4\n[Details of shipments] What are the details related to cross-geographical shipments and\nexpected dock counts on a specific date?\nQ5\n[Historical data analysis] What is the standard deviation of the supplier\u2019s inventory in the\nlast month?\nQ6\n[Visualization] Can you visualize the dock capacity, availability, dates, or delays at a given\nlocation?\n5.2\nDeploying OptiGuide for Azure Supply Chain\nOur current deployment of OptiGuide consists of (i) a front-end service for multiple-user inter-\naction; (ii) an agent service, which is connected to Azure OpenAI for LLM access; (iii) multiple\nvirtual machines (VMs) which host IFS and the application specific components to support\nmultiple users at the same time.\nWe preload VMs\u2019 memories with the input data and solver\u2019s solutions to speedup code\nexecutions for users. The input data for the optimization problem are updated periodically\n(hourly), where the VMs load the updated data in a round-robin fashion so that there are\nalways some VMs available to support users. We use GPT-4 as the LLM.\n5.3\nPreliminary Feedback and Results\nFigure 9 provides examples of interactions between users and OptiGuide.\nThe preliminary feedback we obtained from both planners and engineers has been positive.\nUsers expressed excitement noting the potential of OptiGuide to help them understand the\nunderlying optimization logic. Users especially emphasized the benefits of supporting key what-\nif scenarios, which gives planners more autonomy and may substantially reduce the engineering\non-call burden. For example, before OptiGuide, answering one what-if question would need\nmore than three operators to coordinate the investigation and one on-call engineer to inspect\nthe plan output.\nOur preliminary evaluation indicates that OptiGuide can achieve more than 90% accuracy\nfor our in-distribution evaluation. This result is consistent with the ones obtained in Section 4.\n6\nConcluding Remarks\nWe conclude this paper by discussing current limitations, and highlighting intriguing directions\nfor future work.\n16\n6.1\nCurrent Limitations\nUsers need to be specific.\nThe user needs to ask precise questions. For instance, \u201cCan we\ndock demand xc132 fifteen days earlier?\u201d is ambiguous, because \u201cearlier\u201d can mean \u201c15 days\nbefore today\u201d, \u201c15 days before the currently planned date\u201d, or \u201c15 days before the deadline\u201d.\nConsequently, the LLM might misunderstand the user and yield the wrong code.\nDependency on application-specific components.\nOptiGuide relies on proper design of\napplication-specific components, such as the schema of the database and the helper functions.\nSome of these components might require non-negligible engineering efforts. While there has\nbeen progress in automating some of these components [50], there are still gaps in using them\nin some production settings.\nUndetected mistakes.\nWe observed cases where the LLM writes code that runs smoothly,\nbut it may be totally wrong (e.g., due to string matching mistakes). We expect that things will\nimprove in the future with more advances in LLMs and supporting tools.\nGeneralize to new questions.\nWhile the LLM performs well on seen questions, it still\nstruggles when presented with questions that do not appear in the examples (see, e.g., Table\n1). We believe that future models will have better generalizability.\nBenchmark.\nOur current evaluation quantifies performance only for quantitative questions;\nfor example, we exclude visualization queries from our analysis. Furthermore, the evaluation is\nbased on a specific programming language (Python) and optimization solver (Gurobi).\n6.2\nFuture Directions\nWe see our work as a cornerstone for future research in the area. One interesting direction is\nincorporating human feedback (e.g., from supply chain planners) which could lead to significant\nperformance improvements [51]. Another direction that we are currently examining is using\nsmaller models (see, e.g., [52] and references therein) for the specific tasks of supply chain\noptimization; using such models allows for more affordable hosting and fine-tuning of the model.\nIn particular, we are examining whether fine-tuning can help with interpreting unseen questions.\nOn a related note, it is of interest to consider a hybrid framework that combines the strengths\nof different AI models, for example combining large LMs with smaller ones. A natural longer-\nterm goal is to go beyond explainability and facilitate interactive optimization, where the user\ndirectly influences the optimization outcomes; this will require designing more comprehensive\nsafeguards, to prevent costly mistakes.\nAcknowledgements\nWe thank S\u00b4ebastien Bubeck, Yin Tat Lee, Chi Wang, Erkang Zhu, Leonardo Nunes, Srikanth\nKandula, Adam Kalai, Marco Molinaro, Luke Marshall, Patricia Kovaleski, Hugo Barbalho,\nTamires Santos, Runlong Zhou, Ashley Llorens, Surajit Chaudhuri, and Johannes Gehrke from\nMicrosoft Research for useful discussions. We also thank Brian Houser, Matthew Meyer, Ryan\nMurphy, Russell Borja, Yu Ang Zhang, Rojesh Punnath, Naga Krothapalli, Navaneeth Echam-\nbadi, Apoorav Trehan, Jodi Larson, and Cliff Henson from the Microsoft Cloud Supply Chain\nfor their advice and support.\n17\nReferences\n[1] Michael H Hugos. Essentials of supply chain management. John Wiley & Sons, 2018.\n[2] Douglas M Lambert and Martha C Cooper. Issues in supply chain management. Industrial\nmarketing management, 29(1):65\u201383, 2000.\n[3] OpenAI. Gpt-4 technical report, 2023.\n[4] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\nEce Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[5] Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an\nai chatbot for medicine. New England Journal of Medicine, 388(13):1233\u20131239, 2023.\n[6] GitHub. Github copilot: Your ai pair programmer, 2023.\n[7] Lingjiao Chen, Matei Zaharia, and James Zou.\nFrugalgpt: How to use large language\nmodels while reducing cost and improving performance. arXiv preprint arXiv:2305.05176,\n2023.\n[8] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang,\nYan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications.\narXiv preprint arXiv:2306.05499, 2023.\n[9] Daniel J Garcia and Fengqi You. Supply chain design and optimization: Challenges and\nopportunities. Computers & Chemical Engineering, 81:153\u2013170, 2015.\n[10] Pourya Pourhejazy and Oh Kyoung Kwon. The new generation of operations research\nmethods in supply chain optimization: A review. Sustainability, 8(10):1033, 2016.\n[11] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj\nSen. A survey of the state of explainable ai for natural language processing. arXiv preprint\narXiv:2010.00711, 2020.\n[12] Imran Ahmed, Gwanggil Jeon, and Francesco Piccialli. From artificial intelligence to ex-\nplainable artificial intelligence in industry 4.0: a survey on what, how, and where. IEEE\nTransactions on Industrial Informatics, 18(8):5031\u20135042, 2022.\n[13] Stefan Nickel, Claudius Steinhardt, Hans Schlenker, and Wolfgang Burkart. Decision Op-\ntimization with IBM ILOG CPLEX Optimization Studio: A Hands-On Introduction to\nModeling with the Optimization Programming Language (OPL). Springer Nature, 2022.\n[14] Kristijonas \u02c7Cyras, Dimitrios Letsios, Ruth Misener, and Francesca Toni. Argumentation for\nexplainable scheduling. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 2752\u20132759, 2019.\n[15] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On\nthe opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[16] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word representations, 2018.\n18\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert:\nPre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[18] Corby Rosset. Turing-nlg: A 17-billion-parameter language model by microsoft. Microsoft\nBlog, 1(2), 2020.\n[19] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhan-\ndari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti,\net al.\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale\ngenerative language model. arXiv preprint arXiv:2201.11990, 2022.\n[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\n[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\n2022.\n[22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian\nIchter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An\nembodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\n[24] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March\n2023.\n[25] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi\nZhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for\ncode infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\n[26] Vijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel Cheng,\nNegar Ghorbani, Renuka Fernandez, and Nachiappan Nagappan. Codecompose: A large-\n19\nscale industrial deployment of ai-assisted code authoring. arXiv preprint arXiv:2305.12050,\n2023.\n[27] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken,\nand Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Informa-\ntion Processing Systems, 32, 2019.\n[28] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter\nStone. Llm+ p: Empowering large language models with optimal planning proficiency.\narXiv preprint arXiv:2304.11477, 2023.\n[29] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning.\nJournal of Big data, 3(1):1\u201340, 2016.\n[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\nprompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n[31] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\nfinetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.\n[32] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234,\n2022.\n[33] OpenAI. ChatGPT plugins, 2023.\n[34] OpenAI. Function calling and other API updates, 2023.\n[35] LangChian. Introduction \u2014 langchain, 2023.\n[36] Auto-GPT: An Autonomous GPT-4 Experiment, June 2023.\noriginal-date:\n2023-03-\n16T09:21:07Z.\n[37] BabyAGI. Translations: \u2014 BabyAGI, 2023.\n[38] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and\nMark Steedman. Sources of hallucination by large language models on inference tasks.\narXiv preprint arXiv:2305.14552, 2023.\n[39] Potsawee Manakul, Adian Liusie, and Mark JF Gales.\nSelfcheckgpt:\nZero-resource\nblack-box hallucination detection for generative large language models.\narXiv preprint\narXiv:2303.08896, 2023.\n[40] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang,\nLars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving\nlarge language models with external knowledge and automated feedback. arXiv preprint\narXiv:2302.12813, 2023.\n[41] Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun\nRen.\nContrastive learning reduces hallucination in conversations.\narXiv preprint\narXiv:2212.10400, 2022.\n[42] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. Skcoder: A sketch-based\napproach for automatic code generation. arXiv preprint arXiv:2302.06144, 2023.\n20\n[43] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. Self-planning code\ngeneration with large language model. arXiv preprint arXiv:2303.06689, 2023.\n[44] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu,\nXin Jiang, and Qun Liu. Compilable neural code generation with compiler feedback. arXiv\npreprint arXiv:2203.05132, 2022.\n[45] Angelica Chen, J\u00b4er\u00b4emy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan,\nSamuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by\ntraining with natural language feedback, 2023.\n[46] Bob Bixby. The gurobi optimizer. Transp. Research Part B, 41(2):159\u2013178, 2007.\n[47] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Eval-\nuating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[48] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain gener-\nalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2022.\n[49] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors\nfor word representation. In Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n[50] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language\nmodels as tool makers. arXiv preprint arXiv:2305.17126, 2023.\n[51] Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Re-\nframing human-ai collaboration for generating free-text explanations.\narXiv preprint\narXiv:2112.08674, 2021.\n[52] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00b4esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al.\nTextbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\n21\nA\nIntelligent Fulfillment System\nIn this section, we present a partial formulation of the optimization in the Intelligent Fulfillment\nSystem that assigns and ships servers from the warehouse to the data centers.\nA.1\nMain Decisions\nWe introduce the following variables:\n\u2022 zdt \u2208 {0, 1}: equals 1 if demand d docks on day t, and 0 otherwise\n\u2022 udr \u2208 {0, 1}: equals 1 if demand d docks on row r, and 0 otherwise\n\u2022 wds \u2208 {0, 1}: equals 1 if d is fulfilled using supplier s, and 0 otherwise\n\u2022 yd,dc,t \u2208 {0, 1}: equals 1 if d docks at datacenter dc on day t, and 0 otherwise.\n\u2022 vd,s,t \u2265 0 : whether demand d docks on day t using supplier s or not\nA.2\nConstraints\nThis section describes some of the constraints in the formulation.\nDocking day.\nThe docking for each demand takes place on a single day.\nX\nt\nzdt \u2264 1\n\u2200d\nDatacenter dockings.\nFor each demand d, we dock at a datacenter dc on a specific day t\nonly if the selected row belongs to that datacenter dc and the selected day is that particular\nday t.\nX\ndc\nyd,dc,t \u2264 zdt\n\u2200d, t\nX\nt\nyd,dc,t =\nX\nr\u2208rows(dc)\nudr\n\u2200d, dc\nDatacenters\u2019 daily capacities.\nThere are restrictions restr on the daily amount of dockings\nthat sets of datacenters can handle. Let Rd denote the number of racks required for demand d.\nX\nd,dc\u2208DC(restr)\nyd,dc,t \u00b7 Rd \u2264 DockRestrAvailCap(restr, t)\n\u2200restr \u2208 Restrictions, t\nSingle supplier.\nEach demand must be fulfilled by a single supplier. A row is selected for a\ndemand only if a supplier has been found.\nX\ns\nwds \u2264 1\n\u2200d\nudr \u2264\nX\ns\nwds\n\u2200d, r\n22\nAuxiliary supplier variables.\nConnecting variables vdst with the rest of the variables.\nzdt =\nX\ns\nvdst\n\u2200d, t\nwds =\nX\nt\nvdst\n\u2200d, t\nSupply availability.\nWe have a set of supply pools with a certain capacity (amount of avail-\nable supply) evaluated at times ct. We need to make sure that the supply s we consume from\neach supply pool sp is available at the time t that we consume it. The time where each supply\nbecomes available depends on its lead time.\nX\nd,s\u2208sp,t\u2264leadtime(ct,d,s)\nvdst \u2264 Available Supply(sp, ct)\n\u2200sp, ct\nOverrides.\nSome demand-supply combinations might be undesirable or disallowed for some\nreason. These can be explicitly blocked. Let B denote the set of blocked pairs.\nwds = 0\n\u2200(d, s) \u2208 B\nA.3\nObjective\nOur goal is to minimize the total cost which is the aggregate of multiple components, including\nthe cost of docking too early or too late compared to the ideal dock-date of each demand, the\ncost of not fulfilling demands, and the shipping cost, among others.\nDockCost =\nX\nd,t\nzdt \u00b7 Demand Day DockCost(d, t)\nNoDockCost =\nX\nd\n(1 \u2212\nX\nt\nzdt) \u00b7 Unsatisfied Cost(d)\nShippingCost =\nX\nd,s\nwds \u00b7 Transit Ship Cost(d, s)\nB\nEngineering Details\nFigure 11, at the end of this document, presents a detailed screenshot of OptiGuide with Azure\nIFS, including intermediate results for illustration purposes.\nB.1\nUseful Tricks\nSQL\n: Many LLMs are trained with SQL database. Hence, saving optimization input and\noutput data into SQL could make the system easier to use and more explainable.\nLogical simplification:\nIf the prompt is not designed well, the LLM might make many simple\nlogical mistakes (e.g., \u201cnot use\u201d v.s. \u201cuse\u201d, before v.s. after, etc.).\nIntermediate outputs.\nWhen dealing with complex prompts, providing intermediate out-\nputs can help keep the LLM on track. By returning intermediate results or steps, the LLM can\ncheck the consistency of its process, making it easier to debug and refine.\n23\nB.2\nFailed Attempts\nChain of thought (CoT) failures.\nUnlike many recent studies [50] that have found that\nLLMs have strong CoT abilities, we found CoT is not helpful for writing complex code. This is\nanother reason why we integrated the helper functions in the application-specific tools, which\noutperformed CoT. Our hypothesis is that if the LLM makes one mistake in the thinking chain,\nthen the whole response would be wrong because correcting its own mistakes is hard.\nOveruse of prompt engineering:\nWhile prompt engineering can often lead to improved\nresults, overdoing it can sometimes lead to worse outcomes. When the prompts become too\ncomplex or too specific, the LLM might not understand them correctly or might overfit to the\nspecific prompt structure, limiting its ability to handle a variety of questions.\nC\nCoffee Distribution Example\nC.1\nCode\nimport time\nfrom gurobipy import GRB, Model\n# Example data\ncapacity_in_supplier = {\u2019supplier1\u2019: 150, \u2019supplier2\u2019: 50, \u2019supplier3\u2019: 100}\nshipping_cost_from_supplier_to_roastery = {\n(\u2019supplier1\u2019, \u2019roastery1\u2019): 5,\n(\u2019supplier1\u2019, \u2019roastery2\u2019): 4,\n(\u2019supplier2\u2019, \u2019roastery1\u2019): 6,\n(\u2019supplier2\u2019, \u2019roastery2\u2019): 3,\n(\u2019supplier3\u2019, \u2019roastery1\u2019): 2,\n(\u2019supplier3\u2019, \u2019roastery2\u2019): 7\n}\nroasting_cost_light = {\u2019roastery1\u2019: 3, \u2019roastery2\u2019: 5}\nroasting_cost_dark = {\u2019roastery1\u2019: 5, \u2019roastery2\u2019: 6}\nshipping_cost_from_roastery_to_cafe = {\n(\u2019roastery1\u2019, \u2019cafe1\u2019): 5,\n(\u2019roastery1\u2019, \u2019cafe2\u2019): 3,\n(\u2019roastery1\u2019, \u2019cafe3\u2019): 6,\n(\u2019roastery2\u2019, \u2019cafe1\u2019): 4,\n(\u2019roastery2\u2019, \u2019cafe2\u2019): 5,\n(\u2019roastery2\u2019, \u2019cafe3\u2019): 2\n}\nlight_coffee_needed_for_cafe = {\u2019cafe1\u2019: 20, \u2019cafe2\u2019: 30, \u2019cafe3\u2019: 40}\ndark_coffee_needed_for_cafe = {\u2019cafe1\u2019: 20, \u2019cafe2\u2019: 20, \u2019cafe3\u2019: 100}\n24\ncafes = list(set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()))\nroasteries = list(\nset(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()))\nsuppliers = list(\nset(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()))\n# OPTIGUIDE DATA CODE GOES HERE\n# Create a new model\nmodel = Model(\"coffee_distribution\")\n# Create variables\nx = model.addVars(shipping_cost_from_supplier_to_roastery.keys(),\nvtype=GRB.INTEGER,\nname=\"x\")\ny_light = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\nvtype=GRB.INTEGER,\nname=\"y_light\")\ny_dark = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\nvtype=GRB.INTEGER,\nname=\"y_dark\")\n# Set objective\nmodel.setObjective(\nsum(x[i] * shipping_cost_from_supplier_to_roastery[i]\nfor i in shipping_cost_from_supplier_to_roastery.keys()) +\nsum(roasting_cost_light[r] * y_light[r, c] +\nroasting_cost_dark[r] * y_dark[r, c]\nfor r, c in shipping_cost_from_roastery_to_cafe.keys()) + sum(\n(y_light[j] + y_dark[j]) * shipping_cost_from_roastery_to_cafe[j]\nfor j in shipping_cost_from_roastery_to_cafe.keys()), GRB.MINIMIZE)\n# Conservation of flow constraint\nfor r in set(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()):\nmodel.addConstr(\nsum(x[i]\nfor i in shipping_cost_from_supplier_to_roastery.keys()\nif i[1] == r) == sum(\ny_light[j] + y_dark[j]\nfor j in shipping_cost_from_roastery_to_cafe.keys()\nif j[0] == r), f\"flow_{r}\")\n# Add supply constraints\nfor s in set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()):\nmodel.addConstr(\nsum(x[i]\nfor i in shipping_cost_from_supplier_to_roastery.keys()\nif i[0] == s) <= capacity_in_supplier[s], f\"supply_{s}\")\n25\n# Add demand constraints\nfor c in set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()):\nmodel.addConstr(\nsum(y_light[j]\nfor j in shipping_cost_from_roastery_to_cafe.keys()\nif j[1] == c) >= light_coffee_needed_for_cafe[c],\nf\"light_demand_{c}\")\nmodel.addConstr(\nsum(y_dark[j]\nfor j in shipping_cost_from_roastery_to_cafe.keys()\nif j[1] == c) >= dark_coffee_needed_for_cafe[c], f\"dark_demand_{c}\")\n# Optimize model\nmodel.optimize()\nm = model\n# OPTIGUIDE CONSTRAINT CODE GOES HERE\n# Solve\nm.update()\nmodel.optimize()\nprint(time.ctime())\nif m.status == GRB.OPTIMAL:\nprint(f\u2019Optimal cost: {m.objVal}\u2019)\nelse:\nprint(\"Not solved to optimality. Optimization status:\", m.status)\nC.2\nQuestion and Ground Truth Macros\nQUESTION:\nWhat would happen if demand at cafe {{VALUE-CAFE}} increased by {{VALUE-NUMBER}}%?\nVALUE-CAFE: random.choice(cafes)\nVALUE-NUMBER: random.randrange(5,30)\nDATA CODE:\nlight_coffee_needed_for_cafe[{{VALUE-CAFE}}] =\n\\\nlight_coffee_needed_for_cafe[{{VALUE-CAFE}}] * (1 + {{VALUE-NUMBER}}/100)\ndark_coffee_needed_for_cafe[{{VALUE-CAFE}}] =\n\\\ndark_coffee_needed_for_cafe[{{VALUE-CAFE}}] * (1 + {{VALUE-NUMBER}}/100)\nTYPE: demand-increase\nQUESTION:\nWhat if demand for light coffee at cafe {{VALUE-CAFE}} increased by {{VALUE-NUMBER}}%?\nVALUE-CAFE: random.choice(cafes)\nVALUE-NUMBER: random.randrange(5,30)\nDATA CODE:\nlight_coffee_needed_for_cafe[{{VALUE-CAFE}}] = \\\nlight_coffee_needed_for_cafe[{{VALUE-CAFE}}] * (1 + {{VALUE-NUMBER}}/100)\n26\nTYPE: demand-increase-light\nQUESTION:\nWhat would happen if the demand at all cafes doubled?\nDATA CODE:\nfor c in cafes:\nlight_coffee_needed_for_cafe[c] = light_coffee_needed_for_cafe[c] * 2\ndark_coffee_needed_for_cafe[c] = dark_coffee_needed_for_cafe[c] * 2\nTYPE: demand-increase-all\nQUESTION:\nWhy are we using supplier {{VALUE-SUPPLIER}} for roasting facility {{VALUE-ROASTERY}}?\nVALUE-SHIPPINGS: [(s, r) for (s, r), value in x.items() if value.X >= 0.999]\nVALUE-IDX: random.randint(0, len({{VALUE-SHIPPINGS}}) - 1)\nVALUE-SUPPLIER: {{VALUE-SHIPPINGS}}[{{VALUE-IDX}}][0]\nVALUE-ROASTERY: {{VALUE-SHIPPINGS}}[{{VALUE-IDX}}][1]\nCONSTRAINT CODE:\nm.addConstr(x[{{VALUE-SUPPLIER}},{{VALUE-ROASTERY}}] == 0, \"_\")\nTYPE: supply-roastery\nQUESTION:\nAssume cafe {{VALUE-CAFE}} can exclusively buy coffee from roasting facility\n{{VALUE-ROASTERY}}, and conversely, roasting facility {{VALUE-ROASTERY}}\ncan only sell its coffee to cafe {{VALUE-CAFE}}. How does that affect the outcome?\nVALUE-ROASTERY: random.choice(roasteries)\nVALUE-CAFE: random.choice(cafes)\nCONSTRAINT CODE:\nfor c in cafes:\nif c != {{VALUE-CAFE}}:\nm.addConstr(y_light[{{VALUE-ROASTERY}}, c] == 0, \"_\")\nm.addConstr(y_dark[{{VALUE-ROASTERY}}, c] == 0, \"_\")\nfor r in roasteries:\nif r != {{VALUE-ROASTERY}}:\nm.addConstr(y_light[r,{{VALUE-CAFE}}] == 0, \"_\")\nm.addConstr(y_dark[r,{{VALUE-CAFE}}] == 0, \"_\")\nTYPE: exclusive-roastery-cafe\nQUESTION:\nWhat if roasting facility {{VALUE-ROASTERY}} can only be used for cafe {{VALUE-CAFE}}?\nVALUE-ROASTERY: random.choice(roasteries)\nVALUE-CAFE: random.choice(cafes)\nCONSTRAINT CODE:\nfor c in cafes:\nif c != {{VALUE-CAFE}}:\nm.addConstr(y_light[{{VALUE-ROASTERY}}, c] == 0, \"_\")\nm.addConstr(y_dark[{{VALUE-ROASTERY}}, c] == 0, \"_\")\nTYPE: incompatible-roastery-cafes\nQUESTION:\n27\nWhat if supplier {{VALUE-SUPPLIER}} can now provide only half of the quantity?\nVALUE-SUPPLIER: random.choice(suppliers)\nDATA CODE:\ncapacity_in_supplier[{{VALUE-SUPPLIER}}] = capacity_in_supplier[{{VALUE-SUPPLIER}}]/2\nTYPE: supplier-capacity\nQUESTION:\nThe per-unit cost from supplier {{VALUE-SUPPLIER}} to roasting facility {{VALUE-ROASTERY}}\nis now {{VALUE-NUMBER}}. How does that affect the total cost?\nVALUE-SUPPLIER: random.choice(suppliers)\nVALUE-ROASTERY: random.choice(roasteries)\nVALUE-NUMBER: random.randrange(1,10)\nDATA CODE:\nshipping_cost_from_supplier_to_roastery[{{VALUE-SUPPLIER}},{{VALUE-ROASTERY}}] = \\\n{{VALUE-NUMBER}}\nTYPE: supplier-roastery-shipping\nQUESTION:\nWhat would happen if roastery 2 produced at least as much light coffee as roastery 1?\nCONSTRAINT CODE:\nm.addConstr(sum(y_light[\u2019roastery1\u2019,c] for c in cafes)\n<= sum(y_light[\u2019roastery2\u2019,c] for c in cafes), \"_\")\nTYPE: light-quantities-roasteries\nQUESTION:\nWhat would happen if roastery 1 produced less light coffee than roastery 2?\nCONSTRAINT CODE:\nm.addConstr(sum(y_light[\u2019roastery1\u2019,c] for c in cafes)\n<= sum(y_light[\u2019roastery2\u2019,c] for c in cafes) - 1, \"_\")\nTYPE: light-quantities-roasteries\nQUESTION:\nWhat will happen if supplier 1 ships more to roastery 1 than roastery 2?\nCONSTRAINT CODE:\nm.addConstr(x[\u2019supplier1\u2019,\u2019roastery1\u2019] >= x[\u2019supplier1\u2019,\u2019roastery2\u2019] + 1, \"_\")\nTYPE: shipping-quantities-roasteries\nQUESTION:\nWhat will happen if supplier 1 ships to roastery 1 at least as much as to roastery 2?\nCONSTRAINT CODE:\nm.addConstr(x[\u2019supplier1\u2019,\u2019roastery1\u2019] >= x[\u2019supplier1\u2019,\u2019roastery2\u2019], \"_\")\nTYPE: shipping-quantities-roasteries\nQUESTION:\nWhy not only use a single supplier for roastery 2?\nCONSTRAINT CODE:\nz = m.addVars(suppliers, vtype=GRB.BINARY, name=\"z\")\nm.addConstr(sum(z[s] for s in suppliers) <= 1, \"_\")\nfor s in suppliers:\n28\nm.addConstr(x[s,\u2019roastery2\u2019] <= capacity_in_supplier[s] * z[s], \"_\")\nTYPE: single-supplier-roastery\n0\n2\n4\n6\n8\n10\n60\n70\n80\n90\nNumber of Shots (learning examples)\nCoding Accuracy %\nNearest Neighbours\nRandom Choices\nFigure 10: Out-of-distribution evaluation for GPT-4. We compare the different training example\nselection methods here.\n29\nFigure 11: OptiGuide for Azure IFS. Intermediate results from agents are shown in the screen-\nshot.\n30\n"
  },
  {
    "title": "Large Language Models as General Pattern Machines",
    "link": "https://arxiv.org/pdf/2307.04721.pdf",
    "upvote": "13",
    "text": "Large Language Models as General Pattern Machines\nSuvir Mirchandani1, Fei Xia2, Pete Florence2, Brian Ichter2, Danny Driess2 3,\nMontserrat Gonzalez Arenas2, Kanishka Rao2, Dorsa Sadigh1 2, Andy Zeng2\n1Stanford University, 2Google DeepMind, 3TU Berlin\nhttps://general-pattern-machines.github.io\nAbstract: We observe that pre-trained large language models (LLMs) are capable of au-\ntoregressively completing complex token sequences\u2014from arbitrary ones procedurally\ngenerated by probabilistic context-free grammars (PCFG), to more rich spatial pat-\nterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark,\nprompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be\npartially retained even when the sequences are expressed using tokens randomly sampled\nfrom the vocabulary. These results suggest that without any additional training, LLMs\ncan serve as general sequence modelers, driven by in-context learning. In this work, we\ninvestigate how these zero-shot capabilities may be applied to problems in robotics\u2014\nfrom extrapolating sequences of numbers that represent states over time to complete\nsimple motions, to least-to-most prompting of reward-conditioned trajectories that can\ndiscover and represent closed-loop policies (e.g., a stabilizing controller for CartPole).\nWhile difficult to deploy today for real systems due to latency, context size limitations,\nand compute costs, the approach of using LLMs to drive low-level control may provide\nan exciting glimpse into how the patterns among words could be transferred to actions.\nKeywords: large language models, in-context learning, language for robotics\n1\nIntroduction\nLarge language models (LLMs) are trained to absorb the myriad of patterns that are woven into the structure\nof language. They not only exhibit various out-of-the-box capabilities such as generating chains of reasoning\n[1, 2], solving logic problems [3, 4], and completing math puzzles [5], but also have been applied in robotics\nwhere they can serve as high-level planners for instruction following tasks [6, 7, 8, 9, 10, 11, 12], synthesize\nprograms representing robot policies [13, 14], design reward functions [15, 16], and generalize user pref-\nerences [17]. These settings rely on few-shot in-context examples in text prompts that specify the domain\nand input-output format for their tasks [18, 19], and remain highly semantic in their inputs and outputs.\ninput: \n\u00a00, 0, 0, 0 \n\u00a00, 3, 4, 0 \n\u00a00, 7, 6, 0 \n\u00a00, 0, 0, 0 \noutput: \n\u00a03, 0, 0, 4 \n\u00a00, 0, 0, 0 \n\u00a00, 0, 0, 0 \n\u00a07, 0, 0, 6\ninput: \n\u00a00, 0, 0, 0 \n\u00a00, 5, 6, 0 \n\u00a00, 8, 3, 0 \n\u00a00, 0, 0, 0 \noutput: \n\u00a05, 0, 0, 6 \n\u00a00, 0, 0, 0 \n\u00a00, 0, 0, 0 \n\u00a08, 0, 0, 3\ninput: \n\u00a00, 0, 0, 0 \n\u00a00, +#, B, 0 \n\u00a00, @, \u6176, 0 \n\u00a00, 0, 0, 0 \noutput: \n\u00a0+#, 0, 0, B \n\u00a00, 0, 0, 0 \n\u00a00, 0, 0, 0 \n\u00a0@, 0, 0, \u6176 \nFig. 1: LLMs out-of-the-box\ncan complete (highlighted)\ncomplex ARC patterns [20]\nexpressed in arbitrary tokens.\nA key observation of our work\u2014and perhaps contrary to the predominant\nintuition\u2014is that an LLM\u2019s ability to represent, manipulate, and extrapolate\nmore abstract, nonlinguistic patterns may allow them to serve as basic versions\nof general pattern machines. To illustrate this idea, consider the Abstraction\nand Reasoning Corpus [20], a general AI benchmark that contains collections\nof 2D grids with patterns that evoke abstract concepts (e.g., infilling, counting,\nand rotating shapes). Each problem provides a small number of input-output\nexamples, followed by test input(s) for which the objective is to predict\nthe corresponding output. Most methods (based on program synthesis) are\nmanually engineered with domain-specific languages [21, 22, 23, 24] or\nevaluated on simplified extensions or subsets of the benchmark [25, 26, 27].\nEnd-to-end machine learning methods only solve a handful of test problems\n[28]; however, our experiments indicate that LLMs in-context prompted in\nthe style of ASCII art (see Fig. 1) can correctly predict solutions for up to 85\n(out of 800) problems\u2014exceeding some existing recent systems [21, 22, 24], without additional model\ntraining or fine-tuning. Surprisingly, we find this extends beyond ASCII numbers, and that when they\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2307.04721v2  [cs.AI]  26 Oct 2023\n|\n100\n100 -\n\u00b7\u00b7\u00b7 78, 76, 72, 66, 60, 53, 46 \u00b7\u00b7\u00b7 \nFig. 2: Pre-trained LLMs out-of-the-box may serve as basic versions of general pattern machines that can recognize and\ncomplete sequences of numeric or arbitrary (symbolic) tokens expressing abstract problems in robotics and sequential\ndecision-making. Experiments show that to an extent, LLMs can in-context learn (i) sequence transformations (e.g.,\nto reason over spatial rearrangements of symbols, for dynamics modeling and next state prediction on downsampled\nimages), (ii) completion of simple functions (e.g., to extrapolate kinesthetic demonstrations), or (iii) meta-patterns to\nimprove return-conditioned policies (e.g., to discover oscillatory behaviors to stabilize a CartPole).\nare replaced with a mapping to randomly sampled tokens in the vocabulary, LLMs can still generate\nsome valid solutions. These results suggest an intriguing insight: that LLMs may exhibit more general\ncapabilities of representing and extrapolating symbolic patterns, invariant to the specific tokens involved.\nThis is in-line with\u2014and complementary to\u2014recent observations that using random or abstract label\nmappings for in-context classification retains some performance compared to ground-truth labels [29, 30].\nWe hypothesize that the capabilities that drive pattern reasoning on the ARC may allow general pattern\nmanipulation at various levels of abstraction useful for robotics and sequential decision making [31, 32],\nwherein a diverse array of problems involve patterns that may be difficult to reason about precisely in\nwords. For example, a procedure for spatially rearranging tabletop objects could be represented using\narbitrary tokens (see Fig. 2). As another example, optimizing a trajectory with respect to a reward function\ncan be framed as extrapolating a sequence consisting of state and action tokens with increasing returns.\nOrthogonal and complementary to efforts that develop multi-task policies by pre-training on large amounts\nof robot data [33], or robotics foundation models [34] that can be fine-tuned for downstream tasks [35, 36,\n37], our goal is instead to (i) assess the zero-shot capabilities that LLMs may already contain to perform\nsome degree of general pattern manipulation, and (ii) investigate how these abilities can be used in robotics.\nThese capabilities are certainly not sufficient to replace specialized algorithms; nonetheless, they are useful\nto characterize, and doing so may help inform priorities for training generalist models in robotics.\nWe assess LLMs as pattern machines categorized into three areas: sequence transformation, sequence\ncompletion, and sequence improvement (Fig. 2). First, we show that LLMs are capable of generalizing\ncertain sequence transformations of increasing complexity with a degree of token invariance, and posit\nthat this can carry over to spatial reasoning capabilities in robotic tasks. Next, we assess LLMs\u2019 ability to\ncomplete patterns from simple functions (e.g., sinusoids) and show this can be applied to robotic tasks like\nextending a wiping motion from kinesthetic demonstrations, or drawing patterns on a whiteboard. The\ncombination of in-context sequence transformation and extrapolation further enables LLMs to do basic\nforms of sequence improvement. We show that providing reward-labeled trajectories as context, coupled\nwith online interaction, can enable an LLM-based agent to learn to navigate through a small grid, discover\na simple CartPole controller, and optimize simple trajectories via human-in-the-loop \u201cclicker\u201d reward\ntraining. Code, benchmarks, and videos are made available at https://general-pattern-machines.github.io.\n2\n2\nRelated Work\nIn-Context Learning. Pattern reasoning by prompting pre-trained LLMs with few-shot input-output\nexamples is driven by in-context learning [38, 39]. The examples serve as a form of task specification,\nwhere the model is expected to complete further instances of the task by predicting what comes next. In-\ncontext learning extends the concept of \u201ctask prefixes\u201d (predefined token sequences, e.g., [40]), but swapped\nin with examples instead. Brown et al. [39] observe that it improves (in particular, out-of-distribution\ngeneralization) from scaling model size. This is in contrast to scaling models for pre-training + fine-tuning,\nwhich has been shown to not necessarily improve OOD generalization on language tasks [41]. Nonetheless,\ndespite compelling OOD generalization abilities, in-context learning still comes at a cost, as it continues to\nlag behind in terms of absolute performance on benchmarks compared to task-specific fine-tuning [38, 42].\nExplanations of In-Context Learning. In-context learning is explicitly trained for by packing examples\nfrom the same task and dataset into a context buffer that is fed as input to an LLM with an unsupervised\nautoregressive objective [39], sometimes referred to as meta-training. However, it can also emerge implicitly\nfrom training on datasets where tokens exhibit a Zipfian distribution [43] on Transformer architectures,\nbut not necessarily with recurrent architectures (e.g., vanilla RNNs) [43]. Other works have shown that in-\ncontext learning with Transformers can learn simple function classes on par with least squares [44, 45, 46],\nand can generalize to a seemingly unbounded number of tasks (when trained on tasks from the same task\nfamily) better than multitask MLPs [47], with Bayesian interpretations of this phenomenon [48] [49].\nIn-Context vs. In-Weights Learning. In-context learning occurs during inference without gradient updates\nto the model weights, and can be differentiated from in-weights learning, which relies on information\nstored in the model weights during LLM training [50] (and can be useful for completion tasks such as\n\u201cAbraham Lincoln was born in\n\u201d). Chan et al. [50] observes that generalization of in-context learning\ncan be characterized as more \u201cexemplar-based\u201d (on the basis of similarity to in-context examples [51]),\nas opposed to generalization of in-weights learning which tends to be more \u201crule-based\u201d (on the basis\nof minimal features that support category boundaries in the training data [52]). The vast capabilities of\nLLMs [39, 53, 54, 55, 56] have been driven by a combination of both forms of learning. In this work, we\nare particularly interested in in-context learning, and (depending on the task) using the semantic priors of\nnumeric tokens to drive capabilities such as sequence completion (Section 5) and improvement (Section 6).\nLLMs and Robotics. LLMs have been applied across several areas in robotics\u2014such as decomposing\nhigh-level task descriptions to mid-level plans [6, 7, 57, 58, 59, 60], robot code [13, 17, 14, 61], and plan-\nning domain definition languages [10]. These methods leverage semantic priors stored in LLMs to compose\nplans or parameterize primitive APIs, but whether LLMs can directly influence control (e.g., at the level of\ntrajectories) in a zero-shot manner remains an open problem. We explore how pattern reasoning capabilities\nof LLMs may drive various control tasks, to extend or optimize low-level sequences. While it is possible to\nexplicitly train models for these capabilities [62, 63, 64, 65], this work focuses on the inherent abilities of\nLLMs out-of-the-box, which may have implications for the role of language pre-training for building em-\nbodied AI systems. Related to our work are [42] which studies how LLMs perform on non-language classi-\nfication and regression tasks; [66] which examines analogical reasoning in various text tasks; and [67] which\nstudies how LLMs can represent a rollout policy and world model in-context and then uses Q-learning to\ndrive policy improvement across a collection of toy environments with linguistic representations. Our use of\nLLMs for sequence improvement can be seen as a simplification of in-context policy iteration that supports\nlearning from demonstrations and in-context RL, driven by the generality of LLMs as pattern machines.\n3\nLanguage Models as General Pattern Machines\nThe capacity of LLMs to act as general pattern machines is driven by their ability to perform in-context\nlearning on sequences of numeric or arbitrary tokens. An LLM typically represents sequence modeling\nautoregressively, with a decoder-only Transformer [68], by factorizing the probability of a sequence\nx, which is a sequence of symbols (s1, ..., sn), into the product of conditional probabilities p(x) =\n\u220f\ufe01n\ni=1p(si|s1, ..., si\u22121). To perform in-context learning, the model can be conditioned with a prompt that\nprovides the initial tokens in the sequence s1:k =(s1, ..., sk) and uses the model to complete sk+1:n.\n3\nThe adaptability of in-context learning lies in the amount of flexibility that can be packed into s1:k\u2014this\nprompt sequence can itself contain many sequences, each an input-output pair, and perhaps additional task\nconditioning [38, 29]. Specifically, a model can in-context learn to complete a prompt which is a set of N\nexamples s1:k =(x1, x2, ..., xN) where each xi is a variable-length sequence (si\n1, si\n2, ..., si\nmi).\nRather than investigating in-context learning with natural language tasks [39], in this work we are interested\nin investigating more abstract notions of non-linguistic patterns. The following sections evaluate these\ncapabilities across LLMs, and show how they can be used in robotics. By varying the notion of what each\nxi should be, we can characterize in-context pattern learning capabilities into the following 3 categories.\n\u2022 Sequence Transformation (Section 4): each x1, ..., xN\u22121 is a sequence-to-sequence input-output pair;\ni.e., xi=(xi\ninput,xi\noutput), each subsequence of variable length, and xN is the query input (xN\ninput).\n\u2022 Sequence Completion (Section 5): rather than containing input-output pairs, and rather than containing\nmany examples of different sequences, the prompt x=(s1, ...,sk) corresponds to discrete samples from\na single function, e.g., of the form si=a\u00b7sin(bi), which can be extrapolated.\n\u2022 Sequence Improvement (Section 6): each x1, ..., xN\u22121 is a collection of trajectories (potentially labeled\nwith corresponding total rewards), and xN prompts the model to \u201cimprove\u201d the sequences by inferring a\nbetter one, e.g., with least-to-most prompting [69]\u2014this process can be iterative and applied to a variety\nof formulations, e.g., offline trajectory optimization or online in-context reinforcement learning.\n4\nSequence Transformation\nLLMs are capable of in-context learning the distribution of functions that represent sequence transformations\nby completing abstract patterns observed among examples of input-output sequences xi=(xi\ninput,xi\noutput) of\narbitrary tokens, each drawn from a fixed alphabet A. For example, suppose that we are given a string of\ninput-output examples such as \u201c 5 3 0, 3 5; 7 6 1, 6 7; 9 2 3, 2 9; 4 8 5,\u201d. Here A consists\nof tokens that represent space-prefixed digits 0\u20139, a comma token to separate inputs from outputs, and\na semi-colon token to delineate examples from each other. A general pattern machine should infer the\ncompletion \u201c 8 4\u201d by recognizing that the pattern is to swap the first 2 tokens, then remove the 3rd.\nMethod\nTotal (of 800)\n(g4) gpt-4-0613\n77\n(d3) text-davinci-003\n85\n(d3) w/ random A\n\u202044\u00b16\n(d2) text-davinci-002 [53]\n64\n(p) PaLM [55, 56]\n42\n(d1) text-davinci-001 [39]\n11\n(d1) finetuned\n9\nAinooson et al., 2023 [23]\n\u2217130\nKaggle 1st Place, 2022 [70]\n#164\nXu et al., 2022 [22]\n\u2020\u202057\nAlford et al., 2021 [24]\n\u2217\u221722\nFerr\u00b4e et al., 2021 [21]\n32\n\u2020Numbers averaged across 5 randomly sampled alphabets.\n\u2217Based on brute force search over a hand-designed DSL.\n#Reported out of 400 train tasks, among 3 candidates.\n\u2020\u2020Reported out of a subset of 160 object-oriented problems.\n\u2217\u2217Based on program synthesis, out of 36 symmetry tasks.\nTab. 1: LLMs out-of-the-box can solve a non-\ntrivial number of ARC problems.\nWe use the ARC [20] to evaluate LLMs on such sequence\ntransformations that are substantially more complex, covering\na range of abstract spatial tasks: infilling, counting, rotating\nshapes, etc. Each task has input-output examples (3.3 on aver-\nage), and 1-3 test inputs which can be represented as 2D grids.\nInput and output sizes may differ. LLMs can be used for the\nARC by flattening grids and predicting output grid items in\nrow-major order, which naturally supports variable-length out-\nputs. While LLMs are not specifically trained for rasterizing\nspatial outputs, we hypothesize that a general pattern machine\nwould be capable of implicitly recognizing long-range depen-\ndencies between rows (using positional encoding as a bias\n[71]) to pick up patterns that extend across the 2nd dimension.\nResult: ARC benchmark. Table 1 shows that LLMs (PaLM,\nInstructGPT series in acronyms d1 - d3) prompted with input\ngrids represented as tokens drawn from an alphabet of digits,\ncan correctly infer solutions for up to 85 problems. Surpris-\ningly, this outperforms some recent systems [21, 22] based on\nprogram synthesis that use manually engineered domain-specific languages (DSLs). While LLMs have yet\nto surpass brute-force search [23] to compose functions from a handcrafted API of grid operators, they do\nexhibit non-trivial performance. (We address the important caveat that parts of the ARC may be present in\nthe training data of LLMs later below.) Note that while we are concerned with LLM performance over raw\npatterns, concurrent work finds improvements via object representations [72] and hypothesis search [73].\nObservation: consistent tokenization matters. The ARC can be found among the suite of tasks in BIG-\nBench [74], but has often been overlooked since many language models appear to perform poorly (near or\n4\nat zero performance). We observe this occurs due to the formatting of the benchmark, where grid elements\nare represented as neighboring characters i.e., \u201c8686\u201d (instead of \u201c 8 6 8 6\u201d). While subtle, this difference\nis enough for certain Byte-Pair Encoding (or SentencePiece) tokenizers [75, 76] (that do not tokenize per\ndigit) to group multiple grid elements (\u201c8\u201d and \u201c6\u201d) into a single token (\u201c86\u201d) which maps to a different\ntoken embedding. This causes inconsistencies with how patterns are expressed at the token level. For\nexample, given a task expressed as \u201c8686, 6868; 7979,\u201d if the tokenizer groups pairs of digits 86, 68,\n79, respectively, the sequential inductive patterns of the task (to swap and repeat individual digits) are lost.\nA simple work-around is to directly pass token indices or embeddings to the language model, or use token\nalphabets unlikely to be grouped together (which involves some knowledge about the tokenizer). Even\nbeyond the ARC, we observe it is beneficial to tokenize consistently with the pattern being represented.\nObservation: token mapping invariance. The hypothesis that LLMs can serve as general pattern machines\nstems from the observation that they can still solve a non-trivial number of ARC problems using alphabets\nA sampled randomly from the LLM\u2019s token vocabulary. For instance, given a particular alphabet:{ 8\n\u21a6\u2192 falls, 6 \u21a6\u2192 +#, 7 \u21a6\u2192 Ul, 9 \u21a6\u2192 Chev, 3 \u21a6\u2192 \u6176, 2 \u21a6\u2192 2010}, a pattern machine at sufficient\nproficiency can be expected to complete the prompt \u201cfalls +# falls +#, +# falls +# falls; UI\nChev UI Chev, Chev UI Chev UI; \u6176 2010 \u6176 2010,\u201d by predicting \u201c 2010 \u6176 2010 \u6176\u201d. For\nexample, text-davinci-003 [53, 39] with the following mapping A={ 0 \u21a6\u2192 offence, 1 \u21a6\u2192 Subject,\n2 \u21a6\u2192 Lub, 3 \u21a6\u2192 Fail, 4 \u21a6\u2192 Chev, 5 \u21a6\u2192 symb, 6 \u21a6\u2192 swung, 7 \u21a6\u2192 Ul, 8 \u21a6\u2192 escalate, 9 \u21a6\u2192\nChromebook} solves 52 ARC problems, and across 5 random alphabets solves an average of 43.6 problems.\nInterestingly, we find that token mapping invariance holds to an extent on patterns over randomly sampled\nembeddings as well (not associated with any token in the vocabulary; see Appendix A.3).\nThe implications of token mapping invariance are two-fold. First, note that it is possible that parts of the\nARC are present in the LLM\u2019s training data (i.e., due to contamination). Thus, measuring the performance\nof LLMs under random alphabets may provide a closer estimate of their underlying sequence transformation\ncapabilities. (As further evidence that these abilities are not simply due to memorization, we provide a new\nprocedurally-generated pattern transformation benchmark described below.) Second, we hypothesize that\nthe pattern manipulation capabilities implied by token invariance could help drive positive transfer from\npatterns learned across Internet-scale language data to new modalities or symbolic representations for robot\nreasoning. As an example, (i) Fig. 10 (top) in the Appendix shows a grasp (Skittles) detector which outputs\ntarget coordinates within a downsampled image (with 6 in-context examples), and (ii) Fig. 10 (bottom)\nshows spatial rearrangement via predicting simple forward dynamics where the red bowl moves to the\ngreen plate (with 9 in-context examples of downsampled images as inputs and outputs). The generality\nof what the arbitrary tokens could represent may allow pattern transformation capabilities\u2014especially as\nLLMs improve\u2014to be leveraged at various levels of abstraction in robotics (e.g., pixels or joint positions).\nMethod\nAccuracy (%)\n(d3) text-davinci-003\n75\n(d3) w/ random A\n\u202058 \u00b1 1\n(p) PaLM [55, 56]\n74\n(d2) text-davinci-002 [53]\n69\n(d1) text-davinci-001 [39]\n60\n(c1) text-curie-001\n54\n(b1) text-babbage-001\n50\n(a1) text-ada-001\n39\n\u2020Numbers averaged across 5 randomly sampled alphabets.\nTab. 2: LLMs of varying sizes are capable\nof completing patterns procedurally generated\nwith PCFG, averaged over a range of k and w.\nResult: PCFG benchmark. The ARC is a difficult bench-\nmark, and the performance falloff can be steep (and relatively\nuninformative) across LLMs with decreasing model size and\ndata scale, making it difficult to measure incremental progress\ntowards pattern machines that could be used for sequence trans-\nformation in robotics. Therefore, we introduce an adjustable-\ndifficulty benchmark, where the transformations are procedu-\nrally generated using the probabilistic context-free grammar\n(PCFG) in Hupkes et al. [77]. These transformations include\na collection of lexical rules that may be composed (e.g., re-\nverse, shift, swap, repeat, etc.) over the tokens in the\ninput sequence xi\ninput to generate xi\noutput. Example transforma-\ntions are given in Table 4 in the Appendix. The complexity\nof these transformations can be controlled by varying the number of tokens k used to express sequences\nxi=(s1,...,sk), and the number of lexical rules w used to define the transformation. This is simply the iden-\ntity function when w=0, and progressively appears more complex as w\u2192\u221e. Table 2 aggregates PCFG\npattern completion accuracy across different LLMs over sequence length k =[1,2,4,8,16,32] and com-\nplexity w=[0,1,3,7,15,31], each with 100 runs (see Appendix A.4 for ablations of k,w). This benchmark\n5\nprovides a more unbiased evaluation of pattern reasoning capabilities in LLMs; PCFG completion accuracy\nimproves with model scale, and correlates with ARC performance. We use PCFG for evaluation only (rather\nthan for training [77, 78]) so that one can measure how pre-training regimes or modalities may improve\ngeneral pattern capabilities across sequence transformations. We have released the PCFG benchmark.\n5\nSequence Completion\nCompletion of sinusoids. We start with a simple example where LLMs extrapolate a function of the form\nf(x)=a\u00b7sin(bx). As in Section 4, tokenization matters; we found it effective to discretize outputs among\nintegers 0\u2013100, as these integers are represented by single tokens in the tokenizers of the LLMs we tested.\n0\n2\nx\n100\ny\n0\n2\nx\n100\ny\n0\n300\nError\n0\n2\nx\n100\ny\n0\n2\nx\n100\ny\n0\n300\nError\n0\n2\nx\n100\ny\n0\n2\nx\n100\ny\na1 b1 c1 d1 d2 d3\n0\n300\nError\nFig. 3: LLMs (d3 shown) can extrapolate various func-\ntions y =a\u00b7sin(bx) (top row), y =ax\u00b7sin(bx) (middle\nrow), and y= a\n2x sin(bx) (bottom row) given amounts of\ncontext. Overall, larger models make better predictions\nwith lower error rates (right column). More context also\nhelps prediction accuracy (light vs. dark).\nFig. 3 shows completions of the sine wave by text-\ndavinci-003 over 11 trials given 3 and 5 periods as\ncontext, as well as average distance (computed by Dy-\nnamic Time Warping) of the generated predictions to\nthe ground truth function values across several LLMs.\nMultiple LLMs produce near-perfect continuations\nof the sine wave, especially with more context (i.e.,\nmore periods of the sine wave). We additionally test\nthe function family ax\u00b7sin(bx)\u2014in which the am-\nplitude of the oscillations increases with x-values.\nHere, the LLM must extrapolate to new values un-\nseen in the context, which highlights the utility of\nusing a metric space for the outputs (0\u2013100) where\nthe LLM has priors over the scale of the different to-\nkens. These functions also contain a \u201cmeta-pattern\u201d:\nthe y-values increase, decrease, and then increase in\na single period\u2014and the amplitude of the function\nalso increases over time. This is a form of least-to-most prompting [69], an ability we find useful later for\nsequence improvement in Section 6. We also test the function a\n2x \u00b7sin(bx). Across these three functions,\nwe observe that greater context and larger scale LLMs yield higher quality predictions.\nCompletion of periodic motions. We emphasize that the Sequence Completion capability above is domain-\nagnostic\u2014i.e., we do not use any specialized prompts explaining what function should be completed, nor\ndo we provide any linguistic grounding for the metric tokens. We can therefore operationalize this zero-shot\ncapability of LLMs to simple open-loop motion extrapolation problems in robotics, e.g., by encoding a\nseries of positions sampled from a demonstration, and predicting future positions. We test two simple tasks\non a mobile robot manipulator: Table Sweeping and Whiteboard Drawing (both shown in Fig. 2).\nIn Table Sweeping, the goal is to continue a human-provided kinesthetic demonstration of sweeping a\nportion of a table (see middle Fig. 2). We encode the demonstration as a series of end-effector poses at\napproximately 3 Hz. Each demonstration lasts roughly 20-30 seconds. We represent the 7-dim end-effector\npose as a concatenation of Cartesian position and the quaternion, where each value is binned to an integer\nbetween 0 and 100, and the dimensions are delimited by spaces. We collect 30 demonstrations that\ndemonstrate the sweeping motion. Note that demonstrations in this task are noisier and higher dimensional\nthan the stylized sinusoid functions above. For each demonstration, we construct a context to consist of the\nfirst two-thirds of the provided demonstration, and treat the last one-third as the ground truth for the LLM\nto predict. Larger models quantitatively perform better with generally lower variance (see Appendix).\nIn Whiteboard Drawing, the goal is to continue a scripted demonstration of drawing loops on a whiteboard\n(see Fig. 2). Loops are defined by parametric equations of the form x = ax cos(bt) + dx and y =\naysin(bt)+cyt+dy. We execute the motions using position control and record the end-effector positions at\n5 Hz, then discretize states in between 0 and 300, as finer motion is needed for this task. We provide part of\nthe loop pattern in-context, and assess the ability to extrapolate from 2 loops to do a third loop. LLMs, e.g.,\ntext-davinci-003 perform well\u2014we show more completions with different loop styles in the Appendix.\n6\n6\nSequence Improvement\nIn this section, we explore the synergies between sequence transformation and completion\u2014 and investigate\nimproving a sequence, such as trajectories in a sequential decision process, along some metric, such as a\nreward function. Here, we use an LLM to generate new sequences xN conditioned on previous sequences\n(x1, ..., xN\u22121), which can represent previous iterations of the same sequence (or policy it represents).\nThe improvement can also be return-conditioned, given a reward function r(\u00b7). By inserting as the first\ntoken(s) of each sequence its corresponding total reward x=(r(x),s1 , ..., sk), we can prompt the model to\nconditionally \u201cimprove\u201d by \u201cjust asking\u201d [79] for a higher reward than those seen in-context (i.e., prompting\nLLMs to act as Decision Transformers [80]). New \u201crollouts\u201d can yield new reward labels that then replace\nthe original desired rewards with actual rewards. Iteratively performing this inference and accumulating\ntrajectories may jointly use the model\u2019s general notion of pattern transformation and extrapolation to\nperform improvement of sequences, which can be represented by numeric or symbolic tokens. Note that\nthere are practical considerations, e.g., depending on the task or model, not all sequences can fit in context,\nso options could be to keep the most recent, or the ones with the highest rewards if available (see Appendix\nfor more discussion). In this section, we perform a series of targeted experiments on simple tasks, aiming\nto explore the possibility of using LLMs for sequence improvement in trajectory and policy optimization.\nFig. 4: LLM agents can generate new trajecto-\nries with increasing returns for a Marker in Cup\ntask (right). Performance varies with different\nways of building the context (left).\nExtrapolating simple meta-patterns among trajectories.\nSequence improvement with LLMs enables a simple form of\ntrajectory optimization for a Marker in Cup task on a Franka\nPanda robot, where we define the prefixed reward of a trajec-\ntory to be the negative distance between the final end-effector\nposition and the cup (normalized between 0\u2013100), and initial-\nize the context with a collection of trajectories (stopping at\n20%, 40%, 60%, and 80% of the way to the cup), delimited by\nnewlines and prefixed by rewards (ranging roughly from 70-\n90; see Appendix). For this task, we represent trajectories as\nsequences of Cartesian positions, each dimension normalized\nbetween 0\u2013100. We find that text-davinci-003, to an extent,\nis able to generalize the pattern and generate a trajectory that achieves a reward >90. For this extrapolation\nto occur, we observe that meta-patterns in the context are crucial: in Fig. 4 (left), we compare the average\nreward achieved by text-davinci-003 over 11 trials (each with a different goal position) given contexts with\ndifferent trajectory orderings (sorted by reward, randomly permuted, or with/without reward annotations).\nFig. 5: Average max return\nfor LLM agents a1-d3 on\nGrid compared to random\nexploration (r).\nSampling higher-reward trajectories online. While LLMs can extrapolate\nfrom trajectories that exhibit clear meta-patterns among them, we find that\nthis ability is more limited for less trivial setups. Consider a simple 9 \u00d7 9\nGrid navigation environment with a random goal position and a fixed starting\nposition at the grid center. Episodes terminate after 20 timesteps, and the return\nis based on the distance from the agent to the goal at the final time step. This\nenvironment is inspired by the Dark Room environment from [62] but with a\ncontinuous reward function, reducing the exploration challenge. The agent may\ntake actions (1-5) corresponding to moving right, up, left, down, and no-op. We\ninitialize the context buffer with 20 trajectories of agent grid positions generated\nby a random policy, sorted by total cumulative rewards. These trajectories\nexhibit a more complicated meta-pattern than in the Marker in Cup task; we\ndo not find that LLMs can generate trajectories of higher reward immediately. With that said, we can\nconsider an iterative, online setting, in which the LLM acts as an agent that interacts with the environment\nin a closed-loop. The context consists of the highest reward trajectories in sorted order, appended with a\nhigher reward than was seen in the context, plus states and actions from the current partial trajectory (see\nAppendix). Once an episode terminates, its trajectory is relabeled with the reward achieved, and inserted\ninto the context at the appropriate position. In Fig. 5, we plot the maximum return attained by a1-d3 over\n50 episodes, compared to random exploration, averaged over 5 trials. We find that a1-d1 tend to sometimes\n\u201cexploit\u201d the suboptimal behaviors represented in the context (which initially contains trajectories with\nrewards ranging from 6-78), whereas d3 can consistently find a solution to Grid within 50 episodes.\n7\nFig. 6: Different LLM agents (d3 - c1) on average can\nimprove trajectories (total rewards) with more CartPole\nepisodes (left), and discovers \u201coscillatory behaviors\u201d (right)\nto keep the CartPole upright (later episodes are brighter).\nDiscovering a simple CartPole controller. We\nshow that using LLMs as agents in an online,\nclosed-loop setting can discover a simple controller\nfor CartPole (where observations consist of pole\nangle and velocity, normalized to 0\u2013100, actions\nare 1 (left) and 2 (right), maximum horizon is 200).\nFig. 6 (left) shows that return (number of steps\nthe CartPole is kept upright) improves on average\nacross various LLMs over 100 episodes (where\nthe first 100 are generated by random exploration).\nFig. 6 (right) shows the evolution of trajectories\nover episodes of d3, demonstrating that it discovers oscillatory behaviors to keep the CartPole upright.\n0\n50\nGoal reached\nGoal -\nState (y)\nState (x)\nState (z)\nEpisode resets\n# Steps\nReward signal (online)\nOnline in-context pushing\nt = 0\nState space\nFig. 7: LLMs can in-context react to sparse reward signals\nonline to encourage an end effector to reach a desired goal.\nOnline human-guided trajectory optimization.\nLLMs can also react to sparse binary reward sig-\nnals (e.g., subjectively provided by a human) to\nadjust trajectories online. This is analogous to an\nimplementation of \u201cclicker training\u201d [81, 82] used\nfor training dogs, but instead applied to robots. In\nthis setup, at every time step (2s), the robot exe-\ncutes an action corresponding to a movement of its\nend-effector in a particular direction. The human\nobserves the action and chooses whether to give a\nreward (i.e., by using the clicker) to encourage or discourage similar behaviors. Episodes reset after 30\nseconds, and the first two episodes are generated by random exploration. The (reward, state, action) tuples\nare added as in-context examples (with negative examples followed by positives, and an equal number of\neach) to generate the next action based on the current state. An example context format is given in the\nAppendix. As shown in Fig. 7, applying LLMs\u2019 sequence improvement capabilities in this way enables a\nhuman to guide the robot to push an object.\n7\nDiscussion\nWe are excited about the opportunities of LLMs as pattern machines for robotics\u2014from reasoning and\nextrapolating complex patterns as a prior for control, to online optimization of closed-loop policies via\nsequence improvement. These capabilities present several implications, including (i) perspectives on the\nrole of language pre-training for end-to-end robot learning models [31, 32], and (ii) in-context learning of\narbitrary patterns as a driving mechanism for policy improvement. LLMs also show promise for mixed\nautonomy settings\u2014e.g., real-time pattern extrapolation for assistive teleoperation. We expect many of these\nabilities to continue improving as large models expand from learning patterns within language-only datasets\nto multimodal domains (e.g., images, videos). While this work investigates in-context generalization on\nfairly simple settings without additional data collection or model training, these capabilities presumably\nmay be significantly improved via domain-specific objectives and finetuning [83, 84, 64, 65, 42].\nLimitations & Future Work. Today, the inference costs (and monetary costs) of using LLMs in the control\nloop are quite high. Predicting the next token for every sequence, e.g., every dimension of every time step\nin a trajectory, involves querying an LLM. State-action spaces which are higher dimensional and/or greater\nprecision also result in longer representations, and thereby the extent to which they can be extrapolated or\nsequence optimized is bounded by the context length of models. These limitations may prevent deploying\nthese models on more complex tasks in practice; however, they may be partially mitigated by incorporating\nmechanisms like external memory, and by current efforts to drive improvements in LLM quantization [85]\nand inference efficiency [86]. An additional limitation lies in the fact that, for best performance, some\ncare must be taken to represent patterns with consistent tokenization (which requires knowledge of the\nmodel\u2019s tokenization scheme). Finally, as with any other language-only model, LLM-based control may (i)\nbe unpredictable, and (ii) lack visual/physical grounding; thus, it is not currently suitable for application\noutside of constrained lab settings. We leave the exploration of these important topics for future work.\n8\nAcknowledgments\nThe authors would like to acknowledge Jie Tan, Peng Xu, Carolina Parada, Alexander Herzog, Jensen Gao,\nJoey Hejna, Megha Srivastava, and Allen Ren for valuable feedback and discussions.\nReferences\n[1] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits\nreasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[2] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\n[3] M. Suzgun, N. Scales, N. Sch\u00a8arli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,\nD. Zhou, et al. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv:2210.09261,\n2022.\n[4] A. Creswell, M. Shanahan, and I. Higgins. Selection-Inference: Exploiting large language models for interpretable\nlogical reasoning. arXiv:2205.09712, 2022.\n[5] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag,\nT. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[6] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. In Conference on Robot\nLearning (CoRL), 2022.\n[7] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable\nknowledge for embodied agents. arXiv:2201.07207, 2022.\n[8] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning goals with\nlarge-language models. arXiv:2302.05128, 2023.\n[9] Y. Ding, X. Zhang, C. Paxton, and S. Zhang. Task and motion planning with large language models for object\nrearrangement. arXiv:2303.06247, 2023.\n[10] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. LLM+P: Empowering large language\nmodels with optimal planning proficiency. arXiv:2304.11477, 2023.\n[11] E. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, and N. Haber. Parsel: A (de-) compositional framework for\nalgorithmic reasoning with language models. arXiv:2212.10561, 2023.\n[12] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2Motion: From natural language instructions to\nfeasible plans. arXiv:2303.12153, 2023.\n[13] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as Policies: Language\nmodel programs for embodied control. In International Conference on Robotics and Automation (ICRA), 2023.\n[14] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. ProgPrompt:\nGenerating Situated Robot Task Plans using Large Language Models. In International Conference on Robotics\nand Automation (ICRA), 2023.\n[15] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward Design with Language Models. In International\nConference on Learning Representations (ICLR), 2023.\n[16] H. Hu and D. Sadigh. Language Instructed Reinforcement Learning for Human-AI Coordination. In International\nConference on Machine Learning (ICML), 2023.\n[17] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. TidyBot:\nPersonalized Robot Assistance with Large Language Models. In International Conference on Intelligent Robots\nand Systems (IROS), 2023.\n[18] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What Makes Good In-Context Examples for GPT-3.\nIn Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and\nIntegration for Deep Learning Architectures, 2021.\n[19] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of\nlanguage models. In International Conference on Machine Learning (ICML), 2021.\n9\n[20] F. Chollet. On the measure of intelligence. arXiv:1911.01547, 2019.\n[21] S. Ferr\u00b4e. First Steps of an Approach to the ARC Challenge based on Descriptive Grid Models and the Minimum\nDescription Length Principle. arXiv:2112.00848, 2021.\n[22] Y. Xu, E. B. Khalil, and S. Sanner. Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus.\nIn AAAI Conference on Artificial Intelligence, 2022.\n[23] J. Ainooson, D. Sanyal, J. P. Michelson, Y. Yang, and M. Kunda. An approach for solving tasks on the Abstract\nReasoning Corpus. arXiv:2302.09425, 2023.\n[24] S. Alford. A Neurosymbolic Approach to Abstraction and Reasoning. PhD thesis, Massachusetts Institute of\nTechnology, 2021.\n[25] R. Assouel, P. Rodriguez, P. Taslakian, D. Vazquez, and Y. Bengio. Object-centric Compositional Imagination for\nVisual Abstract Reasoning. In ICLR Workshop on the Elements of Reasoning: Objects, Structure and Causality,\n2022.\n[26] A. Moskvichev, V. V. Odouard, and M. Mitchell. The ConceptARC Benchmark: Evaluating Understanding and\nGeneralization in the ARC Domain. arXiv:2305.07141, 2023.\n[27] V. Kolev, B. Georgiev, and S. Penkov. Neural abstract reasoner. In 4th Knowledge Representation and Reasoning\nMeets Machine Learning Workshop (KR2ML) at NeurIPS, 2020.\n[28] T. Paparaju.\nARC Competition : EDA + PyTorch CNN.\nhttps://www.kaggle.com/code/tarunpaparaju/\narc-competition-eda-pytorch-cnn, 2022. Accessed: 2023-05-30.\n[29] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the Role\nof Demonstrations: What Makes In-Context Learning Work? In Conference on Empirical Methods in Natural\nLanguage Processing, 2022.\n[30] J. Pan, T. Gao, H. Chen, and D. Chen. What In-Context Learning \u201cLearns\u201d In-Context: Disentangling Task\nRecognition and Task Learning. In Findings of the Association for Computational Linguistics, 2023.\n[31] K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. In AAAI\nConference on Artificial Intelligence, 2022.\n[32] M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning?\nIn International\nConference on Learning Representations (ICLR), 2023.\n[33] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog,\nJ. Hsu, et al. RT-1: Robotics transformer for real-world control at scale. In Proceedings of Robotics: Science and\nSystems (RSS), 2022.\n[34] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut,\nE. Brunskill, et al. On the opportunities and risks of foundation models. arXiv:2108.07258, 2021.\n[35] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3M: A universal visual representation for robot\nmanipulation. In Conference on Robot Learning (CoRL), 2022.\n[36] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot learning with masked\nvisual pre-training. In Conference on Robot Learning (CoRL), 2023.\n[37] S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-Driven Representation\nLearning for Robotics. In Proceedings of Robotics: Science and Systems (RSS), 2023.\n[38] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n[39] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n[40] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research\n(JMLR), 21(1):5485\u20135551, 2020.\n[41] D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, and D. Song. Pretrained transformers improve\nout-of-distribution robustness. In Annual Meeting of the Association for Computational Linguistics, 2020.\n10\n[42] T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J.-y. Sohn, D. Papailiopoulos, and K. Lee. LIFT:\nLanguage-interfaced fine-tuning for non-language machine learning tasks. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[43] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill. Data\ndistributional properties drive emergent in-context learning in transformers. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[44] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple\nfunction classes. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[45] E. Aky\u00a8urek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning?\ninvestigations with linear models. In International Conference on Learning Representations (ICLR), 2022.\n[46] J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov.\nTransformers learn in-context by gradient descent. In International Conference on Machine Learning (ICML),\n2023.\n[47] L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by meta-learning\ntransformers. In Workshop on Meta-Learning at NeurIPS, 2022.\n[48] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian\ninference. In International Conference on Learning Representations (ICLR), 2022.\n[49] X. Wang, W. Zhu, and W. Y. Wang. Large language models are implicitly topic models: Explaining and finding\ngood demonstrations for in-context learning. arXiv:2301.11916, 2023.\n[50] S. C. Chan, I. Dasgupta, J. Kim, D. Kumaran, A. K. Lampinen, and F. Hill. Transformers generalize differently\nfrom information stored in context vs in weights. arXiv:2210.05675, 2022.\n[51] R. N. Shepard and J.-J. Chang. Stimulus generalization in the learning of classifications. Journal of Experimental\nPsychology, 65(1):94, 1963.\n[52] F. G. Ashby and J. T. Townsend. Varieties of perceptual independence. Psychological Review, 93(2):154, 1986.\n[53] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\net al. Training language models to follow instructions with human feedback. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[54] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\net al. Evaluating large language models trained on code. arXiv:2107.03374, 2021.\n[55] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv:2204.02311, 2022.\n[56] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al.\nPaLM 2 Technical Report. arXiv:2305.10403, 2023.\n[57] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\nP. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter. Inner Monologue: Embodied\nReasoning through Planning with Language Models. In Conference on Robot Learning (CoRL), 2022.\n[58] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee,\nV. Vanhoucke, et al. Socratic Models: Composing zero-shot multimodal reasoning with language. In International\nConference on Learning Representations (ICLR), 2023.\n[59] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. LLM-Planner: Few-shot grounded\nplanning for embodied agents with large language models. arXiv:2212.04088, 2022.\n[60] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch, S. Levine, K. Hausman, et al.\nGrounded Decoding: Guiding text generation with grounded models for robot control. arXiv:2303.00855, 2023.\n[61] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An Open-Ended\nEmbodied Agent with Large Language Models. arXiv:2305.16291, 2023.\n[62] M. Laskin, L. Wang, J. Oh, E. Parisotto, S. Spencer, R. Steigerwald, D. Strouse, S. Hansen, A. Filos, E. Brooks,\net al. In-context reinforcement learning with algorithm distillation. In International Conference on Learning\nRepresentations (ICLR), 2023.\n11\n[63] M. Xu, Y. Shen, S. Zhang, Y. Lu, D. Zhao, J. Tenenbaum, and C. Gan. Prompting decision transformer for\nfew-shot policy generalization. In International Conference on Machine Learning (ICML), 2022.\n[64] Y. Zhang, D. Huang, B. Liu, S. Tang, Y. Lu, L. Chen, L. Bai, Q. Chu, N. Yu, and W. Ouyang. MotionGPT:\nFinetuned LLMs are General-Purpose Motion Generators. arXiv:2306.10900, 2023.\n[65] J. N. Lee, A. Xie, A. Pacchiano, Y. Chandak, C. Finn, O. Nachum, and E. Brunskill. Supervised Pretraining Can\nLearn In-Context Reinforcemenet Learning. arXiv:2306.14892, 2023.\n[66] T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models. Nature Human\nBehaviour, pages 1\u201316, 2023.\n[67] E. Brooks, L. Walls, R. L. Lewis, and S. Singh. In-Context Policy Iteration. arXiv:2210.03821, 2022.\n[68] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention\nis all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n[69] D. Zhou, N. Sch\u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi.\nLeast-to-Most Prompting Enables Complex Reasoning in Large Language Models. In International Conference\non Learning Representations (ICLR), 2023.\n[70] Abstraction\nand\nRasoning\nChallenge\n1st\nplace\nsolution.\nhttps://www.kaggle.com/competitions/\nabstraction-and-reasoning-challenge/discussion/154597, 2020.\n[71] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced Transformer with Rotary Position\nEmbedding. arXiv:2104.09864, 2021.\n[72] Y. Xu, W. Li, P. Vaezipoor, S. Sanner, and E. B. Khalil. Llms and the abstraction and reasoning corpus: Successes,\nfailures, and the importance of object-based representations. arXiv:2305.18354, 2023.\n[73] R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, and N. D. Goodman. Hypothesis search: Inductive reasoning\nwith language models. arXiv:2309.05660, 2023.\n[74] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,\nA. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language\nmodels. Transactions on Machine Learning Research (TMLR), 2022.\n[75] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. In Annual\nMeeting of the Association for Computational Linguistics, 2015.\n[76] T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for neural text processing. In Conference on Empirical Methods in Natural Language Processing,\n2018.\n[77] D. Hupkes, V. Dankers, M. Mul, and E. Bruni. Compositionality decomposed: How do neural networks\ngeneralise? Journal of Artificial Intelligence Research (JAIR), 2020.\n[78] Z. Allen-Zhu and Y. Li. Physics of Language Models: Part 1, Context-Free Grammar. arXiv:2305.13673, 2023.\n[79] E. Jang. Just Ask for Generalization. In https://evjang.com/2021/10/23/generalization.html, 2022.\n[80] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision\ntransformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021.\n[81] F. Kaplan, P.-Y. Oudeyer, E. Kubinyi, and A. Mikl\u00b4osi. Robotic clicker training. Robotics and Autonomous\nSystems, 38(3-4):197\u2013206, 2002.\n[82] C. Chiandetti, S. Avella, E. Fongaro, and F. Cerri. Can clicker training facilitate conditioning in dogs? Applied\nAnimal Behaviour Science, 184:109\u2013116, 2016.\n[83] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A. Huang, E. Aky\u00a8urek, A. Anandkumar, et al.\nPre-trained language models for interactive decision-making. In Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\n[84] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu,\net al. PaLM-E: An embodied multimodal language model. arXiv:2303.03378, 2023.\n[85] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat. Q8bert: Quantized 8bit bert. In Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE, 2019.\n12\n[86] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00b4e. Flashattention: Fast and memory-efficient exact attention with\nio-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[87] F. Paischer, T. Adler, V. Patil, A. Bitto-Nemling, M. Holzleitner, S. Lehner, H. Eghbal-Zadeh, and S. Hochreiter.\nHistory compression via language models in reinforcement learning. In International Conference on Machine\nLearning (ICML), 2022.\n[88] K. Ellis, C. Wong, M. Nye, M. Sabl\u00b4e-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B.\nTenenbaum. DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning. In\nACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI), 2021.\n[89] K. Ellis, L. Morales, M. Sabl\u00b4e-Meyer, A. Solar-Lezama, and J. Tenenbaum. Learning libraries of subroutines for\nneurally\u2013guided bayesian program induction. In Advances in Neural Information Processing Systems (NeurIPS),\n2022.\n[90] M. F. Cusumano-Towner, F. A. Saad, A. K. Lew, and V. K. Mansinghka. Gen: A General-Purpose Proba-\nbilistic Programming System with Programmable Inference. In ACM SIGPLAN International Conference on\nProgramming Language Design and Implementation (PLDI), 2019.\n13\nA\nSequence Transformation\nA.1\nAbstraction and Reasoning Corpus: Additional Details and Examples\nIn Section 4 of the main paper, we describe how ARC problems require reasoning about a range of different\ntypes of pattern operations\u2014infilling, counting, translating and rotating shapes, and more. In Fig. 8, we\nshow sample problems among the 800 ARC problems for which text-davinci-003 correctly generalizes\nthe pattern shown in a few train examples to a test example. In Fig. 9, we show sample problems that are\nnot correctly solved by text-davinci-003. In Listing 1, we show an example context for an ARC problem\nencoded as integers.\nInput \nOutput \nInput \nOutput \nInput \nOutput \nInput \nOutput \nInput \nOutput\nTrain \nExamples\nTest \nExample\nFig. 8: Sample ARC problems that are correctly solved by text-davinci-003.\nInput \nOutput \nInput \nOutput \nInput \nOutput \nInput \nOutput \nInput \nOutput\nTrain \nExamples\nTest \nExample\nFig. 9: Sample ARC problems that are not correctly solved by text-davinci-003.\ninput:\n0, 0, 0, 0\n0, 3, 4, 0\n0, 7, 6, 0\n0, 0, 0, 0\noutput:\n3, 0, 0, 4\n0, 0, 0, 4\n0, 0, 0, 0\n0, 0, 0, 0\n7, 0, 0, 6\n---\ninput:\n0, 0, 0, 0\n0, 5, 6, 0\n14\n0, 8, 3, 0\n0, 0, 0, 0\noutput:\nListing 1: Example context format for an ARC problem (only one input-output example is shown, along with a query\ninput.\nA.2\nPatterns over Low-Resolution Images\nIn Fig. 10, we show an example in-context grasp detector which outputs target coordinates in a downsampled\nimage, given 6 in-context examples, as well as an example of a simple forward dynamics model predicting\nspatial rearrangement of a red bowl into a green plate, given 9 in-context examples. As LLMs progress on\nthe benchmarks discussed in Section 4, they may become more robust and precise at performing such tasks.\ninput: \n\u00a0123\u00a0 61\u00a0 62\u00a0 93 146\u00a0 92\u00a0 67 67 92 93 \n... \n\u00a0124\u00a0 87\u00a0 62\u00a0 62\u00a0 86\u00a0 91\u00a0 86 86 87 92 \n\u00a0123\u00a0 43\u00a0 44\u00a0 43 \u00a087\u00a0 87\u00a0 91 61 87 87 \n\u00a0123\u00a0 69\u00a0 44\u00a0 68 112 112\u00a0 92 92 93 93 \n\u00a0118 123\u00a0 93 118 117 118\u00a0 87 92 93 93 \noutput: \n\u00a03 6 \ninput: \n\u00a063 47 47 63 77 77 61 57 58 62 \n... \n\u00a063 42 41 42 42 42 37 37 37 42 \n\u00a063 46 46 46 46 46 37 37 41 42 \n\u00a063 62 62 62 62 62 62 62 58 42 \n\u00a063 63 62 62 62 62 62 62 62 62 \noutput: \n\u00a063 47 47 63 77 77 61 57 58 62 \n... \n\u00a063 37 37 42 42 42 42 42 42 42 \n\u00a063 53 53 57 46 42 42 42 42 42 \n\u00a063 58 58 62 46 62 62 62 46 42 \n\u00a063 63 63 63 62 62 62 62 62 62 \nInput\nInput (Low-Res)\nInput & Output (Tokens)\nOutput (Rendered)\nFig. 10: Example LLM prediction as an in-context grasp detector (top) and a simple forward dynamics model (bottom).\nA.3\nToken Invariance for New Token Embeddings\nIn Section 4, we have argued that LLMs are, to a certain extent, invariant to the choice of alphabet a pattern\nis encoded with, in line with prior work on mappings from semantically meaningful tokens to random\ntokens in a pre-trained language model [29, 30, 87]. Here, we present an experiment that investigates\ntoken invariance even further by introducing new token embedding vectors the model has not seen during\ntraining.\nWe sample K many new embedding vectors as Gaussian samples using the mean and 1 or 2 standard\ndeviations of the original LLM embedding matrix statistics in each of embedding vector dimension. This\nway, we create a new token embedding matrix, that mainly consists of the newly sampled token embeddings\nthe model has not seen during training. Additionally, we add embeddings from the original matrix that\ncorrespond to separating tokens (comma, period) to build the input prompts. Although the model has\nnever seen the new embedding vectors during training, we can feed them into the transformer as input and\ncompute cosine similarities at the output analogously to how the original embedding matrix is treated.\nFig. 11 shows the success rate of correctly choosing the target token in a single-token prediction task when\nusing the newly sampled embeddings in comparison with the native embedding matrix. The tasks we are\nconsidering are of the form (1, 1, 2) \u21a6\u2192 2 or (1, 2, 2) \u21a6\u2192 1. We provide in-context examples\nto build a prompt of the form \u201c1, 2, 2, 1 \\n 3, 4, 4, 3 \\n 5, 6, 6, 5 \\n 7, 8, 8,\u201d where\nthe correct prediction should be \u201c7\u201d in this example. Note that the numbers \u201c1\u201d, \u201c2\u201d etc. are randomly\nmapped to the newly sampled token embeddings for indexing purposes and in particular do not enter the\nLLM. As one can see in Fig. 11, for 1\u03c3 noise sampling, the model is able to solve the task with the new\nembeddings with similar performance as with the native embeddings. In case of 2\u03c3, the performance\ndegrades. Although these are relatively simple single-token prediction tasks, this experiment shows that\nLLMs show pattern recognition abilities even when prompted with out-of-distribution continuous input\nembeddings. The results are obtained with K =100, averaged over 3 random seeds when sampling the\ntoken embeddings, 30 instances each, and a context length of 5, 10, or 20 examples. The LLM is the 8B\nparameter variant of [55].\n15\nFig. 11: Token-invariance experiment with newly sampled token embeddings the model has not seen during training.\nShown are success rates when using randomly sampled token embeddings from the native embedding matrix, or newly\nsampled embeddings.\nA.4\nPCFG Benchmark: Additional Details and Ablations\nOur PCFG benchmark is a procedurally generated, adjustable-difficulty benchmark for measuring abstract\nsequence transformation capabilities in LLMs, based on the PCFG from [77]. In Table 3, we show\nillustrations of the primitive operations in the PCFG that can be applied on one or two sequences of\ntokens. In Table 4, we show examples of two transformations (of different complexities) from our\nbenchmark, which are composed of the primitive operations. In Table 5, we show independent ablations of\nsequence length (number of tokens) k and complexity (number of rules) w in the sequence transformations,\nillustrating the way in which the solve rate decreases as either factor increases. In Listing 2, we show an\nexample context for a PCFG problem on integer sequences.\ncopy\nInput\nOutput\nreverse\nshift\nrepeat\necho\nswap\nUnary Functions\nBinary Functions\nappend\nprepend\nremove_first\nremove_second\nInput\nOutput\nName\nName\nTab. 3: Illustrations depicting the unary and binary operators from Hupkes et al. 2020, which we use for our PCFG\nbenchmark.\n6 7 7 8 1 5 9 8 9, 1 5 9 8 9 7 7 6 6; 4 3 0 3 5 0 2 3 8; 5 0 2 3 8 3 3 4\n4; 1 3 3 3 7 0 1 9 9,\nListing 2: Example context format for a PCFG problem (two input-output examples are shown, along with a query\ninput).\n16\nremove_second(swap( , ), )\ns1\ns2\ns3\n5\n3\n0\n5\n3\n7\n6\n1\n6\n7\nExample Inputs\nExample Outputs\necho(copy(swap(swap( \nprepend(remove_second( \nswap(echo(\n)), \n), \n)\ns1s2\ns3s4\ns5s6s7s8s9s10\n6\n7\n7\n8\n1\n5\n9\n8\n9\n1\n5\n9\n8\n9\n7\n7\n6\n6\n4\n3\n0\n3\n5\n0\n2\n3\n8\n5\n0\n2\n3\n8\n3\n3\n4\n4\nFunction\nTab. 4: Illustrations of transformations in our PCFG benchmark. Row 1 shows a transformation composed of w=2\noperations over k=3 tokens, and row 2 shows a transformation composed of w=8 operations over k=10 tokens,\nrespectively. For each transformation function, we show two example inputs and the corresponding outputs.\ntext-davinci-003\nw\nk\n0\n1\n3\n7\n15 31\n1 100\n-\n-\n-\n-\n-\n2 100 100\n-\n-\n-\n-\n4 100 100 100\n-\n-\n-\n8 100\n99\n95\n92\n-\n-\n16 100\n86\n59\n4\n47\n-\n32 100\n74\n32\n14 12 22\ntext-davinci-003 w/ random A\nw\nk\n0\n1\n3\n7\n15\n31\n1 92\n-\n-\n-\n-\n-\n2 91 92\n-\n-\n-\n-\n4 93 92 93\n-\n-\n-\n8 88 82 62 49\n-\n-\n16 84 64 32 17 22\n-\n32 83 40 13\n8\n9\n12\nPaLM\nw\nk\n0\n1\n3\n7\n15 31\n1 100\n-\n-\n-\n-\n-\n2 100 100\n-\n-\n-\n-\n4 100 100 100\n-\n-\n-\n8 100\n89\n74\n82\n-\n-\n16 100\n78\n57\n51 58\n-\n32 100\n68\n23\n18 22 34\nTab. 5: Solve rate (%) for PCFG across number of tokens k and number of rules w for different models.\nA.5\nPCFG Benchmark: Program Synthesis\nWe have run DreamCoder [88] on our PCFG benchmark to contextualize the hardness of the task, and\npresent the results in Table 6. We ran DreamCoder with two different sets of initial primitives:\n\u2022 PCFG Ops. In this version, we provide DreamCoder with an initial set of primitives that corresponds\nto the exact set of unary and binary functions (from [77]) that the PCFG benchmark is based on: copy,\nreverse, shift, swap, repeat, echo, append, prepend, remove first, remove second. We also\ninclude a slicing operator slice, length, and integers 1\u201310.\n\u2022 List Ops. In this version, we provide DreamCoder with a set of list primitives: length, empty, singleton,\nrange, append, map, reduce, true, not, and, or, sort, add, negate, equal, reverse, index, filter,\nslice. These primitives are not specially designed for PCFG and are based on those used in [88, 89].\nIn both cases, the provided primitives are sufficient to define the transformations in the PCFG benchmark.\nFor each (k,w) pair in Table 6, we train on 100 task instances and report the number of tasks which get\nsolved (i.e. a correct program that satisfies the training examples is discovered). We ran DreamCoder\nfor 4 iterations and use the default hyperparameters and timeout. As we would expect, the version of\nDreamCoder with \u201coracle\u201d access to the PCFG operations performs well, in several cases matching or\nexceeding the performance of LLMs. This is especially true when the search problem is easier (i.e. when\nnumber of functions w is smaller). The version of DreamCoder with access to list primitives is also able to\nsolve many of the tasks with small values of w, but there is a sizeable dropoff as the complexity of the tasks\nincreases. These results help to contextualize the difficulty of the PCFG benchmark when given access to\ndifferent amounts of domain-specific information. We also note that we would expect brute-force search\nover the PCFG operators to eventually solve these tasks. Doubling the computation time budget for the\nversion with oracle access to the PCFG operators leads to increased success rates (k=8, w=3) increases\nfrom 80\u219284; (k=16, w=7) increases from 33\u219241.\n17\nDreamCoder w/ PCFG Ops.\nw\nk\n0\n1\n3\n7\n15 31\n1 100\n-\n-\n-\n-\n-\n2 100 100\n-\n-\n-\n-\n4 100 100 98\n-\n-\n-\n8 100 100 80 58\n-\n-\n16 100\n98\n64 38 38\n-\n32 100\n96\n41 24 26 37\nDreamCoder w/ List Ops.\nw\nk\n0\n1\n3\n7\n15 31\n1 100\n-\n-\n-\n-\n-\n2 100 100\n-\n-\n-\n-\n4 100\n85\n51\n-\n-\n-\n8 100\n81\n23 25\n-\n-\n16 100\n63\n26\n4\n17\n-\n32 100\n66\n6\n3\n5\n11\nTab. 6: Solve rate (%) for PCFG across number of tokens k and number of rules w for DreamCoder initialized with\ntwo different sets of primitives.\nB\nSequence Completion\nB.1\nSinusoids: Structure Learning Comparison\nWhile the sinusoid extrapolation could be easily performed with standard regression techniques, we\ncontextualize the task with another method that has no specific prior knowledge of the function being\nextrapolated. We include the structure learning baseline from [90], implemented with Gen [90]. This\nmethod uses a Gaussian Process with an inferred covariance kernel to model time series data. The\ncovariance kernel is inferred using MCMC sampling over a PCFG of covariance functions (e.g. squared\nexponential, periodic). We run the algorithm for 100 iterations and sample from the resulting GP, as shown\nbelow in Fig. 12. The training data is generally fit with low error. However, the quality of the completion\ndiffers for the various functions; the sine wave is generally extrapolated well whereas the sinusoids yield\nhigh variance samples. Similar to the LLMs, greater context generally yields lower error completions. Note\nhowever that the outputs of the structure learning algorithm are high-variance by design, and there are\nmultiple ways to utilize the outputs of the algorithm. We also note that [90] was tested on a larger set of\nfunctions than those we look at here. Though not the goal of our work, it would be interesting future work\nto evaluate how LLMs extrapolate patterns generated by a wider array of function classes. We also refer to\n[42] for an extensive comparison of language models to baselines on regression tasks when formulated\nwith a natural language interface as well as a study on the effects of fine-tuning.\nFig. 12: The structure learning approach (g) extrapolates various functions y = a \u00b7 sin(bx) (top row),\ny = ax\u00b7sin(bx) (middle row), and y = a\n2x sin(bx) (bottom row) with different degrees of error. More\ncontext also generally helps prediction accuracy (light vs. dark).\nB.2\nTable Sweeping: Additional Details\nIn Section 5 of the main paper, we demonstrate how sequence completion capabilities can be applied to\ncontinuation of partial motions, such as sweeping a table. In Fig. 13, we show the average DTW distance\nbetween predicted and ground truth trajectory completions in the Table Sweeping task, given 66% of\nthe trajectory as context, over 30 trials. Each full trajectory consists of 9 sweeping motions across a\ntable. We compare completions made by various language models. We find that larger models generally\n18\nFig. 13: LLM trajectory predictions Table Sweeping improve with larger models.\nperform better; text-davinci-003 performs the best, and also has the lowest variance. On our website, we\nshow qualitative examples of text-davinci-003 completing a table sweeping motion given by a human\ndemonstration.\nB.3\nWhiteboard Drawing: Qualitative Results\nIn Fig. 14, we show example completions for three different loop styles by text-davinci-003 over three trials.\nThe completions generally match the overall shape shown in the two loops given as context. However, the\nresults also qualitatively illustrate that fine motion patterns can be challenging to predict precisely.\nFig. 14: Sampled drawings produced by performing an in-context completion (of one loop, highlighted in green) given\na scripted demonstration of two loops. Each row is a different loop style (narrow, medium, wide), and each column is a\ndifferent trial. Results are shown for text-davinci-003.\nC\nSequence Improvement\nC.1\nMarker in Cup: Additional Details\nIn this task, we use LLMs to generate improved trajectories (according to a reward metric) given a context\nof trajectories that have increasing returns. For this task, states are Cartesian (x, y, z) positions, with each\ndimension normalized between 0 and 200, trajectories are series of states that can be executed via position\ncontrol, and the return of a trajectory is proportional to the negative distance to the goal (cup) plus an offset.\nWe form the trajectories in the context as follows: we take a full trajectory which attains a reward of 100\nand construct trajectories that stop moving 20%, 40%, 60%, and 80% of the way to the goal (such that all\ntrajectories are 50 timesteps). We condition the LLM to generate a 100-reward trajectory by prompting it\nwith \u201c100: start state\u201d. An excerpt of an example context is shown in Listing 3. The results in Figure 5\nfrom the main paper are over 11 trials, each with a different goal position.\n71: 104 83 123, 104 83 123, ...\n72: 104 83 123, 104 83 123, ...\n80: 104 83 123, 104 83 123, ...\n19\n90: 104 83 123, 104 83 123, 104 83 123, 104 83 123, 104 83 123, 104 83\n123, 104 83 123, 104 83 123, 104 83 123, 104 83 123, 104 83 123, 104\n83 123, 104 83 123, 104 83 123, 104 83 123, 105 83 123, 105 83 123,\n106 83 123, 106 83 123, 107 83 123, 108 83 122, 109 83 122, 110 83\n122, 111 83 121, 112 82 120, 113 82 119, 113 82 118, 114 81 118, 115\n81 117, 115 81 116, 115 80 115, 116 80 114, 116 80 113, 117 79 112,\n117 79 111, 118 79 110, 118 78 109, 118 78 109, 118 78 109, 118 78\n109, 118 78 109, 118 78 109, 118 78 109, 118 78 109, 118 78 109, 118\n78 109, 118 78 109, 118 78 109, 118 78 109, 118 78 109\n100: 104 83 123\nListing 3: Example context (excerpt) for a Marker in Cup, illustrating the (reward: state, state, state...) format..\nC.2\nGrid: Additional Details\nIn the Grid environment, observations are x,y positions represented by integers 0\u20138 for each coordinate.\nThere are five possible actions (1, 2, 3, 4, 5) corresponding to (right, up, left, down) movement by one space\nand no-op. A goal is randomly placed in the grid. The agent (which is initialized at the center position)\nreceives a reward of 100 - 10 * distance from the goal to the agent\u2019s final position. Episodes terminate\nafter 20 time steps. For our experiments, we limit the context length to 1024 tokens. At each iteration, the\nLLMs is prompted to generate a trajectory with the maximum seen return from the buffer plus a randomly\nselected offset of up to 20.\nC.3\nCartPole: Additional Details\nWe use a simplified version of the CartPole enviornment in OpenAI Gym. Observations are two-dimensional\n(corresponding to pole angle and velocity, normalized to 0-100) and the maximum time horizon is 200.\nThere are two possible actions (1, 2) corresponding to (left, right), and the agent gets +1 reward for every\ntime step that the CartPole is kept upright. In Listing 4, we show an example context excerpt for CartPole,\nwhere a trajectory history is appended with an encoding of the current trajectory.\n52: 40 50, 1, 40 54, 2, 41 49, 1, 41 54, 1, ...\n60: 45 50, 2, 45 45, 1, 44 50, 2, 44 45, 1, ...\n75: 52 50, 1, 52 55, 2, 53 50, 2, 53 46, 2, ...\n98: 44 50, 1, 44 55, 2, 45 50,\nListing 4: Example context format for a CartPole run. A trajectory history (with each trajectory in the format reward:\nobservation, action, observation, action ...) is followed by an encoding of the current trajectory, up to the current\nobservation.\nBelow, we discuss some additional considerations for forming the context from the trajectory history.\nContext Length. When context length is longer, more trajectories can fit in the context (which yields more\nin-context \u201ctraining data\u201d that could potentially be used to generalize to higher rewards, but also requires\nthe LLM to attend over more tokens). Context length is a limiting factor of using current LLMs in our\ntrajectory improvement setting: the number of tokens required to represent a trajectory history scales with\nthe observation dimensionality, action dimensionality, time horizon, and number of trajectories. For our\nCartPole experiments, we limit the context to 1024 tokens (which is the maximum context length for\ntext-ada-001, text-babbage-001, and text-curie-001 models).\nAction Representation. In initial experiments, we found that the tokens used to represent the action space\n(e.g. \u201c0\u201d for left, \u201c1\u201d for right) can seemingly affect the ability of an LLM to improve trajectories in the\nonline setting. For example, we observed that if \u201c0\u201d is included in the action space, LLMs may \u201cdefault\u201d\nto sampling \u201c0\u201d (likely due to token-specific priors). Therefore, for our experiments, we use 1-indexed\ninteger action representations, which appears to alleviate the bias towards choosing a particular action. The\nfact that action representation can sometimes affect performance complements our observations in the\nSequence Transformation section, in which we find that token mapping invariance holds to some extent,\nbut not entirely.\nC.4\nClicker Training: Additional Details\nIn our clicker training example, the observation consists of the end-effector position and the approximate\nobject position as determined by visual input, with the (x,y,z) values normalized between 0 and 300.\n20\nActions correspond to movements of the end-effector (normalized between 0 and 100, such that 50,50,50\nrepresents no movement). A sample context is given in Listing 5.\n0: 80 ,49 ,138 ,109 ,54 ,133; 45 ,44 ,55\n0: 82 ,32 ,155 ,109 ,54 ,133; 48 ,59 ,48\n0: 82 ,32 ,155 ,109 ,54 ,133; 48 ,59 ,48\n1: 88 ,31 ,154 ,109 ,54 ,133; 45 ,54 ,43\n1: 85 ,36 ,146 ,109 ,54 ,133; 57 ,54 ,46\n1: 93 ,40 ,142 ,109 ,54 ,133; 44 ,52 ,43\n1: ...\nListing 5: Example context format for clicker training. (Reward, observation, action) tuples are ordered by reward\n(with a click corresponding to a reward of 1) with an equal number of reward 0 and reward 1 transitions represented in\nthe context.\n21\n"
  },
  {
    "title": "On decoder-only architecture for speech-to-text and large language model integration",
    "link": "https://arxiv.org/pdf/2307.03917.pdf",
    "upvote": "6",
    "text": "ON DECODER-ONLY ARCHITECTURE FOR SPEECH-TO-TEXT AND LARGE LANGUAGE\nMODEL INTEGRATION\nJian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu,\nTianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu\nMicrosoft, One Microsoft Way, Redmond, USA\nABSTRACT\nLarge language models (LLMs) have achieved remark-\nable success in the field of natural language processing, en-\nabling better human-computer interaction using natural lan-\nguage. However, the seamless integration of speech signals\ninto LLMs has not been explored well. The \u201cdecoder-only\u201d\narchitecture has also not been well studied for speech pro-\ncessing tasks. In this research, we introduce Speech-LLaMA,\na novel approach that effectively incorporates acoustic infor-\nmation into text-based large language models. Our method\nleverages Connectionist Temporal Classification and a sim-\nple audio encoder to map the compressed acoustic features to\nthe continuous semantic space of the LLM. In addition, we\nfurther probe the decoder-only architecture for speech-to-text\ntasks by training a smaller scale randomly initialized speech-\nLLaMA model from speech-text paired data alone. We con-\nduct experiments on multilingual speech-to-text translation\ntasks and demonstrate a significant improvement over strong\nbaselines, highlighting the potential advantages of decoder-\nonly models for speech-to-text conversion.\nIndex Terms\u2014 decoder-only, LLaMA, LoRA, speech\ntranslation\n1. INTRODUCTION\nIn recent times, the large language models (LLMs) have\nshowcased remarkable achievements across various natu-\nral language benchmarks, encompassing question answer-\ning, machine translation, language understanding and more\n[1, 2, 3, 4, 5]. By employing a Transformer-based architec-\nture [6] and training to anticipate forthcoming tokens within\na sequence, this language model excels in contextual learning\nabilities. Not only does this significantly enhance its model-\ning prowess, but more importantly, it enables seamless user\ninteraction that effectively connects cutting-edge research\nwith real-world applications.\nAs speech represents the most innate and instinctive mode\nof human communication, integrating speech and LLMs will\nfurther boost the user experience of human-machine interac-\ntion. Based on this intuition, several attempts in combining\nspeech signals and large language models were carried out\n[7, 8, 9, 10]. Among them, the cascaded approach is the most\nstraightforward solution. In these systems, the speech sig-\nnal is firstly transformed into word tokens through existing\nautomatic speech recognition (ASR) [11] models, and LLM\nprocesses the recognized words for downstream tasks. Later,\ninspired by the integration of image information to LLMs\n[12, 13, 14, 15], researchers also explored the deep combi-\nnation of speech signals [9, 10, 16, 17, 18, 19]. In [16], the\nauthors proposed to jointly model the speech and text tasks\nthrough a unified decoder only network. Similarly, in [19],\nthe authors proposed to optimize the audio token conversion\nmodule together with a off-the-shelf LLM. Instead of word\npieces, discrete tokens of speech representation from a self-\nsupervised model are used in [17].\nWhile there have been promising outcomes, several cru-\ncial challenges regarding the integration of speech and LLMs\nstill require further exploration. Initially, aligning the two\nmodalities (speech and text) using a pretrained LLM poses\nchallenges due to the typically longer sequence length of\nspeech signals compared to text sequences. Moreover, given\nthe costly nature of training LLMs, finding ways to minimize\nthe overall integration cost while maintaining exceptional\nperformance continues to be a challenging task. More impor-\ntantly, considering the remarkable success of the LLMs, it is\ncrucial to explore the untapped potential of using a decoder-\nonly model [3, 16, 20, 21, 22] as the backbone network\narchitecture for speech to text processing.\nIn this study, we aim to tackle the aforementioned chal-\nlenges by exploring an efficient end-to-end integration of\nspeech and language models. Our approach involves design-\ning a simple yet effective architecture where a large language\nmodel that operates on text also incorporates acoustic em-\nbeddings. This integration enables the LM to condition its\ntranscription or translation of the acoustic information. More\nspecifically, our proposed method utilizes a pre-existing LLM\nand incorporates a acoustic feature compressor and an acous-\ntic encoder introducing only a small number of free parame-\nters. Diverging from previous approaches that convert speech\ninto discretized tokens, our model directly maps the continu-\nous representation of speech into the semantic space defined\nby the LM. During the processing stage, the speech feature is\narXiv:2307.03917v3  [eess.AS]  2 Oct 2023\ninitially compressed by the acoustic compressor to reduce the\nsequence length. Subsequently, the acoustic encoder trans-\nforms the compressed speech signal into continuous vectors\nin the same semantic space of the text that can be consumed\nby the LLM. The final output is generated through the decod-\ning process of the LLM.\nWe thoroughly investigate various practical aspects of\nour proposed model, such as selecting the appropriate acous-\ntic compressor, attention mask, and fine-tuning methods.\nAdditionally, we apply the proposed model to the task of\ntranslating speech in 13 different languages into English\n(EN) text and compare its performance against a strong base-\nline on CoVoST dataset. Finally, we demonstrate that the\ndecoder-only model, even trained from scratch using only\nspeech-text paired data, exhibits significant potential and\nseveral advantages over the commonly employed encoder-\ndecoder architecture in speech processing. In this work, our\ncontribution can be summarized as follows:\n\u2022 We introduce an efficient end-to-end integration method\ncalled Speech-LLaMA, which effectively integrates\nexisting text-based large language models with speech\nprocessing.\nWe have achieved substantial improve-\nments in translation performance compared to strong\nbaselines on various speech translation (ST) tasks.\n\u2022 We investigate various practical aspects of the proposed\nspeech-LLM integrations that are crucial for enhancing\nperformance. These aspects include acoustic compres-\nsion of the acoustic feature, attention mask selection,\nand fine-tuning strategy.\n\u2022 On large, diverse and real-world data, we show that the\ndecoder-only architecture can be as competitive as the\nencoder-decoder architecture for speech-to-text tasks.\nWe show that decoder-only to also be more parameter\nefficient.\n2. RELATED WORK\nOur model aims at integrating speech signals into large lan-\nguage models, as well as relates to Connectionist Tempo-\nral Classification (CTC) feature length compression and low-\nrank adaptation (LoRA). We discuss these topics in the fol-\nlowing.\n2.1. Large language models\nLLMs are generally pre-trained on vast amounts of textual\ndata that span a wide variety of domains and languages. They\nusually consist of a stack of transformer layers, following an\nauto-regressive decoder-only architecture, where each output\ntoken is used as the input to predict the next step token. In\nthis work, we select LLaMA-7B [5] as the backbone LLM\nto build the proposed method. LLaMA-7B model consists of\n32 Transformer encoder layers with 32 heads and 4096 atten-\ntion dimension. The tokenizer from the LLaMA work has a\nvocabulary size of 32,000 which covers a group of languages.\n2.2. CTC compressor\nConnectionist Temporal Classification (CTC) compressor\n[23] was proposed to reduce the sequence length via remov-\ning the redundant information in the features. It was applied\nin speech translation task and was shown to yield better mem-\nory consumption and performance. The method adds a linear\nCTC branch in a middle layer of the encoder which is jointly\noptimized with the main cross-entropy criteria . The hidden\nrepresentations of the CTC branch are then compressed ac-\ncording to the distributions of the CTC posteriors and are\npassed to the succeeding layers. The author investigated a\nfew variations within this method of sequence length com-\npression. They found that averaging the consecutive hidden\nrepresentations (corresponding to consecutive CTC predic-\ntions belonging to the same class) gives the best performance.\n2.3. LoRA\nLow-Rank Adaptation (LoRA) [24] is a commonly used tech-\nnology to adapt the large models for new datasets or tasks. It\nintroduces a small amount of free parameters to each Trans-\nformer layer of the source large model, while freezing all\nthe original model parameters. Specifically, for each weight\nmatrix W \u2208 Rd\u00d7k in a Transformer layer, 2 new matrices\nWa \u2208 Rd\u00d7r and Wb \u2208 Rr\u00d7k are introduced such that r \u226a\nmin{d, k}. For each matrix multiplication during training,\nthe input x is firstly multiplied with both original weight W\nand its introduced low-rank approximation Wa, Wb, then the\ntwo outputs are summed to form the output for later compu-\ntation. Only Wa and Wb are updated during fine-tuning while\nW keeps frozen, thus significantly reducing the memory foot-\nprint during training.\n3. OUR APPROACH\nIn this work, we design an architecture named Speech-\nLLaMA where a text-LLM can also accept acoustic embed-\nding as well as text as conditional prompts for text generation.\nBy converting the speech input to a sequence of acoustic em-\nbeddings within the same space of the text embeddings, in\nthe aspect of both length and semantics, the pre-trained text\nLLM can leverage its in-context learning capacity to absorb\nthe speech signal and output corresponding text for speech\ntranslation task.\nOverall, given the text prompt p and audio signals x,\nthe generation of the corresponding text sequence y\n=\n{y0, y1, \u00b7 \u00b7 \u00b7 , yN\u22121} with a text-LLM is formulated as:\np(y|p, x; \u0398LLM) =\nN\u22121\nY\nn=0\np(yn|y<n, p, x; \u0398LLM)\n(1)\nText Prompt\nEmbedding\nAudio Encoder\nCTC Compressor\nAcoustic Features\nLLM\n.......\n....\n....\n...\nPredicted Tokens\nLLM\nFig. 1. High-level architecture of our proposed approach with\nLLM. The green blocks indicate the part of the LLM. In this\nwork, we only learn parameters in the \u201cAudio Encoder\u201d, keep-\ning everything else frozen.\nwhere y<n indicates the generated text sequence before yn.\nOverview Our proposed neural model consists of three\ndistinct parts: a pre-trained text neural LLM, an audio en-\ncoder and a CTC compressor, as shown in Figure 1. The text-\nLLM in our case is a LLaMA-7B [14] but this method can\nbe generalized to LLMs of any scale. The CTC compressor\nreduces the sequence length of the input speech filter-bank to\nmatch the length of the text, and the audio encoder transforms\nthe compressed speech signal into continuous vectors in the\nLLM\u2019s semantic space.\nCTC compressor Different from the prior work that\ntrained the CTC compressor jointly with the main task [23],\nour CTC compressor is a pre-trained module, aiming to match\nthe audio and the text duration to the same scale by selecting\nthe representative frames from the audio signal. In this work,\nwe explore two ways to reduce the sequence length of the\nacoustic features in the CTC compressor: \u201cblank-removal\u201d\nand \u201cframe-averaging\u201d. For \u201cblank-removal\u201d, we simply dis-\ncard all the frames that predicted the blank symbol according\nto the distribution of the CTC posteriors. On the other hand,\nfor \u201cframe-averaging\u201d, we average the hidden states of con-\nsecutive frames without blank frames removed, once their\nCTC predictions belong to the same class.\nAudio encoder The audio encoder is used to bridge rep-\nresentations generated from the CTC compressor to the text\nembeddings of the text-LLM. This module is designed to be\nrelatively small in size and is initialized with random weights.\nDuring the fine-tuning process, the audio encoder is opti-\nmized to effectively integrate the audio information within\nthe LLM, enhancing the overall performance of the system.\nDifferent from the methods in [7, 19], where the audio en-\ncoder is trained to firstly map the speech signal into discrete\ntokens, which is then consumed by LLM, the proposed audio\nencoder is directly optimized to map the compressed acoustic\nsignal to the continuous semantic space of LLM, allowing a\ndeep integration between the audio encoder and the language\nConv2D Encoder\nDecoder\n.......\n....\nPredicted Tokens\n<SOS>\nAcoustic Features\nDecoder\nFig. 2. The architecture of the decoder-only model for the\nfrom-scratch training. We use \u27e8SOS\u27e9 token to indicate the\nstarting of the text generation.\nmodel.\nInstruct learning For each training sample, we prepend\na text prompt that briefly describes the task, e.g., \u201caudio \u21d2\nEnglish\u201d and \u201ctranscribe the audio into English\u201d.\nThe text prompt are sampled from a pre-defined list, where\nsome prompts contains the source language ID following the\nformat \u201ctranslate [source] audio into English\u201d. Dur-\ning evaluation, we fix the text prompt as \u201ctranslate the\naudio into English\u201d for all testing samples.\nLoRA fine-tuning On top of the proposed model, we\napply the LoRA to four attention matrices in each layer of\nthe LLaMA Transformer (e.g., Wq, Wk, Wv, Wo). To stabi-\nlize the training, we adopt a two-stage training scheme which\nmeans we train the audio encoder firstly with the CTC com-\npressor and LLaMA frozen and then introduce LoRA to the\nwell-trained model and perform the second stage optimiza-\ntion. The entire system is still trained with cross-entropy loss\nbetween the LLM output and the reference transcription se-\nquence on the same training data.\nFrom-scratch training To further explore the potential\nof decoder-only architecture as a foundational architecture for\nspeech modeling, we also include a \u201cfrom-scratch\u201d training of\na decoder-only architecture. Here, we replace the text prompt,\naudio encoder, and CTC compressor with a randomly initial-\nized convolutional 2D encoder. We also replace the pretrained\nLLaMA network with a much smaller randomly initialized\nautoregressive network. This architecture is shown in Figure\n2. We add an \u27e8SOS\u27e9 token at the end of the acoustic sequence\nto indicate the starting of the generation. In this case, the gen-\neration of the text sequence y with a decoder-only model is\nconditioned purely on audio signal x and previously gener-\nated text sequence y<n:\np(y|x; \u0398DEC) =\nN\u22121\nY\nn=0\np(yn|y<n, x; \u0398DEC)\n(2)\nwhere \u0398DEC refers to the parameters of the decoder model.\n4. EXPERIMENTS\nThe speech translation (ST) task [25, 26, 27, 28, 29] has been\nchosen as the primary evaluation benchmark for assessing the\nproposed methods. In this task, the goal is to develop a system\nthat can accurately translate spoken language from 13 source\nlanguages to English.\n4.1. Data and metric\nThe 13 source languages we want to translate to EN are Ger-\nman (DE), Chinese (ZH), Arabic (AR), Spanish (ES), French\n(FR), Italian (IT), Dutch (NL), Japanese (JA), Russian (RU),\nPortuguese (PT), Estonian (ET), Swedish (SV) and Slovenian\n(SL). We chose these languages based on availability of train-\ning and testing data. The training data for each language con-\ntains 1K hours of in-house speech data. To make the model\nmore robust, we also include 1K hours of EN data, bringing\nthe total to 14K hours. The original source transcriptions for\nnon-English speech utterances are fed into an in-house trans-\nlation service to generate the corresponding English transcrip-\ntions with both punctuation and capitalization. Those pseudo-\nlabel English transcriptions are used as the target transcription\nfor ST task training. All the training data was anonymized\nwith personally identifiable information removed.\nOur speech translation models are evaluated on the 13 lan-\nguages from the above list. The corresponding test sets are\nselected from CoVoST 2 dataset [30]. We evaluate the BLEU\n[31] scores for the performance comparison.\n4.2. Models configuration\n4.2.1. CTC compressor\nThe CTC compressor contains 2 convolution-2D layers fol-\nlowed by 4 Transformer layers for a 4-times subsampling with\n15.8M parameters in total. Each transformer layer has a 512-\ndimensional self-attention module with 8 heads and a 2048 di-\nmensional feed-forward network (FFN). Each convolution 2D\nlayer has a stride size of 2 and kernel size of 3. We pre-trained\nCTC compressor with the paired speech and text data (i.e.,\nASR task) from 13 languages using the CTC objective func-\ntion because that in our preliminary experiments, the BLEU\nscore with ASR task training is much better than the one with\nthe ST task training. Once CTC compressor is trained, the\nparameters are frozen during later training stages.\nFor comparison, we include a convolution-based subsam-\npling module as a baseline, which shares the same architec-\nture with the CTC compressor but with additional 3 1D con-\nvolution layers on top, allowing 4 \u00d7 8 = 32 times feature\nlength reduction in total. The convolution-based subsampling\nis jointly trained with the audio encoder parameters.\n4.2.2. Audio encoder\nThe audio encoder consists of 4 Transformer layers, where\neach layer has the same setting as in the CTC compressor\nexcept that the output tensor of the last layer is converted to\nthe dimension of 4096, in order to match the dimension of\nsemantic embedding in LLaMA.\nFor each training sample, we concatenate the embeddings\nof the text prompt and representations from the audio encoder\nalong the time axis and use that as the prefix feature sequence\nto feed to the LLaMA model to generate the target language\n(EN) transcriptions.\nTwo attention mask strategies are explored within the\nLLaMA model. The first follows the language model train-\ning, where a causal, i.e., lower triangle attention mask is ap-\nplied for each transformer layer to constrain the self-attention\nto not look into the future. As the proposed model is \u201cnon-\nstreaming\u201d in nature, we also explore a non-causal full at-\ntention mask strategy for the prefix part only [32], i.e., text\nprompt and audio encoder representations, to enable the full\ncontext learning on the acoustic information.\n4.2.3. LoRA fine-tuning\nWe simply choose rank value of 2 for LoRA fine-tuning ex-\nperiments according to the results of the LoRA work [24], i.e.,\n8 rank-2 matrices in the shape of 2 \u00d7 4096 are introduced to\neach LLaMA Transformer layer as an adaptor, which results\nin 2.1M more parameters in total. The LoRA fine-tuning is\nconducted on a well-trained Speech-LLaMA model, where\nthe CTC compressor and the LLaMA parameters are frozen.\nWe still update the audio encoder to learn better representa-\ntions together with the adapted LLaMA.\n4.2.4. Baseline\nWe adopt a seq2seq [33, 34] based speech translation model\nas a baseline. More specifically, we use the Whisper [35]\narchitecture with 240M parameters and train it on the 14K\nhour data mentioned in Section 4.1. It contains a 12-layer\naudio Transformer encoder and a 12-layer text Transformer\nencoder, where the attention dimension and head number\nis 768 and 12, respectively. We optimize the model using\ncross-entropy as the primary objective function but also aug-\nment this architecture with a CTC loss on the encoder. We\ntrain the whole network end-to-end in a multi-task fashion.\nPlease note that, for a fair comparison, we start the seq2seq\ntraining from scratch and do not initialize with pretrained\nopen source Whisper weights. During beam search inference,\nwe do a joint-decoding (prefix-decoding) [36] with CTC. To\nmake the comparison with LLaMA boot-strapped models\nmore appropriate, we also present results with n-best rescor-\ning of the seq2seq model with LLaMA. To accomplish that,\nwe do a simple log-linear interpolation between the scores\nfrom seq2seq and LLaMA for each of the n-best hypotheses\nTable 1. BLEU scores of the 13 languages on the baseline and the proposed models.\nModel\nSeq2seq\nDecoder-only\nSpeech-LLaMA\nID\nB1\nB2\nD1\nE0\nE1\nE2\nE3\nE4\nE5\nE6\nCompressor\n\u2212\n\u2212\n\u2212\n\u00d7\nCTC (remove)\nCTC (average)\nLearnable #Param.\n240M\n150M\n29M\n14M\n14M\n16.1M\n14M\n16.1M\nPrefix Non-causal Mask\n\u2212\n\u2212\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\nLoRA\n\u2212\n\u2212\n\u2212\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nE3\n\u00d7\nE5\nLLaMA Rescore\n\u2212\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nAR\n22.8\n24.9\n21.2\n16.9\n24.6\n24.7\n24.6\n26.3\n25.9\n28.2\nDE\n22.6\n23.6\n21.3\n16.9\n22.6\n22.8\n24.3\n26.0\n25.4\n27.1\nZH\n7.0\n7.2\n6.7\n3.4\n9.6\n10.1\n10.1\n11.4\n10.8\n12.3\nES\n23.7\n24.9\n22.7\n19.6\n23.5\n24.0\n25.4\n27.3\n26.2\n27.9\nFR\n21.8\n22.7\n20.6\n15.4\n20.9\n21.1\n22.6\n24.5\n23.2\n25.2\nIT\n20.7\n21.6\n19.8\n16.7\n21.4\n21.0\n23.7\n25.3\n24.0\n25.9\nNL\n34.6\n36.0\n35.2\n28.3\n32.4\n35.0\n34.1\n36.0\n34.9\n36.5\nJA\n15.3\n15.7\n16.3\n10.3\n17.5\n17.1\n17.7\n19.8\n19.2\n19.9\nRU\n26.4\n27.7\n26.0\n22.8\n31.0\n32.0\n33.3\n35.5\n34.3\n36.8\nPT\n28.9\n30.2\n27.2\n22.8\n26.8\n27.7\n29.2\n31.3\n30.2\n32.0\nET\n9.4\n9.4\n7.4\n15.4\n17.0\n18.3\n17.2\n18.1\n18.0\n18.7\nSV\n24.4\n25.6\n27.5\n26.3\n25.3\n28.8\n26.7\n27.4\n27.2\n29.0\nSL\n13.3\n12.7\n13.3\n22.2\n20.3\n22.9\n22.8\n22.2\n22.1\n22.7\nAverage BLEU \u2191\n20.8\n21.7\n20.4\n18.2\n22.5\n23.5\n24.0\n25.5\n24.7\n26.3\nand then re-rank accordingly. We use n = 5 for seq2seq\nbeam-search decoding and the re-ranking experiments.\n4.2.5. From-scratch training\nIn this setting, the structure of the convolutional 2D encoder\ncontains 2 convolutional layers which is the same as the one in\nthe CTC compressor, which introduces a 4-times subsampling\nrate. For the Transformer decoder, we follow the implemen-\ntation of LLaMA, where pre-normalization, SwiGLU activa-\ntion function [37] and rotary positional embeddings (RoPE)\n[38] are adopted. Similar to the configuration of the seq2seq\nbaseline, each decoder layer contains a 12-head self-attention\nmodule with the 768 attention dimension. The dimension of\nthe feed-forward network is set as 4076.\n4.3. Training and evaluation\nWe extract an 80-dim log mel-filterbank using 25 msec win-\ndow and 10 msec hop size as the acoustic features. Global\nmean and variance normalization is applied. All models were\ntrained with AdamW optimizer [39] with \u03b21 = 0.9 and \u03b22 =\n0.98 on 16 V100 GPUs and a warmup and linear decay learn-\ning rate strategy is used. Batch size varies with the model size.\nCTC compressor was trained for 100K steps with source lan-\nguage transcriptions, tokenized by LLaMA\u2019s tokenizer. The\npeak learning rate was set to 0.001. In the first stage training\nof Speech-LLaMA, We perform a 500K step training with\na peak learning rate of 0.015 while in the later LoRA fine-\ntuning stage, we use additional 100K optimization steps with\na peak learning rate of 2e\u22124. The from-scratch decoder-only\nmodels were trained with a peak learning rate of 0.001 for at\nmost 300K steps. We use the beam search algorithm with a\nbeam size 4 for the decoding of all the decoder-only models,\nunless noted otherwise. Both seq2seq and decoder-only mod-\nels use English-only byte pair encoding (BPE [40]) model for\nthe tokenization which has a vocabulary size of 5,857 while\nthe Speech-LLaMA models keep using LLaMA\u2019s tokenizer.\n5. RESULTS AND DISCUSSIONS\nThe results of the experiments are presented in Table 1, where\nseveral observations can be gleaned.\n5.1. Baselines\nFor baselines, we report results on 2 systems. B1 is a seq2seq\nmodel described in Section 4.2.4 and B2 is B1 with LLaMA\nn-best rescoring. As expected [41], a 0.9 better BLEU score\ncan be observed from B2 system over B1. This suggests that\nshallow integration with LLM can still bring benefits to the\nspeech models.\n5.2. Deeper integration with LLaMA\nWhile shallow integration can boost performance, the gains\nusing a deep integration technique like Speech-LLaMA\nshould be much higher. Systems E1 \u223c E6 describe Speech-\nLLaMA models in various configurations. We can find all\nSpeech-LLaMA configurations significantly outperform the\nbaselines with the limited learnable parameters, resulting in\nup to 4.6 absolute BLEU score improvement (21.2% relative).\nThese results show the efficacy of the proposed system and\nalso suggests the necessity for deeper integration between the\nspeech models and text-LLMs.\n5.3. CTC compressor\nResults from system E0, E1 and E3 describe the importance\nof CTC compressor for audio length reduction, in our design.\nComparing E1 over E0, we obtain consistently better perfor-\nmance showing the effectiveness of CTC compressor over\nthe convolution one. This gain is despite the fact that CTC\ncompressor is frozen during the training while the convolu-\ntion compressor was fine-tuned with the rest of audio encoder.\nOne hypothesis for the better performance of CTC compres-\nsor is that it leverages the transcription of each source lan-\nguage during pre-training stage as we also observe that replac-\ning the current CTC compressor model with the one trained\nwith ST labels brings worse BLEU scores in our preliminary\nexperiments. This observation also suggests that a potentially\nbetter performance might be obtained if the source transcrip-\ntion is also used during the training stage. We leave this line\nof exploration for future works.\nWithin the CTC compressor, comparing system E3 over\nE1, the \u201cframe-averaging\u201d strategy shows a 1.5 better average\nBLEU score over \u201cblank-removal\u201d strategy. We believe that\nit is because the CTC compressor can\u2019t very reliably distill\nall relevant information into non-blank representations. Thus\nthe frames selected by the CTC compressor might lose some\nacoustic information which cause the degradation of the per-\nformance. The averaging strategy is more robust to this com-\npression error which aligns with the prior work [23].\n5.4. Effect of non-causal attention mask\nIt is expected that the full attention mask over text prompt\nand acoustic representations would usually result in better\nspeech representation, and consequently better results. For\neach type of CTC compression strategy, our experiments\ndemonstrate that using a non-causal attention mask over a\ncausal mask can indeed bring gains. Comparing system E2\nover E1, we see that switching to a non-causal mask brings\nan additional gain of 1.5 average BLEU score when using the\n\u201cblank-removal\u201d strategy within CTC compressor. Similarly,\ncomparing systems E5 over E3, we again observe a gain of\n0.7 average BLEU score, when using the \u201cframe-averaging\u201d\nstrategy within CTC compressor. The gain with non-causal\nmask is understandably larger in \u201cblank-removal\u201d strategy,\nsince future acoustic information can help compensate for\npotential loss of information caused due to removal of frames\ncorresponding to the blank symbol of the CTC loss. Even\nin LoRA fine-tuning systems, e.g., comparing E6 and E4,\nwe can still observe a gain of 0.8 average BLEU score with\nnon-causal mask applied.\n5.5. LoRA fine-tuning\nE4 and E6 represent our systems with LoRA fine-tuning.\nComparing E4 over E3 shows the gains using LoRA fine-\ntuning when using a causal attention mask while comparing\nE6 over E5 show corresponding gains when using a non-\ncausal attention mask. We can obtain an additional increase\nof 1.5 and 1.6 average BLEU score, respectively. Note that\nonly 2.1M additional parameters are added as adaptors. Po-\ntentially better performance might be observed when larger\nrank is used. We leave this exploration for future works.\n5.6. Decoder-only vs Encoder-Decoder\nFinally, the results for the randomly initialized decoder-only\nmodel are shown as system D1 in Table 1.\nThis model\nachieves only slightly worse (0.4 lower BLEU score) per-\nformance compared to the seq2seq baseline. But the total\nparameter for the decoder-only model in our study is also\nsignificantly lower than the seq2seq baseline. We think that\ndecoder-only architecture can be more parameter efficient\nthan the encoder-decoder architecture. This is because a sin-\ngle module is used to learn representations for both source\nand target sequences in the former while separate modules\n(encoder and decoder) are used to generate representations\nfor source and target sequences in the latter. This sharing of\nparameters to process input and output jointly can bring out\nbetter parameter efficiency in the decoder-only architecture.\nOur results do seem to validate this theory. In future, we will\nconduct more extensive analysis of how model size effects\nperformance in these 2 architectures.\n6. CONCLUSION & FUTURE WORK\nIn this work, we propose a method to infuse an off-the-\nshelf large language model with acoustic information. The\nproposed model presents a deep integration between the\naudio with the LLM by directly mapping acoustic repre-\nsentation into the semantic space of LLM. We also explore\nseveral practical aspects of the proposed model for better\nperformance including compression of the acoustic fea-\nture, attention mask design and adapter fine-tuning.\nWe\nshow that on a 13 language to English speech translation\ntask, the proposed model significantly outperforms a strong\nsequence-to-sequence baseline model. We also show that the\ndecoder-only architecture, trained from scratch, can achieve\ncomparable performance with around 40% fewer parame-\nters, which verifies the potential of decoder-only models for\ngeneral speech-to-text modeling.\n7. REFERENCES\n[1] OpenAI, \u201cIntroducing chatgpt,\u201d OpenAI Blog, 2022.\n[2] OpenAI,\n\u201cGpt-4 technical report,\u201d\narXiv preprint\narXiv:2303.08774, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al., \u201cLanguage models are few-shot learners,\u201d\nAdvances in neural information processing systems, vol.\n33, pp. 1877\u20131901, 2020.\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al., \u201cPalm: Scaling language model-\ning with pathways,\u201d arXiv preprint arXiv:2204.02311,\n2022.\n[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al., \u201cLlama: Open and efficient foundation lan-\nguage models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin, \u201cAttention is all you need,\u201d Ad-\nvances in neural information processing systems, vol.\n30, 2017.\n[7] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong\nShi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing\nHong, Jiawei Huang, Jinglin Liu, et al., \u201cAudiogpt: Un-\nderstanding and generating speech, music, sound, and\ntalking head,\u201d arXiv preprint arXiv:2304.12995, 2023.\n[8] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang, \u201cHugginggpt: Solv-\ning ai tasks with chatgpt and its friends in huggingface,\u201d\narXiv preprint arXiv:2303.17580, 2023.\n[9] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang\nZhang, Jing Shi, Shuang Xu, and Bo Xu, \u201cX-llm: Boot-\nstrapping advanced large language models by treating\nmulti-modalities as foreign languages,\u201d arXiv preprint\narXiv:2305.04160, 2023.\n[10] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and\nHuaming Wang, \u201cPengi: An audio language model for\naudio tasks,\u201d arXiv preprint arXiv:2305.11834, 2023.\n[11] Jinyu Li,\n\u201cRecent advances in end-to-end automatic\nspeech recognition,\u201d\nAPSIPA Transactions on Signal\nand Information Processing, vol. 11, no. 1, 2022.\n[12] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny,\n\u201cMinigpt-4: Enhancing vision-\nlanguage understanding with advanced large language\nmodels,\u201d arXiv preprint arXiv:2304.10592, 2023.\n[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee,\n\u201cVisual instruction tuning,\u201d\narXiv preprint\narXiv:2304.08485, 2023.\n[14] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al.,\n\u201cLlama-adapter v2: Parameter-\nefficient visual instruction model,\u201d\narXiv preprint\narXiv:2304.15010, 2023.\n[15] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.,\n\u201cFlamingo: a visual language model for few-shot learn-\ning,\u201d Advances in Neural Information Processing Sys-\ntems, vol. 35, pp. 23716\u201323736, 2022.\n[16] Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shu-\njie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu\nWei, \u201cViola: Unified codec language models for speech\nrecognition, synthesis, and translation,\u201d arXiv preprint\narXiv:2305.16107, 2023.\n[17] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu\nWang, Yaqian Zhou, and Xipeng Qiu,\n\u201cSpeechgpt:\nEmpowering large language models with intrinsic\ncross-modal conversational abilities,\u201d\narXiv preprint\narXiv:2305.11000, 2023.\n[18] Eliya Nachmani, Alon Levkovitch, Julian Salazar,\nChulayutsh\nAsawaroengchai,\nSoroosh\nMariooryad,\nRJ Skerry-Ryan, and Michelle Tadmor Ramanovich,\n\u201cLms with a voice: Spoken language modeling beyond\nspeech tokens,\u201d arXiv preprint arXiv:2305.15255, 2023.\n[19] Paul\nK\nRubenstein,\nChulayuth\nAsawaroengchai,\nDuc Dung Nguyen, Ankur Bapna, Zal\u00b4an Borsos, F\u00b4elix\nde Chaumont Quitry, Peter Chen, Dalia El Badawy,\nWei Han, Eugene Kharitonov, et al., \u201cAudiopalm: A\nlarge language model that can speak and listen,\u201d arXiv\npreprint arXiv:2306.12925, 2023.\n[20] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al.,\n\u201cNeural codec lan-\nguage models are zero-shot text to speech synthesizers,\u201d\narXiv preprint arXiv:2301.02111, 2023.\n[21] Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan\nChen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu\nWei, \u201cSpeak foreign languages with your own voice:\nCross-lingual neural codec language modeling,\u201d 2023.\n[22] Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho\nSo, Shengding Hu, Zhiyuan Liu, and Nigel Collier,\n\u201cDecoder-only or encoder-decoder?\ninterpreting lan-\nguage model as a regularized encoder-decoder,\u201d arXiv\npreprint arXiv:2304.04052, 2023.\n[23] Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco\nTurchi, \u201cCtc-based compression for direct speech trans-\nlation,\u201d arXiv preprint arXiv:2102.01578, 2021.\n[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen, \u201cLora: Low-rank adaptation of large lan-\nguage models,\u201d arXiv preprint arXiv:2106.09685, 2021.\n[25] Laura Cross Vila, Carlos Escolano, Jos\u00b4e AR Fonollosa,\nand Marta R Costa-Jussa,\n\u201cEnd-to-end speech trans-\nlation with the transformer.,\u201d in Proceedings of Inter-\nspeech, 2018, pp. 60\u201363.\n[26] Matthias Sperber and Matthias Paulik, \u201cSpeech transla-\ntion and the end-to-end promise: Taking stock of where\nwe are,\u201d\nin Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, 2020,\npp. 7409\u20137421.\n[27] Jian Xue, Peidong Wang, Jinyu Li, and Eric Sun,\n\u201cA weakly-supervised streaming multilingual speech\nmodel with truly zero-shot capability,\u201d arXiv preprint\narXiv:2211.02499, 2022.\n[28] Jian Xue, Peidong Wang, Jinyu Li, Matt Post, and\nYashesh Gaur,\n\u201cLarge-scale streaming end-to-end\nspeech translation with neural transducers,\u201d\nINTER-\nSPEEcH, vol. abs/2204.05352, 2022.\n[29] Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long\nZhou,\nYashesh Gaur,\nShujie Liu,\nand Jinyu Li,\n\u201cLamassu: Streaming language-agnostic multilingual\nspeech recognition and translation using neural trans-\nducers,\u201d INTERSPEECH, 2023.\n[30] Changhan Wang, Anne Wu, and Juan Pino,\n\u201cCovost\n2: A massively multilingual speech-to-text translation\ncorpus,\u201d 2020.\n[31] Matt Post, \u201cA call for clarity in reporting BLEU scores,\u201d\nin Proceedings of the Third Conference on Machine\nTranslation: Research Papers, Brussels, Belgium, Oct.\n2018, pp. 186\u2013191, Association for Computational Lin-\nguistics.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu, \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d\nThe\nJournal of Machine Learning Research, vol. 21, no. 1,\npp. 5485\u20135551, 2020.\n[33] A. Berard, O. Pietquin, C. Servan, and L. Besacier,\n\u201cListen and translate: A proof of concept for end-to-\nend speech-to-text translation,\u201d in NIPS Workshop on\nEnd-to-end Learning for Speech and Audio Processing,\n2016.\n[34] Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui\nWu, and Zhifeng Chen, \u201cSequence-to-sequence models\ncan directly translate foreign speech,\u201d Proc. Interspeech,\npp. 2625\u20132629, 2017.\n[35] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever, \u201cRobust\nspeech recognition via large-scale weak supervision,\u201d in\nInternational Conference on Machine Learning. PMLR,\n2023, pp. 28492\u201328518.\n[36] Takaaki Hori, Shinji Watanabe, and John Hershey,\n\u201cJoint CTC/attention decoding for end-to-end speech\nrecognition,\u201d in Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), Vancouver, Canada, July 2017,\npp. 518\u2013529, Association for Computational Linguis-\ntics.\n[37] Noam Shazeer,\n\u201cGlu variants improve transformer,\u201d\narXiv preprint arXiv:2002.05202, 2020.\n[38] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu, \u201cRoformer: Enhanced trans-\nformer with rotary position embedding,\u201d arXiv preprint\narXiv:2104.09864, 2021.\n[39] Ilya Loshchilov and Frank Hutter,\n\u201cDecoupled\nweight\ndecay\nregularization,\u201d\narXiv\npreprint\narXiv:1711.05101, 2017.\n[40] Taku Kudo and John Richardson, \u201cSentencepiece: A\nsimple and language independent subword tokenizer\nand detokenizer for neural text processing,\u201d\narXiv\npreprint arXiv:1808.06226, 2018.\n[41] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, \u201cPrompt-\ning large language models for zero-shot domain\nadaptation in speech recognition,\u201d\narXiv preprint\narXiv:2306.16007, 2023.\n"
  },
  {
    "title": "Solvent: A Framework for Protein Folding",
    "link": "https://arxiv.org/pdf/2307.04603.pdf",
    "upvote": "5",
    "text": "Solvent: A Framework for Protein Folding\nJaemyung Lee\nKakao Brain\njames.brain@kakaobrain.com\nKyeongtak Han\u2020\nDepartment of Electrical and Computer Engineering, Inha University\nhan00127@inha.edu\nJaehoon Kim\nKakao Brain\njack.brain@kakaobrain.com\nHasun Yu\nKakao Brain\nshawn.yu@kakaobrain.com\nYouhan Lee\nKakao Brain\nyouhan.lee@kakaobrain.com\nAbstract\nConsistency and reliability are crucial for conducting AI research. Many famous re-\nsearch fields, such as object detection, have been compared and validated with solid\nbenchmark frameworks. After AlphaFold2, the protein folding task has entered a\nnew phase, and many methods are proposed based on the component of AlphaFold2.\nThe importance of a unified research framework in protein folding contains imple-\nmentations and benchmarks to consistently and fairly compare various approaches.\nTo achieve this, we present Solvent, a protein folding framework that supports\nsignificant components of state-of-the-art models in the manner of an off-the-shelf\ninterface Solvent contains different models implemented in a unified codebase\nand supports training and evaluation for defined models on the same dataset. We\nbenchmark well-known algorithms and their components and provide experiments\nthat give helpful insights into the protein structure modeling field. We hope that\nSolvent will increase the reliability and consistency of proposed models and give\nefficiency in both speed and costs, resulting in acceleration on protein folding\nmodeling research. The code is available at https://github.com/kakaobrain/solvent,\nand the project will continue to be developed.\n1\nIntroduction\nDeep-learning-based protein structure prediction task has become an essential research area after\nthe witnesses of Alphafold2[1]. Additionally, protein folding methods without multiple sequence\nalignments(MSA) inputs such as ESMFold[2] and OmegaFold[3] have been proposed to model\ngeneral proteins in high-speed from the removal of generating MSA. Especially, those models have\nshown promising results on specific applications, including antibody structure and de-novo structure\nIgFold[4] and tFold-Ab[5]. Each method has published papers and released inference code with\npre-trained models. However, compared to the MSA-dependent method, which has a well-established\n\u2020Kyeongtak Han was an intern at Kakao Brain.\npreprint.\narXiv:2307.04603v5  [q-bio.BM]  31 Jul 2023\nopen-sourced training framework, OpenFold [6], the released code of the mentioned MSA-free\nfolding frameworks does not contain training codes, including the input data preparation pipeline.\nIn addition to this, different implementations they used can produce slightly different results for the\nsame components, making clear comparisons difficult. Furthermore, training and evaluating different\ndatasets makes it difficult to compare methods fairly. Reproducing studies in a unified way and\ncomparing them to other methods provides insights to researchers and allows them to propose new\nmethods which are not easily accessible in protein folding.\nBecause there have been similar difficulties in artificial intelligence fields, it is helpful to study\nhow researchers have overcome them. We bring object detection as a representative example\nbecause a well-established framework achieves generalizability and fair comparison for the field.\nAfter the development of Faster R-CNN[7], various R-CNN-based object detection methods have\nbeen proposed. The individual method verified its contribution as a publication and source code.\nHowever, it was not easy to compare them across studies which is conducted on different codebases.\nFortunately, the generalizability and consistency of experiments have been resolved since frameworks\nlike Detectron2[8] and MMDetection[9] were proposed. Benchmarking was consistently and user-\nfriendly performed within the same framework for various model variants. The framework provided\nunified datasets, evaluation metrics, and module implementations. With all other conditions fixed,\nthe effect of the main contribution was clearly and definitely assessed as the frameworks have been\nused as a de-facto base. In addition, the framework generalized various models as meta-architecture,\nwhich is made up of abstracted components. It reduced the complexity of understanding the object\ndetection pipeline and led to high-quality research, allowing researchers to focus on their ideas, and\nfinally accelerating the field.\nThe protein folding field needs to mature in the same direction as object detection. Like the emergence\nof Faster R-CNN in object detection, AlphaFold2 has emerged in protein structure prediction. Based\non the modules from Alphafold2, single-sequence-based structure prediction models are being actively\nexplored. To achieve acceleration on the protein folding research as Detectron2 or MMDetection,\nwe present Solvent, the protein folding framework that contains major neural networks, which are\nthe main parts of state-of-the-art models. In the Solvent, several methods are implemented with\na unified codebase and represented under meta-architecture. In addition, well-defined datasets are\nprovided for training and validating models. To make Solvent work as a framework, we borrow\nthe pipeline of Detectron2[8], which guarantees the consistency and generalizability on Solvent. In\nthe framework, we represent individual methods using the implementation of OpenFold[6], which\nis the most reliable and well-known Alphafold2 re-implementation project. We abstract the single\nsequence-based folding method into three Embedder, Trunk, and Folding module components. We\ndesign meta-architecture that one model of ESMFold[2], OmegaFold[3], and IgFold[4] can be called\naccording to the specific types of Embedder, Trunk, and Folding module. Also, the specific algorithm\nof each component can be selected and combined interchangeably and user-friendly so that new model\nvariants can be easily defined. Furthermore, a researcher can implement a new type of component and\ncombine it with other built-in components in Solvent. For example, a new proposed protein language\nmodel can be applied as Embedder and experimented with the existing Trunk and Folding module in\nSolvent, allowing researchers efficiency in research. In addition to this, Solvent provides built-in\nsupport for several train and test datasets to benchmark model performance. Single-chain-based\ngeneral protein and multi-chain-based antibody datasets are supported with the appropriate metrics.\nEspecially to maximize the training and inference efficiency, we utilized the power of the recently\nproposed optimization technique, xformers [10]. Also, other optimizations are employed so that the\ntraining speed of Solvent is optimized by about 30 % compared to the original implementations.\nTo confirm and show how Solvent works, we first experimented with reproducing ESMFold to check\nthe reproducibility of the Solvent. We also experimented with combinations of methods for the\ncomponents that comprise the meta-architecture to evaluate which Embedder and which Trunk are\nmore effective. Furthermore, we conduct experiments that provide helpful insights for structure\nprediction studies, such as whether antibody-specific language models can be replaced by general\nprotein language models and how effective Evoformers are. Solvent will be extended to support more\nalgorithms and broader concepts beyond single-sequence protein folding.\n2\nFigure 1: Solvent generalizes model as meta-architecture. The components in meta-architecture is\nabstracted and their specifics determine folding algorithms.\nTable 1: Model abstraction\nMethod\nEmbedder\nTrunk\nFolding\nHeads\nESMFold\nESM\nEvoformer\nAlphafoldStructure\nAlphafoldHeads\nOmegaFold-lite\nOmegaPLM\nGeoformerLite\nAlphafoldStructure\nAlphafoldHeads\nIgFold\nAntiberty\nIgFoldTrunk\nIgFoldStructure\nIgFoldHeads\n2\nSupporting Components\nSolvent is designed to train and evaluate arbitrary models on arbitrary data, with models and data\nmanaged as independent pipelines. This section describes each in more detail.\n2.1\nModels\nSolvent supports several different protein folding models, but each model is abstracted into a meta-\narchitecture. The meta-architecture is composed of the following components.\nEmbedder\nEmbedder takes a sequence as input and outputs a sequence embedding with their pair\nrepresentation computed from pre-trained protein language models (PLM) such as ESM-2. Solvent\nsupports ESM-2[11], OmegaPLM[3], Antiberty[12] as a built-in embedders.\nTrunk\nThe trunk is the main building block of structure prediction. It exchanges information\nbetween sequence embedding and their pair representation computed by the previous component,\nEmbedder, and includes a recycling embedder that utilizes predicted atom positions from the previous\ncycle. Evoformer[1], GeoformerLite, and IgFoldTrunk[4] are built-in supported in the Solvent. Due\nto GPU memory constraints, we provide GeoformerLite, a simplified version of Geoformer[3]. We\nnote that it does not show the full performance of the original Geoformer.\nFolding\nThe folding module takes single representation and pair representation computed\nfrom the previous component, Trunk, and directly predicts the 3D-coordinates of the structure.\nAlphafoldStructure[1], IgfoldStructure[4] is supported in the Solvent.\nHeads\nHeads perform task-specific prediction and loss calculation based on features obtained from\nthe Trunk and Folding module(ex. pLDDT, distogram). Solvent includes all the auxiliary heads used\nin Alphafold2[1] and IgFold[4].\nAll mentioned components are listed in table 1. Researcher creates model variants easily by changing\nspecific method in the component. For example, a new model can be defined by combining off-\nthe-shelf components, such as the ESM-2 Embedder with the GeoformerLite Trunk, making a new\nmodel variant quickly and allowing an accurate comparison between Evoformer and GeoformerLite.\nIn addition to built-in methods, researchers can add new custom methods as components and new\nmodels can be defined with built-in modules.\n3\n2.2\nDatasets\nSolvent supports single-chain based general protein dataaset and multi-chain antibody dataset. The\ndatasets described below.\n2.2.1\nGeneral protein datasets\nPDB\nThe dataset is from Protein Data Bank[13] and we use the data before May, 2020 as same as\nthe ESMFold[2] paper does.\nUniref\nThe dataset is basically from Alphafold predicted dataset(afdb)[1] and we use only the data\ncorresponding to Uniref50. We use the samples with an average pLDDT of 70 or higher.\nCAMEO\nThe dataset is from CAMEO[14] and we use it mainly as evaluation. We use only the\ndata for the 3-months prior to June 25, 2022.\n2.2.2\nAntibody datasets\nSAbDab_20210331\nAntibody dataset based on SAbDab[15] and uses data before March, 2021.\nHeavy-light chain paired samples and Heavy chain only nanobody samples are included in the dataset.\nSAbDab_igfold\nThe selected dataset by IgFold[4] paper for the benchmark is a common set used\nin many papers. We use it mainly as an evaluation.\nExcept for built-in datasets, custom datasets can be added to the Solvent easily and used to train\nmodels with built-in datasets. Any model can be trained and evaluated with well-registered datasets.\n3\nBenchmark\nFirst, we benchmark the reproduction of ESMFold[2] to check the reproducibility of Solvent. Then\nwe experiment with combinations of different types of Embedder and Trunk for experiencing the\nconvenience of the Solvent and benchmarking of simple model variants. Furthermore, we conduct\nadditional experiments to gain insights to help with protein structure prediction.\n3.1\nExperimental settings\nDatasets\nFor general protein, we use pdb and af2_uniref50 for training models. We use\ncameo dataset for evaluation. For antibody-specific models, we use sabdab_20210331 dataset\nand sabdab_igfold dataset for training and evaluating models, respectively.\nTraining Details\nFor general proteins, we followed the training scheme of ESMFold[2]. The crop\nsize of the sequence is fixed at 256 for initial training and 384 for finetuning. Since the models we\nexperiment with have low Trunk depth, we do not apply violation loss even when fine-tuning. The\nbatch size is fixed at 128, as in AlphaFold2, and the batch size per GPU depends on the model size.\nEvaluation Metrics\nTMscore[16] is used for evaluating general proteins. Sequence alignment\nis used as a default option.\nFor antibody models, region-specific RMSD is measured using\nPyRosetta[17].\n3.2\nBenchmark existing models\n3.2.1\nReproducibility of Solvent\nWe define ESMFold through Solvent and benchmark the performance for checking reproducibility.\nRather than reproducing ESMFold full model with 48 Evoformers, we use Trunk-off models. Different\nsize(35M, 150M, 650M) of ESM-2 is used for experiments. As reported in Table 2, some models\nare reproduced with slightly lower TMscore and some with slightly better performance, but they are\nreproduced with comparable level to the paper[2]. We train models until finetuning phase only for\nreproducibility experiments and we train models until initial training for the other experiments.\n4\nTable 2: Comparison ESMFold paper reported TMscore with reproduced ESMFold models by the\nSolvent.\nMethod\nSolvent\nPaper reported\nESM-2(35M)\n0.54\n0.56\nESM-2(150M)\n0.63\n0.65\nESM-2(650M)\n0.71\n0.70\nTable 3: Simple model vairants by combining component from two different methods.\nMethod\nEmbedder\nTrunk\nTMscore\nESMFold\nESM-2(650M)\n2 Evoformer\n0.79\nOmegaFold-lite\nOMEGAPLM(670M)\n2 GeoformerLite\n0.75\nCombinations 1\nOMEGAPLM(670M)\n2 Evoformer\n0.75\nCombinations 2\nESM-2(650M)\n2 GeoformerLite\n0.78\n3.2.2\nCombinations of models\nIn Solvent, it is easy to combine components of different structure prediction models, making perfor-\nmance comparison between methods possible. Using this convenience, we conduct a combination\nstudy of the Embedder and Trunk of two different methods, ESMFold and OmegaFold-lite. Note\nthat the original OmegaFold[3] does not share the IPA weight of the structure module, but we ex-\nperimented with a weight-sharing IPA for solid comparison. As mentioned previous section, we use\nsimplified Geoformer, Geoformer-lite, due to resource constraints. In other words, OmegaFold-lite is\nnot a completely reproduced version of the original OmegaFold. Table 3 shows the performance of\nvarious model variants by permuting components from two different methods. These experiments\ngive researchers a rigorous comparison framework to evaluate which Embedder or Trunk is better.\n3.3\nFurther Study\nThe Solvent provides an easy way to conduct various experiments objectively. This allows researchers\nto make meaningful comparisons between methods. We have conducted some further studies and\nhope the results give insights for structure prediction research.\n3.3.1\nMinimize number of Trunk with trainable language model\nA Trunk module, such as Evoformer, is essential to performance improvement in structure prediction.\nHowever, it is computationally expensive, requiring large GPU memory. Therefore, various engineer-\ning methods[18] have been proposed to achieve efficiency. A trunk contains an information exchange\nprocess between a single representation and a pair representation that contains structural information,\nand it is repeatedly performed through many blocks (e.g., 48 blocks in ESMFold). Meanwhile, most\nalgorithms such as ESMFold, OmegaFold, and IgFold freeze their PLM layers and let only the Trunk\nlearn the structure from the training data. If we unfreeze the parameters of PLMs and let the structure\ninformation be backpropagated into the language model, we can reduce the number of blocks of the\nTrunk. To prove this, we define four different ESMFold variants based on the type of PLMs and the\nnumber of Trunk. In the table 4, comparing variant1 and variant2 shows the effect of a trainable\nPLM. Variant1 and variant3 show the effect of the Evoformer. Trainable PLM results in a 7 percent\nTMscore performance improvement and the use of single Evoformer results in a 10 percent TMscore\nperformance improvement. Adding a single Evoformer onto the trainable PLM model(variant2)\nhas minor performance improvement. We expect the best model as variant4, but a trainable PLM\nwith Evoformer leads to worse performance. All experiments are conducted using ESM-2(35M) as\nEmbedder.\n3.3.2\nUse general protein language model on antibody structure predictions\nVarious large-scale language models exist for general proteins, such as ESM-2(up to 15B) and\nOmegaPLM(670M). However, antibody-specific models are represented by Antiberty(25M), which\nis small compared to general proteins, and the size of the dataset to train them is also small compared\n5\nTable 4: Model variant based on Embedder status and the number Trunk.\nMethod\nEmbedder Status\nTrunk\nTMscore\nvariant1\nFreeze\n0 Evoformer\n0.53\nvariant2\nTrainable\n0 Evoformer\n0.60\nvariant3\nFreeze\n1 Evoformer\n0.63\nvariant4\nTrainable\n1 Evoformer\n0.61\nTable 5: More datasets can be registered and used as testsets.\nMethod\nCASP14\nDe-novo\nOrphan\nvariant1\n0.36\n0.63\n0.49\nvariant2\n0.36\n0.69\n0.51\nvariant3\n0.40\n0.74\n0.51\nvariant4\n0.38\n0.75\n0.52\nTable 6: Antibody model variants based on PLM and their status.\nMethod\nEmbedder\nEmbedder Status\nMeta-arch\nIgFold(reproduced)\nAntiberty(25M)\nFreeze\nIGFold\nIgFold-variant1\nESM-2(35M)\nFreeze\nIGFold\nIgFold-variant2\nESM-2(650M)\nFreeze\nIGFold\nIgFold-variant3\nESM-2(35M)\nTrainable\nIGFold\nTable 7: The performance of various antibody models.\nMethod\nlDDT-C\u03b1\nOCD\nH Fr\nH1\nH2\nH3\nL Fr\nL1\nL2\nL3\nIgFold(paper)\n3.77\n0.45\n0.80\n0.75\n2.99\n0.45\n0.83\n0.51\n1.07\nIgFold(reproduced)\n0.93\n3.74\n0.57\n0.92\n0.80\n3.09\n0.67\n1.12\n0.55\n1.15\nIgFold-variant1\n0.92\n3.76\n0.62\n0.87\n0.94\n3.06\n0.49\n0.90\n0.51\n1.15\nIgFold-variant2\n0.93\n3.77\n0.48\n0.91\n0.94\n3.20\n0.48\n0.94\n0.49\n1.13\nIgFold-variant3\n0.93\n3.88\n0.51\n0.89\n0.85\n3.14\n0.51\n1.00\n0.50\n1.10\nto ESM-2 and OmegaPLM. Currently, the general protein language model is publicly available and\neasy to use. It is worth investigating whether Antiberty, an antibody-specific language model, is\nstill particularly unique for antibody structure prediction compared to general PLM. The details of\nantibody model variants are listed in Table 6. The performance of the listed model is reported in\nTable 7.\nFrom the comparison between IgFold(reproduced) and Igfold-variant1, the performance difference\nbetween using Antiberty and ESM-2(35M) is not very noticeable. The Antiberty model performs\nbetter on some CDRs but does not significantly. In fact, using a large general protein language\nmodel(IgFold-variant2) model seems more effective than using an antibody-specific language model.\nUsing general language model with trainable parameters(IgFold-variant3) does not show performance\nimprovement on most CDRs.\n3.4\nCustom datasets can be added and evaluated on different models\nIn Solvent, any datasets can be registered and used for training and evaluating models. As an example,\nwe register CASP14 datasets, de-novo proteins, and orphan proteins. In the case of CASP14, we\nused 33 publically released samples. T1044 is not included due to memory constraints. In the case\nof de-novo and orphan proteins, we referred to the target lists provided at RGN2[19] repository and\nused samples that were released after May 2020. These samples might be used when training ESM-2,\nwhich causes high performance for de novo proteins. All the samples are listed in the appendix. The\nmodel variants listed in Table 4 can be evaluated on these three different datasets(Table 5).\n6\n4\nConclusion\nTo support a consistent and easy-to-use research framework, we propose Solvent for protein folding\nresearch. We hope that efficient and rigorous experiments on top of the Solvent will further prove\nthe strengths and weaknesses of each algorithm and finally accelerate structural prediction research.\nCurrently, Solvent focuses on MSA-free protein structure prediction models. We will extend Solvent\nto a more general way that takes MSA and template as input and support more validation data such\nas orphan and de-novo protein.\nAcknowledgments\nWe acknowledge the contributions of the Language Model Engineering Team at Kakao Brain, who\nhave optimized Solvent. These optimizations make Solvent efficient in training speed and memory, so\nresearchers can easily tap larger models. Their support has been essential in achieving the outcomes\npresented in this work.\nReferences\n[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex\nBridgland, Clemens Meyer, Simon A A Kohl, Andrew J Ballard, Andrew Cowie, Bernardino\nRomera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,\nDavid Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas\nBerghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray\nKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic-\ntion with AlphaFold. Nature, 596(7873):583\u2013589, 2021.\n[2] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,\nRobert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi,\nTom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic\nlevel protein structure with a language model. bioRxiv, 2022.\n[3] Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan\nWu, Qi Xie, Bonnie Berger, Jianzhu Ma, and Jian Peng. High-resolution de novo structure\nprediction from primary sequence. bioRxiv, 2022.\n[4] Jeffrey A Ruffolo, Lee-Shin Chu, Sai Pooja Mahajan, and Jeffrey J Gray. Fast, accurate\nantibody structure prediction from deep learning on massive set of natural antibodies. Nature\ncommunications, 14(1):2389, 2023.\n[5] Jiaxiang Wu, Fandi Wu, Biaobin Jiang, Wei Liu, and Peilin Zhao. tfold-ab: Fast and accurate\nantibody structure prediction without sequence homologs. bioRxiv, 2022.\n[6] Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J\nO\u2019Donnell, Daniel Berenberg, Ian Fisk, Niccol\u00f2 Zanichelli, Bo Zhang, et al. Openfold: Retrain-\ning alphafold2 yields new insights into its learning mechanisms and capacity for generalization.\nbioRxiv, pages 2022\u201311, 2022.\n[7] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran\nAssociates, Inc., 2015.\n[8] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.\nhttps://github.com/facebookresearch/detectron2, 2019.\n[9] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun,\nWansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng\nCheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping\nShi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection\ntoolbox and benchmark. CoRR, abs/1906.07155, 2019.\n7\n[10] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\nxformers: A modular and hackable transformer modelling library. https://github.com/\nfacebookresearch/xformers, 2022.\n[11] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,\nAllan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language\nmodels of protein sequences at the scale of evolution enable accurate structure prediction.\nbioRxiv, 2022.\n[12] Jeffrey A Ruffolo, Jeffrey J Gray, and Jeremias Sulam. Deciphering antibody affinity maturation\nwith language models and weakly supervised learning. arXiv, 2021.\n[13] Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig,\nIlya N. Shindyalov, and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research,\n28(1):235\u2013242, 01 2000.\n[14] J\"urgen Haas, Alessandro Barbato, Dario Behringer, Gabriel Studer, Steven Roth, Martino\nBertoni, Khaled Mostaguir, Rafal Gumienny, and Torsten Schwede. Continuous automated\nmodel evaluation (cameo) complementing the critical assessment of structure prediction in\ncasp12. Proteins, 86(S1):387\u2013398, 2018.\n[15] James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges,\nJiye Shi, and Charlotte M. Deane. SAbDab: the structural antibody database. Nucleic Acids\nResearch, 42(D1):D1140\u2013D1146, 11 2013.\n[16] Y. Zhang and J. Skolnick. Scoring function for automated assessment of protein structure\ntemplate quality. Proteins, 57(4):702\u2013710, 2004.\n[17] Sidhartha Chaudhury, Sergey Lyskov, and Jeffrey J. Gray. Pyrosetta: a script-based interface for\nimplementing molecular modeling algorithms using rosetta. Bioinformatics, 26(5):689\u2013691,\n2010.\n[18] Shenggan Cheng, Ruidong Wu, Zhongming Yu, Binrui Li, Xiwen Zhang, Jian Peng, and Yang\nYou. Fastfold: Reducing alphafold training time from 11 days to 67 hours, 2022.\n[19] Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Charlotte Rochereau, George M. Church,\nPeter K. Sorger, and Mohammed AlQuraishi. Single-sequence protein structure prediction using\nlanguage models from deep learning. bioRxiv, 2021.\nA\nAppendix\nA.1\nList of samples used in Table 5\nA.1.1\nCASP14\nWe used 33 publicaly released samples. T1044 is not included. T1024, T1025, T1026, T1027, T1029,\nT1030, T1031, T1032, T1033, T1035, T1036s1, T1037, T1038, T1039, T1040, T1041, T1042, T1043,\nT1046s1, T1046s2, T1049, T1050, T1054, T1056, T1064, T1067, T1073, T1074, T1079, T1080,\nT1082, T1090, T1099.\nA.1.2\nDe novo proteins\nFrom the RGN2 repository target lists, we used 111 samples released after May, 2020. 7BQS_A,\n6X1K_A, 6W6X_A, 7A4Y_E, 7BQR_A, 7RX5_A, 7BQC_A, 7BQQ_A, 7DNS_A, 7BQB_A, 6YPI_A,\n6Z35_A, 7BQE_A, 6ZOF_D, 7CZ0_E, 7BPN_A, 7BQD_A, 7CX4_N, 7BPM_A, 6WXO_B, 6XT4_A,\n6XEH_A, 7BPL_A, 7BQN_A, 6XSS_A, 6WMK_A, 7AWY_A, 7AVA_A, 7A4D_E, 7BO9_C, 7DI0_A,\n6YB2_A, 7BO8_B, 6YB1_A, 6VG7_A, 7BIM_A, 7BEY_A, 7AWZ_A, 6YB0_A, 7A8S_A, 7BQM_A,\n6RLH_A, 6WXP_A, 6W9Y_A, 6UIB_C, 6RLI_A, 6U1S_A, 7RGR_B, 6WVS_A, 7BPP_A, 6W9Z_D,\n7ARR_A, 6ZL1_C, 7AX0_A, 7ARS_A, 6XI6_A, 6ZT1_A, 6WRV_D, 7AX2_A, 6Y7N_A, 6VFL_B,\n6WRW_C, 6VGA_A, 6VFK_B, 6Z3X_B, 7BNT_B, 7CUV_A, 7N8J_B, 6VFJ_B, 7KBQ_A, 7CD8_A,\n8\n6YAZ_E, 6VFI_B, 6X8N_A, 6VGB_A, 6VFH_A, 6W70_A, 7BAU_E, 6ZIE_A, 6Z0L_A, 6REO_A,\n6ZV9_B, 6W90_A, 7BAT_A, 7JH6_A, 6Z0M_A, 7A50_B, 7JH5_A, 7BAW_A, 6YWC_C, 6WA0_B,\n7DDR_A, 6Z2I_A, 7BWW_A, 7A48_B, 7BAV_C, 6WRX_C, 6V67_B, 6X9Z_A, 6YWD_C, 6XR1_A,\n6XNS_B, 6VL5_D, 6VEH_A, 7A1T_C, 6XR2_E, 6VL6_A, 7BAS_B, 7CBC_A, 6XH5_B, 7BOA_C\nA.1.3\nOrphan proteins\nFrom the RGN2 repository target lists, we used 6 samples released after May, 2020. 6M64_F,\n6YNS_O, 7AL0_A, 7DGU_A, 7KEI_C, 7OSC_A.\n9\n"
  },
  {
    "title": "SVIT: Scaling up Visual Instruction Tuning",
    "link": "https://arxiv.org/pdf/2307.04087.pdf",
    "upvote": "5",
    "text": "SVIT: Scaling up Visual Instruction Tuning\nBo Zhao * 1 Boya Wu * 1 Muyang He * 1 2 Tiejun Huang 1 2\nAbstract\nThanks to the emerging of foundation models, the\nlarge language and vision models are integrated to\nacquire the multimodal ability of visual caption-\ning, question answering, etc. Although existing\nmultimodal models present impressive perfor-\nmance of visual understanding and reasoning,\ntheir limits are still largely under-explored due\nto the scarcity of high-quality instruction tuning\ndata. To push the limits of multimodal capability,\nwe Scale up Visual Instruction Tuning (SVIT)\nby constructing a dataset of 4.2 million visual\ninstruction tuning data including 1.6M conversa-\ntion question-answer (QA) pairs, 1.6M complex\nreasoning QA pairs, 1.0M referring QA pairs\nand 106K detailed image descriptions. Besides\nthe volume, the proposed dataset is also featured\nby the high quality and rich diversity, which is\ngenerated by prompting GPT-4 with the abundant\nmanual annotations of images. We also propose\na new data recipe to select subset with better\ndiversity and balance, which evokes model\u2019s\nsuperior capabilities.\nExtensive experiments\nverify that SVIT-v1.5, trained on the proposed\ndataset, outperforms state-of-the-art Multimodal\nLarge Language Models on popular benchmarks.\nThe data and code are publicly available at\nhttps://github.com/BAAI-DCAI/\nVisual-Instruction-Tuning.\n1. Introduction\nThe great success of large language models (LLMs), e.g.\nBERT (Devlin et al., 2019), T5 (Raffel et al., 2020), GPT-2\n(Radford et al., 2019), GPT-3 (Brown et al., 2020), have\nmotivated the advancement of vision (Dosovitskiy et al.,\n2021; Liu et al., 2021; He et al., 2022) and multimodal-\nity (Radford et al., 2021; Alayrac et al., 2022; Zhu et al.,\n2023a; Liu et al., 2023b) in terms of architecture design\n*Equal contribution 1Beijing Academy of Artificial Intelligence,\nBeijing, China 2Peking University, Beijing, China. Correspon-\ndence to: Bo Zhao <zhaobo@baai.ac.cn>.\nPreliminary work. Under review.\nand learning paradigm. Recently, GPT-4 (OpenAI, 2023)\ndemonstrates impressive multimodal understanding and rea-\nsoning abilities, accepting image and text inputs. Inspired\nby GPT-4, Multimodal Large Language Models (MLLMs)\nbridging language and vision models have achieved remark-\nable progress in multiple visual understanding and reason-\ning tasks, e.g. visual captioning (Li et al., 2023c), dialogue\n(Alayrac et al., 2022) and question answering (Zhu et al.,\n2023a; Liu et al., 2023b).\nTypically, the multimodal models are pre-trained on large\nmultimodal datasets, e.g. LAION-2B (Schuhmann et al.,\n2022), CC-12M (Changpinyo et al., 2021), YFCC-100M\n(Thomee et al., 2016) and MMC4 (Zhu et al., 2023b), that\ncontain millions to billions roughly-aligned image-text pairs\nfrom the web. Then, precise vision-language data pairs are\nused to finetune the models. Like the success of language\ninstruction tuning, visual instruction tuning has become the\nkey to the multimodal performance. However, due to the\nhigh construction cost, existing visual instruction datasets\nare still in small scale and less informative. Several works\nconvert the image captioning and VQA datasets (Lin et al.,\n2014; Antol et al., 2015; Hudson & Manning, 2019; Goyal\net al., 2017) into instruction tuning data by manually adding\na few instructions (Dai et al., 2023). However, these cap-\ntions and questions/answers are usually short and focus on\nvisual perception and simple questions, which may lead to\nineffective model training (Gong et al., 2023). To generate\nmore informative visual instruction data, GPTs are intro-\nduced. LLaVA (Liu et al., 2023b) contributes a large visual\ninstruction dataset containing 158K data by prompting GPT-\n4 with five captions and a few object bounding boxes asso-\nciated with images from COCO dataset (Lin et al., 2014).\nMeanwhile, MiniGPT-4 (Zhu et al., 2023a) creates 3,500\nimage-text pairs by refining model\u2019s output using ChatGPT.\nThe language-only GPT models have difficulty in precisely\nimagining the whole picture from the limited input. Thus,\nthe generated instruction tuning data lacks diversity and\ncomplexity.\nTo push the limits of large multimodal models, we Scale\nup Visual Instruction Tuning (SVIT) and propose a large-\nscale dataset with 4.2 million informative instruction tuning\ndata, including 1.6M conversation QA pairs, 1.6M complex\nreasoning QA pairs, 1.0M referring QA pairs and 106K\ndetailed descriptions. Table 1 shows that SVIT is 20\u00d7\n1\narXiv:2307.04087v3  [cs.CV]  28 Dec 2023\nSVIT: Scaling up Visual Instruction Tuning\nPre-trained LLM\nPre-trained vision encoder\nLoRA\nVision-language connector\nCaptioning\nQ: Can you describe this image?\nA: The image depicts a lively scene in a\ncity street. A man is standing on a\nplatform, holding a large knife and a\nspatula. He seems to be the center of\nattention as a crowd of people watch\nhim. There are also several buildings,\nincluding a tall clock tower, in the\nbackground.\nUpdated\nFrozen\nPerception\nQ: Can you tell me more details about\nthe man?\nA: The man is shirtless and has a tattoo\non his chest. He's wearing black shorts\nand black shoes. He's holding a large\nknife in one hand and a spatula in the\nother.\nReasoning\nQ: Which country does this scene\nprobably take place?\nA: The scene appears to be taking\nplace in Scotland.\nQ: Which city does this scene probably\ntake place?\nA: The scene likely takes place in\nEdinburgh, Scotland.\nQ:\nIn\nEdinburgh,\nwhich\nevent\nis\nprobably happening in this scene?\nA: Given the presence of a large crowd\nand a man on a platform holding a\nknife and a spatula, it's likely that this\nscene is part of the Edinburgh Festival\nFringe, a major arts festival\nheld\nannually in Edinburgh, Scotland.\nReferring\nQ: Provide the bounding box coordinate\nof a woman in a red dress.\nA: The bounding box for a woman in a red\ndress is [0.21, 0.71, 0.3, 0.996].\nFigure 1. SVIT-v1.5 (LoRA) model architecture and abilities.\nlarger than LLaVA dataset. To enrich the diversity and in-\nformativeness of instruction tuning data, we construct SVIT\nbased on Visual Genome (Krishna et al., 2017) which has\nabundant manual annotations and GPT-4 which has the best\nmultimodal capability. We prompt the language-only GPT-4\nChatBot with image-level descriptions, detailed region de-\nscriptions and object bounding boxes. We further study the\ndata efficiency and propose a new data recipe that outputs\nsubset with better diversity and balance. Then, a more pow-\nerful model, SVIT-v1.5, is trained on the proposed dataset,\nas illustrated in Figure 1. Extensive experiments verify\nthat our model reveals impressive ability in visual percep-\ntion and reasoning, and achieves noticeable performance\nimprovements over the state of the art.\nWe summarize the main contributions of this paper:\n1. We present 4.2M high-quality instruction data of 1.6M\nconversation QA pairs, 1.6M complex reasoning QA\npairs, 1.0M referring QA pairs and 106K detailed im-\nage descriptions.\n2. We propose a new data recipe that selects an informa-\ntive subset of diverse and balanced training data to\nbetter match the downstream tasks.\n3. We scale up visual instruction tuning and contribute a\nbetter model \u2013 SVIT-v1.5 that outperforms state-of-the-\nart MLLMs including LLaVA-v1.5, Qwen-VL-Chat\nand InstructBLIP on popular benchmarks.\n2. Related Work\n2.1. Multimodal Models\nExisting multimodal solutions can be roughly split into two\ncategories: 1) multimodal systems, e.g. Visual ChatGPT\n(Wu et al., 2023), X-Decoder (Zou et al., 2023) and In-\nternGPT (Liu et al., 2023d), in which multiple language and\nvision models are coordinated by a LLM manager/controller\nto deal with different tasks, 2) end-to-end differentiable\nmultimodal models, e.g. Flamingo (Alayrac et al., 2022),\nBLIP-2 (Li et al., 2023c), Kosmos (Huang et al., 2023; Peng\net al., 2023), MiniGPT-4 (Zhu et al., 2023a), LLaVA (Liu\net al., 2023b), InstructBLIP (Dai et al., 2023) which input\nboth vision and language tokens into LLM. In this paper, we\nfocus on the end-to-end differentiable multimodal models,\nwhich are lightweight and concise for research.\nThe end-to-end multimodal models contain pre-trained vi-\nsion and language models and a learnable module to fuse\nboth. Flamingo (Alayrac et al., 2022) learns gated cross-\nattention layers to condition the frozen LLM on visual to-\nkens, demonstrating excellent in-context few-shot learning\nperformance. Li et al. (2023c) design Q-Former to bridge\nthe image encoder and LLM in a two-stage training strategy,\nwhich shows emerging capability of zero-shot instructed\nimage-to-text generation. By leveraging advanced LLMs,\ni.e. LLaMA (Touvron et al., 2023) and Vicuna (Chiang\net al., 2023), multimodal models LLaVA (Liu et al., 2023b)\nand MiniGPT-4 (Zhu et al., 2023a) are built by transforming\n2\nSVIT: Scaling up Visual Instruction Tuning\nTable 1. Comparing SVIT to similar vision-language instruction datasets generated by GPT. \u2217LLaVAR collects 422K noisy instruction-\nfollowing data using OCR results and 16K high-quality data using GPT-4.\nDataset\n#Image\n#Object\n#Region\n#Image\n#Instruction\n#Response\nGPT\nBBox\nDescription\nCaption\nQuestion\nAnswer\nMiniGPT-4\n3.5K\n-\n-\n-\n4\n3.5K\nGPT-3.5\nLLaVAR\u2217\n16K\n-\n-\n-\n16K\n16K\nGPT-4\nLLaVA\n81.5K\n600K\n-\n404.7K\n158K\n158K\nGPT-4\nSVIT\n108.1K\n3.8M\n5.4M\n257.6K\n4.2M\n4.2M\nGPT-4\nvisual tokens to language tokens with only one linear layer,\nwhile InstructBLIP (Dai et al., 2023) learns a Q-Former to\nbridge vison and language models.\n2.2. Multimodal Instruction Tuning\nThe success of multimodal models, e.g. LLaVA, MiniGPT-\n4 and InstructBLIP, relies on the high-quality image-text\ndata for finetuning models, which is named visual instruc-\ntion tuning in Liu et al. (2023b). Previous work (Gong\net al., 2023) finds that simply constructing training set based\non existing VQA datasets (Goyal et al., 2017; Hudson &\nManning, 2019) with short answers will degrade the model\nperformance. To boost the performance, Zhu et al. (2023a)\ncollect 3,500 high-quality image-text pairs by refining their\nmodel\u2019s outputs using ChatGPT. More natural and reliable\nresponses are produced by finetuning the model on the re-\nfined data. Liu et al. (2023b) for the first time systematically\nconstruct a large visual instruction tuning dataset \u2013 LLaVA-\nInstruct-150K. They prompt GPT-4 to generate questions\nand answers by feeding it image-level captions and object\nbounding boxes of each image from COCO dataset (Lin\net al., 2014). To better understand text-rich images, Zhang\net al. (2023c) present LLaVAR that collects 422K noisy\ninstruction-following data using OCR results and 16K high-\nquality data using GPT-4. Dai et al. (2023) collect 26 public\ndatasets including LLaVA-Instruct-150K to construct vi-\nsual instruction tuning data. However, most of these public\ndatasets contain short questions and answers that focus on\nvisual perception. Li et al. (2023d) build M3IT by convert-\ning 40 datasets into a unified vision-to-text schema. They\nutilize ChatGPT to paraphrase the short answers in original\nVQA datasets. Beyond above works, we prompt the pow-\nerful GPT-4 with rich annotations of image-level captions,\nregion-level descriptions and object bounding boxes that\nare from Visual Genome (Krishna et al., 2017) and COCO\ndataset (Lin et al., 2014). The generated 4.2M visual instruc-\ntion data cover diverse tasks of visual perception, reasoning\nand planing.\nThere are also some works that contribute multimodal in-\nstruction data of videos (Li et al., 2023b), RGB-D im-\nages (Li et al., 2023b), speech (Zhang et al., 2023a), audio\n(Zhang et al., 2023b), etc. For instance, EgoCOT (Mu et al.,\n2023) prompts ChatGPT with video captions to generate\ninstructions and responses of detailed embodied planning.\nMIMIC-IT (Li et al., 2023b) collects visual data from multi-\nple datasets, and prompts ChatGPT to generate instruction-\nresponse pairs. Most of its data are constructed based on the\negocentric videos from E4D dataset (Grauman et al., 2022).\n3. Dataset Construction\n3.1. Source Data\nWe build SVIT based on Visual Genome (Krishna et al.,\n2017) dataset that comprises 108,077 images with dense an-\nnotations within each image, including region descriptions,\nobjects, attributes, relationships, etc. Since Visual Genome\nis partially sourced from COCO dataset (Lin et al., 2014),\nwe also collect captions for images from COCO dataset.\nGenerally, each image in COCO dataset has 5 captions, fo-\ncusing on the high-level appearance. As an image usually\ncontains rich objects and regions that cannot be completely\ndescribed in a general caption, Visual Genome serves as a\nvaluable source, offering abundant annotations of the visual\ndetails. On average, Visual Genome provides 42 human-\ngenerated region descriptions and 21 objects per image, with\neach region and object located by a bounding box. Lever-\naging these annotations, we are able to gather thorough\nand detailed descriptions for all images, which are made\nup of three key components: (1) the 257,633 captions from\nCOCO dataset; (2) the 3,802,374 object names and their\ncorresponding bounding boxes from Visual Genome; (3)\nthe 5,406,592 region descriptions and their corresponding\nbounding boxes from Visual Genome.\n3.2. Instruction Data Generation\nInspired by LLaVA (Liu et al., 2023b), we design four tasks\nand prompt the language-only GPT-4 ChatBot to generate\nthe questions and answers accordingly. The prompts are\nsummarized in Figure 7 and Figure 8 in the Appendix. Since\nGPT-4 demonstrates excellent performance even with zero-\nshot learning, we do not provide any examples for GPT-4\nin order to encourage the innovation and diversity of the\ngenerated contents.\n3\nSVIT: Scaling up Visual Instruction Tuning\n \nInput \n(1) captions:  \nStreet workers in orange jackets outside working with a traffic signal. \nThe workers are trying to install a broken signal. \n\u2026 \n(2) objects:  \nman: [0.588, 0.387, 0.692, 0.832] \ntraffic light: [0.16, 0.341, 0.34, 0.571] \n\u2026 \n(3) regions:  \nfour men installing a traffic light: [0.032, 0.197, 0.992, 0.952] \nDirt on the street: [0.13, 0.739, 0.63, 0.856] \nA traffic light leaning on the truck: [0.146, 0.355, 0.984, 0.808] \n\u2026 \nOutput \nConversation: \nTopic: Traffic Light Installation \nUser: How many workers are present in the image? \nGPT: There are four workers actively involved in the installation of the traffic light in the image. \nUser: Can you describe the clothing of the workers? \nGPT: The workers are wearing orange jackets, or vests with shirts underneath. The jackets and vests are safety attire, which is \ntypical for street workers to make them more visible to motorists. \nUser: Are there any safety measures visible in the image? \nGPT: Yes, as a safety measure, there are orange cones with white stripes placed on the street near the working area to alert \nmotorists and pedestrians of the ongoing construction. \n\u2026 \nComplex reasoning: \nQuestion: What precautions are the workers taking to ensure their safety? \nAnswer: The workers are wearing orange jackets, which increase their visibility. Additionally, they've placed orange cones \naround the area where they are working. \nQuestion: What could be a recommendation to improve safety at this work site? \nAnswer: Adding more clear signage to alert oncoming traffic of the construction work ahead could be one recommendation \nto improve safety. \n\u2026 \nReferring QAs: \nQuestion: Why might the <st>car<ed> [0.386, 0.435, 0.996, 0.715] be stopped? \nAnswer: The car might be stopped due to the ongoing construction and the presence of workers on the street fixing the traffic \nlight. \nQuestion: What suggestion would you give to improve safety for the workers? \nAnswer: It would be good to place more <st>cones<ed> [0.354, 0.573, 0.774, 0.875] around the working area and possibly use \nbarriers to ensure vehicles are aware and kept at a distance from the ongoing construction. \n\u2026 \nDetail description: \nIn the image, I see a scene where street workers in orange jackets are outside, working with a traffic signal. \u2026 There is a man \nwearing an orange shirt, and he appears to be the main focus of the image. He is accompanied by several other men who are \nalso wearing orange shirts. They are engaged in various activities, such as fixing a street light in the middle of the street. The \nworkers are standing on a street with dirt and orange cones scattered around. In the background, there is a white car and \nsome windows can be seen on it. \u2026 Overall, this image captures a group of workers actively engaged in road construction and \ntraffic signal maintenance. \n \nFigure 2. The example input to GPT-4 and the responses for three tasks. Note that the image is only shown here for reference and not\nprovided to GPT-4. The colored phrases in referring QAs correspond with bounding boxes of that color in the image.\n4\nSVIT: Scaling up Visual Instruction Tuning\n\u2022 Conversation. We prompt GPT-4 to design 3 conver-\nsations between a person and GPT-4 talking about the\nimage. Each conversation should include 5 question\nand answer pairs (QAs). The content of the conversa-\ntion should be logically connected. GPT-4 thinks about\nthe topic first and then generates the conversation ac-\ncording to the topic. The topics can be about the visual\nperception, reasoning, event planning, etc.\n\u2022 Complex reasoning. 15 complex reasoning QAs about\neach image are generated using GPT-4. The questions\ncan be asking why things happen that way, suggestions\nto the people in the image, etc. When providing the\nanswer to a complex question, we prompt GPT-4 to\nthink step by step and include reasoning details in the\nanswer.\n\u2022 Referring QAs. We prompt GPT-4 to create 10 ques-\ntion and answer pairs of specific regions in the image.\nWhen referring to any object in the question or answer,\nalways wrap it with prefix \u201c<st>\u201d, suffix \u201c<ed>\u201d and\nattach its normalized bounding box after it, in the for-\nmat of \u201c<st>object<ed> [x1, y1, x2, y2]\u201d. If multi-\nple objects are referred to, attach all the corresponding\nbounding boxes after them, e.g., \u201c<st>objects<ed>\n[x1, y1, x2, y2], [x1, y1, x2, y2]\u201d.\n\u2022 Detail description. We use GPT-4 to describe the\nimage in detail. The description may include appear-\nances, actions, the number of objects, object positions,\nbackground details, etc.\nFigure 2 illustrates an example input and the GPT-4 out-\nput for each task. For rich diversity, we further randomly\nsample an instruction for detail description task, e.g., \u201ccan\nyou describe the image in detail\u201d. The complete list of\nthe alternative instructions can be found in Figure 9 in the\nAppendix.\n3.3. Postprocessing\nWhile most of the GPT-4 generated question-answer pairs\nare of high quality, some answers occasionally contain un-\nneeded contents. For example, some answers may tell that\nthe information is based on the given \u201ccaptions\u201d and \u201cde-\nscriptions\u201d. To remove the unneeded content, we find them\nbased on relative words and use GPT-4 to regenerate the\nresponses. In addition, the number of generated conversa-\ntions or QA pairs may be fewer than the requirement. We\nalso remove them and generate new responses. We use the\nsame procedure to filter the regenerated content until it is\nsatisfying.\n3.4. Statistics and Analysis\nStatistics.\nEmploying the two-pass procedure, we obtain\nan extensive collection of data, including 1,565,797 conver-\nsation QAs, 1,556,902 complex reasoning QAs, 1,011,338\nreferring QAs and 106,274 detailed image descriptions. The\naveraging question and answer lengths are 9.6 and 27.9\nwords in conversation subset, 12.6 and 26.6 words in com-\nplex reasoning subset and 11.3 and 20.6 words in referring\nQAs subset, respectively. In contrast, the mean length is\n5.7 words per question and 1.8 words per answer in the\noriginal Visual Genome. The detailed descriptions in our\ndataset have 361.5 words on average, while the length of\nCOCO dataset image captions is 11.3. Therefore, the corpus\nprovided by our SVIT is of higher quality.\nDistribution.\nWe analyze the distribution of question\ntypes in conversation, complex reasoning and referring QAs\ntasks by visualizing the distribution of first three words in\nFigure 3. We can see that \u201cwhat\u201d questions are the largest\ncategory, in conversation (38%), complex reasoning (55%)\nand referring QAs (41%). In the case of conversation, ques-\ntion types are diverse, including simple yes-no questions,\nquestions on object details, conditions and functions, etc.\nRegarding complex reasoning, since we explicitly prompt\nGPT-4 to generate questions that need complex reasoning\nto answer, we collect a larger proportion of complex ques-\ntions that commence with \u201cwhy\u201d (9%) and \u201chow\u201d (11%).\nFurthermore, most questions starting with \u201chow\u201d are sim-\nple object counting questions, i.e. \u201chow many\u201d, in existing\nvisual question answering datasets such as Visual Genome\n(Krishna et al., 2017) and VQA (Goyal et al., 2017), while\nin SVIT, only 11% of questions starting with \u201chow\u201d are the\n\u201chow many\u201d questions. For referring QAs, there are various\ntypes of questions, including those about object positions\nthat start with \u201cwhere\u201d, about object existence that start\nwith \u201cis/are there any\u201d, about suggestions and planning that\nstart with \u201cwhat suggestion\u201d and about reasoning that start\nwith \u201cwhy\u201d. To better distinguish objects in the same image,\nthere is also a notable proportion of questions that starts\nwith \u201cwhich\u201d.\nCorrectness.\nTo assess the correctness of the generated\ncontent, we conduct a manual examination on randomly\nselected 20 images and corresponding data. In general,\naround 5% of the questions in the dataset can be provided\nwith a more accurate or satisfying answer. The identified\nproblems can be categorized into three types.\n\u2022 Errors in original annotations. We construct the visual\ninstruction data based on the manual annotations from\nVisual Genome and COCO dataset, which may contain\nerrors in their original annotations. For example, in\nthe image depicted in Figure 4(a), one caption from\n5\nSVIT: Scaling up Visual Instruction Tuning\nwhat\ncan\nis\nare\nhow\nwhere\ncould\ndoes\nand\ndo hi\nwho\ngpt\nif\nis\n's\ncan\nare\nabout\nkind\ncolor\ndoes\ntype\nelse\nother\nobjects\ncolors\ntypes\ndo\ncould\nmight\ntime\ndetails\nyou\nthere\nthe\nthere\nthe\nthey\nall\nmany\nis\nwould\nabout\ndoes\nare\ncan\ndo's\nis\nare\nyou\nthe\nwhat\nwhere\nthe\nyou\nthe\ngpt\n,\nme\ni\nthe\nhappening\non\nshe\nthe\nhappening\non\nunique\nin\nyou\nthe\nsome\nthey\nthe\nof\nis\nare\nthe\nof\ncan\nis\nobjects\nare\ncan\nare\nof\nthe\nyou\nbe be\nof\ncan\nto\ntell\ndescribe\nidentify\nsee\nprovide\ngive\nany\nanything\na\nanyone\nman\nany\nother\nthe\npeople\nthe\nyou\nthe\nthe\nthe\nyou\nthe\nthe\nthis\nthe\nthese\ndescribe\ntell\nabout\nsee\n,\nmore\nwhat\nhow\nwhy\nis\ncan\nbased\nwhere\nare\ngiven\nfrom\nconsidering\nif\ndoes\njudging\ncould\nis\ncould\ncan\nmight\nkind\ndoes\n's\ntype\nare\ncolor\nseems\ndo\nprecautions\nother\ntime\nsuggestion\nsuggests\nsafety\nunique\npurpose\nsuggestions\nwould\ndetails\nindicates\nadditional\nmaterial\npart\nobjects\nactivity\nfeature\ncolors\nadvice\ncan\ndoes\nis\nwould\nmany\ncould\nmight\nare\ndo\nmight\nis\nare\ndo\nwould\ndoes\ncould\nthere\nthe\nyou\nwe\non\nis\nare\ncan\nmight\ndoes\nthere\nthe\nthe\nthat\nthe\nthe\nthe\nby\nyou\nthe\nunique\na\nunusual\none\nbe\nthe\nbe\nyou\nwe\nbe\nthe\nsuggest\nof\nthe\nthe\nunique\nunusual\nof\nthe\nsome\nis\nare\nto\nthe\nyou\nshould\nof\nwould\nthat\ndoes\nyou\ncan\nthat\nof\nwould\nyou\nwe\nthe\none\nthe\nthe\nyou\npeople\nthe\nthe\nthe\nthe\nthe\nthere\nthere\nthe\nthere\nthe\nyou\nthe\nthere\nthe\nany\nanything\na\nman\ndescribe\ntell\nidentify\nsuggest\ninfer\nguess\ninfer\nthe\nthe\nthe\nany\npresence\nimage\nthe\nwhat\nwhich\nhow\nwhere\nwhy\nis\nare\nif\nbased\ncan\nwho\ndescribe\nsuggest\nconsidering\ngiven\nfrom\nin\nbetween\nis\ncan\ncolor\nsuggestion\nmight\nkind\ncould\ntype\n's\nseems\nare\nobject\nwould\nobjects\nadvice\ndoes\nsuggestions\npart\nfeature\nsurrounds\nobject\npart\nis\nitem\n<\nobjects\nperson\ntwo\nvehicle\nmany\ncan\nis\nwould\ndoes\nare\nis\ncan\nare\nmight\ndo\nthere\nthe\nthere\nthe\nsomeone\nthe\nyou\na\non\nyou\nis\nseems\nthe\na\nthe\nthe\nthe\nthe\nthe\nlocated\nunique\non\ndirectly\npositioned\nplaced\na\nbe\nyou\nis\nare\nwould\ncan\nbe\nthe\nof\nbe\nof\nthe\nto\nthe\nis\nyou\nare\nwould\nthe\nof\nthe\nis\nseems\ncan\nof\ncloser\nis\nst\nare\nyou\none\nthe\nwe\nthe\nyou\nthe\nthe\nthe\nyou\none\nthe\nthe\nthe\nthere\nyou\nany\na\nanything\n<\nany\n<\nwanted\nwants\nthe\ntheir\ndescribe\nidentify\nto\nFigure 3. The distribution of question types in conversations (left), complex reasoning (middle), referring QAs (right) by the first three\nwords. The angle of each sector represents the proportion of each category.\n(a) Wrong caption in COCO dataset: \u201cThree men and one older\nwoman stand near a man who is looking in the mirror with the\ncollar of his white shirt up.\u201d\n(b) Wrong object name in Visual Genome: \u201cteddy bear\u201d.\n(c) The answer discusses how the condition of the boat\u2019s paint\nwould reflect the maintenance instead of answering it directly.\n(d) The generated answer misunderstands the position of the tele-\nphone.\nFigure 4. Problematic examples in generated answers.\n6\nSVIT: Scaling up Visual Instruction Tuning\nCOCO dataset incorrectly states, \u201cThree men and one\nolder woman stand near a man who is looking in the\nmirror with the collar of his white shirt up.\u201d Actually,\nthere are only two men and one woman standing near\nthe man looking at the mirror. Similarly, in Figure 4(b),\nthe object is labeled as a \u201clittle bunny\u201d in the region\ndescription, but wrongly referred to a \u201cteddy bear\u201d in\nthe object name in Visual Genome\u2019s annotation.\n\u2022 Correct but not precisely answer the question. As il-\nlustrated in Figure 4(c), when being asked, \u201cWhat can\nbe inferred about the maintenance of the boat from\nthe condition of the paint?\u201d, the answer states, \u201cThe\ncondition of the boat\u2019s paint could reflect the level of\nmaintenance, if it\u2019s faded or peeling, it may suggest the\nboat hasn\u2019t been maintained well, whereas bright and\nfresh paint may indicate regular upkeep.\u201d Although\nthe answer is correct, it fails to address the question\nprecisely.\n\u2022 Incorrect answers. In Figure 4(d), the generated image\ndescription mentions, \u201cNearby, there\u2019s a round cen-\nter table cluttered with assorted magazines and books,\ncreating a lived-in feel. The table also hosts a yellow\nrotary telephone, a vintage relic of bygone days.\u201d In\nreality, there are two tables in the image and the tele-\nphone is placed on a different table in the bottom left\ncorner, though it needs careful observation.\n4. Method\n4.1. Model Architecture\nWe employ the open-source Multimodal Large Language\nModel - LLaVA (Liu et al., 2023b;a), which consists of a\nvision encoder \u03c8V(\u00b7, \u03b8V), a large language model \u03c8L(\u00b7, \u03b8L)\nand a vision-language connector \u03c8C(\u00b7, \u03b8C). We illustrate the\nmodel in Figure 1. Provided with the input image xV and\ninstruction xI, the vision encoder is utilized to extract the\nimage features f = \u03c8V(xV, \u03b8V). Then a vision-language\nconnector is applied to convert the image features to the lan-\nguage embedding tokens \u03c8C(f, \u03b8C). After that, the vision\nand language tokens are combined and fed into the LLM to\ngenerate the response:\n\u02dc\nxR = \u03c8L([\u03c8C(f, \u03b8C), xI], \u03b8L)\n(1)\nThe training procedure contains two stages, including the\npre-training on image-text pairs and fine-tuning on visual in-\nstruction data. In the pre-training stage, the vision-language\nconnector parameters are updated using image-text pairs,\nwhile the weights of vision encoder and LLM remain frozen.\nIn the fine-tuning stage, we implement full-parameter tun-\ning or Low-rank Adaption (LoRA) tuning (Hu et al., 2022).\nWithout ambiguity, \u03b8L denotes the LLM parameters in full\ntraining setting and the learnable LoRA parameters in LoRA\ntraining setting. Then, the connector and learnable LLM\nparameters are updated using visual instruction data:\n\u03b8\u2217\nC, \u03b8\u2217\nL = arg min\n\u03b8C, \u03b8L\n\u2212\nN\nX\ni=1\nL\nX\nj=1\nlog p( \u02dc\nxR\nj\ni|xVi, xIi, xR\n<j\ni ),\n(2)\nwhere N and L denote the training sample size and the\nlength of each response.\n4.2. Coreset Selection Algorithm\nThe popular benchmarks evaluate different abilities of Multi-\nmodal Large Language Models (MLLM), which require spe-\ncific recipe of training data to evoke the pre-trained model.\nThus, we design a new data recipe, i.e. a coreset selection\nalgorithm, to better adapt those benchmarks and achieve\nbalance between performance and training efficiency.\nDiversity.\nWe construct a set of key concepts that match\nthe popular benchmarks, namely, MME (Fu et al., 2023)\nand MMBench (Liu et al., 2023c). Specifically, we design\nseveral high-level concepts and then use GPT-4 to generate\ndozens of key words about each concept. Then, we filter out\nthose key words that have low frequency in SVIT dataset.\nThe concept set is illustrated in Table 4 in the Appendix.\nWe measure the informativeness of each training sample by\nits overlap with concept set, and select the most informative\nones.\nBalance.\n\u201cYes\u201d or \u201cNo\u201d questions are used to evaluate\nmodels in MME benchmark. However, the proportion of\nthe two choices in GPT-4 generated data is extremely un-\nbalanced, which makes the tuned model has tendency to\nrespond \u201cYes\u201d. We adjust the proportion by re-sampling.\nWe empirically study the relation between \u201cYes:No\u201d propor-\ntion and model performance in Section 5.2.\nWith the above two operations, we obtain the coreset SVIT-\ncore-150K of 157,712 samples, which has the same size as\nLLaVA-Instruct-150K. We also produce SVIT-mix-665K\nby replacing LLaVA-Instruct-150K in LLaVA-v1.5-mix-\n665K (Liu et al., 2023a) with SVIT-core-150K.\n5. Experiments\nFirstly, we compare our model to the state-of-the-art\nMLLMs in Section 5.1. In this sub-section, we tune the\nadvanced LLaVA-v1.5-13B (Liu et al., 2023a) on the con-\nstructed SVIT-mix-665K dataset. Secondly, we implement\nablation study and provide more detailed evaluations in\nSection 5.2. We tune the LLaVA-v1.0 (LLaVA-LLaMA-\n2-7B-Chat) (Liu et al., 2023b) with various data recipes\nfor efficiency. Lastly, qualitative evaluation is provided in\nSection 5.3.\n7\nSVIT: Scaling up Visual Instruction Tuning\nTable 2. Comparison to state-of-the-art MLLMs on 11 benchmarks. Our models outperform LLaVA-v1.5 and others in most of the settings.\nWe evaluate these models on benchmarks: VQA-v2 (Goyal et al., 2017) test-dev split, GQA (Hudson & Manning, 2019) test-dev-balanced\nsplit, VisWiz (Gurari et al., 2018) test-dev split, SQAI: ScienceQA-IMG (Lu et al., 2022) test split, VQAT: TextVQA (Singh et al.,\n2019) validation split, MMEP: MME perception (Fu et al., 2023), MMEC: MME cognition (Fu et al., 2023), MMB: MMBench (Liu\net al., 2023c) test split, MMBCN: MMBench-Chinese (Liu et al., 2023c) test split, SEED: SEED-Bench (Li et al., 2023a), and MMMU\n(Yue et al., 2023) test split. We mark the best performance bold and the runner-up underlined. \u2217The training images of the datasets are\nobserved during training. \u00a7We evaluate the officially released checkpoint by ourselves.\nMethod\nLLM\nVQAv2 GQA VisWiz SQAI\nVQAT\nMMEP MMEC MMB\nMMBCN SEED MMMU\nBLIP-2\nVicuna-13B\n\u2013\n41.0\n19.6\n61.0\n42.5\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nBLIP-2\nFlan-T5-XXL\n65.0\n44.6\n29.4\n64.5\n44.1\n1293.8 290.0\n\u2013\n\u2013\n\u2013\n34.0\nInstructBLIP\nVicuna-7B\n\u2013\n49.2\n34.5\n60.5\n50.1\n\u2013\n\u2013\n33.9\n23.9\n53.4\n\u2013\nInstructBLIP\nVicuna-13B\n\u2013\n49.5\n33.4\n63.1\n50.7\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nInstructBLIP\nFlan-T5-XXL\n\u2013\n47.9\n30.9\n70.6\n46.6\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n33.8\nShikra-7B\nVicuna-7B\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n60.2\n\u2013\n\u2013\n\u2013\nShikra-13B\nVicuna-13B\n77.4\u2217\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nIDEFICS-9B\nLLaMA-7B\n50.9\n\u2013\n35.5\n44.2\n25.9\n\u2013\n\u2013\n45.3\n25.2\n\u2013\n\u2013\nIDEFICS-80B\nLLaMA-65B\n60.0\n\u2013\n36.0\n68.9\n30.9\n\u2013\n\u2013\n54.6\n38.1\n\u2013\n\u2013\nQwen-VL\nQwen-7B\n79.5\u2217\n59.3\u2217 35.2\n67.1\n63.8\u2217\n\u2013\n\u2013\n32.2\n7.8\n56.3\n\u2013\nQwen-VL-Chat\nQwen-7B\n78.2\u2217\n57.5\u2217 38.9\n68.2\n61.5\u2217\n1487.6 360.7\n61.8\n56.3\n58.2\n32.9\nmPLUG-Owl2\nLLaMA2-7B\n79.4\u2217\n56.1\u2217 54.5\n68.7\n58.2\n1450.2 313.2\n66.0\n60.3\n57.8\n32.1\nLLaVA-v1.5 (LoRA)\nVicuna-13B\n80.0\u2217\n63.3\u2217 58.9\n71.2\n60.2\n1541.7 300.4\u00a7\n68.4\u00a7\n62.4\u00a7\n61.3\n33.2\u00a7\nLLaVA-v1.5 (Full)\nVicuna-13B\n80.0\u2217\n63.3\u2217 53.6\n71.6\n61.3\n1531.3 295.4\n67.8\n63.3\n61.6\n33.6\nSVIT-v1.5 (LoRA)\nVicuna-13B\n80.1\u2217\n63.4\u2217 56.7\n69.9\n61.1\n1560.3 364.3\n68.3\n63.2\n61.8\n34.1\nSVIT-v1.5 (Full)\nVicuna-13B\n80.3\u2217\n64.1\u2217 56.4\n70.0\n60.8\n1565.8 323.2\n69.1\n63.1\n61.9\n33.3\n5.1. Comparison to the State of the Art\nWe adopt LLaVA-v1.5-13B (Liu et al., 2023a) architecture\nand pre-training weights, and then tune it on the constructed\nSVIT-mix-665K, which is named SVIT-v1.5. Specifically,\nwe replace LLaVA-v1.5-mix-665K with our SVIT-mix-\n665K in the visual instruction tuning stage. The rest of\nmodel training protocol is kept unchanged for fair compar-\nison. Visual instruction tuning takes about 21 hours for\nboth full-parameter tuning and LoRA tuning on 8 NVIDIA\nTesla A100 GPUs, each with 80GB memory, with Deep-\nSpeed ZeRO Stage 3. We compare SVIT-v1.5 to state-of-the-\nart MLLMs: BLIP-2 (Li et al., 2023c), InstructBLIP (Dai\net al., 2023), Shikra (Chen et al., 2023), IDEFICS (IDEFICS,\n2023), Qwen-VL(-Chat) (Bai et al., 2023), mPLUG-Owl2\n(Ye et al., 2023) and LLaVA-v1.5 (Liu et al., 2023a). We\nevaluate these models on popular benchmarks: VQA-v2\n(Goyal et al., 2017), GQA (Hudson & Manning, 2019),\nVisWiz (Gurari et al., 2018), ScienceQA-IMG (Lu et al.,\n2022), TextVQA (Singh et al., 2019), MME perception (Fu\net al., 2023), MME cognition (Fu et al., 2023), MMBench\n(Liu et al., 2023c), MMBench-Chinese (Liu et al., 2023c),\nSEED-Bench (Li et al., 2023a) and MMMU (Yue et al.,\n2023).\nAs shown in Table 2, our SVIT-v1.5 outperforms LLaVA-\nv1.5 and other models in most settings. Especially, in the\nmost popular benchmark - MME, SVIT-v1.5 (Full) achieves\n1565.8 score in MME perception and overwhelms LLaVA-\nv1.5 (Full) by 34.5 score. In the efficient LoRA training\nsetting, SVIT-v1.5 (LoRA) exceeds LLaVA-v1.5 (LoRA)\nby 63.9 score, namely, 364.3 v.s. 300.4, in MME cognition.\nThe improvements verify the better training effects of SVIT\ndata, since the same data amount and base model are used.\n5.2. Ablation Study\nWe further study the data quality, diversity strategy, bal-\nance strategy and scaling-up effects. 10% of the images\nare randomly sampled from SVIT as the held-out testing\nset for evaluation. The training split is denoted as SVIT-\ntrain. Note that, for saving the training cost, we implement\nablation study with the LLaVA-v1.0 model and evaluate\non MME benchmark. We denote the LLaVA-v1.0 model\ntrained on SVIT data as SVIT-v1.0.\nData Quality.\nLLaVA-v1.0 employs LLaVA-Instruct-80K\nas the visual instruction tuning data. To demonstrate the\nquality of SVIT, we construct a subset of SVIT-train at same\nscale of LLaVA-Instruct-80K and fine-tune LLaVA-v1.0 by\nreplacing LLaVA-Instruct-80K with the SVIT subset. With-\nout loss of generality, the subset is constructed by randomly\nsampling 20K data from conversation, complex reasoning,\nreferring QAs and detail description, leading to a subset of\n80K data in total, denoted as SVIT-80K. We adopt the same\ntraining protocol and hyper-parameters as LLaVA-v1.0. The\n8\nSVIT: Scaling up Visual Instruction Tuning\nTable 3. Evaluating models fine-tuned on LLaVA-Instruct-80K, SVIT-80K (random selection), SVIT-80K-D (enhancing diversity), SVIT-\n80K-B (with \u201cYes/No\u201d balancing) and SVIT-train (SVIT train split) on MME benchmark. Note that the base model is LLaVA-v1.0 (Liu\net al., 2023b). For LLaVA-Instruct-80K, we evaluate the officially released checkpoint by ourselves.\nTask\nSub-task\nLLaVA-Instruct-80K\nSVIT-80K\nSVIT-80K-D\nSVIT-80K-B\nSVIT-train\nOverall\nTotal\n1147.70\n1241.84\n1262.15\n1329.77\n1399.66\nPerception\nTotal\n906.63\n1005.41\n1017.15\n1035.13\n1166.45\nExistence\n90.00\n90.00\n95.00\n120.00\n185.00\nCount\n55.00\n115.00\n110.00\n118.33\n131.67\nPosition\n56.67\n53.33\n58.33\n58.33\n56.67\nColor\n50.00\n50.00\n55.00\n58.33\n100.00\nPosters\n116.33\n143.20\n146.26\n133.67\n134.01\nCelebrity\n85.88\n75.88\n77.06\n84.71\n77.35\nScene\n152.75\n161.25\n159.50\n153.75\n153.25\nLandmark\n130.75\n148.25\n151.00\n137.75\n144.50\nArtwork\n96.75\n111.00\n107.50\n105.25\n104.00\nOCR\n72.50\n57.50\n57.50\n65.00\n80.00\nCognition\nTotal\n241.07\n236.43\n245.00\n294.64\n233.21\nCommonsense reasoning\n83.57\n86.43\n80.00\n87.14\n80.71\nNumerical calculation\n45.00\n57.50\n55.00\n57.50\n47.50\nText translation\n57.50\n50.00\n65.00\n97.50\n50.00\nCode reasoning\n55.00\n42.50\n45.00\n52.50\n55.50\ntraining takes less than 1 hour on 8 NVIDIA Tesla A100\nGPUs, each with 40GB memory, with DeepSpeed ZeRO\nStage 3.\nThe evaluation results on MME benchmark are shown in\nTable 3. The model fine-tuned on SVIT-80K achieves higher\nperformance (+8.2%) than the model fine-tined by LLaVA-\nInstruct-80K. Specially, the model fine-tuned on SVIT-80K\noutperforms on \u201ccount\u201d (+109.1%), \u201cposters\u201d(+23.1%),\n\u201cscene (+5.6%)\u201d, \u201clandmark (+13.4%)\u201d, \u201cartwork (+14.7%)\u201d\nin perception tasks, as well as \u201ccommense reason-\ning\u201d(+3.4%), \u201cnumerical calculation\u201d (+27.8%) in cognition\ntasks. The high performance of SVIT on those tasks can be\ndue to the fact that SVIT dataset is constructed with more\ndetailed manual annotations of the images, and the prompts\nfor GPT-4 to generate QAs are carefully designed to cover\na wide range of tasks, evoking the model to understand the\nimages more accurately and comprehensively.\nDiversity Strategy.\nWe produce an 80K subset selected\nwith diversity strategy and compare it to the randomly se-\nlected SVIT-80K. We first remove half less-informative sam-\nples of SVIT-train based on the measured informativeness of\neach sample. Then we randomly sample 20K data for each\ncategory in SVIT, leading to the 80K subset \u2013 SVIT-80K-D.\nAs showed in Table 3, its performance has an improvement\nof 20.3 score over SVIT-80K, which verifies the effective-\nness of the diversity strategy.\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\"Yes\":\"No\" in the training data\n900\n1000\n1100\n1200\n1300\n1400\nMME total score\nFraction of \"Yes\" in responses\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 5. The relation between \u201cYes:No\u201d proportion in training\ndata and model performance.\nBalance Strategy.\nThe MME benchmark consists of\n2,374 \u201cYes\u201d or \u201cNo\u201d answers with the proportion 1 : 1.\nHowever, the randomly selected SVIT-80K dataset con-\ntains 7.5% \u201cYes\u201d or \u201cNo\u201d QA pairs with the proportion\nY : N = 20. We analyze the relation between the \u201cYes:No\u201d\nproportion and model performance by adjusting the propor-\ntion. As shown in Figure 5, the model trained on randomly\nsampled SVIT-80K with Y : N = 20 responds 1,393 \u201cYes\u201d\nwhile 981 \u201cNo\u201d on MME questions. We adjust the \u201cYes:No\u201d\nproportion in training data by randomly dropping some\n9\nSVIT: Scaling up Visual Instruction Tuning\nPerception & planning\nQ: What does the yellow symbol mean?\nA: The yellow symbol is a gas pump symbol, indicating that the vehicle is low on fuel and needs to be refueled.\nQ: How far can I drive with the light on?\nA: The exact distance depends on the vehicle's fuel efficiency and the current fuel level. However, it's generally recommended to \nrefuel as soon as possible when the light is on.\nCaptioning & suggestion\nQ: What can you see in the image?\nA: The image shows a snowy street scene. There are several cars, some of which are covered in snow. There are also buildings, \ntrees, and a street sign.\nQ: What should I be cautious of if I'm the driver of the car?\nA: As a driver, you should be cautious of the snow-covered road, which can be slippery. Also, the snow-covered cars and the \nsnow-covered sidewalk might make it difficult to see pedestrians or other vehicles.\nReasoning & referring\nQ: What is the atmosphere of the scene?\nA: The scene appears to be a joyful celebration, likely a birthday party. The woman is holding a birthday cake with lit candles, \nand the little boy is smiling, suggesting a happy and festive atmosphere.\nQ: Provide the bounding box coordinate of a birthday cake.\nA: The bounding box for the birthday cake is [0.21, 0.3, 0.48, 0.52].\nPerception & reasoning \nQ: Is this image real?\nA: No, this image is not real. It's a computer-generated image, or CGI, created to depict an astronaut enjoying a beer on the moon.\nQ: What can this image be used for?\nA: This image could be used for a variety of purposes, such as advertising, art, or educational materials. It's a creative \nrepresentation of an astronaut enjoying a beer on the moon, which could be used to promote a brand, or to teach about space \nexploration.\nPerception & suggestion\nQ: Describe this image.\nA: The image depicts a humorous scene of a boy preparing for an exam. He is shown in three different stages of his \npreparation, each with a different expression. The first stage shows him with a smile, the second with a frown, and the third with \nhis eyes wide open. He is surrounded by various objects such as a coffee mug, a book, a pen, and a red thermos.\nQ: What does this image convey?\nA: The image conveys a sense of humor and relatability. It shows the boy's journey from a calm and prepared state to a state \nof panic and confusion, which is often how people feel when they're studying for an exam.\nQ: What suggestion would you give to the boy?\nA: Given the context, I would suggest the boy to take breaks and manage his stress levels. It's important to maintain a balance \nbetween study and relaxation to avoid burnout.\nFigure 6. Demonstration of different abilities of SVIT-v1.5.\nquestions with \u201cYes\u201d answers after random sampling from\nSVIT-train, ensuring a subset with exactly 80,000 samples.\nIt is interesting that the model trained on the equilibrium,\ni.e. Y : N = 1, very likely responds \u201cNo\u201d for any ques-\ntions. The curve indicates that Y : N = 8 is a good data\nrecipe for SVIT-v1.0 model and the produced model will\nrespond \u201cYes\u201d or \u201cNo\u201d uniformly, which is close to the\nprior. We denote the model tuned with this data recipe as\nSVIT-80K-B, and it achieves 7.1% improvement over the\nmodel fine-tuned on SVIT-80K on MME benchmark.\nScaling Up.\nTo investigate whether scaling up the visual\ninstruction tuning dataset actually helps improve the model\u2019s\nperformance, we further conduct larger experiment - train-\ning the model with SVIT-train. In the training process, the\nfine-tuning schedule and other hyper-parameters remain un-\nchanged, while the learning rate is decreased from 2e-5 to\n2e-6 to better fit the larger training data scale. The training\ntakes around 24 hours on 8 NVIDIA Tesla A100 GPUs,\neach with 40GB memory, with DeepSpeed ZeRO Stage 3.\nThe evaluation results are shown in the last columns of\nTable 3. We compare SVIT-train to SVIT-80K (randomly\nselected) without any data recipe. Compared with the model\nfine-tuned on SVIT-80K, the total score of the model fine-\ntuned on SVIT-train achieves +12.7% score improvement\non MME benchmark. Particularly, fine-tuning the model\non more data significantly enhances the model\u2019s ability to\ncomprehend the existence of objects (+105.6%), the color of\nobjects (+100.0%), OCR (+39.1%), etc. The results validate\nthe effectiveness of scaling up the visual instruction tuning\ndataset when fine-tuning MLLMs.\n10\nSVIT: Scaling up Visual Instruction Tuning\n5.3. Qualitative Evaluation\nIn Figure 6, we provide the qualitative evaluation of the\nSVIT-v1.5. The first case demonstrates a conversation dis-\ncussing the scene and asking for suggestions. When de-\nscribing the scene, SVIT-v1.5 depicts the foreground, as\nwell as details in the background. When giving suggestions,\nSVIT-v1.5 offers a comprehensive assessment, taking mul-\ntiple factors into consideration, such as the snow-covered\nroad, cars and sidewalks. In the second case, to evaluate\nthe ability of planning, we ask the model what is happening\nin the image and prompt the model to plan the subsequent\nsteps. SVIT-v1.5 accurately points out the meaning of the\nsymbol and logically make recommendations. In terms of\nthe ability to locate and refer objects, the third case shows\nthat SVIT-v1.5 correctly identifies the location of the men-\ntioned object with a bounding box, in the format of [x1, y1,\nx2, y2], where [x1, y1] are the normalized coordinates of\nthe top-left point and [x2, y2] are the normalized coordi-\nnates of the bottom-right point. Regarding perception and\nreasoning performance, in the fourth case, SVIT-v1.5 is able\nto distinguish between a real image and a synthetic one.\nIt also understands the intended use of the image, such as\nadvertising, art or education. Similarly, the fifth case feeds\nSVIT-v1.5 with comics about exam preparation. SVIT-v1.5\nfigures out that the three sub-figures are the different stages\nof the preparation and infers the theme of the comics. It also\ngenerates appropriate suggestions for the character.\n6. Conclusion\nIn this paper, we scale up visual instruction tuning by pre-\nsenting a large-scale dataset \u2013 SVIT that contains in total\n4.2 million instruction tuning data. We also propose new\ndata recipe of sample selection for better diversity and bal-\nance. The abundant experiments verify that our SVIT-v1.5\ntrained on the proposed dataset and its subsets outperforms\nstate-of-the-art MLLMs on multiple benchmarks.\nAcknowledgment\nThis work is funded by the following grants: National\nKey R&D Program of China (2021ZD0111102) and NSFC-\n62306046.\nReferences\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,\nZitnick, C. L., and Parikh, D. Vqa: Visual question\nanswering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425\u20132433, 2015.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,\nZhou, C., and Zhou, J. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12m: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 3558\u20133568, 2021.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic. arXiv preprint arXiv:2306.15195, 2023.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nDai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W.,\nLi, B., Fung, P., and Hoi, S. InstructBLIP: Towards\ngeneral-purpose vision-language models with instruc-\ntion tuning.\nIn Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023. URL https:\n//openreview.net/forum?id=vvoWPYqZJA.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Burstein, J., Doran, C., and\nSolorio, T. (eds.), Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pp. 4171\u20134186, Min-\nneapolis, Minnesota, June 2019. Association for Compu-\ntational Linguistics.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\nN. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference\non Learning Representations, 2021. URL https://\nopenreview.net/forum?id=YicbFdNTTy.\n11\nSVIT: Scaling up Visual Instruction Tuning\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu,\nZ., Lin, W., Yang, J., Zheng, X., Li, K., Sun, X., and\nJi, R. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023.\nGong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q.,\nLiu, K., Zhang, W., Luo, P., and Chen, K. Multimodal-gpt:\nA vision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790, 2023.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answer-\ning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 6904\u20136913, 2017.\nGrauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari,\nA., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu,\nX., et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n18995\u201319012, 2022.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\nAnswering visual questions from blind people. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3608\u20133617, 2018.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00b4ar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16000\u201316009, 2022.\nHu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y.,\nWang, S., Wang, L., and Chen, W. LoRA: Low-rank adap-\ntation of large language models. In International Confer-\nence on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=nZeVKeeFYf9.\nHuang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma,\nS., Lv, T., Cui, L., Mohammed, O. K., Liu, Q., et al. Lan-\nguage is not all you need: Aligning perception with lan-\nguage models. arXiv preprint arXiv:2302.14045, 2023.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 6700\u2013\n6709, 2019.\nIDEFICS.\nIntroducing idefics: An open reproduction\nof state-of-the-art visual language model. https://\nhuggingface.co/blog/idefics, 2023.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 123:32\u201373, 2017.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.\nSeed-bench: Benchmarking multimodal llms with gener-\native comprehension. arXiv preprint arXiv:2307.16125,\n2023a.\nLi, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C.,\nand Liu, Z. Mimic-it: Multi-modal in-context instruction\ntuning. arXiv preprint arXiv:2306.05425, 2023b.\nLi, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot-\nstrapping language-image pre-training with frozen image\nencoders and large language models. In Krause, A., Brun-\nskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett,\nJ. (eds.), Proceedings of the 40th International Confer-\nence on Machine Learning, volume 202 of Proceedings\nof Machine Learning Research, pp. 19730\u201319742. PMLR,\n23\u201329 Jul 2023c.\nURL https://proceedings.\nmlr.press/v202/li23q.html.\nLi, L., Yin, Y., Li, S., Chen, L., Wang, P., Ren, S., Li,\nM., Yang, Y., Xu, J., Sun, X., Kong, L., and Liu, Q.\nM3it: A large-scale dataset towards multi-modal multilin-\ngual instruction tuning. arXiv preprint arXiv:2306.04387,\n2023d.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013\n755. Springer, 2014.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J.\nVisual instruc-\ntion tuning.\nIn Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023b. URL https:\n//openreview.net/forum?id=w0H2xGHlkw.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin,\nD. Mmbench: Is your multi-modal model an all-around\nplayer? arXiv:2307.06281, 2023c.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,\nS., and Guo, B. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npp. 10012\u201310022, 2021.\n12\nSVIT: Scaling up Visual Instruction Tuning\nLiu, Z., He, Y., Wang, W., Wang, W., Wang, Y., Chen, S.,\nZhang, Q., Lai, Z., Yang, Y., Li, Q., Yu, J., et al. Interngpt:\nSolving vision-centric tasks by interacting with chat-\ngpt beyond language. arXiv preprint arXiv:2305.05662,\n2023d.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. In The 36th Conference on\nNeural Information Processing Systems (NeurIPS), 2022.\nMu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin,\nJ., Wang, B., Dai, J., Qiao, Y., and Luo, P.\nEmbod-\niedGPT: Vision-language pre-training via embodied chain\nof thought.\nIn Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023. URL https:\n//openreview.net/forum?id=IL5zJqfxAa.\nOpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023.\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F.\nKosmos-2: Grounding multimodal\nlarge language models to the world.\narXiv preprint\narXiv:2306.14824, 2023.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\nAdvances in Neural Information Processing Systems, 35:\n25278\u201325294, 2022.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 8317\u20138326, 2019.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM, 59(2):64\u201373, 2016.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N.\nVisual chatgpt: Talking, drawing and editing with visual\nfoundation models. arXiv preprint arXiv:2303.04671,\n2023.\nYe, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang,\nJ., Huang, F., and Zhou, J. mplug-owl2: Revolution-\nizing multi-modal large language model with modality\ncollaboration. arXiv preprint arXiv:2311.04257, 2023.\nYue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G.,\nStevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B.,\nYuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu,\nY., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu:\nA massive multi-discipline multimodal understanding\nand reasoning benchmark for expert agi. arXiv preprint\narXiv:2311.16502, 2023.\nZhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou,\nY., and Qiu, X. Speechgpt: Empowering large language\nmodels with intrinsic cross-modal conversational abilities.\narXiv preprint arXiv:2305.11000, 2023a.\nZhang, H., Li, X., and Bing, L.\nVideo-LLaMA: An\ninstruction-tuned audio-visual language model for video\nunderstanding. In Feng, Y. and Lefever, E. (eds.), Pro-\nceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations,\npp. 543\u2013553, Singapore, December 2023b. Association\nfor Computational Linguistics.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang,\nD., and Sun, T. Llavar: Enhanced visual instruction\ntuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107, 2023c.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023a.\nZhu, W., Hessel, J., Awadalla, A., Gadre, S. Y., Dodge,\nJ., Fang, A., Yu, Y., Schmidt, L., Wang, W. Y., and\nChoi, Y. Multimodal c4: An open, billion-scale cor-\npus of images interleaved with text. In Thirty-seventh\nConference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2023b. URL https:\n//openreview.net/forum?id=tOd8rSjcWz.\n13\nSVIT: Scaling up Visual Instruction Tuning\nZou, X., Dou, Z.-Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X.,\nBehl, H., Wang, J., Yuan, L., et al. Generalized decoding\nfor pixel, image, and language. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 15116\u201315127, 2023.\n14\nAppendix\nA. Prompts\nBased on the captions, object bounding boxes and region descriptions of images, we design four tasks and prompt GPT-4\nto respond accordingly. We do not include the bounding boxes of region descriptions in the input data for conversation,\ncomplex reasoning and detail description, since the context length may exceed the limit of GPT-4 in many cases. The\nprompts share the same paragraph describing the input data at the beginning and then differ in task description, which are\nsummarized in Figure 7. For referring QAs, since the location information plays a vital role in understanding the image\naccurately, we include the bounding boxes of region descriptions in the input data, and shorten the response to 10 QAs for\nevery image to fit in the context limit. The prompt is summarized in Figure 8.\nB. Instructions for Detail Description\nFigure 9 shows the instructions for detail description. We prompt GPT-4 to generate different ways of saying \u201ccan you\ndescribe the image in detail\u201d and accumulate all the instructions. For each image, we randomly sample one from the list as\ninstruction.\nC. Concept Set\nWe design the concept set with the key words for measuring the informativeness of training samples, which is illustrated\nin Table 4. The key words for each concept are generated by prompting GPT-4 and filtering based on their frequencies\noccurring in the dataset.\nTable 4. The concept set and its key words for measuring the informativeness of training samples.\nConcept\nKey Words\ncolor\nbeige, black, brown, color, gold, gray, green, khaki, lavender, mauve, olive,\npeach, pink, red, rose, salmon, white\nmaterial\ncanvas, cardboard, ceramic, cork, denim, fabric, fiberglass, foam, glass,\nglassy, granite, iron, latex, leather, linen, marble, mesh, metal, nylon,\nplaster, plastic, polymer, porcelain, satiny, silk, steel, stone, stony, suede,\nvelvet, vinyl, wood, wooden\nquantity\naccount, being, existence, five, four, number, one, seven, six, substance,\nten, three, total, two\nspatial relation\nabove, adjacent, ahead, backward, below, between, central, close, down,\ndownward, far, in back, inside, left, left direction, near, on, outside, periph-\neral, position, proximate, remote, surrounding, under, up, upstairs, upward,\nwithout\nsize\nbig, compact, compactness, dimension, diminutive, enormity, enormous,\ngiant, gigantic, immense, immensity, large, largeness, magnitude, massive,\nmedium size, microscopic, miniature, minuscule, moderately, oversized,\nproportion, sizeable, slightly, small, smaller, vast, vastness\n15\nSVIT: Scaling up Visual Instruction Tuning\nYou are an AI visual assistant that can analyze a single image. The information of the image is made up of three parts:\n(1) \u201ccaptions\u201d: If it is not empty, it contains five sentences, describing the image you are observing.\n(2) \u201cobjects\u201d: It contains multiple lines, each describing an object of the same image you are observing. Every line is made\nup of an object name and its bounding box. The bounding box is in the form of [x1, y1, x2, y2]. The values are float numbers\nnormalized from 0 to 1, corresponding to the top left x, top left y, bottom right x, and bottom right y.\n(3) \u201cregions\u201d. It contains multiple lines, each describing a region of the same image you are observing.\nConversation:\nDesign three conversations between you and a person asking about this image. A conversation should include five question and\nanswer pairs. The content within the conversation should be logically connected. You can think about the topic first and then\ngenerate the conversation according to the topic. The topic can be the visual content of the image (including the object types,\ncounting the objects, object actions, object locations, relative positions between objects, etc.), the background knowledge of the\nobjects, the events happening in the image, event planning, etc. In the conversation, you are called \u201cGPT\u201d. The person talking with\nyou is called \u201cUser\u201d.\nAsk diverse questions and give corresponding answers. Only include questions that have definite answers. The answer should be\nin a tone that a visual AI assistant is seeing the image and answering the question. The length of the answer would better be within\n50 tokens.\nWhen using the information from the description, do not mention that the information source is the description. When using the\ninformation from the object bounding box, do not mention that the information comes from the bounding box as well. Always\nanswer as if you are directly looking at the image.\nComplex reasoning:\nCreate 15 plausible question and answer pairs about the image with provided information.\nThe question requires commonsense knowledge about the scene and can only be answered with image provided. Avoid asking\nquestions that can be answered with commonsense knowledge alone. Avoid proposing questions that can be answered with simple\nvisual understanding like asking about object type and color. Do not give too many details about the visual content of the image, so\none has to figure it out first to answer the question correctly. The question can be asking why things happen that way, suggestions\nto the people in the image, etc. When providing the answer for complex questions, think step by step and include reasoning details.\nWhen using the information from the description, do not mention that the information source is the description. When using the\ninformation from the object bounding box, do not mention that the information comes from the bounding box as well. Always\nanswer as if you are directly looking at the image.\nDesired format:\nQuestion: ...\nAnswer: ...\nQuestion: ...\nAnswer: ...\nDetail description:\nThe task is describing the image in detail. Though you do not receive the pixel data of the image, utilize above textual information\nto think about the image and describe as if you are directly looking at the image. The description can include what people or\nobjects are doing, object appearance, object count, object position, background details, etc. Only describe the things that you are\nsure about.\nWhen using the information from the description, do not mention that the information source is the description. When using the\ninformation from the object bounding box, do not mention that the information comes from the bounding box as well.\nFigure 7. The prompts of conversation, complex reasoning and detail description to GPT-4.\n16\nSVIT: Scaling up Visual Instruction Tuning\nReferring QAs:\nYou are an AI visual assistant that can analyze a single image. The information of the image is made up of three parts: (1)\n\u201ccaptions\u201d: If it is not empty, it contains five sentences, describing the image you are observing.\n(2) \u201cobjects\u201d: It contains multiple lines, each describing an object of the same image you are observing. Every line is made up of\nthe object name and its bounding box.\n(3) \u201cregions\u201d: It contains multiple lines, each describing a region of the same image you are observing. Every line is made up of\nthe region description and the region\u2019s bounding box.\nThe bounding box is in the form of [x1, y1, x2, y2]. The values are float numbers normalized from 0 to 1, corresponding to the top\nleft x, top left y, bottom right x, and bottom right y. Note that the same object may be described multiple times in \u201cobjects\u201d with\nbounding boxes that are slightly different.\nThe task is creating 10 question and answer pairs of specific regions in the image with the provided information. The question can\nonly be answered with image information provided. Figure out the relative positions of objects and create some questions about\nthat. Also propose some questions that need reasoning, like why things happen that way, suggestions to the people in the image,\netc. When providing answers for complex questions, think step by step and include reasoning details.\nWhen referring to any object in the question or answer, always wrap it with prefix \u201c<st>\u201d, suffix \u201c<ed>\u201d and attach its bounding\nbox after it, in the format of \u201c<st>man<ed> [x1, y1, x2, y2]\u201d. If multiple objects are referred to, attach all the corresponding\nbounding boxes after them, e.g., \u201c<st>men<ed> [x1, y1, x2, y2], [x1, y1, x2, y2]\u201d. Refer to at least one object in the question\nand answer.\nWhen using the information from the description, do not mention that the information source is the description. When using the\ninformation from the bounding box, do not mention that the information comes from the bounding box as well. Always answer as\nif you are directly looking at the image.\nDesired format:\nQuestion: ...\nAnswer: ...\nQuestion: ...\nAnswer: ...\nFigure 8. The prompt of referring QAs to GPT-4.\n17\nSVIT: Scaling up Visual Instruction Tuning\nInstructions for detail description\n\u2022 Can you provide a comprehensive description of the image?\n\u2022 Elaborate on the details of the image.\n\u2022 What are the specifics visible in the image?\n\u2022 Could you offer an in-depth analysis of the image?\n\u2022 Can you depict the image with precise detail?\n\u2022 Give a detailed account of the image.\n\u2022 Explain the image in meticulous detail.\n\u2022 Can you portray the image in words?\n\u2022 Give a thorough narrative of the image.\n\u2022 Please provide an intricate breakdown of the image.\n\u2022 Offer a complete interpretation of the image.\n\u2022 Delve into the particulars of the image.\n\u2022 Explain all the nuances you observe in the image.\n\u2022 Provide a detailed commentary on the image.\n\u2022 Illustrate the image in depth using your words.\n\u2022 Could you give a blow-by-blow description of the image?\n\u2022 Go into detail about the different elements of the image.\n\u2022 Can you dissect the image and describe each element in detail?\n\u2022 Detail the contents of the image extensively.\n\u2022 Can you provide an in-depth explanation of the image?\n\u2022 Provide a comprehensive overview of the image.\n\u2022 Break down the elements of the image in detail.\n\u2022 Can you expound upon the features of the image?\n\u2022 Offer an exhaustive description of the image.\n\u2022 How would you illustrate the image in words?\n\u2022 Please convey the image\u2019s details verbally.\n\u2022 Can you detail the contents of the image?\n\u2022 Narrate what you see in the image in depth.\n\u2022 Kindly provide a meticulous commentary on the image.\n\u2022 Share an extensive description of the image.\n\u2022 Could you interpret the image in a detailed manner?\n\u2022 Present a detailed report of the image\u2019s features.\n\u2022 Can you provide an intricate depiction of the image?\n\u2022 Disclose every detail you see in the image.\nFigure 9. Instructions for detail description.\n18\n"
  },
  {
    "title": "International Institutions for Advanced AI",
    "link": "https://arxiv.org/pdf/2307.04699.pdf",
    "upvote": "4",
    "text": "International Institutions for Advanced AI\nLewis Ho1, Joslyn Barnhart1, Robert Trager2,\nYoshua Bengio3, Miles Brundage4, Allison Carnegie5, Rumman Chowdhury6,\nAllan Dafoe1, Gillian Hadfield7, Margaret Levi8, Duncan Snidal9\n1Google DeepMind, 2Blavatnik School of Government, University of Oxford and\nCentre for the Governance of AI, 3Universit\u00e9 de Montr\u00e9al and Mila, CIFAR Fellow,\n4OpenAI, 5Columbia University, 6Harvard Berkman Klein, 7University of Toronto,\nVector Institute and OpenAI (independent contractor), 8Stanford University, 9Nuffield\nCollege, University of Oxford\nAbstract\nInternational institutions may have an important role to play in ensuring advanced\nAI systems benefit humanity. International collaborations can unlock AI\u2019s ability\nto further sustainable development, and coordination of regulatory efforts can re-\nduce obstacles to innovation and the spread of benefits. Conversely, the potential\ndangerous capabilities of powerful and general-purpose AI systems create global\nexternalities in their development and deployment, and international efforts to further\nresponsible AI practices could help manage the risks they pose. This paper identifies\na set of governance functions that could be performed at an international level to\naddress these challenges, ranging from supporting access to frontier AI systems to\nsetting international safety standards. It groups these functions into four institutional\nmodels that exhibit internal synergies and have precedents in existing organizations:\n1) a Commission on Frontier AI that facilitates expert consensus on opportunities\nand risks from advanced AI, 2) an Advanced AI Governance Organization that sets\ninternational standards to manage global threats from advanced models, supports\ntheir implementation, and possibly monitors compliance with a future governance\nregime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and\n4) an AI Safety Project that brings together leading researchers and engineers to\nfurther AI safety research. We explore the utility of these models and identify open\nquestions about their viability.\nExecutive Summary\nRecent advances in AI have highlighted the potentially transformative impacts of advanced systems.1\nInternational institutions may have an important role to play in ensuring these are globally beneficial.\nInternational collaborations could be important for unlocking AI\u2019s abilities to further sustainable\ndevelopment and benefit humanity. Many societies that could most benefit may not have the\nresources, infrastructure or training to take advantage of current cutting-edge AI systems. Frontier\nAI development may not focus on global needs, and the economic benefits of commercial AI\ntechnologies could primarily benefit developed countries. A failure to coordinate or harmonize\nregulation may also slow innovation.\n1By \u201cadvanced AI systems\u201d we mean systems that are highly capable and general purpose.\narXiv:2307.04699v2  [cs.CY]  11 Jul 2023\nMoreover, international efforts may also be necessary to manage the direct risks posed by powerful\nAI capabilities. Without adequate safeguards, some of these capabilities\u2014automated software\ndevelopment, chemistry and synthetic biology research, text and video generation\u2014may be misused\nby malicious actors around the world with transnational consequences. Furthermore, the significant\ngeopolitical benefits of rapid AI development decreases the likelihood of adequate AI governance\nwithout international cooperation.\nThis paper describes a set of international governance functions, ranging from the distribution of\nfrontier AI systems to the setting of safety standards, that may be needed to harness the opportunities\nand mitigate the risks of advanced AI. Early efforts to perform some of these functions have been\nundertaken by inter-governmental organizations like the Organisation for Economic Co-operation\nand Development (OECD), the Global Partnership on AI (GPAI), the Group of 7 (G7) Hiroshima\nProcess, the International Telecommunication Union (ITU), as well as by private sector initiatives\nlike the Partnership on AI, the ML Commons, and International Standards Organization (ISO) and\nInternational Electrotechnical Commission (IEC) standard-setting initiatives. But the rapid rate of AI\nprogress suggests further institutional efforts in AI global governance could be needed.\nThe functions we identify could be divided in multiple ways across organizations and could involve\nstakeholders from the public sector, the private sector, and civil society. We group these functions\ninto four institutional models that exhibit synergies and have precedents in existing organizations,\nand discuss their strengths and limitations:\n\u2022\nAn intergovernmental Commission on Frontier AI2 could establish a scientific\nposition on opportunities and risks from advanced AI and how they may be\nmanaged. In doing so, it would increase public awareness and understanding of\nAI prospects and issues, contribute to a scientifically informed account of AI use\nand risk mitigation, and be a source of expertise for policymakers.\n\u2022\nAn intergovernmental or multi-stakeholder Advanced AI Governance Organi-\nzation3 could help internationalize and align efforts to address global risks from\nadvanced AI systems by setting governance norms and standards, and assisting\nin their implementation. It may also perform compliance monitoring functions\n(either independently or in association with industry groups) for an international\ngovernance regime.\n\u2022\nA Frontier AI Collaborative4 could promote access to advanced AI as an interna-\ntional public-private partnership. In doing so, it could help underserved societies\nbenefit from cutting-edge AI technology and promote international access to AI\ntechnology for safety and governance objectives.\n\u2022\nAn AI Safety Project5 could bring together leading researchers and engineers, and\nprovide them with access to computing resources and advanced AI models for work\non technical mitigations of AI risks, potentially working with parallel industry\nefforts. It would promote AI safety R&D by increasing its scale, resourcing and\ncoordination.\nThere are important open questions around the viability of such models. A Commission on Frontier\nAI will face significant scientific challenges given the limited scientific research on advanced AI\n2Similar institutions include the Intergovernmental Panel on Climate Change (IPCC), the Intergovernmental Science-\nPolicy Platform on Biodiversity and Ecosystem Services (IPBES), and the Scientific Assessment Panel of the United\nNations Environment Programme.\n3Cf. the International Civil Aviation Organization (ICAO), the International Atomic Energy Agency (IAEA) and the\nFinancial Action Task Force (FATF).\n4Cf. international public-private partnerships like Gavi, the Vaccine Alliance and the Global Fund to Fight AIDS,\nTuberculosis and Malaria; as well as organizations that hold dangerous technologies, like the IAEA\u2019s nuclear fuel bank and\nthe Atomic Development Authority proposed following WWII.\n5Cf. scientific collaborations like the European Organization for Nuclear Research (CERN) and ITER.\n2\nissues, and will have to combat the politicization of its activities. The rapid rate of AI progress will\nmake it difficult for an Advanced AI Governance Organization to set standards that keep up with and\nare appropriately scoped for the risk landscape, and the many difficulties of international coordination\nraise questions about how participation in an Organization regime can be incentivized. The potentially\ndual-use nature of general purpose AI technologies might hamper a Frontier AI Collaborative\u2019s\nability to distribute beneficial systems widely, and the significant obstacles to underserved societies\nmaking use of AI systems raises questions about its effectiveness as a means of promoting sustainable\ndevelopment. Finally, an AI Safety Project could struggle to secure adequate model access to conduct\nsafety research, and it may not be worthwhile to divert safety researchers away from frontier labs.\nTable 1 below summarizes our mapping of institutional functions, the challenges they address, and\nthe existing and possible institutions that perform those functions.\nThere are details about these institutional models that we leave out of scope or that remain uncertain\u2014\nwhether these institutions should be new or evolutions of existing organizations, whether the con-\nditions under which these institutions are likely to be most impactful will obtain, whether other\ngroupings of institutional functions would be more effective\u2014but rapid progress on these topics will\nhelp prepare for the development of advanced AI.\nTable 1: A mapping of international institutional functions, governance objectives and models.\nFunction \u2192\nScience and Technology\nResearch, Development and Diffusion\nInternational Rulemaking and Enforcement\nObjective /\ninstitutions \u2193\nConduct\nor Support\nAI Safety\nResearch\nBuild\nConsensus on\nOpportunities\nand Risks\nDevelop\nFrontier AI\nDistribute\nand Enable\nAccess to AI\nSet Safety\nNorms and\nStandards\nSupport\nImplemen-\ntation of\nStandards\nMonitor\nCompliance\nControl\nInputs\nSpreading\nBeneficial\nTechnology\nNo\nYes\nMaybe\nYes\nNo\nNo\nNo\nNo\nHarmonizing\nRegulation\nNo\nNo\nNo\nNo\nYes\nYes\nNo\nNo\nEnsuring Safe\nDevelopment\nand Use\nMaybe\nYes\nMaybe\nMaybe\nYes\nYes\nMaybe\nMaybe\nManaging\nGeopolitical\nRisk Factors\nNo\nNo\nMaybe\nMaybe\nNo\nNo\nYes\nYes\nExisting Int\u2019l\nInstitutional\nEfforts\nOECD,\nGPAI, G7,\nITU\nISO/IEC\nSemi-\nconductor\nExport\nControls\nPossible\nInstitution\nAI Safety\nProject\nCommission\non Frontier AI\nFrontier AI Collaborative\nAdvanced AI Governance Agency\nKey\nchallenges\nModel\naccess;\ndiverting\ntalent\nPoliticization;\nscientific\nchallenges\nManaging dual-use\ntechnology; education,\ninfrastructure and\necosystem obstacles\nIncentivizing participation;\nquickly changing risk landscape;\nmaintaining appropriate scope\n3\n1\nIntroduction\nThe capabilities of AI systems have grown quickly over the last decade. Employing a growing wealth\nof algorithmic insights, data sources and computation power, AI researchers have created systems\nthat can comprehend language, recognize and generate images and video, write computer programs\nand engage in scientific reasoning. If current trends in AI capabilities continue, AI systems could\nhave transformative impacts on society.\nPowerful AI systems will bring significant benefits and risks. These systems have the capacity to\nsignificantly improve the productivity of workers and economies as a whole, and to help us address\nsome of our most important social and technological challenges. But these systems also present\nchallenges including workforce dislocation, lack of transparency, biased outcomes, inequitably\nshared benefits and threats to national security.\nPromoting AI benefits and managing AI risks both have domestic and international components.\nOn the domestic front, governments and the private sector will need to establish rules and norms\naround how advanced AI systems are developed, distributed, deployed and accessed, addressing\nissues like security, distributive impacts, privacy, bias, and more. A number of challenges have the\npotential to transcend national borders and impact societies and economies worldwide. Accordingly,\npolicymakers, technologists, and AI governance experts have recently begun to call for specific\nglobal AI governance initiatives centered on international institutions.6\nThis paper contributes to these early conversations by discussing why AI governance may be needed\non an international scale and then offering a non-exhaustive taxonomy of the institutional functions\nthat international efforts might require. It explores four possible international institutions to perform\nthese functions: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and\nrisks from advanced AI; 2) an Advanced AI Governance Organization that sets international standards,\nsupports their implementation, and could monitor compliance to future governance regimes; 3) a\nFrontier AI Collaborative that develops and distributes cutting-edge AI; and 4) an AI Safety Project\nthat brings together exceptional researchers, engineers and compute to further AI safety research.\nEach of these approaches seeks to mitigate the societal challenges of advanced AI in different ways\nand each confronts significant challenges to its viability and success.\n2\nThe Need for International Governance\nPowerful AI systems have the potential to transform society, economics and politics in fundamental\nways. Because of characteristics like its high barriers to development/utilization and the possibility\nof cross-border use, it is possible that harnessing AI\u2019s potential for global benefit and managing its\nrisks could require governance functions at the international level.\nPromoting Global Benefits\nAccess to appropriate AI technology might greatly promote prosperity and stability [50], but the\nbenefits might not be evenly distributed or focused on the greatest needs of underrepresented\ncommunities or the developing world. Inadequate access to internet services, computing power, or\navailability of machine learning training/expertise will also hinder certain groups\u2019 ability to benefit\nfully from AI advances.\nInternational institutions have long sought to support sustainable global development. International\nefforts to build consensus on AI opportunities\u2014especially addressing barriers to their effective use\nglobally\u2014could support efforts to distribute and enable access to AI. On top of facilitating access,\nthis could also include building capacity to benefit from AI through education, infrastructure, and\nlocal commercial ecosystems. In some cases, international collaborations (including public-private\n6See, e.g., [10; 11; 13; 22; 25; 33; 46]\n4\npartnerships) to develop frontier AI systems that are suited to the needs to underserved communities\nmay also be appropriate.\nInconsistent national regulations could also slow the development and deployment of AI, as de-\nvelopers of powerful AI technology may be unwilling to export to countries with inconsistent or\nunsuitable technology governance.7 International efforts to set safety norms and standards could\nhelp coordinate governance in a way that supports innovation and serves a broad set of interests.\nManaging Shared Risks\nAdvanced AI capabilities may also create negative global externalities. AI systems today are already\ncapable of not just progressing drug discovery and development, but also of (re)inventing dangerous\nchemicals [48] and solving foundational problems in synthetic biology [24]. Scientific capabilities\nlike these could be weaponized by malicious actors for use worldwide. AI may also be used to\ncreate potent cyberweapons that can generate code, scan codebases for vulnerabilities, and engineer\npolymorphic malware in ways that threaten critical infrastructure [38; 41]. Existing AI systems\nalready pose mis- and dis-information issues, and the introduction of more advanced systems is\nleading malicious actors to explore more sophisticated methods of information warfare.8 Furthermore,\nbuilding systems that act as intended in novel circumstances is a challenging problem that may only\ngrow more difficult [3; 20]. As systems get increasingly capable, there will be greater incentives\nto deploy them in higher stakes domains where accidents could have serious global consequences\n[5; 9].\nImplementing protocols for responsible development and deployment will help address these risks\nof accident and misuse,9 on top of measures targeted at specific downstream issues.10 However,\ncross-border access to AI products and the cross-border effects of misuse and accidents suggests that\nnational regulation may be ineffective for managing the risks of AI even within states. States will\ninevitably be impacted by the development of such capabilities in other jurisdictions.\nTo further the international adoption of safety protocols for advanced models, it would be useful\nto build consensus on risks and how they can be mitigated, and set safety norms and standards\nand support their implementation to help developers and regulators with responsible development\nand use. International efforts to conduct or support AI safety research may be beneficial, if it can\nincrease the rate of safety progress or the reach of its outputs.\nIn the longer term, continued algorithmic and hardware progress could make systems capable of\ncausing significant harm accessible to a large number of actors, greatly increasing the governance\nchallenges.11 In this case, the international community might explore measures like controlling AI\ninputs (although the dual-use/general purpose nature of the technology creates significant tradeoffs\nto doing so) and developing and/or enabling safe forms of access to AI.\nThe significant geopolitical benefits of AI development may disincline states to adequately regulate\nAI: arguments about national competitiveness are already raised against AI regulation,12 and such\npressures may strengthen alongside AI progress. We may eventually need international agreements\n7In addition to compliance costs, they may be concerned about enabling misuse, or the theft of proprietary information\n[6].\n8See, e.g, [7; 17]\n9Potential protocols include: training methods that restrict the dangerous capabilities and increase the reliability\nof systems, subjecting AI systems to risk assessments that ascertain their propensity to cause harm before training or\ndeployment, deployment protocols that secure systems against misuse or the exfiltration of its parameters, post-deployment\nmonitoring to identify and respond to unforeseen risks. See [4].\n10Such as the Digital Services Act for disinformation, and treaties targeting chemical and biological weapons.\n11According to [14] and [21], the compute costs of training a model of a fixed performance level decreases approximately\ntenfold every 2 years.\n12See, e.g., the arguments discussed in [44]\n5\nthat address these geopolitical risks, with institutions that can monitor compliance where feasible.13\nEfforts to control AI inputs may be useful to enable non-proliferation of potentially dangerous\ncapabilities and increase the technical feasibility of monitoring. More speculatively, efforts to\ndevelop frontier AI collectively or distribute and enable access and its benefits could incentivize\nparticipation in a governance regime.\n\u2014\nThe institutional functions identified above can be summarized and grouped into the following two\nbroad categories.\nI.\nScience and Technology Research, Development and Diffusion\n\u2022\nConduct or support AI safety research: Research and develop of measures to\nreduce the risks of AI misuse and accidents stemming from system characteristics\nlike dangerous capabilities and unreliability/misalignment. This includes work\non understanding and evaluating these characteristics and the threats they pose,\ntraining methods to reduce and manage risky behaviors, and examining safe\ndeployment protocols appropriate to different system [3; 20].\n\u2022\nBuild consensus on opportunities and risks: Further international understanding\nof the opportunities and challenges created by advanced AI and possible strategies\nfor mitigating the risks.\n\u2022\nDevelop frontier AI: Build cutting-edge AI systems.\n\u2022\nDistribute and enable access to cutting edge AI: Facilitate access to cutting-edge\nsystems and increase absorptive capacity through education, infrastructure, and\nsupport of the local commercial ecosystem.\nII.\nInternational Rulemaking and Enforcement\n\u2022\nSet safety norms and standards: Establish guidelines and standards around how\nAI can be developed, deployed and regulated to maximize benefit and minimize\nrisks.\n\u2022\nSupport implementation of standards: Provide assistance for the implementa-\ntion of established guidelines and standards.\n\u2022\nMonitor compliance: Conduct audits /evaluations and issue certifications / li-\ncenses to ensure adherence to international standards and agreements.\n\u2022\nControl AI inputs: Manage or monitor models, compute, data and other ingredi-\nents of potentially dangerous technologies.\nInternational bodies already perform some of these functions.14 The OECD\u2019s AI Principles and\nAI Policy Observatory work, the ITU\u2019s AI for Good initiative, and expert reports from the Global\nPartnership on AI\u2019s Working Group on Responsible AI are early efforts at building consensus on AI\nopportunities and risks. Relatedly, the UK\u2019s proposed Foundation Model Taskforce [18] and the US\u2019s\nproposed Multilateral AI Research Institute could emerge as multilateral efforts to conduct AI safety\nresearch, or potentially even develop frontier AI systems, though both are in exploratory phases.15\nAlongside lawmaking efforts like the EU\u2019s AI Act and the Council of Europe\u2019s Convention on AI,\nHuman Rights and Democracy, we have seen early norm and standard setting efforts from ISO/IEC,\nbut little in the way of implementation support, oversight or certification. In terms of controlling\n13Monitoring can vary significantly in intrusiveness and effectiveness: while it will be highly difficult to implement\nadequate monitoring across major geopolitical divides, the safety of advanced systems could be a shared interest of major\npowers and a regime to address risk factors from smaller-scale geopolitical competition may be feasible.\n14See [49] for a more thorough overview.\n15The amount of funding required to stay on the cutting-edge of AI capabilities is significant. See, e.g., [39].\n6\ndangerous inputs: computing resources have been targeted by US, Japanese and Dutch export controls\nthat prevent the sale of certain AI chips and semiconductor manufacturing equipment to China [1].\n3\nInternational Institutions for Advanced AI\nWe have outlined several AI governance functions that might be needed at an international level, and\nshown that only a limited number of these are currently being performed by existing institutions. In\nthis section, we discuss how functional gaps may be filled.\nThe functions could be split in multiple ways across institutions: drawing on existing international\norganizations and proposals, we describe four idealized models. We note that the models described\nin this section describe roles that could be filled by existing or new institutions. Participants in these\ninstitutions could include governments, non-governmental organizations, the private sector, and\nacademia. Table 1 summarizes the previous analysis and the functions of the institutions we discuss.\nTable 1 (repeated): A mapping of international institutional functions, governance objectives and\nmodels.\nFunction \u2192\nScience and Technology\nResearch, Development and Diffusion\nInternational Rulemaking and Enforcement\nObjective /\ninstitutions \u2193\nConduct\nor Support\nAI Safety\nResearch\nBuild\nConsensus on\nOpportunities\nand Risks\nDevelop\nFrontier AI\nDistribute\nand Enable\nAccess to AI\nSet Safety\nNorms and\nStandards\nSupport\nImplemen-\ntation of\nStandards\nMonitor\nCompliance\nControl\nInputs\nSpreading\nBeneficial\nTechnology\nNo\nYes\nMaybe\nYes\nNo\nNo\nNo\nNo\nHarmonizing\nRegulation\nNo\nNo\nNo\nNo\nYes\nYes\nNo\nNo\nEnsuring Safe\nDevelopment\nand Use\nMaybe\nYes\nMaybe\nMaybe\nYes\nYes\nMaybe\nMaybe\nManaging\nGeopolitical\nRisk Factors\nNo\nNo\nMaybe\nMaybe\nNo\nNo\nYes\nYes\nExisting Int\u2019l\nInstitutional\nEfforts\nOECD,\nGPAI, G7,\nITU\nISO/IEC\nSemi-\nconductor\nExport\nControls\nPossible\nInstitution\nAI Safety\nProject\nCommission\non Frontier AI\nFrontier AI Collaborative\nAdvanced AI Governance Agency\nKey\nchallenges\nModel\naccess;\ndiverting\ntalent\nPoliticization;\nscientific\nchallenges\nManaging dual-use\ntechnology; education,\ninfrastructure and\necosystem obstacles\nIncentivizing participation;\nquickly changing risk landscape;\nmaintaining appropriate scope\n3.1\nCommission on Frontier AI: Fostering Scientific Consensus\nThere have been several recent proposals of an intergovernmental body to develop expert consensus\non the challenges and opportunities presented by advanced AI.16 Existing institutions like the\nIntergovernmental Panel on Climate Change (IPCC), the Intergovernmental Science-Policy Platform\n16See, e.g., [27; 33].\n7\non Biodiversity and Ecosystem Services (IPBES) and the Scientific Assessment Panel (SAS), which\nstudies ozone depletion under the Montreal Protocol, provide possible models for an AI-focused\nscientific institution. Like these organizations, the Commission on Frontier AI could facilitate\nscientific consensus by convening experts to conduct rigorous and comprehensive assessments of\nkey AI topics, such as interventions to unlock AI\u2019s potential for sustainable development, the effects\nof AI regulation on innovation, the distribution of benefits, and possible dual-use capabilities from\nadvanced systems and how they ought to be managed.\nFunctions and Motivation\nInternational consensus on the opportunities and risks from advanced AI has the potential to facilitate\neffective action addressing them, for example, by engendering a shared desire for the development\nand adoption of effective risk mitigation strategies.\nCurrently, there is significant disagreement even among experts about the different opportunities and\nchallenges created by advanced AI,17 and this lack of consensus may worsen over time as the effects\nof AI systems increase in scale and number, hindering collective action on the scale necessary to\nensure that AI is developed for the benefit of all. Furthermore, there are several challenges from\nadvanced AI that may require international action before risks materialize, and the lack of a widely\naccepted account or even mapping of AI development trajectories makes it difficult to take such\npreparatory actions. Facilitating consensus among an internationally representative group of experts\ncould be a promising first step to expanding our levels of confidence in predicting and responding to\ntechnological trends.\nChallenges and Risks\nScientific challenges of understanding risks on the horizon:\nUnderstanding frontier AI risks\nand their mitigation is technically challenging. The nature of future AI capabilities and their impact\nis difficult to predict, especially given the fast rate of progress. To increase chances of success, a\nCommission should foreground scientific rigor and the selection of highly competent AI experts\nwho work at the cutting edge of technological development and who can continually interpret the\never-changing technological and risk landscape.\nUnfortunately, there is a relative lack of existing scientific research on the risks of advanced AI.18 To\naddress the lack of existing scientific research, a Commission might undertake activities that draw and\nfacilitate greater scientific attention, such as organizing conferences and workshops and publishing\nresearch agendas. It may be helpful to write a foundational \u201cConceptual Framework\u201d\u2014following the\nexample of the IPBES\u2014to create a common language and framework that allows the integration of\ndisparate strands of existing work and paves the way for future efforts [12].\nPoliticization:\nA Commission on Frontier AI would benefit from, if not require, a clear buffer\nbetween experts charged with developing consensus narratives around the risks and opportunities of\nAI and policymakers acting on the political and economic interests of their states, which might push\npolicies in different directions. The scientific understanding of the impacts of AI should ideally be\nseen as a universal good and not be politicized.\nMembership structure can affect a Commission\u2019s impartiality and legitimacy: ideally, there would be\nbroad geographic representation in the main decisionmaking bodies, and a predominance of scientific\nexperts in working groups.19 Unfortunately, given the uncertain and controversial nature of advanced\n17See, for example, the disagreement around whether advanced AI could pose an extinction risk: [9; 31].\n18The recent IPCC assessment, for reference, was written by 234 scientists from 66 states and drew on 14,000 scientific\npapers.\n19If legitimacy is the primary concern, the Commission might adopt the IPCC\u2019s innovation of writing key documents\nby consensus, balancing inclusion (states\u2019 representatives and scientists review, discuss and approve the report line by\nline) and scientific rigor (all suggested amendments must be consisted with working group\u2019s scientific report that is being\nsummarized) [36].\n8\nAI risks and opportunities, representation may trade off against a Commission\u2019s ability to overcome\nscientific challenges and generate meaningful consensus.20 In addition to striking the correct balance\nin membership, a Commission should carefully scope the subject matter of their research\u2014it may, for\nexample, adopt the IPCC\u2019s objective of being \u201cpolicy-relevant\u201d without being \u201cpolicy-prescriptive.\u201d\nAssessment\nThe objective of a Commission on Frontier AI is worthwhile in most circumstances, but the scientific\nchallenges and potential of politicization imply that a Commission\u2014especially one that aims at broad\npolitical representation\u2014may not be able to build scientific consensus effectively. The extraordinary\npace of technological change may require more nimble policy responses, such as less institutionalized\nand politically authoritative scientific advisory panels on advanced AI.\n3.2\nAdvanced AI Governance Organization: Promoting Norms and Standards,\nProviding Implementation Support, Monitoring Compliance\nAs discussed above, certain misuse and accident risks of advanced AI systems may pose significant\nglobal threats, and international efforts aimed at managing these risks could be worthwhile. An\nintergovernmental or multi-stakeholder organization could perform a variety of governance functions\nfurthering the regulation of such systems, in particular norm and standard setting, implementation\nassistance, and perhaps monitoring compliance with governance frameworks.21\nFunctions and Motivation\nWe identify two main objectives for an Advanced AI Governance Organization. How much emphasis\nit should place on each depends on the challenges it aims to address.\nObjective 1: Internationalizing and harmonizing AI regulation.\nRegulatory regimes that set\nstandards and provide implementation support may help ensure that powerful AI capabilities do not\npose misuse or accident risks. Standard setting would facilitate widespread international adoption by:\n1) reducing the burden on domestic regulators to identify necessary safety regulations and protocols,\n2) generating normative pressure for safety protocol adoption, and 3) reducing frictions around the\ndevelopment of international frameworks. Implementation support would assist the establishment\nand maintenance of regulatory regimes meeting these frameworks. Examples of organizations\nthat perform similar functions include the Financial Action Task Force (FATF), the International\nTelecommunication Union (ITU) and the International Civil Aviation Organization (ICAO).\nThe same functions are useful for harmonizing regulation: international standard setting would\nreduce cross-border frictions due to differing domestic regulatory regimes. (It is possible that\nfuture regulations will limit access to powerful AI technologies in jurisdictions with inadequate AI\ngovernance.) Implementation support would help reduce obstacles to countries meeting international\nstandards and therefore enable greater access to advanced AI.\nObjective 2: Monitoring compliance.\nWhere states have incentives to undercut each other's\nregulatory commitments, international institutions may be needed to support and incentivize best\npractices. That may require monitoring standards compliance. At the least intrusive end of the\nspectrum is self-reporting of compliance with international standards (as in the Paris Agreement\u2014see\nproposals for self-reporting/registration of training runs [4]). Organizations like the FATF, ICAO, and\nthe International Maritime Organization (IMO) take a somewhat more intrusive approach, monitoring\njurisdictions to ensure they adopt best practice regulations, and in some cases checking on the\nenforcement of domestic regulations embodying international standard. In the case of advanced\n20If it follows the IPCC model, experts will be nominated by member states, but there will not be a robust climate\nscience discipline to buffer against political interests.\n21See, e.g., [2; 10; 13; 28; 46] for discussions and proposals of an institution of this type.\n9\nAI, some observers have asked whether more intrusive forms of international oversight might be\nnecessary, including detection and inspections of large data centers (partly analogous to IAEA\nsafeguards). The more intense and intrusive any monitoring, the more challenging it may be to get to\nconsensus [35; 43].\nChallenges and Risks\nSpeed and comprehensiveness in standard setting:\nOne challenge for a Governance Organization\nis that standard setting (especially in an international and multistakeholder context) tends to be a slow\nprocess, while the rapid and unpredictable nature of frontier AI progress may require more rapid\ninternational action. A Governance Organization may need to partner with faster-moving expert\nbodies and expedited standard-setting approaches. The breadth of membership may also represent a\ntrade-off between speed and diversity of perspectives. Broader membership may be important where\nlong-term consensus is important, while urgent risks may need to be addressed at first by smaller\ngroups of frontier AI states, or aligned states with relevant expertise.\nIncentivizing participation:\nThe impact of a Governance Organization depends on states adopting\nits standards and/or agreeing to monitoring. Broad agreement (or agreement among frontier AI states\nat least) about the risks that standards and monitoring address and financial and technical support for\nstandards\u2019 implementation may help induce states\u2019 participation. Many states\u2014even those that are\nnot full members of the organization\u2014adopt FATF standards because they view them as in their own\ninterests [45]. Other AI-specific incentives for participation include conditioning on participation\naccess to AI technology (possibly from a Frontier AI Collaborative) or computing resources.22 States\nmight also adopt import restrictions on AI from countries that are not certified by a Governance\nOrganization\u2014similar, for instance, to the way states prohibit flights from jurisdictions without\nICAO-certification from entering their airspace.\nIn the more distant case of high stakes agreements governing AI development by states (such as\narms control treaties), some states may be especially reluctant to join due to fear of clandestine\nnoncompliance by other states. They may also worry that international inspections could compromise\nstate secrets to the benefit of adversaries (which information security protocols could address in\npart). Again, the current reliance of advanced AI development on significant computing resources\nmay make it easier to track significant AI efforts.23 Automated (even AI-enabled) monitoring may\nallow closer inspection of large training runs without compromising secrets. Such measures would\nlikely hinge on negotiated verification regimes rather than national technical means\u2014and negotiating\nverification is always fraught (e.g., in the case of the Chemical Weapons Convention) and often\nunsuccessful (e.g., in the case of the Biological Weapons Convention)[15; 23].\nScoping challenges:\nUnlike many other technologies\u2014from nuclear resources to aviation\u2014AI\nis already broadly deployed and used by billions of people every day. To operate efficiently and at\nappropriate scale, a Governance Organization should focus primarily on advanced AI systems that\npose significant global risks, but it will be difficult in practice to decide on the nature and sophisti-\ncation of AI tools that should be broadly available and uncontrolled versus the set of systems that\nshould be subject to national or international governance. The rapid evolution of these technologies\ncompounds the problem, as the technological frontier is advancing quickly, and models that were\n\u201cfrontier\u201d a year ago are now both outdated and widely available.\n22The cloud compute industry and the underlying semiconductor supply chain are concentrated in a small number of\ncountries.\n23Oversight of data centers may allow the detection of large training runs that are subject to international controls. See\n[8; 35].\n10\nAssessment\nIf advanced AI poses misuse and accident risks of a global scope, and unilateral technical defenses\nare not sufficient to protect against them, an international Governance Organization may be valuable.\nHowever, its effectiveness will depend on its membership, governance and standard-setting processes.\nIt may be important for governance to apply to all countries, and particularly to those whose firms\nare on the frontier of AI development. Yet, aligned countries may seek to form governance clubs, as\nthey have in other domains. This facilitates decision-making, but may make it harder to enlist other\ncountries later in the process. It is unclear what institutional processes would satisfy the demands of\nlegitimacy and effectiveness, and incentivize the participation of important groups of stakeholders.\n3.3\nFrontier AI Collaborative: Enabling International Access to AI\nPolicymakers and pundits have also proposed collaborations to develop and distribute cutting-\nedge AI systems, or to ensure such technologies are accessible to a broad international coalition\n[22; 25; 34; 40]. Given the significant cost of developing advanced AI systems, a Frontier AI\nCollaborative could take the form of an international private-public partnership that leverages existing\ntechnology and capacity in industry, for example by contracting access to or funding innovation in\nappropriate AI technology from frontier AI developers. Such an organization could draw inspiration\nfrom international public-private partnerships like Gavi - the Vaccine Alliance or The Global Fund to\nFight AIDS, Tuberculosis and Malaria; as well as international organizations that hold and control\npowerful technologies, like the IAEA\u2019s nuclear fuel bank [32] or the Atomic Development Authority\nthat was proposed following WWII [51].\nFunctions and Motivation\nA Frontier AI Collaborative could be designed to spread beneficial technology or serve as a channel\nfor legitimate international access to advanced AI.\nSpreading beneficial technology:\nA Collaborative could be established to ensure the benefits of\ncutting-edge AI reach groups that are otherwise underserved by AI development. One motivation for\nthis objective is that the resources required to develop advanced systems make their development\nunavailable to many societies. This may result in technologies being inadequately designed for and\nsupplied to groups that may benefit most from them for a variety of reasons:\n1.\nSystems developed by private actors may not adequately cater to all societies\nor demographics: they may not reflect the right values, have the right language\ncapabilities, or work efficiently in diverse geographies[30].\n2.\nPrivate firms may not price their products in ways that allow for equitable or broad\ndistribution of benefits.\n3.\nIn order to protect proprietary information, private AI firms may not grant deep\naccess to their models (e.g. they may restrict API access to prevent model imitation\n[42]), which could preclude the development of use cases with significant social\nbenefit.\nA Collaborative could acquire or develop and then distribute AI systems to address these gaps, pooling\nresources from member states and international development programs, working with frontier AI\nlabs to provide appropriate technology, and partnering with local businesses, NGOs, and beneficiary\ngovernments to better understand technological needs and overcome barriers to use.24 It could enable\nthe development of technology that better caters to the underserved, [30] price access to AI models\n24For example, Gavi promotes immunization e.g. by funding innovation, and negotiating bulk contracts with phar-\nmaceutical companies (especially advanced market commitments) for vaccination programs in low-income countries\n[16].\n11\nin a way that is equitable, provide education and build infrastructure to allow the effective utilization\nof AI technology, and set a paradigm for responsible and inclusive AI development. By pooling\nthe resources of multiple parties towards these ends (including safety talent, which is currently very\nscarce in the AI community), one or more of the aims could potentially be pursued more quickly and\neffectively than under the status quo.\nFacilitating legitimate international access to powerful AI:\nMore speculatively, a sufficiently\nambitious, responsible and legitimately governed AI Collaborative could further AI governance\nobjectives and reduce geopolitical instability amidst fierce AI competition among states. For example,\nmembership in a Collaborative and access to its safe technology could be offered as an incentive\nfor countries to participate in a governance regime that enforces responsibility (such as agreements\nto enact stricter regulation, or restrict military AI development). The existence of a technologically\nempowered neutral coalition may also mitigate the destabilizing effects of an AI race between\nstates, by reducing the strategic consequences of one party falling behind or moderating the power\nconcentrated among competing powers.\nIn addition, the Collaborative\u2019s technology could be used to increase global resilience to misused\nor misaligned AI systems by giving experts a head start in studying the kinds of threats likely to be\nposed by other AI systems, and by being deployed for \u201cprotective\u201d purposes such as fixing security\nvulnerabilities in critical infrastructure, detecting and counteracting disinformation campaigns,\nidentifying misuse or failures of deployed systems, or monitoring compliance with AI regulations.\nThis would be especially useful in scenarios where sharply falling training costs (due to algorithmic\nprogress and Moore\u2019s law) means the ability to train dangerous models is widely spread.\nChallenges and Risks\nObstacles to benefiting from AI access:\nIt is likely to be difficult to meaningfully empower\nunderserved populations with AI technology, as the obstacles to their benefiting from AI run much\ndeeper than access alone. Any Collaborative whose primary objective is global benefit needs to be\nadequately integrated into the global development ecosystem and set up with significant capacity\nor partnerships for activities beyond AI development such as: understanding the needs of member\ncountries, building absorptive capacity through education and infrastructure, and supporting the\ndevelopment of a local commercial ecosystem to make use of the technology [29; 47]. The resources\nrequired to overcome these obstacles is likely to be substantial, and it is unclear whether such a\nCollaborative would be an effective means of promoting development.\nDiffusion of dual-use technologies:\nAnother challenge for the Collaborative would be managing\nthe risk of diffusing dangerous technologies. On the one hand, in order to fulfill its objectives, the\nCollaborative would need to significantly promote access to the benefits of advanced AI (objective\n1), or put control of cutting-edge AI technology in the hands of a broad coalition (objective 2). On\nthe other hand, it may be difficult to do this without diffusing dangerous AI technologies around the\nworld, if the most powerful AI systems are general purpose, dual-use, and proliferate easily.25 This\nis especially the case if the Collaborative aims to deploy cutting-edge general purpose systems to\nmanage AI risks: the kinds of systems (and their underlying source code and algorithms) capable\nof meaningfully protecting against dangerous AI or furthering governance objectives may pose an\nexceptional misuse risk, as they will likely be engineered from highly capable, general purpose\nmodels.\nTo address such a challenge, it would be important for the Collaborative to have a clear mandate\nand purpose. Members of a Collaborative would need to have a strong understanding of the risks\nof the models being developed now and in the future, and their implications for model distribution,\norganization security (especially restrictions on the movement of Collaborative model weights), and\nother activities that may impact their ability to benefit from the Collaborative. Only by doing this\n25For example: it may be difficult to protect broadly-deployed models from imitation, and it may be difficult to secure\nthe deployment pipeline from attempts to copy model weights.\n12\nwould the Collaborative be able to consistently implement the necessary controls to manage frontier\nsystems. It may also be necessary to exclude from participation states who are likely to want to use\nAI technology in non-peaceful ways, or make participation in a governance regime the precondition\nfor membership.\nAssessment\nA Frontier AI Collaborative may indeed be a viable way of spreading AI benefits. However, the\nsignificant obstacles to societies benefiting from AI access raise questions about its competitiveness\n(relative to other development initiatives) as a means of promoting the welfare of underserved\ncommunities.\nThe viability of a Collaborative as a site of legitimately controlled technology also unclear: it depends\non whether a balance can be struck between legitimately pursuing technologies powerful enough to\npositively affect international stability, and managing the proliferation of dangerous systems.\n3.4\nAI Safety Project: Conducting Technical Safety Research\nThe final model we discuss is an international collaboration to conduct technical AI safety research26\nat an ambitious scale.27\nTthe Safety Project would be modeled after large-scale scientific collaborations like ITER and CERN.\nConcretely, it would be an institution with significant compute, engineering capacity and access to\nmodels (obtained via agreements with leading AI developers), and would recruit the world\u2019s leading\nexperts in AI, AI safety and other relevant fields to work collaboratively on how to engineer and\ndeploy advanced AI systems such that they are reliable and less able to be misused. CERN and ITER\nare intergovernmental collaborations; we note that an AI Safety Project need not be, and should be\norganized to benefit from the AI Safety expertise in civil society and the private sector.\nFunctions and Motivation\nThe motivation behind an international Safety Project would be to accelerate AI safety research\nby increasing its scale, resourcing and coordination, thereby expanding the ways in which AI can\nbe safely deployed, and mitigating risks stemming from powerful general purpose capabilities.28\nResearchers\u2014including those who would not otherwise be working on AI safety\u2014could be drawn by\nits international stature and enabled by the project\u2019s exceptional compute, engineers and model access.\nThe Project would become a vibrant research community that benefits from tighter information flows\nand a collective focus on AI safety. The Project should also have exceptional leaders and governance\nstructures that ensure its efforts are most effectively targeted at critical questions on the path to safer\nAI systems.\nBecause perceptions of AI risk vary around the world, such an effort would likely be spearheaded by\nfrontier risk-conscious actors like the US and UK governments, AGI labs and civil society groups. In\nthe long run, it would be important for membership to be broad to ensure its research is recognized\nand informs AI development and deployment around the world.29\n26This could include work on understanding and evaluating characteristics of systems such as alignment/reliability and\ndangerous capabilities, training methods to reduce and manage these characteristics, and deployment protocols (such as\nsystem security, monitoring, accident-response) that are appropriate to different system characteristics.\n27See, e.g., [11; 19]\n28Being a public good, AI safety may be underfunded by default, which the Safety Project would address as a site of\ncollective contribution.\n29While safety-relevant insights should be publicized for international use, other innovations with commercial value can\nbe collectively owned by or affordably licensed to member states to incentivize broad participation. See, e.g., CERN\u2019s\napproach to this [26].\n13\nChallenges and Risks\nPulling safety research away from frontier developers:\nOne potential effect of this model is\nthat it diverts safety research away from the sites of frontier AI development. It is possible that\nsafety research is best conducted in close proximity to AI development to deepen safety researchers\u2019\nunderstanding of the processes and systems they are trying to make safe and to ensure there is\nadequate safety expertise in-house. This risk could be addressed by offering safety researchers within\nAI labs dual appointments or advisory roles in the Project, and may become less of an issue if the\npractice of AI safety becomes institutionalized and separated from research and development.\nSecurity concerns and model access:\nIn order to be effective, participants in the Project need to\nhave access to advanced AI models, which may allow them to illegally copy the model\u2019s weights,\nclone the model via access to its outputs [42], or understand how it could be replicated (by determining\nits architecture or training process). Given the importance of these assets to the business interests of\nfrontier labs, it may be difficult to negotiate agreements where adequate model access is granted. It\nmay also lead to the diffusion of dangerous technologies.\nThis issue may be addressed by restricting membership in the Safety Project and by information\nsecurity measures. In particular, it may be possible to silo information, structure model access, and\ndesign internal review processes in such a way that meaningfully reduces this risk while ensuring\nresearch results are subject to adequate scientific scrutiny. Certain types of research, such as the\ndevelopment of model evaluations and red-teaming protocols, can proceed effectively with API\naccess to the models, while others such as mechanistic interpretability will require access to the\nmodel weights and architectures, but may not need to work with the latest (and therefore most\nsensitive) models [37].\nAssessment\nTechnical progress on how to increase the reliability of advanced AI systems and protect them from\nmisuse will likely be a priority in AI governance. It remains to be seen whether\u2014due to issues\nof model access and the allocation of experts between a Safety Project and sites of frontier AI\ndevelopment\u2014an AI Safety Project will be the most effective way of pursuing this goal.\n3.5\nCombining Institutional Functions\nWe can imagine institutions taking on the role of several of the models above. For example, the\nCommission on Frontier AI and the AI Safety Project make an obvious pairing: a Commission\ncould scale up research functions to supplement the synthesis and consensus-building efforts, or a\nProject could conduct synthesis work in the course of its activities and gradually take on a consensus-\nestablishing role. A Frontier AI Collaborative would also likely conduct safety research, and could\neasily absorb additional resourcing to become a world-leading Safety Project.\n4\nConclusion\nThis paper has outlined several reasons why the world may want to expand existing initiatives in\nAI governance and safety and discussed the strengths and limitations of four possible institutional\nmodels to address these needs.\nTo better harness advanced AI for global benefit, international efforts to help underserved societies\naccess and use advanced AI systems will be important. A Frontier AI Collaborative that acquires\nand distributes AI systems could be helpful, if it can effectively enable underserved groups to take\nfull advantage of such systems. A Commission on Frontier AI could help identify the areas where\ninternational efforts can most effectively achieve these goals, if it can prevent the politicization of its\nwork. Relatedly, it will be important for governance approaches around the world to be coordinated,\n14\nso as to reduce frictions to innovation and access: an Advanced AI Governance Organization that\nsets international standards for governance of the most advanced models could facilitate this.\nTo manage global risks from powerful AI systems, effective AI governance regimes may be needed\naround the world. An Advanced AI Governance Organization that establishes governance frameworks\nfor managing global threats from advanced systems and helps with their implementation may help\ninternationalize effective regulatory measures, but it may be difficult to establish reliable standards if\nAI progress continues at the present rate, and also difficult to incentivize adoption of an Organization\u2019s\nstandards if there is a lack of global consensus on AI risks. A Commission on Frontier AI could\nalso support governance efforts by building scientific consensus around AI risks and their mitigation,\nalthough its task of providing a scientifically credible and internationally recognized account of a\nquickly changing risk landscape will be challenging, especially given the relative lack of existing\nscientific research on the topic. An AI Safety Project could accelerate the rate at which technical\nmethods of mitigating AI risks are developed\u2014provided it can overcome its efficiency and model\naccess hurdles, and a Frontier AI Collaborative\u2019s technology might be used to increase global\nresilience to misused or misaligned AI systems. More speculatively, the functions of a Governance\nOrganization and Collaborative could in some cases counteract the geopolitical factors exacerbating\nAI risks.\nThe taxonomy of functions we have presented is not exhaustive, nor do we argue that our institutional\ngrouping is the most promising. Given the immense global opportunities and challenges presented\nby AI systems that may be on the horizon, the topic of international institutions for AI governance\ndemands much greater analytical and practical attention.\nAcknowledgements\nWe are grateful to the following people for discussion and input: Michael Aird, Jeff Alstott, Jon\nBateman, Alexandra Belias, Dorothy Chou, Jack Clark, Lukas Finnveden, Iason Gabriel, Ben\nGarfinkel, Erich Grunewald, Oliver Guest, Jackie Kay, Noam Kolt, Sebastien Krier, Lucy Lim,\nNicklas Lundblad, Stewart Patrick, George Perkovich, Toby Shevlane, Kent Walker and Ankur Vora.\nWe would also like to thank participants of the September 2022 and June 2023 IGOTAI Seminars, in\nwhich early work was discussed.\nReferences\n[1] The Netherlands joins the U.S. in restricting semiconductor exports to China. Allen Overy,\nMarch 2023. URL: https://www.allenovery.com/en-gb/global/news-and-insight\ns/publications/the-netherlands-joins-the-us-in-restricting-semiconduct\nor-exports-to-china.\n[2] Sam Altman, Greg Brockman, and Ilya Sutskever. Governance of superintelligence. OpenAI,\n2023. URL: https://openai.com/blog/governance-of-superintelligence.\n[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan\nMan\u00e9. Concrete Problems in AI Safety, July 2016. arXiv:1606.06565 [cs]. URL: http:\n//arxiv.org/abs/1606.06565.\n[4] Markus Anderljung, Joslyn Barnhart, Jade Leung, Anton Korinek, Cullen O\u2019Keefe, Jess\nWhittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang,\nTantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sarah Hooker, Eric Horvitz,\nNoam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf.\nFrontier AI Regulation: Managing Emerging Risks to Public Safety, 2023.\n[5] Zachary Arnold and Helen Toner. AI Accidents: An Emerging Threat. Center for Security and\nEmerging Technology, 2021. URL: https://cset.georgetown.edu/publication/ai-a\nccidents-an-emerging-threat/.\n15\n[6] Titus Awokuse and Hong Yin. Do Stronger Intellectual Property Rights Protection Induce More\nBilateral Trade? Evidence from China\u2019s Imports. World Development, 38(8), 2010.\n[7] Shannon Bond. Fake viral images of an explosion at the Pentagon were probably created by AI.\nNPR, May 2023. URL: https://www.npr.org/2023/05/22/1177590231/fake-viral-i\nmages-of-an-explosion-at-the-pentagon-were-probably-created-by-ai.\n[8] Miles Brundage, Girish Sastry, Lennart Heim, Haydn Belfield, Julian Hazell, Markus An-\nderljung, Shahar Avin, Jade Leung, Cullen O\u2019Keefe, and Richard Ngo. Computing Power and\nthe Governance of Artificial Intelligence.\n[9] Statement on AI Risk. Center for AI Safety, 2023. URL: https://www.safe.ai/statemen\nt-on-ai-risk.\n[10] Rumman Chowdhury. AI Desperately Needs Global Oversight. Wired, 2023. Section: tags.\nURL: https://www.wired.com/story/ai-desperately-needs-global-oversight/.\n[11] Stephen Dubner. Satya Nadella\u2019s Intelligence Is Not Artificial. URL: https://freakonomi\ncs.com/podcast/satya-nadellas-intelligence-is-not-artificial/.\n[12] Sandra D\u00edaz, Sebsebe Demissew, Julia Carabias, Carlos Joly, Mark Lonsdale, Neville Ash,\nAnne Larigauderie, Jay Ram Adhikari, Salvatore Arico, Andr\u00e1s B\u00e1ldi, Ann Bartuska, Ivar An-\ndreas Baste, Adem Bilgin, Eduardo Brondizio, Kai MA Chan, Viviana Elsa Figueroa, Anantha\nDuraiappah, Markus Fischer, Rosemary Hill, Thomas Koetz, Paul Leadley, Philip Lyver,\nGeorgina M Mace, Berta Martin-Lopez, Michiko Okumura, Diego Pacheco, Unai Pascual,\nEdgar Selvin P\u00e9rez, Belinda Reyers, Eva Roth, Osamu Saito, Robert John Scholes, Nalini\nSharma, Heather Tallis, Randolph Thaman, Robert Watson, Tetsukazu Yahara, Zakri Abdul\nHamid, Callistus Akosim, Yousef Al-Hafedh, Rashad Allahverdiyev, Edward Amankwah,\nStanley T Asah, Zemede Asfaw, Gabor Bartus, L Anathea Brooks, Jorge Caillaux, Gemedo\nDalle, Dedy Darnaedi, Amanda Driver, Gunay Erpul, Pablo Escobar-Eyzaguirre, Pierre Failler,\nAli Moustafa Mokhtar Fouda, Bojie Fu, Haripriya Gundimeda, Shizuka Hashimoto, Floyd\nHomer, Sandra Lavorel, Gabriela Lichtenstein, William Armand Mala, Wadzanayi Mandi-\nvenyi, Piotr Matczak, Carmel Mbizvo, Mehrasa Mehrdadi, Jean Paul Metzger, Jean Bruno\nMikissa, Henrik Moller, Harold A Mooney, Peter Mumby, Harini Nagendra, Carsten Nesshover,\nAlfred Apau Oteng-Yeboah, Gy\u00f6rgy Pataki, Marie Rou\u00e9, Jennifer Rubis, Maria Schultz,\nPeggy Smith, Rashid Sumaila, Kazuhiko Takeuchi, Spencer Thomas, Madhu Verma, Youn\nYeo-Chang, and Diana Zlatanova. The IPBES Conceptual Framework \u2014 connecting nature\nand people. Current Opinion in Environmental Sustainability, 14:1\u201316, June 2015. URL:\nhttps://www.sciencedirect.com/science/article/pii/S187734351400116X,\ndoi:10.1016/j.cosust.2014.11.002.\n[13] The Elders urge global co-operation to manage risks and share benefits of AI. The Elders, May\n2023. URL: https://theelders.org/news/elders-urge-global-co-operation-man\nage-risks-and-share-benefits-ai.\n[14] Ege Erdil and Tamay Besiroglu. Algorithmic Progress in Computer Vision, 2023. URL:\nhttps://arxiv.org/abs/2212.05153.\n[15] Nancy Gallagher. The Politics of Verification. Johns Hopkins University Press, 1999.\n[16] What is an Advance Market Commitment and how could it help beat COVID-19? | Gavi, the\nVaccine Alliance. Gavi, the Vaccine Alliance, 2020. URL: https://www.gavi.org/vacci\nneswork/what-advance-market-commitment-and-how-could-it-help-beat-covid\n-19.\n16\n[17] Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina\nSedova. Generative Language Models and Automated Influence Operations: Emerging Threats\nand Potential Mitigations, January 2023. arXiv:2301.04246 [cs]. URL: http://arxiv.org/\nabs/2301.04246, doi:10.48550/arXiv.2301.04246.\n[18] PM urges tech leaders to grasp generational opportunities and challenges of AI. GOV.UK, 2023.\nURL: https://www.gov.uk/government/news/pm-urges-tech-leaders-to-grasp-g\nenerational-opportunities-and-challenges-of-ai.\n[19] Samuel Hammond. Opinion | We Need a Manhattan Project for AI Safety. POLITICO, May\n2023. URL: https://www.politico.com/news/magazine/2023/05/08/manhattan-p\nroject-for-ai-safety-00095779.\n[20] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved Problems in\nML Safety, June 2022. arXiv:2109.13916 [cs]. URL: http://arxiv.org/abs/2109.13916.\n[21] Marius Hobbhahn. Trends in GPU price-performance. Epoch, 2022. URL: https://epocha\ni.org/blog/trends-in-gpu-price-performance.\n[22] Ian Hogarth. We must slow down the race to God-like AI | Financial Times, 2023. URL:\nhttps://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2.\n[23] Richard Jordan, Nicholas Emery-Xu, and Robert Trager. International Governance of Advanced\nAI.\n[24] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex\nBridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino\nRomera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,\nDavid Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas\nBerghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray\nKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic-\ntion with AlphaFold. Nature, 596(7873):583\u2013589, August 2021. Number: 7873 Publisher:\nNature Publishing Group. URL: https://www.nature.com/articles/s41586-021-038\n19-2, doi:10.1038/s41586-021-03819-2.\n[25] Jeegar Kakkad, Benedict Macon-Cooney, Jess Northend, James Phillips, Nitarshan Rajkumar,\nLuke Stanley, and Tom Westgarth. A New National Purpose: Innovation Can Power the Future\nof Britain, 2023. URL: https://www.institute.global/insights/politics-and-gov\nernance/new-national-purpose-innovation-can-power-future-britain.\n[26] Antoine Le Gall. How CERN intellectual property helps entrepreneurship. CERN, 2021. URL:\nhttps://home.cern/news/news/knowledge-sharing/how-cern-intellectual-pro\nperty-helps-entrepreneurship.\n[27] Nicolas Mailhe. Why We Need an Intergovernmental Panel for Artificial Intelligence - Our\nWorld, 2018. URL: https://ourworld.unu.edu/en/why-we-need-an-intergovernme\nntal-panel-for-artificial-intelligence.\n[28] Gary Marcus and Anka Reuel. The world needs an international agency for artificial intelligence,\nsay two AI experts. The Economist, 2023. URL: https://www.economist.com/by-invit\nation/2023/04/18/the-world-needs-an-international-agency-for-artificia\nl-intelligence-say-two-ai-experts.\n[29] Shakespeare Maya. Capacity Building for Technology Transfer in the African Context: Priori-\nties and Strategies. UNFCC, 2010.\n17\n[30] Shakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: Decolonial Theory as\nSociotechnical Foresight in Artificial Intelligence. Philosophy & Technology, 33(4):659\u2013684,\nDecember 2020. doi:10.1007/s13347-020-00405-8.\n[31] Editorial: Stop talking about tomorrow\u2019s AI doomsday when AI poses risks today. Nature,\n618(7967):885\u2013886, June 2023. Bandiera_abtest: a Cg_type: Editorial Number: 7967 Publisher:\nNature Publishing Group Subject_term: Machine learning, Authorship, Ethics. URL: https:\n//www.nature.com/articles/d41586-023-02094-7.\n[32] Tariq Rauf. From \u2018Atoms for Peace\u2019 to an IAEA Nuclear Fuel Bank | Arms Control Association.\nArms Control Association, October 2015. URL: https://www.armscontrol.org/act/20\n15-10/features/%E2%80%98atoms-peace%E2%80%99-iaea-nuclear-fuel-bank.\n[33] Martin Rees, Shivaji Sondhi, and Krishnaswamy VijayRaghavan. G20 must set up an in-\nternational panel on technological change. Hindustan Times, March 2023. URL: https:\n//www.hindustantimes.com/opinion/g20-must-set-up-an-international-panel\n-on-technological-change-101679237287848.html.\n[34] Eric Schmidt, Robert Work, Safra Catz, Eric Horvitz, Steve Chien, Andrew Jassy, Mignon\nClyburn, Gilman Louie, Chris Darby, William Mark, Kenneth Ford, Jason Matheny, Jose-Marie\nGriffiths, Katharina McFarland, and Andrew Moore. NSCAI Final Report. Technical report,\nNational Security Commission on Artificial Intelligence, 2021. URL: https://www.nscai.\ngov/2021-final-report/.\n[35] Yonadav Shavit. What does it take to catch a Chinchilla? Verifying Rules on Large-Scale\nNeural Network Training via Compute Monitoring, May 2023. arXiv:2303.11341 [cs]. URL:\nhttp://arxiv.org/abs/2303.11341, doi:10.48550/arXiv.2303.11341.\n[36] Alison Shaw and John Robinson. Relevant But Not Prescriptive: Science Policy Models within\nthe IPCC. Philosophy Today, 48:84\u201395, January 2004. doi:10.5840/philtoday200448Sup\nplement9.\n[37] Toby Shevlane. Structured access: an emerging paradigm for safe AI deployment, April 2022.\narXiv:2201.05159 [cs]. URL: http://arxiv.org/abs/2201.05159, doi:10.48550/arX\niv.2201.05159.\n[38] Eran Shimony and Omar Tsarfati. Chatting Our Way Into Creating a Polymorphic Malware,\n2023. URL: https://www.cyberark.com/resources/threat-research-blog/chatti\nng-our-way-into-creating-a-polymorphic-malware.\n[39] Kyle Wiggers Singh, Devin Coldewey and Manish. Anthropic\u2019s $5B, 4-year plan to take on\nOpenAI. TechCrunch, April 2023. URL: https://techcrunch.com/2023/04/06/anthro\npics-5b-4-year-plan-to-take-on-openai/.\n[40] Arthur Spirling. Why open-source generative AI models are an ethical way forward for science.\nNature, 616(7957):413\u2013413, April 2023. Bandiera_abtest: a Cg_type: World View Number:\n7957 Publisher: Nature Publishing Group Subject_term: Ethics, Machine learning, Technology,\nScientific community. URL: https://www.nature.com/articles/d41586-023-01295-4,\ndoi:10.1038/d41586-023-01295-4.\n[41] Jacob Steinhart. What will GPT-2030 look like? Bounded Regret, June 2023. URL: https:\n//bounded-regret.ghost.io/what-will-gpt-2030-look-like/.\n[42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori Hashimoto. Alpaca: A Strong, Replicable Instruction-Following Model.\nStanford CRFM, March 2023. URL: https://crfm.stanford.edu/2023/03/13/alpaca.\nhtml.\n18\n[43] Henrietta Toivanen. The Significance of Strategic Foresight in Verification Technologies:\nA Case Study of the INF Treaty. Technical Report LLNL-TR\u2013738786, 1502006, 892173,\nSeptember 2017. URL: https://www.osti.gov/servlets/purl/1502006/, doi:\n10.2172/1502006.\n[44] Helen Toner, Jenny Xiao, and Jeffrey Ding. The Illusion of China\u2019s AI Prowess. Foreign\nAffairs, June 2023. URL: https://www.foreignaffairs.com/china/illusion-china\ns-ai-prowess-regulation.\n[45] Memorandum by the Financial Action Task Force (FATF) Secretariat. UK Parliament, 2008.\nURL: https://publications.parliament.uk/pa/ld200809/ldselect/ldeucom/13\n2/132we08.htm.\n[46] Secretary-General Urges Broad Engagement from All Stakeholders towards United Nations\nCode of Conduct for Information Integrity on Digital Platforms | UN Press. United Nations,\nJune 2023. URL: https://press.un.org/en/2023/sgsm21832.doc.htm.\n[47] UNFCCC Technology Mechanism. UNFCCC. URL: https://unfccc.int/ttclear/sup\nport/technology-mechanism.html.\n[48] Fabio Urbina, Filippa Lentzos, C\u00e9dric Invernizzi, and Sean Ekins. Dual use of artificial-\nintelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189\u2013191, March 2022.\nNumber: 3 Publisher: Nature Publishing Group. URL: https://www.nature.com/article\ns/s42256-022-00465-9, doi:10.1038/s42256-022-00465-9.\n[49] Michael Veale, Kira Matus, and Robert Gorwa. AI and Global Governance: Modalities,\nRationales, Tensions. Annual Review of Law and Social Science, 19, 2023.\n[50] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum,\nSami Domisch, Anna Fell\u00e4nder, Simone Daniela Langhans, Max Tegmark, and Francesco\nFuso Nerini. The role of artificial intelligence in achieving the Sustainable Development\nGoals. Nature Communications, 11(1):233, January 2020. Number: 1 Publisher: Nature\nPublishing Group. URL: https://www.nature.com/articles/s41467-019-14108-y,\ndoi:10.1038/s41467-019-14108-y.\n[51] Waqar Zaidi and Allan Dafoe. International Control of Powerful Technology: Lessons from\nthe Baruch Plan for Nuclear Weapons. Working Paper, 2019. URL: https://www.fhi.ox.a\nc.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Techn\nology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf.\n19\n"
  },
  {
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "link": "https://arxiv.org/pdf/2307.04349.pdf",
    "upvote": "4",
    "text": "Published in Transactions on Machine Learning Research (11/2023)\nRLTF: Reinforcement Learning from Unit Test Feedback\nJiate Liu\u2217\njiateliu@tencent.com\nTencent\nYiqin Zhu\u2217\nyiqinzhu@tencent.com\nTencent\nKaiwen Xiao\nloktarxiao@tencent.com\nTencent\nQiang Fu\nleonfu@tencent.com\nTencent\nXiao Han\nharoldhan@tencent.com\nTencent\nWei Yang\nwillyang@tencent.com\nTencent\nDeheng Ye\u2020\ndericye@tencent.com\nTencent\nReviewed on OpenReview: https://openreview.net/forum?id=hjYmsV6nXZ&noteId=7gJrNDYNSs\nAbstract\nThe goal of program synthesis, or code generation, is to generate executable code based\non given descriptions. Recently, there has been an increasing number of studies employing\nreinforcement learning (RL) to improve the performance of large language models (LLMs) for\ncode. However, current representative works either rely solely on offline frameworks, limiting\nthe exploration of new sample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these issues, we propose\nRLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework\nwith unit test feedback of multi-granularity for refining code LLMs. Our approach generates\ndata in real-time during training and simultaneously utilizes fine-grained feedback signals to\nguide the model towards producing higher-quality code. Extensive experiments show that\nRLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n1\nIntroduction\nProgram synthesis, or code generation, involves creating an executable program that solves a given problem.\nThis research area has gained attention due to its potential to improve productivity and accessibility in\nprogramming. An AI model that can generate programs based on human requirements could significantly\ntransform programming tools and workflows (Zan et al., 2023).\n\u2217Co-first authors.\n\u2020Corresponding author.\n1\narXiv:2307.04349v2  [cs.AI]  13 Nov 2023\nPublished in Transactions on Machine Learning Research (11/2023)\nRecently, there has been a significant increase in the development of large language models (LLMs) built on\nTransformer (Vaswani et al., 2017) architectures, trained on large unlabelled code datasets. These formidable\nmodels possess the power to generate code without explicit programming instructions and have shown\nremarkable results in program synthesis. Codex (Chen et al., 2021) is a noteworthy example of an LLM with\n12 billion parameters that can successfully solve over 70% of complex Python programming challenges created\nby humans. Moreover, these models have been proven to enhance productivity and coding effectiveness in\nreal-world applications.\nSince Codex\u2019s emergence, many other LLMs tailored for the code domain have\nappeared, ranging in model size from millions to billions of parameters. AlphaCode (Li et al., 2022), for\ninstance, aspires to address competitive-level programming challenges, while InCoder (Fried et al., 2022)\nenables code insertion at arbitrary junctures utilizing bidirectional contexts. Other acclaimed models include\nCodeT5 (Wang et al., 2021), CodeGen (Nijkamp et al., 2022), PaLM-Coder (Chowdhery et al., 2022),\nPanGu-Coder (Christopoulou et al., 2022), CodeGeex (Zheng et al., 2023), and SantaCoder (Allal et al.,\n2023). As the size of these LLMs increases, they demonstrate emergent competencies, including human-like\nprogramming prowess and debugging aptitude (Zhang et al., 2022; Saunders et al., 2022).\nWhile LLMs have shown promising results in basic programming tasks, there is still progress to be made in\ntackling more challenging problems such as program competitions. Additionally, pretrained code models can\nbe limited by their reliance on NLP\u2019s self-supervised masked language modeling (MLM) and may struggle\nwith ensuring the syntactic and functional correctness of generated code.\nTo improve code generation\nperformance, very recently, reinforcement learning (RL) based algorithms for better utilizing code LLMs\nhave been proposed. For example, CodeRL (Le et al., 2022) has explored the integration of reinforcement\nlearning with unit test signals to fine-tune program synthesis models, and proposes a post-process method\ncalled \u201cCritic Sampling\u201d to further improve the performance of the model. PPOCoder (Shojaee et al., 2023)\nuses the Proximal Policy Optimization algorithm to improve CodeRL, although it does not achieve better\nresults on the competition algorithm benchmark APPS, compared to CodeRL.\nIt is worth noting that several existing RL-based methods (Le et al., 2022) are offline, meaning that they\ndo not interact with the environment dynamically during training, but rather learn from pre-collected data.\nThis can result in RL training quality being limited by the diversity of the dataset and hinder the effective\nexploration of the environment (Fujimoto et al., 2019; Kumar et al., 2019). Studies have also shown that\noffline RL can face issues such as distribution shift (Fujimoto et al., 2019; Schrittwieser et al., 2021), leading\nto unstable training and suboptimal policy performance. In contrast, online RL refers to the scenario where\nan agent can interact with the environment in real-time, selecting actions and observing their impact to\nobtain state and reward signals. Compared to offline RL, online RL training is often more stable, enables\nbetter exploration of the environment, and leads to the development of more optimal policies.\nOn the\nother hand, while existing RL-based methods do employ the results of unit tests as feedback signals, their\nimplementation is rather simple and coarse-grained. They assign the same reward to the entire episode (a\ncomplete code segment) based on the program\u2019s submission result (true, false, runtime error, or compiler\nerror), without considering that a part of the code may be correct while another part contains bugs. As a\nresult, these approaches fail to capture the nuances in identifying individual code components that contribute\nto the overall functionality and fixing bugs specific to certain parts of the code. Additionally, in contrast to\ndomains such as healthcare, autonomous driving, and human-aligned in LLMs (Levine et al., 2020; Prudencio\net al., 2023), where real-time interaction costs are high or data is difficult to obtain in real-time, the feedback\nfrom unit tests in the program synthesis task does not incur notable costs. This can potentially make the\nuse of an online framework even more beneficial in this context.\nTo address the aforementioned issues, we introduce RLTF, a novel online framework with multi-granularity\nunit test feedback that enhances pretrained LLMs for program synthesis using reinforcement learning. First,\nwe develop an online framework, which consists of two main parts: one for training the model and updating\nthe parameters, and the other for generating programs using the latest model parameters and appending\nthem to the online buffer for the subsequent model training. This framework enables the model to obtain\nreal-time unit test feedback. Additionally, it provides a diverse set of samples, enabling the code LLM agent\nto explore the environment effectively while also promoting training stability. Second, we have analyzed\nthe distribution of error types in programs and extracted more detailed information from unit test feedback\nfor training, such as \u201cfine-grained feedback\u201d and \u201cadaptive feedback\u201d. Fine-grained feedback categorizes\n2\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 1: RLTF and PPOCoder adopt an online framework, which allows the model to generate new samples\nin real-time, while CodeRL rely solely on offline frameworks.\nRLTF extracts more detailed information\nfrom unit test feedback for training, such as fine-grained feedback and adaptive feedback. Both RLTF and\nCodeRL use a post-processing technique called Critic Sampling, which was first introduced in (Le et al.,\n2022).\nFeature\nCodeRL (Le et al., 2022)\nPPOCoder (Shojaee et al., 2023)\nRLTF\nFramework\noffline\nonline\nonline\nSupervised Learning\n\u2713\n\u2713\n\u2713\nCoarse-grained Feedback\n\u2713\n\u2713\n\u2713\nFine-grained Feedback\n\u00d7\n\u00d7\n\u2713\nAdapative Feedback\n\u00d7\n\u00d7\n\u2713\nCritic Sampling\n\u2713\n\u00d7\n\u2713\nerrors based on their reported information and location, penalizing the specific erroneous parts of the code\naccordingly. Adaptive feedback provides varying rewards to programs based on the ratio of test cases they\npass. More detailed information can be found in Sec. 3.3.2.\nThe main contributions of our paper are as follows:\n\u2022 We propose RLTF, a novel online framework designed specifically for program synthesis tasks. By\ngenerating new samples in real-time throughout the training process and utilizing unit test results\nas feedback signals, RLTF significantly enhances overall model performance. This unique approach\nallows models to effectively learn the intricacies of code errors and subsequently refine their perfor-\nmance in a targeted manner.\n\u2022 Built upon this framework, we develop multi-granularity feedback that is automatically extracted\nfrom unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to\nprograms with errors, aimed at punishing the specific segments of code where the errors appear. For\nprograms that do not pass all test cases, we propose an adaptive feedback mechanism that assigns\nvarying penalties based on the ratio of passed test cases.\n\u2022 For the program synthesis task, our method achieves state-of-the-art results on the APPS and MBPP\nbenchmarks, and a detailed ablation study demonstrates the effectiveness of our approach. Addi-\ntionally, we perform tests on different LLMs (e.g., CodeT5, CodeGen), illustrating the robustness\nof our method and its applicability to different base models.\n2\nRelated Work\n2.1\nPretrained LLMs for Program Synthesis\nRecent research has delved into leveraging pretrained large language models (LLMs) from the natural lan-\nguage processing (NLP) field to automate program synthesis tasks, using vast-scale code corpus data mined\nfrom open-source repositories. Notably, there are several prominent examples of such pretrained models in-\ncluding the encoder-only CodeBERT (Feng et al., 2020), decoder-only CodeGPT (Lu et al., 2021), CodeGen\n(Nijkamp et al., 2022), PaLM-Coder (Chowdhery et al., 2022), PanGu-Coder (Christopoulou et al., 2022),\nCodeGeex (Zheng et al., 2023), and SantaCoder (Allal et al., 2023), as well as encoder-decoder transformer\narchitectures like PLABRT (Ahmad et al., 2021) and CodeT5 (Wang et al., 2021). These pretrained prob-\nabilistic language (PL) models are already capable of generating code that appears visually impressive and\nwell-structured. However, these models still face challenges in guaranteeing the syntactic and functional\ncorrectness of the generated codes (Chen et al., 2021; Hendrycks et al., 2021; Shojaee et al., 2023).\n3\nPublished in Transactions on Machine Learning Research (11/2023)\nCompiler\nshared weight\ngradient\ngradient \nupdate\nFor Problem 1, Generated Program 1\n    Pass!\nFor Problem 1, Generated Program 2\n    Failure! \nFor Problem 1002, Generated Program 17\n    Error! Index Error in line 27.\nFor Problem 1078, Generated Program 8\n    Error! Syntax Error in line 34.\n\u2026\nUnit Test\nOnline buffer\nGenerate \nPrograms\nProblem\nProblem\nGround Truth \nPrograms\nLLM\nLLM\nSL Loss\nRL Loss\n(a) Online reinforcement learning framework.\nOnline buffer\nFresh data\nOld data\ndelete\nupdate\n. . .\n(b) Online buffer workflow.\nFigure 1: The overall framework of the proposed RLTF. (a) Online reinforcement learning framework: Two\nLLMs with shared weight are utilized in the training process. One generates the target program and interacts\nwith the compiler to produce a training data pair, which is then stored in the online buffer. The other one\nutilizes ground truth data and online buffer data to calculate loss and update the model weights through\ngradient feedback. (b) Online buffer workflow: Online buffers are utilized to retain the newly generated data\nfor RL training. As fresh data is received, old data is deleted. Here SL refers to supervised learning and RL\nrefers to reinforcement learning.\n2.2\nReinforcement Learning for Program Synthesis\nReinforcement Learning (RL) is a method of learning the optimal policy by obtaining reward signals from the\nreal environment (Sutton et al., 1998). In recent years, RL has shown promise in optimizing non-differentiable\nmetrics for sequence generation tasks, such as improving BLEU and ROUGE scores in summarization and\ntranslation models respectively (Husain et al., 2019; Ren et al., 2020). Unlike text generation, program\nsynthesis requires both syntactic and functional accuracy since the generated code must pass compilation and\nunit tests (Hendrycks et al., 2021; Chen et al., 2021). Recently, execution-guided approaches and RL-based\nfine-tuning mechanisms have been employed to enhance the quality of generated codes. For instance, CodeRL\n(Le et al., 2022) investigates the fusion of reinforcement learning and unit test signals to enhance program\nsynthesis models, introducing a post-processing technique named Critic Sampling that further boosts model\nperformance. PPOCoder (Shojaee et al., 2023) employs the Proximal Policy Optimization algorithm from\nreinforcement learning to refine CodeRL. However, existing RL-based methods still face several limitations.\nFirst, a notable limitation of the existing RL methods is that they have only used a single offline framework,\nwhich constrain their ability to explore new sample spaces. Moreover, the current techniques employing unit\ntest signals are relatively simple, neglecting to consider specific error information and locations within the\ncode. Our RLTF method adopts an online framework and incorporates multi-grained feedback for model\ntraining. Overall, the differences between RLTF and existing approaches such as CodeRL and PPOCoder\nare summarized in Table 1.\n4\nPublished in Transactions on Machine Learning Research (11/2023)\n3\nApproach\n3.1\nTask Definition\nProgram synthesis is an automated process that generates computer code W based on a high-level description\nof desired behavior D. This process aims to increase the likelihood of generating a code that can solve a\ngiven problem, expressed as a conditional probability P(W|D).\nThe task can be expressed as a sequence-to-sequence problem, and the objective is to maximize the condi-\ntional probability of generating the correct output, given the input and the LLM model parameters:\nmax P(W|D, \u03b8) = max\nT\nY\nt=1\np(wt|D, \u03b8, w1:t\u22121)\n(1)\nwhere \u03b8 is the parameters of the LLM model. The LLM model is used to capture the conditional distribution,\nand it is trained using a set of input-output pairs. During training, the parameters \u03b8 are adjusted to maximize\nthe likelihood of generating the correct output for a given input.\n3.2\nOnline Reinforcement Learning Framework\nWhile supervised learning can yield valuable outcomes, the vast search space involved in program synthesis\ncan make it challenging to acquire a comprehensive dataset. As such, we propose an online reinforcement\nlearning framework, which continuously improves the code ability of LLM through online generated training\nsamples and fine-grained feedback provided by the compiler, to circumvent this predicament. Specifically,\nour framework incorporates an online buffer during the training process, which is crucial in improving the\nmodel\u2019s effectiveness. The buffer maintains a dynamic flow of data pairs that are utilized in reinforcement\nlearning training in real-time. Each data pair comprises a problem description D, the most recent model-\ngenerated code \u02c6W, and feedback from the compiler FB( \u02c6W). This approach allows the model to adapt to\nchanges in the data distribution and facilitate more efficient learning. The present framework incorporates\nthe actor and critic in training, similar to the methodology employed in CodeRL (Le et al., 2022) and\nPPOCoder (Shojaee et al., 2023).\nThe overall framework is shown in Fig. 1. The training process involves two LLMs with shared weights. The\nfirst LLM updates model parameters using data obtained from buffer for training, while the second LLM\ncollaborates with the compiler to dynamically generate code, evaluate its soundness, generate data pairs,\nand continuously refresh buffer in real-time. The incorporation of a continuously updated buffer ensures\nthe promptness of the data used for training, thereby enabling the model to explore a wider sample space\ncomprehensively.\n3.3\nReinforcement Learning from Unit Test Feedback\nWe propose RLTF, a method to improve the quality of synthesized codes by exploiting unit test feedback to\nexplore the target space. Following the reinforcement algorithm Sutton et al. (1998)Sutton & Barto (2018),\nwe construct reinforcement learning loss with the explored code \u02c6W as follows:\nLrl = \u2212R( \u02c6W) log P( \u02c6W|D, \u03b8) = \u2212R( \u02c6W)\nE\nX\nt=S\nlog p( \u02c6wt|D, \u03b8, \u02c6w1:t\u22121)\n(2)\nwhere R(\u2217) is the reward coefficient, S and E represent the start and end positions of the penalized code\nsnippets. Their values are determined by different feedback categories.\n3.3.1\nFeedback Category\nUnit test feedback FB( \u02c6W) can be obtained from the program compiler and can be categorized into three\nmain groups, namely Error, Failure, and Pass.\n5\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 2: Definition of sub-error types in Python. We divide different sub-errors into three categories based\non their characteristics: Uglobal, Uline, and Uignore.\nSub-error\nDescription\nCategory\nSyntax Error\nCode contains syntax errors that cause the compilation to fail\nUline or Uglobal\nIndex Error\nIndex operation is out of bounds\nUline\nType Error\nAn operation or function was applied to an object of an inappropriate type\nUline\nValue Error\nAn operation or function received an argument with the correct type but\nwith an inappropriate value\nUline\nEOF Error\nThe input() function encountered an end-of-file condition (EOF) without\nreading any data\nUline\nTimeout Error\nCode execution time exceeds time limit\nUglobal\nName Error\nA local or global name is not defined\nUline\nKey Error\nA mapping (dictionary) key is not found in the set of existing keys\nUline\nImport Error\nThe imported package is not found\nUline\nZeroDivision Error\nThe second argument of a division or modulo operation is zero\nUline\nRecursion Error\nCode execution recursive operation exceeds the maximum limit\nUglobal\nTriple-quoted Error\nTriple quotes are incomplete\nUignore\nIndentation Error\nWrong indentation format\nUignore\nElse\n-\nUline\nError refers to a situation where the generated code fails to compile or results in errors such as exceptions\nthat prevent the program from functioning normally. This category can be further subdivided into syntax\nerrors, index errors, type errors, among others.\nFailure indicates that the generated code can be compiled or run, but it fails to meet the expected functional\nor performance requirements. This situation requires additional improvement or optimization of the program.\nCode belonging to this category fails at least one unit test.\nPass implies that the generated code can be compiled and run successfully and meets the expected functional\nand performance requirements. This category indicates that the program has passed all unit tests and can\nbe used without further modification.\n3.3.2\nMulti-granularity Feedback\nCoarse-grained Feedback. The coarse-grained feedback serves as an incentive mechanism for the language\nmodel to enhance the probability of producing accurate code while reducing the likelihood of generating\nerroneous code. We use the same setting as CodeRL (Le et al., 2022) and PPOCoder (Shojaee et al., 2023):\nRcoarse( \u02c6W) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1.0, FB( \u02c6W) is pass\n\u22120.3, FB( \u02c6W) is failure\n\u22120.6, FB( \u02c6W) is error except syntax error\n\u22121.0, FB( \u02c6W) is syntax error\n, Scoarse = 0, Ecoarse = T\n(3)\nThe value of Rcoarse is determined by complier feedback FB( \u02c6W). The start position Scoarse and end position\nEcoarse are always 0 and T.\nFine-grained Feedback. The coarse-grained feedback serves to inform the model that it has made an\nerror, but does not indicate the specific location of the error. In contrast, the purpose of the fine-grained\nfeedback is to provide the model with information regarding the specific reasons for its errors, with the goal\nof reducing the likelihood of future errors. The reasons for code errors can vary widely, sometimes stemming\nfrom specific errors in the code (such as the use of undefined variables), and at other times from issues with\nthe overall logic of the code (such as recursion errors in Python). For the former, we aim for the model to\naddress the error on the current line, while for the latter, we hope the model can re-evaluate and revise the\n6\nPublished in Transactions on Machine Learning Research (11/2023)\ncode. To this end, we have developed a set of rewards for different error subtypes, which can be found in\nTable 2.\nTypically, we are able to identify the erroneous code snippets from compiler prompts. We divide the error\ncodes into three categories (Uglobal, Uline, Uignore) based on their specific subtypes. The Uglobal category\nmeans that penalties will be applied to the entire code generated, usually because of a logic issue that affects\nthe entire code segment, and cannot be exactly identified on a specific line of code. The Uline category\napplies penalties to the specific line of code that caused the error because fixing this line of code will prevent\nthat error from recurring. The Uignore category temporarily avoids punishment because the reasons for the\nerror are numerous and cannot be accurately located. In cases of Syntax Error, if it is caused by unfinished\ncode due to the maximum token length limit, it is classified as Uglobal, otherwise, it is Uline.\nRfine( \u02c6W) =\n(\n0.0, if \u02c6W \u2208 Uignore\n\u22120.3, else\n, Sfine =\n(\ntline_start, if \u02c6W \u2208 Uline\n0, else\n, Efine =\n(\ntline_end, if \u02c6W \u2208 Uline\nT, else\n(4)\nWe set Rfine( \u02c6W) = 0.0 for Uignore, otherwise \u22120.3. tline_start and tline_end are the start and end position\nof the line that complier feedback.\nAdaptive Feedback.\nAdaptive feedback is designed to motivate the model to generate code that can\nsuccessfully pass a maximum number of test samples. To attain this objective, the reward value of adaptive\nfeedback is directly proportional to the number of test samples that the generated code can pass. The specific\nconfiguration of the reward is as follows:\nRadaptive( \u02c6W) = \u22120.3 + 1.3 \u2217\nNpass\nNpass + Nfail\n, Sadaptive = 0, Eadaptive = T\n(5)\nThe value of Radaptive is positively correlated with the pass rate of unit tests. The start and end position\nare always 0 and T.\n3.4\nOptimization Objective\nIn order to further enhance the performance of the pre-trained LLM via RLTF, the following optimization\nobjectives can be used for fine-tuning:\nLtotal = Lsl + Lcoarse + Lfine + Ladaptive\n(6)\nwhere Lsl is supervised learning loss which makes the training process stable. We adopt cross-entropy loss\nas Lsl as follows:\nLsl = \u2212 log P(W|D, \u03b8) = \u2212\nT\nX\nt=1\nlog p(wt|D, \u03b8, w1:t\u22121)\n(7)\nAnd Lcoarse, Lfine, Ladaptive are variants of the reinforcement learning loss shown in Eq.2, representing three\ntypes of feedback granularity, which are as follows:\n7\nPublished in Transactions on Machine Learning Research (11/2023)\nLcoarse = \u2212(Rcoarse( \u02c6W) \u2212 Rcoarse( \u02c6Wbaseline))\nEcoarse\nX\nt=Scoarse\nlog p( \u02c6wt|D, \u03b8, \u02c6w1:t\u22121)\n(8)\nLfine = \u2212\u03b1Rfine( \u02c6W)\nEfine\nX\nt=Sfine\nlog p( \u02c6wt|D, \u03b8, \u02c6w1:t\u22121)\n(9)\nLadaptive = \u2212(Radaptive( \u02c6W) \u2212 Radaptive( \u02c6Wbaseline))\nEadaptive\nX\nt=Sadaptive\nlog p( \u02c6wt|D, \u03b8, \u02c6w1:t\u22121)\n(10)\nTo address the amplitude fluctuation in fine-grained feedback caused by variable values of S and E, we use\nan adaptive weight \u03b1 that balances the equation. This weight is calculated as \u03b1 =\nT\nEfine\u2212Sfine , making\nthe fine-grained feedback equivalent to coarse-grained feedback and adaptive feedback. Here \u02c6Wbaseline is\nthe baseline dynamically updated during the training process, which represents the historical optimal code\ngenerated by the model under the natural language description D. This item indicates that we want the\nmodel to continuously outperform its own historical version. For fine-grained feedback, its baseline is hard\nto define, so we do not add this item to its loss function.\n4\nExperiments\nOur baselines are the state-of-the-art works on incorporating RL with code LLMs (PPOCoder (Shojaee et al.,\n2023) and CodeRL (Le et al., 2022)), and we use the same benchmarks and setting as theirs for evaluation.\n4.1\nBenchmarks\nAPPS Benchmark. We first evaluate using the challenging APPS (Automated Programming Progress\nStandard) program synthesis benchmark presented by (Hendrycks et al., 2021), as it features a diverse range\nof coding problems from various coding websites, presenting differing levels of difficulty. The benchmark\nconsists of a total of 10,000 coding problems, with an equal train-test split. On average, each problem is\naccompanied by 23.2 accurate Python programs and 21.2 unit tests. The mean length of each problem is 293.2\nwords, whereas the average program length is 18.0 lines. The dataset is classified into three difficulty levels:\nIntroductory (3,639; train/test = 2,639/1,000), Interview (5,000; train/test = 2,000/3,000), and Competition\n(1,361; train/test = 361/1,000). Typically, each sample has 20 unit tests to assess the functional correctness\nof the programs. We adhere to the same preprocessing steps as those in (Hendrycks et al., 2021) for generating\ninput sequences based on problem descriptions.\nNote that, during the implementation of our online framework, we encountered an issue in the APPS open-\nsource code, specifically in the unit test portion. If the generated code raised a \u201csegmentation fault\u201d error, it\ncould not be bypassed using \u201ctry\u201d and \u201cexcept\u201d, resulting in the main program getting stuck and disrupting\nthe sample generation process.\nTo address this issue, we modified all unit test code procedures to be\nexecuted through subprocess calls, ensuring the main program does not get stuck and allowing the online\nsample generation process to run smoothly. We believe it necessary to expose this to the community for\nreproducibility considerations.\nFor the APPS benchmark, we employ the RLTF framework to fine-tune the pretrained CodeT5 model.\nWe utilized a machine equipped with 8 NVIDIA V100 GPUs, each with 32GB of memory, for training\npurposes. Each GPU carried a batch size of 32, and a learning rate of 2e-6 was employed. The training\nprocess took approximately 24 hours. Concurrently, three additional machines with similar 8-card V100\nGPU configurations were used to generate the latest samples. We updated the online buffer every 50 steps.\nFollowing the same approach as CodeRL, half of the steps were for SL training, while the other half focused\non RL training.\nThe length of the online buffer was set to 6400.\nAfter generating 6400 samples using\nthe initial model, the training process officially commenced, and the online buffer was updated as a queue.\n8\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 3: Quantitative evaluation on APPS benchmark.\nMethod\nSize\nCS\npass@1\npass@5\npass@1000\nIntro\nInter\nComp\nall\nIntro\nInter\nComp\nall\nIntro\nInter\nComp\nall\nCodex\n12B\nw/o\n4.14\n0.14\n0.02\n0.92\n9.65\n0.51\n0.09\n2.25\n25.02\n3.70\n3.23\n7.87\nAlphaCode\n1B\nw/o\n-\n-\n-\n-\n-\n-\n-\n-\n17.67\n5.24\n7.06\n8.09\nGPT3\n175B\nw/o\n0.20\n0.03\n0.00\n0.06\n-\n-\n-\n-\n-\n-\n-\n-\nGPT2\n0.1B\nw/o\n1.00\n0.33\n0.00\n0.40\n2.70\n0.73\n0.00\n1.02\n-\n-\n-\n-\nGPT2\n1.5B\nw/o\n1.30\n0.70\n0.00\n0.68\n3.60\n1.03\n0.00\n1.58\n27.90\n9.83\n11.40\n13.76\nGPT-Neo\n2.7B\nw/o\n3.90\n0.57\n0.00\n1.12\n5.50\n0.80\n0.00\n1.58\n27.90\n9.83\n11.40\n13.76\nCodeRL\n770M\nw/o\n4.00\n0.78\n0.15\n1.30\n9.83\n2.03\n0.69\n3.32\n35.30\n13.33\n13.60\n17.78\nPPOCoder(original)\n770M\nw/o\n5.20\n1.00\n0.50\n1.74\n9.10\n2.50\n1.20\n3.56\n35.20\n13.35\n13.90\n17.77\nPPOCoder(scale)\n770M\nw/o\n4.06\n0.79\n0.15\n1.32\n9.97\n2.06\n0.70\n3.37\n35.42\n13.37\n13.65\n17.84\nRLTF\n770M\nw/o\n4.16\n0.97\n0.20\n1.45\n10.12\n2.65\n0.82\n3.78\n38.30\n15.13\n15.90\n19.92\nCodeRL\n770M\nw\n7.08\n1.86\n0.75\n2.69\n16.37\n4.95\n2.84\n6.81\n40.00\n15.67\n17.90\n20.98\nRLTF\n770M\nw\n8.40\n2.28\n1.10\n3.27\n18.60\n5.57\n3.70\n7.80\n39.70\n15.03\n16.80\n20.32\nDuring testing, we employed Nucleus sampling with a top value of 0.95 and set the temperature parameter\nto 0.6.\nMBPP Benchmark.\nTo further evaluate our framework, we also employ an additional, smaller, and\nsimpler Python program synthesis dataset called MBPP (Mostly Basic Programming Problems), introduced\nby (Austin et al., 2021). The dataset consists of 974 instances, with 374 instances for training, 90 instances for\nvalidation, and 500 instances for testing, while reserving 10 instances for few-shot learning evaluations. The\nproblems are predominantly brief, often featuring only a single sentence of natural language descriptions.\nEach problem is accompanied by one correct solution (averaging 6.8 lines of code) and three unit tests,\nwhich are assert statements to verify functional correctness. In contrast to APPS, MBPP\u2019s unit tests are\nnot hidden and are explicitly integrated into the source sequences for program synthesis models. Although\nthis occasionally encourages models to overfit the assert statements through hard-coding an if-expression, we\nadhere to the same source sequence construction as previous work to ensure a fair comparison with baselines.\nSpecifically, we use the same prompt format as (Austin et al., 2021) to prepare the input sequence, which\nis composed of problem descriptions + \u201cYour code should satisfy these tests:\u201d + three assert statements.\nWe experiment with the MBPP dataset in a zero-shot setting, as the same as CodeRL. While generating\nsamples, we also used Nucleus sampling with a top value of 0.95 and set the temperature parameter to 1.2.\n4.2\nQuantitative Evaluation on APPS\nFor a fair comparison, we employ the CodeT5 770M as the base model, similarly to CodeRL (Le et al., 2022),\nPPOCoder(Shojaee et al., 2023). Table 3 presents the overall results of our RLTF approach based on the\nCodeT5 model on the APPS benchmark. We compared our method with larger models such as Codex (Chen\net al., 2021), AlphaCode (Li et al., 2022), GPT2 (Radford et al., 2019), GPT3 (Brown et al., 2020), GPT-\nNeo (Black et al., 2021), and other CodeT5-based methods like CodeRL (Le et al., 2022) and PPOCoder\n(Shojaee et al., 2023). Note that by default, results of pretrained LMs (except for Codex and GPT3) are\nfrom models fine-tuned on APPS using the standard loss Lce only, and results of CodeRL, PPOCoder, RLTF\nare from models fine-tuned on APPS using the both loss Lce and Lrl. The results indicate that methods\nbased on CodeT5 outperform those using other larger-parameter base models such as GPTs. Furthermore,\nemploying the RLTF approach leads to additional performance improvements, surpassing other CodeT5-\nbased methods such as CodeRL and PPOCoder, thereby achieving a new state-of-the-art (SOTA) in the\nfield. It is important to note that we utilized the official open-source CodeRL model and applied nucleus\nsampling with a temperature of 0.6. We noticed that our pass@1 score was lower than the result reported\nin their paper, while our pass@5 score was higher. Similar results were also found by others and discussed\nat https://github.com/salesforce/CodeRL/issues/30. Regarding PPOCoder, the results reported in\ntheir paper also differ from those in the CodeRL paper. Since the authors did not open-source the APPS\nbenchmark code and models, we are uncertain about their exact evaluation process. Therefore, we used the\nresults obtained from the open-source CodeRL model as a standard and employed proportional scaling to\n9\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 4: Ablation studies: Impact of framework.\nFramework\npass@1\npass@5\npass@10\nIntro\nInter\nComp\nall\nIntro\nInter\nComp\nall\nIntro\nInter\nComp\nall\nOffline\n3.67\n0.84\n0.16\n1.29\n9.25\n2.40\n0.74\n3.43\n12.41\n3.36\n1.33\n4.76\nOffline+RLTF\n3.71\n0.93\n0.19\n1.34\n9.28\n2.51\n0.85\n3.53\n12.60\n3.50\n1.50\n4.92\nOnline\n3.94\n0.92\n0.17\n1.37\n9.68\n2.43\n0.75\n3.50\n13.01\n3.41\n1.35\n4.92\nOnline+RLTF\n4.16\n0.97\n0.20\n1.45\n10.12\n2.65\n0.82\n3.78\n13.59\n3.67\n1.45\n5.21\nderive the final results. To provide readers with a clear understanding of the result sources, we concurrently\nincluded the results for PPOCoder(original) and PPOCoder(scale) in Table 3. Here, PPOCoder(original)\nrepresents the results from the original paper; however, since we did not directly compare against it, we\nhave shaded it in gray. PPOCoder(scale), on the other hand, denotes the scaled results. This ensures a fair\ncomparison between the different methods, while acknowledging the discrepancies in the reported values.\nEven without selecting the scaled results, our performance is better than the results reported in the original\npaper, except for pass@1.\nAdditionally, we compare the performance of our model with the post-processing approach Critic Sampling,\nfirst proposed in (Le et al., 2022). Due to the unavailability of the open-source Critic Sampling code from\nCodeRL, we reproduced the Critic Sampling based on the details provided in the original paper. We employed\nboth refine and repair and set M, the number of top candidates selected from program samples that fail\nexample unit tests to 1. We also limited the maximum number of repair and refine iterations to 1. It is worth\nnoting that we found a crucial parameter N not explicitly mentioned in the original paper: the total number\nof program samples generated during each refine/repair step. When measuring pass@1, we set N = 5; for\npass@5, we set N = 20; and for pass@1000, we set N = 1000. Our results show that Critic Sampling\nachieves better performance when k = 1 or 5 in pass@k. However, when k is larger, the improvement in\nperformance is relatively small, which differs from the results presented in the original paper. Overall, we\nthink Critic Sampling is a post-processing method that focuses on how to better use the model rather than\nhow to better train the model. Therefore, comparing our approach with results without Critic Sampling\nprovides a fairer evaluation.\n4.3\nAblation Studies\nWe conduct extensive ablations to understand the effectiveness of the techniques we develop.\nImpact of Framework.\nTable 4 presents the ablation study comparing different training frameworks\n(online or offline) and the use of the RLTF method. All the results have utilized the combined loss Lsl\nand Lcoarse. The results show that the combination of the online framework and RLTF yields the best\nperformance, while the offline framework without RLTF produces the poorest outcome. Applying either the\nonline framework or RLTF method individually leads to a certain degree of improvement, validating the\neffectiveness of both approaches. Interestingly, we observe that the performance boost brought by the RLTF\nmethod is more substantial within the online framework. This can be attributed to the real-time generation\nof samples in the online setting, which helps the model identify more potential errors in programs and thus\nfacilitates better training.\nImpact of Feedback.\nTable 5 presents the ablation study comparing different feedback combinations\nduring reinforcement learning (RL) training. The results show that using only supervised learning yields\nthe poorest performance. In contrast, employing reinforcement learning and progressively incorporating the\nrewards mentioned in Section 3 leads to increasingly better model performance. The best results are obtained\nwhen using a combination of coarse-grained, fine-grained, and adaptive feedback, verifying the effectiveness\nof these different reward designs. Additionally, we observe that the inclusion of the fine-grained feedback\ncontributes the most significant performance boost among the rewards evaluated.\nWe also evaluate the\nresults with all feedback except the coarse one: pass@1 1.38%, pass@5 3.58%, pass@10 5.05%. The purpose\nof the coarse feedback is to inform the model about the overall test results of the code (true, false, runtime\n10\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 5: Ablation: impact of different combinations of feedback.\nSL\nCoarse\nFine\nAdaptive\npass@1\npass@5\npass@10\npass@100\npass@1000\n\u221a\n-\n-\n-\n1.30\n3.39\n4.68\n10.51\n17.80\n\u221a\n\u221a\n-\n-\n1.37\n3.50\n4.92\n10.63\n18.31\n\u221a\n\u221a\n\u221a\n-\n1.41\n3.67\n5.10\n11.08\n19.32\n\u221a\n\u221a\n\u221a\n\u221a\n1.45\n3.78\n5.21\n11.23\n19.92\nTable 6: Ablation: impact of the fine-grained\nfeedback Rfine( \u02c6W).\nRfine( \u02c6\nW)\npass@1\npass@5\npass@10\n0.0\n1.31\n3.42\n4.72\n-0.1\n1.39\n3.59\n4.99\n-0.2\n1.38\n3.67\n5.10\n-0.3\n1.40\n3.62\n5.04\n-0.4\n1.39\n3.68\n5.06\n-0.5\n1.36\n3.57\n5.00\nTable 7: Ablation: impact of the temperature\nduring training.\nTemperature\npass@1\npass@5\npass@10\n0.2\n1.34\n3.56\n4.99\n0.6\n1.37\n3.60\n5.06\n1.0\n1.45\n3.78\n5.21\nerror, compile error). Therefore, we think it is also quite important. As can be seen, removing the coarse\nfeedback leads to a decline in the performance metrics. It is also worth noting that the performance without\ncoarse feedback is better than the performance with only coarse feedback.\nImpact of Rfine( \u02c6W). Table 6 illustrates the impact of different Rfine( \u02c6W) for fine-grained feedback on\nmodel training. We can observe that various values of Rfine( \u02c6W) result in improved model performance,\nwith each offering better outcomes than those obtained without any penalty.\nImpact of Temperature. Table 7 demonstrates the effect of varying the temperature during the training\nprocess. A higher temperature leads to the generation of more diverse samples, while a lower temperature\nresults in more conservative samples. We observe that the model performs better when the temperature is\nset to 1, and comparatively worse when set to 0.2. This suggests that generating more diverse samples during\nthe training process helps the model explore a broader range of sample space, facilitating RL training.\nImpact of Language Model. To demonstrate the robustness of the RLTF approach, we conduct exper-\niments using another base model, CodeGen 2.7B, in addition to CodeT5. As shown in Table 8, the results\nindicate that applying the RLTF method on CodeGen 2.7B also yields impressive performance, resulting in\nan almost 1% improvement in pass@10. Notably, we find that the larger the base model, the greater the\nperformance boost provided by RLTF. This suggests that the RLTF approach can effectively unleash the\npotential of different base models to generate better codes, with the impact being more pronounced when\nthe base model size is larger.\n4.4\nQuantitative Evaluation on MBPP\nTo further demonstrate the effectiveness of RLTF, we evaluate the zero-shot performance of the CodeT5\nmodel trained with RLTF on the APPS benchmark when tested on the MBPP (Mostly Basic Python\nProblems) benchmark, as shown in Table 9. For this evaluation, the methods based on the CodeT5 model,\nincluding CodeRL, PPOCoder, and RLTF, were assessed in a zero-shot setting. For the GPT-based models,\nthey were pretrained on 2.93 billion web documents using standard language modeling objective, and fine-\ntuned on the training set of the MBPP dataset. The results presented in the table were obtained from\nFigure 3 of the original paper (Austin et al., 2021). The results revealed that RLTF outperformed different\nsizes of GPT models on the MBPP benchmark and attained a new state-of-the-art performance among the\nCodeT5-based methods.\n11\nPublished in Transactions on Machine Learning Research (11/2023)\nTable 8: Ablation: different large language models as the backbone.\nLanguage Model\nRLTF\npass@1\npass@5\npass@10\npass@100\npass@1000\nCodeT5 770M\nw/o\n1.30\n3.39\n4.68\n10.51\n17.80\nCodeT5 770M\nw\n1.45\n3.78\n5.21\n11.23\n19.92\nCodeGen 2.7B\nw/o\n1.64\n4.23\n5.79\n12.48\n21.44\nCodeGen 2.7B\nw\n2.04\n5.04\n6.80\n14.17\n23.96\nTable 9: Quantitative evaluation on the MBPP benchmark (zero-shot).\nMethod\nSize\nstate\npass@1\npass@80\nGPT\n224M\nfine-tuned\n-\n7.2\nGPT\n442M\nfine-tuned\n-\n12.6\nGPT\n1B\nfine-tuned\n-\n22.4\nGPT\n4B\nfine-tuned\n-\n33.0\nGPT\n8B\nfine-tuned\n-\n40.6\nGPT\n68B\nfine-tuned\n-\n53.6\nGPT\n137B\nfine-tuned\n-\n61.4\nCodeT5 + CodeRL\n770M\nzero-shot\n25.7\n68.1\nCodeT5 + PPOCoder\n770M\nzero-shot\n26.1\n68.2\nCodeT5 + RLTF\n770M\nzero-shot\n30.4\n71.3\n4.5\nQualitative Analysis by Unit Test Outcomes\nFigure 2 presents the qualitative analysis by unit test outcomes before and after applying the RLTF method\non the CodeGen model. For the 5000 problems in the APPS benchmark, we utilize Nucleus sampling to\ngenerate an average of 20 solutions per problem and recorded their results. Figure 2a depicts the percentages\nof different unit test results, as defined in Eq.3, including error, failure, and pass. We can observe that the\nRLTF method can reduce the proportion of programs resulting in errors and increase the proportion of\nprograms that pass, especially for problems with introductory difficulty levels. The observed increase in\nfailure rate stems from the fixing of error codes, resulting in either pass or fail outcomes. This illustrate\nthat RLTF is more effective in addressing runtime and compiler errors compared to semantic errors, which\nremain more challenging. Figure 2b illustrates the percentages of different sub-errors among erroneous results\nbefore and after applying the RLTF method. Most errors show a decline in proportion after using the RLTF\nmethod, particularly syntax errors. It is also noteworthy that the proportion of timeout errors exhibits\na minor increase, which can be attributed to the correction of other grammar-related errors resulting in\ntimeout errors.\n5\nConclusions and Future Work\nIn this paper, we have proposed RLTF (Reinforcement Learning from unit Test Feedback), a novel online\nRL framework with unit test feedback of multi-granularity, for refining large language models on program\nsynthesis tasks. Compared with existing works, our approach generates data on-the-fly during training and\nsimultaneously utilizes finer granularity in feedback signals to guide the model towards producing higher-\nquality code. Extensive experiments demonstrate that RLTF surpasses existing RL methods for programming\nand can be applied to various LLMs, including CodeT5 and CodeGen. Furthermore, it achieves state-of-the-\nart performance on widely-used benchmarks including APPS and MBPP.\nIn the future, there are several directions to further improve RLTF. For instance, the input-output examples\nin the existing benchmarks may not be sufficiently diverse, and the programs generated using hidden input-\noutput examples may not be the correct final code versions. Such limitations can restrict the performance\nof RLTF. Therefore, using LLMs to create a more diverse and accurate set of input-output examples is\na potential research direction worth exploring. Additionally, whether finer-grained feedback signals, such\nas those from static code analyzers, can further enhance RLTF\u2019s performance, is another possible avenue\nto pursue. Also, note that transferability is an importance issue. The manual categorization of sub-error\n12\nPublished in Transactions on Machine Learning Research (11/2023)\n0.0\n10.0\n20.0\n30.0\n40.0\n50.0\nIntro\nInter\nComp\nAll\nError (%)\nw/o RLTF\nw RLTF\n0.0\n20.0\n40.0\n60.0\n80.0\nIntro\nInter\nComp\nAll\nFailure (%)\nw/o RLTF\nw RLTF\n0.0\n2.0\n4.0\n6.0\n8.0\nIntro\nInter\nComp\nAll\nPass (%)\nw/o RLTF\nw RLTF\n(a) Percentages of unit test results.\n0.0\n2.0\n4.0\n6.0\n8.0\n10.0\nSyntaxError\nIndexError\nTypeError\nValueError\nEOFError\nTimeoutError\nNameError\nKeyError\nImportError\nZeroDivisionError\nElse\nSubError (%)\nw/o RLTF\nw RLTF\n(b) Percentages of different sub-errors.\nFigure 2: Qualitative analysis by unit test outcomes on CodeGen.\ntypes we employed makes it challenging to transfer RLTF to other programming languages, which should be\nconsidered as another limitation of this work. As a next step, we believe the implementation of automated\ncategorization will significantly enhance the applicability of RLTF in a broader context.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\nUnified pre-training for\nprogram understanding and generation. arXiv preprint arXiv:2103.06333, 2021.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferran-\ndis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don\u2019t reach for the\nstars! arXiv preprint arXiv:2301.03988, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive\nlanguage modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58,\n2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, J Kaplan, H Ed-\nwards, Y Burda, N Joseph, G Brockman, et al. Evaluating large language models trained on code.(2021).\narXiv preprint arXiv:2107.03374, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n13\nPublished in Transactions on Machine Learning Research (11/2023)\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li,\nQi Zhang, Meng Xiao, Bo Shen, Lin Li, et al.\nPangu-coder: Program synthesis with function-level\nlanguage modeling. arXiv preprint arXiv:2207.11280, 2022.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages.\narXiv preprint arXiv:2002.08155, 2020.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999, 2022.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.\nIn International conference on machine learning, pp. 2052\u20132062. PMLR, 2019.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv\npreprint arXiv:2105.09938, 2021.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning\nvia bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mas-\ntering code generation through pretrained models and deep reinforcement learning. Advances in Neural\nInformation Processing Systems, 35:21314\u201321328, 2022.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode.\nScience, 378(6624):1092\u20131097, 2022.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. arXiv preprint arXiv:2102.04664, 2021.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint\narXiv:2203.13474, 2022.\nRafael Figueiredo Prudencio, Marcos ROA Maximo, and Esther Luna Colombini. A survey on offline rein-\nforcement learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and\nLearning Systems, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint\narXiv:2009.10297, 2020.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.\nSelf-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.\n14\nPublished in Transactions on Machine Learning Research (11/2023)\nJulian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou,\nand David Silver. Online and offline reinforcement learning by planning with a learned model. Advances\nin Neural Information Processing Systems, 34:27580\u201327591, 2021.\nParshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. Execution-based code generation\nusing deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.\nRichard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press\nCambridge, 1998.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained\nencoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang\nLou. Large language models meet nl2code: A survey, 2023.\nJialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.\nRepairing bugs in python assignments using large language models. arXiv preprint arXiv:2209.14876, 2022.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,\nYang Li, et al.\nCodegeex: A pre-trained model for code generation with multilingual evaluations on\nhumaneval-x. arXiv preprint arXiv:2303.17568, 2023.\n15\n"
  },
  {
    "title": "Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement",
    "link": "https://arxiv.org/pdf/2307.04751.pdf",
    "upvote": "3",
    "text": "Shelving, Stacking, Hanging: Relational Pose Diffusion\nfor Multi-modal Rearrangement\nAnthony Simeonov1,3, Ankit Goyal2,\u2217, Lucas Manuelli2,\u2217, Lin Yen-Chen1,\nAlina Sarmiento1,3, Alberto Rodriguez1, Pulkit Agrawal1,3,\u2020, Dieter Fox2,\u2020\n1Massachusetts Institute of Technology\n2NVIDIA\n3Improbable AI Lab\nAbstract: We propose a system for rearranging objects in a scene to achieve a\ndesired object-scene placing relationship, such as a book inserted in an open slot of\na bookshelf. The pipeline generalizes to novel geometries, poses, and layouts of\nboth scenes and objects, and is trained from demonstrations to operate directly on\n3D point clouds. Our system overcomes challenges associated with the existence\nof many geometrically-similar rearrangement solutions for a given scene. By\nleveraging an iterative pose de-noising training procedure, we can fit multi-modal\ndemonstration data and produce multi-modal outputs while remaining precise and\naccurate. We also show the advantages of conditioning on relevant local geometric\nfeatures while ignoring irrelevant global structure that harms both generalization\nand precision. We demonstrate our approach on three distinct rearrangement tasks\nthat require handling multi-modality and generalization over object shape and\npose in both simulation and the real world. Project website, code, and videos:\nhttps://anthonysimeonov.github.io/rpdiff-multi-modal\nKeywords: Object Rearrangement, Multi-modality, Manipulation, Point Clouds\n1\nIntroduction\nConsider Figure 1, which illustrates (1) placing a book on a partially-filled shelf and (2) hanging\na mug on one of the multiple racks on a table. These tasks involve reasoning about geometric\ninteractions between an object and the scene to achieve a goal, which is a key requirement in many\ncleanup and de-cluttering tasks of interest to the robotics community [1]. In this work, we enable\na robotic system to perform one important family of such tasks: 6-DoF rearrangement of rigid\nobjects [2]. Our system uses point clouds obtained from depth cameras, allowing real-world operation\nwith unknown 3D geometries. The rearrangement behavior is learned from a dataset of examples that\nshow the desired object-scene relationship \u2013 a scene and (segmented) object point cloud are observed\nand a demonstrator transforms the object into a final configuration.\nReal-world scenes are often composed of objects whose shapes and poses can vary independently.\nSuch composition creates scenes that (i) present combinatorial variation in geometric appearance and\nlayout (e.g., individual racks may be placed anywhere on a table) and (ii) offer many locations and\ngeometric features for object-scene interaction (e.g., multiple slots for placing the book and multiple\nracks for hanging the mug). These features of real-world scenes bring about two key challenges for\nlearning that go hand-in-hand: multi-modal placements and generalization to diverse scene layouts.\n\u2022 Multi-modality appears in the rearrangement outputs. There may be many scene locations to place\nan object, and these multiple possibilities create difficulties during both learning and deployment.\nNamely, a well-known challenge in learning from demonstrations is fitting a dataset containing\nsimilar inputs that have different associated targets (modes). Moreover, during deployment,\npredicting multiple candidate rearrangements can help the robot choose the ones that also satisfy\nany additional constraints, such as workspace limits and collision avoidance. Therefore, the system\nmust predict multi-modal outputs that span as many different rearrangement solutions as possible.\nWork done in part during NVIDIA internship, \u2217Equal contribution, \u2020 Equal advising\nContact: asimeono@mit.edu\narXiv:2307.04751v1  [cs.RO]  10 Jul 2023\n(A)\n(B)\nFigure 1: By learning from a set of demonstrations of a rearrangement task, such as place the book\nin the shelf (A) and hang the mug on the rack (B), Relational Pose Diffusion (RPDiff) can produce\nmultiple transformations that achieve the same object-scene relationship for new object/scene pairs.\n\u2022 Generalization must be addressed when processing the inputs to the system. A scene is composed\nof many elements that vary in both shape and layout. For example, a shelf can be located anywhere\nin the environment, and there are many possible book arrangements within a shelf. The point clouds\nthat are presented to the model reflect this diversity. Generalizing to such input variability is harder\nthan generalizing to shape and pose variations for a single object, due to the combinatorially many\narrangements and layouts of scenes. Moreover, the system must also generalize to any possible\ninitial configuration of the object.\nGiven a dataset of final object-scene point clouds (obtained by transforming the observed object\npoint cloud into its resultant configuration at the end of the demo), we can synthesize many initial\nobject configurations as perturbations of the final point clouds. Using this data, we can naturally\ncast rearrangement prediction as point cloud pose de-noising. From a final object-scene point cloud,\nwe create a \u201cnoised\u201d point cloud by randomly transforming the object and train a neural network\nto predict how to transform the noised point cloud back into the original configuration (using the\nknown perturbation for ground truth supervision). During deployment, we similarly predict a de-\nnoising object transformation that satisfies the learned relation with the scene and use this predicted\ntransformation as the rearrangement action. The robot executes the predicted rearrangement using a\ncombination of grasp sampling, inverse kinematics, and motion planning.\nUnfortunately, learning to de-noise from large perturbations in one step can be ineffective when\nconsidering multi-modality [3] \u2013 creating similar-looking noised point clouds with prediction targets\nthat differ can lead the model to learn an average solution that fits the data poorly. We overcome\nthis difficulty by training the predictor as a diffusion model [4, 5] to perform iterative de-noising.\nBy creating a multi-step noising process, diffusion models are trained to incrementally reverse the\nprocess one step at a time. Intuitively, early steps in this reverse process are closer to the ground truth\nand the associated prediction targets are more likely to be unique across samples \u2013 the prediction\n\u201clooks more unimodal\u201d to the model. The model similarly generates the test-time output in an iterative\nfashion. By starting this inference procedure from a diverse set of initial guesses, the predictions can\nconverge to a diverse set of final solutions.\nWhile iterative de-noising helps with multi-modality, we must consider how to support generalization\nto novel scene layouts. To achieve this, we propose to locally encode the scene point cloud by\ncropping a region near the object. Locally cropping the input helps the model generalize by focusing\non details in a local neighborhood and ignoring irrelevant and distant distractors. The features for\nrepresenting smaller-scale patches can also be re-used across different spatial regions and scene\ninstances [6\u20139]. We use a larger crop size on the initial iterations because the inference procedure\nstarts from random guesses that may be far from a good solution. As the solution converges over\nmultiple iterations, we gradually reduce the crop size to emphasize a more local scene context.\n2\nNetwork predicts SE(3) transform from object and cropped scene\n(C)\nTraining: Iterative pose de-noising for object-scene point cloud\n(B)\nGround truth\nDe-noising\n(predictions)\n\u2026\nNoising\n(data gen)\nEvaluation: Starting from diverse initial poses, pose diffusion outputs a diverse set of rearrangement solutions\n(A)\nOutput: Rearrangement \ntransforms\nInput: Diverse initial \nposes\nFigure 2: Method Overview. (A) Starting from an object and scene point cloud PO and PS, we\ntransform PO to a diverse set of initial poses. RPDiff takes the initial object-scene point clouds as\ninput, iteratively updates the object pose, and outputs a set of object configurations that satisfy a\ndesired relationship with the scene. This enables integrating RPDiff with a planner to search for\na placement to execute while satisfying additional system constraints. (B) The model is trained to\nperform iterative pose de-noising. Starting from object-scene point clouds that satisfy the desired task,\nwe apply a sequence of perturbations to the object and train the model to predict SE(3) transforms\nthat remove the noise one step at a time. (C) To facilitate generalization to novel scene layouts, we\ncrop the scene point cloud to the region near the object point cloud.\nIn summary, we present Relational Pose Diffusion (RPDiff), a method that performs 6-DoF relational\nrearrangement conditioned on an object and scene point cloud, that (1) generalizes across shapes,\nposes, and scene layouts, and (2) gracefully handles scenarios with multi-modality. We evaluate our\napproach in simulation and the real world on three tasks, (i) comparing to existing methods that either\nstruggle with multi-modality and complex scenes or fail to achieve precise rearrangement, and (ii)\nablating the various components of our overall pipeline.\n2\nProblem Setup\nOur goal is to predict a set of SE(3) transformations {Tk}K\nk=1 that accomplish an object rearrange-\nment task given the scene S and the object O, represented as 3D point clouds (PS \u2208 RM\u00d73 and\nPO \u2208 RN\u00d73, respectively). By selecting (i.e., via a learned scoring function) and applying one\ntransformation from this set, we can place the object in a manner that fulfills the desired geometric\nrelationship with the scene. We assume the object point cloud is segmented from the whole scene,\nwhich does not have any additional segmented objects (e.g., we cannot segment any individual\nbooks on the shelf). We also assume a training dataset D = {(PO, PS)}L\nl=1 where each data point\nrepresents an object placed at the desired configuration. For example, D could include point clouds of\nbooks and bookshelves (with different shapes, poses, and configurations of books on the shelf), and\nSE(3) transformations that place the books in one of the available slots. These demonstrations could\ncome from a human or a scripted algorithm with access to ground truth object states in simulation.\nCritically, depending on constraints imposed by other system components (e.g., available grasps,\nrobot reachability, collision obstacles), the system must be capable of producing multi-modal output\ntransformations. Predicting diverse outputs enables searching for a placement that can be feasibly\nexecuted. For execution on a robot, the robot has access to a grasp sampler [10], inverse kinematics\n(IK) solver, and motion planner to support generating and following a pick-and-place trajectory.\n3\nMethod\nThe main idea is to iteratively de-noise the 6-DoF pose of the object until it satisfies the desired\ngeometric relationship with the scene point cloud. An overview of our framework is given in Fig. 2.\n3\n3.1\nObject-Scene Point Cloud Diffusion via Iterative Pose De-noising\nWe represent a rearrangement action T as the output of a multi-step de-noising process for a\ncombined object-scene point cloud, indexed by discrete time variable t = 0, ..., T. This process\nreflects a transformation of the object point cloud in its initial noisy configuration PO\n(T ) to a\nfinal configuration PO\n(0) that satisfies a desired relationship with the scene point cloud PS, i.e.,\nPO\n(0) = TPO\n(T ). To achieve this, we train neural network f\u03b8 : RN\u00d73 \u00d7RM\u00d73 \u2192 SE(3) to predict\nan SE(3) transformation from the combined object-scene point cloud at each step. The network is\ntrained as a diffusion model [4, 5] to incrementally reverse a manually constructed noising process\nthat gradually perturbs the object point clouds until they match a distribution PO\n(T ) \u223c p(T )\nO (\u00b7 | PS),\nwhich we can efficiently sample from during deployment to begin de-noising at test time.\nTest-time Evaluation. Starting with PO and PS, we sample K initial transforms { \u02c6T(I)\nk }K\nk=1* and\napply these to PO to create initial object point clouds {\u02c6P(I)\nO,k}K\nk=1 where \u02c6P(I)\nO,k = \u02c6T(I)\nk PO. For each\nof the K initial transforms, we then perform the following update for I steps.\u2020 At each iteration i:\nT(i)\n\u2206 = TRand\n\u2206\nf\u03b8\n\u0010\n\u02c6P(i)\nO , PS, pos emb\n\u0000t\n\u0001\u0011\nt = i to t(i)\n(1)\n\u02c6T(i\u22121) = T(i)\n\u2206 \u02c6T(i)\n\u02c6P(i\u22121)\nO\n= T(i)\n\u2206 \u02c6P(i)\nO\n(2)\nThe update T(i)\n\u2206 is formed by multiplying the denoising transform predicted by our model f\u03b8 with\na perturbation transform TRand\n\u2206\nthat is sampled from an iteration-conditioned normal distribution\nwhich converges toward deterministically producing an identify transform as i tends toward 0. In the\nde-noising process, TRand\n\u2206\nhelps each of the K samples converge to different multi-modal pose basins\n(analogously to the perturbation term in Stochastic Langevin Dynamics [11]). The function pos emb\nrepresents a sinusoidal position embedding. Since f\u03b8 is only trained on a finite set of t values (i.e.,\nt = 1, ..., 5) but we might want to perform the update in Eq. 2 for a larger number of steps, we use\nthe function i to t to map the iteration i to a timestep value t that the model has been trained on.\nDetails on external noise scheduling and mapping i to t can be found in Appendix A3.\nGenerally, we search through K solutions { \u02c6T(0)\nk }K\nk=1 for one that can be executed while satisfying\nall other constraints (e.g., collision-free trajectory). However, we also want a way to select a single\noutput to execute assuming there are no other constraints to satisfy. We may also want to reject\n\u201clocally optimal\u201d solutions that fail to complete the desired task. To achieve this, we use a separate\nclassifier h\u03d5 to score the predicted poses (i.e., sk = h\u03d5(PO\n(0)\nk , PS) where s \u2208 [0, 1]), such that the\nsample indexed with kexec = argmax {sk}K\nk=1 can be selected for execution\u2021.\nTraining. Given a dataset sample (PO, PS), we start with final \u201cplaced\u201d object point cloud PO\n(0) =\nPO and randomly sampled timestep t \u2208 [1, T]. We then obtain a perturbation transform T(t)\nnoisefrom\na timestep-conditioned distribution with appropriately scaled variance and create a noised point\ncloud PO\n(t) = T(t)\nnoisePO. The task is to predict a transformation that takes one de-noising step as\n\u02c6T(t)\n\u2206 = f\u03b8(P(t)\nO , PS, pos emb(t)). Network parameters \u03b8 are trained to minimize a loss between\nthe prediction \u02c6T(t)\n\u2206 and a ground truth target T(t)\n\u2206,GT. The loss is composed of the mean-squared\ntranslation error, a geodesic rotation distance error [12, 13], and the chamfer distance between the\npoint cloud obtained by applying the predicted transform and the ground-truth next point cloud.\nA natural target for f\u03b8 to predict is the inverse of the perturbation, i.e., T(t)\n\u2206,GT = T(t)\nnoise,inv =\n\u0002\nT(t)\nnoise\n\u0003\u22121, to encourage recovering the original sample. However, as the perturbation magnitude\nvaries across timesteps, this requires output predictions of different scales for different timesteps.\nIn supervised learning with neural networks, it is advisable to keep the magnitudes of both input\nand output signals consistent in order to minimize large fluctuations in gradient magnitudes between\nsamples [14]. For this reason, an alternative approach is to encourage the network to take shorter\n\u201cunit steps\u201d in the direction of the original sample. We achieve this by uniformly interpolating the full\n*\nInitial rotations are drawn from a uniform grid over SO(3) , and we uniformly sample translations that\nposition the object within the bounding box of the scene point cloud.\n\u2020\nWe denote application of SE(3) transform T = (R, t) to 3D point x as Tx = Rx + t\n\u2021\nSee Appendix A7 for results showing that scoring with h\u03d5 performs better than, e.g., uniform output sampling\n4\ninverse perturbation as {T(s)\ninterp}t\ns=1 = interp(T(t)\nnoise, inv, t) and training the network to predict one\ninterval in this interpolated set, i.e., T(t)\n\u2206,GT = [T(t\u22121)\ninterp ]\u22121T(t)\ninterp (details in Appendix A2 and A7).\nFor the success classifier, we generate positive and negative rearrangement examples, where positives\nuse the final demonstration point cloud, PO\n(0), and negatives are obtained by sampling diverse\nperturbations of PO\n(0). The classifier weights \u03d5 (separate from weights \u03b8) are trained to minimize a\nbinary cross-entropy loss between the predicted likelihood and the ground truth success labels.\n3.2\nArchitecture\nWe use a Transformer [15] for processing point clouds and making pose predictions. We use a\nTransformer because it learns to (i) identify important geometric parts within the object and the\nscene and (ii) capture relationships that occur between the important parts of the object and the\nscene. Starting with PO and PS, we tokenize the point clouds to obtain input features. This can be\nperformed by passing through a point cloud encoder [16, 17], but we simply downsample and append\na one-hot feature to each point indicating whether it is part of the object or the scene. We then pass\nthese input tokens through a Transformer encoder and decoder, which performs self-attention on\nthe scene point cloud, and cross-attention between the scene and the object. This produces output\nfeatures for each point, which are mean-pooled to obtain a global feature vector. The global feature\nis passed to a set of MLPs which predict the rotation R \u2208 SO(3) and a translation t \u2208 R3. As in\n[10, 18], we represent the rotation by predicting vectors a \u2208 R3 and b \u2208 R3, finding the component\nof b that is orthogonal to a, and normalizing to obtain \u02c6a and \u02c6b. We then take a cross product to obtain\n\u02c6c = \u02c6a \u00d7 \u02c6b, and construct R as\n\u0002\n\u02c6a\n\u02c6b\n\u02c6c\n\u0003\n. We incorporate iteration t by passing pos emb(t) as a\nglobal token in the decoder and adding it to the global output feature. To predict success likelihood,\nwe process point clouds with the same Transformer but output a single scalar followed by a sigmoid.\n3.3\nLocal Conditioning\nThe approach described above conditions the transform regression on both the object and the scene.\nHowever, distant global information can act as a distraction and hamper both precision and gener-\nalization. Prior work has also observed this and suggested hard attention mechanisms on the input\nobservation like cropping task-relevant regions to improve generalization by ignoring irrelevant\ndistractors [8, 9]. Building on this intuition, we modify the scene point cloud by cropping PS to only\ninclude points that are near the current object point cloud PO\n(i). Our modified pose prediction thus\nbecomes \u02c6T(i)\n\u2206 = f\u03b8\n\u0010\n\u02c6P(i)\nO , \u00afP(i)\nS , pos emb\n\u0000i to t(i)\n\u0001\u0011\nwhere \u00afP(i)\nS = crop(\u02c6P(i)\nO , PS). The function\ncrop returns the points in PS that are within an axis-aligned box centered at the mean of \u02c6P(i)\nO . We try\none variant of the crop function that returns a fixed-size crop, and another that adjusts the crop size\ndepending on the iteration variable i (the size starts large and gradually decreases for later iterations).\n4\nExperiments: Design and Setup\nOur quantitative experiments in simulation are designed to answer the following questions:\n1. How well does RPDiff achieve the desired tasks compared to other methods for rearrangement?\n2. How successful is RPDiff in producing a diverse set of transformations compared to baselines?\n3. How does our performance change with different components modified or removed?\nWe also demonstrate RPDiff within a pick-and-place pipeline in the real world to further highlight\nthe benefits of multi-modal generation and our ability to transfer from simulation to the real world.\n4.1\nTask Descriptions and Training Data Generation\nWe evaluate our method on three tasks that emphasize multiple available object placements: (1)\nplacing a book on a partially-filled bookshelf, (2) stacking a can on a stack of cans or an open shelf\nregion, and, (3) hanging a mug on one of many racks with many hooks. As a sanity check for our\nbaseline implementations, we also include two easier versions of \u201cmug on rack\u201d tasks that are \u201cless\n5\nMethod\nMug/EasyRack Mug/MedRack Book/Shelf Mug/Multi-MedRack Can/Cabinet\nC2F Q-attn\n0.31\n0.31\n0.57\n0.26\n0.51\nR-NDF-base\n0.75\n0.29\n0.00\n0.00\n0.14\nNSM-base\n0.83\n0.17\n0.02\n0.01\n0.08\nNSM-base + CVAE\n\u2013\n0.39\n0.17\n0.27\n0.19\nRPDiff (ours)\n0.92\n0.83\n0.94\n0.86\n0.85\nTable 1: Rearrangement success rates in simulation. On tasks with a unimodal solution space and\nsimpler scene geometry, each method performs well (see Mug/EasyRack task). However, on tasks\ninvolving more significant shape variation and multi-modality, RPDiff works better than all other\napproaches.\nmulti-modal\u201d. These consist of (i) hanging a mug on one rack with a single hook and (ii) hanging a\nmug on one rack with two hooks. We programmatically generate \u223c1k-3k demonstrations of each\ntask in simulation with a diverse set of procedurally generated shapes (details in Appendix A2). We\nuse each respective dataset to train both RPDiff and each baseline (one model for each task). For our\nreal-world experiments, we directly transfer and deploy the models trained on simulated data.\n4.2\nEvaluation Environment Setup\nSimulation. We conduct quantitative experiments in the PyBullet [19] simulation engine. The\npredicted transform is applied to the object by simulating an insertion controller which directly\nactuates the object\u2019s center of mass (i.e., there is no robot in the simulator). The insertion is executed\nfrom a \u201cpre-placement\u201d pose that is offset from the predicted placement. This offset is obtained using\nprior knowledge about the task and the objects and is not predicted (see Appendix A6 for details). To\nquantify performance, we report the success rate over 100 trials, using the final simulator state to\ncompute success. We also quantify coverage by comparing the set of predictions to a ground truth set\nof feasible solutions and computing the corresponding precision and recall. Details on the insertion\ncontroller, computation of Tpre-place, and the task success criteria can be found in the Appendix.\nReal World. We also apply RPDiff to object rearrangement in the real world using a Franka Panda\nrobotic arm with a Robotiq 2F140 parallel jaw gripper. We use four calibrated depth cameras to\nobserve the tabletop environment. From the cameras, we obtain point clouds PO and PS of object O\nand scene S and apply our method to predict transformation T. T is applied to O by transforming\nan initial grasp pose Tgrasp (using a separate grasp predictor [10]) by T to obtain a placing pose\nTplace = TTgrasp, and inverse kinematics and motion planning is used to reach Tgrasp and Tplace.\n4.3\nBaselines\nCoarse-to-Fine Q-attention (C2F-QA). This method adapts the classification-based approach\nproposed in [8] to relational rearrangement. We train a fully convolutional network to predict a\ndistribution of scores over a voxelized representation of the scene, denoting a heatmap over candidate\ntranslations of the object centroid. The model runs in a \u201ccoarse-to-fine\u201d fashion by performing this\noperation multiple times over a smaller volume at higher resolutions. On the last step, we pool the\nvoxel features and predict a distribution over a discrete set of rotations to apply to the object. We use\nour success classifier to rank the predicted transforms and execute the output with the top score.\nRelational Neural Descriptor Fields (R-NDF). R-NDF [20] uses a neural field shape representation\ntrained on category-level 3D models as a feature space wherein local coordinate frames can be\nmatched via nearest-neighbor search. R-NDFs have been used to perform relational rearrangement\ntasks via the process of encoding and localizing task-relevant coordinate frames near the object parts\nthat must align to achieve the desired rearrangement. We call this method \u201cR-NDF-base\u201d because it\ndoes not feature the additional energy-based model for refinement proposed in the original work.\nNeural Shape Mating (NSM) + CVAE. Neural Shape Mating (NSM) [3] uses a Transformer to\nprocess a pair of point clouds and predict how to align them. Architecturally, NSM is the same as\nour relative pose regression model, with the key differences of (i) being trained on arbitrarily large\nperturbations of the demonstration point clouds, (ii) not using local cropping, and (iii) only making\n6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecall\n0.6\n0.8\n1.0\nPrecision\nk=4\nk=8\nk=16\nk=32 k=64\nk=128\nk=256\nk=4\nk=8\nk=16\nk=32\nk=64\nk=128\nk=256\nPrecision vs. Recall (per # of inits)\nmrp (ours)\nc2f-qa\n(a)\nCrop Method\nSuccess Rate\nMug/Rack Book/Shelf Can/Cabinet\nNone\n0.58\n0.62\n0.42\nFixed\n0.76\n0.92\n0.75\nVarying\n0.86\n0.94\n0.85\n(b)\nFigure 3: (a) Coverage evaluation in simulation. Both RPDiff and C2F-QA achieve high placement\ncoverage, but the prediction quality of C2F-QA reduces with an increase in coverage, while RPDiff\nproduces outputs that remain precise while achieving high coverage. (b) Cropping ablations. Success\nrate of RPDiff with different kinds of scene point cloud conditioning. The increased success rate\nachieved when using local scene cropping highlights the generalization and precision benefits of\nfocusing on a local spatial region.\na single prediction. We call this baseline \u201cNSM-base\u201d because we do not consider the auxiliary\nsigned-distance prediction and learned discriminator proposed in the original approach [3]. While\nthe method performs well on unimodal tasks, the approach is not designed to handle multi-modality.\nTherefore, we modify NSM to act as a conditional variational autoencoder (CVAE) [21] to better\nenable learning from multi-modal data. We use NSM+CVAE to predict multiple transforms and\nexecute the output with the top score produced by our success classifier.\n5\nResults\n5.1\nSimulation: Success Rate Evaluation\nTable 1 shows the success rates achieved by each method on each task and highlights that our method\nperforms best across the board. The primary failure mode from C2F-QA is low precision in the\nrotation prediction. Qualitatively, the C2F-QA failures are often close to a successful placement but\nstill cause the insertion to fail. In contrast, our refinement procedure outputs very small rotations that\ncan precisely align the object relative to the scene.\nSimilarly, we find R-NDF performs poorly on more complex scenes with many available placements.\nWe hypothesize this is because R-NDF encodes scene point clouds into a global latent representation.\nSince the single set of latent variables must capture all possible configurations of the individual\nscene components, global encodings fail to represent larger-scale scenes with significant geometric\nvariability [6, 7]. For instance, R-NDF can perform well with individual racks that all have a single\nhook, but fails when presented with multiple racks.\nFinally, while NSM+CVAE improves upon the unimodal version of NSM, we find the generated\ntransforms vary too smoothly between the discrete modes (e.g., book poses that lie in between the\navailable shelf slots), an effect analogous to the typical limitation of VAE-based generators producing\nblurry outputs in image generation. We hypothesize this over-smoothing is caused by trying to make\nthe approximate posterior match the unimodal Gaussian prior. This contrasts RPDiff\u2019s ability to\n\u201csnap on\u201d to the available placing locations in a given scene. More discussion on the performance\nobtained by the baseline methods and how they are implemented can be found in Appendix A6.\n5.2\nSimulation: Coverage Evaluation\nNext, we evaluate the ability to produce multi-modal outputs that cover the space of rearrangement\nsolutions and examine the tradeoff between prediction quality and coverage. Since coverage is\naffected by the number of parallel runs we perform, we compute average recall and average precision\nfor different values of K (the number of initial poses that are refined). Precision and recall are\ncomputed with respect to a set of ground truth rearrangement solutions for a given object-scene\ninstance. We consider positive predictions as those that are within a 3.5cm position and 5-degree\nrotation threshold of a ground truth solution.\nFig. 3a shows the results for our approach along with C2F-QA, the best-performing baseline. We\nobserve a trend of better coverage (higher recall) with more outputs for both approaches. For a modest\n7\n(B)\n(A)\n(C)\nFigure 4: Real-world multi-modal rearrangement. Executing Can/Cabinet (A), Book/Shelf (B),\nand Mug/Rack (C) in the real world. For each task, the initial object-scene configuration is shown in\nthe top-left image, and examples of executing multiple inferred placements are shown in the main\nimage sequence.\nvalue of K = 32, we observe RPDiff is able to cover over half of the available placement solutions\non average, with C2F-QA achieving slightly lower coverage. However, we see a stark difference\nbetween the methods in terms of precision as the number of outputs is increased. C2F-QA suffers\nfrom more outputs being far away from any ground truth solution, while our approach maintains\nconsistently high generation quality even when outputting upwards of 200 rearrangement poses.\n5.3\nSimulation: Local Cropping Ablations and Modifications\nFinally, we evaluate the benefits of introducing local scene conditioning into our relative pose\nregression model. Table 3b shows the performance variation of our method with different kinds\nof scene point cloud conditioning. We achieve the best performance with the version of local\nconditioning that varies the crop sizes on a per-iteration basis. Using a fixed crop size marginally\nreduces performance, while conditioning on the whole uncropped scene point cloud performs much\nworse. This highlights the generalization and precision benefits of focusing on a local spatial region\nnear the object in its imagined configuration. It also suggests an advantage of using a coarse-to-fine\napproach that considers a larger region on earlier iterations. Additional results examining the effect of\nthe success classifier, external noise, and parameterization of i to t can be found in Appendix A7.\n5.4\nReal World: Object rearrangement via pick-and-place\nFinally, we use RPDiff to perform relational rearrangement via pick-and-place on real-world objects\nand scenes. Fig. 1 and Fig. 4 show the robot executing multiple inferred placements on our three\ntasks. We relied on our approach\u2019s ability to output multiple solutions, as some geometrically valid\nplacements were not kinematically feasible for the robot based on its workspace limits and the\nsurrounding collision geometry. Please see the supplemental video for real-world execution.\n6\nRelated Work\nObject Rearrangement from Perception. Object rearrangement prediction using perceptual inputs\nhas been an area of growing interest [2, 3, 8, 20, 22\u201348]. One straightforward method is end-to-end\ntraining to directly regress the relative transformation, as in Neural Shape Mating (NSM) [3]. Others\nhave explored identifying task-relevant object parts and then solving for the desired alignment, as in\nTAX-Pose and R-NDF [20, 40, 48]. However, many of these approaches in their naive form struggle\nwhen there is multi-modality (NSM and TAX-Pose can only output a single solution). There has\nbeen success addressing multi-modality by performing classification over a discretized version of the\nsearch space [8, 42, 44, 46, 47, 49], but these methods are typically less precise.\nDenoising Diffusion and Iterative Regression. Diffusion models [4, 50] use an iterative de-noising\nprocess to perform generative modeling. While they were originally designed for generating images,\nthey have been extended to other domains including waveforms [51, 52], 3D shapes [53, 54], and\n8\ndecision-making[55\u201357]. In robotics, diffusion models (and related energy-based models) have\nbeen applied to policy learning [58, 59], trajectory optimization [60\u201363], grasping [56], and object\nrearrangement [20, 41]. Iterative regression has also been successful in domains such as pose\nestimation [64\u201367], and recent work has illustrated connections between iterative prediction and\nde-noising diffusion [68, 69].\nSE(3)-DiffusionFields [56] integrate learned 6-DoF grasp distributions within a trajectory optimiza-\ntion framework, and LEGO-Net [57] employs iterative de-noising to generate realistic-looking room\nlayouts. Our work differs in that we do not assume known object states or 3D models. Most similar\nto our work, StructDiffusion [41] uses a diffusion model to perform language-conditioned object\nrearrangement with point clouds. While the focus in [41] is to rearrange multiple objects into\nabstract structures (e.g., circles, lines) specified via natural language, we emphasize covering all\nrearrangement modes and integrating with sampling-based planners.\n7\nLimitations and Conclusion\nLimitations. The amount of demonstration data we use can only be easily obtained via scripted\npolicies in simulation. Future work could explore pre-trained representations and multi-task learning\nto reduce data requirements for new tasks. We also suffer from some amount of sim2real gap due to\ntraining on simulated point clouds. Incorporating broader sim2real transfer techniques and finetuning\non real-world data would be useful to investigate. Finally, we execute the predicted placement open\nloop. These types of rearrangement tasks would benefit from a closed-loop policy that can track\nprogress toward the predicted rearrangement goal and react to/recover from disturbances.\nConclusion. This work presents an approach for rearranging objects in a scene to achieve a desired\nplacing relationship, while operating with novel geometries, poses, and scene layouts. Our system\ncan produce multi-modal distributions of object transformations for rearrangement, overcoming the\ndifficulty of fitting multi-modal demonstration datasets and facilitating integration with planning\nalgorithms that require diverse actions to search through. Our results illustrate the capabilities of our\nframework across a diverse range of rearrangement tasks involving objects and scenes that present a\nlarge number of feasible rearrangement solutions.\n8\nAcknowledgement\nThe authors would like to thank NVIDIA Seattle Robotics Lab members and the MIT Improbable\nAI Lab for their valuable feedback and support in developing this project. In particular, we would\nlike to acknowledge Idan Shenfeld, Anurag Ajay, and Antonia Bronars for helpful suggestions on\nimproving the clarity of the draft. This work was partly supported by Sony Research Awards and\nAmazon Research Awards. Anthony Simeonov is supported in part by the NSF Graduate Research\nFellowship.\nAuthor Contributions\nAnthony Simeonov conceived the overall project goals, investigated several approaches for address-\ning multi-modality in rearrangement prediction, implemented the pose diffusion framework, wrote\nall the code, ran simulation and real-world experiments, and was the primary author of the paper.\nAnkit Goyal advised the project, made technical suggestions on clarifying the method and improving\nthe experimental evaluation, and supported iteration on obtaining real robot results, and helped with\nwriting the paper.\nLucas Manuelli engaged in research discussions about rearrangement prediction, suggested initial\nideas for addressing multi-modality, advised the project in its early stages, and provided valuable\nfeedback on the paper.\nLin Yen-Chen supported early project brainstorming, helped develop direct connections with diffu-\nsion models, gave feedback on evaluation tasks, and helped edit the paper.\n9\nAlina Sarmiento helped implement the framework on the real robot and implemented the grasp\ngeneration model that enabled the pick-and-place demos on the Franka Panda.\nAlberto Rodriguez engaged in technical discussions on the connections to iterative optimization\nmethods and integrating the framework in the context of a sampling-based planner.\nPulkit Agrawal suggested connections to work on iterative regression that came before diffusion\nmodels, helped clarify key technical insights on the benefits of iterative prediction, suggested ablations,\nhelped with paper writing/editing, and co-advised the project.\nDieter Fox was involved in technical discussions on relational tasks involving object part interactions,\nproposed some of the evaluation tasks, helped formalize connections to other related work, and\nadvised and supported the overall project.\nReferences\n[1] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, C. Wang, G. Levine,\nM. Lingelbach, J. Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday\nactivities and realistic simulation. In Conference on Robot Learning. PMLR, 2023.\n[2] D. Batra, A. X. Chang, S. Chernova, A. J. Davison, J. Deng, V. Koltun, S. Levine, J. Malik,\nI. Mordatch, R. Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint\narXiv:2011.01975, 2020.\n[3] Y.-C. Chen, H. Li, D. Turpin, A. Jacobson, and A. Garg. Neural shape mating: Self-supervised\nobject assembly with adversarial shape priors. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12724\u201312733, 2022.\n[4] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. arXiv preprint\narxiv:2006.11239, 2020.\n[5] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\n[6] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann,\nand S. Sridhar. Neural fields in visual computing and beyond. Computer Graphics Forum, 2022.\nISSN 1467-8659. doi:10.1111/cgf.14505.\n[7] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional occupancy\nnetworks. In Proc. ECCV, 2020.\n[8] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning\nfor visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13739\u201313748, 2022.\n[9] S. James and A. J. Davison. Q-attention: Enabling efficient learning for vision-based robotic\nmanipulation. IEEE Robotics and Automation Letters, 7(2):1612\u20131619, 2022.\n[10] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox. Contact-graspnet: Efficient 6-dof\ngrasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 13438\u201313444. IEEE, 2021.\n[11] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In\nProceedings of the 28th international conference on machine learning (ICML-11), 2011.\n[12] J. J. Kuffner. Effective sampling and distance metrics for 3d rigid body path planning. In IEEE\nInternational Conference on Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004,\nvolume 4, pages 3993\u20133998. IEEE, 2004.\n10\n[13] D. Q. Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical\nImaging and Vision, 35(2):155\u2013164, 2009.\n[14] L. Huang, J. Qin, Y. Zhou, F. Zhu, L. Liu, and L. Shao. Normalization techniques in training\ndnns: Methodology, analysis and application. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[16] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Advances in neural information processing systems, 2017.\n[17] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph\ncnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1\u201312, 2019.\n[18] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li. On the continuity of rotation representations in\nneural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5745\u20135753, 2019.\n[19] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics\nand machine learning. GitHub repository, 2016.\n[20] A. Simeonov, Y. Du, L. Yen-Chen, , A. Rodriguez, , L. P. Kaelbling, T. L. Perez, and P. Agrawal.\nSe(3)-equivariant relational rearrangement with neural descriptor fields. In Conference on Robot\nLearning (CoRL). PMLR, 2022.\n[21] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional\ngenerative models. In Advances in neural information processing systems, 2015.\n[22] L. Manuelli, W. Gao, P. Florence, and R. Tedrake. kpam: Keypoint affordances for category-\nlevel robotic manipulation. In The International Symposium of Robotics Research, pages\n132\u2013157. Springer, 2019.\n[23] S. Cheng, K. Mo, and L. Shao. Learning to regrasp by learning to place. In 5th Annual Confer-\nence on Robot Learning, 2021. URL https://openreview.net/forum?id=Qdb1ODTQTnL.\n[24] R. Li, C. Esteves, A. Makadia, and P. Agrawal. Stable object reorientation using contact plane\nregistration. In 2022 International Conference on Robotics and Automation (ICRA), 2022.\n[25] S. Thompson, L. P. Kaelbling, and T. Lozano-Perez. Shape-based transfer of generic skills. In\n2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021.\n[26] A. Simeonov, Y. Du, B. Kim, F. R. Hogan, J. Tenenbaum, P. Agrawal, and A. Rodriguez. A\nlong horizon planning framework for manipulating rigid pointcloud objects. In Conference on\nRobot Learning (CoRL), 2020.\n[27] S. Lu, R. Wang, Y. Miao, C. Mitash, and K. Bekris. Online object model reconstruction and\nreuse for lifelong improvement of robot manipulation. In 2022 International Conference on\nRobotics and Automation (ICRA), pages 1540\u20131546. IEEE, 2022.\n[28] M. Gualtieri and R. Platt. Robotic pick-and-place with uncertain object instance segmentation\nand shape completion. IEEE robotics and automation letters, 6(2):1753\u20131760, 2021.\n[29] P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy\nlearning. IEEE Robotics and Automation Letters, 2019.\n[30] A. Curtis, X. Fang, L. P. Kaelbling, T. Lozano-P\u00b4erez, and C. R. Garrett. Long-horizon manipu-\nlation of unknown objects via task and motion planning with estimated affordances. In 2022\nInternational Conference on Robotics and Automation (ICRA), pages 1940\u20131946. IEEE, 2022.\n11\n[31] C. Paxton, C. Xie, T. Hermans, and D. Fox. Predicting stable configurations for semantic\nplacement of novel objects. In Conference on Robot Learning, pages 806\u2013815. PMLR, 2022.\n[32] W. Yuan, C. Paxton, K. Desingh, and D. Fox. Sornet: Spatial object-centric representations for\nsequential manipulation. In 5th Annual Conference on Robot Learning. PMLR, 2021.\n[33] A. Goyal, A. Mousavian, C. Paxton, Y.-W. Chao, B. Okorn, J. Deng, and D. Fox.\nIfor:\nIterative flow minimization for robotic object rearrangement. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 14787\u201314797, 2022.\n[34] A. H. Qureshi, A. Mousavian, C. Paxton, M. Yip, and D. Fox. NeRP: Neural Rearrangement\nPlanning for Unknown Objects. In Proceedings of Robotics: Science and Systems, July 2021.\n[35] D. Driess, J.-S. Ha, and M. Toussaint. Learning to solve sequential physical reasoning problems\nfrom a scene image. The International Journal of Robotics Research, 2021.\n[36] D. Driess, J.-S. Ha, and M. Toussaint. Deep visual reasoning: Learning to predict action\nsequences for task and motion planning from an initial scene image. In Robotics: Science and\nSystems 2020 (RSS 2020). RSS Foundation, 2020.\n[37] W. Liu, C. Paxton, T. Hermans, and D. Fox. Structformer: Learning spatial structure for\nlanguage-guided semantic rearrangement of novel objects. In 2022 International Conference on\nRobotics and Automation (ICRA), pages 6322\u20136329. IEEE, 2022.\n[38] W. Goodwin, S. Vaze, I. Havoutis, and I. Posner. Semantically grounded object matching\nfor robust robotic scene rearrangement. In 2022 International Conference on Robotics and\nAutomation (ICRA), pages 11138\u201311144. IEEE, 2022.\n[39] M. Danielczuk, A. Mousavian, C. Eppner, and D. Fox. Object rearrangement using learned\nimplicit collision functions. In 2021 IEEE International Conference on Robotics and Automation\n(ICRA), pages 6010\u20136017. IEEE, 2021.\n[40] C. Pan, B. Okorn, H. Zhang, B. Eisner, and D. Held. Tax-pose: Task-specific cross-pose\nestimation for robot manipulation. In 6th Annual Conference on Robot Learning.\n[41] W. Liu, T. Hermans, S. Chernova, and C. Paxton. Structdiffusion: Object-centric diffusion for\nsemantic rearrangement of novel objects. arXiv preprint arXiv:2211.04604, 2022.\n[42] L. Yen-Chen, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia,\nand P. Isola. MIRA: Mental imagery for robotic affordances. In Conference on Robot Learning\n(CoRL), 2022.\n[43] K. Wada, S. James, and A. J. Davison. ReorientBot: Learning object reorientation for specific-\nposed placement. In IEEE International Conference on Robotics and Automation (ICRA),\n2022.\n[44] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin,\nD. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for\nrobotic manipulation. Conference on Robot Learning (CoRL), 2020.\n[45] H. Huang, D. Wang, R. Walters, and R. Platt. Equivariant Transporter Network. In Proceedings\nof Robotics: Science and Systems, New York City, NY, USA, June 2022.\n[46] K. Mo, Y. Qin, F. Xiang, H. Su, and L. Guibas. O2O-Afford: Annotation-free large-scale\nobject-object affordance learning. In Conference on Robot Learning (CoRL), 2021.\n[47] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n12\n[48] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitz-\nmann. Neural descriptor fields: Se (3)-equivariant object representations for manipulation. In\n2022 International Conference on Robotics and Automation (ICRA), 2022.\n[49] N. M. M. Shafiullah, Z. J. Cui, A. Altanzaya, and L. Pinto. Behavior transformers: Cloning k\nmodes with one stone. In Thirty-Sixth Conference on Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=agTr-vRQsa.\n[50] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning,\npages 2256\u20132265. PMLR, 2015.\n[51] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\n[52] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan. Wavegrad: Estimating\ngradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.\n[53] S. Luo and W. Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.\n[54] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: A system for generating 3d\npoint clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n[55] M. Janner, Y. Du, J. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior\nsynthesis. In International Conference on Machine Learning, 2022.\n[56] J. Urain, N. Funk, J. Peters, and G. Chalvatzaki. Se(3)-diffusionfields: Learning smooth\ncost functions for joint grasp and motion optimization through diffusion. IEEE International\nConference on Robotics and Automation (ICRA), 2023.\n[57] Q. A. Wei, S. Ding, J. J. Park, R. Sajnani, A. Poulenard, S. Sridhar, and L. Guibas. Lego-net:\nLearning regular rearrangements of objects in rooms. arXiv preprint arXiv:2301.09629, 2023.\n[58] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:\nVisuomotor policy learning via action diffusion, 2023.\n[59] P. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch,\nand J. Tompson. Implicit behavioral cloning. Conference on Robot Learning (CoRL), 2021.\n[60] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior\nsynthesis, 2022.\n[61] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal. Is conditional generative\nmodeling all you need for decision-making?, 2022.\n[62] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S.-C. Zhu. Diffusion-based gen-\neration, optimization, and planning in 3d scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 16750\u201316761, June 2023.\n[63] Y. Du, T. Lin, and I. Mordatch. Model based planning with energy based models, 2021.\n[64] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Human pose estimation with iterative error\nfeedback. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4733\u20134742, 2016.\n[65] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. Deepim: Deep iterative matching for 6d pose\nestimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n683\u2013698, 2018.\n13\n[66] Y. Labb\u00b4e, J. Carpentier, M. Aubry, and J. Sivic. Cosypose: Consistent multi-view multi-object\n6d pose estimation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XVII 16, pages 574\u2013591. Springer, 2020.\n[67] Y. Labb\u00b4e, L. Manuelli, A. Mousavian, S. Tyree, S. Birchfield, J. Tremblay, J. Carpentier,\nM. Aubry, D. Fox, and J. Sivic. Megapose: 6d pose estimation of novel objects via render &\ncompare. arXiv preprint arXiv:2212.06870, 2022.\n[68] M. Delbracio and P. Milanfar. Inversion by direct iteration: An alternative to denoising diffusion\nfor image restoration. arXiv preprint arXiv:2303.11435, 2023.\n[69] A. Bansal, E. Borgnia, H.-M. Chu, J. S. Li, H. Kazemi, F. Huang, M. Goldblum, J. Geiping,\nand T. Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. arXiv\npreprint arXiv:2208.09392, 2022.\n[70] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva,\nS. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint\narXiv:1512.03012, 2015.\n[71] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based\ngenerative models. arXiv preprint arXiv:2206.00364, 2022.\n[72] T. Chen.\nOn the importance of noise scheduling for diffusion models.\narXiv preprint\narXiv:2301.10972, 2023.\n[73] I. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[74] J. Sola, J. Deray, and D. Atchuthan. A micro lie theory for state estimation in robotics. arXiv\npreprint arXiv:1812.01537, 2018.\n[75] T. Chen, A. Simeonov, and P. Agrawal. AIRobot. https://github.com/Improbable-AI/\nairobot, 2019.\n[76] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al. A density-based algorithm for discovering\nclusters in large spatial databases with noise. In kdd, volume 96, pages 226\u2013231, 1996.\n[77] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d\nclassification and segmentation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 652\u2013660, 2017.\n[78] K. Murphy, C. Esteves, V. Jampani, S. Ramalingam, and A. Makadia. Implicit-pdf: Non-\nparametric representation of probability distributions on the rotation manifold. arXiv preprint\narXiv:2106.05965, 2021.\n[79] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor, M. Liu,\nE. Romo, et al. Robotic pick-and-place of novel objects in clutter with multi-affordance grasping\nand cross-domain image matching. In 2018 IEEE international conference on robotics and\nautomation, pages 1\u20138. IEEE, 2018.\n[80] C. Deng, O. Litany, Y. Duan, A. Poulenard, A. Tagliasacchi, and L. J. Guibas. Vector neurons:\nA general framework for so(3)-equivariant networks. In ICCV, 2021.\n[81] A. Fishman, A. Murali, C. Eppner, B. Peele, B. Boots, and D. Fox. Motion policy networks. In\nConference on Robot Learning, pages 967\u2013977. PMLR, 2023.\n14\nShelving, Stacking, Hanging: Relational Pose Diffusion for\nMulti-modal Rearrangement \u2013 Supplementary Material\nSection A1 includes additional visualizations of iterative test-time evaluation on simulated shapes\nand examples of object-scene point clouds that were used as training data. In Section A2, we present\ndetails on data generation, model architecture, and training for RPDiff. In Section A3 we elaborate\nin more detail on the multi-step iterative regression inference procedure which predicts the set of\nrearrangement transforms. Section A4 describes more details about how the success classifier is\ntrained and used in conjunction with our transform predictor as a simple mechanism for selecting\nwhich among multiple candidate transforms to execute. In Section A5, we describe more details\nabout our experimental setup, and Section A6 discusses more details on the evaluation tasks and robot\nexecution pipelines. In Section A7 we present an additional set of ablations to highlight the impact\nof other hyperparameters and design decisions. Section A8 describes additional implementation\ndetails for the real-world executions along with an expanded discussion on limitations and avenues\nfor future work. Finally, Section A9 shows model architecture diagrams a summarized set of relevant\nhyperparameters that were used in training and evaluation.\nA1\nAdditional Test-time and Training Data Visualizations\nHere, we show additional visualizations of the tasks used in our simulation experiments and the noised\npoint clouds used to train our pose regression model. Figure A1 shows snapshots of performing the\niterative de-noising at evaluation time with simulated objects, and Figure A2 shows examples of\nthe combined object-scene point clouds and their corresponding noised versions that were used for\ntraining to perform iterative de-noising.\nA2\nIterative Pose Regression Training and Data Generation\nIn this section, we present details on the data used for training the pose diffusion model in RPDiff,\nthe neural network architecture we used for processing point clouds and predicting SE(3) transforms,\nand details on training the model.\nA2.1\nTraining Data Generation\nObjects used in simulated rearrangement demonstrations. We create the rearrangement demon-\nstrations in simulation with a set of synthetic 3D objects. The three tasks we consider include objects\nfrom five categories: mugs, racks, cans, books, \u201cbookshelves\u201d (shelves partially filled with books),\nand \u201ccabinets\u201d (shelves partially-filled with stacks of cans). We use ShapeNet [70] for the mugs and\nprocedurally generate our own dataset of .obj files for the racks, books, shelves, and cabinets. See\nFigure A3 for representative samples of the 3D models from each category.\nProcedurally generated rearrangement demonstrations in simulation. The core regression\nmodel f\u03b8 in RPDiff is trained to process a combined object-scene point cloud and predict an\nSE(3) transformation updates the pose of the object point cloud. To train the model to make these\nrelative pose predictions, we use a dataset of demonstrations showing object and scene point clouds\nin final configurations that satisfy the desired rearrangement tasks. Here we describe how we obtain\nthese \u201cfinal point cloud\u201d demonstrations\nWe begin by initializing the objects on a table in PyBullet [19] in random positions and orientations\nand render depth images with the object segmented from the background using multiple simulated\ncameras. These depth maps are converted to 3D point clouds and fused into the world coordinate\nframe using known camera poses. To obtain a diverse set of point clouds, we randomize the number\nof cameras (1-4), camera viewing angles, distances between the cameras and objects, object scales,\nand object poses. Rendering point clouds in this way allows the model to see some of the occlusion\npatterns that occur when the objects are in different orientations and cannot be viewed from below\nthe table. To see enough of the shelf/cabinet region, we use the known state of the shelf/cabinet to\nposition two cameras that roughly point toward the open side of the shelf/cabinet.\n15\n(a) Mug/Rack\n(b) Book/Shelf\n(c) Can/Cabinet\nFigure A1: Visualizations of multiple steps of iterative de-noising on simulated objects. Starting\nfrom the left side, each object is initialized in a random SE(3) pose in the vicinity of the scene. Over\nmultiple iterations, RPDiff updates the object pose. The right side shows the final set of converged\nsolutions.\n16\nCan/Cabinet\nFinal combined\nobject/scene \nInterpolated \nnoise steps\nSample w/\ncropped scene\nBook/Shelf\nFinal combined\nobject/scene \nInterpolated \nnoise steps\nSample w/\ncropped scene\nMug/Rack-Multi\nFinal combined\nobject/scene \nInterpolated \nnoise steps\nSample w/\ncropped scene\nFigure A2: Example point clouds from the demonstrations for each task of Can/Cabinet (top),\nBook/Shelf (middle) and Mug/RackMed-Multi (bottom). For each task, the top row shows the\nground truth combined object-scene point cloud. Scene point clouds are in black and object point\nclouds are in dark blue. The middle row in each task shows an example of creating multiple steps of\nnoising perturbations by uniformly interpolating a single randomly sampled perturbation transform\n(with a combination of linear interpolation for the translation and SLERP for the rotation). Different\ncolors show the point clouds at different interpolated poses. The bottom row shows a sampled step\namong these interpolated poses, with the corresponding \u201cnoised\u201d object point cloud (dark blue),\nground truth target point cloud (light blue), and cropped scene point cloud (red).\n17\nmugs\nracks\nbooks +\nshelves\ncans +\ncabinets\nFigure A3: Example 3D models used to train RPDiff and deploy RPDiff on our rearrangement tasks.\nMugs are from ShapeNet [70] while we procedurally generated our own synthetic racks, books, cans,\nshelves, and cabinets.\nAfter obtaining the initial object and scene point clouds, we obtain an SE(3) transform to apply to\nthe object, such that transforming into a \u201cfinal\u201d objct pose using this transform results in the desired\nplacement. This transform is used to translate and rotate the initial object point cloud, such that\nthe combined \u201cfinal object\u201d and scene point cloud can be used for generating training examples.\nFigure A2 shows example visualizations of the final point clouds in the demonstrations for each task.\nWe obtain the final configuration that satisfies these tasks using a combination of privileged knowledge\nabout the objects in the simulator (e.g., ground truth state, approximate locations of task-relevant\nobject parts, 3D mesh models for each object, known placing locations that are available) and human\nintuition about the task. To create mug configurations that satisfy \u201changing\u201d on one of the pegs of a\nrack, we first approximately locate one of the pegs on one of the racks (we select one uniformly at\nrandom) and the handle on the mug (which is straightforward because all the ShapeNet mugs are\naligned with the handle pointing in the +y axis of the body frame). We then transform the mug so that\nthe handle is approximately \u201con\u201d the selected hook. Finally, we sample small perturbations about this\nnominal pose until we find one that does not lead to any collision/penetration between the two shapes.\nWe perform an analogous process for the other tasks, where the ground truth available slots in the\nbookshelf and positions that work for placing the mug (e.g., on top of a stack, or on a flat shelf region\nin between existing stacks) are recorded when the 3D models for the shelves/cabinets are created.\nThe exact methods for generating these shapes and their corresponding rearrangement poses can be\nfound in our code.\n18\nA2.2\nPose Prediction Architecture\nTransformer point cloud processing and pose regression. We follow the Transformer [15] architec-\nture proposed in Neural Shape Mating [3] for processing point clouds and computing shape features\nthat are fed to the output MLPs for pose prediction.\nWe first downsample the observed point clouds PO \u2208 RN \u2032\u00d73 and PS \u2208 RM \u2032\u00d73 using farthest\npoint sampling into \u00afPO \u2208 RN\u00d73 and \u00afPS \u2208 RM\u00d73. We then normalize to create PO\nnorm \u2208 RN\u00d73\nand PS\nnorm \u2208 RM\u00d73, based on the centroid of the scene point cloud and a scaling factor that\napproximately scales the combined point cloud to have extents similar to a unit bounding box:\n\u00afPS =\n\uf8ee\n\uf8ef\uf8f0\npS\n1\npS\n2\n...\npS\nM\n\uf8f9\n\uf8fa\uf8fb\n\u00afPO =\n\uf8ee\n\uf8ef\uf8f0\npO\n1\npO\n2\n...\npO\nM\n\uf8f9\n\uf8fa\uf8fb\npS,cent = 1\nM\nM\nX\ni=1\npS\ni\na = max{pS\ni } \u2212 min{pS\ni }\nPS\nnorm =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\npS,norm\n1\npS,norm\n2\n...\npS,norm\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb\npS,norm\ni\n= a(pS\ni \u2212 pS,cent)\n\u2200 i \u2208 1, ..., M\nPO\nnorm =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\npO,norm\n1\npO,norm\n2\n...\npO,norm\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb\npO,norm\ni\n= a(pO\ni \u2212 pS,cent)\n\u2200 j \u2208 1, ..., N\nNext, we \u201ctokenize\u201d the normalized object/scene point clouds into d-dimensional input features\n\u03d5O \u2208 RN\u00d7d and \u03d5S \u2208 RM\u00d7d by concatenating a two-dimensional one-hot feature to each point in\nPO\nnorm and PS\nnorm (to explicitly inform the Transformer which points correspond to the object and\nthe scene) and projecting to a d-dimensional vector with a linear layer Win \u2208 Rd\u00d75:\n\u03d5S =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nWin\u00afpS,norm\n1\nWin\u00afpS,norm\n2\n...\nWin\u00afpS,norm\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00afpS,norm\ni\n= pS,norm\ni\n\u2295 [1, 0]\n\u2200 i \u2208 1, ..., M\n\u03d5O =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nWin\u00afpO,norm\n1\nWin\u00afpO,norm\n2\n...\nWin\u00afpO,norm\nM\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00afpO,norm\nj\n= pO,norm\ni\n\u2295 [0, 1]\n\u2200 j \u2208 1, ..., N\nNote we could also pass the point cloud through a point cloud encoder to pool local features together,\nas performed in NSM via DGCNN [17]. We did not experiment with this as we obtained satisfactory\nresults by directly operating on the individual point features, but it would likely perform similarly or\neven better if we first passed through a point cloud encoder. We also incorporate the timestep t that\nthe current prediction corresponds to by including the position-encoded timestep as an additional\ninput token together with the object point tokens as \u00af\u03d5O \u2208 R(N+1)\u00d7d where \u00af\u03d5O =\n\u0014\n\u03d5O\npos emb(t)\n\u0015\n.\nWe then use a Transformer encoder and decoder to process the combined tokenized point cloud (see\nFigure A4 for visual depiction). This consists of performing multiple rounds of self-attention on the\nscene features (encoder) and then performing a combination of self-attention on the object point\ncloud together with cross-attention between the object point cloud and the output features of the\nscene point cloud (decoder):\n19\nTimestep embedding\nPool\nSelf\nAttn\nTransform \nSelf\nAttn\nCross\nAttn\nPoint tokens\nMLP\nto\nFigure A4: Architecture diagram showing a combination of self-attention and cross-attention among\nobject and scene point cloud for SE(3) transform prediction. The scene point cloud is processed\nvia multiple rounds of self-attention, while the object features are combined via a combination of\nself-attention and cross-attention with the scene point cloud. The timestep embedding is incorporated\nas both an input token and via a residual connection with the pooled output feature. The global output\nfeature is used to predict the translation and rotation that are applied to the object point cloud.\nqS = QE(\u03d5S)\nkS = KE(\u03d5S)\nvS = VE(\u03d5S)\nsS = Attention(qS, kS, vS) = softmax\n\u0010qSkS\nT\n\u221a\nd\n\u0011\nvS\nqO = QD(\u00af\u03d5O)\nkO = KD(\u00af\u03d5O)\nvO = VD(\u00af\u03d5O)\nsO = Attention(qO, kO, vO) = softmax\n\u0010qOkO\nT\n\u221a\nd\n\u0011\nvO\nhO = Attention(q = sO, k = sS, v = sS) = softmax\n\u0010sOsST\n\u221a\nd\n\u0011\nsS\nThis gives a set of output features hO \u2208 R(N+1)\u00d7d where d is the dimension of the embedding\nspace. We compute a global feature by mean-pooling the output point features and averaging with\nthe timestep embedding as a residual connection, and then use a set of output MLPs to predict the\ntranslation and rotation (the rotation is obtained by converting a pair of 3D vectors into an orthonormal\nbasis and then stacking into a rotation matrix [10, 18]):\n\u00afhO = 1\n2\n\u0010 1\nN\nN+1\nX\ni=1\nhO,i + pos emb(t)\n\u0011\n\u00afhO \u2208 Rd\nt = MLPtrans(\u00afhO)\nt \u2208 R3\na, b = MLProt(\u00afhO)\na \u2208 R3, b \u2208 R3\n\u02c6a =\na\n||a||\n\u02c6b = b \u2212 \u27e8\u02c6a, b\u27e9\u02c6a\n||b||\n\u02c6c = \u02c6a \u00d7 \u02c6b\nR =\n\uf8ee\n\uf8f0\n|\n|\n|\n\u02c6a\n\u02c6b\n\u02c6c\n|\n|\n|\n\uf8f9\n\uf8fb\nLocal scene point cloud cropping. As shown in the experimental results, local cropping helps\nimprove performance due to increasing precision while generalizing well to unseen layouts of the\nscene. Our \u201cFixed\u201d cropping method uses a box with fixed side length Lbox = Lmin, centered at the\ncurrent object point cloud iterate across all timesteps, and selects scene point cloud points that lie\nwithin this box. Our \u201cVarying\u201d cropping method adjusts the length of the box based on the timestep,\n20\nwith larger timesteps using a larger crop, and smaller timesteps using a smaller crop. We parameterize\nthis as a function of the timestep t via the following linear decay function:\nLbox = Lmin + (Lmax \u2212 Lmin)T \u2212 t\nT\nwhere Lmin and Lmax are hyperparameters.\nApplying Predicted Transforms to Object Point Cloud. We apply the predicted rotation and\ntranslation by first mean-centering the object point cloud, applying the rotation, and then translating\nback to the original world frame position, and then finally translating by the predicted translation.\nThis helps reduce sensitivity to the rotation prediction, whereas if we rotate about the world frame\ncoordinate axes, a small rotation can cause a large configuration change in the object.\nA2.3\nTraining Details\nHere we elaborate on details regarding training the RPDiff pose diffusion model using the demonstra-\ntion data and model architecture described in the sections above. A dataset sample consists of a tuple\n(PO, PS). From this tuple, we want to construct a perturbed object point cloud PO\n(t) for a particular\ntimestep t \u2208 1, ..., T, where lower values of t correspond to noised point clouds that are more similar\nto the ground truth, and larger values of T are more perturbed. At the limit, the distribution of point\nclouds corresponding to t = T should approximately match the distribution we will sample from\nwhen initializing the iterative refinement procedure at test time.\nNoising schedules and perturbation schemes are an active area of research currently in the diffusion\nmodeling litierature [71, 72], and there are many options available for applying noise to the data\nsamples. We apply a simple method that makes use of uniformly interpolated SE(3) transforms.\nFirst, we sample one \u201clarge\u201d transform from the same distribution we use to initialize the test-time\nevaluation procedure from \u2013 rotations are sampled uniformly from SO(3) and translations are\nsampled uniformly within a bounding box around the scene point cloud. We then use a combination\nof linear interpolation on the translations, and spherical-linear interpolation (SLERP) on the rotations,\nto obtain a sequence of T uniformly-spaced transforms (see Fig. A2 for example visualizations).\nBased on the sampled timestep t, we select the transform corresponding to timestep t in this sequence\nas the noising perturbation T(t)\nnoise, and use the transform corresponding to timestep t \u2212 1 to compute\nthe \u201cincremental\u201d/\u201cinterval\u201d transform to use as a prediction target. As discussed in Section 3.1, using\nthe incremental transform as a prediction target helps maintain a more uniform output scale among\nthe predictions across samples, which is beneficial for neural network optimization as it minimizes\ngradient fluctuations [14]. We also provide quantitative evidence that predicting only the increment\ninstead of the full inverse perturbation benefits overall performance. See Section A7 for details.\nThe main hyperparameter for this procedure is the number of steps T. In our experiments, we\nobserved it is important to find an appropriate value for T. When T is too large, the magnitude of the\ntransforms between consecutive timesteps is very small, and the iterative predictions at evaluation\ntime make tiny updates to the point cloud pose, oftentimes failing to converge. When T is too small,\nmost of the noised point clouds will be very far from the ground truth and might look similar across\ntraining samples but require conflicting prediction targets, which causes the model to fit the data\npoorly. We found that values in the vicinity of T = 5 work well across our tasks (T = 2 and T = 50\nboth did not work well). This corresponds to an average perturbation magnitude of 2.5cm for the\ntranslation and 18 degrees for the rotation.\nAfter obtaining the ground truth prediction target, we compute the gradient with respect to the loss\nbetween the prediction and the ground truth, which is composed of the mean-squared translation error,\na geodesic rotation distance error [12, 13], and the chamfer distance between the point cloud obtained\nby applying the predicted transform and the ground-truth next point cloud. We also found the model\nto work well using either just the chamfer distance loss or the combined translation/rotation losses.\nWe trained a separate model for each task, with each model training for 500 thousand iterations on\na single NVIDIA V100 GPU with a batch size of 16. We used a learning rate schedule of linear\nwarmup and cosine decay, with a maximum learning rate of 1e-4. Training takes about three days.\n21\nSkill Type\nNumber\nof samples\nMug/EasyRack\n3190\nMug/MedRack\n950\nMug/Multi-MedRack\n3240\nBook/Shelf\n1720\nCan/Cabinet\n2790\nTable 2: Number of demonstrations used in each task. The same set of demonstrations is used to train\nboth our method and each baseline method.\nWe train the models using the AdamW [73] optimizer. Table 2 includes the number of demonstrations\nwe used for each task.\nA3\nTest time evaluation\nHere, we elaborate in more detail on the iterative de-noising procedure performed at test time. Starting\nwith PO and PS, we sample K initial transforms { \u02c6T(I)\nk }K\nk=1, where initial rotations are drawn from\na uniform grid over SO(3) , and we uniformly sample translations that position the object within the\nbounding box of the scene point cloud. We create K copies of PO and apply each corresponding\ntransform to create initial object point clouds {\u02c6P(I)\nO,k}K\nk=1 where \u02c6P(I)\nO,k = \u02c6T(I)\nk PO. We then perform\nthe following update for I steps for each of the K initial transforms:\n\u02c6T(i\u22121) = (TRand\n\u2206\n\u02c6T\u2206) \u02c6T(n)\n\u02c6P(n\u22121)\nO\n= (TRand\n\u2206\n\u02c6T\u2206)\u02c6P(i)\nO\nwhere transform \u02c6T\u2206 is obtained as \u02c6T\u2206 = f\u03b8(\u02c6P(i)\nO , PS, pos emb(i to t(i))). Transform TRand\n\u2206\nis\nsampled from a timestep-conditioned uniform distribution that converges toward deterministically\nproducing an identify transform as i tends toward 0. We obtain the random noise by sampling from\na Gaussian distribution for both translation and rotation. For the translation, we directly output a\n3D vector with random elements. For the rotation, we represent the random noise via axis angle 3D\nrotation R0\naa \u2208 R3 and convert it to a rotation matrix using the SO(3) exponential map [74] (and a\n3D translation t0 \u2208 R3). We exponentially decay the variance of these noise distributions so that\nthey produce nearly zero effect as the iterations tend toward 0. We perform the updates in a batch.\nThe full iterative inference procedure can be found in Alg. 1.\nEvaluation timestep scheduling and prediction behavior for different timestep values.. The\nfunction i to t is used to map the iteration number i to a timestep value t that the model has been\ntrained on. This allows the number of steps during evaluation (I) to differ from the number of steps\nduring training (T). For example, we found values of T = 5 to work well during training but used\na default value of I = 50 for evaluation. We observed this benefits performance since running\nthe iterative evaluation procedure for many steps helps convergence and enables \u201cbouncing out\u201d of\n\u201clocally optimal\u201d solutions. However, we found that if we provide values for i that go beyond the\nsupport of what the model is trained on (i.e., for i > T), the predictions perform poorly. Thus, the\nfunction i to t ensures all values i \u2208 1, ..., I are mapped to an appropriate value t \u2208 1, ..., T that\nthe model has seen previously.\nThere are many ways to obtain this mapping, and different implementations produce different kinds\nof behavior. This is because different i to t schedules emphasize using the model in different ways\nsince the model learns qualitatively different behavior for different values of t. Specifically, for\nsmaller values of t, the model has only been trained on \u201csmall basins of attraction\u201d and thus the\npredictions are more precise and local, which allows the model to \u201csnap on\u201d to any solution in the\nimmediate vicinity of the current object iterate. Figure A5 shows this in a set of artifically constrained\nevaluation runs where the model is constrained to use the same timestep for every step i = 1, ..., I.\nHowever, this can also lead the model to get stuck near regions that are far from any solution. On the\nother hand, for larger perturbations, the data starts to look more multi-modal and the model averages\n22\nAlgorithm 1 Rearrangement Transform Inference via Iterative Point Cloud De-noising\n1: Input: Scene point cloud PS, object point cloud PO, number of parallel runs K, number of\niterations to use in evaluation I, number of iterations used in training T, pose regression model f\u03b8,\nsuccess classifier h\u03d5, function to map from evaluation iteration values to training iteration values\ni to t, parameters for controlling what fraction of evaluation iterations correspond to smaller\ntraining timestep values A, local cropping function crop, distribution for sampling external pose\nnoise pAnnealedRandSE(3)\n# Init transforms, transformed object, and cropped scene\n2: for k in 1,...,K do\n3:\nR(H)\nk\n\u223c pUnifSO(3)(\u00b7)\n4:\nt(H)\nk\n\u223c pUnifBoundingBox( \u00b7 | PO, PS)\n5:\n\u02c6T(H)\nk\n=\n\u0014\nR\nt\n0\n1\n\u0015\n6:\n\u02c6P(H)\nO,k = \u02c6T(H)\nk\nPO\n7:\n\u00afP(H)\nS,k = crop(\u02c6P(H)\nO,k, PS)\n8: end for\n# Init set of transform and final point cloud solutions and classifier scores\n9: init S = \u2205\n10: init T = \u2205\n11: init P = \u2205\n# Iterative pose regression\n12: for i in I,...,1 do\n# Map evaluation timestep to in-distribution training timestep\n13:\nt = i to t(i, A)\n14:\nfor k in 1,...,K do\n15:\n\u02c6T\u2206,k = f\u03b8(P(t)\nO,k, \u00afP(t)\nS,k, pos emb(t))\n16:\nif i > (0.2 \u2217 I) then\n# Apply random external noise, with noise magnitude annealed as i approaches 0\n17:\nTRand\n\u2206,k \u223c pAnnealedRandSE(3)(\u00b7 | i)\n18:\nelse\n# Remove all external noise for the last 20% of the iterations\n19:\nTRand\n\u2206,k = I4\n20:\nend if\n21:\n\u02c6T(i\u22121)\nk\n= TRand\n\u2206,k T\u2206,k \u02c6T(i)\nk\n22:\n\u02c6P(i\u22121)\nO,k\n= TRand\n\u2206,k T\u2206,k \u02c6P(i)\nO,k\n23:\n\u00afP(i\u22121)\nS,k\n= crop(\u02c6P(i\u22121)\nO,k , PS, t, T)\n24:\nif i == 1 then\n# Predict success probabilities from final objects\n25:\nsk = h\u03d5(P(0)\nO,k, PS)\n# Save final rearrangement solutions and predicted scores\n26:\nS = S \u222a {sk}\n27:\nT = T \u222a {\u02c6T(0)\nk }\n28:\nP = T \u222a {\u02c6P(0)\nO,k}\n29:\nend if\n30:\nend for\n31: end for\n# Decision rule (e.g., argmax) for output\n32: kout = argmax(S)\n33: Tout = T [kout]\n# Return top-scoring transform and full set of solutions for potential downstream planning/search\n34: return Tout, T , P, S\n23\nFigure A5: Examples of running our full iterative evaluation procedure for I steps with the model\nconstrained to use a fixed value for t on each iteration. This highlights the different behavior the\nmodel has learned for different timesteps in the de-noising process. For timesteps near 1, the model\nhas learned to make very local updates that \u201csnap on\u201d to whatever features are in the immediate\nvicinity of the object. As the timesteps get larger, the model considers a more global context and\nmakes predictions that reach solutions that are farther away from the initial object pose. However,\nthese end up more biased to a single solution in a region where there may be many nearby solutions\n(see the top row of shelves where there are four slots that the model finds when using timestep 1, but\nthe model only reaches two of them with timestep t = 2 and one of them with t = 3). For even larger\nvalues of t, the model has learned a much more biased and \u201caveraged out\u201d solution that fails to rotate\nthe object and only approximately reaches the scene regions corresponding to valid placements.\nout toward either a biased solution in the direction of a biased region, or just an identity transform\nthat doesn\u2019t move the object at all.\nWe find the pipeline performs best when primarily using predictions corresponding to smaller\ntimesteps, but still incorporating predictions from higher timesteps. We thus parameterize the\ntimestep schedule i to t such that it exponentially increases the number of predictions used for\nsmaller values of t. While there are many ways this can be implemented, we use the following\nprocedure: we construct an array D of length I where each element lies between 1 and T, and define\nthe mapping i to t as\nt = i to t(i) = Di\nsubscript i denotes the i-th element of D\nThe array D is parameterized by a constant value A (where a higher value of A corresponds to using\nmore predictions with smaller timesteps, while A = 1 corresponds to using each timestep an equal\nnumber of times) and ensures that predictions for each timestep are made at least once:\nB = [AT , , AT \u22121..., A2, A1]\nExponentially decreasing values\nC = \u2308\nA \u2217 I\nPT\ni=1 Ai\n\u2309\nNormalize, scale up by I, and round up (minimum value per element is 1)\n\u00afC = \u2308\nC \u2217 I\nPT\ni=1 Ci\n\u2309\nNormalize again so\nT\nX\ni=1\n\u00afCi \u2248 I with \u00afCi \u2208 N \u2200 i = 1, ..., T\n\u00afC1 = \u00afC0 \u2212 (\nT\nX\ni=1\n\u00afCi \u2212 I)\nEnsure\nT\nX\ni=1\n\u00afCi = I exactly\n24\nThen, from \u00afC, we construct multiple arrays with values ranging from 1 to T, each with lengths\ncorresponding to values in \u00afC,\n\u00afD1 = [ \u00afD1,1 \u00afD1,2 ...] with \u00afD1,k = 1 \u2200k \u2208 1, ..., \u00afC1\n\u00afD2 = [ \u00afD2,1 \u00afD2,2 ...] with \u00afD2,k = 2 \u2200k \u2208 1, ..., \u00afC2\n...\n\u00afDT = [ \u00afDT,1 \u00afDT,2 ...] with \u00afDT,k = T \u2200k \u2208 1, ..., \u00afCT\nand then stack these arrays together to obtain D as a complete array of length I:\nD = [ \u00afD1 \u00afD2 ... \u00afDT ]\nA4\nSuccess Classifier Details\nIn this section, we present details on training and applying the success classifier h\u03d5 that we use for\nranking and filtering the set of multiple predicted SE(3) transforms produced by RPDiff.\nTraining Data. To train the success classifier, we use the demonstrations to generate positive and\nnegative examples, where the positives are labeled with success likelihood 1.0 and the negatives have\nsuccess likelihood 0.0. The positives are simply the unperturbed final point clouds and the negatives\nare perturbations of the final object point clouds. We use the same sampling scheme of sampling a\nrotation from a uniform distribution over SO(3) and sampling a translation uniformly from within a\nbounding box around the scene point cloud.\nModel Architecture. We use an identical Transformer architecture as described in Section A2, except\nthat we use a single output MLP followed by a sigmoid to output the predicted success likelihood, we\ndo not condition on the timestep, and we provide the uncropped scene point cloud.\nTraining Details. We supervise the success classifier predictions with a binary cross entropy loss\nbetween the predicted and ground truth success likelihood. We train for 500k iterations with batch size\n64 on a V100 GPU which takes 5 days. We augment the data by rotating the combined object-scene\npoint cloud by random 3D rotations to increase dataset diversity.\nA5\nExperimental Setup\nThis section describes the details of our experimental setup in simulation and the real world.\nA5.1\nSimulated Experimental Setup\nWe use PyBullet [19] and the AIRobot [75] library to set up the tasks in the simulation and quantita-\ntively evaluate our method along with the baselines. The environment consists of a table with the\nshapes that make up the object and the scene, and the multiple simulated cameras that are used to\nobtain the fused 3D point cloud. We obtain segmentation masks of the object and the scene using\nPyBullet\u2019s built-in segmentation abilities.\nA5.2\nReal World Experimental Setup\nIn the real world, we use a Franka Robot arm with a Robotiq 2F140 parallel jaw gripper attached\nfor executing the predicted rearrangements. We also use four Realsense D415 RGB-D cameras\nwith known extrinsic parameters. Two of these cameras are mounted to provide a clear, close-up\nview of the object, and the other two are positioned to provide a view of the scene objects. We\nuse a combination of Mask-RCNN, density-based Euclidean clustering [76], and manual keypoint\nannotation to segment the object, and use simple cropping heuristics to segment the overall scene\nfrom the rest of the background/observation (e.g., remove the table and the robot from the observation\nso we just see the bookshelf with the books on it).\n25\nA6\nEvaluation Details\nThis section presents further details on the tasks we used in our experiments, the baseline methods\nwe compared RPDiff against, and the mechanisms we used to apply the predicted rearrangement to\nthe object in simulation and the real world.\nA6.1\nTasks and Evaluation Criteria\nTask Descriptions. We consider three relational rearrangement tasks for evaluation: (1) hanging a\nmug on the hook of a rack, where there might be multiple racks on the table, and each rack might\nhave multiple hooks, (2) inserting a book into one of the multiple open slots on a randomly posed\nbookshelf that is partially filled with existing books, and (3) placing a cylindrical can upright either\non an existing stack of cans or on a flat open region of a shelf where there are no cans there. Each\nof these tasks features many placing solutions that achieve the desired relationship between the\nobject and the scene (e.g., multiple slots and multiple orientations can be used for placing, multiple\nracks/hooks and multiple orientations about the hook can be used for hanging, multiple stacks and/or\nmultiple regions in the cabinet can be used for placing the can, which itself can be placed with either\nflat side down and with any orientation about its cylindrical axis).\nEvaluation Metrics and Success Criteria. To quantify performance, we report the average success\nrate over 100 trials, where we use the ground truth simulator state to compute success. For a trial to\nbe successful, the object O and S must be in contact and the object must have the correct orientation\nrelative to the scene (for instance, the books must be on the shelf, and must not be oriented with the\nlong side facing into the shelf). For the can/cabinet task, we also ensure that the object O did not run\ninto any existing stacks in the cabinet, to simulate the requirement of avoiding hitting the stacks and\nknocking them over.\nWe also quantify coverage via recall between the full set of predicted solutions and the precomputed\nset of solutions that are available for a given task instance. This is computed by finding the closest\nprediction to each of the precomputed solutions and checking whether the translation and rotation\nerror between the prediction and the solution is within a threshold (we use 3.5cm for the translation\nand 5 degrees for the rotation). If the error is within this threshold, we count the solution as \u201cdetected\u201d.\nWe compute recall for a trial as the total number of \u201cdetected solutions\u201d divided by the total number of\nsolutions available and report overall recall as the average over the 100 trials. Precision is computed\nin an analogous fashion but instead checks whether each prediction is within the threshold for at least\none of the ground truth available solutions.\nA6.2\nBaseline Implementation and Discussion\nIn this section, we elaborate on the implementation of each baseline approach in more detail and\ninclude further discussion on the observed behavior and failure modes of each approach.\nA6.2.1\nCoarse-to-Fine Q-attention (C2F-QA).\nC2F-QA adapts the classification-based approach proposed in [8], originally designed for pick-and-\nplace with a fixed robotic gripper, to the problem of relational object rearrangement. We voxelize the\nscene and use a local PointNet [77] that operates on the points in each voxel to compute per-voxel\ninput features. We then pass this voxel feature grid through a set of 3D convolution layers to compute\nan output voxel feature grid. Finally, the per-voxel output features are each passed through a shared\nMLP which predicts per-voxel scores. These scores are normalized with a softmax across the grid to\nrepresent a distribution of \u201caction values\u201d representing the \u201cquality\u201d of moving the centroid of the\nobject to the center of each respective voxel. This architecture is based on the convolutional point\ncloud encoder used in Convolutional Occupany Networks [7].\nTo run in a coarse-to-fine fashion, we take the top-scoring voxel position (or the top-k voxels if\nmaking multiple predictions), translate the object point cloud to this position, and crop the scene\npoint cloud to a box around the object centroid position. From this cropped scene and the translated\n26\nobject, we form a combined object-scene input point cloud and re-voxelize just this local portion of\nthe point cloud at a higher resolution. We then compute a new set of voxel features with a separate\nhigh-resolution convolutional point cloud encoder. Finally, we pool the output voxel features from\nthis step and predict a distribution over a discrete set of rotations to apply to the object. We found\ndifficulty in using the discretized Euler angle method that was applied in [8], and instead directly\nclassify in a binned version of SO(3) by using an approximate uniform rotation discretization method\nthat was used in [78].\nWe train the model to minimize the cross entropy loss for both the translation and the rotation (i.e.,\nbetween the ground truth voxel coordinate containing the object centroid in the demonstrations and\nthe ground truth discrete rotation bin). We use the same object point cloud perturbation scheme to\ncreate initial \u201cnoised\u201d point clouds for the model to de-noise but have the model directly predict how\nto invert the perturbation transform in one step.\nOutput coverage evaluation. Since C2F-QA performs the best in terms of task success among\nall the baselines and is naturally suited for handling multi-modality by selecting more than just the\nargmax among the binned output solutions, we evaluate the ability of our method and C2F-QA to\nachieve high coverage among the available placing solutions while still achieving good precision\n(see Section 5.2). To obtain multiple output predictions from C2F-QA, we first select multiple voxel\npositions using the top-k voxel scores output by the PointNet \u2192 3D CNN \u2192 MLP pipeline. We then\ncopy the object point cloud and translate it to each of the selected voxel positions. For each selected\nposition, we pool the local combined object-scene point cloud features and use the pooled features\nto predict a distribution of scores over the discrete space of rotations. Similar to selecting multiple\nvoxel positions, we select the top-k scoring rotations and use this full set of multiple translations +\nmultiple rotations-per-translation as the set of output transforms to use for computing coverage.\nRelationship to other \u201cdiscretize-then-classify\u201d methods. C2F-QA computes per-voxel features\nfrom the scene and uses these to output a normalized distribution of scores representing the quality of\na \u201ctranslation\u201d action executed at each voxel coordinate. This idea of discretizing the scene and using\neach discrete location as a representation of a translational action has been successfully applied by a\nnumber of works in both 2D and 3D [44, 47, 79]. In most of these pipelines, the translations typically\nrepresent gripper positions, i.e., for grasping. In our case, the voxel coordinates represent a location\nto move the object for rearrangement.\nHowever, techniques used by \u201cdiscreteize-then-classify\u201d methods for rotation prediction somewhat\ndiverge. C2F-QA and the recently proposed PerceiverActor [47] directly classify the best discrete\nrotation based on pooled network features. On the other hand, TransporterNets [44] and O2O-\nAfford [46] exhaustively evaluate the quality of different rotation actions by \u201cconvolving\u201d some\nrepresentation of the object being rearranged (e.g., a local image patch or a segmented object point\ncloud) in all possible object orientations, with respect to each position in the entire discretized scene\n(e.g., each pixel in the overall image or each point in the full scene point cloud). The benefit is the\nability to help the model more explicitly consider the \u201cinteraction affordance\u201d between the object and\nthe scene at various locations and object orientations and potentially make a more accurate prediction\nof the quality of each candidate rearrangement action. However, the downside of this \u201cexhaustive\nsearch\u201d approach is the computational and memory requirements are much greater, hence these\nmethods have remained limited to lower dimensions.\nA6.2.2\nRelational Neural Descriptor Fields (R-NDF).\nR-NDF [20] uses a neural field shape representation trained on category-level 3D models of the objects\nused in the task. This consists of a PointNet encoder with SO(3)-equivariant Vector Neuron [80]\nlayers and an MLP decoder. The decoder takes as input a 3D query point and the output of the point\ncloud encoder, and predicts either the occupancy or signed distance of the 3D query point relative to\nthe shape. After training, a point or a rigid set of points in the vicinity of the shape can be encoded by\nrecording their feature activations of the MLP decoder. The corresponding point/point set relative to a\nnew shape can then be found by locating the point/point set with the most similar decoder activations.\n27\nThese point sets can be used to parameterize the pose of local oriented coordinate frames, which can\nrepresent the pose of a secondary object or a gripper that must interact with the encoded object.\nR-NDFs have been used to perform relational rearrangement tasks via the process of encoding task-\nrelevant coordinate frames near the object parts that must align to achieve the desired rearrangement,\nand then localizing the corresponding parts on test-time objects so a relative transform that aligns them\ncan be computed. We use the point clouds from the demonstrations to record a set of task-relevant\ncoordinate frames that must be localized at test time to perform each of the tasks in our experiments.\nThe main downside of R-NDF is if the neural field representation fails to faithfully represent the shape\ncategory, the downstream corresponding matching also tends to fail. Indeed, owing to the global\npoint cloud encoding used by R-NDF, the reconstruction quality on our multi-rack/bookshelf/cabinet\nscenes is quite poor, so the subsequent correspondence matching does not perform well on any of the\ntasks we consider.\nA6.2.3\nNeural Shape Mating (NSM) + CVAE.\nNeural Shape Mating (NSM) [3] uses a Transformer to process a pair of point clouds and predict how\nto align them. The method was originally deployed on the task of \u201cmating\u201d two parts of an object that\nhas been broken but can be easily repurposed for the analogous task of relational rearrangement given\na point cloud of a manipulated object and a point cloud of a scene/\u201cparent object\u201d. Architecturally,\nNSM is the same as our relative pose regression model, with the key differences of (i) being trained\non arbitrarily large perturbations of the demonstration point clouds, (ii) not using local cropping,\nand (iii) only making a single prediction. We call this baseline \u201cNSM-base\u201d because we do not\nconsider the auxiliary signed-distance prediction and learned discriminator proposed in the original\napproach [3]. As shown in Table 1, the standard version of NSM fails to perform well on any of\nthe tasks that feature multi-modality in the solution space (nor can the model successfully fit the\ndemonstration data). Therefore, we adapted it into a conditional variational autoencoder (CVAE) that\nat least has the capacity to learn from multi-modal data and output a distribution of transformations.\nWe use the same Transformer architecture for both the CVAE encoder and decoder with some small\nmodifications to the inputs and outputs to accommodate (i) the encoder also encoding the ground\ntruth de-noising transforms and predicting a latent variable z, and (ii) the decoder conditioning on z\nin addition to the combined object-scene point cloud to reconstruct the transform. We implement\nthis with the same method that was used to incorporate the timestep information in our architecture \u2013\nfor the encoder, we include the ground truth transform as both an additional input token and via a\nresidual connection with the global output feature, and for the decoder, we include the latent variable\nin the same fashion. We also experimented with concatenating the residually connected features and\ndid not find any benefit. We experimented with different latent variable dimensions and weighting\ncoefficients for the reconstruction and the KL divergence loss terms, since the CVAE models still\nstruggled to fit the data well when the KL loss weight was too high relative to the reconstruction.\nHowever, despite this tuning to enable the CVAE to fit the training data well, we found it struggled to\nperform well at test time on unseen objects and scenes.\nA6.3\nCommon failure modes\nThis section discusses some of the common failure modes for each method on our three tasks.\nFor Book/Shelf, our method occasionally outputs a solution that ignores an existing book already\nplaced in the shelf. We also sometimes face slight imprecision in either the translation or rotation\nprevents the book from being able to be inserted. Similarly, the main failure modes on this task\nfrom the baselines are more severe imprecision. C2F-QA is very good at predicting voxel positions\naccurately (i.e., detecting voxels near open slots of the shelf) and the rotation predictions are regularly\nclose to something that would work for book placement, but the predicted book orientations are\nregularly too misaligned with the shelf to allow the insertion to be completed.\nFor Mug/Rack, a scenario where our predictions sometimes fail is when there is a tight fit between\nthe nearby peg and the handle of the mug. For C2F-QA, the predictions appear to regularly ignore the\n28\nlocation of the handle when orienting the mug \u2013 the positions are typically reasonable (e.g., right next\nto one of the pegs on a rack) but the orientation oftentimes appears arbitrary. We also find C2F-QA\nachieves the highest training loss on this task (and hypothesize this occurs for the same reason).\nFinally, for Can/Cabinet, a common failure mode across the board is predicting a can position that\ncauses a collision between the can being placed and an existing stack of cans, which we don\u2019t allow\nto simulate the requirement of avoiding knocking over an existing stack.\nA6.4\nTask Execution\nThis section describes additional details about the pipelines used for executing the inferred relations\nin simulation and the real world.\nA6.4.1\nSimulated Execution Pipeline\nThe evaluation pipeline mirrors the demonstration setup. Objects from the 3D model dataset for the\nrespective categories are loaded into the scene with randomly sampled position and orientation. We\nsample a rotation matrix uniformly from SO(3), load the object with this orientation, and constrain\nthe object in the world frame to be fixed in this orientation. We do not allow it to fall on the table\nunder gravity, as this would bias the distribution of orientations covered to be those that are stable on\na horizontal surface, whereas we want to evaluate the ability of each method to generalize over all\nof SO(3). In both cases, we randomly sample a position on/above the table that are in view for the\nsimulated cameras.\nAfter loading object and the scene, we obtain point clouds PO and PS and use RPDiff to obtain a\nrearrangement transform to execute. The predicted transformation is applied by resetting the object\nstate to a \u201cpre-placement\u201d pose and directly actuating the object with a position controller to follow a\nstraight-line path. Task success is then checked based on the criteria described in the section above.\nPre-placement Offset and Insertion Controller. Complications with automatic success evaluation\ncan arise when directly resetting the object state based on the predicted transform. To avoid such\ncomplications, we simulate a process that mimics a closed-loop controller executing the last few\ninches of the predicted rearrangement from a \u201cpre-placement\u201d pose that is a pure translational offset\nfrom the final predicted placement. For our quantitative evaluations, we use the ground truth state of\nthe objects in the simulator together with prior knowledge about the task to determine the direction of\nthis translational offset. For the mug/rack task, we determine the axis that goes through the handle\nand offset by a fixed distance in the direction of this axis (taking care to ensure it does not go in the\nopposite direction that would cause an approach from the wrong side of the rack). For the can/cabinet\ntask and the book/bookshelf task, we use the known top-down yaw component of the shelf/cabinet\nworld frame orientation to obtain a direction that offsets along the opening of the shelf/cabinet.\nTo execute the final insertion, we reset to the computed pre-placement pose and directly actuate the\nobject with a position controller to follow a straight line path from the pre-placement pose to the\nfinal predicted placement. To simulate some amount of reactivity that such an insertion controller\nwould likely possess in a full-stack rearrangement system, we use the simulator to query contact\nforces that are detected between the object and the scene. If the object pose is not close to the final\npredicted pose when contacts are detected, we back off and sample a small \u201cdelta\u201d translation and\nbody-frame rotation to apply to the object before attempting another straight line insertion. These\nsmall adjustments are attempted up to a maximum of 10 times before the execution is counted as a\nfailure. If, upon detecting contact between the object and the scene, the object is within a threshold of\nits predicted place pose, the controller is stopped and the object is dropped and allowed to fall under\ngravity (which either allows it to settle stably in its final placement among the scene object, or causes\nit to fall away from the scene). We use this same procedure across all methods that we evaluated in\nour experiments.\nWe justify the use of this combination of a heuristically-computed pre-placement pose and \u201ctrial-and-\nerror\u201d insertion controller because (i) it removes the need for a full object-path planning component\n29\nthat searches for a feasible path the object should follow to the predicted placement pose (as this\nplanning problem would be very challenging to solve to due all the nearby collisions between the\nobject and the scene), (ii) it helps avoid other artificial execution failures that can arise when we\nperform the insertion from the pre-placement pose in a purely open-loop fashion, and (iii) it enables\nus to avoid complications that can arise from directly resetting the object state based on the predicted\nrearrangement transform.\nA6.4.2\nReal World Execution Pipeline\nHere, we repeat the description of how we execute the inferred transformation using a robot arm with\nadditional details. At test time, we are given point clouds PO and PS of object and scene, and we\nobtain T, the SE(3) transform to apply to the object from RPDiff. T is applied to O by transforming\nan initial grasp pose Tgrasp, which is obtained using a separate grasp predictor [10], by T to obtain a\nplacing pose Tplace = TTgrasp. As in the simulation setup, we use a set of task-dependent heuristics\nto compute an additional \u201cpre-placement\u201d pose Tpre-place, from which we follow a straight-line end-\neffector path to reach Tplace. We then use off-the-shelf inverse kinematics and motion planning to\nmove the end-effector to Tgrasp and Tplace.\nTo ease the burden of collision-free planning with a grasped object whose 3D geometry is unknown,\nwe also compute an additional set of pre-grasp and post-grasp waypoints which are likely to avoid\ncausing collisions between the gripper and the object during the execution to the grasp pose, and\ncollisions between the object and the table or the rest of the scene when moving the object to the\npre-placement pose. Each phase of the overall path is executed by following the joint trajectory in\nposition control mode and opening/closing the fingers at the correct respective steps. The whole\npipeline can be run multiple times in case the planner returns infeasibility, as the inference methods\nfor both grasp and placement generation have the capacity to produce multiple solutions.\nA7\nExtra Ablations\nIn this section, we perform additional experiments wherein different system components are modified\nand/or ablated.\nWith vs. Without Success Classifier. We use neural network h\u03d5 to act as a success classifier and\nsupport selecting a \u201cbest\u201d output among the K predictions made by our iterative de-noising procedure.\nAnother simple mechanism for selecting an output index kexec for execution would be to uniformly\nsample among the K outputs. However, due to the local nature of the predictions at small values of t\nand the random guess initializations used to begin the inference procedure, some final solutions end\nin configurations that don\u2019t satisfy the task (see the book poses that converge to a region where there\nis no available slot for placement in Figure A5 for A = 10).\nTherefore, a secondary benefit of incorporating h\u03d5 is to filter out predictions that may have converged\nto these \u201clocally optimal\u201d solutions, as these resemble some of the negatives that the classifier has\nseen during training. Indeed, we find the average success rate across tasks with RPDiff when using the\nsuccess classifier is 0.88, while the average success when uniformly sampling the output predictions\nis 0.83. This difference is relatively marginal, indicating that the majority of the predictions made\nby the pose de-noising procedure in RPDiff are precise enough to achieve the task. However, the\nperformance gap indicates that there is an additional benefit of using a final success classifier to rank\nand filter the outputs based on predicted success.\nNoise vs. No Noise. In each update of the iterative evaluation procedure, we update the overall\npredicted pose and the object point cloud by a combination of a transform predicted by f\u03b8 and a\nrandomly sampled \u201cexternal noise\u201d transform TRand\n\u2206\n. The distribution that TRand\n\u2206\nis sampled from is\nparameterized by the iteration number i to converge toward producing an identity transform so the\nfinal pose updates are purely a function of the network f\u03b8.\nThe benefit of incorporating the external noise is to better balance between precision and coverage.\nFirst, external noise helps the pose/point cloud at each iteration \u201cbounce out\u201d of any locally optimal\n30\nNo external noise\nWith external noise (            )\nsmall noise scale \nmedium noise scale\nFigure A6: Examples of running our full iterative evaluation procedure for I steps with different\nvalues of A (and subsequently, D) in our i to t function (which maps from test-time iteration values\nn = 1, ..., I to the timestep values t = 1, .., T that were used in training), and with different amounts\nof external noise TRand\n\u2206\nadded from the annealed external noise distribution pAnnealedRandSE(3)(\u00b7). We\nobserve that with large values of A, the model makes more predictions with smaller values of t. These\npredictions are more local and the overall solutions converge to a more broad set of rearrangement\ntransforms. This sometimes leads to \u201clocally optimal\u201d solutions that fail at the desired task (see top\nright corner with A = 10). With small A, the early iterations are more biased toward the average of\na general region, so the set of transforms tends to collapse on a single solution within a region. By\nincorporating external noise, a better balance of coverage for smaller values of A and \u201clocal optima\u201d\navoidance for larger values of A can be obtained.\nregions and end up near regions where a high quality solution exists. Furthermore, if there are\nmany high-quality solutions close together, the external noise on later iterations helps maintain some\nvariation in the pose so that more overall diversity is obtained in the final set of transform solutions.\nFor instance, see the qualitative comparisons in Figure A6 that include iterative predictions both with\nand without external noise. For a value of A = 1 in i to t, only two of the available shelf slots are\nfound when no noise is included. With noise, however, the method finds placements that cover four\nof the available slots. Quantitatively, we also find that incorporating external noise helps in terms of\noverall success rate and coverage achieved across tasks. The average\n\u0000Success Rate, Recall\n\u0001\nacross\nour three tasks with and without noise was found to be (0.88, 0.44) and (0.83, 0.36), respectively.\nNumber of diffusion steps T during training. The total number of steps T and the noise distribution\nfor obtaining perturbation a transform T(t)\nnoise affects the magnitude of the translation and rotation\npredictions that must be made by the model f\u03b8. While we did not exhaustively search over these\nhyperparameters, early in our experiments we found that very small values of T (e.g., T = 2) cause\nthe predictions to be much more imprecise. This is due to the averaging that occurs between training\nsamples when they are too far away from the ground truth. In this regime, the examples almost\nalways \u201clook multi-modal\u201d to the model. On the other hand, for large values of T (e.g., T = 50),\nthe incremental transforms that are used to de-noise become very small and close to the identity\ntransform. When deployed, models trained on this data end up failing to move the object from its\ninitial configuration because the network has only learned to make extremely small pose updates.\n31\nMetric\nValue of A in i to t\n1\n2\n5\n10\n20\nSuccess Rate\n1.00\n0.95\n0.96\n0.94\n0.90\nRecall (coverage)\n0.37\n0.41\n0.48\n0.48\n0.52\nTable 3: Performance for different values of A in i to t. Larger values of A obtain marginally better\nprecision at the expense of worse coverage (lower recall).\nWe found a moderate value of T = 5 works well across each of our tasks, though other similar values\nin this range can likely also provide good performance. This approximately leads the average output\nscale of the model to be near 2.5cm translation and 18-degree rotation. We also observe a benefit in\nbiasing sampling for the timesteps t = 1, ..., T to focus on smaller values. This causes the model to\nsee more examples close to the ground truth and make more precise predictions on later iterations\nduring deployment. We achieve this biased sampling by sampling t from an exponentially decaying\ncategorical probability distribution over discrete values 1, 2, ..., T.\nIncremental targets vs. full targets. As discussed in Section 3.1, encouraging the network f\u03b8 to\npredict values with roughly equal magnitude is beneficial. To confirm this observation from the\nliterature, we quantitatively evaluate a version of the de-noising model f\u03b8 trained to predict the full\nde-noising transform\n\u0002\nT(t)\nnoise\n\u0003\u22121. The quantitative\n\u0000Success Rate, Recall\n\u0001\nresults averaged across\nour three tasks with the incremental de-noising targets are (0.88, 0.44), while the model trained on\nfull de-noising targets are (0.76, 0.34). These results indicate a net benefit in using the incremental\ntransforms as de-noising prediction targets during training.\nValue of A in i to t. In this section, we discuss the effect of the value A in the i to t function used\nduring the iterative evaluation procedure. The function i to t maps evaluation iteration values i to\ntimestep values t that were seen during training. For instance, we may run the evaluation procedure\nfor 50 iterations, while the model may have only been trained to take values up to t = 5 as input. Our\ni to t function is parameterized by A such that larger values of A lead to more evaluation iterations\nwith small values of t. As A approaches 1, the number of iterations for each value of t becomes equal\n(i.e., for A = 1, the number of predictions made for each value of t is equal to I/T).\nFigure A6 shows qualitative visualizations of de-noising the pose of a book relative to a shelf with\nmultiple available slots with different values of A in the i to t function. This example shows that\nthe solutions are more biased to converge toward a single solution for smaller values of A. This is\nbecause more of the predictions use larger values of t, which correspond to perturbed point clouds\nthat are farther from the ground truth in training. For these perturbed point clouds, their association\nwith the correct target pose compared to other nearby placement regions is more ambiguous. Thus,\nfor large t, the model learns an averaged-out solution that is biased toward a region near the average\nof multiple placement regions that may be close together. On the other hand, for large A, more\npredictions correspond to small values of t like t = 1 and t = 0. For these timesteps, the model has\nlearned to precisely snap onto whatever solutions may exist nearby. Hence, the pose updates are\nmore local and the overall coverage across the K parallel runs is higher. The tradeoff is that these\npredictions are more likely to remain stuck near a \u201clocally optimal\u201d region where a valid placement\npose may not exist. Table 3 shows the quantitative performance variation on the Book/Shelf task for\ndifferent values of A in the i to t function. These results reflect the trend toward higher coverage\nand marginally lower success rate for larger values of A.\nA8\nFurther Discussion on Real-world System Engineering and Limitations\nThis section provides more details on executing rearrangement via pick-and-place on the real robot\n(to obtain the results shown in Figures 1 and 4) and discusses additional limitations of our approach.\n32\nA8.1\nExecuting multiple predicted transforms in sequence in real-world experiments\nThe output of the pose diffusion process in RPDiff is a set of K SE(3) transforms {T(0)\nk }K\nk=1. To\nselect one for execution, we typically score the outputs with success classifier h\u03d5 and search through\nthe solutions while considering other feasibility constraints such as collision avoidance and robot\nworkspace limits. However, to showcase executing a diverse set of solutions in our real-world\nexperiments, a human operator performs a final step of visually inspecting the set of feasible solutions\nand deciding which one to execute. This was mainly performed to ease the burden of recording robot\nexecutions that span the space of different solutions (i.e., to avoid the robot executing multiple similar\nsolutions, which would fail to showcase the diversity of the solutions produced by our method).\nA8.2\nExpanded set of limitations and limiting assumptions\nSection 7 mentions some of the key limitations of our approach. Here, we elaborate on these and\ndiscuss additional limitations, as well as potential avenues for resolving them in future work.\n\u2022 We train from scratch on demonstrations and do not leverage any pre-training or feature-sharing\nacross multiple tasks. This means we require many demonstrations for training. A consequence of\nthis is that we cannot easily provide enough demonstrations to train the diffusion model in the real\nworld (while still enabling it to generalize to unseen shapes, poses, and scene layouts). Furthermore,\nbecause we train only in simulation and directly transfer to the real world, the domain gap causes\nsome challenges in sim2real transfer, so we do observe worse overall prediction performance in the\nreal world. This could be mitigated if the number of demonstrations required was lower and we\ncould train the model directly on point clouds that appear similar to those seen during deployment.\n\u2022 In both simulation and the real world, we manually completed offset poses for moving the object\nbefore executing the final placement. A more ideal prediction pipeline would involve generating\n\u201cwaypoint poses\u201d along the path to the desired placement (or even the full collision-free path, e.g.,\nas in [81]) to support the full insertion trajectory rather than just specifying the final pose.\n\u2022 Our method operates using a purely geometric representation of the object and scene. As such,\nthere is no notion of physical/contact interaction between the object and the scene. If physical\ninteractions were considered in addition to purely geometric/kinematic interactions/alignment, the\nmethod may be even more capable of accurate final placement prediction and avoid some of the\nsmall errors that sometimes occur. For instance, a common error in hanging a mug on a rack is\nto have the handle just miss the hook on the rack. While these failed solutions are geometrically\nvery close to being correct, physically, they are completely different (i.e., in one, contact occurs\nbetween the two shapes, while in the other, there is no contact that can support the mug hanging).\n\u2022 Our method operates using 3D point clouds which are currently obtained from depth cameras.\nWhile this permits us to perform rearrangements with a wide variety of real-world objects/scenes\nthat can be sensed by depth cameras, there are many objects which cannot be observed by depth\ncameras (e.g., thin, shiny, transparent objects). Investigating a way to perform similar relational\nobject-scene reasoning in 6D using signals extracted from RGB sensors would be an exciting\navenue to investigate.\n33\nA9\nModel Architecture Diagrams\nParameter\nValue\nNumber of PO and PS points\n1024\nBatch size\n16\nTransformer encoder blocks\n4\nTransformer decoder blocks\n4\nAttention heads\n1\nTimestep position embedding\nSinusoidal\nTransformer embedding dimension\n256\nTraining iterations\n500k\nOptimizer\nAdamW\nLearning rate\n1e-4\nMinimum learning rate\n1e-6\nLearning rate schedule\nlinear warmup, cosine decay\nWarmup epochs\n50\nOptimizer momentum\n\u03b21 = 0.9, \u03b22 = 0.95\nWeight decay\n0.1\nMaximum training timestep T\n5\nMaximum PS crop size Lmax\nPS bounding box maximum extent\nMinimum PS crop size Lmin\n18cm\nTable 4: Training hyperparameters\nParameter\nValue\nNumber of evaluation iterations I\n50\nNumber of parallel runs K\n32\nDefault value of A in i to t\n10\nExpression for pAnnealedRandSE(3)(\u00b7 | i)\nN(\u00b7 | 0, \u03c3(i))\n\u03c3(i) in pAnnealedRandSE(3) (for trans and rot)\na \u2217 exp(\u2212bi/I)\nValue of a (axis-angle rotation, in degrees)\n20\nValue of b (axis-angle rotation)\n6\nValue of a (translation, in cm)\n3\nValue of b (translation)\n6\nTable 5: Evaluation hyperparameters\n34\nDownsample point clouds\n(N + M) \u00d7 3\nOne-hot concat\n(N + M) \u00d7 5\nLinear\n(N + M) \u00d7 d\nConcat pos emb(t)\n(N + M + 1) \u00d7 d\n\u0002\nSelf-attention (scene)\n\u0003\n\u00d7 4\nM \u00d7 d\n\u0014\nSelf-attention (object)\nCross-attention (object, scene)\n\u0015\n\u00d7 4\n(N + 1) \u00d7 d\nGlobal Pooling\nd\nResidual pos emb(t)\nd\nMLP (translation)\nd \u2192 3\nMLP \u2192 orthonormalize (rotation)\nd \u2192 6 \u2192 3 \u00d7 3\nTable 6: Transformer architecture for predicting SE(3) transforms\nDownsample point clouds\n(N + M) \u00d7 3\nOne-hot concat\n(N + M) \u00d7 5\nLinear\n(N + M) \u00d7 d\n\u0002\nSelf-attention (scene)\n\u0003\n\u00d7 4\nM \u00d7 d\n\u0014\nSelf-attention (object)\nCross-attention (object, scene)\n\u0015\n\u00d7 4\nN \u00d7 d\nGlobal Pooling\nd\nMLP \u2192 sigmoid (success)\nd \u2192 1\nTable 7: Transformer architecture for predicting success likelihood\n35\n"
  },
  {
    "title": "Toward Interactive Dictation",
    "link": "https://arxiv.org/pdf/2307.04008.pdf",
    "upvote": "3",
    "text": "Toward Interactive Dictation\nBelinda Li\u2660\u2217\nJason Eisner3\nAdam Pauls3\nSam Thomson3\n\u2660MIT CSAIL\n3Microsoft Semantic Machines\n\u2660bzl@mit.edu\n3{jason.eisner,adam.pauls,samuel.thomson}@microsoft.com\nAbstract\nVoice dictation is an increasingly important\ntext input modality. Existing systems that al-\nlow both dictation and editing-by-voice restrict\ntheir command language to flat templates in-\nvoked by trigger words. In this work, we study\nthe feasibility of allowing users to interrupt\ntheir dictation with spoken editing commands\nin open-ended natural language. We introduce\na new task and dataset, TERTiUS, to experi-\nment with such systems. To support this flexi-\nbility in real-time, a system must incrementally\nsegment and classify spans of speech as either\ndictation or command, and interpret the spans\nthat are commands. We experiment with using\nlarge pre-trained language models to predict\nthe edited text, or alternatively, to predict a\nsmall text-editing program. Experiments show\na natural trade-off between model accuracy and\nlatency: a smaller model achieves 28% single-\ncommand interpretation accuracy with 1.3 sec-\nonds of latency, while a larger model achieves\n55% with 7 seconds of latency.\n1\nIntroduction\nSpeech can be preferable for text entry, especially\non mobile devices or while the user\u2019s hands are\noccupied, and for some users for whom typing is\nalways slow or impossible. While fast and accu-\nrate automatic speech recognition (ASR) is now\nubiquitous (Kumar et al., 2012; Xiong et al., 2016;\nChiu et al., 2018; Radford et al., 2022), ASR itself\nonly transcribes speech. In practice, users may\nalso wish to edit transcribed text. The ASR output\nmight be incorrect; the user might have misspoken;\nor they might change their mind about what to say\nor how to phrase it, perhaps after seeing or hearing\ntheir previous version. Azenkot and Lee (2013)\nfound that users with visual impairment spent 80%\nof time editing text vs. 20% dictating it.\n\u2217 Work performed during a research internship at Mi-\ncrosoft Semantic Machines.\nJust wanted to ask about the event on \nFriday the 23rd. Is the event still on?\nJust wanted to ask about the event on the 23rd, on \nFriday the 23rd. Is the event still on? Change\u201cthe \nevent\u201d to \u201cit\u201d in the last sentence.\nJust wanted to ask about the event on the \n23rd.\nJust wanted to ask about the event on \nFriday the 23rd.\nJust wanted to check in about the event \non Friday the 23rd. Is it still on?\n(c)\n(d)\n(b)\n(d)\n(a)\n(c)\n(a)\n(b)\n(START)\ncursor\nFigure 1: A user writes an email using speech in-\nput, interleaving dictation (a,c) and commanding (b,d).\nTop shows the continuous user utterance, while bottom\nshows the document state at each point of the utterance.\nDictations are transcribed verbatim, while commands\nare interpreted and executed. Our system supports open-\nended commanding (i.e., b,d both invoke a replace\noperation but use vastly different phrasing).\nIn this work, we study the task of interactive\ndictation, in which users can both perform verba-\ntim dictation and utter open-ended commands in\norder to edit the existing text, in a single uninter-\nrupted speech stream. See Figure 1 for an exam-\nple. Unlike commercial systems like Dragon (DNS;\nNuance, 1997, 2022) and dictation for Word (Mi-\ncrosoft, 2022) that require reserved trigger words\nfor commanding, the commands in our data are\ninvoked using unrestricted natural language (NL).\nFor example, in Figure 1, both (b) and (d) invoke\nreplace commands, but (d) uses nested syntax\nto specify both an edit action and location, while\n(b) is implicit (as natural speech repairs often are).\nIn interactive dictation, users do not need to\nmemorize a list of specific trigger words or tem-\nplates in order to invoke their desired functionality.\nA dictation system should be as intuitive as dic-\narXiv:2307.04008v1  [cs.CL]  8 Jul 2023\ntating to a human assistant\u2014a situation in which\npeople quite naturally and successfully intersperse\nspeech repairs and commands with their dictation.\nBeyond eliminating the learning curve, letting users\nspeak naturally should also allow them to focus on\nwhat they want to say, without being repeatedly\ndistracted by the frustrating separate task of getting\nthose words into the computer.\nBecause we accept unrestricted NL for com-\nmands, both segmentation and interpretation be-\ncome nontrivial for a system to perform.1 Seg-\nmentation requires capturing (sometimes subtle)\nchanges in intent, and is especially difficult in cases\nwhere command boundaries do not align with ASR\nboundaries.2 We collect a dataset of 1320 docu-\nments dictated in an interactive environment with\nlive, incremental ASR transcription and Wizard-of-\nOz\u2013style interpretation of user commands. Anno-\ntators were not told a set of editing features they\nwere allowed to use, but simply instructed to make\ntheir commands understandable and executable by\na hypothetical human helper. Collection required\ndesigning a novel data collection interface. Both\nthe interface and dataset will be publicly released\nto help unlock further work in this area.3\nFinally, we experiment with two strategies for\nimplementing the proposed system: one that uses a\npre-trained language model to directly predict the\nedited text given unedited text and a command, and\nanother that interprets the command as a program\nspecifying how to edit. Predicting intermediate\nprograms reduces latency because the programs are\nshort, at the expense of accuracy. This strategy also\nrequires additional work to design and implement\na set of editing functions and annotate commands\nwith programs that use these functions.\nFor each strategy, we also experimented with two\nchoices of pre-trained language model: a small fine-\ntuned T5 model and a large prompted GPT3 model.\nUsing the smaller model significantly improves\nlatency, though again at the cost of accuracy.\nIn summary, our contributions are: (1) a novel\n1In template-based systems, by contrast, commands can be\ndetected and parsed using regular expressions. An utterance is\nconsidered a command if and only if it matches one of these\nregular expressions.\n2In Figure 1, for example, we must segment the first sen-\ntence into two parts, a dictation (\u201cJust wanted to ask about the\nevent on the 23rd\u201d) and a command (\u201con Friday the 23rd\u201d).\nASR can also overpredict boundaries when speakers pause in\nthe middle of a sentence. For example, in our data \u201cChange\nelude mansion to elude mentioned.\u201d was misrecognized by\nMSS as \u201cChange. Elude mansion to elude mentioned.\u201d\n3https://aka.ms/tertius\ntask (interactive dictation), (2) a novel data collec-\ntion interface for the task, with which we collect a\nnew dataset, and (3) a system that implements said\ntask, with experiments and analysis.\n2\nBackground & Related Work\nMany modern speech input tools only support di-\nrect speech-to-text (e.g.,\nRadford et al., 2022).\nOccasionally, these models also perform disflu-\nency correction, which includes removing filler\nwords (e.g., um), repeated words, false starts, etc.\n(e.g., Microsoft Azure, 2022). One form of disflu-\nency that has received particular attention is speech\nrepair, where the speaker corrects themself mid-\nutterance. For example, let\u2019s chat tomorrow uh I\nmean Friday contains a speech repair, where the\nuser corrects \u201ctomorrow\u201d with \u201cFriday.\u201d The re-\npaired version of this should be let\u2019s chat Friday.\nPrior work has collected datasets and built systems\nspecifically for speech repair (Heeman and Allen,\n1994, 1999; Johnson and Charniak, 2004). Ad-\nditionally, ASR systems themselves make errors\nthat humans may like to correct post-hoc; there\nhas been work on correcting ASR errors through\nrespeaking misdetected transcriptions (McNair and\nWaibel, 1994; Ghosh et al., 2020; Vertanen and\nKristensson, 2009; Sperber et al., 2013).\nBeyond disfluencies that were not automatically\nrepaired but were transcribed literally, humans\nmust fix many other mistakes while dictating. They\noften change their mind about what to say\u2014the\nhuman writing process is rarely linear\u2014and ASR\nitself commonly introduces transcription errors.\nMost systems require the user to manually fix these\nerrors through keyboard-and-mouse or touchscreen\nediting (e.g., Kumar et al., 2012), which can be\ninconvenient for someone who already relies on\nvoice for dictation. Furthermore, most commercial\nsystems that support editing through speech (DNS,\nWord) require templated commands. Thus, while\nspeech input is often used to write short-form, im-\nprecise text (e.g., search queries or text messages),\nit is not as popular as it might be, and it is used less\nwhen writing longer and more precise documents.\nIn our work, we study making edits through\nspoken natural language commands.\nInterpret-\ning flexible natural language commands is a well-\nstudied problem within NLP, with work in seman-\ntic parsing (Zelle and Mooney, 1993; Zettlemoyer\nand Collins, 2009; Artzi and Zettlemoyer, 2013),\ninstruction-following (Chen and Mooney, 2011;\nBranavan et al., 2009; Tellex et al., 2011; Anderson\net al., 2018; Misra et al., 2017), and task-oriented\ndialogue (Budzianowski et al., 2018). Virtual as-\nsistants like Siri (Apple, 2011), Alexa (Amazon,\n2014), and Google Assistant (Google, 2016) have\nbeen built to support a wide range of functionalities,\nincluding interacting with smart devices, querying\nsearch engines, scheduling events, etc. Due to ad-\nvances in language technologies, modern-day as-\nsistants can support flexible linguistic expressions\nfor invoking commands, accept feedback and per-\nform reinterpretation (Semantic Machines et al.,\n2020), and work in an online and incremental man-\nner (Zhou et al., 2022). Our work falls in this\nrealm but: (1) in a novel interactive dictation set-\nting, (2) with unrestricted commanding, and (3)\nwhere predicting boundaries between dictations\nand commands is part of the task.\nRecently, a line of work has emerged examining\nhow large language models (LLMs) can serve as\ncollaborative writing/coding assistants. Because of\ntheir remarkable ability to generate coherent texts\nover a wide range of domains and topics, LLMs\nhave proven surprisingly effective for editing, elab-\noration, infilling, etc., across a wide range of do-\nmains (Malmi et al., 2022; Bavarian et al., 2022;\nDonahue et al., 2020). Though our system also\nmakes use of LLMs, it supports a different mode\nof editing than these prior works. Some works\nuse edit models for other types of sequence-to-\nsequence tasks (e.g. summarization, text simpli-\nfication, style transfer) (Malmi et al., 2019; Dong\net al., 2019; Reid and Zhong, 2021), while others\nuse much coarser-grained editing commands than\nwe do, expecting the LLM to (sometimes) gener-\nate new text (Bavarian et al., 2022; Zhang et al.,\n2023). In addition to these differences, our editing\ncommands may be misrecognized because they are\nspoken, and may be misdetected/missegmented be-\ncause they are provided through the same channel\nas text entry.\n3\nTask Framework\nWe now formalize our interactive dictation set-\nting. A user who is editing a document speaks\nto a system that both transcribes user dictation and\nresponds to user commands. This process results\nin a interactive dictation trajectory\u2014a sequence\nof timestamped events: the user keeps speaking,\nseveral trained modules keep making predictions,\nand the document keeps being updated.\nSupervision could be provided to the predictive\nmodules in various ways, ranging from direct su-\npervision to delayed indirect reward signals. In this\npaper, we collect supervision that can be used to\nbootstrap an initial system. We collect gold trajec-\ntories in which every prediction is correct\u2014except\nfor ASR predictions, where we preserve the errors\nsince part of our motivation is to allow the user\nto fix dictation errors.4 All predictions along the\ntrajectory are provided in the dataset.\nOur dataset is not completely generic, since it as-\nsumes that certain predictive modules will exist and\ninteract in particular ways, although it is agnostic to\nhow they make their predictions. It is specifically\nintended to train a system that is a pipeline of the\nfollowing modules (Figure 2):\n(a) ASR\nAs the user speaks, the ASR module\nproposes transcripts for spans of the audio stream.\nDue to ASR system latency, each ASR result nor-\nmally arrives some time after the end of the span it\ndescribes. The ASR results are transcripts of suc-\ncessive disjoint spans of the audio, and we refer to\ntheir concatenation as the current transcript (U\nin Figure 2(a)).\n(b) Segmentation\nWhen the current transcript\nchanges, the system can update its segmentation. It\ndoes so by partitioning the current transcript U into\na sequence of segments ui, labeling each as being\neither a dictation or a command.\n(c) Normalization (optional)\nEach segment ui\ncan be passed through a normalization module,\nwhich transforms it from a literal transcript into\nclean text that should be inserted or interpreted.\nThis involves speech repair as well as text normal-\nization to handle orthographic conventions such as\nacronyms, punctuation, and numerals.\nWhile the module (a) may already attempt some\nversion of these transformations, an off-the-shelf\nASR module does not have access to the document\nstate or history. It may do an incomplete job and\nthere may be no way to tune it on gold normalized\nresults. This normalization module can be trained\nto finish the job. Including it also ensures that our\ngold trajectories include the intended normalized\ntext of the commands.\n(d) Interpretation\nGiven a document state di\u22121\nand a segment ui, the interpretation module pre-\ndicts the new document state di that ui is meant\n4In module (c) below, we predicted repairs for command\nsegments, so the gold trajectory interprets accurate clean text\nfor commands. But we did not predict repairs for dictation\nsegments, so their errors persist even in the gold trajectories.\nAttached are the espeak events. Capitalize the S&E speak. Please review.\n1. Segmentation Model \u2133SEG\n3. Execution \nEngine (EE)\nCapitalize the S in eSpeak.\nAttached are the eSpeak \nevents. Please review.\n5. Execution \nEngine\nAttached are the espeak \nevents.\nAttached are the eSpeak \nevents.\n4. Interpretation \nStep \u2133INT\n(+EE)\n(a) ASR\n(b) Segmentation\n(d) Interpretation\n\ud4b0\nD0\nD1\nD2\nD3\nAttached are the espeak events. Capitalize the S&E speak. Please review.\nu1, Dictation\nu2, Command\n, Dictation\nu3\n2. ASR Repair Step \u2133NOR\nu\u2032 2\n(c) Normalization\nFigure 2: Diagram of an interactive dictation system. First, the ASR system (a) transcribes speech, which the\nsegmentation system (b) parses into separate dictation and command segments. Next, an optional normalization\nmodule (c) fixes the any ASR or speech errors in the segment. Finally, the interpretation system (d) returns the\nresult of each operation. On the right is the concrete instantiation of our system.\nto achieve.5 The document is then immediately\nupdated to state di; the change could be temporar-\nily highlighted for the user to inspect. Here di\u22121\nis the result of having already applied the updates\npredicted for segments u1, . . . , ui\u22121, where d0 is\nthe initial document state. Concretely, we take a\ndocument state to consist of the document content\ntogether with the current cursor position.6\nWhen ui is a dictation segment, no prediction\nis needed: the state update simply inserts the cur-\nrent segment at the cursor. However, when ui is\na command segment, predicting the state update\nthat the user wanted requires a text understanding\nmodel. Note that commands can come in many\nforms. Commonly they are imperative commands,\nas in Figure 1d. But one can even treat speech re-\npairs such as Figure 1b as commands, in a system\nthat does not handle repairs at stage (a) or (c).\nRather than predict di directly, an alternative de-\nsign is to predict a program pi and apply it to di\u22121\nto obtain di. In this case, the gold trajectory in our\ndataset includes a correct program pi, which repre-\nsents the intensional semantics of the command ui\n(and could be applied to different document states).\n5This prediction can also condition on earlier segments,\nwhich provide some context for interpreting ui. It might\nalso depend on document states other than di\u22121\u2014such as the\nstate or states that were visible to the user while the user was\nactually uttering ui, for example.\n6The cursor may have different start and end positions\nif a span of text is selected, but otherwise has width\n0.\nFor example, the document state d1 in Figure 2 is\n(\"Attached are the espeak events.\", (31, 31)).\nChange Propagation\nThe ASR engine we use\nfor module (a) sometimes revises its results. It\nmay replace the most recent of the ASR results,\nadding new words that the user has spoken and/or\nimproving the transcription of earlier words. The\nengine marks an ASR result as partial or final\naccording to whether it will be replaced.7\nTo make use of streaming partial and final ASR\nresults, our pipeline supports change propagation.\nThis requires the predictive modules to compute\nadditional predictions. If a module is notified that\nits input has changed, it recomputes its output ac-\ncordingly. For example, if module (a) changes the\ncurrent transcript, then module (b) may change\nthe segmentation. Then module (c) may recompute\nnormalized versions of segments that have changed.\nFinally, module (d) may recompute the document\nstate di for all i such that di\u22121 or ui has changed.\nThe visible document is always synced with the\nlast document state. This sync can revert and re-\nplace the effects on the document of previous in-\ncorrectly handled dictations and commands, poten-\ntially even from much earlier segments. To avoid\nconfusing the user with such changes, and to reduce\ncomputation, a module can freeze its older or more\nconfident inputs so that they reject change notifi-\ncations (Appendix B). Modules (b)\u2013(d) could also\nadopt the strategy of module (a)\u2014quickly return\nprovisional results from a \u201cfirst-pass\u201d system with\nthe freedom to revise them later. This could further\n7Full details and examples can be found in Appendix A.1.\nimprove the responsiveness of the experience.\n4\nDataset Creation\nTo our knowledge, no public dataset exists for the\ntask of interactive dictation. As our task is dis-\ntinct from prior work in a number of fundamental\nways (\u00a72), we create a new dataset, TERTiUS.8\nOur data collection involves two stages. First,\na human demonstrator speaks to the system and\nprovides the gold segmentations, as well as demon-\nstrating the normalizations and document state up-\ndates for the command segments. Later, for each\ncommand segment, an annotator fills in a gold\nprogram that would yield its gold state update.\nFor a command segments, we update the docu-\nment during demonstration using the demonstrated\nstate updates\u2014that is, they do double duty as gold\nand actual state updates. Thus, we follow a gold\ntrajectory, as if the demonstrator is using an oracle\nsystem that perfectly segments their speech into dic-\ntations (though these may have ASR errors) versus\ncommands, and then perfectly interprets the com-\nmands. A future data collection effort could instead\nupdate the document using the imperfect system\nthat we later built (\u00a75), in which case the demon-\nstrator would have to react to cascading errors.\n4.1\nCollecting Interactive Dictation\nWe build a novel data collection framework that\nallows us to collect speech streams and record gold\nand actual events.\nWe used an existing ASR system, Microsoft\nSpeech Services (MSS; Microsoft Azure, 2022).\nWe asked the demonstrator to play both the role of\nthe user (issuing the speech stream), and also the\nroles of the segmentation, normalization, and inter-\npretation parts of the system (Figures 2b\u2013d). Thus,\nwe collect actual ASR results, while asking the\ndemonstrator to demonstrate gold predictions for\nsegmentation, normalization, and interpretation.\nThe demonstration interface is shown in Fig-\nure 3. demonstrators were trained to use the inter-\nface, and told during training how their data would\nbe used.9 A demonstrator is given a task of dic-\ntating an email into our envisioned system (shown\nin the yellow textbox). We collected data in three\nscenarios:\n8Transcribing and Editing in Real-Time with Unrestricted\nSpeech. Named for the human amanuensis Tertius of Iconium.\n9We met with demonstrators ahead of time and provided\nthem with written instructions, which are in Appendix A.2.\n1. Replicate doc: Exactly recreate an email\nfrom the Enron Email Dataset (Klimt and\nYang, 2004).10\n2. Elaborate doc: Expand a terse description of\nan email into an full email. The exact wording\nof the full email is up to the demonstrator.\n3. Replicate segment: Exactly recreate the post-\nstate di of a single command segment ui (ran-\ndomly sampled from the already-collected\nReplicate doc and Elaborate doc data), start-\ning from its pre-state di\u22121. This does not have\nto be done with a single command.\nA demonstrator must then reach the target state (ei-\nther exactly for Replicate doc or Replicate segment,\nor to their satisfaction for Elaborate doc), following\nthese three steps:\nStep 1 (ASR, segmentation)\nThe demonstrator\nstarts speaking, which gets transcribed in real time\nby the built-in ASR system into ASR results. As\nthey speak, they demonstrate what the segmen-\ntation system should do by holding down a key\nwhenever they are speaking a command (as op-\nposed to dictating). They can specify consecutive\ncommands by quickly releasing and re-pressing the\nkey.11 This gives us a list of time intervals when\nthe key was held down. By matching these to the\nASR timestamps, we identify the gold command\nsegments in the ASR transcript. The remaining seg-\nments of the transcript are labeled as dictation.12\nStep 2 (normalization)\nAll labeled segments are\ndisplayed in the right column of the UI. After the\ndemonstrator has finished speaking, they fill in the\nnormalized text for each command segment. (The\nsegment shows original and normalized text in the\nASR and Gold ASR fields.)\nStep 3 (interpretation)\nFinally, for each com-\nmand segment, the demonstrator manually carries\nout the gold state update.13 They do this by click-\ning on a command segment ui in the right column,\nwhich pulls up the associated document state di in\nthe left column. Initially di is set to equal the pre-\nstate di\u22121, and the demonstrator edits it with their\n10Preprocessing details can be found in Appendix A.3.\n11We do not allow two dictation segments to be adjacent\u2014\nthat would be equivalent to one concatenated segment.\n12More details on how the ASR results are combined/seg-\nmented can be found in Appendix A.1.\n13For dictation segments, the system automatically com-\nputes the gold state update by inserting the segment at the\nselection. This segment is an actual ASR result and may\ncontain errors.\nTarget document state Dn\nDocument state after selected segment Di\nDocument state before selected segment Di\u22121\nChange in document state from segment \u0394(Di\u22121, Di)\nNormalized Utterance u\u2032 i\nSelected \nsegment \nto edit Oi\nDictation Segments\nCommand Segments\nActual Literal Utterance\nLiteral Utterance ui\nFigure 3: Data collection UI. Demonstrator speech is transcribed by a built-in ASR system. Demonstrators specify\ngold segmentations by pressing a key to initiate a command segment (editText) and releasing the key to initiate\na dictation segment (insertText). The resulting transcribed segments appear in the ASR fields of the boxes in\nthe right column. For a command segment, the demonstrator specifies the normalized version in the Gold ASR\nfield, and demonstrates the command interpretation by editing the document post-state. Document states are shown\nin the left column: selecting a segment makes its post-state (and pre-state) appear there.\nmouse and keyboard until it reflects the desired\npost-state after applying command ui. For refer-\nence, the UI also displays the pre-state di\u22121 and a\ncontinuously updated visual diff \u2206(di\u22121, di)).\nDemonstrators can move freely among these\nsteps, editing normalizations or state updates at any\ntime, or appending new segments by speaking.14\nWe believe our framework is well-equipped to\ncollect natural, flexible, and intuitive dictation and\ncommanding data, for several reasons: (1) We do\nnot restrict the capabilities of commands or the\nforms of their utterances, but instead ask demon-\nstrators to command in ways they find most natural.\n(2) We simulate natural, uninterrupted switching\nbetween segments by making it easy for demonstra-\ntors to specify segment boundaries in real-time. (3)\nWe collect a realistic distribution over speech errors\nand corrections by using an existing ASR system\nand asking demonstrators to replicate real emails.\nIn the future, the distribution could be made more\nrealistic if we sometimes updated the document by\nusing predicted normalizations and state updates\nrather than gold ones, as in the DAgger imitation\nlearning method (Ross et al., 2011).\n14They are also allowed to back up and remove the final\nsegments, typically in order to redo them.\n4.2\nAnnotating Programs for Commands\nAfter obtaining sequences of demonstrated dia-\nlogues using the above procedure, we extract each\ncommand segment and manually annotate it with\na program pi that represents the intensional se-\nmantics of the command. This program should in\ntheory output the correct di when given di\u22121 as\ninput. Program annotation is done post-hoc with a\ndifferent set of annotators from \u00a74.1.\nWe design a domain-specific Lisp-like language\nfor text-manipulating programs, and an execution\nengine for it. We implement a library consisting of\ncomposable actions, constraints, and combinators.\nA program consists of actions applied to one or\nmore text targets, which are specified by contraints.\nCombinators allow us to create complex constraints\nby composing them. For example, in Figure 2,\nCapitalize the S in eSpeak, has the program\n(capitalize\n(theText\n(and\n(like \"S\")\n(in (theText (like \"eSpeak\"))))))\nwhere capitalize is the action, (like \"S\")\nand (like \"eSpeak\") are constraints, and\nand is a combinator. More examples are in Ap-\npendix A.4.\nTrajectories\nSegments\nTask\nDict.\nCmd.\nTotal\nReplicate doc\n372\n473\n1453\n1926\nElaborate doc\n343\n347\n473\n820\nReplicate segment\n605\n139\n1299\n1438\nTotal\n1320\n959\n3225\n4184\nTable 1: Dataset size statistics.\n4.3\nHandling of partial ASR results\nThe current transcript sometimes ends in a partial\nASR result and then is revised to end in another\npartial ASR result or a final ASR result. All ver-\nsions of this transcript\u2014\u201cpartial\u201d and \u201cfinal\u201d\u2014will\nbe passed to the segmenter, thanks to change prop-\nagation. During demonstration, we record the gold\nlabeled segmentations for all versions, based on the\ntiming of the demonstrator\u2019s keypresses.\nHowever, only the segments of the \u201cfinal\u201d ver-\nsion are shown to the demonstrator for further anno-\ntation. A segment of a \u201cpartial\u201d version can simply\ncopy its gold normalized text from the segment\nof the \u201cfinal\u201d version that starts at the same time.\nThese gold data will allow us to train the normaliza-\ntion model to predict a normalized command based\non partial ASR results, when the user has not yet\nfinished speaking the command or the ASR engine\nhas not yet finished recognizing it.\nIn the same way, a command segment ui of the\n\u201cpartial\u201d version could also copy its gold document\npost-state di and its gold program pi from the cor-\nresponding \u201cfinal\u201d segment. However, that would\nsimply duplicate existing gold data for training the\ninterpretation module, so we do not include gold\nversions of these predictions in our dataset.15\n4.4\nDataset details & statistics\nIn the first stage (\u00a74.1), eleven human demonstra-\ntors demonstrated 1372 interactive dictation tra-\njectories (see Table 1 for details). In the second\nstage (\u00a74.2), two human annotators annotated pro-\ngrams for 868 commands.16 The dataset was then\nsplit into training, validation, and test sets with 991\n15The gold pre-state di\u22121 may occasionally be different,\nowing to differences between the two versions in earlier dicta-\ntion segments. In this case, the interpretation example would\nno longer be duplicative (because it has a different input). Un-\nfortunately, in this case it is no longer necessarily correct to\ncopy the post-state di, since some differences between the\ntwo versions in the pre-state might need to be preserved in the\npost-state.\n16The rest of the programs were auto-generated by GPT3.\nSee details in Appendix C.2.\ntraining trajectories (consisting of 3199 demon-\nstrated segments), 173 validation trajectories (562\nsegments), and 156 test trajectories (423 segments).\nAll demonstrators and annotators were native\nEnglish speakers. The dataset is currently only En-\nglish, and the editor supports unformatted plain text.\nHowever, the annotation framework could handle\nother languages that have spoken and written forms,\nand could be extended to allow formatted text.\nA key goal of our system is flexibility. We quan-\ntify how well TERTiUS captures flexibility by mea-\nsuring the diversity of natural language used to\ninvoke each state change.17 We count the num-\nber of distinct first tokens (mainly verbs) used to\ninvoke each action. These results are reported in Ta-\nble 4 in the Appendix, alongside a comparison with\nDNS.18 We see that TERTiUS contains at least 22\nways to invoke a correction, while DNS sup-\nports only 1. In short, these results show that doing\nwell on TERTiUS requires a much more flexible\nsystem that supports a wider array of functions and\nways of invoking those functions than what existing\nsystems provide.\n5\nModeling & Training\nThe overall system we build for interactive dicta-\ntion follows our pipeline from Figure 2 and \u00a73:\n1. A segmentation model MSEG takes the cur-\nrent transcript U, and predicts a segmenta-\ntion u1, . . . , un, simultaneously predicting\nwhether each ui corresponds to a dictation\nor command segment.\n2. Each dictation segment is directly spliced into\nthe document at the current cursor position.\n3. For each command segment:\n(a) A normalization model MNOR predicts\nthe normalized utterance u\u2032\ni, repairing\nany ASR misdetections.\n(b) An interpretation model, MINT(state)\nor MINT(program), either: 1. directly\npredicts the end state of the command\ndi, or 2. predicts the command program\npi, which is then executed to di by the\n17The system we build can theoretically support more flexi-\nbility than what is captured in TERTiUS. However, for TER-\nTiUS to be a useful testbed (and training set) for flexibility,\nwe would like it to be itself diverse.\n18We also measure the diversity of state changes captured\nby TERTiUS in Appendix A.5.\nexecution engine. We experiment with\nboth types of interpretation model.\nBelow we describe the specific models we use.\n5.1\nSegmentation\nThe segmentation model partitions U into segments\nui, each of which is labeled by mi as being either\ndictation or command:\nMSEG(U) = [(u0, m0), \u00b7 \u00b7 \u00b7 , (un, mn)],\ns.t. U = u0 + u1 + \u00b7 \u00b7 \u00b7 + un\nmi \u2208 {command, dictation}\n(1)\nConcretely, the segmentation model does this us-\ning BIOES tagging (Jurafsky and Martin, 2009,\nChapter 5). Here each command is tagged with a\nsequence of the form BI\u2217E (\u201cbeginning, inside, . . . ,\ninside, end\u201d) or with the length-1 sequence S (\u201csin-\ngleton\u201d). Maximal sequences of tokens tagged with\nO (\u201coutside\u201d) then correspond to the dictation seg-\nments. Note that two dictation segments cannot be\nadjacent. We implement the segmentation model as\na T5-base encoder (Raffel et al., 2022) followed by\na two-layer MLP prediction module. More details\non why each tag is necessary and how we trained\nthis model can be found in Appendix C.1.\n5.2\nNormalization and Interpretation\nFor each ui that is predicted as a command segment,\nwe first predict the normalized utterance u\u2032\ni,19\nMNOR(di\u22121, ui) = u\u2032\ni.\n(2)\nWe then interpret u\u2032\ni in context to predict either the\ndocument state di or an update program pi.\nMINT(state)(di\u22121, u\u2032\ni) = di,\nMINT(program)(di\u22121, u\u2032\ni) = pi.\n(3)\nWe then update the document state accordingly.\nWe experiment with two ways of implement-\ning the two steps: we either fine-tune two sepa-\nrate T5-base models (Raffel et al., 2022) that run\nin a pipeline for each command, or we prompt\nGPT3 (Brown et al., 2020)20 to generate both the\nnormalized utterance21 and the interpretation out-\nput in a single inference step. Training and prompt-\ning details can be found in Appendix C.2.\n19Note that the normalization step additionally conditions\non the state di\u22121, allowing it to consider what command would\nhave been sensible in this context. Concrete examples are\n6\nResults\nWe evaluate the segmentation model in isolation,\nand the normalization and interpretation steps to-\ngether. (Appendices D.2 and D.3 evaluate the nor-\nmalization and interpretation steps in isolation.)\nFor simplicity, we evaluate the models only on\ncurrent transcripts U that end in final ASR results\n(though at training time and in actual usage, they\nalso process transcripts that end in partial ones).22\n6.1\nSegmentation\nMetrics\nExact match (EM) returns 0 or 1 ac-\ncording to whether the entire labeled segmentation\nof the final transcript U is correct. We also evaluate\nmacro-averaged labeled F1, which considers how\nmany of the gold labeled segments appear in the\nmodel\u2019s output segmentation and vice versa. Two\nlabeled segments are considered to be the same if\nthey have the same start and end points in U and\nthe same label (dictation or command).\nResults\nSegmentation results on an evaluation\ndataset of transcripts U (see Appendix D.1) are\nshown in the top section of Table 2. All results are\nfrom single runs of the model. The model performs\ndecently on TERTiUS, and in some cases is even\nable to fix erroneous sentence boundaries detected\nby the base ASR system (Appendix D.1.2). How-\never, these cases are also difficult for the model:\na qualitative analysis of errors find that, generally,\nerrors arise either when the model is misled by er-\nroneous over- and under-segmentation by the base\nASR system, or when commands are phrased in\nways similar to dictation. Examples are in in Ap-\npendix D.1.1.\n6.2\nNormalization & Interpretation\nMetrics\nWe evaluate normalization and interpre-\ntation in conjunction. Given a gold normalized\ncommand utterance ui and the document\u2019s gold\npre-state di\u22121, we measure how well we can re-\nconstruct its post-state di. We measure state ex-\nact match (EM)23 between the predicted and gold\npost-states. If the interpretation model predicts\ngiven in Appendix D.2.\n20Specifically, the text-davinci-003 model.\n21Although the normalized utterance is not used for the final\nstate prediction, early experiments indicated that this auxiliary\ntask helped the model with state prediction, possibly due to a\nchain-of-thought effect (Wei et al., 2022).\n22See Appendix D for details.\n23We disregard the cursor position in this evaluation.\nMetric\nT5\nGPT3\nSegmentation\nF1\n90.9%\n-\nSegmentation EM\n85.3%\n-\nRuntime (s/it)\n0.097\n-\nprog\nstate\nprog\nstate\nASR Repair +\nState EM\n28.3%\n29.5%\n38.6%\n55.1%\nInterpretation\nProgram EM\n28.3%\n-\n41.9%\n-\nRuntime (s/it)\n1.28\n3.46\n5.32\n6.92\nTable 2: We evaluate segmentation (top) and the ASR repair and in-\nterpretation components jointly (bottom). We report accuracy metrics\n(F1, EM) as well as runtime (in seconds per example). Segmentation is\nrelatively fast and performs decently. For ASR repair and interpretation,\nwe experiment with a fine-tuned T5 vs. a prompted GPT3 model, each\noutputting either the end state (state) or a program to carry out the\ncommand (prog).\nFigure 4: Runtime vs. State EM. GPT3\nmodels produce more accurate state up-\ndates than T5, but use an unreasonable\namount of time. Directly predicting the\nupdated documents is more often correct\nthan predicting update programs, again\nat the cost of time.\nintermediate programs, then we also measure pro-\ngram exact match (EM) between the predicted\nprogram and the gold program.\nResults\nThe bottom of Table 2 shows these re-\nsults. All results are from single runs of the model.\nGPT3 generally outperforms T5, likely due to\nits larger-scale pretraining. When we evaluated\nASR repair and interpretation separately in Appen-\ndices D.2 and D.3, we found that GPT3 was better\nthan T5 at both ASR repair and interpretation.\nFurthermore, we find that both GPT3 and T5 are\nbetter at directly generating states (55.1 vs. 38.6\nstate EM and 29.5 vs. 28.3 state EM). However,\nthe gap is larger for GPT3. We suspect that GPT3\nhas a better prior over well-formed English text\nand can more easily generate edited documents d\ndirectly, without needing the abstraction of an inter-\nmediate program. T5-base, on the other hand, finds\nit easier to learn the distinctive (and more direct)\nrelationship between u and the short program p.\nOther than downstream data distribution shift,\nwe hypothesize that program accuracy is lower than\nstate accuracy because the interpretation model is\ntrained mostly on auto-generated program annota-\ntions, and because the execution engine is imper-\nfect. We anticipate that program accuracy would\nimprove with more gold program annotations and\na better execution engine.\n6.3\nEfficiency\nTable 2 reports runtimes for each component. This\nallows us to identify bottlenecks in the system and\nconsider trade-offs between model performance\nand efficiency. We see that segmentation is gener-\nally quick and the ASR repair and interpretation\nsteps are the main bottlenecks. The T5 model also\nruns much faster than the GPT3 model,24 despite\nperforming significantly worse, indicating a trade-\noff between speed and accuracy.\nFigure 4 shows that by generating programs in-\nstead of states, we achieve faster runtimes (as the\nprograms are shorter), at the expense of accuracy.\n7\nConclusion\nMost current speech input systems do not support\nvoice editing. Those that do usually only support a\nnarrow set of commands specified through a fixed\nvocabulary. We introduce a new task for flexible\ninvocation of commands through natural language,\nwhich may be interleaved with dictation. Solving\nthis task requires both segmenting and interpret-\ning commands. We introduce a novel data collec-\ntion framework that allows us to collect a pilot\ndataset, TERTiUS, for this task. We explore trade-\noffs between model accuracy and efficiency. Future\nwork can examine techniques to push out the Pareto\nfrontier, such as model distillation to improve speed\nand training on larger datasets to improve accuracy.\nFuture work can also look at domains outside of\n(work) emails, integrate other types of text trans-\nformation commands (e.g., formatting), and may\nallow the system to respond to the user in ways\nbeyond updating the document.\n24Note that GPT3 is called via an external API, while T5 is\nrun on a local GPU. GPT3 runtimes thus include an unknown\ncommunication overhead, which will not be present when run\non local hardware.\n8\nLimitations\nTERTiUS is a pilot dataset. In particular, its test set\ncan support segment-level metrics, but is not large\nenough to support reliable dialogue-level evalua-\ntion metrics. Due to resource constraints, we also\ndo not report inter-annotator agreement measure-\nments. While we made effort to make our interface\nlow-friction, the demonstration setting still differs\nfrom the test-time scenario it is meant to emulate,\nand such a mismatch may also result in undesired\ndata biases. Because our dialogues were collected\nbefore having a trained interpretation model, trajec-\ntories always follow gold interpretations. Because\nof this, the main sources of errors are ASR misde-\ntections or user speech errors. In particular, TER-\nTiUS contains data on: 1. misdetections and speech\nerrors in transcription, and how to fix them through\ncommands, 2. misdetections and speech errors in\nedits, and what intent they correspond to. We leave\nto future work the task of addressing semantic er-\nrors and ambiguities which result from incorrect\ninterpretation of user intent. Some of these limi-\ntations can be addressed by incorporating trained\nmodels into the demonstration interface, which will\nallow faster demonstration, and capture trajectories\nthat include actual system (non-gold) interpreta-\ntions.\nThough the trained system runs, we have\nnot done user studies with it because it is not\nproduction-ready. The T5-base models are effi-\ncient enough, but the prompted GPT3 model is too\nslow for a responsive interactive experience. Nei-\nther model is accurate enough at interpretation. We\nwelcome more research on this task!\nWhen a human dictates to another human, inter-\nleaved corrections and commands are often marked\nprosodically (by pitch melody, intensity, and tim-\ning). Our current system examines only the textual\nASR output; we have given no account of how to\nincorporate prosody, a problem that we leave to\nfuture work. We also haven\u2019t considered how to\nmake use of speech lattices or n-best lists, but they\ncould be very useful if the user is correcting our\nmistranscription\u2014both to figure out what text the\nuser is referring to, and to fix it.\n9\nImpact Statement\nThis work makes progress toward increasing ac-\ncessibility for those who cannot use typing inputs.\nThe nature of the data makes it highly unlikely that\nartifacts produced by this work could be used (in-\ntentionally or unintentionally) to quickly generate\nfactually incorrect, hateful, or otherwise malignant\ntext.\nThe fact that all speakers in our dataset were\nnative speakers of American English could con-\ntribute to exacerbating the already present disparity\nin usability for English vs. non-English speakers.\nFuture work should look to expand the diversity of\nlanguages, dialects, and accents covered.\nReferences\nAmazon. 2014. Amazon Alexa.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. 2018. Vision-\nand-language navigation:\nInterpreting visually-\ngrounded navigation instructions in real environ-\nments. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3674\u2013\n3683.\nApple. 2011. Siri.\nYoav Artzi and Luke Zettlemoyer. 2013. Weakly su-\npervised learning of semantic parsers for mapping\ninstructions to actions. Transactions of the Associa-\ntion for Computational Linguistics, 1:49\u201362.\nShiri Azenkot and Nicole B. Lee. 2013. Exploring the\nuse of speech input by blind people on mobile de-\nvices. Proceedings of the 15th International ACM\nSIGACCESS Conference on Computers and Accessi-\nbility.\nMohammad Bavarian, Angela Jiang, Heewoo Jun, and\nHenrique Pond\u00e9. 2022. New gpt-3 capabilities: Edit\n& insert. [Online; posted 15-March-2022].\nS.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and\nRegina Barzilay. 2009. Reinforcement learning for\nmapping instructions to actions. In Proceedings of\nthe Joint Conference of the 47th Annual Meeting\nof the ACL and the 4th International Joint Confer-\nence on Natural Language Processing of the AFNLP,\npages 82\u201390, Suntec, Singapore. Association for\nComputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga\u0161i\u00b4c. 2018. MultiWOZ - a large-\nscale multi-domain Wizard-of-Oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016\u20135026, Brussels,\nBelgium. Association for Computational Linguistics.\nDavid L. Chen and Raymond J. Mooney. 2011. Learn-\ning to interpret natural language navigation instruc-\ntions from observations.\nIn Proceedings of the\nTwenty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI\u201911, page 859\u2013865. AAAI Press.\nChung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Ro-\nhit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, An-\njuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina\nGonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and\nMichiel Bacchiani. 2018.\nState-of-the-art speech\nrecognition with sequence-to-sequence models. In\n2018 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), page\n4774\u20134778. IEEE Press.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to fill in the blanks. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2492\u2013\n2501, Online. Association for Computational Lin-\nguistics.\nYue Dong, Zichao Li, Mehdi Rezagholizadeh, and\nJackie Chi Kit Cheung. 2019. EditNTS: An neural\nprogrammer-interpreter model for sentence simplifi-\ncation through explicit editing. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 3393\u20133402, Florence, Italy.\nAssociation for Computational Linguistics.\nDebjyoti Ghosh, Can Liu, Shengdong Zhao, and Kotaro\nHara. 2020. Commanding and re-dictation: Develop-\ning eyes-free voice-based interaction for editing dic-\ntated text. ACM Transactions on Computer-Human\nInteraction, 27.\nGoogle. 2016. Google Assistant.\nPeter Heeman and James F Allen. 1994. Detecting and\ncorrecting speech repairs. In Proceedings of the 32nd\nAnnual Meeting of the Association for Computational\nLinguistics, pages 295\u2013302, Las Cruces. New Mexico\nState University.\nPeter A. Heeman and James Allen. 1999.\nSpeech\nrepairs, intonational phrases and discourse mark-\ners: Modeling speakers\u2019 utterances in spoken dialog.\nComputational Linguistics, 25(4):527\u2013572.\nMark Johnson and Eugene Charniak. 2004. A TAG-\nbased noisy-channel model of speech repairs. In\nProceedings of the 42nd Annual Meeting of the As-\nsociation for Computational Linguistics (ACL-04),\npages 33\u201339, Barcelona, Spain.\nD. Jurafsky and J.H. Martin. 2009. Speech and Lan-\nguage Processing: An Introduction to Natural Lan-\nguage Processing, Computational Linguistics, and\nSpeech Recognition. Prentice Hall series in artificial\nintelligence. Pearson Prentice Hall.\nBryan Klimt and Yiming Yang. 2004. The Enron corpus:\nA new dataset for email classification research. In\nProceedings of the 15th European Conference on\nMachine Learning, ECML\u201904, page 217\u2013226, Berlin,\nHeidelberg. Springer-Verlag.\nAnuj Kumar, Tim Paek, and Bongshin Lee. 2012. Voice\ntyping: A new speech interaction model for dicta-\ntion on touchscreen devices. In Proceedings of CHI,\n2012,, pages 2277\u20132286. ACM.\nEric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr\nChuklin, Jakub Adamek, Daniil Mirylenka, Felix\nStahlberg, Sebastian Krause, Shankar Kumar, and\nAliaksei Severyn. 2022. Text generation with text-\nediting models. In Proceedings of the 2022 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies: Tutorial Abstracts, pages 1\u20137,\nSeattle, United States. Association for Computational\nLinguistics.\nEric Malmi, Sebastian Krause, Sascha Rothe, Daniil\nMirylenka, and Aliaksei Severyn. 2019. Encode, tag,\nrealize: High-precision text editing. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5054\u20135065, Hong Kong,\nChina. Association for Computational Linguistics.\nArthur E. McNair and Alex Waibel. 1994. Improving\nrecognizer acceptance through robust, natural speech\nrepair. In Proc. 3rd International Conference on\nSpoken Language Processing (ICSLP 1994), pages\n1299\u20131302.\nMicrosoft. 2022. Dictation for Microsoft Word.\nMicrosoft Azure. 2022. Cognitive speech services.\nDipendra Misra, John Langford, and Yoav Artzi. 2017.\nMapping instructions and visual observations to ac-\ntions with reinforcement learning. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1004\u20131015, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nNuance. 1997. Dragon NaturallySpeaking.\nNuance. 2022. Dragon Speech Recognition Solutions.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K\u00f6pf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. 2019. PyTorch: An Im-\nperative Style, High-Performance Deep Learning Li-\nbrary. Curran Associates Inc., Red Hook, NY, USA.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2022. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nMachel Reid and Victor Zhong. 2021. LEWIS: Lev-\nenshtein editing for unsupervised text style transfer.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 3932\u20133944,\nOnline. Association for Computational Linguistics.\nStephane Ross, Geoff J. Gordon, and J. Andrew Bag-\nnell. 2011. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In\nProceedings of AISTATS.\nSemantic Machines, Jacob Andreas, John Bufe, David\nBurkett, Charles Chen, Josh Clausman, Jean Craw-\nford, Kate Crim, Jordan DeLoach, Leah Dorner, Ja-\nson Eisner, Hao Fang, Alan Guo, David Hall, Kristin\nHayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Sm-\nriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lan-\nman, Percy Liang, Christopher H. Lin, Ilya Lints-\nbakh, Andy McGovern, Aleksandr Nisnevich, Adam\nPauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro\nRoy, Jesse Rusak, Beth Short, Div Slomin, Ben Sny-\nder, Stephon Striplin, Yu Su, Zachary Tellman, Sam\nThomson, Andrei Vorobev, Izabela Witoszko, Jason\nWolfe, Abby Wray, Yuchen Zhang, and Alexander\nZotov. 2020. Task-oriented dialogue as dataflow syn-\nthesis. Transactions of the Association for Computa-\ntional Linguistics, 8:556\u2013571.\nMatthias Sperber, Graham Neubig, Christian F\u00fcgen,\nSatoshi Nakamura, and Alex Waibel. 2013. Efficient\nspeech transcription through respeaking. pages 1087\u2013\n1091.\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew R. Walter, Ashis Gopal Banerjee, Seth\nTeller, and Nicholas Roy. 2011.\nUnderstanding\nnatural language commands for robotic navigation\nand mobile manipulation.\nIn Proceedings of the\nTwenty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI\u201911, page 1507\u20131514. AAAI Press.\nKeith Vertanen and Per Ola Kristensson. 2009. Auto-\nmatic selection of recognition errors by respeaking\nthe intended text. In ASRU \u201909: IEEE Workshop on\nAutomatic Speech Recognition and Understanding,\npages 130\u2013135.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nW. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer,\nA. Stolcke, D. Yu, and G. Zweig. 2016. Achieving\nhuman parity in conversational speech recognition.\nJohn M. Zelle and Raymond J. Mooney. 1993. Learn-\ning semantic grammars with constructive induc-\ntive logic programming.\nIn Proceedings of the\nEleventh National Conference on Artificial Intelli-\ngence, AAAI\u201993, page 817\u2013822. AAAI Press.\nLuke Zettlemoyer and Michael Collins. 2009. Learn-\ning context-dependent mappings from sentences to\nlogical form. In Proceedings of the Joint Conference\nof the 47th Annual Meeting of the ACL and the 4th\nInternational Joint Conference on Natural Language\nProcessing of the AFNLP, pages 976\u2013984, Suntec,\nSingapore. Association for Computational Linguis-\ntics.\nJiyang Zhang, Sheena Panthaplackel, Pengyu Nie,\nJunyi Jessy Li, and Milos Gligoric. 2023. Coditt5:\nPretraining for source code and natural language edit-\ning. In Proceedings of the 37th IEEE/ACM Interna-\ntional Conference on Automated Software Engineer-\ning, ASE \u201922, New York, NY, USA. Association for\nComputing Machinery.\nJiawei Zhou, Jason Eisner, Michael Newman, Em-\nmanouil Antonios Platanios, and Sam Thomson.\n2022. Online semantic parsing for latency reduc-\ntion in task-oriented dialogue. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1554\u20131576, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nA\nDataset\nA.1\nASR results\nTypes of segments\nBelow we describe the types\nof ASR results we collect in TERTiUS. As dia-\nlogues are uttered, we obtain a stream of times-\ntamped partial and full ASR results from MSS.\nExamples of partial and full ASR results can be\nfound below:\n0:00.00: attached\n0:00.30: attached is\n0:00.60: attached is the\n0:01.05: attached is the draft\n0:02.15: Attached is the draft.\nThe first four lines are partial ASR results upartial\nthat are computed quickly and returned by MSS\nin real time as the user is speaking. The last line\nis the final ASR result, which takes slightly longer\nto compute, but represents a more reliable and pol-\nished ASR result. After a final result ufinal has been\ncomputed, it obsolesces prior partial ASR results.\nWhile not used in present experiments, collect-\ning partial ASR results enables building an incre-\nmental system that can be faster and more respon-\nsive in real time; rather than waiting for ends of\nsentences to execute commands, a system can rely\non partial ASRs to anticipate commands ahead of\ntime (akin to Zhou et al. (2022)). Collecting timing\ninformation is also helpful for evaluating the speed\nof our system: the system runtime continges on the\nrate at which it obtains new ASR results and how\nlong it takes to process them.\nFurthermore, MSS additionally returns n-best\nlists for each final ASR results. These are a list of\ncandidate final ASRs that may feasibly correspond\nwith user audio, e.g.,\nAttached is the draft.\nAttached his draft.\nAttacked is the draft.\n\u00b7 \u00b7 \u00b7\nAggregation segments\nFor long user audio\nstreams, partial and final results are returned se-\nquentially, each describing roughly a single sen-\ntence. The most recent ASR result is concatenated\ntogether with the previous history of final ASR re-\nsults, to return the full partial or final ASR result\nfor the entire stream. For example, after the user\nutters the first sentence in the example above, the\nuser may continue by saying:\nplease\nplease re\nplease review\nplease review win\nplease review when pause\nplease review when possible\nPlease review when possible.\nWe concatenate each of these new ASR results with\nthe previous final ASR results to obtain the current\ntranscript U (see \u00a73),which evolves over time as\nfollows:\nAttached is the draft. please\nAttached is the draft. please re\nAttached is the draft. please review\nAttached is the draft. please review win\nAttached is the draft. please review when\npause\nAttached is the draft. please review when\npossible\nAttached is the draft. Please review when\npossible.\nSegmenting ASR results into Segments During\nAnnotation\nDuring annotation (\u00a74.1), all these\npartial and final ASR results get mapped to seg-\nments, forming ufinal\ni\nand upartial\ni\n. This is done by\nidentifying the timestamp of each token within each\npartial and final result. For example, in the exam-\nple ASR results sequence at the beginning of this\nsection A.1, suppose the user specifies an segment\nboundary at time 0:00.45, (separating \u201cAttached\nis\u201d from \u201cthe draft.\u201d). We get the following ASR\nresults for the first segment:\nattached\nattached is\nAttached is\n(we refer to the first two as partial ASRs for the\nsegment, as they are derived from partial ASR, and\nthe third as the final ASR for the segment), and the\nfollowing ASR results for the second segment:\nthe\nthe draft\nthe draft.\nA.2\nAnnotation Instructions (\u00a74.1)\nThe full text of written instructions given to anno-\ntators during the first round of annotation (\u00a74.1) is\nprovided below:\n1. Transcribing\nYour goal is to replicate the prompt in the tar-\nget box verbatim / expand the prompt in the\nyellow textbox into a coherent email, start-\ning from the given (potentially non-empty)\nstarting document in the \u2018Transcription out-\nput\u2018 box. You are expected to do so using\na series of speech-to-text transcriptions and\ncommands. Try to use the starting document\nas much as possible (i.e. do not delete the\nentire document and start over).\nYou can easily see what changes are to be\nmade by toggling the \u2018See Diff View\u2018 button.\nOnce that mode is on, the text you need to add\nwill be highlighted in green, while the text\nyou need to delete will by highlighted in red.\nOnce there is no colored text, your text box\nmatches the target text box and you are done.\nBegin this process by hitting the \u2018Begin tran-\nscription\u2019 button. This will cause a new \u2018in-\nsertText\u2019 command to appear in the command\nlog on the right.\nYou are now in transcription mode. Whatever\nyou say will appear in the \u2018Transcription out-\nput\u2018 box.\n2. Editing\nYou can fix mistakes in transcription, add for-\nmatting, etc. through adding \u2018editText\u2019 com-\nmands.\nHold down \u2018ctrl\u2019 on your keyboard to issue a\nnew \u2018editText\u2019 command.\nWhile holding down \u2018ctrl\u2019 you will be in edit\nmode. In this mode, you can manually use\nmouse-and-keyboard to change the output.\nHowever, you must describe the edit you are\nmaking before you make it.\nBegin by describing your edit using your\nvoice. Whatever you say now will appear in\nthe editText ASR box, but not in the \u2018Tran-\nscription output\u2018.\nBecause the ASR system is imperfect, the tex-\ntual description may be faulty. Fix any mis-\ntakes in the detected speech in the \u2018Gold ASR\u2019\nbox.\nFinally, manually edit the \u2018Transcription out-\nput\u2019 box to correspond the effect of your edit\ncommand.\nNote: It is important that you vocalize your\nchange before making any edits to either\n\u2018Gold ASR\u2019 or \u2018Transcription output\u2019, as the\nASR system stops recording as soon as you\nclick into either one of these boxes.\n3. Undoing, Reseting, Submitting, & Saving\nYou can click on previous commands in the\ncommand log to revisit them. Note that if\nyou edit the output associated with a \u2018edit-\nText\u2019 prior in the history, you will erase the\nchanges associated with subsequent \u2018editText\u2019\noperations.\nIf you would like to undo some portion of\ncommand log, you can use the \u2018Delete Se-\nlected Command & Afterwards\u2018 button. Sim-\nply click on the first command you would like\nto remove, then click the button to remove that\ncommand and all commands after it.\nYou can clear the entire command log by hit-\nting \"Reset\".\nIf you would like to work on transcribing an-\nother target, use the green arrow keys below\nthe target. This will present you with a new\ntarget while saving progress on your current\ntarget. To delete a target prompt, press the red\n\u2018X\u2019.\nOnce you are done editing, click \"Submit\"\nbutton.\nPlease double-check each command before\nsubmission! In particular, commands will ap-\npear red if they are potentially problematic\n(e.g. they are not associated with any change\nto the underlying text). Please check to make\nsure there are no red commands that you do\nnot intend to be there!\nA.3\nTarget Text Preprocessing\nFor replicating Enron emails, we process emails\nfrom the Enron Email Dataset to create our tar-\nget final states. We break the email threads into\nindividual emails, filtering out email headers and\nnon-well-formed emails (emails that are either less\nthan 50 characters or more than 5000 characters\nlong, or contain too many difficult-to-specify non-\nEnglish symbols). Annotators also had the option\nto skip annotating certain emails, if they found the\nemail too difficult to annotate.\nA.4\nAnnotation Programs\nExamples of programs can be found below:\nActions\nConstraints & Combinators\ncombineSentences\nunion\nbetween\nparenthesize\nor\nendsWith\nallCaps\nand\nat\ndo\nin\natStart\nrespell\nnthToLast\natEnd\ndelete\nnth\nexactly\nspell\nfindAll\nhasSubstring\ncapitalize\nthePosition\npassage\ncombine\ntheText\nline\nquote\nempty\nsentence\nlowercase\nextra\nparenthetical\nmove\nnextTo\nphrase\nmoveCursor\ntake\nword\nreplace\ncontains\nletter\ninsert\nbefore\ntext\ncorrection\nafter\nlike\nstartsWith\nalwaysTrue\nTable 3: List of functions present in TERTiUS.\n1. ASR: Lower case the W in the word when.\nProgram:\n(lowercase\n(theText\n(and\n(like \"W\")\n(in\n(theText\n(and\n(word)\n(like \"when\")))))))\n2. ASR: Get rid of the space in between the two\nwords off site and replace that with a -.\nProgram:\n(replace\n(theText\n(and\n(like \" \")\n(between\n(theText (like \"off\"))\n(theText (like \"site\")))))\n\"-\")\nA.5\nDataset Analysis\nTo assess the diversity of state changes, we quantify\nthe number of distinct actions, constraints, and\nconstraint combinators (see \u00a74.2) that appear in\nthe annotated programs. In Table 3, we list out\nall actions, constraints, and constraint combinators\npresent in TERTiUS. TERTiUS contains at least 15\ntypes of actions (and allows for action composition\nwith sequential chaining operation do), with 34\ntypes of constraint and constraint combinators.\nIn Table 4, we approximate the invocation di-\nversity represented in TERTiUS, by measuring the\nnumber of distinct first tokens used to invoke each\ntype of actions. For actions that overlap in function\nCommand action\n# of distinct\n# of distinct\nfirst tokens\nfirst tokens\n(TERTiUS)\n(DNS)\ncapitalize\n12\n2\nreplace\n83\n-\ndelete\n22\n5*\nquote\n2\n1\nparenthesize\n3\n1\ndo\n44\n-\ninsert\n51\n1\ncorrection\n22\n1\nlowercase\n12\n1\nallCaps\n8\n1\nspell\n17\n1\nmove\n3\n-\nrespell\n1\n-\ncombineSentences\n7\n-\nmoveCursor\n3\n1\ncombine\n1\n-\nTable 4: Number of ways to invoke various commands,\nin terms of number of distinct first tokens used to invoke\nthat command. Second column shows the number of\ndistinct first invokation tokens as present in TERTiUS,\nwhile third column shows the number of distinct first\ninvokation tokens for comparable commands supported\nby DNS.\n*Counting undo, backspace, and scratch that as delete com-\nmands, despite being less general than our delete functionality\n(can only delete most recent tokens).\nwith ones supported by DNS, we also report a sim-\nilar diversity metric against the full set of trigger\nwords supported by DNS.25\nB\nRunning Online\nWhen running the system online in real time, we\nmust consider efficiency and usability.\nWe in-\ntroduce a \u201ccommit point\u201d that signifies that the\nsystem cannot re-segment, re-normalize, or re-\ninterpret anything before that point. We only want\nto consider recent ASR results because the system\nquickly becomes inefficient as the dialogue length\ngrows (the interpretation step, which is the bot-\ntleneck of the system, must run for every single\ncommand.) Furthermore, users often refer to and\ncorrect only recent dictations and commands; re-\nverting early changes can have potentially large\nand undesirable downstream effects, leaving users\npotentially highly confused and frustrated.\nConcretely, the commit point is implemented\nas the system treating the document state at that\npoint as the new \u201cinitial state,\u201d so that it is unable\nto access segments and the history of document\nstates from before that point. We implement this\n25https://www.nuance.com/asset/en_us/\ncollateral/dragon/command-cheat-sheet/\nct-dragon-naturally-speaking-en-us.pdf\npoint so that it must coincide with the end of a\nfinal ASR result. We feed into the system this\nstate as the initial state, and the entire sequence of\nASR results starting from that point. All dictations\nand command segments returned by the model are\nexecuted in sequence from the commit point.\nWe decide to set a commit point based on sys-\ntem confidence and time since last commit. Sys-\ntem confidence is derived from the confidences of\neach component model at each step of the predic-\ntion. We measure the system confidence of the end\nstate predicted by the system, by summing the log-\nprobabilities of: 1. the segmentation model result,\n(summing the log-probabilities of each BIOES tag\npredicted for each token), 2. the ASR repair model\nresult for each command (log-probability of the\nresulting sentence), 3. the interpretation model re-\nsult for each command (the log-probability of the\nend state or program). Once the system confidence\nexceeds a threshold \u03c4commit, we decide to commit\nimmediately at that point. Otherwise, if we have\nobtained more than 4 final ASR results since the\nlast commit, we must commit at our most confident\npoint from within the last 4 turns.\nC\nModel Training Details\nIn this section, we describe how we trained each\ncomponent of the system. See \u00a75 for a descrip-\ntion of the inputs, outputs, and architecture of each\nmodel. Our final system is incremental, able to\nprocess both partial and final ASR results.\nC.1\nSegmentation Model\nWe use BIOES for the segmentation model. Note\nthat we cannot just predict a binary command/dicta-\ntion tag for each token, because it would be unable\nto discern two consecutive commands from one\ncontinuous command. Thus, we need to use B to\nspecify the beginning of a new command segment.\nE is also necessary for the model to predict whether\nthe final segment, in particular, is an incomplete\nand ongoing (requiring the ASR repair model to\npredict the future completion) or complete (requir-\ning the ASR repair model to only correct errors).\nWe expect in the final online version of the end-\nto-end system, the segmentation model will: 1. run\noften, being able to accept and segment both par-\ntial and final ASR results, 2. run on only the most\nrecent ASR, to avoid completely resegmenting an\nentire document that\u2019s been transcribed. Thus, we\nconstruct the training data for this model in a way\nto simulate these conditions. We extract all se-\nquences of turns of length between 1 \u2013 4 from TER-\nTiUS (capping to at most 4 for condition 2), take\ntheir segments u, and concatenate them to simulate\nU, asking the model to segment them back into\ntheir individual u. For the final turn of each chosen\nsequence, we include in the training data both the\nfinal ASR result and all partial ASR results. We\nfine-tune on this data with a learning rate of 1e-4\nand batch size of 4 until convergence.\nC.2\nASR Repair & Interpretation Models\nBelow we describe the concrete implementations\nand training details of each model:\nT5\nIn the T5 implementation, both MNOR and\nMINT are T5-base encoder-decoder models.\nAs described in \u00a74.4, we do not have annotations\nof programs for the full training split. Thus, we\nautomatically generate the missing programs using\nGPT3.\nWe have an initial training reservoir that con-\nsists solely of data points with program annotations\nDannot. For each example in the remaining training\nset, we retrieve a subset of samples from Dannot\nto form the prompt. We also use GPT3 for this\nretrieval step26.\nWe then annotate programs in the remaining\ntraining set in an iterative manner: as new programs\nare annotated, we use the execution engine to check\nwhether it executes to the correct end state, and if\nso, we add it to Dannot, such that future examples\ncan include these programs in their prompt.\nGPT3\nIn the GPT3 implementation, both the\nASR repair and interpretation steps occur in a sin-\ngle inference step, with GPT3 being prompted to\npredict both outputs in sequence. Specifically, it is\nprompted with:\n[Input State:]\ndi\u22121\n[Utterance ASR:] u\u2032\ni\n[Gold Utterance:] ui\n[Final State:]\ndi\nThe model is shown demonstrations in this for-\nmat from the training data, then asked to infer, for\neach test sample, the highlighted portions from the\nnon-highlighted portions.\n26we compute similarity between two prompts by looking\nat the the similarity over next-token distributions when condi-\ntioned on each of the prompts\nMetric\nT5\nGPT3\nASR Repair\nEM\n47.3\n70.7\nInterpretation\nProgram EM\n36.1\n-\nState EM\n33.7\n54.2\nTable 5: We evaluate the ASR repair and interpretation\ncomponents in isolation. We experiment with a fine-\ntuned T5 vs. a prompted GPT3 model.\nIn the setting that we are predicting programs\ninstead of end states, the final 2 lines are replaced\nwith\n[Lispress:] \u2113i\nD\nResults\nD.1\nSegmentation\nWe run all the error analyses in this section on a\nmodel trained and tested exclusively on the Repli-\ncate doc task (where annotators were asked to repli-\ncate emails from the Enron Email Dataset).\nWe do not evaluate the segmentation model on\nall of the transcripts that arise during a trajectory,\nmany of which are prefixes of one another. Doing\nso would pay too little attention to the later seg-\nments of the trajectory. (F1 measure on the final\ntranscript will weight all of the segments equally,\nbut F1 measure on the earlier transcripts does not\nconsider the later segments at all.)\nInstead, we create an evaluation set of shorter\ntranscripts. For each trajectory, we form its final\nfull transcript by concatenating all of its final ASR\nresult results. Each sequence of up to 4 consecutive\ngold segments of this full transcript is concatenated\nto form a short transcript that the segmentation\nmodel should split back into its gold segments. For\nexample, if the full transcript consists of 8 gold\nsegments, then it will have 8+7+6+5 evaluation\nexamples of 1 to 4 segments each.\nD.1.1\nError Analysis\nBelow, we list some examples of segmentation er-\nrors ([\u00b7] is used to specify segment boundaries, yel-\nlow-highlighted segments correspond to command\nsegments, while non-highlighted segments corre-\nspond to dictation segments).\n1. Input: Take out the word it. Before the word\nshould. And then replace third with three.\nTrue Segmentation: [Take out the word it. Be-\nfore the word should. And then replace third\nwith three.]\nPred Segmentation: [Take out the word it.]\n[Before the word should. And then replace\nthird with three.]\n2. Input: You learned. You lie not you learned.\nTrue Segmentation: [You learned.] [You lie\nnot you learned.]\nPred Segmentation: [You learned. You lie\nnot you learned.]\n3. Input: Skillings calendar is amazingly full!\nLet\u2019s shoot for one of the following.Skillings\nshould be apostrophe s Let\u2019s schedule it ASAP.\nTrue Segmentation: [Skillings calendar is\namazingly full! Let\u2019s shoot for one of the fol-\nlowing.] [Skillings should be apostrophe s]\n[Let\u2019s schedule it ASAP.]\nPred Segmentation: [Skillings calendar is\namazingly full! Let\u2019s shoot for one of the fol-\nlowing.Skillings should be apostrophe s Let\u2019s\nschedule it ASAP.]\nThese examples illustrate two prototypical modes\nof errors: (i) the ASR system making erroneous\njudgments about sentence boundary locations, lead-\ning the segmentation model astray, and (ii) com-\nmands being phrased in ways that disguise them as\ndictations. The first example illustrate error type\n(i): the ASR system oversegments the input (which\nshould\u2019ve been a single sentence) into three sepa-\nrate sentences, consequently leading the segmenta-\ntion system to believe \u201cTake out the word it\u201d and\n\u201cBefore the word should...\u201d are separate commands.\nThe second example illustrates error type (ii): \u201cYou\nlie not you learned.\u201d is supposed to be a replace\ncommand indicating \u201cYou learned\u201d should be re-\nplaced with \u201cYou lie\u201d, but it is not phrased as an\nexplicitly command. Finally, the third example\nillustrates both error types: we see that the ASR\nsystem undersegments the input and combines the\nsentence \u201cSkillings should be apostrophe s\u201d with\nthe sentence \u201cLet\u2019s schedule it ASAP\u201d without a pe-\nriod. Combined with the fact that \u201cSkillings should\nbe apostrophe s\u201d is not issued explicitly as a com-\nmand, this confuses the segmentation model into\nthinking that it is in fact part of the dictation.\nD.1.2\nSuccess Cases: Fixing Erroneous\nSegmentations\nThe above examples illustrated certain cases where\nthe segmentation model was misled by erroneous\nASR judgments about sentence boundary locations.\nIn some cases, however, the segmentation model is\nable to fix these judgements, as shown below:\n1. Input: Take out the extra space. In between\nthe two words, but and should.\nTrue/pred Segmentation: [Take out the ex-\ntra space. In between the two words, but and\nshould.]\n2. Input: Replace the period. With a comma\nafter restructuring.\nTrue/pred Segmentation: [Replace the pe-\nriod. With a comma after restructuring.]\nD.2\nASR Repair\nMetrics\nTo measure the ASR repair step in isola-\ntion, we take noisy utterances ui corresponding to\neach command and measure to what extent we are\nable to reconstruct the ground-truth utterance. We\nmeasure the percent of ui for which our predicted\nrepaired utterance exactly matches the ground-truth\nutterance (EM).\nResults\nFrom Table 5, we see that the GPT3\nmodel is much better at repairing speech disfluen-\ncies and ASR errors than the T5 model, achieving\n70% EM. We suspect this is due to the fact that\nGPT3 was pretrained on much more (English) lan-\nguage data than T5, giving GPT3 a greater ability\nto produce grammatically coherent and permissible\nEnglish sentences, and likely also a better sense of\ncommon disfluencies.\nQualitative Analysis\nRecall that we designed\nthe ASR repair step to condition on not just the\nutterance ui but the state di\u22121. This allows it take\ndi\u22121 into account when repairing ui.\nFor example, when given the following utter-\nance:\nDelete the period after events.\nAn ASR repair model that looks at ASR alone may\nnot see any issue with this utterance. However,\ngiven the document state:\nEric, I shall be glad to talk to you about\nit. The first three days of the next week\nwould work for me. Vince.\n(note the word events does not appear anywhere in\nthis text), the ASR repair model realizes that the\nactual utterance should\u2019ve been,\nDelete the period after Vince.\nIndeed, the T5 ASR repair model is able to make\nthe appropriate correction to this utterance.\nD.3\nInterpretation\nMetrics\nTo measure the interpretation step in\nisolation, we take normalized utterances u\u2032\ni cor-\nresponding to each command and measure to how\nwell the interpretation model is able to reconstruct\nthe ground-truth final state for the command di.\nWe use the same set of metrics described in \u00a76.2\n(state EM, program EM). However, insteading of\nfeeding the interpretation model ASR repair results,\nwe feed in ground-truth utterances u.\nResults\nWe evaluate a T5 interpretation model\nthat produces programs (which is then executed by\nour execution engine) and a GPT3 interpretation\nmodel that directly generates states. Results are\nreported in Table 5.\nWe can also compare these isolated interpreta-\ntion results with the joint ASR and interpretation\nresults reported in Table 2. Due to error propaga-\ntion, the T5 model is \u223c5\u20138% worse when asked to\njointly perform ASR repair and interpretation from\nnoisy ASR, than when simply asked to interpret\nnormalized utterances. Surprisingly however, the\nGPT3 model performs nearly as well in the joint\nevaluation as the isolated evaluation. We suspect\nthat even if the GPT3 ASR repair model does re-\nturn the exactly normalized utterances, it is still\nable to reconstruct a semantically equivalent/simi-\nlar command.\nE\nInfrastructure and Reproducibility\nWe trained 220M-parameter T5-base model on a\nsingle NVIDIA Tesla A100 GPU machine. Each\ntraining run for each component of the model took\nat most a few hours (<8). We also prompted a\n12B-parameter GPT3 model.\nWe used PyTorch (Paszke et al., 2019) and Hug-\ngingface Transformers (Wolf et al., 2020) for im-\nplementing and training T5-base models. We use\nOpenAI\u2019s API27 for querying GPT3. We use the\ntext-davinci-003 model.\n27https://beta.openai.com/\n"
  },
  {
    "title": "AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System",
    "link": "https://arxiv.org/pdf/2307.04577.pdf",
    "upvote": "1",
    "text": "AnyTeleop: A General Vision-Based Dexterous\nRobot Arm-Hand Teleoperation System\nYuzhe Qin1\u2217,\nWei Yang2,\nBinghao Huang1,\nKarl Van Wyk2\nHao Su1,\nXiaolong Wang1,\nYu-Wei Chao2,\nDieter Fox2\n1UC San Diego\n2NVIDIA\nhttp://anyteleop.com\nSAPIEN\nSimulator\nIsaacGym\nSimulator\nReal\nWorld\nFig. 1: We present AnyTeleop, a vision-based teleoperation system for a variety of scenarios to solve a wide range of\nmanipulation tasks. AnyTeleop can be used for various robot arms with different robot hands. It also supports teleoperation\nwithin different realities, such as IsaacGym (top row), and SAPIEN simulator (middle row), and real world (bottom rows).\nAbstract\u2014Vision-based teleoperation offers the possibility\nto endow robots with human-level intelligence to physically\ninteract with the environment, while only requiring low-cost\ncamera sensors. However, current vision-based teleoperation\nsystems are designed and engineered towards a particular robot\nmodel and deploy environment, which scales poorly as the pool\nof the robot models expands and the variety of the operating\nenvironment increases. In this paper, we propose AnyTeleop,\na unified and general teleoperation system to support multiple\ndifferent arms, hands, realities, and camera configurations\nwithin a single system. Although being designed to provide\ngreat flexibility to the choice of simulators and real hardware,\nour system can still achieve great performance. For real-world\n* Yuzhe Qin was an intern at NVIDIA during the project.\nexperiments, AnyTeleop can outperform a previous system that\nwas designed for a specific robot hardware with a higher\nsuccess rate, using the same robot. For teleoperation in simula-\ntion, AnyTeleop leads to better imitation learning performance,\ncompared with a previous system that is particularly designed\nfor that simulator.\nI. INTRODUCTION\nA grand goal of robotics is to endow robots with human-\nlevel intelligence to physically interact with the environment.\nTeleoperation [39], as a direct means to acquire human\ndemonstrations for teaching robots, has been a powerful\nparadigm to approach this goal [22, 11, 67, 17, 34, 6, 19,\n5, 38, 53, 63]. Compared to gripper-based manipulators,\narXiv:2307.04577v2  [cs.RO]  2 Aug 2023\nteleoperating dexterous hand-arm systems poses unprece-\ndented challenges and often requires specialized apparatus\nthat comes with high costs and setup efforts, such as Virtual\nReality (VR) devices [4, 17, 15], wearable gloves [29, 30],\nhandheld controller [47, 48, 20], haptic sensors [12, 23,\n52, 55], or motion capture trackers [68]. Fortunately, recent\ndevelopments in vision-based teleoperation [2, 24, 16, 26,\n43, 27, 21, 22, 3] have provided a low-cost and more\ngeneralizable alternative for teleoperating dexterous robot\nsystems.\nDespite the progress, the current paradigm of vision-\nbased teleoperation systems still falls short when it comes to\nscaling up data collection for robot teaching. First, prior sys-\ntems are often designed and engineered towards a particular\nrobot model or deployment environment. For example, some\nsystems rely on vision-based hand tracking models trained\non datasets collected in the deployed studio [24, 16], and\nsome rely on human-robot retargeting models [65, 13] or\ncollision avoidance models [54] trained for the particular\nrobot at use. These systems will scale poorly as the pool\nof robot models expands and the variety of operating en-\nvironments increases. Second, each system is created and\ncoupled with one specific \u201creality\u201d, either only in the real\nworld or with a particular choice of simulators. For example,\nthe HAPTIX [23] motion capture system is only developed\nfor teleoperation in MuJoCo-based environments [58]. To\nfacilitate large-scale data collection with simulation as well\nas closing sim-to-real gaps, we need teleoperation systems to\noperate both in virtual (with arbitrary choices of simulators)\nand in the real world. Finally, existing teleoperation systems\nare often tailored for single-operator and single-robot set-\ntings. To teach robots how to collaborate with other robot\nagents as well as with human agents, a teleoperation system\nshould be designed to support multiple pilot-robot partners\nwhere the robots can physically interact with each other in\na shared environment.\nIn this paper, we aim to set the foundation for scaling\nup data collection with vision-based dexterous teleoperation,\nby filling in the aforementioned gaps. To this end, we pro-\npose AnyTeleop, a unified and general teleoperation system\n(Fig. 1), which can be used for:\n\u2022 Diverse robot arm and dexterous hand models;\n\u2022 Diverse realities, i.e. different choices of simulators or\nthe real world;\n\u2022 Teleoperation from diverse geographic locations, via\na browser-based web visualizer developed for remote\nvisual feedback;\n\u2022 Diverse camera configurations, e.g. RGB camera with\nor without depth, single or multiple cameras;\n\u2022 Diverse operator-robot partnerships, e.g. two operators\nseparately piloting two robots to collaboratively solve\na manipulation task.\nTo achieve this goal, we first develop a general and high-\nperformance motion retargeting library to translate human\nmotion to robot motion in real time without learned models.\nOur collision avoidance module is also learning-free and\npowered by CUDA-based geometry queries. They can adapt\nto new robots given only the kinematic model, i.e., URDF\nfiles. Second, we develop a web-based viewer compatible\nwith standard browsers, to achieve simulator-agnostic visu-\nalization and enable remote teleoperation across the internet.\nThird, we define a general software interface for visual-\nbased teleoperation, which standardizes and decouples each\nmodule inside the teleoperation system. It enables smooth\ndeployment on different simulators or real hardware.\nWhile being very general to support many settings with a\nsingle system, our system can still achieve great performance\nin the experiments. For real-world teleoperation, AnyTeleop\ncan outperform a previous system [54] designed for specific\nrobot hardware with higher success rates on 8 out of 10\ntasks proposed in their paper, using the same robot as [54].\nFor simulated environment teleportation, the smoother and\ncollision-free demonstrations collected by AnyTeleop can\nbring better imitation learning results with higher success\nrates on 5 out of 6 tasks proposed in their paper, compared\nwith a previous system [43] specifically designed for that\nsimulator. Finally, we demonstrate that AnyTeleop can be\nextended to support collaborative manipulation, which to our\nbest knowledge has neither been achieved in the literature\nof vision-based teleoperation nor on dexterous hands.\nOur system is also packaged to be easily-deployable. The\ncontainerized design makes installation easy and frees users\nfrom handling software dependencies. We are committed to\nopen-sourcing the system and benefiting the community.\nII. RELATED WORK\nVision-based Robot Teleoperation. Recent years have\nwitnessed an increasing interest in teleoperation of dex-\nterous robot hands by human hands. It relies on accurate\ntracking of human hand motions and finger articulations.\nCompared to the costly wearable hand tracking solutions,\nsuch as gloves [29, 30, 37], marker-based motion capture\nsystems [68, 31], inertia sensors [66] or VR headsets [4, 28,\n51, 41], vision-based hand tracking is particularly favorable\ndue to its low cost and low intrusion to the human operator.\nEarly research in vision-based teleoperation focused on\nimproving the performance of hand tracking [57, 60, 1] and\nmapping human hand pose to robot hand pose [24, 2, 5, 35].\nRecent works have expanded the scope of teleoperating a\nsingle robot hand to complete arm-hand systems [16, 25, 67].\nHowever, these systems are designed and engineered towards\na particular robot model (e.g., Kuka arm with Allegro hand\nin [16], and PR2 arm with Shadow hand in [26]), and rely on\nretargeting or collision detection models trained for specific\nrobot hardware (e.g., Allegro hand and XArm6) [16, 54] ,\nmaking them difficult to transfer to new arm-hand systems\nand new environments. In contrast, our system is highly\nmodularized with a versatile hand-tracking solution compat-\nShared space\nReal\nLocation A\nReal\nLocation B\nReal\nLocation C\nReal - Location X or Virtual\nCollaborative Setting\nIndependent Setting\nVisual Feedback\nControl\nReal (local or remote)\nor Virtual\nPilot\nRobot\nFig. 2: Paradigms of vision-based teleoperation systems in independent and collaborative settings. The system should\nsupport any arm-hand models, existed in either virtual or the real world, can operate with flexible camera configurations,\nprovide visual feedback for both local or remote presence, and support multiple robots piloted in a shared space.\nible with an arbitrary number of cameras, and configurable\nrobot hand retargeting and motion generation modules for\neasy adaption to various robot arms and robot hand choices.\nThis allows our system to achieve better performance com-\npared to prior systems on various tasks while generalizing to\na set of robot arm-hand systems and multiple environments.\nTeleoperation in Different Reality. Manipulation with a\ndexterous robot hand is challenging due to its high degree of\nfreedom. In recent years, dexterous robot teleoperation has\nbeen actively studied and shown promising progress in con-\ntrolling a multi-fingered hand to perform manipulation tasks\nin the real world [4, 16, 36] by leveraging the morphological\nsimilarity between the dexterous hand and the human hand.\nWith the advancement of data-driven approaches for\nrobot manipulation [12, 18], there is a growing need to\ncollect human demonstrations in robotics. To enable easy\nand scalable data collection, teleoperation has also gained\nattention in simulated environments [62, 32, 58, 14, 8]. This\nprovides a scalable solution to data collection by eliminating\nthe need for real hardware, while maintaining access to\noracle world information. For example, Mandlekar et al. [33]\ndeveloped a crowd-sourcing platform to teleoperate robots\nvia mobile devices as controllers. Tung et al. [59] further\nextended this framework to allow multi-arm collaborative\nteleoperation. The above frameworks rely on inertial sensors\nfor control signals and thus are limited to parallel-gripper\nand simple tasks such as pick-and-place. Our system offers\nthe ability to perform a wide range of dexterous tasks with\nrobots of different morphologies by utilizing state-of-the-\nart techniques in perception, optimization, and control. In\naddition, AnyTeleop is designed to support teleoperation in\nboth virtual and the real world with a unified framework.\nIII. SYSTEM OVERVIEW\nFig. 2 illustrates our proposed paradigms of vision-based\nteleoperation systems. Below we introduce the features and\ndesigns of our system which realize the paradigms.\nA. System Features\n1) Any arm-hand. As shown in Fig. 1, AnyTeleop is\ndesigned for arbitrary dexterous arm-hand systems that\nare not limited to any specific robot type.\n2) Any reality. AnyTeleop is decoupled from specific\nhardware drivers or physics simulators. It can support\ndifferent realities as visualized in Fig. 1.\n3) Anywhere remote teleoperation. AnyTeleop provides\na web-based visualizer to monitor the teleoperation\nand simulation in standard web browsers, e.g. Chrome.\n4) Any camera configuration. AnyTeleop can consume\ndata from both RGB and RGB-D cameras, and from\neither single or multiple cameras. Most importantly,\nit does not require extrinsic calibration as in most\nprevious systems. This allows more flexible camera\nconfigurations and lower deployment overhead.\n5) Any\nnumber\nof\noperator-robot\npartnerships.\nAnyTeleop supports collaborative settings where op-\nerators separately pilot two robots to collaboratively\nsolve a manipulation task.\n6) Simple deployment. AnyTeleop and all libraries are\nencapsulated as a docker image that can be down-\nloaded and deployed on any Linux machine, which\nfrees users from handling troublesome dependencies.\nTable I compares AnyTeleop with other vision-based\ndexterous teleoperation systems. We compare the systems\nSensor Requirements\nRobot-related Support\nUse Case\nCalibration\nContact\nDepth\nMultiple\nMultiple\nReality\nCollision\nRemote\nCollaborative\nFree\nFree\nFree\nArms\nHands\nFree\nTeleop\nTeleop\nDexPilot [16]\n\u2713\nReal\n\u2713\nHolo-Dex [4]\n\u2713\n\u2713\n\u2713\nNo Arm\nReal\n\u2713\nDIME [5]\n\u2713\n\u2713\nNo Arm\nReal\n\u2713\nTeachNet [24]\n\u2713\n\u2713\nNo Arm\nSim&Real\nTelekinesis [54]\n\u2713\n\u2713\nReal\n\u2713\n\u2713\nQin et al. [43]\n\u2713\n\u2713\nNo Arm\n\u2713\nSim\n\u2713\nMVP-Real [45]\n\u2713\nNo Arm\nReal\n\u2713\nTransteleop [26]\nReal\n\u2713\n\u2713\nMosbach et al. [37]\n\u2713\nSim\n\u2713\nAnyTeleop\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nSim&Real\n\u2713\n\u2713\n\u2713\nTABLE I: Comparison of Vision-Based Teleoperation System. We compare AnyTeleop\u2019s capabilities with related visual\nteleoperation system for multi-fingered dexterous robots. \u201cNo Arm\u201d in the column of \u201cMultiple Arms\u201d means this system\ncan only control hand motion but not arm-hand systems.\nin three dimensions: (i) sensor requirements; (ii) robot-\nrelated support; (iii) afforded use cases. Among all these\nteleoperation systems, AnyTeleop is the only one which\ncan support different robot arms and enable collaborative\nteleoperation. It is also one of the only two systems that can\nsupport different dexterous hands.\nB. System Design\nThe architecture of the teleoperation system is shown\nin Fig. 3. The teleoperation server (Section IV) receives\nthe camera stream from the driver, detects the hand pose,\nand then converts it to joint control commands. The client\nreceives these commands via network communication and\nuses them to control a simulated or real robot. The sys-\ntem is designed with three key principles: modularity,\ncommunication-focused, and containerization. Modularity is\nachieved by implementing well-defined input-output inter-\nfaces for each sub-component, allowing for wide applicabil-\nity to different robot arms, dexterous hands, cameras, and\nrealities. Communication-focused design allows for remote\nand collaborative teleoperation and reduces computation\nrequirements on the operator\u2019s side by deploying heavy\ncomputations on a powerful server. Finally, the containerized\ndesign makes installation and deployment easier compared\nto other robotics systems with heavy software dependencies.\nIV. TELEOPERATION SERVER\nThe teleoperation server, outlined in Section III, utilizes\nthe RGB or RGB-D data from one or multiple cameras\nand generates smooth and collision-free control commands\nfor the robot arm and dexterous hand. It consists of four\nmodules: (i) the hand pose detection module, which predicts\nhand wrist and finger poses from the camera stream, (ii) the\ndetection fusion module, which integrates the results from\nmultiple cameras, (iii) the hand pose retargeting module,\nwhich maps human hand poses to the dexterous robot hand,\nand (iv) the motion generation module, which produces high-\nfrequency control signals for the robot arm. A standardized\nsoftware interface is defined for all four modules to facilitate\nflexibility and generalizability in AnyTeleop .\nA. Hand Pose Detection\nThe hand pose detection module offers a unique feature to\nutilize input from various camera configurations, including\nRGB or RGB-D cameras, and single or multiple cameras.\nThe design principle is to leverage more information, such as\ndepth, and additional cameras, to improve performance when\navailable. But it can also perform the task with minimal\ninput, i.e. a single RGB camera. The detection module has\ntwo outputs: local finger keypoint positions in the wrist\nframe and global 6D wrist pose in the camera frame. The\nfinger keypoint detection only requires RGB data while the\nwrist pose detection can optionally use depth information to\nachieve better results.\nFinger Keypoint Detection. Our finger keypoint detec-\ntion utilizes MediaPipe [64], a lightweight, RGB-based hand\ndetection tool that can operate in real-time on a CPU. The\nMediaPipe detector can accurately locate 3D keypoints of\n21 hand-knuckle coordinates in the wrist frame and 2D\nkeypoints on the image.\nWrist Pose Detection from RGB-D. We use the pixel\npositions of the detected keypoints to retrieve the corre-\nsponding depth values from the depth image. Then, utilizing\nknown intrinsic camera parameters, we compute the 3D po-\nsitions of the keypoints in the camera frame. The alignment\nof the RGB and depth images is handled by the camera\ndriver. With the 3D keypoint positions in both the local wrist\nframe and global camera frame, we can estimate the wrist\npose using the Perspective-n-Point (PnP) algorithm.\nWrist Pose Detection from RGB only. The orientation\nof the hand can be computed analytically from the local\npositions of the detected keypoints. However, determining\nthe wrist position in the camera frame can be challenging\nwithout explicit 3D information. To enhance MediaPipe for\nglobal wrist pose estimation, we adopt the approach used\nin FrankMocap [50] by incorporating an additional neural\nTeleoperation Server\nJoint \nControl \nCommand\nWeb Visualizer\nHand Detection\nDetection Fusion\nHand Retargeting\nMotion Generation\nPhysical Environment\nTeleoperation \nClient\nSimulator or \nReal Robot\nCamera Driver\nFig. 3: System Architecture. AnyTeleop is composed of four components: (i) camera driver, which captures the human\nhand pose in RGB or RGB-D format; (ii) teleportation server, the core component in our system, which performs hand pose\ndetection and converts detection results to robot control commands; (iii) teleoperated robot, which is either a real robot or\na simulated robot in a virtual environment; (iv) web visualizer, which enables remote visualization across the internet.\nnetwork that predicts the weak perspective transformation\nscale of the hand. The weak perspective transformation\napproximates the original perspective camera model by\nassuming that the observed object is farther from the camera\nthan its size. Together with intrinsic parameters, this scale\nfactor can be used to approximate the 3D position of the\nhand. The wrist position computed this way has a larger\nerror than depth camera, but it is still sufficient for many\ndownstream teleoperation tasks.\nB. Detection Fusion\nThe detection fusion module integrates multiple camera\ndetection results. Self-occlusion can be a problem when\nperforming hand pose detection, especially when the hand is\nperpendicular to the camera plane. Using multiple cameras\ncan alleviate this problem by providing additional views.\nHowever, there are two main challenges in fusing multiple\ndetection results: (i) each camera can only estimate the hand\npose in its own frame and (ii) there is no straightforward\nmetric to quantify the confidence of each detection result.\nTo overcome the first challenge, we perform an auto-\ncalibration process using the human hand as a natural\nmarker. We use the first N frames of hand detection results\nfrom multiple cameras to calculate the relative rotation\nbetween each camera, expressed in SO(3). We find that\nalthough the absolute position of detected hand pose is not\nso accurate in RGB-only setting, the relative motion between\nconsecutive frames is more robust. With orientation between\neach camera, we can transform the detected relative motion\nfrom different cameras into a single frame.\nTo address the second challenge, we use the SMPL-X [40]\nhand shape parameters predicted from the detection module,\nas inspired by Qin et al. [43]. During teleoperation, the\ntrue shape parameters should remain constant for a given\noperator, but the predicted values can contain errors dur-\ning self-occlusion. We observe that larger shape parameter\nprediction errors often correspond to larger pose errors. To\napproximate the confidence score, we take the mean of\nthe estimated shape parameters in the first N frames as\na reference and compute the error between the predicted\nshape parameters and the reference. Implementation-wise,\nwe require the operator to spread their fingers during the\nfirst N frames to ensure an accurate reference value of\nshape parameters. The fusion module then selects the relative\nmotion captured by the camera with the highest confidence\nscore and forwards it to the next module. In implementation,\nwe choose N = 50.\nC. Hand Pose Retargeting\nThe hand pose retargeting module maps the human hand\npose data obtained from perception algorithms into joint\npositions of the teleoperated robot hand. This process is\noften formulated as an optimization problem [42, 16], where\nthe difference between the keypoint vectors of the human\nand robot hand is minimized. The optimization can be\ndefined as follows:\nmin\nqt\nN\n\u2211\ni=0\n||\u03b1vi\nt \u2212 fi(qt)||2 +\u03b2||qt \u2212qt\u22121||2\ns.t.\nql \u2264 qt \u2264 qu,\n(1)\nwhere qt represents the joint positions of the robot hand at\ntime step t, vi\nt is the i-th keypoint vector for human hand\ncomputed from the detected finger keypoints, fi(qt) is the\ni-th forward kinematics function which takes the robot hand\njoint positions qt as input and computes the i-th keypoint\nvector for the robot hand, ql and qu are the lower and upper\nlimits of the joint position, \u03b1 is a scaling factor to account\nfor hand size difference. An additional penalty term with\nweight \u03b2 is included to improve temporal smoothness. When\nretargeting to a different morphology, such as a Dclaw in\nFigure 1, we need to specify the keypoint vectors mapping\nbetween the robot and human fingers manually. It is worth\nnoting that this module only considers the robot hand.\nD. Motion Generation\nGiven the detected wrist and hand pose, our goal is to\ngenerate smooth and collision-free motion of robot arm\nto reach the target Cartesian end-effector pose. Real-time\nmotion generation methods are required to have a smooth\nteleoperation experience. In the prior work of [16], the\nrobot motion is driven by Riemannian Motion Policies\nFig. 4: Real Robot Teleoperation Tasks. We replicate the ten manipulation tasks proposed in Sivakumar et al. [54] using\nsame or similar objects. Top row, left to right: Pickup Box Object, Pickup Fabric Toy, Box Rotation, Scissor Pickup, Cup\nStack. Bottom row, left to right: Two Cup Stacking, Pouring Cubes onto Plate, Cup Into Plate, Open Drawer and Open\nDrawer and Pickup Cup.\n(RMPs) [49, 7] that can calculate acceleration fields in\nreal-time. However, accelerations towards a particular end-\neffector pose do not guarantee natural trajectories. In this\nwork, we adopt CuRobo [56], a highly parallelized collision-\nfree robot motion generation library accelerated by GPUs,\nto generate natural and reactive robot motion in real-time.\nIn AnyTeleop, the motion generation module receives the\nCartesian pose of the end-effector at a low frequency (25\nHz) from the hand detection and retargeting modules, and\ngenerates collision-free joint-space trajectories within joint\nlimits at a higher frequency (120 Hz). The generated trajec-\ntories are ready for safe execution by impedance controllers\non either a simulated or real robot.\nV. WEB-BASED TELEOPERATION VIEWER\nTo better support the teleoperation tasks, we implement\na web-based visualization module to facilitate remote and\ncollaborative teleoperation, especially for teleoperation in\nsimulated environments. It has the following features: (i)\nbrowser-based viewer, which makes it easily accessible\nremotely; (ii) synchronized visualization, i.e. two operators\nworking on the same collaborative task should see the same\nscene synchronously from their own local view ports. The\nviewer is developed based upon the meshcat [10] library\nand utilize Three.js [9] for rendering. The visualization\nserver ports the simulation results onto the browser after each\nsimulation iteration. Operators can get visual feedback from\nthe browser window and move their hands to control the\ncorresponding robot. More details about the implementation\nof our viewer can be found in the supplementary materials.\nHardWare\nDesktop\nLaptop\nGPU\nRTX 3090\nRTX 2070\nCPU\ni9-10980XE\ni7-8750\nProfiling\nModules\nTime (ms)\nTime (ms)\nHand Pose (RGB)\n26\u00b15\n34\u00b15\nHand Pose (RGB-D)\n27\u00b15\n35\u00b15\nFusion\n1\u00b10\n1\u00b10\nRetargeting\n9\u00b17\n10\u00b19\nMotion\n8\u00b13\n11\u00b15\nTABLE II: Profiling Results. We profile different modules\ninside teleoperation server on both desktop and laptop. The\ntime is measured when all teleoperation modules are run on\nthe same computer simultaneously.\nVI. SYSTEM EVALUATION\nA. Profiling Analysis\nWe perform profiling on modules mentioned in Section IV\non a desktop and a laptop. As shown in Table II, the most\ntime-consuming module is hand pose detection, which runs\non a GPU for real-time inference. The designed maximum\nfrequency for hand pose detection is 25Hz, so both the\ndesktop and laptop can meet the requirement. Both the\nretargeting module and the fusion module run at the same\nfrequency as the hand detection module due to the publisher\nand subscriber logic. For best performance, the motion gen-\neration module should run at 120Hz but can still work with\na lower frequency. Notably, we found it difficult to achieve\nthis throughput when running all these modules on the same\ncomputer. Luckily, with our communication-oriented design,\nwe can run the control modules on a separate machine to\nachieve the best performance.\nB. Real Robot Teleoperation\nIn this section, we will test our AnyTeleop system across\na wide range of real-world tasks that covers diverse ob-\nRelocate \nFlip Mug \nOpen Door\nManipulation Task\nRL\nBaseline [43]\nOurs\nFloating-Hand\nRelocate\n36.3\u00b115.3\n49.7\u00b118.3\n53.7\u00b112.2\nFlip Mug\n33.7\u00b115.0\n51.3\u00b134.7\n47.3\u00b128.3\nOpen Door\n69.3\u00b138.0\n64.7\u00b114.7\n73.3\u00b1 9.0\nArm-Hand\nRelocate\n33.7\u00b129.3\n40.3\u00b136.7\n70.0\u00b1 9.8\nFlip Mug\n31.0\u00b128.7\n36.0\u00b132.4\n53.7\u00b124.0\nOpen Door\n34.7\u00b131.7\n51.3\u00b130.7\n79.7\u00b115.5\nTABLE III: Imitation Learning Experiments in SAPIEN Environments. The left figure visualizes the three tasks we use\nfor both teleoperation data collection and imitation learning. The transparent object represents the goal of the task while\nthe black arrow represents the steps of the task. The right table shows the success rate of the evaluated methods. We use\n\u00b1 to represent the mean and standard deviation over three random seeds. The success rate is computed from 100 trials.\nTask\nAnyTeleop\nTelekinesis [54]\nPickup Box Object\n1.0\n0.9\nPickup Fabric Toy\n1.0\n0.9\nBox Rotation\n0.6\n0.6\nScissor Pickup\n0.8\n0.7\nCup Stack\n0.9\n0.6\nTwo Cup Stacking\n0.7\n0.3\nPouring Cubes onto Plate\n0.7\n0.7\nCup Into Plate\n1.0\n0.8\nOpen Drawer\n1.0\n0.9\nOpen Drawer and Pickup Object\n0.9\n0.6\nTABLE IV: Real Robot Teleoperation Results. We repli-\ncate the experiment settings and tasks in [54] and compare\nwith [54]. For the baseline method, we use the success rate\nreported in their paper [54]\njects and manipulation skills. Besides, we will compare\nour teleoperation performance of AnyTeleop with a similar\nteleoperation system. A fair comparison of real-robot tasks\nis often very challenging due to the difficulty in replicating\nthe baseline methods delicately. To ensure a more fair com-\nparison, we replicate the ten manipulation tasks proposed\nin Robotic Telekinesis [54] with the same XArm6 robot,\nAllegro hand, and similar objects. A trained operator attempt\nto solve this tasks using AnyTeleop system. The ten tasks\nare visualized in Fig. 4. Same as\n[54], we run each task\nten times for AnyTeleop and use a single Intel RealSense\ncamera. For the baseline method, we directly use the results\nreported in their paper.\nAs shown in Table IV, AnyTeleop can get a higher success\nrate of 8/10 tasks and the same success rate on 2/10 com-\npared with the baseline. Although AnyTeleop is designed\nto be more general, it can still outperform the baseline\nsystem that was specifically designed for the XArm6-Allegro\nhardware. We find that the major advantage of our system is\nthe capability to handle objects with thin-walled structures,\nsuch as the cup-stack, two-cup-stacking, and cup-into-plate\ntasks. Our optimization-based retargeting module can close\nthe distance between finger tips, which makes grasping the\ncup more stable. However, the network-based retargeting can\nhardly translate the fine-grained precision grasp from human\nto robot, which leads to a lower success rate.\nVII. APPLICATIONS\nA. Imitation Learning\nThe most important application of the proposed system is\nimitation learning from demonstration. We can first collect\ndemonstrations on several dexterous manipulation tasks and\nthen use the data to train imitation learning algorithms. In\nthis experiment, we will show that the teleoperation data col-\nlected using our AnyTeleop can better support downstream\nimitation learning tasks. In the following subsection, we will\nfirst introduce the experiment setting and baseline and then\ndiscuss the experimental results.\nBaseline and Comparison. To fairly compare with pre-\nvious teleoperation systems, we need to align both the\ntask setting and robot configuration precisely. It is often\nchallenging for real-robot hardware but much easier for a\nsimulated environment. Thus, we choose a recent vision-\nbased teleoperation work [43] that can be used for simulated\nrobots as our baseline. It is worth noting that we are\ncomparing two teleoperation systems via the demonstration\ndata collected by each system. Thus, we compared with the\nbaseline by training the same learning algorithm on different\ndemonstration data collected via the baseline system and\nour teleoperation system. We follow [43] to choose Demo\nAugmented Policy Gradient (DAPG) [46] as the imitation\nalgorithm. We also compare it with a pure reinforcement\nlearning (RL) based algorithm from [44] which does not\nutilize demonstrations. We provide the same dense reward\nfor RL training as previous work [43].\nManipulation Tasks. We directly use the manipulation\ntasks proposed by the baseline work [43] for comparison,\nwhich include three tasks: (i) Relocate, where the robot picks\nan object on the table and moves it to the target position;\n(ii) Flip Mug, where the robot needs to rotate the mug for\n90 degrees to flip it back; (iii) Open Door, where the robot\nneeds to first rotate the lever to unlock the door, and then\npull it to open the door. These tasks are visualized in the\nleft figure of Table III. The manipulated objects in all three\ntasks are randomly initialized and the target position is also\nrandomized in Relocate. Each manipulation task has two\nvariants: the floating-hand variant and the arm-hand variant.\nThe floating-hand is a dexterous hand without a robot arm\nRobot 1 \nControlled by Operator #1\nOn Computer #1\nRobot 2\nControlled by Operator #2\nOn Computer #2\nFig. 5: Collaborative Teleoperation for Handover Task.\nOperator #1 act as the UR10-Schunk robot and operator #2\nacts as the Kuka-Shadow robot. In this task, the operator #2\nneeds to pick up an object on the table and handover it to\noperator #1.\nthat can move freely in space. The arm-hand means the hand\nis mounted on a robot arm with a fixed base, which is a more\nrealistic setting.\nDemonstration Details. For the baseline teleoperation\nsystem [43], we directly use the demonstration collected\nby the original authors with 50 demonstration trajectories\nfor each task. The baseline system only utilizes a single\nRGB-D camera. For fairness, we also collect 50 trajectories\nfor each task using the single camera setup. The baseline\nsystem can only handle floating hands and they propose a\ndemonstration translation pipeline to convert the demonstra-\ntion with floating hands to demonstrations with arm-hand.\nFor our AnyTeleop , we collect demonstrations using the\narm-hand setting and convert the demonstration to floating\nhand so that the demonstration can be used by both the\nfloating-hand variant and arm-hand variant.\nResults and Discussion. For each method on each task,\nwe train policies with three different random seeds. For each\npolicy, we evaluate it on 100 trials. More details about the\nsuccess metrics can be found in [43]. As shown in Table III,\nthe imitation learning algorithm trained on demonstration\ncollected by AnyTeleop can outperform baseline and RL on\nmost tasks with one exception. Compared with the demon-\nstration collected via the baseline system, our system has two\nbenefits that contribute to better performance in imitation\nlearning: (i) The collected trajectory is more smooth, which\nmeans that the state-action pairs are more consistent and\neasier to be consumed by the network. (ii) Different from the\nbaseline, our system explicitly supports teleoperation with\narm-hand system and guarantees no self-collision. On the\ncontrary, the baseline system utilizes retargeting to generate\njoint trajectory for robot arm, which may lead to several\nself-collision for robot arm. Thus we can observe significant\nperformance gain of our system for manipulation tasks with\narm-hand. For the flip mug task, the difficulty of collecting\ndemonstration with arm is much larger than with a floating\nWeb Visualization \nServer\nPhysical \nSimulation\nHeadless \nServer\nComputer\nBrowser\nCamera\nOperator\nTeleoperation Unit #1\nVisualization \nData\nTeleoperation \nCommand\nTeleoperation Unit #N\nComputer\nBrowser\nCamera\nOperator\nVisualization \nData\nFig. 6: Collaborative Teleoperation System. Our system\ncan be extended to collaborative manipulation tasks even\nwhen operators are not in the same physical location. Each\noperator can use a local computer with camera to detect\nthe hand pose and send the detection results to a central\nserver. Meanwhile, they can use the web browser to visualize\nthe current simulation environment, including the robot\ncontrolled by other operators.\nhand, which influences the demonstration quality.\nB. Collaborative Manipulation\nCollaborative manipulation is a key technology for the de-\nvelopment of human-robot systems [59]. Collecting demon-\nstration data for collaborative manipulation tasks has been a\nchallenging task since it requires multiple operators to work\ntogether seamlessly. With our modularized and extensible\nsystem design and web-based visualization, our system\nenables convenient data collection on collaborative tasks,\neven if operators are not in the same physical location. In\nthis section, we show that our teleoperation system can be\nextended to a collaborative setting where multiple opera-\ntors coordinate together to perform manipulation tasks. We\nchoose human-to-robot handover as an example as shown in\nFig. 5. In this setting, operator #1 control a robot hand, and\noperator #2 control a human hand.\nCollaborative Teleoperation System Design. Fig. 6 il-\nlustrates the system architecture for multi-operator collab-\noration, which includes two components. (i) Teleoperation\nUnits: It is composed of a computer that is connected to at\nleast one camera and a human operator. In each teleoperation\nunit, the human operator will watch the real-time visualiza-\ntion on a web browser and move the hand accordingly to\nperform manipulation tasks. (ii) Central Server: it runs the\nphysical simulation and the web visualization server. The\ndetection results from multiple teleoperation units are sent\nto the server and converted into robot control commands\nbased on the pipeline in Section IV. Meanwhile, the web\nvisualizer server will keep synchronized with the simulated\nenvironment and maintain the visualization resources as\ndescribed in Section V.\nVIII. FAILURE MODES\nAs illustrated on the project page, we have identified two\nfailure modes: (i) loss of tracking during fast human hand\nmotion, which triggers a pause and re-detection process; (ii)\nunreliable hand pose when the hand is in self-occlusion.\nFor mode (i), the workaround is to instruct the operator to\nslow down their hand motion. The issue (ii) can be solved\nby incorporating multiple cameras for tasks that require\nsignificant hand rotation.\nIX. CONCLUSION\nIn this paper, we introduced AnyTeleop, a versatile teleop-\neration system that can be applied to diverse robots, assorted\nreality, and varied camera setup, and can be operated by\na flexible number of users from any geographic locations.\nThe experiments show that AnyTeleop outperforms previous\nsystems in both simulation and real-world scenarios while\noffering superior generalizability and flexibility. Our com-\nmitment to an open-source approach will facilitate further\nresearch in the field of teleoperation.\nX. ACKNOWLEDGEMENT\nWe express our gratitude to Ankur Handa, Balakumar\nSundaralingam, and Nick Walker for their insightful dis-\ncussions throughout the development process of the motion\ncontrol module in AnyTeleop. We would also like to extend\nour thanks to Isabella Liu, An-Chieh Cheng, Ruihan Yang,\nYang Fu, Linghao Chen, Jiarui Xu, Xinyu Zhang, Xinyue\nWei, Jiteng Mu, and Jianglong Ye for their efforts in testing\nand evaluating the teleoperation system.\nREFERENCES\n[1] Insaf Ajili, Malik Mallem, and Jean-Yves Didier. Ges-\nture recognition for humanoid robot teleoperation. In\n2017 26Th IEEE international symposium on robot and\nhuman interactive communication (RO-MAN), pages\n1115\u20131120. IEEE, 2017.\n[2] Dafni Antotsiou, Guillermo Garcia-Hernando, and Tae-\nKyun Kim. Task-oriented hand motion retargeting for\ndexterous manipulation imitation. In ECCV Workshops,\n2018.\n[3] Reuben M Aronson and Henny Admoni. Gaze comple-\nments control input for goal prediction during assisted\nteleoperation. In Robotics science and systems, 2022.\n[4] Sridhar Pandian Arunachalam, Irmak G\u00a8uzey, Soumith\nChintala, and Lerrel Pinto. Holo-dex: Teaching dex-\nterity with immersive mixed reality.\narXiv preprint\narXiv:2210.06463, 2022.\n[5] Sridhar Pandian Arunachalam, Sneha Silwal, Ben\nEvans, and Lerrel Pinto.\nDexterous imitation made\neasy: A learning-based framework for efficient dexter-\nous manipulation.\narXiv preprint arXiv:2203.13251,\n2022.\n[6] Sean Chen, Jensen Gao, Siddharth Reddy, Glen\nBerseth, Anca D Dragan, and Sergey Levine. Asha: As-\nsistive teleoperation via human-in-the-loop reinforce-\nment learning. In 2022 International Conference on\nRobotics and Automation (ICRA), pages 7505\u20137512.\nIEEE, 2022.\n[7] Ching-An Cheng, Mustafa Mukadam, Jan Issac, Stan\nBirchfield, Dieter Fox, Byron Boots, and Nathan\nRatliff.\nRmp flow: A computational graph for au-\ntomatic motion policy generation.\nIn Algorithmic\nFoundations of Robotics XIII: Proceedings of the 13th\nWorkshop on the Algorithmic Foundations of Robotics\n13, pages 441\u2013457. Springer, 2020.\n[8] Erwin Coumans and Yunfei Bai. PyBullet, a python\nmodule for physics simulation for games, robotics and\nmachine learning. GitHub repository, 2016.\n[9] Brian Danchilla and Brian Danchilla. Three. js frame-\nwork. Beginning WebGL for HTML5, pages 173\u2013203,\n2012.\n[10] Robin Deits.\nMeshcat.\nhttps://github.com/rdeits/\nmeshcat, 2018.\n[11] Guanglong Du, Ping Zhang, Jianhua Mai, and Zel-\ning Li.\nMarkerless kinect-based hand tracking for\nrobot teleoperation. International Journal of Advanced\nRobotic Systems, 9(2):36, 2012.\n[12] Jean Elsner, Gerhard Reinerth, Luis Figueredo, Abdeld-\njallil Naceri, Ulrich Walter, and Sami Haddadin. Parti-a\nhaptic virtual reality control station for model-mediated\nrobotic applications.\nFrontiers in Virtual Reality, 3,\n2022.\n[13] Bin Fang, Xiao Ma, Jiachun Wang, and Fuchun\nSun. Vision-based posture-consistent teleoperation of\nrobotic arm using multi-stage deep neural network.\nRobotics and Autonomous Systems, 131:103592, 2020.\n[14] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian\nMrowca, Martin Schrimpf, James Traer, Julian De Fre-\nitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick\nHaber, et al.\nThreedworld: A platform for interac-\ntive multi-modal physical simulation. arXiv preprint\narXiv:2007.04954, 2020.\n[15] Zaid Gharaybeh, Howard Chizeck, and Andrew Stew-\nart. Telerobotic control in virtual reality. IEEE, 2019.\n[16] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang,\nYu-Wei Chao, Qian Wan, Stan Birchfield, Nathan\nRatliff, and Dieter Fox. Dexpilot: Vision-based tele-\noperation of dexterous robotic hand-arm system.\nIn\nICRA, 2020.\n[17] Hooman Hedayati, Michael Walker, and Daniel Szafir.\nImproving collocated robot teleoperation with aug-\nmented reality. In International Conference on Human-\nRobot Interaction, 2018.\n[18] Zebin Huang, Ziwei Wang, Weibang Bai, Yanpei\nHuang, Lichao Sun, Bo Xiao, and Eric M Yeatman. A\nnovel training and collaboration integrated framework\nfor human\u2013agent teleoperation. Sensors, 21(24):8341,\n2021.\n[19] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kap-\npler, Frederik Ebert, Corey Lynch, Sergey Levine, and\nChelsea Finn. Bc-z: Zero-shot task generalization with\nrobotic imitation learning.\nIn Conference on Robot\nLearning, pages 991\u20131002. PMLR, 2022.\n[20] Bachir El Khadir, Jake Varley, and Vikas Sindhwani.\nTeleoperator imitation with continuous-time safety.\narXiv preprint arXiv:1905.09499, 2019.\n[21] Jonathan Kofman, Xianghai Wu, Timothy J Luu, and\nSiddharth Verma.\nTeleoperation of a robot manip-\nulator using a vision-based human-robot interface.\nIEEE transactions on industrial electronics, 52(5):\n1206\u20131219, 2005.\n[22] Jonathan Kofman, Siddharth Verma, and Xianghai Wu.\nRobot-manipulator teleoperation by markerless vision-\nbased hand-arm tracking.\nInternational Journal of\nOptomechatronics, 2007.\n[23] Vikash Kumar and Emanuel Todorov. Mujoco haptix:\nA virtual reality system for hand manipulation.\nIn\nInternational Conference on Humanoid Robots (Hu-\nmanoids), 2015.\n[24] Shuang Li, Xiaojian Ma, Hongzhuo Liang, Michael\nG\u00a8orner, Philipp Ruppel, Bin Fang, Fuchun Sun, and\nJianwei Zhang. Vision-based teleoperation of shadow\ndexterous hand using end-to-end deep neural network.\nIn ICRA, 2019.\n[25] Shuang Li, Jiaxi Jiang, Philipp Ruppel, Hongzhuo\nLiang, Xiaojian Ma, Norman Hendrich, Fuchun Sun,\nand Jianwei Zhang. A mobile robot hand-arm teleop-\neration system by vision and IMU. In IROS, 2020.\n[26] Shuang\nLi,\nNorman\nHendrich,\nHongzhuo\nLiang,\nPhilipp Ruppel, Changshui Zhang, and Jianwei Zhang.\nA dexterous hand-arm teleoperation system based on\nhand pose estimation and active vision. IEEE Trans-\nactions on Cybernetics, 2022.\n[27] Jacky Liang, Ankur Handa, Karl Van Wyk, Viktor\nMakoviychuk, Oliver Kroemer, and Dieter Fox.\nIn-\nhand object pose tracking via contact feedback and\ngpu-accelerated robotic simulation.\nIn 2020 IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 6203\u20136209. IEEE, 2020.\n[28] Jeffrey I Lipton, Aidan J Fay, and Daniela Rus. Bax-\nter\u2019s homunculus: Virtual reality spaces for teleopera-\ntion in manufacturing. IEEE Robotics and Automation\nLetters, 3(1):179\u2013186, 2017.\n[29] Hangxin Liu, Xu Xie, Matt Millar, Mark Edmonds,\nFeng Gao, Yixin Zhu, Veronica J Santos, Brandon\nRothrock, and Song-Chun Zhu. A glove-based system\nfor studying hand-object manipulation via joint pose\nand force sensing.\nIn 2017 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 6617\u20136624. IEEE, 2017.\n[30] Hangxin Liu, Zhenliang Zhang, Xu Xie, Yixin Zhu,\nYue Liu, Yongtian Wang, and Song-Chun Zhu. High-\nfidelity grasping in virtual reality using a glove-based\nsystem. In 2019 international conference on robotics\nand automation (icra), pages 5180\u20135186. IEEE, 2019.\n[31] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and\nXiaolong Wang. Semi-supervised 3d hand-object poses\nestimation with interactions in time. In CVPR, 2021.\n[32] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong\nGuo, Michelle Lu, Kier Storey, Miles Macklin, David\nHoeller, Nikita Rudin, Arthur Allshire, Ankur Handa,\net al.\nIsaac gym: High performance gpu-based\nphysics simulation for robot learning. arXiv preprint\narXiv:2108.10470, 2021.\n[33] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan\nBooher, Max Spero, Albert Tung, Julian Gao, John\nEmmons, Anchit Gupta, Emre Orbay, et al. Roboturk:\nA crowdsourcing platform for robotic skill learning\nthrough imitation. In Conference on Robot Learning,\npages 879\u2013893. PMLR, 2018.\n[34] Ajay Mandlekar, Danfei Xu, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nYuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-\nthe-loop imitation learning using remote teleoperation.\narXiv preprint arXiv:2012.06733, 2020.\n[35] Cassie Meeker, Maximilian Haas-Heger, and Matei\nCiocarlie. A continuous teleoperation subspace with\nempirical and algorithmic mapping algorithms for\nnonanthropomorphic hands.\nIEEE Transactions on\nAutomation Science and Engineering, 19(1):373\u2013386,\n2020.\n[36] C Mizera, T Delrieu, V Weistroffer, C Andriot, A De-\ncatoire, and J-P Gazeau. Evaluation of hand-tracking\nsystems in teleoperation and virtual dexterous manipu-\nlation. IEEE Sensors Journal, 20(3):1642\u20131655, 2019.\n[37] Malte Mosbach, Kara Moraw, and Sven Behnke. Ac-\ncelerating interactive human-like manipulation learning\nwith GPU-based simulation and high-quality demon-\nstrations. In Humanoids, 2022.\n[38] Katharina\nMuelling,\nArun\nVenkatraman,\nJean-\nSebastien Valois, John Downey, Jeffrey Weiss, Shervin\nJavdani,\nMartial\nHebert,\nAndrew\nB\nSchwartz,\nJennifer\nL\nCollinger,\nand\nJ\nAndrew\nBagnell.\nAutonomy\ninfused\nteleoperation\nwith\napplication\nto bci manipulation. arXiv preprint arXiv:1503.05451,\n2015.\n[39] G\u00a8unter Niemeyer, Carsten Preusche, Stefano Stramigi-\noli, and Dongjun Lee. Telerobotics. Springer handbook\nof robotics, pages 1085\u20131108, 2016.\n[40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas,\nand Michael J. Black.\nExpressive body capture: 3d\nhands, face, and body from a single image. In CVPR,\n2019.\n[41] Polina Ponomareva, Daria Trinitatova, Aleksey Fe-\ndoseev, Ivan Kalinov, and Dzmitry Tsetserukou. Gras-\nplook: a vr-based telemanipulation system with r-cnn-\ndriven augmentation of virtual environment. In 2021\n20th International Conference on Advanced Robotics\n(ICAR), pages 166\u2013171. IEEE, 2021.\n[42] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen\nJiang, Ruihan Yang, Yang Fu, and Xiaolong Wang.\nDexmv: Imitation learning for dexterous manipulation\nfrom human videos. arXiv preprint arXiv:2108.05877,\n2021.\n[43] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one\nhand to multiple hands: Imitation learning for dex-\nterous manipulation from single-camera teleoperation.\nRA-L, 7(4):10873\u201310881, 2022.\n[44] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su,\nand Xiaolong Wang.\nDexpoint: Generalizable point\ncloud reinforcement learning for sim-to-real dexterous\nmanipulation. In Conference on Robot Learning, pages\n594\u2013605. PMLR, 2023.\n[45] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter\nAbbeel, Jitendra Malik, and Trevor Darrell. Real-world\nrobot learning with masked visual pre-training. arXiv\npreprint arXiv:2210.03109, 2022.\n[46] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta,\nGiulia Vezzani, John Schulman, Emanuel Todorov, and\nSergey Levine. Learning complex dexterous manipula-\ntion with deep reinforcement learning and demonstra-\ntions. In RSS, 2018.\n[47] Daniel Rakita, Bilge Mutlu, and Michael Gleicher. A\nmotion retargeting method for effective mimicry-based\nteleoperation of robot arms. In Proceedings of the 2017\nACM/IEEE International Conference on Human-Robot\nInteraction, pages 361\u2013370, 2017.\n[48] Daniel Rakita, Bilge Mutlu, and Michael Gleicher.\nRemote telemanipulation with adapting viewpoints in\nvisually complex environments. Robotics: Science and\nSystems XV, 2019.\n[49] Nathan D Ratliff, Jan Issac, Daniel Kappler, Stan\nBirchfield, and Dieter Fox. Riemannian motion poli-\ncies. arXiv preprint arXiv:1801.02854, 2018.\n[50] Yu\nRong,\nTakaaki\nShiratori,\nand\nHanbyul\nJoo.\nFrankmocap: Fast monocular 3d hand and body motion\ncapture by regression and integration. arXiv preprint\narXiv:2008.08324, 2020.\n[51] Eric Rosen, David Whitney, Michael Fishman, Daniel\nUllman, and Stefanie Tellex.\nMixed reality as a\nbidirectional communication interface for human-robot\ninteraction. In 2020 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages\n11431\u201311438. IEEE, 2020.\n[52] M Salvato, Negin Heravi, Allison M Okamura, and\nJeannette Bohg.\nPredicting hand-object interaction\nfor improved haptic feedback in mixed reality. IEEE\nRobotics and Automation Letters, 7(2):3851\u20133857,\n2022.\n[53] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak.\nVideodex: Learning dexterity from internet videos.\narXiv preprint arXiv:2212.04498, 2022.\n[54] Aravind\nSivakumar,\nKenneth\nShaw,\nand\nDeepak\nPathak. Robotic telekinesis: learning a robotic hand im-\nitator by watching humans on youtube. arXiv preprint\narXiv:2202.10448, 2022.\n[55] Hyoung Il Son, Antonio Franchi, Lewis L Chuang,\nJunsuk Kim, Heinrich H Bulthoff, and Paolo Robuffo\nGiordano. Human-centered design and evaluation of\nhaptic cueing for teleoperation of multiple mobile\nrobots. IEEE transactions on cybernetics, 43(2):597\u2013\n609, 2013.\n[56] Balakumar Sundaralingam, Siva Hari, Adam Fishman,\nCaelan Garrett, Karl Van Wyk, Valts Blukis, Alexan-\nder Millane, Helen Oleynikova, Ankur Handa, Fabio\nRamos, Nathan Ratliff, and Dieter Fox.\nCuRobo:\nParallelized collision-free robot motion generation. In\nProceedings of the IEEE International Conference on\nRobotics and Automation (ICRA), June 2023.\n[57] Christian Theobalt, Irene Albrecht, J\u00a8org Haber, Marcus\nMagnor, and Hans-Peter Seidel. Pitching a baseball:\ntracking high-speed motion with multi-exposure im-\nages. In ACM SIGGRAPH 2004 Papers, pages 540\u2013\n547. 2004.\n[58] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco:\nA physics engine for model-based control. In IROS,\n2012.\n[59] Albert Tung, Josiah Wong, Ajay Mandlekar, Roberto\nMart\u00b4\u0131n-Mart\u00b4\u0131n, Yuke Zhu, Li Fei-Fei, and Silvio\nSavarese.\nLearning multi-arm manipulation through\ncollaborative teleoperation. In 2021 IEEE International\nConference on Robotics and Automation (ICRA), pages\n9212\u20139219. IEEE, 2021.\n[60] Robert Y Wang and Jovan Popovi\u00b4c. Real-time hand-\ntracking with a color glove.\nACM transactions on\ngraphics (TOG), 28(3):1\u20138, 2009.\n[61] Dong Wei, Bidan Huang, and Qiang Li. Multi-view\nmerging for robot teleoperation with virtual reality.\nIEEE Robotics and Automation Letters, 6(4):8537\u2013\n8544, 2021.\n[62] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia,\nHao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang,\nYifu Yuan, He Wang, et al. Sapien: A simulated part-\nbased interactive environment. In CVPR, 2020.\n[63] Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe\nQin, and Xiaolong Wang. Learning continuous grasp-\ning function with a dexterous hand from human demon-\nstrations. IEEE Robotics and Automation Letters, 8(5):\n2882\u20132889, 2023.\n[64] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov,\nAndrei Tkachenka, George Sung, Chuo-Ling Chang,\nand Matthias Grundmann.\nMediapipe hands: On-\ndevice\nreal-time\nhand\ntracking.\narXiv\npreprint\narXiv:2006.10214, 2020.\n[65] Haodong Zhang, Weijie Li, Yuwei Liang, Zexi Chen,\nYuxiang Cui, Yue Wang, and Rong Xiong. Human-\nrobot motion retargeting via neural latent optimization.\nCoRR, 2021.\n[66] Heng Zhang, Zeming Zhao, Yang Yu, Kai Gui, Xinjun\nSheng, and Xiangyang Zhu.\nA feasibility study on\nan intuitive teleoperation system combining imu with\nsemg sensors. In Intelligent Robotics and Applications:\n11th International Conference, ICIRA 2018, Newcastle,\nNSW, Australia, August 9\u201311, 2018, Proceedings, Part\nI 11, pages 465\u2013474. Springer, 2018.\n[67] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee,\nXi Chen, Ken Goldberg, and Pieter Abbeel.\nDeep\nimitation learning for complex manipulation tasks from\nvirtual reality teleoperation. In ICRA, 2018.\n[68] Wenping Zhao, Jinxiang Chai, and Ying-Qing Xu.\nCombining marker-based mocap and rgb-d camera for\nacquiring high-fidelity hand motion data. In Proceed-\nings of the ACM SIGGRAPH/eurographics symposium\non computer animation, pages 33\u201342, 2012.\nAPPENDIX\nA. Supplementary Overview\nThis supplementary material provides more details, results\nand visualizations accompanying the main paper, including\n\u2022 More details and visualization about teleoperation\nserver, including detection and retargeting modules;\n\u2022 More details about web-based teleoperation viewer;\n\u2022 Additional experimental results on system evaluation.\nMore visualization can be found at our project page:\nhttp://anyteleop.com.\nB. Teleoperation Server\nIn this section, we will show more intermediate results\nfrom our system, including visualization of hand pose de-\ntection results and the retargeting results of various robot\nhands.\nVisualization of Hand Pose Detection We visualize\nthe hand pose detection results in Figure 7. We showcase\nfive typical cases, which include: (i) a hand spreading out\nthe fingers for teleoperation initialization, (ii) fingers facing\ndownwards in preparation for a top-down grasp, (iii) a\nprecision grasp using the thumb and index finger, (iv) a\npower grasp using all five fingers, and (v) a failure case\nwhere the hand is positioned vertically relative to the camera\nplane.\nVisualization of Hand Pose Retargeting We demonstrate\nthe results of hand pose retargeting in Figure 10. The figure\ndisplays seven gestures being performed using four different\ndexterous hands.\nC. Web-based Teleoperation Viewer\nIn this section, we demonstrate how the web-based vi-\nsualizer provides accessibility and multi-view support for\nteleoperation through its lightweight rendering and capabil-\nity to run in multiple browser windows. Figure 8 shows\nscreenshots of the web-based visualizer when it is used to\nvisualize the five IsaacGym tasks depicted in the Figure 1\nin the main paper.\nLightweight Rendering vs High Visual Quality. The\ndesign of our web-based viewer prioritizes accessibility and\nconvenience, as it can be used on any device with a browser\nand provides minimal but sufficient rendering capabilities for\nteleoperation. Although the rendering quality may not be as\nadvanced as the original simulator viewer, simulation states\ncan be saved for offline rendering to produce high-quality\nvisual data. For example, in visual reinforcement learning\ntasks using RGB images as inputs, the rendered data can be\ngenerated using a more powerful engine such as a ray tracer\nafter teleoperation is completed.\nMulti-View Support for Teleoperation. In teleoperation,\nhuman operators often require a clear understanding of the\nspatial relationships between objects and robots to make in-\nformed decisions. This information can be provided through\nmulti-view rendering, which is a widely used technique in\nCamera Configuration\nCompletion Time\nError Percentage (%)\nSingle RGB\n109s\n28.1%\nSingle RGB-D\n87s\n21.8%\nTwo RGB-D\n74s\n12.5%\nTABLE V: Comparison of Camera Configurations. We\nevaluate the teleoperation performance on the Play Piano\ntask with different camera configurations.\nprevious teleoperation works [26, 43, 61]. Our web-based\nviewer offers multi-view support to the operator by simply\nopening multiple browser windows. As shown in Figure 9,\nan example of the operator using two views to perform a\nmanipulation task is displayed. The operator is able to open\nas many windows as needed to enhance their teleoperation\nexperience.\nIn this section, we examine the impact of different\ncamera configurations on the teleoperation performance of\nour system, AnyTeleop , which is capable of supporting\ndiverse configurations including RGB, RGB-D, and single or\nmultiple cameras. Even with a minimal configuration, i.e. a\nsingle RGB camera, the system can still perform effectively.\nAdditionally, by adding more resources, such as multiple\ncameras, our system can achieve better performance.\nWe use the Play Piano task implemented in Isaac-\nGym [32] as the evaluation scenario, which requires the\nrobot hand to press piano keys in a specific order. The task\nis shown in the Figure 1 in the main paper and the video in\nthe supplementary material. To quantify performance, we\nintroduce two task metrics: (i) completion time, i.e. the\nelapsed time from start to finish, and (ii) the percentage\nof incorrect key presses, which measures the number of\nincorrectly pressed keys relative to the total number of keys.\nA trained operator performs the task ten times for each\ncamera configuration. As reported in Table V, with addi-\ntional information, such as depth, and increasing number of\ncameras, the task can be completed faster and with fewer\nerrors, which demonstrates that our system allows users to\neasily trade-off between efficiency and system cost based on\ntheir use case.\nFig. 7: Hand Pose Detection Visualization. This figure visualizes the hand detection results, with the white bounding box\nhighlighting the predicted area and red points marking the identified finger key points. The hand skeleton is represented by\nthe grey lines connecting the key points. Additionally, the small grey points depict the 2D projection of 3D vertices from\nthe SMPL-X hand model. The figure showcases five diverse cases, from left to right: (i) a hand spreading out the fingers\nto initiate teleoperation, (ii) fingers facing downwards in preparation for a top-down grasp, (iii) a precision grasp using the\nthumb and index finger, (iv) a power grasp using all five fingers, and (v) a failure scenario where the hand is positioned\nvertically relative to the camera plane.\nFig. 8: Web-based Viewer. The visualization of teleoperation process can be perform in the web-based viewer, which\nfeatures the five tasks from the IsaacGym tasks shown in the Figure 1 of the main paper. The viewer utilizes the three.js\nlibrary for real-time rendering through a web browser.\nFig. 9: Multi-view Support with More Browser Windows. The web-based viewer offers multiple views to enhance the\noperator\u2019s understanding of the 3D object relationships. Additional views can be accessed by simply opening more browser\nwindows.\nHand \nPose\nSVH\nHand\nShadow\nHand\nDLR\nHand\nAllegro\nHand\nFig. 10: Visualization of Hand Pose Retargeting. The figure presents the results of hand pose retargeting for seven\ngestures and four different dexterous robot hands. The four hands are displayed in order from left to right: (i) Schunk SVH\nhand; (ii) Shadow Hand; (iii) DLR Hand; (iv) Allegro Hand.\n"
  }
]